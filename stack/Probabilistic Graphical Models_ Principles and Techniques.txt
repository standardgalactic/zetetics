
Probabilistic Graphical Models

Adaptive Computation and Machine Learning
Thomas Dietterich, Editor
Christopher Bishop, David Heckerman, Michael Jordan, and Michael Kearns, Associate Editors
Bioinformatics: The Machine Learning Approach, Pierre Baldi and Søren Brunak
Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto
Graphical Models for Machine Learning and Digital Communication, Brendan J. Frey
Learning in Graphical Models, Michael I. Jordan
Causation, Prediction, and Search, 2nd ed., Peter Spirtes, Clark Glymour, and Richard Scheines
Principles of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth
Bioinformatics: The Machine Learning Approach, 2nd ed., Pierre Baldi and Søren Brunak
Learning Kernel Classiﬁers: Theory and Algorithms, Ralf Herbrich
Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, Bern-
hard Schölkopf and Alexander J. Smola
Introduction to Machine Learning, Ethem Alpaydin
Gaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K. I. Williams
Semi-Supervised Learning, Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien, eds.
The Minimum Description Length Principle, Peter D. Grünwald
Introduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, eds.
Probabilistic Graphical Models: Principles and Techniques, Daphne Koller and Nir Friedman

Probabilistic Graphical Models
Principles and Techniques
Daphne Koller
Nir Friedman
The MIT Press
Cambridge, Massachusetts
London, England

©2009 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form by any electronic
or mechanical means (including photocopying, recording, or information storage and retrieval)
without permission in writing from the publisher.
For information about special quantity discounts, please email special_sales@mitpress.mit.edu
This book was set by the authors in LATEX2￿.
Printed and bound in the United States of America.
Library of Congress Cataloging-in-Publication Data
Koller, Daphne.
Probabilistic Graphical Models: Principles and Techniques / Daphne Koller and Nir Friedman.
p. cm. – (Adaptive computation and machine learning)
Includes bibliographical references and index.
ISBN 978-0-262-01319-2 (hardcover : alk. paper)
1. Graphical modeling (Statistics) 2. Bayesian statistical decision theory—Graphic methods. I.
Koller, Daphne. II. Friedman, Nir.
QA279.5.K65 2010
519.5’420285–dc22
2009008615
10
9
8
7
6
5

To our families
my parents Dov and Ditza
my husband Dan
my daughters Natalie and Maya
D.K.
my parents Noga and Gad
my wife Yael
my children Roy and Lior
N.F.


As far as the laws of mathematics refer to reality, they are not certain, as far as they are
certain, they do not refer to reality.
Albert Einstein, 1921
When we try to pick out anything by itself, we ﬁnd that it is bound fast by a thousand
invisible cords that cannot be broken, to everything in the universe.
John Muir, 1869
The actual science of logic is conversant at present only with things either certain, impossible,
or entirely doubtful . . . Therefore the true logic for this world is the calculus of probabilities,
which takes account of the magnitude of the probability which is, or ought to be, in a
reasonable man’s mind.
James Clerk Maxwell, 1850
The theory of probabilities is at bottom nothing but common sense reduced to calculus; it
enables us to appreciate with exactness that which accurate minds feel with a sort of instinct
for which ofttimes they are unable to account.
Pierre Simon Laplace, 1819
Misunderstanding of probability may be the greatest of all impediments to scientiﬁc literacy.
Stephen Jay Gould


Contents
Acknowledgments
xxiii
List of Figures
xxv
List of Algorithms
xxxi
List of Boxes
xxxiii
1
Introduction
1
1.1
Motivation
1
1.2
Structured Probabilistic Models
2
1.2.1
Probabilistic Graphical Models
3
1.2.2
Representation, Inference, Learning
5
1.3
Overview and Roadmap
6
1.3.1
Overview of Chapters
6
1.3.2
Reader’s Guide
9
1.3.3
Connection to Other Disciplines
11
1.4
Historical Notes
12
2
Foundations
15
2.1
Probability Theory
15
2.1.1
Probability Distributions
15
2.1.2
Basic Concepts in Probability
18
2.1.3
Random Variables and Joint Distributions
19
2.1.4
Independence and Conditional Independence
23
2.1.5
Querying a Distribution
25
2.1.6
Continuous Spaces
27
2.1.7
Expectation and Variance
31
2.2
Graphs
34
2.2.1
Nodes and Edges
34
2.2.2
Subgraphs
35
2.2.3
Paths and Trails
36

x
CONTENTS
2.2.4
Cycles and Loops
36
2.3
Relevant Literature
39
2.4
Exercises
39
I
Representation
43
3
The Bayesian Network Representation
45
3.1
Exploiting Independence Properties
45
3.1.1
Independent Random Variables
45
3.1.2
The Conditional Parameterization
46
3.1.3
The Naive Bayes Model
48
3.2
Bayesian Networks
51
3.2.1
The Student Example Revisited
52
3.2.2
Basic Independencies in Bayesian Networks
56
3.2.3
Graphs and Distributions
60
3.3
Independencies in Graphs
68
3.3.1
D-separation
69
3.3.2
Soundness and Completeness
72
3.3.3
An Algorithm for d-Separation
74
3.3.4
I-Equivalence
76
3.4
From Distributions to Graphs
78
3.4.1
Minimal I-Maps
79
3.4.2
Perfect Maps
81
3.4.3
Finding Perfect Maps ⋆
83
3.5
Summary
92
3.6
Relevant Literature
93
3.7
Exercises
96
4
Undirected Graphical Models
103
4.1
The Misconception Example
103
4.2
Parameterization
106
4.2.1
Factors
106
4.2.2
Gibbs Distributions and Markov Networks
108
4.2.3
Reduced Markov Networks
110
4.3
Markov Network Independencies
114
4.3.1
Basic Independencies
114
4.3.2
Independencies Revisited
117
4.3.3
From Distributions to Graphs
120
4.4
Parameterization Revisited
122
4.4.1
Finer-Grained Parameterization
123
4.4.2
Overparameterization
128
4.5
Bayesian Networks and Markov Networks
134
4.5.1
From Bayesian Networks to Markov Networks
134
4.5.2
From Markov Networks to Bayesian Networks
137

CONTENTS
xi
4.5.3
Chordal Graphs
139
4.6
Partially Directed Models
142
4.6.1
Conditional Random Fields
142
4.6.2
Chain Graph Models ⋆
148
4.7
Summary and Discussion
151
4.8
Relevant Literature
152
4.9
Exercises
153
5
Local Probabilistic Models
157
5.1
Tabular CPDs
157
5.2
Deterministic CPDs
158
5.2.1
Representation
158
5.2.2
Independencies
159
5.3
Context-Speciﬁc CPDs
162
5.3.1
Representation
162
5.3.2
Independencies
171
5.4
Independence of Causal Inﬂuence
175
5.4.1
The Noisy-Or Model
175
5.4.2
Generalized Linear Models
178
5.4.3
The General Formulation
182
5.4.4
Independencies
184
5.5
Continuous Variables
185
5.5.1
Hybrid Models
189
5.6
Conditional Bayesian Networks
191
5.7
Summary
193
5.8
Relevant Literature
194
5.9
Exercises
195
6
Template-Based Representations
199
6.1
Introduction
199
6.2
Temporal Models
200
6.2.1
Basic Assumptions
201
6.2.2
Dynamic Bayesian Networks
202
6.2.3
State-Observation Models
207
6.3
Template Variables and Template Factors
212
6.4
Directed Probabilistic Models for Object-Relational Domains
216
6.4.1
Plate Models
216
6.4.2
Probabilistic Relational Models
222
6.5
Undirected Representation
228
6.6
Structural Uncertainty ⋆
232
6.6.1
Relational Uncertainty
233
6.6.2
Object Uncertainty
235
6.7
Summary
240
6.8
Relevant Literature
242
6.9
Exercises
243

xii
CONTENTS
7
Gaussian Network Models
247
7.1
Multivariate Gaussians
247
7.1.1
Basic Parameterization
247
7.1.2
Operations on Gaussians
249
7.1.3
Independencies in Gaussians
250
7.2
Gaussian Bayesian Networks
251
7.3
Gaussian Markov Random Fields
254
7.4
Summary
257
7.5
Relevant Literature
258
7.6
Exercises
258
8
The Exponential Family
261
8.1
Introduction
261
8.2
Exponential Families
261
8.2.1
Linear Exponential Families
263
8.3
Factored Exponential Families
266
8.3.1
Product Distributions
266
8.3.2
Bayesian Networks
267
8.4
Entropy and Relative Entropy
269
8.4.1
Entropy
269
8.4.2
Relative Entropy
272
8.5
Projections
273
8.5.1
Comparison
274
8.5.2
M-Projections
277
8.5.3
I-Projections
282
8.6
Summary
282
8.7
Relevant Literature
283
8.8
Exercises
283
II
Inference
285
9
Exact Inference: Variable Elimination
287
9.1
Analysis of Complexity
288
9.1.1
Analysis of Exact Inference
288
9.1.2
Analysis of Approximate Inference
290
9.2
Variable Elimination: The Basic Ideas
292
9.3
Variable Elimination
296
9.3.1
Basic Elimination
297
9.3.2
Dealing with Evidence
303
9.4
Complexity and Graph Structure: Variable Elimination
305
9.4.1
Simple Analysis
306
9.4.2
Graph-Theoretic Analysis
306
9.4.3
Finding Elimination Orderings ￿
310
9.5
Conditioning ￿
315

CONTENTS
xiii
9.5.1
The Conditioning Algorithm
315
9.5.2
Conditioning and Variable Elimination
318
9.5.3
Graph-Theoretic Analysis
322
9.5.4
Improved Conditioning
323
9.6
Inference with Structured CPDs ￿
325
9.6.1
Independence of Causal Inﬂuence
325
9.6.2
Context-Speciﬁc Independence
329
9.6.3
Discussion
335
9.7
Summary and Discussion
336
9.8
Relevant Literature
337
9.9
Exercises
338
10 Exact Inference: Clique Trees
345
10.1
Variable Elimination and Clique Trees
345
10.1.1
Cluster Graphs
346
10.1.2
Clique Trees
346
10.2
Message Passing: Sum Product
348
10.2.1
Variable Elimination in a Clique Tree
349
10.2.2
Clique Tree Calibration
355
10.2.3
A Calibrated Clique Tree as a Distribution
361
10.3
Message Passing: Belief Update
364
10.3.1
Message Passing with Division
364
10.3.2
Equivalence of Sum-Product and Belief Update Messages
368
10.3.3
Answering Queries
369
10.4
Constructing a Clique Tree
372
10.4.1
Clique Trees from Variable Elimination
372
10.4.2
Clique Trees from Chordal Graphs
374
10.5
Summary
376
10.6
Relevant Literature
377
10.7
Exercises
378
11
Inference as Optimization
381
11.1
Introduction
381
11.1.1
Exact Inference Revisited ￿
382
11.1.2
The Energy Functional
384
11.1.3
Optimizing the Energy Functional
386
11.2
Exact Inference as Optimization
386
11.2.1
Fixed-Point Characterization
388
11.2.2
Inference as Optimization
390
11.3
Propagation-Based Approximation
391
11.3.1
A Simple Example
391
11.3.2
Cluster-Graph Belief Propagation
396
11.3.3
Properties of Cluster-Graph Belief Propagation
399
11.3.4
Analyzing Convergence ￿
401
11.3.5
Constructing Cluster Graphs
404

xiv
CONTENTS
11.3.6
Variational Analysis
411
11.3.7
Other Entropy Approximations ⋆
414
11.3.8
Discussion
428
11.4
Propagation with Approximate Messages ⋆
430
11.4.1
Factorized Messages
431
11.4.2
Approximate Message Computation
433
11.4.3
Inference with Approximate Messages
436
11.4.4
Expectation Propagation
442
11.4.5
Variational Analysis
445
11.4.6
Discussion
448
11.5
Structured Variational Approximations
448
11.5.1
The Mean Field Approximation
449
11.5.2
Structured Approximations
456
11.5.3
Local Variational Methods ⋆
469
11.6
Summary and Discussion
473
11.7
Relevant Literature
475
11.8
Exercises
477
12 Particle-Based Approximate Inference
487
12.1
Forward Sampling
488
12.1.1
Sampling from a Bayesian Network
488
12.1.2
Analysis of Error
490
12.1.3
Conditional Probability Queries
491
12.2
Likelihood Weighting and Importance Sampling
492
12.2.1
Likelihood Weighting: Intuition
492
12.2.2
Importance Sampling
494
12.2.3
Importance Sampling for Bayesian Networks
498
12.2.4
Importance Sampling Revisited
504
12.3
Markov Chain Monte Carlo Methods
505
12.3.1
Gibbs Sampling Algorithm
505
12.3.2
Markov Chains
507
12.3.3
Gibbs Sampling Revisited
512
12.3.4
A Broader Class of Markov Chains ⋆
515
12.3.5
Using a Markov Chain
518
12.4
Collapsed Particles
526
12.4.1
Collapsed Likelihood Weighting ⋆
527
12.4.2
Collapsed MCMC
531
12.5
Deterministic Search Methods ⋆
536
12.6
Summary
540
12.7
Relevant Literature
541
12.8
Exercises
544
13 MAP Inference
551
13.1
Overview
551
13.1.1
Computational Complexity
551

CONTENTS
xv
13.1.2
Overview of Solution Methods
552
13.2
Variable Elimination for (Marginal) MAP
554
13.2.1
Max-Product Variable Elimination
554
13.2.2
Finding the Most Probable Assignment
556
13.2.3
Variable Elimination for Marginal MAP ⋆
559
13.3
Max-Product in Clique Trees
562
13.3.1
Computing Max-Marginals
562
13.3.2
Message Passing as Reparameterization
564
13.3.3
Decoding Max-Marginals
565
13.4
Max-Product Belief Propagation in Loopy Cluster Graphs
567
13.4.1
Standard Max-Product Message Passing
567
13.4.2
Max-Product BP with Counting Numbers ⋆
572
13.4.3
Discussion
575
13.5
MAP as a Linear Optimization Problem ⋆
577
13.5.1
The Integer Program Formulation
577
13.5.2
Linear Programming Relaxation
579
13.5.3
Low-Temperature Limits
581
13.6
Using Graph Cuts for MAP
588
13.6.1
Inference Using Graph Cuts
588
13.6.2
Nonbinary Variables
592
13.7
Local Search Algorithms ⋆
595
13.8
Summary
597
13.9
Relevant Literature
598
13.10
Exercises
601
14 Inference in Hybrid Networks
605
14.1
Introduction
605
14.1.1
Challenges
605
14.1.2
Discretization
606
14.1.3
Overview
607
14.2
Variable Elimination in Gaussian Networks
608
14.2.1
Canonical Forms
609
14.2.2
Sum-Product Algorithms
611
14.2.3
Gaussian Belief Propagation
612
14.3
Hybrid Networks
615
14.3.1
The Diﬃculties
615
14.3.2
Factor Operations for Hybrid Gaussian Networks
618
14.3.3
EP for CLG Networks
621
14.3.4
An “Exact” CLG Algorithm ⋆
626
14.4
Nonlinear Dependencies
630
14.4.1
Linearization
631
14.4.2
Expectation Propagation with Gaussian Approximation
637
14.5
Particle-Based Approximation Methods
642
14.5.1
Sampling in Continuous Spaces
642
14.5.2
Forward Sampling in Bayesian Networks
643

xvi
CONTENTS
14.5.3
MCMC Methods
644
14.5.4
Collapsed Particles
645
14.5.5
Nonparametric Message Passing
646
14.6
Summary and Discussion
646
14.7
Relevant Literature
647
14.8
Exercises
649
15 Inference in Temporal Models
651
15.1
Inference Tasks
652
15.2
Exact Inference
653
15.2.1
Filtering in State-Observation Models
653
15.2.2
Filtering as Clique Tree Propagation
654
15.2.3
Clique Tree Inference in DBNs
655
15.2.4
Entanglement
656
15.3
Approximate Inference
661
15.3.1
Key Ideas
661
15.3.2
Factored Belief State Methods
663
15.3.3
Particle Filtering
665
15.3.4
Deterministic Search Techniques
675
15.4
Hybrid DBNs
675
15.4.1
Continuous Models
676
15.4.2
Hybrid Models
683
15.5
Summary
688
15.6
Relevant Literature
690
15.7
Exercises
692
III
Learning
695
16 Learning Graphical Models: Overview
697
16.1
Motivation
697
16.2
Goals of Learning
698
16.2.1
Density Estimation
698
16.2.2
Speciﬁc Prediction Tasks
700
16.2.3
Knowledge Discovery
701
16.3
Learning as Optimization
702
16.3.1
Empirical Risk and Overﬁtting
703
16.3.2
Discriminative versus Generative Training
709
16.4
Learning Tasks
711
16.4.1
Model Constraints
712
16.4.2
Data Observability
712
16.4.3
Taxonomy of Learning Tasks
714
16.5
Relevant Literature
715
17 Parameter Estimation
717
17.1
Maximum Likelihood Estimation
717

CONTENTS
xvii
17.1.1
The Thumbtack Example
717
17.1.2
The Maximum Likelihood Principle
720
17.2
MLE for Bayesian Networks
722
17.2.1
A Simple Example
723
17.2.2
Global Likelihood Decomposition
724
17.2.3
Table-CPDs
725
17.2.4
Gaussian Bayesian Networks ⋆
728
17.2.5
Maximum Likelihood Estimation as M-Projection ⋆
731
17.3
Bayesian Parameter Estimation
733
17.3.1
The Thumbtack Example Revisited
733
17.3.2
Priors and Posteriors
737
17.4
Bayesian Parameter Estimation in Bayesian Networks
741
17.4.1
Parameter Independence and Global Decomposition
742
17.4.2
Local Decomposition
746
17.4.3
Priors for Bayesian Network Learning
748
17.4.4
MAP Estimation ⋆
751
17.5
Learning Models with Shared Parameters
754
17.5.1
Global Parameter Sharing
755
17.5.2
Local Parameter Sharing
760
17.5.3
Bayesian Inference with Shared Parameters
762
17.5.4
Hierarchical Priors ⋆
763
17.6
Generalization Analysis ⋆
769
17.6.1
Asymptotic Analysis
769
17.6.2
PAC-Bounds
770
17.7
Summary
776
17.8
Relevant Literature
777
17.9
Exercises
778
18 Structure Learning in Bayesian Networks
783
18.1
Introduction
783
18.1.1
Problem Deﬁnition
783
18.1.2
Overview of Methods
785
18.2
Constraint-Based Approaches
786
18.2.1
General Framework
786
18.2.2
Independence Tests
787
18.3
Structure Scores
790
18.3.1
Likelihood Scores
791
18.3.2
Bayesian Score
794
18.3.3
Marginal Likelihood for a Single Variable
797
18.3.4
Bayesian Score for Bayesian Networks
799
18.3.5
Understanding the Bayesian Score
801
18.3.6
Priors
804
18.3.7
Score Equivalence ⋆
807
18.4
Structure Search
807
18.4.1
Learning Tree-Structured Networks
808

xviii
CONTENTS
18.4.2
Known Order
809
18.4.3
General Graphs
811
18.4.4
Learning with Equivalence Classes ⋆
821
18.5
Bayesian Model Averaging ⋆
824
18.5.1
Basic Theory
824
18.5.2
Model Averaging Given an Order
826
18.5.3
The General Case
828
18.6
Learning Models with Additional Structure
832
18.6.1
Learning with Local Structure
833
18.6.2
Learning Template Models
837
18.7
Summary and Discussion
838
18.8
Relevant Literature
840
18.9
Exercises
843
19 Partially Observed Data
849
19.1
Foundations
849
19.1.1
Likelihood of Data and Observation Models
849
19.1.2
Decoupling of Observation Mechanism
853
19.1.3
The Likelihood Function
856
19.1.4
Identiﬁability
860
19.2
Parameter Estimation
862
19.2.1
Gradient Ascent
863
19.2.2
Expectation Maximization (EM)
868
19.2.3
Comparison: Gradient Ascent versus EM
887
19.2.4
Approximate Inference ⋆
893
19.3
Bayesian Learning with Incomplete Data ⋆
897
19.3.1
Overview
897
19.3.2
MCMC Sampling
899
19.3.3
Variational Bayesian Learning
904
19.4
Structure Learning
908
19.4.1
Scoring Structures
909
19.4.2
Structure Search
917
19.4.3
Structural EM
920
19.5
Learning Models with Hidden Variables
925
19.5.1
Information Content of Hidden Variables
926
19.5.2
Determining the Cardinality
928
19.5.3
Introducing Hidden Variables
930
19.6
Summary
933
19.7
Relevant Literature
934
19.8
Exercises
935
20 Learning Undirected Models
943
20.1
Overview
943
20.2
The Likelihood Function
944
20.2.1
An Example
944

CONTENTS
xix
20.2.2
Form of the Likelihood Function
946
20.2.3
Properties of the Likelihood Function
947
20.3
Maximum (Conditional) Likelihood Parameter Estimation
949
20.3.1
Maximum Likelihood Estimation
949
20.3.2
Conditionally Trained Models
950
20.3.3
Learning with Missing Data
954
20.3.4
Maximum Entropy and Maximum Likelihood ⋆
956
20.4
Parameter Priors and Regularization
958
20.4.1
Local Priors
958
20.4.2
Global Priors
961
20.5
Learning with Approximate Inference
961
20.5.1
Belief Propagation
962
20.5.2
MAP-Based Learning ⋆
967
20.6
Alternative Objectives
969
20.6.1
Pseudolikelihood and Its Generalizations
970
20.6.2
Contrastive Optimization Criteria
974
20.7
Structure Learning
978
20.7.1
Structure Learning Using Independence Tests
979
20.7.2
Score-Based Learning: Hypothesis Spaces
981
20.7.3
Objective Functions
982
20.7.4
Optimization Task
985
20.7.5
Evaluating Changes to the Model
992
20.8
Summary
996
20.9
Relevant Literature
998
20.10 Exercises
1001
IV
Actions and Decisions
1007
21 Causality
1009
21.1
Motivation and Overview
1009
21.1.1
Conditioning and Intervention
1009
21.1.2
Correlation and Causation
1012
21.2
Causal Models
1014
21.3
Structural Causal Identiﬁability
1017
21.3.1
Query Simpliﬁcation Rules
1017
21.3.2
Iterated Query Simpliﬁcation
1020
21.4
Mechanisms and Response Variables ⋆
1026
21.5
Partial Identiﬁability in Functional Causal Models ⋆
1031
21.6
Counterfactual Queries ⋆
1034
21.6.1
Twinned Networks
1034
21.6.2
Bounds on Counterfactual Queries
1037
21.7
Learning Causal Models
1040
21.7.1
Learning Causal Models without Confounding Factors
1041
21.7.2
Learning from Interventional Data
1044

xx
CONTENTS
21.7.3
Dealing with Latent Variables ⋆
1048
21.7.4
Learning Functional Causal Models ⋆
1051
21.8
Summary
1053
21.9
Relevant Literature
1054
21.10
Exercises
1055
22 Utilities and Decisions
1059
22.1
Foundations: Maximizing Expected Utility
1059
22.1.1
Decision Making Under Uncertainty
1059
22.1.2
Theoretical Justiﬁcation ⋆
1062
22.2
Utility Curves
1064
22.2.1
Utility of Money
1065
22.2.2
Attitudes Toward Risk
1066
22.2.3
Rationality
1067
22.3
Utility Elicitation
1068
22.3.1
Utility Elicitation Procedures
1068
22.3.2
Utility of Human Life
1069
22.4
Utilities of Complex Outcomes
1071
22.4.1
Preference and Utility Independence ⋆
1071
22.4.2
Additive Independence Properties
1074
22.5
Summary
1081
22.6
Relevant Literature
1082
22.7
Exercises
1084
23 Structured Decision Problems
1085
23.1
Decision Trees
1085
23.1.1
Representation
1085
23.1.2
Backward Induction Algorithm
1087
23.2
Inﬂuence Diagrams
1088
23.2.1
Basic Representation
1089
23.2.2
Decision Rules
1090
23.2.3
Time and Recall
1092
23.2.4
Semantics and Optimality Criterion
1093
23.3
Backward Induction in Inﬂuence Diagrams
1095
23.3.1
Decision Trees for Inﬂuence Diagrams
1096
23.3.2
Sum-Max-Sum Rule
1098
23.4
Computing Expected Utilities
1100
23.4.1
Simple Variable Elimination
1100
23.4.2
Multiple Utility Variables: Simple Approaches
1102
23.4.3
Generalized Variable Elimination ⋆
1103
23.5
Optimization in Inﬂuence Diagrams
1107
23.5.1
Optimizing a Single Decision Rule
1107
23.5.2
Iterated Optimization Algorithm
1108
23.5.3
Strategic Relevance and Global Optimality ⋆
1110
23.6
Ignoring Irrelevant Information ⋆
1119

CONTENTS
xxi
23.7
Value of Information
1121
23.7.1
Single Observations
1122
23.7.2
Multiple Observations
1124
23.8
Summary
1126
23.9
Relevant Literature
1127
23.10 Exercises
1130
24 Epilogue
1133
A
Background Material
1137
A.1
Information Theory
1137
A.1.1
Compression and Entropy
1137
A.1.2
Conditional Entropy and Information
1139
A.1.3
Relative Entropy and Distances Between Distributions
1140
A.2
Convergence Bounds
1143
A.2.1
Central Limit Theorem
1144
A.2.2
Convergence Bounds
1145
A.3
Algorithms and Algorithmic Complexity
1146
A.3.1
Basic Graph Algorithms
1146
A.3.2
Analysis of Algorithmic Complexity
1147
A.3.3
Dynamic Programming
1149
A.3.4
Complexity Theory
1150
A.4
Combinatorial Optimization and Search
1154
A.4.1
Optimization Problems
1154
A.4.2
Local Search
1154
A.4.3
Branch and Bound Search
1160
A.5
Continuous Optimization
1161
A.5.1
Characterizing Optima of a Continuous Function
1161
A.5.2
Gradient Ascent Methods
1163
A.5.3
Constrained Optimization
1167
A.5.4
Convex Duality
1171
Bibliography
1173
Notation Index
1211
Subject Index
1215


Acknowledgments
This book owes a considerable debt of gratitude to the many people who contributed to its
creation, and to those who have inﬂuenced our work and our thinking over the years.
First and foremost, we want to thank our students, who, by asking the right questions, and
forcing us to formulate clear and precise answers, were directly responsible for the inception of
this book and for any clarity of presentation.
We have been fortunate to share the same mentors, who have had a signiﬁcant impact on
our development as researchers and as teachers: Joe Halpern, Stuart Russell. Much of our core
views on probabilistic models have been inﬂuenced by Judea Pearl. Judea through his persuasive
writing and vivid presentations inspired us, and many other researchers of our generation, to
plunge into research in this ﬁeld.
There are many people whose conversations with us have helped us in thinking through
some of the more diﬃcult concepts in the book: Nando de Freitas, Gal Elidan, Dan Geiger,
Amir Globerson, Uri Lerner, Chris Meek, David Sontag, Yair Weiss, and Ramin Zabih. Others,
in conversations and collaborations over the year, have also inﬂuenced our thinking and the
presentation of the material: Pieter Abbeel, JeﬀBilmes, Craig Boutilier, Moises Goldszmidt,
Carlos Guestrin, David Heckerman, Eric Horvitz, Tommi Jaakkola, Michael Jordan, Kevin Murphy,
Andrew Ng, Ben Taskar, and Sebastian Thrun.
We especially want to acknowledge Gal Elidan for constant encouragement, valuable feedback,
and logistic support at many critical junctions, throughout the long years of writing this book.
Over the course of the years of work on this book, many people have contributed to it
by providing insights, engaging in enlightening discussions, and giving valuable feedback. It is
impossible to individually acknowledge all of the people who made such contributions. However,
we speciﬁcally wish to express our gratitude to those people who read large parts of the book
and gave detailed feedback: Rahul Biswas, James Cussens, James Diebel, Yoni Donner, Tal El-
Hay, Gal Elidan, Stanislav Funiak, Amir Globerson, Russ Greiner, Carlos Guestrin, Tim Heilman,
Geremy Heitz, Maureen Hillenmeyer, Ariel Jaimovich, Tommy Kaplan, Jonathan Laserson, Ken
Levine, Brian Milch, Kevin Murphy, Ben Packer, Ronald Parr, Dana Pe’er, and Christian Shelton.
We are deeply grateful to the following people, who contributed speciﬁc text and/or ﬁgures,
mostly to the case studies and concept boxes without which this book would be far less
interesting: Gal Elidan, to chapter 11, chapter 18, and chapter 19; Stephen Gould, to chapter 4
and chapter 13; Vladimir Jojic, to chapter 12; Jonathan Laserson, to chapter 19; Uri Lerner, to
chapter 14; Andrew McCallum and Charles Sutton, to chapter 4; Brian Milch, to chapter 6; Kevin

xxiv
Acknowledgments
Murphy, to chapter 15; and Benjamin Packer, to many of the exercises used throughout the book.
In addition, we are very grateful to Amir Globerson, David Sontag and Yair Weiss whose insights
on chapter 13 played a key role in the development of the material in that chapter.
Special thanks are due to Bob Prior at MIT Press who convinced us to go ahead with this
project and was constantly supportive, enthusiastic and patient in the face of the recurring
delays and missed deadlines. We thank Greg McNamee, our copy editor, and Mary Reilly, our
artist, for their help in improving this book considerably. We thank Chris Manning, for allowing
us to use his LATEX macros for typesetting this book, and for providing useful advice on how to
use them. And we thank Miles Davis for invaluable technical support.
We also wish to thank the many colleagues who used drafts of this book in teaching provided
enthusiastic feedback that encouraged us to continue this project at times where it seemed
unending. Sebastian Thrun deserves a special note of thanks, for forcing us to set a deadline for
completion of this book and to stick to it.
We also want to thank the past and present members of the DAGS group at Stanford, and the
Computational Biology group at the Hebrew University, many of whom also contributed ideas,
insights, and useful comments. We speciﬁcally want to thank them for bearing with us while
we devoted far too much of our time to working on this book.
Finally, noone deserves our thanks more than our long-suﬀering families — Natalie Anna
Koller Avida, Maya Rika Koller Avida, and Dan Avida; Lior, Roy, and Yael Friedman — for their
continued love, support, and patience, as they watched us work evenings and weekends to
complete this book. We could never have done this without you.

List of Figures
1.1
Diﬀerent perspectives on probabilistic graphical models
4
1.2
A reader’s guide to the structure and dependencies in this book
10
2.1
Example of a joint distribution P(Intelligence, Grade)
22
2.2
Example PDF of three Gaussian distributions
29
2.3
An example of a partially directed graph K
35
2.4
Induced graphs and their upward closure
35
2.5
An example of a polytree
38
3.1
Simple Bayesian networks for the student example
48
3.2
The Bayesian network graph for a naive Bayes model
50
3.3
The Bayesian Network graph for the Student example
52
3.4
Student Bayesian network Bstudent with CPDs
53
3.5
The four possible two-edge trails
70
3.6
A simple example for the d-separation algorithm
76
3.7
Skeletons and v-structures in a network
77
3.8
Three minimal I-maps for PBstudent, induced by diﬀerent orderings
80
3.9
Network for the OneLetter example
82
3.10
Attempted Bayesian network models for the Misconception example
83
3.11
Simple example of compelled edges in an equivalence class.
88
3.12
Rules for orienting edges in PDAG
89
3.13
More complex example of compelled edges in an equivalence class
90
3.14
A Bayesian network with qualitative inﬂuences
97
3.15
A simple network for a burglary alarm domain
98
3.16
Illustration of the concept of a self-contained set
101
4.1
Factors for the Misconception example
104
4.2
Joint distribution for the Misconception example
105
4.3
An example of factor product
107
4.4
The cliques in two simple Markov networks
109
4.5
An example of factor reduction
111
4.6
Markov networks for the factors in an extended Student example
112

xxvi
LIST OF FIGURES
4.7
An attempt at an I-map for a nonpositive distribution P
122
4.8
Diﬀerent factor graphs for the same Markov network
123
4.9
Energy functions for the Misconception example
124
4.10
Alternative but equivalent energy functions
128
4.11
Canonical energy function for the Misconception example
130
4.12
Example of alternative deﬁnition of d-separation based on Markov networks
137
4.13
Minimal I-map Bayesian networks for a nonchordal Markov network
138
4.14
Diﬀerent linear-chain graphical models
143
4.15
A chain graph K and its moralized version
149
4.16
Example for deﬁnition of c-separation in a chain graph
150
5.1
Example of a network with a deterministic CPD
160
5.2
A slightly more complex example with deterministic CPDs
161
5.3
The Student example augmented with a Job variable
162
5.4
A tree-CPD for P(J | A, S, L)
163
5.5
The OneLetter example of a multiplexer dependency
165
5.6
tree-CPD for a rule-based CPD
169
5.7
Example of removal of spurious edges
173
5.8
Two reduced CPDs for the OneLetter example
174
5.9
Decomposition of the noisy-or model for Letter
176
5.10
The behavior of the noisy-or model
177
5.11
The behavior of the sigmoid CPD
180
5.12
Example of the multinomial logistic CPD
181
5.13
Independence of causal inﬂuence
182
5.14
Generalized linear model for a thermostat
191
5.15
Example of encapsulated CPDs for a computer system model
193
6.1
A highly simpliﬁed DBN for monitoring a vehicle
203
6.2
HMM as a DBN
203
6.3
Two classes of DBNs constructed from HMMs
205
6.4
A simple 4-state HMM
208
6.5
One possible world for the University example
215
6.6
Plate model for a set of coin tosses sampled from a single coin
217
6.7
Plate models and ground Bayesian networks for a simpliﬁed Student example
219
6.8
Illustration of probabilistic interactions in the University domain
220
6.9
Examples of dependency graphs
227
7.1
Examples of 2-dimensional Gaussians
249
8.1
Example of M- and I-projections into the family of Gaussian distributions
275
8.2
Example of M- and I-projections for a discrete distribution
276
8.3
Relationship between parameters, distributions, and expected suﬃcient statistics
279
9.1
Network used to prove NP-hardness of exact inference
289
9.2
Computing P(D) by summing out the joint distribution
294
9.3
The ﬁrst transformation on the sum of ﬁgure 9.2
295

LIST OF FIGURES
xxvii
9.4
The second transformation on the sum of ﬁgure 9.2
295
9.5
The third transformation on the sum of ﬁgure 9.2
295
9.6
The fourth transformation on the sum of ﬁgure 9.2
295
9.7
Example of factor marginalization
297
9.8
The Extended-Student Bayesian network
300
9.9
Understanding intermediate factors in variable elimination
303
9.10
Variable elimination as graph transformation in the Student example
308
9.11
Induced graph and clique tree for the Student example
309
9.12
Networks where conditioning performs unnecessary computation
321
9.13
Induced graph for the Student example using both conditioning and elimination
323
9.14
Diﬀerent decompositions for a noisy-or CPD
326
9.15
Example Bayesian network with rule-based structure
329
9.16
Conditioning in a network with CSI
334
10.1
Cluster tree for the VE execution in table 9.1
346
10.2
Simpliﬁed clique tree T for the Extended Student network
349
10.3
Message propagations with diﬀerent root cliques in the Student clique tree
350
10.4
An abstract clique tree that is not chain-structured
352
10.5
Two steps in a downward pass in the Student network
356
10.6
Final beliefs for the Misconception example
362
10.7
An example of factor division
365
10.8
A modiﬁed Student BN with an unambitious student
373
10.9
A clique tree for the modiﬁed Student BN of ﬁgure 10.8
373
10.10
Example of clique tree construction algorithm
375
11.1
An example of a cluster graph versus a clique tree
391
11.2
An example run of loopy belief propagation
392
11.3
Two examples of generalized cluster graph for an MRF
393
11.4
An example of a 4 × 4 two-dimensional grid network
398
11.5
An example of generalized cluster graph for a 3 × 3 grid network
399
11.6
A generalized cluster graph for the 3 × 3 grid when viewed as pairwise MRF
405
11.7
Examples of generalized cluster graphs for network with potentials {A, B, C},
{B, C, D}, {B, D, F}, {B, E} and {D, E}
406
11.8
Examples of generalized cluster graphs for networks with potentials {A, B, C},
{B, C, D}, and {A, C, D}
407
11.9
An example of simple region graph
420
11.10
The region graph corresponding to the Bethe cluster graph of ﬁgure 11.7a
421
11.11
The messages participating in diﬀerent region graph computations
425
11.12
A cluster for a 4 × 4 grid network
430
11.13
Eﬀect of diﬀerent message factorizations on the beliefs in the receiving factor
431
11.14
Example of propagation in cluster tree with factorized messages
433
11.15
Markov network used to demonstrate approximate message passing
438
11.16
An example of a multimodal mean ﬁeld energy functional landscape
456
11.17
Two structures for variational approximation of a 4 × 4 grid network
457
11.18
A diamond network and three possible approximating structures
462

xxviii
LIST OF FIGURES
11.19
Simpliﬁcation of approximating structure in cluster mean ﬁeld
468
11.20
Illustration of the variational bound −ln(x) ≥−λx + ln(λ) + 1
469
12.1
The Student network Bstudent revisited
488
12.2
The mutilated network Bstudent
I=i1,G=g2 used for likelihood weighting
499
12.3
The Grasshopper Markov chain
507
12.4
A simple Markov chain
509
12.5
A Bayesian network with four students, two courses, and ﬁve grades
514
12.6
Visualization of a Markov chain with low conductance
520
12.7
Networks illustrating collapsed importance sampling
528
13.1
Example of the max-marginalization factor operation for variable B
555
13.2
A network where a marginal MAP query requires exponential time
561
13.3
The max-marginals for the Misconception example
564
13.4
Two induced subgraphs derived from ﬁgure 11.3a
570
13.5
Example graph construction for applying min-cut to the binary MAP problem
590
14.1
Gaussian MRF illustrating convergence properties of Gaussian belief propagation
615
14.2
CLG network used to demonstrate hardness of inference
615
14.3
Joint marginal distribution p(X1, X2) for a network as in ﬁgure 14.2
616
14.4
Summing and collapsing a Gaussian mixture
619
14.5
Example of unnormalizable potentials in a CLG clique tree
623
14.6
A simple CLG and possible clique trees with diﬀerent correctness properties
624
14.7
Diﬀerent Gaussian approximation methods for a nonlinear dependency
636
15.1
Clique tree for HMM
654
15.2
Diﬀerent clique trees for the Car DBN of ﬁgure 6.1
659
15.3
Nonpersistent 2-TBN and diﬀerent possible clique trees
660
15.4
Performance of likelihood weighting over time
667
15.5
Illustration of the particle ﬁltering algorithm
669
15.6
Likelihood weighting and particle ﬁltering over time
670
15.7
Three collapsing strategies for CLG DBNs, and their EP perspective
687
16.1
The eﬀect of ignoring hidden variables
714
17.1
A simple thumbtack tossing experiment
718
17.2
The likelihood function for the sequence of tosses H, T, T, H, H
718
17.3
Meta-network for IID samples of a random variable
734
17.4
Examples of Beta distributions for diﬀerent choices of hyperparameters
736
17.5
The eﬀect of the Beta prior on our posterior estimates
741
17.6
The eﬀect of diﬀerent priors on smoothing our parameter estimates
742
17.7
Meta-network for IID samples from X →Y with global parameter independence
743
17.8
Meta-network for IID samples from X →Y with local parameter independence
746
17.9
Two plate models for the University example, with explicit parameter variables
758
17.10
Example meta-network for a model with shared parameters
763
17.11
Independent and hierarchical priors
765

LIST OF FIGURES
xxix
18.1
Marginal training likelihood versus expected likelihood on underlying distribution
796
18.2
Maximal likelihood score versus marginal likelihood for the data ⟨H, T, T, H, H⟩.
797
18.3
The eﬀect of correlation on the Bayesian score
801
18.4
The Bayesian scores of three structures for the ICU-Alarm domain
802
18.5
Example of a search problem requiring edge deletion
813
18.6
Example of a search problem requiring edge reversal
814
18.7
Performance of structure and parameter learning for instances from ICU-Alarm
network
820
18.8
MCMC structure search using 500 instances from ICU-Alarm network
830
18.9
MCMC structure search using 1,000 instances from ICU-Alarm network
831
18.10
MCMC order search using 1,000 instances from ICU-Alarm network
833
18.11
A simple module network
847
19.1
Observation models in two variants of the thumbtack example
851
19.2
An example satisfying MAR but not MCAR
853
19.3
A visualization of a multimodal likelihood function with incomplete data
857
19.4
The meta-network for parameter estimation for X →Y
858
19.5
Contour plots for the likelihood function for X →Y
858
19.6
A simple network used to illustrate learning algorithms for missing data
864
19.7
The naive Bayes clustering model
875
19.8
The hill-climbing process performed by the EM algorithm
882
19.9
Plate model for Bayesian clustering
902
19.10
Nondecomposability of structure scores in the case of missing data
918
19.11
An example of a network with a hierarchy of hidden variables
931
19.12
An example of a network with overlapping hidden variables
931
20.1
Log-likelihood surface for the Markov network A—B—C
945
20.2
A highly connected CRF that allows simple inference when conditioned
952
20.3
Laplacian distribution (β = 1) and Gaussian distribution (σ2 = 1)
959
21.1
Mutilated Student networks representing interventions
1015
21.2
Causal network for Simpson’s paradox
1016
21.3
Models where P(Y | do(X)) is identiﬁable
1025
21.4
Models where P(Y | do(X)) is not identiﬁable
1025
21.5
A simple functional causal model for a clinical trial
1030
21.6
Twinned counterfactual network with an intervention
1036
21.7
Models corresponding to the equivalence class of the Student network
1043
21.8
Example PAG and members of its equivalence class
1050
21.9
Learned causal network for exercise 21.12
1057
22.1
Example curve for the utility of money
1066
22.2
Utility curve and its consequences to an agent’s attitude toward risk
1067
23.1
Decision trees for the Entrepreneur example
1086
23.2
Inﬂuence diagram IF for the basic Entrepreneur example
1089
23.3
Inﬂuence diagram IF,C for Entrepreneur example with market survey
1091

xxx
LIST OF FIGURES
23.4
Decision tree for the inﬂuence diagram IF,C in the Entrepreneur example
1096
23.5
Iterated optimization versus variable elimination
1099
23.6
An inﬂuence diagram with multiple utility variables
1101
23.7
Inﬂuence diagrams, augmented to test for s-reachability
1112
23.8
Inﬂuence diagrams and their relevance graphs
1114
23.9
Clique tree for the imperfect-recall inﬂuence diagram of ﬁgure 23.5.
1116
23.10
More complex inﬂuence diagram IS for the Student scenario
1120
23.11
Example for computing value of information using an inﬂuence diagram
1123
A.1
Illustration of asymptotic complexity
1149
A.2
Illustration of line search with Brent’s method
1165
A.3
Two examples of the convergence problem with line search
1166

List of Algorithms
3.1
Algorithm for ﬁnding nodes reachable from X given Z via active trails
75
3.2
Procedure to build a minimal I-map given an ordering
80
3.3
Recovering the undirected skeleton for a distribution P that has a P-map
85
3.4
Marking immoralities in the construction of a perfect map
86
3.5
Finding the class PDAG characterizing the P-map of a distribution P
89
5.1
Computing d-separation in the presence of deterministic CPDs
160
5.2
Computing d-separation in the presence of context-speciﬁc CPDs
173
9.1
Sum-product variable elimination algorithm
298
9.2
Using Sum-Product-VE for computing conditional probabilities
304
9.3
Maximum cardinality search for constructing an elimination ordering
312
9.4
Greedy search for constructing an elimination ordering
314
9.5
Conditioning algorithm
317
9.6
Rule splitting algorithm
332
9.7
Sum-product variable elimination for sets of rules
333
10.1
Upward pass of variable elimination in clique tree
353
10.2
Calibration using sum-product message passing in a clique tree
357
10.3
Calibration using belief propagation in clique tree
367
10.4
Out-of-clique inference in clique tree
371
11.1
Calibration using sum-product belief propagation in a cluster graph
397
11.2
Convergent message passing for Bethe cluster graph with convex counting
numbers
418
11.3
Algorithm to construct a saturated region graph
423
11.4
Projecting a factor set to produce a set of marginals over a given set of scopes
434
11.5
Modiﬁed version of BU-Message that incorporates message projection
441
11.6
Message passing step in the expectation propagation algorithm
443
11.7
The Mean-Field approximation algorithm
455
12.1
Forward Sampling in a Bayesian network
489
12.2
Likelihood-weighted particle generation
493
12.3
Likelihood weighting with a data-dependent stopping rule
502
12.4
Generating a Gibbs chain trajectory
506
12.5
Generating a Markov chain trajectory
509
13.1
Variable elimination algorithm for MAP
557

xxxii
LIST OF ALGORITHMS
13.2
Max-product message computation for MAP
562
13.3
Calibration using max-product BP in a Bethe-structured cluster graph
573
13.4
Graph-cut algorithm for MAP in pairwise binary MRFs with submodular
potentials
591
13.5
Alpha-expansion algorithm
593
13.6
Eﬃcient min-sum message passing for untruncated 1-norm energies
603
14.1
Expectation propagation message passing for CLG networks
622
15.1
Filtering in a DBN using a template clique tree
657
15.2
Likelihood-weighted particle generation for a 2-TBN
666
15.3
Likelihood weighting for ﬁltering in DBNs
666
15.4
Particle ﬁltering for DBNs
670
18.1
Data perturbation search
817
19.1
Computing the gradient in a network with table-CPDs
867
19.2
Expectation-maximization algorithm for BN with table-CPDs
873
19.3
The structural EM algorithm for structure learning
922
19.4
The incremental EM algorithm for network with table-CPDs
939
19.5
Proposal distribution for collapsed Metropolis-Hastings over data completions
941
19.6
Proposal distribution over partitions in the Dirichlet process priof
942
20.1
Greedy score-based structure search algorithm for log-linear models
986
23.1
Finding the MEU strategy in a decision tree
1088
23.2 Generalized variable elimination for joint factors in inﬂuence diagrams
1105
23.3 Iterated optimization for inﬂuence diagrams with acyclic relevance graphs
1116
A.1
Topological sort of a graph
1146
A.2
Maximum weight spanning tree in an undirected graph
1147
A.3
Recursive algorithm for computing Fibonacci numbers
1150
A.4
Dynamic programming algorithm for computing Fibonacci numbers
1150
A.5
Greedy local search algorithm with search operators
1155
A.6
Local search with tabu list
1157
A.7
Beam search
1158
A.8
Greedy hill-climbing search with random restarts
1159
A.9
Branch and bound algorithm
1161
A.10 Simple gradient ascent algorithm
1164
A.11
Conjugate gradient ascent
1167

List of Boxes
Box 3.A
Concept: The Naive Bayes Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Box 3.B
Case Study: The Genetics Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Figure 3.B.1
Modeling Genetic Inheritance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58
Box 3.C
Skill: Knowledge Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Box 3.D
Case Study: Medical Diagnosis Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Box 4.A
Concept: Pairwise Markov Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
Figure 4.A.1
A pairwise Markov network (MRF) structured as a grid. . . . . . . . . . . . . . . . . . . . . . 110
Box 4.B
Case Study: Markov Networks for Computer Vision. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .112
Figure 4.B.1
Two examples of image segmentation results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .114
Box 4.C
Concept: Ising Models and Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
Box 4.D
Concept: Metric MRFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Box 4.E
Case Study: CRFs for Text Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
Figure 4.E.1
Two models for text analysis based on a linear chain CRF. . . . . . . . . . . . . . . . . . .147
Box 5.A
Case Study: Context-Speciﬁcity in Diagnostic Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 166
Figure 5.A.1
Context-speciﬁc independencies for diagnostic networks. . . . . . . . . . . . . . . . . . . . 167
Box 5.B
Concept: Multinets and Similarity Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
Box 5.C
Concept: BN2O Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
Figure 5.C.1
A two-layer noisy-or network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Box 5.D
Case Study: Noisy Rule Models for Medical Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
Box 5.E
Case Study: Robot Motion and Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
Figure 5.E.1
Probabilistic model for robot localization track. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
Box 6.A
Case Study: HMMs and Phylo-HMMs for Gene Finding . . . . . . . . . . . . . . . . . . . . . . . . . . 206
Box 6.B
Case Study: HMMs for Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
Figure 6.B.1
A phoneme-level HMM for a fairly complex phoneme. . . . . . . . . . . . . . . . . . . . . . 210
Box 6.C
Case Study: Collective Classiﬁcation of Web Pages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
Box 6.D
Case Study: Object Uncertainty and Citation Matching . . . . . . . . . . . . . . . . . . . . . . . . . . 238
Figure 6.D.1
Two template models for citation-matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
Box 9.A
Concept: The Network Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
Box 9.B
Concept: Polytrees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .313
Box 9.C
Case Study: Variable Elimination Orderings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
Figure 9.C.1
Comparison of algorithms for selecting variable elimination ordering. . . . . . . . . 316

xxxiv
List of Boxes
Box 9.D
Case Study: Inference with Local Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
Box 10.A
Skill: Eﬃcient Implementation of Factor Manipulation Algorithms . . . . . . . . . . . . . . . . 358
Algorithm 10.A.1
Eﬃcient implementation of a factor product operation. . . . . . . . . . . . . . . . 359
Box 11.A
Case Study: Turbocodes and loopy belief propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
Figure 11.A.1
Two examples of codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
Box 11.B
Skill: Making loopy belief propagation work in practice . . . . . . . . . . . . . . . . . . . . . . . . . . 407
Box 11.C
Case Study: BP in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
Figure 11.C.1
Example of behavior of BP in practice on an 11 × 11 Ising grid. . . . . . . . . . . . . 410
Box 12.A
Skill: Sampling from a Discrete Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
Box 12.B
Skill: MCMC in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522
Box 12.C
Case Study: The bugs System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
Figure 12.C.1
Example of bugs model speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
Box 12.D
Concept: Correspondence and Data Association . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
Figure 12.D.1
Results of a correspondence algorithm for 3D human body scans . . . . . . . . . . 535
Box 13.A
Concept: Tree-Reweighted Belief Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576
Box 13.B
Case Study: Energy Minimization in Computer Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
Figure 13.B.1
MAP inference for stereo reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .594
Box 15.A
Case Study: Tracking, Localization, and Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Figure 15.A.1
Illustration of Kalman ﬁltering for tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679
Figure 15.A.2
Sample trajectory of particle ﬁltering for robot localization. . . . . . . . . . . . . . . . .681
Figure 15.A.3
Kalman ﬁlters for the SLAM problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 682
Figure 15.A.4
Collapsed particle ﬁltering for SLAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 684
Box 16.A
Skill: Design and Evaluation of Learning Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705
Algorithm 16.A.1
Algorithms for holdout and cross-validation tests.. . . . . . . . . . . . . . . . . . . . . .707
Box 16.B
Concept: PAC-bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 708
Box 17.A
Concept: Naive Bayes Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727
Box 17.B
Concept: Nonparametric Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 730
Box 17.C
Case Study: Learning the ICU-Alarm Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 749
Figure 17.C.1
The ICU-Alarm Bayesian network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750
Figure 17.C.2
Learning curve for parameter estimation for the ICU-Alarm network . . . . . . . . 751
Box 17.D
Concept: Representation Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752
Box 17.E
Concept: Bag-of-Word Models for Text Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766
Figure 17.E.1
Diﬀerent plate models for text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 768
Box 18.A
Skill: Practical Collection of Suﬃcient Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 819
Box 18.B
Concept: Dependency Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 822
Box 18.C
Case Study: Bayesian Networks for Collaborative Filtering . . . . . . . . . . . . . . . . . . . . . . . 823
Figure 18.C.1
Learned Bayesian network for collaborative ﬁltering. . . . . . . . . . . . . . . . . . . . . . . 823
Box 19.A
Case Study: Discovering User Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 877
Figure 19.A.1
Application of Bayesian clustering to collaborative ﬁltering. . . . . . . . . . . . . . . . . 878
Box 19.B
Case Study: EM in Practice. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .885
Figure 19.B.1
Convergence of EM run on the ICU Alarm network. . . . . . . . . . . . . . . . . . . . . . . . 885
Figure 19.B.2
Local maxima in likelihood surface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 886
Box 19.C
Skill: Practical Considerations in Parameter Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 888
Box 19.D
Case Study: EM for Robot Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 892
Figure 19.D.1
Sample results from EM-based 3D plane mapping . . . . . . . . . . . . . . . . . . . . . . . . 893
9

List of Boxes
xxxv
Box 19.E
Skill: Sampling from a Dirichlet distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 900
Box 19.F
Concept: Laplace Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 909
Box 19.G
Case Study: Evaluating Structure Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 915
Figure 19.G.1
Evaluation of structure scores for a naive Bayes clustering model . . . . . . . . . . . 916
Box 20.A
Concept: Generative and Discriminative Models for Sequence Labeling . . . . . . . . . . . 952
Figure 20.A.1
Diﬀerent models for sequence labeling: HMM, MEMM, and CRF . . . . . . . . . . . 953
Box 20.B
Case Study: CRFs for Protein Structure Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 968
Box 21.A
Case Study: Identifying the Eﬀect of Smoking on Cancer . . . . . . . . . . . . . . . . . . . . . . . 1021
Figure 21.A.1
Three candidate models for smoking and cancer. . . . . . . . . . . . . . . . . . . . . . . . . 1022
Figure 21.A.2
Determining causality between smoking and cancer. . . . . . . . . . . . . . . . . . . . . . 1023
Box 21.B
Case Study: The Eﬀect of Cholestyramine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1033
Box 21.C
Case Study: Persistence Networks for Diagnosis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1037
Box 21.D
Case Study: Learning Cellular Networks from Intervention Data . . . . . . . . . . . . . . . . . 1046
Box 22.A
Case Study: Prenatal Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1079
Figure 22.A.1
Typical utility function decomposition for prenatal diagnosis . . . . . . . . . . . . . 1080
Box 22.B
Case Study: Utility Elicitation in Medical Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1080
Box 23.A
Case Study: Decision Making for Prenatal Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1094
Box 23.B
Case Study: Coordination Graphs for Robot Soccer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1117
Box 23.C
Case Study: Decision Making for Troubleshooting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1125


1
Introduction
1.1
Motivation
Most tasks require a person or an automated system to reason: to take the available information
and reach conclusions, both about what might be true in the world and about how to act.
For example, a doctor needs to take information about a patient — his symptoms, test results,
personal characteristics (gender, weight) — and reach conclusions about what diseases he may
have and what course of treatment to undertake. A mobile robot needs to synthesize data from
its sonars, cameras, and other sensors to conclude where in the environment it is and how to
move so as to reach its goal without hitting anything. A speech-recognition system needs to
take a noisy acoustic signal and infer the words spoken that gave rise to it.
In this book, we describe a general framework that can be used to allow a computer system
to answer questions of this type.
In principle, one could write a special-purpose computer
program for every domain one encounters and every type of question that one may wish to
answer. The resulting system, although possibly quite successful at its particular task, is often
very brittle: If our application changes, signiﬁcant changes may be required to the program.
Moreover, this general approach is quite limiting, in that it is hard to extract lessons from one
successful solution and apply it to one which is very diﬀerent.
We focus on a diﬀerent approach, based on the concept of a declarative representation. In
declarative
representation
this approach, we construct, within the computer, a model of the system about which we would
model
like to reason. This model encodes our knowledge of how the system works in a computer-
readable form. This representation can be manipulated by various algorithms that can answer
questions based on the model. For example, a model for medical diagnosis might represent our
knowledge about diﬀerent diseases and how they relate to a variety of symptoms and test results.
A reasoning algorithm can take this model, as well as observations relating to a particular patient,
and answer questions relating to the patient’s diagnosis. The key property of a declarative

representation is the separation of knowledge and reasoning. The representation has its
own clear semantics, separate from the algorithms that one can apply to it. Thus, we can
develop a general suite of algorithms that apply any model within a broad class, whether
in the domain of medical diagnosis or speech recognition. Conversely, we can improve
our model for a speciﬁc application domain without having to modify our reasoning
algorithms constantly.
Declarative representations, or model-based methods, are a fundamental component in many
ﬁelds, and models come in many ﬂavors. Our focus in this book is on models for complex sys-

2
Chapter 1. Introduction
tems that involve a signiﬁcant amount of uncertainty. Uncertainty appears to be an inescapable
uncertainty
aspect of most real-world applications. It is a consequence of several factors. We are often
uncertain about the true state of the system because our observations about it are partial: only
some aspects of the world are observed; for example, the patient’s true disease is often not
directly observable, and his future prognosis is never observed. Our observations are also noisy
— even those aspects that are observed are often observed with some error. The true state of
the world is rarely determined with certainty by our limited observations, as most relationships
are simply not deterministic, at least relative to our ability to model them. For example, there
are few (if any) diseases where we have a clear, universally true relationship between the disease
and its symptoms, and even fewer such relationships between the disease and its prognosis.
Indeed, while it is not clear whether the universe (quantum mechanics aside) is deterministic
when modeled at a suﬃciently ﬁne level of granularity, it is quite clear that it is not determin-
istic relative to our current understanding of it. To summarize, uncertainty arises because of
limitations in our ability to observe the world, limitations in our ability to model it, and possibly
even because of innate nondeterminism.
Because of this ubiquitous and fundamental uncertainty about the true state of world, we
need to allow our reasoning system to consider diﬀerent possibilities. One approach is simply
to consider any state of the world that is possible. Unfortunately, it is only rarely the case
that we can completely eliminate a state as being impossible given our observations. In our
medical diagnosis example, there is usually a huge number of diseases that are possible given a
particular set of observations. Most of them, however, are highly unlikely. If we simply list all of
the possibilities, our answers will often be vacuous of meaningful content (e.g., “the patient can
have any of the following 573 diseases”). Thus, to obtain meaningful conclusions, we need

to reason not just about what is possible, but also about what is probable.
The calculus of probability theory (see section 2.1) provides us with a formal framework for
probability theory
considering multiple possible outcomes and their likelihood. It deﬁnes a set of mutually exclusive
and exhaustive possibilities, and associates each of them with a probability — a number between
0 and 1, so that the total probability of all possibilities is 1. This framework allows us to consider
options that are unlikely, yet not impossible, without reducing our conclusions to content-free
lists of every possibility.

Furthermore, one ﬁnds that probabilistic models are very liberating. Where in a more
rigid formalism we might ﬁnd it necessary to enumerate every possibility, here we can
often sweep a multitude of annoying exceptions and special cases under the “probabilistic
rug,” by introducing outcomes that roughly correspond to “something unusual happens.”
In fact, as we discussed, this type of approximation is often inevitable, as we can only rarely
(if ever) provide a deterministic speciﬁcation of the behavior of a complex system. Probabilistic
models allow us to make this fact explicit, and therefore often provide a model which is more
faithful to reality.
1.2
Structured Probabilistic Models
This book describes a general-purpose framework for constructing and using probabilistic mod-
els of complex systems. We begin by providing some intuition for the principles underlying
this framework, and for the models it encompasses. This section requires some knowledge of

1.2. Structured Probabilistic Models
3
basic concepts in probability theory; a reader unfamiliar with these concepts might wish to read
section 2.1 ﬁrst.
Complex systems are characterized by the presence of multiple interrelated aspects, many of
which relate to the reasoning task. For example, in our medical diagnosis application, there
are multiple possible diseases that the patient might have, dozens or hundreds of symptoms
and diagnostic tests, personal characteristics that often form predisposing factors for disease,
and many more matters to consider. These domains can be characterized in terms of a set of
random variables, where the value of each variable deﬁnes an important property of the world.
random variable
For example, a particular disease, such as Flu, may be one variable in our domain, which takes
on two values, for example, present or absent; a symptom, such as Fever, may be a variable in
our domain, one that perhaps takes on continuous values. The set of possible variables and
their values is an important design decision, and it depends strongly on the questions we may
wish to answer about the domain.
Our task is to reason probabilistically about the values of one or more of the variables,
possibly given observations about some others. In order to do so using principled probabilistic
reasoning, we need to construct a joint distribution over the space of possible assignments to
joint probability
distribution
some set of random variables X. This type of model allows us to answer a broad range of
interesting queries. For example, we can make the observation that a variable Xi takes on the
speciﬁc value xi, and ask, in the resulting posterior distribution, what the probability distribution
posterior
distribution
is over values of another variable Xj.
Example 1.1
Consider a very simple medical diagnosis setting, where we focus on two diseases — ﬂu and
hayfever; these are not mutually exclusive, as a patient can have either, both, or none.
Thus,
we might have two binary-valued random variables, Flu and Hayfever. We also have a 4-valued
random variable Season, which is correlated both with ﬂu and hayfever. We may also have two
symptoms, Congestion and Muscle Pain, each of which is also binary-valued. Overall, our probability
space has 2 × 2 × 4 × 2 × 2 = 64 values, corresponding to the possible assignments to these ﬁve
variables. Given a joint distribution over this space, we can, for example, ask questions such as how
likely the patient is to have the ﬂu given that it is fall, and that she has sinus congestion but no
muscle pain; as a probability expression, this query would be denoted
P(Flu = true | Season = fall, Congestion = true, Muscle Pain = false).
1.2.1
Probabilistic Graphical Models
Specifying a joint distribution over 64 possible values, as in example 1.1, already seems fairly
daunting. When we consider the fact that a typical medical- diagnosis problem has dozens or
even hundreds of relevant attributes, the problem appears completely intractable. This book
describes the framework of probabilistic graphical models, which provides a mechanism for
exploiting structure in complex distributions to describe them compactly, and in a way that
allows them to be constructed and utilized eﬀectively.
Probabilistic graphical models use a graph-based representation as the basis for compactly
encoding a complex distribution over a high-dimensional space. In this graphical representation,
illustrated in ﬁgure 1.1, the nodes (or ovals) correspond to the variables in our domain, and the
edges correspond to direct probabilistic interactions between them. For example, ﬁgure 1.1a (top)

4
Chapter 1. Introduction
Bayesian networks
Markov networks
Graph Representation
Muscle-Pain
Congestion
Flu
Hayfever
Season
B
D
C
A
Independencies
(F ⊥H | S)
(C ⊥S | F, H)
(M ⊥H, C | F)
(M ⊥C | F)
(A ⊥C | B, D)
(B ⊥D | A, C)
Factorization
P(S, F, H, C, M) = P(S)P(F | S)
P(A, B, C, D) = 1
Z φ1(A, B)
P(H | S)P(C | F, H)P(M | F)
φ2(B, C)φ3(C, D)φ4(A, D)
(a)
(b)
Figure 1.1
Diﬀerent perspectives on probabilistic graphical models: top — the graphical representa-
tion; middle — the independencies induced by the graph structure; bottom — the factorization induced
by the graph structure. (a) A sample Bayesian network. (b) A sample Markov network.
illustrates one possible graph structure for our ﬂu example. In this graph, we see that there is
no direct interaction between Muscle Pain and Season, but both interact directly with Flu.
There is a dual perspective that one can use to interpret the structure of this graph. From
one perspective, the graph is a compact representation of a set of independencies that hold
in the distribution; these properties take the form X is independent of Y given Z, denoted
(X ⊥Y | Z), for some subsets of variables X, Y , Z. For example, our “target” distribution
P for the preceding example — the distribution encoding our beliefs about this particular
situation — may satisfy the conditional independence (Congestion ⊥Season | Flu, Hayfever).
This statement asserts that
P(Congestion | Flu, Hayfever, Season) = P(Congestion | Flu, Hayfever);
that is, if we are interested in the distribution over the patient having congestion, and we know
whether he has the ﬂu and whether he has hayfever, the season is no longer informative. Note
that this assertion does not imply that Season is independent of Congestion; only that all of the
information we may obtain from the season on the chances of having congestion we already
obtain by knowing whether the patient has the ﬂu and has hayfever. Figure 1.1a (middle) shows
the set of independence assumptions associated with the graph in ﬁgure 1.1a (top).

1.2. Structured Probabilistic Models
5
The other perspective is that the graph deﬁnes a skeleton for compactly representing a high-
dimensional distribution: Rather than encode the probability of every possible assignment to
all of the variables in our domain, we can “break up” the distribution into smaller factors, each
factor
over a much smaller space of possibilities. We can then deﬁne the overall joint distribution
as a product of these factors.
For example, ﬁgure 1.1(a-bottom) shows the factorization of
the distribution associated with the graph in ﬁgure 1.1 (top).
It asserts, for example, that
the probability of the event “spring, no ﬂu, hayfever, sinus congestion, muscle pain” can be
obtained by multiplying ﬁve numbers: P(Season = spring), P(Flu = false | Season = spring),
P(Hayfever = true | Season = spring), P(Congestion = true | Hayfever = true, Flu = false),
and P(Muscle Pain = true | Flu = false). This parameterization is signiﬁcantly more compact,
requiring only 3+4+4+4+2 = 17 nonredundant parameters, as opposed to 63 nonredundant
parameters for the original joint distribution (the 64th parameter is fully determined by the
others, as the sum over all entries in the joint distribution must sum to 1). The graph structure
deﬁnes the factorization of a distribution P associated with it — the set of factors and the
variables that they encompass.

It turns out that these two perspectives — the graph as a representation of a set of
independencies, and the graph as a skeleton for factorizing a distribution — are, in a
deep sense, equivalent. The independence properties of the distribution are precisely
what allow it to be represented compactly in a factorized form. Conversely, a particular
factorization of the distribution guarantees that certain independencies hold.
We describe two families of graphical representations of distributions. One, called Bayesian
Bayesian network
networks, uses a directed graph (where the edges have a source and a target), as shown in
ﬁgure 1.1a (top). The second, called Markov networks, uses an undirected graph, as illustrated in
Markov network
ﬁgure 1.1b (top). It too can be viewed as deﬁning a set of independence assertions (ﬁgure 1.1b
[middle] or as encoding a compact factorization of the distribution (ﬁgure 1.1b [bottom]). Both
representations provide the duality of independencies and factorization, but they diﬀer in the
set of independencies they can encode and in the factorization of the distribution that they
induce.
1.2.2
Representation, Inference, Learning
The graphical language exploits structure that appears present in many distributions that we
want to encode in practice: the property that variables tend to interact directly only with very
few others. Distributions that exhibit this type of structure can generally be encoded naturally
and compactly using a graphical model.
This framework has many advantages. First, it often allows the distribution to be written
down tractably, even in cases where the explicit representation of the joint distribution is
astronomically large.
Importantly, the type of representation provided by this framework is
transparent, in that a human expert can understand and evaluate its semantics and properties.
This property is important for constructing models that provide an accurate reﬂection of our
understanding of a domain. Models that are opaque can easily give rise to unexplained, and
even undesirable, answers.
Second, as we show, the same structure often also allows the distribution to be used eﬀectively
for inference — answering queries using the distribution as our model of the world. In particular,
inference
we provide algorithms for computing the posterior probability of some variables given evidence

6
Chapter 1. Introduction
on others. For example, we might observe that it is spring and the patient has muscle pain,
and we wish to know how likely he is to have the ﬂu, a query that can formally be written as
P(Flu = true | Season = spring, Muscle Pain = true). These inference algorithms work directly
on the graph structure and are generally orders of magnitude faster than manipulating the joint
distribution explicitly.
Third, this framework facilitates the eﬀective construction of these models, whether by a hu-
man expert or automatically, by learning from data a model that provides a good approximation
to our past experience. For example, we may have a set of patient records from a doctor’s oﬃce
and wish to learn a probabilistic model encoding a distribution consistent with our aggregate
experience. Probabilistic graphical models support a data-driven approach to model construction
data-driven
approach
that is very eﬀective in practice. In this approach, a human expert provides some rough guide-
lines on how to model a given domain. For example, the human usually speciﬁes the attributes
that the model should contain, often some of the main dependencies that it should encode, and
perhaps other aspects. The details, however, are usually ﬁlled in automatically, by ﬁtting the
model to data. The models produced by this process are usually much better reﬂections of the
domain than models that are purely hand-constructed. Moreover, they can sometimes reveal
surprising connections between variables and provide novel insights about a domain.

These three components — representation, inference, and learning — are critical com-
ponents in constructing an intelligent system. We need a declarative representation that
is a reasonable encoding of our world model. We need to be able to use this representa-
tion eﬀectively to answer a broad range of questions that are of interest. And we need to
be able to acquire this distribution, combining expert knowledge and accumulated data.
Probabilistic graphical models are one of a small handful of frameworks that support all
three capabilities for a broad range of problems.
1.3
Overview and Roadmap
1.3.1
Overview of Chapters
The framework of probabilistic graphical models is quite broad, and it encompasses both a
variety of diﬀerent types of models and a range of methods relating to them.
This book
describes several types of models. For each one, we describe the three fundamental cornerstones:
representation, inference, and learning.
We begin in part I, by describing the most basic type of graphical models, which are the focus
of most of the book. These models encode distributions over a ﬁxed set X of random variables.
We describe how graphs can be used to encode distributions over such spaces, and what the
properties of such distributions are.
Speciﬁcally, in chapter 3, we describe the Bayesian network representation, based on directed
graphs. We describe how a Bayesian network can encode a probability distribution. We also
analyze the independence properties induced by the graph structure.
In chapter 4, we move to Markov networks, the other main category of probabilistic graphical
models.
Here also we describe the independencies deﬁned by the graph and the induced
factorization of the distribution. We also discuss the relationship between Markov networks and
Bayesian networks, and brieﬂy describe a framework that uniﬁes both.
In chapter 5, we delve a little deeper into the representation of the parameters in probabilistic

1.3. Overview and Roadmap
7
models, focusing mostly on Bayesian networks, whose parameterization is more constrained. We
describe representations that capture some of the ﬁner-grained structure of the distribution, and
show that, here also, capturing structure can provide signiﬁcant gains.
In chapter 6, we turn to formalisms that extend the basic framework of probabilistic graphical
models to settings where the set of variables is no longer rigidly circumscribed in advance.
One such setting is a temporal one, where we wish to model a system whose state evolves
over time, requiring us to consider distributions over entire trajectories, We describe a compact
representation — a dynamic Bayesian network — that allows us to represent structured systems
that evolve over time. We then describe a family of extensions that introduce various forms
of higher level structure into the framework of probabilistic graphical models. Speciﬁcally, we
focus on domains containing objects (whether concrete or abstract), characterized by attributes,
and related to each other in various ways. Such domains can include repeated structure, since
diﬀerent objects of the same type share the same probabilistic model. These languages provide
a signiﬁcant extension to the expressive power of the standard graphical models.
In chapter 7, we take a deeper look at models that include continuous variables. Speciﬁcally,
we explore the properties of the multivariate Gaussian distribution and the representation of
such distributions as both directed and undirected graphical models.
Although the class of
Gaussian distributions is a limited one and not suitable for all applications, it turns out to play
a critical role even when dealing with distributions that are not Gaussian.
In chapter 8, we take a deeper, more technical look at probabilistic models, deﬁning a general
framework called the exponential family, that encompasses a broad range of distributions. This
chapter provides some basic concepts and tools that will turn out to play an important role in
later development.
We then turn, in part II, to a discussion of the inference task. In chapter 9, we describe
the basic ideas underlying exact inference in probabilistic graphical models. We ﬁrst analyze
the fundamental diﬃculty of the exact inference task, separately from any particular inference
algorithm we might develop.
We then present two basic algorithms for exact inference —
variable elimination and conditioning — both of which are equally applicable to both directed
and undirected models. Both of these algorithms can be viewed as operating over the graph
structure deﬁned by the probabilistic model.
They build on basic concepts, such as graph
properties and dynamic programming algorithms, to provide eﬃcient solutions to the inference
task. We also provide an analysis of their computational cost in terms of the graph structure,
and we discuss where exact inference is feasible.
In chapter 10, we describe an alternative view of exact inference, leading to a somewhat
diﬀerent algorithm. The beneﬁt of this alternative algorithm is twofold. First, it uses dynamic
programming to avoid repeated computations in settings where we wish to answer more than a
single query using the same network. Second, it deﬁnes a natural algorithm that uses message
passing on a graph structure; this algorithm forms the basis for approximate inference algorithms
developed in later chapters.
Because exact inference is computationally intractable for many models of interest, we then
proceed to describe approximate inference algorithms, which trade oﬀaccuracy with computa-
tional cost. We present two main classes of such algorithms. In chapter 11, we describe a class
of methods that can be viewed from two very diﬀerent perspectives: On one hand, they are
direct generalizations of the graph-based message-passing approach developed for the case of
exact inference in chapter 10. On the other hand, they can be viewed as solving an optimization

8
Chapter 1. Introduction
problem: one where we approximate the distribution of interest using a simpler representation
that allows for feasible inference. The equivalence of these views provides important insights
and suggests a broad family of algorithms that one can apply to approximate inference.
In chapter 12, we describe a very diﬀerent class of methods: particle-based methods, which
approximate a complex joint distribution by considering samples from it (also known as par-
ticles). We describe several methods from this general family. These methods are generally
based on core techniques from statistics, such as importance sampling and Markov-chain Monte
Carlo methods. Once again, the connection to this general class of methods suggests multiple
opportunities for new algorithms.
While the representation of probabilistic graphical models applies, to a great extent, to models
including both discrete and continuous-valued random variables, inference in models involving
continuous variables is signiﬁcantly more challenging than the purely discrete case. In chapter 14,
we consider the task of inference in continuous and hybrid (continuous/discrete) networks, and
we discuss whether and how the exact and approximate inference methods developed in earlier
chapters can be applied in this setting.
The representation that we discussed in chapter 6 allows a compact encoding of networks
whose size can be unboundedly large. Such networks pose particular challenges to inference
algorithms. In this chapter, we discuss some special-purpose methods that have been developed
for the particular settings of networks that model dynamical systems.
We then turn, in part III, to the third of our main topics — learning probabilistic models from
data. We begin in chapter 16 by reviewing some of the fundamental concepts underlying the
general task of learning models from data. We then present the spectrum of learning problems
that we address in this part of the book. These problems vary along two main axes: the extent to
which we are given prior knowledge specifying the model, and whether the data from which we
learn contain complete observations of all of the relevant variables. In contrast to the inference
task, where the same algorithms apply equally to Bayesian networks and Markov networks, the
learning task is quite diﬀerent for these two classes of models. We begin with studying the
learning task for Bayesian networks.
In chapter 17, we focus on the most basic learning task: learning parameters for a Bayesian
network with a given structure, from fully observable data. Although this setting may appear
somewhat restrictive, it turns out to form the basis for our entire development of Bayesian
network learning. As we show, the factorization of the distribution, which was central both to
representation and to inference, also plays a key role in making inference feasible.
We then move, in chapter 18, to the harder problem of learning both Bayesian network
structure and the parameters, still from fully observed data. The learning algorithms we present
trade oﬀthe accuracy with which the learned network represents the empirical distribution for
the complexity of the resulting structure. As we show, the type of independence assumptions
underlying the Bayesian network representation often hold, at least approximately, in real-world
distributions. Thus, these learning algorithms often result in reasonably compact structures that
capture much of the signal in the distribution.
In chapter 19, we address the Bayesian network learning task in a setting where we have
access only to partial observations of the relevant variables (for example, when the available
patient records have missing entries). This type of situation occurs often in real-world settings.
Unfortunately, the resulting learning task is considerably harder, and the resulting algorithms are
both more complex and less satisfactory in terms of their performance.

1.3. Overview and Roadmap
9
We conclude the discussion of learning in chapter 20 by considering the problem of learning
Markov networks from data.
It turns out that the learning tasks for Markov networks are
signiﬁcantly harder than the corresponding problem for Bayesian networks.
We explain the
diﬃculties and discuss the existing solutions.
Finally, in part IV, we turn to a diﬀerent type of extension, where we consider the use of this
framework for other forms of reasoning. Speciﬁcally, we consider cases where we can act, or
intervene, in the world.
In chapter 21, we focus on the semantics of intervention and its relation to causality. We
present the notion of a causal model, which allows us to answer not only queries of the form “if
I observe X, what do I learn about Y,” but also intervention queries, of the form “if I manipulate
X, what eﬀect does it have on Y.”
We then turn to the task of decision making under uncertainty. Here, we must consider not
only the distribution over diﬀerent states of the world, but also the preferences of the agent
regarding these outcomes. In chapter 22, we discuss the notion of utility functions and how
they can encode an agent’s preferences about complex situations involving multiple variables.
As we show, the same ideas that we used to provide compact representations of probability
distribution can also be used for utility functions.
In chapter 23, we describe a uniﬁed representation for decision making, called inﬂuence
diagrams. Inﬂuence diagrams extend Bayesian networks by introducing actions and utilities. We
present algorithms that use inﬂuence diagrams for making decisions that optimize the agent’s
expected utility. These algorithms utilize many of the same ideas that formed the basis for exact
inference in Bayesian networks.
We conclude with a high-level synthesis of the techniques covered in this book, and with
some guidance on how to use them in tackling a new problem.
1.3.2
Reader’s Guide
As we mentioned, the topics described in this book relate to multiple ﬁelds, and techniques
from other disciplines — probability theory, computer science, information theory, optimization,
statistics, and more — are used in various places throughout it. While it is impossible to present
all of the relevant material within the scope of this book, we have attempted to make the book
somewhat self-contained by providing a very brief review of the key concepts from these related
disciplines in chapter 2.
Some of this material, speciﬁcally the review of probability theory and of graph-related con-
cepts, is very basic yet central to most of the development in this book. Readers who are less
familiar with these topics may wish to read these sections carefully, and even knowledgeable
readers may wish to brieﬂy review them to gain familiarity with the notations used.
Other
background material, covering such topics as information theory, optimization, and algorithmic
concepts, can be found in the appendix.
The chapters in the book are structured as follows. The main text in each chapter provides the
detailed technical development of the key ideas. Beyond the main text, most chapters contain
boxes that contain interesting material that augments these ideas. These boxes come in three
types: Skill boxes describe “hands-on” tricks and techniques, which, while often heuristic in
nature, are important for getting the basic algorithms described in the text to work in practice.
Case study boxes describe empirical case studies relating to the techniques described in the text.

10
Chapter 1. Introduction
Exact Inference
9.1-4, 10.1-2
Approx. Inference
11.3.1-5, 12.1,
12.3.1-3
BN Learning
16, 17.1-2, 19.1.1,
19.1.3, 19.2.2
Learning
Undirected Models
16, 20.1-2, 20.3.1-2 
Representation
Core
2, 3.1-2, 4.1-2
Bayesian Networks
3.3-4, 5.1-4
Undirected Models
4.3-7
Continuous Models
5.5, 7, 14.1-2,
14.3.1-2, 14.5.1-3
Relational Models
6.3-4, 17.5, (18.6.2)
MAP Inference
13.1-4
Structure Learning
17.3-4, 18.1, 18.3-4,
18.6
Causality
21.1-2, 21.6.1 (21.7)
Decision Making
22.1-2, 23.1-2,
23.4-5
Advanced Approx.
Inference
8, 10.3, 11, 12.3-4 
Advanced Learning
18.5, 19, 20
Temporal Models
6.2, 15.1-2, 15.3.1,
15.3.3
Figure 1.2
A reader’s guide to the structure and dependencies in this book
These case studies include both empirical results on how the algorithms perform in practice
and descriptions of applications of these algorithms to interesting domains, illustrating some of
the issues encountered in practice. Finally, concept boxes present particular instantiations of the
material described in the text, which have had signiﬁcant impact in their own right.
This textbook is clearly too long to be used in its entirety in a one-semester class. Figure 1.2
tries to delineate some coherent subsets of the book that can be used for teaching and other
purposes. The small, labeled boxes represent “units” of material on particular topics. Arrows
between the boxes represent dependencies between these units. The ﬁrst enclosing box (solid
line) represents material that is fundamental to everything else, and that should be read by
anyone using this book. One can then use the dependencies between the boxes to expand or
reduce the depth of the coverage on any given topic. The material in the larger box (dashed
line) forms a good basis for a one-semester (or even one-quarter) overview class. Some of the
sections in the book are marked with an asterisk, denoting the fact that they contain more
technically advanced material. In most cases, these sections are self-contained, and they can be
skipped without harming the reader’s ability to understand the rest of the text.
We have attempted in this book to present a synthesis of ideas, most of which have been
developed over many years by multiple researchers. To avoid futile attempts to divide up the
credit precisely, we have omitted all bibliographical references from the technical presentation

1.3. Overview and Roadmap
11
in the chapters. Rather, each chapter ends with a section called “Relevant Literature,” which
describes the historical evolution of the material in the chapter, acknowledges the papers and
books that developed the key concepts, and provides some additional readings on material
relevant to the chapter. We encourage the reader who is interested in a topic to follow up on
some of these additional readings, since there are many interesting developments that we could
not cover in this book.
Finally, each chapter includes a set of exercises that explore in additional depth some of the
material described in the text and present some extensions to it. The exercises are annotated
with an asterisk for exercises that are somewhat more diﬃcult, and with two asterisks for ones
that are truly challenging.
Additional material related to this book, including slides and ﬁgures, solutions to some of the
exercises, and errata, can be found online at http://pgm.stanford.edu.
1.3.3
Connection to Other Disciplines
The ideas we describe in this book are connected to many ﬁelds. From probability theory, we
inherit the basic concept of a probability distribution, as well as many of the operations we
can use to manipulate it. From computer science, we exploit the key idea of using a graph
as a data structure, as well as a variety of algorithms for manipulating graphs and other data
structures. These algorithmic ideas and the ability to manipulate probability distributions using
discrete data structures are some of the key elements that make the probabilistic manipulations
tractable.
Decision theory extends these basic ideas to the task of decision making under
uncertainty and provides the formal foundation for this task.
From computer science, and speciﬁcally from artiﬁcial intelligence, these models inherit the
idea of using a declarative representation of the world to separate procedural reasoning from
our domain knowledge. This idea is of key importance to the generality of this framework and
its applicability to such a broad range of tasks.
Various ideas from other disciplines also arise in this ﬁeld. Statistics plays an important role
both in certain aspects of the representation and in some of the work on learning models from
data. Optimization plays a role in providing algorithms both for approximate inference and for
learning models from data. Bayesian networks ﬁrst arose, albeit in a restricted way, in the setting
of modeling genetic inheritance in human family trees; in fact, restricted version of some of the
exact inference algorithms we discuss were ﬁrst developed in this context. Similarly, undirected
graphical models ﬁrst arose in physics as a model for systems of electrons, and some of the
basic concepts that underlie recent work on approximate inference developed from that setting.
Information theory plays a dual role in its interaction with this ﬁeld. Information-theoretic
concepts such as entropy and information arise naturally in various settings in this framework,
such as evaluating the quality of a learned model. Thus, tools from this discipline are a key
component in our analytic toolkit. On the other side, the recent successes in coding theory,
based on the relationship between inference in probabilistic models and the task of decoding
messages sent over a noisy channel, have led to a resurgence of work on approximate inference
in graphical models. The resulting developments have revolutionized both the development of
error-correcting codes and the theory and practice of approximate message-passing algorithms
in graphical models.

12
Chapter 1. Introduction
1.3.3.1
What Have We Gained?
Although the framework we describe here shares common elements with a broad range of
other topics, it has a coherent common core: the use of structure to allow a compact repre-
sentation, eﬀective reasoning, and feasible learning of general-purpose, factored, probabilistic
models. These elements provide us with a general infrastructure for reasoning and learning
about complex domains.
As we discussed earlier, by using a declarative representation, we essentially separate out the
description of the model for the particular application, and the general-purpose algorithms used
for inference and learning. Thus, this framework provides a general algorithmic toolkit that can
be applied to many diﬀerent domains.
Indeed, probabilistic graphical models have made a signiﬁcant impact on a broad spectrum
of real-world applications. For example, these models have been used for medical and fault
diagnosis, for modeling human genetic inheritance of disease, for segmenting and denoising
images, for decoding messages sent over a noisy channel, for revealing genetic regulatory pro-
cesses, for robot localization and mapping, and more. Throughout this book, we will describe
how probabilistic graphical models were used to address these applications and what issues
arise in the application of these models in practice.
In addition to practical applications, these models provide a formal framework for a variety
of fundamental problems. For example, the notion of conditional independence and its explicit
graph-based representation provide a clear formal semantics for irrelevance of information. This
framework also provides a general methodology for handling data fusion — we can introduce
sensor variables that are noisy versions of the true measured quantity, and use Bayesian condi-
tioning to combine the diﬀerent measurements. The use of a probabilistic model allows us to
provide a formal measure for model quality, in terms of a numerical ﬁt of the model to observed
data; this measure underlies much of our work on learning models from data. The temporal
models we deﬁne provide a formal framework for deﬁning a general trend toward persistence of
state over time, in a way that does not raise inconsistencies when change does occur.
In general, part of the rich development in this ﬁeld is due to the close and continuous
interaction between theory and practice. In this ﬁeld, unlike many others, the distance between
theory and practice is quite small, and there is a constant ﬂow of ideas and problems between
them.
Problems or ideas arise in practical applications and are analyzed and subsequently
developed in more theoretical papers. Algorithms for which no theoretical analysis exists are
tried out in practice, and the proﬁle of where they succeed and fail often provides the basis for
subsequent analysis. This rich synergy leads to a continuous and vibrant development, and it is
a key factor in the success of this area.
1.4
Historical Notes
The foundations of probability theory go back to the sixteenth century, when Gerolamo Cardano
began a formal analysis of games of chance, followed by additional key developments by Pierre
de Fermat and Blaise Pascal in the seventeenth century.
The initial development involved
only discrete probability spaces, and the analysis methods were purely combinatorial.
The
foundations of modern probability theory, with its measure-theoretic underpinnings, were laid
by Andrey Kolmogorov in the 1930s.

1.4. Historical Notes
13
Particularly central to the topics of this book is the so-called Bayes theorem, shown in the
eighteenth century by the Reverend Thomas Bayes (Bayes 1763). This theorem allows us to use
a model that tells us the conditional probability of event a given event b (say, a symptom given
a disease) in order to compute the contrapositive: the conditional probability of event b given
event a (the disease given the symptom). This type of reasoning is central to the use of graphical
models, and it explains the choice of the name Bayesian network.
The notion of representing the interactions between variables in a multidimensional distribu-
tion using a graph structure originates in several communities, with very diﬀerent motivations.
In the area of statistical physics, this idea can be traced back to Gibbs (1902), who used an
undirected graph to represent the distribution over a system of interacting particles. In the
area of genetics, this idea dates back to the work on path analysis of Sewal Wright (Wright
1921, 1934). Wright proposed the use of a directed graph to study inheritance in natural species.
This idea, although largely rejected by statisticians at the time, was subsequently adopted by
economists and social scientists (Wold 1954; Blalock, Jr. 1971). In the ﬁeld of statistics, the idea
of analyzing interactions between variables was ﬁrst proposed by Bartlett (1935), in the study
of contingency tables, also known as log-linear models. This idea became more accepted by the
statistics community in the 1960s and 70s (Vorobev 1962; Goodman 1970; Haberman 1974).
In the ﬁeld of computer science, probabilistic methods lie primarily in the realm of Artiﬁcial
Intelligence (AI). The AI community ﬁrst encountered these methods in the endeavor of building
expert systems, computerized systems designed to perform diﬃcult tasks, such as oil-well location
expert systems
or medical diagnosis, at an expert level. Researchers in this ﬁeld quickly realized the need for
methods that allow the integration of multiple pieces of evidence, and that provide support
for making decisions under uncertainty. Some early systems (de Bombal et al. 1972; Gorry and
Barnett 1968; Warner et al. 1961) used probabilistic methods, based on the very restricted naive
Bayes model. This model restricts itself to a small set of possible hypotheses (e.g., diseases) and
assumes that the diﬀerent evidence variables (e.g., symptoms or test results) are independent
given each hypothesis. These systems were surprisingly successful, performing (within their area
of expertise) at a level comparable to or better than that of experts. For example, the system
of de Bombal et al. (1972) averaged over 90 percent correct diagnoses of acute abdominal pain,
whereas expert physicians were averaging around 65 percent.
Despite these successes, this approach fell into disfavor in the AI community, owing to a com-
bination of several factors. One was the belief, prevalent at the time, that artiﬁcial intelligence
should be based on similar methods to human intelligence, combined with a strong impression
that people do not manipulate numbers when reasoning. A second issue was the belief that the
strong independence assumptions made in the existing expert systems were fundamental to the
approach. Thus, the lack of a ﬂexible, scalable mechanism to represent interactions between
variables in a distribution was a key factor in the rejection of the probabilistic framework.
The rejection of probabilistic methods was accompanied by the invention of a range of
alternative formalisms for reasoning under uncertainty, and the construction of expert systems
based on these formalisms (notably Prospector by Duda, Gaschnig, and Hart 1979 and Mycin by
Buchanan and Shortliﬀe 1984). Most of these formalisms used the production rule framework,
where each rule is augmented with some number(s) deﬁning a measure of “conﬁdence” in
its validity. These frameworks largely lacked formal semantics, and many exhibited signiﬁcant
problems in key reasoning patterns. Other frameworks for handling uncertainty proposed at
the time included fuzzy logic, possibility theory, and Dempster-Shafer belief functions. For a

14
Chapter 1. Introduction
discussion of some of these alternative frameworks see Shafer and Pearl (1990); Horvitz et al.
(1988); Halpern (2003).
The widespread acceptance of probabilistic methods began in the late 1980s, driven forward
by two major factors. The ﬁrst was a series of seminal theoretical developments. The most
inﬂuential among these was the development of the Bayesian network framework by Judea Pearl
and his colleagues in a series of paper that culminated in Pearl’s highly inﬂuential textbook
Probabilistic Reasoning in Intelligent Systems (Pearl 1988).
In parallel, the key paper by S.L.
Lauritzen and D.J. Spiegelhalter 1988 set forth the foundations for eﬃcient reasoning using
probabilistic graphical models. The second major factor was the construction of large-scale,
highly successful expert systems based on this framework that avoided the unrealistically strong
assumptions made by early probabilistic expert systems. The most visible of these applications
was the Pathﬁnder expert system, constructed by Heckerman and colleagues (Heckerman et al.
1992; Heckerman and Nathwani 1992b), which used a Bayesian network for diagnosis of pathology
samples.
At this time, although work on other approaches to uncertain reasoning continues, proba-
bilistic methods in general, and probabilistic graphical models in particular, have gained almost
universal acceptance in a wide range of communities. They are in common use in ﬁelds as
diverse as medical diagnosis, fault diagnosis, analysis of genetic and genomic data, communica-
tion and coding, analysis of marketing data, speech recognition, natural language understanding,
and many more. Several other books cover aspects of this growing area; examples include Pearl
(1988); Lauritzen (1996); Jensen (1996); Castillo et al. (1997a); Jordan (1998); Cowell et al. (1999);
Neapolitan (2003); Korb and Nicholson (2003). The Artiﬁcial Intelligence textbook of Russell and
Norvig (2003) places this ﬁeld within the broader endeavor of constructing an intelligent agent.

2
Foundations
In this chapter, we review some important background material regarding key concepts from
probability theory, information theory, and graph theory. This material is included in a separate
introductory chapter, since it forms the basis for much of the development in the remainder
of the book.
Other background material — such as discrete and continuous optimization,
algorithmic complexity analysis, and basic algorithmic concepts — is more localized to particular
topics in the book. Many of these concepts are presented in the appendix; others are presented
in concept boxes in the appropriate places in the text. All of this material is intended to focus
only on the minimal subset of ideas required to understand most of the discussion in the
remainder of the book, rather than to provide a comprehensive overview of the ﬁeld it surveys.
We encourage the reader to explore additional sources for more details about these areas.
2.1
Probability Theory
The main focus of this book is on complex probability distributions. In this section we brieﬂy
review basic concepts from probability theory.
2.1.1
Probability Distributions
When we use the word “probability” in day-to-day life, we refer to a degree of conﬁdence that
an event of an uncertain nature will occur. For example, the weather report might say “there
is a low probability of light rain in the afternoon.” Probability theory deals with the formal
foundations for discussing such estimates and the rules they should obey.
Before we discuss the representation of probability, we need to deﬁne what the events are to
which we want to assign a probability. These events might be diﬀerent outcomes of throwing
a die, the outcome of a horse race, the weather conﬁgurations in California, or the possible
failures of a piece of machinery.
2.1.1.1
Event Spaces
Formally, we deﬁne events by assuming that there is an agreed upon space of possible outcomes,
event
outcome space
which we denote by Ω. For example, if we consider dice, we might set Ω= {1, 2, 3, 4, 5, 6}. In
the case of a horse race, the space might be all possible orders of arrivals at the ﬁnish line, a
much larger space.

16
Chapter 2. Foundations
In addition, we assume that there is a set of measurable events S to which we are willing to
measurable event
assign probabilities. Formally, each event α ∈S is a subset of Ω. In our die example, the event
{6} represents the case where the die shows 6, and the event {1, 3, 5} represents the case of
an odd outcome. In the horse-race example, we might consider the event “Lucky Strike wins,”
which contains all the outcomes in which the horse Lucky Strike is ﬁrst.
Probability theory requires that the event space satisfy three basic properties:
•
It contains the empty event ∅, and the trivial event Ω.
•
It is closed under union. That is, if α, β ∈S, then so is α ∪β.
•
It is closed under complementation. That is, if α ∈S, then so is Ω−α.
The requirement that the event space is closed under union and complementation implies that
it is also closed under other Boolean operations, such as intersection and set diﬀerence.
2.1.1.2
Probability Distributions
Deﬁnition 2.1
A probability distribution P over (Ω, S) is a mapping from events in S to real values that satisﬁes
probability
distribution
the following conditions:
• P(α) ≥0 for all α ∈S.
• P(Ω) = 1.
• If α, β ∈S and α ∩β = ∅, then P(α ∪β) = P(α) + P(β).
The ﬁrst condition states that probabilities are not negative. The second states that the “trivial
event,” which allows all possible outcomes, has the maximal possible probability of 1. The third
condition states that the probability that one of two mutually disjoint events will occur is the
sum of the probabilities of each event. These two conditions imply many other conditions. Of
particular interest are P(∅) = 0, and P(α ∪β) = P(α) + P(β) −P(α ∩β).
2.1.1.3
Interpretations of Probability
Before we continue to discuss probability distributions, we need to consider the interpretations
that we might assign to them. Intuitively, the probability P(α) of an event α quantiﬁes the
degree of conﬁdence that α will occur. If P(α) = 1, we are certain that one of the outcomes
in α occurs, and if P(α) = 0, we consider all of them impossible. Other probability values
represent options that lie between these two extremes.
This description, however, does not provide an answer to what the numbers mean. There are
two common interpretations for probabilities.
The frequentist interpretation views probabilities as frequencies of events. More precisely, the
frequentist
interpretation
probability of an event is the fraction of times the event occurs if we repeat the experiment
indeﬁnitely. For example, suppose we consider the outcome of a particular die roll. In this case,
the statement P(α) = 0.3, for α = {1, 3, 5}, states that if we repeatedly roll this die and record
the outcome, then the fraction of times the outcomes in α will occur is 0.3. More precisely, the
limit of the sequence of fractions of outcomes in α in the ﬁrst roll, the ﬁrst two rolls, the ﬁrst
three rolls, . . ., the ﬁrst n rolls, . . . is 0.3.

2.1. Probability Theory
17
The frequentist interpretation gives probabilities a tangible semantics.
When we discuss
concrete physical systems (for example, dice, coin ﬂips, and card games) we can envision how
these frequencies are deﬁned. It is also relatively straightforward to check that frequencies must
satisfy the requirements of proper distributions.
The frequentist interpretation fails, however, when we consider events such as “It will rain
tomorrow afternoon.” Although the time span of “Tomorrow afternoon” is somewhat ill deﬁned,
we expect it to occur exactly once. It is not clear how we deﬁne the frequencies of such events.
Several attempts have been made to deﬁne the probability for such an event by ﬁnding a
reference class of similar events for which frequencies are well deﬁned; however, none of them
reference class
has proved entirely satisfactory. Thus, the frequentist approach does not provide a satisfactory
interpretation for a statement such as “the probability of rain tomorrow afternoon is 0.3.”

An alternative interpretation views probabilities as subjective degrees of belief. Under
subjective
interpretation
this interpretation, the statement P(α) = 0.3 represents a subjective statement about
one’s own degree of belief that the event α will come about. Thus, the statement “the
probability of rain tomorrow afternoon is 50 percent” tells us that in the opinion of the speaker,
the chances of rain and no rain tomorrow afternoon are the same. Although tomorrow afternoon
will occur only once, we can still have uncertainty about its outcome, and represent it using
numbers (that is, probabilities).
This description still does not resolve what exactly it means to hold a particular degree of
belief. What stops a person from stating that the probability that Bush will win the election
is 0.6 and the probability that he will lose is 0.8? The source of the problem is that we need
to explain how subjective degrees of beliefs (something that is internal to each one of us) are
reﬂected in our actions.
This issue is a major concern in subjective probabilities. One possible way of attributing
degrees of beliefs is by a betting game. Suppose you believe that P(α) = 0.8. Then you would
be willing to place a bet of $1 against $3. To see this, note that with probability 0.8 you gain a
dollar, and with probability 0.2 you lose $3, so on average this bet is a good deal with expected
gain of 20 cents. In fact, you might be even tempted to place a bet of $1 against $4. Under
this bet the average gain is 0, so you should not mind. However, you would not consider it
worthwhile to place a bet $1 against $4 and 10 cents, since that would have negative expected
gain. Thus, by ﬁnding which bets you are willing to place, we can assess your degrees of beliefs.
The key point of this mental game is the following. If you hold degrees of belief that do not
satisfy the rule of probability, then by a clever construction we can ﬁnd a series of bets that
would result in a sure negative outcome for you. Thus, the argument goes, a rational person
must hold degrees of belief that satisfy the rules of probability.1
In the remainder of the book we discuss probabilities, but we usually do not explicitly state
their interpretation. Since both interpretations lead to the same mathematical rules, the technical
deﬁnitions hold for both interpretations.
1. As stated, this argument assumes as that people’s preferences are directly proportional to their expected earnings. For
small amounts of money, this assumption is quite reasonable. We return to this topic in chapter 22.

18
Chapter 2. Foundations
2.1.2
Basic Concepts in Probability
2.1.2.1
Conditional Probability
To use a concrete example, suppose we consider a distribution over a population of students
taking a certain course. The space of outcomes is simply the set of all students in the population.
Now, suppose that we want to reason about the students’ intelligence and their ﬁnal grade. We
can deﬁne the event α to denote “all students with grade A,” and the event β to denote “all
students with high intelligence.” Using our distribution, we can consider the probability of these
events, as well as the probability of α ∩β (the set of intelligent students who got grade A). This,
however, does not directly tell us how to update our beliefs given new evidence. Suppose we
learn that a student has received the grade A; what does that tell us about her intelligence?
This kind of question arises every time we want to use distributions to reason about the real
world. More precisely, after learning that an event α is true, how do we change our probability
about β occurring?
The answer is via the notion of conditional probability.
Formally, the
conditional
probability
conditional probability of β given α is deﬁned as
P(β | α) = P(α ∩β)
P(α)
(2.1)
That is, the probability that β is true given that we know α is the relative proportion of outcomes
satisfying β among these that satisfy α. (Note that the conditional probability is not deﬁned
when P(α) = 0.)
The conditional probability given an event (say α) satisﬁes the properties of deﬁnition 2.1 (see
exercise 2.4), and thus it is a probability distribution by its own right. Hence, we can think
of the conditioning operation as taking one distribution and returning another over the same
probability space.
2.1.2.2
Chain Rule and Bayes Rule
From the deﬁnition of the conditional distribution, we immediately see that
P(α ∩β) = P(α)P(β | α).
(2.2)
This equality is known as the chain rule of conditional probabilities.
More generally, if
chain rule
α1, . . . , αk are events, then we can write
P(α1 ∩. . . ∩αk) = P(α1)P(α2 | α1) · · · P(αk | α1 ∩. . . ∩αk−1).
(2.3)
In other words, we can express the probability of a combination of several events in terms of the
probability of the ﬁrst, the probability of the second given the ﬁrst, and so on. It is important
to notice that we can expand this expression using any order of events — the result will remain
the same.
Another immediate consequence of the deﬁnition of conditional probability is Bayes’ rule
Bayes’ rule
P(α | β) = P(β | α)P(α)
P(β)
.
(2.4)

2.1. Probability Theory
19
A more general conditional version of Bayes’ rule, where all our probabilities are conditioned on
some background event γ, also holds:
P(α | β ∩γ) = P(β | α ∩γ)P(α | γ)
P(β | γ)
.
Bayes’ rule is important in that it allows us to compute the conditional probability P(α | β)
from the “inverse” conditional probability P(β | α).
Example 2.1
Consider the student population, and let Smart denote smart students and GradeA denote stu-
dents who got grade A. Assume we believe (perhaps based on estimates from past statistics) that
P(GradeA | Smart) = 0.6, and now we learn that a particular student received grade A. Can
we estimate the probability that the student is smart? According to Bayes’ rule, this depends on
our prior probability for students being smart (before we learn anything about them) and the
prior
prior probability of students receiving high grades. For example, suppose that P(Smart) = 0.3
and P(GradeA) = 0.2, then we have that P(Smart | GradeA) = 0.6 ∗0.3/0.2 = 0.9. That
is, an A grade strongly suggests that the student is smart. On the other hand, if the test was
easier and high grades were more common, say, P(GradeA) = 0.4 then we would get that
P(Smart | GradeA) = 0.6 ∗0.3/0.4 = 0.45, which is much less conclusive about the student.
Another classic example that shows the importance of this reasoning is in disease screening.
To see this, consider the following hypothetical example (none of the mentioned ﬁgures are
related to real statistics).
Example 2.2
Suppose that a tuberculosis (TB) skin test is 95 percent accurate. That is, if the patient is TB-infected,
then the test will be positive with probability 0.95, and if the patient is not infected, then the test
will be negative with probability 0.95. Now suppose that a person gets a positive test result. What is
the probability that he is infected? Naive reasoning suggests that if the test result is wrong 5 percent
of the time, then the probability that the subject is infected is 0.95. That is, 95 percent of subjects
with positive results have TB.
If we consider the problem by applying Bayes’ rule, we see that we need to consider the prior
probability of TB infection, and the probability of getting positive test result. Suppose that 1 in
1000 of the subjects who get tested is infected. That is, P(TB) = 0.001. What is the probability of
getting a positive test result? From our description, we see that 0.001 · 0.95 infected subjects get a
positive result, and 0.999·0.05 uninfected subjects get a positive result. Thus, P(Positive) = 0.0509.
Applying Bayes’ rule, we get that P(TB | Positive) = 0.001·0.95/0.0509 ≈0.0187. Thus, although
a subject with a positive test is much more probable to be TB-infected than is a random subject,
fewer than 2 percent of these subjects are TB-infected.
2.1.3
Random Variables and Joint Distributions
2.1.3.1
Motivation
Our discussion of probability distributions deals with events. Formally, we can consider any
event from the set of measurable events.
The description of events is in terms of sets of
outcomes.
In many cases, however, it would be more natural to consider attributes of the
outcome. For example, if we consider a patient, we might consider attributes such as “age,”

20
Chapter 2. Foundations
“gender,” and “smoking history” that are relevant for assigning probability over possible diseases
and symptoms. We would like then consider events such as “age > 55, heavy smoking history,
and suﬀers from repeated cough.”
To use a concrete example, consider again a distribution over a population of students in a
course. Suppose that we want to reason about the intelligence of students, their ﬁnal grades,
and so forth. We can use an event such as GradeA to denote the subset of students that received
the grade A and use it in our formulation. However, this discussion becomes rather cumbersome
if we also want to consider students with grade B, students with grade C, and so on. Instead, we
would like to consider a way of directly referring to a student’s grade in a clean, mathematical
way.
The formal machinery for discussing attributes and their values in diﬀerent outcomes are
random variables. A random variable is a way of reporting an attribute of the outcome. For
random variable
example, suppose we have a random variable Grade that reports the ﬁnal grade of a student,
then the statement P(Grade = A) is another notation for P(GradeA).
2.1.3.2
What Is a Random Variable?
Formally, a random variable, such as Grade, is deﬁned by a function that associates with each
outcome in Ωa value. For example, Grade is deﬁned by a function fGrade that maps each person
in Ωto his or her grade (say, one of A, B, or C). The event Grade = A is a shorthand for
the event {ω ∈Ω: fGrade(ω) = A}. In our example, we might also have a random variable
Intelligence that (for simplicity) takes as values either “high” or “low.” In this case, the event
“Intelligence = high” refers, as can be expected, to the set of smart (high intelligence) students.
Random variables can take diﬀerent sets of values. We can think of categorical (or discrete)
random variables that take one of a few values, as in our two preceding examples. We can also
talk about random variables that can take inﬁnitely many values (for example, integer or real
values), such as Height that denotes a student’s height. We use Val(X) to denote the set of
values that a random variable X can take.
In most of the discussion in this book we examine either categorical random variables or
random variables that take real values. We will usually use uppercase roman letters X, Y, Z
to denote random variables. In discussing generic random variables, we often use a lowercase
letter to refer to a value of a random variable. Thus, we use x to refer to a generic value of X.
For example, in statements such as “P(X = x) ≥0 for all x ∈Val(X).” When we discuss
categorical random variables, we use the notation x1, . . . , xk, for k = |Val(X)| (the number
of elements in Val(X)), when we need to enumerate the speciﬁc values of X, for example, in
statements such as
k
X
i=1
P(X = xi) = 1.
The distribution over such a variable is called a multinomial. In the case of a binary-valued
multinomial
distribution
random variable X, where Val(X) = {false, true}, we often use x1 to denote the value true for
X, and x0 to denote the value false. The distribution of such a random variable is called a
Bernoulli distribution.
Bernoulli
distribution
We also use boldface type to denote sets of random variables. Thus, X, Y , or Z are typically
used to denote a set of random variables, while x, y, z denote assignments of values to the

2.1. Probability Theory
21
variables in these sets. We extend the deﬁnition of Val(X) to refer to sets of variables in the
obvious way. Thus, x is always a member of Val(X). For Y ⊆X, we use x⟨Y ⟩to refer to the
assignment within x to the variables in Y . For two assignments x (to X) and y (to Y ), we say
that x ∼y if they agree on the variables in their intersection, that is, x⟨X ∩Y ⟩= y⟨X ∩Y ⟩.
In many cases, the notation P(X = x) is redundant, since the fact that x is a value of X
is already reported by our choice of letter. Thus, in many texts on probability, the identity of a
random variable is not explicitly mentioned, but can be inferred through the notation used for
its value. Thus, we use P(x) as a shorthand for P(X = x) when the identity of the random
variable is clear from the context.
Another shorthand notation is that P
x refers to a sum
over all possible values that X can take. Thus, the preceding statement will often appear as
P
x P(x) = 1. Finally, another standard notation has to do with conjunction. Rather than write
P((X = x) ∩(Y = y)), we write P(X = x, Y = y), or just P(x, y).
2.1.3.3
Marginal and Joint Distributions
Once we deﬁne a random variable X, we can consider the distribution over events that can be
described using X. This distribution is often referred to as the marginal distribution over the
marginal
distribution
random variable X. We denote this distribution by P(X).
Returning to our population example, consider the random variable Intelligence. The marginal
distribution over Intelligence assigns probability to speciﬁc events such as P(Intelligence = high)
and P(Intelligence = low), as well as to the trivial event P(Intelligence ∈{high, low}). Note
that these probabilities are deﬁned by the probability distribution over the original space. For
concreteness, suppose that P(Intelligence = high) = 0.3, P(Intelligence = low) = 0.7.
If we consider the random variable Grade, we can also deﬁne a marginal distribution. This is a
distribution over all events that can be described in terms of the Grade variable. In our example,
we have that P(Grade = A) = 0.25, P(Grade = B) = 0.37, and P(Grade = C) = 0.38.
It should be fairly obvious that the marginal distribution is a probability distribution satisfying
the properties of deﬁnition 2.1. In fact, the only change is that we restrict our attention to the
subsets of S that can be described with the random variable X.
In many situations, we are interested in questions that involve the values of several random
variables. For example, we might be interested in the event “Intelligence = high and Grade = A.”
To discuss such events, we need to consider the joint distribution over these two random
joint distribution
variables. In general, the joint distribution over a set X = {X1, . . . , Xn} of random variables
is denoted by P(X1, . . . , Xn) and is the distribution that assigns probabilities to events that
are speciﬁed in terms of these random variables. We use ξ to refer to a full assignment to the
variables in X, that is, ξ ∈Val(X).
The joint distribution of two random variables has to be consistent with the marginal distri-
bution, in that P(x) = P
y P(x, y). This relationship is shown in ﬁgure 2.1, where we compute
the marginal distribution over Grade by summing the probabilities along each row. Similarly,
we ﬁnd the marginal distribution over Intelligence by summing out along each column. The
resulting sums are typically written in the row or column margins, whence the term “marginal
distribution.”
Suppose we have a joint distribution over the variables X = {X1, . . . , Xn}.
The most
ﬁne-grained events we can discuss using these variables are ones of the form “X1 = x1 and
X2 = x2, . . ., and Xn = xn” for a choice of values x1, . . . , xn for all the variables. Moreover,

22
Chapter 2. Foundations
Intelligence
low
high
A
0.07
0.18
0.25
Grade
B
0.28
0.09
0.37
C
0.35
0.03
0.38
0.7
0.3
1
Figure 2.1
Example of a joint distribution P(Intelligence, Grade): Values of Intelligence (columns) and
Grade (rows) with the associated marginal distribution on each variable.
any two such events must be either identical or disjoint, since they both assign values to all the
variables in X. In addition, any event deﬁned using variables in X must be a union of a set of
such events. Thus, we are eﬀectively working in a canonical outcome space: a space where each
canonical
outcome space
outcome corresponds to a joint assignment to X1, . . . , Xn. More precisely, all our probability
computations remain the same whether we consider the original outcome space (for example,
all students), or the canonical space (for example, all combinations of intelligence and grade).
We use ξ to denote these atomic outcomes: those assigning a value to each variable in X. For
atomic outcome
example, if we let X = {Intelligence, Grade}, there are six atomic outcomes, shown in ﬁgure 2.1.
The ﬁgure also shows one possible joint distribution over these six outcomes.
Based on this discussion, from now on we will not explicitly specify the set of outcomes and
measurable events, and instead implicitly assume the canonical outcome space.
2.1.3.4
Conditional Probability
The notion of conditional probability extends to induced distributions over random variables. For
example, we use the notation P(Intelligence | Grade = A) to denote the conditional distribution
conditional
distribution
over the events describable by Intelligence given the knowledge that the student’s grade is A.
Note that the conditional distribution over a random variable given an observation of the value
of another one is not the same as the marginal distribution. In our example, P(Intelligence =
high) = 0.3, and P(Intelligence = high | Grade = A) = 0.18/0.25 = 0.72.
Thus, clearly
P(Intelligence | Grade = A) is diﬀerent from the marginal distribution P(Intelligence). The latter
distribution represents our prior knowledge about students before learning anything else about a
particular student, while the conditional distribution represents our more informed distribution
after learning her grade.
We will often use the notation P(X | Y ) to represent a set of conditional probability
distributions. Intuitively, for each value of Y , this object assigns a probability over values of X
using the conditional probability. This notation allows us to write the shorthand version of the
chain rule: P(X, Y ) = P(X)P(Y | X), which can be extended to multiple variables as
P(X1, . . . , Xk) = P(X1)P(X2 | X1) · · · P(Xk | X1, . . . , Xk−1).
(2.5)
Similarly, we can state Bayes’ rule in terms of conditional probability distributions:
P(X | Y ) = P(X)P(Y | X)
P(Y )
.
(2.6)

2.1. Probability Theory
23
2.1.4
Independence and Conditional Independence
2.1.4.1
Independence
As we mentioned, we usually expect P(α | β) to be diﬀerent from P(α). That is, learning that
β is true changes our probability over α. However, in some situations equality can occur, so
that P(α | β) = P(α). That is, learning that β occurs did not change our probability of α.
Deﬁnition 2.2
We say that an event α is independent of event β in P, denoted P |= (α ⊥β), if P(α | β) =
independent
events
P(α) or if P(β) = 0.
We can also provide an alternative deﬁnition for the concept of independence:
Proposition 2.1
A distribution P satisﬁes (α ⊥β) if and only if P(α ∩β) = P(α)P(β).
Proof Consider ﬁrst the case where P(β) = 0; here, we also have P(α ∩β) = 0, and so
the equivalence immediately holds. When P(β) ̸= 0, we can use the chain rule; we write
P(α∩β) = P(α | β)P(β). Since α is independent of β, we have that P(α | β) = P(α). Thus,
P(α ∩β) = P(α)P(β). Conversely, suppose that P(α ∩β) = P(α)P(β). Then, by deﬁnition,
we have that
P(α | β) = P(α ∩β)
P(β)
= P(α)P(β)
P(β)
= P(α).
As an immediate consequence of this alternative deﬁnition, we see that independence is a
symmetric notion. That is, (α ⊥β) implies (β ⊥α).
Example 2.3
For example, suppose that we toss two coins, and let α be the event “the ﬁrst toss results in a head”
and β the event “the second toss results in a head.” It is not hard to convince ourselves that we
expect that these two events to be independent. Learning that β is true would not change our
probability of α. In this case, we see two diﬀerent physical processes (that is, coin tosses) leading
to the events, which makes it intuitive that the probabilities of the two are independent. In certain
cases, the same process can lead to independent events. For example, consider the event α denoting
“the die outcome is even” and the event β denoting “the die outcome is 1 or 2.” It is easy to check
that if the die is fair (each of the six possible outcomes has probability 1
6), then these two events are
independent.
2.1.4.2
Conditional Independence

While independence is a useful property, it is not often that we encounter two indepen-
dent events. A more common situation is when two events are independent given an
additional event. For example, suppose we want to reason about the chance that our student
is accepted to graduate studies at Stanford or MIT. Denote by Stanford the event “admitted to
Stanford” and by MIT the event “admitted to MIT.” In most reasonable distributions, these two
events are not independent. If we learn that a student was admitted to Stanford, then our
estimate of her probability of being accepted at MIT is now higher, since it is a sign that she is
a promising student.

24
Chapter 2. Foundations
Now, suppose that both universities base their decisions only on the student’s grade point
average (GPA), and we know that our student has a GPA of A. In this case, we might argue
that learning that the student was admitted to Stanford should not change the probability that
she will be admitted to MIT: Her GPA already tells us the information relevant to her chances
of admission to MIT, and ﬁnding out about her admission to Stanford does not change that.
Formally, the statement is
P(MIT | Stanford, GradeA) = P(MIT | GradeA).
In this case, we say that MIT is conditionally independent of Stanford given GradeA.
Deﬁnition 2.3
We say that an event α is conditionally independent of event β given event γ in P, denoted
conditional
independence
P |= (α ⊥β | γ), if P(α | β ∩γ) = P(α | γ) or if P(β ∩γ) = 0.
It is easy to extend the arguments we have seen in the case of (unconditional) independencies
to give an alternative deﬁnition.
Proposition 2.2
P satisﬁes (α ⊥β | γ) if and only if P(α ∩β | γ) = P(α | γ)P(β | γ).
2.1.4.3
Independence of Random Variables
Until now, we have focused on independence between events. Thus, we can say that two events,
such as one toss landing heads and a second also landing heads, are independent. However, we
would like to say that any pair of outcomes of the coin tosses is independent. To capture such
statements, we can examine the generalization of independence to sets of random variables.
Deﬁnition 2.4
Let X, Y , Z be sets of random variables. We say that X is conditionally independent of Y given
conditional
independence
Z in a distribution P if P satisﬁes (X = x ⊥Y = y | Z = z) for all values x ∈Val(X),
y ∈Val(Y ), and z ∈Val(Z). The variables in the set Z are often said to be observed. If the set
observed variable
Z is empty, then instead of writing (X ⊥Y | ∅), we write (X ⊥Y ) and say that X and Y
are marginally independent.
marginal
independence
Thus, an independence statement over random variables is a universal quantiﬁcation over all
possible values of the random variables.
The alternative characterization of conditional independence follows immediately:
Proposition 2.3
The distribution P satisﬁes (X ⊥Y | Z) if and only if P(X, Y | Z) = P(X | Z)P(Y | Z).
Suppose we learn about a conditional independence. Can we conclude other independence
properties that must hold in the distribution? We have already seen one such example:
•
Symmetry:
symmetry
(X ⊥Y | Z) =⇒(Y ⊥X | Z).
(2.7)
There are several other properties that hold for conditional independence, and that often
provide a very clean method for proving important properties about distributions. Some key
properties are:

2.1. Probability Theory
25
•
Decomposition:
decomposition
(X ⊥Y , W | Z) =⇒(X ⊥Y | Z).
(2.8)
•
Weak union:
weak union
(X ⊥Y , W | Z) =⇒(X ⊥Y | Z, W ).
(2.9)
•
Contraction:
contraction
(X ⊥W | Z, Y ) & (X ⊥Y | Z) =⇒(X ⊥Y , W | Z).
(2.10)
An additional important property does not hold in general, but it does hold in an important
subclass of distributions.
Deﬁnition 2.5
A distribution P is said to be positive if for all events α ∈S such that α ̸= ∅, we have that
positive
distribution
P(α) > 0.
For positive distributions, we also have the following property:
•
Intersection: For positive distributions, and for mutually disjoint sets X, Y , Z, W :
intersection
(X ⊥Y | Z, W ) & (X ⊥W | Z, Y ) =⇒(X ⊥Y , W | Z).
(2.11)
The proof of these properties is not diﬃcult. For example, to prove Decomposition, assume
that (X ⊥Y, W | Z) holds. Then, from the deﬁnition of conditional independence, we have
that P(X, Y, W | Z) = P(X | Z)P(Y, W | Z). Now, using basic rules of probability and
arithmetic, we can show
P(X, Y | Z)
=
X
w
P(X, Y, w | Z)
=
X
w
P(X | Z)P(Y, w | Z)
=
P(X | Z)
X
w
P(Y, w | Z)
=
P(X | Z)P(Y | Z).
The only property we used here is called “reasoning by cases” (see exercise 2.6). We conclude
that (X ⊥Y | Z).
2.1.5
Querying a Distribution
Our focus throughout this book is on using a joint probability distribution over multiple random
variables to answer queries of interest.

26
Chapter 2. Foundations
2.1.5.1
Probability Queries
Perhaps the most common query type is the probability query. Such a query consists of two
probability query
parts:
•
The evidence: a subset E of random variables in the model, and an instantiation e to these
evidence
variables;
•
the query variables: a subset Y of random variables in the network.
query variables
Our task is to compute
P(Y | E = e),
that is, the posterior probability distribution over the values y of Y , conditioned on the fact that
posterior
distribution
E = e. This expression can also be viewed as the marginal over Y , in the distribution we
obtain by conditioning on e.
2.1.5.2
MAP Queries
A second important type of task is that of ﬁnding a high-probability joint assignment to some
subset of variables. The simplest variant of this type of task is the MAP query (also called
most probable explanation (MPE)), whose aim is to ﬁnd the MAP assignment — the most likely
MAP assignment
assignment to all of the (non-evidence) variables. More precisely, if we let W = X −E, our
task is to ﬁnd the most likely assignment to the variables in W given the evidence E = e:
MAP(W | e) = arg max
w P(w, e),
(2.12)
where, in general, arg maxx f(x) represents the value of x for which f(x) is maximal. Note
that there might be more than one assignment that has the highest posterior probability. In this
case, we can either decide that the MAP task is to return the set of possible assignments, or to
return an arbitrary member of that set.
It is important to understand the diﬀerence between MAP queries and probability queries. In
a MAP query, we are ﬁnding the most likely joint assignment to W . To ﬁnd the most likely
assignment to a single variable A, we could simply compute P(A | e) and then pick the most
likely value.
However, the assignment where each variable individually picks its most

likely value can be quite diﬀerent from the most likely joint assignment to all variables
simultaneously. This phenomenon can occur even in the simplest case, where we have no
evidence.
Example 2.4
Consider a two node chain A →B where A and B are both binary-valued. Assume that:
a0
a1
0.4
0.6
A
b0
b1
a0
0.1
0.9
a1
0.5
0.5
(2.13)
We can see that P(a1) > P(a0), so that MAP(A) = a1. However, MAP(A, B) = (a0, b1): Both
values of B have the same probability given a1. Thus, the most likely assignment containing a1 has
probability 0.6 × 0.5 = 0.3. On the other hand, the distribution over values of B is more skewed
given a0, and the most likely assignment (a0, b1) has the probability 0.4 × 0.9 = 0.36. Thus, we
have that arg maxa,b P(a, b) ̸= (arg maxa P(a), arg maxb P(b)).

2.1. Probability Theory
27
2.1.5.3
Marginal MAP Queries
To motivate our second query type, let us return to the phenomenon demonstrated in exam-
ple 2.4. Now, consider a medical diagnosis problem, where the most likely disease has multiple
possible symptoms, each of which occurs with some probability, but not an overwhelming prob-
ability. On the other hand, a somewhat rarer disease might have only a few symptoms, each
of which is very likely given the disease. As in our simple example, the MAP assignment to
the data and the symptoms might be higher for the second disease than for the ﬁrst one. The
solution here is to look for the most likely assignment to the disease variable(s) only, rather than
the most likely assignment to both the disease and symptom variables. This approach suggests
the use of a more general query type. In the marginal MAP query, we have a subset of variables
marginal MAP
Y that forms our query. The task is to ﬁnd the most likely assignment to the variables in Y
given the evidence E = e:
MAP(Y | e) = arg max
y
P(y | e).
If we let Z = X −Y −E, the marginal MAP task is to compute:
MAP(Y | e) = arg max
Y
X
Z
P(Y , Z | e).
Thus, marginal MAP queries contain both summations and maximizations; in a way, it contains
elements of both a conditional probability query and a MAP query.
Note that example 2.4 shows that marginal MAP assignments are not monotonic: the most
likely assignment MAP(Y1 | e) might be completely diﬀerent from the assignment to Y1 in
MAP({Y1, Y2} | e). Thus, in particular, we cannot use a MAP query to give us the correct
answer to a marginal MAP query.
2.1.6
Continuous Spaces
In the previous section, we focused on random variables that have a ﬁnite set of possible values.
In many situations, we also want to reason about continuous quantities such as weight, height,
duration, or cost that take real numbers in IR.
When dealing with probabilities over continuous random variables, we have to deal with some
technical issues. For example, suppose that we want to reason about a random variable X that
can take values in the range between 0 and 1. That is, Val(X) is the interval [0, 1]. Moreover,
assume that we want to assign each number in this range equal probability. What would be the
probability of a number x? Clearly, since each x has the same probability, and there are inﬁnite
number of values, we must have that P(X = x) = 0. This problem appears even if we do not
require uniform probability.
2.1.6.1
Probability Density Functions
How do we deﬁne probability over a continuous random variable?
We say that a function
p : IR 7→IR is a probability density function or (PDF) for X if it is a nonnegative integrable
density function

28
Chapter 2. Foundations
function such that
Z
Val(X)
p(x)dx = 1.
That is, the integral over the set of possible values of X is 1. The PDF deﬁnes a distribution for
X as follows: for any x in our event space:
P(X ≤a) =
a
Z
−∞
p(x)dx.
The function P is the cumulative distribution for X.
We can easily employ the rules of
cumulative
distribution
probability to see that by using the density function we can evaluate the probability of other
events. For example,
P(a ≤X ≤b) =
b
Z
a
p(x)dx.
Intuitively, the value of a PDF p(x) at a point x is the incremental amount that x adds to the
cumulative distribution in the integration process. The higher the value of p at and around x,
the more mass is added to the cumulative distribution as it passes x.
The simplest PDF is the uniform distribution.
Deﬁnition 2.6
A variable X has a uniform distribution over [a, b], denoted X ∼Unif[a, b] if it has the PDF
uniform
distribution
p(x) =

1
b−a
b ≥x ≥a
0
otherwise.
Thus, the probability of any subinterval of [a, b] is proportional its size relative to the size of
[a, b]. Note that, if b −a < 1, then the density can be greater than 1. Although this looks
unintuitive, this situation can occur even in a legal PDF, if the interval over which the value is
greater than 1 is not too large. We have only to satisfy the constraint that the total area under
the PDF is 1.
As a more complex example, consider the Gaussian distribution.
Deﬁnition 2.7
A random variable X has a Gaussian distribution with mean µ and variance σ2, denoted X ∼
Gaussian
distribution
N
 µ; σ2
, if it has the PDF
p(x) =
1
√
2πσ e−(x−µ)2
2σ2 .
A standard Gaussian is one with mean 0 and variance 1.
standard
Gaussian
A Gaussian distribution has a bell-like curve, where the mean parameter µ controls the
location of the peak, that is, the value for which the Gaussian gets its maximum value. The
variance parameter σ2 determines how peaked the Gaussian is: the smaller the variance, the

2.1. Probability Theory
29
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
–10
–5
10
5
0
(0,42)
(0,1)
(5,22)
Figure 2.2
Example PDF of three Gaussian distributions
more peaked the Gaussian. Figure 2.2 shows the probability density function of a few diﬀerent
Gaussian distributions.
More technically, the probability density function is speciﬁed as an exponential, where the
expression in the exponent corresponds to the square of the number of standard deviations σ
that x is away from the mean µ. The probability of x decreases exponentially with the square
of its deviation from the mean, as measured in units of its standard deviation.
2.1.6.2
Joint Density Functions
The discussion of density functions for a single variable naturally extends for joint distributions
of continuous random variables.
Deﬁnition 2.8
Let P be a joint distribution over continuous random variables X1, . . . , Xn. A function p(x1, . . . , xn)
is a joint density function of X1, . . . , Xn if
joint density
• p(x1, . . . , xn) ≥0 for all values x1, . . . , xn of X1, . . . , Xn.
• p is an integrable function.
• For any choice of a1, . . . , an, and b1, . . . , bn,
P(a1 ≤X1 ≤b1, . . . , an ≤Xn ≤bn) =
b1
Z
a1
· · ·
bn
Z
an
p(x1, . . . , xn)dx1 . . . dxn.
Thus, a joint density speciﬁes the probability of any joint event over the variables of interest.
Both the uniform distribution and the Gaussian distribution have natural extensions to the
multivariate case. The deﬁnition of a multivariate uniform distribution is straightforward. We
defer the deﬁnition of the multivariate Gaussian to section 7.1.
From the joint density we can derive the marginal density of any random variable by inte-
grating out the other variables. Thus, for example, if p(x, y) is the joint density of X and Y ,

30
Chapter 2. Foundations
then
p(x) =
∞
Z
−∞
p(x, y)dy.
To see why this equality holds, note that the event a ≤X ≤b is, by deﬁnition, equal to the
event “a ≤X ≤b and −∞≤Y ≤∞.” This rule is the direct analogue of marginalization for
discrete variables. Note that, as with discrete probability distributions, we abuse notation a bit
and use p to denote both the joint density of X and Y and the marginal density of X. In cases
where the distinction is not clear, we use subscripts, so that pX will be the marginal density, of
X, and pX,Y the joint density.
2.1.6.3
Conditional Density Functions
As with discrete random variables, we want to be able to describe conditional distributions of
continuous variables. Suppose, for example, we want to deﬁne P(Y | X = x). Applying the
deﬁnition of conditional distribution (equation (2.1)), we run into a problem, since P(X = x) =
0. Thus, the ratio of P(Y, X = x) and P(X = x) is undeﬁned.
To avoid this problem, we might consider conditioning on the event x −ϵ ≤X ≤x + ϵ,
which can have a positive probability. Now, the conditional probability is well deﬁned. Thus, we
might consider the limit of this quantity when ϵ →0. We deﬁne
P(Y | x) = lim
ϵ→0 P(Y | x −ϵ ≤X ≤x + ϵ).
When does this limit exist? If there is a continuous joint density function p(x, y), then we can
derive the form for this term. To do so, consider some event on Y , say a ≤Y ≤b. Recall that
P(a ≤Y ≤b | x −ϵ ≤X ≤x + ϵ)
=
P(a ≤Y ≤b, x −ϵ ≤X ≤x + ϵ)
P(x −ϵ ≤X ≤x + ϵ)
=
R b
a
R x+ϵ
x−ϵ p(x′, y)dydx′
R x+ϵ
x−ϵ p(x′)dx′
.
When ϵ is suﬃciently small, we can approximate
x+ϵ
Z
x−ϵ
p(x′)dx′ ≈2ϵp(x).
Using a similar approximation for p(x′, y), we get
P(a ≤Y ≤b | x −ϵ ≤X ≤x + ϵ)
≈
R b
a 2ϵp(x, y)dy
2ϵp(x)
=
b
Z
a
p(x, y)
p(x) dy.
We conclude that p(x,y)
p(x) is the density of P(Y | X = x).

2.1. Probability Theory
31
Deﬁnition 2.9
Let p(x, y) be the joint density of X and Y . The conditional density function of Y given X is
conditional
density function
deﬁned as
p(y | x) = p(x, y)
p(x)
When p(x) = 0, the conditional density is undeﬁned.
The conditional density p(y | x) characterizes the conditional distribution P(Y | X = x) we
deﬁned earlier.
The properties of joint distributions and conditional distributions carry over to joint and
conditional density functions. In particular, we have the chain rule
p(x, y) = p(x)p(y | x)
(2.14)
and Bayes’ rule
p(x | y) = p(x)p(y | x)
p(y)
.
(2.15)
As a general statement, whenever we discuss joint distributions of continuous random vari-
ables, we discuss properties with respect to the joint density function instead of the joint
distribution, as we do in the case of discrete variables. Of particular interest is the notion of
(conditional) independence of continuous random variables.
Deﬁnition 2.10
Let X, Y , and Z be sets of continuous random variables with joint density p(X, Y , Z). We say
that X is conditionally independent of Y given Z if
conditional
independence
p(x | z) = p(x | y, z) for all x, y, z such that p(z) > 0.
2.1.7
Expectation and Variance
2.1.7.1
Expectation
Let X be a discrete random variable that takes numerical values; then the expectation of X
expectation
under the distribution P is
IEP [X] =
X
x
x · P(x).
If X is a continuous variable, then we use the density function
IEP [X] =
Z
x · p(x)dx.
For example, if we consider X to be the outcome of rolling a fair die with probability 1/6
for each outcome, then IE[X] = 1 · 1
6 + 2 · 1
6 + · · · + 6 · 1
6 = 3.5. On the other hand, if
we consider a biased die where P(X = 6) = 0.5 and P(X = x) = 0.1 for x < 6, then
IE[X] = 1 · 0.1 + · · · + 5 · 0.1 + · · · + 6 · 0.5 = 4.5.

32
Chapter 2. Foundations
Often we are interested in expectations of a function of a random variable (or several random
variables). Thus, we might consider extending the deﬁnition to consider the expectation of a
functional term such as X2 + 0.5X. Note, however, that any function g of a set of random
variables X1, . . . , Xk is essentially deﬁning a new random variable Y : For any outcome ω ∈Ω,
we deﬁne the value of Y as g(fX1(ω), . . . , fXk(ω)).
Based on this discussion, we often deﬁne new random variables by a functional term. For
example Y = X2, or Y = eX. We can also consider functions that map values of one or more
categorical random variables to numerical values. One such function that we use quite often is
the indicator function, which we denote 11{X = x}. This function takes value 1 when X = x,
indicator function
and 0 otherwise.
In addition, we often consider expectations of functions of random variables without bothering
to name the random variables they deﬁne. For example IEP [X + Y ]. Nonetheless, we should
keep in mind that such a term does refer to an expectation of a random variable.
We now turn to examine properties of the expectation of a random variable.
First, as can be easily seen, the expectation of a random variable is a linear function in that
random variable. Thus,
IE[a · X + b] = aIE[X] + b.
A more complex situation is when we consider the expectation of a function of several random
variables that have some joint behavior.
An important property of expectation is that the
expectation of a sum of two random variables is the sum of the expectations.
Proposition 2.4
IE[X + Y ] = IE[X] + IE[Y ].
This property is called linearity of expectation. It is important to stress that this identity is true
linearity of
expectation
even when the variables are not independent. As we will see, this property is key in simplifying
many seemingly complex problems.
Finally, what can we say about the expectation of a product of two random variables? In
general, very little:
Example 2.5
Consider two random variables X and Y , each of which takes the value +1 with probability 1/2,
and the value −1 with probability 1/2. If X and Y are independent, then IE[X · Y ] = 0. On the
other hand, if X and Y are correlated in that they always take the same value, then IE[X · Y ] = 1.
However, when X and Y are independent, then, as in our example, we can compute the
expectation simply as a product of their individual expectations:
Proposition 2.5
If X and Y are independent, then
IE[X · Y ] = IE[X] · IE[Y ].
We often also use the expectation given some evidence. The conditional expectation of X
conditional
expectation
given y is
IEP [X | y] =
X
x
x · P(x | y).

2.1. Probability Theory
33
2.1.7.2
Variance
The expectation of X tells us the mean value of X. However, It does not indicate how far X
deviates from this value. A measure of this deviation is the variance of X.
variance
VVarP [X] = IEP
h
(X −IEP [X])2i
.
Thus, the variance is the expectation of the squared diﬀerence between X and its expected
value. It gives us an indication of the spread of values of X around the expected value.
An alternative formulation of the variance is
VVar[X] = IE

X2
−(IE[X])2 .
(2.16)
(see exercise 2.11).
Similar to the expectation, we can consider the expectation of a functions of random variables.
Proposition 2.6
If X and Y are independent, then
VVar[X + Y ] = VVar[X] + VVar[Y ].
It is straightforward to show that the variance scales as a quadratic function of X.
In
particular, we have:
VVar[a · X + b] = a2VVar[X].
For this reason, we are often interested in the square root of the variance, which is called the
standard deviation of the random variable. We deﬁne
standard
deviation
σX =
p
VVar[X].
The intuition is that it is improbable to encounter values of X that are farther than several
standard deviations from the expected value of X.
Thus, σX is a normalized measure of
“distance” from the expected value of X.
As an example consider the Gaussian distribution of deﬁnition 2.7.
Proposition 2.7
Let X be a random variable with Gaussian distribution N(µ, σ2), then IE[X] = µ and VVar[X] =
σ2.
Thus, the parameters of the Gaussian distribution specify the expectation and the variance of
the distribution. As we can see from the form of the distribution, the density of values of X
drops exponentially fast in the distance x−µ
σ .
Not all distributions show such a rapid decline in the probability of outcomes that are distant
from the expectation. However, even for arbitrary distributions, one can show that there is a
decline.
Theorem 2.1
(Chebyshev inequality):
Chebyshev’s
inequality
P(|X −IEP [X]| ≥t) ≤VVarP [X]
t2
.

34
Chapter 2. Foundations
We can restate this inequality in terms of standard deviations: We write t = kσX to get
P(|X −IEP [X]| ≥kσX) ≤1
k2 .
Thus, for example, the probability of X being more than two standard deviations away from
IE[X] is less than 1/4.
2.2
Graphs
Perhaps the most pervasive concept in this book is the representation of a probability distribution
using a graph as a data structure. In this section, we survey some of the basic concepts in graph
theory used in the book.
2.2.1
Nodes and Edges
A graph is a data structure K consisting of a set of nodes and a set of edges. Throughout most
this book, we will assume that the set of nodes is X = {X1, . . . , Xn}. A pair of nodes Xi, Xj
can be connected by a directed edge Xi →Xj or an undirected edge Xi—Xj. Thus, the set
directed edge
undirected edge
of edges E is a set of pairs, where each pair is one of Xi →Xj, Xj →Xi, or Xi—Xj, for
Xi, Xj ∈X, i < j. We assume throughout the book that, for each pair of nodes Xi, Xj, at
most one type of edge exists; thus, we cannot have both Xi →Xj and Xj →Xi, nor can
we have Xi →Xj and Xi—Xj.2 The notation Xi ←Xj is equivalent to Xj →Xi, and the
notation Xj—Xi is equivalent to Xi—Xj. We use Xi ⇌Xj to represent the case where Xi
and Xj are connected via some edge, whether directed (in any direction) or undirected.
In many cases, we want to restrict attention to graphs that contain only edges of one kind
or another. We say that a graph is directed if all edges are either Xi →Xj or Xj →Xi. We
directed graph
usually denote directed graphs as G. We say that a graph is undirected if all edges are Xi—Xj.
undirected graph
We denote undirected graphs as H. We sometimes convert a general graph to an undirected
graph by ignoring the directions on the edges.
Deﬁnition 2.11
Given a graph K = (X, E), its undirected version is a graph H = (X, E′) where E′ = {X—Y :
graph’s
undirected
version
X ⇌Y ∈E}.
Whenever we have that Xi →Xj ∈E, we say that Xj is the child of Xi in K, and that
child
Xi is the parent of Xj in K. When we have Xi—Xj ∈E, we say that Xi is a neighbor of
parent
neighbor
Xj in K (and vice versa). We say that X and Y are adjacent whenever X ⇌Y ∈E. We use
PaX to denote the parents of X, ChX to denote its children, and NbX to denote its neighbors.
We deﬁne the boundary of X, denoted BoundaryX, to be PaX ∪NbX; for DAGs, this set is
boundary
simply X’s parents, and for undirected graphs X’s neighbors.3 Figure 2.3 shows an example of
a graph K. There, we have that A is the only parent of C, and F, I are the children of C. The
only neighbor of C is D, but its adjacent nodes are A, D, F, I. The degree of a node X is the
degree
number of edges in which it participates. Its indegree is the number of directed edges Y →X.
indegree
The degree of a graph is the maximal degree of a node in the graph.
2. Note that our deﬁnition is somewhat restricted, in that it disallows cycles of length two, where Xi →Xj →Xi,
and allows self-loops where Xi →Xi.
3. When the graph is not clear from context, we often add the graph as an additional argument.

2.2. Graphs
35
C
H
E
A
B
D
F
I
G
Figure 2.3
An example of a partially directed graph K
C
B
A
B
A
D
H
E
D
C
E
D
C
I
I
(c)
(b)
(a)
Figure 2.4
Induced graphs and their upward closure: (a) The induced subgraph K[C, D, I]. (b) The
upwardly closed subgraph K+[C]. (c) The upwardly closed subgraph K+[C, D, I].
2.2.2
Subgraphs
In many cases, we want to consider only the part of the graph that is associated with a particular
subset of the nodes.
Deﬁnition 2.12
Let K = (X, E), and let X ⊂X. We deﬁne the induced subgraph K[X] to be the graph (X, E′)
induced
subgraph
where E′ are all the edges X ⇌Y ∈E such that X, Y ∈X.
For example, ﬁgure 2.4a shows the induced subgraph K[C, D, I].
A type of subgraph that is often of particular interest is one that contains all possible edges.
Deﬁnition 2.13
A subgraph over X is complete if every two nodes in X are connected by some edge. The set X
complete
subgraph
is often called a clique; we say that a clique X is maximal if for any superset of nodes Y ⊃X,
clique
Y is not a clique.
Although the subset of nodes X can be arbitrary, we are often interested in sets of nodes
that preserve certain aspects of the graph structure.
Deﬁnition 2.14
We say that a subset of nodes X ∈X is upwardly closed in K if, for any X ∈X, we have that
BoundaryX ⊂X. We deﬁne the upward closure of X to be the minimal upwardly closed subset
upward closure

36
Chapter 2. Foundations
Y that contains X. We deﬁne the upwardly closed subgraph of X, denoted K+[X], to be the
induced subgraph over Y , K[Y ].
For example, the set A, B, C, D, E is the upward closure of the set {C} in K. The upwardly
closed subgraph of {C} is shown in ﬁgure 2.4b. The upwardly closed subgraph of {C, D, I} is
shown in ﬁgure 2.4c.
2.2.3
Paths and Trails
Using the basic notion of edges, we can deﬁne diﬀerent types of longer-range connections in
the graph.
Deﬁnition 2.15
We say that X1, . . . , Xk form a path in the graph K = (X, E) if, for every i = 1, . . . , k −1,
path
we have that either Xi →Xi+1 or Xi—Xi+1. A path is directed if, for at least one i, we have
Xi →Xi+1.
Deﬁnition 2.16
We say that X1, . . . , Xk form a trail in the graph K = (X, E) if, for every i = 1, . . . , k −1, we
trail
have that Xi ⇌Xi+1.
In the graph K of ﬁgure 2.3, A, C, D, E, I is a path, and hence also a trail. On the other hand,
A, C, F, G, D is a trail, which is not a path.
Deﬁnition 2.17
A graph is connected if for every Xi, Xj there is a trail between Xi and Xj.
connected graph
We can now deﬁne longer-range relationships in the graph.
Deﬁnition 2.18
We say that X is an ancestor of Y in K = (X, E), and that Y is a descendant of X, if there
ancestor
descendant
exists a directed path X1, . . . , Xk with X1 = X and Xk = Y . We use DescendantsX to denote
X’s descendants, AncestorsX to denote X’s ancestors, and NonDescendantsX to denote the set of
nodes in X −DescendantsX.
In our example graph K, we have that F, G, I are descendants of C. The ancestors of C are A,
via the path A, C, and B, via the path B, E, D, C.
A ﬁnal useful notion is that of an ordering of the nodes in a directed graph that is consistent
with the directionality its edges.
Deﬁnition 2.19
Let G = (X, E) be a graph. An ordering of the nodes X1, . . . , Xn is a topological ordering relative
topological
ordering
to K if, whenever we have Xi →Xj ∈E, then i < j.
Appendix A.3.1 presents an algorithm for ﬁnding such a topological ordering.
2.2.4
Cycles and Loops
Note that, in general, we can have a cyclic path that leads from a node to itself, making that
node its own descendant.

2.2. Graphs
37
Deﬁnition 2.20
A cycle in K is a directed path X1, . . . , Xk where X1 = Xk. A graph is acyclic if it contains no
cycle
acyclic
cycles.
For most of this book, we will restrict attention to graphs that do not allow such cycles, since it
is quite diﬃcult to deﬁne a coherent probabilistic model over graphs with directed cycles.
A directed acyclic graph (DAG) is one of the central concepts in this book, as DAGs are the
DAG
basic graphical representation that underlies Bayesian networks. For some of this book, we also
use acyclic graphs that are partially directed. The graph K of ﬁgure 2.3 is acyclic. However, if
we add the undirected edge A—E to K, we have a path A, C, D, E, A from A to itself. Clearly,
adding a directed edge E →A would also lead to a cycle. Note that prohibiting cycles does
not imply that there is no trail from a node to itself. For example, K contains several trails:
C, D, E, I, C as well as C, D, G, F, C.
An acyclic graph containing both directed and undirected edges is called a partially directed
acyclic graph or PDAG. The acyclicity requirement on a PDAG implies that the graph can be
PDAG
decomposed into a directed graph of chain components, where the nodes within each chain
chain component
component are connected to each other only with undirected edges. The acyclicity of a PDAG
guarantees us that we can order the components so that all edges point from lower-numbered
components to higher-numbered ones.
Deﬁnition 2.21
Let K be a PDAG over X. Let K1, . . . , Kℓbe a disjoint partition of X such that:
• the induced subgraph over Ki contains no directed edges;
• for any pair of nodes X ∈Ki and Y ∈Kj for i < j, an edge between X and Y can only
be a directed edge X →Y .
Each component Ki is called a chain component.
chain component
Because of its chain structure, a PDAG is also called a chain graph.
chain graph
Example 2.6
In the PDAG of ﬁgure 2.3, we have six chain components: {A}, {B}, {C, D, E}, {F, G}, {H},
and {I}. This ordering of the chain components is one of several possible legal orderings.
Note that when the PDAG is an undirected graph, the entire graph forms a single chain
component. Conversely, when the PDAG is a directed graph (and therefore acyclic), each node
in the graph is its own chain component.

38
Chapter 2. Foundations
Figure 2.5
An example of a polytree
Diﬀerent from a cycle is the notion of a loop:
Deﬁnition 2.22
A loop in K is a trail X1, . . . , Xk where X1 = Xk. A graph is singly connected if it contains
loop
singly connected
no loops. A node in a singly connected graph is called a leaf if it has exactly one adjacent node.
leaf
A singly connected directed graph is also called a polytree. A singly connected undirected graph is
polytree
called a forest; if it is also connected, it is called a tree.
forest
tree
We can also deﬁne a notion of a forest, or of a tree, for directed graphs.
Deﬁnition 2.23
A directed graph is a forest if each node has at most one parent. A directed forest is a tree if it is
also connected.
Note that polytrees are very diﬀerent from trees. For example, ﬁgure 2.5 shows a graph that is a
polytree but is not a tree, because several nodes have more than one parent. As we will discuss
later in the book, loops in the graph increase the computational cost of various tasks.
We conclude this section with a ﬁnal deﬁnition relating to loops in the graph. This deﬁnition
will play an important role in evaluating the cost of reasoning using graph-based representations.
Deﬁnition 2.24
Let X1—X2— · · · —Xk—X1 be a loop in the graph; a chord in the loop is an edge connecting
Xi and Xj for two nonconsecutive nodes Xi, Xj. An undirected graph H is said to be chordal if
chordal graph
any loop X1—X2— · · · —Xk—X1 for k ≥4 has a chord.
Thus, for example, a loop A—B—C—D—A (as in ﬁgure 1.1b) is nonchordal, but adding an
edge A—C would render it chordal. In other words, in a chordal graph, the longest “minimal
loop” (one that has no shortcut) is a triangle.
Thus, chordal graphs are often also called
triangulated.
triangulated
graph
We can extend the notion of chordal graphs to graphs that contain directed edges.
Deﬁnition 2.25
A graph K is said to be chordal if its underlying undirected graph is chordal.

2.3. Relevant Literature
39
2.3
Relevant Literature
Section 1.4 provides some history on the development of of probabilistic methods. There are
many good textbooks about probability theory; see, for example, DeGroot (1989), Ross (1988) or
Feller (1970). The distinction between the frequentist and subjective view of probability was a
major issue during much of the late nineteenth and early twentieth centuries. Some references
that touch on this discussion include Cox (2001) and Jaynes (2003) on the Bayesian side, and
Feller (1970) on the frequentist side; these books also contain much useful general material about
probabilistic reasoning.
Dawid (1979, 1980) was the ﬁrst to propose the axiomatization of conditional independence
properties, and he showed how they can help unify a variety of topics within probability and
statistics.
These axioms were studied in great detail by Pearl and colleagues, work that is
presented in detail in Pearl (1988).
2.4
Exercises
Exercise 2.1
Prove the following properties using basic properties of deﬁnition 2.1:
a. P(∅) = 0.
b. If α ⊆β, then P(α) ≤P(β).
c. P(α ∪β) = P(α) + P(β) −P(α ∩β).
Exercise 2.2
a. Show that for binary random variables X, Y , the event-level independence (x0 ⊥y0) implies random-
variable independence (X ⊥Y ).
b. Show a counterexample for nonbinary variables.
c. Is it the case that, for a binary-valued variable Z, we have that (X ⊥Y | z0) implies (X ⊥Y | Z)?
Exercise 2.3
Consider two events α and β such that P(α) = pa and P(β) = pb. Given only that knowledge, what is
the maximum and minimum values of the probability of the events α∩β and α∪β. Can you characterize
the situations in which each of these extreme values occurs?
Exercise 2.4⋆
Let P be a distribution over (Ω, S), and let a ∈S be an event such that P(α) > 0. The conditional
probability P(· | α) assigns a value to each event in S. Show that it satisﬁes the properties of deﬁnition 2.1.
Exercise 2.5
Let X, Y , Z be three disjoint subsets of variables such that X = X ∪Y ∪Z. Prove that P |= (X ⊥
Y | Z) if and only if we can write P in the form:
P(X) = φ1(X, Z)φ2(Y , Z).
Exercise 2.6
An often useful rule in dealing with probabilities is known as reasoning by cases. Let X, Y , and Z be
random variables, then
P(X | Y ) =
X
z
P(X, z | Y ).
Prove this equality using the chain rule of probabilities and basic properties of (conditional) distributions.

40
Chapter 2. Foundations
Exercise 2.7⋆
In this exercise, you will prove the properties of conditional independence discussed in section 2.1.4.3.
a. Prove that the weak union and contraction properties hold for any probability distribution P.
b. Prove that the intersection property holds for any positive probability distribution P.
c. Provide a counterexample to the intersection property in cases where the distribution P is not positive.
Exercise 2.8
a. Show that for binary random variables X and Y , (x1 ⊥y1) implies (X ⊥Y ).
b. Provide a counterexample to this property for nonbinary variables.
c. Is it the case that, for binary Z, (X ⊥Y | z1) implies (X ⊥Y | Z)?
Prove or provide a
counterexample.
Exercise 2.9
Show how you can use breadth-ﬁrst search to determine whether a graph K is cyclic.
Exercise 2.10⋆
In appendix A.3.1, we describe an algorithm for ﬁnding a topological ordering for a directed graph. Extend
this algorithm to one that ﬁnds a topological ordering for the chain components in a PDAG. Your algorithm
should construct both the chain components of the PDAG, as well as an ordering over them that satisﬁes
the conditions of deﬁnition 2.21. Analyze the asymptotic complexity of your algorithm.
Exercise 2.11
Use the properties of expectation to show that we can rewrite the variance of a random variable X as
VVar[X] = IE

X2
−(IE[X])2 .
Exercise 2.12⋆
Prove the following property of expectations
Theorem 2.2
(Markov inequality): Let X be a random variable such that P(X ≥0) = 1, then for any t ≥0,
Markov inequality
P(X ≥t) ≤IEP [X]
t
.
You may assume in your proof that X is a discrete random variable with a ﬁnite number of values.
Exercise 2.13
Prove Chebyshev’s inequality using the Markov inequality shown in exercise 2.12. (Hint: deﬁne a new
random variable Y , so that the application of the Markov inequality with respect to this random variable
gives the required result.)
Exercise 2.14⋆
Let X ∼N
 µ; σ2
, and deﬁne a new variable Y = a · X + b. Show that Y ∼N
 a · µ + b; a2σ2
.

2.4. Exercises
41
Exercise 2.15⋆
A function f is concave if for any 0 ≤α ≤1 and any x and y, we have that f(αx+(1−α)y) ≥αf(x)+
concave function
(1−α)f(y). A function is convex if the opposite holds, that is, f(αx+(1−α)y) ≤αf(x)+(1−α)f(y).
convex function
a. Prove that a continuous and diﬀerentiable function f is concave if and only if f ′′(x) ≤0 for all x.
b. Show that log(x) is concave (over the positive real numbers).
Exercise 2.16⋆
Proposition 2.8
Jensen’s inequality: Let f be a concave function and P a distribution over a random variable X. Then
Jensen inequality
IEP [f(X)] ≤f(IEP [X])
Use this inequality to show that:
a. IHP (X) ≤log |Val(X)|.
b. IHP (X) ≥0.
c. ID(P||Q) ≥0.
See appendix A.1 for the basic deﬁnitions.
Exercise 2.17
Show that, for any probability distribution P(X), we have that
IHP (X) = log K −ID(P(X)||Pu(X))
where Pu(X) is the uniform distribution over Val(X) and K = |Val(X)|.
Exercise 2.18⋆
Prove proposition A.3, and use it to show that II(X; Y ) ≥0.
Exercise 2.19
As with entropies, we can deﬁne the notion of conditional mutual information
conditional
mutual
information
IIP (X; Y | Z) = IEP

log P(X | Y, Z)
P(X | Z)

.
Prove that:
a. IIP (X; Y | Z) = IHP (X | Z) −IHP (X, Y | Z).
b. The chain rule of mutual information:
chain rule of
mutual
information
IIP (X; Y, Z) = IIP (X; Y ) + IIP (X; Z | Y ).
Exercise 2.20
Use the chain law of mutual information to prove that
IIP (X; Y ) ≤IIP (X; Y, Z).
That is, the information that Y and Z together convey about X cannot be less than what Y alone conveys
about X.

42
Chapter 2. Foundations
Exercise 2.21⋆
Consider a sequence of N independent samples from a binary random variable X whose distribution is
P(x1) = p, P(x0) = 1 −p. As in appendix A.2, let SN be the number of trials whose outcome is x1.
Show that
P(SN = r) ≈exp[−N · ID((p, 1 −p)||(r/N, 1 −r/N))].
Your proof should use Stirling’s approximation to the factorial function:
m! ≈
1
2πmmme−m.

Part I
Representation


3
The Bayesian Network Representation
Our goal is to represent a joint distribution P over some set of random variables X =
{X1, . . . , Xn}.
Even in the simplest case where these variables are binary-valued, a joint
distribution requires the speciﬁcation of 2n −1 numbers — the probabilities of the 2n diﬀerent
assignments of values x1, . . . , xn. For all but the smallest n, the explicit representation of

the joint distribution is unmanageable from every perspective. Computationally, it is very
expensive to manipulate and generally too large to store in memory. Cognitively, it is
impossible to acquire so many numbers from a human expert; moreover, the numbers
are very small and do not correspond to events that people can reasonably contemplate.
Statistically, if we want to learn the distribution from data, we would need ridiculously
large amounts of data to estimate this many parameters robustly. These problems were
the main barrier to the adoption of probabilistic methods for expert systems until the
development of the methodologies described in this book.
In this chapter, we ﬁrst show how independence properties in the distribution can be used
to represent such high-dimensional distributions much more compactly. We then show how
a combinatorial data structure — a directed acyclic graph — can provide us with a general-
purpose modeling language for exploiting this type of structure in our representation.
3.1
Exploiting Independence Properties
The compact representations we explore in this chapter are based on two key ideas: the repre-
sentation of independence properties of the distribution, and the use of an alternative parame-
terization that allows us to exploit these ﬁner-grained independencies.
3.1.1
Independent Random Variables
To motivate our discussion, consider a simple setting where we know that each Xi represents
the outcome of a toss of coin i. In this case, we typically assume that the diﬀerent coin tosses
are marginally independent (deﬁnition 2.4), so that our distribution P will satisfy (Xi ⊥Xj)
for any i, j. More generally (strictly more generally — see exercise 3.1), we assume that the
distribution satisﬁes (X ⊥Y ) for any disjoint subsets of the variables X and Y . Therefore,
we have that:
P(X1, . . . , Xn) = P(X1)P(X2) · · · P(Xn).

46
Chapter 3. The Bayesian Network Representation
If we use the standard parameterization of the joint distribution, this independence structure
is obscured, and the representation of the distribution requires 2n parameters. However, we
can use a more natural set of parameters for specifying this distribution: If θi is the probability
with which coin i lands heads, the joint distribution P can be speciﬁed using the n parameters
parameters
θ1, . . . , θn. These parameters implicitly specify the 2n probabilities in the joint distribution. For
example, the probability that all of the coin tosses land heads is simply θ1 · θ2 · . . . · θn. More
generally, letting θxi = θi when xi = x1
i and θxi = 1 −θi when xi = x0
i , we can deﬁne:
P(x1, . . . , xn) =
Y
i
θxi.
(3.1)
This representation is limited, and there are many distributions that we cannot capture by
choosing values for θ1, . . . , θn. This fact is obvious not only from intuition, but also from a
somewhat more formal perspective. The space of all joint distributions is a 2n −1 dimensional
subspace of IR2n — the set {(p1, . . . , p2n) ∈IR2n
:
p1 + . . . + p2n = 1}. On the other
hand, the space of all joint distributions speciﬁed in a factorized way as in equation (3.1) is an
n-dimensional manifold in IR2n.
A key concept here is the notion of independent parameters — parameters whose values are
independent
parameters
not determined by others. For example, when specifying an arbitrary multinomial distribution
over a k dimensional space, we have k −1 independent parameters: the last probability is fully
determined by the ﬁrst k −1. In the case where we have an arbitrary joint distribution over
n binary random variables, the number of independent parameters is 2n −1. On the other
hand, the number of independent parameters for distributions represented as n independent
binomial coin tosses is n. Therefore, the two spaces of distributions cannot be the same. (While
this argument might seem trivial in this simple case, it turns out to be an important tool for
comparing the expressive power of diﬀerent representations.)
As this simple example shows, certain families of distributions — in this case, the distributions
generated by n independent random variables — permit an alternative parameterization that is
substantially more compact than the naive representation as an explicit joint distribution. Of
course, in most real-world applications, the random variables are not marginally independent.
However, a generalization of this approach will be the basis for our solution.
3.1.2
The Conditional Parameterization
Let us begin with a simple example that illustrates the basic intuition. Consider the problem
faced by a company trying to hire a recent college graduate. The company’s goal is to hire
intelligent employees, but there is no way to test intelligence directly. However, the company
has access to the student’s SAT scores, which are informative but not fully indicative. Thus,
our probability space is induced by the two random variables Intelligence (I) and SAT (S). For
simplicity, we assume that each of these takes two values: Val(I) = {i1, i0}, which represent
the values high intelligence (i1) and low intelligence (i0); similarly Val(S) = {s1, s0}, which
also represent the values high (score) and low (score), respectively.
Thus, our joint distribution in this case has four entries. For example, one possible joint

3.1. Exploiting Independence Properties
47
distribution P would be
I
S
P(I, S)
i0
s0
0.665
i0
s1
0.035
i1
s0
0.06
i1
s1
0.24.
(3.2)
There is, however, an alternative, and even more natural way of representing the same joint
distribution. Using the chain rule of conditional probabilities (see equation (2.5)), we have that
P(I, S) = P(I)P(S | I).
Intuitively, we are representing the process in a way that is more compatible with causality. Var-
ious factors (genetics, upbringing, . . . ) ﬁrst determined (stochastically) the student’s intelligence.
His performance on the SAT is determined (stochastically) by his intelligence. We note that the
models we construct are not required to follow causal intuitions, but they often do. We return
to this issue later on.
From a mathematical perspective, this equation leads to the following alternative way of
representing the joint distribution. Instead of specifying the various joint entries P(I, S), we
would specify it in the form of P(I) and P(S | I). Thus, for example, we can represent the
joint distribution of equation (3.2) using the following two tables, one representing the prior
prior distribution
distribution over I and the other the conditional probability distribution (CPD) of S given I:
CPD
i0
i1
0.7
0.3
I
s0
s1
i0
0.95
0.05
i1
0.2
0.8
(3.3)
The CPD P(S | I) represents the probability that the student will succeed on his SATs in the
two possible cases: the case where the student’s intelligence is low, and the case where it is
high. The CPD asserts that a student of low intelligence is extremely unlikely to get a high SAT
score (P(s1 | i0) = 0.05); on the other hand, a student of high intelligence is likely, but far from
certain, to get a high SAT score (P(s1 | i1) = 0.8).
It is instructive to consider how we could parameterize this alternative representation. Here,
we are using three binomial distributions, one for P(I), and two for P(S | i0) and P(S | i1).
Hence, we can parameterize this representation using three independent parameters, say θi1,
θs1|i1, and θs1|i0. Our representation of the joint distribution as a four-outcome multinomial
also required three parameters. Thus, although the conditional representation is more natural
than the explicit representation of the joint, it is not more compact. However, as we will soon
see, the conditional parameterization provides a basis for our compact representations of more
complex distributions.
Although we will only deﬁne Bayesian networks formally in section 3.2.2, it is instructive
to see how this example would be represented as one. The Bayesian network, as shown in
ﬁgure 3.1a, would have a node for each of the two random variables I and S, with an edge from
I to S representing the direction of the dependence in this model.

48
Chapter 3. The Bayesian Network Representation
Intelligence
SAT
Grade
SAT
Intelligence
(a)
(b)
Figure 3.1
Simple Bayesian networks for the student example
3.1.3
The Naive Bayes Model
We now describe perhaps the simplest example where a conditional parameterization is com-
bined with conditional independence assumptions to produce a very compact representation
of a high-dimensional probability distribution.
Importantly, unlike the previous example of
fully independent random variables, none of the variables in this distribution are (marginally)
independent.
3.1.3.1
The Student Example
Elaborating our example, we now assume that the company also has access to the student’s
grade G in some course. In this case, our probability space is the joint distribution over the
three relevant random variables I, S, and G. Assuming that I and S are as before, and that
G takes on three values g1, g2, g3, representing the grades A, B, and C, respectively, then the
joint distribution has twelve entries.
Before we even consider the speciﬁc numerical aspects of our distribution P in this example,
we can see that independence does not help us: for any reasonable P, there are no indepen-
dencies that hold. The student’s intelligence is clearly correlated both with his SAT score and
with his grade. The SAT score and grade are also not independent: if we condition on the fact
that the student received a high score on his SAT, the chances that he gets a high grade in his
class are also likely to increase. Thus, we may assume that, for our particular distribution P,
P(g1 | s1) > P(g1 | s0).
However, it is quite plausible that our distribution P in this case satisﬁes a conditional
independence property. If we know that the student has high intelligence, a high grade on the
SAT no longer gives us information about the student’s performance in the class. More formally:
P(g | i1, s1) = P(g | i1).
More generally, we may well assume that
P |= (S ⊥G | I).
(3.4)
Note that this independence statement holds only if we assume that the student’s intelligence
is the only reason why his grade and SAT score might be correlated. In other words, it assumes
that there are no correlations due to other factors, such as the student’s ability to take timed
exams. These assumptions are also not “true” in any formal sense of the word, and they are
often only approximations of our true beliefs. (See box 3.C for some further discussion.)

3.1. Exploiting Independence Properties
49
As in the case of marginal independence, conditional independence allows us to provide a
compact speciﬁcation of the joint distribution. Again, the compact representation is based on a
very natural alternative parameterization. By simple probabilistic reasoning (as in equation (2.5)),
we have that
P(I, S, G) = P(S, G | I)P(I).
But now, the conditional independence assumption of equation (3.4) implies that
P(S, G | I) = P(S | I)P(G | I).
Hence, we have that
P(I, S, G) = P(S | I)P(G | I)P(I).
(3.5)
Thus, we have factorized the joint distribution P(I, S, G) as a product of three conditional
probability distributions (CPDs). This factorization immediately leads us to the desired alternative
parameterization. In order to specify fully a joint distribution satisfying equation (3.4), we need
the following three CPDs: P(I), P(S | I), and P(G | I). The ﬁrst two might be the same as in
equation (3.3). The latter might be
I
g1
g2
g3
i0
0.2
0.34
0.46
i1
0.74
0.17
0.09
Together, these three CPDs fully specify the joint distribution (assuming the conditional inde-
pendence of equation (3.4)). For example,
P(i1, s1, g2)
=
P(i1)P(s1 | i1)P(g2 | i1)
=
0.3 · 0.8 · 0.17 = 0.0408.
Once again, we note that this probabilistic model would be represented using the Bayesian
network shown in ﬁgure 3.1b.
In this case, the alternative parameterization is more compact than the joint. We now have
three binomial distributions — P(I), P(S | i1) and P(S | i0), and two three-valued multino-
mial distributions — P(G | i1) and P(G | i0). Each of the binomials requires one independent
parameter, and each three-valued multinomial requires two independent parameters, for a total
of seven.
By contrast, our joint distribution has twelve entries, so that eleven independent
parameters are required to specify an arbitrary joint distribution over these three variables.
It is important to note another advantage of this way of representing the joint: modularity.
When we added the new variable G, the joint distribution changed entirely. Had we used the
explicit representation of the joint, we would have had to write down twelve new numbers. In
the factored representation, we could reuse our local probability models for the variables I and
S, and specify only the probability model for G — the CPD P(G | I). This property will turn
out to be invaluable in modeling real-world systems.
3.1.3.2
The General Model
This example is an instance of a much more general model commonly called the naive Bayes
naive Bayes

50
Chapter 3. The Bayesian Network Representation
Class
X2
X1
Xn
. . .
Figure 3.2
The Bayesian network graph for a naive Bayes model
model (also known as the Idiot Bayes model). The naive Bayes model assumes that instances
fall into one of a number of mutually exclusive and exhaustive classes. Thus, we have a class
variable C that takes on values in some set {c1, . . . , ck}. In our example, the class variable
is the student’s intelligence I, and there are two classes of instances — students with high
intelligence and students with low intelligence.
The model also includes some number of features X1, . . . , Xn whose values are typically
features
observed. The naive Bayes assumption is that the features are conditionally independent given
the instance’s class. In other words, within each class of instances, the diﬀerent properties can
be determined independently. More formally, we have that
(Xi ⊥X−i | C)
for all i,
(3.6)
where X−i = {X1, . . . , Xn} −{Xi}.
This model can be represented using the Bayesian
network of ﬁgure 3.2.
In this example, and later on in the book, we use a darker oval to
represent variables that are always observed when the network is used.
Based on these independence assumptions, we can show that the model factorizes as:
factorization
P(C, X1, . . . , Xn) = P(C)
n
Y
i=1
P(Xi | C).
(3.7)
(See exercise 3.2.) Thus, in this model, we can represent the joint distribution using a small set
of factors: a prior distribution P(C), specifying how likely an instance is to belong to diﬀerent
classes a priori, and a set of CPDs P(Xj | C), one for each of the n ﬁnding variables. These
factors can be encoded using a very small number of parameters. For example, if all of the
variables are binary, the number of independent parameters required to specify the distribution
is 2n+1 (see exercise 3.6). Thus, the number of parameters is linear in the number of variables,
as opposed to exponential for the explicit representation of the joint.
Box 3.A — Concept: The Naive Bayes Model. The naive Bayes model, despite the strong as-
sumptions that it makes, is often used in practice, because of its simplicity and the small number
of parameters required. The model is generally used for classiﬁcation — deciding, based on the
classiﬁcation
values of the evidence variables for a given instance, the class to which the instance is most likely to
belong. We might also want to compute our conﬁdence in this decision, that is, the extent to which
our model favors one class c1 over another c2. Both queries can be addressed by the following ratio:
P(C = c1 | x1, . . . , xn)
P(C = c2 | x1, . . . , xn) = P(C = c1)
P(C = c2)
n
Y
i=1
P(xi | C = c1)
P(xi | C = c2);
(3.8)

3.2. Bayesian Networks
51
see exercise 3.2). This formula is very natural, since it computes the posterior probability ratio of c1
versus c2 as a product of their prior probability ratio (the ﬁrst term), multiplied by a set of terms
P (xi|C=c1)
P (xi|C=c2) that measure the relative support of the ﬁnding xi for the two classes.
This model was used in the early days of medical diagnosis, where the diﬀerent values of the
medical diagnosis
class variable represented diﬀerent diseases that the patient could have. The evidence variables
represented diﬀerent symptoms, test results, and the like. Note that the model makes several strong
assumptions that are not generally true, speciﬁcally that the patient can have at most one disease,
and that, given the patient’s disease, the presence or absence of diﬀerent symptoms, and the values
of diﬀerent tests, are all independent. This model was used for medical diagnosis because the small
number of interpretable parameters made it easy to elicit from experts. For example, it is quite
natural to ask of an expert physician what the probability is that a patient with pneumonia has
high fever. Indeed, several early medical diagnosis systems were based on this technology, and some
were shown to provide better diagnoses than those made by expert physicians.
However, later experience showed that the strong assumptions underlying this model decrease its
diagnostic accuracy. In particular, the model tends to overestimate the impact of certain evidence
by “overcounting” it. For example, both hypertension (high blood pressure) and obesity are strong
indicators of heart disease. However, because these two symptoms are themselves highly correlated,
equation (3.8), which contains a multiplicative term for each of them, double-counts the evidence
they provide about the disease. Indeed, some studies showed that the diagnostic performance of
a naive Bayes model degraded as the number of features increased; this degradation was often
traced to violations of the strong conditional independence assumption. This phenomenon led to
the use of more complex Bayesian networks, with more realistic independence assumptions, for this
application (see box 3.D).
Nevertheless, the naive Bayes model is still useful in a variety of applications, particularly in
the context of models learned from data in domains with a large number of features and a rela-
tively small number of instances, such as classifying documents into topics using the words in the
documents as features; see box 17.E).
3.2
Bayesian Networks
Bayesian networks build on the same intuitions as the naive Bayes model by exploiting con-
ditional independence properties of the distribution in order to allow a compact and natural
representation.
However, they are not restricted to representing distributions satisfying the
strong independence assumptions implicit in the naive Bayes model. They allow us the ﬂexibil-
ity to tailor our representation of the distribution to the independence properties that appear
reasonable in the current setting.
The core of the Bayesian network representation is a directed acyclic graph (DAG) G, whose
nodes are the random variables in our domain and whose edges correspond, intuitively, to direct
inﬂuence of one node on another. This graph G can be viewed in two very diﬀerent ways:

•
as a data structure that provides the skeleton for representing a joint distribution
compactly in a factorized way;

52
Chapter 3. The Bayesian Network Representation
Grade
Letter
SAT
Intelligence
Difﬁculty
Figure 3.3
The Bayesian Network graph for the Student example
•
as a compact representation for a set of conditional independence assumptions about
a distribution.
As we will see, these two views are, in a strong sense, equivalent.
3.2.1
The Student Example Revisited
We begin our discussion with a simple toy example, which will accompany us, in various
versions, throughout much of this book.
3.2.1.1
The Model
Consider our student from before, but now consider a slightly more complex scenario. The
student’s grade, in this case, depends not only on his intelligence but also on the diﬃculty of
the course, represented by a random variable D whose domain is Val(D) = {easy, hard}. Our
student asks his professor for a recommendation letter. The professor is absentminded and never
remembers the names of her students. She can only look at his grade, and she writes her letter
for him based on that information alone. The quality of her letter is a random variable L, whose
domain is Val(L) = {strong, weak}. The actual quality of the letter depends stochastically on
the grade. (It can vary depending on how stressed the professor is and the quality of the coﬀee
she had that morning.)
We therefore have ﬁve random variables in this domain: the student’s intelligence (I), the
course diﬃculty (D), the grade (G), the student’s SAT score (S), and the quality of the recom-
mendation letter (L). All of the variables except G are binary-valued, and G is ternary-valued.
Hence, the joint distribution has 48 entries.
As we saw in our simple illustrations of ﬁgure 3.1, a Bayesian network is represented using a
directed graph whose nodes represent the random variables and whose edges represent direct
inﬂuence of one variable on another. We can view the graph as encoding a generative sampling
process executed by nature, where the value for each variable is selected by nature using a
distribution that depends only on its parents.
In other words, each variable is a stochastic
function of its parents.
Based on this intuition, perhaps the most natural network structure for the distribution
in this example is the one presented in ﬁgure 3.3.
The edges encode our intuition about

3.2. Bayesian Networks
53
Grade
Letter
SAT
Intelligence
Difﬁculty
d1
d0
0.6
0.4
i1
i0
0.7
0.3
i0
i1
s1
s0
0.95
0.2
0.05
0.8
g1
g2
g3
l1
l 0
0.1
0.4
0.99
0.9
0.6
0.01
i0,d0
i0,d1
i1,d0
i1,d1
g2
g3
g1
0.3
0.05
0.9
0.5
0.4
0.25
0.08
0.3
0.3
0.7
0.02
0.2
Figure 3.4
Student Bayesian network Bstudent with CPDs
the way the world works. The course diﬃculty and the student’s intelligence are determined
independently, and before any of the variables in the model. The student’s grade depends on
both of these factors. The student’s SAT score depends only on his intelligence. The quality
of the professor’s recommendation letter depends (by assumption) only on the student’s grade
in the class. Intuitively, each variable in the model depends directly only on its parents in the
network. We formalize this intuition later.
The second component of the Bayesian network representation is a set of local probability
local probability
model
models that represent the nature of the dependence of each variable on its parents. One such
model, P(I), represents the distribution in the population of intelligent versus less intelligent
student. Another, P(D), represents the distribution of diﬃcult and easy classes. The distribution
over the student’s grade is a conditional distribution P(G | I, D). It speciﬁes the distribution
over the student’s grade, inasmuch as it depends on the student’s intelligence and the course
diﬃculty. Speciﬁcally, we would have a diﬀerent distribution for each assignment of values i, d.
For example, we might believe that a smart student in an easy class is 90 percent likely to get
an A, 8 percent likely to get a B, and 2 percent likely to get a C. Conversely, a smart student
in a hard class may only be 50 percent likely to get an A. In general, each variable X in the
model is associated with a conditional probability distribution (CPD) that speciﬁes a distribution
CPD
over the values of X given each possible joint assignment of values to its parents in the model.
For a node with no parents, the CPD is conditioned on the empty set of variables. Hence, the
CPD turns into a marginal distribution, such as P(D) or P(I). One possible choice of CPDs for
this domain is shown in ﬁgure 3.4. The network structure together with its CPDs is a Bayesian
network B; we use Bstudent to refer to the Bayesian network for our student example.

54
Chapter 3. The Bayesian Network Representation
How do we use this data structure to specify the joint distribution? Consider some particular
state in this space, for example, i1, d0, g2, s1, l0. Intuitively, the probability of this event can be
computed from the probabilities of the basic events that comprise it: the probability that the
student is intelligent; the probability that the course is easy; the probability that a smart student
gets a B in an easy class; the probability that a smart student gets a high score on his SAT; and
the probability that a student who got a B in the class gets a weak letter. The total probability
of this state is:
P(i1, d0, g2, s1, l0)
=
P(i1)P(d0)P(g2 | i1, d0)P(s1 | i1)P(l0 | g2)
=
0.3 · 0.6 · 0.08 · 0.8 · 0.4 = 0.004608.
Clearly, we can use the same process for any state in the joint probability space. In general, we
will have that
P(I, D, G, S, L) = P(I)P(D)P(G | I, D)P(S | I)P(L | G).
(3.9)
This equation is our ﬁrst example of the chain rule for Bayesian networks which we will deﬁne
chain rule for
Bayesian
networks
in a general setting in section 3.2.3.2.
3.2.1.2
Reasoning Patterns
A joint distribution PB speciﬁes (albeit implicitly) the probability PB(Y = y | E = e) of
any event y given any observations e, as discussed in section 2.1.3.3: We condition the joint
distribution on the event E = e by eliminating the entries in the joint inconsistent with our
observation e, and renormalizing the resulting entries to sum to 1; we compute the probability
of the event y by summing the probabilities of all of the entries in the resulting posterior
distribution that are consistent with y. To illustrate this process, let us consider our Bstudent
network and see how the probabilities of various events change as evidence is obtained.
Consider a particular student, George, about whom we would like to reason using our model.
We might ask how likely George is to get a strong recommendation (l1) from his professor in
Econ101. Knowing nothing else about George or Econ101, this probability is about 50.2 percent.
More precisely, let PBstudent be the joint distribution deﬁned by the preceding BN; then we have
that PBstudent(l1) ≈0.502. We now ﬁnd out that George is not so intelligent (i0); the probability
that he gets a strong letter from the professor of Econ101 goes down to around 38.9 percent;
that is, PBstudent(l1 | i0) ≈0.389. We now further discover that Econ101 is an easy class (d0). The
probability that George gets a strong letter from the professor is now PBstudent(l1 | i0, d0) ≈0.513.
Queries such as these, where we predict the “downstream” eﬀects of various factors (such as
George’s intelligence), are instances of causal reasoning or prediction.
causal reasoning
Now, consider a recruiter for Acme Consulting, trying to decide whether to hire George based
on our previous model. A priori, the recruiter believes that George is 30 percent likely to be
intelligent. He obtains George’s grade record for a particular class Econ101 and sees that George
received a C in the class (g3).
His probability that George has high intelligence goes down
signiﬁcantly, to about 7.9 percent; that is, PBstudent(i1 | g3) ≈0.079. We note that the probability
that the class is a diﬃcult one also goes up, from 40 percent to 62.9 percent.
Now, assume that the recruiter fortunately (for George) lost George’s transcript, and has
only the recommendation letter from George’s professor in Econ101, which (not surprisingly) is

3.2. Bayesian Networks
55
weak. The probability that George has high intelligence still goes down, but only to 14 percent:
PBstudent(i1 | l0) ≈0.14. Note that if the recruiter has both the grade and the letter, we have the
same probability as if he had only the grade: PBstudent(i1 | g3, l0) ≈0.079; we will revisit this
issue. Queries such as this, where we reason from eﬀects to causes, are instances of evidential
evidential
reasoning
reasoning or explanation.
Finally, George submits his SAT scores to the recruiter, and astonishingly, his SAT score is high.
The probability that George has high intelligence goes up dramatically, from 7.9 percent to 57.8
percent: PBstudent(i1 | g3, s1) ≈0.578. Intuitively, the reason that the high SAT score outweighs
the poor grade is that students with low intelligence are extremely unlikely to get good scores
on their SAT, whereas students with high intelligence can still get C’s. However, smart students
are much more likely to get C’s in hard classes. Indeed, we see that the probability that Econ101
is a diﬃcult class goes up from the 62.9 percent we saw before to around 76 percent.
This last pattern of reasoning is a particularly interesting one. The information about the SAT
gave us information about the student’s intelligence, which, in conjunction with the student’s
grade in the course, told us something about the diﬃculty of the course. In eﬀect, we have one
causal factor for the Grade variable — Intelligence — giving us information about another —
Diﬃculty.
Let us examine this pattern in its pure form. As we said, PBstudent(i1 | g3) ≈0.079. On the
other hand, if we now discover that Econ101 is a hard class, we have that PBstudent(i1 | g3, d1) ≈
0.11. In eﬀect, we have provided at least a partial explanation for George’s grade in Econ101. To
take an even more striking example, if George gets a B in Econ 101, we have that PBstudent(i1 |
g2) ≈0.175. On the other hand, if Econ101 is a hard class, we get PBstudent(i1 | g2, d1) ≈0.34. In
eﬀect we have explained away the poor grade via the diﬃculty of the class. Explaining away is
explaining away

an instance of a general reasoning pattern called intercausal reasoning, where diﬀerent
intercausal
reasoning
causes of the same eﬀect can interact. This type of reasoning is a very common pattern
in human reasoning. For example, when we have fever and a sore throat, and are concerned
about mononucleosis, we are greatly relieved to be told we have the ﬂu. Clearly, having the
ﬂu does not prohibit us from having mononucleosis. Yet, having the ﬂu provides an alternative
explanation of our symptoms, thereby reducing substantially the probability of mononucleosis.
This intuition of providing an alternative explanation for the evidence can be made very
precise. As shown in exercise 3.3, if the ﬂu deterministically causes the symptoms, the probability
of mononucleosis goes down to its prior probability (the one prior to the observations of any
symptoms). On the other hand, if the ﬂu might occur without causing these symptoms, the
probability of mononucleosis goes down, but it still remains somewhat higher than its base
level. Explaining away, however, is not the only form of intercausal reasoning. The inﬂuence can
go in any direction. Consider, for example, a situation where someone is found dead and may
have been murdered. The probabilities that a suspect has motive and opportunity both go up.
If we now discover that the suspect has motive, the probability that he has opportunity goes up.
(See exercise 3.4.)
It is important to emphasize that, although our explanations used intuitive concepts such
as cause and evidence, there is nothing mysterious about the probability computations we
performed. They can be replicated simply by generating the joint distribution, as deﬁned in
equation (3.9), and computing the probabilities of the various events directly from that.

56
Chapter 3. The Bayesian Network Representation
3.2.2
Basic Independencies in Bayesian Networks
As we discussed, a Bayesian network graph G can be viewed in two ways. In the previous
section, we showed, by example, how it can be used as a skeleton data structure to which we
can attach local probability models that together deﬁne a joint distribution. In this section, we
provide a formal semantics for a Bayesian network, starting from the perspective that the graph
encodes a set of conditional independence assumptions. We begin by understanding, intuitively,
the basic conditional independence assumptions that we want a directed graph to encode. We
then formalize these desired assumptions in a deﬁnition.
3.2.2.1
Independencies in the Student Example
In the Student example, we used the intuition that edges represent direct dependence. For
example, we made intuitive statements such as “the professor’s recommendation letter depends
only on the student’s grade in the class”; this statement was encoded in the graph by the fact
that there are no direct edges into the L node except from G. This intuition, that “a node
depends directly only on its parents,” lies at the heart of the semantics of Bayesian networks.
We give formal semantics to this assertion using conditional independence statements. For
example, the previous assertion can be stated formally as the assumption that L is conditionally
independent of all other nodes in the network given its parent G:
(L ⊥I, D, S | G).
(3.10)
In other words, once we know the student’s grade, our beliefs about the quality of his rec-
ommendation letter are not inﬂuenced by information about any other variable. Similarly, to
formalize our intuition that the student’s SAT score depends only on his intelligence, we can say
that S is conditionally independent of all other nodes in the network given its parent I:
(S ⊥D, G, L | I).
(3.11)
Now, let us consider the G node. Following the pattern blindly, we may be tempted to assert
that G is conditionally independent of all other variables in the network given its parents. How-
ever, this assumption is false both at an intuitive level and for the speciﬁc example distribution
we used earlier. Assume, for example, that we condition on i1, d1; that is, we have a smart
student in a diﬃcult class. In this setting, is G independent of L? Clearly, the answer is no: if
we observe l1 (the student got a strong letter), then our probability in g1 (the student received
an A in the course) should go up; that is, we would expect
P(g1 | i1, d1, l1) > P(g1 | i1, d1).
Indeed, if we examine our distribution, the latter probability is 0.5 (as speciﬁed in the CPD),
whereas the former is a much higher 0.712.
Thus, we see that we do not expect a node to be conditionally independent of all other nodes
given its parents. In particular, even given its parents, it can still depend on its descendants.
Can it depend on other nodes? For example, do we expect G to depend on S given I and D?
Intuitively, the answer is no. Once we know, say, that the student has high intelligence, his SAT
score gives us no additional information that is relevant toward predicting his grade. Thus, we

3.2. Bayesian Networks
57
would want the property that:
(G ⊥S | I, D).
(3.12)
It remains only to consider the variables I and D, which have no parents in the graph. Thus,
in our search for independencies given a node’s parents, we are now looking for marginal inde-
pendencies. As the preceding discussion shows, in our distribution PBstudent, I is not independent
of its descendants G, L, or S. Indeed, the only nondescendant of I is D. Indeed, we assumed
implicitly that Intelligence and Diﬃculty are independent. Thus, we expect that:
(I ⊥D).
(3.13)
This analysis might seem somewhat surprising in light of our earlier examples, where learning
something about the course diﬃculty drastically changed our beliefs about the student’s intelli-
gence. In that situation, however, we were reasoning in the presence of information about the
student’s grade. In other words, we were demonstrating the dependence of I and D given G.
This phenomenon is a very important one, and we will return to it.
For the variable D, both I and S are nondescendants. Recall that, if (I ⊥D) then (D ⊥I).
The variable S increases our beliefs in the student’s intelligence, but knowing that the student is
smart (or not) does not inﬂuence our beliefs in the diﬃculty of the course. Thus, we have that
(D ⊥I, S).
(3.14)
We can see a pattern emerging. Our intuition tells us that the parents of a variable “shield”
it from probabilistic inﬂuence that is causal in nature. In other words, once I know the value
of the parents, no information relating directly or indirectly to its parents or other ancestors
can inﬂuence my beliefs about it. However, information about its descendants can change my
beliefs about it, via an evidential reasoning process.
3.2.2.2
Bayesian Network Semantics
We are now ready to provide the formal deﬁnition of the semantics of a Bayesian network
structure. We would like the formal deﬁnition to match the intuitions developed in our example.
Deﬁnition 3.1
A Bayesian network structure G is a directed acyclic graph whose nodes represent random variables
Bayesian network
structure
X1, . . . , Xn. Let PaG
Xi denote the parents of Xi in G, and NonDescendantsXi denote the vari-
ables in the graph that are not descendants of Xi. Then G encodes the following set of conditional
independence assumptions, called the local independencies, and denoted by Iℓ(G):
local
independencies
For each variable Xi: (Xi ⊥NonDescendantsXi | PaG
Xi).
In other words, the local independencies state that each node Xi is conditionally independent
of its nondescendants given its parents.
Returning to the Student network Gstudent, the local Markov independencies are precisely the
ones dictated by our intuition, and speciﬁed in equation (3.10) – equation (3.14).

58
Chapter 3. The Bayesian Network Representation
Jackie
Selma
Marge
Homer
Bart
Clancy
(b)
(a)
Maggie
Lisa
BBart
BLisa
BMaggie
GSelma
GJackie
GBart
GHomer
GLisa
BJackie
GMaggie
GMarge
BMarge
BHomer
BSelma
GClancy
BClancy
Figure 3.B.1 — Modeling Genetic Inheritance (a) A small family tree.
(b) A simple BN for genetic
inheritance in this domain. The G variables represent a person’s genotype, and the B variables the result
of a blood-type test.
Box 3.B — Case Study: The Genetics Example. One of the very earliest uses of a Bayesian net-
work model (long before the general framework was deﬁned) is in the area of genetic pedigrees.
In this setting, the local independencies are particularly intuitive. In this application, we want to
model the transmission of a certain property, say blood type, from parent to child. The blood type of
a person is an observable quantity that depends on her genetic makeup. Such properties are called
phenotypes. The genetic makeup of a person is called genotype.
To model this scenario properly, we need to introduce some background on genetics. The human
genetic material consists of 22 pairs of autosomal chromosomes and a pair of the sex chromosomes
(X and Y). Each chromosome contains a set of genetic material, consisting (among other things) of
genes that determine a person’s properties. A region of the chromosome that is of interest is called
a locus; a locus can have several variants, called alleles.
For concreteness, we focus on autosomal chromosome pairs. In each autosomal pair, one chro-
mosome is the paternal chromosome, inherited from the father, and the other is the maternal
chromosome, inherited from the mother. For genes in an autosomal pair, a person has two copies
of the gene, one on each copy of the chromosome. Thus, one of the gene’s alleles is inherited from
the person’s mother, and the other from the person’s father. For example, the region containing the
gene that encodes a person’s blood type is a locus. This gene comes in three variants, or alleles: A,
B, and O. Thus, a person’s genotype is denoted by an ordered pair, such as ⟨A, B⟩; with three
choices for each entry in the pair, there are 9 possible genotypes. The blood type phenotype is a
function of both copies of the gene. For example, if the person has an A allele and an O allele, her
observed blood type is “A.” If she has two O alleles, her observed blood type is “O.”
To represent this domain, we would have, for each person, two variables: one representing the
person’s genotype, and the other her phenotype. We use the name G(p) to represent person p’s
genotype, and B(p) to represent her blood type.
In this example, the independence assumptions arise immediately from the biology. Since the

3.2. Bayesian Networks
59
blood type is a function of the genotype, once we know the genotype of a person, additional
evidence about other members of the family will not provide new information about the blood type.
Similarly, the process of genetic inheritance implies independence assumption. Once we know the
genotype of both parents, we know what each of them can pass on to the oﬀspring. Thus, learning
new information about ancestors (or nondescendants) does not provide new information about the
genotype of the oﬀspring. These are precisely the local independencies in the resulting network
structure, shown for a simple family tree in ﬁgure 3.B.1. The intuition here is clear; for example,
Bart’s blood type is correlated with that of his aunt Selma, but once we know Homer’s and Marge’s
genotype, the two become independent.
To deﬁne the probabilistic model fully, we need to specify the CPDs. There are three types of CPDs
in this model:
• The penetrance model P(B(c) | G(c)), which describes the probability of diﬀerent variants
of a particular phenotype (say diﬀerent blood types) given the person’s genotype. In the case of
the blood type, this CPD is a deterministic function, but in other cases, the dependence can be
more complex.
• The transmission model P(G(c) | G(p), G(m)), where c is a person and p, m her father and
mother, respectively. Each parent is equally likely to transmit either of his or her two alleles to
the child.
• Genotype priors P(G(c)), used when person c has no parents in the pedigree. These are the
general genotype frequencies within the population.
Our discussion of blood type is simpliﬁed for several reasons.
First, some phenotypes, such
as late-onset diseases, are not a deterministic function of the genotype. Rather, an individual
with a particular genotype might be more likely to have the disease than an individual with other
genotypes. Second, the genetic makeup of an individual is deﬁned by many genes. Some phenotypes
might depend on multiple genes. In other settings, we might be interested in multiple phenotypes,
which (naturally) implies a dependence on several genes. Finally, as we now discuss, the inheritance
patterns of diﬀerent genes are not independent of each other.
Recall that each of the person’s autosomal chromosomes is inherited from one of her parents.
However, each of the parents also has two copies of each autosomal chromosome. These two copies,
within each parent, recombine to produce the chromosome that is transmitted to the child. Thus,
the maternal chromosome inherited by Bart is a combination of the chromosomes inherited by his
mother Marge from her mother Jackie and her father Clancy. The recombination process is stochastic,
but only a handful recombination events take place within a chromosome in a single generation.
Thus, if Bart inherited the allele for some locus from the chromosome his mother inherited from
her mother Jackie, he is also much more likely to inherit Jackie’s copy for a nearby locus. Thus,
to construct an appropriate model for multilocus inheritance, we must take into consideration the
probability of a recombination taking place between pairs of adjacent loci.
We can facilitate this modeling by introducing selector variables that capture the inheritance
pattern along the chromosome. In particular, for each locus ℓand each child c, we have a variable
S(ℓ, c, m) that takes the value 1 if the locus ℓin c’s maternal chromosome was inherited from c’s
maternal grandmother, and 2 if this locus was inherited from c’s maternal grandfather. We have
a similar selector variable S(ℓ, c, p) for c’s paternal chromosome. We can now model correlations
induced by low recombination frequency by correlating the variables S(ℓ, c, m) and S(ℓ′, c, m) for
adjacent loci ℓ, ℓ′.

60
Chapter 3. The Bayesian Network Representation
This type of model has been used extensively for many applications. In genetic counseling and
prediction, one takes a phenotype with known loci and a set of observed phenotype and genotype
data for some individuals in the pedigree to infer the genotype and phenotype for another person
in the pedigree (say, a planned child). The genetic data can consist of direct measurements of the
relevant disease loci (for some individuals) or measurements of nearby loci, which are correlated
with the disease loci.
In linkage analysis, the task is a harder one: identifying the location of disease genes from
pedigree data using some number of pedigrees where a large fraction of the individuals exhibit a
disease phenotype. Here, the available data includes phenotype information for many individuals in
the pedigree, as well as genotype information for loci whose location in the chromosome is known.
Using the inheritance model, the researchers can evaluate the likelihood of these observations under
diﬀerent hypotheses about the location of the disease gene relative to the known loci. By repeated
calculation of the probabilities in the network for diﬀerent hypotheses, researchers can pinpoint the
area that is “linked” to the disease. This much smaller region can then be used as the starting point
for more detailed examination of genes in that area. This process is crucial, for it can allow the
researchers to focus on a small area (for example, 1/10, 000 of the genome).
As we will see in later chapters, the ability to describe the genetic inheritance process using a
sparse Bayesian network provides us the capability to use sophisticated inference algorithms that
allow us to reason about large pedigrees and multiple loci. It also allows us to use algorithms
for model learning to obtain a deeper understanding of the genetic inheritance process, such as
recombination rates in diﬀerent regions or penetrance probabilities for diﬀerent diseases.
3.2.3
Graphs and Distributions
The formal semantics of a Bayesian network graph is as a set of independence assertions. On the
other hand, our Student BN was a graph annotated with CPDs, which deﬁned a joint distribution
via the chain rule for Bayesian networks. In this section, we show that these two deﬁnitions are,
in fact, equivalent. A distribution P satisﬁes the local independencies associated with a graph
G if and only if P is representable as a set of CPDs associated with the graph G. We begin by
formalizing the basic concepts.
3.2.3.1
I-Maps
We ﬁrst deﬁne the set of independencies associated with a distribution P.
Deﬁnition 3.2
Let P be a distribution over X. We deﬁne I(P) to be the set of independence assertions of the
independencies
in P
form (X ⊥Y | Z) that hold in P.
We can now rewrite the statement that “P satisﬁes the local independencies associated with
G” simply as Iℓ(G) ⊆I(P). In this case, we say that G is an I-map (independency map) for
P. However, it is useful to deﬁne this concept more broadly, since diﬀerent variants of it will
be used throughout the book.
Deﬁnition 3.3
Let K be any graph object associated with a set of independencies I(K). We say that K is an
I-map for a set of independencies I if I(K) ⊆I.
I-map

3.2. Bayesian Networks
61
We now say that G is an I-map for P if G is an I-map for I(P).
As we can see from the direction of the inclusion, for G to be an I-map of P, it is necessary
that G does not mislead us regarding independencies in P: any independence that G asserts
must also hold in P. Conversely, P may have additional independencies that are not reﬂected
in G.
Let us illustrate the concept of an I-map on a very simple example.
Example 3.1
Consider a joint probability space over two independent random variables X and Y . There are
three possible graphs over these two nodes: G∅, which is a disconnected pair X
Y ; GX→Y , which
has the edge X →Y ; and GY →X, which contains Y →X. The graph G∅encodes the assumption
that (X ⊥Y ). The latter two encode no independence assumptions.
Consider the following two distributions:
X
Y
P(X, Y )
x0
y0
0.08
x0
y1
0.32
x1
y0
0.12
x1
y1
0.48
X
Y
P(X, Y )
x0
y0
0.4
x0
y1
0.3
x1
y0
0.2
x1
y1
0.1
In the example on the left, X and Y are independent in P; for example, P(x1) = 0.48 + 0.12 =
0.6, P(y1) = 0.8, and P(x1, y1) = 0.48 = 0.6 · 0.8. Thus, (X ⊥Y ) ∈I(P), and we have
that G∅is an I-map of P. In fact, all three graphs are I-maps of P: Iℓ(GX→Y ) is empty, so that
trivially P satisﬁes all the independencies in it (similarly for GY →X). In the example on the right,
(X ⊥Y ) ̸∈I(P), so that G∅is not an I-map of P. Both other graphs are I-maps of P.
3.2.3.2
I-Map to Factorization
A BN structure G encodes a set of conditional independence assumptions; every distribution
for which G is an I-map must satisfy these assumptions. This property is the key to allowing
the compact factorized representation that we saw in the Student example in section 3.2.1. The
basic principle is the same as the one we used in the naive Bayes decomposition in section 3.1.3.
Consider any distribution P for which our Student BN Gstudent is an I-map.
We will de-
compose the joint distribution and show that it factorizes into local probabilistic models, as in
section 3.2.1. Consider the joint distribution P(I, D, G, L, S); from the chain rule for probabil-
ities (equation (2.5)), we can decompose this joint distribution in the following way:
P(I, D, G, L, S) = P(I)P(D | I)P(G | I, D)P(L | I, D, G)P(S | I, D, G, L).
(3.15)
This transformation relies on no assumptions; it holds for any joint distribution P. However, it
is also not very helpful, since the conditional probabilities in the factorization on the right-hand
side are neither natural nor compact. For example, the last factor requires the speciﬁcation of
24 conditional probabilities: P(s1 | i, d, g, l) for every assignment of values i, d, g, l.
This form, however, allows us to apply the conditional independence assumptions induced
from the BN. Let us assume that Gstudent is an I-map for our distribution P. In particular, from
equation (3.13), we have that (D ⊥I) ∈I(P). From that, we can conclude that P(D | I) =
P(D), allowing us to simplify the second factor on the right-hand side. Similarly, we know from

62
Chapter 3. The Bayesian Network Representation
equation (3.10) that (L ⊥I, D | G) ∈I(P). Hence, P(L | I, D, G) = P(L | G), allowing us
to simplify the third term. Using equation (3.11) in a similar way, we obtain that
P(I, D, G, L, S) = P(I)P(D)P(G | I, D)P(L | G)P(S | I).
(3.16)
This factorization is precisely the one we used in section 3.2.1.
This result tells us that any entry in the joint distribution can be computed as a product of
factors, one for each variable. Each factor represents a conditional probability of the variable
given its parents in the network. This factorization applies to any distribution P for which
Gstudent is an I-map.
We now state and prove this fundamental result more formally.
Deﬁnition 3.4
Let G be a BN graph over the variables X1, . . . , Xn. We say that a distribution P over the same
space factorizes according to G if P can be expressed as a product
factorization
P(X1, . . . , Xn) =
n
Y
i=1
P(Xi | PaG
Xi).
(3.17)
This equation is called the chain rule for Bayesian networks. The individual factors P(Xi | PaG
Xi)
chain rule for
Bayesian
networks
are called conditional probability distributions (CPDs) or local probabilistic models.
CPD
Deﬁnition 3.5
A Bayesian network is a pair B = (G, P) where P factorizes over G, and where P is speciﬁed as
Bayesian network
a set of CPDs associated with G’s nodes. The distribution P is often annotated PB.
We can now prove that the phenomenon we observed for Gstudent holds more generally.
Theorem 3.1
Let G be a BN structure over a set of random variables X, and let P be a joint distribution over
the same space. If G is an I-map for P, then P factorizes according to G.
Proof Assume, without loss of generality, that X1, . . . , Xn is a topological ordering of the
topological
ordering
variables in X relative to G (see deﬁnition 2.19). As in our example, we ﬁrst use the chain rule
for probabilities:
P(X1, . . . , Xn) =
n
Y
i=1
P(Xi | X1, . . . , Xi−1).
Now, consider one of the factors P(Xi | X1, . . . , Xi−1). As G is an I-map for P, we have
that (Xi ⊥NonDescendantsXi | PaG
Xi) ∈I(P). By assumption, all of Xi’s parents are in the
set X1, . . . , Xi−1. Furthermore, none of Xi’s descendants can possibly be in the set. Hence,
{X1, . . . , Xi−1} = PaXi ∪Z
where Z ⊆NonDescendantsXi. From the local independencies for Xi and from the decom-
position property (equation (2.8)) it follows that (Xi ⊥Z | PaXi). Hence, we have that
P(Xi | X1, . . . , Xi−1) = P(Xi | PaXi).
Applying this transformation to all of the factors in the chain rule decomposition, the result
follows.

3.2. Bayesian Networks
63
Thus, the conditional independence assumptions implied by a BN structure G allow us to
factorize a distribution P for which G is an I-map into small CPDs. Note that the proof is con-
structive, providing a precise algorithm for constructing the factorization given the distribution
P and the graph G.
The resulting factorized representation can be substantially more compact, particularly for
sparse structures.
Example 3.2
In our Student example, the number of independent parameters is ﬁfteen: we have two binomial
distributions P(I) and P(D), with one independent parameter each; we have four multinomial
distributions over G — one for each assignment of values to I and D — each with two independent
parameters; we have three binomial distributions over L, each with one independent parameter; and
similarly two binomial distributions over S, each with an independent parameter. The speciﬁcation
of the full joint distribution would require 48 −1 = 47 independent parameters.
More generally, in a distribution over n binary random variables, the speciﬁcation of the joint
distribution requires 2n −1 independent parameters. If the distribution factorizes according to
a graph G where each node has at most k parents, the total number of independent parameters
required is less than n · 2k (see exercise 3.6). In many applications, we can assume a certain
locality of inﬂuence between variables: although each variable is generally correlated with many
of the others, it often depends directly on only a small number of other variables. Thus, in many
cases, k will be very small, even though n is large. As a consequence, the number of parameters
in the Bayesian network representation is typically exponentially smaller than the number of
parameters of a joint distribution. This property is one of the main beneﬁts of the Bayesian
network representation.
3.2.3.3
Factorization to I-Map
Theorem 3.1 shows one direction of the fundamental connection between the conditional in-
dependencies encoded by the BN structure and the factorization of the distribution into local
probability models: that the conditional independencies imply factorization. The converse also
holds: factorization according to G implies the associated conditional independencies.
Theorem 3.2
Let G be a BN structure over a set of random variables X and let P be a joint distribution over the
same space. If P factorizes according to G, then G is an I-map for P.
We illustrate this theorem by example, leaving the proof as an exercise (exercise 3.9). Let P be
some distribution that factorizes according to Gstudent. We need to show that Iℓ(Gstudent) holds
in P. Consider the independence assumption for the random variable S — (S ⊥D, G, L | I).
To prove that it holds for P, we need to show that
P(S | I, D, G, L) = P(S | I).
By deﬁnition,
P(S | I, D, G, L) = P(S, I, D, G, L)
P(I, D, G, L) .

64
Chapter 3. The Bayesian Network Representation
By the chain rule for BNs equation (3.16), the numerator is equal to P(I)P(D)P(G | I, D)P(L |
G)P(S | I).
By the process of marginalizing over a joint distribution, we have that the
denominator is:
P(I, D, G, L)
=
X
S
P(I, D, G, L, S)
=
X
S
P(I)P(D)P(G | I, D)P(L | G)P(S | I)
=
P(I)P(D)P(G | I, D)P(L | G)
X
S
P(S | I)
=
P(I)P(D)P(G | I, D)P(L | G),
where the last step is a consequence of the fact that P(S | I) is a distribution over values of S,
and therefore it sums to 1. We therefore have that
P(S | I, D, G, L)
=
P(S, I, D, G, L)
P(I, D, G, L)
=
P(I)P(D)P(G | I, D)P(L | G)P(S | I)
P(I)P(D)P(G | I, D)P(L | G)
=
P(S | I).
Box 3.C — Skill: Knowledge Engineering. Our discussion of Bayesian network construction fo-
cuses on the process of going from a given distribution to a Bayesian network. Real life is not like
that. We have a vague model of the world, and we need to crystallize it into a network structure
and parameters. This task breaks down into several components, each of which can be quite subtle.
Unfortunately, modeling mistakes can have signiﬁcant consequences for the quality of the answers
obtained from the network, or to the cost of using the network in practice.
Picking variables
When we model a domain, there are many possible ways to describe the
relevant entities and their attributes. Choosing which random variables to use in the model is
often one of the hardest tasks, and this decision has implications throughout the model. A common
problem is using ill-deﬁned variables. For example, deciding to include the variable Fever to describe
a patient in a medical domain seems fairly innocuous. However, does this random variable relate to
the internal temperature of the patient? To the thermometer reading (if one is taken by the medical
staﬀ)? Does it refer to the temperature of the patient at a speciﬁc moment (for example, the time of
admission to the hospital) or to occurrence of a fever over a prolonged period? Clearly, each of these
might be a reasonable attribute to model, but the interaction of Fever with other variables depends
on the speciﬁc interpretation we use.
As this example shows, we must be precise in deﬁning the variables in the model. The clarity
clarity test
test is a good way of evaluating whether they are suﬃciently well deﬁned. Assume that we are
a million years after the events described in the domain; can an omniscient being, one who saw
everything, determine the value of the variable? For example, consider a Weather variable with a
value sunny. To be absolutely precise, we must deﬁne where we check the weather, at what time,

3.2. Bayesian Networks
65
and what fraction of the sky must be clear in order for it to be sunny. For a variable such as
Heart-attack, we must specify how large the heart attack has to be, during what period of time it
has to happen, and so on. By contrast, a variable such as Risk-of-heart-attack is meaningless, as
even an omniscient being cannot evaluate whether a person had high risk or low risk, only whether
the heart attack occurred or not. Introducing variables such as this confounds actual events and
their probability. Note, however, that we can use a notion of “risk group,” as long as it is deﬁned in
terms of clearly speciﬁed attributes such as age or lifestyle.
If we are not careful in our choice of variables, we will have a hard time making sure that
evidence observed and conclusions made are coherent.
Generally speaking, we want our model to contain variables that we can potentially observe
or that we may want to query. However, sometimes we want to put in a hidden variable that is
hidden variable
neither observed nor directly of interest. Why would we want to do that? Let us consider an example
relating to a cholesterol test. Assume that, for the answers to be accurate, the subject has to have
eaten nothing after 10:00 PM the previous evening. If the person eats (having no willpower), the
results are consistently oﬀ. We do not really care about a Willpower variable, nor can we observe it.
However, without it, all of the diﬀerent cholesterol tests become correlated. To avoid graphs where
all the tests are correlated, it is better to put in this additional hidden variable, rendering them
conditionally independent given the true cholesterol level and the person’s willpower.
On the other hand, it is not necessary to add every variable that might be relevant. In our
Student example, the student’s SAT score may be aﬀected by whether he goes out for drinks on the
night before the exam. Is this variable important to represent? The probabilities already account for
the fact that he may achieve a poor score despite being intelligent. It might not be worthwhile to
include this variable if it cannot be observed.
It is also important to specify a reasonable domain of values for our variables. In particular, if
our partition is not ﬁne enough, conditional independence assumptions may be false. For example,
we might want to construct a model where we have a person’s cholesterol level, and two cholesterol
tests that are conditionally independent given the person’s true cholesterol level. We might choose
to deﬁne the value normal to correspond to levels up to 200, and high to levels above 200. But
it may be the case that both tests are more likely to fail if the person’s cholesterol is marginal
(200–240). In this case, the assumption of conditional independence given the value (high/normal)
of the cholesterol test is false. It is only true if we add a marginal value.
Picking structure
As we saw, there are many structures that are consistent with the same set
of independencies. One successful approach is to choose a structure that reﬂects the causal order
and dependencies, so that causes are parents of the eﬀect. Such structures tend to work well. Either
because of some real locality of inﬂuence in the world, or because of the way people perceive the
world, causal graphs tend to be sparser. It is important to stress that the causality is in the world,
not in our inference process. For example, in an automobile insurance network, it is tempting to
put Previous-accident as a parent of Good-driver, because that is how the insurance company thinks
about the problem. This is not the causal order in the world, because being a bad driver causes
previous (and future) accidents. In principle, there is nothing to prevent us from directing the edges
in this way. However, a noncausal ordering often requires that we introduce many additional edges
to account for induced dependencies (see section 3.4.1).
One common approach to constructing a structure is a backward construction process. We begin
with a variable of interest, say Lung-Cancer. We then try to elicit a prior probability for that

66
Chapter 3. The Bayesian Network Representation
variable. If our expert responds that this probability is not determinable, because it depends on
other factors, that is a good indication that these other factors should be added as parents for that
variable (and as variables into the network). For example, we might conclude using this process that
Lung-Cancer really should have Smoking as a parent, and (perhaps not as obvious) that Smoking
should have Gender and Age as a parent. This approach, called extending the conversation, avoids
probability estimates that result from an average over a heterogeneous population, and therefore
leads to more precise probability estimates.
When determining the structure, however, we must also keep in mind that approximations are
inevitable. For many pairs of variables, we can construct a scenario where one depends on the
other. For example, perhaps Diﬃculty depends on Intelligence, because the professor is more likely
to make a class diﬃcult if intelligent students are registered. In general, there are many weak

inﬂuences that we might choose to model, but if we put in all of them, the network can
become very complex. Such networks are problematic from a representational perspective: they
are hard to understand and hard to debug, and eliciting (or learning) parameters can get very
diﬃcult. Moreover, as reasoning in Bayesian networks depends strongly on their connectivity (see
section 9.4), adding such edges can make the network too expensive to use.
This ﬁnal consideration may lead us, in fact, to make approximations that we know to be wrong.
For example, in networks for fault or medical diagnosis, the correct approach is usually to model
each possible fault as a separate random variable, allowing for multiple failures. However, such
networks might be too complex to perform eﬀective inference in certain settings, and so we may
sometimes resort to a single fault approximation, where we have a single random variable encoding
the primary fault or disease.
Picking probabilities
One of the most challenging tasks in constructing a network manually is
eliciting probabilities from people. This task is somewhat easier in the context of causal models,
since the parameters tend to be natural and more interpretable. Nevertheless, people generally
dislike committing to an exact estimate of probability.
One approach is to elicit estimates qualitatively, using abstract terms such as “common,” “rare,”
and “surprising,” and then assign these to numbers using a predeﬁned scale. This approach is fairly
crude, and often can lead to misinterpretation. There are several approaches developed for assisting
in eliciting probabilities from people. For example, one can visualize the probability of the event
as an area (slice of a pie), or ask people how they would compare the probability in question to
certain predeﬁned lotteries. Nevertheless, probability elicitation is a long, diﬃcult process, and one
whose outcomes are not always reliable: the elicitation method can often inﬂuence the results, and
asking the same question using diﬀerent phrasing can often lead to signiﬁcant diﬀerences in the
answer. For example, studies show that people’s estimates for an event such as “Death by disease”
are signiﬁcantly lower than their estimates for this event when it is broken down into diﬀerent
possibilities such as “Death from cancer,” “Death from heart disease,” and so on.
How important is it that we get our probability estimates exactly right? In some cases, small errors
have very little eﬀect. For example, changing a conditional probability of 0.7 to 0.75 generally does
not have a signiﬁcant eﬀect. Other errors, however, can have a signiﬁcant eﬀect:
• Zero probabilities: A common mistake is to assign a probability of zero to an event that is
extremely unlikely, but not impossible. The problem is that one can never condition away

a zero probability, no matter how much evidence we get. When an event is unlikely

3.2. Bayesian Networks
67
but not impossible, giving it probability zero is guaranteed to lead to irrecoverable
errors. For example, in one of the early versions of the the Pathﬁnder system (box 3.D), 10
percent of the misdiagnoses were due to zero probability estimates given by the expert to events
that were unlikely but not impossible. As a general rule, very few things (except deﬁnitions) have
probability zero, and we must be careful in assigning zeros.
• Orders of magnitude: Small diﬀerences in very low probability events can make a large
diﬀerence to the network conclusions. Thus, a (conditional) probability of 10−4 is very diﬀerent
from 10−5.
• Relative values: The qualitative behavior of the conclusions reached by the network — the
value that has the highest probability — is fairly sensitive to the relative sizes of P(x | y) for
diﬀerent values y of PaX. For example, it is important that the network encode correctly that
the probability of having a high fever is greater when the patient has pneumonia than when he
has the ﬂu.
A very useful tool for estimating network parameters is sensitivity analysis, which allows us
sensitivity
analysis
to determine the extent to which a given probability parameter aﬀects the outcome. This process
allows us to evaluate whether it is important to get a particular CPD entry right. It also helps us
ﬁgure out which CPD entries are responsible for an answer to some query that does not match our
intuitions.
Box 3.D — Case Study: Medical Diagnosis Systems. One of the earliest applications of Bayesian
networks was to the task of medical diagnosis. In the 1980s, a very active area of research was the
medical diagnosis
construction of expert systems — computer-based systems that replace or assist an expert in per-
expert system
forming a complex task. One such task that was tackled in several ways was medical diagnosis. This
task, more than many others, required a treatment of uncertainty, due to the complex, nondeter-
ministic relationships between ﬁndings and diseases. Thus, it formed the basis for experimentation
with various formalisms for uncertain reasoning.
The Pathﬁnder expert system was designed by Heckerman and colleagues (Heckerman and Nath-
Pathﬁnder
wani 1992a; Heckerman et al. 1992; Heckerman and Nathwani 1992b) to help a pathologist diagnose
diseases in lymph nodes. Ultimately, the model contained more than sixty diﬀerent diseases and
around a hundred diﬀerent features. It evolved through several versions, including some based on
nonprobabilistic formalisms, and several that used variants of Bayesian networks. Its diagnostic
ability was evaluated over real pathological cases and compared to the diagnoses of pathological
experts.
One of the ﬁrst models used was a simple naive Bayes model, which was compared to the models
based on alternative uncertainty formalisms, and judged to be superior in its diagnostic ability. It
therefore formed the basis for subsequent development of the system.
The same evaluation pointed out important problems in the way in which parameters were
elicited from the expert. First, it was shown that 10 percent of the cases were diagnosed incorrectly,
because the correct disease was ruled out by a ﬁnding that was unlikely, but not impossible, to
manifest in that disease. Second, in the original construction, the expert estimated the probabilities
P(Finding | Disease) by ﬁxing a single disease and evaluating the probabilities of all its ﬁndings.

68
Chapter 3. The Bayesian Network Representation
It was found that the expert was more comfortable considering a single ﬁnding and evaluating its
probability across all diseases. This approach allows the expert to compare the relative values of the
same ﬁnding across multiple diseases, as described in box 3.C.
With these two lessons in mind, another version of Pathﬁnder — Pathﬁnder III — was con-
structed, still using the naive Bayes model. Finally, Pathﬁnder IV used a full Bayesian network,
with a single disease hypothesis but with dependencies between the features. Pathﬁnder IV was con-
structed using a similarity network (see box 5.B), signiﬁcantly reducing the number of parameters
that must be elicited. Pathﬁnder IV, viewed as a Bayesian network, had a total of around 75,000
parameters, but the use of similarity networks allowed the model to be constructed with fewer than
14,000 distinct parameters. Overall, the structure of Pathﬁnder IV took about 35 hours to deﬁne,
and the parameters 40 hours.
A comprehensive evaluation of the performance of the two models revealed some important
insights. First, the Bayesian network performed as well or better on most cases than the naive
Bayes model. In most of the cases where the Bayesian network performed better, the use of richer
dependency models was a contributing factor. As expected, these models were useful because they
address the strong conditional independence assumptions of the naive Bayes model, as described
in box 3.A.
Somewhat more surprising, they also helped in allowing the expert to condition
the probabilities on relevant factors other than the disease, using the process of extending the
conversation described in box 3.C, leading to more accurate elicited probabilities. Finally, the use
of similarity networks led to more accurate models, for the smaller number of elicited parameters
reduced irrelevant ﬂuctuations in parameter values (due to expert inconsistency) that can lead to
spurious dependencies.
Overall, the Bayesian network model agreed with the predictions of an expert pathologist in
50/53 cases, as compared with 47/53 cases for the naive Bayes model, with signiﬁcant therapeutic
implications. A later evaluation showed that the diagnostic accuracy of Pathﬁnder IV was at least
as good as that of the expert used to design the system. When used with less expert pathologists, the
system signiﬁcantly improved the diagnostic accuracy of the physicians alone. Moreover, the system
showed greater ability to identify important ﬁndings and to integrate these ﬁndings into a correct
diagnosis.
Unfortunately, multiple reasons prevent the widespread adoption of Bayesian networks as an
aid for medical diagnosis, including legal liability issues for misdiagnoses and incompatibility with
the physicians’ workﬂow. However, several such systems have been ﬁelded, with signiﬁcant success.
Moreover, similar technology is being used successfully in a variety of other diagnosis applications
(see box 23.C).
3.3
Independencies in Graphs
Dependencies and independencies are key properties of a distribution and are crucial for under-
standing its behavior. As we will see, independence properties are also important for answering
queries: they can be exploited to reduce substantially the computation cost of inference. There-
fore, it is important that our representations make these properties clearly visible both to a user
and to algorithms that manipulate the BN data structure.

3.3. Independencies in Graphs
69
As we discussed, a graph structure G encodes a certain set of conditional independence
assumptions Iℓ(G). Knowing only that a distribution P factorizes over G, we can conclude
that it satisﬁes Iℓ(G). An immediate question is whether there are other independencies that
we can “read oﬀ” directly from G. That is, are there other independencies that hold for every
distribution P that factorizes over G?
3.3.1
D-separation
Our aim in this section is to understand when we can guarantee that an independence (X ⊥
Y | Z) holds in a distribution associated with a BN structure G. To understand when a property
is guaranteed to hold, it helps to consider its converse: “Can we imagine a case where it does
not?” Thus, we focus our discussion on analyzing when it is possible that X can inﬂuence Y
given Z. If we construct an example where this inﬂuence occurs, then the converse property
(X ⊥Y | Z) cannot hold for all of the distributions that factorize over G, and hence the
independence property (X ⊥Y | Z) cannot follow from Iℓ(G).
We therefore begin with an intuitive case analysis: Here, we try to understand when an
observation regarding a variable X can possibly change our beliefs about Y , in the presence
of evidence about the variables Z. Although this analysis will be purely intuitive, we will show
later that our conclusions are actually provably correct.
Direct connection
We begin with the simple case, when X and Y are directly connected via
an edge, say X →Y . For any network structure G that contains the edge X →Y , it is possible
to construct a distribution where X and Y are correlated regardless of any evidence about any
of the other variables in the network. In other words, if X and Y are directly connected, we
can always get examples where they inﬂuence each other, regardless of Z.
In particular, assume that Val(X) = Val(Y ); we can simply set X = Y . That, by itself,
however, is not enough; if (given the evidence Z) X deterministically takes some particular
value, say 0, then X and Y both deterministically take that value, and are uncorrelated. We
therefore set the network so that X is (for example) uniformly distributed, regardless of the
values of any of its parents. This construction suﬃces to induce a correlation between X and
Y , regardless of the evidence.
Indirect connection
Now consider the more complicated case when X and Y are not directly
connected, but there is a trail between them in the graph. We begin by considering the simplest
such case: a three-node network, where X and Y are not directly connected, but where there
is a trail between them via Z. It turns out that this simple case is the key to understanding the
whole notion of indirect interaction in Bayesian networks.
There are four cases where X and Y are connected via Z, as shown in ﬁgure 3.5. The ﬁrst
two correspond to causal chains (in either direction), the third to a common cause, and the
fourth to a common eﬀect. We analyze each in turn.
Indirect causal eﬀect (ﬁgure 3.5a). To gain intuition, let us return to the Student example,
where we had a causal trail I →G →L. Let us begin with the case where G is not observed.
Intuitively, if we observe that the student is intelligent, we are more inclined to believe that he
gets an A, and therefore that his recommendation letter is strong. In other words, the probability
of these latter events is higher conditioned on the observation that the student is intelligent.

70
Chapter 3. The Bayesian Network Representation
Y
(a)
(b)
(c)
(d)
X
Z
Z
X
Y
X
Z
Y
Z
X
Y
Figure 3.5
The four possible two-edge trails from X to Y via Z: (a) An indirect causal eﬀect; (b) An
indirect evidential eﬀect; (c) A common cause; (d) A common eﬀect.
In fact, we saw precisely this behavior in the distribution of ﬁgure 3.4. Thus, in this case, we
believe that X can inﬂuence Y via Z.
Now assume that Z is observed, that is, Z ∈Z. As we saw in our analysis of the Student
example, if we observe the student’s grade, then (as we assumed) his intelligence no longer
inﬂuences his letter. In fact, the local independencies for this network tell us that (L ⊥I | G).
Thus, we conclude that X cannot inﬂuence Y via Z if Z is observed.
Indirect evidential eﬀect (ﬁgure 3.5b). Returning to the Student example, we have a chain
I →G →L. We have already seen that observing a strong recommendation letter for the
student changes our beliefs in his intelligence. Conversely, once the grade is observed, the letter
gives no additional information about the student’s intelligence. Thus, our analysis in the case
Y →Z →X here is identical to the causal case: X can inﬂuence Y via Z, but only if Z is not
observed. The similarity is not surprising, as dependence is a symmetrical notion. Speciﬁcally,
if (X ⊥Y ) does not hold, then (Y ⊥X) does not hold either.
Common cause (ﬁgure 3.5c). This case is one that we have analyzed extensively, both within
the simple naive Bayes model of section 3.1.3 and within our Student example. Our example has
the student’s intelligence I as a parent of his grade G and his SAT score S. As we discussed, S
and G are correlated in this model, in that observing (say) a high SAT score gives us information
about a student’s intelligence and hence helps us predict his grade. However, once we observe
I, this correlation disappears, and S gives us no additional information about G. Once again,
for this network, this conclusion follows from the local independence assumption for the node
G (or for S). Thus, our conclusion here is identical to the previous two cases: X can inﬂuence
Y via Z if and only if Z is not observed.
Common eﬀect (ﬁgure 3.5d). In all of the three previous cases, we have seen a common
pattern: X can inﬂuence Y via Z if and only if Z is not observed. Therefore, we might expect
that this pattern is universal, and will continue through this last case. Somewhat surprisingly,
this is not the case. Let us return to the Student example and consider I and D, which are
parents of G. When G is not observed, we have that I and D are independent. In fact, this
conclusion follows (once again) from the local independencies from the network. Thus, in this
case, inﬂuence cannot “ﬂow” along the trail X →Z ←Y if the intermediate node Z is not
observed.
On the other hand, consider the behavior when Z is observed. In our discussion of the

3.3. Independencies in Graphs
71
Student example, we analyzed precisely this case, which we called intercausal reasoning; we
showed, for example, that the probability that the student has high intelligence goes down
dramatically when we observe that his grade is a C (G = g3), but then goes up when we
observe that the class is a diﬃcult one D = d1. Thus, in presence of the evidence G = g3, we
have that I and D are correlated.
Let us consider a variant of this last case. Assume that we do not observe the student’s grade,
but we do observe that he received a weak recommendation letter (L = l0). Intuitively, the
same phenomenon happens. The weak letter is an indicator that he received a low grade, and
therefore it suﬃces to correlate I and D.
When inﬂuence can ﬂow from X to Y via Z, we say that the trail X ⇌Z ⇌Y is active.
The results of our analysis for active two-edge trails are summarized thus:
•
Causal trail X →Z →Y : active if and only if Z is not observed.
•
Evidential trail X ←Z ←Y : active if and only if Z is not observed.
•
Common cause X ←Z →Y : active if and only if Z is not observed.
•
Common eﬀect X →Z ←Y : active if and only if either Z or one of Z’s descendants is
observed.
A structure where X →Z ←Y (as in ﬁgure 3.5d) is also called a v-structure.
v-structure
It is useful to view probabilistic inﬂuence as a ﬂow in the graph. Our analysis here tells us
when inﬂuence from X can “ﬂow” through Z to aﬀect our beliefs about Y .
General Case
Now consider the case of a longer trail X1 ⇌· · · ⇌Xn.
Intuitively, for
inﬂuence to “ﬂow” from X1 to Xn, it needs to ﬂow through every single node on the trail. In
other words, X1 can inﬂuence Xn if every two-edge trail Xi−1 ⇌Xi ⇌Xi+1 along the trail
allows inﬂuence to ﬂow.
We can summarize this intuition in the following deﬁnition:
Deﬁnition 3.6
Let G be a BN structure, and X1 ⇌. . . ⇌Xn a trail in G. Let Z be a subset of observed
observed variable
variables. The trail X1 ⇌. . . ⇌Xn is active given Z if
active trail
• Whenever we have a v-structure Xi−1 →Xi ←Xi+1, then Xi or one of its descendants are
in Z;
• no other node along the trail is in Z.
Note that if X1 or Xn are in Z the trail is not active.
In our Student BN, we have that D →G ←I →S is not an active trail for Z = ∅, because
the v-structure D →G ←I is not activated. That same trail is active when Z = {L}, because
observing the descendant of G activates the v-structure. On the other hand, when Z = {L, I},
the trail is not active, because observing I blocks the trail G ←I →S.
What about graphs where there is more than one trail between two nodes? Our ﬂow intuition
continues to carry through: one node can inﬂuence another if there is any trail along which
inﬂuence can ﬂow. Putting these intuitions together, we obtain the notion of d-separation, which
d-separation
provides us with a notion of separation between nodes in a directed graph (hence the term
d-separation, for directed separation):

72
Chapter 3. The Bayesian Network Representation
Deﬁnition 3.7
Let X, Y , Z be three sets of nodes in G. We say that X and Y are d-separated given Z, denoted
d-sepG(X; Y | Z), if there is no active trail between any node X ∈X and Y ∈Y given Z.
We use I(G) to denote the set of independencies that correspond to d-separation:
I(G) = {(X ⊥Y | Z) : d-sepG(X; Y | Z)}.
This set is also called the set of global Markov independencies. The similarity between the nota-
global Markov
independencies
tion I(G) and our notation I(P) is not coincidental: As we discuss later, the independencies
in I(G) are precisely those that are guaranteed to hold for every distribution over G.
3.3.2
Soundness and Completeness
So far, our deﬁnition of d-separation has been based on our intuitions regarding ﬂow of inﬂu-
ence, and on our one example. As yet, we have no guarantee that this analysis is “correct.”
Perhaps there is a distribution over the BN where X can inﬂuence Y despite the fact that all
trails between them are blocked.
Hence, the ﬁrst property we want to ensure for d-separation as a method for determining
independence is soundness: if we ﬁnd that two nodes X and Y are d-separated given some Z,
soundness of
d-separation
then we are guaranteed that they are, in fact, conditionally independent given Z.
Theorem 3.3
If a distribution P factorizes according to G, then I(G) ⊆I(P).
In other words, any independence reported by d-separation is satisﬁed by the underlying dis-
tribution. The proof of this theorem requires some additional machinery that we introduce in
chapter 4, so we defer the proof to that chapter (see section 4.5.1.1).
A second desirable property is the complementary one — completeness: d-separation detects
completeness of
d-separation
all possible independencies. More precisely, if we have that two variables X and Y are indepen-
dent given Z, then they are d-separated. A careful examination of the completeness property
reveals that it is ill deﬁned, inasmuch as it does not specify the distribution in which X and Y
are independent.
To formalize this property, we ﬁrst deﬁne the following notion:
Deﬁnition 3.8
A distribution P is faithful to G if, whenever (X ⊥Y | Z) ∈I(P), then d-sepG(X; Y | Z). In
faithful
other words, any independence in P is reﬂected in the d-separation properties of the graph.
We can now provide one candidate formalization of the completeness property is as follows:
•
For any distribution P that factorizes over G, we have that P is faithful to G; that is, if X
and Y are not d-separated given Z in G, then X and Y are dependent in all distributions P
that factorize over G.
This property is the obvious converse to our notion of soundness: If true, the two together
would imply that, for any P that factorizes over G, we have that I(P) = I(G). Unfortunately,
this highly desirable property is easily shown to be false: Even if a distribution factorizes over
G, it can still contain additional independencies that are not reﬂected in the structure.

3.3. Independencies in Graphs
73
Example 3.3
Consider a distribution P over two variables A and B, where A and B are independent. One
possible I-map for P is the network A →B. For example, we can set the CPD for B to be
b0
b1
a0
0.4
0.6
a1
0.4
0.6
This example clearly violates the ﬁrst candidate deﬁnition of completeness, because the graph G is
an I-map for the distribution P, yet there are independencies that hold for this distribution but do
not follow from d-separation. In fact, these are not independencies that we can hope to discover by
examining the network structure.
Thus, the completeness property does not hold for this candidate deﬁnition of completeness.
We therefore adopt a weaker yet still useful deﬁnition:
•
If (X ⊥Y | Z) in all distributions P that factorize over G, then d-sepG(X; Y | Z). And the
contrapositive: If X and Y are not d-separated given Z in G, then X and Y are dependent
in some distribution P that factorizes over G.
Using this deﬁnition, we can show:
Theorem 3.4
Let G be a BN structure. If X and Y are not d-separated given Z in G, then X and Y are
dependent given Z in some distribution P that factorizes over G.
Proof The proof constructs a distribution P that makes X and Y correlated. The construction
is roughly as follows. As X and Y are not d-separated, there exists an active trail U1, . . . , Uk
between them. We deﬁne CPDs for the variables on the trail so as to make each pair Ui, Ui+1
correlated; in the case of a v-structure Ui →Ui+1 ←Ui+2, we deﬁne the CPD of Ui+1 so
as to ensure correlation, and also deﬁne the CPDs of the path to some downstream evidence
node, in a way that guarantees that the downstream evidence activates the correlation between
Ui and Ui+2. All other CPDs in the graph are chosen to be uniform, and thus the construction
guarantees that inﬂuence only ﬂows along this single path, preventing cases where the inﬂuence
of two (or more) paths cancel out.
The details of the construction are quite technical and
laborious, and we omit them.
We can view the completeness result as telling us that our deﬁnition of I(G) is the maximal
one. For any independence assertion that is not a consequence of d-separation in G, we can
always ﬁnd a counterexample distribution P that factorizes over G. In fact, this result can be
strengthened signiﬁcantly:
Theorem 3.5
For almost all distributions P that factorize over G, that is, for all distributions except for a set of
measure zero in the space of CPD parameterizations, we have that I(P) = I(G).1
1. A set has measure zero if it is inﬁnitesimally small relative to the overall space. For example, the set of all rationals
has measure zero within the interval [0, 1]. A straight line has measure zero in the plane. This intuition is deﬁned
formally in the ﬁeld of measure theory.

74
Chapter 3. The Bayesian Network Representation
This result strengthens theorem 3.4 in two distinct ways: First, whereas theorem 3.4 shows that
any dependency in the graph can be found in some distribution, this new result shows that there
exists a single distribution that is faithful to the graph, that is, where all of the dependencies in
the graph hold simultaneously. Second, not only does this property hold for a single distribution,
but it also holds for almost all distributions that factorize over G.
Proof At a high level, the proof is based on the following argument: Each conditional inde-
pendence assertion is a set of polynomial equalities over the space of CPD parameters (see
exercise 3.13). A basic property of polynomials is that a polynomial is either identically zero or
it is nonzero almost everywhere (its set of roots has measure zero). Theorem 3.4 implies that
polynomials corresponding to assertions outside I(G) cannot be identically zero, because they
have at least one counterexample. Thus, the set of distributions P, which exhibit any one of
these “spurious” independence assertions, has measure zero. The set of distributions that do not
satisfy I(P) = I(G) is the union of these separate sets, one for each spurious independence
assertion. The union of a ﬁnite number of sets of measure zero is a set of measure zero, proving
the result.

These results state that for almost all parameterizations P of the graph G (that is,
for almost all possible choices of CPDs for the variables), the d-separation test precisely
characterizes the independencies that hold for P.
In other words, even if we have a
distribution P that satisﬁes more independencies than I(G), a slight perturbation of the CPDs
of P will almost always eliminate these “extra” independencies. This guarantee seems to state
that such independencies are always accidental, and we will never encounter them in practice.
However, as we illustrate in example 3.7, there are cases where our CPDs have certain local
structure that is not accidental, and that implies these additional independencies that are not
detected by d-separation.
3.3.3
An Algorithm for d-Separation
The notion of d-separation allows us to infer independence properties of a distribution P that
factorizes over G simply by examining the connectivity of G. However, in order to be useful,
we need to be able to determine d-separation eﬀectively. Our deﬁnition gives us a constructive
solution, but a very ineﬃcient one: We can enumerate all trails between X and Y , and check
each one to see whether it is active. The running time of this algorithm depends on the number
of trails in the graph, which can be exponential in the size of the graph.
Fortunately, there is a much more eﬃcient algorithm that requires only linear time in the
size of the graph. The algorithm has two phases. We begin by traversing the graph bottom
up, from the leaves to the roots, marking all nodes that are in Z or that have descendants in
Z. Intuitively, these nodes will serve to enable v-structures. In the second phase, we traverse
breadth-ﬁrst from X to Y , stopping the traversal along a trail when we get to a blocked node.
A node is blocked if: (a) it is the “middle” node in a v-structure and unmarked in phase I, or (b)
is not such a node and is in Z. If our breadth-ﬁrst search gets us from X to Y , then there is
an active trail between them.
The precise algorithm is shown in algorithm 3.1.
The ﬁrst phase is straightforward.
The
second phase is more subtle. For eﬃciency, and to avoid inﬁnite loops, the algorithm must keep
track of all nodes that have been visited, so as to avoid visiting them again. However, in graphs

3.3. Independencies in Graphs
75
Algorithm 3.1 Algorithm for ﬁnding nodes reachable from X given Z via active trails
Procedure Reachable (
G,
// Bayesian network graph
X,
// Source variable
Z
// Observations
)
1
// Phase I: Insert all ancestors of Z into A
2
L ←Z
// Nodes to be visited
3
A ←∅
// Ancestors of Z
4
while L ̸= ∅
5
Select some Y from L
6
L ←L −{Y }
7
if Y ̸∈A then
8
L ←L ∪PaY
// Y ’s parents need to be visited
9
A ←A ∪{Y }
// Y is ancestor of evidence
10
11
// Phase II: traverse active trails starting from X
12
L ←{(X, ↑)}
// (Node,direction) to be visited
13
V ←∅
// (Node,direction) marked as visited
14
R ←∅
// Nodes reachable via active trail
15
while L ̸= ∅
16
Select some (Y, d) from L
17
L ←L −{(Y, d)}
18
if (Y, d) ̸∈V then
19
if Y ̸∈Z then
20
R ←R ∪{Y }
// Y is reachable
21
V ←V ∪{(Y, d)}
// Mark (Y, d) as visited
22
if d =↑and Y ̸∈Z then
// Trail up through Y active if Y not in Z
23
for each Z ∈PaY
24
L ←L ∪{(Z, ↑)}
// Y ’s parents to be visited from bottom
25
for each Z ∈ChY
26
L ←L ∪{(Z, ↓)}
// Y ’s children to be visited from top
27
else if d =↓then
// Trails down through Y
28
if Y ̸∈Z then
29
// Downward trails to Y ’s children are active
30
for each Z ∈ChY
31
L ←L∪{(Z, ↓)}
// Y ’s children to be visited from top
32
if Y ∈A then
// v-structure trails are active
33
for each Z ∈PaY
34
L ←L∪{(Z, ↑)}
// Y ’s parents to be visited from bottom
35
return R

76
Chapter 3. The Bayesian Network Representation
Z
Y
X
W
Figure 3.6
A simple example for the d-separation algorithm
with loops (multiple trails between a pair of nodes), an intermediate node Y might be involved
in several trails, which may require diﬀerent treatment within the algorithm:
Example 3.4
Consider the Bayesian network of ﬁgure 3.6, where our task is to ﬁnd all nodes reachable from X.
Assume that Y is observed, that is, Y ∈Z. Assume that the algorithm ﬁrst encounters Y via the
direct edge Y →X. Any extension of this trail is blocked by Y , and hence the algorithm stops the
traversal along this trail. However, the trail X ←Z →Y ←W is not blocked by Y . Thus, when
we encounter Y for the second time via the edge Z →Y , we should not ignore it. Therefore, after
the ﬁrst visit to Y , we can mark it as visited for the purpose of trails coming in from children of
Y , but not for the purpose of trails coming in from parents of Y .
In general, we see that, for each node Y , we must keep track separately of whether it has been
visited from the top and whether it has been visited from the bottom. Only when both directions
have been explored is the node no longer useful for discovering new active trails.
Based on this intuition, we can now show that the algorithm achieves the desired result:
Theorem 3.6
The algorithm Reachable(G, X, Z) returns the set of all nodes reachable from X via trails that are
active in G given Z.
The proof is left as an exercise (exercise 3.14).
3.3.4
I-Equivalence
The notion of I(G) speciﬁes a set of conditional independence assertions that are associated
with a graph. This notion allows us to abstract away the details of the graph structure, viewing
it purely as a speciﬁcation of independence properties. In particular, one important implication
of this perspective is the observation that very diﬀerent BN structures can actually be equivalent,
in that they encode precisely the same set of conditional independence assertions. Consider, for
example, the three networks in ﬁgure 3.5a,(b),(c). All three of them encode precisely the same
independence assumptions: (X ⊥Y | Z).
Deﬁnition 3.9
Two graph structures K1 and K2 over X are I-equivalent if I(K1) = I(K2). The set of all graphs
I-equivalence
over X is partitioned into a set of mutually exclusive and exhaustive I-equivalence classes, which
are the set of equivalence classes induced by the I-equivalence relation.

3.3. Independencies in Graphs
77
W
V
X
Y
Z
W
V
X
Y
Z
Figure 3.7
Skeletons and v-structures in a network. The two networks shown have the same skeleton
and v-structures (X →Y ←Z).
Note that the v-structure network in ﬁgure 3.5d induces a very diﬀerent set of d-separation
assertions, and hence it does not fall into the same I-equivalence class as the ﬁrst three. Its
I-equivalence class contains only that single network.
I-equivalence of two graphs immediately implies that any distribution P that can be factorized
over one of these graphs can be factorized over the other. Furthermore, there is no intrinsic

property of P that would allow us to associate it with one graph rather than an equivalent
one. This observation has important implications with respect to our ability to determine
the directionality of inﬂuence. In particular, although we can determine, for a distribution
P(X, Y ), whether X and Y are correlated, there is nothing in the distribution that can help us
determine whether the correct structure is X →Y or Y →X. We return to this point when
we discuss the causal interpretation of Bayesian networks in chapter 21.
The d-separation criterion allows us to test for I-equivalence using a very simple graph-based
algorithm. We start by considering the trails in the networks.
Deﬁnition 3.10
The skeleton of a Bayesian network graph G over X is an undirected graph over X that contains
skeleton
an edge {X, Y } for every edge (X, Y ) in G.
In the networks of ﬁgure 3.7, the networks (a) and (b) have the same skeleton.
If two networks have a common skeleton, then the set of trails between two variables X and
Y is same in both networks. If they do not have a common skeleton, we can ﬁnd a trail in
one network that does not exist in the other and use this trail to ﬁnd a counterexample for the
equivalence of the two networks.
Ensuring that the two networks have the same trails is clearly not enough. For example, the
networks in ﬁgure 3.5 all have the same skeleton. Yet, as the preceding discussion shows, the
network of ﬁgure 3.5d is not equivalent to the networks of ﬁgure 3.5a–(c). The diﬀerence, is of
course, the v-structure in ﬁgure 3.5d. Thus, it seems that if the two networks have the same
skeleton and exactly the same set of v-structures, they are equivalent. Indeed, this property
provides a suﬃcient condition for I-equivalence:
Theorem 3.7
Let G1 and G2 be two graphs over X. If G1 and G2 have the same skeleton and the same set of
v-structures then they are I-equivalent.
The proof is left as an exercise (see exercise 3.16).
Unfortunately, this characterization is not an equivalence:
there are graphs that are I-
equivalent but do not have the same set of v-structures. As a counterexample, consider complete
graphs over a set of variables. Recall that a complete graph is one to which we cannot add

78
Chapter 3. The Bayesian Network Representation
additional arcs without causing cycles. Such graphs encode the empty set of conditional in-
dependence assertions. Thus, any two complete graphs are I-equivalent. Although they have
the same skeleton, they invariably have diﬀerent v-structures. Thus, by using the criterion on
theorem 3.7, we can conclude (in certain cases) only that two networks are I-equivalent, but we
cannot use it to guarantee that they are not.
We can provide a stronger condition that does correspond exactly to I-equivalence. Intuitively,
the unique independence pattern that we want to associate with a v-structure X →Z ←Y is
that X and Y are independent (conditionally on their parents), but dependent given Z. If there
is a direct edge between X and Y , as there was in our example of the complete graph, the ﬁrst
part of this pattern is eliminated.
Deﬁnition 3.11
A v-structure X →Z ←Y is an immorality if there is no direct edge between X and Y . If there
immorality
is such an edge, it is called a covering edge for the v-structure.
covering edge
Note that not every v-structure is an immorality, so that two networks with the same immoralities
do not necessarily have the same v-structures. For example, two diﬀerent complete directed
graphs always have the same immoralities (none) but diﬀerent v-structures.
Theorem 3.8
Let G1 and G2 be two graphs over X. Then G1 and G2 have the same skeleton and the same set of
immoralities if and only if they are I-equivalent.
The proof of this (more diﬃcult) result is also left as an exercise (see exercise 3.17).
We conclude with a ﬁnal characterization of I-equivalence in terms of local operations on the
graph structure.
Deﬁnition 3.12
An edge X →Y in a graph G is said to be covered if PaG
Y = PaG
X ∪{X}.
covered edge
Theorem 3.9
Two graphs G and G′ are I-equivalent if and only if there exists a sequence of networks G =
G1, . . . , Gk = G′ that are all I-equivalent to G such that the only diﬀerence between Gi and Gi+1
is a single reversal of a covered edge.
The proof of this theorem is left as an exercise (exercise 3.18).
3.4
From Distributions to Graphs
In the previous sections, we showed that, if P factorizes over G, we can derive a rich set of
independence assertions that hold for P by simply examining G. This result immediately leads
to the idea that we can use a graph as a way of revealing the structure in a distribution. In
particular, we can test for independencies in P by constructing a graph G that represents P
and testing d-separation in G. As we will see, having a graph that reveals the structure in P
has other important consequences, in terms of reducing the number of parameters required to
specify or learn the distribution, and in terms of the complexity of performing inference on the
network.
In this section, we examine the following question: Given a distribution P, to what extent can
we construct a graph G whose independencies are a reasonable surrogate for the independencies

3.4. From Distributions to Graphs
79
in P? It is important to emphasize that we will never actually take a fully speciﬁed distribution
P and construct a graph G for it: As we discussed, a full joint distribution is much too large
to represent explicitly. However, answering this question is an important conceptual exercise,
which will help us later on when we try to understand the process of constructing a Bayesian
network that represents our model of the world, whether manually or by learning from data.
3.4.1
Minimal I-Maps
One approach to ﬁnding a graph that represents a distribution P is simply to take any graph that
is an I-map for P. The problem with this naive approach is clear: As we saw in example 3.3, the
complete graph is an I-map for any distribution, yet it does not reveal any of the independence
structure in the distribution. However, examples such as this one are not very interesting. The
graph that we used as an I-map is clearly and trivially unrepresentative of the distribution, in
that there are edges that are obviously redundant. This intuition leads to the following deﬁnition,
which we also deﬁne more broadly:
Deﬁnition 3.13
A graph K is a minimal I-map for a set of independencies I if it is an I-map for I, and if the
minimal I-map
removal of even a single edge from K renders it not an I-map.
This notion of an I-map applies to multiple types of graphs, both Bayesian networks and
other types of graphs that we will encounter later on. Moreover, because it refers to a set of
independencies I, it can be used to deﬁne an I-map for a distribution P, by taking I = I(P),
or to another graph K′, by taking I = I(K′).
Recall that deﬁnition 3.5 deﬁnes a Bayesian network to be a distribution P that factorizes
over G, thereby implying that G is an I-map for P. It is standard to restrict the deﬁnition even
further, by requiring that G be a minimal I-map for P.
How do we obtain a minimal I-map for the set of independencies induced by a given dis-
tribution P? The proof of the factorization theorem (theorem 3.1) gives us a procedure, which
is shown in algorithm 3.2. We assume we are given a predetermined variable ordering, say,
variable ordering
{X1, . . . , Xn}. We now examine each variable Xi, i = 1, . . . , n in turn. For each Xi, we pick
some minimal subset U of {X1, . . . , Xi−1} to be Xi’s parents in G. More precisely, we require
that U satisfy (Xi ⊥{X1, . . . , Xi−1} −U | U), and that no node can be removed from U
without violating this property. We then set U to be the parents of Xi.
The proof of theorem 3.1 tells us that, if each node Xi is independent of X1, . . . , Xi−1 given
its parents in G, then P factorizes over G. We can then conclude from theorem 3.2 that G is an
I-map for P. By construction, G is minimal, so that G is a minimal I-map for P.
Note that our choice of U may not be unique. Consider, for example, a case where two
variables A and B are logically equivalent, that is, our distribution P only gives positive
probability to instantiations where A and B have the same value. Now, consider a node C that
is correlated with A. Clearly, we can choose either A or B to be a parent of C, but having
chosen the one, we cannot choose the other without violating minimality. Hence, the minimal
parent set U in our construction is not necessarily unique. However, one can show that, if the
distribution is positive (see deﬁnition 2.5), that is, if for any instantiation ξ to all the network
variables X we have that P(ξ) > 0, then the choice of parent set, given an ordering, is unique.
Under this assumption, algorithm 3.2 can produce all minimal I-maps for P: Let G be any

80
Chapter 3. The Bayesian Network Representation
Algorithm 3.2 Procedure to build a minimal I-map given an ordering
Procedure Build-Minimal-I-Map (
X1, . . . , Xn
// an ordering of random variables in X
I
// Set of independencies
)
1
Set G to an empty graph over X
2
for i = 1, . . . , n
3
U ←{X1, . . . , Xi−1}
// U is the current candidate for parents of Xi
4
for U ′ ⊆{X1, . . . , Xi−1}
5
if U ′ ⊂U and (Xi ⊥{X1, . . . , Xi−1} −U ′ | U ′) ∈I then
6
U ←U ′
7
// At this stage U
is a minimal set satisfying (Xi
⊥
{X1, . . . , Xi−1} −U | U)
8
// Now set U to be the parents of Xi
9
for Xj ∈U
10
Add Xj →Xi to G
11
return G
(a)
(b)
(c)
G
L
L
I
D
S
G
D
S
I
L
G
D
S
I
Figure 3.8
Three minimal I-maps for PBstudent, induced by diﬀerent orderings: (a) D, I, S, G, L; (b)
L, S, G, I, D; (C) L, D, S, I, G.
minimal I-map for P. If we give call Build-Minimal-I-Map with an ordering ≺that is topological
for G, then, due to the uniqueness argument, the algorithm must return G.
At ﬁrst glance, the minimal I-map seems to be a reasonable candidate for capturing the
structure in the distribution: It seems that if G is a minimal I-map for a distribution P, then we
should be able to “read oﬀ” all of the independencies in P directly from G. Unfortunately, this
intuition is false.
Example 3.5
Consider the distribution PBstudent, as deﬁned in ﬁgure 3.4, and let us go through the process of
constructing a minimal I-map for PBstudent. We note that the graph Gstudent precisely reﬂects the
independencies in this distribution PBstudent (that is, I(PBstudent) = I(Gstudent)), so that we can use
Gstudent to determine which independencies hold in PBstudent.
Our construction process starts with an arbitrary ordering on the nodes; we will go through this

3.4. From Distributions to Graphs
81
process for three diﬀerent orderings. Throughout this process, it is important to remember that we
are testing independencies relative to the distribution PBstudent. We can use Gstudent (ﬁgure 3.4) to
guide our intuition about which independencies hold in PBstudent, but we can always resort to testing
these independencies in the joint distribution PBstudent.
The ﬁrst ordering is a very natural one: D, I, S, G, L. We add one node at a time and see which
of the possible edges from the preceding nodes are redundant. We start by adding D, then I. We
can now remove the edge from D to I because this particular distribution satisﬁes (I ⊥D), so
I is independent of D given its other parents (the empty set). Continuing on, we add S, but we
can remove the edge from D to S because our distribution satisﬁes (S ⊥D | I). We then add
G, but we can remove the edge from S to G, because the distribution satisﬁes (G ⊥S | I, D).
Finally, we add L, but we can remove all edges from D, I, S. Thus, our ﬁnal output is the graph
in ﬁgure 3.8a, which is precisely our original network for this distribution.
Now, consider a somewhat less natural ordering: L, S, G, I, D. In this case, the resulting I-map
is not quite as natural or as sparse. To see this, let us consider the sequence of steps. We start by
adding L to the graph. Since it is the ﬁrst variable in the ordering, it must be a root. Next, we
consider S. The decision is whether to have L as a parent of S. Clearly, we need an edge from L
to S, because the quality of the student’s letter is correlated with his SAT score in this distribution,
and S has no other parents that help render it independent of L. Formally, we have that (S ⊥L)
does not hold in the distribution. In the next iteration of the algorithm, we introduce G. Now, all
possible subsets of {L, S} are potential parents set for G. Clearly, G is dependent on L. Moreover,
although G is independent of S given I, it is not independent of S given L. Hence, we must
add the edge between S and G. Carrying out the procedure, we end up with the graph shown in
ﬁgure 3.8b.
Finally, consider the ordering: L, D, S, I, G. In this case, a similar analysis results in the graph
shown in ﬁgure 3.8c, which is almost a complete graph, missing only the edge from S to G, which
we can remove because G is independent of S given I.
Note that the graphs in ﬁgure 3.8b,c really are minimal I-maps for this distribution. However,
they fail to capture some or all of the independencies that hold in the distribution. Thus, they
show that the fact that G is a minimal I-map for P is far from a guarantee that G captures the
independence structure in P.
3.4.2
Perfect Maps
We aim to ﬁnd a graph G that precisely captures the independencies in a given distribution P.
Deﬁnition 3.14
We say that a graph K is a perfect map (P-map) for a set of independencies I if we have that
perfect map
I(K) = I. We say that K is a perfect map for P if I(K) = I(P).
If we obtain a graph G that is a P-map for a distribution P, then we can (by deﬁnition) read
the independencies in P directly from G. By construction, our original graph Gstudent is a P-map
for PBstudent.
If our goal is to ﬁnd a perfect map for a distribution, an immediate question is whether every
distribution has a perfect map. Unfortunately, the answer is no, and for several reasons. The
ﬁrst type of counterexample involves regularity in the parameterization of the distribution that
cannot be captured in the graph structure.

82
Chapter 3. The Bayesian Network Representation
Letter1
Letter2
Choice
Job
Figure 3.9
Network for the OneLetter example
Example 3.6
Consider a joint distribution P over 3 random variables X,Y ,Z such that:
P(x, y, z) =
 1/12
x ⊕y ⊕z = false
1/6
x ⊕y ⊕z = true
where ⊕is the XOR (exclusive OR) function. A simple calculation shows that (X ⊥Y ) ∈I(P),
and that Z is not independent of X given Y or of Y given X. Hence, one minimal I-map for this
distribution is the network X →Z ←Y , using a deterministic XOR for the CPD of Z. However,
this network is not a perfect map; a precisely analogous calculation shows that (X ⊥Z) ∈I(P),
but this conclusion is not supported by a d-separation analysis.
Thus, we see that deterministic relationships can lead to distributions that do not have a P-map.
Additional examples arise as a consequence of other regularities in the CPD.
Example 3.7
Consider a slight elaboration of our Student example. During his academic career, our student
George has taken both Econ101 and CS102. The professors of both classes have written him letters,
but the recruiter at Acme Consulting asks for only a single recommendation. George’s chance of
getting the job depends on the quality of the letter he gives the recruiter. We thus have four random
variables: L1 and L2, corresponding to the quality of the recommendation letters for Econ101
and CS102 respectively; C, whose value represents George’s choice of which letter to use; and J,
representing the event that George is hired by Acme Consulting.
The obvious minimal I-map for this distribution is shown in ﬁgure 3.9. Is this a perfect map?
Clearly, it does not reﬂect independencies that are not at the variable level. In particular, we have
that (L1 ⊥J | C = 2). However, this limitation is not surprising; by deﬁnition, a BN structure
makes independence assertions only at the level of variables. (We return to this issue in section 5.2.2.)
However, our problems are not limited to these ﬁner-grained independencies. Some thought reveals
that, in our target distribution, we also have that (L1 ⊥L2 | C, J)! This independence is not
implied by d-separation, because the v-structure L1 →J ←L2 is enabled. However, we can
convince ourselves that the independence holds using reasoning by cases. If C = 1, then there is
no dependence of J on L2. Intuitively, the edge from L2 to J disappears, eliminating the trail
between L1 and L2, so that L1 and L2 are independent in this case. A symmetric analysis applies
in the case that C = 2. Thus, in both cases, we have that L1 and L2 are independent. This
independence assertion is not captured by our minimal I-map, which is therefore not a P-map.
A diﬀerent class of examples is not based on structure within a CPD, but rather on symmetric
variable-level independencies that are not naturally expressed within a Bayesian network.

3.4. From Distributions to Graphs
83
C
(a)
(b)
(c)
A
D
B
B
D
A
C
B
D
A
C
Figure 3.10
Attempted Bayesian network models for the Misconception example: (a) Study pairs over
four students. (b) First attempt at a Bayesian network model. (c) Second attempt at a Bayesian network
model.
A second class of distributions that do not have a perfect map are those for which the inde-
pendence assumptions imposed by the structure of Bayesian networks is simply not appropriate.
Example 3.8
Consider a scenario where we have four students who get together in pairs to work on the homework
for a class. For various reasons, only the following pairs meet: Alice and Bob; Bob and Charles;
Charles and Debbie; and Debbie and Alice. (Alice and Charles just can’t stand each other, and Bob
and Debbie had a relationship that ended badly.) The study pairs are shown in ﬁgure 3.10a.
In this example, the professor accidentally misspoke in class, giving rise to a possible miscon-
ception among the students in the class. Each of the students in the class may subsequently have
ﬁgured out the problem, perhaps by thinking about the issue or reading the textbook. In subsequent
study pairs, he or she may transmit this newfound understanding to his or her study partners. We
therefore have four binary random variables, representing whether the student has the misconcep-
tion or not. We assume that for each X ∈{A, B, C, D}, x1 denotes the case where the student
has the misconception, and x0 denotes the case where he or she does not.
Because Alice and Charles never speak to each other directly, we have that A and C are con-
ditionally independent given B and D. Similarly, B and D are conditionally independent given
A and C. Can we represent this distribution (with these independence properties) using a BN?
One attempt is shown in ﬁgure 3.10b.
Indeed, it encodes the independence assumption that
(A ⊥C | {B, D}).
However, it also implies that B and D are independent given only A,
but dependent given both A and C. Hence, it fails to provide a perfect map for our target dis-
tribution. A second attempt, shown in ﬁgure 3.10c, is equally unsuccessful. It also implies that
(A ⊥C | {B, D}), but it also implies that B and D are marginally independent. It is clear that
all other candidate BN structures are also ﬂawed, so that this distribution does not have a perfect
map.
3.4.3
Finding Perfect Maps ⋆
Earlier we discussed an algorithm for ﬁnding minimal I-maps. We now consider an algorithm
for ﬁnding a perfect map (P-map) of a distribution. Because the requirements from a P-map are
stronger than the ones we require from an I-map, the algorithm will be more involved.

84
Chapter 3. The Bayesian Network Representation
Throughout the discussion in this section, we assume that P has a P-map. In other words,
there is an unknown DAG G∗that is P-map of P. Since G∗is a P-map, we will interchangeably
refer to independencies in P and in G∗(since these are the same). We note that the algorithms
we describe do fail when they are given a distribution that does not have a P-map. We discuss
this issue in more detail later.
Thus, our goal is to identify G∗from P. One obvious diﬃculty that arises when we consider
this goal is that G∗is, in general, not uniquely identiﬁable from P. A P-map of a distribution,
if one exists, is generally not unique: As we saw, for example, in ﬁgure 3.5, multiple graphs can
encode precisely the same independence assumptions. However, the P-map of a distribution is
unique up to I-equivalence between networks. That is, a distribution P can have many P-maps,
but all of them are I-equivalent.
If we require that a P-map construction algorithm return a single network, the output we get
may be some arbitrary member of the I-equivalence class of G∗. A more correct answer would
be to return the entire equivalence class, thus avoiding an arbitrary commitment to a possibly
incorrect structure. Of course, we do not want our algorithm to return a (possibly very large) set
of distinct networks as output. Thus, one of our tasks in this section is to develop a compact
representation of an entire equivalence class of DAGs. As we will see later in the book, this
representation plays a useful role in other contexts as well.
This formulation of the problem points us toward a solution.
Recall that, according to
theorem 3.8, two DAGs are I-equivalent if they share the same skeleton and the same set of
immoralities. Thus, we can construct the I-equivalence class for G∗by determining its skeleton
and its immoralities from the independence properties of the given distribution P. We then use
both of these components to build a representation of the equivalence class.
3.4.3.1
Identifying the Undirected Skeleton
At this stage we want to construct an undirected graph S that contains an edge X—Y if X
and Y are adjacent in G∗; that is, if either X →Y or Y →X is an edge in G∗.
The basic idea is to use independence queries of the form (X ⊥Y | U) for diﬀerent sets
of variables U. This idea is based on the observation that if X and Y are adjacent in G∗, we
cannot separate them with any set of variables.
Lemma 3.1
Let G∗be a P-map of a distribution P, and let X and Y be two variables such that X →Y is in
G∗. Then, P ̸|= (X ⊥Y | U) for any set U that does not include X and Y .
Proof Assume that that X →Y ∈G∗, and let U be a set of variables.
According to d-
separation the trail X →Y cannot be blocked by the evidence set U. Thus, X and Y are not
d-separated by U. Since G∗is a P-map of P, we have that P ̸|= (X ⊥Y | U).
This lemma implies that if X and Y are adjacent in G∗, all conditional independence queries
that involve both of them would fail.
Conversely, if X and Y are not adjacent in G, we
would hope to be able to ﬁnd a set of variables that makes these two variables conditionally
independent. Indeed, as we now show, we can provide a precise characterization of such a set:
Lemma 3.2
Let G∗be an I-map of a distribution P, and let X and Y be two variables that are not adjacent
in G∗. Then either P |= (X ⊥Y | PaG∗
X ) or P |= (X ⊥Y | PaG∗
Y ).

3.4. From Distributions to Graphs
85
The proof is left as an exercise (exercise 3.19).
Thus, if X and Y are not adjacent in G∗, then we can ﬁnd a set U so that P |= (X ⊥Y | U).
We call this set U a witness of their independence. Moreover, the lemma shows that we can
witness
ﬁnd a witness of bounded size. Thus, if we assume that G∗has bounded indegree, say less than
or equal to d, then we do not need to consider witness sets larger than d.
Algorithm 3.3 Recovering the undirected skeleton for a distribution P that has a P-map
Procedure Build-PMap-Skeleton (
X = {X1, . . . , Xn},
// Set of random variables
P,
// Distribution over X
d
// Bound on witness set
)
1
Let H be the complete undirected graph over X
2
for Xi, Xj in X
3
U Xi,Xj ←∅
4
for U ∈Witnesses(Xi, Xj, H, d)
5
// Consider U as a witness set for Xi, Xj
6
if P |= (Xi ⊥Xj | U) then
7
U Xi,Xj ←U
8
Remove Xi—Xj from H
9
break
10
return (H,{U Xi,Xj : i, j ∈{1, . . . , n})
With these tools in hand, we can now construct an algorithm for building a skeleton of G∗,
shown in algorithm 3.3. For each pair of variables, we consider all potential witness sets and
test for independence. If we ﬁnd a witness that separates the two variables, we record it (we
will soon see why) and move on to the next pair of variables. If we do not ﬁnd a witness, then
we conclude that the two variables are adjacent in G∗and add them to the skeleton. The list
Witnesses(Xi, Xj, H, d) in line 4 speciﬁes the set of possible witness sets that we consider for
separating Xi and Xj. From our earlier discussion, if we assume a bound d on the indegree,
then we can restrict attention to sets U of size at most d. Moreover, using the same analysis,
we saw that we have a witness that consists either of the parents of Xi or of the parents of
Xj. In the ﬁrst case, we can restrict attention to sets U ⊆NbH
Xi −{Xj}, where NbH
Xi are the
neighbors of Xi in the current graph H; in the second, we can similarly restrict attention to
sets U ⊆NbH
Xj −{Xi}. Finally, we note that if U separates Xi and Xj, then also many of
U’s supersets will separate Xi and Xj. Thus, we search the set of possible witnesses in order
of increasing size.
This algorithm will recover the correct skeleton given that G∗is a P-map of P and has
bounded indegree d. If P does not have a P-map, then the algorithm can fail; see exercise 3.22.
This algorithm has complexity of O(nd+2) since we consider O(n2) pairs, and for each we
perform O((n −2)d) independence tests. We greatly reduce the number of independence tests
by ordering potential witnesses accordingly, and by aborting the inner loop once we ﬁnd a
witness for a pair (after line 9). However, for pairs of variables that are directly connected in the
skeleton, we still need to evaluate all potential witnesses.

86
Chapter 3. The Bayesian Network Representation
Algorithm 3.4 Marking immoralities in the construction of a perfect map
Procedure Mark-Immoralities (
X = {X1, . . . , Xn},
S
// Skeleton
{U Xi,Xj : 1 ≤i, j ≤n}
// Witnesses found by Build-PMap-Skeleton
)
1
K ←S
2
for Xi, Xj, Xk such that Xi—Xj—Xk ∈S and Xi—Xk ̸∈S
3
// Xi—Xj—Xk is a potential immorality
4
if Xj ̸∈U Xi,Xk then
5
Add the orientations Xi →Xj and Xj ←Xk to K
6
return K
3.4.3.2
Identifying Immoralities
At this stage we have reconstructed the undirected skeleton S using Build-PMap-Skeleton. Now,
we want to reconstruct edge direction. The main cue for learning about edge directions in G∗
are immoralities. As shown in theorem 3.8, all DAGs in the equivalence class of G∗share the
same set of immoralities. Thus, our goal is to consider potential immoralities in the skeleton and
for each one determine whether it is indeed an immorality. A triplet of variables X, Z, Y is a
potential immorality if the skeleton contains X—Z—Y but does not contain an edge between
potential
immorality
X and Y . If such a triplet is indeed an immorality in G∗, then X and Y cannot be independent
given Z. Nor will they be independent given a set U that contains Z. More precisely,
Proposition 3.1
Let G∗be a P-map of a distribution P, and let X, Y and Z be variables that form an immorality
X →Z ←Y . Then, P ̸|= (X ⊥Y | U) for any set U that contains Z.
Proof Let U be a set of variables that contains Z. Since Z is observed, the trail X →Z ←Y
is active, and so X and Y are not d-separated in G∗. Since G∗is a P-map of P, we have that
P ∗̸|= (X ⊥Y | U).
What happens in the complementary situation? Suppose X—Z—Y in the skeleton, but is
not an immorality. This means that one of the following three cases is in G∗: X →Z →Y ,
Y →Z →X, or X ←Z →Y . In all three cases, X and Y are d-separated only if Z is
observed.
Proposition 3.2
Let G∗be a P-map of a distribution P, and let the triplet X, Y, Z be a potential immorality in the
skeleton of G∗, such that X →Z ←Y is not in G∗. If U is such that P |= (X ⊥Y | U), then
Z ∈U.
Proof Consider all three conﬁgurations of the trail X ⇌Z ⇌Y . In all three, Z must be
observed in order to block the trail. Since G∗is a P-map of P, we have that if P |= (X ⊥Y |
U), then Z ∈U.
Combining these two results, we see that a potential immorality X—Z—Y is an immorality
if and only if Z is not in the witness set(s) for X and Y . That is, if X—Z—Y is an immorality,

3.4. From Distributions to Graphs
87
then proposition 3.1 shows that Z is not in any witness set U; conversely, if X—Z—Y is not
an immorality, the Z must be in every witness set U. Thus, we can use the speciﬁc witness
set U X,Y that we recorded for X, Y in order to determine whether this triplet is an immorality
or not: we simply check whether Z ∈U X,Y . If Z ̸∈U X,Y , then we declare the triplet an
immorality. Otherwise, we declare that it is not an immorality. The Mark-Immoralities procedure
shown in algorithm 3.4 summarizes this process.
3.4.3.3
Representing Equivalence Classes
Once we have the skeleton and identiﬁed the immoralities, we have a speciﬁcation of the
equivalence class of G∗. For example, to test if G is equivalent to G∗we can check whether it
has the same skeleton as G∗and whether it agrees on the location of the immoralities.
The description of an equivalence class using only the skeleton and the set of immoralities is
somewhat unsatisfying. For example, we might want to know whether the fact that our network
is in the equivalence class implies that there is an arc X →Y . Although the deﬁnition does
tell us whether there is some edge between X and Y , it leaves the direction unresolved. In
other cases, however, the direction of an edge is fully determined, for example, by the presence
of an immorality. To encode both of these cases, we use a graph that allows both directed and
undirected edges, as deﬁned in section 2.2. Indeed, as we show, the chain graph, or PDAG,
representation (deﬁnition 2.21) provides precisely the right framework.
Deﬁnition 3.15
Let G be a DAG. A chain graph K is a class PDAG of the equivalence class of G if shares the same
class PDAG
skeleton as G, and contains a directed edge X →Y if and only if all G′ that are I-equivalent to G
contain the edge X →Y .2
In other words, a class PDAG represents potential edge orientations in the equivalence classes.
If the edge is directed, then all the members of the equivalence class agree on the orientation
of the edge. If the edge is undirected, there are two DAGs in the equivalence class that disagree
on the orientation of the edge.
For example, the networks in ﬁgure 3.5a–(c) are I-equivalent. The class PDAG of this equiva-
lence class is the graph X—Z—Y , since both edges can be oriented in either direction in some
member of the equivalence class. Note that, although both edges in this PDAG are undirected,
not all joint orientations of these edges are in the equivalence class. As discussed earlier, setting
the orientations X →Z ←Y results in the network of ﬁgure 3.5d, which does not belong this
equivalence class. More generally, if the class PDAG has k undirected edges, the equivalence
class can contain at most 2k networks, but the actual number can be much smaller.
Can we eﬀectively construct the class PDAG K for G∗from the reconstructed skeleton and
immoralities?
Clearly, edges involved in immoralities must be directed in K.
The obvious
question is whether K can contain directed edges that are not involved in immoralities. In other
words, can there be additional edges whose direction is necessarily the same in every member
of the equivalence class? To understand this issue better, consider the following example:
Example 3.9
Consider the DAG of ﬁgure 3.11a. This DAG has a single immorality A →C ←B. This immorality
implies that the class PDAG of this DAG must have the arcs A →C and B →C directed, as
2. For consistency with standard terminology, we use the PDAG terminology when referring to the chain graph repre-
senting an I-equivalence class.

88
Chapter 3. The Bayesian Network Representation
(c)
(b)
(a)
C
D
A
B
C
D
A
B
C
D
A
B
Figure 3.11
Simple example of compelled edges in the representation of an equivalence class. (a)
Original DAG G∗. (b) Skeleton of G∗annotated with immoralities. (c) a DAG that is not equivalent to G∗.
shown in ﬁgure 3.11b. This PDAG representation suggests that the edge C—D can assume either
orientation. Note, however, that the DAG of ﬁgure 3.11c, where we orient the edge between C and
D as D →C, contains additional immoralities (that is, A →C ←D and B →C ←D). Thus,
this DAG is not equivalent to our original DAG.
In this example, there is only one possible orientation of C—D that is consistent with the
ﬁnding that A—C—D is not an immorality. Thus, we conclude that the class PDAG for the DAG of
ﬁgure 3.11a is simply the DAG itself. In other words, the equivalence class of this DAG is a singleton.
As this example shows, a negative result in an immorality test also provides information about
edge orientation. In particular, in any case where the PDAG K contains a structure X →Y —Z
and there is no edge from X to Z, then we must orient the edge Y →Z, for otherwise we
would create an immorality X →Y ←Z.
Some thought reveals that there are other local conﬁgurations of edges where some ways of
orienting edges are inconsistent, forcing a particular direction for an edge. Each such conﬁgu-
ration can be viewed as a local constraint on edge orientation, give rise to a rule that can be
used to orient more edges in the PDAG. Three such rules are shown in ﬁgure 3.12.
Let us understand the intuition behind these rules. Rule R1 is precisely the one we discussed
earlier. Rule R2 is derived from the standard acyclicity constraint: If we have the directed path
X →Y →Z, and an undirected edge X—Z, we cannot direct the edge X ←Z without
creating a cycle. Hence, we can conclude that the edge must be directed X →Z. The third
rule seems a little more complex, but it is also easily motivated. Assume, by contradiction, that
we direct the edge Z →X. In this case, we cannot direct the edge X—Y1 as X →Y1 without
creating a cycle; thus, we must have Y1 →X. Similarly, we must have Y2 →X. But, in this
case, Y1 →X ←Y2 forms an immorality (as there is no edge between Y1 and Y2), which
contradicts the fact that the edges X—Y1 and X—Y2 are undirected in the original PDAG.
These three rules can be applied constructively in an obvious way: A rule applies to a PDAG
whenever the induced subgraph on a subset of variables exactly matches the graph on the
left-hand side of the rule. In that case, we modify this subgraph to match the subgraph on the
right-hand side of the rule. Note that, by applying one rule and orienting a previously undirected
edge, we create a new graph. This might create a subgraph that matches the antecedent of a
rule, enforcing the orientation of additional edges. This process, however, must terminate at

3.4. From Distributions to Graphs
89
Y
R1
R2
R3
Z
X
X
Z
Y
Z
X
Y
Z
X
Y
Z
X
Y1
Y2
X
Z
Y1
Y2
Figure 3.12
Rules for orienting edges in PDAG. Each rule lists a conﬁguration of edges before and after
an application of the rule.
some point (since we are only adding orientations at each step, and the number of edges is
ﬁnite). This implies that iterated application of this local constraint to the graph (a process
known as constraint propagation) is guaranteed to converge.
constraint
propagation
Algorithm 3.5 Finding the class PDAG characterizing the P-map of a distribution P
Procedure Build-PDAG (
X = {X1, . . . , Xn}
// A speciﬁcation of the random variables
P
// Distribution of interest
)
1
S, {U Xi,Xj} ←Build-PMap-Skeleton(X, P)
2
K ←Find-Immoralities(X, S, {U Xi,Xj})
3
while not converged
4
Find a subgraph in K matching the left-hand side of a rule R1–R3
5
Replace the subgraph with the right-hand side of the rule
6
return K
Algorithm 3.5 implements this process. It builds an initial graph using Build-PMap-Skeleton
and Mark-Immoralities, and then iteratively applies the three rules until convergence, that is,
until we cannot ﬁnd a subgraph that matches a left-hand side of any of the rules.

90
Chapter 3. The Bayesian Network Representation
(a)
(b)
(c)
A
B
C
D
E
F
G
A
B
C
D
E
F
G
A
B
C
D
E
F
G
Figure 3.13
More complex example of compelled edges in the representation of an equivalence
class. (a) Original DAG G∗. (b) Skeleton of G∗annotated with immoralities. (c) Complete PDAG represen-
tation of the equivalence class of G∗.
Example 3.10
Consider the DAG shown in ﬁgure 3.13a. After checking for immoralities, we ﬁnd the graph shown
in ﬁgure 3.13b. Now, we can start applying the preceding rules. For example, consider the variables
B, E, and F. They induce a subgraph that matches the left-hand side of rule R1. Thus, we orient
the edge between E and F to E →F. Now, consider the variables C, E, and F. Their induced
subgraph matches the left-hand side of rule R2, so we now orient the edge between C and F to
C →F. At this stage, if we consider the variables E, F, G, we can apply the rule R1, and orient
the edge F →G. (Alternatively, we could have arrived at the same orientation using C, F, and
G.) The resulting PDAG is shown in ﬁgure 3.13c.
It seems fairly obvious that this algorithm is guaranteed to be sound: Any edge that is
oriented by this procedure is, indeed, directed in exactly the same way in all of the members
of the equivalence class. Much more surprising is the fact that it is also complete: Repeated
application of these three local rules is guaranteed to capture all edge orientations in the
equivalence class, without the need for additional global constraints. More precisely, we can
prove that this algorithm produces the correct class PDAG for the distribution P:
Theorem 3.10
Let P be a distribution that has a P-map G∗, and let K be the PDAG returned by Build-PDAG(X, P).
Then, K is a class PDAG of G∗.
The proof of this theorem can be decomposed into several aspects of correctness. We have
already established the correctness of the skeleton found by Build-PMap-Skeleton.
Thus, it
remains to show that the directionality of the edges is correct. Speciﬁcally, we need to establish
three basic facts:
•
Acyclicity: The graph returned by Build-PDAG(X,P) is acyclic.

3.4. From Distributions to Graphs
91
•
Soundness: If X →Y ∈K, then X →Y appears in all DAGs in G∗’s I-equivalence class.
•
Completeness: If X—Y ∈K, then we can ﬁnd a DAG G that is I-equivalent to G∗such
that X →Y ∈G.
The last condition establishes completeness, since there is no constraint on the direction of the
arc. In other words, the same condition can be used to prove the existence of a graph with
X →Y and of a graph with Y →X. Hence, it shows that either direction is possible within
the equivalence class.
We begin with the soundness of the procedure.
Proposition 3.3
Let P be a distribution that has a P-map G∗, and let K be the graph returned by Build-PDAG(X, P).
Then, if X →Y ∈K, then X →Y appears in all DAGs in the I-equivalence class of G∗.
The proof is left as an exercise (exercise 3.23).
Next, we consider the acyclicity of the graph.
We start by proving a property of graphs
returned by the procedure. (Note that, once we prove that the graph returned by the procedure
is the correct PDAG, it will follow that this property also holds for class PDAGs in general.)
Proposition 3.4
Let K be the graph returned by Build-PDAG. Then, if X →Y ∈K and Y —Z ∈K, then
X →Z ∈K.
The proof is left as an exercise (exercise 3.24).
Proposition 3.5
Let K be the chain graph returned by Build-PDAG. Then K is acyclic.
Proof Suppose, by way of contradiction, that K contains a cycle. That is, there is a (partially)
directed path X1 ⇌X2 ⇌. . . ⇌Xn ⇌X1. Without loss of generality, assume that this
path is the shortest cycle in K. We claim that the path cannot contain an undirected edge. To
see that, suppose that the the path contains the triplet Xi →Xi+1—Xi+2. Then, invoking
proposition 3.4, we have that Xi →Xi+2 ∈K, and thus, we can construct a shorter path
without Xi+1 that contains the edge Xi →Xi+2. At this stage, we have a directed cycle
X1 →X2 →. . . Xn →X1. Using proposition 3.3, we conclude that this cycle appears in
any DAG in the I-equivalence class, and in particular in G∗. This conclusion contradicts the
assumption that G∗is acyclic. It follows that K is acyclic.
The ﬁnal step is the completeness proof. Again, we start by examining a property of the
graph K.
Proposition 3.6
The PDAG K returned by Build-PDAG is necessarily chordal.
The proof is left as an exercise (exercise 3.25).
This property allows us to characterize the structure of the PDAG K returned by Build-PDAG.
Recall that, since K is an undirected chain graph, we can partition X into chain components
K1, . . . , Kℓ, where each chain component contains variables that are connected by undirected
edges (see deﬁnition 2.21). It turns out that, in an undirected chordal graph, we can orient any
edge in any direction without creating an immorality.

92
Chapter 3. The Bayesian Network Representation
Proposition 3.7
Let K be a undirected chordal graph over X, and let X, Y ∈X. Then, there is a DAG G such that
(a) The skeleton of G is K.
(b) G does not contain immoralities.
(c) X →Y ∈G.
The proof of this proposition requires some additional machinery that we introduce in chapter 4,
so we defer the proof to that chapter.
Using this proposition, we see that we can orient edges in the chain component Kj without
introducing immoralities within the component. We still need to ensure that orienting an edge
X—Y within a component cannot introduce an immorality involving edges from outside the
component.
To see why this situation cannot occur, suppose we orient the edge X →Y ,
and suppose that Z →Y ∈K. This seems like a potential immorality. However, applying
proposition 3.4, we see that since Z →Y and Y —X are in K, then so must be Z →X. Since
Z is a parent of both X and Y , we have that X →Y ←Z is not an immorality. This argument
applies to any edge we orient within an undirected component, and thus no new immoralities
are introduced.
With these tools, we can complete the completeness proof of Build-PDAG.
Proposition 3.8
Let P be a distribution that has a P-map G∗, and let K be the graph returned by Build-PDAG(X, P).
If X—Y ∈K, then we can ﬁnd a DAG G that is I-equivalent to G∗such that X →Y ∈G.
Proof Suppose we have an undirected edge X—Y ∈K. We want to show that there is a DAG
G that has the same skeleton and immoralities as K such that X →Y ∈G. If can build such
a graph G, then clearly it is in the I-equivalence class of G∗.
The construction is simple. We start with the chain component that contains X—Y , and
use proposition 3.7 to orient the edges in the component so that X →Y is in the resulting
DAG. Then, we use the same construction to orient all other chain components. Since the chain
components are ordered and acyclic, and our orientation of each chain component is acyclic, the
resulting directed graph is acyclic. Moreover, as shown, the new orientation in each component
does not introduce immoralities. Thus, the resulting DAG has exactly the same skeleton and
immoralities as K.
3.5
Summary
In this chapter, we discussed the issue of specifying a high-dimensional joint distribution com-
pactly by exploiting its independence properties. We provided two complementary deﬁnitions
of a Bayesian network. The ﬁrst is as a directed graph G, annotated with a set of conditional
probability distributions P(Xi | PaXi). The network together with the CPDs deﬁne a distribu-
tion via the chain rule for Bayesian networks. In this case, we say that P factorizes over G. We
also deﬁned the independence assumptions associated with the graph: the local independencies,
the set of basic independence assumptions induced by the network structure; and the larger
set of global independencies that are derived from the d-separation criterion. We showed the

3.6. Relevant Literature
93
equivalence of these three fundamental notions: P factorizes over G if and only if P satisﬁes
the local independencies of G, which holds if and only if P satisﬁes the global independencies
derived from d-separation. This result shows the equivalence of our two views of a Bayesian
network: as a scaﬀolding for factoring a probability distribution P, and as a representation of a
set of independence assumptions that hold for P. We also showed that the set of independen-
cies derived from d-separation is a complete characterization of the independence properties
that are implied by the graph structure alone, rather than by properties of a speciﬁc distribution
over G.
We deﬁned a set of basic notions that use the characterization of a graph as a set of indepen-
dencies. We deﬁned the notion of a minimal I-map and showed that almost every distribution
has multiple minimal I-maps, but that a minimal I-map for P does not necessarily capture all
of the independence properties in P. We then deﬁned a more stringent notion of a perfect
map, and showed that not every distribution has a perfect map. We deﬁned I-equivalence, which
captures an independence-equivalence relationship between two graphs, one where they specify
precisely the same set of independencies.
Finally, we deﬁned the notion of a class PDAG, a partially directed graph that provides a
compact representation for an entire I-equivalence class, and we provided an algorithm for
constructing this graph.
These deﬁnitions and results are fundamental properties of the Bayesian network represen-
tation and its semantics. Some of the algorithms that we discussed are never used as is; for
example, we never directly use the procedure to ﬁnd a minimal I-map given an explicit rep-
resentation of the distribution. However, these results are crucial to understanding the cases
where we can construct a Bayesian network that reﬂects our understanding of a given domain,
and what the resulting network means.
3.6
Relevant Literature
The use of a directed graph as a framework for analyzing properties of distributions can be
traced back to the path analysis of Wright (1921, 1934).
The use of a directed acyclic graph to encode a general probability distribution (not within a
speciﬁc domain) was ﬁrst proposed within the context of inﬂuence diagrams, a decision-theoretic
inﬂuence
diagram
framework for making decisions under uncertainty (see chapter 23). Within this setting, Howard
and Matheson (1984b) and Smith (1989) both proved the equivalence between the ability to rep-
resent a distribution as a DAG and the local independencies (our theorem 3.1 and theorem 3.2).
The notion of Bayesian networks as a qualitative data structure encoding independence rela-
tionships was ﬁrst proposed by Pearl and his colleagues in a series of papers (for example, Verma
and Pearl 1988; Geiger and Pearl 1988; Geiger et al. 1989, 1990), and in Pearl’s book Probabilistic
Reasoning in Intelligent Systems (Pearl 1988). Our presentation of I-maps, P-maps, and Bayesian
networks largely follows the trajectory laid forth in this body of work.
The deﬁnition of d-separation was ﬁrst set forth by Pearl (1986b), although without formal
justiﬁcation. The soundness of d-separation was shown by Verma (1988), and its completeness
for the case of Gaussian distributions by Geiger and Pearl (1993). The measure-theoretic notion
of completeness of d-separation, stating that almost all distributions are faithful (theorem 3.5),
was shown by Meek (1995b). Several papers have been written exploring the yet stronger notion

94
Chapter 3. The Bayesian Network Representation
of completeness for d-separation (faithfulness for all distributions that are minimal I-maps), in
various subclasses of models (for example, Becker et al. 2000). The BayesBall algorithm, an
BayesBall
elegant and eﬃcient algorithm for d-separation and a class of related problems, was proposed
by (Shachter 1998).
The notion of I-equivalence was deﬁned by Verma and Pearl (1990, 1992), who also provided
and proved the graph-theoretic characterization of theorem 3.8. Chickering (1995) provided the
alternative characterization of I-equivalence in terms of covered edge reversal. This deﬁnition
provides an easy mechanism for proving important properties of I-equivalent networks. As we
will see later in the book, the notion of I-equivalence class plays an important role in identifying
networks, particularly when learning networks from data. The ﬁrst algorithm for constructing
a perfect map for a distribution, in the form of an I-equivalence class, was proposed by Pearl
and Verma (1991); Verma and Pearl (1992). This algorithm was subsequently extended by Spirtes
et al. (1993) and by Meek (1995a). Meek also provides an algorithm for ﬁnding all of the directed
edges that occur in every member of the I-equivalence class.
A notion related to I-equivalence is that of inclusion, where the set of independencies I(G′)
inclusion
is included in the set of independencies I(G) (so that G is an I-map for any distribution that
factorizes over G′). Shachter (1989) showed how to construct a graph G′ that includes a graph G,
but with one edge reversed. Meek (1997) conjectured that inclusion holds if and only if one can
transform G to G′ using the operations of edge addition and covered edge reversal. A limited
version of this conjecture was subsequently proved by Koˇcka, Bouckaert, and Studený (2001).
The naive Bayes model, although naturally represented as a graphical model, far predates this
view. It was applied with great success within expert systems in the 1960s and 1970s (de Bombal
et al. 1972; Gorry and Barnett 1968; Warner et al. 1961). It has also seen signiﬁcant use as a
simple yet highly eﬀective method for classiﬁcation tasks in machine learning, starting as early
as the 1970s (for example, Duda and Hart 1973), and continuing to this day.
The general usefulness of the types of reasoning patterns supported by a Bayesian network,
including the very important pattern of intercausal reasoning, was one of the key points raised
by Pearl in his book (Pearl 1988). These qualitative patterns were subsequently formalized by
Wellman (1990) in his framework of qualitative probabilistic networks, which explicitly annotate
qualitative
probabilistic
networks
arcs with the direction of inﬂuence of one variable on another. This framework has been used to
facilitate knowledge elicitation and knowledge-guided learning (Renooij and van der Gaag 2002;
Hartemink et al. 2002) and to provide verbal explanations of probabilistic inference (Druzdzel
1993).
There have been many applications of the Bayesian network framework in the context of real-
world problems. The idea of using directed graphs as a model for genetic inheritance appeared
as far back as the work on path analysis of Wright (1921, 1934). A presentation much closer to
modern-day Bayesian networks was proposed by Elston and colleagues in the 1970s (Elston and
Stewart 1971; Lange and Elston 1975). More recent developments include the development of
better algorithms for inference using these models (for example, Kong 1991; Becker et al. 1998;
Friedman et al. 2000) and the construction of systems for genetic linkage analysis based on this
technology (Szolovits and Pauker 1992; Schäﬀer 1996).
Many of the ﬁrst applications of the Bayesian network framework were to medical expert sys-
tems. The Pathﬁnder system is largely the work of David Heckerman and his colleagues (Hecker-
man and Nathwani 1992a; Heckerman et al. 1992; Heckerman and Nathwani 1992b). The success
of this system as a diagnostic tool, including its ability to outperform expert physicians, was one

3.6. Relevant Literature
95
of the major factors that led to the rise in popularity of probabilistic methods in the early 1990s.
Several other large diagnostic networks were developed around the same period, including Munin
(Andreassen et al. 1989), a network of over 1000 nodes used for interpreting electromyographic
data, and qmr-dt (Shwe et al. 1991; Middleton et al. 1991), a probabilistic reconstruction of the
qmr/internist system (Miller et al. 1982) for general medical diagnosis.
The problem of knowledge acquisition of network models has received some attention. Prob-
ability elicitation is a long-standing question in decision analysis; see, for example, Spetzler and
von Holstein (1975); Chesley (1978). Unfortunately, elicitation of probabilities from humans is a
diﬃcult process, and one subject to numerous biases (Tversky and Kahneman 1974; Daneshkhah
2004). Shachter and Heckerman (1987) propose the “backward elicitation” approach for obtaining
both the network structure and the parameters from an expert. Similarity networks (Heckerman
similarity
network
and Nathwani 1992a; Geiger and Heckerman 1996) generalize this idea by allowing an expert to
construct several small networks for diﬀerentiating between “competing” diagnoses, and then
superimposing them to construct a single large network. Morgan and Henrion (1990) provide an
overview of knowledge elicitation methods.
The diﬃculties in eliciting accurate probability estimates from experts are well recognized
across a wide range of disciplines. In the speciﬁc context of Bayesian networks, this issue has
been tackled in several ways. First, there has been both empirical (Pradhan et al. 1996) and
theoretical (Chan and Darwiche 2002) analysis of the extent to which the choice of parameters
aﬀects the conclusions of the inference. Overall, the results suggest that even fairly signiﬁcant
changes to network parameters cause only small degradations in performance, except when the
changes relate to extreme parameters — those very close to 0 and 1.
Second, the concept
of sensitivity analysis (Morgan and Henrion 1990) is used to allow researchers to evaluate the
sensitivity
analysis
sensitivity of their speciﬁc network to variations in parameters. Largely, sensitivity has been
measured using the derivative of network queries relative to various parameters (Laskey 1995;
Castillo et al. 1997b; Kjærulﬀand van der Gaag 2000; Chan and Darwiche 2002), with the focus
of most of the work being on properties of sensitivity values and on eﬃcient algorithms for
estimating them.
As pointed out by Pearl (1988), the notion of a Bayesian network structure as a representation
of independence relationships is a fundamental one, which transcends the speciﬁcs of proba-
bilistic representations. There have been many proposed variants of Bayesian networks that use
a nonprobabilistic “parameterization” of the local dependency models. Examples include vari-
ous logical calculi (Darwiche 1993), Dempster-Shafer belief functions (Shenoy 1989), possibility
values (Dubois and Prade 1990), qualitative (order-of-magnitude) probabilities (known as kappa
rankings; Darwiche and Goldszmidt 1994), and interval constraints on probabilities (Fertig and
Breese 1989; Cozman 2000).
The acyclicity constraint of Bayesian networks has led to many concerns about its ability
to express certain types of interactions. There have been many proposals intended to address
this limitation. Markov networks, based on undirected graphs, present a solution for certain
types of interactions; this class of probability models are described in chapter 4.
Dynamic
Bayesian networks “stretch out” the interactions over time, therefore providing an acyclic version
of feedback loops; these models are are described in section 6.2.
There has also been some work on directed models that encode cyclic dependencies directly.
Cyclic graphical models (Richardson 1994; Spirtes 1995; Koster 1996; Pearl and Dechter 1996) are
cyclic graphical
model
based on distributions over systems of simultaneous linear equations.
These models are a

96
Chapter 3. The Bayesian Network Representation
natural generalization of Gaussian Bayesian networks (see chapter 7), and are also associated
with notions of d-separation or I-equivalence. Spirtes (1995) shows that this connection breaks
down when the system of equations is nonlinear and provides a weaker version for the cyclic
case.
Dependency networks (Heckerman et al. 2000) encode a set of local dependency models,
dependency
network
representing the conditional distribution of each variable on all of the others (which can be
compactly represented by its dependence on its Markov blanket). A dependency network rep-
resents a probability distribution only indirectly, and is only guaranteed to be coherent under
certain conditions. However, it provides a local model of dependencies that is very naturally
interpreted by people.
3.7
Exercises
Exercise 3.1
Provide an example of a distribution P(X1, X2, X3) where for each i ̸= j, we have that (Xi ⊥Xj) ∈
I(P), but we also have that (X1, X2 ⊥X3) ̸∈I(P).
Exercise 3.2
a. Show that the naive Bayes factorization of equation (3.7) follows from the naive Bayes independence
assumptions of equation (3.6).
b. Show that equation (3.8) follows from equation (3.7).
c. Show that, if all the variables C, X1, . . . , Xn are binary-valued, then log P (C=c1|x1,...,xn)
P (C=c2|x1,...,xn) is a linear
function of the value of the ﬁnding variables, that is, can be written as Pn
i=1 αiXi+α0 (where Xi = 0
if X = x0 and 1 otherwise).
Exercise 3.3
Consider a simple example (due to Pearl), where a burglar alarm (A) can be set oﬀby either a burglary (B)
or an earthquake (E).
a. Deﬁne constraints on the CPD of P(A | B, E) that imply the explaining away property.
b. Show that if our model is such that the alarm always (deterministically) goes oﬀwhenever there is a
earthquake:
P(a1 | b1, e1) = P(a1 | b0, e1) = 1
then P(b1 | a1, e1) = P(b1), that is, observing an earthquake provides a full explanation for the
alarm.
Exercise 3.4
We have mentioned that explaining away is one type of intercausal reasoning, but that other type of
intercausal interactions are also possible. Provide a realistic example that exhibits the opposite type of
interaction.
More precisely, consider a v-structure X →Z ←Y over three binary-valued variables.
Construct a CPD P(Z | X, Y ) such that:
•
X and Y both increase the probability of the eﬀect, that is, P(z1 | x1) > P(z1) and P(z1 | y1) >
P(z1),
•
each of X and Y increases the probability of the other, that is, P(x1 | z1) < P(x1 | y1, z1), and
similarly P(y1 | z1) < P(y1 | x1, z1).

3.7. Exercises
97
Test for high cholesterol
T
High cholesterol
C
Good diet
D
Weight normal
W
Health conscious
H
Little free time
F
Exercise
E
-
-
+
+
+
+
+
Figure 3.14
A Bayesian network with qualitative inﬂuences
Note that strong (rather than weak) inequality must hold in all cases.
Your example should be realistic, that is, X, Y, Z should correspond to real-world variables, and the CPD
should be reasonable.
Exercise 3.5
Consider the Bayesian network of ﬁgure 3.14.
Assume that all variables are binary-valued. We do not know the CPDs, but do know how each ran-
dom variable qualitatively aﬀects its children. The inﬂuences, shown in the ﬁgure, have the following
interpretation:
•
X
+
→Y means P(y1 | x1, u) > P(y1 | x0, u), for all values u of Y ’s other parents.
•
X
−
→Y means P(y1 | x1, u) < P(y1 | x0, u), for all values u of Y ’s other parents.
We also assume explaining away as the interaction for all cases of intercausal reasoning.
For each of the following pairs of conditional probability queries, use the information in the network to
determine if one is larger than the other, if they are equal, or if they are incomparable. For each pair of
queries, indicate all relevant active trails, and their direction of inﬂuence.
(a)
P(t1 | d1)
P(t1)
(b)
P(d1 | t0)
P(d1)
(c)
P(h1 | e1, f 1)
P(h1 | e1)
(d)
P(c1 | f 0)
P(c1)
(e)
P(c1 | h0)
P(c1)
(f)
P(c1 | h0, f 0)
P(c1 | h0)
(g)
P(d1 | h1, e0)
P(d1 | h1)
(h)
P(d1 | e1, f 0, w1)
P(d1 | e1, f 0)
(i)
P(t1 | w1, f 0)
P(t1|w1)
Exercise 3.6
Consider a set of variables X1, . . . , Xn where each Xi has |Val(Xi)| = ℓ.

98
Chapter 3. The Bayesian Network Representation
TV
Earthquake
Burglary
JohnCall
MaryCall
Alarm
Nap
Figure 3.15
A simple network for a burglary alarm domain
a. Assume that we have a Bayesian network over X1, . . . , Xn, such that each node has at most k parents.
What is a simple upper bound on the number of independent parameters in the Bayesian network?
How many independent parameters are in the full joint distribution over X1, . . . , Xn?
b. Now, assume that each variable Xi has the parents X1, . . . , Xi−1. How many independent parameters
are there in the Bayesian network? What can you conclude about the expressive power of this type of
network?
c. Now, consider a naive Bayes model where X1, . . . , Xn are evidence variables, and we have an addi-
tional class variable C, which has k possible values c1, . . . , ck. How many independent parameters
are required to specify the naive Bayes model? How many independent parameters are required for an
explicit representation of the joint distribution?
Exercise 3.7
Show how you could eﬃciently compute the distribution over a variable Xi given some assignment to
all the other variables in the network: P(Xi | x1, . . . , xi−1, xi+1, . . . , xn).
Your procedure should
not require the construction of the entire joint distribution P(X1, . . . , Xn). Specify the computational
complexity of your procedure.
Exercise 3.8
Let B = (G, P) be a Bayesian network over some set of variables X. Consider some subset of evidence
nodes Z, and let X be all of the ancestors of the nodes in Z. Let B′ be a network over the induced
subgraph over X, where the CPD for every node X ∈X is the same in B′ as in B. Prove that the joint
distribution over X is the same in B and in B′. The nodes in X −X are called barren nodes relative to
barren node
X, because (when not instantiated) they are irrelevant to computations concerning X.
Exercise 3.9⋆
Prove theorem 3.2 for a general BN structure G. Your proof should not use the soundness of d-separation.
Exercise 3.10
Prove that the global independencies, derived from d-separation, imply the local independencies. In other
words, prove that a node is d-separated from its nondescendants given its parents.
Exercise 3.11⋆
One operation on Bayesian networks that arises in many settings is the marginalization of some node in
the network.
a. Consider the Burglary Alarm network B shown in ﬁgure 3.15. Construct a Bayesian network B′ over all of
the nodes except for Alarm that is a minimal I-map for the marginal distribution PB(B, E, T, N, J, M).
Be sure to get all dependencies that remain from the original network.

3.7. Exercises
99
b. Generalize the procedure you used to solve the preceding problem into a node elimination algorithm.
That is, deﬁne an algorithm that transforms the structure of G into G′ such that one of the nodes
Xi of G is not in G′ and G′ is an I-map of the marginal distribution over the remaining variables as
deﬁned by G.
Exercise 3.12⋆⋆
Another operation on Bayesian networks that arises often is edge reversal. This involves transforming a
edge reversal
Bayesian network G containing nodes X and Y as well as arc X →Y into another Bayesian network G′
with reversed arc Y →X. However, we want G′ to represent the same distribution as G; therefore, G′
will need to be an I-map of the original distribution.
a. Consider the Bayesian network structure of ﬁgure 3.15. Suppose we wish to reverse the arc B →A.
What additional minimal modiﬁcations to the structure of the network are needed to ensure that the
new network is an I-map of the original distribution? Your network should not reverse any additional
edges, and it should diﬀer only minimally from the original in terms of the number of edge additions
or deletions. Justify your response.
b. Now consider a general Bayesian network G. For simplicity, assume that the arc X →Y is the only
directed trail from X to Y . Deﬁne a general procedure for reversing the arc X →Y , that is, for
constructing a graph G′ is an I-map for the original distribution, but that contains an arc Y →X and
otherwise diﬀers minimally from G (in the same sense as before). Justify your response.
c. Suppose that we use the preceding method to transform G into a graph G′ with a reversed arc between
X and Y .
Now, suppose we reverse that arc back to its original direction in G by repeating the
preceding method, transforming G′ into G′′. Are we guaranteed that the ﬁnal network structure is
equivalent to the original network structure (G = G′′)?
Exercise 3.13⋆
Let B = (G, P) be a Bayesian network over X. The Bayesian network is parameterized by a set of CPD
parameters of the form θx|u for X ∈X, U = PaG
X, x ∈Val(X), u ∈Val(U). Consider any conditional
independence statement of the form (X ⊥Y | Z). Show how this statement translates into a set of
polynomial equalities over the set of CPD parameters θx|u. (Note: A polynomial equality is an assertion of
the form aθ2
1 + bθ1θ2 + cθ3
2 + d = 0.)
Exercise 3.14⋆
Prove theorem 3.6.
Exercise 3.15
Consider the two networks:
(a)
B
D
C
A
(b)
B
D
C
A
For each of them, determine whether there can be any other Bayesian network that is I-equivalent to it.
Exercise 3.16⋆
Prove theorem 3.7.

100
Chapter 3. The Bayesian Network Representation
Exercise 3.17⋆⋆
We proved earlier that two networks that have the same skeleton and v-structures imply the same condi-
tional independence assumptions. As shown, this condition is not an if and only if. Two networks can have
diﬀerent v-structures, yet still imply the same conditional independence assumptions. In this problem, you
will provide a condition that precisely relates I-equivalence and similarity of network structure.
a. A key notion in this question is that of a minimal active trail. We deﬁne an active trail X1 . . . Xm to
minimal active
trail
be minimal if there is no other active trail from X1 to Xm that “shortcuts” some of the nodes, that is,
there is no active trail X1 ⇌Xi1 ⇌. . . ⇌Xik ⇌Xm for 1 < i1 < . . . < ik < m.
Our ﬁrst goal is to analyze the types of “triangles” that can occur in a minimal active trail, that is, cases
where we have Xi−1 ⇌Xi ⇌Xi+1 with a direct edge between Xi−1 and Xi+1. Prove that the only
possible triangle in a minimal active trail is one where Xi−1 ←Xi →Xi+1, with an edge between
Xi−1 and Xi+1, and where either Xi−1 or Xi+1 are the center of a v-structure in the trail.
b. Now, consider two networks G1 and G2 that have the same skeleton and same immoralities. Prove, using
the notion of minimal active trail, that G1 and G2 imply precisely the same conditional independence
assumptions, that is, that if X and Y are d-separated given Z in G1, then X and Y are also d-separated
given Z in G2.
c. Finally, prove the other direction. That is, prove that two networks G1 and G2 that induce the same
conditional independence assumptions must have the same skeleton and the same immoralities.
Exercise 3.18⋆
In this exercise, you will prove theorem 3.9.
This result provides an alternative reformulation of I-
equivalence in terms of local operations on the graph structure.
a. Let G be a directed graph with a covered edge X →Y (as in deﬁnition 3.12), and G′ the graph that
covered edge
results by reversing the edge X →Y to produce Y →X, but leaving everything else unchanged.
Prove that G and G′ are I-equivalent.
b. Provide a counterexample to this result in the case where X →Y is not a covered edge.
c. Now, prove that for every pair of I-equivalent networks G and G′, there exists a sequence of covered
edge reversal operations that converts G to G′. Your proof should show how to construct this sequence.
Exercise 3.19⋆
Prove lemma 3.2.
Exercise 3.20⋆
In this question, we will consider the sensitivity of a particular query P(X | Y ) to the CPD of a particular
node Z. Let X and Z be nodes, and Y be a set of nodes. We say that Z has a requisite CPD for answering
requisite CPD
the query P(X | Y ) if there are two networks B1 and B2 that have identical graph structure G and
identical CPDs everywhere except at the node Z, and where PB1(X | Y ) ̸= PB2(X | Y ); in other words,
the CPD of Z aﬀects the answer to this query.
This type of analysis is useful in various settings, including determining which CPDs we need to acquire
for a certain query (and others that we discuss later in the book).
Show that we can test whether Z is a requisite probability node for P(X | Y ) using the following
procedure: We modify G into a graph G′ that contains a new “dummy” parent bZ, and then test whether
bZ has an active trail to X given Y .
•
Show that this is a sound criterion for determining whether Z is a requisite probability node for
P(X | Y ) in G, that is, for all pairs of networks B1, B2 as before, PB1(X | Y ) = PB2(X | Y ).
•
Show that this criterion is weakly complete (like d-separation), in the sense that, if it fails to identify Z
as requisite in G, there exists some pair of networks B1, B2 as before, PB1(X | Y ) ̸= PB2(X | Y ).

3.7. Exercises
101
[h]
U
Z
Y
Figure 3.16
Illustration of the concept of a self-contained set
Exercise 3.21⋆
Deﬁne a set Z of nodes to be self-contained if, for every pair of nodes A, B ∈Z, and any directed path
between A and B, all nodes along the trail are also in Z.
a. Consider a self-contained set Z, and let Y be the set of all nodes that are a parent of some node in
Z but are not themselves in Z. Let U be the set of nodes that are an ancestor of some node in Z
but that are not already in Y ∪Z. (See ﬁgure 3.16.)
Prove, based on the d-separation properties of the network, that (Z ⊥U | Y ). Make sure that your
proof covers all possible cases.
b. Provide a counterexample to this result if we retract the assumption that Z is self-contained. (Hint: 4
nodes are enough.)
Exercise 3.22⋆
We showed that the algorithm Build-PMap-Skeleton of algorithm 3.3 constructs the skeleton of the P-map
of a distribution P if P has a P-map (and that P-map has indegrees bounded by the parameter d). In this
question, we ask you consider what happens if P does not have a P-map.
There are two types of errors we might want to consider:
•
Missing edges: The edge X ⇌Y appears in all the minimal I-maps of P, yet X—Y is not in the
skeleton S returned by Build-PMap-Skeleton.
•
Spurious edges: The edge X ⇌Y does not appear in all of the minimal I-maps of P (but may
appear in some of them), yet X—Y is in the skeleton S returned by Build-PMap-Skeleton.
For each of these two types of errors, either prove that they cannot happen, or provide a counterexample
(that is, a distribution P for which Build-PMap-Skeleton makes that type of an error).
Exercise 3.23⋆
In this exercise, we prove proposition 3.3. To help us with the proof, we need an auxiliary deﬁnition. We
say that a partially directed graph K is a partial class graph for a DAG G∗if
a. K has the same skeleton as G∗;
b. K has the same immoralities as G∗;
c. if X →Y ∈K, then X →Y ∈G for any DAG G that is I-equivalent to G∗.

102
Chapter 3. The Bayesian Network Representation
Clearly, the graph returned by by Mark-Immoralities is a partial class graph of G∗.
Prove that if K is a partial class graph of G∗, and we apply one of the rules R1–R3 of ﬁgure 3.12, then the
resulting graph is also a partial class graph G∗. Use this result to prove proposition 3.3 by induction.
Exercise 3.24⋆
Prove proposition 3.4. Hint: consider the diﬀerent cases by which the edge X →Y was oriented during
the procedure.
Exercise 3.25
Prove proposition 3.6. Hint: Show that this property is true of the graph returned by Mark-Immoralities.
Exercise 3.26
Implement an eﬃcient algorithm that takes a Bayesian network over a set of variables X and a full
instantiation ξ to X, and computes the probability of ξ according to the network.
Exercise 3.27
Implement Reachable of algorithm 3.1.
Exercise 3.28⋆
Implement an eﬃcient algorithm that determines, for a given set Z of observed variables and all pairs of
nodes X and Y , whether X, Y are d-separated in G given Z. Your algorithm should be signiﬁcantly more
eﬃcient than simply running Reachable of algorithm 3.1 separately for each possible source variable Xi.

4
Undirected Graphical Models
So far, we have dealt only with directed graphical models, or Bayesian networks. These models
are useful because both the structure and the parameters provide a natural representation for
many types of real-world domains. In this chapter, we turn our attention to another important
class of graphical models, deﬁned on the basis of undirected graphs.
As we will see, these models are useful in modeling a variety of phenomena where one
cannot naturally ascribe a directionality to the interaction between variables. Furthermore, the
undirected models also oﬀer a diﬀerent and often simpler perspective on directed models, in
terms of both the independence structure and the inference task. We also introduce a combined
framework that allows both directed and undirected edges. We note that, unlike our results in
the previous chapter, some of the results in this chapter require that we restrict attention to
distributions over discrete state spaces.
4.1
The Misconception Example
To motivate our discussion of an alternative graphical representation, let us reexamine the
Misconception example of section 3.4.2 (example 3.8). In this example, we have four students
who get together in pairs to work on their homework for a class. The pairs that meet are shown
via the edges in the undirected graph of ﬁgure 3.10a.
As we discussed, we intuitively want to model a distribution that satisﬁes (A ⊥C | {B, D})
and (B ⊥D | {A, C}), but no other independencies. As we showed, these independencies
cannot be naturally captured in a Bayesian network: any Bayesian network I-map of such a
distribution would necessarily have extraneous edges, and it would not capture at least one
of the desired independence statements. More broadly, a Bayesian network requires that we
ascribe a directionality to each inﬂuence. In this case, the interactions between the variables
seem symmetrical, and we would like a model that allows us to represent these correlations
without forcing a speciﬁc direction to the inﬂuence.
A representation that implements this intuition is an undirected graph. As in a Bayesian
network, the nodes in the graph of a Markov network represent the variables, and the edges
Markov network
correspond to a notion of direct probabilistic interaction between the neighboring variables —
an interaction that is not mediated by any other variable in the network. In this case, the graph
of ﬁgure 3.10, which captures the interacting pairs, is precisely the Markov network structure
that captures our intuitions for this example. As we will see, this similarity is not an accident.

104
Chapter 4. Undirected Graphical Models
φ1(A, B)
φ2(B, C)
φ3(C, D)
φ4(D, A)
a0
b0
30
a0
b1
5
a1
b0
1
a1
b1
10
b0
c0
100
b0
c1
1
b1
c0
1
b1
c1
100
c0
d0
1
c0
d1
100
c1
d0
100
c1
d1
1
d0
a0
100
d0
a1
1
d1
a0
1
d1
a1
100
(a)
(b)
(c)
(d)
Figure 4.1
Factors for the Misconception example
The remaining question is how to parameterize this undirected graph. Because the interaction
is not directed, there is no reason to use a standard CPD, where we represent the distribution
over one node given others. Rather, we need a more symmetric parameterization. Intuitively,
what we want to capture is the aﬃnities between related variables. For example, we might want
to represent the fact that Alice and Bob are more likely to agree than to disagree. We associate
with A, B a general-purpose function, also called a factor:
Deﬁnition 4.1
Let D be a set of random variables. We deﬁne a factor φ to be a function from Val(D) to IR. A
factor
factor is nonnegative if all its entries are nonnegative. The set of variables D is called the scope of
scope
the factor and denoted Scope[φ].
Unless stated otherwise, we restrict attention to nonnegative factors.
In our example, we have a factor φ1(A, B) : Val(A, B) 7→IR+. The value associated with a
particular assignment a, b denotes the aﬃnity between these two values: the higher the value
φ1(a, b), the more compatible these two values are.
Figure 4.1a shows one possible compatibility factor for these variables. Note that this factor is
not normalized; indeed, the entries are not even in [0, 1]. Roughly speaking, φ1(A, B) asserts
that it is more likely that Alice and Bob agree. It also adds more weight for the case where they
are both right than for the case where they are both wrong. This factor function also has the
property that φ1(a1, b0) < φ1(a0, b1). Thus, if they disagree, there is less weight for the case
where Alice has the misconception but Bob does not than for the converse case.
In a similar way, we deﬁne a compatibility factor for each other interacting pair: {B, C},
{C, D}, and {A, D}. Figure 4.1 shows one possible choice of factors for all four pairs. For
example, the factor over C, D represents the compatibility of Charles and Debbie. It indicates
that Charles and Debbie argue all the time, so that the most likely instantiations are those where
they end up disagreeing.
As in a Bayesian network, the parameterization of the Markov network deﬁnes the local
interactions between directly related variables. To deﬁne a global model, we need to combine
these interactions. As in Bayesian networks, we combine the local models by multiplying them.
Thus, we want P(a, b, c, d) to be φ1(a, b) · φ2(b, c) · φ3(c, d) · φ4(d, a). In this case, however,
we have no guarantees that the result of this process is a normalized joint distribution. Indeed,
in this example, it deﬁnitely is not. Thus, we deﬁne the distribution by taking the product of

4.1. The Misconception Example
105
Assignment
Unnormalized
Normalized
a0
b0
c0
d0
300, 000
0.04
a0
b0
c0
d1
300, 000
0.04
a0
b0
c1
d0
300, 000
0.04
a0
b0
c1
d1
30
4.1 · 10−6
a0
b1
c0
d0
500
6.9 · 10−5
a0
b1
c0
d1
500
6.9 · 10−5
a0
b1
c1
d0
5, 000, 000
0.69
a0
b1
c1
d1
500
6.9 · 10−5
a1
b0
c0
d0
100
1.4 · 10−5
a1
b0
c0
d1
1, 000, 000
0.14
a1
b0
c1
d0
100
1.4 · 10−5
a1
b0
c1
d1
100
1.4 · 10−5
a1
b1
c0
d0
10
1.4 · 10−6
a1
b1
c0
d1
100, 000
0.014
a1
b1
c1
d0
100, 000
0.014
a1
b1
c1
d1
100, 000
0.014
Figure 4.2
Joint distribution for the Misconception example. The unnormalized measure and the
normalized joint distribution over A, B, C, D, obtained from the parameterization of ﬁgure 4.1. The value
of the partition function in this example is 7, 201, 840.
the local factors, and then normalizing it to deﬁne a legal distribution. Speciﬁcally, we deﬁne
P(a, b, c, d) = 1
Z φ1(a, b) · φ2(b, c) · φ3(c, d) · φ4(d, a),
where
Z =
X
a,b,c,d
φ1(a, b) · φ2(b, c) · φ3(c, d) · φ4(d, a)
is a normalizing constant known as the partition function. The term “partition” originates from
partition function
the early history of Markov networks, which originated from the concept of Markov random ﬁeld
Markov random
ﬁeld
(or MRF) in statistical physics (see box 4.C); the “function” is because the value of Z is a function
of the parameters, a dependence that will play a signiﬁcant role in our discussion of learning.
In our example, the unnormalized measure (the simple product of the four factors) is shown
in the next-to-last column in ﬁgure 4.2. For example, the entry corresponding to a1, b1, c0, d1
is obtained by multiplying:
φ1(a1, b1) · φ2(b1, c0) · φ3(c0, d1) · φ4(d1, a1) = 10 · 1 · 100 · 100 = 100, 000.
The last column shows the normalized distribution.
We can use this joint distribution to answer queries, as usual. For example, by summing out
A, C, and D, we obtain P(b1) ≈0.732 and P(b0) ≈0.268; that is, Bob is 26 percent likely to
have the misconception. On the other hand, if we now observe that Charles does not have the
misconception (c0), we obtain P(b1 | c0) ≈0.06.

106
Chapter 4. Undirected Graphical Models
The beneﬁt of this representation is that it allows us great ﬂexibility in representing inter-
actions between variables. For example, if we want to change the nature of the interaction
between A and B, we can simply modify the entries in that factor, without having to deal with
normalization constraints and the interaction with other factors. The ﬂip side of this ﬂexibility,
as we will see later, is that the eﬀects of these changes are not always intuitively understandable.
As in Bayesian networks, there is a tight connection between the factorization of the dis-
tribution and its independence properties.
The key result here is stated in exercise 2.5:
P |= (X ⊥Y | Z) if and only if we can write P in the form P(X) = φ1(X, Z)φ2(Y , Z).
In our example, the structure of the factors allows us to decompose the distribution in several
ways; for example:
P(A, B, C, D) =
 1
Z φ1(A, B)φ2(B, C)

φ3(C, D)φ4(A, D).
From this decomposition, we can infer that P |= (B ⊥D | A, C). We can similarly infer that
P |= (A ⊥C | B, D). These are precisely the two independencies that we tried, unsuccessfully,
to achieve using a Bayesian network, in example 3.8. Moreover, these properties correspond to
our intuition of “paths of inﬂuence” in the graph, where we have that B and D are separated
given A, C, and that A and C are separated given B, D. Indeed, as in a Bayesian network,
independence properties of the distribution P correspond directly to separation properties in
the graph over which P factorizes.
4.2
Parameterization
We begin our formal discussion by describing the parameterization used in the class of undi-
rected graphical models that are the focus of this chapter. In the next section, we make the
connection to the graph structure and demonstrate how it captures the independence properties
of the distribution.
To represent a distribution, we need to associate the graph structure with a set of parameters,
in the same way that CPDs were used to parameterize the directed graph structure. However,
the parameterization of Markov networks is not as intuitive as that of Bayesian networks, since
the factors do not correspond either to probabilities or to conditional probabilities. As a con-
sequence, the parameters are not intuitively understandable, making them hard to elicit from
people. As we will see in chapter 20, they are also signiﬁcantly harder to estimate from data.
4.2.1
Factors
A key issue in parameterizing a Markov network is that the representation is undirected, so
that the parameterization cannot be directed in nature. We therefore use factors, as deﬁned in
deﬁnition 4.1. Note that a factor subsumes both the notion of a joint distribution and the notion
of a CPD. A joint distribution over D is a factor over D: it speciﬁes a real number for every
assignment of values of D. A conditional distribution P(X | U) is a factor over {X} ∪U.
However, both CPDs and joint distributions must satisfy certain normalization constraints (for
example, in a joint distribution the numbers must sum to 1), whereas there are no constraints
on the parameters in a factor.

4.2. Parameterization
107
a1
a1
a1
a1
a2
a2
a2
a2
a3
a3
a3
a3
b1
b1
b2
b2
b1
b1
b2
b2
b1
b1
b2
b2
c1
c2
c1
c2
c1
c2
c1
c2
c1
c2
c1
c2
0.5⋅0.5 = 0.25
0.5⋅0.7 = 0.35
0.8⋅0.1 = 0.08
0.8⋅0.2 = 0.16
0.1⋅0.5 = 0.05
0.1⋅0.7 = 0.07
 0⋅0.1 = 0
 0⋅0.2 = 0
0.3⋅0.5 = 0.15
0.3⋅0.7 = 0.21
0.9⋅0.1 = 0.09
0.9⋅0.2 = 0.18
a1
a1
a2
a2
a3
a3
b1
b2
b1
b2
b1
b2
0.5
0.8
0.1
0
0.3
0.9
b1
b1
b2
b2
c1
c2
c1
c2
0.5
0.7
0.1
0.2
Figure 4.3
An example of factor product
As we discussed, we can view a factor as roughly describing the “compatibilities” between
diﬀerent values of the variables in its scope. We can now parameterize the graph by associating
a set of a factors with it. One obvious idea might be to associate parameters directly with
the edges in the graph. However, a simple calculation will convince us that this approach is
insuﬃcient to parameterize a full distribution.
Example 4.1
Consider a fully connected graph over X; in this case, the graph speciﬁes no conditional indepen-
dence assumptions, so that we should be able to specify an arbitrary joint distribution over X. If
all of the variables are binary, each factor over an edge would have 4 parameters, and the total
number of parameters in the graph would be 4
 n
2

. However, the number of parameters required to
specify a joint distribution over n binary variables is 2n −1. Thus, pairwise factors simply do not
have enough parameters to encompass the space of joint distributions. More intuitively, such factors
capture only the pairwise interactions, and not interactions that involve combinations of values of
larger subsets of variables.
A more general representation can be obtained by allowing factors over arbitrary subsets of
variables. To provide a formal deﬁnition, we ﬁrst introduce the following important operation
on factors.
Deﬁnition 4.2
Let X, Y , and Z be three disjoint sets of variables, and let φ1(X, Y ) and φ2(Y , Z) be two
factors. We deﬁne the factor product φ1 × φ2 to be a factor ψ : Val(X, Y , Z) 7→IR as follows:
factor product
ψ(X, Y , Z) = φ1(X, Y ) · φ2(Y , Z).
The key aspect to note about this deﬁnition is the fact that the two factors φ1 and φ2 are
multiplied in a way that “matches up” the common part Y .
Figure 4.3 shows an example
of the product of two factors.
We have deliberately chosen factors that do not correspond
either to probabilities or to conditional probabilities, in order to emphasize the generality of this
operation.

108
Chapter 4. Undirected Graphical Models
As we have already observed, both CPDs and joint distributions are factors. Indeed, the chain
rule for Bayesian networks deﬁnes the joint distribution factor as the product of the CPD factors.
For example, when computing P(A, B) = P(A)P(B | A), we always multiply entries in the
P(A) and P(B | A) tables that have the same value for A.
Thus, letting φXi(Xi, PaXi)
represent P(Xi | PaXi), we have that
P(X1, . . . , Xn) =
Y
i
φXi.
4.2.2
Gibbs Distributions and Markov Networks
We can now use the more general notion of factor product to deﬁne an undirected parameteri-
zation of a distribution.
Deﬁnition 4.3
A distribution PΦ is a Gibbs distribution parameterized by a set of factors Φ = {φ1(D1), . . . , φK(DK)}
Gibbs
distribution
if it is deﬁned as follows:
PΦ(X1, . . . , Xn) = 1
Z
˜PΦ(X1, . . . , Xn),
where
˜PΦ(X1, . . . , Xn) = φ1(D1) × φ2(D2) × · · · × φm(Dm)
is an unnormalized measure and
Z =
X
X1,...,Xn
˜PΦ(X1, . . . , Xn)
is a normalizing constant called the partition function.
partition function
It is tempting to think of the factors as representing the marginal probabilities of the variables
in their scope.
Thus, looking at any individual factor, we might be led to believe that the
behavior of the distribution deﬁned by the Markov network as a whole corresponds to the
behavior deﬁned by the factor. However, this intuition is overly simplistic. A factor is only one

contribution to the overall joint distribution. The distribution as a whole has to take into
consideration the contributions from all of the factors involved.
Example 4.2
Consider the distribution of ﬁgure 4.2. The marginal distribution over A, B, is
a0
b0
0.13
a0
b1
0.69
a1
b0
0.14
a1
b1
0.04
The most likely conﬁguration is the one where Alice and Bob disagree. By contrast, the highest
entry in the factor φ1(A, B) in ﬁgure 4.1 corresponds to the assignment a0, b0. The reason for the
discrepancy is the inﬂuence of the other factors on the distribution. In particular, φ3(C, D) asserts
that Charles and Debbie disagree, whereas φ2(B, C) and φ4(D, A) assert that Bob and Charles
agree and that Debbie and Alice agree. Taking just these factors into consideration, we would
conclude that Alice and Bob are likely to disagree. In this case, the “strength” of these other factors is
much stronger than that of the φ1(A, B) factor, so that the inﬂuence of the latter is overwhelmed.

4.2. Parameterization
109
(b)
(a)
B
D
A
C
B
D
A
C
Figure 4.4
The cliques in two simple Markov networks. In (a), the cliques are the pairs {A, B},
{B, C}, {C, D}, and {D, A}. In (b), the cliques are {A, B, D} and {B, C, D}.
We now want to relate the parameterization of a Gibbs distribution to a graph structure. If
our parameterization contains a factor whose scope contains both X and Y , we are introducing
a direct interaction between them. Intuitively, we would like these direct interactions to be
represented in the graph structure. Thus, if our parameterization contains such a factor, we
would like the associated Markov network structure H to contain an edge between X and Y .
Deﬁnition 4.4
We say that a distribution PΦ with Φ = {φ1(D1), . . . , φK(DK)} factorizes over a Markov
Markov network
factorization
network H if each Dk (k = 1, . . . , K) is a complete subgraph of H.
The factors that parameterize a Markov network are often called clique potentials.
clique potentials
As we will see, if we associate factors only with complete subgraphs, as in this deﬁnition, we
are not violating the independence assumptions induced by the network structure, as deﬁned
later in this chapter.
Note that, because every complete subgraph is a subset of some (maximal) clique, we can
reduce the number of factors in our parameterization by allowing factors only for maximal
cliques. More precisely, let C1, . . . , Ck be the cliques in H. We can parameterize P using a
set of factors φ1(C1), . . . , φl(Cl). Any factorization in terms of complete subgraphs can be
converted into this form simply by assigning each factor to a clique that encompasses its scope
and multiplying all of the factors assigned to each clique to produce a clique potential. In our
Misconception example, we have four cliques: {A, B}, {B, C}, {C, D}, and {A, D}. Each of
these cliques can have its own clique potential. One possible setting of the parameters in these
clique potential is shown in ﬁgure 4.1. Figure 4.4 shows two examples of a Markov network and
the (maximal) cliques in that network.

Although it can be used without loss of generality, the parameterization using maximal
clique potentials generally obscures structure that is present in the original set of factors.
For example, consider the Gibbs distribution described in example 4.1. Here, we have a potential
for every pair of variables, so the Markov network associated with this distribution is a single
large clique containing all variables. If we associate a factor with this single clique, it would be
exponentially large in the number of variables, whereas the original parameterization in terms of
edges requires only a quadratic number of parameters. See section 4.4.1.1 for further discussion.

110
Chapter 4. Undirected Graphical Models
A2,1
A2,2
A3,1
A3,2
A3,3
A3,4
A4,1
A4,2
A4,3
A4,4
A2,3
A2,4
A1,1
A1,2
A1,3
A1,4
Figure 4.A.1 — A pairwise Markov network (MRF) structured as a grid.
Box 4.A — Concept: Pairwise Markov Networks. A subclass of Markov networks that arises
in many contexts is that of pairwise Markov networks, representing distributions where all of the
pairwise Markov
network
factors are over single variables or pairs of variables. More precisely, a pairwise Markov network over
a graph H is associated with a set of node potentials {φ(Xi) : i = 1, . . . , n} and a set of edge
node potential
edge potential
potentials {φ(Xi, Xj) : (Xi, Xj) ∈H}. The overall distribution is (as always) the normalized
product of all of the potentials (both node and edge). Pairwise MRFs are attractive because of their
simplicity, and because interactions on edges are an important special case that often arises in
practice (see, for example, box 4.C and box 4.B).
A class of pairwise Markov networks that often arises, and that is commonly used as a benchmark
for inference, is the class of networks structured in the form of a grid, as shown in ﬁgure 4.A.1. As we
discuss in the inference chapters of this book, although these networks have a simple and compact
representation, they pose a signiﬁcant challenge for inference algorithms.
4.2.3
Reduced Markov Networks
We end this section with one ﬁnal concept that will prove very useful in later sections. Consider
the process of conditioning a distribution on some assignment u to some subset of variables
U. Conditioning a distribution corresponds to eliminating all entries in the joint distribution
that are inconsistent with the event U = u, and renormalizing the remaining entries to sum
to 1. Now, consider the case where our distribution has the form PΦ for some set of factors
Φ. Each entry in the unnormalized measure ˜PΦ is a product of entries from the factors Φ, one
entry from each factor. If, in some factor, we have an entry that is inconsistent with U = u,
it will only contribute to entries in ˜PΦ that are also inconsistent with this event. Thus, we can
eliminate all such entries from every factor in Φ.
More generally, we can deﬁne:

4.2. Parameterization
111
a1
a1
a2
a2
a3
a3
b1
b2
b1
b2
b1
b2
c1
c1
c1
c1
c1
c1
0.25
0.08
0.05
0
0.15
0.09
Figure 4.5
Factor reduction: The factor computed in ﬁgure 4.3, reduced to the context C = c1.
Deﬁnition 4.5
Let φ(Y ) be a factor, and U = u an assignment for U ⊆Y . We deﬁne the reduction of the
factor reduction
factor φ to the context U = u, denoted φ[U = u] (and abbreviated φ[u]), to be a factor over scope
Y ′ = Y −U, such that
φ[u](y′) = φ(y′, u).
For U ̸⊂Y , we deﬁne φ[u] to be φ[U ′ = u′], where U ′ = U ∩Y , and u′ = u⟨U ′⟩, where
u⟨U ′⟩denotes the assignment in u to the variables in U ′.
Figure 4.5 illustrates this operation, reducing the of ﬁgure 4.3 to the context C = c1.
Now, consider a product of factors. An entry in the product is consistent with u if and only
if it is a product of entries that are all consistent with u. We can therefore deﬁne:
Deﬁnition 4.6
Let PΦ be a Gibbs distribution parameterized by Φ = {φ1, . . . , φK} and let u be a context. The
reduced Gibbs distribution PΦ[u] is the Gibbs distribution deﬁned by the set of factors Φ[u] =
reduced Gibbs
distribution
{φ1[u], . . . , φK[u]}.
Reducing the set of factors deﬁning PΦ to some context u corresponds directly to the opera-
tion of conditioning PΦ on the observation u. More formally:
Proposition 4.1
Let PΦ(X) be a Gibbs distribution. Then PΦ[u] = PΦ(W | u) where W = X −U.
Thus, to condition a Gibbs distribution on a context u, we simply reduce every one of its
factors to that context. Intuitively, the renormalization step needed to account for u is simply
folded into the standard renormalization of any Gibbs distribution.
This result immediately
provides us with a construction for the Markov network that we obtain when we condition the
associated distribution on some observation u.
Deﬁnition 4.7
Let H be a Markov network over X and U = u a context. The reduced Markov network H[u]
reduced Markov
network
is a Markov network over the nodes W = X −U, where we have an edge X—Y if there is an
edge X—Y in H.
Proposition 4.2
Let PΦ(X) be a Gibbs distribution that factorizes over H, and U = u a context. Then PΦ[u]
factorizes over H[u].

112
Chapter 4. Undirected Graphical Models
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
(a)
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
(b)
Letter
Job
Happy
Coherence
Intelligence
Difﬁculty
(c)
Figure 4.6
Markov networks for the factors in an extended Student example: (a) The initial set of
factors; (b) Reduced to the context G = g; (c) Reduced to the context G = g, S = s.
Note the contrast to the eﬀect of conditioning in a Bayesian network: Here, conditioning on a
context u only eliminates edges from the graph; in a Bayesian network, conditioning on evidence
can activate a v-structure, creating new dependencies. We return to this issue in section 4.5.1.1.
Example 4.3
Consider, for example, the Markov network shown in ﬁgure 4.6a; as we will see, this network is
the Markov network required to capture the distribution encoded by an extended version of our
Student Bayesian network (see ﬁgure 9.8). Figure 4.6b shows the same Markov network reduced
over a context of the form G = g, and (c) shows the network reduced over a context of the form
G = g, S = s. As we can see, the network structures are considerably simpliﬁed.
Box 4.B — Case Study: Markov Networks for Computer Vision. One important application area
for Markov networks is computer vision. Markov networks, typically called MRFs in this vision com-
munity, have been used for a wide variety of visual processing tasks, such as image segmentation,
removal of blur or noise, stereo reconstruction, object recognition, and many more.
In most of these applications, the network takes the structure of a pairwise MRF, where the
variables correspond to pixels and the edges (factors) to interactions between adjacent pixels in
the grid that represents the image; thus, each (interior) pixel has exactly four neighbors. The value
space of the variables and the exact form of factors depend on the task. These models are usually
formulated in terms of energies (negative log-potentials), so that values represent “penalties,” and a
lower value corresponds to a higher-probability conﬁguration.
In image denoising, for example, the task is to restore the “true” value of all of the pixels
image denoising
given possibly noisy pixel values. Here, we have a node potential for each pixel Xi that penalizes
large discrepancies from the observed pixel value yi. The edge potential encodes a preference for
continuity between adjacent pixel values, penalizing cases where the inferred value for Xi is too

4.2. Parameterization
113
far from the inferred pixel value for one of its neighbors Xj. However, it is important not to
overpenalize true disparities (such as edges between objects or regions), leading to oversmoothing of
the image. Thus, we bound the penalty, using, for example, some truncated norm, as described in
box 4.D: ϵ(xi, xj) = min(c∥xi −xj∥p, distmax) (for p ∈{1, 2}).
Slight variants of the same model are used in many other applications. For example, in stereo
stereo
reconstruction
reconstruction, the goal is to reconstruct the depth disparity of each pixel in the image. Here, the
values of the variables represent some discretized version of the depth dimension (usually more ﬁnely
discretized for distances close to the camera and more coarsely discretized as the distance from the
camera increases). The individual node potential for each pixel Xi uses standard techniques from
computer vision to estimate, from a pair of stereo images, the individual depth disparity of this
pixel. The edge potentials, precisely as before, often use a truncated metric to enforce continuity of
the depth estimates, with the truncation avoiding an overpenalization of true depth disparities (for
example, when one object is partially in front of the other). Here, it is also quite common to make
the penalty inversely proportional to the image gradient between the two pixels, allowing a smaller
penalty to be applied in cases where a large image gradient suggests an edge between the pixels,
possibly corresponding to an occlusion boundary.
In image segmentation, the task is to partition the image pixels into regions corresponding to
image
segmentation
distinct parts of the scene. There are diﬀerent variants of the segmentation task, many of which
can be formulated as a Markov network. In one formulation, known as multiclass segmentation,
each variable Xi has a domain {1, . . . , K}, where the value of Xi represents a region assignment
for pixel i (for example, grass, water, sky, car). Since classifying every pixel can be computationally
expensive, some state-of-the-art methods for image segmentation and other tasks ﬁrst oversegment
the image into superpixels (or small coherent regions) and classify each region — all pixels within
a region are assigned the same label. The oversegmented image induces a graph in which there is
one node for each superpixel and an edge between two nodes if the superpixels are adjacent (share
a boundary) in the underlying image. We can now deﬁne our distribution in terms of this graph.
Features are extracted from the image for each pixel or superpixel. The appearance features
depend on the speciﬁc task. In image segmentation, for example, features typically include statistics
over color, texture, and location. Often the features are clustered or provided as input to local
classiﬁers to reduce dimensionality. The features used in the model are then the soft cluster assign-
ments or local classiﬁer outputs for each superpixel. The node potential for a pixel or superpixel
is then a function of these features. We note that the factors used in deﬁning this model depend
on the speciﬁc values of the pixels in the image, so that each image deﬁnes a diﬀerent probability
distribution over the segment labels for the pixels or superpixels. In eﬀect, the model used here is a
conditional random ﬁeld, a concept that we deﬁne more formally in section 4.6.1.
conditional
random ﬁeld
The model contains an edge potential between every pair of neighboring superpixels Xi, Xj.
Most simply, this potential encodes a contiguity preference, with a penalty of λ whenever Xi ̸= Xj.
Again, we can improve the model by making the penalty depend on the presence of an image
gradient between the two pixels. An even better model does more than penalize discontinuities. We
can have nondefault values for other class pairs, allowing us to encode the fact that we more often
ﬁnd tigers adjacent to vegetation than adjacent to water; we can even make the model depend on
the relative pixel location, allowing us to encode the fact that we usually ﬁnd water below vegetation,
cars over roads, and sky above everything.
Figure 4.B.1 shows segmentation results in a model containing only potentials on single pixels
(thereby labeling each of them independently) versus results obtained from a model also containing

114
Chapter 4. Undirected Graphical Models
car
road
building
cow
grass
(a)
(b)
(c)
(d)
Figure 4.B.1 — Two examples of image segmentation results (a) The original image. (b) An overseg-
mentation known as superpixels; each superpixel is associated with a random variable that designates its
segment assignment. The use of superpixels reduces the size of the problems. (c) Result of segmentation
using node potentials alone, so that each superpixel is classiﬁed independently. (d) Result of segmentation
using a pairwise Markov network encoding interactions between adjacent superpixels.
pairwise potentials. The diﬀerence in the quality of the results clearly illustrates the importance of
modeling the correlations between the superpixels.
4.3
Markov Network Independencies
In section 4.1, we gave an intuitive justiﬁcation of why an undirected graph seemed to capture
the types of interactions in the Misconception example. We now provide a formal presentation
of the undirected graph as a representation of independence assertions.
4.3.1
Basic Independencies
As in the case of Bayesian networks, the graph structure in a Markov network can be viewed
as encoding a set of independence assumptions. Intuitively, in Markov networks, probabilistic
inﬂuence “ﬂows” along the undirected paths in the graph, but it is blocked if we condition on
the intervening nodes.
Deﬁnition 4.8
Let H be a Markov network structure, and let X1— . . . —Xk be a path in H. Let Z ⊆X be a set
of observed variables. The path X1— . . . —Xk is active given Z if none of the Xi’s, i = 1, . . . , k,
observed variable
active path
is in Z.
Using this notion, we can deﬁne a notion of separation in the graph.

4.3. Markov Network Independencies
115
Deﬁnition 4.9
We say that a set of nodes Z separates X and Y in H, denoted sepH(X; Y | Z), if there is no
separation
active path between any node X ∈X and Y ∈Y given Z. We deﬁne the global independencies
global
independencies
associated with H to be:
I(H) = {(X ⊥Y | Z) : sepH(X; Y | Z)}.
As we will discuss, the independencies in I(H) are precisely those that are guaranteed to
hold for every distribution P over H. In other words, the separation criterion is sound for
detecting independence properties in distributions over H.
Note that the deﬁnition of separation is monotonic in Z, that is, if sepH(X; Y | Z), then
sepH(X; Y | Z′) for any Z′ ⊃Z. Thus, if we take separation as our deﬁnition of the inde-
pendencies induced by the network structure, we are eﬀectively restricting our ability to encode
nonmonotonic independence relations. Recall that in the context of intercausal reasoning in
Bayesian networks, nonmonotonic reasoning patterns are quite useful in many situations —
for example, when two diseases are independent, but dependent given some common symp-
tom. The nature of the separation property implies that such independence patterns cannot be
expressed in the structure of a Markov network. We return to this issue in section 4.5.
As for Bayesian networks, we can show a connection between the independence properties
implied by the Markov network structure, and the possibility of factorizing a distribution over
the graph. As before, we can now state the analogue to both of our representation theorems for
Bayesian networks, which assert the equivalence between the Gibbs factorization of a distribution
P over a graph H and the assertion that H is an I-map for P, that is, that P satisﬁes the Markov
assumptions I(H).
4.3.1.1
Soundness
We ﬁrst consider the analogue to theorem 3.2, which asserts that a Gibbs distribution satisﬁes
the independencies associated with the graph. In other words, this result states the soundness
soundness
of the separation criterion.
Theorem 4.1
Let P be a distribution over X, and H a Markov network structure over X. If P is a Gibbs
distribution that factorizes over H, then H is an I-map for P.
Proof Let X, Y , Z be any three disjoint subsets in X such that Z separates X and Y in H.
We want to show that P |= (X ⊥Y | Z).
We start by considering the case where X ∪Y ∪Z = X. As Z separates X from Y , there
are no direct edges between X and Y . Hence, any clique in H is fully contained either in
X ∪Z or in Y ∪Z. Let IX be the indexes of the set of cliques that are contained in X ∪Z,
and let IY be the indexes of the remaining cliques. We know that
P(X1, . . . , Xn) = 1
Z
Y
i∈IX
φi(Di) ·
Y
i∈IY
φi(Di).
As we discussed, none of the factors in the ﬁrst product involve any variable in Y , and none in
the second product involve any variable in X. Hence, we can rewrite this product in the form:
P(X1, . . . , Xn) = 1
Z f(X, Z)g(Y , Z).

116
Chapter 4. Undirected Graphical Models
From this decomposition, the desired independence follows immediately (exercise 2.5).
Now consider the case where X ∪Y ∪Z ⊂X. Let U = X −(X ∪Y ∪Z). We can
partition U into two disjoint sets U 1 and U 2 such that Z separates X ∪U 1 from Y ∪U 2 in
H. Using the preceding argument, we conclude that P |= (X, U 1 ⊥Y , U 2 | Z). Using the
decomposition property (equation (2.8)), we conclude that P |= (X ⊥Y | Z).
The other direction (the analogue to theorem 3.1), which goes from the independence prop-
erties of a distribution to its factorization, is known as the Hammersley-Cliﬀord theorem. Unlike
Hammersley-
Cliﬀord
theorem
for Bayesian networks, this direction does not hold in general. As we will show, it holds only
under the additional assumption that P is a positive distribution (see deﬁnition 2.5).
Theorem 4.2
Let P be a positive distribution over X, and H a Markov network graph over X. If H is an I-map
for P, then P is a Gibbs distribution that factorizes over H.
To prove this result, we would need to use the independence assumptions to construct a set
of factors over H that give rise to the distribution P. In the case of Bayesian networks, these
factors were simply CPDs, which we could derive directly from P. As we have discussed, the
correspondence between the factors in a Gibbs distribution and the distribution P is much
more indirect. The construction required here is therefore signiﬁcantly more subtle, and relies
on concepts that we develop later in this chapter; hence, we defer the proof to section 4.4
(theorem 4.8).
This result shows that, for positive distributions, the global independencies imply that

the distribution factorizes according the network structure. Thus, for this class of distri-
butions, we have that a distribution P factorizes over a Markov network H if and only if
H is an I-map of P. The positivity assumption is necessary for this result to hold:
Example 4.4
Consider a distribution P over four binary random variables X1, X2, X3, X4, which gives proba-
bility 1/8 to each of the following eight conﬁgurations, and probability zero to all others:
(0,0,0,0)
(1,0,0,0)
(1,1,0,0)
(1,1,1,0)
(0,0,0,1)
(0,0,1,1)
(0,1,1,1)
(1,1,1,1)
Let H be the graph X1—X2—X3—X4—X1. Then P satisﬁes the global independencies with
respect to H. For example, consider the independence (X1 ⊥X3 | X2, X4). For the assignment
X2 = x1
2, X4 = x0
4, we have that only assignments where X1 = x1
1 receive positive probability.
Thus, P(x1
1 | x1
2, x0
4) = 1, and X1 is trivially independent of X3 in this conditional distribution.
A similar analysis applies to all other cases, so that the global independencies hold. However, the
distribution P does not factorize according to H. The proof of this fact is left as an exercise (see
exercise 4.1).
4.3.1.2
Completeness
The preceding discussion shows the soundness of the separation condition as a criterion for
detecting independencies in Markov networks: any distribution that factorizes over G satisﬁes
the independence assertions implied by separation. The next obvious issue is the completeness
completeness
of this criterion.

4.3. Markov Network Independencies
117
As for Bayesian networks, the strong version of completeness does not hold in this setting. In
other words, it is not the case that every pair of nodes X and Y that are not separated in H
are dependent in every distribution P which factorizes over H. However, as in theorem 3.3, we
can use a weaker deﬁnition of completeness that does hold:
Theorem 4.3
Let H be a Markov network structure. If X and Y are not separated given Z in H, then X and
Y are dependent given Z in some distribution P that factorizes over H.
Proof The proof is a constructive one: we construct a distribution P that factorizes over H
where X and Y are dependent. We assume, without loss of generality, that all variables are
binary-valued. If this is not the case, we can treat them as binary-valued by restricting attention
to two distinguished values for each variable.
By assumption, X and Y are not separated given Z in H; hence, they must be connected by
some unblocked trail. Let X = U1—U2— . . . —Uk = Y be some minimal trail in the graph
such that, for all i, Ui ̸∈Z, where we deﬁne a minimal trail in H to be a path with no shortcuts:
thus, for any i and j ̸= i ± 1, there is no edge Ui—Uj. We can always ﬁnd such a path: If we
have a nonminimal path where we have Ui—Uj for j > i + 1, we can always “shortcut” the
original trail, converting it to one that goes directly from Ui to Uj.
For any i = 1, . . . , k −1, as there is an edge Ui—Ui+1, it follows that Ui, Ui+1 must both
appear in some clique Ci. We pick some very large weight W, and for each i we deﬁne the
clique potential φi(Ci) to assign weight W if Ui = Ui+1 and weight 1 otherwise, regardless of
the values of the other variables in the clique. Note that the cliques Ci for Ui, Ui+1 and Cj for
Uj, Uj+1 must be diﬀerent cliques: If Ci = Cj, then Uj is in the same clique as Ui, and we
have an edge Ui—Uj, contradicting the minimality of the trail. Hence, we can deﬁne the clique
potential for each clique Ci separately. We deﬁne the clique potential for any other clique to
be uniformly 1.
We now consider the distribution P resulting from multiplying all of these clique potentials.
Intuitively, the distribution P(U1, . . . , Uk) is simply the distribution deﬁned by multiplying the
pairwise factors for the pairs Ui, Ui+1, regardless of the other variables (including the ones in
Z). One can verify that, in P(U1, . . . , Uk), we have that X = U1 and Y = Uk are dependent.
We leave the conclusion of this argument as an exercise (exercise 4.5).
We can use the same argument as theorem 3.5 to conclude that, for almost all distributions
P that factorize over H (that is, for all distributions except for a set of measure zero in the
space of factor parameterizations) we have that I(P) = I(H).
Once again, we can view this result as telling us that our deﬁnition of I(H) is the maximal
one. For any independence assertion that is not a consequence of separation in H, we can
always ﬁnd a counterexample distribution P that factorizes over H.
4.3.2
Independencies Revisited
When characterizing the independencies in a Bayesian network, we provided two deﬁnitions:
the local independencies (each node is independent of its nondescendants given its parents),
and the global independencies induced by d-separation.
As we showed, these two sets of
independencies are equivalent, in that one implies the other.

118
Chapter 4. Undirected Graphical Models
So far, our discussion for Markov networks provides only a global criterion. While the global
criterion characterizes the entire set of independencies induced by the network structure, a local
criterion is also valuable, since it allows us to focus on a smaller set of properties when examining
the distribution, signiﬁcantly simplifying the process of ﬁnding an I-map for a distribution P.
Thus, it is natural to ask whether we can provide a local deﬁnition of the independencies
induced by a Markov network, analogously to the local independencies of Bayesian networks.
Surprisingly, as we now show, in the context of Markov networks, there are three diﬀerent pos-
sible deﬁnitions of the independencies associated with the network structure — two local ones
and the global one in deﬁnition 4.9. While these deﬁnitions are related, they are equivalent
only for positive distributions. As we will see, nonpositive distributions allow for deterministic
dependencies between the variables. Such deterministic interactions can “fool” local indepen-
dence tests, allowing us to construct networks that are not I-maps of the distribution, yet the
local independencies hold.
4.3.2.1
Local Markov Assumptions
The ﬁrst, and weakest, deﬁnition is based on the following intuition: Whenever two variables
are directly connected, they have the potential of being directly correlated in a way that is not
mediated by other variables. Conversely, when two variables are not directly linked, there must
be some way of rendering them conditionally independent. Speciﬁcally, we can require that X
and Y be independent given all other nodes in the graph.
Deﬁnition 4.10
Let H be a Markov network. We deﬁne the pairwise independencies associated with H to be:
pairwise
independencies
Ip(H) = {(X ⊥Y | X −{X, Y }) : X—Y ̸∈H}.
Using this deﬁnition, we can easily represent the independencies in our Misconception example
using a Markov network: We simply connect the nodes up in exactly the same way as the
interaction structure between the students.
The second local deﬁnition is an undirected analogue to the local independencies associated
with a Bayesian network. It is based on the intuition that we can block all inﬂuences on a node
by conditioning on its immediate neighbors.
Deﬁnition 4.11
For a given graph H, we deﬁne the Markov blanket of X in H, denoted MBH(X), to be the
Markov blanket
neighbors of X in H. We deﬁne the local independencies associated with H to be:
local
independencies
Iℓ(H) = {(X ⊥X −{X} −MBH(X) | MBH(X)) : X ∈X}.
In other words, the local independencies state that X is independent of the rest of the nodes
in the graph given its immediate neighbors.
We will show that these local independence
assumptions hold for any distribution that factorizes over H, so that X’s Markov blanket in H
truly does separate it from all other variables.
4.3.2.2
Relationships between Markov Properties
We have now presented three sets of independence assertions associated with a network struc-
ture H. For general distributions, Ip(H) is strictly weaker than Iℓ(H), which in turn is strictly
weaker than I(H). However, all three deﬁnitions are equivalent for positive distributions.

4.3. Markov Network Independencies
119
Proposition 4.3
For any Markov network H, and any distribution P, we have that if P |= Iℓ(H) then P |= Ip(H).
The proof of this result is left as an exercise (exercise 4.8).
Proposition 4.4
For any Markov network H, and any distribution P, we have that if P |= I(H) then P |= Iℓ(H).
The proof of this result follows directly from the fact that if X and Y are not connected by an
edge, then they are necessarily separated by all of the remaining nodes in the graph.
The converse of these inclusion results holds only for positive distributions (see deﬁnition 2.5).
More speciﬁcally, if we assume the intersection property (equation (2.11)), all three of the Markov
conditions are equivalent.
Theorem 4.4
Let P be a positive distribution. If P satisﬁes Ip(H), then P satisﬁes I(H).
Proof We want to prove that for all disjoint sets X, Y , Z:
sepH(X; Y | Z) =⇒P |= (X ⊥Y | Z).
(4.1)
The proof proceeds by descending induction on the size of Z.
The base case is |Z| = n−2; equation (4.1) follows immediately from the deﬁnition of Ip(H).
For the inductive step, assume that equation (4.1) holds for every Z′ with size |Z′| = k, and
let Z be any set such that |Z| = k −1. We distinguish between two cases.
In the ﬁrst case, X ∪Z ∪Y = X. As |Z| < n −2, we have that either |X| ≥2 or |Y | ≥2.
Without loss of generality, assume that the latter holds; let A ∈Y and Y ′ = Y −{A}. From the
fact that sepH(X; Y | Z), we also have that sepH(X; Y ′ | Z) on one hand and sepH(X; A |
Z) on the other hand. As separation is monotonic, we also have that sepH(X; Y ′ | Z ∪{A})
and sepH(X; A | Z ∪Y ′). The separating sets Z ∪{A} and Z ∪Y ′ are each at least size
|Z| + 1 = k in size, so that equation (4.1) applies, and we can conclude that P satisﬁes:
(X ⊥Y ′ | Z ∪{A})
&
(X ⊥A | Z ∪Y ′).
Because P is positive, we can apply the intersection property (equation (2.11)) and conclude that
P |= (X ⊥Y ′ ∪{A} | Z), that is, (X ⊥Y | Z).
The second case is where X ∪Y ∪Z ̸= X. Here, we might have that both X and Y
are singletons. This case requires a similar argument that uses the induction hypothesis and
properties of independence. We leave it as an exercise (exercise 4.9).
Our previous results entail that, for positive distributions, the three conditions are equivalent.
Corollary 4.1
The following three statements are equivalent for a positive distribution P:
1. P |= Iℓ(H).
2. P |= Ip(H).
3. P |= I(H).
This equivalence relies on the positivity assumption. In particular, for nonpositive distribu-
tions, we can provide examples of a distribution P that satisﬁes one of these properties, but not
the stronger one.

120
Chapter 4. Undirected Graphical Models
Example 4.5
Let P be any distribution over X = {X1, . . . , Xn}; let X ′ = {X′
1, . . . , X′
n}. We now construct
a distribution P ′(X, X ′) whose marginal over X1, . . . , Xn is the same as P, and where X′
i is
deterministically equal to Xi. Let H be a Markov network over X, X ′ that contains no edges other
than Xi—X′
i. Then, in P ′, Xi is independent of the rest of the variables in the network given
its neighbor X′
i, and similarly for X′
i; thus, H satisﬁes the local independencies for every node in
the network. Yet clearly H is not an I-map for P ′, since H makes many independence assertions
regarding the Xi’s that do not hold in P (or in P ′).
Thus, for nonpositive distributions, the local independencies do not imply the global ones.
A similar construction can be used to show that, for nonpositive distributions, the pairwise
independencies do necessarily imply the local independencies.
Example 4.6
Let P be any distribution over X = {X1, . . . , Xn}, and now consider two auxiliary sets of vari-
ables X ′ and X ′′, and deﬁne X ∗= X ∪X ′ ∪X ′′. We now construct a distribution P ′(X ∗) whose
marginal over X1, . . . , Xn is the same as P, and where X′
i and X′′
i are both deterministically
equal to Xi. Let H be the empty Markov network over X ∗. We argue that this empty network
satisﬁes the pairwise assumptions for every pair of nodes in the network. For example, Xi and
X′
i are rendered independent because X ∗−{Xi, X′
i} contains X′′
i . Similarly, Xi and Xj are
independent given X′
i. Thus, H satisﬁes the pairwise independencies, but not the local or global
independencies.
4.3.3
From Distributions to Graphs
Based on our deeper understanding of the independence properties associated with a Markov
network, we can now turn to the question of encoding the independencies in a given distribution
P using a graph structure. As for Bayesian networks, the notion of an I-map is not suﬃcient by
itself: The complete graph implies no independence assumptions and is hence an I-map for any
distribution. We therefore return to the notion of a minimal I-map, deﬁned in deﬁnition 3.13,
which was deﬁned broadly enough to apply to Markov networks as well.
How can we construct a minimal I-map for a distribution P? Our discussion in section 4.3.2
immediately suggests two approaches for constructing a minimal I-map: one based on the
pairwise Markov independencies, and the other based on the local independencies.
In the ﬁrst approach, we consider the pairwise independencies. They assert that, if the edge
{X, Y } is not in H, then X and Y must be independent given all other nodes in the graph,
regardless of which other edges the graph contains. Thus, at the very least, to guarantee that H
is an I-map, we must add direct edges between all pairs of nodes X and Y such that
P ̸|= (X ⊥Y | X −{X, Y }).
(4.2)
We can now deﬁne H to include an edge X—Y for all X, Y for which equation (4.2) holds.
In the second approach, we use the local independencies and the notion of minimality. For
each variable X, we deﬁne the neighbors of X to be a minimal set of nodes Y that render X
independent of the rest of the nodes. More precisely, deﬁne:

4.3. Markov Network Independencies
121
Deﬁnition 4.12
A set U is a Markov blanket of X in a distribution P if X ̸∈U and if U is a minimal set of
Markov blanket
nodes such that
(X ⊥X −{X} −U | U) ∈I(P).
(4.3)
We then deﬁne a graph H by introducing an edge {X, Y } for all X and all Y ∈MBP (X).
As deﬁned, this construction is not unique, since there may be several sets U satisfying equa-
tion (4.3). However, theorem 4.6 will show that there is only one such minimal set. In fact, we
now show that any positive distribution P has a unique minimal I-map, and that both of these
constructions produce this I-map.
We begin with the proof for the pairwise deﬁnition:
Theorem 4.5
Let P be a positive distribution, and let H be deﬁned by introducing an edge {X, Y } for all X, Y
for which equation (4.2) holds. Then the Markov network H is the unique minimal I-map for P.
Proof The fact that H is an I-map for P follows immediately from fact that P, by construction,
satisﬁes Ip(H), and, therefore, by corollary 4.1, also satisﬁes I(H). The fact that it is minimal
follows from the fact that if we eliminate some edge {X, Y } from H, the graph would imply the
pairwise independence (X ⊥Y | X −{X, Y }), which we know to be false for P (otherwise,
the edge would have been omitted in the construction of H). The uniqueness of the minimal
I-map also follows trivially: By the same argument, any other I-map H′ for P must contain
at least the edges in H and is therefore either equal to H or contains additional edges and is
therefore not minimal.
It remains to show that the second deﬁnition results in the same minimal I-map.
Theorem 4.6
Let P be a positive distribution. For each node X, let MBP (X) be a minimal set of nodes U
satisfying equation (4.3). We deﬁne a graph H by introducing an edge {X, Y } for all X and all
Y ∈MBP (X). Then the Markov network H is the unique minimal I-map for P.
The proof is left as an exercise (exercise 4.11).
Both of the techniques for constructing a minimal I-map make the assumption that the
distribution P is positive. As we have shown, for nonpositive distributions, neither the pairwise
independencies nor the local independencies imply the global one. Hence, for a nonpositive
distribution P, constructing a graph H such that P satisﬁes the pairwise assumptions for H
does not guarantee that H is an I-map for P. Indeed, we can easily demonstrate that both of
these constructions break down for nonpositive distributions.
Example 4.7
Consider a nonpositive distribution P over four binary variables A, B, C, D that assigns nonzero
probability only to cases where all four variables take on exactly the same value; for example, we
might have P(a1, b1, c1, d1) = 0.5 and P(a0, b0, c0, d0) = 0.5. The graph H shown in ﬁgure 4.7
is one possible output of applying the local independence I-map construction algorithm to P: For
example, P |= (A ⊥C, D | B), and hence {B} is a legal choice for MBP (A). A similar analysis
shows that this network satisﬁes the Markov blanket condition for all nodes. However, it is not an
I-map for the distribution.

122
Chapter 4. Undirected Graphical Models
B
A
D
C
Figure 4.7
An attempt at an I-map for a nonpositive distribution P
If we use the pairwise independence I-map construction algorithm for this distribution, the
network constructed is the empty network. For example, the algorithm would not place an edge
between A and B, because P |= (A ⊥B | C, D). Exactly the same analysis shows that no edges
will be placed into the graph. However, the resulting network is not an I-map for P.
Both these examples show that deterministic relations between variables can lead to failure
in the construction based on local and pairwise independence. Suppose that A and B are two
variables that are identical to each other and that both C and D are variables that correlated
to both A and B so that (C ⊥D | A, B) holds. Since A is identical to B, we have that
both (A, D ⊥C | B) and (B, D ⊥C | A) hold. In other words, it suﬃces to observe one of
these two variables to capture the relevant information both have about C and separate C from
D. In this case the Markov blanket of C is not uniquely deﬁned. This ambiguity leads to the
failure of both local and pairwise constructions. Clearly, identical variables are only one way of
getting such ambiguities in local independencies. Once we allow nonpositive distribution, other
distributions can have similar problems.
Having deﬁned the notion of a minimal I-map for a distribution P, we can now ask to
what extent it represents the independencies in P. More formally, we can ask whether every
distribution has a perfect map. Clearly, the answer is no, even for positive distributions:
Example 4.8
Consider a distribution arising from a three-node Bayesian network with a v-structure, for example,
the distribution induced in the Student example over the nodes Intelligence, Diﬃculty, and Grade
(ﬁgure 3.3). In the Markov network for this distribution, we must clearly have an edge between I
and G and between D and G. Can we omit the edge between I and D? No, because we do
not have that (I ⊥D | G) holds for the distribution; rather, we have the opposite: I and D are
dependent given G. Therefore, the only minimal I-map for this P is the fully connected graph,
which does not capture the marginal independence (I ⊥D) that holds in P.
This example provides another counterexample to the strong version of completeness men-
tioned earlier. The only distributions for which separation is a sound and complete criterion for
determining conditional independence are those for which H is a perfect map.
4.4
Parameterization Revisited
Now that we understand the semantics and independence properties of Markov networks, we
revisit some alternative representations for the parameterization of a Markov network.

4.4. Parameterization Revisited
123
(b)
(c)
(a)
C
B
A
C
B
A
C
B
A
Vf3
Vf
Vf1
Vf2
Figure 4.8
Diﬀerent factor graphs for the same Markov network: (a) One factor graph over A, B, C,
with a single factor over all three variables. (b) An alternative factor graph, with three pairwise factors. (c)
The induced Markov network for both is a clique over A, B, C.
4.4.1
Finer-Grained Parameterization
4.4.1.1
Factor Graphs
A Markov network structure does not generally reveal all of the structure in a Gibbs param-
eterization. In particular, one cannot tell from the graph structure whether the factors in the
parameterization involve maximal cliques or subsets thereof. Consider, for example, a Gibbs
distribution P over a fully connected pairwise Markov network; that is, P is parameterized by a
factor for each pair of variables X, Y ∈X. The clique potential parameterization would utilize
a factor whose scope is the entire graph, and which therefore uses an exponential number of
parameters. On the other hand, as we discussed in section 4.2.1, the number of parameters
in the pairwise parameterization is quadratic in the number of variables. Note that the com-
plete Markov network is not redundant in terms of conditional independencies — P does not
factorize over any smaller network. Thus, although the ﬁner-grained structure does not imply
additional independencies in the distribution (see exercise 4.6), it is still very signiﬁcant.
An alternative representation that makes this structure explicit is a factor graph. A factor
graph is a graph containing two types of nodes: one type corresponds, as usual, to random
variables; the other corresponds to factors over the variables. Formally:
Deﬁnition 4.13
A factor graph F is an undirected graph containing two types of nodes: variable nodes (denoted as
factor graph
ovals) and factor nodes (denoted as squares). The graph only contains edges between variable nodes
and factor nodes. A factor graph F is parameterized by a set of factors, where each factor node Vφ
is associated with precisely one factor φ, whose scope is the set of variables that are neighbors of Vφ
in the graph. A distribution P factorizes over F if it can be represented as a set of factors of this
factorization
form.
Factor graphs make explicit the structure of the factors in the network. For example, in a
fully connected pairwise Markov network, the factor graph would contain a factor node for each
of the
 n
2

pairs of nodes; the factor node for a pair Xi, Xj would be connected to Xi and
Xj; by contrast, a factor graph for a distribution with a single factor over X1, . . . , Xn would
have a single factor node connected to all of X1, . . . , Xn (see ﬁgure 4.8). Thus, although the
Markov networks for these two distributions are identical, their factor graphs make explicit the

124
Chapter 4. Undirected Graphical Models
ϵ1(A, B)
ϵ2(B, C)
ϵ3(C, D)
ϵ4(D, A)
a0
b0
−3.4
a0
b1
−1.61
a1
b0
0
a1
b1
−2.3
b0
c0
−4.61
b0
c1
0
b1
c0
0
b1
c1
−4.61
c0
d0
0
c0
d1
−4.61
c1
d0
−4.61
c1
d1
0
d0
a0
−4.61
d0
a1
0
d1
a0
0
d1
a1
−4.61
(a)
(b)
(c)
(d)
Figure 4.9
Energy functions for the Misconception example
diﬀerence in their factorization.
4.4.1.2
Log-Linear Models
Although factor graphs make certain types of structure more explicit, they still encode factors as
complete tables over the scope of the factor. As in Bayesian networks, factors can also exhibit
a type of context-speciﬁc structure — patterns that involve particular values of the variables.
These patterns are often more easily seen in terms of an alternative parameterization of the
factors that converts them into log-space.
More precisely, we can rewrite a factor φ(D) as
φ(D) = exp(−ϵ(D)),
where ϵ(D) = −ln φ(D) is often called an energy function. The use of the word “energy”
energy function
derives from statistical physics, where the probability of a physical state (for example, a conﬁgu-
ration of a set of electrons), depends inversely on its energy. In this logarithmic representation,
we have
P(X1, . . . , Xn) ∝exp
"
−
m
X
i=1
ϵi(Di)
#
.
The logarithmic representation ensures that the probability distribution is positive. Moreover,
the logarithmic parameters can take any value along the real line.
Any Markov network parameterized using positive factors can be converted to a logarithmic
representation.
Example 4.9
Figure 4.9 shows the logarithmic representation of the clique potential parameters in ﬁgure 4.1. We
can see that the “1” entries in the clique potentials translate into “0” entries in the energy function.
This representation makes certain types of structure in the potentials more apparent. For
example, we can see that both ϵ2(B, C) and ϵ4(D, A) are constant multiples of an energy
function that ascribes 1 to instantiations where the values of the two variables agree, and 0 to
the instantiations where they do not.
We can provide a general framework for capturing such structure using the following notion:

4.4. Parameterization Revisited
125
Deﬁnition 4.14
Let D be a subset of variables. We deﬁne a feature f(D) to be a function from Val(D) to IR.
feature
A feature is simply a factor without the nonnegativity requirement.
One type of feature of
particular interest is the indicator feature that takes on value 1 for some values y ∈Val(D) and
indicator feature
0 otherwise.
Features provide us with an easy mechanism for specifying certain types of interactions more
compactly.
Example 4.10
Consider a situation where A1 and A2 each have ℓvalues a1, . . . , aℓ. Assume that our distribution
is such that we prefer situations where A1 and A2 take on the same value, but otherwise have no
preference. Thus, our energy function might have the following form:
ϵ(A1, A2) =
 −3
A1 = A2
0
otherwise
Represented as a full factor, this clique potential requires ℓ2 values.
However, it can also be
represented as a log-linear function in terms of a feature f(A1, A2) that is an indicator function
for the event A1 = A2. The energy function is then simply a constant multiple −3 of this feature.
Thus, we can provide a more general deﬁnition for our notion of log-linear models:
Deﬁnition 4.15
A distribution P is a log-linear model over a Markov network H if it is associated with:
log-linear model
• a set of features F = {f1(D1), . . . , fk(Dk)}, where each Di is a complete subgraph in H,
• a set of weights w1, . . . , wk,
such that
P(X1, . . . , Xn) = 1
Z exp
"
−
k
X
i=1
wifi(Di)
#
.
Note that we can have several features over the same scope, so that we can, in fact, represent a
standard set of table potentials. (See exercise 4.13.)
The log-linear model provides a much more compact representation for many distributions,
especially in situations where variables have large domains such as text (such as box 4.E).
4.4.1.3
Discussion
We now have three representations of the parameterization of a Markov network. The Markov
network denotes a product over potentials on cliques. A factor graph denotes a product of factors.
And a set of features denotes a product over feature weights. Clearly, each representation is ﬁner-
grained than the previous one and as rich. A factor graph can describe the Gibbs distribution,
and a set of features can describe all the entries in each of the factors of a factor graph.

Depending on the question of interest, diﬀerent representations may be more appropri-
ate. For example, a Markov network provides the right level of abstraction for discussing
independence queries: The ﬁner-grained representations of factor graphs or log-linear

126
Chapter 4. Undirected Graphical Models
models do not change the independence assertions made by the model. On the other
hand, as we will see in later chapters, factor graphs are useful when we discuss inference,
and features are useful when we discuss parameterizations, both for hand-coded models
and for learning.
Box 4.C — Concept: Ising Models and Boltzmann Machines. One of the earliest types of Markov
network models is the Ising model, which ﬁrst arose in statistical physics as a model for the energy
Ising model
of a physical system involving a system of interacting atoms. In these systems, each atom is associ-
ated with a binary-valued random variable Xi ∈{+1, −1}, whose value deﬁnes the direction of
the atom’s spin. The energy function associated with the edges is deﬁned by a particularly simple
parametric form:
ϵi,j(xi, xj) = wi,jxixj
(4.4)
This energy is symmetric in Xi, Xj; it makes a contribution of wi,j to the energy function when
Xi = Xj (so both atoms have the same spin) and a contribution of −wi,j otherwise. Our model
also contains a set of parameters ui that encode individual node potentials; these bias individual
variables to have one spin or another.
As usual, the energy function deﬁnes the following distribution:
P(ξ) = 1
Z exp

−
X
i<j
wi,jxixj −
X
i
uixi

.
As we can see, when wi,j > 0 the model prefers to align the spins of the two atoms; in this case,
the interaction is called ferromagnetic. When wi,j < 0 the interaction is called antiferromagnetic.
When wi,j = 0 the atoms are non-interacting.
Much work has gone into studying particular types of Ising models, attempting to answer a
variety of questions, usually as the number of atoms goes to inﬁnity. For example, we might ask the
probability of a conﬁguration in which a majority of the spins are +1 or −1, versus the probability
of more mixed conﬁgurations. The answer to this question depends heavily on the strength of the
interaction between the variables; so, we can consider adapting this strength (by multiplying all
weights by a temperature parameter) and asking whether this change causes a phase transition in
temperature
parameter
the probability of skewed versus mixed conﬁgurations. These questions, and many others, have been
investigated extensively by physicists, and the answers are known (in some cases even analytically)
for several cases.
Related to the Ising model is the Boltzmann distribution; here, the variables are usually taken
Boltzmann
distribution
to have values {0, 1}, but still with the energy form of equation (4.4). Here, we get a nonzero
contribution to the model from an edge (Xi, Xj) only when Xi = Xj = 1; however, the resulting
energy can still be reformulated in terms of an Ising model (exercise 4.12).
The popularity of the Boltzmann machine was primarily driven by its similarity to an activation
model for neurons. To understand the relationship, we note that the probability distribution over
each variable Xi given an assignment to is neighbors is sigmoid(z) where
z = −(
X
j
wi,jxj) −wi.

4.4. Parameterization Revisited
127
This function is a sigmoid of a weighted combination of Xi’s neighbors, weighted by the strength and
direction of the connections between them. This is the simplest but also most popular mathematical
approximation of the function employed by a neuron in the brain. Thus, if we imagine a process
by which the network continuously adapts its assignment by resampling the value of each variable
as a stochastic function of its neighbors, then the “activation” probability of each variable resembles
a neuron’s activity. This model is a very simple variant of a stochastic, recurrent neural network.
Box 4.D — Concept: Metric MRFs. One important class of MRFs comprises those used for la-
labeling MRF
beling. Here, we have a graph of nodes X1, . . . , Xn related by a set of edges E, and we wish
to assign to each Xi a label in the space V = {v1, . . . , vK}. Each node, taken in isolation, has
its preferences among the possible labels. However, we also want to impose a soft “smoothness”
constraint over the graph, in that neighboring nodes should take “similar” values.
We encode the individual node preferences as node potentials in a pairwise MRF and the smooth-
ness preferences as edge potentials. For reasons that will become clear, it is traditional to encode
these models in negative log-space, using energy functions. As our objective in these models is
inevitably the MAP objective, we can also ignore the partition function, and simply consider the
energy function:
E(x1, . . . , xn) =
X
i
ϵi(xi) +
X
(i,j)∈E
ϵi,j(xi, xj).
(4.5)
Our goal is then to minimize the energy:
arg
min
x1,...,xn E(x1, . . . , xn).
We now need to provide a formal deﬁnition for the intuition of “smoothness” described earlier.
There are many diﬀerent types of conditions that we can impose; diﬀerent conditions allow diﬀerent
methods to be applied.
One of the simplest in this class of models is a slight variant of the Ising model, where we have
Ising model
that, for any i, j:
ϵi,j(xi, xj) =
 0
xi = xj
λi,j
xi ̸= xj,
(4.6)
for λi,j ≥0. In this model, we obtain the lowest possible pairwise energy (0) when two neighboring
nodes Xi, Xj take the same value, and a higher energy λi,j when they do not.
This simple model has been generalized in many ways. The Potts model extends it to the setting
Potts model
of more than two labels. An even broader class contains models where we have a distance function
on the labels, and where we prefer neighboring nodes to have labels that are a smaller distance
apart. More precisely, a function µ : V × V 7→[0, ∞) is a metric if it satisﬁes:
metric function
• Reﬂexivity: µ(vk, vl) = 0 if and only if k = l;
• Symmetry: µ(vk, vl) = µ(vl, vk);

128
Chapter 4. Undirected Graphical Models
ϵ′
1(A, B)
ϵ′
2(B, C)
a0
b0
−4.4
a0
b1
−1.61
a1
b0
−1
a1
b1
−2.3
b0
c0
−3.61
b0
c1
+1
b1
c0
0
b1
c1
−4.61
(a)
(b)
Figure 4.10
Alternative but equivalent energy functions
• Triangle Inequality: µ(vk, vl) + µ(vl, vm) ≥µ(vk, vm).
We say that µ is a semimetric if it satisﬁes reﬂexivity and symmetry. We can now deﬁne a metric
semimetric
MRF (or a semimetric MRF) by deﬁning ϵi,j(vk, vl) = µ(vk, vl) for all i, j, where µ is a metric
(semimetric). We note that, as deﬁned, this model assumes that the distance metric used is the
same for all pairs of variables. This assumption is made because it simpliﬁes notation, it often
holds in practice, and it reduces the number of parameters that must be acquired. It is not required
for the inference algorithms that we present in later chapters. Metric interactions arise in many
applications, and play a particularly important role in computer vision (see box 4.B and box 13.B).
For example, one common metric used is some form of truncated p-norm (usually p = 1 or p = 2):
truncated norm
ϵ(xi, xj) = min(c∥xi −xj∥p, distmax).
(4.7)
4.4.2
Overparameterization
Even if we use ﬁner-grained factors, and in some cases, even features, the Markov network
parameterization is generally overparameterized. That is, for any given distribution, there are
multiple choices of parameters to describe it in the model. Most obviously, if our graph is a
single clique over n binary variables X1, . . . , Xn, then the network is associated with a clique
potential that has 2n parameters, whereas the joint distribution only has 2n −1 independent
parameters.
A more subtle point arises in the context of a nontrivial clique structure. Consider a pair
of cliques {A, B} and {B, C}.
The energy function ϵ1(A, B) (or its corresponding clique
potential) contains information not only about the interaction between A and B, but also about
the distribution of the individual variables A and B. Similarly, ϵ2(B, C) gives us information
about the individual variables B and C. The information about B can be placed in either of
the two cliques, or its contribution can be split between them in arbitrary ways, resulting in
many diﬀerent ways of specifying the same distribution.
Example 4.11
Consider the energy functions ϵ1(A, B) and ϵ2(B, C) in ﬁgure 4.9. The pair of energy functions
shown in ﬁgure 4.10 result in an equivalent distribution: Here, we have simply subtracted 1 from
ϵ1(A, B) and added 1 to ϵ2(B, C) for all instantiations where B = b0. It is straightforward to

4.4. Parameterization Revisited
129
check that this results in an identical distribution to that of ﬁgure 4.9. In instances where B ̸= b0
the energy function returns exactly the same value as before. In cases where B = b0, the actual
values of the energy functions have changed. However, because the sum of the energy functions on
each instance is identical to the original sum, the probability of the instance will not change.
Intuitively, the standard Markov network representation gives us too many places to account
for the inﬂuence of variables in shared cliques. Thus, the same distribution can be represented
as a Markov network (of a given structure) in inﬁnitely many ways. It is often useful to pick one
of this inﬁnite set as our chosen parameterization for the distribution.
4.4.2.1
Canonical Parameterization
The canonical parameterization provides one very natural approach to avoiding this ambiguity
canonical
parameterization
in the parameterization of a Gibbs distribution P. This canonical parameterization requires that
the distribution P be positive. It is most convenient to describe this parameterization using
energy functions rather then clique potentials. For this reason, it is also useful to consider a log-
transform of P: For any assignment ξ to X, we use ℓ(ξ) to denote ln P(ξ). This transformation
is well deﬁned because of our positivity assumption.
The canonical parameterization of a Gibbs distribution over H is deﬁned via a set of energy
functions over all cliques. Thus, for example, the Markov network in ﬁgure 4.4b would have
energy functions for the two cliques {A, B, D} and {B, C, D}, energy functions for all possible
pairs of variables except the pair {A, C} (a total of ﬁve pairs), energy functions for all four
singleton sets, and a constant energy function for the empty clique.
At ﬁrst glance, it appears that we have only increased the number of parameters in the
speciﬁcation. However, as we will see, this approach uniquely associates the interaction param-
eters for a subset of variables with that subset, avoiding the ambiguity described earlier. As a
consequence, many of the parameters in this canonical parameterization are often zero.
The canonical parameterization is deﬁned relative to a particular ﬁxed assignment ξ∗=
(x∗
1, . . . , x∗
n) to the network variables X. This assignment can be chosen arbitrarily. For any
subset of variables Z, and any assignment x to some subset of X that contains Z, we deﬁne
the assignment xZ to be x⟨Z⟩, that is, the assignment in x to the variables in Z. Conversely,
we deﬁne ξ∗
−Z to be ξ∗⟨X −Z⟩, that is, the assignment in ξ∗to the variables outside Z. We
can now construct an assignment (xZ, ξ∗
−Z) that keeps the assignments to the variables in Z
as speciﬁed in x, and augments it using the default values in ξ∗.
The canonical energy function for a clique D is now deﬁned as follows:
canonical energy
function
ϵ∗
D(d) =
X
Z⊆D
(−1)|D−Z|ℓ(dZ, ξ∗
−Z),
(4.8)
where the sum is over all subsets of D, including D itself and the empty set ∅. Note that all
of the terms in the summation have a scope that is contained in D, which in turn is part of a
clique, so that these energy functions are legal relative to our Markov network structure.
This formula performs an inclusion-exclusion computation.
For a set {A, B, C}, it ﬁrst
subtracts out the inﬂuence of all of the pairs: {A, B}, {B, C}, and {C, A}. However, this
process oversubtracts the inﬂuence of the individual variables. Thus, their inﬂuence is added
back in, to compensate. More generally, consider any subset of variables Z ⊆D. Intuitively, it

130
Chapter 4. Undirected Graphical Models
￿∗
1(A, B)
￿∗
2(B, C)
￿∗
3(C, D)
￿∗
4(D, A)
a0
b0
0
a0
b1
0
a1
b0
0
a1
b1
4.09
b0
c0
0
b0
c1
0
b1
c0
0
b1
c1
9.21
c0
d0
0
c0
d1
0
c1
d0
0
c1
d1
−9.21
d0
a0
0
d0
a1
0
d1
a0
0
d1
a1
9.21
￿∗
5(A)
￿∗
6(B)
￿∗
7(C)
￿∗
8(D)
￿∗
9(∅)
a0
0
a1
−8.01
b0
0
b1
−6.4
c0
0
c1
0
d0
0
d1
0
−3.18
Figure 4.11
Canonical energy function for the Misconception example
makes a “contribution” once for every subset U ⊇Z. Except for U = D, the number of times
that Z appears is even — there is an even number of subsets U ⊇Z — and the number of
times it appears with a positive sign is equal to the number of times it appears with a negative
sign. Thus, we have e￿ectively eliminated the net contribution of the subsets from the canonical
energy function.
Let us consider the e￿ect of the canonical transformation on our Misconception network.
Example 4.12
Let us choose (a0, b0, c0, d0) as our arbitrary assignment on which to base the canonical param-
eterization. The resulting energy functions are shown in ﬁgure 4.11. For example, the energy value
￿∗
1(a1, b1) was computed as follows:
￿(a1, b1, c0, d0) −￿(a1, b0, c0, d0) −￿(a0, b1, c0, d0) + ￿(a0, b0, c0, d0) =
−13.49 −−11.18 −−9.58 + −3.18 = 4.09
Note that many of the entries in the energy functions are zero. As discussed earlier, this phenomenon
is fairly general, and occurs because we have accounted for the inﬂuence of small subsets of variables
separately, leaving the larger factors to deal only with higher-order inﬂuences. We also note that these
canonical parameters are not very intuitive, highlighting yet again the di￿culties of constructing a
reasonable parameterization of a Markov network by hand.
This canonical parameterization deﬁnes the same distribution as our original distribution P:
Theorem 4.7
Let P be a positive Gibbs distribution over H, and let ￿∗(Di) for each clique Di be deﬁned as
speciﬁed in equation (4.8). Then
P(ξ) = exp
￿￿
i
￿∗
Di(ξ￿Di￿)
￿
.
The proof for the case where H consists of a single clique is fairly simple, and it is left as an
exercise (exercise 4.4). The general case follows from results in the next section.
The canonical parameterization gives us the tools to prove the Hammersley-Cli￿ord theorem,
which we restate for convenience.

4.4. Parameterization Revisited
131
Theorem 4.8
Let P be a positive distribution over X, and H a Markov network graph over X. If H is an I-map
for P, then P is a Gibbs distribution over H.
Proof To prove this result, we need to show the existence of a Gibbs parameterization for
any distribution P that satisﬁes the Markov assumptions associated with H.
The proof is
constructive, and simply uses the canonical parameterization shown earlier in this section.
Given P, we deﬁne an energy function for all subsets D of nodes in the graph, regardless of
whether they are cliques in the graph. This energy function is deﬁned exactly as in equation (4.8),
relative to some speciﬁc ﬁxed assignment ξ∗used to deﬁne the canonical parameterization. The
distribution deﬁned using this set of energy functions is P: the argument is identical to the
proof of theorem 4.7, for the case where the graph consists of a single clique (see exercise 4.4).
It remains only to show that the resulting distribution is a Gibbs distribution over H. To show
that, we need to show that the factors ϵ∗(D) are identically 0 whenever D is not a clique in
the graph, that is, whenever the nodes in D do not form a fully connected subgraph. Assume
that we have X, Y ∈D such that there is no edge between X and Y . For this proof, it helps
to introduce the notation
σZ[x] = (xZ, ξ∗
−Z).
Plugging this notation into equation (4.8), we have that:
ϵ∗
D(d) =
X
Z⊆D
(−1)|D−Z|ℓ(σZ[d]).
We now rearrange the sum over subsets Z into a sum over groups of subsets.
Let W ⊆
D −{X, Y }; then W , W ∪{X}, W ∪{Y }, and W ∪{X, Y } are all subsets of Z. Hence,
we can rewrite the summation over subsets of D as a summation over subsets of D −{X, Y }:
ϵ∗
D(d)
=
X
W ⊆D−{X,Y }
(−1)|D−{X,Y }−W |
(4.9)
(ℓ(σW [d]) −ℓ(σW ∪{X}[d]) −ℓ(σW ∪{Y }[d]) + ℓ(σW ∪{X,Y }[d])).
Now consider a speciﬁc subset W in this sum, and let u∗be ξ∗⟨X −D⟩— the assignment
to X −D in ξ. We now have that:
ℓ(σW ∪{X,Y }[d])) −ℓ(σW ∪{X}[d])
=
ln P(x, y, w, u∗)
P(x, y∗, w, u∗)
=
ln P(y | x, w, u∗)P(x, w, u∗)
P(y∗| x, w, u∗)P(x, w, u∗)
=
ln P(y | x∗, w, u∗)P(x, w, u∗)
P(y∗| x∗, w, u∗)P(x, w, u∗)
=
ln P(y | x∗, w, u∗)P(x∗, w, u∗)
P(y∗| x∗, w, u∗)P(x∗, w, u∗)
=
ln P(x∗, y, w, u∗)
P(x∗, y∗, w, u∗)
=
ℓ(σW ∪{Y }[d])) −ℓ(σW [d]),

132
Chapter 4. Undirected Graphical Models
where the third equality is a consequence of the fact that X and Y are not connected directly
by an edge, and hence we have that P |= (X ⊥Y | X −{X, Y }). Thus, we have that each
term in the outside summation in equation (4.9) adds to zero, and hence the summation as a
whole is also zero, as required.
For positive distributions, we have already shown that all three sets of Markov assumptions are
equivalent; putting these results together with theorem 4.1 and theorem 4.2, we obtain that, for

positive distributions, all four conditions — factorization and the three types of Markov
assumptions — are all equivalent.
4.4.2.2
Eliminating Redundancy
An alternative approach to the issue of overparameterization is to try to eliminate it entirely. We
can do so in the context of a feature-based representation, which is suﬃciently ﬁne-grained to
allow us to eliminate redundancies without losing expressive power. The tools for detecting and
eliminating redundancies come from linear algebra.
We say that a set of features f1, . . . , fk is linearly dependent if there are constants α0, α1, . . . , αk,
linear
dependence
not all of which are 0, so that for all ξ
α0 +
X
i
αifi(ξ) = 0.
This is the usual deﬁnition of linear dependencies in linear algebra, where we view each feature
as a vector whose entries are the value of the feature in each of the possible instantiations.
Example 4.13
Consider again the Misconception example. We can encode the log-factors in example 4.9 as a set
of features by introducing indicator features of the form:
fa,b(A, B) =
 1
A = a, B = b
0
otherwise.
Thus, to represent ϵ1(A, B), we introduce four features that correspond to the four entries in the
energy function. Since A, B take on exactly one of these possible four values, we have that
fa0,b0(A, B) + fa0,b1(A, B) + fa1,b0(A, B) + fa1,b1(A, B) = 1.
Thus, this set of features is linearly dependent.
Example 4.14
Now consider also the features that capture ϵ2(B, C) and their interplay with the features that
capture ϵ1(A, B). We start by noting that the sum fa0,b0(A, B)+fa1,b0(A, B) is equal to 1 when
B = b0 and 0 otherwise. Similarly, fb0,c0(B, C) + fb0,c1(B, C) is also an indicator for B = b0.
Thus we get that
fa0,b0(A, B) + fa1,b0(A, B) −fb0,c0(B, C) −fb0,c1(B, C) = 0.
And so these four features are linearly dependent.
As we now show, linear dependencies imply non-unique parameterization.

4.4. Parameterization Revisited
133
Proposition 4.5
Let f1, . . . , fk be a set of features with weights w = {w1, . . . , wk} that form a log-linear repre-
sentation of a distribution P. If there are coeﬃcients α0, α1, . . . , αk such that for all ξ
α0 +
X
i
αifi(ξ) = 0
(4.10)
then the log-linear model with weights w′ = {w1 + α1, . . . , wk + αk} also represents P.
Proof Consider the distribution
Pw′(ξ) ∝exp
(
−
X
i
(wi + αi)fi(ξ)
)
.
Using equation (4.10) we see that
−
X
i
(wi + αi)fi(ξ) = α0 −
X
i
wifi(ξ).
Thus,
Pw′(ξ) ∝eα0 exp
(
−
X
i
wifi(ξ)
)
∝P(ξ).
We conclude that Pw′(ξ) = P(ξ).
Motivated by this result, we say that a set of linearly dependent features is redundant. A
redundant
nonredundant set of features is one where the features are not linearly dependent on each
other. In fact, if the set of features is nonredundant, then each set of weights describes a unique
distribution.
Proposition 4.6
Let f1, . . . , fk be a set of nonredundant features, and let w, w′ ∈IRk. If w ̸= w′ then Pw ̸= Pw′.
Example 4.15
Can we construct a nonredundant set of features for the Misconception example? We can determine
the number of nonredundant features by building the 16×16 matrix of the values of the 16 features
(four factors with four features each) in the 16 instances of the joint distribution. This matrix has
rank of 9, which implies that a subset of 8 features will be a nonredundant subset. In fact, there
are several such subsets. In particular, the canonical parameterization shown in ﬁgure 4.11 has nine
features of nonzero weight, which form a nonredundant parameterization. The equivalence of the
canonical parameterization (theorem 4.7) implies that this set of features has the same expressive
power as the original set of features. To verify this, we can show that adding any other feature will
lead to a linear dependency. Consider, for example, the feature fa1,b0. We can verify that
fa1,b0 + fa1,b1 −fa1 = 0.
Similarly, consider the feature fa0,b0. Again we can ﬁnd a linear dependency on other features:
fa0,b0 + fa1 + fb1 −fa1,b1 = 1.
Using similar arguments, we can show that adding any of the original features will lead to redun-
dancy. Thus, this set of features can represent any parameterization in the original model.

134
Chapter 4. Undirected Graphical Models
4.5
Bayesian Networks and Markov Networks
We have now described two graphical representation languages: Bayesian networks and Markov
networks. Example 3.8 and example 4.8 show that these two representations are incomparable
as a language for representing independencies: each can represent independence constraints
that the other cannot. In this section, we strive to provide more insight about the relationship
between these two representations.
4.5.1
From Bayesian Networks to Markov Networks
Let us begin by examining how we might take a distribution represented using one of these
frameworks, and represent it in the other.
One can view this endeavor from two diﬀerent
perspectives: Given a Bayesian network B, we can ask how to represent the distribution PB
as a parameterized Markov network; or, given a graph G, we can ask how to represent the
independencies in G using an undirected graph H. In other words, we might be interested in
ﬁnding a minimal I-map for a distribution PB, or a minimal I-map for the independencies I(G).
We can see that these two questions are related, but each perspective oﬀers its own insights.
Let us begin by considering a distribution PB, where B is a parameterized Bayesian network
over a graph G. Importantly, the parameterization of B can also be viewed as a parameterization
for a Gibbs distribution: We simply take each CPD P(Xi | PaXi) and view it as a factor of
scope Xi, PaXi. This factor satisﬁes additional normalization properties that are not generally
true of all factors, but it is still a legal factor. This set of factors deﬁnes a Gibbs distribution,
one whose partition function happens to be 1.
What is more important, a Bayesian network conditioned on evidence E = e also induces

a Gibbs distribution: the one deﬁned by the original factors reduced to the context E = e.
Proposition 4.7
Let B be a Bayesian network over X and E = e an observation. Let W = X −E. Then
PB(W | e) is a Gibbs distribution deﬁned by the factors Φ = {φXi}Xi∈X , where
φXi = PB(Xi | PaXi)[E = e].
The partition function for this Gibbs distribution is P(e).
The proof follows directly from the deﬁnitions.
This result allows us to view any Bayesian
network conditioned as evidence as a Gibbs distribution, and to bring to bear techniques
developed for analysis of Markov networks.
What is the structure of the undirected graph that can serve as an I-map for a set of factors
in a Bayesian network? In other words, what is the I-map for the Bayesian network structure
G? Going back to our construction, we see that we have created a factor for each family of Xi,
containing all the variables in the family. Thus, in the undirected I-map, we need to have an
edge between Xi and each of its parents, as well as between all of the parents of Xi. This
observation motivates the following deﬁnition:
Deﬁnition 4.16
The moral graph M[G] of a Bayesian network structure G over X is the undirected graph over X
moralized graph
that contains an undirected edge between X and Y if: (a) there is a directed edge between them
(in either direction), or (b) X and Y are both parents of the same node.1
1. The name moralized graph originated because of the supposed “morality” of marrying the parents of a node.

4.5. Bayesian Networks and Markov Networks
135
For example, ﬁgure 4.6a shows the moralized graph for the extended Bstudent network of ﬁg-
ure 9.8.
The preceding discussion shows the following result:
Corollary 4.2
Let G be a Bayesian network structure. Then for any distribution PB such that B is a parameteri-
zation of G, we have that M[G] is an I-map for PB.
One can also view the moralized graph construction purely from the perspective of the
independencies encoded by a graph, avoiding completely the discussion of parameterizations of
the network.
Proposition 4.8
Let G be any Bayesian network graph. The moralized graph M[G] is a minimal I-map for G.
Proof We want to build a Markov network H such that I(H) ⊆I(G), that is, that H is an
I-map for G (see deﬁnition 3.3). We use the algorithm for constructing minimal I-maps based
on the Markov independencies. Consider a node X in X: our task is to select as X’s neighbors
the smallest set of nodes U that are needed to render X independent of all other nodes in the
network. We deﬁne the Markov blanket of X in a Bayesian network G, denoted MBG(X), to be
Markov blanket
the nodes consisting of X’s parents, X’s children, and other parents of X’s children. We now
need to show that MBG(X) d-separates X from all other variables in G; and that no subset of
MBG(X) has that property. The proof uses straightforward graph-theoretic properties of trails,
and it is left as an exercise (exercise 4.14).
Now, let us consider how “close” the moralized graph is to the original graph G. Intuitively,
the addition of the moralizing edges to the Markov network H leads to the loss of inde-

pendence information implied by the graph structure. For example, if our Bayesian network
G has the form X →Z ←Y , with no edge between X and Y , the Markov network M[G] loses
the information that X and Y are marginally independent (not given Z). However, information
is not always lost. Intuitively, moralization causes loss of information about independencies only
when it introduces new edges into the graph. We say that a Bayesian network G is moral if it
moral graph
contains no immoralities (as in deﬁnition 3.11); that is, for any pair of variables X, Y that share
a child, there is a covering edge between X and Y . It is not diﬃcult to show that:
Proposition 4.9
If the directed graph G is moral, then its moralized graph M[G] is a perfect map of G.
Proof Let H = M[G]. We have already shown that I(H) ⊆I(G), so it remains to show the
opposite inclusion. Assume by contradiction that there is an independence (X ⊥Y | Z) ∈
I(G) which is not in I(H). Thus, there must exist some trail from X to Y in H which is
active given Z. Consider some such trail that is minimal, in the sense that it has no shortcuts.
As H and G have precisely the same edges, the same trail must exist in G. As, by assumption, it
cannot be active in G given Z, we conclude that it must contain a v-structure X1 →X2 ←X3.
However, because G is moralized, we also have some edge between X1 and X3, contradicting
the assumption that the trail is minimal.
Thus, a moral graph G can be converted to a Markov network without losing independence
assumptions. This conclusion is fairly intuitive, inasmuch as the only independencies in G that
are not present in an undirected graph containing the same edges are those corresponding to

136
Chapter 4. Undirected Graphical Models
v-structures. But if any v-structure can be short-cut, it induces no independencies that are not
represented in the undirected graph.
We note, however, that very few directed graphs are moral. For example, assume that we have
a v-structure X →Y ←Z, which is moral due to the existence of an arc X →Z. If Z has
another parent W, it also has a v-structure X →Z ←W, which, to be moral, requires some
edge between X and W. We return to this issue in section 4.5.3.
4.5.1.1
Soundness of d-Separation
The connection between Bayesian networks and Markov networks provides us with the tools for
proving the soundness of the d-separation criterion in Bayesian networks.
The idea behind the proof is to leverage the soundness of separation in undirected graphs, a
result which (as we showed) is much easier to prove. Thus, we want to construct an undirected
graph H such that active paths in H correspond to active paths in G. A moment of thought
shows that the moralized graph is not the right construct, because there are paths in the
undirected graph that correspond to v-structures in G that may or may not be active.
For
example, if our graph G is X →Z ←Y and Z is not observed, d-separation tells us that X
and Y are independent; but the moralized graph for G is the complete undirected graph, which
does not have the same independence.
Therefore, to show the result, we ﬁrst want to eliminate v-structures that are not active, so as to
remove such cases. To do so, we ﬁrst construct a subgraph where remove all barren nodes from
barren node
the graph, thereby also removing all v-structures that do not have an observed descendant. The
elimination of the barren nodes does not change the independence properties of the distribution
over the remaining variables, but does eliminate paths in the graph involving v-structures that
are not active. If we now consider only the subgraph, we can reduce d-separation to separation
and utilize the soundness of separation to show the desired result.
We ﬁrst use these intuitions to provide an alternative formulation for d-separation. Recall
that in deﬁnition 2.14 we deﬁned the upward closure of a set of nodes U in a graph to be
upward closure
U ∪AncestorsU. Letting U ∗be the closure of a set U, we can deﬁne the network induced over
U ∗; importantly, as all parents of every node in U ∗are also in U ∗, we have all the variables
mentioned in every CPD, so that the induced graph deﬁnes a coherent probability distribution.
We let G+[U] be the induced Bayesian network over U and its ancestors.
Proposition 4.10
Let X, Y , Z be three disjoint sets of nodes in a Bayesian network G. Let U = X ∪Y ∪Z, and
let G′ = G+[U] be the induced Bayesian network over U ∪AncestorsU. Let H be the moralized
graph M[G′]. Then d-sepG(X; Y | Z) if and only if sepH(X; Y | Z).
Example 4.16
To gain some intuition for this result, consider the Bayesian network G of ﬁgure 4.12a (which
extends our Student network). Consider the d-separation query d-sepG(D; I | L). In this case,
U = {D, I, L}, and hence the moralized graph M[G+[U]] is the graph shown in ﬁgure 4.12b,
where we have introduced an undirected moralizing edge between D and I. In the resulting graph,
D and I are not separated given L, exactly as we would have concluded using the d-separation
procedure on the original graph.
On the other hand, consider the d-separation query d-sepG(D; I | S, A). In this case, U =
{D, I, S, A}. Because D and I are not spouses in G+[U], the moralization process does not add

4.5. Bayesian Networks and Markov Networks
137
(b)
I
D
G
L
(a)
L
G
S
I
J
D
A
(c)
S
I
D
A
Figure 4.12
Example of alternative deﬁnition of d-separation based on Markov networks.
(a) A Bayesian network G.
(b) The Markov network M[G+[D, I, L]].
(c) The Markov network
M[G+[D, I, A, S]].
an edge between them. The resulting moralized graph is shown in ﬁgure 4.12c. As we can see, we
have that sepM[G+[U]](D; I | S, A), as desired.
The proof for the general case is similar and is left as an exercise (exercise 4.15).
With this result, the soundness of d-separation follows easily. We repeat the statement of
theorem 3.3:
Theorem 4.9
If a distribution PB factorizes according to G, then G is an I-map for P.
Proof As in proposition 4.10, let U = X ∪Y ∪Z, let U ∗= U ∪AncestorsU, let GU ∗= G+[U]
be the induced graph over U ∗, and let H be the moralized graph M[GU ∗]. Let PU ∗be the
Bayesian network distribution deﬁned over GU ∗in the obvious way: the CPD for any variable
in U ∗is the same as in B. Because U ∗is upwardly closed, all variables used in these CPDs are
in U ∗.
Now, consider an independence assertion (X ⊥Y | Z) ∈I(G); we want to prove that
PB |= (X ⊥Y | Z). By deﬁnition 3.7, if (X ⊥Y | Z) ∈I(G), we have that d-sepG(X; Y |
Z). It follows that sepH(X; Y | Z), and hence that (X ⊥Y | Z) ∈I(H). PU ∗is a Gibbs
distribution over H, and hence, from theorem 4.1, PU ∗|= (X ⊥Y | Z). Using exercise 3.8, the
distribution PU ∗(U ∗) is the same as PB(U ∗). Hence, it follows also that PB |= (X ⊥Y | Z),
proving the desired result.
4.5.2
From Markov Networks to Bayesian Networks
The previous section dealt with the conversion from a Bayesian network to a Markov network.
We now consider the converse transformation: ﬁnding a Bayesian network that is a minimal
I-map for a Markov network. It turns out that the transformation in this direction is signiﬁcantly
more diﬃcult, both conceptually and computationally. Indeed, the Bayesian network that is a
minimal I-map for a Markov network might be considerably larger than the Markov network.

138
Chapter 4. Undirected Graphical Models
(b)
(a)
D
B
A
F
E
C
D
B
A
F
E
C
Figure 4.13
Minimal I-map Bayesian networks for a nonchordal Markov network. (a) A Markov
network Hℓwith a loop. (b) A minimal I-map GℓBayesian network for H.
Example 4.17
Consider the Markov network structure Hℓof ﬁgure 4.13a, and assume that we want to ﬁnd a
Bayesian network I-map for Hℓ. As we discussed in section 3.4.1, we can ﬁnd such an I-map
by enumerating the nodes in X in some ordering, and deﬁne the parent set for each one in turn
according to the independencies in the distribution. Assume we enumerate the nodes in the order
A, B, C, D, E, F. The process for A and B is obvious. Consider what happens when we add C.
We must, of course, introduce A as a parent for C. More interestingly, however, C is not independent
of B given A; hence, we must also add B as a parent for C. Now, consider the node D. One of its
parents must be B. As D is not independent of C given B, we must add C as a parent for B. We
do not need to add A, as D is independent of A given B and C. Similarly, E’s parents must be C
and D. Overall, the minimal Bayesian network I-map according to this ordering has the structure
Gℓshown in ﬁgure 4.13b.
A quick examination of the structure Gℓshows that we have added several edges to the graph,
resulting in a set of triangles crisscrossing the loop. In fact, the graph Gℓin ﬁgure 4.13b is
chordal: all loops have been partitioned into triangles.
One might hope that a diﬀerent ordering might lead to fewer edges being introduced. Un-
fortunately, this phenomenon is a general one: any Bayesian network I-map for this Markov
network must add triangulating edges into the graph, so that the resulting graph is chordal (see
deﬁnition 2.24). In fact, we can show the following property, which is even stronger:
Theorem 4.10
Let H be a Markov network structure, and let G be any Bayesian network minimal I-map for H.
Then G can have no immoralities (see deﬁnition 3.11).
Proof Let X1, . . . , Xn be a topological ordering for G. Assume, by contradiction, that there is
some immorality Xi →Xj ←Xk in G such that there is no edge between Xi and Xk; assume
(without loss of generality) that i < k < j.
Owing to minimality of the I-map G, if Xi is a parent of Xj, then Xi and Xj are not
separated by Xj’s other parents. Thus, H necessarily contains one or more paths between Xi

4.5. Bayesian Networks and Markov Networks
139
and Xj that are not cut by Xk (or by Xj’s other parents). Similarly, H necessarily contains one
or more paths between Xk and Xj that are not cut by Xi (or by Xj’s other parents).
Consider the parent set U that was chosen for Xk. By our previous argument, there are one
or more paths in H between Xi and Xk via Xj. As i < k, and Xi is not a parent of Xk (by
our assumption), we have that U must cut all of those paths. To do so, U must cut either all of
the paths between Xi and Xj, or all of the paths between Xj and Xk: As long as there is at
least one active path from Xi to Xj and one from Xj to Xk, there is an active path between
Xi and Xk that is not cut by U. Assume, without loss of generality, that U cuts all paths
between Xj and Xk (the other case is symmetrical). Now, consider the choice of parent set for
Xj, and recall that it is the (unique) minimal subset among X1, . . . , Xj−1 that separates Xj
from the others. In a Markov network, this set consists of all nodes in X1, . . . , Xj−1 that are
the ﬁrst on some uncut path from Xj. As U separates Xk from Xj, it follows that Xk cannot
be the ﬁrst on any uncut path from Xj, and therefore Xk cannot be a parent of Xj. This result
provides the desired contradiction.
Because any nontriangulated loop of length at least 4 in a Bayesian network graph necessarily
contains an immorality, we conclude:
Corollary 4.3
Let H be a Markov network structure, and let G be any minimal I-map for H. Then G is necessarily
chordal.
Thus, the process of turning a Markov network into a Bayesian network requires that we
add enough edges to a graph to make it chordal. This process is called triangulation. As in
triangulation
the transformation from Bayesian networks to Markov networks, the addition of edges leads
to the loss of independence information. For instance, in example 4.17, the Bayesian network
Gℓin ﬁgure 4.13b loses the information that C and D are independent given A and F. In
the transformation from directed to undirected models, however, the edges added are only the
ones that are, in some sense, implicitly there — the edges required by the fact that each factor
in a Bayesian network involves an entire family (a node and its parents).
By contrast, the
transformation from Markov networks to Bayesian networks can lead to the introduction of a
large number of edges, and, in many cases, to the creation of very large families (exercise 4.16).
4.5.3
Chordal Graphs
We have seen that the conversion in either direction between Bayesian networks to Markov
networks can lead to the addition of edges to the graph and to the loss of independence
information implied by the graph structure. It is interesting to ask when a set of independence
assumptions can be represented perfectly by both a Bayesian network and a Markov network. It
turns out that this class is precisely the class of undirected chordal graphs.
The proof of one direction is fairly straightforward, based on our earlier results.

140
Chapter 4. Undirected Graphical Models
Theorem 4.11
Let H be a nonchordal Markov network. Then there is no Bayesian network G which is a perfect
map for H (that is, such that I(H) = I(G)).
Proof The proof follows from the fact that the minimal I-map for G must be chordal. Hence,
any I-map G for I(H) must include edges that are not present in H. Because any additional
edge eliminates independence assumptions, it is not possible for any Bayesian network G to
precisely encode I(H).
To prove the other direction of this equivalence, we ﬁrst prove some important properties of
chordal graphs. As we will see, chordal graphs and the properties we now show play a central
role in the derivation of exact inference algorithms for graphical models. For the remainder
of this discussion, we restrict attention to connected graphs; the extension to the general case
is straightforward. The basic result we show is that we can decompose any connected chordal
graph H into a tree of cliques — a tree whose nodes are the maximal cliques in H — so that the
structure of the tree precisely encodes the independencies in H. (In the case of disconnected
graphs, we obtain a forest of cliques, rather than a tree.)
We begin by introducing some notation. Let H be a connected undirected graph, and let
C1, . . . , Ck be the set of maximal cliques in H. Let T be any tree-structured graph whose
nodes correspond to the maximal cliques C1, . . . , Ck. Let Ci, Cj be two cliques in the tree
that are directly connected by an edge; we deﬁne Si,j = Ci ∩Cj to be a sepset between Ci
sepset
and Cj. Let W <(i,j) (W <(j,i)) be all of the variables that appear in any clique on the Ci
(Cj) side of the edge. Thus, each edge decomposes X into three disjoint sets: W <(i,j) −Si,j,
W <(j,i) −Si,j, and Si,j.
Deﬁnition 4.17
We say that a tree T is a clique tree for H if:
clique tree
• each node corresponds to a clique in H, and each maximal clique in H is a node in T ;
• each sepset Si,j separates W <(i,j) and W <(j,i) in H.
Note that this deﬁnition implies that each separator Si,j renders its two sides conditionally
independent in H.
Example 4.18
Consider the Bayesian network graph Gℓin ﬁgure 4.13b. Since it contains no immoralities, its
moralized graph H′
ℓis simply the same graph, but where all edges have been made undirected.
As Gℓis chordal, so is H′
ℓ. The clique tree for H′
ℓis simply a chain {A, B, C} →{B, C, D} →
{C, D, E} →{D, E, F}, which clearly satisﬁes the separation requirements of the clique tree
deﬁnition.
Theorem 4.12
Every undirected chordal graph H has a clique tree T .
Proof We prove the theorem by induction on the number of nodes in the graph. The base case
of a single node is trivial. Now, consider a chordal graph H of size > 1. If H consists of a
single clique, then the theorem holds trivially. Therefore, consider the case where we have at
least two nodes X1, X2 that are not connected directly by an edge. Assume that X1 and X2

4.5. Bayesian Networks and Markov Networks
141
are connected, otherwise the inductive step holds trivially. Let S be a minimal subset of nodes
that separates X1 and X2.
The removal of the set S breaks up the graph into at least two disconnected components —
one containing X1, another containing X2, and perhaps additional ones. Let W 1, W 2 be some
partition of the variables in X −S into two disjoint components, such that W i encompasses
the connected component containing Xi. (The other connected components can be assigned to
W 1 or W 2 arbitrarily.) We ﬁrst show that S must be a complete subgraph. Let Z1, Z2 be any
two variables in S. Due to the minimality of S, each Zi must lie on a path between X1 and
X2 that does not go through any other node in S. (Otherwise, we could eliminate Zi from S
while still maintaining separation.) We can therefore construct a minimal path from Z1 to Z2
that goes only through nodes in W 1 by constructing a path from Z1 to X1 to Z2 that goes
only through W 1, and by eliminating any shortcuts. We can similarly construct a minimal path
from Z1 to Z2 that goes only through nodes in W 2. The two paths together form a cycle of
length ≥4. Because of chordality, the cycle must have a chord, which, by construction, must
be the edge Z1—Z2.
Now consider the induced graph H1 = H[W 1 ∪S]. As X2 ̸∈H1, this induced graph is
smaller than H. Moreover, H1 is chordal, so we can apply the inductive hypothesis. Let T1 be
the clique tree for H1. Because S is a complete connected subgraph, it is either a maximal
clique or a subset of some maximal clique in H1. Let C1 be some clique in T1 containing
S (there may be more than one such clique). We can similarly deﬁne H2 and C2 for X2. If
neither C1 nor C2 is equal to S, we construct a tree T that contains the union of the cliques
in T1 and T2, and connects C1 and C2 by an edge. Otherwise, without loss of generality, let
C1 = S; we create T by merging T1 minus C1 into T2, making all of C1’s neighbors adjacent
to C2 instead.
It remains to show that the resulting structure is a clique tree for H. First, we note that
there is no clique in H that intersects both W 1 and W 2; hence, any maximal clique in H is
a maximal clique in either H1 or H2 (or both in the possible case of S), so that all maximal
cliques in H appear in T .
Thus, the nodes in T are precisely the maximal cliques in H.
Second, we need to show that any Si,j separates W <(i,j) and W <(j,i). Consider two variables
X ∈W <(i,j) and Y ∈W <(j,i). First, assume that X, Y ∈H1; as all the nodes in H1 are on
the T1 side of the tree, we also have that Si,j ⊂H1. Any path between two nodes in H1 that
goes through W 2 can be shortcut to go only through H1. Thus, if Si,j separates X, Y in H1,
then it also separates them in H. The same argument applies for X, Y ∈H2. Now, consider
X ∈W 1 and Y ∈W 2. If Si,j = S, the result follows from the fact that S separates W 1 and
W 2. Otherwise, assume that Si,j is in T1, on the path from X to C1. In this case, we have
that Si,j separates X from S, and S separates Si,j from Y . The conclusion now follows from
the transitivity of graph separation.
We have therefore constructed a clique tree for H, proving the inductive claim.
Using this result, we can show that the independencies in an undirected graph H can be
captured perfectly in a Bayesian network if and only if H is chordal.
Theorem 4.13
Let H be a chordal Markov network. Then there is a Bayesian network G such that I(H) = I(G).
Proof Let T be the clique tree for H, whose existence is guaranteed by theorem 4.12. We can
select an ordering over the nodes in the Bayesian network as follows. We select an arbitrary

142
Chapter 4. Undirected Graphical Models
clique C1 to be the root of the clique tree, and then order the cliques C1, . . . , Ck using any
topological ordering, that is, where cliques closer to the root are ordered ﬁrst. We now order the
nodes in the network in any ordering consistent with the clique ordering: if Xl ﬁrst appears in
Ci and Xm ﬁrst appears in Cj, for i < j, then Xl must precede Xm in the ordering. We now
construct a Bayesian network using the procedure Build-Minimal-I-Map of algorithm 3.2 applied
to the resulting node ordering X1, . . . , Xn and to I(H).
Let G be the resulting network. We ﬁrst show that, when Xi is added to the graph, then Xi’s
parents are precisely U i = NbXi ∩{X1, . . . , Xi−1}, where NbXi is the set of neighbors of Xi
in H. In other words, we want to show that Xi is independent of {X1, . . . , Xi−1} −U i given
U i. Let Ck be the ﬁrst clique in the clique ordering to which Xi belongs. Then U i ⊂Ck.
Let Cl be the parent of Ck in the rooted clique tree. According to our selected ordering, all of
the variables in Cl are ordered before any variable in Ck −Cl. Thus, Sl,k ⊂{X1, . . . , Xi−1}.
Moreover, from our choice of ordering, none of {X1, . . . , Xi−1} −U i are in any descendants
of Ck in the clique tree. Thus, they are all in W <(l,k). From theorem 4.12, it follows that Sl,k
separates Xi from all of {X1, . . . , Xi−1} −U i, and hence that Xi is independent of all of
{X1, . . . , Xi−1} −U i given U i. It follows that G and H have the same set of edges. Moreover,
we note that all of U i are in Ck, and hence are connected in G. Therefore, G is moralized. As
H is the moralized undirected graph of G, the result now follows from proposition 4.9.
For example, the graph Gℓof ﬁgure 4.13b, and its moralized network H′
ℓencode precisely the
same independencies.
By contrast, as we discussed, there exists no Bayesian network that
encodes precisely the independencies in the nonchordal network Hℓof ﬁgure 4.13a.
Thus, we have shown that chordal graphs are precisely the intersection between Markov
networks and Bayesian networks, in that the independencies in a graph can be represented
exactly in both types of models if and only if the graph is chordal.
4.6
Partially Directed Models
So far, we have presented two distinct types of graphical models, based on directed and undi-
rected graphs. We can unify both representations by allowing models that incorporate both
directed and undirected dependencies. We begin by describing the notion of conditional ran-
dom ﬁeld, a Markov network with a directed dependency on some subset of variables. We then
present a generalization of this framework to the class of chain graphs, an entire network in
which undirected components depend on each other in a directed fashion.
4.6.1
Conditional Random Fields
So far, we have described the Markov network representation as encoding a joint distribution
over X. The same undirected graph representation and parameterization can also be used to
encode a conditional distribution P(Y | X), where Y is a set of target variables and X is
target variable
a (disjoint) set of observed variables. We will also see a directed analogue of this concept in
observed variable
section 5.6. In the case of Markov networks, this representation is generally called a conditional
random ﬁeld (CRF).

4.6. Partially Directed Models
143
(a)
(b)
(c)
X1
Y1
X2
Y2
X3
Y3
X4
Y4
X5
Y5
X1
Y1
X2
Y2
X3
Y3
X4
Y4
X5
Y5
X1
Y1
X2
Y2
X3
Y3
X4
Y4
X5
Y5
Figure 4.14
Diﬀerent linear-chain graphical models: (a) a linear-chain-structured conditional random
ﬁeld, where the feature variables are denoted using grayed-out ovals; (b) a partially directed variant; (c) a
fully directed, non-equivalent model. The Xi’s are assumed to be always observed when the network is
used, and hence they are shown as darker gray.
4.6.1.1
CRF Representation and Semantics
More formally, a CRF is an undirected graph whose nodes correspond to Y ∪X. At a high level,
this graph is parameterized in the same way as an ordinary Markov network, as a set of factors
φ1(D1), . . . , φm(Dm). (As before, these factors can also be encoded more compactly as a
log-linear model; for uniformity of presentation, we view the log-linear model as encoding a set
of factors.) However, rather than encoding the distribution P(Y , X), we view it as representing
the conditional distribution P(Y | X). To have the network structure and parameterization
correspond naturally to a conditional distribution, we want to avoid representing a probabilistic
model over X. We therefore disallow potentials that involve only variables in X.
Deﬁnition 4.18
A conditional random ﬁeld is an undirected graph H whose nodes correspond to X ∪Y ; the
conditional
random ﬁeld
network is annotated with a set of factors φ1(D1), . . . , φm(Dm) such that each Di ̸⊆X. The
network encodes a conditional distribution as follows:
P(Y | X)
=
1
Z(X)
˜P(Y , X)
˜P(Y , X)
=
m
Y
i=1
φi(Di)
(4.11)
Z(X)
=
X
Y
˜P(Y , X).
Two variables in H are connected by an (undirected) edge whenever they appear together in the
scope of some factor.
The only diﬀerence between equation (4.11) and the (unconditional) Gibbs distribution of deﬁni-
tion 4.3 is the diﬀerent normalization used in the partition function Z(X). The deﬁnition of
a CRF induces a diﬀerent value for the partition function for every assignment x to X. This
diﬀerence is denoted graphically by having the feature variables grayed out.
Example 4.19
Consider a CRF over Y = {Y1, . . . , Yk} and X = {X1, . . . , Xk}, with an edge Yi—Yi+1
(i = 1, . . . , k −1) and an edge Yi—Xi (i = 1, . . . , k), as shown in ﬁgure 4.14a. The distribution

144
Chapter 4. Undirected Graphical Models
represented by this network has the form:
P(Y | X)
=
1
Z(X)
˜P(Y , X)
˜P(Y , X)
=
k−1
Y
i=1
φ(Yi, Yi+1)
k
Y
i=1
φ(Yi, Xi)
Z(X)
=
X
Y
˜P(Y , X).
Note that, unlike the deﬁnition of a conditional Bayesian network, the structure of a CRF may
still contain edges between variables in X, which arise when two such variables appear together
in a factor that also contains a target variable. However, these edges do not encode the structure
of any distribution over X, since the network explicitly does not encode any such distribution.

The fact that we avoid encoding the distribution over the variables in X is one of the
main strengths of the CRF representation. This ﬂexibility allows us to incorporate into
the model a rich set of observed variables whose dependencies may be quite complex
or even poorly understood.
It also allows us to include continuous variables whose
distribution may not have a simple parametric form. This ﬂexibility allows us to use
domain knowledge in order to deﬁne a rich set of features characterizing our domain,
without worrying about modeling their joint distribution.
For example, returning to
the vision MRFs of box 4.B, rather than deﬁning a joint distribution over pixel values and
their region assignment, we can deﬁne a conditional distribution over segment assignments
given the pixel values. The use of a conditional distribution here allows us to avoid making a
parametric assumption over the (continuous) pixel values. Even more important, we can use
image-processing routines to deﬁne rich features, such as the presence or direction of an image
gradient at a pixel. Such features can be highly informative in determining the region assignment
of a pixel. However, the deﬁnition of such features usually relies on multiple pixels, and deﬁning
a correct joint distribution or a set of independence assumptions over these features is far from
trivial. The fact that we can condition on these features and avoid this whole issue allows us
the ﬂexibility to include them in the model. See box 4.E for another example.
4.6.1.2
Directed and Undirected Dependencies
A CRF deﬁnes a conditional distribution of Y on X; thus, it can be viewed as a partially
directed graph, where we have an undirected component over Y , which has the variables in X
as parents.
Example 4.20
Consider a CRF over the binary-valued variables X = {X1, . . . , Xk} and Y = {Y }, and a
pairwise potential between Y and each Xi; this model is sometimes known as a naive Markov
naive Markov
model, due to its similarity to the naive Bayes model. Assume that the pairwise potentials deﬁned
via the following log-linear model
φi(Xi, Y ) = exp {wi11{Xi = 1, Y = 1}} .
We also introduce a single-node potential φ0(Y ) = exp {w011{Y = 1}}. Following equation (4.11),

4.6. Partially Directed Models
145
we now have:
˜P(Y = 1 | x1, . . . , xk)
=
exp
(
w0 +
k
X
i=1
wixi
)
˜P(Y = 0 | x1, . . . , xk)
=
exp {0} = 1.
In this case, we can show (exercise 5.16) that
P(Y = 1 | x1, . . . , xk) = sigmoid
 
w0 +
k
X
i=1
wixi
!
,
where
sigmoid(z) =
ez
1 + ez
is the sigmoid function. This conditional distribution P(Y | X) is of great practical interest: It
sigmoid
deﬁnes a CPD that is not structured as a table, but that is induced by a small set of parameters
w0, . . . , wk — parameters whose number is linear, rather than exponential, in the number of
parents. This type of CPD, often called a logistic CPD, is a natural model for many real-world
logistic CPD
applications, inasmuch as it naturally aggregates the inﬂuence of diﬀerent parents. We discuss this
CPD in greater detail in section 5.4.2 as part of our general presentation of structured CPDs.
The partially directed model for the CRF of example 4.19 is shown in ﬁgure 4.14b. We may be
tempted to believe that we can construct an equivalent model that is fully directed, such as the
one in ﬁgure 4.14c. In particular, conditioned on any assignment x, the posterior distributions
over Y in the two models satisfy the same independence assignments (the ones deﬁned by
the chain structure). However, the two models are not equivalent: In the Bayesian network, we
have that Y1 is independent of X2 if we are not given Y2. By contrast, in the original CRF, the
unnormalized marginal measure of Y depends on the entire parameterization of the chain, and
speciﬁcally the values of all of the variables in X. A sound conditional Bayesian network for
this distribution would require edges from all of the variables in X to each of the variables Yi,
thereby losing much of the structure in the distribution. See also box 20.A for further discussion.
Box 4.E — Case Study: CRFs for Text Analysis. One important use for the CRF framework is
in the domain of text analysis. Various models have been proposed for diﬀerent tasks, including
part-of-speech labeling, identifying named entities (people, places, organizations, and so forth), and
extracting structured information from the text (for example, extracting from a reference list the
publication titles, authors, journals, years, and the like). Most of these models share a similar
structure: We have a target variable for each word (or perhaps short phrase) in the document,
which encodes the possible labels for that word. Each target variable is connected to a set of feature
variables that capture properties relevant to the target distinction. These methods are very popular
in text analysis, both because the structure of the networks is a good ﬁt for this domain, and because
they produce state-of-the-art results for a broad range of natural-language processing problems.
As a concrete example, consider the named entity recognition task, as described by Sutton and
McCallum (2004, 2007). Entities often span multiple words, and the type of an entity may not be

146
Chapter 4. Undirected Graphical Models
apparent from individual words; for example, “New York” is a location, but “New York Times” is
an organization. The problem of extracting entities from a word sequence of length T can be cast
as a graphical model by introducing for each word, Xt, 1 ≤t ≤T, a target variable, Yt, which
indicates the entity type of the word. The outcomes of Yt include B-Person, I-Person, B-Location,
I-Location, B-Organization, I-Organization, and Other. In this so-called “BIO notation,” Other
indicates that the word is not part of an entity, the B- outcomes indicate the beginning of a named
entity phrase, and the I- outcomes indicate the inside or end of the named entity phrase. Having a
distinguishing label for the beginning versus inside of an entity phrase allows the model to segment
adjacent entities of the same type.
A common structure for this problem is a linear-chain CRF often having two factors for each
linear-chain CRF
word: one factor φ1
t(Yt, Yt+1) to represent the dependency between neighboring target variables,
and another factor φ2
t(Yt, X1, . . . , XT ) that represents the dependency between a target and its
context in the word sequence. Note that the second factor can depend on arbitrary features of
the entire input word sequence. We generally do not encode this model using table factors, but
using a log-linear model. Thus, the factors are derived from a number of feature functions, such
as ft(Yt, Xt) = 11{Yt = B-Organization, Xt = “Times”}. We note that, just as logistic CPDs are
the conditional analog of the naive Bayes classiﬁer (example 4.20), the linear-chain CRF is the
conditional analog of the hidden Markov model (HMM) that we present in section 6.2.3.1.
hidden Markov
model
A large number of features of the word Xt and neighboring words are relevant to the named
entity decision. These include features of the word itself: is it capitalized; does it appear in a list of
common person names; does it appear in an atlas of location names; does it end with the character
string “ton”; is it exactly the string “York”; is the following word “Times.” Also relevant are aggregate
features of the entire word sequence, such as whether it contains more than two sports-related
words, which might be an indicator that “New York” is an organization (sports team) rather than
a location. In addition, including features that are conjunctions of all these features often increases
accuracy. The total number of features can be quite large, often in the hundreds of thousands or
more if conjunctions of word pairs are used as features. However, the features are sparse, meaning
that most features are zero for most words.
Note that the same feature variable can be connected to multiple target variables, so that Yt
would typically be dependent on the identity of several words in a window around position t. These
contextual features are often highly indicative: for example, “Mrs.” before a word and “spoke” after
a word are both strong indicators that the word is a person. These context words would generally
be used as a feature for multiple target variables. Thus, if we were using a simple naive-Bayes-style
generative model, where each target variable is a parent of its associated feature, we either would
have to deal with the fact that a context word has multiple parents or we would have to duplicate
its occurrences (with one copy for each target variable for which it is in the context), and thereby
overcount its contribution.
Linear-chain CRFs frequently provide per-token accuracies in the high 90 percent range on many
natural data sets. Per-ﬁeld precision and recall (where the entire phrase category and boundaries
must be correct) are more often around 80–95 percent, depending on the data set.
Although the linear-chain model is often eﬀective, additional information can be incorporated
into the model by augmenting the graphical structure. For example, often when a word occurs
multiple times in the same document, it often has the same label. This knowledge can be incor-
porated by including factors that connect identical words, resulting in a skip-chain CRF, as shown
skip-chain CRF
in ﬁgure 4.E.1a. The ﬁrst occurrence of the word “Green” has neighboring words that provide strong

4.6. Partially Directed Models
147
 Mrs.
Green
spoke
today
in
New
York
(a)
(b)
Green
chairs
the
ﬁnance committee
B-PER
I-PER
OTH
OTH
OTH
B-LOC
I-LOC
B-PER
OTH
OTH
OTH
OTH
its
withdrawal
from
the
UAL
Airways
rose
after
announcing
KEY
Begin person name
Within person name
Begin location name
B-PER
I-PER
B-LOC
Within location name
Not an entitiy
I-LOC
OTH
 British
deal
ADJ
N
V
IN
V
PRP
N
IN
N
N
DT
B
I
O
O
O
B
I
O
I
POS
NP
I
B
Begin noun phrase
Within noun phrase
Not a noun phrase
Noun
Adjective
B
I
O
N
ADJ
Verb
Preposition
Possesive pronoun
Determiner (e.g., a, an, the)
V
IN
PRP
DT
KEY
Figure 4.E.1 — Two models for text analysis based on a linear chain CRF Gray nodes indicate X
and clear nodes Y . The annotations inside the Y are the true labels. (a) A skip chain CRF for named
entity recognition, with connections between adjacent words and long-range connections between multiple
occurrences of the same word. (b) A pair of coupled linear-chain CRFs that performs joint part-of-speech
labeling and noun-phrase segmentation. Here, B indicates the beginning of a noun phrase, I other words in
the noun phrase, and O words not in a noun phrase. The labels for the second chain are parts of speech.

148
Chapter 4. Undirected Graphical Models
evidence that it is a Person’s name; however, the second occurrence is much more ambiguous.
By augmenting the original linear-chain CRF with an additional long-range factor that prefers its
connected target variables to have the same value, the model is more likely to predict correctly that
the second occurrence is also a Person. This example demonstrates another ﬂexibility of conditional
models, which is that the graphical structure over Y can easily depend on the value of the X’s.
CRFs having a wide variety of model structures have been successfully applied to many diﬀer-
ent tasks. Joint inference of both part-of-speech labels and noun-phrase segmentation has been
performed with two connected linear chains (somewhat analogous to a coupled hidden Markov
coupled HMM
mode, shown in ﬁgure 6.3). This structure is illustrated in ﬁgure 4.E.1b.
4.6.2
Chain Graph Models ⋆
We now present a more general framework that builds on the CRF representation and can be
used to provide a general treatment of the independence assumptions made in these partially
directed models. Recall from deﬁnition 2.21 that, in a partially directed acyclic graph (PDAG),
partially directed
acyclic graph
the nodes can be disjointly partitioned into several chain components. An edge between two
chain component
nodes in the same chain component must be undirected, while an edge between two nodes in
diﬀerent chain components must be directed. Thus, PDAGs are also called chain graphs.
chain graph
4.6.2.1
Factorization
As in our other graphical representations, the structure of a PDAG K can be used to deﬁne
a factorization for a probability distribution over K.
Intuitively, the factorization for PDAGs
represents the distribution as a product of each of the chain components given its parents.
Thus, we call such a representation a chain graph model.
chain graph
model
Intuitively, each chain component Ki in the chain graph model is associated with a CRF that
deﬁnes P(Ki | PaKi) — the conditional distribution of Ki given its parents in the graph.
More precisely, each is deﬁned via a set of factors that involve the variables in Ki and their
parents; the distribution P(Ki | PaKi) is deﬁned by using the factors associated with Ki to
deﬁne a CRF whose target variables are Ki and whose observable variables are PaKi.
To provide a formal deﬁnition, it helps to introduce the concept of a moralized PDAG.
Deﬁnition 4.19
Let K be a PDAG and K1, . . . , Kℓbe its chain components. We deﬁne PaKi to be the parents of
nodes in Ki. The moralized graph of K is an undirected graph M[K] produced by ﬁrst connecting,
moralized graph
using undirected edges, any pair of nodes X, Y ∈PaKi for all i = 1, . . . , ℓ, and then converting
all directed edges into undirected edges.
This deﬁnition generalizes our earlier notion of a moralized directed graph.
In the case of
directed graphs, each node is its own chain component, and hence we are simply adding
undirected edges between the parents of each node.
Example 4.21
Figure 4.15 shows a chain graph and its moral graph. We have added the edge between A and B,
since they are both parents of the chain component {C, D, E}, and edges between C, E, and H,
because they are parents of the chain component {I}. Note that we did not add an edge between

4.6. Partially Directed Models
149
D
B
A
I
F
G
E
C
H
D
B
A
I
F
G
E
C
H
Figure 4.15
A chain graph K and its moralized version
D and H (even though D and C, E are in the same chain component), since D is not a parent of
I.
We can now deﬁne the factorization of a chain graph:
Deﬁnition 4.20
Let K be a PDAG, and K1, . . . , Kℓbe its chain components.
A chain graph distribution is
chain graph
distribution
deﬁned via a set of factors φi(Di) (i = 1, . . . , m), such that each Di is a complete subgraph in
the moralized graph M[K+[Di]]. We associate each factor φi(Di) with a single chain component
Kj, such that Di ⊆Ki ∪PaKi and deﬁne P(Ki | PaKi) as a CRF with these factors, and
with Y i = Ki and Xi = PaKi. We now deﬁne
P(X) =
ℓ
Y
i=1
P(Ki | PaKi).
We say that a distribution P factorizes over K if it can be represented as a chain graph
distribution over K.
Example 4.22
In the chain graph model deﬁned by the graph of ﬁgure 4.15, we require that the conditional
distribution P(C, D, E | A, B) factorize according to the graph of ﬁgure 4.16a. Speciﬁcally, we
would have to deﬁne the conditional probability as a normalized product of factors:
1
Z(A, B)φ1(A, C)φ2(B, E)φ3(C, D)φ4(D, E).
A similar factorization applies to P(F, G | C, D).
4.6.2.2
Independencies in Chain Graphs
As for undirected graphs, there are three distinct interpretations for the independence properties
induced by a PDAG. Recall that in a PDAG, we have both the notion of parents of X (variables Y
such that Y →X is in the graph) and neighbors of X (variables Y such that Y —X is in the
graph). Recall that the union of these two sets is the boundary of X, denoted BoundaryX. Also
boundary
recall, from deﬁnition 2.15, that the descendants of X are those nodes Y that can be reached
using any directed path, where a directed path can involve both directed and undirected edges
but must contain at least one edge directed from X to Y , and no edges directed from Y to

150
Chapter 4. Undirected Graphical Models
D
(a)
(b)
B
A
E
C
D
B
A
I
E
C
H
Figure 4.16
Example for deﬁnition of c-separation in a chain graph.
(a) The Markov network
M[K+[C, D, E]]. (b) The Markov network M[K+[C, D, E, I]].
X. Thus, in the case of PDAGs, it follows that if Y is a descendant of X, then Y must be in a
“lower” chain component.
Deﬁnition 4.21
For a PDAG K, we deﬁne the pairwise independencies associated with K to be:
pairwise
independencies
Ip(K) = {(X ⊥Y | (NonDescendantsX −{X, Y })) :
X, Y non-adjacent, Y ∈NonDescendantsX}.
This deﬁnition generalizes the pairwise independencies for undirected graphs: in an undirected
graph, nodes have no descendants, so NonDescendantsX = X. Similarly, it is not too hard to
show that these independencies also hold in a directed graph.
Deﬁnition 4.22
For a PDAG K, we deﬁne the local independencies associated with K to be:
local
independencies
Iℓ(K) = {(X ⊥NonDescendantsX −BoundaryX | BoundaryX) : X ∈X}.
This deﬁnition generalizes the deﬁnition of local independencies for both directed and undi-
rected graphs. For directed graphs, NonDescendantsX is precisely the set of nondescendants,
whereas BoundaryX is the set of parents. For undirected graphs, NonDescendantsX is X,
whereas BoundaryX = NbX.
We deﬁne the global independencies in a PDAG using the deﬁnition of moral graph. Our
deﬁnition follows the lines of proposition 4.10.
Deﬁnition 4.23
Let X, Y , Z ⊂X be three disjoint sets, and let U = X ∪Y ∪Z. We say that X is c-separated
c-separation
from Y given Z if X is separated from Y given Z in the undirected graph M[K+[X ∪Y ∪Z]].
Example 4.23
Consider again the PDAG of ﬁgure 4.15. Then C is c-separated from E given D, A, because C and E
are separated given D, A in the undirected graph M[K+[{C, D, E}]], shown in ﬁgure 4.16a. How-
ever, C is not c-separated from E given only D, since there is a path between C and E via A, B.
On the other hand, C is not separated from E given D, A, I. The graph M[K+[{C, D, E, I}]] is
shown in ﬁgure 4.16b. As we can see, the introduction of I into the set U causes us to introduce
a direct edge between C and E in order to moralize the graph. Thus, we cannot block the path
between C and E using D, A, I.

4.7. Summary and Discussion
151
This notion of c-separation clearly generalizes the notion of separation in undirected graphs,
since the ancestors of a set U in an undirected graph are simply the entire set of nodes X. It
also generalizes the notion of d-separation in directed graphs, using the equivalent deﬁnition
provided in proposition 4.10. Using the deﬁnition of c-separation, we can ﬁnally deﬁne the
notion of global Markov independencies:
Deﬁnition 4.24
Let K be a PDAG. We deﬁne the global independencies associated with K to be:
global
independencies
I(K) = {(X ⊥Y | Z) : X, Y , Z ⊂X, X is c-separated from Y given Z}.
As in the case of undirected models, these three criteria for independence are not equivalent
for nonpositive distributions. The inclusions are the same: the global independencies imply
the local independencies, which in turn imply the pairwise independencies. Because undirected
models are a subclass of PDAGs, the same counterexamples used in section 4.3.3 show that the
inclusions are strict for nonpositive distributions. For positive distributions, we again have that
the three deﬁnitions are equivalent.
We note that, as in the case of Bayesian networks, the parents of a chain component are
always fully connected in M[K[Ki ∪PaKi]]. Thus, while the structure over the parents helps
factorize the distribution over the chain components containing the parents, it does not give
rise to independence assertions in the conditional distribution over the child chain component.
Importantly, however, it does give rise to structure in the form of the parameterization of
P(Ki | PaKi), as we saw in example 4.20.
As in the case of directed and undirected models, we have an equivalence between the
requirement of factorization of a distribution and the requirement that it satisfy the indepen-
dencies associated with the graph. Not surprisingly, since PDAGs generalize undirected graphs,
this equivalence only holds for positive distributions:
Theorem 4.14
A positive distribution P factorizes over a PDAG K if and only if P |= I(K).
We omit the proof.
4.7
Summary and Discussion
In this chapter, we introduced Markov networks, an alternative graphical modeling language for
probability distributions, based on undirected graphs.
We showed that Markov networks, like Bayesian networks, can be viewed as deﬁning a set
of independence assumptions determined by the graph structure. In the case of undirected
models, there are several possible deﬁnitions for the independence assumptions induced by
the graph, which are equivalent for positive distributions. As in the case of Bayesian network,
we also showed that the graph can be viewed as a data structure for specifying a probability
distribution in a factored form. The factorization is deﬁned as a product of factors (general
nonnegative functions) over cliques in the graph. We showed that, for positive distributions, the
two characterizations of undirected graphs — as specifying a set of independence assumptions
and as deﬁning a factorization — are equivalent.
Markov networks also provide useful insight on Bayesian networks. In particular, we showed
how a Bayesian network can be viewed as a Gibbs distribution. More importantly, the unnor-
malized measure we obtain by introducing evidence into a Bayesian network is also a Gibbs

152
Chapter 4. Undirected Graphical Models
distribution, whose partition function is the probability of the evidence. This observation will
play a critical role in providing a uniﬁed view of inference in graphical models.
We investigated the relationship between Bayesian networks and Markov networks and showed
that the two represent diﬀerent families of independence assumptions. The diﬀerence in these
independence assumptions is a key factor in deciding which of the two representations to
use in encoding a particular domain.
There are domains where interactions have a natural
directionality, often derived from causal intuitions. In this case, the independencies derived

from the network structure directly reﬂect patterns such as intercausal reasoning. Markov
networks represent only monotonic independence patterns: observing a variable can only
serve to remove dependencies, not to activate them. Of course, we can encode a distribution
with “causal” connections as a Gibbs distribution, and it will exhibit the same nonmonotonic
independencies. However, these independencies will not be manifest in the network structure.

In other domains, the interactions are more symmetrical, and attempts to force a
directionality give rise to models that are unintuitive and that often are incapable of cap-
turing the independencies in the domain (see, for example, section 6.6). As a consequence,
the use of undirected models has increased steadily, most notably in ﬁelds such as computer
vision and natural language processing, where the acyclicity requirements of directed graphical
models are often at odds with the nature of the model. The ﬂexibility of the undirected model
also allows the distribution to be decomposed into factors over multiple overlapping “features”
without having to worry about deﬁning a single normalized generating distribution for each
variable. Conversely, this very ﬂexibility and the associated lack of clear semantics for the

model parameters often make it diﬃcult to elicit models from experts. Therefore, many
recent applications use learning techniques to estimate parameters from data, avoiding
the need to provide a precise semantic meaning for each of them.
Finally, the question of which class of models better encodes the properties of the distribution
is only one factor in the selection of a representation. There are other important distinctions
between these two classes of models, especially when it comes to learning from data. We return
to these topics later in the book (see, for example, box 20.A).
4.8
Relevant Literature
The representation of a probability distribution as an undirected graph has its roots in the
contingency table representation that is a staple in statistical modeling. The idea of representing
contingency table
probabilistic interactions in this representation dates back at least as early as the work of Bartlett
(1935). This line of work is reviewed in detail by Whittaker (1990) and Lauritzen (1996), and we
refer the reader to those sources and the references therein.
A parallel line of work involved the development of the Markov network (or Markov random
ﬁeld) representation. Here, the starting point was a graph object rather than a distribution object
(such as a contingency table). Isham (1981) surveys some of the early work along these lines.
The connection between the undirected graph representation and the Gibbs factorization of
the distribution was ﬁrst made in the unpublished work of Hammersley and Cliﬀord (1971). As
a consequence, they also showed the equivalence of the diﬀerent types of (local, pairwise, and
global) independence properties for undirected graphs in the case of positive distributions.
Lauritzen (1982) made the connection between MRFs and contingency tables, and proved

4.9. Exercises
153
some of the key results regarding the independence properties arising form the undirected
representation.
The line of work analyzing independence properties was then signiﬁcantly
extended by Pearl and Paz (1987). The history of these developments and other key references
are presented by Pearl (1988) and Lauritzen (1996). The independence properties of chain graphs
were studied in detail by Frydenberg (1990); see also Lauritzen (1996). Studený and Bouckaert
(1998) also provide an alternative deﬁnition of the independence properties in chain graphs, one
that is equivalent to c-separation but more directly analogous to the deﬁnition of d-separation
in directed graphs.
Factor graphs were presented by Kschischang et al. (2001a) and extended by Frey (2003) to en-
compass both Bayesian networks and Markov networks. The framework of conditional random
ﬁelds (CRFs) was ﬁrst proposed by Laﬀerty, McCallum, and Pereira (2001). They have subse-
quently been used in a broad range of applications in natural language processing, computer
vision, and many more. Skip-chain CRFs were introduced by Sutton and McCallum (2004), and
factorial CRFs by Sutton et al. (2007). Sutton and McCallum (2007) also provide an overview of
this framework and some of its applications.
Ising models were ﬁrst proposed by Ising (1925). The literature on this topic is too vast to
mention; we refer the reader to any textbook in the area of statistical physics. The connection
between Markov networks and these models in statistical physics is the origin of some of the
terminology associated with these models, such as partition function or energy. In fact, many
of the recent developments in inference for these models arise from approximations that were
ﬁrst proposed in the statistical physics community. Boltzmann machines were ﬁrst proposed by
Hinton and Sejnowski (1983).
Computer vision is another application domain that has motivated much of the work in
undirected graphical models. The applications of MRFs to computer vision are too numerous to
list; they span problems in low-level vision (such as image denoising, stereo reconstruction, or
image segmentation) and in high-level vision (such as object recognition). Li (2001) provides a
detailed description of some early applications; Szeliski et al. (2008) describe some applications
that are viewed as standard benchmark problems for MRFs in the computer vision ﬁeld.
4.9
Exercises
Exercise 4.1
Complete the analysis of example 4.4, showing that the distribution P deﬁned in the example does not
factorize over H. (Hint: Use a proof by contradiction.)
Exercise 4.2
In this exercise, you will prove that the modiﬁed energy functions ϵ′
1(A, B) and ϵ′
2(B, C) of ﬁgure 4.10
result in precisely the same distribution as our original energy functions. More generally, for any constants
λ1 and λ0, we can redeﬁne
ϵ′
1(a, bi)
:=
ϵ1(a, bi) + λi
ϵ′
2(bi, c)
:=
ϵ2(bi, c) −λi
Show that the resulting energy function is equivalent.
Exercise 4.3⋆
Provide an example a class of Markov networks Hn over n such that the size of the largest clique in Hn
is constant, yet any Bayesian network I-map for Hn is exponentially large in n.

154
Chapter 4. Undirected Graphical Models
Exercise 4.4⋆
Prove theorem 4.7 for the case where H consists of a single clique.
Exercise 4.5
Complete the proof of theorem 4.3, by showing that U1 and Uk are dependent given Z in the distribution
P deﬁned by the product of potentials described in the proof.
Exercise 4.6⋆
Consider a factor graph F, as in deﬁnition 4.13. Deﬁne the minimal Markov network H that is guaranteed
to be an I-map for any distribution deﬁned over F. Prove that H is a sound and complete representation
of the independencies in F:
a. If sepH(X; Y | Z) holds, then (X ⊥Y | Z) holds for all distributions over F.
b. If sepH(X; Y | Z) does not hold, then there is some distribution P that factorizes over F such that
(X ⊥Y | Z) does not hold in P.
Exercise 4.7⋆
The canonical parameterization in the Hammersley-Cliﬀord theorem is stated in terms of the maximal
cliques in a Markov network.
In this exercise, you will show that it also captures the ﬁner-grained
representation of factor graphs. Speciﬁcally, let P be a distribution that factorizes over a factor graph F,
as in deﬁnition 4.13. Show that the canonical parameterization of P also factorizes over F.
Exercise 4.8
Prove proposition 4.3. More precisely, let P satisfy Iℓ(H), and assume that X and Y are two nodes in H
that are not connected directly by an edge. Prove that P satisﬁes (X ⊥Y | X −{X, Y }).
Exercise 4.9⋆
Complete the proof of theorem 4.4. Assume that equation (4.1) holds for all disjoint sets X, Y , Z, with
|Z| ≥k. Prove that equation (4.1) also holds for any disjoint X, Y , Z such that X ∪Y ∪Z ̸= X and
|Z| = k −1.
Exercise 4.10
We deﬁne the following properties for a set of independencies:
•
Strong Union:
strong union
(X ⊥Y | Z) =⇒(X ⊥Y | Z, W ).
(4.12)
In other words, additional evidence W cannot induce dependence.
•
Transitivity: For all disjoint sets X, Y , Z and all variables A:
transitivity
¬(X ⊥A | Z)&¬(A ⊥Y | Z) =⇒¬(X ⊥Y | Z).
(4.13)
Intuitively, this statement asserts that if X and Y are both correlated with some A (given Z), then they
are also correlated with each other (given Z). We can also write the contrapositive of this statement,
which is less obvious but easier to read. For all X, Y , Z, A:
(X ⊥Y | Z) −→(X ⊥A | Z) ∨(A ⊥Y | Z).
Prove that if I = I(H) for some Markov network H, then I satisﬁes strong union and transitivity.
Exercise 4.11⋆
In this exercise you will prove theorem 4.6. Consider some speciﬁc node X, and let U be the set of all
subsets U satisfying deﬁnition 4.12. Deﬁne U ∗to be the intersection of all U ∈U.
a. Prove that U ∗∈U. Conclude that MBP (X) = U ∗.

4.9. Exercises
155
b. Prove that if P |= (X ⊥Y | X −{X, Y }), then Y ̸∈MBP (X).
c. Prove that if Y ̸∈MBP (X), then P |= (X ⊥Y | X −{X, Y }).
d. Conclude that MBP (X) is precisely the set of neighbors of X in the graph deﬁned in theorem 4.5,
showing that the construction of theorem 4.6 also produces a minimal I-map.
Exercise 4.12
Show that a Boltzmann machine distribution (with variables taking values in {0, 1}) can be rewritten as
an Ising model, where we use the value space {−1, +1} (mapping 0 to −1).
Exercise 4.13
Show that we can represent any Gibbs distribution as a log-linear model, as deﬁned in deﬁnition 4.15.
Exercise 4.14
Complete the proof of proposition 4.8. In particular, show the following:
a. For any variable X, let W = X −{X} −MBG(X). Then d-sepG(X; W | MBG(X)).
b. The set MBG(X) is the minimal set for which this property holds.
Exercise 4.15
Prove proposition 4.10.
Exercise 4.16⋆
Provide an example of a class of Markov networks Hn over n nodes for arbitrarily large n (not necessarily
for every n), where the size of the largest clique is a constant independent of n, yet the size of the largest
clique in any chordal graph HC
n that contains Hn is exponential in n. Explain why the size of the largest
clique is necessarily exponential in n for all HC
n .
Exercise 4.17⋆
In this exercise, you will prove that the chordality requirement for graphs is equivalent to two other
conditions of independent interest.
Deﬁnition 4.25
Let X, Y , Z be disjoint sets such that X = X ∪Y ∪Z and X, Y ̸= ∅. We say that (X, Z, Y ) is a
decomposition of a Markov network H if Z separates X from Y and Z is a complete subgraph in H.
Markov network
decomposition
Deﬁnition 4.26
We say that a graph H is decomposable if there is a decomposition (X, Z, Y ) of H, such that the graphs
induced by X ∪Z and Y ∪Z are also decomposable.
Show that, for any undirected graph H, the following conditions are equivalent:
a. H is decomposable;
b. H is chordal;
c. for every X, Y , every minimal set Z that separates X and Y is complete.
The proof of equivalence proceeds by induction on the number of vertices in X. Assume that the three
conditions are equivalent for all graphs with |X| ≤n, and consider a graph H with |X| = n + 1.
a. Prove that if H is decomposable, it is chordal.
b. Prove that if H is chordal, then for any X, Y , and any minimal set Z that separates X and Y , Z is
complete.
c. Prove that for any X, Y , any minimal set Z that separates X and Y is complete, then H is decom-
posable.

156
Chapter 4. Undirected Graphical Models
Exercise 4.18
Let G be a Bayesian network structure and H a Markov network structure over X such that the skeleton
of G is precisely H. Prove that if G has no immoralities, then I(G) = I(H).
Exercise 4.19
Consider the PDAG of ﬁgure 4.15. Write down all c-separation statements that are valid given {G}; write
down all valid statements given {G, D}; write down all statements that are valid given {C, D, E}.

5
Local Probabilistic Models
In chapter 3 and chapter 4, we discussed the representation of global properties of independence
by graphs. These properties of independence allowed us to factorize a high-dimensional joint
distribution into a product of lower-dimensional CPDs or factors. So far, we have mostly ignored
the representation of these factors. In this chapter, we examine CPDs in more detail. We describe
a range of representations and consider their implications in terms of additional regularities we
can exploit. We have chosen to phrase our discussion in terms of CPDs, since they are more
constrained than factors (because of the local normalization constraints). However, many of the
representations we discuss in the context of CPDs can also be applied to factors.
5.1
Tabular CPDs
When dealing with spaces composed solely of discrete-valued random variables, we can always
resort to a tabular representation of CPDs, where we encode P(X | PaX) as a table that
contains an entry for each joint assignment to X and PaX. For this table to be a proper CPD,
we require that all the values are nonnegative, and that, for each value paX, we have
X
x∈Val(X)
P(x | paX) = 1.
(5.1)
It is clear that this representation is as general as possible. We can represent every possible
discrete CPD using such a table. As we will also see, table-CPDs can be used in a natural way in
inference algorithms that we discuss in chapter 9. These advantages often lead to the perception
that table-CPDs, also known as conditional probability tables (CPTs), are an inherent part of the
table-CPD
Bayesian network representation.
However, the tabular representation also has several signiﬁcant disadvantages. First, it is clear
that if we consider random variables with inﬁnite domains (for example, random variables with
continuous values), we cannot store each possible conditional probability in a table. But even
in the discrete setting, we encounter diﬃculties. The number of parameters needed to describe
a table-CPD is the number of joint assignments to X and PaX, that is, |Val(PaX)| · |Val(X)|.1
This number grows exponentially in the number of parents. Thus, for example, if we have 5
binary parents of a binary variable X, we need specify 25 = 32 values; if we have 10 parents,
we need to specify 210 = 1, 024 values.
1. We can save some space by storing only independent parameters, but this saving is not signiﬁcant.

158
Chapter 5. Local Probabilistic Models
Clearly, the tabular representation rapidly becomes large and unwieldy as the number of
parents grows. This problem is a serious one in many settings. Consider a medical domain
where a symptom, Fever, depends on 10 diseases. It would be quite tiresome to ask our expert
1,024 questions of the format: “What is the probability of high fever when the patient has disease
A, does not have disease B, . . . ?” Clearly, our expert will lose patience with us at some point!
This example illustrates another problem with the tabular representation: it ignores structure
within the CPD. If the CPD is such that there are no similarity between the various cases, that
is, each combination of disease has drastically diﬀerent probability of high fever, then the expert
might be more patient. However, in this example, like many others, there is some regularity in
the parameters for diﬀerent values of the parents of X. For example, it might be the case that, if
the patient suﬀers from disease A, then she is certain to have high fever and thus P(X | paX)
is the same for all values paX in which A is true. Indeed, many of the representations we
consider in this chapter attempt to describe such regularities explicitly and to exploit them in
order to reduce the number of parameters needed to specify a CPD.
The key insight that allows us to avoid these problems is the following observation: A CPD

needs to specify a conditional probability P(x | paX) for every assignment of values paX
and x, but it does not have to do so by listing each such value explicitly. We should view
CPDs not as tables listing all of the conditional probabilities, but rather as functions that
given pax and x, return the conditional probability P(x | paX). This implicit representation
suﬃces in order to specify a well-deﬁned joint distribution as a BN. In the remainder of the
chapter, we will explore some of the possible representations of such functions.
5.2
Deterministic CPDs
5.2.1
Representation
Perhaps the simplest type of nontabular CPD arises when a variable X is a deterministic function
deterministic
CPD
of its parents PaX. That is, there is a function f : Val(PaX) 7→Val(X), such that
P(x | paX) =
 1
x = f(paX)
0
otherwise.
For example, in the case of binary-valued variables, X might be the “or” of its parents. In a
continuous domain, we might want to assert in P(X | Y, Z) that X is equal to Y + Z.
Of course, the extent to which this representation is more compact than a table (that is, takes
less space in the computer) depends on the expressive power that our BN modeling language
oﬀers us for specifying deterministic functions. For example, some languages might allow a
vocabulary that includes only logical OR and AND of the parents, so that all other functions
must be speciﬁed explicitly as a table.2 In a domain with continuous variables, a language might
choose to allow only linear dependencies of the form X = 2Y + −3Z + 1, and not arbitrary
functions such as X = sin(y + ez).
Deterministic relations are useful in modeling many domains. In some cases, they occur
naturally. Most obviously, when modeling constructed artifacts such as machines or electronic
circuits, deterministic dependencies are often part of the device speciﬁcation. For example, the
2. Other logical functions, however, can be described by introducing intermediate nodes and composing ORs and ANDs.

5.2. Deterministic CPDs
159
behavior of an OR gate in an electronic circuit (in the case of no faults) is that the gate output
is a deterministic OR of the gate inputs. However, we can also ﬁnd deterministic dependencies
in “natural” domains.
Example 5.1
Recall that the genotype of a person is determined by two copies of each gene, called alleles. Each
allele can take on one of several values corresponding to diﬀerent genetic tendencies. The person’s
phenotype is often a deterministic function of these values. For example, the gene responsible for
determining blood type has three values: a, b, and o. Letting G1 and G2 be variables representing
the two alleles, and T the variable representing the phenotypical blood type, then we have that:
T =















ab
if G1 or G2 is a and the other is b
a
if at least one of G1 or G2 is equal to a and the other is
either a or o
b
if at least one of G1 or G2 is equal to b and the other is
either b or o
o
if G1 = o and G2 = o
Deterministic variables can also help simplify the dependencies in a complex model.
Example 5.2
When modeling a car, we might have four variables T1, . . . , T4, each corresponding to a ﬂat in
one of the four tires. When one or more of these tires is ﬂat, there are several eﬀects; for example,
the steering may be aﬀected, the ride can be rougher, and so forth. Naively, we can make all
of the Ti’s parents of all of the aﬀected variables — Steering, Ride, and so on. However, it can
signiﬁcantly simplify the model to introduce a new variable Flat-Tire, which is the deterministic OR
of T1, . . . , T4. We can then replace a complex dependency of Steering and Ride on T1, . . . , T4 with
a dependency on a single parent Flat-Tire, signiﬁcantly reducing their indegree. If these variables
have other parents, the savings can be considerable.
5.2.2
Independencies
Aside from a more compact representation, we get an additional advantage from making the
structure explicit. Recall that conditional independence is a numerical property — it is deﬁned
using equality of probabilities. However, the graphical structure in a BN makes certain properties
of a distribution explicit, allowing us to deduce that some independencies hold without looking
at the numbers. By making structure explicit in the CPD, we can do even more of the same.
Example 5.3
Consider the simple network structure in ﬁgure 5.1. If C is a deterministic function of A and B,
what new conditional independencies do we have? Suppose that we are given the values of A and
B. Then, since C is deterministic, we also know the value of C. As a consequence, we have that D
and E are independent. Thus, we conclude that (D ⊥E | A, B) holds in the distribution. Note
that, had C not been a deterministic function of A and B, this independence would not necessarily
hold. Indeed, d-separation would not deduce that D and E are independent given A and B.
Can we augment the d-separation procedure to discover independencies in cases such as this?
Consider an independence assertion (X ⊥Y | Z); in our example, we are interested in the
case where Z = {A, B}. The variable C is not in Z and is therefore not considered observed.

160
Chapter 5. Local Probabilistic Models
B
A
E
D
C
Figure 5.1
Example of a network with a deterministic CPD. The double-line notation represents the
fact that C is a deterministic function of A and B.
Algorithm 5.1 Computing d-separation in the presence of deterministic CPDs
Procedure det-sep(
Graph, // network structure
D, // set of deterministic variables
X, Y , Z // query
)
Let Z+ ←Z
While there is an Xi such that
(1) Xi ∈D // Xi has a deterministic CPD
(2) PaXi ⊆Z+
Z+ ←Z+ ∪{Xi}
return d-sepG(X; Y | Z+)
But when A and B are observed, then the value of C is also known with certainty, so we can
consider it as part of our observed set Z. In our example, this simple modiﬁcation would suﬃce
for inferring that D and E are independent given A and B.
In other examples, however, we might need to continue this process. For example, if we had
another variable F that was a deterministic function of C, then F is also de facto observed
when C is observed, and hence when A and B are observed. Thus, F should also be introduced
into Z. Thus, we have to extend Z iteratively to contain all the variables that are determined
by it. This discussion suggests the simple procedure shown in algorithm 5.1.
This algorithm provides a procedural deﬁnition for deterministic separation of X from Y
deterministic
separation
given Z. This deﬁnition is sound, in the same sense that d-separation is sound.
Theorem 5.1
Let G be a network structure, and let D, X, Y , Z be sets of variables. If X is deterministically
separated from Y given Z (as deﬁned by det-sep(G, D, X, Y , Z)), then for all distributions P
such that P |= Iℓ(G) and where, for each X ∈D, P(X | PaX) is a deterministic CPD, we have
that P |= (X ⊥Y | Z).
The proof is straightforward and is left as an exercise (exercise 5.1).
Does this procedure capture all of the independencies implied by the deterministic functions?
As with d-separation, the answer must be qualiﬁed: Given only the graph structure and the set

5.2. Deterministic CPDs
161
B
A
D
C
E
Figure 5.2
A slightly more complex example with deterministic CPDs
of deterministic CPDs, we cannot ﬁnd additional independencies.
Theorem 5.2
Let G be a network structure, and let D, X, Y , Z be sets of variables. If det-sep(G, D, X, Y , Z)
returns false, then there is a distribution P such that P |= Iℓ(G) and where, for each X ∈D,
P(X | PaX) is deterministic CPD, but we have that P ̸|= (X ⊥Y | Z).
Of course, the det-sep procedure detects independencies that are derived purely from the
fact that a variable is a deterministic function of its parents. However, particular deterministic
functions can imply additional independencies.
Example 5.4
Consider the network of ﬁgure 5.2, where C is the exclusive or of A and B. What additional
independencies do we have here? In the case of XOR (although not for all other deterministic
functions), the values of C and B fully determine that of A. Therefore, we have that (D ⊥E |
B, C) holds in the distribution.
Speciﬁc deterministic functions can also induce other independencies, ones that are more
reﬁned than the variable-level independencies discussed in chapter 3.
Example 5.5
Consider the Bayesian network of ﬁgure 5.1, but where we also know that the deterministic function
at C is an OR. Assume we are given the evidence A = a1. Because C is an OR of its parents, we
immediately know that C = c1, regardless of the value of B. Thus, we can conclude that B and D
are now independent: In other words, we have that
P(D | B, a1) = P(D | a1).
On the other hand, if we are given A = a0, the value of C is not determined, and it does depend
on the value of B. Hence, the corresponding statement conditioned on a0 is false.
Thus, deterministic variables can induce a form of independence that is diﬀerent from the
standard notion on which we have focused so far. Up to now, we have restricted attention
to independence properties of the form (X ⊥Y | Z), which represent the assumption that
P(X | Y , Z) = P(X | Z) for all values of X, Y and Z. Deterministic functions can imply a
type of independence that only holds for particular values of some variables.

162
Chapter 5. Local Probabilistic Models
Grade
Letter
Job
Apply
SAT
Intelligence
Difﬁculty
Figure 5.3
The Student example augmented with a Job variable
Deﬁnition 5.1
Let X, Y , Z be pairwise disjoint sets of variables, let C be a set of variables (that might overlap
with X ∪Y ∪Z), and let c ∈Val(C). We say that X and Y are contextually independent
context-speciﬁc
independence
given Z and the context c denoted (X ⊥c Y | Z, c), if
P(X | Y , Z, c) = P(X | Z, c) whenever P(Y , Z, c) > 0.
Independence statements of this form are called context-speciﬁc independencies (CSI). They arise
in many forms in the context of deterministic dependencies.
Example 5.6
As we saw in example 5.5, we can have that some value of one parent A can be enough to determine
the value of the child C. Thus, we have that (C ⊥c B | a1), and hence also that (D ⊥c B | a1).
We can make additional conclusions if we use properties of the OR function. For example, if we
know that C = c0, we can conclude that both A = a0 and B = b0. Thus, in particular, we can
conclude both that (A ⊥c B | c0) and that (D ⊥c E | c0). Similarly, if we know that C = c1
and B = b0, we can conclude that A = a1, and hence we have that (D ⊥c E | b0, c1).
It is important to note that context-speciﬁc independencies can also arise when we have

tabular CPDs.
However, in the case of tabular CPDs, the independencies would only
become apparent if we examine the network parameters. By making the structure of the
CPD explicit, we can use qualitative arguments to deduce these independencies.
5.3
Context-Speciﬁc CPDs
5.3.1
Representation
Structure in CPDs does not arise only in the case of deterministic dependencies. A very common
type of regularity arises when we have precisely the same eﬀect in several contexts.
Example 5.7
We augment our Student example to model the event that the student will be oﬀered a job at Acme
Consulting. Thus, we have a binary-valued variable J, whose value is j1 if the student is oﬀered
this job, and j0 otherwise. The probability of this event depends on the student’s SAT scores and
the strength of his recommendation letter. We also have to represent the fact that our student might

5.3. Context-Speciﬁc CPDs
163
(0.9,0.1)
(0.4,0.6)
L
l1
l0
(0.8,0.2)
A
a1
a0
(0.1,0.9)
S
s1
s0
Figure 5.4
A tree-CPD for P(J | A, S, L). Internal nodes in the tree denote tests on parent variables.
Leaves are annotated with the distribution over J.
choose not to apply for a job at Acme Consulting. Thus, we have a binary variable Applied, whose
value (a1 or a0) indicates whether the student applied or not. The structure of the augmented
network is shown in ﬁgure 5.3.
Now, we need to describe the CPD P(J | A, S, L). In our domain, even if the student does not
apply, there is still a chance that Acme Consulting is suﬃciently desperate for employees to oﬀer him
a job anyway. (This phenomenon was quite common during the days of the Internet Gold Rush.) In
this case, however, the recruiter has no access to the student’s SAT scores or recommendation letters,
and therefore the decision to make an oﬀer cannot depend on these variables. Thus, among the 8
values of the parents A, S, L, the four that have A = a0 must induce an identical distribution over
the variable J.
We can elaborate this model even further. Assume that our recruiter, knowing that SAT scores
are a far more reliable indicator of the student’s intelligence than a recommendation letter, ﬁrst
considers the SAT score. If it is high, he generates an oﬀer immediately. (As we said, Acme Consulting
is somewhat desperate for employees.) If, on the other hand, the SAT score is low, he goes to the
eﬀort of obtaining the professor’s letter of recommendation, and makes his decision accordingly. In
this case, we have yet more regularity in the CPD: P(J | a1, s1, l1) = P(J | a1, s1, l0).
In this simple example, we have a CPD in which several values of PaJ specify the same
conditional probability over J. In general, we often have CPDs where, for certain partial assign-
ments u to subsets U ⊂PaX, the values of the remaining parents are not relevant. In such
cases, several diﬀerent distributions P(X | paX) are identical. In this section, we discuss how
we might capture this regularity in our CPD representation and what implications this structure
has on conditional independence. There are many possible approaches for capturing functions
over a scope X that are constant over certain subsets of instantiations to X. In this section,
we present two common and useful choices: trees and rules.
5.3.1.1
Tree-CPDs
A very natural representation for capturing common elements in a CPD is via a tree, where the
leaves of the tree represent diﬀerent possible (conditional) distributions over J, and where the
path to each leaf dictates the contexts in which this distribution is used.

164
Chapter 5. Local Probabilistic Models
Example 5.8
Figure 5.4 shows a tree for the CPD of the variable J in example 5.7. Given this tree, we ﬁnd
P(J | A, S, L) by traversing the tree from the root downward. At each internal node, we see a
test on one of the attributes. For example, in the root node of our tree we see a test on the value
A. We then follow the arc that is labeled with the value a, which is given in the current setting
of the parents. Assume, for example, that we are interested in P(J | a1, s1, l0). Thus, we have
that A = a1, and we would follow the right-hand arc labeled a1. The next test is over S. We
have S = s1, and we would also follow the right-hand arc. We have now reached a leaf, which is
annotated with a particular distribution over J: P(j1) = 0.9, and P(j0) = 0.1. This distribution
is the one we use for P(J | a1, s1, l0).
Formally, we use the following recursive deﬁnition of trees.
Deﬁnition 5.2
A tree-CPD representing a CPD for variable X is a rooted tree; each t-node in the tree is either a
tree-CPD
leaf t-node or an interior t-node. Each leaf is labeled with a distribution P(X). Each interior
t-node is labeled with some variable Z ∈PaX. Each interior t-node has a set of outgoing arcs to
its children, each one associated with a unique variable assignment Z = zi for zi ∈Val(Z).
A branch β through a tree-CPD is a path beginning at the root and proceeding to a leaf node.
We assume that no branch contains two interior nodes labeled by the same variable. The parent
context induced by branch β is the set of variable assignments Z = zi encountered on the arcs
along the branch.
Note that, to avoid confusion, we use t-nodes and arcs for a tree-CPD, as opposed to our use of
nodes and edges as the terminology in a BN.
Example 5.9
Consider again the tree in ﬁgure 5.4. There are four branches in this tree. One induces the parent
context ⟨a0⟩, corresponding to the situation where the student did not apply for the job. A second
induces the parent context ⟨a1, s1⟩, corresponding to an application with a high SAT score. The
remaining two branches induce complete assignments to all the parents of J: ⟨a1, s0, l1⟩and
⟨a1, s0, l0⟩.
Thus, this representation breaks down the conditional distribution of J given its
parents into four parent contexts by grouping the possible assignments in Val(PaJ) into subsets
that have the same eﬀect on J. Note that now we need only 4 parameters to describe the behavior
of J, instead of 8 in the table representation.
Regularities of this type occur in many domains. Some events can occur only in certain
situations. For example, we can have a Wet variable, denoting whether we get wet; that variable
would depend on the Raining variable, but only in the context where we are outside. Another
type of example arises in cases where we have a sensor, for example, a thermometer; in general,
the thermometer depends on the temperature, but not if it is broken.
This type of regularity is very common in cases where a variable can depend on one of a
large set of variables: it depends only on one, but we have uncertainty about the choice of
variable on which it depends.
Example 5.10
Let us revisit example 3.7, where George had to decide whether to give the recruiter at Acme
Consulting the letter from his professor in Computer Science 101 or his professor in Computer
Science 102. George’s chances of getting a job can depend on the quality of both letters L1 and L2,

5.3. Context-Speciﬁc CPDs
165
Letter1
Letter2
Letter1
Letter2
Choice
Job
(a)
(b)
(c)
Choice
Letter
Job
(0.9,0.1)
(0.3,0.7)
l1
0
l1
1
(0.8,0.2)
(0.1,0.9)
l2
0
l2
1
C
c1
c2
L1
L2
Figure 5.5
The OneLetter example. (a) The network fragment. (b) tree-CPD for P(J | C, L1, L2). (c)
Modiﬁed network with a new variable L that has a multiplexer CPD.
and hence both are parents. However, depending on which choice C George makes, the dependence
will only be on one of the two. Figure 5.5a shows the network fragment, and b shows the tree-CPD
for the variable J. (For simplicity, we have eliminated the dependence on S and A that we had in
ﬁgure 5.4.)
More formally, we deﬁne the following:
Deﬁnition 5.3
A CPD P(Y | A, Z1, . . . , Zk) is said to be a multiplexer CPD if Val(A) = {1, . . . , k}, and
multiplexer CPD
P(Y | a, Z1, . . . , Zk) = 11{Y = Za},
where a is the value of A. The variable A is called the selector variable for the CPD.
selector variable
In other words, the value of the multiplexer variable is a copy of the value of one of its parents,
Z1, . . . , Zk. The role of A is to select the parent who is being copied. Thus, we can think of a
multiplexer CPD as a switch.
We can apply this deﬁnition to example 5.10 by introducing a new variable L, which is a
multiplexer of L1 and L2, using C as the selector. The variable J now depends directly only
on L. The modiﬁed network is shown in ﬁgure 5.5c.
This type of model arises in many settings. For example, it can arise when we have diﬀerent
actions in our model; it is often the case that the set of parents for a variables varies considerably
based on the action taken. Conﬁguration variables also result in such situations: depending on
the speciﬁc conﬁguration of a physical system, the interactions between variables might diﬀer
(see box 5.A).
Another setting where this type of model is particularly useful is in dealing with uncertainty
about correspondence between diﬀerent objects.
This problem arises, for example, in data
correspondence
data association
association, where we obtain sensor measurements about real-world objects, but we are uncertain
about which object gave rise to which sensor measurement. For example, we might get a blip
on a radar screen without knowing which of of several airplanes the source of the signal.
Such cases also arise in robotics (see box 15.A and box 19.D). Similar situations also arise in
other applications, such as identity resolution: associating names mentioned in text to the real-
identity
resolution

166
Chapter 5. Local Probabilistic Models
world objects to which they refer (see box 6.D). We can model this type of situation using a
correspondence variable U that associates, with each sensor measurement, the identity u of the
correspondence
variable
object that gave rise to the measurement. The actual sensor measurement is then deﬁned using
a multiplexer CPD that depends on the correspondence variable U (which plays the role of the
selector variable), and on the value of A(u) for all u from which the measurement could have
been derived. The value of the measurement will be the value of A(u) for U = u, usually with
some added noise due to measurement error. Box 12.D describes this problem in more detail
and presents algorithms for dealing with the diﬃcult inference problem it entails.
Trees provide a very natural framework for representing context-speciﬁcity in the CPD. In
particular, it turns out that people ﬁnd it very convenient to represent this type of structure
using trees. Furthermore, the tree representation lends itself very well to automated learning
algorithms that construct a tree automatically from a data set.
Box 5.A — Case Study: Context-Speciﬁcity in Diagnostic Networks. A common setting where
context-speciﬁc CPDs arise is in troubleshooting of physical systems, as described, for example, by
troubleshooting
Heckerman, Breese, and Rommelse (1995). In such networks, the context speciﬁcity is due to the
presence of alternative conﬁgurations. For example, consider a network for diagnosis of faults in a
printer, developed as part of a suite of troubleshooting networks for Microsoft’s Windows 95TM op-
erating system. This network, shown in ﬁgure 5.A.1a, models the fact that the printer can be hooked
up either to the network via an Ethernet cable or to a local computer via a cable, and therefore
depends on both the status of the local transport medium and the network transport medium.
However, the status of the Ethernet cable only aﬀects the printer’s output if the printer is hooked
up to the network. The tree-CPD for the variable Printer-Output is shown in ﬁgure 5.A.1b. Even in
this very simple network, this use of local structure in the CPD reduced the number of parameters
required from 145 to 55.
We return to the topic of Bayesian networks for troubleshooting in box 21.C and box 23.C.
5.3.1.2
Rule CPDs
As we seen, trees are appealing for several reasons. However, trees are a global representation
that captures the entire CPD in a single data structure. In many cases, it is easier to reason
using a CPD if we break down the dependency structure into ﬁner-grained elements. A ﬁner-
grained representation of context-speciﬁc dependencies is via rules. Roughly speaking, each rule
corresponds to a single entry in the CPD of the variable. It speciﬁes a context in which the CPD
entry applies and its numerical value.
Deﬁnition 5.4
A rule ρ is a pair ⟨c; p⟩where c is an assignment to some subset of variables C, and p ∈[0, 1].
rule
We deﬁne C to be the scope of ρ, denoted Scope[ρ].
scope
This representation decomposes a tree-CPD into its most basic elements.

5.3. Context-Speciﬁc CPDs
167
Application Data
Print Spooling
GDI Input
GDI Output OK
PrintDataOut
Print to File
Net Printer Pathname
Printer Output
Network Connection
Print Icon Appearance
Net Printer On and Online
Net Printer Paper Supply
LOCAL Transport
NET Transport
Local Printer OK
Net Printer OK
Correct Driver
Driver Conﬁguration
Driver File Status
Correct Printer Selection
Cable/Port Hardware
Local Printer Cable
Local Paper Supply
Local Printer On and Online
Printer Location
Local Disk Space
Application
Document
(a)
(b)
Figure 5.A.1 — Context-speciﬁc independencies for diagnostic networks. (a) A real-life Bayesian
network that uses context-speciﬁc dependencies. The network is used for diagnosing printing problems in
Microsoft’s online troubleshooting system. (b) The structure of the tree-CPD for the Printer Output variable
in that network.

168
Chapter 5. Local Probabilistic Models
Example 5.11
Consider the tree of ﬁgure 5.4. There are eight entries in the CPD tree, such that each one corresponds
to a branch in the tree and an assignment to the variable J itself. Thus, the CPD deﬁnes eight rules:























ρ1:⟨a0, j0; 0.8⟩
ρ2:⟨a0, j1; 0.2⟩
ρ3:⟨a1, s0, l0, j0; 0.9⟩
ρ4:⟨a1, s0, l0, j1; 0.1⟩
ρ5:⟨a1, s0, l1, j0; 0.4⟩
ρ6:⟨a1, s0, l1, j1; 0.6⟩
ρ7:⟨a1, s1, j0; 0.1⟩
ρ8:⟨a1, s1, j1; 0.9⟩























For example, the rule ρ4 is derived by following the branch a1, s0, l0 and then selecting the
probability associated with the assignment J = j1.
Although we can decompose any tree-CPD into its constituent rules, we wish to deﬁne rule-
based CPDs as an independent notion. To deﬁne a coherent CPD from a set of rules, we need to
make sure that each conditional distribution of the form P(X | paX) is speciﬁed by precisely
one rule. Thus, the rules in a CPD must be mutually exclusive and exhaustive.
Deﬁnition 5.5
A rule-based CPD P(X | PaX) is a set of rules R such that:
rule-based CPD
• For each rule ρ ∈R, we have that Scope[ρ] ⊆{X} ∪PaX.
• For each assignment (x, u) to {X} ∪PaX, we have precisely one rule ⟨c; p⟩∈R such that
c is compatible with (x, u). In this case, we say that P(X = x | PaX = u) = p.
• The resulting CPD P(X | U) is a legal CPD, in that
X
x
P(x | u) = 1.
The rule set in example 5.11 satisﬁes these conditions.
Consider the following, more complex, example.
Example 5.12
Let X be a variable with PaX = {A, B, C}, and assume that X’s CPD is deﬁned via the following
set of rules:
ρ1:⟨a1, b1, x0; 0.1⟩
ρ2:⟨a1, b1, x1; 0.9⟩
ρ3:⟨a0, c1, x0; 0.2⟩
ρ4:⟨a0, c1, x1; 0.8⟩
ρ5:⟨b0, c0, x0; 0.3⟩
ρ6:⟨b0, c0, x1; 0.7⟩
ρ7:⟨a1, b0, c1, x0; 0.4⟩
ρ8:⟨a1, b0, c1, x1; 0.6⟩
ρ9:⟨a0, b1, c0; 0.5⟩
This set of rules deﬁnes the following CPD:
X
a0b0c0
a0b0c1
a0b1c0
a0b1c1
a1b0c0
a1b0c1
a1b1c0
a1b1c1
x0
0.3
0.2
0.5
0.2
0.3
0.4
0.1
0.1
x1
0.7
0.8
0.5
0.8
0.7
0.6
0.9
0.9

5.3. Context-Speciﬁc CPDs
169
(0.3,0.7)
(0.5,0.5)
(0.3,0.7)
(0.4,0.6)
A
B
C
a0
c1
c0
(0.2,0.8)
C
c1
c0
b1
b0
(0.1,0.9)
B
b1
b0
a1
Figure 5.6
A tree-CPD for the rule-based CPD P(X | A, B, C) of example 5.12.
For example, the CPD entry P(x0 | a0, b1, c1) is determined by the rule ρ3, resulting in the CPD
entry 0.2; we can verify that no other rule is compatible with the context a0, b1, c1, x1. We can
also verify that each of the CPD entries is also compatible with precisely one context, and hence
that the diﬀerent contexts are mutually exclusive and exhaustive.
Note that both CPD entries P(x1 | a0, b1, c0) and P(x0 | a0, b1, c0) are determined by a
single rule ρ9. As the probabilities for the diﬀerent contexts in this case must sum up to 1, this
phenomenon is only possible when the rule deﬁnes a uniform distribution, as it does in this case.
This perspective views rules as a decomposition of a CPD. We can also view a rule as a
ﬁner-grained factorization of an entire distribution.
Proposition 5.1
Let B be a Bayesian network, and assume that each CPD P(X | PaX) in B is represented as
a set of rules RX. Let R be the multiset deﬁned as ⊎X∈X RX, where ⊎denotes multiset join,
which puts together all of the rule instances (including duplicates). Then, the probability of any
instantiation ξ to the network variables X can be computed as
P(ξ) =
Y
⟨c;p⟩∈R,ξ∼c
p.
The proof is left as an exercise (exercise 5.3).
The rule representation is more than a simple transformation of tree-CPDs. In particular,
although every tree-CPDs can be represented compactly as a set of rules, the converse does not
necessarily hold: not every rule-based CPD can be represented compactly as a tree.
Example 5.13
Consider the rule-based CPD of example 5.12. In any rule set that is derived from a tree, one variable
— the one at the root — appears in all rules. In the rule set R, none of the parent variables A, B, C
appears in all rules, and hence the rule set is not derived from a tree. If we try to represent it
as a tree-CPD, we would have to select one of A, B, or C to be the root. Say, for example, that
we select A to be the root. In this case, rules that do not contain A would necessarily correspond
to more than one branch (one for a1 and one for a0). Thus, the transformation would result in
more branches than rules. For example, ﬁgure 5.6 shows a minimal tree-CPD that represents the
rule-based CPD of example 5.12.

170
Chapter 5. Local Probabilistic Models
5.3.1.3
Other Representations
The tree and rule representations provide two possibilities for representing context-speciﬁc
structure. We have focused on these two approaches as they have been demonstrated to be
useful for representation, for inference, or for learning. However, other representations are also
possible, and can also be used for these tasks. In general, if we abstract away from the details
of these representations, we see that they both simply induce partitions of Val({X} ∪PaX),
deﬁned by the branches in the tree on one hand or the rule contexts on the other.
Each
partition is associated with a diﬀerent entry in X’s CPD.
This perspective allows us to understand the strengths and limitations of the diﬀerent rep-
resentations. In both trees and rules, all the partitions are described via an assignment to a
subset of the variables.
Thus, for example, we cannot represent the partition that contains
only a1, s1, l0 and a1, s0, l1, a partition that we might obtain if the recruiter lumped together
candidates that had a high SAT score or a strong recommendation letter, but not both.
As
deﬁned, these representations also require that we either split on a variable (within a branch of
the tree or within a rule) or ignore it entirely. In particular, this restriction does not allow us to
capture dependencies that utilize a taxonomic hierarchy on some parent attribute, as described
in box 5.B
Of course, we can still represent distributions with these properties by simply having multiple
tree branches or multiple rules that are associated with the same parameterization. However,
this solution both is less compact and fails to capture some aspects of the structure of the CPD.
A very ﬂexible representation, that allows these structures, might use general logical formulas to
describe partitions. This representation is very ﬂexible, and it can precisely capture any partition
we might consider; however, the formulas might get fairly complex. Somewhat more restrictive is
the use of a decision diagram, which allows diﬀerent t-nodes in a tree to share children, avoiding
decision diagram
duplication of subtrees where possible. This representation is more general than trees, in that
any structure that can be represented compactly as a tree can be represented compactly as a
decision diagram, but the converse does not hold. Decision diagrams are incomparable to rules,
in that there are examples where each is more compact than the other. In general, diﬀerent
representations oﬀer diﬀerent trade-oﬀs and might be appropriate for diﬀerent applications.
Box 5.B — Concept: Multinets and Similarity Networks. The multinet representation provides
multinet
a more global approach to capturing context-speciﬁc independence. In its simple form, a multinet
is a network centered on a single distinguished class variable C, which is a root of the network.
The multinet deﬁnes a separate network Bc for each value of C, where the structure as well as
the parameters can diﬀer for these diﬀerent networks. In most cases, a multinet deﬁnes a single
network where every variable X has as its parents C, and all variables Y in any of the networks
Bc. However, the CPD of X is such that, in context C = c, it depends only on PaBc
X . In some
cases, however, a subtlety arises, where Y is a parent of X in Bc1, and X is a parent of Y in
Bc2. In this case, the Bayesian network induced by the multinet is cyclic; nevertheless, because
of the context-speciﬁc independence properties of this network, it speciﬁes a coherent distribution.
(See also exercise 5.2.) Although, in most cases, a multinet can be represented as a standard BN
with context-speciﬁc CPDs, it is nevertheless useful, since it explicitly shows the independencies in a
graphical form, making them easier to understand and elicit.

5.3. Context-Speciﬁc CPDs
171
A related representation, the similarity network, was developed as part of the Pathﬁnder system
similarity
network
(see box 3.D).
In a similarity network, we deﬁne a network BS for certain subsets of values
S ⊂Val(C), which contains only those attributes relevant for distinguishing between the values in
S. The underlying assumption is that, if a variable X does not appear in the network BS, then
P(X | C = c) is the same for all c ∈S. Moreover, if X does not have Y as a parent in this
network, then X is contextually independent of Y given C ∈S and X’s other parents in this
network. A similarity network easily captures structure where the dependence of X on C is deﬁned
in terms of a taxonomic hierarchy on C. For example, we might have that our class variable is
Disease. While Sore-throat depends on Disease, it does not have a diﬀerent conditional distribution
for every value d of Disease. For example, we might partition diseases into diseases that do not
cause sore throat and those that do, and the latter might be further split into diﬀuse disease (causing
soreness throughout the throat) and localized diseases (such as abscesses). Using this partition, we
might have only three diﬀerent conditional distributions for P(Sore-Throat | Disease = d). Multinets
facilitate elicitation both by focusing the expert’s attention on attributes that matter, and by reducing
the number of distinct probabilities that must be elicited.
5.3.2
Independencies
In many of our preceding examples, we used phrases such as “In the case a0, where the student
does not apply, the recruiter’s decision cannot depend on the variables S and L.” These phrases
suggest that context-speciﬁc CPDs induce context-speciﬁc independence. In this section, we
analyze the independencies induced by context-speciﬁc dependency models.
Consider a CPD P(X | PaX), where certain distributions over X are shared across diﬀerent
instantiations of PaX. The structure of such a CPD allows us to infer certain independencies
locally without having to consider any global aspects of the network.
Example 5.14
Returning to example 5.7, we can see that (J ⊥c S, L | a0): By the deﬁnition of the CPD,
P(J | a0, s, l) is the same for all values of s and l. Note that this equality holds regardless of the
structure or the parameters of the network in which this CPD is embedded. Similarly, we have that
(J ⊥c L | a1, s1).
In general, if we deﬁne c to be the context associated with a branch in the tree-CPD for X,
then X is independent of the remaining parents (PaX −Scope[c]) given the context c. However,
there might be additional CSI statements that we can determine locally, conditioned on contexts
that are not induced by complete branches.
Example 5.15
Consider, the tree-CPD of ﬁgure 5.5b. Here, once George chooses to request a letter from one professor,
his job prospects still depend on the quality of that professor’s letter, but not on that of the other.
More precisely, we have that (J ⊥c L2 | c1); note that c1 is not the full assignment associated with
a branch.
Example 5.16
More interestingly, consider again the tree of ﬁgure 5.4, and suppose we are given the context s1.
Clearly, we should only consider branches that are consistent with this value. There are two such

172
Chapter 5. Local Probabilistic Models
branches. One associated with the assignment a0 and the other with the assignment a1, s1. We
can immediately see that the choice between these two branches does not depend on the value of
L. Thus, we conclude that (J ⊥c L | s1) holds in this case.
We can generalize this line of reasoning by considering the rules compatible with a particular
context c. Intuitively, if none of these rules mentions a particular parent Y of X, then X is
conditionally independent of Y given c. More generally, we can deﬁne the notion of conditioning
a rule on a context:
Deﬁnition 5.6
Let ρ = ⟨c′; p⟩be a rule and C = c be a context. If c′ is compatible with c, we say that
ρ ∼c. In this case, let c′′ = c′⟨Scope[c′] −Scope[c]⟩be the assignment in c′ to the variables in
Scope[c′] −Scope[c]. We then deﬁne the reduced rule ρ[c] = ⟨c′′; p⟩. For R a set of rules, we
reduced rule
deﬁne the reduced rule set
R[c] = {ρ[c] : ρ ∈R, ρ ∼c}.
Example 5.17
In the rule set R of example 5.12, R[a1] is the set
ρ′
1:⟨b1, x0; 0.1⟩
ρ2:⟨b1, x1; 0.9⟩
ρ5:⟨b0, c0, x0; 0.3⟩
ρ6:⟨b0, c0, x1; 0.7⟩
ρ′
7:⟨b0, c1, x0; 0.4⟩
ρ′
8:⟨b0, c1, x1; 0.6⟩.
Thus, we have left only the rules compatible with a1, and eliminated a1 from the context in the
rules where it appeared.
Proposition 5.2
Let R be the rules in the rule-based CPD for a variable X, and let Rc be the rules in R that are
compatible with c. Let Y ⊆PaX be some subset of parents of X such that Y ∩Scope[c] = ∅. If
for every ρ ∈R[c], we have that Y ∩Scope[ρ] = ∅, then (X ⊥c Y | PaX −Y , c).
The proof is left as an exercise (exercise 5.4).
This proposition speciﬁes a computational tool for deducing “local” CSI relations from the
rule representation. We can check whether a variable Y is being tested in the reduced rule
set given a context in linear time in the number of rules. (See also exercise 5.6 for a similar
procedure for trees.)
This procedure, however, is incomplete in two ways.
First, since the procedure does not
examine the actual parameter values, it can miss additional independencies that are true for
the speciﬁc parameter assignments. However, as in the case of completeness for d-separation in
BNs, this violation only occurs in degenerate cases. (See exercise 5.7.)
The more severe limitation of this procedure is that it only tests for independencies between
X and some of its parents given a context and the other parents. Are there are other, more
global, implications of such CSI relations?
Example 5.18
Consider example 5.7 again. In general, ﬁnding out that Gump got the job at Acme will increase
our belief that he is intelligent, via evidential reasoning. However, now assume that we know that
Gump did not apply. Intuitively, we now learn nothing about his intelligence from the fact that he
got the job.

5.3. Context-Speciﬁc CPDs
173
Grade
Letter
Job
(a)
(b)
Apply
SAT
Intelligence
Difﬁculty
Grade
Letter
Job
Apply
SAT
Intelligence
Difﬁculty
Figure 5.7
The graph of ﬁgure 5.3, after we remove spurious edges: (a) in the context A = a0; (b) in
the context S = s1.
Can we capture this intuition formally? Consider the dependence structure in the context
A = a0. Intuitively, in this context, the edges S →J and L →J are both redundant, since we
know that (J ⊥c S, L | a0). Thus, our intuition is that we should check for d-separation in the
graph without this edge. Indeed, we can show that this is a sound check for CSI conditions.
Deﬁnition 5.7
Let P(X | PaX) be a CPD, let Y ∈PaX, and let c be a context.
We say that the edge
Y →X is spurious in the context c if P(X | PaX) satisﬁes (X ⊥c Y | PaX −{Y }, c′), where
spurious edge
c′ = c⟨PaX⟩is the restriction of c to variables in PaX.
If we represent CPDs with rules, then we can determine whether an edge is spurious by examin-
ing the reduced rule set. Let R be the rule-based CPD for P(X | PaX), then the edge Y →X
is spurious in context c if Y does not appear in the reduced rule set R[c].
Algorithm 5.2 Computing d-separation in the presence of context-speciﬁc CPDs
Procedure CSI-sep (
G,
// Bayesian network structure
c,
// Context
X, Y , Z
// Is X CSI-separated from Y given Z, c
)
1
G′ ←G
2
for each edge Y →X in G′
3
if Y →X is spurious given c in G then
4
Remove Y →X in G′
5
return d-sepG′(X; Y | Z, c)
6
Now we can deﬁne CSI-separation, a variant of d-separation that takes CSI into account. This
CSI-separation
notion, deﬁned procedurally in algorithm 5.2, is straightforward: we use local considerations to
remove spurious edges and then apply standard d-separation to the resulting graph. We say that

174
Chapter 5. Local Probabilistic Models
Letter1
Letter2
Choice
Job
(a)
(b)
Letter1
Letter2
Choice
Job
Figure 5.8
Two reductions of the CPD for the OneLetter example:
(a) in the context C = c1; (b) in
the context C = c2.
X is CSI-separated from Y given Z in the context c if CSI-sep(G, c, X, Y , Z) returns true.
As an example, consider the network of example 5.7, in the context A = a0. In this case, we
get that the arcs S →J and L →J are spurious, leading to the reduced graph in ﬁgure 5.7a.
As we can see, J and I are d-separated in the reduced graph, as are J and D. Thus, using
CSI-sep, we get that I and J are d-separated given the context a0. Figure 5.7b shows the reduced
graph in the context s1.
It is not hard to show that CSI-separation provides a sound test for determining context-
speciﬁc independence.
Theorem 5.3
Let G be a network structure, let P be a distribution such that P |= Iℓ(G), let c be a context, and
let X, Y , Z be sets of variables. If X is CSI-separated from Y given Z in the context c, then
P |= (X ⊥c Y | Z, c).
The proof is left as an exercise (exercise 5.8).
Of course, we also want to know if CSI-separation is complete — that is, whether it discovers
all the context-speciﬁc independencies in the distribution. At best, we can hope for the same
type of qualiﬁed completeness that we had before: discovering all CSI assertions that are a
direct consequence of the structural properties of the model, regardless of the particular choice
of parameters. In this case, the structural properties consist of the graph structure (as usual)
and the structure of the rule sets or trees. Unfortunately, even this weak notion of completeness
does not hold in this case.
Example 5.19
Consider the example of ﬁgure 5.5b and the context C = c1. In this context, the arc L2 →J is
spurious. Thus, there is no path between L1 and L2, even given J. Hence, CSI-sep will report that
L1 and L2 are d-separated given J and the context C = c1. This case is shown in ﬁgure 5.8a.
Therefore, we conclude that (L1 ⊥c L2 | J, c1).
Similarly, in the context C = c2, the arc
L1 →J is spurious, and we have that L1 and L2 are d-separated given J and c2, and hence that
(L1 ⊥c L2 | J, c2). Thus, reasoning by cases, we conclude that once we know the value of C, we
have that L1 and L2 are always d-separated given J, and hence that (L1 ⊥L2 | J, C).
Can we get this conclusion using CSI-separation? Unfortunately, the answer is no. If we invoke
CSI-separation with the empty context, then no edges are spurious and CSI-separation reduces to
d-separation. Since both L1 and L2 are parents of J, we conclude that they are not separated given
J and C.

5.4. Independence of Causal Inﬂuence
175
The problem here is that CSI-separation does not perform reasoning by cases. Of course, if
we want to determine whether X and Y are independent given Z and a context c, we can
invoke CSI-separation on the context c, z for each possible value of Z, and see if X and Y
are separated in all of these contexts. This procedure, however, is exponential in the number
of variables of Z. Thus, it is practical only for small evidence sets. Can we do better than
reasoning by cases? The answer is that sometimes we cannot. See exercise 5.10 for a more
detailed examination of this issue.
5.4
Independence of Causal Inﬂuence
In this section, we describe a very diﬀerent type of structure in the local probability model.
Consider a variable Y whose distribution depends on some set of causes X1, . . . , Xk.
In
general, Y can depend on its parents in arbitrary ways — the Xi can interact with each
other in complex ways, making the eﬀect of each combination of values unrelated to any other
combination. However, in many cases, the combined inﬂuence of the Xi’s on Y is a simple
combination of the inﬂuence of each of the Xi’s on Y in isolation. In other words, each of the
Xi’s inﬂuences Y independently, and the inﬂuence of several of them is simply combined in
some way.
We begin by describing two very useful models of this type — the noisy-or model, and
the class of generalized linear models. We then provide a general deﬁnition for this type of
interaction.
5.4.1
The Noisy-Or Model
Let us begin by considering an example in which a diﬀerent professor writes a recommendation
letter for a student. Unlike our earlier example, this professor teaches a small seminar class,
where she gets to know every student. The quality of her letter depends on two things: whether
the student participated in class, for example, by asking good questions (Q); and whether he
wrote a good ﬁnal paper (F). Roughly speaking, each of these events is enough to cause the
professor to write a good letter. However, the professor might fail to remember the student’s
participation. On the other hand, she might not have been able to read the student’s handwriting,
and hence may not appreciate the quality of his ﬁnal paper. Thus, there is some noise in the
process.
Let us consider each of the two causes in isolation.
Assume that P(l1 | q1, f 0) = 0.8,
that is, the professor is 80 percent likely to remember class participation. On the other hand,
P(l1 | q0, f 1) = 0.9, that is, the student’s handwriting is readable in 90 percent of the cases.
What happens if both occur: the student participates in class and writes a good ﬁnal paper?
The key assumption is that these are two independent causal mechanisms for causing a strong
causal
mechanism
letter, and that the letter is weak only if neither of them succeeded. The ﬁrst causal mechanism
— class participation q1 — fails with probability 0.2. The second mechanism — a good ﬁnal
paper f 1 — fails with probability 0.1. If both q1 and f 1 occurred, the probability that both
mechanisms fail (independently) is 0.2 · 0.1 = 0.02. Thus, we have that P(l0 | q1, f 1) = 0.02

176
Chapter 5. Local Probabilistic Models
Questions'
FinalPaper'
Questions
FinalPaper
Letter
or
Figure 5.9
Decomposition of the noisy-or model for Letter
and P(l1 | q1, f 1) = 0.98. In other words, our CPD for P(L | Q, F) is:
Q, F
l0
l1
q0f 0
1
0
q0f 1
0.1
0.9
q1f 0
0.2
0.8
q1f 1
0.02
0.98
This type of interaction between causes is called the noisy-or model. Note that we assumed that
noisy-or CPD
a student cannot end up with a strong letter if he neither participated in class nor wrote a good
ﬁnal paper. We relax this assumption later on.
An alternative way of understanding this interaction is by assuming that the letter-writing
process can be represented by a more elaborate probabilistic model, as shown in ﬁgure 5.9.
This ﬁgure represents the conditional distribution for the Letter variable given Questions and
FinalPaper. It also uses two intermediate variables that reveal the associated causal mechanisms.
The variable Q′ is true if the professor remembers the student’s participation; the variable F ′ is
true if the professor could read and appreciate the student’s high-quality ﬁnal paper. The letter
is strong if and only if one of these events holds. We can verify that the conditional distribution
P(L | Q, F) induced by this network is precisely the one shown before.
The probability that Q causes L (0.8 in this example) is called the noise parameter, and
noise parameter
denoted λQ. In the context of our decomposition, λQ = P(q′1 | q1). Similarly, we have a noise
parameter λF , which in this context is λF = P(f ′1 | f 1).
We can also incorporate a leak probability that represents the probability — say 0.0001 —
leak probability
that the professor would write a good recommendation letter for no good reason, simply because
she is having a good day. We simply introduce another variable into the network to represent
this event. This variable has no parents, and is true with probability λ0 = 0.0001. It is also a
parent of the Letter variable, which remains a deterministic or.
The decomposition of this CPD clearly shows why this local probability model is called a
noisy-or. The basic interaction of the eﬀect with its causes is that of an OR, but there is some
noise in the “eﬀective value” of each cause.
We can deﬁne this model in the more general setting:
Deﬁnition 5.8
Let Y be a binary-valued random variable with k binary-valued parents X1, . . . , Xk. The CPD
P(Y | X1, . . . , Xk) is a noisy-or if there are k + 1 noise parameters λ0, λ1, . . . , λk such that
noisy-or CPD

5.4. Independence of Causal Inﬂuence
177
(a)
(b)
P(y1)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
l
0
5
10
15
20
# of X’s true
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0
5
10
15
20
# of X’s true
1
0.8
0.6
0.4
0.2
0
P(y1)
1
0.8
0.6
0.4
0.2
0
l
Figure 5.10
The behavior of the noisy-or model as a function of λ and the number of parents that
have value true: (a) with a leak probability of 0; (b) with a leak probability of 0.5.
P(y0 | X1, . . . , Xk)
=
(1 −λ0)
Y
i : Xi=x1
i
(1 −λi)
(5.2)
P(y1 | X1, . . . , Xk)
=
1 −[(1 −λ0)
Y
i : Xi=x1
i
(1 −λi)]
We note that, if we interpret x1
i as 1 and x0
i as 0, we can rewrite equation (5.2) somewhat more
compactly as:
P(y0 | x1, . . . , xk) = (1 −λ0)
k
Y
i=1
(1 −λi)xi.
(5.3)
Although this transformation might seem cumbersome, it will turn out to be very useful in a
variety of settings.
Figure 5.10 shows a graph of the behavior of a special-case noisy or model, where all the
variables have the same noise parameter λ. The graph shows the probability of the child Y in
terms of the parameter λ and the number of Xi’s that have the value true.
The noisy-or model is applicable in a wide variety of settings, but perhaps the most obvious is
in the medical domain. For example, as we discussed earlier, a symptom variable such as Fever
usually has a very large number of parents, corresponding to diﬀerent diseases that can cause
the symptom. However, it is often a reasonable approximation to assume that the diﬀerent
diseases use diﬀerent causal mechanisms, and that if any disease succeeds in activating its
mechanism, the symptom is present. Hence, the noisy-or model is a reasonable approximation.
Box 5.C — Concept: BN2O Networks. A class of networks that has received some attention in
the domain of medical diagnosis is the class of BN2O networks.
BN2O network
A BN2O network, illustrated in ﬁgure 5.C.1, is a two-layer Bayesian network, where the top layer
corresponds to a set of causes, such as diseases, and the second to ﬁndings that might indicate these

178
Chapter 5. Local Probabilistic Models
. . .
. . .
D2
D1
F2
F1
F3
Dn
Fm
Figure 5.C.1 — A two-layer noisy-or network
causes, such as symptoms or test results. All variables are binary-valued, and the variables in the
second layer all have noisy-or models. Speciﬁcally, the CPD of Fi is given by:
P(f 0
i | PaFi)
=
(1 −λi,0)
Y
Dj∈PaFi
(1 −λi,j)dj.
These networks are conceptually very simple and require a small number of easy-to-understand
parameters: Each edge denotes a causal association between a cause di and a ﬁnding fj; each
is associated with a parameter λi,j that encodes the probability that di, in isolation, causes fj
to manifest. Thus, these networks resemble a simple set of noisy rules, a similarity that greatly
facilitates the knowledge-elicitation task. Although simple, BN2O networks are a reasonable ﬁrst
approximation for a medical diagnosis network.
BN2O networks also have another useful property. In Bayesian networks, observing a variable
generally induces a correlation between all of its parents. In medical diagnosis networks, where
ﬁndings can be caused by a large number of diseases, this phenomenon might lead to signiﬁcant
complexity, both cognitively and in terms of inference. However, in medical diagnosis, most of the
ﬁndings in any speciﬁc case are false — a patient generally only has a small handful of symptoms.
As discussed in section 5.4.4, the parents of a noisy-or variable F are conditionally independent
given that we observe that F is false. As a consequence, a BN2O network where we observe F = f 0
is equivalent to a network where F disappears from the network entirely (see exercise 5.13). This
observation can greatly reduce the cost of inference.
5.4.2
Generalized Linear Models
An apparently very diﬀerent class of models that also satisfy independence of causal inﬂuence
are the generalized linear models. Although there are many models of this type, in this section
generalized linear
model
we focus on models that deﬁne probability distributions P(Y | X1, . . . , Xk) where Y takes on
values in some discrete ﬁnite space. We ﬁrst discuss the case where Y and all of the Xi’s are
binary-valued. We then extend the model to deal with the multinomial case.
5.4.2.1
Binary-Valued Variables
Roughly speaking, our models in this case are a soft version of a linear threshold function. As a
motivating example, we can think of applying this model in a medical setting: In practice, our
body’s immune system is constantly ﬁghting oﬀmultiple invaders. Each of them adds to the

5.4. Independence of Causal Inﬂuence
179
burden, with some adding more than others. We can imagine that when the total burden passes
some threshold, we begin to exhibit a fever and other symptoms of infection. That is, as the
total burden increases, the probability of fever increases. This requires us to clarify two terms
in this discussion. The ﬁrst is the “total burden” value and how it depends on the particular
possible disease causes. The second is a speciﬁcation of how the probability of fever depends
on the total burden.
More generally, we examine a CPD of Y given X1, . . . , Xk. We assume that the eﬀect of the
Xi’s on Y can be summarized via a linear function f(X1, . . . , Xk) = Pk
i=1 wiXi, where we
again interpret x1
i as 1 and x0
i as 0. In our example, this function will be the total burden on
the immune system, and the wi coeﬃcient describes how much burden is contributed by each
disease cause.
The next question is how the probability of Y
= y1 depends on f(X1, . . . , Xk).
In
general, this probability undergoes a phase transition around some threshold value τ: when
f(X1, . . . , Xk) ≥τ, then Y is very likely to be 1; when f(X1, . . . , Xk) < τ, then Y is very
likely to be 0. It is easier to eliminate τ by simply deﬁning f(X1, . . . , Xk) = w0 + Pk
i=1 wiXi,
so that w0 takes the role of −τ.
To provide a realistic model for immune system example and others, we do not use a hard
threshold function to deﬁne the probability of Y , but rather a smoother transition function. One
common choice (although not the only one) is the sigmoid or logit function:
sigmoid
sigmoid(z) =
ez
1 + ez .
Figure 5.11a shows the sigmoid function. This function implies that the probability saturates
to 1 when f(X1, . . . , Xk) is large, and saturates to 0 when f(X1, . . . , Xk) is small. And so,
activation of another disease cause for a sick patient will not change the probability of fever by
much, since it is already close to 1. Similarly, if the patient is healthy, a minor burden on the
immune system will not increase the probability of fever, since f(X1, . . . , Xk) is far from the
threshold. In the area of the phase transition, the behavior is close to linear.
We can now deﬁne:
Deﬁnition 5.9
Let Y be a binary-valued random variable with k parents X1, . . . , Xk that take on numerical
values. The CPD P(Y | X1, . . . , Xk) is a logistic CPD if there are k + 1 weights w0, w1, . . . , wk
logistic CPD
such that:
P(y1 | X1, . . . , Xk)
=
sigmoid(w0 +
k
X
i=1
wiXi).
We have already encountered this CPD in example 4.20, where we saw that it can be derived
by taking a naive Markov network and reformulating it as a conditional distribution.
We can interpret the parameter wi in terms of its eﬀect on the log-odds of Y . In general, the
log-odds
odds ratio for a binary variable is the ratio of the probability of y1 and the probability of y0. It
is the same concept used when we say that the odds of some event (for example, a sports team
winning the Super Bowl) are “2 to 1.” Consider the odds ratio for the variable Y , where we use
Z to represent w0 + P
i wiXi:
O(X) = P(y1 | X1, . . . , Xk)
P(y0 | X1, . . . , Xk) = eZ/(1 + eZ)
1/(1 + eZ) = eZ.

180
Chapter 5. Local Probabilistic Models
–10
–5
0
5
(a)
(b)
(c)
(d)
10
z
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
w
0
5
10
15
20
# of X’s true
sigmoid (z)
P(y1)
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
w
0
5
10
15
20
# of X’s true
P(y1)
P(y1)
1
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
w
0
5
10
15
20
# of X’s true
1
0.8
0.6
0.4
0.2
0
Figure 5.11
The behavior of the sigmoid CPD: (a) The sigmoid function. (b),(c), & (d) The behavior of
the linear sigmoid model as a function of w and the number of parents that have value true: (b) when the
threshold w0 = 0; (c) when w0 = −5; (d) when w and w0 are multiplied by 10.
Now, consider the eﬀect on this odds ratio as some variable Xj changes its value from false to
true. Let X−j be the variables in X1, . . . , Xk except for Xj. Then:
O(X−j, x1
j)
O(X−j, x0
j)
=
exp(w0+P
i̸=j wiXi+wj)
exp(w0+P
i̸=j wiXi)
= ewj.
Thus, Xj = true changes the odds ratio by a multiplicative factor of ewj. A positive coeﬃcient
wj > 0 implies that ewj > 1 so that the odds ratio increases, hence making y1 more likely.
Conversely, a negative coeﬃcient wj < 0 implies that ewj < 1 and hence the odds ratio
decreases, making y1 less likely.
Figure 5.11b shows a graph of the behavior of a special case of the logistic CPD model, where
all the variables have the same weight w. The graph shows P(Y | X1, . . . , Xk) as a function
of w and the number of Xi’s that take the value true. The graph shows two cases: one where
w0 = 0 and the other where w0 = −5. In the ﬁrst case, the probability starts out at 0.5

5.4. Independence of Causal Inﬂuence
181
P(y1)
–5
–5
5
5
–10
0
10 –10
10
0
1
0.8
0.6
0.4
0.2
0
X1
X2
Figure 5.12
A multinomial logistic CPD: The distribution of P(Y | X1, X2) using the multinomial
logistic model for ℓ1(X1, X2) = −3X1 −2X2 + 1, ℓ2(X1, X2) = 5X1 −8X2 −4, and ℓ3 = x −y + 10.
when none of the causes are in eﬀect, and rapidly goes up to 1; the rate of increase is, as
expected, much higher for high values of w. It is interesting to compare this graph to the graph
of ﬁgure 5.10b that shows the behavior of the noisy-or model with λ0 = 0.5. The graphs exhibit
very similar behavior for λ = w, showing that the incremental eﬀect of a new cause is similar in
both. However, the logistic CPD also allows for a negative inﬂuence of some Xi on Y by making
wi negative. Furthermore, the parameterization of the logistic model also provides substantially
more ﬂexibility in generating qualitatively diﬀerent distributions.
For example, as shown in
ﬁgure 5.11c, setting w0 to a diﬀerent value allows us to obtain the threshold eﬀect discussed
earlier. Furthermore, as shown in ﬁgure 5.11d, we can adapt the scale of the parameters to obtain
a sharper transition. However, the noisy-or model is cognitively very plausible in many settings.
Furthermore, as we discuss, it has certain beneﬁts both in reasoning with the models and in
learning the models from data.
5.4.2.2
Multivalued Variables
We can extend the logistic CPD to the case where Y takes on multiple values y1, . . . , ym. In
this case, we can imagine that the diﬀerent values of Y are each supported in a diﬀerent way
by the Xi’s, where the support is again deﬁned via a linear function. The choice of Y can
be viewed as a soft version of “winner takes all,” where the yi that has the most support gets
probability 1 and the others get probability 0.
More precisely, we have:
Deﬁnition 5.10
Let Y be an m-valued random variable with k parents X1, . . . , Xk that take on numerical values.
The CPD P(Y | X1, . . . , Xk) is a multinomial logistic if for each j = 1, . . . , m, there are k + 1
multinomial
logistic CPD

182
Chapter 5. Local Probabilistic Models
Z0
Zk
Z2
Z1
Z
X1
X2
Xk
Y
. . .
Figure 5.13
Independence of causal inﬂuence
weights wj,0, wj,1, . . . , wj,k such that:
ℓj(X1, . . . , Xk)
=
wj,0 +
k
X
i=1
wj,iXi
P(yj | X1, . . . , Xk)
=
exp(ℓj(X1, . . . , Xk))
Pm
j′=1 exp(ℓj′(X1, . . . , Xk)).
Figure 5.12 shows one example of this model for the case of two parents and a three-valued
child Y . We note that one of the weights wj,1, . . . , wj,k is redundant, as it can be folded into
the bias term wj,0.
We can also deal with the case where the parent variables Xi take on more than two values.
The approach taken is usually straightforward. If X=
i x1
i , . . . , xm
i , we deﬁne a new set of binary-
valued variables Xi,1, . . . , Xi,m, where Xi,j = x1
i,j precisely when Xi = j. Each of these new
variables gets its own coeﬃcient (or set of coeﬃcients) in the logistic function. For example, if
we have a binary-valued child Y with an m-valued parent X, our logistic function would be
parameterized using m + 1 weights, w0, w1, . . . , wm, such that
P(y1 | X)
=
sigmoid(w0 +
m
X
j=1
wj11{X = xj}).
(5.4)
We note that, for any assignment to Xi, precisely one of the weights w1, . . . , wm will make a
contribution to the linear function. As a consequence, one of the weights is redundant, since it
can be folded into the bias weight w0.
We noted before that we can view a binary-valued logistic CPD as a conditional version of a
naive Markov model. We can generalize this observation to the nonbinary case, and show that
the multinomial logit CPD is also a particular type of pairwise CRF (see exercise 5.16).
5.4.3
The General Formulation
Both of these models are specials case of a general class of local probability models, which
satisfy a property called causal independence or independence of causal inﬂuence (ICI). These
causal
independence

5.4. Independence of Causal Inﬂuence
183
models all share the property that the inﬂuence of multiple causes can be decomposed into
separate inﬂuences. We can deﬁne the resulting class of models more precisely as follows:
Deﬁnition 5.11
Let Y be a random variable with parents X1, . . . , Xk. The CPD P(Y | X1, . . . , Xk) exhibits
independence of causal inﬂuence if it is described via a network fragment of the structure shown
in ﬁgure 5.13, where the CPD of Z is a deterministic function f.
Intuitively, each variable Xi can be transformed separately using its own individual noise
model. The resulting variables Zi are combined using some deterministic combination function.
Finally, an additional stochastic choice can be applied to the result Z, so that the ﬁnal value
of Y is not necessarily a deterministic function of the variables Zi’s. The key here is that any
stochastic parts of the model are applied independently to each of the Xi’s, so that there can
be no interactions between them. The only interaction between the Xi’s occurs in the context
of the function f.
As stated, this deﬁnition is not particularly meaningful. Given an arbitrarily complex function
f, we can represent any CPD using the representation of ﬁgure 5.13. (See exercise 5.15.) It
is possible to place various restrictions on the form of the function f that would make the
deﬁnition more meaningful.
For our purposes, we provide a fairly stringent deﬁnition that
fortunately turns out to capture the standard uses of ICI models.
Deﬁnition 5.12
We say that a deterministic binary function x ⋄y is commutative if x ⋄y = y ⋄x, and associative
if (x ⋄y) ⋄z = x ⋄(y ⋄z). We say that a function f(x1, . . . , xk) is a symmetric decomposable
function if there is a commutative associative function x ⋄y such that f(x1, . . . , xk) = x1 ⋄x2 ⋄
· · · ⋄xk.3
Deﬁnition 5.13
We say that the CPD P(Y | X1, . . . , Xk) exhibits symmetric ICI if it is described via a network
symmetric ICI
fragment of the structure shown in ﬁgure 5.13, where the CPD of Z is a deterministic symmetric
decomposable function f. The CPD exhibits fully symmetric ICI if the CPDs of the diﬀerent Zi
variables are identical.
There are many instantiations of the symmetric ICI model, with diﬀerent noise models —
P(Zi | Xi) — and diﬀerent combination functions. Our noisy-or model uses the combination
function OR and a simple noise model with binary variables. The generalized linear models use
the Zi to produce wiXi, and then summation as the combination function f. The ﬁnal soft
thresholding eﬀect is accomplished in the distribution of Y given Z.
These types of models turn out to be very useful in practice, both because of their cognitive
plausibility and because they provide a signiﬁcant reduction in the number of parameters
required to represent the distribution. The number of parameters in the CPD is linear in the
number of parents, as opposed to the usual exponential.
Box 5.D — Case Study: Noisy Rule Models for Medical Diagnosis. As discussed in box 5.C,
noisy rule interactions such as noisy-or are a simple yet plausible ﬁrst approximation of models for
medical diagnosis. A generalization that is also useful in this setting is the noisy-max model. Like
noisy-max
3. Because ⋄is associative, the order of application of the operations does not matter.

184
Chapter 5. Local Probabilistic Models
the application of the noisy-or model for diagnosis, the parents Xi correspond to diﬀerent diseases
that the patient might have. In this case, however, the value space of the symptom variable Y can
be more reﬁned than simply {present, absent}; it can encode the severity of the symptom. Each Zi
corresponds (intuitively) to the eﬀect of the disease Xi on the symptom Y in isolation, that is, the
severity of the symptom in case only the disease Xi is present. The value of Z is the maximum of
the diﬀerent Zi’s.
Both noisy-or and noisy-max models have been used in several medical diagnosis networks.
Two of the largest are the QMR-DT (Shwe et al. 1991) and CPCS (Pradhan et al. 1994) networks,
both based on various versions of a knowledge-based system called QMR (Quick Medical Reference),
compiled for diagnosis of internal medicine. QMR-DT is a BN2O network (see box 5.C) that contains
more than ﬁve hundred signiﬁcant diseases, about four thousand associated ﬁndings, and more
than forty thousand disease-ﬁnding associations.
CPCS is a somewhat smaller network, containing close to ﬁve hundred variables and more than
nine hundred edges. Unlike QMR-DT, the network contains not only diseases and ﬁndings but also
variables for predisposing factors and intermediate physiological states. Thus, CPCS has at least
four distinct layers. All variables representing diseases and intermediate states take on one of four
values. A speciﬁcation of the network using full conditional probability tables would require close
to 134 million parameters. However, the network is constructed using only noisy-or and noisy-max
interactions, so that the number of actual parameters is only 8,254.
Furthermore, most of the
parameters were generated automatically from “frequency weights” in the original knowledge base.
Thus, the number of parameters that were, in fact, elicited during the construction of the network is
around 560.
Finally, the symmetric ICI models allow certain decompositions of the CPD that can be
exploited by probabilistic inference algorithms for computational gain, when the domain of the
variables Zi and the variable Z are reasonably small.
5.4.4
Independencies
As we have seen, structured CPDs often induce independence properties that go beyond those
represented explicitly in the Bayesian network structure. Understanding these independencies
can be useful for gaining insight into the properties of our distribution. Also, as we will see,
the additional structure can be exploited for improving the performance of various probabilistic
inference algorithms.
The additional independence properties that arise in general ICI models P(Y | X1, . . . , Xk)
are more indirect than those we have seen in the context of deterministic CPDs or tree-CPDs.
In particular, they do not manifest directly in terms of the original variables, but only if we
decompose it by adding auxiliary variables. In particular, as we can easily see from ﬁgure 5.13,
each Xi is conditionally independent of Y , and of the other Xj’s, given Zi.
We can obtain even more independencies by decomposing the CPD of Z in various ways. For
example, assume that k = 4, so that our CPD has the form P(Y | X1, X2, X3, X4). We can

5.5. Continuous Variables
185
introduce two new variables W1 and W2, such that:
W1
=
Z0 ⋄Z1 ⋄Z2
W2
=
Z3 ⋄Z4
Z
=
W1 ⋄W2
By the associativity of ⋄, the decomposed CPD is precisely equivalent to the original one. In this
CPD, we can use the results of section 5.2 to conclude, for example, that X4 is independent of
Y given W2.
Although these independencies might appear somewhat artiﬁcial, it turns out that the associ-
ated decomposition of the network can be exploited by inference algorithms (see section 9.6.1).
However, as we will see, they are only useful when the domain of the intermediate variables
(W1 and W2 in our example) are small. This restriction should not be surprising given our
earlier observation that any CPD can be decomposed in this way if we allow the Zi’s and Z to
be arbitrarily complex.
The independencies that we just saw are derived simply from the fact that the CPD of Z is
deterministic and symmetric. As in section 5.2, there are often additional independencies that are
associated with the particular choice of deterministic function. The best-known independence
of this type is the one arising for noisy-or models:
Proposition 5.3
Let P(Y | X1, . . . , Xk) be a noisy-or CPD. Then for each i ̸= j, Xi is independent of Xj given
Y = y0.
The proof is left as an exercise (exercise 5.11). Note that this independence is not derived from
the network structure via d-separation: Instantiating Y enables the v-structure between Xi and
Xj, and hence potentially renders them correlated. Furthermore, this independence is context-
speciﬁc: it holds only for the speciﬁc value Y = y0. Other deterministic functions are associated
with other context-speciﬁc independencies.
5.5
Continuous Variables
So far, we have restricted attention to discrete variables with ﬁnitely many values. In many
situations, some variables are best modeled as taking values in some continuous space. Examples
include variables such as position, velocity, temperature, and pressure. Clearly, we cannot use
a table representation in this case. One common solution is to circumvent the entire issue
by discretizing all continuous variables.
Unfortunately, this solution can be problematic in
many cases. In order to get a reasonably accurate model, we often have to use a fairly ﬁne
discretization, with tens or even hundreds of values. For example, when applying probabilistic
models to a robot navigation task, a typical discretization granularity might be 15 centimeters for
the x and y coordinates of the robot location. For a reasonably sized environment, each of these
variables might have more than a thousand values, leading to more than a million discretized
values for the robot’s position. CPDs of this magnitude are outside the range of most systems.
Furthermore, when we discretize a continuous variable we often lose much of the

structure that characterizes it. It is not generally the case that each of the million values
that deﬁnes a robot position can be associated with an arbitrary probability.
Basic

186
Chapter 5. Local Probabilistic Models
continuity assumptions that hold in almost all domains imply certain relationships that
hold between probabilities associated with “nearby” discretized values of a continuous
variable. However, such constraints are very hard to capture in a discrete distribution,
where there is no notion that two values of the variable are “close” to each other.
Fortunately, nothing in our formulation of a Bayesian network requires that we restrict at-
tention to discrete variables. Our only requirement is that the CPD P(X | PaX) represent,
for every assignment of values paX to PaX, a distribution over X. In this case, X might be
continuous, in which case the CPD would need to represent distributions over a continuum of
values; we might also have some of X’s parents be continuous, so that the CPD would also
need to represent a continuum of diﬀerent probability distributions. However, as we now show,
we can provide implicit representations for CPDs of this type, allowing us to apply all of the
machinery we developed for the continuous case as well as for hybrid networks involving both
hybrid network
discrete and continuous variables.
In this section, we describe how continuous variables can be integrated into the BN framework.
We ﬁrst describe the purely continuous case, where the CPDs involve only continuous variables,
both as parents and as children. We then examine the case of hybrid networks, which involve
both discrete and continuous variables.
There are many possible models one could use for any of these cases; we brieﬂy describe only
one prototypical example for each of them, focusing on the models that are most commonly
used. Of course, there is an unlimited range of representations that we can use: any parametric
representation for a CPD is eligible in principle. The only diﬃculty, as far as representation
is concerned, is in creating a language that allows for it. Other tasks, such as inference and
learning, are a diﬀerent issue. As we will see, these tasks can be diﬃcult even for very simple
hybrid models.
The most commonly used parametric form for continuous density functions is the Gaussian
distribution. We have already described the univariate Gaussian distribution in chapter 2. We
now describe how it can be used within the context of a Bayesian network representation.
First, let us consider the problem of representing a dependency of a continuous variable Y
on a continuous parent X. One simple solution is to decide to model the distribution of Y as
a Gaussian, whose parameters depend on the value of X. In this case, we need to have a set
of parameters for every one of the inﬁnitely many values x ∈Val(X). A common solution is
to decide that the mean of Y is a linear function of X, and that the variance of Y does not
depend on X. For example, we might have that
p(Y | x) = N (−2x + 0.9; 1) .
Example 5.20
Consider a vehicle (for example, a car) moving over time. For simplicity, assume that the vehicle is
moving along a straight line, so that its position (measured in meters) at the t’th second is described
using a single variable X(t). Let V (t) represent the velocity of the car at the kth second, measured
in meters per second. Then, under ideal motion, we would have that X(t+1) = X(t) + V (t) — if
the car is at meter #510 along the road, and its current velocity is 15 meters/second, then we expect
its position at the next second to be meter #525. However, there is invariably some stochasticity in
the motion. Hence, it is much more realistic to assert that the car’s position X(t+1) is described
using a Gaussian distribution whose mean is 525 and whose variance is 5 meters.
This type of dependence is called a linear Gaussian model. It extends to multiple continuous

5.5. Continuous Variables
187
parents in a straightforward way:
Deﬁnition 5.14
Let Y be a continuous variable with continuous parents X1, . . . , Xk. We say that Y has a linear
linear Gaussian
CPD
Gaussian model if there are parameters β0, . . . , βk and σ2 such that
p(Y | x1, . . . , xk) = N
 β0 + β1x1 + · · · + βkxk; σ2
.
In vector notation,
p(Y | x) = N

β0 + βT x; σ2
.
Viewed slightly diﬀerently, this formulation says that Y is a linear function of the variables
X1, . . . , Xk, with the addition of Gaussian noise with mean 0 and variance σ2:
Y = β0 + β1x1 + · · · + βkxk + ϵ,
where ϵ is a Gaussian random variable with mean 0 and variance σ2, representing the noise in
the system.
This simple model captures many interesting dependencies. However, there are certain facets
of the situation that it might not capture. For example, the variance of the child variable Y
cannot depend on the actual values of the parents. In example 5.20, we might wish to construct
a model in which there is more variance about a car’s future position if it is currently moving
very quickly. The linear Gaussian model cannot capture this type of interaction.
Of course, we can easily extend this model to have the mean and variance of Y depend
on the values of its parents in arbitrary way. For example, we can easily construct a richer
representation where we allow the mean of Y to be sin(x1)x2 and its variance to be (x3/x4)2.
However, the linear Gaussian model is a very natural one, which is a useful approximation in
many practical applications. Furthermore, as we will see in section 7.2, networks based on the
linear Gaussian model provide us with an alternative representation for multivariate Gaussian
distributions, one that directly reveals more of the underlying structure.
Box 5.E — Case Study: Robot Motion and Sensors. One interesting application of hybrid mod-
els is in the domain of robot localization. In this application, the robot must keep track of its
robot localization
location as it moves in an environment, and obtains sensor readings that depend on its location.
This application is an example of a temporal model, a topic that will be discussed in detail in
section 6.2; we also return to the robot example speciﬁcally in box 15.A. There are two main local
probability models associated with this application. The ﬁrst speciﬁes the robot dynamics — the
distribution over its position at the next time step L′ given its current position L and the action
taken A; the second speciﬁes the robot sensor model — the distribution over its observed sensor
reading S at the current time given its current location L.
We describe one model for this application, as proposed by Fox et al. (1999) and Thrun et al.
(2000). Here, the robot location L is a three-dimensional vector containing its X, Y coordinates
and an angular orientation θ. The action A speciﬁes a distance to travel and a rotation (oﬀset
from the current θ). The model uses the assumption that the errors in both translation and rotation
are normally distributed with zero mean. Speciﬁcally, P(L′ | L, A) is deﬁned as a product of two
independent Gaussians with cut oﬀtails, P(θ′ | θ, A) and P(X′, Y ′ | X, Y, A), whose variances

188
Chapter 5. Local Probabilistic Models
Approximated
Measured
Sonar
Laser
Sonar
Laser
0
100
200
300
400
500
0.025
0.05
0.075
0.1
0.125
Measured distance di [cm]
Probability p(di/l)
0
100
200
300
400
500
0.025
0.05
0.075
0.1
0.125
Measured distance di [cm]
(d)
(c)
0
100
200
300
400
500
0.025
0.05
0.075
0.1
0.125
Measured distance di [cm]
Probability Pm(di/l)
(b)
(a)
Probability Pu(di)
Figure 5.E.1 — Probabilistic model for robot localization track. (a) A typical “banana-shaped” dis-
tribution for the robot motion model. The ﬁgure shows the projection of the conditional distribution over
L′ = ⟨X′, Y ′, θ′⟩onto the X′, Y ′ space, given the robot’s starting position and action shown. (b) Two
distributions Pm(D | L) for the distance returned by a range sensor given that the distance to the closest
obstacle is oL = 230cm. The ﬁgure shows a distribution for both an ultrasound sensor and a laser range
ﬁnder; the laser sensor has a higher accuracy than the ultrasound sensor, as indicated by the smaller vari-
ance. (c) Two distributions Pu(D) for the distance returned by a range sensor, for an ultrasound sensor
and a laser range ﬁnder. The relatively large probability of measuring 500 centimeters owes to the fact that
the maximum range of the proximity sensors is set to 500 centimeters. Thus, this distance represents the
probability of measuring at least 500 centimeters. (d) Overall model distribution (solid line) and empirical
distribution (dashed line) of P(D | oL), for oL = 230cm for a laser sensor.

5.5. Continuous Variables
189
are proportional to the length of the motion. The robot’s conditional distribution over (X′, Y ′) is a
banana-shaped cloud (see ﬁgure 5.E.1a, where the banana shape is due to the noise in the rotation.
The sensor is generally some type of range sensor, either a sonar or a laser, which provides a
reading D of the distance between the robot and the nearest obstacle along the direction of the
sensor. There are two distinct cases to consider. If the sensor signal results from an obstacle in
the map, then the resulting distribution is modeled by a Gaussian distribution with mean at the
distance to this obstacle. Letting oL be the distance to the closest obstacle to the position L (along
the sensor beam), we can deﬁne Pm(D | L) = N
 oL; σ2
, where the variance σ2 represents the
uncertainty of the measured distance, based on the accuracy of the world model and the accuracy
of the sensor. Figure 5.E.1b shows an example of such a distribution for an ultrasound sensor and a
laser range ﬁnder. The laser sensor has a higher accuracy than the ultrasound sensor, as indicated
by the smaller variance.
The second case arises when the sensor beam is reﬂected by an obstacle not represented in the
world model (for example, a dynamic obstacle, such as a person or a chair, which is not in the
robot’s map). Assuming that these objects are equally distributed in the environment, the probability
Pu(D) of detecting an unknown obstacle at distance D is independent of the location of the robot
and can be modeled by an exponential distribution. This distribution results from the observation
that a distance d is measured if the sensor is not reﬂected by an obstacle at a shorter distance and
is reﬂected at distance d. An example exponential distribution is shown in ﬁgure 5.E.1c.
Only one of these two cases can hold for a given measurement. Thus, P(D | L) is a combi-
nation of the two distributions Pm and Pu. The combined probability P(D | L) is based on the
observation that d is measured in one of two cases:
• The sensor beam in not reﬂected by an unknown obstacle before reaching distance d, and is
reﬂected by the known obstacle at distance d (an event that happens only with some probability).
• The beam is reﬂected neither by an unknown obstacle nor by the known obstacle before reaching
distance d, and it is reﬂected by an unknown obstacle at distance d.
Overall, the probability of sensor measurements is computed incrementally for the diﬀerent distances
starting at 0cm; for each distance, we consider the probability that the sensor beam reaches the
corresponding distance and is reﬂected either by the closest obstacle in the map (along the sensor
beam) or by an unknown obstacle. Putting these diﬀerent cases together, we obtain a single distribu-
tion for P(D | L). This distribution is shown in ﬁgure 5.E.1d, along with an empirical distribution
obtained from data pairs consisting of the distance oL to the closest obstacle on the map and the
measured distance d during the typical operation of the robot.
5.5.1
Hybrid Models
We now turn our attention to models incorporating both discrete and continuous variables. We
have to address two types of dependencies: a continuous variable with continuous and discrete
parents, and a discrete variable with continuous and discrete parents.
Let us ﬁrst consider the case of a continuous child X. If we ignore the discrete parents of
X, we can simply represent the CPD of X as a linear Gaussian of X’s continuous parents. The

190
Chapter 5. Local Probabilistic Models
simplest way of making the continuous variable X depend on a discrete variable U is to deﬁne
a diﬀerent set of parameters for every value of the discrete parent. More precisely:
Deﬁnition 5.15
Let X be a continuous variable, and let U = {U1, . . . , Um} be its discrete parents and Y =
{Y1, . . . , Yk} be its continuous parents. We say that X has a conditional linear Gaussian (CLG)
conditional linear
Gaussian CPD
CPD if, for every value u ∈Val(U), we have a set of k + 1 coeﬃcients au,0, . . . , au,k and a
variance σ2
u such that
p(X | u, y) = N
 
au,0 +
k
X
i=1
au,iyi; σ2
u
!
If we restrict attention to this type of CPD, we get an interesting class of models. More precisely,
we have:
Deﬁnition 5.16
A Bayesian network is called a CLG network if every discrete variable has only discrete parents and
CLG network
every continuous variable has a CLG CPD.
Note that the conditional linear Gaussian model does not allow for continuous variables to have
discrete children. A CLG model induces a joint distribution that has the form of a mixture —
Gaussian mixture
distribution
a weighted average — of Gaussians. The mixture contains one Gaussian component for each
instantiation of the discrete network variables; the weight of the component is the probability of
that instantiation. Thus, the number of mixture components is (in the worst case) exponential
in the number of discrete network variables.
Finally, we address the case of a discrete child with a continuous parent. The simplest model
is a threshold model. Assume we have a binary discrete variable U with a continuous parent
Y . We may want to deﬁne:
P(u1) =
 0.9
y ≤65
0.05
otherwise.
Such a model may be appropriate, for example, if Y is the temperature (in Fahrenheit) and U is
the thermostat turning the heater on.
The problem with the threshold model is that the change in probability is discontinuous as a
function of Y , which is both inconvenient from a mathematical perspective and implausible in
many settings. However, we can address this problem by simply using the logistic model or its
multinomial extension, as deﬁned in deﬁnition 5.9 or deﬁnition 5.10.
Figure 5.14 shows how a multinomial CPD can be used to model a simple sensor that has
three values: low, medium and high. The probability of each of these values depends on the
value of the continuous parent Y . As discussed in section 5.4.2, we can easily accommodate
a variety of noise models for the sensor: we can make it less reliable in borderline situations
by making the transitions between regions more moderate. It is also fairly straightforward to
generalize the model to allow the probabilities of the diﬀerent values in each of the regions to
be values other than 0 or 1.
As for the conditional linear Gaussian CPD, we address the existence of discrete parents for Y
by simply introducing a separate set of parameters for each instantiation of the discrete parents.

5.6. Conditional Bayesian Networks
191
1
0
–1
0
1
P(C = low X)
P(C = medium X)
P(C = high X)
Figure 5.14
Generalized linear model for a thermostat
5.6
Conditional Bayesian Networks
The previous sections all describe various compact representations of a CPD. Another very useful
way of compactly representing a conditional probability distribution is via a Bayesian network
fragment. We have already seen one very simple example of this idea: Our decomposition of
the noisy-or CPD for the Letter variable, shown in ﬁgure 5.9. There, our decomposition used a
Bayesian network to represent the internal model of the Letter variable. The network included
explicit variables for the parents of the variable, as well as auxiliary variables that are not in
the original network. This entire network represented the CPD for Letter. In this section, we
generalize this idea to a much wider setting.
Note that the network fragment in this example is not a full Bayesian network. In particular, it
does not specify a probabilistic model — parents and a CPD — for the parent variables Questions
and FinalPaper. This network fragment speciﬁes not a joint distribution over the variables in
the fragment, but a conditional distribution of Letter given Questions and FinalPaper. More
generally, we can deﬁne the following:
Deﬁnition 5.17
A conditional Bayesian network B over Y given X is deﬁned as a directed acyclic graph G whose
conditional
Bayesian network
nodes are X ∪Y ∪Z, where X, Y , Z are disjoint. The variables in X are called inputs, the
variables in Y outputs, and the variables in Z encapsulated. The variables in X have no parents
in G. The variables in Z ∪Y are associated with a conditional probability distribution. The
network deﬁnes a conditional distribution using a chain rule:
PB(Y , Z | X) =
Y
X∈Y ∪Z
P(X | PaG
X).
The distribution PB(Y | X) is deﬁned as the marginal of PB(Y , Z | X):
PB(Y | X) =
X
Z
PB(Y , Z | X).
The conditional random ﬁeld of section 4.6.1 is the undirected analogue of this deﬁnition.
conditional
random ﬁeld
The notion of a conditional BN turns out to be useful in many settings. In particular, we can
use it to deﬁne an encapsulated CPD.

192
Chapter 5. Local Probabilistic Models
Deﬁnition 5.18
Let Y be a random variable with k parents X1, . . . , Xk.
The CPD P(Y | X1, . . . , Xk) is
an encapsulated CPD if it is represented using a conditional Bayesian network over Y given
encapsulated
CPD
X1, . . . , Xk.
At some level, it is clear that the representation of an individual CPD for a variable Y
as a conditional Bayesian network BY does not add expressive power to the model.
After
all, we could simply take the network BY and “substitute it in” for the atomic CPD P(Y |
PaY ). One key advantage of the encapsulated representation over a more explicit model is that
the encapsulation can simplify the model signiﬁcantly from a cognitive perspective. Consider
again our noisy-or model. Externally, to the rest of the network, we can still view Letter as
a single variable with its two parents: Questions and FinalPaper. All of the internal structure
is encapsulated, so that, to the rest of the network, the variable can be viewed as any other
variable. In particular, a knowledge engineer specifying the network does not have to ascribe
meaning to the encapsulated variables.
The encapsulation advantage can be even more signiﬁcant when we want to describe a
complex system where components are composed of other, lower-level, subsystems.
When
specifying a model for such a system, we would like to model each subsystem separately,
without having to consider the internal model of its lower level components.
In particular, consider a model for a physical device such as a computer; we might construct
such a model for fault diagnosis purposes. When modeling the computer, we would like to avoid
thinking about the detailed structure and fault models of its individual components, such as the
hard drive, and within the hard drive the disk surfaces, the controller, and more, each of which
has yet other components. By using an encapsulated CPD, we can decouple the model of the
computer from the detailed model of the hard drive. We need only specify which global aspects
of the computer state the hard drive behavior depends on, and which it inﬂuences. Furthermore,
we can hierarchically compose encapsulated CPDs, modeling, in turn, the hard drive’s behavior
in terms of its yet-lower-level components.
In ﬁgure 5.15 we show a simple hierarchical model for a computer system. This high-level
model for a computer, ﬁgure 5.15a, uses encapsulated CPDs for Power-Source, Motherboard, Hard-
Drive, Printer, and more. The Hard-Drive CPD has inputs Temperature, Age and OS-Status, and
the outputs Status and Full. Although the hard drive has a rich internal state, the only aspects
of its state that inﬂuence objects outside the hard drive are whether it is working properly
and whether it is full. The Temperature input of the hard drive in a computer is outside the
probabilistic model and will be mapped to the Temperature parent of the Hard-Drive variable in
the computer model. A similar mapping happens for other inputs.
The Hard-Drive encapsulated network, ﬁgure 5.15b, in turn uses encapsulated CPDs for Con-
troller, Surface1, Drive-Mechanism, and more. The hierarchy can continue as necessary. In this
case, the model for the variable Motor (in the Drive-Mechanism) is “simple,” in that none of its
CPDs are encapsulated.
One obvious observation that can be derived from looking at this example is that an en-
capsulated CPD is often appropriate for more than one variable in the model. For example,
the encapsulated CPD for the variable Surface1 in the hard drive is almost certainly the same
as the CPDs for the variables Surface2, Surface2, and Surface4. Thus, we can imagine creating
a template of an encapsulated CPD, and reusing it multiple times, for several variables in the
model. This idea forms the basis for a framework known as object-oriented Bayesian networks.
object-oriented
Bayesian network

5.7. Summary
193
Read Event
Print Event
Dust
Mouse
Temperature
OS
Printer
Fan
Write Event
Power Supply
Power Source
Warmed Up
Monitor
Motherboard
Keyboard
Age
Crash
Hard Drive
Computer
Spill
Motor
Head Crash
Head
Data Transfer
Data Access
Drive
Mechanism
Status
Connected
Controller Ok
Age
Motor
Stiction
Temperature
Age
Disk Spins
Dead
Temperature
Controller
Temperature
Age
Cable
MBR
Head Crash
Bootable
Usable
DBR
FAT
Lost Clusters
Capacity
Surface Damage
Status
Hard Drive
Used
OS-Status
Full
Drive Mechanism
Surface 1
Surface 2
Surface 3
Surface 4
Figure 5.15
Example of encapsulated CPDs for a computer system model: Four levels of a hierarchy
of encapsulated CPDs in a model of a computer system. Variables with encapsulated CPDs are shown as
rectangles, while nonhierarchical variables are ovals (as usual). Each encapsulated CPD is contained in a
box. Input variables intersect the top edge of the box, indicating the fact that their values are received from
outside the class, while output variables intersect the bottom. The rectangles representing the complex
components also have little bubbles on their borders, showing that variables are passed into and out of
those components.
5.7
Summary
In this chapter, we have shown that our ability to represent structure in the distribution does
not end at the level of the graph. In many cases, here is important structure within the CPDs
that we wish to make explicit. In particular, we discussed several important types of discrete
structured CPDs.
•
deterministic functions;
•
asymmetric, or context speciﬁc, dependencies;
•
cases where diﬀerent inﬂuences combine independently within the CPD, including noisy-or,
logistic functions, and more.
In many cases, we showed that the additional structure provides not only a more compact
parameterization, but also additional independencies that are not visible at the level of the
original graph.

194
Chapter 5. Local Probabilistic Models
As we discussed, the idea of structured CPDs is critical in the case of continuous variables,
where a table-based representation is clearly irrelevant. We discussed various representations
for CPDs in hybrid (discrete/continuous) networks, of which the most common is the linear
Gaussian representation. For this case, we showed some important connections between the
linear Gaussian representation and multivariate Gaussian distributions.
Finally, we discussed the notion of a conditional Bayesian network, which allows us to de-
compose a conditional probability distribution recursively, into another Bayesian network.
5.8
Relevant Literature
This chapter addresses the issue of modeling the probabilistic dependence of one variable on
some set of others. This issue plays a central role in both statistics and (supervised) machine
learning, where much of the work is devoted precisely to the prediction of one variable from
others. Indeed, many of the representations described in this chapter are derived from statistical
models such as decision trees or regression models. More information on these models can be
found in a variety of statistics textbooks (for example, Breiman et al. 1984; Duda et al. 2000;
Hastie et al. 2001; McCullagh and Nelder 1989).
The extension of d-separation to the case of Bayesian networks with deterministic CPDs
was done by Geiger et al. (1990). Nielsen et al. (2000) proposed a speciﬁc representation for
deterministic CPDs with particular beneﬁts for inference.
The notion of context-speciﬁc independence, which recurs throughout much of this chapter,
was ﬁrst formalized by Shimony (1991). Representations for asymmetric dependencies, which
most directly capture CSI, were a key component in Heckerman’s similarity networks and the
related multinets (Heckerman and Nathwani 1992a; Geiger and Heckerman 1996). Smith, Holtz-
man, and Matheson (1993) proposed a “conditional” version of inﬂuence diagrams, which used
many of the same concepts. There have been various proposals for local probabilistic models
that encode a mapping between context and parameters. Poole (1993a); Poole and Zhang (2003)
used a rule-based representation; Boutilier, Friedman, Goldszmidt, and Koller (1996) proposed
both the use of a a fully general partition, and the more speciﬁc tree-CPDs; and Chickering,
Heckerman, and Meek (1997) suggested the use of general DAG-structured CPDs, which allow
“paths” corresponding to diﬀerent context to “merge.” Boutilier et al. also deﬁne the notion
of CSI-separation, which extends the notion of d-separation to networks involving asymmetric
dependencies; they provide an eﬃcient algorithm for CSI-separation based on cutting spurious
arcs, and they discuss how to check for spurious arcs for various types of local probability
models.
The notion of causal independence, and speciﬁcally the noisy-or model, was proposed in-
dependently by Pearl (1986b) and by Peng and Reggia (1986). This model was subsequently
generalized to allow a variety of interactions, such as AND or MAX (Pearl 1988; Heckerman 1993;
Srinivas 1993; Pradhan et al. 1994).
Lauritzen and Wermuth (1989) introduced the notion of conditional linear Gaussian Bayesian
networks.
Shachter and Kenley (1989) used linear Gaussian dependencies in the context of
Gaussian inﬂuence diagrams, a framework that was later extended to the hybrid case Poland
(1994). Lerner et al. (2001) suggest the use of a softmax CPD as a local probability model for
discrete variables with continuous parents. Murphy (1998) and Lerner (2002) provide a good

5.9. Exercises
195
introduction to these topics.
Several papers have proposed the use of richer (semiparametric or nonparametric) models
for representing dependencies in networks involving continuous variables, including kernel es-
timators (Hofmann and Tresp 1995), neural networks (Monti and Cooper 1997), and Gaussian
processes (Friedman and Nachman 2000). These representations are generally opaque to hu-
mans, and therefore are useful only in the context of learning networks from data.
The notion of encapsulated Bayesian networks as a representation of local probability models
was introduced by Srinivas (1994) in the context of modeling hierarchically structured physical
systems.
This idea was later generalized within the framework of object-oriented Bayesian
networks by Koller and Pfeﬀer (1997), which allows the deﬁnition of a general object model
as a network fragment, in a way that encapsulates it from the rest of the model. A further
generalization was proposed by Heckerman and Meek (1997), which allows the dependencies in
the encapsulated network fragment to be oriented in the opposite direction than is implied by
the parent-child relations of the CPD.
5.9
Exercises
Exercise 5.1⋆
Prove theorem 5.1.
Exercise 5.2⋆⋆
a. Show that a multinet where each Bayesian network Bc is acyclic always deﬁnes a coherent probability
distribution — one where all of the probabilities sum to 1. Your proof should apply even when the
induced Bayesian network that contains the union of all of the edges in the networks Bc contains a
cycle.
b. Now, consider a more general case, where each variable X is associated with a rule-based CPD
P(X | PaX) (as in deﬁnition 5.5). Provide a general suﬃcient condition on the set of rule-based CPDs
that guarantee that the distribution deﬁned by the chain rule:
P(X1, . . . , Xn) =
Y
i
P(Xi | PaXi)
is coherent. Your condition should also encompass cases (including the case of multinets) where the
induced global network — one containing a directed edge Y →X whenever Y ∈PaX — is not
necessarily acyclic.
Exercise 5.3
Prove proposition 5.1.
Exercise 5.4⋆
Prove proposition 5.2.
Exercise 5.5
In this exercise, we consider the use of tree-structured local models in the undirected setting.
a. Show how we can use a structure similar to tree-CPDs to represent a factor in a Markov network. What
do the values at the leaves of such a tree represent?
b. Given a context U = u, deﬁne a simple algorithm that takes a tree factor φ(Y ) and returns the
reduced factor φ[U = u](Y −U) (see deﬁnition 4.5).

196
Chapter 5. Local Probabilistic Models
c. The preceding expression takes Y −U to be the scope of the reduced factor. In some cases it turns
out that we can further reduce the scope. Give an example and specify a general rule for when a
variable in Y −U can be eliminated from the scope of the reduced tree-factor.
Exercise 5.6
Provide an algorithm for constructing a tree-CPD that is the reduced tree reduced for a given context c. In
other words, assume you are given a tree-CPD for P(X | PaX) and a context c such that Scope[c] ⊂PaX.
Provide a linear time algorithm for constructing a tree that represents P(X | PaX, c).
Exercise 5.7
Consider a BN B with a variable X that has a tree-CPD. Assume that all of the distributions at the leaves
of X’s tree-CPD are diﬀerent. Let c ∈Val(C) for C ⊆PaX be a context, and let Z ⊆PaX. Show that
if PB |= (X ⊥c Z | PaX −Z, c), then T c does not test any variable in Z.
Exercise 5.8⋆
Prove theorem 5.3. (Hint: Use exercise 5.5 and follow the lines of the proof of theorem 4.9.)
Exercise 5.9⋆
Prove the following statement, or disprove it by ﬁnding a counterexample: CSI-separation statements are
monotonic in the context; that is, if c is an assignment to a set of variables C, and C ⊂C′, and c′ is
an assignment to C′ that is consistent with c, then if X and Y are CSI-separated given c, they are also
CSI-separated given c′.
Exercise 5.10⋆⋆
Prove that the problem of determining CSI-separation given a set of variables is NP-complete. More
precisely, we deﬁne the following decision problem:
Each instance is a graph G, where some of the variables have tree-structured CPDs of known
structure, and a query X, Y, Z. An instance is in the language if there is a z ∈Val(Z) such that
X and Y are not CSI-separated given z.
Show that this problem is NP-complete.
a. Show that this problem is in NP.
b. Provide a reduction to this problem from 3-SAT. Hint: The set Z corresponds to the propositional
variables, and an assignment z to some particular truth assignment. Deﬁne a variable Yi for the ith
clause in the 3-SAT formula. Provide a construction that contains an active path from Yi to Yi+1 in a
context z iﬀthe assignment z satisﬁes the ith clause.
Exercise 5.11
In this exercise, we consider the context-speciﬁc independencies arising in ICI models.
a. Prove proposition 5.3.
b. What context-speciﬁc independencies arise if P(Y | X1, . . . , Xk) is a noisy-and (analogous to ﬁg-
ure 5.9, but where the aggregation function is a deterministic AND)?
c. What context-speciﬁc independencies arise if P(Y | X1, . . . , Xk) is a noisy-max (where the aggre-
gation function is a deterministic max), where we assume that Y and the Xi’s take ordinal values
v1, . . . , vl?
Exercise 5.12⋆
In this exercise, we study intercausal reasoning in noisy-or models. Consider the v-structure Bayesian
network: X →Z ←Y , where X, Y , and Z are all binary random variables. Assume that the CPD for Z
is a noisy-or, as in equation (5.2). Show that this network must satisfy the explaining away property:
explaining away
P(x1 | z1) ≥P(x1 | y1, z1).

5.9. Exercises
197
Exercise 5.13
Consider a BN2O network B, as described in box 5.C, and assume we are given a negative observation
F1 = F 0
1 . Show that the the posterior distribution PB(· | F1 = F 0
1 can be encoded using another BN2O
network B′ that has the same structure as B, except that F1 is omitted from the network. Specify the
parameters of B′ in terms of the parameters of B.
Exercise 5.14⋆
Consider a BN2O network B, as described in box 5.C, and the task of medical diagnosis: Computing the
posterior probability in some set of diseases given evidence concerning some of the ﬁndings. However, we
are only interested in computing the probability of a particular subset of the diseases, so that we wish (for
reasons of computational eﬃciency) to remove from the network those disease variables that are not of
interest at the moment.
a. Begin by considering a particular variable Fi, and assume (without loss of generality) that the parents
of Fi are D1, . . . , Dk and that we wish to maintain only the parents D1, . . . , Dℓfor ℓ< k. Show
how we can construct a new noisy-or CPD for Fi that preserves the correct joint distribution over
D1, . . . , Dℓ, Fi.
b. We now remove some ﬁxed set of disease variables D from the network, executing this pruning
procedure for all the ﬁnding variables Fi, removing all parents Dj ∈D. Is this transformation exact?
In other words, if we compute the posterior probability over some variable Di ̸∈D, will we get the
correct posterior probability (relative to our original model)? Justify your answer.
Exercise 5.15⋆
Consider the symmetric ICI model, as deﬁned in deﬁnition 5.13. Show that, if we allow the domain of the
intermediate variables Zi to be arbitrarily large, we can represent any CPD P(Y | X1, . . . , Xk) using this
type of model.
Exercise 5.16
a. Consider a naive Markov model over the multivalued variables X = {X1, . . . , Xk} and Y = {Y },
with the pairwise potentials deﬁned via the following log-linear model
φi(xl
i, ym) = exp
n
wlm
i 11{Xi = xl
i, Y = ym}
o
.
Again, we have a single-node potential φ0(ym) = exp {wm
0 11{Y = ym}}. Show that the distribution
P(Y | X1, . . . , Xk) deﬁned by this model when viewed as a CRF is equivalent to the multinomial
logistic CPD of equation (5.4).
b. Determine the appropriate form of CPD for which this result holds for a CRF deﬁned in terms of a
general log-linear model, where features are not necessarily pairwise.


6
Template-Based Representations
6.1
Introduction
A probabilistic graphical model (whether a Bayesian network or a Markov network) speciﬁes a
joint distribution over a ﬁxed set X of random variables. This ﬁxed distribution is then used in
a variety of diﬀerent situations. For example, a network for medical diagnosis can be applied
to multiple patients, each with diﬀerent symptoms and diseases. However, in this example, the
diﬀerent situations to which the network is applied all share the same general structure — all
patients can be described by the same set of attributes, only the attributes’ values diﬀer across
patients. We call this type of model variable-based, since the focus of the representation is a set
of random variables.
In many domains, however, the probabilistic model relates to a much more complex space
than can be encoded as a ﬁxed set of variables. In a temporal setting, we wish to represent
distributions over systems whose state changes over time. For example, we may be monitoring
a patient in an intensive care unit. In this setting, we obtain sensor readings at regular intervals
— heart rate, blood pressure, EKG — and are interested in tracking the patient’s state over time.
As another example, we may be interested in tracking a robot’s location as it moves in the world
and gathers observations. Here, we want a single model to apply to trajectories of diﬀerent
lengths, or perhaps even inﬁnite trajectories.
An even more complex setting arises in our Genetics example; here, each pedigree (family tree)
consists of an entire set of individuals, all with their own properties. Our probabilistic model
should encode a joint distribution over the properties of all of the family members. Clearly, we
cannot deﬁne a single variable-based model that applies universally to this application: each
family has a diﬀerent family tree; the networks that represent the genetic inheritance process
within the tree have diﬀerent random variables, and diﬀerent connectivities. Yet the mechanism
by which genes are transmitted from parent to child is identical both for diﬀerent individuals
within a pedigree and across diﬀerent pedigrees.
In both of these examples, and in many others, we might hope to construct a single, com-
pact model that provides a template for an entire class of distributions from the same type:
trajectories of diﬀerent lengths, or diﬀerent pedigrees. In this chapter, we deﬁne representations
that allow us to deﬁne distributions over richly structured spaces, consisting of multiple objects,
interrelated in a variety of ways. These template-based representations have been used in two
main settings. The ﬁrst is temporal modeling, where the language of dynamic Bayesian networks
allows us to construct a single compact model that captures the properties of the system dy-

200
Chapter 6. Template-Based Representations
namics, and to produce distributions over diﬀerent trajectories. The second involves domains
such as the Genetics example, where we have multiple objects that are somehow related to each
other. Here, various languages have been proposed that allow us to produce distributions over
diﬀerent worlds, each with its own set of individuals and set of relations between them.
Once we consider higher-level representations that allow us to model objects, relations, and
probabilistic statements about those entities, we open the door to very rich and expressive
languages and to queries about concepts that are not even within the scope of a variable-based
framework. For example, in the Genetics example, our space consists of multiple people with
diﬀerent types of relationships such as Mother, Father-of, and perhaps Married. In this type
probability space, we can also express uncertainty about the identity of Michael’s father, or how
many children Great-aunt Ethel had. Thus, we may wish to construct a probability distribution
over a space consisting of distinct pedigree structures, which may even contain a varying set of
objects. As we will see, this richer modeling language will allow us both to answer new types of
queries, and to provide more informed answers to “traditional” queries.
6.2
Temporal Models
Our focus in this section is on modeling dynamic settings, where we are interested in reasoning
about the state of the world as it evolves over time. We can model such settings in terms of a
system state, whose value at time t is a snapshot of the relevant attributes (hidden or observed) of
system state
the system at time t. We assume that the system state is represented, as usual, as an assignment
of values to some set of random variables X. We use X(t)
i
to represent the instantiation of the
variable Xi at time t. Note that Xi itself is no longer a variable that takes a value; rather, it is a
template variable. This template is instantiated at diﬀerent points in time t, and each X(t)
i
is a
template variable
variable that takes a value in Val(Xi). For a set of variables X ⊆X, we use X(t1:t2) (t1 < t2)
to denote the set of variables {X(t) : t ∈[t1, t2]}. As usual, we use the notation x(t:t′) for an
assignment of values to this set of variables.
Each “possible world” in our probability space is now a trajectory: an assignment of values to
trajectory
each variable X(t)
i
for each relevant time t. Our goal therefore is to represent a joint distribution
over such trajectories. Clearly, the space of possible trajectories is a very complex probability
space, so representing such a distribution can be very diﬃcult. We therefore make a series of
simplifying assumptions that help make this representational problem more tractable.
Example 6.1
Consider a vehicle localization task, where a moving car tries to track its current location using the
data obtained from a, possibly faulty, sensor. The system state can be encoded (very simply) using
the: Location — the car’s current location, Velocity — the car’s current velocity, Weather — the
current weather, Failure — the failure status of the sensor, and Obs — the current observation. We
have one such set of variables for every point t. A joint probability distribution over all of these sets
deﬁnes a probability distribution over trajectories of the car. Using this distribution, we can answer
a variety queries, such as: Given a sequence of observations about the car, where is it now? Where
is it likely to be in ten minutes? Did it stop at the red light?

6.2. Temporal Models
201
6.2.1
Basic Assumptions
Our ﬁrst simpliﬁcation is to discretize the timeline into a set of time slices: measurements of the
time slice
system state taken at intervals that are regularly spaced with a predetermined time granularity
∆. Thus, we can now restrict our set of random variables to X (0), X (1), . . ., where X (t) are the
ground random variables that represent the system state at time t·∆. For example, in the patient
monitoring example, we might be interested in monitoring the patient’s state every second, so
that ∆= 1sec. This assumption simpliﬁes our problem from representing distributions over
a continuum of random variables to representing distributions over countably many random
variables, sampled at discrete intervals.
Consider a distribution over trajectories sampled over a preﬁx of time t = 0, . . . , T —
P(X (0), X (1), . . . , X (T )), often abbreviated P(X (0:T )). We can reparameterize the distribution
using the chain rule for probabilities, in a direction consistent with time:
P(X (0:T )) = P(X (0))
T −1
Y
t=0
P(X (t+1) | X (0:t)).
Thus, the distribution over trajectories is the product of conditional distributions, for the vari-
ables in each time slice given the preceding ones. We can considerably simplify this formulation
by using our usual tool — conditional independence assumptions. One very natural approach
is to assume that the future is conditionally independent of the past given the present:
Deﬁnition 6.1
We say that a dynamic system over the template variables X satisﬁes the Markov assumption if, for
Markov
assumption
all t ≥0,
(X (t+1) ⊥X (0:(t−1)) | X (t)).
Such systems are called Markovian.
Markovian system
The Markov assumptions states that the variables in X (t+1) cannot depend directly on variables
in X (t′) for t′ < t.
If we were to draw our dependency model as an (inﬁnite) Bayesian
network, the Markov assumption would correspond to the constraint on the graph that there
are no edges into X (t+1) from variables in time slices t −1 or earlier.
Like many other
conditional independence assumptions, the Markov assumption allows us to deﬁne a more
compact representation of the distribution:
P(X (0), X (1), . . . , X (T )) = P(X (0))
T −1
Y
t=0
P(X (t+1) | X (t)).
(6.1)
Like any conditional independence assumption, the Markov assumption may or may not be
reasonable in a particular setting.
Example 6.2
Let us return to the setting of example 6.1, but assume we had, instead, selected X = {L, O}, where
L is the location of the object and O its observed location. At ﬁrst glance, we might be tempted to
make the Markov assumption in this setting: after all, the location at time t + 1 does not appear
to depend directly on the location at time t −1. However, assuming the object’s motion is coherent,
the location at time t + 1 is not independent of the previous locations given only the location at

202
Chapter 6. Template-Based Representations
time t, because the previous locations give us information about the object’s direction of motion and
speed. By adding Velocity, we make the Markov assumption closer to being satisﬁed. If, however,
the driver is more likely to accelerate and decelerate sharply in certain types of weather (say heavy
winds), then our V, L model does not satisfy the Markov assumption relative to V ; we can, again,
make the model more Markovian by adding the Weather variable. Finally, in many cases, a sensor
failure at one point is usually accompanied with a sensor failure at nearby time points, rendering
nearby Obs variables correlated. By adding all of these variables into our state model, we deﬁne a
state space whereby the Markov assumption is arguably a reasonable approximation.
Philosophically, one might argue whether, given a suﬃciently rich description of the world state,
the past is independent of the future given the present. However, that question is not central
to the use of the Markov assumption in practice. Rather, we need only consider whether

the Markov assumption is a suﬃciently reasonable approximation to the dependencies
in our distribution.
In most cases, if we use a reasonably rich state description, the
approximation is quite reasonable.
In other cases, we can also deﬁne models that are
semi-Markov, where the independence assumption is relaxed (see exercise 6.1).
semi-Markov
Because the process can continue indeﬁnitely, equation (6.1) still leaves us with the task
of acquiring an inﬁnite set of conditional distributions, or a very large one, in the case of
ﬁnite-horizon processes. Therefore, we usually make one last simplifying assumption:
Deﬁnition 6.2
We say that a Markovian dynamic system is stationary (also called time invariant or homogeneous)
stationary
dynamical system
if P(X (t+1) | X (t)) is the same for all t. In this case, we can represent the process using a
transition model P(X ′ | X), so that, for any t ≥0,
transitional
model
P(X (t+1) = ξ′ | X (t) = ξ) = P(X ′ = ξ′ | X = ξ).
6.2.2
Dynamic Bayesian Networks
The Markov and stationarity assumptions described in the previous section allow us to represent
the probability distribution over inﬁnite trajectories very compactly: We need only represent
the initial state distribution and the transition model P(X ′ | X).
This transition model is
a conditional probability distribution, which we can represent using a conditional Bayesian
network, as described in section 5.6.
Example 6.3
Let us return to the setting of example 6.1. Here, we might want to represent the system dynamics
using the model shown in ﬁgure 6.1a, the current observation depends on the car’s location (and
the map, which is not explicitly modeled) and on the error status of the sensor. Bad weather makes
the sensor more likely to fail. And the car’s location depends on the previous position and the
velocity. All of the variables are interface variables except for Obs, since we assume that the sensor
observation is generated at each time point independently given the other variables.
This type of conditional Bayesian network is called a 2-time-slice Bayesian network (2-TBN).
Deﬁnition 6.3
A 2-time-slice Bayesian network (2-TBN) for a process over X is a conditional Bayesian network
2-TBN
over X ′ given XI, where XI ⊆X is a set of interface variables.
interface variable

6.2. Temporal Models
203
Obs0
Weather0
Velocity0
Location0
Failure0
Obs0
Weather0
Velocity0
Location0
Failure0
Obs1
Weather1
Velocity1
Location1
Failure1
Obs2
Weather2
Velocity2
Location2
Failure2
Obs'
Weather
Weather'
Velocity
Velocity'
Location
Location'
Failure
Failure'
(c) DBN unrolled over 3 steps
(b)
0
(a)
→
Time slice t
Time slice t+1
Time slice 0
Time slice 0
Time slice 1
Time slice 2
Figure 6.1
A highly simpliﬁed DBN for monitoring a vehicle: (a) the 2-TBN; (b) the time 0 network;
(c) resulting unrolled DBN over three time slices.
(a)
(b)
S
S0
S1
S2
S3
O1
O2
O3
S´
O´
Figure 6.2
HMM as a DBN: (a) The 2-TBN for a generic HMM. (b) The unrolled DBN for four time slices.
As a reminder, in a conditional Bayesian network, only the variables X ′ have parents or CPDs.
The interface variables XI are those variables whose values at time t have a direct eﬀect on the
variables at time t + 1. Thus, only the variables in XI can be parents of variables in X ′. In our
example, all variables except O are in the interface.
Overall, the 2-TBN represents the conditional distribution:
P(X ′ | X) = P(X ′ | XI) =
n
Y
i=1
P(X′
i | PaX′
i).
(6.2)
For each template variable Xi, the CPD P(X′
i | PaX′
i) is a template factor: it will be instantiated
template factor
multiple times within the model, for multiple variables X(t)
i
(and their parents).
Perhaps the simplest nontrivial example of a temporal model of this kind is the hidden Markov
hidden Markov
model
model (see section 6.2.3.1). It has a single state variable S and a single observation variable O.
Viewed as a DBN, an HMM has the structure shown in ﬁgure 6.2.

204
Chapter 6. Template-Based Representations
Example 6.4
Consider a robot moving around in a grid. Most simply, the robot is the only aspect of the world
that is changing, so that the state of the system S is simply the robot’s position. Our transition
model P(S′ | S) then represents the probability that, if the robot is in some state (position) s,
it will move to another state s′. Our task is to keep track of the robot’s location, using a noisy
sensor (for example, a sonar) whose value depends on the robot’s location. The observation model
P(O | S) tells us the probability of making a particular sensor reading o given that the robot’s
current position is s. (See box 5.E for more details on the state transition and observation models in
a real robot localization task.)
In a 2-TBN, some of the edges are inter-time-slice edges, going between time slices, whereas
inter-time-slice
edge
others are intra-time-slice edges, connecting variables in the same time slice. Intuitively, our
intra-time-slice
edge
decision of how to relate two variables depends on how tight the coupling is between them. If
the eﬀect of one variable on the other is immediate — much shorter than the time granularity in
the model — the inﬂuence would manifest (roughly) within a time slice. If the eﬀect is slightly
longer-term, the inﬂuence manifests from one time slice to the next. In our simple examples,
the eﬀect on the observations is almost immediate, and hence is modeled as an intra-time-slice
edge, whereas other dependencies are inter-time-slice. In other examples, when time slices have
a coarser granularity, more eﬀects might be short relative to the length of the time slice, and so
we might have other dependencies that are intra-time-slice.
Many of the inter-time-slice edges are of the form X →X′. Such edges are called persistence
persistence edge
edges, and they represent the tendency of the variable X (for example, sensor failure) to persist
over time with high probability. A variable X for which we have an edge X →X′ in the 2-TBN
is called a persistent variable.
persistent
variable
Based on the stationarity property, a 2-TBN deﬁnes the probability distribution P(X (t+1) |
X (t)) for any t. Given a distribution over the initial states, we can unroll the network over
sequences of any length, to deﬁne a Bayesian network that induces a distribution over trajectories
of that length. In these networks, all the copies of the variable X(t)
i
for t > 0 have the same
dependency structure and the same CPD. Figure 6.1 demonstrates a transition model, initial
state network, and a resulting unrolled DBN, for our car example.
Deﬁnition 6.4
A dynamic Bayesian network (DBN) is a pair ⟨B0, B→⟩, where B0 is a Bayesian network over X (0),
dynamic
Bayesian network
representing the initial distribution over states, and B→is a 2-TBN for the process. For any desired
time span T ≥0, the distribution over X (0:T ) is deﬁned as a unrolled Bayesian network, where,
unrolled Bayesian
network
for any i = 1, . . . , n:
• the structure and CPDs of X(0)
i
are the same as those for Xi in B0,
• the structure and CPD of X(t)
i
for t > 0 are the same as those for X′
i in B→.
Thus, we can view a DBN as a compact representation from which we can generate an inﬁnite
set of Bayesian networks (one for every T > 0).
Figure 6.3 shows two useful classes of DBNs that are constructed from HMMs. A factorial
factorial HMM
HMM, on the left, is a DBN whose 2-TBN has the structure of a set of chains Xi →X′
i
(i = 1, . . . , n), with a single (always) observed variable Y ′, which is a child of all the variables
X′
i. This type of model is very useful in a variety of applications, for example, when several
sources of sound are being heard simultaneously through a single microphone. A coupled HMM,
coupled HMM

6.2. Temporal Models
205
(a)
(b)
X1
X2
X3
X '
X1
X '
X2
X '
Y'
Y'
1
2
Y'3
Y '
X3
1
2
3
X4
X'1
X'2
X'3
X'4
Figure 6.3
Two classes of DBNs constructed from HMMs: (a) A factorial HMM. (b) A coupled HMM.
on the right, is also constructed from a set of chains Xi, but now, each chain is an HMM
with its private own observation variable Yi. The chains now interact directly via their state
variables, with each chain aﬀecting its adjacent chains. These models are also useful in a variety
of applications. For example, consider monitoring the temperature in a building over time (for
example, for ﬁre alarms). Here, Xi might be the true (hidden) state of the ith room, and Yi the
value returned by the room’s own temperature sensor. In this case, we would expect to have
interactions between the hidden states of adjacent rooms.
In DBNs, it is often the case that our observation pattern is constant over time. That is, we
can partition the variables X into disjoint subsets X and O, such that the variables in X(t)
are always hidden and O(t) are always observed. For uniformity of presentation, we generally
make this assumption; however, the algorithms we present also apply to the more general case.
A DBN can enable fairly sophisticated reasoning patterns.
Example 6.5
By explicitly encoding sensor failure, we allow the agent to reach the conclusion that the sensor
has failed. Thus, for example, if we suddenly get a reading that tells us something unexpected, for
example, the car is suddenly 15 feet to the left of where we thought it was 0.1 seconds ago, then
in addition to considering the option that the car has suddenly teleported, we will also consider
the option that the sensor has simply failed. Note that the model only considers options that are
built into it. If we had no “sensor failure” variable, and had the sensor reading depend only on the
current location, then the diﬀerent sensor readings would be independent given the car’s trajectory,
so that there would be no way to explain correlations of unexpected sensor readings except via the
trajectory. Similarly, if the system knows (perhaps from a weather report or from prior observations)
that it is raining, it will expect the sensor to be less accurate, and therefore be less likely to believe
that the car is out of position.

206
Chapter 6. Template-Based Representations
Box 6.A — Case Study: HMMs and Phylo-HMMs for Gene Finding. HMMs are a primary tool
in algorithms that extract information from biological sequences. Key applications (among many)
include: modeling families of related proteins within and between organisms, ﬁnding genes in DNA
sequences, and modeling the correlation structure of the genetic variation between individuals in a
population. We describe the second of these applications, as an illustration of the methods used.
The DNA of an organism is composed of two paired helical strands consisting of a long sequence
of nucleotides, each of which can take on one of four values — A,C,G,T; in the double helix structure,
A is paired with T and C is paired with G, to form a base pair. The DNA sequence consists of multiple
regions that can play diﬀerent roles. Some regions are genes, whose DNA is transcribed into mRNA,
some of which is subsequently translated into protein. In the translation process, triplets of base
pairs, known as codons, are converted into amino acids. There are 43 = 64 diﬀerent codons,
but only 20 diﬀerent amino acids, so that the code is redundant. Not all transcribed regions are
necessarily translated. Genes can contain exons, which are translated, and introns, which are spliced
out during the translation process. The DNA thus consists of multiple genes that are separated by
intergenic regions; and genes are themselves structured, consisting of multiple exons separated by
introns. The sequences in each of these regions is characterized by certain statistical properties;
for example, a region that produces protein has a very regular codon structure, where the codon
triplets exhibit the usage statistics of the amino acids they produce. Moreover, boundaries between
these regions are also often demarcated with sequence elements that help the cell determine where
transcription should begin and end, and where translation ought to begin and end. Nevertheless,
the signals in the sequence are not always clear, and therefore identifying the relevant sequence
units (genes, exons, and more) is a diﬃcult task.
HMMs are a critical tool in this analysis. Here, we have a hidden state variable for each base pair,
which denotes the type of region to which this base pair belongs. To satisfy the Markov assumption,
one generally needs to reﬁne the state space. For example, to capture the codon structure, we
generally include diﬀerent hidden states for the ﬁrst, second, and third base pairs within a codon.
This larger state space allows us to encode the fact that coding regions are sequences of triplets of
base pairs, as well as encode the diﬀerent statistical properties of these three positions. We can
further reﬁne the state space to include diﬀerent statistics for codons in the ﬁrst exon and in the
last exon in the gene, which can exhibit diﬀerent characteristics than exons in the middle of the
gene. The observed state of the HMM naturally includes the base pair itself, with the observation
model reﬂecting the diﬀerent statistics of the nucleotide composition of the diﬀerent regions. It can
also include other forms of evidence, such as the extent to which measurements of mRNA taken
from the cell have suggested that a particular region is transcribed. And, very importantly, it can
contain evidence regarding the conservation of a base pair across other species. This last key piece
of evidence derives from the fact that base pairs that play a functional role in the cell, such as those
that code for protein, are much more likely to be conserved across related species; base pairs that
are nonfunctional, such as most of those in the intergenic regions, evolve much more rapidly, since
they are not subject to selective pressure. Thus, we can use conservation as evidence regarding the
role of a particular base pair.
One way of incorporating the evolutionary model more explicitly into the model is via a phylo-
phylogenetic
HMM
genetic HMM (of which we now present a simpliﬁed version). Here, we encode not a single DNA
sequence, but the sequences of an entire phylogeny (or evolutionary tree) of related species. We

6.2. Temporal Models
207
let Xk,i be the ith nucleotide for species sk. We also introduce a species-independent variable
Yi denoting the functional role of the ith base pair (intergenic, intron, and so on). The base pair
Xk,i will depend on the corresponding base pair Xℓ,i where sℓis the ancestral species from which
sk evolved. The parameters of this dependency will depend on the evolutionary distance between
sk and sl (the extent to which sk has diverged) and on the rate at which a base pair playing a
particular role evolves. For example, as we mentioned, a base pair in an intergenic region generally
evolves much faster than one in a coding region. Moreover, the base pair in the third position in a
codon also often evolves more rapidly, since this position encodes most of the redundancy between
codons and amino acids, and so allows evolution without changing the amino acid composition.
Thus, overall, we deﬁne Xk,i’s parents in the model to be Yi (the type of region in which Xk,i
resides), Xk,i−1 (the previous nucleotide in species s) and Xℓ,i (the ith nucleotide in the parent
species sℓ). This model captures both the correlations in the functional roles (as in the simple gene
ﬁnding model) and the fact that evolution of a particular base pair can depend on the adjacent
base pairs. This model allows us to combine information from multiple species in order to infer
which are the regions that are functional, and to suggest a segmentation of the sequence into its
constituent units.
Overall, the structure of this model is roughly a set of trees connected by chains: For each i we
have a tree over the variables {Xk,i}sk, where the structure of the tree is that of the evolutionary
tree; in addition, all of the Xk,i are connected by chains to Xk,i+1; ﬁnally, we also have the
variables Yi, which also form a chain and are parents of all of the Xk,i.
Unfortunately, the
structure of this model is highly intractable for inference, and requires the use of approximate
inference methods; see exercise 11.29.
6.2.3
State-Observation Models
An alternative way of thinking about a temporal process is as a state-observation model. In a state-
state-observation
model
observation model, we view the system as evolving naturally on its own, with our observations
of it occurring in a separate process. This view separates out the system dynamics from our
observation model, allowing us to consider each of them separately. It is particularly useful
when our observations are obtained from a (usually noisy) sensor, so that it makes sense to
model separately the dynamics of the system and our ability to sense it.
A state-observation model utilizes two independence assumptions: that the state variables
evolve in a Markovian way, so that
(X(t+1) ⊥X(0:(t−1)) | X(t));
and that the observation variables at time t are conditionally independent of the entire state
sequence given the state variables at time t:
(O(t) ⊥X(0:(t−1)), X(t+1:∞) | X(t)).
We now view our probabilistic model as consisting of two components: the transition model,
transition model
P(X′ | X), and the observation model, P(O | X).
observation
model
From the perspective of DBNs, this type of model corresponds to a 2-TBN structure where the
observation variables O′ are all leaves, and have parents only in X′. This type of situation arises

208
Chapter 6. Template-Based Representations
S1
0.3
0.5
0.7
0.4
0.5
0.6
0.9
0.1
S2
S3
S4
Figure 6.4
A simple 4-state HMM
quite naturally in a variety of real-world systems, where we do not have direct observations of
the system state, but only access to a set of (generally noisy) sensors that depend on the state.
The sensor observations do not directly eﬀect the system dynamics, and therefore are naturally
viewed as leaves.
We note that we can convert any 2-TBN to a state-observation representation as follows: For
any observed variable Y that does not already satisfy the structural restrictions, we introduce a
new variable ˜Y whose only parent is Y , and that is deterministically equal to Y . Then, we view
Y as being hidden, and we interpret our observations of Y as observations on ˜Y . In eﬀect,
we construct ˜Y to be a perfectly reliable sensor of Y . Note, however, that, while the resulting
transformed network is probabilistically equivalent to the original, it does obscure structural
independence properties of the network (for example, various independencies given that Y is
observed), which are now apparent only if we account for the deterministic dependency between
Y and ˜Y .
It is often convenient to view a temporal system as a state-observation model, both because
it lends a certain uniformity of notation to a range of diﬀerent systems, and because the state
transition and observation models often induce diﬀerent computational operations, and it is
convenient to consider them separately.
State-observation models encompass two important architectures that have been used in a
wide variety of applications: hidden Markov models and linear dynamical systems. We now
brieﬂy describe each of them.
6.2.3.1
Hidden Markov Models
A hidden Markov model, which we illustrated in ﬁgure 6.2, is the simplest example of a state-
hidden Markov
model
observation model.
While an HMM is a special case of a simple DBN, it is often used to
encode structure that is left implicit in the DBN representation. Speciﬁcally, the transition model
P(S′ | S) in an HMM is often assumed to be sparse, with many of the possible transitions having
zero probability. In such cases, HMMs are often represented using a diﬀerent graphical notation,
which visualizes this sparse transition model. In this representation, the HMM transition model
is encoded using a directed (generally cyclic) graph, whose nodes represent the diﬀerent states of
the system, that is, the values in Val(S). We have a directed arc from s to s′ if it is possible to
transition from s to s′ — that is, P(s′ | s) > 0. The edge from s to s′ can also be annotated
with its associated transition probability P(s′ | s).
Example 6.6
Consider an HMM with a state variable S that takes 4 values s1, s2, s3, s4, and with a transition

6.2. Temporal Models
209
model:
s1
s2
s3
s4
s1
0.3
0.7
0
0
s2
0
0
0.4
0.6
s3
0.5
0
0
0.5
s4
0
0.9
0
0.1
where the rows correspond to states s and the columns to successor states s′ (so that each row must
sum to 1). The transition graph for this model is shown in ﬁgure 6.4.
Importantly, the transition graph for an HMM is a very diﬀerent entity from the graph encoding
a graphical model. Here, the nodes in the graph are state, or possible values of the state variable;
the directed edges represent possible transitions between the states, or entries in the CPD that
have nonzero probability. Thus, the weights of the edges leaving a node must sum to 1. This
graph representation can also be viewed as probabilistic ﬁnite-state automaton. Note that this
probabilistic
ﬁnite-state
automaton
graph-based representation does not encode the observation model of the HMM. In some cases,
the observation model is deterministic, in that, for each s, there is a single observation o for
which P(o | s) = 1 (although the same observation can arise in multiple states). In this case,
the observation is often annotated on the node associated with the state.
It turns out that HMMs, despite their simplicity, are an extremely useful architecture. For
example, they are the primary architecture for speech recognition systems (see box 6.B) and for
many problems related to analysis of biological sequences (see, for example, box 6.A). Moreover,
these applications and others have inspired a variety of valuable generalizations of the basic
HMM framework (see, for example, Exercises 6.2–6.5).
Box 6.B — Case Study: HMMs for Speech Recognition. Hidden Markov models are currently
the key technology in all speech- recognition systems. The HMM for speech is composed of three
speech
recognition
distinct layers: the language model, which generates sentences as sequences of words; the word
language model
model, where words are described as a sequence of phonemes; and the acoustic model, which
shows the progression of the acoustic signal through a phoneme.
At the highest level, the language model represents a probability distribution over sequences of
words in the language. Most simply, one can use a bigram model, which is a Markov model over
words, deﬁned via a probability distribution P(Wi | Wi−1) for each position i in the sentence.
We can view this model as a Markov model where the state is the current word in the sequence.
(Note that this model does not take into account the actual position in the sentence, so that
P(Wi | Wi−1) is the same for all i > 1.) A somewhat richer model is the trigram model, where
the states correspond to pairs of successive words in the sentence, so that our model deﬁnes a
probability distribution P(Wi | Wi−1, Wi−2). Both of these distributions deﬁne a ridiculously
naive model of language, since they only capture local correlations between neighboring words,
with no attempt at modeling global coherence. Nevertheless, these models prove surprisingly hard
to beat, probably because they are quite easy to train robustly from the (virtually unlimited amounts
of) available training data, without the need for any manual labeling.
The middle layer describes the composition of individual words in terms of phonemes — basic
phonetic units corresponding to distinct sounds. These units vary not just on the basic sound uttered

210
Chapter 6. Template-Based Representations
S1
S2
S4
S6
S5
S3
S7
Figure 6.B.1 — A phoneme-level HMM for a fairly complex phoneme.
(“p” versus “b”), but also on whether the sound is breathy, aspirated, nasalized, and more. There
is an international agreement on an International Phonetic Alphabet, which contains about 100
phonemes. Each word is modeled as a sequence of phonemes. Of course, a word can have multiple
diﬀerent pronunciations, in which case it corresponds to several such sequences.
At the acoustic level, the acoustic signal is segmented into short time frames (around 10–25ms).
A given phoneme lasts over a sequence of these partitions. The phoneme is also not homogenous.
Diﬀerent acoustics are associated with its beginning, its middle, and its end. We thus create an
HMM for each phoneme, with its hidden variable corresponding to stages in the expression of
the phoneme. HMMs for phonemes are usually quite simple, with three states, but can get more
complicated, as in ﬁgure 6.B.1. The observation represents some set of features extracted from the
acoustic signal; the feature vector is generally either discretized into a set of bins or treated as a
continuous observation with a Gaussian or mixture of Gaussian distribution.
Given these three models, we can put them all together to form a single huge hierarchical HMM
hierarchical HMM
that deﬁnes a joint probability distribution over a state space encompassing words, phonemes, and
basic acoustic units. In a bigram model, the states in the space have the form (w, i, j), where
w is the current word, i is a phoneme within that word, and j is an acoustic position within
that phoneme. The sequence of states corresponding to a word w is governed by a word-HMM
representing the distribution over pronunciations of w. This word-HMM has a start-state and an
end-state. When we exit from the end-state of the HMM for one word w, we branch to the start-state
of another word w′ with probability P(w′ | w). Each sequence is thus a trajectory through acoustic
HMMs of individual phonemes, transitioning from the end-state of one phoneme’s HMM to the
start-state of the next phoneme’s HMM.
A hierarchical HMM can be converted into a DBN, whose variables represent the states of the
diﬀerent levels of the hierarchy (word, phoneme, and intraphone state), along with some auxiliary
variables to capture the “control architecture” of the hierarchical HMM; see exercise 6.5. The DBN
formulation has the beneﬁt of being a much more ﬂexible framework in which to introduce exten-
sions to the model. One extension addresses the coarticulation problem, where the proximity of
one phoneme changes the pronunciation of another. Thus, for example, the last phoneme in the
word “don’t” sounds very diﬀerent if the word after it is “go” or if it is “you.” Similarly, we often
pronounce “going to” as “gonna.” The reason for coarticulation is the fact that a person’s speech
articulators (such as the tongue or the lips) have some inertia and therefore do not always move
all the way to where they are supposed to be. Within the DBN framework, we can easily solve this
problem by introducing a dependency of the pronunciation model for one phoneme on the value of
the preceding phoneme and the next one. Note that “previous” and “next” need to be interpreted

6.2. Temporal Models
211
with care: These are not the values of the phoneme variable at the previous or next states in the
HMM, which are generally exactly the same as the current phoneme; rather, these are the values of
the variables prior to the previous phoneme change, and following the next phoneme change. This
extension gives rise to a non-Markovian model, which is more easily represented as a structured
graphical model. Another extension that is facilitated by a DBN structure is the introduction of
variables that denote states at which a transition between phonemes occurs. These variables can
then be connected to observations that are indicative of such a transition, such as a signiﬁcant
change in the spectrum. Such features can also be incorporated into the standard HMM model, but
it is di￿cult to restrict the model so that these features a￿ect only our beliefs in phoneme transitions.
Finally, graphical model structure has also been used to model the structure in the Gaussian
distribution over the acoustic signal features given the state. Here, two “traditional” models are:
a diagonal Gaussian over the features, a model that generally loses many important correlations
between the features; and a full covariance Gaussian, a model that requires many parameters and
is hard to estimate from data (especially since the Gaussian is di￿erent for every state in the HMM).
As we discuss in chapter 7, graphical models provide an intermediate point along the spectrum: we
can use a Gaussian graphical model that captures the most important of the correlations between
the features. The structure of this Gaussian can be learned from data, allowing a ﬂexible trade-o￿
to be determined based on the available data.
6.2.3.2
Linear Dynamical Systems
Another very useful temporal model is a linear dynamical system, which represents a system
linear dynamical
system
of one or more real-valued variables that evolve linearly over time, with some Gaussian noise.
Such systems are also often called Kalman ﬁlters, after the algorithm used to perform tracking.
Kaman ﬁlter
A linear dynamical system can be viewed as a dynamic Bayesian network where the
variables are all continuous and all of the dependencies are linear Gaussian.
Linear dynamical systems are often used to model the dynamics of moving objects and to
track their current positions given noisy measurements. (See also box 15.A.)
Example 6.7
Recall example 5.20, where we have a (vector) variable X denoting a vehicle’s current position
(in each relevant dimension) and a variable V denoting its velocity (also in each dimension).
As we discussed earlier, a ﬁrst level approximation may be a model where P(X￿| X, V ) =
N
￿
X + V ∆; σ2
X
￿
and P(V ￿| V ) = N
￿
V ; σ2
V
￿
(where ∆, as before, is the length of our time
slice). The observation — for example, a GPS signal measured from the car — is a noisy Gaussian
measurement of X.
These systems and their extensions are at the heart of most target tracking systems, for example,
tracking airplanes in an air tra￿c control system using radar data.
Traditionally, linear dynamical systems have not been viewed from the perspective of factor-
ized representations of the distribution. They are traditionally represented as a state-observation
model, where the state and observation are both vector-valued random variables, and the transi-
tion and observation models are encoded using matrices. More precisely, the model is generally

212
Chapter 6. Template-Based Representations
deﬁned via the following set of equations:
P(X(t) | X(t−1))
=
N

AX(t−1); Q

,
(6.3)
P(O(t) | X(t))
=
N

HX(t); R

,
(6.4)
where: X is an n-vector of state variables, O is an m-vector of observation variables, A is an
n × n matrix deﬁning the linear transition model, Q is an n × n matrix deﬁning the Gaussian
noise associated with the system dynamics, H is an n×m matrix deﬁning the linear observation
model, and R is an m×m matrix deﬁning the Gaussian noise associated with the observations.
This type of model encodes independence structure implicitly, in the parameters of the matrices
(see exercise 7.5).
There are many interesting generalizations of the basic linear dynamical system, which can
also be placed within the DBN framework. For example, a nonlinear variant, often called an
extended Kalman ﬁlter, is a system where the state and observation variables are still vectors of
extended Kalman
ﬁlter
real numbers, but where the state transition and observation models can be nonlinear functions
rather than linear matrix multiplications as in equation (6.3) and equation (6.4). Speciﬁcally, we
usually write:
P(X(t) | X(t−1))
=
f(X(t−1), U (t−1))
P(O(t) | X(t))
=
g(X(t), W (t)),
where f and g are deterministic nonlinear functions, and U (t), W (t) are Gaussian random
variables that explicitly encode the noise in the transition and observation models, respectively.
In other words, rather than model the system in terms of stochastic CPDs, we use an equivalent
representation that partitions the model into a deterministic function and a noise component.
Another interesting class of models are systems where the continuous dynamics are linear, but
that also include discrete variables. For example, in our tracking example, we might introduce
a discrete variable that denotes the driver’s target lane in the freeway: the driver can stay in
her current lane, or she can switch to a lane on the right or a lane on the left. Each of these
discrete settings leads to diﬀerent dynamics for the vehicle velocity, in both the lateral (across
the road) and frontal velocity.
Systems that model such phenomena are called switching linear dynamical system (SLDS). In
switching linear
dynamical system
such models, we system can switch between a set of discrete modes. While within a ﬁxed mode,
the system evolves using standard linear (or nonlinear) Gaussian dynamics, but the equations
governing the dynamics are diﬀerent in diﬀerent modes. We can view this type of system as
a DBN by including a discrete variable D that encodes the mode, where PaD′ = {D}, and
allowing D to be the parent of (some of) the continuous variables in the model, so that they
use a conditional linear Gaussian CPDs.
6.3
Template Variables and Template Factors
Having seen one concrete example of a template-based model, we now describe a more general
framework that provides the fundamental building blocks for a template-based model.
This
framework provides a more formal perspective on the temporal models of the previous section,
and a sound foundation for the richer models in the remaining sections of this chapter.

6.3. Template Variables and Template Factors
213
Template Attributes
The key concept in the deﬁnition of the models we describe in this
chapter is that of a template that is instantiated many times within the model. A template for
a random variable allows us to encode models that contain multiple random variables with the
same value space and the same semantics. For example, we can have a Blood-Type template,
which has a particular value space (say, A, B, AB, or O) and is reused for multiple individuals
within a pedigree. That is, when reasoning about a pedigree, as in box 3.B, we would want to
have multiple instances of blood-type variables, such as Blood-Type(Bart) or Blood-Type(Homer).
We use the word attribute or template variable to distinguish a template, such as Blood-Type,
attribute
template variable
from a speciﬁc random variable, such as Blood-Type(Bart).
In a very diﬀerent type of example, we can have a template attribute Location, which can be
instantiated to produce random variables Location(t) for a set of diﬀerent time points t. This
type of model allows us to represent a joint distribution over a vehicle’s position at diﬀerent
points in time, as in the previous section.
In these example, the template was a property of a single object — a person. More broadly,
attributes may be properties of entire tuples of objects. For example, a student’s grade in a
course is associated with a student-course pair; a person’s opinion of a book is associated with
the person-book pair; the aﬃnity between a regulatory protein in the cell and one of its gene
targets is also associated with the pair. More speciﬁcally, in our Student example, we want
to have a Grade template, which we can instantiate for diﬀerent (student,course) pairs s, c to
produce multiple random variables Grade(s, c), such as Grade(George, CS101).
Because many domains involve heterogeneous objects, such as courses and students, it is
convenient to view the world as being composed of a set of objects. Most simply, objects can be
object
divided into a set of mutually exclusive and exhaustive classes Q = Q1, . . . , Qk. In the Student
class
scenario, we might have a Student class and a Course class.
Attributes have a tuple of arguments, each of which is associated with a particular class of
argument
objects. This class deﬁnes the set of objects that can be used to instantiate the argument in
a given domain. For example, in our Grade template, we have one argument S that can be
instantiated with a “student” object, and another argument C that can be instantiated with a
“course” object. Template attributes thus provide us with a “generator” for random variables in
a given probability space.
Deﬁnition 6.5
An attribute A is a function A(U1, . . . , Uk), whose range is some set Val(A) and where each argu-
attribute
ment Ui is a typed logical variable associated with a particular class Q[Ui]. The tuple U1, . . . , Uk
logical variable
is called the argument signature of the attribute A, and denoted α(A).
argument
signature
From here on, we assume without loss of generality that each logical variable Ui is uniquely
associated with a particular class Q[Ui]; thus, any mention of a logical variable Ui uniquely
speciﬁes the class over which it ranges.
For example, the argument signature of the Grade attribute would have two logical variables
S, C, where S is of class Student and C is of class Course. We note that the classes associated
with an attribute’s argument signature are not necessarily distinct. For example, we might have
a binary-valued Cited attribute with argument signature A1, A2, where both are of type Article.
We assume, for simplicity of presentation, that attribute names are uniquely deﬁned; thus, for
example, the attribute denoting the age for a person will be named diﬀerently from the attribute
denoting the age for a car.
This last example demonstrates a basic concept in this framework: that of a relation.
A
relation

214
Chapter 6. Template-Based Representations
relation is a property of a tuple of objects, which tells us whether the objects in this tuple satisfy
a certain relationship with each other. For example, Took-Course is a relation over student-course
object pairs s, c, which is true is student s took the course c. As another example Mother is a
relation between person-person pairs p1, p2, which is true if p1 is the mother of p2. Relations
are not restricted to involving only pairs of objects. For example, we can have a Go relation,
which takes triples of objects — a person, a source location, and a destination location.
At some level, a relation is simply a binary-valued attribute, as in our Cited example. However,
this perspective obscures the fundamental property of a relation — that it relates a tuple of
objects to each other. Thus, when we introduce the notion of probabilistic dependencies that
can occur between related objects, the presence or absence of a relation between a pair of
objects will play a central role in deﬁning the probabilistic dependency model.
Instantiations
Given a set of template attributes, we can instantiate them in diﬀerent ways,
to produce probability spaces with multiple random variables of the same type. For example,
we can consider a particular university, with a set of students and a set of courses, and
use the notion of a template attribute to deﬁne a probability space that contains a random
variable Grade(s, c) for diﬀerent (student,course) pairs s, c. The resulting model encodes a joint
distribution over the grades of multiple students in multiple courses. Similarly, in a temporal
model, we can have the template attribute Location(T); we can then select a set of relevant time
points and generate a trajectory with speciﬁc random variables Location(t)
To instantiate a set of template attributes to a particular setting, we need to deﬁne a set
of objects for each class in our domain. For example, we may want to take a particular set
of students and set of courses and deﬁne a model that contains a ground random variable
Intelligence(s) and SAT(s) for every student object s, a ground random variable Diﬃculty(c) for
every course object c, and ground random variables Grade(s, c) and Satisfaction(s, c) for every
valid pair of (student,course) objects.
More formally, we now show how a set of template attributes can be used to generate an
inﬁnite set of probability spaces, each involving instantiations of the template attributes induced
by some set of objects. We begin with a simple deﬁnition, deferring discussion of some of the
more complicated extensions to section 6.6.
Deﬁnition 6.6
Let Q be a set of classes, and ℵa set of template attributes over Q. An object skeleton κ speciﬁes
object skeleton
a ﬁxed, ﬁnite set of objects Oκ[Q] for every Q ∈Q. We also deﬁne
Oκ[U1, . . . , Uk] = Oκ[Q[U1]] × . . . × Oκ[Q[Uk]].
By default, we deﬁne Γκ[A] = Oκ[α(A)] to be the set of possible assignments to the logical
variables in the argument signature of A. However, an object skeleton may also specify a subset of
legal assignments. Γκ[A] ⊂Oκ[α(A)].
We can now deﬁne the set of instantiations of the attributes:

6.3. Template Variables and Template Factors
215
Student
Intelligence
SAT
George
low
High
Alice
high
High
Course
Diﬃculty
CS101
high
Econ101
low
Student
Course
Grade
Satisfaction
George
CS101
C
low
George
Econ101
A
high
Alice
CS101
A
low
Figure 6.5
One possible world for the University example. Here, we have two student objects and
two course objects. The attributes Grade and Satisfaction are restricted to three of their possible four legal
assignments.
Deﬁnition 6.7
Let κ be an object skeleton over Q, ℵ. We deﬁne sets of ground random variables:
ground random
variable
Xκ[A]
=
{A(γ) : γ ∈Γκ[A]}
Xκ[ℵ]
=
∪A∈ℵXκ[A].
(6.5)
Note that we are abusing notation here, identifying an assignment γ = ⟨U1 7→u1, . . . , Uk 7→uk⟩
with the tuple ⟨u1 . . . , uk⟩; this abuse of notation is unambiguous in this context due to the
ordering of the tuples.
The ability to specify a subset of Oκ[α(A)] is useful in eliminating the need to consider
random variables that do not really appear in the model. For example, in most cases, not every
student takes every course, and so we would not want to include a Grade variable for every
possible (student, course) pair at our university. See ﬁgure 6.5 as an example.
Clearly, the set of random variables is diﬀerent for diﬀerent skeletons; hence the model is a
template for an inﬁnite set of probability distributions, each spanning a diﬀerent set of objects
that induces a diﬀerent set of random variables. In a sense, this is similar to the situation
we had in DBNs, where the same 2TBN could induce a distribution over diﬀerent numbers of
time slices. Here, however, the variation between the diﬀerent instantiations of the template is
signiﬁcantly greater.
Our discussion so far makes several important simplifying assumptions. First, we portrayed
the skeleton as deﬁning a set of objects for each of the classes. As we discuss in later sections,
it can be important to allow the skeleton to provide additional background information about
the set of possible worlds, such as some relationships that hold between objects (such as the
structure of a family tree). Conversely, we may also want the skeleton to provide less information:
In particular, the premise underlying equation (6.5) is that the set of objects is predeﬁned by
the skeleton. As we brieﬂy discuss in section 6.6.2, we may also want to deal with settings in
which we have uncertainty over the number of objects in the domain. In this case, diﬀerent
possible worlds may have diﬀerent sets of objects, so that a random variable such as A(u) may
be deﬁned in some worlds (those that contain the object u) but not in others. Settings like this
pose signiﬁcant challenges, a discussion of which is outside the scope of this book.

216
Chapter 6. Template-Based Representations
Template Factors
The ﬁnal component in a template-based probabilistic model is one that
deﬁnes the actual probability distribution over a set of a ground random variables generated
from a set of template attributes. Clearly, we want the speciﬁcation of the model to be deﬁned
in a template-based way. Speciﬁcally, we would like to take a factor — whether an undirected
factor or a CPD — and instantiate it to apply to multiple scopes in the domain. We have already
seen one simple example of this notion: in a 2-TBN, we had a template CPD P(X′
i | PaX′
i),
which we instantiated to apply to diﬀerent scopes X(t)
i , PaX(t)
i , by instantiating any occurrence
X′
j to X(t)
j , and any occurrence Xj to X(t−1)
j
. In eﬀect, there we had template variables of
the form Xj and X′
j as arguments to the CPD, and we instantiated them in diﬀerent ways for
diﬀerent time points. We can now generalize this notion by deﬁning a factor with arguments.
Recall that a factor φ is a function from a tuple of random variables X = Scope[φ] to the reals;
this function returns a number for each assignment x to the variables X. We can now deﬁne
Deﬁnition 6.8
A template factor is a function ξ deﬁned over a tuple of template attributes A1, . . . , Al, where
template factor
each Aj has a range Val(A). It deﬁnes a mapping from Val(A1) × . . . × Val(Al) to IR. Given
a tuple of random variables X1, . . . , Xl, such that Val(Xj) = Val(Aj) for all j = 1, . . . , l, we
deﬁne ξ(X1, . . . , Xl) to be the instantiated factor from X to IR.
instantiated
factor
In the subsequent sections of this chapter, we use these notions to deﬁne various languages
for encoding template-based probabilistic models. As we will see, some of these representational
frameworks subsume and generalize on the DBN framework deﬁned earlier.
6.4
Directed Probabilistic Models for Object-Relational Domains
Based on the framework described in the previous section, we now describe template-based
representation languages that can encode directed probabilistic models.
6.4.1
Plate Models
We begin our discussion by presenting the plate model, the simplest and best-established of the
plate model
object-relational frameworks. Although restricted in several important ways, the plate modeling
framework is perhaps the approach that has been most commonly used in practice, notably
for encoding the assumptions made in various learning tasks. This framework also provides
an excellent starting point for describing the key ideas of template-based languages and for
motivating some of the extensions that have been pursued in richer languages.
In the plate formalism, object types are called plates. The fact that multiple objects in the
class share the same set of attributes and same probabilistic model is the basis for the use of
the term “plate,” which suggests a stack of identical objects. We begin with some motivating
examples and then describe the formal framework.
6.4.1.1
Examples
Example 6.8
The simplest example of a plate model, shown in ﬁgure 6.6, describes multiple random variables
generated from the same distribution. In this case, we have a set of random variables X(d) (d ∈D)

6.4. Directed Probabilistic Models for Object-Relational Domains
217
X
qX
Data m
Figure 6.6
Plate model for a set of coin tosses sampled from a single coin
that all have the same domain Val(X) and are sampled from the same distribution. In a plate
representation, we encode the fact that these variables are all generated from the same template by
drawing only a single node X(d) and enclosing it in a box denoting that d ranges over D, so that
we know that the box represents an entire “stack” of these identically distributed variables. This box
is called a plate, with the analogy that it represents a stack of identical plates.
plate
In many cases, we want to explicitly encode the fact that these variables have an identical
distribution. We therefore often explicitly add to the model the variable θX, which denotes the
parameterization of the CPD from which the variables X(d) are sampled. Most simply, if the X’s
are coin tosses of a single (possibly biased) coin, θX would take on values in the range [0, 1], and
its value would denote the bias of the coin (the probability with which it comes up “Heads”).
The idea of including the CPD parameters directly in the probabilistic model plays a central
role in our discussion of learning later in this book (see section 17.3). For the moment, we
note only that including the parameters directly in the model allows us to make explicit the
fact that all of the variables X(d) are sampled from the same CPD. By contrast, we could also
have used a model where a variable θX(d) is included inside the plate, allowing us to encode
the setting where each of the coin tosses was sampled from a diﬀerent coin. We note that this
transformation is equivalent to adding the coin ID d as a parent to X; however, the explicit
placement of θX within the plate makes the nature of the dependence more explicit. In this
chapter, to reduce clutter, we use the convention that parameters not explicitly included in the
model (or in the ﬁgure) are outside of all plates.
Example 6.9
Let us return to our Student example. We can have a Student plate that includes the attributes
I(S), G(S). As shown in ﬁgure 6.7a, we can have G(S) depend on I(s). In this model we have
a set of (Intelligence,Grade) pairs, one for each student. The ﬁgure also shows the ground Bayesian
ground Bayesian
network
network that would result from instantiating this model for two students. As we discussed, this
model implicitly makes the assumption that the CPDs for P(I(s)) and for P(G(s) | I(s)) is
the same for all students s. Clearly, we can further enrich the model by introducing additional
variables, such as an SAT-score variable for each student.
Our examples thus far have included only a single type of object, and do not signiﬁcantly
expand the expressive power of our language beyond that of plain graphical models. The key
beneﬁt of the plate framework is that it allows for multiple plates that can overlap with each
other in various ways.

218
Chapter 6. Template-Based Representations
Example 6.10
Assume we want to capture the fact that a course has multiple students in it, each with his or her
own grade, and that this grade depends on the diﬃculty of the course. Thus, we can introduce a
second type of plate, labeled Course, where the Grade attribute is now associated with a (student,
course) pair. There are several ways in which we can modify the model to include courses. In
ﬁgure 6.7b, the Student plate is nested within the Course plate. The Diﬃculty variable is enclosed
nested plate
within the Course plate, whereas Intelligence and Grade are enclosed within both plates. We thus
have that Grade(s, c) for a particular (student, course) pair (s, c) depends on Diﬃculty(c) and on
Intelligence(s, c).
This formulation ignores an important facet of this problem. As illustrated in ﬁgure 6.7b, it
induces networks where the Intelligence variable is associated not with a student, but rather
with a (student, course) pair. Thus, if we have the same student in two diﬀerent courses, we
would have two diﬀerent variables corresponding to his intelligence, and these could take on
diﬀerent values. This formulation may make sense in some settings, where diﬀerent notions of
“intelligence” may be appropriate to diﬀerent topics (for example, math versus art); however, it is
clearly not a suitable model for all settings. Fortunately, the plate framework allows us to come
up with a diﬀerent formulation.
Example 6.11
Figure 6.7c shows a construction that avoids this limitation. Here, the Student plate and the Course
plates intersect, so that the Intelligence attribute is now associated only with the Student plate,
plate intersection
and Diﬃculty with the Course plate; the Grade attribute is associated with the pair (comprising the
intersection between the plates). The interpretation of this dependence is that, for any pair of (student,
course) objects s, c, the attribute Grade(s, c) depends on Intelligence(s) and on Diﬃculty(c). The
ﬁgure also shows the network that results for two students both taking two courses.
In these examples, we see that even simple plate representations can induce fairly complex
ground Bayesian networks. Such networks model a rich network of interdependencies between
diﬀerent variables, allowing for paths of inﬂuence that one may not anticipate.
Example 6.12
Consider the plate model of ﬁgure 6.7c, where we know that a student Jane took CS101 and got an
A. This fact changes our belief about Jane’s intelligence and increases our belief that CS101 is an easy
class. If we now observe that Jane got a C in Math 101, it decreases our beliefs that she is intelligent,
and therefore should increase our beliefs that CS101 is an easy class. If we now observe that George
got a C in CS101, our probability that George has high intelligence is signiﬁcantly lower. Thus, our
beliefs about George’s intelligence can be aﬀected by the grades of other students in other classes.
Figure 6.8 shows a ground Bayesian network induced by a more complex skeleton involving
ﬁfteen students and four courses. Somewhat surprisingly, the additional pieces of “weak” evidence
regarding other students in other courses can accumulate to change our conclusions fairly radically:
Considering only the evidence that relates directly to George’s grades in the two classes that he took,
our posterior probability that George has high intelligence is 0.8. If we consider our entire body
of evidence about all students in all classes, this probability decreases from 0.8 to 0.25. When we
examine the evidence more closely, this conclusion is quite intuitive. We note, for example, that of
the students who took CS101, only George got a C. In fact, even Alice, who got a C in both of her
other classes, got an A in CS101. This evidence suggests strongly that CS101 is not a diﬃcult class, so
that George’s grade of a C in CS101 is a very strong indicator that he does not have high intelligence.

6.4. Directed Probabilistic Models for Object-Relational Domains
219
(b)
(a)
(c)
Grade
Intelligence
Grade
Students s
Courses c
Courses c
Students s
Students s
Intelligence
Grade
Intelligence
Difﬁculty
Difﬁculty
G(S1,C1)
G(S1,C2)
G(S2,C1)
G(S2,C2)
D(C1)
D(C2)
G(S1,C1)
G(S2,C1)
G(S1,C1)
G(S2,C1)
I(S1,C1)
I(S1,C2)
I(S2,C2)
I(S2,C1)
D(C1)
D(C2)
I(S1)
I(S2)
I(S1)
I(S2)
G(S1)
G(S2)
Figure 6.7
Plate models and induced ground Bayesian networks for a simpliﬁed Student example.
(a) Single plate: Multiple independent samples from the same distribution. (b) Nested plates: Multiple
courses, each with a separate set of students. (c) Intersecting plates: Multiple courses with overlapping sets
of students.
Thus, we obtain much more informed conclusions by deﬁning probabilistic models that encompass
all of the relevant evidence.
As we can see, a plate model provides a language for encoding models with repeated

structure and shared parameters. As in the case of DBNs, the models are represented
at the template level; given a particular set of objects, they can then be instantiated to
induce a ground Bayesian network over the random variables induced by these objects.
Because there are inﬁnitely many sets of objects, this template can induce an inﬁnite set
of ground networks.

220
Chapter 6. Template-Based Representations
A
B
C
Alice
George
CS101
Econ101
easy/hard
low/high
Figure 6.8
Illustration of probabilistic interactions in the University domain. The ground network
contains random variables for the Intelligence of ﬁfteen students (right ovals), including George (denoted by
the white oval), and the Diﬃculty for four courses (left ovals), including CS101 and Econ101. There are also
observed random variables for some subset of the (student, course) pairs. For clarity, these observed grades
are not denoted as variables in the network, but rather as edges relating the relevant (student, course) pairs.
Thus, for example George received an A in Econ101 but a C in CS101. Also shown are the ﬁnal probabilities
obtained by running inference over the resulting network.
6.4.1.2
Plate Models: Formal Framework
We now provide a more formal description of the plate modeling language: its representation
and its semantics. The plate formalism uses the basic object-relational framework described in
section 6.4. As we mentioned earlier, plates correspond to object types.
Each template attribute in the model is embedded in zero, one, or more plates (when plates
intersect). If an attribute A is embedded in a set of plates Q1, . . . , Qk, we can view it as being
associated with the argument signature U1, . . . , Uk, where each logical variable Ui ranges over
the objects in the plate (class) Qi. Recall that a plate model can also have attributes that are
external to any plate; these are attributes for which there is always a single copy in the model.
We can view this attribute as being associated with an argument signature of arity zero.
In a plate model, the set of random variables induced by a template attribute A is deﬁned
by the complete set of assignments: Γκ[A] = Oκ[α(A)]. Thus, for example, we would have
a Grade random variable for every (student,course) pair, whereas, intuitively, these variables are
only deﬁned in cases where the student has taken the course. We can take the values of such
variables to be unobserved, and, if the model is well designed, its descendants in the probabilistic
dependency graph will also be unobserved. In this case, the resulting random variables in the

6.4. Directed Probabilistic Models for Object-Relational Domains
221
network will be barren, and can be dropped from the network without aﬀecting the marginal
distribution over the others. This solution, however, while providing the right semantics, is not
particularly elegant.
We now deﬁne the probabilistic dependency structures allowed in the plate framework. To
provide a simple, well-speciﬁed dependency structure, plate models place strong restrictions on
the types of dependencies allowed. For example, in example 6.11, if we deﬁne Intelligence to be
a parent of Diﬃculty (reﬂecting, say, our intuition that intelligent students may choose to take
harder classes), the semantics of the ground model is not clear: for a ground random variable
D(c), the model does not specify which speciﬁc I(s) is the parent. To avoid this problem,
plate models require that an attribute can only depend on attributes in the same plate. This
requirement is precisely the intuition behind the notion of plate intersection: Attributes in the
intersection of plates can depend on other attributes in any of the plates to which they belong.
Formally, we have the following deﬁnition:
Deﬁnition 6.9
A plate model MPlate deﬁnes, for each template attribute A ∈ℵwith argument signature
plate model
U1, . . . , Uk:
• a set of template parents
template parent
PaA = {B1(U 1), . . . , Bl(U l)}
such that for each Bi(U i), we have that U i ⊆{U1, . . . , Uk}. The variables U i are the
argument signature of the parent Bi.
parent argument
signature
• a template CPD P(A | PaA).
This deﬁnition allows the Grade attribute Grade(S, C) to depend on Intelligence(S), but not
vice versa. Note that, as in Bayesian networks, this deﬁnition allows any form of CPD, with or
without local structure.
Note that the straightforward graphical representation of plates fails to make certain distinc-
tions that are clear in the symbolic representation.
Example 6.13
Assume that our model contains an attribute Cited(U1, U2), where U1, U2 are both in the Paper
class. We might want the dependency model of this attribute to depend on properties of both papers,
for example, Topic(U1), Topic(U2), or Review-Paper(U1). To encode this dependency graphically,
we ﬁrst need to have two sets of attributes from the Paper class, one for U1 and the other for
U2. Moreover, we need to denote somehow which attributes of which of the two arguments are the
parents. The symbolic representation makes these distinctions unambiguously.
To instantiate the template parents and template CPDs, it helps to introduce some shorthand
notation. Let γ = ⟨U1 7→u1, . . . , Uk 7→uk⟩be some assignment to some set of logical vari-
ables, and B(Ui1, . . . , Uil) be an attribute whose argument signature involves only a subset of
these variables. We deﬁne B(γ) to be the ground random variable B(ui1, . . . , uil).
The template-level plate model, when applied to a particular skeleton, deﬁnes a ground
probabilistic model, in the form of a Bayesian network:
Deﬁnition 6.10
A plate model MPlate and object skeleton κ deﬁne a ground Bayesian network BMPlate
κ
as fol-
ground Bayesian
network

222
Chapter 6. Template-Based Representations
lows.
Let A(U1, . . . , Uk) be any template attribute in ℵ.
Then, for any assignment γ =
⟨U1 7→u1, . . . , Uk 7→uk⟩∈Γκ[A], we have a variable A(γ) in the ground network, with parents
B(γ) for all B ∈PaA, and the instantiated CPD P(A(γ) | PaA(γ)).
Thus, in our example, we have that the network contains a set of ground random variables
Grade(s, c), one for every student s and every course c.
Each such variable depends on
Intelligence(s) and on Diﬃculty(c).
The ground network BMPlate
κ
speciﬁes a well-deﬁned joint distribution over Xκ[ℵ], as required.
The BN in ﬁgure 6.7b is precisely the network structure we would obtain from this deﬁni-
tion, using the plate model of ﬁgure 6.7 and the object skeleton Oκ[Student] = {s1, s2} and
Oκ[Course] = {c1, c2}. In general, despite the compact parameterization (only one local prob-
abilistic model for every attribute in the model), the resulting ground Bayesian network can be
quite complex, and models a rich set of interactions. As we saw in example 6.12, the ability
to incorporate all of the relevant evidence into the single network shown in the ﬁgure can
signiﬁcantly improve our ability to obtain meaningful conclusions even from weak indicators.

The plate model is simple and easy to understand, but it is also highly limited in several
ways. Most important is the ﬁrst condition of deﬁnition 6.9, whereby A(U1, . . . , Uk) can
only depend on attributes of the form B(Ui1, . . . , Uil), where Ui1, . . . , Uil is a subtuple of
U1, . . . , Uk. This restriction signiﬁcantly constrains our ability to encode a rich network
of probabilistic dependencies between the objects in the domain. For example, in the
Genetics domain, we cannot encode a dependence of Genotype(U1) on Genotype(U2),
where U2 is (say) the mother of U1. Similarly, we cannot encode temporal models such
as those described in section 6.2, where the car’s position at a point in time depends on
its position at the previous time point. In the next section, we describe a more expressive
representation that addresses these limitations.
6.4.2
Probabilistic Relational Models
As we discussed, the greatest limitation of the plate formalism is its restriction on the argument
signature of an attribute’s parents. In particular, in our genetic inheritance example, we would
like to have a model where Genotype(u) depends on Genotype(u′), where u′ is the mother of u.
This type of dependency is not encodable within plate models, because it uses a logical variable
in the attribute’s parent that is not used within the attribute itself. To allow such models, we must
relax this restriction on plate models. However, relaxing this assumption without care can lead to
nonsensical models. In particular, if we simply allow Genotype(U) to depend on Genotype(U ′),
we end up with a dependency model where every ground variable Genotype(u) depends on
every other such variable. Such models are intractably dense, and (more importantly) cyclic.
What we really want is to allow a dependence of Genotype(U) on Genotype(U ′), but only for
those assignments to U ′ that correspond to U’s mother. We now describe one representation
that allows such dependencies, and then discuss some of the subtleties that arise when we
introduce this signiﬁcant extension to our expressive power.
6.4.2.1
Contingent Dependencies
To capture such situations, we introduce the notion of a contingent dependency, which speciﬁes
the context in which a particular dependency holds. A contingent dependency is deﬁned in

6.4. Directed Probabilistic Models for Object-Relational Domains
223
terms of a guard — a formula that must hold for the dependency to be applicable.
Example 6.14
Consider again our University example. As usual, we can deﬁne Grade(S, C) for a student S
and a course C to have the parents Diﬃculty(C) and Intelligence(S). Now, however, we can
make this dependency contingent on the guard Registered(S, C).
Here, the parent’s argument
signature is the same as the child’s. More interestingly, contingent dependencies allow us to model
the dependency of the student’s satisfaction in a course, Satisfaction(S, C), on the teaching ability
of the professor who teaches the course. In this setting, we can make Teaching-Ability(P) the
parent of Satisfaction(S, C), where the dependency is contingent on the guard Registered(S, C) ∧
Teaches(P, C). Note that here, we have more logical variables in the parents of Satisfaction(S, C)
than in the attribute itself: the attribute’s argument signature is S, C, whereas its parent argument
signature is the tuple S, C, P.
We can also represent chains of dependencies within objects in the same class.
Example 6.15
For example, to encode temporal models, we could have Location(U) depend on Location(V ),
contingent on the guard Precedes(V, U). In our Genetics example, for the attribute Genotype(U),
we would deﬁne the template parents Genotype(V ) and Genotype(W), the guard Mother(V, U) ∧
Father(W, U), and the parent signature U, V, W.
We now provide the formal deﬁnition underlying these examples:
Deﬁnition 6.11
For a template attribute A, we deﬁne a contingent dependency model as a tuple consisting of:
contingent
dependency
model
• A parent argument signature α(PaA), which is a tuple of typed logical variables Ui such that
parent argument
signature
α(PaA) ⊇α(A).
• A guard Γ, which is a binary-valued formula deﬁned in terms of a set of template attributes
guard
PaΓ
A over the argument signature α(PaA).
• a set of template parents
template parent
PaA = {B1(U 1), . . . , Bl(U l)}
such that for each Bi(U i), we have that U i ⊆α(PaA).
A probabilistic relational model (PRM) MPRM deﬁnes, for each A ∈ℵa contingent dependency
probabilistic
relational model
model, as in deﬁnition 6.11, and a template CPD. The structure of the template CPD in this case
is more complex, and we discuss it in detail in section 6.4.2.2.
Intuitively, the template parents in a PRM, as in the plate model, deﬁne a template for the
parent assignments in the ground network, which will correspond to speciﬁc assignments of the
logical variables to objects of the appropriate type. In this setting, however, the set of logical
variables in the parents is not necessarily a subset of the logical variables in the child.
The ability to introduce new logical variables into the speciﬁcation of an attribute’s par-
ents gives us signiﬁcant expressive power, but introduces some signiﬁcant challenges. These
challenges clearly manifest in the construction of the ground network.

224
Chapter 6. Template-Based Representations
Deﬁnition 6.12
A PRM MPRM and object skeleton κ deﬁne a ground Bayesian network BMPRM
κ
as follows. Let
ground Bayesian
network
A(U1, . . . , Uk) be any template attribute in ℵ. Then, for any assignment γ ∈Γκ[A], we have
a variable A(γ) in the ground network. This variable has, for any B ∈PaΓ
A ∪PaA and any
assignment γ′ to α(PaA) −α(A), the parent that is the instantiated variable B(γ, γ′).
An important subtlety in this deﬁnition is that the attributes that appear in the guard are also
parents of the ground variable. This requirement is necessary, because the values of the guard
attributes determine whether there is a dependency on the parents or not, and hence they aﬀect
the probabilistic model.
Using this deﬁnition for the model of example 6.14, we have that Satisfaction(s, c) has the
parents:
Teaching-Ability(p), Registered(s, c), and Teaches(p, c) for every professor p.
The
guard in the contingent dependency is intended to encode the fact that the dependency on
Teaching-Ability(p) is only present for a subset of individuals p, but it is not obvious how that
fact aﬀects the construction of our model. The situation is even more complex in example 6.15,
where we have as parents of Genotype(u) all of the variables of the form Father(v, u) and
Genotype(v), for all person objects v, and similarly for Mother(v, u) and Genotype(v). In both
cases, the resulting ground network is very densely connected. In the Genetics network, it is
also obviously cyclic. We will describe how to encode such dependencies correctly within the
CPD of the ground network, and how to deal with the issues of potential cyclicity.
6.4.2.2
CPDs in the Ground Network
As we just discussed, the ground network induced by a PRM can introduce a dependency of
a variable on a set of parents that is not ﬁxed in advance, and which may be arbitrarily large.
How do we encode a probabilistic dependency model for such dependencies?
Exploiting the Guard Structure
The ﬁrst key observation is that the notion of a contin-
gent dependency is intended to speciﬁcally capture context-speciﬁc independence: In deﬁni-
tion 6.12, if the guard for a parent B of A is false for a particular assignment (γ, γ′), then
there is no dependency of A(γ) on B(γ, γ′).
For example, unless Mother(v, u) is true for
a particular pair (u, v), we have no dependence of Genotype(u) on Genotype(v).
Similarly,
unless Registered(s, c) ∧Teaches(p, c) is true, there is no dependence of Satisfaction(s, c) on
Teaching-Ability(p). We can easily capture this type of context-speciﬁc independence in the
CPD using a variant of the multiplexer CPD of deﬁnition 5.3.
While this approach helps us specify the CPD in these networks of potentially unbounded
indegree, it does not address the fundamental problem: the dense, and often cyclic, connectivity
structure.
A common solution to this problem is to assume that the guard predicates are
properties of the basic relational structure in the domain, and are often ﬁxed in advance. For
example, in the temporal setting, the Precedes relation is always ﬁxed: time point t −1 always
precedes time point t. Somewhat less obviously, in our Genetics example, it may be reasonable
to assume that the pedigree is known in advance.
We can encode this assumption by deﬁning a relational skeleton κr, which deﬁnes a certain set
relational
skeleton
of facts (usually relationships between objects) that are given in advance, and are not part of the
probabilistic model. In cases where the values of the attributes in the guards are speciﬁed as part
of the relational skeleton, we can simply use that information to determine the set of parents that

6.4. Directed Probabilistic Models for Object-Relational Domains
225
are active in the model, usually a very limited set. Thus, for example, if Registered and Teaches
are part of the relational skeleton, then Satisfaction(s, c) has the parent Teaching-Ability(p)
only when Registered(s, c) and Teaches(p, c) both hold. Similarly, in the Genetics example, if
the pedigree structure is given in the skeleton, we would have that Genotype(v) is a parent
of Genotype(u) only if Mother(v, u) or Father(v, u) are present in the skeleton.
Moreover,
we see that, assuming a legal pedigree, the resulting ground network in the Genetics domain
is guaranteed to be acyclic. Indeed, the resulting model produces ground networks that are
precisely of the same type demonstrated in box 3.B. The use of contingent dependencies

allows us to exploit relations that are determined by our skeleton to produce greatly
simpliﬁed models, and to make explicit the fact that the model is acyclic.
The situation becomes more complex, however, if the guard predicates are associated with a
probabilistic model, and therefore are random variables in the domain. Because the guards are
typically associated with relational structure, we refer to this type of uncertainty as relational
relational
uncertainty
uncertainty. Relational uncertainty introduces signiﬁcant complexity into our model, as we now
cannot use background knowledge (from our skeleton) to simplify the dependency structure in
contingent dependencies. In this case, when the family tree is uncertain, we may indeed have
that Genotype(u) can depend on every other variable Genotype(v), a model that is cyclic and ill
deﬁned. However, if we restrict the distribution over the Mother and Father relations so as to
ensure that only “reasonable” pedigrees get positive probability, we can still guarantee that our
probabilistic model deﬁnes a coherent probability distribution. However, deﬁning a probability
distribution over the Mother and Father relations that is guaranteed to have this property is far
from trivial; we return to this issue in section 6.6.
Aggregating Dependencies
By itself, the use of guards may not fully address the problem
of deﬁning a parameterization for the CPDs in a PRM. Consider again a dependency of A on
an attribute B that involves some set of logical variables U ′ that are not in α(A). Even if we
assume that we have a relational skeleton that fully determines the values of all the guards, there
may be multiple assignments γ′ to U ′ for which the guard holds, and hence multiple diﬀerent
ground parents B(γ, γ′) — one for each distinct assignment γ′ to U ′. Even in our simple
University example, there may be multiple instructors for a course c, and therefore multiple
ground variables Teaching-Ability(p) that are parents of a ground variable Satisfaction(s, c). In
general, the number of possible instantiations of a given parent B is not known in advance, and
may not even be bounded. Thus, we need to deﬁne a mechanism for specifying a template-level
local dependency model that allows a variable number of parents. Moreover, because the parents
corresponding to diﬀerent instantiations are interchangeable, the local dependency model must
be symmetric.
There are many possible ways of specifying such a model. One approach is to use one of
the symmetric local probability models that we saw in chapter 5. For example, we can use a
noisy-or (section 5.4.1) or logistic model (see section 5.4.2), where all parents have the same
parameter. An alternative approach is to deﬁne an aggregator CPD that uses certain aggregate
aggregator CPD
statistics or summaries of the set of parents of a variable. (See exercise 6.7 for some analysis of
the expressive power of such CPDs.)
Example 6.16
Let us consider again the dependence of Satisfaction(S, C) on Teaching-Ability(P), with the guard
Taking(S, C) ∧Teaching(C, P). Assuming, for the moment, that both Satisfaction and Teaching-

226
Chapter 6. Template-Based Representations
Ability are binary-valued, we might use a noisy-or model: Given a parameterization for Satisfac-
tion given a single Teaching-Ability, we can use the noisy-or model to deﬁne a general CPD for
Satisfaction(s, c) given any set of parents Teaching-Ability(p1), . . . , Teaching-Ability(pm). Alter-
natively, we can assume that the student’s satisfaction depends on the worst instructor and best
instructor in the course. In this case, we might aggregate the teaching abilities using the min
and max functions, and then use a CPD of our choosing to denote the student’s satisfaction as a
function of the resulting pair of values. As another example, a student’s job prospects can depend
on the average grade in all the courses she has taken.
When designing such a combination rule, it is important to consider any possible boundary
cases. On one side, in many settings the set of parents can be empty: a course may have no
instructors (if it is a seminar composed entirely of invited lecturers); or a person may not have
a parent in the pedigree. On the other side, we may also need to consider cases where the
number of parents is large, in which case noisy-or and logistic models often become degenerate
(see ﬁgure 5.10 and ﬁgure 5.11).
The situation becomes even more complex when there are multiple distinct parents at the
template level, each of which may result in a set of ground parents. For example, a student’s
satisfaction in a course may depend both on the teaching ability of multiple instructors and
on the quality of the design of the diﬀerent problem sets in the course. We therefore need
to address both the aggregation of each type of parent set (instructors or problem sets) as
well as combining them into a single CPD. Thus, we need to deﬁne some way of combining
a set of CPDs {P(X | Yi,1, . . . , Yi,ji)
:
i = 1, . . . , l}, to a single joint CPD {P(X |
Y1,1, . . . , Y1,j1, . . . , Yl,1, . . . , Yl,jl). Here, as before, there is no single right answer, and the
particular choice is likely to depend heavily on the properties of the application.
We note that the issue of multiple parents is distinct from the multiple parents that arise
when we have relational uncertainty. In the case of relational uncertainty, we also have multiple
parents for a variable in the ground network; yet, it may well be the case that, in any situation,
at most one assignment to the logical variables in the parent signature will satisfy the guard
condition. For example, even if we are uncertain about John’s paternity, we would like it to be
the case that, in any world, there is a unique object v for which Father(v, John) holds.
(As we discuss in section 6.6, however, deﬁning a probabilistic model that ensures this type
of constraint can be far from trivial.) In this type of situation, the concept of a guard is again
useful, since it allows us to avoid deﬁning local dependency models with a variable number of
parents in domains (such as genetic inheritance) where such situations do not actually arise.
6.4.2.3
Checking Acyclicity
One important issue in relational dependency models is that the dependency structure of the
ground network is, in general, not determined in advance, but rather induced from the model
structure and the skeleton. How, then, can we guarantee that we obtain a coherent probability
distribution? Most obviously, we can simply check, post hoc, that any particular ground network
resulting from this process is acyclic. However, this approach is unsatisfying from a model
design perspective. When constructing a model, whether by hand or using learning methods,
we would like to have some guarantees that it will lead to coherent probability distributions.
Thus, we would like to provide a test that we can execute on a model MPRM at the template
level, and which will guarantee that ground distributions induced from MPRM will be coherent.

6.4. Directed Probabilistic Models for Object-Relational Domains
227
(a)
SAT
Difﬁculty
Teaching Ability
Teaches
Registered
Grade
Satisfaction
(b)
Blood Type
Mother
Father
Genotype
Intelligence
Figure 6.9
Examples of dependency graphs: (a) Dependency graph for the University example. (b)
Dependency graph for the Genetics example.
One approach for doing so is to construct a template-level graph that encodes a set of potential
dependencies that may happen at the ground level. The nodes in the graph are the template-
level attributes; there is an edge from B to A if there is any possibility that a ground variable
of type B will inﬂuence one of type A.
Deﬁnition 6.13
A template dependency graph for a template dependency model MPRM contains a node for each
template
dependency
graph
template-level attribute A, and a directed edge from B to A whenever there is an attribute of type
B in PaΓ
A ∪PaA.
This graph can easily be constructed from the deﬁnition of the dependency model. For example,
the template dependency graph of our University model (example 6.14) is shown in ﬁgure 6.9a.
It is not diﬃcult to show that if template dependency graph for a model MPRM is acyclic (as
in this case), it is clear that any ground network generated from MPRM must also be acyclic
(see exercise 6.8).
However, a cycle in the template dependency graph for MPRM does not
imply that every ground network induced by MPRM is cyclic. (This is the case, however, for
any nondegenerate instantiation of a plate model; see exercise 6.9.) Indeed, there are template
models that, although cyclic at the template levels, reﬂect a natural dependency structure whose
ground networks are guaranteed to be acyclic in practice.
Example 6.17
Consider the template dependency graph of our Genetics domain, shown in ﬁgure 6.9b.
The
template-level self-loop involving Genotype(Person) reﬂects a ground-level dependency of a person’s
genotype on that of his or her parents. This type of dependency can only lead to cycles in the ground
network if the pedigree is cyclic, that is, a person is his/her own ancestor. Because such cases (time
travel aside) are impossible, this template model cannot result in cyclic ground networks for the
skeletons that arise in practice. Intuitively, in this case, we have an (acyclic) ordering ≺over the
objects (people) in the domain, which implies that u′ can be a parent of u only when u′ ≺u;
therefore, Genotype(u) can depend on Genotype(u′) only when u′ ≺u. This ordering on objects is
acyclic, and therefore so is the resulting dependency structure.
The template dependency graph does not account for these constraints on the skeleton, and
therefore we cannot conclude by examining the graph whether cycles can occur in ground

228
Chapter 6. Template-Based Representations
networks for such ordered skeletons.
However, exercise 6.10 discusses a richer form of the
template dependency network that explicitly incorporates such constraints, and it is therefore
able to determine that our Genetics model results in acyclic ground networks for any skeleton
representing an acyclic pedigree.
So far in our discussion of acyclicity, we have largely sidestepped the issue of relational
uncertainty. As we discussed, in the case of relational uncertainty, the ground network contains
many “potential” edges, only a few of which will ever be “active” simultaneously. In such cases,
the resulting ground network may not even be acyclic, even though it may well deﬁne a coherent
(and sparse) probability distribution for every relational structure. Indeed, as we discussed in
example 6.17, there are models that are potentially cyclic but are guaranteed to be acyclic by
virtue of speciﬁc constraints on the dependency structure.
It is thus possible to guarantee
the coherence of a cyclic model of this type by ensuring that there is no positive-probability
assignment to the guards that actually induces a cyclic dependence between the attributes.
6.5
Undirected Representation
The previous sections describe template-based formalisms that use a directed graphical model
as their foundation. One can deﬁne similar extensions for undirected graphical models. Many of
the ideas underlying this extension are fairly similar to the directed case. However, the greater
ﬂexibility of the undirected representation in avoiding local normalization requirements and
acyclicity constraints can be particularly helpful in the context of these richer representations.
Eliminating these requirements allows us to easily encode a much richer set of patterns about
the relationships between objects in the domain; see, for example, box 6.C. In particular, as we
discuss in section 6.6, these beneﬁts can be very signiﬁcant when we wish to deﬁne distributions
over complex relational structures.
The basic component in a template-based undirected model is some expression, written in
terms of template-level attributes with logical variables as arguments, and associated with a
template factor. For a given object skeleton, each possible assignment γ to the logical variables
in the expression induces a factor in the ground undirected network, all sharing the same
parameters. As for variable-based undirected representations, one can parameterize a template-
based undirected probabilistic model using full factors, or using features, as in a log-linear
model. This decision is largely orthogonal to other issues. We choose to use log-linear features,
which are the ﬁnest-grained representation and subsume table factors.
Example 6.18
Let us begin by revisiting our Misconception example in section 4.1. Now, assume that we are
interested in deﬁning a probabilistic model over an entire set of students, where some number of
pairs study together. We deﬁne a binary predicate (relation) Study-Pair(S1, S2), which is true when
two students S1, S2 study together, and a predicate (attribute) Misconception(S) that encodes the
level of understanding of a student S. We can now deﬁne a template feature fM, over pairs
template feature
Misconception(S1), Misconception(S2), which takes value 1 whenever
[Study-Pair(S1, S2) = true ∧Misconception(S1) = Misconception(S2)] = true
(6.6)
and has value 0 otherwise.

6.5. Undirected Representation
229
Deﬁnition 6.14
A relational Markov network MRMN is deﬁned in terms of a set Λ of template features, where each
relational Markov
network
λ ∈Λ comprises:
• a real-valued template feature fλ whose arguments are ℵ(λ) = {A1(U 1), . . . , Al(Ul)};
feature argument
• a weight wλ ∈IR.
We deﬁne α(λ) so that for all i, U i ⊆α(λ).
In example 6.18, we have that α(λM) = {S1, S2}, both of type Student; and
ℵ(λM) = {Study-Pair(S1, S2), Misconception(S1), Misconception(S2)}.
To specify a ground network using an RMN, we must provide an object skeleton κ that deﬁnes
object skeleton
a ﬁnite set of objects Oκ[Q] for each class Q. As before, we can also deﬁne a restricted set
Γκ[A] ⊂Oκ[α(A)]. Given a skeleton, we can now deﬁne a ground Gibbs distribution in the
natural way:
Example 6.19
Continuing example 6.18, assume we are given a skeleton containing a particular set of students
and the set of study pairs within this set. This model induces a Markov network where the ground
random variables have the form Misconception(s) for every student s in the domain.
In this
model, we have a feature fM for every triple of variables Misconception(s1), Misconception(s2),
Study-Pair(s1, s2). As usual in log-linear models, features can be associated with a weight; in
this example, we might choose wM = 10. In this case, the unnormalized measure for a given
assignment to the ground variables would be exp(10K), where K is the number of pairs s1, s2 for
which equation (6.6) holds.
More formally:
Deﬁnition 6.15
Given an RMN MRMN and an object skeleton κ, we can deﬁne a ground Gibbs distribution P MRMN
κ
ground Gibbs
distribution
as follows:
• The variables in the network are Xκ[ℵ] (as in deﬁnition 6.7);
• P MRMN
κ
contains a term
exp(wλ · fλ(γ))
for each feature template λ ∈Λ and each assignment γ ∈Γκ[α(λ)].
As always, a (ground) Gibbs distribution deﬁnes a Markov network, where we connect every pair
of variables that appear together in some factor.
In the directed setting, the dense connectivity arising in ground networks raised several
concerns: acyclicity, aggregation of dependencies, and computational cost of the resulting model.
The ﬁrst of these is obviously not a concern in undirected models. The other two, however,
deserve some discussion.
Although better hidden, the issue of aggregating the contribution of multiple assignments of
a feature also arises in undirected model. Here, the deﬁnition of the Gibbs distribution dictates

230
Chapter 6. Template-Based Representations
the form of the aggregation we use. In this case, each grounding of the feature deﬁnes a factor
in the unnormalized measure, and they are combined by a product operation, or an addition in
log-space. In other words, each occurrence of the feature has a log-linear contribution to the
unnormalized density. Importantly, however, this type of aggregation may not be appropriate for
every application.
Example 6.20
Consider a model for “viral marketing” — a social network of individuals related by the Friends(P, P ′)
relation, where the attribute of interest Gadget(P) is the purchase of some cool new gadget G. We
may want to construct a model where it is more likely that two friends either both own or both do
not own G. That is, we have a feature similar to λM in example 6.18. In the log-linear model, the
unnormalized probability that a person p purchases G grows log-linearly with the number kp of
his friends who own G. However, a more realistic model may involve a saturation eﬀect, where the
impact diminishes as kp grows; that is, the increase in probability of Gadget(P) between kp = 0
and kp = 1 is greater than the increase in probability between kp = 20 and kp = 21.
Thus, in concrete applications, we may wish to extend the framework to allow for other forms of
combination, or even simply to deﬁne auxiliary variables corresponding to relevant aggregates
(for example, the value of kp in our example).
The issue of dense connectivity is as much an issue in the undirected case as in the directed
case. The typical solution is similar: If we have background knowledge in the form of a relational
skeleton, we can signiﬁcantly simplify the resulting model. Here, the operation is very simple:
we simply reduce every one of the factors in our model using the evidence contained in the
relational skeleton, producing a reduced Markov network, as in deﬁnition 4.7. In this network, we
would eliminate any ground variables whose values are observed in the skeleton and instantiate
their (ﬁxed) values in any ground factors containing them. In many cases, this process can
greatly simplify the resulting features, often making them degenerate.
Example 6.21
Returning to example 6.18, assume now that our skeleton speciﬁes the instantiation of the relation
Study-Pair, so that we know exactly which pairs of students study together and which do not. Now,
consider the reduced Markov network obtained by conditioning on the skeleton. As all the variables
Study-Pair(s1, s2) are observed, they are all eliminated from the network. Moreover, for any pair
of students s1, s2 for which Study-Pair(s1, s2) = false, the feature λM(s1, s2) necessarily takes
the value 0, regardless of the values of the other variables in the feature. Because this ground
feature is vacuous and has no impact on the distribution, it can be eliminated from the model. The
resulting Markov network is much simpler, containing edges only between pairs of students who
study together (according to the information in the relational skeleton).
We note that we could have introduced the notion of a guarded dependency, as we did for
PRMs. However, this component is far less useful here than it was in the directed case, where it
also served a role in eliminating the need to aggregate parents that are not actually present in
the network and in helping clarify the acyclicity in the model. Neither of these issues arises in
the undirected framework, obviating the need for the additional notational complexity.
Finally, we mention one subtlety that is speciﬁc to the undirected setting. An undirected
model uses nonlocal factors, which can have a dramatic inﬂuence on the global probability
measure of the model. Thus, the probability distribution deﬁned by an undirected relational
model is not modular: Introducing a new object into the domain can drastically change the

6.5. Undirected Representation
231
distribution over the properties of existing objects, even when the newly introduced object seems
to have no meaningful interactions with the previous objects.
Example 6.22
Let us return to our example 6.19, and assume that any pair of students study together with some
probability p; that is, we have an additional template feature over Study-Pair(S1, S2) that takes
the value log p when this binary attribute is true and log(1 −p) otherwise.
Assume that we have a probability distribution over the properties of some set of students
Oκ[Student] = {s1, . . . , sn}, and let us study how this distribution changes if we add a new
student sn+1. Consider an assignment to the properties of s1, . . . , sn in which m of the n students
si have Misconception(si) = 1, whereas the remaining n −m have Misconception(si) = 0. We
can now consider the following situations with respect to sn+1: he studies with k of the m students
for whom Misconception(si) = 1, with ℓof the n −m students for whom Misconception(si) = 0,
and himself has Misconception(sn+1) = c (for c ∈{0, 1}). The probability of each such event is
m
k
(n −m)
ℓ

pℓ(1 −p)(n−m−ℓ))(10kc · 10ℓ(1−c)),
where the ﬁrst two terms come from the factors over the Study-Pair(S1, S2) structure, and the
ﬁnal term comes from the template feature λM. We want to compute the marginal distribution
over our original variables (not involving sn+1), to see whether introducing sn+1 changes this
distribution. Thus, we sum out over all of the preceding events, which (using simple algebra) is
(10p + (1 −p))m + (10p + (1 −p))n−m.
This analysis shows that the assignments to our original variables are multiplied by very diﬀerent
terms, depending on the value m. In particular, the probability of joint assignments where m = 0,
so that all students agree, are multiplied by a factor of (10p + (1 −p))n, whereas the probability
of joint assignments where the students are equally divided in their opinion are multiplied by
(10p + (1 −p))n/2, an exponentially smaller factor.
Thus, adding a new student, even one
about whom we appear to know nothing, can drastically change the properties of our probability
distribution.

Thus, for undirected models, it can be problematic to construct (by learning or by
hand) a template-based model for domains of a certain size, and apply it to models of a
very diﬀerent size. The impact of the domain size on the probability distribution varies,
and therefore the implications regarding our ability to apply learning in this setting need
to be evaluated per application.
Box 6.C — Case Study: Collective Classiﬁcation of Web Pages. One application that calls for
interesting models of interobject relationships is the classiﬁcation of a network of interlinked web-
pages. One example is that of a university website, where webpages can be associated with students,
faculty members, courses, projects, and more. We can associate each webpage w with a hidden
variable T(w) whose value denotes the type of the entity to which the webpage belongs. In a
standard classiﬁcation setting, we would use some learned classiﬁer to label each webpage based
on its features, such as the words on the page. However, we can obtain more information by also
considering the interactions between the entities and the correlations they induce over their labels.
For example, an examination of the data reveals that student pages are more likely to link to faculty
webpages than to other student pages.

232
Chapter 6. Template-Based Representations
One can capture this type of interaction both in a directed and in an undirected model. In
a directed model, we might have a binary attribute Links(W1, W2) that takes the value true if
W1 links to W2 and false otherwise. We can then have Links(W1, W2) depend on T(W1) and
T(W2), capturing the dependence of the link probability on the classes of the two linked pages.
An alternative approach is to use an undirected model, where we directly introduce a pairwise
template feature over T(W1), T(W2) for pairs W1, W2 such that Links(W1, W2). Here, we can
give higher potentials to pairs of types that tend to link, for example, student-faculty, faculty-course,
faculty-project, project-student, and more.
A priori, both models appear to capture the basic structure of the domain. However, the directed
model has some signiﬁcant disadvantages in this setting.
First, since the link structure of the
webpages is known, the Links(W1, W2) is always observed. Thus, we have an active v-structure
connecting every pair of webpages, whether they are linked or not. The computational disadvantages
of this requirement are obvious. Less obvious but equally important is the fact that there are many
more non-links than links, and so the signal from the absent links tends to overwhelm the signal
that could derived from the links that are present. In an undirected model, the absent links are
simply omitted from the model; we simply introduce a potential that correlates the topics of two
webpages only if they are linked. Therefore, an undirected model generally achieves much better
performance on this task.
Another important advantage of the undirected model for this task is its ﬂexibility in incorporating
a much richer set of interactions. For example, it is often the case that a faculty member has a
section in her webpage where she lists courses that she teaches, and another section that lists
students whom she advises. Thus, another useful correlation that we may wish to model is one
between the types of two webpages that are both linked from a third, and whose links are in
close proximity on the page. We can model this type of interaction using features of the form
Close-Links(W, W1, W2)∧T(W1) = t1 ∧T(W2) = t2, where Close-Links(W, W1, W2) is derived
directly from the structure of the page.
Finally, an extension of the same model can be used to label not only the entities (webpages)
but also the links between them. For example, we might want to determine whether a student-
professor (s, p) pair with a link from s to p represents an advising relationship, or whether a linked
professor-course pair represents an instructor relationship. Once again, a standard classiﬁer would
make use of features such as words in the vicinity of the hyperlink. At the next level, we can use
an extension of the model described earlier to classify jointly both the types of the entities and the
types of the links that relate them. In a more interesting extension, a relational model can also
utilize higher-level patterns; for example, using a template feature over triplets of template attributes
T(W), we can encode the fact that students and their advisors belong to the same research group,
or that students often serve as teaching assistants in courses that their advisors teach.
6.6
Structural Uncertainty ⋆
The object-relational probabilistic models we described allow us to encode a very rich family of
distributions over possible worlds. In addition to encoding distributions over the attributes of
objects, these approaches can allow us to encode structural uncertainty — a probabilistic model
structural
uncertainty
over the actual structure of the worlds, both the set of objects they contain and the relations

6.6. Structural Uncertainty ⋆
233
between them. The diﬀerent models we presented exhibit signiﬁcant diﬀerences in the types
of structural uncertainty that they naturally encompass. In this section, we discuss some of
the major issues that arise when representing structural uncertainty, and how these issues are
handled by the diﬀerent models.
There are two main types of structural uncertainty that we can consider: relational uncertainty,
relational
uncertainty
which models a distribution over the presence or absence of relations between objects; and
object uncertainty, which models a distribution over the existence or number of actual objects
object
uncertainty
in the domain. We discuss each in turn.
6.6.1
Relational Uncertainty
The template representations we have already developed already allow us to encode uncertainty
about the relational structure.
As in example 6.22, we can simply make the existence of a
relationship a stochastic event. What types of probability distributions over relational structure
can we encode using these representational tools?
In example 6.22, each possible relation
Study-Pair(s1, s2) is selected independently, at random, with probability p. Unfortunately, such
graphs are not representative of most relational structures that we observe in real-world settings.
Example 6.23
Let us select an even simpler example, where the graph we are constructing is bipartite. Consider
the relation Teaches(P, C), and assume that it takes the value true with probability 0.1. Consider
a skeleton that contains 10 professors and 20 courses. Then the expected number of courses per
professor is 2, and the expected number of professors per course is 1. So far, everything appears
quite reasonable. However, the probability that, in the resulting graph, a particular professor teaches
ℓcourses is distributed binomially:
 20
ℓ

0.1ℓ0.920−ℓ. For example, the probability that any single
professor teaches 5 or more courses is 4.3 percent, and the probability that at least one of them
does is around 29 percent. This is much higher than is realistic in real-world graphs. The situation
becomes much worse if we increase the number of professors and courses in our skeleton.
Of course, we can add parents to this attribute. For example, we can let the presence of an edge
depend on the research area of the professor and the topic of the course, so that this attribute is
more likely to take the value true if the area and topic match. However, this solution does not
address the fundamental problem: it is still the case that, given all of the research areas and topics,
the relationship status for diﬀerent pairs of objects (the edges in the relational graph) are chosen
independently.
In this example, we wish to model certain global constraints on the distribution over the graph:
the fact that each faculty member tends to teach only a small subset of courses. Unfortunately,
it is far from clear how to incorporate this constraint into a template-level generative (directed)
model over attributes corresponding to the presence of individual relations. Indeed, consider
even the simpler case where we wish to encode the prior knowledge that each course has
exactly one instructor.
This model induces a correlation among all of the binary random
variables corresponding to diﬀerent instantiations Teaches(p, c) for diﬀerent professors p and
the same course c: once we have Teaches(p, c) = true, we must have Teaches(p′, c) = false
for all p′ ̸= p. In order to incorporate this correlation, we would have to deﬁne a generative
process that “selects” the relation variables Teaches(p, c) in some sequence, in a way that allows
each Teaches(p′, c) to depend on all of the preceding variables Teaches(p, c).
This induces

234
Chapter 6. Template-Based Representations
dependency models with dense connectivity, an arbitrary number of parents per variable (in the
ground network), and a fairly complex dependency structure.
An alternative approach is to use a diﬀerent encoding for the course-instructor relationship.
In logical languages, an alternative mechanism for relating objects to each other is via functions.
A function, or object-valued attribute takes as argument a tuple of objects from a given set of
object-valued
attribute
classes, and returns a set of objects in another class. Thus, for example, rather than having
a relation Mother(P1, P2), we might use a function Mother-of(P1) 7→Person that takes, as
argument, a person-object p1, and returns the person object p2, which is p1’s mother. In this
case, the return-value of the function is just a single object, but, in general, we can deﬁne
functions that return an entire set of objects. In our University example, the relation Teaches
deﬁnes the function Courses-Of, which maps from professors to the courses they teach, and the
function Instructor, which maps from courses to the professors that teach them. We note that
these functions are inverses of each other: We have that a professor p is in Instructor(c) if and
only if c is in Courses-Of(p).
As we can see, we can easily convert between set-valued functions and relations. Indeed, as
long as the relational structure is ﬁxed, the decision on which representation to use is largely
a matter of convenience (or convention). However, once we introduce probabilistic models over
the relational structure, the two representations lend themselves more naturally to quite diﬀerent
types of model. Thus, for example, if we encode the course-instructor relationship as a function
from professors to courses, then rather than select pairwise relations at random, we might select,
for any professor p, the set of courses Courses-Of(p). We can deﬁne a distribution over sets in
two components: a distribution over the size ℓof the set, and a distribution that then selects ℓ
distinct objects that will make up the set.
Example 6.24
Assume we want to deﬁne a probability distribution over the set Courses-Of(p) of courses taught by
a professor p. We may ﬁrst deﬁne a distribution over the number ℓof courses c in Courses-Of(p).
This distribution may depend on properties of the professor, such as her department or her level
of seniority.
Given the size ℓ, we now have to select the actual set of ℓcourses taught by p.
We can deﬁne a model that selects ℓcourses independently from among the set of courses at the
university. This choice can depend on properties of both the professor and the course. For example,
if the professor’s specialization is in artiﬁcial intelligence, she is more likely to teach a course in
that area than in operating systems. Thus, the probability of selecting c to be in Courses-Of(p)
depends both on Topic(c) and on Research-Area(p). Importantly, since we have already chosen
ℓ= |Courses-Of(p)|, we need to ensure that we actually select ℓdistinct courses, that is, we must
sample from the courses without replacement. Thus, our ℓsampling events for the diﬀerent courses
cannot be completely independent.
While useful in certain settings, this model does not solve the fundamental problem. For
example, although it allows us to enforce that every professor teaches between two and four
courses, it still leaves open the possibility that a single course is taught by ten professors. We can,
of course, consider a model that reverses the direction of the function, encoding a distribution
over the instructors of each course rather than the courses taught by a professor, but this solution
would simply raise the converse problem of the possibility that a single professor teaches a large
number of classes.
It follows from this discussion that it is diﬃcult, in generative (directed) representations, to

deﬁne distributions over relational structures that guarantee (or prefer) certain structural

6.6. Structural Uncertainty ⋆
235
properties of the relation. For example, there is no natural way in which we can construct
a probabilistic model that exhibits (a preference for) transitivity, that is, one satisfying that if
R(u, v) and R(v, w) then (it is more likely that) R(u, w).
These problems have a natural solution within the undirected framework. For example, a
preference for transitivity can be encoded simply as a template feature that ascribes a high value
to the (template) event
R(U, V ) = true, R(V, W) = true, R(U, W) = true.
A (soft) constraint enforcing at most one instructor per course can be encoded similarly as a
(very) low potential on the template event
Teaches(P, C) = true, Teaches(P ′, C) = true.
A constraint enforcing at least one instructor per course cannot be encoded in the framework
of relational Markov networks, which allow only features with a bounded set of arguments.
However, it is not diﬃcult to extend the language to include potentials over unbounded sets of
variables, as long as these potentials have a compact, ﬁnite-size representation. For example,
we could incorporate an aggregator feature that counts the number tp of objects c such that
Teaches(p, c), and introduce a potential over the value of tp. This extension would allow us
to incorporate arbitrary preferences about the number of courses taught by a professor.
At
the same time, the model could also include potentials over the aggregator ic that counts the
number of instructors p for a course c. Thus, we can simultaneously include global preferences
on both sides of the relation between courses and professors.
However, while this approach addresses the issue of expressing such constraints, it leaves
unresolved the problem of the complexity of the resulting ground network.
In all of these
examples, the induced ground network is very densely connected, with a ground variable for
every potential edge in the relational graph (for example, R(u, v)), and a factor relating every
pair or even every triple of these variables. In the latter examples, involving the aggregator, we
have potentials whose scope is unbounded, containing all of the ground variables R(u, v).
6.6.2
Object Uncertainty
So far, we have focused our discussion on representing probabilistic models about the presence
or absence of certain relations, given a set of base objects. One can also consider settings in
which even the set of objects in the world is not predetermined, and so we wish to deﬁne a
probability distribution over this set.
Perhaps the most common setting in which this type of reasoning arises is in situations where
diﬀerent objects in our domain may be equal to each other. This situation arises quite often.
For example, a single person can be student #34 in CS101, student #57 in Econ203, the eldest
daughter of John and Mary, the girlfriend of Tom, and so on.
One solution is to allow objects in the domain to correspond to diﬀerent “names,” or ways of
referring to an object, but explicitly reason about the probability that some of these names refer
to the same object. But how do we model a distribution over equality relationships between the
objects playing diﬀerent roles in the model?
The key insight is to introduce explicitly into the model the notion of a “reference” to an
object, where the same object can be referred to in several diﬀerent ways. That is, we include in

236
Chapter 6. Template-Based Representations
the model objects that correspond to the diﬀerent “references” to the object. Thus, for example,
we could have a class of “person objects” and another class for “person reference objects.” We
can use a relation-based representation in this setting, using a relation Refers-to(r, p) that is true
whenever the reference r refers to a person p. However, we must also introduce uniqueness
constraints to ensure that a reference r refers to precisely a single person p. Alternatively, a more
natural approach is to use a function, or object-valued attribute, Referent(r), which designates
the person to whom r refers. This approach automatically enforces the uniqueness constraints,
and it is thus perhaps more appropriate to this application.
In either case, the relationship between references and the objects to which they refer is
generally probabilistic and interacts probabilistically with other attributes in the domain. In
particular, we would generally introduce factors that model the similarity of the properties of a
“reference object” r and those of the true object p to which it refers. These attribute similarity
potentials can be constructed to allow for noise and variation. For example, we can model the
fact that a person whose name is “John Franklin Adams” may decide to go by “J.F. Adams” in
one setting and “Frank Adams” in another, but is unlikely to go by the name “Peggy Smith.”
We can also model the fact that a person may decide to “round down” his or her reported age
in some settings (for example, social interactions) but not in others (for example, tax forms).
The problem of determining the correspondence between references and the entities to which
they refer is an instance of the correspondence problem, which is described in detail in box 12.D.
correspondence
Box 6.D describes an application of this type of model to the problem of matching bibliographical
citations.
In an alternative approach, we might go one step further, we can eliminate any mention of
the true underlying objects, and restrict the model only to object references. In this solution,
the domain contains only “reference objects” (at least for some classes).
Now, rather than
mapping references to the object to which they refer, we simply allow for diﬀerent references
to “correspond” to each other. Speciﬁcally, we might include a binary predicate Same-as(r, r′),
which asserts that r and r′ both refer to the same underlying object (not included as an object
in the domain).
To ensure that Same-As is consistent with the semantics of an equality relation, we need to
introduce various constraints on its properties. (Because these constraints are standard axioms
of equality, we can include them as part of the formalism rather than require each user to
specify them.) First, using the ideas described in section 6.6.1, we can introduce undirected
(hard) potentials to constrain the relation to satisfy:
•
Reﬂexivity — Same-As(r, r);
•
Symmetry — Same-As(r, r′) if and only if Same-As(r′, r);
•
Transitivity — Same-As(r, r′) and Same-As(r′, r′′) implies Same-As(r, r′′).
These conditions imply that the Same-As relation deﬁnes an equivalence relation on reference
objects, and thus partitions them into mutually exclusive and exhaustive equivalence classes.
Importantly, however, these constraints can only be encoded in an undirected model, and
therefore this approach to dealing with equality only applies in that setting. In addition, we
include in the model attribute similarity potentials, as before, which indicate the extent to which
we expect attributes or predicates for two Same-As reference objects r and r′ to be similar to
each other. This approach, applied to a set of named objects, tends to cluster them together

6.6. Structural Uncertainty ⋆
237
into groups whose attributes are similar and that participate in relations with objects that are
also in equivalent groups.
There are, however, several problems with the reference-only solution. First, there is no natural
place to put factors that should apply once per underlying entity.
Example 6.25
Suppose we are interested in inferring people’s gender from their names. We might have a potential
saying that someone named “Alex” is more likely to be male than female. But if we make this
a template factor on {Name(R), Gender(R)} where R ranges over references, then the factor will
apply many times to people with many references. Thus, the probability that a person named “Alex”
is male will increase exponentially with the number of references to that person.
A related but more subtle problem is the dependence of the outcome of our inference on the
number of references.
Example 6.26
Consider a very simple example where we have only references to one type of object and only the at-
tribute A, which takes values 1, 2, 3. For each pair of object references r, r′ such that Same-As(r, r′)
holds, we have an attribute similarity potential relating A(r) and A(r′): the cases of A(r) = A(r′)
have the highest weight w; A(r) = 1, A(r′) = 3 has very low weight; and A(r) = 2, A(r′) = 1
and A(r) = 2, A(r′) = 3 both have the same medium potential q. Now, consider the graph of
people related by the Same-As relation: since Same-As is an equivalence relation, the graph is a
set of mutually exclusive and exhaustive partitions, each corresponding to a set of references that
correspond to the same object. Now, assume we have a conﬁguration of evidence where we observe
ki references with A(r) = i, for i = 1, 2, 3. The most likely assignment relative to this model will
have one cluster with all the A(r) = 1 references, and another with all the A(r) = 3 references.
What about the references with A(r) = 2?
Somewhat surprisingly, their disposition depends on the relative sizes of the clusters. To under-
stand why, we ﬁrst note that (assuming w > 1) there are only three solutions with reasonably high
probability: three separate clusters; a “1+2” and a “3” cluster; and a “1” and a “2+3” cluster. All other
solutions have much lower probability, and the discrepancy decays exponentially with the size of
the domain. Now, consider the case where k2 = 1, so that there only one r∗with A(r) = 2. If
we add r∗to the “1” cluster, we introduce an attribute similarity potential between A(r∗) and all
of the A(r)’s in the “1” cluster. This multiplies the overall probability of the conﬁguration by qk1.
Similarly, if we add r∗to the “3” cluster, the probability of the conﬁguration is multiplied by qk3.
Thus, if q < 1, the reference r∗is more likely to be placed in the smaller of the two clusters; if
q > 1, it is more likely to be placed in the larger cluster. As k2 grows, the optimal solution may
now be one where we put the 2’s into their own, separate cluster; the beneﬁt of doing so depends on
the relative sizes of the diﬀerent parameters q, w, k1, k2, k3.
Thus, in this type of model, the resulting posterior is often highly peaked, and the probabilities
of the diﬀerent high-probability outcomes very sensitive to the parameters. By contrast, a model
where each equivalence cluster is associated with a single actual object is a lot “smoother,” for
the number of attribute similarity potentials induced by a cluster of references grows linearly,
not quadratically, in the size of the cluster.

238
Chapter 6. Template-Based Representations
Box 6.D — Case Study: Object Uncertainty and Citation Matching. Being able to browse the
network of citations between academic works is a valuable tool for research. For instance, given one
citation to a relevant publication, one might want a list of other papers that cite the same work.
There are several services that attempt to construct such lists automatically by extracting citations
from online papers. This task is diﬃcult because the citations come in a wide variety of formats,
and often contain errors — owing both to the original author and to the vagaries of the extraction
process. For example, consider the two citations:
Elston R, Stewart A. A General Model for the Genetic Analysis of
Pedigree Data.
Hum.
Hered.
1971;21:523–542.
Elston RC, Stewart J (1971):
A general model for the analysis of
pedigree data.
Hum Hered 21523–542.
These citations refer to the same paper, but the ﬁrst one gives the wrong ﬁrst initial for J. Stewart,
and the second one omits the word “genetic” in the title. The colon between the journal volume and
page numbers has also been lost in the second citation. A citation matching system must handle
this kind of variation, but must also avoid lumping together distinct papers that have similar titles
and author lists.
Probabilistic object-relational models have proven to be an eﬀective approach to this problem.
One way to handle the inherent object uncertainty is to use a directed model with a Citation class,
as well as Publication and Author classes. The set of observed Citation objects can be included in
the object skeleton, but the number of Publication and Author objects is unknown.
A directed object-relational model for this problem (based roughly on the model of Milch et al.
(2004)) is shown in ﬁgure 6.D.1a. The model includes random variables for the sizes of the Author
and Publication classes. The Citation class has an object-valued attribute PubCited(C), whose value
is the Publication object that the citation refers to. The Publication class has a set-valued attribute
Authors(P), indicating the set of authors on the publication. These attributes are given very simple
CPDs: for PubCited(C), we use a uniform distribution over the set of Publication objects, and for
Authors(P) we use a prior for the number of contributors along with a uniform selection distribution.
To complete this model, we include string-valued attributes Name(A) and Title(P), whose CPDs
encode prior distributions over name and title strings (for now, we ignore other attributes such as
date and journal name). Finally, the Citation class has an attribute Text(C), containing the observed
text of the citation.
The citation text attribute depends on the title and author names of the
publication it refers to; its CPD encodes the way citation strings are formatted, and the probabilities
of various errors and abbreviations.
Thus, given observed values for all the Text(ci) attributes, our goal is to infer an assignment
of values to the PubCited attributes — which induces a partition of the citations into coreferring
groups.
To get a sense of how this process works, consider the two preceding citations.
One
hypothesis, H1, is that the two citations c1 and c2 refer to a single publication p1, which has
“genetic” in its title. An alternative, H2, is that there is an additional publication p2 whose title is
identical except for the omission of “genetic,” and c2 refers to p2 instead. H1 obviously involves an
unlikely event — a word being left out of a citation; this is reﬂected in the probability of Text(c2)
given Title(p1). But the probability of H2 involves an additional factor for Title(p2), reﬂecting the
prior probability of the string “A general model for the analysis of pedigree data” under our model
of academic paper titles. Since there are so many possible titles, this probability will be extremely
small, allowing H1 to win out. As this example shows, probabilistic models of this form exhibit

6.6. Structural Uncertainty ⋆
239
(a)
Text
Authors a
Publications p
Citations c
#Pubs
Name
Title
Authors
PubCited
Same(C1,C3)
Text(C2)
Text(C1)
Text(C3)
(b)
#Authors
a ∈Authors(PubCited(c))
p = PubCited(c)
Same(C2,C3)
fequiv
f1
fk
f1
fk
Same(C1,C2)
. . .
f1
fk
. . .
. . .
Figure 6.D.1 — Two template models for citation-matching (a) A directed model. (b) An undirected
model instantiated for three citations.

240
Chapter 6. Template-Based Representations
a built-in Ockham’s razor eﬀect: the highest probability goes to hypotheses that do not include
any more objects — and hence any more instantiated attributes — than necessary to explain the
observed data.
Another line of work (for example, Wellner et al. (2004)) tackle the citation-matching problem
using undirected template models, whose ground instantiation is a CRF (as in section 4.6.1). As we
saw in the main text, one approach is to eliminate the Author and Publication classes and simply
reason about a relation Same(C, C′) between citations (constrained to be an equivalence relation).
Figure 6.D.1b shows an instantiation of such a model for three citations. For each pair of citations
C, C′, there is an array of factors φ1, . . . , φk that look at various features of Text(C) and Text(C′)
— whether they have same surname for the ﬁrst author, whether their titles are within an edit
distance of two, and so on — and relate these features to Same(C1, C2). These factors encode
preferences for and against coreference more explicitly than the factors in the directed model.
However, as we have discussed, a reference-only model produces overly peaked posteriors that are
very sensitive to parameters and to the number of mentions. Moreover, there are some examples
where pairwise compatibility factors are insuﬃcient for ﬁnding the right partition. For instance,
suppose we have three references to people: “Jane,” which is clearly a female’s given name; “Smith,”
which is clearly a surname; and “Stanley,” which could be a surname or a male’s given name. Any
pair of these references could refer to the same person: there could easily be a Jane Smith, a Stanley
Smith, or a Jane Stanley. But it is unlikely that all three names corefer. Thus, a reasonable approach
uses an undirected model that has explicit (hidden) variables for each entity and its attributes. The
same potentials can be used as in the reference-only model. However, due to the use of undirected
dependencies, we can allow the use of a much richer feature set, as described in box 4.E.
Systems that use template-based probabilistic models can now achieve accuracies in the high
90s for identifying coreferent citations. Identifying multiple mentions of the same author is harder;
accuracies vary considerably depending on the data set, but tend to be around 70 percent. These
models are also useful for segmenting citations into ﬁelds such as the title, author names, journal,
and date. This is done by treating the citation text not as a single attribute but as a sequence of
tokens (words and punctuation marks), each of which has an associated variable indicating which
ﬁeld it belongs to. These “ﬁeld” variables can be thought of as the state variables in a hidden
Markov model in the directed setting, or a conditional random ﬁeld in the undirected setting (as
in box 4.E). The resulting model can segment ambiguous citations more accurately than one that
treats each citation in isolation, because it prefers for segmentations of coreferring citations to be
consistent.
6.7
Summary
The representation languages discussed in earlier chapters — Bayesian networks and Markov
networks — allow us to write down a model that encodes a speciﬁc probability distribution
over a ﬁxed, ﬁnite set of random variables. In this chapter, we have provided a general frame-
work for deﬁning templates for fragments of the probabilistic model. These templates can be
reused both within a single model, and across multiple models of diﬀerent structures. Thus,

a template-based representation language allows us to encode a potentially inﬁnite set
of distributions, over arbitrarily large probability spaces. The rich models that one can

6.7. Summary
241
produce from such a representation can capture complex interactions between many
interrelated objects, and thus utilize many pieces of evidence that we may otherwise
ignore; as we have seen, these pieces of evidence can provide substantial improvements
in the quality of our predictions.
We described several diﬀerent representation languages: one specialized to temporal rep-
resentations, and several that allow the speciﬁcation of models over general object-relational
domains. In the latter category, we ﬁrst described two directed representations: plate models,
and probabilistic relational models. The latter allow a considerably richer set of dependencies
to be encoded, but at the cost of both conceptual and computational complexity.
We also
described an undirected representation, which, by avoiding the need to guarantee acyclicity and
coherent local probability models, avoids some of the complexities of the directed models. As we
discussed, the ﬂexibility of undirected models is particularly valuable when we want to encode a
probability distribution over richer representations, such as the structure of the relational graph.
There are, of course, other ways to produce these large, richly structured models.
Most
obviously, for any given application, we can deﬁne a procedural method that can take a skeleton,
and produce a concrete model for that speciﬁc set of objects (and possibly relations).
For
example, we can easily build a program that takes a pedigree and produces a Bayesian network
for genetic inheritance over that pedigree. The beneﬁt of the template-based representations that
we have described here is that they provide a uniform, modular, declarative language for models
of this type.
Unlike specialized representations, such a language allows the template-based
model to be modiﬁed easily, whether by hand or as part of an automated learning algorithm.
Indeed, learning is perhaps one of the key advantages of the template-based representations. In
particular, as we will discuss, the model is learned at the template level, allowing a model to
be learned from a domain with one set of objects, and applied seamlessly to a domain with a
completely diﬀerent set of objects (see section 17.5.1.2 and section 18.6.2).
In addition, by making objects and relations ﬁrst-class citizens in the model, we have laid
a foundation for the option of allowing probability distributions over probability spaces that
are signiﬁcantly richer than simply properties of objects.
For example, as we saw, we can
consider modeling uncertainty about the network of interrelationships between objects, and
even about the actual set of objects included in our domain.
These extensions raise many
important and diﬃcult questions regarding the appropriate type of distribution that one should
use for such richly structured probability spaces. These questions become even more complex
as we introduce more of the expressive power of relational languages, such as function symbols,
quantiﬁers, and more. These issues are an active area of research.
These representations also raise important questions regarding inference.
At ﬁrst glance,
the problem appears straightforward: The semantics for each of our representation languages
depends on instantiating the template-based model to produce a speciﬁc ground network; clearly,
we can simply run standard inference algorithms on the resulting network. This approach is
has been called knowledge-based model construction, because a knowledge-base (or skeleton)
knowledge-based
model
construction
is used to construct a model.
However, this approach is problematic, because the models
produced by this process can pose a signiﬁcant challenge to inference algorithms. First, the
network produced by this process is often quite large — much larger than models that one
can reasonably construct by hand. Second, such models are often quite densely connected, due
to the multiple interactions between variables. Finally, structural uncertainty, both about the
relations and about the presence of objects, also makes for densely connected models. On the

242
Chapter 6. Template-Based Representations
other side, such models often have unique characteristics, such as multiple similar fragments
across the network, or large amounts of context-speciﬁc independence, which could, perhaps, be
exploited by an appropriate choice of inference algorithm. Chapter 15 presents some techniques
for addressing the inference problems in temporal models. The question of inference in the
models deﬁned by the object-relational frameworks — and speciﬁcally of inference algorithms
that exploit their special structure — is very much a topic of current work.
6.8
Relevant Literature
Probabilistic models of temporal processes go back many years. Hidden Markov models were
discussed as early as Rabiner and Juang (1986), and expanded on in Rabiner (1989). Kalman ﬁlters
were ﬁrst described by Kalman (1960). The ﬁrst temporal extension of probabilistic graphical
models is due to Dean and Kanazawa (1989), who also coined the term dynamic Bayesian
network.
Much work has been done on deﬁning various representations that are based on
hidden Markov models or on dynamic Bayesian networks; these include generalizations of the
basic framework, or special cases that allow more tractable inference. Examples include mixed-
memory Markov models (Saul and Jordan 1999); variable-duration HMMs (Rabiner 1989) and their
extension segment models (Ostendorf et al. 1996); factorial HMMs (Ghahramani and Jordan 1997);
and hierarchical HMMs (Fine et al. 1998; Bui et al. 2001). Smyth, Heckerman, and Jordan (1997) is
a review paper that was inﬂuential in providing a clear exposition of the connections between
HMMs and DBNs. Murphy and Paskin (2001) show how hierarchical HMMs can be reduced to
DBNs, a connection that provided a much faster inference algorithm than previously proposed
for this representation. Murphy (2002) provides an excellent tutorial on the topics of dynamic
Bayesian networks and related representations.
Nodelman et al. (2002, 2003) build on continuous-time Markov processes to deﬁne continuous
continuous time
Bayesian network
time Bayesian networks.
As the name suggests, this representation is similar to a dynamic
Bayesian network but encodes a probability distribution over trajectories over a continuum of
time points.
The topic of integrating object-relational frameworks and probabilistic representations has
received much attention over the past few years. Getoor and Taskar (2007) contains reviews of
many of the important contributions, and citations to others. Work on this topic goes back to the
idea of knowledge-based model construction, which was proposed in the early 1990s; Wellman,
knowledge-based
model
construction
Breese, and Goldman (1992) review some of this earlier work. These ideas were then extended
and formalized, using logic programming as a foundation (Poole 1993a; Ngo and Haddawy 1996;
Kersting and De Raedt 2007).
Plate models were introduced by Gilks, Thomas, and Spiegelhalter (1994) and Buntine (1994) as
a language for sharing parameters within and between models. Probabilistic relational models
were proposed in Friedman et al. (1999); see Getoor et al. (2007) for a more detailed presentation.
Heckerman, Meek, and Koller (2007) deﬁne a language that uniﬁes plate models and probabilistic
relational models, which was the inspiration for our presentation of PRMs in terms of contingent
dependencies.
Undirected probabilistic models for relational domains originated with the framework of rela-
tional Markov networks of Taskar et al. (2002, 2007). Richardson and Domingos (2006) provide
a particularly elegant representation of features, in terms of logical formulas. In a Markov logic

6.9. Exercises
243
network (MLN), there is no separation between the speciﬁcation of cliques and the speciﬁcation
of features in the potential. Rather, the model is deﬁned in terms of a collection of logical
formulas, each associated with a weight.
Getoor et al. (2002) discuss some strategies for modeling structural uncertainty in a directed
setting. Taskar et al. (2002) investigate the same issues in an undirected setting, and demonstrate
the advantages of the increased ﬂexibility. Reasoning about object identity has been used in
various applications, including data association (Pasula et al. 1999), coreference resolution in
natural language text (McCallum and Wellner 2005; Culotta et al. 2007), and the citation matching
application discussed in box 6.D (Pasula et al. 2002; Wellner et al. 2004; Milch et al. 2004; Poon
and Domingos 2007). Milch et al. (2005, 2007) deﬁne BLOG (Bayesian Logic), a directed language
explicitly designed to model uncertainty over the number of objects in the domain.
In addition to the logic-based representations we discuss in this chapter, a very diﬀer-
ent perspective on incorporating template-based structure in probabilistic models utilizes a
programming-language framework. Here, we can view a random variable as a stochastic func-
tion from its inputs (its parents) to its output. If we explicitly deﬁne the stochastic function, one
can then reuse it in in multiple places. More importantly, one can deﬁne functions that call
other functions, or perhaps even functions that recursively call themselves. Important languages
based on this framework include probabilistic context-free grammars, which play a key role in
probabilistic
context-free
grammar
statistical models for natural language (see, for example, Manning and Schuetze (1999)) and in
modeling RNA secondary structure (see, for example, Durbin et al. 1998), and object-oriented
Bayesian networks (Koller and Pfeﬀer 1997; Pfeﬀer et al. 1999), which generalizes encapsulated
Bayesian networks to allow for repeated elements.
6.9
Exercises
Exercise 6.1
Consider a temporal process where the state variables at time t depend directly not only on the variables
at time t −1, but rather on the variables at time t −1, . . . , t −k for some ﬁxed k. Such processes are
called semi-Markov of order k.
semi-Markov
order k
a. Extend deﬁnition 6.3 and deﬁnition 6.4 to richer notions, that encode such a kth order semi-Markov
processes.
b. Show how you can convert a kth order Markov process to a regular (ﬁrst-order) Markov process
representable by a DBN over an extended set of state variables. Describe both the variables and the
transition model.
Exercise 6.2⋆
Markov models of diﬀerent orders are the standard representation of text sequences. For example, in
a ﬁrst-order Markov model, we deﬁne our distribution over word sequences in terms of a probability
P(W (t) | W (t−1)).
This model is also called a bigram model, because it requires that we collected
statistics over pairs of words. A second-order Markov model, often called a trigram model, deﬁnes the
distribution is terms of a probability P(W (t) | W (t−1), W (t−2)).
Unfortunately, because the set of words in our vocabulary is very large, trigram models deﬁne very large
CPDs with very many parameters. These are very hard to estimate reliably from data (see section 17.2.3).
One approach for producing more robust estimates while still making use of higher-order dependencies is
shrinkage. Here, we deﬁne our transition model to be a weighted average of transition models of diﬀerent
shrinkage

244
Chapter 6. Template-Based Representations
orders:
P(W (t) | W (t−1), W (t−2)) = α0(W (t−1), W (t−2))Q0(W (t))+
α1(W (t−1), W (t−2))Q1(W (t) | W (t−1)) + α2(W (t−1), W (t−2))Q2(W (t) | W (t−1), W (t−2)),
where the Qi’s are diﬀerent transition models, and the αi’s are nonnegative coeﬃcients such that, for
every W (t−1), W (t−2),
α0(W (t−1), W (t−2)) + α1(W (t−1), W (t−2)) + α2(W (t−1), W (t−2)) = 1.
Show how we can construct a DBN model that gives rise to equivalent dynamics using standard CPDs, by
introducing a new hidden variable S(t). This model is called mixed-memory HMM.
mixed-memory
HMM
Exercise 6.3
In this exercise, we construct an HMM model that allows for a richer class of distributions over the duration
for which the process stays in a given state.
a. Consider an HMM where the hidden variable has k states, and let P(s′
j | si) denote the transition
model. Assuming that the process is at state si at time t, what is the distribution over the number of
steps until it ﬁrst transitions out of state si (that is, the smallest number d such that S(t+d) ̸= si).
b. Construct a DBN model that allows us to incorporate an arbitrary distribution over the duration di that
a process stays in state si after it ﬁrst transitions to si. Your model should allow the distribution over
di to depend on si. Do not worry about parameterizing the distribution over di. (Hint: Your model
can include variables whose value changes deterministically.) This type of model is called a duration
duration HMM
HMM.
Exercise 6.4⋆
A segment HMM is a Markov chain over the hidden states, but where each state emits not a single symbol
segment HMM
as output, but rather a string of unknown length.
Thus, at each state S(t) = s, the model selects
a segment length L(t), using a distribution that can depend on s. The model then emits a segment
Y (t,1), . . . , Y (t,L(t)) of length L(t).
In this exercise, we assume that the distribution on the output
segment is modeled by a separate HMM Hs. Write down a 2-TBN model that encodes this model. (Hint:
Use your answer to exercise 6.3.)
Exercise 6.5⋆
A hierarchical HMM is similar to the segment HMM, except that there is no explicit selection of the segment
hierarchical HMM
length. Rather, the HMM at a state calls a “subroutine” HMM Hs that deﬁnes the output at the state s;
when the “subroutine” HMM enters a ﬁnish-state, the control returns to the top-level HMM, which then
transitions to its next state. This hierarchical HMM (with three levels) is precisely the framework used as
the standard speech recognition architecture.
a. Show how a three-level hierarchical HMM can be represented as a DBN. (Hint: Use “ﬁnish variables”
— binary variables that are true when a lower-level HMMs ﬁnishes its transition.)
b. Explain how you would modify the hierarchical HMM framework to deal with a motion tracking task,
where, for example, the higher-level HMM represents motion between ﬂoors, the mid-level HMM motion
between corridors, and the lowest-level HMM motion between rooms. (Hint: Consider situations where
there are multiple staircases between ﬂoors.)
Exercise 6.6⋆
Consider the following data association problem. We track K moving objects u1, . . . , uK, using readings
data association
obtained over a trajectory of length T. Each object k has some (unknown) basic appearance Ak, and some
position X(t)
k
at every time point t. Our sensor provides, at each time point t, a set of L noisy sensor

6.9. Exercises
245
readings, each corresponding to one object: for each l = 1, . . . , L, it returns B(t)
l
— the measured object
appearance, and Y (t)
l
— the measured object position. Unfortunately, our sensor cannot determine the
identity of the sensed objects, so sensed object l does not generally correspond to the true object l. In
fact, the labeling of the sensed objects is completely arbitrary — all labelings are equally likely.
Write down a DBN that represents the dynamics of this model.
Exercise 6.7
Consider a template-level CPD where A(U) depends on B(U, V ), allowing for situations where the ground
variable A(u) can depend on unbounded number of ground variables B(u, v). As discussed in the text,
we can specify the parameterization for the resulting CPD in various ways: we can use a symmetric
noisy-or or sigmoid model, or deﬁne a dependency of A(u) on some aggregated statistics of the parent
set {B(u, v)}. Assume that both A(U) and B(U, V ) are binary-valued.
Show that both a symmetric noisy-or model and a symmetric logistic model can be formulated easily using
an aggregator CPDs.
aggregator CPD
Exercise 6.8
Consider the template dependency graph for a model MPRM, as speciﬁed in deﬁnition 6.13. Show that
if the template dependency graph is acyclic, then for any skeleton κ, the ground network BMPRM
κ
is also
acyclic.
Exercise 6.9
Let MPlate be a plate model, and assume that its template dependency graph contains a cycle. Let κ be
any skeleton such that Oκ[Q] ̸= ∅for every class Q. Show that BMPlate
κ
is necessarily cyclic.
Exercise 6.10⋆⋆
Consider the cyclic dependency graph for the Genetics model shown in ﬁgure 6.9b. Clearly, for any valid
pedigree — one where a person cannot be his or her own ancestor — the ground network is acyclic. We
now describe a reﬁnement of the dependency graph structure that would allow us to detect such acyclicity
in this and other similar settings. Here, we assume for simplicity that all attributes in the guards are part
of the relational skeleton, and therefore not part of the probabilistic model.
Let γ denote a tuple of objects from our skeleton. Assume that we have some prior knowledge about our
domain in the following form: for any skeleton κ, there necessarily exists a partial ordering ≺on tuples of
objects γ that is transitive (γ1 ≺γ2 and γ2 ≺γ3 implies γ1 ≺γ3) and irreﬂexive (γ ̸≺γ). For example,
in the Genetics example, we can use ancestry to deﬁne our ordering, where u′ ≺u whenever u′ is an
ancestor of u. We further assume that some of the guards used in the probabilistic model imply ordering
constraints.
More precisely, let B(U ′) ∈PaU(A). We say that a pair of assignments γ to U and γ′ to U ′ is valid if
they agree on the assignment to the overlapping variables in U ∩U ′ and if they are consistent with the
guard for A. The valid pairs are those that lead to actual edges B(γ′) →A(γ) in the ground Bayesian
network. (The deﬁnition here is slightly diﬀerent than deﬁnition 6.12 because there γ′ is an assignment to
the variables in U ′ but not in U.) We say that the dependence of A on B is ordering-consistent if, for any
valid pair of assignments γ to U and γ′ to U ′, we have that γ′ ≺γ. Continuing our example, consider
the dependence of Genotype(U) on Genotype(V ) subject to the guard Mother(V, U). Here, for any pair of
assignments u to U and v to V such that the guard Mother(v, u) holds, we have that v ≺u. Thus, this
dependence is ordering-consistent.
We now deﬁne the following extension to our dependency graph. Let U ′(B) ∈PaU(A).
•
If U ′ = U, we introduce an edge from B to A whose color is yellow.
•
If the dependence is ordering-consistent, we introduce an edge from B to A whose color is green.
•
Otherwise, we introduce an edge from B to A whose color is red.
Prove that if every cycle in the colored dependency graph for MPRM has at least one green edge and no
red edges, then for any skeleton satisfying the ordering constraints, the ground BN BMPRM
κ
is acyclic.


7
Gaussian Network Models
Although much of our presentation focuses on discrete variables, we mentioned in chapter 5
that the Bayesian network framework, and the associated results relating independencies to
factorization of the distribution, also apply to continuous variables. The same statement holds
for Markov networks.
However, whereas table CPDs provide a general-purpose mechanism
for describing any discrete distribution (albeit potentially not very compactly), the space of
possible parameterizations in the case of continuous variables is essentially unbounded. In this
chapter, we focus on a type of continuous distribution that is of particular interest: the class of
multivariate Gaussian distributions. Gaussians are a particularly simple subclass of distributions
that make very strong assumptions, such as the exponential decay of the distribution away
from its mean, and the linearity of interactions between variables. While these assumptions
are often invalid, Gaussians are nevertheless a surprisingly good approximation for many real-
world distributions. Moreover, the Gaussian distribution has been generalized in many ways, to
nonlinear interactions, or mixtures of Gaussians; many of the tools developed for Gaussians can
be extended to that setting, so that the study of Gaussian provides a good foundation for dealing
with a broad class of distributions.
In the remainder of this chapter, we ﬁrst review the class of multivariate Gaussian distributions
and some of its properties. We then discuss how a multivariate Gaussian can be encoded using
probabilistic graphical models, both directed and undirected.
7.1
Multivariate Gaussians
7.1.1
Basic Parameterization
We have already described the univariate Gaussian distribution in chapter 2. We now describe its
generalization to the multivariate case. As we discuss, there are two diﬀerent parameterizations
for a joint Gaussian density, with quite diﬀerent properties.
The univariate Gaussian is deﬁned in terms of two parameters: a mean and a variance.
In its most common representation, a multivariate Gaussian distribution over X1, . . . , Xn is
characterized by an n-dimensional mean vector µ, and a symmetric n × n covariance matrix Σ;
mean vector
covariance matrix
the density function is most often deﬁned as:
p(x) =
1
(2π)n/2|Σ|1/2 exp

−1
2(x −µ)T Σ−1(x −µ)

(7.1)

248
Chapter 7. Gaussian Network Models
where |Σ| is the determinant of Σ.
We extend the notion of a standard Gaussian to the multidimensional case, deﬁning it to be
standard
Gaussian
a Gaussian whose mean is the all-zero vector 0 and whose covariance matrix is the identity
matrix I, which has 1’s on the diagonal and zeros elsewhere. The multidimensional standard
Gaussian is simply a product of independent standard Gaussians for each of the dimensions.
In order for this equation to induce a well-deﬁned density (that integrates to 1), the matrix Σ
must be positive deﬁnite: for any x ∈IRn such that x ̸= 0, we have that xT Σx > 0. Positive
positive deﬁnite
deﬁnite matrices are guaranteed to be nonsingular, and hence have nonzero determinant, a
necessary requirement for the coherence of this deﬁnition. A somewhat more complex deﬁnition
can be used to generalize the multivariate Gaussian to the case of a positive semi-deﬁnite
positive
semi-deﬁnite
covariance matrix: for any x ∈IRn, we have that xT Σx ≥0. This extension is useful, since it
allows for singular covariance matrices, which arise in several applications. For the remainder of
our discussion, we focus our attention on Gaussians with positive deﬁnite covariance matrices.
Because positive deﬁnite matrices are invertible, one can also utilize an alternative parameter-
ization, where the Gaussian is deﬁned in terms of its inverse covariance matrix J = Σ−1, called
information matrix (or precision matrix). This representation induces an alternative form for the
information
matrix
Gaussian density. Consider the expression in the exponent of equation (7.1):
−1
2(x −µ)T Σ−1(x −µ)
=
−1
2(x −µ)T J(x −µ)
=
−1
2

xT Jx −2xT Jµ + µT Jµ

.
The last term is constant, so we obtain:
p(x) ∝exp

−1
2xT Jx + (Jµ)T x

.
(7.2)
This formulation of the Gaussian density is generally called the information form, and the vector
information form
h = Jµ is called the potential vector. The information form deﬁnes a valid Gaussian density if
and only if the information matrix is symmetric and positive deﬁnite, since Σ is positive deﬁnite
if and only if Σ−1 is positive deﬁnite. The information form is useful in several settings, some
of which are described here.
Intuitively, a multivariate Gaussian distribution speciﬁes a set of ellipsoidal contours around
the mean vector µ. The contours are parallel, and each corresponds to some particular value
of the density function. The shape of the ellipsoid, as well as the “steepness” of the contours,
are determined by the covariance matrix Σ. Figure 7.1 shows two multivariate Gaussians, one
where the covariances are zero, and one where they are positive. As in the univariate case,
the mean vector and covariance matrix correspond to the ﬁrst two moments of the normal
distribution. In matrix notation, µ = IE[X] and Σ = IE[XXT ] −IE[X]IE[X]T . Breaking this
expression down to the level of individual variables, we have that µi is the mean of Xi, Σi,i
is the variance of Xi, and Σi,j = Σj,i (for i ̸= j) is the covariance between Xi and Xj:
CCov[Xi; Xj] = IE[XiXj] −IE[Xi]IE[Xj].
Example 7.1
Consider a particular joint distribution p(X1, X2, X3) over three random variables.
We can

7.1. Multivariate Gaussians
249
x
y
P(x,y)
P(x,y)
(a)
(b)
x
y
Figure 7.1
Gaussians over two variables X and Y . (a) X and Y uncorrelated. (b) X and Y correlated.
parameterize it via a mean vector µ and a covariance matrix Σ:
µ =


1
−3
4


Σ =


4
2
−2
2
5
−5
−2
−5
8


As we can see, the covariances CCov[X1; X3] and CCov[X2; X3] are both negative. Thus, X3 is
negatively correlated with X1: when X1 goes up, X3 goes down (and similarly for X3 and X2).
7.1.2
Operations on Gaussians
There are two main operations that we wish to perform on a distribution: compute the marginal
distribution over some subset of the variables Y , and conditioning the distribution on some
assignment of values Z = z. It turns out that each of these operations is very easy to perform
in one of the two ways of encoding a Gaussian, and not so easy in the other.

250
Chapter 7. Gaussian Network Models
Marginalization is trivial to perform in the covariance form. Speciﬁcally, the marginal Gaussian
distribution over any subset of the variables can simply be read from the mean and covariance
matrix. For instance, in example 7.1, we can obtain the marginal Gaussian distribution over X2
and X3 by simply considering only the relevant entries in both the mean vector the covariance
matrix. More generally, assume that we have a joint normal distribution over {X, Y } where
X ∈IRn and Y ∈IRm. Then we can decompose the mean and covariance of this joint
distribution as follows:
p(X, Y ) = N
 µX
µY

;
 ΣXX
ΣXY
ΣY X
ΣY Y

(7.3)
where µX ∈IRn, µY ∈IRm, ΣXX is a matrix of size n × n, ΣXY is a matrix of size n × m,
ΣY X = ΣT
XT is a matrix of size m × n and ΣY Y is a matrix of size m × m.
Lemma 7.1
Let {X, Y } have a joint normal distribution deﬁned in equation (7.3). Then the marginal distri-
bution over Y is a normal distribution N (µY ; ΣY Y ).
The proof follows directly from the deﬁnitions (see exercise 7.1).
On the other hand, conditioning a Gaussian on an observation Z = z is very easy to perform
in the information form. We simply assign the values Z = z in equation (7.2). This process
turns some of the quadratic terms into linear terms or even constant terms, and some of the
linear terms into constant terms. The resulting expression, however, is still in the same form as
in equation (7.2), albeit over a smaller subset of variables.
In summary, although the two representations both encode the same information, they have
diﬀerent computational properties. To marginalize a Gaussian over a subset of the variables,

one essentially needs to compute their pairwise covariances, which is precisely generating
the distribution in its covariance form. Similarly, to condition a Gaussian on an obser-
vation, one essentially needs to invert the covariance matrix to obtain the information
form. For small matrices, inverting a matrix may be feasible, but in high-dimensional
spaces, matrix inversion may be far too costly.
7.1.3
Independencies in Gaussians
For multivariate Gaussians, independence is easy to determine directly from the parameters of
the distribution.
Theorem 7.1
Let X = X1, ..., Xn have a joint normal distribution N (µ; Σ). Then Xi and Xj are independent
if and only if Σi,j = 0.
The proof is left as an exercise (exercise 7.2).
Note that this property does not hold in general. In other words, if p(X, Y ) is not Gaussian,
then it is possible that CCov[X; Y ] = 0 while X and Y are still dependent in p. (See exercise 7.2.)
At ﬁrst glance, it seems that conditional independencies are not quite as apparent as marginal
independencies. However, it turns out that the independence structure in the distribution is
apparent not in the covariance matrix, but in the information matrix.

7.2. Gaussian Bayesian Networks
251
Theorem 7.2
Consider a Gaussian distribution p(X1, . . . , Xn) = N (µ; Σ), and let J = Σ−1 be the informa-
tion matrix. Then Ji,j = 0 if and only if p |= (Xi ⊥Xj | X −{Xi, Xj}).
The proof is left as an exercise (exercise 7.3).
Example 7.2
Consider the covariance matrix of example 7.1. Simple algebraic operations allow us to compute its
inverse:
J =


0.3125
−0.125
0
−0.125
0.5833
0.3333
0
0.3333
0.3333


As we can see, the entry in the matrix corresponding to X1, X3 is zero, reﬂecting the fact that they
are conditionally independent given X2.
Theorem 7.2 asserts the fact that the information matrix captures independencies between
pairs of variables, conditioned on all of the remaining variables in the model. These are precisely
the same independencies as the pairwise Markov independencies of deﬁnition 4.10. Thus, we
can view the information matrix J for a Gaussian density p as precisely capturing the pairwise
Markov independencies in a Markov network representing p. Because a Gaussian density is
a positive distribution, we can now use theorem 4.5 to construct a Markov network that is a
unique minimal I-map for p: As stated in this theorem, the construction simply introduces an
edge between Xi and Xj whenever (Xi ⊥Xj | X −{Xi, Xj}) does not hold in p. But this
latter condition holds precisely when Ji,j ̸= 0. Thus, we can view the information matrix

as directly deﬁning a minimal I-map Markov network for p, whereby nonzero entries
correspond to edges in the network.
7.2
Gaussian Bayesian Networks
We now show how we can deﬁne a continuous joint distribution using a Bayesian network.
This representation is based on the linear Gaussian model, which we deﬁned in deﬁnition 5.14.
Although this model can be used as a CPD within any network, it turns out that continuous
networks deﬁned solely in terms of linear Gaussian CPDs are of particular interest:
Deﬁnition 7.1
We deﬁne a Gaussian Bayesian network to be a Bayesian network all of whose variables are
Gaussian
Bayesian network
continuous, and where all of the CPDs are linear Gaussians.
An important and surprising result is that linear Gaussian Bayesian networks are an alternative
representation for the class of multivariate Gaussian distributions. This result has two parts. The
ﬁrst is that a linear Gaussian network always deﬁnes a joint multivariate Gaussian distribution.
Theorem 7.3
Let Y be a linear Gaussian of its parents X1, . . . , Xk:
p(Y | x) = N

β0 + βT x; σ2
.
Assume that X1, . . . , Xk are jointly Gaussian with distribution N (µ; Σ). Then:

252
Chapter 7. Gaussian Network Models
• The distribution of Y is a normal distribution p(Y ) = N
 µY ; σ2
Y

where:
µY
=
β0 + βT µ
σ2
Y
=
σ2 + βT Σβ.
• The joint distribution over {X, Y } is a normal distribution where:
CCov[Xi; Y ] =
k
X
j=1
βjΣi,j.
From this theorem, it follows easily by induction that if B is a linear Gaussian Bayesian network,
then it deﬁnes a joint distribution that is jointly Gaussian.
Example 7.3
Consider the linear Gaussian network X1 →X2 →X3, where
p(X1)
=
N (1; 4)
p(X2 | X1)
=
N (0.5X1 −3.5; 4)
p(X3 | X2)
=
N (−X2 + 1; 3) .
Using the equations in theorem 7.3, we can compute the joint Gaussian distribution p(X1, X2, X3).
For the mean, we have that:
µ2
=
0.5µ1 −3.5 = 0.5 · 1 −3.5 = −3
µ3
=
(−1)µ2 + 1 = (−1) · (−3) + 1 = 4.
The variance of X2 and X3 can be computed as:
Σ22
=
4 + (1/2)2 · 4 = 5
Σ33
=
3 + (−1)2 · 5 = 8.
We see that the variance of the variable is a sum of two terms: the variance arising from its own
Gaussian noise parameter, and the variance of its parent variables weighted by the strength of the
dependence. Finally, we can compute the covariances as follows:
Σ12
=
(1/2) · 4 = 2
Σ23
=
(−1) · Σ22 = −5
Σ13
=
(−1) · Σ12 = −2.
The third equation shows that, although X3 does not depend directly on X1, they have a nonzero
covariance. Intuitively, this is clear: X3 depends on X2, which depends on X1; hence, we expect
X1 and X3 to be correlated, a fact that is reﬂected in their covariance.
As we can see, the
covariance between X1 and X3 is the covariance between X1 and X2, weighted by the strength of
the dependence of X3 on X2.
In general, putting these results together, we can see that the mean and covariance matrix for
p(X1, X2, X3) is precisely our covariance matrix of example 7.1.

7.2. Gaussian Bayesian Networks
253
The converse to this theorem also holds: the result of conditioning is a normal distribution
where there is a linear dependency on the conditioning variables. The expressions for converting
a multivariate Gaussian to a linear Gaussian network appear complex, but they are based on
simple algebra. They can be derived by taking the linear equations speciﬁed in theorem 7.3, and
reformulating them as deﬁning the parameters βi in terms of the means and covariance matrix
entries.
Theorem 7.4
Let {X, Y } have a joint normal distribution deﬁned in equation (7.3). Then the conditional density
p(Y | X) = N

β0 + βT X; σ2
,
is such that:
β0
=
µY −ΣY XΣ−1
XXµX
β
=
Σ−1
XXΣY X
σ2
=
ΣY Y −ΣY XΣ−1
XXΣXY .
This result allows us to take a joint Gaussian distribution and produce a Bayesian network,
using an identical process to our construction of a minimal I-map in section 3.4.1.
Theorem 7.5
Let X = {X1, . . . , Xn}, and let p be a joint Gaussian distribution over X. Given any ordering
X1, . . . , Xn over X, we can construct a Bayesian network graph G and a Bayesian network B
over G such that:
1. PaG
Xi ⊆{X1, . . . , Xi−1};
2. the CPD of Xi in B is a linear Gaussian of its parents;
3. G is a minimal I-map for p.
The proof is left as an exercise (exercise 7.4). As for the case of discrete networks, the minimal
I-map is not unique: diﬀerent choices of orderings over the variables will lead to diﬀerent
network structures. For example, the distribution in ﬁgure 7.1b can be represented either as the
network where X →Y or as the network where Y →X.
This equivalence between Gaussian distributions and linear Gaussian networks has important
practical ramiﬁcations.
On one hand, we can conclude that, for linear Gaussian networks,
the joint distribution has a compact representation (one that is quadratic in the number of
variables). Furthermore, the transformations from the network to the joint and back have a fairly
simple and eﬃciently computable closed form. Thus, we can easily convert one representation
to another, using whichever is more convenient for the current task. Conversely, while the

two representations are equivalent in their expressive power, there is not a one-to-one
correspondence between their parameterizations. In particular, although in the worst
case, the linear Gaussian representation and the Gaussian representation have the same
number of parameters (exercise 7.6), there are cases where one representation can be
signiﬁcantly more compact than the other.

254
Chapter 7. Gaussian Network Models
Example 7.4
Consider a linear Gaussian network structured as a chain:
X1 →X2 →· · · →Xn.
Assuming the network parameterization is not degenerate (that is, the network is a minimal I-map
of its distribution), we have that each pair of variables Xi, Xj are correlated. In this case, as shown
in theorem 7.1, the covariance matrix would be dense — none of the entries would be zero. Thus,
the representation of the covariance matrix would require a quadratic number of parameters. In
the information matrix, however, for all Xi, Xj that are not neighbors in the chain, we have that
Xi and Xj are conditionally independent given the rest of the variables in the network; hence, by
theorem 7.2, Ji,j = 0. Thus, the information matrix has most of the entries being zero; the only
nonzero entries are on the tridiagonal (the entries i, j for j = i −1, i, i + 1).
However, not all structure in a linear Gaussian network is represented in the information
matrix.
Example 7.5
In a v-structure X →Z ←Y , we have that X and Y are marginally independent, but not con-
ditionally independent given Z. Thus, according to theorem 7.2, the X, Y entry in the information
matrix would not be 0. Conversely, because the variables are marginally independent, the X, Y
entry in the covariance entry would be zero.
Complicating the example somewhat, assume that X and Y also have a joint parent W; that
is, the network is structured as a diamond. In this case, X and Y are still not independent given
the remaining network variables Z, W, and hence the X, Y entry in the information matrix is
nonzero. Conversely, they are also not marginally independent, and thus the X, Y entry in the
covariance matrix is also nonzero.
These examples simply recapitulate, in the context of Gaussian networks, the fundamental
diﬀerence in expressive power between Bayesian networks and Markov networks.
7.3
Gaussian Markov Random Fields
We now turn to the representation of multivariate Gaussian distributions via an undirected
graphical model. We ﬁrst show how a Gaussian distribution can be viewed as an MRF. This
formulation is derived almost immediately from the information form of the Gaussian. Consider
again equation (7.2). We can break up the expression in the exponent into two types of terms:
those that involve single variables Xi and those that involve pairs of variables Xi, Xj. The
terms that involve only the variable Xi are:
−1
2Ji,ix2
i + hixi,
(7.4)
where we recall that the potential vector h = Jµ. The terms that involve the pair Xi, Xj are:
−1
2[Ji,jxixj + Jj,ixjxi] = −Ji,jxixj,
(7.5)
due to the symmetry of the information matrix. Thus, the information form immediately induces
a pairwise Markov network, whose node potentials are derived from the potential vector and the

7.3. Gaussian Markov Random Fields
255
diagonal elements of the information matrix, and whose edge potentials are derived from the
oﬀ-diagonal entries of the information matrix. We also note that, when Ji,j = 0, there is no
edge between Xi and Xj in the model, corresponding directly to the independence assumption
of the Markov network.
Thus, any Gaussian distribution can be represented as a pairwise Markov network with
quadratic node and edge potentials. This Markov network is generally called a Gaussian Markov
Gaussian MRF
random ﬁeld (GMRF). Conversely, consider any pairwise Markov network with quadratic node and
edge potentials. Ignoring constant factors, which can be assimilated into the partition function,
we can write the node and edge energy functions (log-potentials) as:
ϵi(xi) = di
0 + di
1xi + di
2x2
i
ϵi,j(xi, xj) = ai,j
00 + ai,j
01xi + ai,j
10xj + ai,j
11xixj + ai,j
02x2
i + ai,j
20x2
j,
(7.6)
where we used the log-linear notation of section 4.4.1.2.
By aggregating like terms, we can
reformulate any such set of potentials in the log-quadratic form:
p′(x) = exp(−1
2xT Jx + hT x),
(7.7)
where we can assume without loss of generality that J is symmetric. This Markov network
deﬁnes a valid Gaussian density if and only if J is a positive deﬁnite matrix. If so, then J is a
legal information matrix, and we can take h to be a potential vector, resulting in a distribution
in the form of equation (7.2).
However, unlike the case of Gaussian Bayesian networks, it is not the case that every set of
quadratic node and edge potentials induces a legal Gaussian distribution. Indeed, the decom-
position of equation (7.4) and equation (7.5) can be performed for any quadratic form, including
one not corresponding to a positive deﬁnite matrix. For such matrices, the resulting function
exp(xT Ax + bT x) will have an inﬁnite integral, and cannot be normalized to produce a valid
density. Unfortunately, other than generating the entire information matrix and testing

whether it is positive deﬁnite, there is no simple way to check whether the MRF is valid.
In particular, there is no local test that can be applied to the network parameters that
precisely characterizes valid Gaussian densities. However, there are simple tests that are
suﬃcient to induce a valid density. While these conditions are not necessary, they appear to
cover many of the cases that occur in practice.
We ﬁrst provide one very simple test that can be veriﬁed by direct examination of the
information matrix.
Deﬁnition 7.2
A quadratic MRF parameterized by J is said to be diagonally dominant if, for all i,
diagonally
dominant
X
j̸=i
|Ji,j| < Ji,i.
For example, the information matrix in example 7.2 is diagonally dominant; for instance, for
i = 2 we have:
| −0.125| + 0.3333 < 0.5833.

256
Chapter 7. Gaussian Network Models
One can now show the following result:
Proposition 7.1
Let p′(x) = exp(−1
2xT Jx + hT x) be a quadratic pairwise MRF. If J is diagonally dominant,
then p′ deﬁnes a valid Gaussian MRF.
The proof is straightforward algebra and is left as an exercise (exercise 7.8).
The following condition is less easily veriﬁed, since it cannot be tested by simple examination
of the information matrix.
Rather, it checks whether the distribution can be written as a
quadratic pairwise MRF whose node and edge potentials satisfy certain conditions. Speciﬁcally,
recall that a Gaussian MRF consists of a set of node potentials, which are log-quadratic forms
in xi, and a set of edge potentials, which are log-quadratic forms in xi, xj. We can state a
condition in terms of the coeﬃcients for the nonlinear components of this parameterization:
Deﬁnition 7.3
A quadratic MRF parameterized as in equation (7.6) is said to be pairwise normalizable if:
pairwise
normalizable
• for all i, di
2 > 0;
• for all i, j, the 2 × 2 matrix

ai,j
02
ai,j
11/2
ai,j
11/2
ai,j
20

is positive semideﬁnite.
Intuitively, this deﬁnition states that each edge potential, considered in isolation, is normalizable
(hence the name “pairwise-normalizable”).
We can show the following result:
Proposition 7.2
Let p′(x) be a quadratic pairwise MRF, parameterized as in equation (7.6).
If p′ is pairwise
normalizable, then it deﬁnes a valid Gaussian distribution.
Once again, the proof follows from standard algebraic manipulations, and is left as an exercise
(exercise 7.9).
We note that, like the preceding conditions, this condition is suﬃcient but not necessary:
Example 7.6
Consider the following information matrix:


1
0.6
0.6
0.6
1
0.6
0.6
0.6
1


It is not diﬃcult to show that this information matrix is positive deﬁnite, and hence deﬁnes a legal
Gaussian distribution. However, it turns out that it is not possible to decompose this matrix into a
set of three edge potentials, each of which is positive deﬁnite.
Unfortunately, evaluating whether pairwise normalizability holds for a given MRF is not al-
ways trivial, since it can be the case that one parameterization is not pairwise normalizable,
yet a diﬀerent parameterization that induces precisely the same density function is pairwise
normalizable.

7.4. Summary
257
Example 7.7
Consider the information matrix of example 7.2, with a mean vector 0.
We can deﬁne this
distribution using an MRF by simply choosing the node potential for Xi to be Ji,ix2
i and the
edge potential for Xi, Xj to be 2Ji,jxixj.
Clearly, the X1, X2 edge does not deﬁne a nor-
malizable density over X1, X2, and hence this MRF is not pairwise normalizable.
However,
as we discussed in the context of discrete MRFs, the MRF parameterization is nonunique, and
the same density can be induced using a continuum of diﬀerent parameterizations.
In this
case, one alternative parameterization of the same density is to deﬁne all node potentials as
ϵi(xi) = 0.05x2
i , and the edge potentials to be ϵ1,2(x1, x2) = 0.2625x2
1 + 0.0033x2
2 −0.25x1x2,
and ϵ2,3(x2, x3) = 0.53x2
2 + 0.2833x2
3 + 0.6666x2x3. Straightforward arithmetic shows that this
set of potentials induces the information matrix of example 7.2. Moreover, we can show that this
formulation is pairwise normalizable: The three node potentials are all positive, and the two edge
potentials are both positive deﬁnite. (This latter fact can be shown either directly or as a conse-
quence of the fact that each of the edge potentials is diagonally dominant, and hence also positive
deﬁnite.)
This example illustrates that the pairwise normalizability condition is easily checked for a
speciﬁc MRF parameterization. However, if our aim is to encode a particular Gaussian density
as an MRF, we may have to actively search for a decomposition that satisﬁes the relevant
constraints. If the information matrix is small enough to manipulate directly, this process is
not diﬃcult, but if the information matrix is large, ﬁnding an appropriate parameterization may
incur a nontrivial computational cost.
7.4
Summary
This chapter focused on the representation and independence properties of Gaussian networks.
We showed an equivalence of expressive power between three representational classes: mul-
tivariate Gaussians, linear Gaussian Bayesian networks, and Gaussian MRFs. In particular, any
distribution that can be represented in one of those forms can also be represented in another.
We provided closed-form formulas that allow us convert between the multivariate Gaussian rep-
resentation and the linear Gaussian Bayesian network. The conversion for Markov networks is
simpler in some sense, inasmuch as there is a direct mapping between the entries in the infor-
mation (inverse covariance) matrix of the Gaussian and the quadratic forms that parameterize
the edge potentials in the Markov network. However, unlike the case of Bayesian networks, here
we must take care, since not every quadratic parameterization of a pairwise Markov network
induces a legal Gaussian distribution: The quadratic form that arises when we combine all the
pairwise potentials may not have a ﬁnite integral, and therefore may not be normalizable. In
general, there is no local way of determining whether a pairwise MRF with quadratic potentials
is normalizable; however, we provided some easily checkable suﬃcient conditions that are often
suﬃcient in practice.
The equivalence between the diﬀerent representations is analogous to the equivalence of
Bayesian networks, Markov networks, and discrete distributions: any discrete distribution can
be encoded both as a Bayesian network and as a Markov network, and vice versa. However,
as in the discrete case, this equivalence does not imply equivalence of expressive power with
respect to independence assumptions. In particular, the expressive power of the directed


258
Chapter 7. Gaussian Network Models
and undirected representations in terms of independence assumptions is exactly the
same as in the discrete case: Directed models can encode the independencies associated
with immoralities, whereas undirected models cannot; conversely, undirected models can
encode a symmetric diamond, whereas directed models cannot. As we saw, the undirected
models have a particularly elegant connection to the natural representation of the Gaussian
distribution in terms of the information matrix; in particular, zeros in the information matrix for
p correspond precisely to missing edges in the minimal I-map Markov network for p.
Finally, we note that the class of Gaussian distributions is highly restrictive, making strong
assumptions that often do not hold in practice. Nevertheless, it is a very useful class, due to its
compact representation and computational tractability (see section 14.2). Thus, in many cases,
we may be willing to make the assumption that a distribution is Gaussian even when that is
only a rough approximation. This approximation may happen a priori, in encoding a distribution
as a Gaussian even when it is not. Or, in many cases, we perform the approximation as part
of our inference process, representing intermediate results as a Gaussian, in order to keep the
computation tractable.
Indeed, as we will see, the Gaussian representation is ubiquitous in
methods that perform inference in a broad range of continuous models.
7.5
Relevant Literature
The equivalence between the multivariate and linear Gaussian representations was ﬁrst derived
by Wermuth (1980), who also provided the one-to-one transformations between them.
The
introduction of linear Gaussian dependencies into a Bayesian network framework was ﬁrst
proposed by Shachter and Kenley (1989), in the context of inﬂuence diagrams.
Speed and Kiiveri (1986) were the ﬁrst to make the connection between the structure of the
information matrix and the independence assumptions in the distribution. Building on earlier
results for discrete Markov networks, they also made the connection to the undirected graph as
a representation. Lauritzen (1996, Chapter 5) and Malioutov et al. (2006) give a good overview of
the properties of Gaussian MRFs.
7.6
Exercises
Exercise 7.1
Prove lemma 7.1. Note that you need to show both that the marginal distribution is a Gaussian, and that it
is parameterized as N (µY ; ΣY Y ).
Exercise 7.2
a. Show that, for any joint density function p(X, Y ), if we have (X ⊥Y ) in p, then CCov[X; Y ] = 0.
b. Show that, if p(X, Y ) is Gaussian, and CCov[X; Y ] = 0, then (X ⊥Y ) holds in p.
c. Show a counterexample to 2 for non-Gaussian distributions. More precisely, show a construction of a
joint density function p(X, Y ) such that CCov[X; Y ] = 0, while (X ⊥Y ) does not hold in p.
Exercise 7.3
Prove theorem 7.2.
Exercise 7.4
Prove theorem 7.5.

7.6. Exercises
259
Exercise 7.5
Consider a Kalman ﬁlter whose transition model is deﬁned in terms of a pair of matrices A, Q, and
whose observation model is deﬁned in terms of a pair of matrices H, R, as speciﬁed in equation (6.3) and
equation (6.4). Describe how we can extract a 2-TBN structure representing the conditional independencies
in this process from these matrices. (Hint: Use theorem 7.2.)
Exercise 7.6
In this question, we compare the number of independent parameters in a multivariate Gaussian distribution
and in a linear Gaussian Bayesian network.
a. Show that the number of independent parameters in Gaussian distribution over X1, . . . , Xn is the
same as the number of independent parameters in a fully connected linear Gaussian Bayesian network
over X1, . . . , Xn.
b. In example 7.4, we showed that the number of parameters in a linear Gaussian network can be sub-
stantially smaller than in its multivariate Gaussian representation. Show that the converse phenomenon
can also happen. In particular, show an example of a distribution where the multivariate Gaussian
representation requires a linear number of nonzero entries in the covariance matrix, while a cor-
responding linear Gaussian network (one that is a minimal I-map) requires a quadratic number of
nonzero parameters. (Hint: The minimal I-map does not have to be the optimal one.)
Exercise 7.7
Let p be a joint Gaussian density over X with mean vector µ and information matrix J. Let Xi ∈X, and
Z ⊂X −{Xi}. We deﬁne the conditional covariance of Xi, Xj given Z as:
conditional
covariance
CCovp[Xi; Xj | Z] = IEp[(Xi −µi)(Xj −µj) | Z] = IEz∼p(Z)

IEp(Xi,Xj|z)[(xi −µi)(xj −µj)]

.
The conditional variance of Xi is deﬁned by setting j = i. We now deﬁne the partial correlation coeﬃcient
partial correlation
coeﬃcient
ρi,j =
CCovp[Xi; Xj | X −{Xi, Xj}]
p
VVarp[Xi | X −{Xi, Xj}]VVarp[Xj | X −{Xi, Xj}]
.
Show that
ρi,j = −
Ji,j
p
Ji,iJj,j
.
Exercise 7.8
Prove proposition 7.1.
Exercise 7.9
Prove proposition 7.2.


8
The Exponential Family
8.1
Introduction
In the previous chapters, we discussed several diﬀerent representations of complex distributions.
These included both representations of global structures (for example, Bayesian networks and
Markov networks) and representations of local structures (for example, representations of CPDs
and of potentials).
In this chapter, we revisit these representations and view them from a
diﬀerent perspective. This view allows us to consider several basic questions and derive generic
answers for these questions for a wide variety of representations. As we will see in later chapters,
these solutions play a role in both inference and learning for the diﬀerent representations we
consider.
We note, however, that this chapter is somewhat abstract and heavily mathematical. Although
the ideas described in this chapter are of central importance to understanding the theoretical
foundations of learning and inference, the algorithms themselves can be understood even without
the material presented in this chapter. Thus, this chapter can be skipped by readers who are
interested primarily in the algorithms themselves.
8.2
Exponential Families
Our discussion so far has focused on the representation of a single distribution (using, say, a
Bayesian or Markov network). We now consider families of distributions. Intuitively, a family
parametric family
is a set of distributions that all share the same parametric form and diﬀer only in choice of
particular parameters (for example, the entries in table-CPDs). In general, once we choose the
global structure and local structure of the network, we deﬁne a family of all distributions that
can be attained by diﬀerent parameters for this speciﬁc choice of CPDs.
Example 8.1
Consider the empty graph structure G∅over the variables X = {X1, . . . , Xn}. We can deﬁne
the family P∅to be the set of distributions that are consistent with G∅. If all the variables in
X are binary, then we can specify a particular distribution in the family by using n parameters,
θ = {P(x1
i ) : i = 1, . . . , n}.
We will be interested in families that can be written in a particular form.
Deﬁnition 8.1
Let X be a set of variables. An exponential family P over X is speciﬁed by four components:
exponential
family

262
Chapter 8. The Exponential Family
• A suﬃcient statistics function τ from assignments to X to RK.
suﬃcient statistic
function
• A parameter space that is a convex set Θ ⊆RM of legal parameters.
parameter space
legal parameter
• A natural parameter function t from RM to RK.
natural parameter
• An auxiliary measure A over X.
Each vector of parameters θ ∈Θ speciﬁes a distribution Pθ in the family as
Pθ(ξ) =
1
Z(θ)A(ξ) exp {⟨t(θ), τ(ξ)⟩}
(8.1)
where ⟨t(θ), τ(ξ)⟩is the inner product of the vectors t(θ) and τ(ξ), and
Z(θ) =
X
ξ
A(ξ) exp {⟨t(θ), τ(ξ)⟩}
is the partition function of P, which must be ﬁnite. The parametric family P is deﬁned as:
partition function
P = {Pθ : θ ∈Θ}.
We see that an exponential family is a concise representation of a class of probability dis-
tributions that share a similar functional form. A member of the family is determined by the
parameter vector θ in the set of legal parameters. The suﬃcient statistic function τ summarizes
the aspects of an instance that are relevant for assigning it a probability. The function t maps
the parameters to space of the suﬃcient statistics.
The measure A assigns additional preferences among instances that do not depend on the
parameters. However, in most of the examples we consider here A is a constant, and we will
mention it explicitly only when it is not a constant.
Although this deﬁnition seems quite abstract, many distributions we already have encountered
are exponential families.
Example 8.2
Consider a simple Bernoulli distribution. In this case, the distribution over a binary outcome (such
as a coin toss) is controlled by a single parameter θ that represents the probability of x1. To show
that this distribution is in the exponential family, we can set
τ(X) = ⟨11{X = x1}, 11{X = x0}⟩,
(8.2)
a numerical vector representation of the value of X, and
t(θ) = ⟨ln θ, ln(1 −θ)⟩.
(8.3)
It is easy to see that for X = x1, we have τ(X) = ⟨1, 0⟩, and thus
exp {⟨t(θ), τ(X)⟩} = e1·ln θ+0·ln(1−θ) = θ.
Similarly, for X = x0, we get that exp {⟨t(θ), τ(X)⟩} = 1 −θ. We conclude that, by setting
Z(θ) = 1, this representation is identical to the Bernoulli distribution.

8.2. Exponential Families
263
Example 8.3
Consider a Gaussian distribution over a single variable. Recall that
P(x) =
1
√
2πσ exp

−(x −µ)2
2σ2

.
Deﬁne
τ(x)
=
⟨x, x2⟩
(8.4)
t(µ, σ2)
=
⟨µ
σ2 , −1
2σ2 ⟩
(8.5)
Z(µ, σ2)
=
√
2πσ exp
 µ2
2σ2

.
(8.6)
We can easily verify that
P(x) =
1
Z(µ, σ2) exp {⟨t(θ), τ(X)⟩} .
In fact, most of the parameterized distributions we encounter in probability textbooks can
be represented as exponential families.
This includes the Poisson distributions, exponential
distributions, geometric distributions, Gamma distributions, and many others (see, for example,
exercise 8.1).
We can often construct multiple exponential families that encode precisely the same class
of distributions.
There are, however, desiderata that we want from our representation of a
class of distributions as an exponential family.
First, we want the parameter space Θ to
be “well-behaved,” in particular, to be a convex, open subset of RM. Second, we want the
parametric family to be nonredundant — to have each choice of parameters represent a unique
nonredundant
parameterization
distribution.
More precisely, we want θ ̸= θ′ to imply Pθ ̸= Pθ′.
It is easy check that
a family is nonredundant if and only if the function t is invertible (over the set Θ).
Such
exponential families are called invertible. As we will discuss, these desiderata help us execute
invertible
exponential
family
certain operations eﬀectively, in particular, ﬁnding a distribution Q in some exponential family
that is a “good approximation” to some other distribution P.
8.2.1
Linear Exponential Families
A special class of exponential families is made up of families where the function t is the identity
function. This implies that the parameters are the same dimension K as the representation of
the data. Such parameters are also called the natural parameters for the given suﬃcient statistic
natural parameter
function. The name reﬂects that these parameters do not need to be modiﬁed in the exponential
form. When using natural parameters, equation (8.1) simpliﬁes to
Pθ(ξ) =
1
Z(θ) exp {⟨θ, τ(ξ)⟩} .
Clearly, for any given suﬃcient statistics function, we can reparameterize the exponential
family using the natural parameters.
However, as we discussed earlier, we want the space
of parameters Θ to satisfy certain desiderata, which may not hold for the space of natural

264
Chapter 8. The Exponential Family
parameters.
In fact, for the case of linear exponential families, we want to strengthen our
desiderata, and require that any parameter vector in RK deﬁnes a distribution in the family.
Unfortunately, as stated, this desideratum is not always achievable. To understand why, recall
that the deﬁnition of a legal parameter space Θ requires that each parameter vector θ ∈Θ
give rise to a legal (normalizable) distribution Pθ. These normalization requirements can impose
constraints on the space of legal parameters.
Example 8.4
Consider again the Gaussian distribution. Suppose we deﬁne a new parameter space using the
deﬁnition of t. That is let η = t(µ, σ2) = ⟨2µ
2σ2 , −
1
2σ2 ⟩be the natural parameters that corresponds
to θ = ⟨µ, σ2⟩. Clearly, we can now write
Pη(x) ∝exp {⟨η, τ(x)⟩} .
However, not every choice of η would lead to a legal distribution.
For the distribution to be
normalized, we need to be able to compute
Z(η)
=
Z
exp {⟨η, τ(x)⟩} dx
=
∞
Z
−∞
exp

η1x + η2x2	
dx.
If η2 ≥0 this integral is undeﬁned, since the function grows when x approaches ∞and −∞. When
η2 < 0, the integral has a ﬁnite value. Fortunately, if we consider η = t(µ, σ2) of equation (8.5),
we see that the second component is always negative (since σ2 > 0). In fact, we can see that the
image of the original parameter space, ⟨µ, σ2⟩∈R × R+, through the function t(µ, σ2), is the
space R × R−. We can verify that, for every η in that space, the normalization constant is well
deﬁned.
More generally, when we consider natural parameters for a suﬃcient statistics function τ,
we deﬁne the set of allowable natural parameters, the natural parameter space, to be the set of
natural parameter
space
natural parameters that can be normalized
Θ =

θ ∈RK :
Z
exp {⟨θ, τ(ξ)⟩} dξ < ∞

.
In the case of distributions over ﬁnite discrete spaces, all parameter choices lead to normalizable
distributions, and so Θ = RK.
In other examples, such as the Gaussian distribution, the
natural parameter space can be more constrained.
An exponential family over the natural
parameter space, and for which the natural parameter space is open and convex, is called a
linear exponential family.
linear
exponential
family
The use of linear exponential families signiﬁcantly simpliﬁes the deﬁnition of a family. To
specify such a family, we need to deﬁne only the function τ; all other parts of the deﬁnition
are implicit based on this function. This gives us a tool to describe distributions in a concise
manner. As we will see, linear exponential families have several additional attractive properties.
Where do ﬁnd linear exponential families? The two examples we presented earlier were not
phrased as linear exponential families. However, as we saw in example 8.4, we may be able to
provide an alternative parameterization of a nonlinear exponential family as a linear exponential
family. This example may give rise to the impression that any family can be reparameterized in
a trivial manner. However, there are more subtle situations.

8.2. Exponential Families
265
Example 8.5
Consider the Bernoulli distribution. Again, we might reparameterize θ by t(θ). However, the image
of the function t of example 8.2 is the curve ⟨ln θ, ln(1 −θ)⟩. This curve is not a convex set, and
it is clearly a subspace of the natural parameter space.
Alternatively, we might consider using the entire natural parameter space R2, corresponding to
the suﬃcient statistic function τ(X) = ⟨11{X = x1}, 11{X = x0}⟩of equation (8.2). This gives rise
to the parametric form:
Pθ(x) ∝exp {⟨θ, τ(x)⟩} = exp

θ111{X = x1} + θ211{X = x0}
	
.
Because the probability space is ﬁnite, this form does deﬁne a distribution for every choice of
⟨θ1, θ2⟩. However, it is not diﬃcult to verify that this family is redundant: for every constant c, the
parameters ⟨θ1 + c, θ2 + c⟩deﬁne the same distribution as ⟨θ1, θ2⟩.
Thus, a two-dimensional space is overparameterized for this distribution; conversely, the one-
dimensional subspace deﬁned by the natural parameter function is not well behaved. The solution
is to use an alternative representation of a one-dimensional space. Since we have a redundancy,
we may as well clamp θ2 to be 0. This results in the following representation of the Bernoulli
distribution:
τ(x)
=
11{x = x1}
t(θ)
=
ln
θ
1 −θ.
We see that
exp

⟨t(θ), τ(x1)⟩
	
=
θ
1 −θ
exp

⟨t(θ), τ(x0)⟩
	
=
1.
Thus,
Z(θ) = 1 +
θ
1 −θ =
1
1 −θ.
Using these, we can verify that
Pθ(x1) = (1 −θ)
θ
1 −θ = θ.
We conclude that this exponential representation captures the Bernoulli distribution. Notice now
that, in the new representation, the image of t is the whole real line R. Thus, we can deﬁne a linear
exponential family with this suﬃcient statistic function.
Example 8.6
Now, consider a multinomial variable X with k values x1, . . . , xk. The situation here is similar to
the one we had with the Bernoulli distribution. If we use the simplest exponential representation,
we ﬁnd that the legal natural parameters are on a curved manifold of Rk. Thus, instead we deﬁne
the suﬃcient statistic as a function from values of x to Rk−1:
τ(x) = ⟨11{x = x2}, . . . , 11{x = xk}⟩.

266
Chapter 8. The Exponential Family
Using a similar argument as with the Bernoulli distribution, we see that if we deﬁne
t(θ) = ⟨ln θ2
θ1
, . . . , ln θk
θ1
⟩,
then we reconstruct the original multinomial distribution. It is also easy to check that the image of
t is Rk−1. Thus, by reparameterizing, we get a linear exponential family.
All these examples deﬁne linear exponential families. An immediate question is whether there
exist families that are not linear. As we will see, there are such cases. However, the examples
we present require additional machinery.
8.3
Factored Exponential Families
The two examples of exponential families so far were of univariate distributions. Clearly, we
can extend the notion to multivariate distributions as well. In fact, we have already seen one
such example. Recall that, in deﬁnition 4.15, we deﬁned log-linear models as distributions of the
form:
P(X1, . . . , Xn) ∝exp
( k
X
i=1
θi · fi(Di)
)
where each feature fi is a function whose scope is Di. Such a distribution is clearly a linear
exponential family where the suﬃcient statistics are the vector of features
τ(ξ) = ⟨f1(d1), . . . , fk(dk)⟩.
As we have shown, by choosing the appropriate features, we can devise a log-linear model to
represent a given discrete Markov network structure. This suﬃces to show that discrete Markov
networks are linear exponential families.
8.3.1
Product Distributions
What about other distributions with product forms? Initially the issues seem deceptively easy.
A product form of terms corresponds to a simple composition of exponential families
Deﬁnition 8.2
An (unnormalized) exponential factor family Φ is deﬁned by τ, t, A, and Θ (as in the exponential
exponential
factor family
family). A factor in this family is
φθ(ξ) = A(ξ) exp {⟨t(θ), τ(ξ)⟩} .
Deﬁnition 8.3
Let Φ1, . . . , Φk be exponential factor families, where each Φi is speciﬁed by τi, ti, Ai, and
Θi. The composition of Φ1, . . . , Φk is the family Φ1 × Φ2 × · · · × Φk parameterized by θ =
family
composition
θ1 ◦θ2 ◦· · · ◦θk ∈Θ1 × Θ2 × · · · × Θk, deﬁned as
Pθ(ξ) ∝
Y
i
φθi(ξ) =
 Y
i
Ai(ξ)
!
exp
(X
i
⟨ti(θi), τi(ξ)⟩
)
where φθi is a factor in the i’th factor family.

8.3. Factored Exponential Families
267
It is clear from this deﬁnition that the composition of exponential factors is an exponential
family with τ(ξ) = τ1(ξ) ◦τ2(ξ) ◦· · · ◦τk(ξ) and natural parameters t(θ) = t1(θ1) ◦t2(θ2) ◦
· · · ◦tk(θk).
This simple observation suﬃces to show that if we have exponential representation for po-
tentials in a Markov network (not necessarily simple potentials), then their product is also an
exponential family. Moreover, it follows that the product of linear exponential factor families is
a linear exponential family.
8.3.2
Bayesian Networks
Taking the same line of reasoning, we can also show that, if we have a set of CPDs from an
exponential family, then their product is also in the exponential family. Thus, we can conclude
that a Bayesian network with exponential CPDs deﬁnes an exponential family. To show this, we
ﬁrst note that many of the CPDs we saw in previous chapters can be represented as exponential
factors.
Example 8.7
We start by examining a simple table-CPD P(X | U). Similar to the case of Bernoulli distribution,
we can deﬁne the suﬃcient statistics to be indicators for diﬀerent entries in P(X | U). Thus, we
set
τP (X|U)(X) = ⟨11{X = x, U = u} : x ∈Val(X), u ∈Val(U)⟩.
We set the natural parameters to be the corresponding parameters
tP (X|U)(θ) = ⟨ln P(x | u) : x ∈Val(X), u ∈Val(U)⟩.
It is easy to verify that
P(x | u) = exp

⟨tP (X|U)(θ), τP (X|U)(x, u)⟩
	
,
since exactly one entry of τP (X|U)(x, u) is 1 and the rest are 0. Note that this representation is
not a linear exponential factor.
Clearly, we can use the same representation to capture any CPD for discrete variables. In some
cases, however, we can be more eﬃcient. In tree-CPDs, for example, we can have a feature set
for each leaf in tree, since all parent assignment that reach the leaf lead to the same parameter
over the children.
What happens with continuous CPDs? In this case, not every CPD can be represented by an
exponential factor. However, some cases can.
Example 8.8
Consider a linear Gaussian CPD for P(X | U) where
X = β0 + β1u1 + · · · + βkuk + ϵ,
where ϵ is a Gaussian random variable with mean 0 and variance σ2, representing the noise in the
system. Stated diﬀerently, the conditional density function of X is
P(x | u) =
1
√
2πσ exp

−1
2σ2 (x −(β0 + β1u1 + · · · + βkuk))2

.

268
Chapter 8. The Exponential Family
By expanding the squared term, we ﬁnd that the suﬃcient statistics are the ﬁrst and second moments
of all the variables
τP (X|U)(X) = ⟨1, x, u1, . . . , uk, x2, xu1, . . . , xuk, u2
1, u1u2, . . . , u2
k⟩,
and the natural parameters are the coeﬃcients of each of these terms.
As the product of exponential factors is an exponential family, we conclude that a Bayesian
network that is the product of CPDs that have exponential form deﬁnes an exponential family.
However, there is one subtlety that arises in the case of Bayesian networks that does not arise
for a general product form. When we deﬁned the product of a set of exponential factors in
deﬁnition 8.3, we ignored the partition functions of the individual factors, allowing the partition
function of the overall distribution to ensure global normalization.
However, in both of our examples of exponential factors for CPDs, we were careful to construct
a normalized conditional distribution. This allows us to use the chain rule to compose these
factors into a joint distribution without the requirement of a partition function. This requirement
turns out to be critical: We cannot construct a Bayesian network from a product of unnormalized
exponential factors.
Example 8.9
Consider the network structure A →B, with binary variables. Now, suppose we want to represent
the CPD P(B | A) using a more concise representation than the one of example 8.7. As suggested
by example 8.5, we might consider deﬁning
τ(A, B) = ⟨11{A = a1}, 11{B = b1, A = a1}, 11{B = b1, A = a0}⟩.
That is, for each conditional distribution, we have an indicator only for one of the two relevant
cases. The representation of example 8.5 suggests that we should deﬁne
t(θ) =

ln θa1
θa0 , ln θb1|a1
θb0|a1 , ln θb1|a0
θb0|a0

.
Does this construction give us the desired distribution? Under this construction, we would have
Pθ(a1, b1) =
1
Z(θ)
θa1θb1|a1
θa0θb0|a1 .
Thus, if this representation was faithful for the intended interpretation of the parameter values, we
would have Z(θ) =
1
θa0θb0|a1 . On the other hand,
Pθ(a0, b0) =
1
Z(θ),
which requires that Z(θ) =
1
θa0θb0|a0 in order to be faithful to the desired distribution. Because
these two constants are, in general, not equal, we conclude that this representation cannot be faithful
to the original Bayesian network.
The failure in this example is that the global normalization constant cannot play the role of a
local normalization constant within each conditional distribution. This implies that to have an
exponential representation of a Bayesian network, we need to ensure that each CPD is locally

8.4. Entropy and Relative Entropy
269
normalized. For every exponential CPD this is easy to do. We simply increase the dimension
of τ by adding another dimension that has a constant value, say 1. Then the matching element
of t(θ) can be the logarithm of the partition function.
This is essentially what we did in
example 8.8.
We still might wonder whether a Bayesian network deﬁnes a linear exponential family.
Example 8.10
Consider the network structure A →C ←B, with binary variables. Assuming a representation
that captures general CPDs, our suﬃcient statistics need to include features that distinguish between
the following four assignments:
ξ1
=
⟨a1, b1, c1⟩
ξ2
=
⟨a1, b0, c1⟩
ξ3
=
⟨a0, b1, c1⟩
ξ4
=
⟨a0, b0, c1⟩
More precisely, we need to be able to modify the CPD P(C | A, B) to change the probability of
one of these assignments without modifying the probability of the other three. This implies that
τ(ξ1), . . . , τ(ξ4) must be linearly independent: otherwise, we could not change the probability
of one assignment without changing the others. Because our model is a linear function of the
suﬃcient statistics, we can choose any set of orthogonal basis vectors that we want; in particular,
we can assume without loss of generality that the ﬁrst four coordinates of the suﬃcient statistics are
τi(ξ) = 11{ξ = ξi}, and that any additional coordinates of the suﬃcient statistics are not linearly
dependent on these four. Moreover, since the model is over a ﬁnite set of events, any choice of
parameters can be normalized. Thus, the space of natural parameters is RK, where K is dimension
of the suﬃcient statistics vector. The linear family over such features is essentially a Markov network
over the clique {A, B, C}. Thus, the parameterization of this family includes cases where A and
B are not independent, violating the independence properties of the Bayesian network.
Thus, this simple Bayesian network cannot be represented by a linear family. More broadly,

although a Bayesian network with suitable CPDs deﬁnes an exponential family, this family
is not generally a linear one. In particular, any network that contains immoralities does
not induce a linear exponential family.
8.4
Entropy and Relative Entropy
We now explore some of the consequences of representation of models in factored form and
of their exponential family representation.
These both suggest some implications of these
representations and will be useful in developments in subsequent chapters.
8.4.1
Entropy
We start with the notion of entropy. Recall that the entropy of a distribution is a measure of
the amount of “stochasticity” or “noise” in the distribution. A low entropy implies that most
of the distribution mass is on a few instances, while a larger entropy suggests a more uniform
distribution. Another interpretation we discussed in appendix A.1 is the number of bits needed,
on average, to encode instances in the distribution.

270
Chapter 8. The Exponential Family
In various tasks we need to compute the entropy of given distributions. As we will see, we
also encounter situations where we want to choose a distribution that maximizes the entropy
subject to some constraints. A characterization of entropy will allow us to perform both tasks
more eﬃciently.
8.4.1.1
Entropy of an Exponential Model
We now consider the task of computing the entropy for distributions in an an exponential family
deﬁned by τ and t.
Theorem 8.1
Let Pθ be a distribution in an exponential family deﬁned by the functions τ and t. Then
IHPθ(X) = ln Z(θ) −⟨IEPθ[τ(X)], t(θ)⟩.
(8.7)
While this formulation seems fairly abstract, it does provide some insight. The entropy decom-
poses as a diﬀerence of two terms. The ﬁrst is the partition function Z(θ). The second depends
only on the expected value of the suﬃcient statistics τ(X). Thus, instead of considering each
assignment to X, we need to know only the expectations of the statistics under Pθ. As we will
see, this is a recurring theme in our discussion of exponential families.
Example 8.11
We now apply this result to a Gaussian distribution X ∼N(µ, σ2), as formulated in the expo-
nential family in example 8.3. Plugging into equation (8.7) the deﬁnitions of τ, t, and Z from
equation (8.4), equation (8.5), and equation (8.6), respectively, we get
IHP (X)
=
1
2 ln 2πσ2 + µ2
2σ2 −2µ
2σ2 IEP [X] +
1
2σ2 IEP

X2
=
1
2 ln 2πσ2 + µ2
2σ2 −2µ
2σ2 µ +
1
2σ2 (σ2 + µ2)
=
1
2 ln 2πeσ2
where we used the fact that IEP [X] = µ and IEP

X2
= µ2 + σ2.
We can apply the formulation of theorem 8.1 directly to write the entropy of a Markov network.
Proposition 8.1
If P(X) = 1
Z
Q
k φk(Dk) is a Markov network, then
IHP (X) = ln Z +
X
k
IEP [−ln φk(Dk)].
Example 8.12
Consider a simple Markov network with two potentials β1(A, B) and β2(B, C), so that
β1(A, B)
a0
b0
2
a0
b1
1
a1
b0
1
a1
b1
5
β2(B, C)
b0
c0
6
b0
c1
1
b1
c0
1
b1
c1
0.5

8.4. Entropy and Relative Entropy
271
Simple calculations show that Z = 30, and the marginal distributions are
A
B
P(A, B)
a0
b0
0.47
a0
b1
0.05
a1
b0
0.23
a1
b1
0.25
B
C
P(B, C)
b0
c0
0.6
b0
c1
0.1
b1
c0
0.2
b1
c1
0.1
Using proposition 8.1, we can calculate the entropy:
IHP (A, B, C)
=
ln Z + IEP [−ln β1(A, B)] + IEP [−ln β2(B, C)]
=
ln Z
−P(a0, b0) ln β1(a0, b0) −P(a0, b1) ln β1(a0, b1)
−P(a1, b0) ln β1(a1, b0) −P(a1, b1) ln β1(a1, b1)
−P(b0, c0) ln β2(b0, c0) −P(b0, c1) ln β2(b0, c1)
−P(b1, c0) ln β2(b1, c0) −P(b1, c1) ln β2(b1, c1)
=
3.4012
−0.47 ∗0.69 −0.05 ∗0 −0.23 ∗0 −0.25 ∗1.60
−0.6 ∗1.79 −0.1 ∗0 −0.2 ∗0 −0.1 ∗−0.69
=
1.670.
In this example, the number of terms we evaluated is the same as what we would have
considered using the original formulation of the entropy where we sum over all possible joint
assignments. However, if we consider more complex networks, the number of joint assignments
is exponentially large while the number of potentials is typically reasonable, and each one
involves the joint assignments to only a few variables.
Note, however, that to use the formulation of proposition 8.1 we need to perform a global
computation to ﬁnd the value of the partition function Z as well as the marginal distribution
over the scope of each potential Dk. As we will see in later chapters, in some network structures,
these computations can be done eﬃciently.
Terms such as IEP [−ln βk(Dk)] resemble the entropy of Dk. However, since the marginal
over Dk is usually not identical to the potential βk, such terms are not entropy terms. In some
sense we can think of ln Z as a correction for this discrepancy. For example, if we multiply all
the entries of βk by a constant c, the corresponding term IEP [−ln βk(Dk)] will decrease by
ln c. However, at the same time ln Z will increase by the same constant, since it is canceled
out in the normalization.
8.4.1.2
Entropy of Bayesian Networks
We now consider the entropy of a Bayesian network. Although we can address this computation
using our general result in theorem 8.1, it turns out that the formulation for Bayesian networks
is simpler. Intuitively, as we saw, we can represent Bayesian networks as an exponential family
where the partition function is 1. This removes the global term from the entropy.

272
Chapter 8. The Exponential Family
Theorem 8.2
If P(X) = Q
i P(Xi | PaG
i ) is a distribution consistent with a Bayesian network G, then
IHP (X) =
X
i
IHP (Xi | PaG
i )
Proof
IHP (X)
=
IEP [−ln P(X)]
=
IEP
"
−
X
i
ln P(Xi | PaG
i )
#
=
X
i
IEP

−ln P(Xi | PaG
i )

=
X
i
IHP (Xi | PaG
i ),
where the ﬁrst and last steps invoke the deﬁnitions of entropy and conditional entropy.
We see that the entropy of a Bayesian network decomposes as a sum of conditional entropies
of the individual conditional distributions. This representation suggests that the entropy of a
Bayesian network can be directly “read oﬀ” from the CPDs. This impression is misleading. Recall
that the conditional entropy term IHP (Xi | PaG
i ) can be written as a weighted average of simpler
entropies of conditional distributions
IHP (Xi | PaG
i ) =
X
paG
i
P(paG
i )IHP (Xi | paG
i ).
While each of the simpler entropy terms in the summation can be computed based on the CPD
entries alone, the weighting term P(paG
i ) is a marginal over paG
i of the joint distribution, and
depends on other CPDs upstream of Xi. Thus, computing the entropy of the network requires
that we answer probability queries over the network.
However, based on local considerations alone, we can analyze the amount of entropy intro-
duced by each CPD, and thereby provide bounds on the overall entropy:
Proposition 8.2
If P(X) = Q
i P(Xi | PaG
i ) is a distribution consistent with a Bayesian network G, then
X
i
min
paG
i
IHP (Xi | paG
i ) ≤IHP (X) ≤
X
i
max
paG
i
IHP (Xi | paG
i ).
Thus, if all the CPDs in a Bayesian network are almost deterministic (low conditional entropy
given each parent conﬁguration), then the overall entropy of the network is small. Conversely,
if all the CPDs are highly stochastic (high conditional entropy) then the overall entropy of the
network is high.
8.4.2
Relative Entropy
A related notion is the relative entropy between models. This measure of distance plays an
important role in many of the developments of later chapters.

8.5. Projections
273
If we consider the relative entropy between an arbitrary distribution Q and a distribution Pθ
within an exponential family, we see that the form of Pθ can be exploited to simplify the form
of the relative entropy.
Theorem 8.3
Consider a distribution Q and a distribution Pθ in an exponential family deﬁned by τ and t. Then
ID(Q||Pθ) = −IHQ(X) −⟨IEQ[τ(X)], t(θ)⟩+ ln Z(θ).
The proof is left as an exercise (exercise 8.2).
We see that the quantities of interest are again the expected suﬃcient statistics and the
partition function. Unlike the entropy, in this case we compute the expectation of the suﬃcient
statistics according to Q.
If both distributions are in the same exponential family, then we can further simplify the
form of the relative entropy.
Theorem 8.4
Consider two distribution Pθ1 and Pθ2 within the same exponential family. Then
ID(Pθ1||Pθ2) = ⟨IEPθ1[τ(X)], t(θ1) −t(θ2)⟩−ln Z(θ1)
Z(θ2)
Proof Combine theorem 8.3 with theorem 8.1.
When we consider Bayesian networks, we can use the fact that the partition function is
constant to simplify the terms in both results.
Theorem 8.5
If P is a distribution consistent with a Bayesian network G, then
ID(Q||P) = −IHQ(X) −
X
i
X
paG
i
Q(paG
i )IEQ(Xi|paG
i )

ln P(Xi | paG
i )

;
If Q is also consistent with G, then
ID(Q||P) =
X
i
X
paG
i
Q(paG
i )ID(Q(Xi | paG
i )||P(Xi | paG
i )).
The second result shows that, analogously to the form of the entropy of Bayesian networks,
we can write the relative entropy between two distributions consistent with G as a weighted
sum of the relative entropies between the conditional distributions. These conditional relative
entropies can be evaluated directly using the CPDs of the two networks. The weighting of these
relative entropies depends on the the joint distribution Q.
8.5
Projections
As we discuss in appendix A.1.3, we can view the relative entropy as a notion of distance
between two distributions. We can therefore use it as the basis for an important operation —
the projection operation — which we will utilize extensively in subsequent chapters. Similar
projection
to the geometric concept of projecting a point onto a hyperplane, we consider the problem of
ﬁnding the distribution, within a given exponential family, that is closest to a given distribution

274
Chapter 8. The Exponential Family
in terms of relative entropy.
For example, we want to perform such a projection when we
approximate a complex distribution with one with a simple structure. As we will see, this is
a crucial strategy for approximate inference in networks where exact inference is infeasible. In
such an approximation we would like to ﬁnd the best (that is, closest) approximation within a
family in which we can perform inference. Moreover, the problem of learning a graphical model
can also be posed as a projection problem of the empirical distribution observed in the data
onto a desired family.
Suppose we have a distribution P and we want to approximate it with another distribution
Q in a class of distributions Q (for example, an exponential family). For example, we might
want to approximate P with a product of marginal distributions. Because the notion of relative
entropy is not symmetric, we can use it to deﬁne two types of approximations.
Deﬁnition 8.4
Let P be a distribution and let Q be a convex set of distributions.
• The I-projection (information projection) of P onto Q is the distribution
I-projection
QI = arg min
Q∈Q ID(Q||P).
• The M-projection (moment projection) of P onto Q is the distribution
M-projection
QM = arg min
Q∈Q ID(P||Q).
8.5.1
Comparison
We can think of both QI and QM as the projection of P into the set Q in the sense that it is
the distribution closest to P. Moreover, if P ∈Q, then in both deﬁnitions the projection would
be P. However, because the relative entropy is not symmetric, these two projections are, in
general, diﬀerent. To understand the diﬀerences between these two projections, let us consider
a few examples.
Example 8.13
Suppose we have a non-Gaussian distribution P over the reals. We can consider the M-projection
and the I-projection on the family of Gaussian distributions. As a concrete example, consider the
distribution P of ﬁgure 8.1. As we can see, the two projections are diﬀerent Gaussian distributions.
(The M-projection was found using the analytic form that we will discuss, and the I-projection by
gradient ascent in the (µ, σ2) space.) Although the means of the two projected distributions are
relatively close, the M-projection has larger variance than the I-projection.
We can better understand these diﬀerences if we examine the objective function optimized
by each projection. Recall that the M-projection QM minimizes
ID(P||Q) = −IHP (X) + IEP [−ln Q(X)].
We see that, in general, we want QM to have high density in regions that are probable according
to P, since a small −ln Q(X) in these regions will lead to a smaller second term. At the same
time, there is a high penalty for assigning low density to regions where P(X) is nonnegligible.

8.5. Projections
275
P
M-projection
I-projection
Figure 8.1
Example of M- and I-projections into the family of Gaussian distributions
As a consequence, although the M-projection attempts to match the main mass of P, its high
variance is a compromise to ensure that it assigns reasonably high density to all regions that are
in the support of P.
On the other hand, the I-projection minimizes
ID(Q||P) = −IHQ(X) + IEQ[−ln P(X)].
Thus, the ﬁrst term incurs a penalty for low entropy, which in the case of a Gaussian Q translates
to a penalty on small variance. The second term, IEQ[−ln P(X)], encodes a preference for
assigning higher density to regions where P(X) is large and very low density to regions where
P(X) is small. Without the ﬁrst term, we can minimize the second by putting all of the mass of
Q on the most probable point according to P. The compromise between the two terms results
in the distribution we see in ﬁgure 8.1.
A similar phenomenon occurs in discrete distributions.
Example 8.14
Now consider the projection of a distribution P(A, B) onto the family of factored distributions
Q(A, B) = Q(A)Q(B). Suppose P(A, B) is the following distribution:
P(a0, b0)
=
0.45
P(a0, b1)
=
0.05
P(a1, b0)
=
0.05
P(a1, b1)
=
0.45.
That is, the distribution P puts almost all of the mass on the event A = B. This distribution is a
particularly diﬃcult one to approximate using a factored distribution, since in P the two variables
A and B are highly correlated, a dependency that cannot be captured using a fully factored Q.
Again, it is instructive to compare the M-projection and the I-projection of this distribution (see
ﬁgure 8.2). It follows from example A.7 (appendix A.5.3) that the M-projection of this distribution is

276
Chapter 8. The Exponential Family
P
QM
QI
Figure 8.2
Example of M- and I-projections of a two variable discrete distribution where P(a0 =
b0) = P(a1 = b1) = 0.45 and P(a0 = b1) = P(a0 = b1) = 0.05 onto factorized distribution. Each
axis denotes the probability of an instance: P(a1, b1), P(a1, b0), and P(a0, b1). The wire surfaces mark
the region of legal distributions. The solid surface shows the distributions where A and independent of B.
The points show P and its two projections.
the uniform distribution:
QM(a0, b0)
=
0.5 ∗0.5 = 0.25
QM(a0, b1)
=
0.5 ∗0.5 = 0.25
QM(a1, b0)
=
0.5 ∗0.5 = 0.25
QM(a1, b1)
=
0.5 ∗0.5 = 0.25.
In contrast, the I-projection focuses on one of the two “modes” of the distribution, either when both
A and B are true or when both are false. Since the distribution is symmetric about these modes,
there are two I-projections. One of them is
QI(a0, b0)
=
0.25 ∗0.25 = 0.0625
QI(a0, b1)
=
0.25 ∗0.75 = 0.1875
QI(a1, b0)
=
0.75 ∗0.25 = 0.1875
QI(a1, b1)
=
0.75 ∗0.75 = 0.5625.
The second I-projection is symmetric around the opposite mode a0, b0.

8.5. Projections
277
As in example 8.13, we can understand these diﬀerences by considering the underlying math-
ematics. The M-projection attempts to give all assignments reasonably high probability,

whereas the I-projection attempts to focus on high-probability assignments in P while
maintaining a reasonable entropy. In this case, this behavior results in a uniform distribution
for the M-projection, whereas the I-projection places most of the probability mass on one of the
two assignments where P has high probability.
8.5.2
M-Projections
Can we say more about the form of these projections? We start by considering M-projections
onto a simple family of distributions.
Proposition 8.3
Let P be a distribution over X1, . . . , Xn, and let Q be the family of distributions consistent with
G∅, the empty graph. Then
QM = arg min
Q|=G∅
ID(P||Q)
is the distribution:
QM(X1, . . . , Xn) = P(X1)P(X2) · · · P(Xn).
Proof Consider a distribution Q |= G∅. Since Q factorizes, we can rewrite ID(P||Q):
ID(P||Q)
=
IEP [ln P(X1, . . . , Xn) −ln Q(X1, . . . , Xn)]
=
IEP [ln P(X1, . . . , Xn)] −
X
i
IEP [ln Q(Xi)]
=
IEP

ln P(X1, . . . , Xn)
P(X1) · · · P(Xn)

+
X
i
IEP

ln P(Xi)
Q(Xi)

=
ID(P||QM) +
X
i
ID(P(Xi)||Q(Xi))
≥
ID(P||QM).
The last step relies on the nonnegativity of the relative entropy. We conclude that ID(P||Q) ≥
ID(P||QM) with equality only if Q(Xi) = P(Xi) for all i. That is, only when Q = QM.
Hence, the M-projection of P onto factored distribution is simply the product of marginals of
P.
This theorem is an instance of a much more general result. To understand the generalization,
we observe that the family Q of fully factored distributions is characterized by a vector of
suﬃcient statistics that simply counts, for each variable Xi, the number of occurrences of each
of its values. The marginal distributions over the Xi’s are simply the expectations, relative to P,
of these suﬃcient statistics. We see that, by selecting Q to match these expectations, we obtain
the M-projection.
As we now show, this is not an accident. The characterization of a distribution P that is
relevant to computing its M-projection into Q is precisely the expectation, relative to P, of the
suﬃcient statistic function of Q.

278
Chapter 8. The Exponential Family
Theorem 8.6
Let P be a distribution over X, and let Q be an exponential family deﬁned by the functions τ(ξ)
and t(θ). If there is a set of parameters θ such that IEQθ[τ(X)] = IEP [τ(X)], then the M-projection
of P is Qθ.
Proof Suppose that IEP [τ(X)] = IEQθ[τ(X)], and let θ′ be some set of parameters. Then,
ID(P||Qθ′) −ID(P||Qθ)
=
−IHP (X) −⟨IEP [τ(X)], t(θ′)⟩+ ln Z(θ′)
+IHP (X) + ⟨IEP [τ(X)], t(θ)⟩−ln Z(θ)
=
⟨IEP [τ(X)], t(θ) −t(θ′)⟩−ln Z(θ)
Z(θ′)
=
⟨IEQθ[τ(X)], t(θ) −t(θ′)⟩−ln Z(θ)
Z(θ′)
=
ID(Qθ||Qθ′) ≥0.
We conclude that the M-projection of P is Qθ.
This theorem suggests that we can consider both the distribution P and the distributions
in Q in terms of the expectations of τ(X). Thus, instead of describing a distribution in the
family by the set of parameters, we can describe it in terms of the expected suﬃcient statistics.
expected
suﬃcient
statistics
To formalize this intuition, we need some additional notation. We deﬁne a mapping from legal
parameters in Θ to vectors of suﬃcient statistics
ess(θ) = IEQθ[τ(X)].
Theorem 8.6 shows that if IEP [τ(X)] is in the image of ess, then the M-projection of P is the
distribution Qθ that matches the expected suﬃcient statistics of P. In other words,
IEQM [τ(X)] = IEP [τ(X)].
This result explains why M-projection is also referred to as moment matching. In many ex-
moment
matching
ponential families the suﬃcient statistics are moments (mean, variance, and so forth) of the
distribution. In such cases, the M-projection of P is the distribution in the family that matches
these moments in P.
We illustrate these concepts in ﬁgure 8.3. As we can see, the mapping ess(θ) directly relates
parameters to expected suﬃcient statistics. By comparing the expected suﬃcient statistics of P
to these of distributions in Q, we can ﬁnd the M-projection.
Moreover, using theorem 8.6, we obtain a general characterization of the M-projection function
M-project(s), which maps a vector of expected suﬃcient statistics to a parameter vector:
Corollary 8.1
Let s be a vector. If s ∈image(ess) and ess is invertible, then
M-project(s) = ess−1(s).
That is, the parameters of the M-projection of P are simply the inverse of the ess mapping,
applied to the expected suﬃcient statistic vector of P. This result allows us to describe the
M-projection operation in terms of a speciﬁc function. This result assumes, of course, that
IEP [τ] is in the image of ess and that ess is invertible. In many examples that we consider, the
image of ess includes all possible vectors of expected suﬃcient statistics we might encounter.
Moreover, if the parameterization is nonredundant, then ess is invertible.

8.5. Projections
279
Distributions
Expected statistics
Parameters
Qq
P
EP[t(X)]
EQq[t(X)]
image
of
ess(q)
ess(q)
q
Exponential
family
Figure 8.3
Illustration of the relations between parameters, distributions and expected suﬃcient
statistics. Each parameter corresponds to a distribution, which in turn corresponds to a value of the
expected statistics.
The function ess maps parameters directly to expected statistics.
If the expected
statistics of P and Qθ match, then Qθ is the M-projection of P.
Example 8.15
Consider the exponential family of Gaussian distributions. Recall that the suﬃcient statistics func-
tion for this family is τ(x) = ⟨x, x2⟩. Given parameters θ = ⟨µ, σ2⟩, the expected value of τ
is
ess(⟨µ, σ2⟩) = IEQ⟨µ,σ2⟩[τ(X)] = ⟨µ, σ2 + µ2⟩.
It is not diﬃcult to show that, for any distribution P, IEP [τ(X)] must be in the image of this
function (see exercise 8.4). Thus, for any choice of P, we can apply theorem 8.6.
Finally, we can easily invert this function:
M-project(⟨s1, s2⟩) = ess−1(⟨s1, s2⟩) = ⟨s1, s2 −s2
1⟩.
Recall that s1 = IEP [X] and s2 = IEP

X2
. Thus, the estimated parameters are the mean and
variance of X according to P, as we would expect.
This example shows that the “naive” choice of Gaussian distribution, obtained by matching
the mean and variance of a variable X, provides the best Gaussian approximation (in the M-
projection sense) to a non-Gaussian distribution over X. We have also provided a solution to
the M-projection problem in the case of a factored product of multinomials, in proposition 8.3,
which can be viewed as a special case of theorem 8.6. In a more general application of this
result, we show in section 11.4.4 a general result on the form of the M-projection for a linear
exponential family over discrete state space, including the class of Markov networks.

280
Chapter 8. The Exponential Family
The analysis for other families of distributions can be subtler.
Example 8.16
We now consider a more complex example of M-projection onto a chain network. Suppose we have
a distribution P over variables X1, . . . , Xn, and want to project it onto the family of distributions
Q of the distributions that are consistent with the network structure X1 →X2 →· · · →Xn.
What are the suﬃcient statistics for this network? Based on our previous discussion, we see that
each conditional distribution Q(Xi+1 | Xi) requires a statistic of the form
τxi,xi+1(ξ) = 11{Xi = xi, Xi+1 = xi+1} ∀⟨xi, xi+1⟩∈Val(Xi) × Val(Xi+1).
These statistics are suﬃcient but are redundant. To see this, note that the “marginal statistics” must
agree. That is,
X
xi
τxi,xi+1(ξ) =
X
xi+2
τxi+1,xi+2(ξ) ∀xi+1 ∈Val(Xi+1).
(8.8)
Although this representation is redundant, we can still apply the mechanisms discussed earlier and
consider the function ess that maps parameters of such a network to the suﬃcient statistics. The
expectation of an indicator function is the marginal probability of that event, so that
IEQθ

τxi,xi+1(X)

= Qθ(xi, xi+1).
Thus, the function ess simply maps from θ to the pairwise marginals of consecutive variables in
Qθ. Because these are pairwise marginals of an actual distribution, it follows that these suﬃcient
statistics satisfy the consistency constraints of equation (8.8).
How do we invert this function? Given the statistics from P, we want to ﬁnd a distribution
Q that matches them. We start building Q along the structure of the chain. We choose Q(X1)
and Q(X2 | X1) so that Q(x1, x2) = IEP [τx1,x2(X)] = P(x1, x2). In fact, there is a unique
choice that satisﬁes this equality, where Q(X1, X2) = P(X1, X2). This choice implies that the
marginal distribution Q(X2) matches the marginal distribution P(X2). Now, consider our choice
of Q(X3 | X2). We need to ensure that
Q(x3, x2) = IEP [τx2,x3(X)] = P(x2, x3).
We note that, because Q(x3, x2) = Q(x3 | x2)Q(x2) = Q(x3 | x2)P(x2), we can achieve this
equality by setting Q(x3 | x2) = P(x3 | x2). Moreover, this implies that Q(x3) = P(x3). We
can continue this construction recursively to set
Q(xi+1 | xi) = P(xi+1 | xi).
Using the preceding argument, we can show that this choice will match the suﬃcient statistics of P.
This suﬃces to show that this Q is the M-projection of P.
Note that, although this choice of Q coincides with P on pairwise marginals of consecutive
variables, it does not necessarily agree with P on other marginals. As an extreme example, consider
a distribution P where X1 and X3 are identical and both are independent of X2. If we project
this distribution onto a distribution Q with the structure X1 →X2 →X3, then P and Q will
not necessarily agree on the joint marginals of X1, X3. In Q this distribution will be
Q(x1, x3) =
X
x2
Q(x1, x2)Q(x3 | x2).

8.5. Projections
281
Since Q(x1, x2) = P(x1, x2) = P(x1)P(x2) and Q(x3 | x2) = P(x3 | x2) = P(x3), we
conclude that Q(x1, x3) = P(x1)P(x3), losing the equality between X1 and X3 in P.
This analysis used a redundant parameterization; exercise 8.6 shows how we can reparam-
eterize a directed chain within the linear exponential family and thereby obtain an alternative
perspective on the M-projection operation.
So far, all of our examples have had the characteristic that the vector of expected suﬃcient
statistics for a distribution P is always in the image of ess; thus, our task has only been to invert
ess. Unfortunately, there are examples where not every vector of expected suﬃcient statistics
can also be derived from a distribution in our exponential family.
Example 8.17
Consider again the family Q from example 8.10, of distributions parameterized using network
structure A →C ←B, with binary variables A, B, C. We can show that the suﬃcient statistics
for this distribution are indicators for all the joint assignments to A, B, and C except one. That is,
τ(A, B, C)
= ⟨
11{A = a1, B = b1, C = c1},
11{A = a0, B = b1, C = c1},
11{A = a1, B = b0, C = c1},
11{A = a1, B = b1, C = c0},
11{A = a1, B = b0, C = c0},
11{A = a0, B = b1, C = c0},
11{A = a0, B = b0, C = c1}⟩.
If we look at the expected value of these statistics given some member of the family, we have that,
since A and B are independent in Qθ, Qθ(a1, b1) = Qθ(a1)Qθ(b1). Thus, the expected statistics
should satisfy
IEQθ

11{A = a1, B = b1, C = c1}

+ IEQθ

11{A = a1, B = b1, C = c0}

=
 IEQθ

11{A = a1, B = b1, C = c1}

+ IEQθ

11{A = a1, B = b1, C = c0}

+IEQθ

11{A = a1, B = b0, C = c1}

+ IEQθ

11{A = a1, B = b0, C = c0}

 IEQθ

11{A = a1, B = b1, C = c1}

+ IEQθ

11{A = a1, B = b1, C = c0}

+IEQθ

11{A = a0, B = b1, C = c1}

+ IEQθ

11{A = a0, B = b1, C = c0}

.
This constraint is not typically satisﬁed by the expected statistics from a general distribution P we
might consider projecting. Thus, in this case, there are expected statistics vectors that do not fall
within the image of ess.
In such cases, and in Bayesian networks in general, the projection procedure is more complex
than inverting the ess function. Nevertheless, we can show that the projection operation still has
an analytic solution.
Theorem 8.7
Let P be a distribution over X1, . . . , Xn, and let G be a Bayesian network structure. Then the
M-projection QM is:
QM(X1, . . . , Xn) =
Y
i
P(Xi | PaG
Xi).

282
Chapter 8. The Exponential Family
Because the mapping ess for Bayesian networks is not invertible, the proof of this result (see
exercise 8.5) does not build on theorem 8.6 but rather directly on theorem 8.5. This result turns
out to be central to our derivation of Bayesian network learning in chapter 17.
8.5.3
I-Projections
What about I-projections? Recall that
ID(Q||P) = −IHQ(X) −IEQ[ln P(X)].
If Q is in some exponential family, we can use the derivation of theorem 8.1 to simplify the
entropy term. However, the exponential form of Q does not provide insights into the second
term. When dealing with the I-projection of a general distribution P, we are left without further
simpliﬁcations. However, if the distribution P has some structure, we might be able to simplify
IEQ[ln P(X)] into simpler terms, although the projection problem is still a nontrivial one. We
discuss this problem in much more detail in chapter 11.
8.6
Summary
In this chapter, we presented some of the basic technical concepts that underlie many of the
techniques we explore in depth later in the book. We deﬁned the formalism of exponential
families, which provides the fundamental basis for considering families of related distributions.
We also deﬁned the subclass of linear exponential families, which are signiﬁcantly simpler and
yet cover a large fraction of the distributions that arise in practice.
We discussed how the types of distributions described so far in this book ﬁt into this frame-
work, showing that Gaussians, linear Gaussians, and multinomials are all in the linear exponen-
tial family. Any class of distributions representable by parameterizing a Markov network of some
ﬁxed structure is also in the linear exponential family. By contrast, the class of distributions
representable by a Bayesian network of some ﬁxed structure is in the exponential family, but is
not in the linear exponential family when the network structure includes an immorality.
We showed how we can use the formulation of an exponential family to facilitate computations
such as the entropy of a distribution or the relative entropy between two distributions. The latter
computation formed the basis for analyzing a basic operation on distributions: that of projecting
a general distribution P into some exponential family Q, that is, ﬁnding the distribution within
Q that is closest to P. Because the notion of relative entropy is not symmetric, this concept gave
rise to two diﬀerent deﬁnitions: I-projection, where we minimize ID(Q||P), and M-projection,
where we minimize ID(P||Q). We analyzed the diﬀerences between these two deﬁnitions and
showed that solving the M-projection problem can be viewed in a particularly elegant way,
constructing a distribution Q that matches the expected suﬃcient statistics (or moments) of P.
As we discuss later in the book, both the I-projection and M-projection turn out to play an
important role in graphical models. The M-projection is the formal foundation for addressing
the learning problem: there, our goal is to ﬁnd a distribution in a particular class (for example,
a Bayesian network or Markov network of a given structure) that is closest (in the M-projection
sense) to the empirical distribution observed in a data set from which we wish to learn (see
equation (16.4)). The I-projection operation is used when we wish to take a given graphical
model P and answer probability queries; when P is too complex to allow queries to be answered

8.7. Relevant Literature
283
eﬃciently, one strategy is to construct a simpler distribution Q, which is a good approximation
to P (in the I-projection sense).
8.7
Relevant Literature
The concept of exponential families plays a central role in formal statistic theory. Much of the
theory is covered by classic textbooks such as Barndorﬀ-Nielsen (1978). See also Lauritzen (1996).
Geiger and Meek (1998) discuss the representation of graphical models as exponential families
and show that a Bayesian network usually does not deﬁne a linear exponential family.
The notion of I-projections was introduced by Csiszàr (1975), who developed the “information
geometry” of such projections and their connection to diﬀerent estimation procedures. In his
terminology, M-projections are called “reverse I-projections.”
The notion of M-projection is
closely related to parameter learning, which we revisit in chapter 17 and chapter 20.
8.8
Exercises
Exercise 8.1⋆
A variable X with Val(X) = 0, 1, 2, . . . is Poisson-distributed with parameter θ > 0 if
Poisson
distribution
P(X = k) = 1
k! exp −θθk.
This distribution has the property that IEP [X] = θ.
a. Show how to represent the Poisson distribution as a linear exponential family. (Note that unlike most
of our running examples, you need to use the auxiliary measure A in the deﬁnition.)
b. Use results developed in this chapter to ﬁnd the entropy of a Poisson distribution and the relative
entropy between two Poisson distributions.
c. What is the function ess associated with this family? Is it invertible?
Exercise 8.2
Prove theorem 8.3.
Exercise 8.3
In this exercise, we will provide a characterization of when two distributions P1 and P2 will have the same
M-projection.
a. Let P1 and P2 be two distribution over X, and let Q be an exponential family deﬁned by the functions
τ(ξ) and t(θ). If IEP1[τ(X)] = IEP2[τ(X)], then the M-projection of P1 and P2 onto Q is identical.
b. Now, show that if the function ess(θ) is invertible, then we can prove the converse, showing that the
M-projection of P1 and P2 is identical only if IEP1[τ(X)] = IEP2[τ(X)]. Conclude that this is the case
for linear exponential families.
Exercise 8.4
Consider the function ess for Gaussian variables as described in example 8.15.
a. What is the image of ess?
b. Consider terms of the form IEP [τ(X)] for the Gaussian suﬃcient statistics from that example. Show
that for any distribution P, the expected suﬃcient statistics is in the image of ess.

284
Chapter 8. The Exponential Family
Exercise 8.5￿
Prove theorem 8.7. (Hint: Use theorem 8.5.)
Exercise 8.6￿
Let X1, . . . , Xn be binary random variables. Suppose we are given a family Q of chain distributions of
the form Q(X1, . . . , Xn) = Q(X1)Q(X2 | X1) · · · Q(Xn | Xn−1). We now show how to reformulate
this family as a linear exponential family.
a. Show that the following vector of statistics is su￿cient and nonredundant for distributions in the family:
τ(X1, . . . , Xn) =








11{X1 = x1
1},
. . .
11{Xn = x1
n},
11{X1 = x1
1, X2 = x1
2},
. . .
11{Xn−1 = x1
n−1, Xn = x1
n}








.
b. Show that you can reconstruct the distributions Q(X1) and Q(Xi+1 | Xi) from the the expectation
IEQ[τ(X1, . . . , Xn)]. This shows that given the expected su￿cient statistics you can reconstruct Q.
c. Suppose you know Q. Show how to reparameterize it as a linear exponential model
Q(X1, . . . , Xn) = 1
Z exp
￿￿
i
θi11{Xi = x1
i } +
￿
i
θi,i+111{Xi = x1
i , Xi+1 = x1
i+1}
￿
.
(8.9)
Note that, because the statistics are su￿cient, we know that there are some parameters for which we
get equality; the question is to determine their values. Speciﬁcally, show that if we choose:
θi = ln Q(x0
1, . . . , x0
i−1, x1
i , x0
i+1, . . . , x0
n)
Q(x0
1, . . . , x0n)
and
θi,i+1 = ln Q(x0
1, . . . , x0
i−1, x1
i , x1
i+1x0
i+2, . . . , x0
n)
Q(x0
1, . . . , x0n)
−θi −θi+1
then we get equality in equation (8.9) for all assignments to X1, . . . , Xn.

Part II
Inference


9
Exact Inference: Variable Elimination
In this chapter, we discuss the problem of performing inference in graphical models. We show
that the structure of the network, both the conditional independence assertions it makes and
the associated factorization of the joint distribution, is critical to our ability to perform inference
eﬀectively, allowing tractable inference even in complex networks.
Our focus in this chapter is on the most common query type: the conditional probability
conditional
probability query
query, P(Y | E = e) (see section 2.1.5). We have already seen several examples of conditional
probability queries in chapter 3 and chapter 4; as we saw, such queries allow for many useful
reasoning patterns, including explanation, prediction, intercausal reasoning, and many more.
By the deﬁnition of conditional probability, we know that
P(Y | E = e) = P(Y , e)
P(e)
.
(9.1)
Each of the instantiations of the numerator is a probability expression P(y, e), which can be
computed by summing out all entries in the joint that correspond to assignments consistent
with y, e. More precisely, let W = X −Y −E be the random variables that are neither query
nor evidence. Then
P(y, e) =
X
w
P(y, e, w).
(9.2)
Because Y , E, W are all of the network variables, each term P(y, e, w) in the summation is
simply an entry in the joint distribution.
The probability P(e) can also be computed directly by summing out the joint. However, it
can also be computed as
P(e) =
X
y
P(y, e),
(9.3)
which allows us to reuse our computation for equation (9.2). If we compute both equation (9.2)
and equation (9.3), we can then divide each P(y, e) by P(e), to get the desired conditional
probability P(y | e).
Note that this process corresponds to taking the vector of marginal
probabilities P(y1, e), . . . , P(yk, e) (where k = |Val(Y )|) and renormalizing the entries to
renormalization
sum to 1.

288
Chapter 9. Variable Elimination
9.1
Analysis of Complexity
In principle, a graphical model can be used to answer all of the query types described earlier.
We simply generate the joint distribution and exhaustively sum out the joint (in the case of a
conditional probability query), search for the most likely entry (in the case of a MAP query), or
both (in the case of a marginal MAP query). However, this approach to the inference problem is
not very satisfactory, since it returns us to the exponential blowup of the joint distribution that
the graphical model representation was precisely designed to avoid.
Unfortunately, we now show that exponential blowup of the inference task is (almost

certainly) unavoidable in the worst case: The problem of inference in graphical models is
NP-hard, and therefore it probably requires exponential time in the worst case (except
in the unlikely event that P = NP). Even worse, approximate inference is also NP-hard.
Importantly, however, the story does not end with this negative result. In general, we care
not about the worst case, but about the cases that we encounter in practice. As we show
in the remainder of this part of the book, many real-world applications can be tackled
very eﬀectively using exact or approximate inference algorithms for graphical models.
In our theoretical analysis, we focus our discussion on Bayesian networks.
Because any
Bayesian network can be encoded as a Markov network with no increase in its representation
size, a hardness proof for inference in Bayesian networks immediately implies hardness of
inference in Markov networks.
9.1.1
Analysis of Exact Inference
To address the question of the complexity of BN inference, we need to address the question of
how we encode a Bayesian network. Without going into too much detail, we can assume that
the encoding speciﬁes the DAG structure and the CPDs. For the following results, we assume
the worst-case representation of a CPD as a full table of size |Val({Xi} ∪PaXi)|.
As we discuss in appendix A.3.4, most analyses of complexity are stated in terms of decision
problems. We therefore begin with a formulation of the inference problem as a decision prob-
lem, and then discuss the numerical version. One natural decision version of the conditional
probability task is the problem BN-Pr-DP, deﬁned as follows:
Given a Bayesian network B over X, a variable X ∈X, and a value x ∈Val(X), decide
whether PB(X = x) > 0.
Theorem 9.1
The decision problem BN-Pr-DP is NP-complete.
Proof It is straightforward to prove that BN-Pr-DP is in NP: In the guessing phase, we guess a
full assignment ξ to the network variables. In the veriﬁcation phase, we check whether X = x
in ξ, and whether P(ξ) > 0. One of these guesses succeeds if and only if P(X = x) > 0.
Computing P(ξ) for a full assignment of the network variables requires only that we multiply
the relevant entries in the factors, as per the chain rule for Bayesian networks, and hence can
be done in linear time.
To prove NP-hardness, we need to show that, if we can answer instances in BN-Pr-DP,
we can use that as a subroutine to answer questions in a class of problems that is known
to be NP-hard. We will use a reduction from the 3-SAT problem deﬁned in deﬁnition A.8.
3-SAT

9.1. Analysis of Complexity
289
Q1
Qn
Q4
Q3
Q2
C1
A1
X
Am–2
A2
Cm
Cm–1
C3
C2
. . .
. . .
Figure 9.1
An outline of the network structure used in the reduction of 3-SAT to Bayesian network
inference.
To show the reduction, we show the following: Given any 3-SAT formula φ, we can create a
Bayesian network Bφ with some distinguished variable X, such that φ is satisﬁable if and only if
PBφ(X = x1) > 0. Thus, if we can solve the Bayesian network inference problem in polynomial
time, we can also solve the 3-SAT problem in polynomial time. To enable this conclusion, our
BN Bφ has to be constructible in time that is polynomial in the length of the formula φ.
Consider a 3-SAT instance φ over the propositional variables q1, . . . , qn. Figure 9.1 illustrates
the structure of the network constructed in this reduction. Our Bayesian network Bφ has a node
Qk for each propositional variable qk; these variables are roots, with P(q1
k) = 0.5. It also has a
node Ci for each clause Ci. There is an edge from Qk to Ci if qk or ¬qk is one of the literals
in Ci. The CPD for Ci is deterministic, and chosen such that it exactly duplicates the behavior
of the clause. Note that, because Ci contains at most three variables, the CPD has at most eight
distributions, and at most sixteen entries.
We want to introduce a variable X that has the value 1 if and only if all the Ci’s have
the value 1. We can achieve this requirement by having C1, . . . , Cm be parents of X. This
construction, however, has the property that P(X | C1, . . . , Cm) is exponentially large when
written as a table. To avoid this diﬃculty, we introduce intermediate “AND” gates A1, . . . , Am−2,
so that A1 is the “AND” of C1 and C2, A2 is the “AND” of A1 and C3, and so on. The last
variable X is the “AND” of Am−2 and Cm. This construction achieves the desired eﬀect: X has
value 1 if and only if all the clauses are satisﬁed. Furthermore, in this construction, all variables
have at most three (binary-valued) parents, so that the size of Bφ is polynomial in the size of φ.
It follows that PBφ(x1 | q1, . . . , qn) = 1 if and only if q1, . . . , qn is a satisfying assignment
for φ. Because the prior probability of each possible assignment is 1/2n, we get that the overall
probability PBφ(x1) is the number of satisfying assignments to φ, divided by 2n.
We can
therefore test whether φ has a satisfying assignment simply by checking whether P(x1) > 0.
This analysis shows that the decision problem associated with Bayesian network inference is
NP-complete. However, the problem is originally a numerical problem. Precisely the same
construction allows us to provide an analysis for the original problem formulation. We deﬁne
the problem BN-Pr as follows:

290
Chapter 9. Variable Elimination
Given: a Bayesian network B over X, a variable X ∈X, and a value x ∈Val(X),
compute PB(X = x).
Our task here is to compute the total probability of network instantiations that are consistent
with X = x. Or, in other words, to do a weighted count of instantiations, with the weight being
the probability. An appropriate complexity class for counting problems is #P: Whereas NP
represents problems of deciding “are there any solutions that satisfy certain requirements,” #P
represents problems that ask “how many solutions are there that satisfy certain requirements.” It
is not surprising that we can relate the complexity of the BN inference problem to the counting
class #P:
Theorem 9.2
The problem BN-Pr is #P-complete.
We leave the proof as an exercise (exercise 9.1).
9.1.2
Analysis of Approximate Inference
Upon noting the hardness of exact inference, a natural question is whether we can circumvent
the diﬃculties by compromising, to some extent, on the accuracies of our answers. Indeed, in
many applications we can tolerate some imprecision in the ﬁnal probabilities: it is often unlikely
that a change in probability from 0.87 to 0.92 will change our course of action. Thus, we now
explore the computational complexity of approximate inference.
To analyze the approximate inference task formally, we must ﬁrst deﬁne a metric for evaluating
the quality of our approximation. We can consider two perspectives on this issue, depending on
how we choose to deﬁne our query. Consider ﬁrst our previous formulation of the conditional
probability query task, where our goal is to compute the probability P(Y | e) for some set of
variables Y and evidence e. The result of this type of query is a probability distribution over
Y . Given an approximate answer to this query, we can evaluate its quality using any of the
distance metrics we deﬁne for probability distributions in appendix A.1.3.3.
There is, however, another way of looking at this task, one that is somewhat simpler and will
be very useful for analyzing its complexity. Consider a speciﬁc query P(y | e), where we are
focusing on one particular assignment y. The approximate answer to this query is a number ρ,
whose accuracy we wish to evaluate relative to the correct probability. One way of evaluating
the accuracy of an estimate is as simple as the diﬀerence between the approximate answer and
the right one.
Deﬁnition 9.1
An estimate ρ has absolute error ϵ for P(y | e) if:
absolute error
|P(y | e) −ρ| ≤ϵ.
This deﬁnition, although plausible, is somewhat weak.
Consider, for example, a situation
in which we are trying to compute the probability of a really rare disease, one whose true
probability is, say, 0.00001. In this case, an absolute error of 0.0001 is unacceptable, even
though such an error may be an excellent approximation for an event whose probability is 0.3.
A stronger deﬁnition of accuracy takes into consideration the value of the probability that we
are trying to estimate:

9.1. Analysis of Complexity
291
Deﬁnition 9.2
An estimate ρ has relative error ϵ for P(y | e) if:
relative error
ρ
1 + ϵ ≤P(y | e) ≤ρ(1 + ϵ).
Note that, unlike absolute error, relative error makes sense even for ϵ > 1. For example, ϵ = 4
means that P(y | e) is at least 20 percent of ρ and at most 600 percent of ρ. For probabilities,
where low values are often very important, relative error appears much more relevant than
absolute error.
With these deﬁnitions, we can turn to answering the question of whether approximate in-
ference is actually an easier problem. A priori, it seems as if the extra slack provided by the
approximation might help. Unfortunately, this hope turns out to be unfounded. As we now
show, approximate inference in Bayesian networks is also NP-hard.
This result is straightforward for the case of relative error.
Theorem 9.3
The following problem is NP-hard:
Given a Bayesian network B over X, a variable X ∈X, and a value x ∈Val(X), ﬁnd a
number ρ that has relative error ϵ for PB(X = x).
Proof The proof is obvious based on the original NP-hardness proof for exact Bayesian network
inference (theorem 9.1). There, we proved that it is NP-hard to decide whether PB(x1) > 0.
Now, assume that we have an algorithm that returns an estimate ρ to the same PB(x1), which
is guaranteed to have relative error ϵ for some ϵ > 0. Then ρ > 0 if and only if PB(x1) > 0.
Thus, achieving this relative error is as NP-hard as the original problem.
We can generalize this result to make ϵ(n) a function that grows with the input size n. Thus,
for example, we can deﬁne ϵ(n) = 22n and the theorem still holds. Thus, in a sense, this result
is not so interesting as a statement about hardness of approximation. Rather, it tells us that
relative error is too strong a notion of approximation to use in this context.
What about absolute error? As we will see in section 12.1.2, the problem of just approximating
P(X = x) up to some ﬁxed absolute error ϵ has a randomized polynomial time algorithm.
Therefore, the problem cannot be NP-hard unless NP = RP. This result is an improvement
on the exact case, where even the task of computing P(X = x) is NP-hard.
Unfortunately, the good news is very limited in scope, in that it disappears once we introduce
evidence. Speciﬁcally, it is NP-hard to ﬁnd an absolute approximation to P(x | e) for any
ϵ < 1/2.
Theorem 9.4
The following problem is NP-hard for any ϵ ∈(0, 1/2):
Given a Bayesian network B over X, a variable X ∈X, a value x ∈Val(X), and an
observation E = e for E ⊂X and e ∈Val(E), ﬁnd a number ρ that has absolute error ϵ
for PB(X = x | e).
Proof The proof uses the same construction that we used before. Consider a formula φ, and
consider the analogous BN B, as described in theorem 9.1. Recall that our BN had a variable
Qi for each propositional variable qi in our Boolean formula, a bunch of other intermediate

292
Chapter 9. Variable Elimination
variables, and then a variable X whose value, given any assignment of values q1
1, q0
1 to the Qi’s,
was the associated truth value of the formula. We now show that, given such an approximation
algorithm, we can decide whether the formula is satisﬁable. We begin by computing P(Q1 | x1).
We pick the value v1 for Q1 that is most likely given x1, and we instantiate it to this value. That
is, we generate a network B2 that does not contain Q1, and that represents the distribution
B conditioned on Q1 = v1. We repeat this process for Q2, . . . , Qn. This results in some
assignment v1, . . . , vn to the Qi’s. We now prove that this is a satisfying assignment if and only
if the original formula φ was satisﬁable.
We begin with the easy case. If φ is not satisﬁable, then v1, . . . , vn can hardly be a satisfying
assignment for it. Now, assume that φ is satisﬁable. We show that it also has a satisfying
assignment with Q1 = v1. If φ is satisﬁable with both Q1 = q1
1 and Q1 = q0
1, then this is
obvious. Assume, however, that φ is satisﬁable, but not when Q1 = v. Then necessarily, we
will have that P(Q1 = v | x1) is 0, and the probability of the complementary event is 1. If
we have an approximation ρ whose error is guaranteed to be < 1/2, then choosing the v that
maximizes this probability is guaranteed to pick the v whose probability is 1. Thus, in either
case the formula has a satisfying assignment where Q1 = v.
We can continue in this fashion, proving by induction on k that φ has a satisfying assignment
with Q1 = v1, . . . , Qk = vk. In the case where φ is satisﬁable, this process will terminate with
a satisfying assignment. In the case where φ is not, it clearly will not terminate with a satisfying
assignment. We can determine which is the case simply by checking whether the resulting
assignment satisﬁes φ. This gives us a polynomial time process for deciding satisﬁability.
Because ϵ = 1/2 corresponds to random guessing, this result is quite discouraging. It tells
us that, in the case where we have evidence, approximate inference is no easier than exact
inference, in the worst case.
9.2
Variable Elimination: The Basic Ideas
We begin our discussion of inference by discussing the principles underlying exact inference in
graphical models. As we show, the same graphical structure that allows a compact represen-
tation of complex distributions also help support inference. In particular, we can use dynamic
programming techniques (as discussed in appendix A.3.3) to perform inference even for certain
large and complex networks in a very reasonable time. We now provide the intuition underlying
these algorithms, an intuition that is presented more formally in the remainder of this chapter.
We begin by considering the inference task in a very simple network A →B →C →
D. We ﬁrst provide a phased computation, which uses results from the previous phase for
the computation in the next phase.
We then reformulate this process in terms of a global
computation on the joint distribution.
Assume that our ﬁrst goal is to compute the probability P(B), that is, the distribution over
values b of B. Basic probabilistic reasoning (with no assumptions) tells us that
P(B) =
X
a
P(a)P(B | a).
(9.4)
Fortunately, we have all the required numbers in our Bayesian network representation: each
number P(a) is in the CPD for A, and each number P(b | a) is in the CPD for B. Note that

9.2. Variable Elimination: The Basic Ideas
293
if A has k values and B has m values, the number of basic arithmetic operations required is
O(k × m): to compute P(b), we must multiply P(b | a) with P(a) for each of the k values of
A, and then add them up, that is, k multiplications and k −1 additions; this process must be
repeated for each of the m values b.
Now, assume we want to compute P(C). Using the same analysis, we have that
P(C) =
X
b
P(b)P(C | b).
(9.5)
Again, the conditional probabilities P(c | b) are known: they constitute the CPD for C. The
probability of B is not speciﬁed as part of the network parameters, but equation (9.4) shows us
how it can be computed. Thus, we can compute P(C). We can continue the process in an
analogous way, in order to compute P(D).
Note that the structure of the network, and its eﬀect on the parameterization of the CPDs, is
critical for our ability to perform this computation as described. Speciﬁcally, assume that A had
been a parent of C. In this case, the CPD for C would have included A, and our computation
of P(B) would not have suﬃced for equation (9.5).
Also note that this algorithm does not compute single values, but rather sets of values at a
time. In particular equation (9.4) computes an entire distribution over all of the possible values
of B. All of these are then used in equation (9.5) to compute P(C). This property turns out to
be critical for the performance of the general algorithm.
Let us analyze the complexity of this process on a general chain. Assume that we have a
chain with n variables X1 →. . . →Xn, where each variable in the chain has k values. As
described, the algorithm would compute P(Xi+1) from P(Xi), for i = 1, . . . , n−1. Each such
step would consist of the following computation:
P(Xi+1) =
X
xi
P(Xi+1 | xi)P(xi),
where P(Xi) is computed in the previous step. The cost of each such step is O(k2): The
distribution over Xi has k values, and the CPD P(Xi+1 | Xi) has k2 values; we need to
multiply P(xi), for each value xi, with each CPD entry P(xi+1 | xi) (k2 multiplications), and
then, for each value xi+1, sum up the corresponding entries (k × (k −1) additions). We need
to perform this process for every variable X2, . . . , Xn; hence, the total cost is O(nk2).
By comparison, consider the process of generating the entire joint and summing it out, which
requires that we generate kn probabilities for the diﬀerent events x1, . . . , xn. Hence, we have
at least one example where, despite the exponential size of the joint distribution, we can do
inference in linear time.
Using this process, we have managed to do inference over the joint distribution without
ever generating it explicitly. What is the basic insight that allows us to avoid the exhaustive
enumeration? Let us reexamine this process in terms of the joint P(A, B, C, D). By the chain
rule for Bayesian networks, the joint decomposes as
P(A)P(B | A)P(C | B)P(D | C)
To compute P(D), we need to sum together all of the entries where D = d1, and to (separately)
sum together all of the entries where D = d2.
The exact computation that needs to be

294
Chapter 9. Variable Elimination
P(a1)
P(b1 | a1)
P(c1 | b1)
P(d1 | c1)
+ P(a2)
P(b1 | a2)
P(c1 | b1)
P(d1 | c1)
+ P(a1)
P(b2 | a1)
P(c1 | b2)
P(d1 | c1)
+ P(a2)
P(b2 | a2)
P(c1 | b2)
P(d1 | c1)
+ P(a1)
P(b1 | a1)
P(c2 | b1)
P(d1 | c2)
+ P(a2)
P(b1 | a2)
P(c2 | b1)
P(d1 | c2)
+ P(a1)
P(b2 | a1)
P(c2 | b2)
P(d1 | c2)
+ P(a2)
P(b2 | a2)
P(c2 | b2)
P(d1 | c2)
P(a1)
P(b1 | a1)
P(c1 | b1)
P(d2 | c1)
+ P(a2)
P(b1 | a2)
P(c1 | b1)
P(d2 | c1)
+ P(a1)
P(b2 | a1)
P(c1 | b2)
P(d2 | c1)
+ P(a2)
P(b2 | a2)
P(c1 | b2)
P(d2 | c1)
+ P(a1)
P(b1 | a1)
P(c2 | b1)
P(d2 | c2)
+ P(a2)
P(b1 | a2)
P(c2 | b1)
P(d2 | c2)
+ P(a1)
P(b2 | a1)
P(c2 | b2)
P(d2 | c2)
+ P(a2)
P(b2 | a2)
P(c2 | b2)
P(d2 | c2)
Figure 9.2
Computing P(D) by summing over the joint distribution for a chain A →B →C →
D; all of the variables are binary valued.
performed, for binary-valued variables A, B, C, D, is shown in ﬁgure 9.2.1
Examining this summation, we see that it has a lot of structure. For example, the third and
fourth terms in the ﬁrst two entries are both P(c1 | b1)P(d1 | c1). We can therefore modify
the computation to ﬁrst compute
P(a1)P(b1 | a1) + P(a2)P(b1 | a2)
and only then multiply by the common term. The same structure is repeated throughout the
table. If we perform the same transformation, we get a new expression, as shown in ﬁgure 9.3.
We now observe that certain terms are repeated several times in this expression. Speciﬁcally,
P(a1)P(b1 | a1) + P(a2)P(b1 | a2) and P(a1)P(b2 | a1) + P(a2)P(b2 | a2) are each
repeated four times. Thus, it seems clear that we can gain signiﬁcant computational savings by
computing them once and then storing them. There are two such expressions, one for each
value of B. Thus, we deﬁne a function τ1 : Val(B) 7→IR, where τ1(b1) is the ﬁrst of these
two expressions, and τ1(b2) is the second. Note that τ1(B) corresponds exactly to P(B).
The resulting expression, assuming τ1(B) has been computed, is shown in ﬁgure 9.4. Examin-
ing this new expression, we see that we once again can reverse the order of a sum and a product,
resulting in the expression of ﬁgure 9.5. And, once again, we notice some shared expressions,
that are better computed once and used multiple times. We deﬁne τ2 : Val(C) 7→IR.
τ2(c1)
=
τ1(b1)P(c1 | b1) + τ1(b2)P(c1 | b2)
τ2(c2)
=
τ1(b1)P(c2 | b1) + τ1(b2)P(c2 | b2)
1. When D is binary-valued, we can get away with doing only the ﬁrst of these computations. However, this trick does
not carry over to the case of variables with more than two values or to the case where we have evidence. Therefore, our
example will show the computation in its generality.

9.2. Variable Elimination: The Basic Ideas
295
(P(a1)P(b1 | a1) + P(a2)P(b1 | a2))
P(c1 | b1)
P(d1 | c1)
+ (P(a1)P(b2 | a1) + P(a2)P(b2 | a2))
P(c1 | b2)
P(d1 | c1)
+ (P(a1)P(b1 | a1) + P(a2)P(b1 | a2))
P(c2 | b1)
P(d1 | c2)
+ (P(a1)P(b2 | a1) + P(a2)P(b2 | a2))
P(c2 | b2)
P(d1 | c2)
(P(a1)P(b1 | a1) + P(a2)P(b1 | a2))
P(c1 | b1)
P(d2 | c1)
+ (P(a1)P(b2 | a1) + P(a2)P(b2 | a2))
P(c1 | b2)
P(d2 | c1)
+ (P(a1)P(b1 | a1) + P(a2)P(b1 | a2))
P(c2 | b1)
P(d2 | c2)
+ (P(a1)P(b2 | a1) + P(a2)P(b2 | a2))
P(c2 | b2)
P(d2 | c2)
Figure 9.3
The ﬁrst transformation on the sum of ﬁgure 9.2
τ1(b1)
P(c1 | b1)
P(d1 | c1)
+ τ1(b2)
P(c1 | b2)
P(d1 | c1)
+ τ1(b1)
P(c2 | b1)
P(d1 | c2)
+ τ1(b2)
P(c2 | b2)
P(d1 | c2)
τ1(b1)
P(c1 | b1)
P(d2 | c1)
+ τ1(b2)
P(c1 | b2)
P(d2 | c1)
+ τ1(b1)
P(c2 | b1)
P(d2 | c2)
+ τ1(b2)
P(c2 | b2)
P(d2 | c2)
Figure 9.4
The second transformation on the sum of ﬁgure 9.2
(τ1(b1)P(c1 | b1) + τ1(b2)P(c1 | b2))
P(d1 | c1)
+ (τ1(b1)P(c2 | b1) + τ1(b2)P(c2 | b2))
P(d1 | c2)
(τ1(b1)P(c1 | b1) + τ1(b2)P(c1 | b2))
P(d2 | c1)
+ (τ1(b1)P(c2 | b1) + τ1(b2)P(c2 | b2))
P(d2 | c2)
Figure 9.5
The third transformation on the sum of ﬁgure 9.2
τ2(c1)
P(d1 | c1)
+ τ2(c2)
P(d1 | c2)
τ2(c1)
P(d2 | c1)
+ τ2(c2)
P(d2 | c2)
Figure 9.6
The fourth transformation on the sum of ﬁgure 9.2
The ﬁnal expression is shown in ﬁgure 9.6.
Summarizing, we begin by computing τ1(B), which requires four multiplications and two
additions. Using it, we can compute τ2(C), which also requires four multiplications and two
additions.
Finally, we can compute P(D), again, at the same cost.
The total number of
operations is therefore 18. By comparison, generating the joint distribution requires 16 · 3 = 48

296
Chapter 9. Variable Elimination
multiplications (three for each of the 16 entries in the joint), and 14 additions (7 for each of
P(d1) and P(d2)).
Written somewhat more compactly, the transformation we have performed takes the following
steps: We want to compute
P(D) =
X
C
X
B
X
A
P(A)P(B | A)P(C | B)P(D | C).
We push in the ﬁrst summation, resulting in
X
C
P(D | C)
X
B
P(C | B)
X
A
P(A)P(B | A).
We compute the product ψ1(A, B) = P(A)P(B | A) and then sum out A to obtain the func-
tion τ1(B) = P
A ψ1(A, B). Speciﬁcally, for each value b, we compute τ1(b) = P
A ψ1(A, b) =
P
A P(A)P(b | A). We then continue by computing:
ψ2(B, C)
=
τ1(B)P(C | B)
τ2(C)
=
X
B
ψ2(B, C).
This computation results in a new vector τ2(C), which we then proceed to use in the ﬁnal
phase of computing P(D).
This procedure is performing dynamic programming (see appendix A.3.3); doing this sum-
dynamic
programming
mation the naive way would have us compute every P(b) = P
A P(A)P(b | A) many times,
once for every value of C and D. In general, in a chain of length n, this internal summation
would be computed exponentially many times. Dynamic programming “inverts” the order of
computation — performing it inside out instead of outside in. Speciﬁcally, we perform the
innermost summation ﬁrst, computing once and for all the values in τ1(B); that allows us to
compute τ2(C) once and for all, and so on.

To summarize, the two ideas that help us address the exponential blowup of the joint
distribution are:
•
Because of the structure of the Bayesian network, some subexpressions in the joint
depend only on a small number of variables.
•
By computing these expressions once and caching the results, we can avoid generating
them exponentially many times.
9.3
Variable Elimination
To formalize the algorithm demonstrated in the previous section, we need to introduce some
basic concepts. In chapter 4, we introduced the notion of a factor φ over a scope Scope[φ] = X,
factor
which is a function φ : Val(X) 7→IR. The main steps in the algorithm described here can be
viewed as a manipulation of factors. Importantly, by using the factor-based view, we can deﬁne
the algorithm in a general form that applies equally to Bayesian networks and Markov networks.

9.3. Variable Elimination
297
a1
a1
a1
a1
a2
a2
a2
a2
a3
a3
a3
a3
b1
b1
b2
b2
b1
b1
b2
b2
b1
b1
b2
b2
c1
c2
c1
c2
c1
c2
c1
c2
c1
c2
c1
c2
0.25
0.35
0.08
0.16
0.05
0.07
  0
  0
0.15
0.21
0.09
0.18
a1
a1
a2
a2
a3
a3
c1
c2
c1
c2
c1
c2
0.33
0.51
0.05
0.07
0.24
0.39
Figure 9.7
Example of factor marginalization: summing out B.
9.3.1
Basic Elimination
9.3.1.1
Factor Marginalization
The key operation that we are performing when computing the probability of some subset of
variables is that of marginalizing out variables from a distribution. That is, we have a distribution
over a set of variables X, and we want to compute the marginal of that distribution over some
subset X. We can view this computation as an operation on a factor:
Deﬁnition 9.3
Let X be a set of variables, and Y ̸∈X a variable. Let φ(X, Y ) be a factor. We deﬁne the factor
factor
marginalization
marginalization of Y in φ, denoted P
Y φ, to be a factor ψ over X such that:
ψ(X) =
X
Y
φ(X, Y ).
This operation is also called summing out of Y in ψ.
The key point in this deﬁnition is that we only sum up entries in the table where the values of
X match up. Figure 9.7 illustrates this process.
The process of marginalizing a joint distribution P(X, Y ) onto X in a Bayesian network
is simply summing out the variables Y in the factor corresponding to P. If we sum out all
variables, we get a factor consisting of a single number whose value is 1. If we sum out all of
the variables in the unnormalized distribution ˜PΦ deﬁned by the product of factors in a Markov
network, we get the partition function.
A key observation used in performing inference in graphical models is that the operations of
factor product and summation behave precisely as do product and summation over numbers.
Speciﬁcally, both operations are commutative, so that φ1 · φ2 = φ2 · φ1 and P
X
P
Y φ =
P
Y
P
X φ. Products are also associative, so that (φ1·φ2)·φ3 = φ1·(φ2·φ3). Most importantly,

298
Chapter 9. Variable Elimination
Algorithm 9.1 Sum-product variable elimination algorithm
Procedure Sum-Product-VE (
Φ,
// Set of factors
Z,
// Set of variables to be eliminated
≺
// Ordering on Z
)
1
Let Z1, . . . , Zk be an ordering of Z such that
2
Zi ≺Zj if and only if i < j
3
for i = 1, . . . , k
4
Φ ←Sum-Product-Eliminate-Var(Φ, Zi)
5
φ∗←Q
φ∈Φ φ
6
return φ∗
Procedure Sum-Product-Eliminate-Var (
Φ,
// Set of factors
Z
// Variable to be eliminated
)
1
Φ′ ←{φ ∈Φ : Z ∈Scope[φ]}
2
Φ′′ ←Φ −Φ′
3
ψ ←Q
φ∈Φ′ φ
4
τ ←P
Z ψ
5
return Φ′′ ∪{τ}
we have a simple rule allowing us to exchange summation and product: If X ̸∈Scope[φ1], then
X
X
(φ1 · φ2) = φ1 ·
X
X
φ2.
(9.6)
9.3.1.2
The Variable Elimination Algorithm
The key to both of our examples in the last section is the application of equation (9.6). Speciﬁ-
cally, in our chain example of section 9.2, we can write:
P(A, B, C, D) = φA · φB · φC · φD.
On the other hand, the marginal distribution over D is
P(D) =
X
C
X
B
X
A
P(A, B, C, D).

9.3. Variable Elimination
299
Applying equation (9.6), we can now conclude:
P(D)
=
X
C
X
B
X
A
φA · φB · φC · φD
=
X
C
X
B
φC · φD ·
 X
A
φA · φB
!
=
X
C
φD ·
 X
B
φC ·
 X
A
φA · φB
!!
,
where the diﬀerent transformations are justiﬁed by the limited scope of the CPD factors; for
example, the second equality is justiﬁed by the fact that the scope of φC and φD does not
contain A. In general, any marginal probability computation involves taking the product of all
the CPDs, and doing a summation on all the variables except the query variables. We can do
these steps in any order we want, as long as we only do a summation on a variable X after
multiplying in all of the factors that involve X.
In general, we can view the task at hand as that of computing the value of an expression of
the form:
X
Z
Y
φ∈Φ
φ.
We call this task the sum-product inference task.
The key insight that allows the eﬀective
sum-product
computation of this expression is the fact that the scope of the factors is limited, allowing us
to “push in” some of the summations, performing them over the product of only a subset of
factors. One simple instantiation of this algorithm is a procedure called sum-product variable
variable
elimination
elimination (VE), shown in algorithm 9.1. The basic idea in the algorithm is that we sum out
variables one at a time. When we sum out any variable, we multiply all the factors that mention
that variable, generating a product factor. Now, we sum out the variable from this combined
factor, generating a new factor that we enter into our set of factors to be dealt with.
Based on equation (9.6), the following result follows easily:
Theorem 9.5
Let X be some set of variables, and let Φ be a set of factors such that for each φ ∈Φ, Scope[φ] ⊆X.
Let Y ⊂X be a set of query variables, and let Z = X −Y . Then for any ordering ≺over Z,
Sum-Product-VE(Φ, Z, ≺) returns a factor φ∗(Y ) such that
φ∗(Y ) =
X
Z
Y
φ∈Φ
φ.
We can apply this algorithm to the task of computing the probability distribution PB(Y ) for
a Bayesian network B. We simply instantiate Φ to consist of all of the CPDs:
Φ = {φXi}n
i=1
where φXi = P(Xi | PaXi).
We then apply the variable elimination algorithm to the set
{Z1, . . . , Zm} = X −Y (that is, we eliminate all the nonquery variables).
We can also apply precisely the same algorithm to the task of computing conditional prob-
abilities in a Markov network. We simply initialize the factors to be the clique potentials and

300
Chapter 9. Variable Elimination
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
Figure 9.8
The Extended-Student Bayesian network
run the elimination algorithm. As for Bayesian networks, we then apply the variable elimination
algorithm to the set Z = X −Y . The procedure returns an unnormalized factor over the query
variables Y . The distribution over Y can be obtained by normalizing the factor; the partition
function is simply the normalizing constant.
Example 9.1
Let us demonstrate the procedure on a nontrivial example. Consider the network demonstrated in
ﬁgure 9.8, which is an extension of our Student network. The chain rule for this network asserts
that
P(C, D, I, G, S, L, J, H)
=
P(C)P(D | C)P(I)P(G | I, D)P(S | I)
P(L | G)P(J | L, S)P(H | G, J)
=
φC(C)φD(D, C)φI(I)φG(G, I, D)φS(S, I)
φL(L, G)φJ(J, L, S)φH(H, G, J).
We will now apply the VE algorithm to compute P(J). We will use the elimination ordering:
C, D, I, H, G, S, L:
1. Eliminating C: We compute the factors
ψ1(C, D)
=
φC(C) · φD(D, C)
τ1(D)
=
X
C
ψ1.
2. Eliminating D: Note that we have already eliminated one of the original factors that involve D
— φD(D, C) = P(D | C). On the other hand, we introduced the factor τ1(D) that involves

9.3. Variable Elimination
301
D. Hence, we now compute:
ψ2(G, I, D)
=
φG(G, I, D) · τ1(D)
τ2(G, I)
=
X
D
ψ2(G, I, D).
3. Eliminating I: We compute the factors
ψ3(G, I, S)
=
φI(I) · φS(S, I) · τ2(G, I)
τ3(G, S)
=
X
I
ψ3(G, I, S).
4. Eliminating H: We compute the factors
ψ4(G, J, H)
=
φH(H, G, J)
τ4(G, J)
=
X
H
ψ4(G, J, H).
Note that τ4 ≡1 (all of its entries are exactly 1): we are simply computing P
H P(H | G, J),
which is a probability distribution for every G, J, and hence sums to 1. A naive execution of this
algorithm will end up generating this factor, which has no value. Generating it has no impact
on the ﬁnal answer, but it does complicate the algorithm. In particular, the existence of this
factor complicates our computation in the next step.
5. Eliminating G: We compute the factors
ψ5(G, J, L, S)
=
τ4(G, J) · τ3(G, S) · φL(L, G)
τ5(J, L, S)
=
X
G
ψ5(G, J, L, S).
Note that, without the factor τ4(G, J), the results of this step would not have involved J.
6. Eliminating S: We compute the factors
ψ6(J, L, S)
=
τ5(J, L, S) · φJ(J, L, S)
τ6(J, L)
=
X
S
ψ6(J, L, S).
7. Eliminating L: We compute the factors
ψ7(J, L)
=
τ6(J, L)
τ7(J)
=
X
L
ψ7(J, L).
We summarize these steps in table 9.1.
Note that we can use any elimination ordering. For example, consider eliminating variables in
the order G, I, S, L, H, C, D. We would then get the behavior of table 9.2. The result, as before,
is precisely P(J). However, note that this elimination ordering introduces factors with much larger
scope. We return to this point later on.

302
Chapter 9. Variable Elimination
Step
Variable
Factors
Variables
New
eliminated
used
involved
factor
1
C
φC(C), φD(D, C)
C, D
τ1(D)
2
D
φG(G, I, D), τ1(D)
G, I, D
τ2(G, I)
3
I
φI(I), φS(S, I), τ2(G, I)
G, S, I
τ3(G, S)
4
H
φH(H, G, J)
H, G, J
τ4(G, J)
5
G
τ4(G, J), τ3(G, S), φL(L, G)
G, J, L, S
τ5(J, L, S)
6
S
τ5(J, L, S), φJ(J, L, S)
J, L, S
τ6(J, L)
7
L
τ6(J, L)
J, L
τ7(J)
Table 9.1
A run of variable elimination for the query P(J)
Step
Variable
Factors
Variables
New
eliminated
used
involved
factor
1
G
φG(G, I, D), φL(L, G), φH(H, G, J)
G, I, D, L, J, H
τ1(I, D, L, J, H)
2
I
φI(I), φS(S, I), τ1(I, D, L, S, J, H)
S, I, D, L, J, H
τ2(D, L, S, J, H)
3
S
φJ(J, L, S), τ2(D, L, S, J, H)
D, L, S, J, H
τ3(D, L, J, H)
4
L
τ3(D, L, J, H)
D, L, J, H
τ4(D, J, H)
5
H
τ4(D, J, H)
D, J, H
τ5(D, J)
6
C
φC(C), φD(D, C)
D, J, C
τ6(D)
7
D
τ5(D, J), τ6(D)
D, J
τ7(J)
Table 9.2
A diﬀerent run of variable elimination for the query P(J)
9.3.1.3
Semantics of Factors
It is interesting to consider the semantics of the intermediate factors generated as part of
this computation. In many of the examples we have given, they correspond to marginal or
conditional probabilities in the network. However, although these factors often correspond to
such probabilities, this is not always the case. Consider, for example, the network of ﬁgure 9.9a.
The result of eliminating the variable X is a factor
τ(A, B, C) =
X
X
P(X) · P(A | X) · P(C | B, X).
This factor does not correspond to any probability or conditional probability in this network.
To understand why, consider the various options for the meaning of this factor.
Clearly, it
cannot be a conditional distribution where B is on the left hand side of the conditioning bar
(for example, P(A, B, C)), as P(B | A) has not yet been multiplied in. The most obvious
candidate is P(A, C | B). However, this conjecture is also false. The probability P(A | B)
relies heavily on the properties of the CPD P(B | A); for example, if B is deterministically
equal to A, P(A | B) has a very diﬀerent form than if B depends only very weakly on A. Since
the CPD P(B | A) was not taken into consideration when computing τ(A, B, C), it cannot
represent the conditional probability P(A, C | B). In general, we can verify that this factor

9.3. Variable Elimination
303
(a)
B
A
C
X
(b)
B
A
C
X
Figure 9.9
Understanding intermediate factors in variable elimination as conditional probabilities:
(a) A Bayesian network where elimination does not lead to factors that have an interpretation as conditional
probabilities. (b) A diﬀerent Bayesian network where the resulting factor does correspond to a conditional
probability.
does not correspond to any conditional probability expression in this network.
It is interesting to note, however, that the resulting factor does, in fact, correspond to a
conditional probability P(A, C | B), but in a diﬀerent network: the one shown in ﬁgure 9.9b,
where all CPDs except for B are the same. In fact, this phenomenon is a general one (see
exercise 9.2).
9.3.2
Dealing with Evidence
It remains only to consider how we would introduce evidence. For example, assume we observe
the value i1 (the student is intelligent) and h0 (the student is unhappy). Our goal is to compute
P(J | i1, h0).
First, we reduce this problem to computing the unnormalized distribution
P(J, i1, h0). From this intermediate result, we can compute the conditional probability as in
equation (9.1), by renormalizing by the probability of the evidence P(i1, h0).
How do we compute P(J, i1, h0)?
The key observation is proposition 4.7, which shows
us how to view, as a Gibbs distribution, an unnormalized measure derived from introducing
evidence into a Bayesian network. Thus, we can view this computation as summing out all of
the entries in the reduced factor: P[i1h0] whose scope is {C, D, G, L, S, J}. This factor is no
factor reduction
longer normalized, but it is still a valid factor.
Based on this observation, we can now apply precisely the same sum-product variable elim-
ination algorithm to the task of computing P(Y , e). We simply apply the algorithm to the set
of factors in the network, reduced by E = e, and eliminate the variables in X −Y −E. The
returned factor φ∗(Y ) is precisely P(Y , e). To obtain P(Y | e) we simply renormalize φ∗(Y )
by multiplying it by 1
α to obtain a legal distribution, where α is the sum over the entries in our
unnormalized distribution, which represents the probability of the evidence. To summarize, the
algorithm for computing conditional probabilities in a Bayesian or Markov network is shown in
algorithm 9.2.
We demonstrate this process on the example of computing P(J, i1, h0). We use the same

304
Chapter 9. Variable Elimination
Algorithm 9.2 Using Sum-Product-VE for computing conditional probabilities
Procedure Cond-Prob-VE (
K,
// A network over X
Y ,
// Set of query variables
E = e
// Evidence
)
1
Φ ←Factors parameterizing K
2
Replace each φ ∈Φ by φ[E = e]
3
Select an elimination ordering ≺
4
Z ←
= X −Y −E
5
φ∗←Sum-Product-VE(Φ, ≺, Z)
6
α ←P
y∈Val(Y ) φ∗(y)
7
return α, φ∗
Step
Variable
Factors
Variables
New
eliminated
used
involved
factor
1’
C
φC(C), φD(D, C)
C, D
τ ′
1(D)
2’
D
φG[I = i1](G, D), φI[I = i1](), τ ′
1(D)
G, D
τ ′
2(G)
5’
G
τ ′
2(G), φL(L, G), φH[H = h0](G, J)
G, L, J
τ ′
5(L, J)
6’
S
φS[I = i1](S), φJ(J, L, S)
J, L, S
τ ′
6(J, L)
7’
L
τ ′
6(J, L), τ ′
5(J, L)
J, L
τ ′
7(J)
Table 9.3
A run of sum-product variable elimination for P(J, i1, h0)
elimination ordering that we used in table 9.1. The results are shown in table 9.3; the step num-
bers correspond to the steps in table 9.1. It is interesting to note the diﬀerences between the two
runs of the algorithm. First, we notice that steps (3) and (4) disappear in the computation with
evidence, since I and H do not need to be eliminated. More interestingly, by not eliminating I,
we avoid the step that correlates G and S. In this execution, G and S never appear together in
the same factor; they are both eliminated, and only their end results are combined. Intuitively,
G and S are conditionally independent given I; hence, observing I renders them independent,
so that we do not have to consider their joint distribution explicitly. Finally, we notice that
φI[I = i1] = P(i1) is a factor over an empty scope, which is simply a number. It can be
multiplied into any factor at any point in the computation. We chose arbitrarily to incorporate
it into step (2′). Note that if our goal is to compute a conditional probability given the evidence,
and not the probability of the evidence itself, we can avoid multiplying in this factor entirely,
since its eﬀect will disappear in the renormalization step at the end.
Box 9.A — Concept: The Network Polynomial. The network polynomial provides an interest-
network
polynomial
ing and useful alternative view of variable elimination. We begin with describing the concept for
the case of a Gibbs distribution parameterized via a set of full table factors Φ. The polynomial fΦ

9.4. Complexity and Graph Structure: Variable Elimination
305
is deﬁned over the following set of variables:
• For each factor φc ∈Φ with scope Xc, we have a variable θxc for every xc ∈Val(Xc).
• For each variable Xi and every value xi ∈Val(Xi), we have a binary-valued variable λxi.
In other words, the polynomial has one argument for each of the network parameters and for each
possible assignment to a network variable. The polynomial fΦ is now deﬁned as follows:
fΦ(θ, λ) =
X
x1,...,xn

Y
φc∈Φ
θxc ·
n
Y
i=1
λxi

.
(9.7)
Evaluating the network polynomial is equivalent to the inference task. In particular, let Y = y
be an assignment to some subset of network variables; deﬁne an assignment λy as follows:
• for each Yi ∈Y , deﬁne λy
yi = 1 and λy
y′
i = 0 for all y′
i ̸= yi;
• for each Yi ̸∈Y , deﬁne λy
yi = 1 for all yi ∈Val(Yi).
With this deﬁnition, we can now show (exercise 9.4a) that:
fΦ(θ, λy) = ˜PΦ(Y = y | θ).
(9.8)
The derivatives of the network polynomial are also of signiﬁcant interest. We can show (exer-
cise 9.4b) that
∂fΦ(θ, λy)
∂λxi
= ˜PΦ(xi, y−i | θ),
(9.9)
where y−i is the assignment in y to all variables other than Xi. We can also show that
∂fΦ(θ, λy)
∂θxc
=
˜PΦ(y, xc | θ)
θxc
;
(9.10)
this fact is proved in lemma 19.1. These derivatives can be used for various purposes, including
retracting or modifying evidence in the network (exercise 9.4c), and sensitivity analysis — comput-
sensitivity
analysis
ing the eﬀect of changes in a network parameter on the answer to a particular probabilistic query
(exercise 9.5).
Of course, as deﬁned, the representation of the network polynomial is exponentially large in the
number of variables in the network. However, we can use the algebraic operations performed in a
run of variable elimination to deﬁne a network polynomial that has precisely the same complexity
as the VE run. More interesting, we can also use the same structure to compute eﬃciently all of the
derivatives of the network polynomial, relative both to the λi and the θxc (see exercise 9.6).
9.4
Complexity and Graph Structure: Variable Elimination
From the examples we have seen, it is clear that the VE algorithm can be computationally much
more eﬃcient than a full enumeration of the joint. In this section, we analyze the complexity
of the algorithm, and understand the source of the computational gains.
We also note that, aside from the asymptotic analysis, a careful implementation of this
algorithm can have signiﬁcant ramiﬁcations on performance; see box 10.A.

306
Chapter 9. Variable Elimination
9.4.1
Simple Analysis
Let us begin with a simple analysis of the basic computational operations taken by algorithm 9.1.
Assume we have n random variables, and m initial factors; in a Bayesian network, we have
m = n; in a Markov network, we may have more factors than variables. For simplicity, assume
we run the algorithm until all variables are eliminated.
The algorithm consists of a set of elimination steps, where, in each step, the algorithm picks
a variable Xi, then multiplies all factors involving that variable. The result is a single large
factor ψi. The variable then gets summed out of ψi, resulting in a new factor τi whose scope
is the scope of ψi minus Xi. Thus, the work revolves around these factors that get created and
processed. Let Ni be the number of entries in the factor ψi, and let Nmax = maxi Ni.
We begin by counting the number of multiplication steps.
Here, we note that the total
number of factors ever entered into the set of factors Φ is m + n: the m initial factors, plus
the n factors τi. Each of these factors φ is multiplied exactly once: when it is multiplied in
line 3 of Sum-Product-Eliminate-Var to produce a large factor ψi, it is also extracted from Φ.
The cost of multiplying φ to produce ψi is at most Ni, since each entry of φ is multiplied into
exactly one entry of ψi. Thus, the total number of multiplication steps is at most (n + m)Ni ≤
(n + m)Nmax = O(mNmax).
To analyze the number of addition steps, we note that the
marginalization operation in line 4 touches each entry in ψi exactly once. Thus, the cost of
this operation is exactly Ni; we execute this operation once for each factor ψi, so that the
total number of additions is at most nNmax. Overall, the total amount of work required is
O(mNmax).
The source of the inevitable exponential blowup is the potentially exponential size of the
factors ψi. If each variable has no more than v values, and a factor ψi has a scope that contains
ki variables, then Ni ≤vki. Thus, we see that the computational cost of the VE algorithm is
dominated by the sizes of the intermediate factors generated, with an exponential growth in the
number of variables in a factor.
9.4.2
Graph-Theoretic Analysis
Although the size of the factors created during the algorithm is clearly the dominant quantity in
the complexity of the algorithm, it is not clear how it relates to the properties of our problem
instance. In our case, the only aspect of the problem instance that aﬀects the complexity of
the algorithm is the structure of the underlying graph that induced the set of factors on which
the algorithm was run. In this section, we reformulate our complexity analysis in terms of this
graph structure.
9.4.2.1
Factors and Undirected Graphs
We begin with the observation that the algorithm does not care whether the graph that generated
the factors is directed, undirected, or partly directed. The algorithm’s input is a set of factors Φ,
and the only relevant aspect to the computation is the scope of the factors. Thus, it is easiest
to view the algorithm as operating on an undirected graph H.
More precisely, we can deﬁne the notion of an undirected graph associated with a set of
factors:
Deﬁnition 9.4

9.4. Complexity and Graph Structure: Variable Elimination
307
Let Φ be a set of factors. We deﬁne
Scope[Φ] = ∪φ∈ΦScope[φ]
to be the set of all variables appearing in any of the factors in Φ.
We deﬁne HΦ to be the
undirected graph whose nodes correspond to the variables in Scope[Φ] and where we have an edge
Xi—Xj ∈HΦ if and only if there exists a factor φ ∈Φ such that Xi, Xj ∈Scope[φ].
In words, the undirected graph HΦ introduces a fully connected subgraph over the scope of
each factor φ ∈Φ, and hence is the minimal I-map for the distribution induced by Φ.
We can now show that:
Proposition 9.1
Let P be a distribution deﬁned by multiplying the factors in Φ and normalizing to deﬁne a
distribution. Letting X = Scope[Φ],
P(X) = 1
Z
Y
φ∈Φ
φ,
where Z = P
X
Q
φ∈Φ φ. Then HΦ is the minimal Markov network I-map for P, and the factors
Φ are a parameterization of this network that deﬁnes the distribution P.
The proof is left as an exercise (exercise 9.7).
Note that, for a set of factors Φ deﬁned by a Bayesian network G, in the case without evidence,
the undirected graph HΦ is precisely the moralized graph of G. In this case, the product of the
factors is a normalized distribution, so the partition function of the resulting Markov network is
simply 1. Figure 4.6a shows the initial graph for our Student example.
More interesting is the Markov network induced by a set of factors Φ[e] deﬁned by the
reduction of the factors in a Bayesian network to some context E = e. In this case, recall that
the variables in E are removed from the factors, so X = Scope[Φe] = X −E. Furthermore, as
we discussed, the unnormalized product of the factors is P(X, e), and the partition function
of the resulting Markov network is precisely P(e).
Figure 4.6b shows the initial graph for
our Student example with evidence G = g, and ﬁgure 4.6c shows the case with evidence
G = g, S = s.
9.4.2.2
Elimination as Graph Transformation
Now, consider the eﬀect of a variable elimination step on the set of factors maintained by the
algorithm and on the associated Markov network. When a variable X is eliminated, several
operations take place. First, we create a single factor ψ that contains X and all of the variables
Y with which it appears in factors. Then, we eliminate X from ψ, replacing it with a new
factor τ that contains all of the variables Y but does not contain X. Let ΦX be the resulting
set of factors.
How does the graph HΦX diﬀer from HΦ?
The step of constructing ψ generates edges
between all of the variables Y ∈Y . Some of them were present in HΦ, whereas others are
introduced due to the elimination step; edges that are introduced by an elimination step are
called ﬁll edges. The step of eliminating X from ψ to construct τ has the eﬀect of removing X
ﬁll edge
and all of its incident edges from the graph.

308
Chapter 9. Variable Elimination
Grade
Letter
Job
Happy
SAT
Intelligence
Difﬁculty
(a)
Grade
Letter
Job
Happy
SAT
Intelligence
(b)
Grade
Letter
Job
Happy
SAT
(c)
Figure 9.10
Variable elimination as graph transformation in the Student example, using the elimi-
nation order of table 9.1: (a) after eliminating C; (b) after eliminating D; (c) after eliminating I.
Consider again our Student network, in the case without evidence. As we said, ﬁgure 4.6a
shows the original Markov network. Figure 9.10a shows the result of eliminating the variable C.
Note that there are no ﬁll edges introduced in this step.
After an elimination step, the subsequent elimination steps use the new set of factors. In
other words, they can be seen as operations over the new graph. Figure 9.10b and c show the
graphs resulting from eliminating ﬁrst D and then I. Note that the step of eliminating I results
in a (new) ﬁll edge G—S, induced by the factor G, I, S.
The computational steps of the algorithm are reﬂected in this series of graphs. Every factor
that appears in one of the steps in the algorithm is reﬂected in the graph as a clique. In fact,
we can summarize the computational cost using a single graph structure.
9.4.2.3
The Induced Graph
We deﬁne an undirected graph that is the union of all of the graphs resulting from the diﬀerent
steps of the variable elimination algorithm.
Deﬁnition 9.5
Let Φ be a set of factors over X = {X1, . . . , Xn}, and ≺be an elimination ordering for some
subset X ⊆X. The induced graph IΦ,≺is an undirected graph over X, where Xi and Xj
induced graph
are connected by an edge if they both appear in some intermediate factor ψ generated by the VE
algorithm using ≺as an elimination ordering.
For a Bayesian network graph G, we use IG,≺to denote the induced graph for the factors Φ
corresponding to the CPDs in G; similarly, for a Markov network H, we use IH,≺to denote the
induced graph for the factors Φ corresponding to the potentials in H.
The induced graph IG,≺for our Student example is shown in ﬁgure 9.11a. We can see that
the ﬁll edge G—S, introduced in step (3) when we eliminated I, is the only ﬁll edge introduced.
As we discussed, each factor ψ used in the computation corresponds to a complete subgraph
of the graph IG,≺and is therefore a clique in the graph. The connection between cliques in
IG,≺and factors ψ is, in fact, much tighter:

9.4. Complexity and Graph Structure: Variable Elimination
309
Grade
Letter
Job
Happy
Coherence
SAT
G,J,S,L
G,H,J
G,I,S
G,I,D
C,D
G,J
G,S
G,I
D
Intelligence
Difﬁculty
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
(a)
(b)
(c)
Figure 9.11
Induced graph and clique tree for the Student example. (a) Induced graph for variable
elimination in the Student example, using the elimination order of table 9.1. (b) Cliques in the induced
graph: {C, D}, {D, I, G}, {G, I, S}, {G, J, S, L}, and {G, H, J}. (c) Clique tree for the induced graph.
Theorem 9.6
Let IΦ,≺be the induced graph for a set of factors Φ and some elimination ordering ≺. Then:
1. The scope of every factor generated during the variable elimination process is a clique in IΦ,≺.
2. Every maximal clique in IΦ,≺is the scope of some intermediate factor in the computation.
Proof We begin with the ﬁrst statement. Consider a factor ψ(Y1, . . . , Yk) generated during the
VE process. By the deﬁnition of the induced graph, there must be an edge between each Yi and
Yj. Hence Y1, . . . , Yk form a clique.
To prove the second statement, consider some maximal clique Y = {Y1, . . . , Yk}. Assume,
without loss of generality, that Y1 is the ﬁrst of the variables in Y in the ordering ≺, and is
therefore the ﬁrst among this set to be eliminated. Since Y is a clique, there is an edge from
Y1 to each other Yi. Note that, once Y1 is eliminated, it can appear in no more factors, so
there can be no new edges added to it. Hence, the edges involving Y1 were added prior to
this point in the computation. The existence of an edge between Y1 and Yi therefore implies
that, at this point, there is a factor containing both Y1 and Yi. When Y1 is eliminated, all these
factors must be multiplied. Therefore, the product step results in a factor ψ that contains all
of Y1, Y2, . . . , Yk. Note that this factor can contain no other variables; if it did, these variables
would also have an edge to all of Y1, . . . , Yk, so that Y1, . . . , Yk would not constitute a maximal
connected subgraph.

310
Chapter 9. Variable Elimination
Let us verify that the second property holds for our example. Figure 9.11b shows the maximal
cliques in IG,≺:
C1
=
{C, D}
C2
=
{D, I, G}
C3
=
{I, G, S}
C4
=
{G, J, L, S}
C5
=
{G, H, J}.
Both these properties hold for this set of cliques. For example, C3 corresponds to the factor ψ
generated in step (5).

Thus, there is a direct correspondence between the maximal factors generated by our
algorithm and maximal cliques in the induced graph. Importantly, the induced graph and
the size of the maximal cliques within it depend strongly on the elimination ordering.
Consider, for example, our other elimination ordering for the Student network. In this case, we
can verify that our induced graph has a maximal clique over G, I, D, L, J, H, a second over
S, I, D, L, J, H, and a third over C, D, J; indeed, the graph is missing only the edge between
S and G, and some edges involving C. In this case, the largest clique contains six variables, as
opposed to four in our original ordering. Therefore, the cost of computation here is substantially
more expensive.
Deﬁnition 9.6
We deﬁne the width of an induced graph to be the number of nodes in the largest clique in the
graph minus 1. We deﬁne the induced width wK,≺of an ordering ≺relative to a graph K (directed
induced width
or undirected) to be the width of the graph IK,≺induced by applying VE to K using the ordering ≺.
We deﬁne the tree-width of a graph K to be its minimal induced width w∗
K = min≺w(IK,≺).
tree-width
The minimal induced width of the graph K provides us a bound on the best performance we
can hope for by applying VE to a probabilistic model that factorizes over K.
9.4.3
Finding Elimination Orderings ⋆
How can we compute the minimal induced width of the graph, and the elimination ordering
achieving that width? Unfortunately, there is no easy way to answer this question.
Theorem 9.7
The following decision problem is NP-complete:
Given a graph H and some bound K, determine whether there exists an elimination ordering
achieving an induced width ≤K.
It follows directly that ﬁnding the optimal elimination ordering is also NP-hard. Thus, we
cannot easily tell by looking at a graph how computationally expensive inference on it will be.
Note that this NP-completeness result is distinct from the NP-hardness of inference itself.
That is, even if some oracle gives us the best elimination ordering, the induced width might still
be large, and the inference task using that ordering can still require exponential time.
However, as usual, NP-hardness is not the end of the story. There are several techniques
that one can use to ﬁnd good elimination orderings. The ﬁrst uses an important graph-theoretic
property of induced graphs, and the second uses heuristic ideas.

9.4. Complexity and Graph Structure: Variable Elimination
311
9.4.3.1
Chordal Graphs
Recall from deﬁnition 2.24 that an undirected graph is chordal if it contains no cycle of length
chordal graph
greater than three that has no “shortcut,” that is, every minimal loop in the graph is of length
three. As we now show, somewhat surprisingly, the class of induced graphs is equivalent to the
class of chordal graphs. We then show that this property can be used to provide one heuristic
for constructing an elimination ordering.
Theorem 9.8
Every induced graph is chordal.
Proof Assume by contradiction that we have such a cycle X1—X2— . . . —Xk—X1 for k > 3,
and assume without loss of generality that X1 is the ﬁrst variable to be eliminated. As in the
proof of theorem 9.6, no edge incident on X1 is added after X1 is eliminated; hence, both edges
X1—X2 and X1—Xk must exist at this point. Therefore, the edge X2—Xk will be added at
the same time, contradicting our assumption.
Indeed, we can verify that the graph of ﬁgure 9.11a is chordal. For example, the loop H →
G →L →J →H is cut by the chord G →J.
The converse of this theorem states that any chordal graph H is an induced graph for some
ordering. One way of showing that is to show that there is an elimination ordering for H for
which H itself is the induced graph.
Theorem 9.9
Any chordal graph H admits an elimination ordering that does not introduce any ﬁll edges into
the graph.
Proof We prove this result by induction on the number of nodes in the tree. Let H be a chordal
graph with n nodes. As we showed in theorem 4.12, there is a clique tree T for H. Let Ck be a
clique in the tree that is a leaf, that is, it has only a single other clique as a neighbor. Let Xi be
some variable that is in Ck but not in its neighbor. Let H′ be the graph obtained by eliminating
Xi. Because Xi belongs only to the clique Ck, its neighbors are precisely Ck −{Xi}. Because
all of them are also in Ck, they are connected to each other. Hence, eliminating Xi introduces
no ﬁll edges. Because H′ is also chordal, we can now apply the inductive hypothesis, proving
the result.

312
Chapter 9. Variable Elimination
Algorithm 9.3 Maximum cardinality search for constructing an elimination ordering
Procedure Max-Cardinality (
H
// An undirected graph over X
)
1
Initialize all nodes in X as unmarked
2
for k = |X| . . . 1
3
X ←unmarked variable in X with largest number of marked neighbors
4
π(X) ←k
5
Mark X
6
return π
Example 9.2
We can illustrate this construction on the graph of ﬁgure 9.11a. The maximal cliques in the induced
graph are shown in b, and a clique tree for this graph is shown in c. One can easily verify that
each sepset separates the two sides of the tree; for example, the sepset {G, S} separates C, I, D (on
the left) from L, J, H (on the right). The elimination ordering C, D, I, H, G, S, L, J, an extension
of the elimination in table 9.1 that generated this induced graph, is one ordering that might arise
from the construction of theorem 9.9. For example, it ﬁrst eliminates C, D, which are both in a
leaf clique; it then eliminates I, which is in a clique that is now a leaf, following the elimination
of C, D. Indeed, it is not hard to see that this ordering introduces no ﬁll edges. By contrast, the
ordering in table 9.2 is not consistent with this construction, since it begins by eliminating the
variables G, I, S, none of which are in a leaf clique. Indeed, this elimination ordering introduces
additional ﬁll edges, for example, the edge H →D.
An alternative method for constructing an elimination ordering that introduces no ﬁll edges
in a chordal graph is the Max-Cardinality algorithm, shown in algorithm 9.3. This method does
maximum
cardinality
not use the clique tree as its starting point, but rather operates directly on the graph. When
applied to a chordal graph, it constructs an elimination ordering that eliminates cliques one at
a time, starting from the leaves of the clique tree; and it does so without ever considering the
clique tree structure explicitly.
Example 9.3
Consider applying Max-Cardinality to the chordal graph of ﬁgure 9.11. Assume that the ﬁrst node
selected is S. The second node selected must be one of S’s neighbors, say J. The node that has the
largest number of marked neighbors are now G and L, which are chosen subsequently. Now, the
unmarked nodes that have the largest number of marked neighbors (two) are H and I. Assume
we select I. Then the next nodes selected are D and H, in any order. The last node to be selected
is C. One possible resulting ordering in which nodes are marked is thus S, J, G, L, I, H, D, C.
Importantly, the actual elimination ordering proceeds in reverse. Thus, we ﬁrst eliminate C, D, then
H, and so on. We can now see that this ordering always eliminates a variable from a clique that is
a leaf clique at the time. For example, we ﬁrst eliminate C, D from a leaf clique, then H, then G
from the clique {G, I, D}, which is now (following the elimination of C, D) a leaf.
As in this example, Max-Cardinality always produces an elimination ordering that is consistent
with the construction of theorem 9.9. As a consequence, it follows that Max-Cardinality, when
applied to a chordal graph, introduces no ﬁll edges.

9.4. Complexity and Graph Structure: Variable Elimination
313
Theorem 9.10
Let H be a chordal graph. Let π be the ranking obtained by running Max-Cardinality on H. Then
Sum-Product-VE (algorithm 9.1), eliminating variables in order of increasing π, does not introduce
any ﬁll edges.
The proof is left as an exercise (exercise 9.8).
The maximum cardinality search algorithm can also be used to construct an elimination
ordering for a nonchordal graph. However, it turns out that the orderings produced by this
method are generally not as good as those produced by various other algorithms, such as those
described in what follows.
To summarize, we have shown that, if we construct a chordal graph that contains the graph
HΦ corresponding to our set of factors Φ, we can use it as the basis for inference using Φ. The
process of turning a graph H into a chordal graph is also called triangulation, since it ensures
triangulation
that the largest unbroken cycle in the graph is a triangle. Thus, we can reformulate our goal of
ﬁnding an elimination ordering as that of triangulating a graph H so that the largest clique in
the resulting graph is as small as possible. Of course, this insight only reformulates the problem:
Inevitably, the problem of ﬁnding such a minimal triangulation is also NP-hard. Nevertheless,
there are several graph-theoretic algorithms that address this precise problem and oﬀer diﬀerent
levels of performance guarantee; we discuss this task further in section 10.4.2.
Box 9.B — Concept: Polytrees. One particularly simple class of chordal graphs is the class of
Bayesian networks whose graph G is a polytree. Recall from deﬁnition 2.22 that a polytree is a
polytree
graph where there is at most one trail between every pair of nodes.
Polytrees received a lot of attention in the early days of Bayesian networks, because the ﬁrst
widely known inference algorithm for any type of Bayesian network was Pearl’s message passing
algorithm for polytrees. This algorithm, a special case of the message passing algorithms described
in subsequent chapters of this book, is particularly compelling in the case of polytree networks, since
it consists of nodes passing messages directly to other nodes along edges in the graph. Moreover,
the cost of this computation is linear in the size of the network (where the size of the network is
measured as the total sizes of the CPDs in the network, not the number of nodes; see exercise 9.9).
From the perspective of the results presented in this section, this simplicity is not surprising: In
a polytree, any maximal clique is a family of some variable in the network, and the clique tree
structure roughly follows the network topology. (We simply throw out families that do not correspond
to a maximal clique, because they are subsumed by another clique.)
Somewhat ironically, the compelling nature of the polytree algorithm gave rise to a long-standing
misconception that there was a sharp tractability boundary between polytrees and other networks,
in that inference was tractable only in polytrees and NP-hard in other networks. As we discuss in
this chapter, this is not the case; rather, there is a continuum of complexity deﬁned by the size of
the largest clique in the induced graph.
9.4.3.2
Minimum Fill/Size/Weight Search
An alternative approach for ﬁnding elimination orderings is based on a very straightforward
intuition. Our goal is to construct an ordering that induces a “small” graph. While we cannot

314
Chapter 9. Variable Elimination
Algorithm 9.4 Greedy search for constructing an elimination ordering
Procedure Greedy-Ordering (
H
// An undirected graph over X
,
s
// An evaluation metric
)
1
Initialize all nodes in X as unmarked
2
for k = 1 . . . |X|
3
Select an unmarked variable X ∈X that minimizes s(H, X)
4
π(X) ←k
5
Introduce edges in H between all neighbors of X
6
Mark X
7
return π
ﬁnd an ordering that achieves the global minimum, we can eliminate variables one at a time in
a greedy way, so that each step tends to lead to a small blowup in size.
The general algorithm is shown in algorithm 9.4. At each point, the algorithm evaluates each
of the remaining variables in the network based on its heuristic cost function. Some common
cost criteria that have been used for evaluating variables are:
•
Min-neighbors: The cost of a vertex is the number of neighbors it has in the current graph.
•
Min-weight:The cost of a vertex is the product of weights — domain cardinality — of its
neighbors.
•
Min-ﬁll: - The cost of a vertex is the number of edges that need to be added to the graph
due to its elimination.
•
Weighted-min-ﬁll: The cost of a vertex is the sum of weights of the edges that need to
be added to the graph due to its elimination, where a weight of an edge is the product of
weights of its constituent vertices.
Intuitively, min-neighbors and min-weight count the size or weight of the largest clique in H
after eliminating X. Min-ﬁll and weighted-min-ﬁll count the number or weight of edges that
would be introduced into H by eliminating X. It can be shown (exercise 9.10) that none of these
criteria is universally better than the others.
This type of greedy search can be done either deterministically (as shown in algorithm 9.4), or
stochastically. In the stochastic variant, at each step we select some number of low-scoring
vertices, and then choose among them using their score (where lower-scoring vertices are
selected with higher probability). In the stochastic variants, we run multiple iterations of the
algorithm, and then select the ordering that leads to the most eﬃcient elimination — the one
where the sum of the sizes of the factors produced is smallest.
Empirical results show that these heuristic algorithms perform surprisingly well in practice.
Generally, Min-Fill and Weighted-Min-Fill tend to work better on more problems. Not surpris-
ingly, Weighted-Min-Fill usually has the most signiﬁcant gains when there is some signiﬁcant
variability in the sizes of the domains of the variables in the network. Box 9.C presents a case
study comparing these algorithms on a suite of standard benchmark networks.

9.5. Conditioning ⋆
315
Box 9.C — Case Study: Variable Elimination Orderings. Fishelson and Geiger (2003) performed
a comprehensive case study of diﬀerent heuristics for computing an elimination ordering, testing
them on eight standard Bayesian network benchmarks, ranging from 24 nodes to more than 1,000.
For each network, they compared both to the best elimination ordering known previously, obtained
by an expensive process of simulated annealing search, and to the network obtained by a state-
of-the-art Bayesian network package. They compared to stochastic versions of the four heuristics
described in the text, running each of them for 1 minute or 10 minutes, and selecting the best
network obtained in the diﬀerent random runs. Maximum cardinality search was not used, since
it is known to perform quite poorly in practice.
The results, shown in ﬁgure 9.C.1, suggest several conclusions. First, we see that running the
stochastic algorithms for longer improves the quality of the answer obtained, although usually not
by a huge amount. We also see that diﬀerent heuristics can result in orderings whose computational
cost can vary in almost an order of magnitude. Overall, Min-Fill and Weighted-Min-Fill achieve
the best performance, but they are not universally better. The best answer obtained by the greedy
algorithms is generally very good; it is often signiﬁcantly better than the answer obtained by a
deterministic state-of-the-art scheme, and it is usually quite close to the best-known ordering, even
when the latter is obtained using much more expensive techniques. Because the computational cost
of the heuristic ordering-selection algorithms is usually negligible relative to the running time of
the inference itself, we conclude that for large networks it is worthwhile to run several heuristic
algorithms in order to ﬁnd the best ordering obtained by any of them.
9.5
Conditioning ⋆
An alternative approach to inference is based on the idea of conditioning. The conditioning
conditioning
algorithm is based on the fact (illustrated in section 9.3.2), that observing the value of certain
variables can simplify the variable elimination process. When a variable is not observed, we can
use a case analysis to enumerate its possible values, perform the simpliﬁed VE computation, and
then aggregate the results for the diﬀerent values. As we will discuss, in terms of number of

operations, the conditioning algorithm oﬀers no beneﬁt over the variable elimination al-
gorithm. However, it oﬀers a continuum of time-space trade-oﬀs, which can be extremely
important in cases where the factors created by variable elimination are too big to ﬁt in
main memory.
9.5.1
The Conditioning Algorithm
The conditioning algorithm is easiest to explain in the context of a Markov network. Let Φ be a
set of factors over X and PΦ be the associated distribution. We assume that any observations
were already assimilated into Φ, so that our goal is to compute PΦ(Y ) for some set of query
variables Y . For example, if we want to do inference in the Student network given the evidence
G = g, we would reduce the factors reduced to this context, giving rise to the network structure
shown in ﬁgure 4.6b.

316
Chapter 9. Variable Elimination
250
200
150
100
50
0
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Munin1
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Munin2
3.4
3.35
3.3
3.25
3.2
3.15
3.1
3.05
3
2.95
2.9
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Munin3
30
25
20
15
10
5
0
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Munin4
9
8
7
6
5
4
3
2
1
0
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Water
80
70
60
50
40
30
20
10
0
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Diabetes
90
80
70
60
50
40
30
20
10
0
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Link
30
25
20
15
10
5
0
Best
known
MIN-
N
(1)
MIN-
W
(1)
MIN-
Fill
(1)
WMIN-
Fill
(1)
MIN-
N
(10)
MIN-
W
(10)
MIN-
Fill
(10)
WMIN-
Fill
(10)
HUGIN
Barley
Figure 9.C.1 — Comparison of algorithms for selecting variable elimination ordering.
Computational cost of variable elimination inference in a range of benchmark networks, obtained
by various algorithms for selecting an elimination ordering. The cost is measured as the size of the factors
generated during the process of variable elimination. For each network, we see the cost of the best-known
ordering, the ordering obtained by Hugin (a state-of-the-art Bayesian network package), and the ordering
obtained by stochastic greedy search using four diﬀerent search heuristics — Min-Neighbors, Min-Weight,
Min-Fill, and Weighted-Min-Fill — run for 1 minute and for 10 minutes.

9.5. Conditioning ⋆
317
Algorithm 9.5 Conditioning algorithm
Procedure Sum-Product-Conditioning (
Φ,
// Set of factors, possibly reduced by evidence
Y ,
// Set of query variables
U
// Set of variables on which to condition
)
1
for each u ∈Val(U)
2
Φu ←{φ[U = u] : φ ∈Φ}
3
Construct HΦu
4
(αu, φu(Y )) ←Cond-Prob-VE(HΦu, Y , ∅)
5
φ∗(Y ) ←
P
u φu(Y )
P
u αu
6
Return φ∗(Y )
The conditioning algorithm is based on the following simple derivation. Let U ⊆X be any
set of variables. Then we have that:
˜PΦ(Y ) =
X
u∈Val(U)
˜PΦ(Y , u).
(9.11)
The key observation is that each term ˜PΦ(Y , u) can be computed by marginalizing out the
variables in X −U −Y in the unnormalized measure ˜PΦ[u] obtained by reducing ˜PΦ to the
context u. As we have already discussed, the reduced measure is simply the measure deﬁned
by reducing each of the factors to the context u. The reduction process generally produces a
simpler structure, with a reduced inference cost.
We can use this formula to compute PΦ(Y ) as follows: We construct a network HΦ[u] for
each assignment u; these networks have identical structures, but diﬀerent parameters. We run
sum-product inference in each of them, to obtain a factor over the desired query set Y . We
then simply add up these factors to obtain ˜PΦ(Y ). We can also derive PΦ(Y ) by renormalizing
this factor to obtain a distribution. As usual, the normalizing constant is the partition function
for PΦ. However, applying equation (9.11) to the case of Y = ∅, we conclude that
ZΦ =
X
u
ZΦ[u].
Thus, we can derive the overall partition function from the partition functions for the diﬀerent
subnetworks HΦ[u]. The ﬁnal algorithm is shown in algorithm 9.5. (We note that Cond-Prob-VE
was called without evidence, since we assumed for simplicity that our factors Φ have already
been reduced with the evidence.)

318
Chapter 9. Variable Elimination
Example 9.4
Assume that we want to compute P(J) in the Student network with evidence G = g1, so that our
initial graph would be the one shown in ﬁgure 4.6b. We can now perform inference by enumerating
all of the assignments s to the variable S. For each such assignment, we run inference on a graph
structured as in ﬁgure 4.6c, with the factors reduced to the assignment g1, s. In each such network
we compute a factor over J, and add them all up. Note that the reduced network contains two
disconnected components, and so we might be tempted to run inference only on the component
that contains J. However, that procedure would not produce a correct answer: The value we get by
summing out the variables in the second component multiplies our ﬁnal factor. Although this is a
constant multiple for each value of s, these values are generally diﬀerent for the diﬀerent values of S.
Because the factors are added before the ﬁnal renormalization, this constant inﬂuences the weight
of one factor in the summation relative to the other. Thus, if we ignore this constant component, the
answers we get from the s1 computation and the s0 computation would be weighted incorrectly.
Historically, owing to the initial popularity of the polytree algorithm, the conditioning ap-
proach was mostly used in the case where the transformed network is a polytree. In this case,
the algorithm is called cutset conditioning.
cutset
conditioning
9.5.2
Conditioning and Variable Elimination
At ﬁrst glance, it might appear as if this process saves us considerable computational cost over
the variable elimination algorithm.
After all, we have reduced the computation to one that
performs variable elimination in a much simpler network. The cost arises, of course, from the
fact that, when we condition on U, we need to perform variable elimination on the conditioned
network multiple times, once for each assignment u ∈Val(U). The cost of this computation is
O(|Val(U)|), which is exponential in the number of variables in U. Thus, we have not avoided
the exponential blowup associated with the probabilistic inference process.
In this section,
we provide a formal complexity analysis of the conditioning algorithm, and compare it to the
complexity of elimination. This analysis also reveals various interesting improvements to the
basic conditioning algorithm, which can dramatically improve its performance in certain cases.
To understand the operation of the conditioning algorithm, we return to the basic description
of the probabilistic inference task. Consider our query J in the Extended Student network. We
know that:
p(J) =
X
C
X
D
X
I
X
S
X
G
X
L
X
H
P(C, D, I, S, G, L, H, J).
Reordering this expression slightly, we have that:
p(J) =
X
g
"X
C
X
D
X
I
X
S
X
L
X
H
P(C, D, I, S, g, L, H, J)
#
.
The expression inside the parentheses is precisely the result of computing the probability of J
in the network HΦG=g, where Φ is the set of CPD factors in B.
In other words, the conditioning algorithm is simply executing parts of the basic summation
deﬁning the inference task by case analysis, enumerating the possible values of the conditioning

9.5. Conditioning ⋆
319
Step
Variable
Factors
Variables
New
eliminated
used
involved
factor
1
C
φ+
C(C, G), φ+
D(D, C, G)
C, D, G
τ1(D, G)
2
D
φ+
G(G, I, D), τ1(D, G)
G, I, D
τ2(G, I)
3
I
φ+
I (I, G), φ+
S (S, I, G), τ2(G, I)
G, S, I
τ3(G, S)
4
H
φ+
H(H, G, J)
H, G, J
τ4(G, J)
5
S
τ3(G, S), φ+
J (J, L, S, G)
J, L, S, G
τ5(J, L, G)
6
L
τ5(J, L, G), φ+
L(L, G)
J, L
τ6(J)
7
—
τ6(J), τ4(G, J)
G, J
τ7(G, J)
Table 9.4
Example of relationship between variable elimination and conditioning. A run of variable
elimination for the query P(J) corresponding to conditioning on G.
variables. By contrast, variable elimination performs the same summation from the inside out,
using dynamic programming to reuse computation.
Indeed, if we simply did conditioning on all of the variables, the result would be an explicit
summation of the entire joint distribution. In conditioning, however, we perform the condi-
tioning step only on some of the variables, and use standard variable elimination — dynamic
programming — to perform the rest of the summation, avoiding exponential blowup (at least
over that part).
In general, it follows that both algorithms are performing the same set of basic operations
(sums and products). However, where the variable elimination algorithm uses the caching of
dynamic programming to save redundant computation throughout the summation, conditioning
uses a full enumeration of cases for some of the variables, and dynamic programming only at
the end.
From this argument, it follows that conditioning always performs no fewer steps than variable
elimination.
To understand why, consider the network of example 9.4 and assume that we
are trying to compute P(J). The conditioned network HΦG=g has a set of factors most of
which are identical to those in the original network. The exceptions are the reduced factors:
φL[G = g](L) and φH[G = g](H, J). For each of the three values g of G, we are performing
variable elimination over these factors, eliminating all variables except for G and J.
We can imagine “lumping” these three computations into one, by augmenting the scope of
each factor with the variable G. More precisely, we deﬁne a set of augmented factors φ+ as
follows: The scope of the factor φG already contains G, so φ+
G(G, D, I) = φG(G, D, I). For
the factor φ+
L, we simply combine the three factors φL,g(L), so that φ+
L(L, g) = φL[G = g](L)
for all g.
Not surprisingly, the resulting factor φ+
L(L, G) is simply our original CPD factor
φL(L, G). We deﬁne φ+
H in the same way. The remaining factors are unrelated to G. For each
other variable X over scope Y , we simply deﬁne φ+
X(Y , G) = φX(Y ); that is, the value of the
factor does not depend on the value of G.
We can easily verify that, if we run variable elimination over the set of factors F+
X for
X ∈{C, D, I, G, S, L, J, H}, eliminating all variables except for J and G, we are perform-
ing precisely the same computation as the three iterations of variable elimination for the three
diﬀerent conditioned networks HΦG=g: Factor entries involving diﬀerent values g of G never in-

320
Chapter 9. Variable Elimination
Step
Variable
Factors
Variables
New
eliminated
used
involved
factor
1
C
φC(C), φD(D, C)
C, D
τ1(D)
2
D
φG(G, I, D), τ1(D)
G, I, D
τ2(G, I)
3
I
φI(I), φS(S, I), τ2(G, I)
G, S, I
τ3(G, S)
4
H
φH(H, G, J)
H, G, J
τ4(G, J)
5
S
τ3(G, S), φJ(J, L, S)
J, L, S, G
τ5(J, L, G)
6
L
τ5(J, L, G), φL(L, G)
J, L
τ6(J)
7
G
τ6(J), τ4(G, J)
G, J
τ7(J)
Table 9.5
A run of variable elimination for the query P(J) with G eliminated last
teract, and the computation performed for the entries where G = g is precisely the computation
performed in the network HΦG=g.
Speciﬁcally, assume we are using the ordering C, D, I, H, S, L to perform the elimination
within each conditioned network HΦG=g. The steps of the computation are shown in table 9.4.
Step (7) corresponds to the product of all of the remaining factors, which is the last step in
variable elimination. The ﬁnal step in the conditioning algorithm, where we add together the
results of the three computations, is precisely the same as eliminating G from the resulting
factor τ7(G, J).
It is instructive to compare this execution to the one obtained by running variable elimination
on the original set of factors, with the elimination ordering C, D, I, H, S, L, G; that is, we follow
the ordering used within the conditioned networks for the variables other than G, J, and then
eliminate G at the very end. In this process, shown in table 9.5, some of the factors involve
G, but others do not. In particular, step (1) in the elimination algorithm involves only C, D,
whereas in the conditioning algorithm, we are performing precisely the same computation over
C, D three times: once for each value g of G.
In general, we can show:
Theorem 9.11
Let Φ be a set of factors, and Y be a query.
Let U be a set of conditioning variables, and
Z = X −Y −U. Let ≺be the elimination ordering over Z used by the variable elimination
algorithm over the network HΦu in the conditioning algorithm. Let ≺+ be an ordering that is
consistent with ≺over the variables in Z, and where, for each variable U ∈U, we have that
Z ≺+ U. Then the number of operations performed by the conditioning is no less than the number
of operations performed by variable elimination with the ordering ≺+.
We omit the proof of this theorem, which follows precisely the lines of our example.
Thus, conditioning always requires no fewer computations than variable elimination with
some particular ordering (which may or may not be a good one). In our example, the wasted
computation from conditioning is negligible. In other cases, however, as we will discuss, we can
end up with a large amount of redundant computation. In fact, in some cases, conditioning can
be signiﬁcantly worse:
Example 9.5
Consider the network shown in ﬁgure 9.12a, and assume we choose to condition on Ak in order

9.5. Conditioning ⋆
321
(a)
(b)
Ak
Bk
Ck
D
A1
B1
C1
A2
A1
A2
Ak
B
C
D
Figure 9.12
Networks where conditioning performs unnecessary computation
to cut the single loop in the network. In this case, we would perform the entire elimination of the
chain A1 →. . . →Ak−1 multiple times — once for every value of Ak.
Example 9.6
Consider the network shown in ﬁgure 9.12b and assume that we wish to use cutset conditioning,
where we cut every loop in the network. The most eﬃcient way of doing so is to condition on every
other Ai variable, for example, A2, A4, . . . , Ak (assuming for simplicity that k is even). The cost
of the conditioning algorithm in this case is exponential in k, whereas the induced width of the
network is 2, and the cost of variable elimination is linear in k.
Given this discussion, one might wonder why anyone bothers with the conditioning algorithm.
There are two main reasons. First, variable elimination gains its computational savings from
caching factors computed as intermediate results. In complex networks, these factors can grow
very large. In cases where memory is scarce, it might not be possible to keep these factors
in memory, and the variable elimination computation becomes infeasible (or very costly due
to constant thrashing to disk). On the other hand, conditioning does not require signiﬁcant
amounts of memory: We run inference separately for each assignment u to U and simply
accumulate the results. Overall, the computation requires space that is linear only in the size
of the network. Thus, we can view the trade-oﬀof conditioning versus variable elimination as
a time-space trade-oﬀ. Conditioning saves space by not storing intermediate results in memory,
but then it may cost additional time by having to repeat the computation to generate them.
The second reason for using conditioning is that it forms the basis for a useful approximate
inference algorithm. In particular, in certain cases, we can get a reasonable approximate solution

322
Chapter 9. Variable Elimination
by enumerating only some of the possible assignment u ∈Val(U). We return to this approach
in section 12.5
9.5.3
Graph-Theoretic Analysis
As in the case of variable elimination, it helps to reformulate the complexity analysis of the
conditioning algorithm in graph-theoretic terms. Assume that we choose to condition on a set
U, and perform variable elimination on the remaining variables. We can view each of these
steps in terms of its eﬀect on the graph structure.
Let us begin with the step of conditioning the network on some variable U. Once again, it
is easiest to view this process in terms of its eﬀect on an undirected graph. As we discussed,
this step eﬀectively introduces U into every factor parameterizing the current graph. In graph-
theoretic terms, we have introduced U into every clique in the graph, or, more simply, introduced
an edge between U and every other node currently in the graph.
When we ﬁnish the conditioning process, we perform elimination on the remaining variables.
We have already analyzed the eﬀect on the graph of eliminating a variable X: When we
eliminate X, we add edges between all of the current neighbors of X in the graph. We then
remove X from the graph.
We can now deﬁne an induced graph for the conditioning algorithm. Unlike the graph for
variable elimination, this graph has two types of ﬁll edges: those induced by conditioning steps,
and those induced by the elimination steps for the remaining variables.
Deﬁnition 9.7
Let Φ be a set of factors over X = {X1, . . . , Xn}, U ⊂X be a set of conditioning variables, and
≺be an elimination ordering for some subset X ⊆X −U. The induced graph IΦ,≺,U is an
conditioning
induced graph
undirected graph over X with the following edges:
• a conditioning edge between every variable U ∈U and every other variable X ∈X;
• a factor edge between every pair of variables Xi, Xj ∈X that both appear in some interme-
diate factor ψ generated by the VE algorithm using ≺as an elimination ordering.
Example 9.7
Consider the Student example of ﬁgure 9.8, where our query is P(J). Assume that (for some
reason) we condition on the variable L and perform elimination on the remaining variables using
the ordering C, D, I, H, G, S. The graph induced by this conditioning set and this elimination
ordering is shown in ﬁgure 9.13, with the conditioning edges shown as dashed lines and the factor
edges shown, as usual, by complete lines. The step of conditioning on L causes the introduction of
the edges between L and all the other variables. The set of factors we have after the conditioning
step immediately leads to the introduction of all the factor edges except for the edge G—S; this
latter edge results from the elimination of I.
We can now use this graph to analyze the complexity of the conditioning algorithm.
Theorem 9.12
Consider an application of the conditioning algorithm to a set of factors Φ, where U ⊂X is the
set of conditioning variables, and ≺is the elimination ordering used for the eliminated variables
X ⊆X −U. Then the running time of the algorithm is O(n · vm), where v is a bound on
the domain size of any variable, and m is the size of the largest clique in the graph, using both
conditioning and factor edges.

9.5. Conditioning ⋆
323
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
Figure 9.13
Induced graph for the Student example using both conditioning and elimination: we
condition on L and eliminate the remaining variables using the ordering C, D, I, H, G, S.
The proof is left as an exercise (exercise 9.12).
This theorem provides another perspective on the trade-oﬀbetween conditioning and elimi-
nation in terms of their time complexity. Consider, as we did earlier, an algorithm that simply
defers the elimination of the conditioning variables U until the end. Consider the eﬀect on the
graph of the earlier steps of the elimination algorithm (those preceding the elimination of U).
As variables are eliminated, certain edges might be added between the variables in U and other
variables (in particular, we add an edge between X and U ∈U whenever they are both neigh-
bors of some eliminated variable Y ). However, conditioning adds edges between the variables
U and all other variables X. Thus, conditioning always results in a graph that contains at least
as many edges as the induced graph from elimination using this ordering.
However, we can also use the same graph to precisely estimate the time-space trade-oﬀ
provided by the conditioning algorithm.
Theorem 9.13
Consider an application of the conditioning algorithm to a set of factors Φ, where U ⊂X is the
set of conditioning variables, and ≺is the elimination ordering used for the eliminated variables
X ⊆X −U. The space complexity of the algorithm is O(n · vmf ), where v is a bound on the
domain size of any variable, and mf is the size of the largest clique in the graph using only factor
edges.
The proof is left as an exercise (exercise 9.13).
By comparison, the asymptotic space complexity of variable elimination is the same as its
time complexity: exponential in the size of the largest clique containing both types of edges.
Thus, we see precisely that conditioning allows us to perform the computation using less space,
at the cost (usually) of additional running time.
9.5.4
Improved Conditioning
As we discussed, in terms of the total operations performed, conditioning cannot be better than
variable elimination. As we now show, conditioning, naively applied, can be signiﬁcantly worse.

324
Chapter 9. Variable Elimination
However, the insights gained from these examples can be used to improve the conditioning
algorithm, reducing its cost signiﬁcantly in many cases.
9.5.4.1
Alternating Conditioning and Elimination
As we discussed, the main problem associated with conditioning is the fact that all computations
are repeated for all values of the conditioning variables, even in cases where the diﬀerent
computations are, in fact, identical. This phenomenon arose in the network of example 9.5.
It seems clear, in this example, that we would prefer to eliminate the chain A1 →. . . →Ak−1
once and for all, before conditioning on Ak. Having eliminated the chain, we would then end
up with a much simpler network, involving factors only over Ak, B, C, and D, to which we can
then apply conditioning.
The perspective described in section 9.5.3 provides the foundation for implementing this idea.
As we discussed, variable elimination works from the inside out, summing out variables in the
innermost summation ﬁrst and caching the results. On the other hand, conditioning works from
the outside in, performing the entire internal summation (using elimination) for each value of
the conditioning variables, and only then summing the results. However, there is nothing that
forces us to split our computation on the outermost summations before considering the inner
ones. Speciﬁcally, we can eliminate one or more variables on the inside of the summation before
conditioning on any variable on the outside.
Example 9.8
Consider again the network of ﬁgure 9.12a, and assume that our goal is to compute P(D). We
might formulate the expression as:
X
Ak
X
B
X
C
X
A1
. . .
X
Ak−1
P(A1, . . . , Ak, B, C, D).
We can ﬁrst perform the internal summations on Ak−1, . . . , A1, resulting in a set of factors over
the scope Ak, B, C, D. We can now condition this network (that is, the Markov network induced
by the resulting set of factors) on Ak, resulting in a set of simpliﬁed networks over B, C, D (one for
each value of Ak). In each such network, we use variable elimination on B and C to compute a
factor over D, and aggregate the factors from the diﬀerent networks, as in standard conditioning.
In this example, we ﬁrst perform some elimination, then condition, and then elimination on
the remaining network. Clearly, we can generalize this idea to deﬁne an algorithm that alternates
the operations of elimination and conditioning arbitrarily. (See exercise 9.14.)
9.5.4.2
Network Decomposition
A second class of examples where we can signiﬁcantly improve the performance of condition-
ing arises in networks where conditioning on some subset of variables splits the graph into
independent pieces.
Example 9.9
Consider the network of example 9.6, and assume that k = 16, and that we begin by conditioning
on A2. After this step, the network is decomposed into two independent pieces. The standard
conditioning algorithm would continue by conditioning further, say on A3.
However, there is
really no need to condition the top part of the network — the one associated with the variables

9.6. Inference with Structured CPDs ⋆
325
A1, B1, C1 on the variable A3: none of the factors mention A3, and we would be repeating exactly
the same computation for each of its values.
Clearly, having partitioned the network into two completely independent pieces, we can
now perform the computation on each of them separately, and then combine the results. In
particular, the conditioning variables used on one part would not be used at all to condition
the other. More precisely, we can deﬁne an algorithm that checks, after each conditioning step,
whether the resulting set of factors has been disconnected or not. If it has, it simply partitions
them into two or more disjoint sets and calls the algorithm recursively on each subset.
9.6
Inference with Structured CPDs ⋆
We have seen that BN inference exploits the network structure, in particular the conditional
independence and the locality of inﬂuence. But when we discussed representation, we also
allowed for the representation of ﬁner-grained structure within the CPDs. It turns out that a
carefully designed inference algorithm can also exploit certain types of local CPD structure.
We focus on two types of structure where this issue has been particularly well studied —
independence of causal inﬂuence, and asymmetric dependencies — using each of them to
illustrate a diﬀerent type of method for exploiting local structure in variable elimination. We
defer the discussion of inference in networks involving continuous variables to chapter 14.
9.6.1
Independence of Causal Inﬂuence
The earliest and simplest instance of exploiting local structure was for CPDs that exhibit inde-
pendence of causal inﬂuence, such as noisy-or.
9.6.1.1
Noisy-Or Decompositions
Consider a simple network consisting of a binary variable Y and its four binary parents
X1, X2, X3, X4, where the CPD of Y is a noisy-or. Our goal is to compute the probability
of Y . The operations required to execute this process, assuming we use an optimal ordering, is:
•
4 multiplications for P(X1) · P(X2)
•
8 multiplications for P(X1, X2) · P(X3)
•
16 multiplications for P(X1, X2, X3) · P(X4)
•
32 multiplications for P(X1, X2, X3, X4) · P(Y | X1, X2, X3, X4)
The total is 60 multiplications, plus another 30 additions to sum out X1, . . . , X4, in order to
reduce the resulting factor P(X1, X2, X3, X4, Y ), of size 32, into the factor P(Y ) of size 2.
However, we can exploit the structure of the CPD to substantially reduce the amount of
computation. As we discussed in section 5.4.1, a noisy-or variable can be decomposed into a de-
terministic OR of independent noise variables, resulting in the subnetwork shown in ﬁgure 9.14a.
This transformation, by itself, is not very helpful. The factor P(Y | Z1, Z2, Z3, Z4) is still of
size 32 if we represent it as a full factor, so we achieve no gains.
The key idea is that the deterministic OR variable can be decomposed into various cascades
of deterministic OR variables, each with a very small indegree. Figure 9.14b shows a simple

326
Chapter 9. Variable Elimination
O1
O2
Y
X1
X2
X3
X4
Z1
Z2
Z3
Z4
O1
O2
Y
Y
X1
X2
X3
X4
Y
(a)
(b)
(c)
(d)
X1
X2
X3
X4
Z1
Z2
Z3
Z4
X1
X2
X3
X4
O1
O2
O3
Figure 9.14
Diﬀerent decompositions for a noisy-or CPD: (a) The standard decomposition of a noisy-
or. (b) A tree decomposition of the deterministic-or. (c) A tree-based decomposition of the noisy-or. (d) A
chain-based decomposition of the noisy-or.
decomposition of the deterministic OR as a tree. We can simplify this construction by eliminating
the intermediate variables Zi, integrating the “noise” for each Xi into the appropriate Oi. In
particular, O1 would be the noisy-or of X1 and X2, with the original noise parameters and a
leak parameter of 0. The resulting construction is shown in ﬁgure 9.14c.
We can now revisit the inference task in this apparently more complex network. An optimal
ordering for variable elimination is X1, X2, X3, X4, O1, O2. The cost of performing elimination
of X1, X2 is:
•
8 multiplications for ψ1(X1, X2, O1) = P(X1) · P(O1 | X1, X2)
•
4 additions to sum out X1 in τ1(X2, O1) = P
X1 ψ1(X1, X2, O1)
•
4 multiplications for ψ2(X2, O1) = τ1(X2, O1) · P(X2)
•
2 additions for τ2(O1) = P
X2 ψ2(X2, O1)
The cost for eliminating X3, X4 is identical, as is the cost for subsequently eliminating O1, O2.
Thus, the total number of operations is 3 · (8 + 4) = 36 multiplications and 3 · (4 + 2) = 18
additions.
A diﬀerent decomposition of the OR variable is as a simple cascade, where each Zi is consec-
utively OR’ed with the previous intermediate result. This decomposition leads to the construction

9.6. Inference with Structured CPDs ⋆
327
of ﬁgure 9.14d. For this construction, an optimal elimination ordering is X1, O1, X2, O2, X3, O3, X4.
A simple analysis shows that it takes 4 multiplications and 2 additions to eliminate each of
X1, . . . , X4, and 8 multiplications and 4 additions to eliminate each of O1, O2, O3. The total
cost is 4 · 4 + 3 · 8 = 40 multiplications and 4 · 2 + 3 · 4 = 20 additions.
9.6.1.2
The General Decomposition
Clearly, the construction used in the preceding example is a general one that can be applied to
more complex networks and other types of CPDs that have independence of causal inﬂuence. We
take a variable whose CPD has independence of causal inﬂuence, and generate its decomposition
into a set of independent noise models and a deterministic function, as in ﬁgure 5.13.
We then cascade the computation of the deterministic function into a set of smaller steps.
Given our assumption about the symmetry and associativity of the deterministic function in
the deﬁnition of symmetric ICI (deﬁnition 5.13), any decomposition of the deterministic function
results in the same answer. Speciﬁcally, consider a variable Y with parents X1, . . . , Xk, whose
CPD satisﬁes deﬁnition 5.13. We can decompose Y by introducing k −1 intermediate variables
O1, . . . , Ok−1, such that:
•
the variable Z, and each of the Oi’s, has exactly two parents in Z1, . . . , Zk, O1, . . . , Oi−1;
•
the CPD of Z and of Oi is the deterministic ⋄of its two parents;
•
each Zl and each Oi is a parent of at most one variable in O1, . . . , Ok−1, Z.
These conditions ensure that Z = Z1⋄Z2⋄. . .⋄Zk, but that this function is computed gradually,
where the node corresponding to each intermediate result has an indegree of 2.
We note that we can save some extraneous nodes, as in our example, by aggregating the noisy
dependence of Zi on Xi into the CPD where Zi is used.
After executing this decomposition for every ICI variable in the network, we can simply
apply variable elimination to the decomposed network with the smaller factors. As we saw, the
complexity of the inference can go down substantially if we have smaller CPDs and thereby
smaller factors.
We note that the sizes of the intermediate factors depend not only on the number of variables
in their scope, but also on the domains of these variables. For the case of noisy-or variables (as
well as noisy-max, noisy-and, and so on), the domain size of these variables is ﬁxed and fairly
small. However, in other cases, the domain might be quite large. In particular, in the case of
generalized linear models, the domain of the intermediate variable Z generally grows linearly
with the number of parents.
Example 9.10
Consider a variable Y with PaY = {X1, . . . , Xk}, where each Xi is binary. Assume that Y ’s CPD
is a generalized linear model, whose parameters are w0 = 0 and wi = w for all i > 1. Then the
domain of the intermediate variable Z is {0, 1, . . . , k}. In this case, the decomposition provides a
trade-oﬀ: The size of the original CPD for P(Y | X1, . . . , Xk) grows as 2k; the size of the factors in
the decomposed network grow roughly as k3. In diﬀerent situations, one approach might be better
than the other.
Thus, the decomposition of symmetric ICI variables might not always be beneﬁcial.

328
Chapter 9. Variable Elimination
9.6.1.3
Global Structure
Our decomposition of the function f that deﬁnes the variable Z can be done in many ways, all
of which are equivalent in terms of their ﬁnal result. However, they are not equivalent from the
perspective of computational cost. Even in our simple example, we saw that one decomposition
can result in fewer operations than the other. The situation is signiﬁcantly more complicated
when we take into consideration other dependencies in the network.
Example 9.11
Consider the network of ﬁgure 9.14c, and assume that X1 and X2 have a joint parent A. In
this case, we eliminate A ﬁrst, and end up with a factor over X1, X2. Aside from the 4 + 8 =
12 multiplications and 4 additions required to compute this factor τ0(X1, X2), it now takes 8
multiplications to compute ψ1(X1, X2, O1) = τ0(X1, X2) · P(O1 | X1, X2), and 4 + 2 = 6
additions to sum out X1 and X2 in ψ1. The rest of the computation remains unchanged. Thus,
the total number of operations required to eliminate all of X1, . . . , X4 (after the elimination of A)
is 8 + 12 = 20 multiplications and 6 + 6 = 12 additions.
Conversely, assume that X1 and X3 have the joint parent A. In this case, it still requires 12
multiplications and 4 additions to compute a factor τ0(X1, X3), but the remaining operations
become signiﬁcantly more complex. In particular, it takes:
• 8 multiplications for ψ1(X1, X2, X3) = τ0(X1, X3) · P(X2)
• 16 multiplications for ψ2(X1, X2, X3, O1) = ψ1(X1, X2, X3) · P(O1 | X1, X2)
• 8 additions for τ2(X3, O1) = P
X1,X2 ψ2(X1, X2, X3, O1)
The same number of operations is required to eliminate X3 and X4. (Once these steps are completed,
we can eliminate O1, O2 as usual.) Thus, the total number of operations required to eliminate all
of X1, . . . , X4 (after the elimination of A) is 2 · (8 + 16) = 48 multiplications and 2 · 8 = 16
additions, considerably more than our previous case.
Clearly, in the second network structure, had we done the decomposition of the noisy-or
variable so as to make X1 and X3 parents of O1 (and X2, X4 parents of O2), we would get
the same cost as we did in the ﬁrst case. However, in order to do that, we need to take into
consideration the global structure of the network, and even the order in which other variables
are eliminated, at the same time that we are determining how to decompose a particular variable
with symmetric ICI. In particular, we should determine the structure of the decomposition at
the same time that we are considering the elimination ordering for the network as a whole.
9.6.1.4
Heterogeneous Factorization
An alternative approach that achieves this goal uses a diﬀerent factorization for a network —
one that factorizes the joint distribution for the network into CPDs, as well as the CPDs of
symmetric ICI variables into smaller components. This factorization is heterogeneous, in that
some factors must be combined by product, whereas others need to be combined using the type
of operation that corresponds to the symmetric ICI function in the corresponding CPD. One
can then deﬁne a heterogeneous variable elimination algorithm that combines factors, using
whichever operation is appropriate, and that eliminates variables. Using this construction, we
can determine a global ordering for the operations that determines the order in which both local

9.6. Inference with Structured CPDs ⋆
329
E
A
D
(q2,1-q2)
(q3,1-q3)
A
(q1,1-q1)
B
b1
b0
a1
a0
C
B
(b)
(a)
Figure 9.15
A Bayesian network with rule-based structure: (a) the network structure; (b) the CPD for
the variable D.
factors and global factors are combined. Thus, in eﬀect, the algorithm determines the order in
which the components of an ICI CPD are “recombined” in a way that takes into consideration
the structure of the factors created in a variable elimination algorithm.
9.6.2
Context-Speciﬁc Independence
A second important type of local CPD structure is the context-speciﬁc independence, typically
encoded in a CPD as trees or rules.
As in the case of ICI, there are two main ways of
exploiting this type of structure in the context of a variable elimination algorithm. One approach
(exercise 9.15) uses a decomposition of the CPD, which is performed as a preprocessing step on
the network structure; standard variable elimination can then be performed on the modiﬁed
network.
The second approach, which we now describe, modiﬁes the variable elimination
algorithm itself to conduct its basic operations on structured factors. We can also exploit this
structure within the context of a conditioning algorithm.
9.6.2.1
Rule-Based Variable Elimination
An alternative approach is to introduce the structure directly into the factors used in the variable
elimination algorithm, allowing it to take advantage of the ﬁner-grained structure. It turns out
that this approach is easier to understand and implement for CPDs and factors represented as
rules, and hence we present the algorithm in this context.
As speciﬁed in section 5.3.1.2, a rule-based CPD is described as a set of mutually exclusive and
exhaustive rules, where each rule ρ has the form ⟨c; p⟩. As we already discussed, a tree-CPD
and a tabular CPD can each be converted into a set of rules in the obvious way.
Example 9.12
Consider the network structure shown in ﬁgure 9.15a. Assume that the CPD for the variable D is a
tree, whose structure is shown in ﬁgure 9.15b. Decomposing this CPD into rules, we get the following

330
Chapter 9. Variable Elimination
set of rules:



















ρ1
⟨b0, d0; 1 −q1⟩
ρ2
⟨b0, d1; q1⟩
ρ3
⟨a0, b1, d0; 1 −q2⟩
ρ4
⟨a0, b1, d1; q2⟩
ρ5
⟨a1, b1, d0; 1 −q3⟩
ρ6
⟨a1, b1, d0; q3⟩



















Assume that the CPD P(E | A, B, C, D) is also associated with a set of rules. Our discussion will
focus on rules involving the variable D, so we show only that part of the rule set:



























ρ7
⟨a0, d0, e0; 1 −p1⟩
ρ8
⟨a0, d0, e1; p1⟩
ρ9
⟨a0, d1, e0; 1 −p2⟩
ρ10
⟨a0, d1, e1; p2⟩
ρ11
⟨a1, b0, c1, d0, e0; 1 −p4⟩
ρ12
⟨a1, b0, c1, d0, e1; p4⟩
ρ13
⟨a1, b0, c1, d1, e0; 1 −p5⟩
ρ14
⟨a1, b0, c1, d1, e1; p5⟩



























Using this type of process, the entire distribution can be factorized into a multiset of rules
R, which is the union of all of the rules associated with the CPDs of the diﬀerent variables in
the network. Then, the probability of any instantiation ξ to the network variables X can be
computed as
P(ξ) =
Y
⟨c;p⟩∈R,ξ∼c
p,
where we recall that ξ ∼c holds if the assignments ξ and c are compatible, in that they assign
the same values to those variables that are assigned values in both.
Thus, as for the tabular CPDs, the distribution is deﬁned in terms of a product of smaller
components. In this case, however, we have broken up the tables into their component rows.
This deﬁnition immediately suggests that we can use similar ideas to those used in the table-
based variable elimination algorithm. In particular, we can multiply rules with each other and
sum out a variable by adding up rules that give diﬀerent values to the variables but are the
same otherwise.
In general, we deﬁne the following two key operations:
Deﬁnition 9.8
Let ρ1 = ⟨c; p1⟩and ρ2 = ⟨c; p2⟩be two rules. Then their product ρ1 · ρ2 = ⟨c; p1 · p2⟩.
rule product
This deﬁnition is signiﬁcantly more restricted than the product of tabular factors, since it requires
that the two rules have precisely the same context. We return to this issue in a moment.
Deﬁnition 9.9
Let Y be a variable with Val(Y ) = {y1, . . . , yk}, and let ρi for i = 1, . . . , k be a rule of the form
ρi = ⟨c, Y = yi; pi⟩. Then for R = {ρ1, . . . , ρk}, the sum P
Y R = ⟨c; Pk
i=1 pi⟩.
rule sum

9.6. Inference with Structured CPDs ⋆
331
After this operation, Y is summed out in the context c.
Both of these operations can only be applied in very restricted settings, that is, to sets of
rules that satisfy certain stringent conditions. In order to make our set of rules amenable to the
application of these operations, we might need to reﬁne some of our rules. We therefore deﬁne
the following ﬁnal operation:
Deﬁnition 9.10
Let ρ = ⟨c; p⟩be a rule, and let Y be a variable. We deﬁne the rule split Split(ρ∠Y ) as follows:
rule split
If Y ∈Scope[c], then Split(ρ∠Y ) = {ρ}; otherwise,
Split(ρ∠Y ) = {⟨c, Y = y; p⟩: y ∈Val(Y )}.
In general, the purpose of rule splitting is to make the context of one rule ρ = ⟨c; p⟩compatible
with the context c′ of another rule ρ′. Naively, we might take all the variables in Scope[c′] −
Scope[c] and split ρ recursively on each one of them. However, this process creates unnecessarily
many rules.
Example 9.13
Consider ρ2 and ρ14 in example 9.12, and assume we want to multiply them together. To do so,
we need to split ρ2 in order to produce a rule with an identical context. If we naively split ρ2 on
all three variables A, C, E that appear in ρ14 and not in ρ2, the result would be eight rules of
the form: ⟨a, b0, c, d1, e; q1⟩, one for each combination of values a, c, e. However, the only rule we
really need in order to perform the rule product operation is ⟨a1, b0, c1, d1, e1; q1⟩.
Intuitively, having split ρ2 on the variable A, it is wasteful to continue splitting the rule whose
context is a0, since this rule (and any derived from it) will not participate in the desired rule product
operation with ρ14. Thus, a more parsimonious split of ρ14 that still generates this last rule is:







⟨a0, b0, d1; q1⟩
⟨a1, b0, c0, d1; q1⟩
⟨a1, b0, c1, d1, e0; q1⟩
⟨a1, b0, c1, d1, e1; q1⟩







This new rule set is still a mutually exclusive and exhaustive partition of the space originally covered
by ρ2, but contains only four rules rather than eight.
In general, we can construct these more parsimonious splits using the recursive procedure
shown in algorithm 9.6. This procedure gives precisely the desired result shown in the example.
Rule splitting gives us the tool to take a set of rules and reﬁne them, allowing us to apply
either the rule-product operation or the rule-sum operation. The elimination algorithm is shown
in algorithm 9.7. Note that the ﬁgure only shows the procedure for eliminating a single variable
Y . The outer loop, which iteratively eliminates nonquery variables one at a time, is precisely
the same as the Sum-Product-VE procedure in algorithm 9.1, except that it takes as input a set
of rule factors rather than table factors.
To understand the operation of the algorithm more concretely, consider the following example:
Example 9.14
Consider the network in example 9.12, and assume that we want to eliminate D in this network.
Our initial rule set R+ is the multiset of all of the rules whose scope contains D, which is precisely
the set {ρ1, . . . , ρ14}. Initially, none of the rules allows for the direct application of either rule
product or rule sum. Hence, we have to split rules.

332
Chapter 9. Variable Elimination
Algorithm 9.6 Rule splitting algorithm
Procedure Rule-Split (
ρ = ⟨c; p⟩,
// Rule to be split
c′
// Context to split on
)
1
if c ̸∼c′ then return ρ
2
if Scope[c] ⊆Scope[c′] then return ρ
3
Select Y ∈Scope[c′] −Scope[c]
4
R ←Split(ρ∠Y )
5
R′ ←
∪ρ′′∈R Rule-Split(ρ′′, c′)
6
return R′
The rules ρ3 on the one hand, and ρ7, ρ8 on the other, have compatible contexts, so we can
choose to combine them. We begin by splitting ρ3 and ρ7 on each other’s context, which results in:











ρ15
⟨a0, b1, d0, e0; 1 −q2⟩
ρ16
⟨a0, b1, d0, e1; 1 −q2⟩
ρ17
⟨a0, b0, d0, e0; 1 −p1⟩
ρ18
⟨a0, b1, d0, e0; 1 −p1⟩











The contexts of ρ15 and ρ18 match, so we can now apply rule product, replacing the pair by:
 ρ19
⟨a0, b1, d0, e0; (1 −q2)(1 −p1)⟩	
We can now split ρ8 using the context of ρ16 and multiply the matching rules together, obtaining
 ρ20
⟨a0, b0, d0, e1; p1⟩
ρ21
⟨a0, b1, d0, e1; (1 −q2)p1⟩

.
The resulting rule set contains ρ17, ρ19, ρ20, ρ21 in place of ρ3, ρ7, ρ8.
We can apply a similar process to ρ4 and ρ9, ρ10, which leads to their substitution by the rule
set:







ρ22
⟨a0, b0, d1, e0; 1 −p2⟩
ρ23
⟨a0, b1, d1, e0; q2(1 −p2)⟩
ρ24
⟨a0, b0, d1, e1; p2⟩
ρ25
⟨a0, b1, d1, e1; q2p2⟩







.
We can now eliminate D in the context a0, b1, e1.
The only rules in R+ compatible with
this context are ρ21 and ρ25.
We extract them from R+ and sum them; the resulting rule
⟨a0, b1, e1; (1 −q2)p1 + q2p2⟩, is then inserted into R−. We can similarly eliminate D in the
context a0, b1, e0.
The process continues, with rules being split and multiplied. When D has been eliminated in a
set of mutually exclusive and exhaustive contexts, then we have exhausted all rules involving D; at
this point, R+ is empty, and the process of eliminating D terminates.

9.6. Inference with Structured CPDs ⋆
333
Algorithm 9.7 Sum-product variable elimination for sets of rules
Procedure Rule-Sum-Product-Eliminate-Var (
R,
// Set of rules
Y
// Variable to be eliminated
)
1
R+ ←{ρ ∈R : Scope[ρ] ∋Y }
2
R−←R −R+
3
while R+ ̸= ∅
4
Apply one of the following actions, when applicable
5
Rule sum:
6
Select Rc ⊆R+ such that
7
Rc = {⟨c, Y = y1; p1⟩, . . . , ⟨c, Y = yk; pk⟩}
8
no other ρ ∈R+ is compatible with c)
9
R−←R−∪P
Y Rc
10
R+ ←R+ −Rc
11
Rule product:
12
Select ⟨c; p1⟩, ⟨c; p2⟩∈R+
13
R+ ←R+ −{⟨c; p1⟩, ⟨c; p2⟩} ∪{⟨c; p1 · p2⟩}
14
Rule splitting for rule product:
15
Select ρ1, ρ2 ∈R+ such that
16
ρ1 = ⟨c1; p1⟩
17
ρ2 = ⟨c2; p2⟩
18
c1 ∼c2
19
R+ ←R+ −{ρ1, ρ2} ∪Rule-Split(ρ1, c2) ∪Rule-Split(ρ2, c1)
20
Rule splitting for rule sum:
21
Select ρ1, ρ2 ∈R+ such that
22
ρ1 = ⟨c1, Y = yi; p1⟩
23
ρ2 = ⟨c2, Y = yj; p2⟩
24
c1 ∼c2
25
i ̸= j
26
R+ ←R+ −{ρ1, ρ2} ∪Rule-Split(ρ1, c2) ∪Rule-Split(ρ2, c1)
27
return R−
A diﬀerent way of understanding the algorithm is to consider its application to rule sets that
originate from standard table-CPDs.
It is not diﬃcult to verify that the algorithm performs
exactly the same set of operations as standard variable elimination. For example, the standard
operation of factor product is simply the application of rule splitting on all of the rules that
constitute the two tables, followed by a sequence of rule product operations on the resulting
rule pairs. (See exercise 9.16.)
To prove that the algorithm computes the correct result, we need to show that each operation
performed in the context of the algorithm maintains a certain correctness invariant. Let R be
the current set of rules maintained by the algorithm, and W be the variables that have not yet
been eliminated. Each operation must maintain the following condition:

334
Chapter 9. Variable Elimination
(a)
E
A
D
C
B
(b)
E
A
D
C
B
Figure 9.16
Conditioning a Bayesian network whose CPDs have CSI: (a) conditioning on a0; (b)
conditioning on a1.
The probability of a context c such that Scope[c] ⊆W can be obtained by multiplying
all rules ⟨c′; p⟩∈R whose context is compatible with c.
It is not diﬃcult to show that the invariant holds initially, and that each step in the algorithm
maintains it. Thus, the algorithm as a whole is correct.
9.6.2.2
Conditioning
We can also use other techniques for exploiting CSI in inference. In particular, we can generalize
the notion of conditioning to this setting in an interesting way. Consider a network B, and
assume that we condition it on a variable U. So far, we have assumed that the structure of the
diﬀerent conditioned networks, for the diﬀerent values u of U, is the same. When the CPDs are
tables, with no extra structure, this assumption generally holds. However, when the CPDs have
CSI, we might be able to utilize the additional structure to simplify the conditioned networks
considerably.
Example 9.15
Consider the network shown in ﬁgure 9.15, as described in example 9.12. Assume we condition this
network on the variable A. If we condition on a0, we see that the reduced CPD for E no longer
depends on C. Thus, the conditioned Markov network for this set of factors is the one shown in
ﬁgure 9.16a. By contrast, when we condition on a1, the reduced factors do not “lose” any variables
aside from A, and we obtain the conditioned Markov network shown in ﬁgure 9.16b. Note that the
network in ﬁgure 9.16a is so simple that there is no point performing any further conditioning on it.
Thus, we can continue the conditioning process for only one of the two branches of the computation
— the one corresponding to a1.
In general, we can extend the conditioning algorithm of section 9.5 to account for CSI in the
CPDs or in the factors of a Markov network. Consider a single conditioning step on a variable
U. As we enumerate the diﬀerent possible values u of U, we generate a possibly diﬀerent
conditioned network for each one. Depending on the structure of this network, we select which
step to take next in the context of this particular network. In diﬀerent networks, we might
choose a diﬀerent variable to use for the next conditioning step, or we might decide to stop the
conditioning process for some networks altogether.

9.6. Inference with Structured CPDs ⋆
335
9.6.3
Discussion
We have presented two approaches to variable elimination in the case of local structure in
the CPDs: preprocessing followed by standard variable elimination, and specialized variable
elimination algorithms that use a factorization of the structured CPD. These approaches oﬀer
diﬀerent trade-oﬀs. On the one hand, the specialized variable elimination approach reveals more
of the structure of the CPDs to the inference algorithm, allowing the algorithm more ﬂexibility
in exploiting this structure. Thus, this approach can achieve lower computational cost than any
ﬁxed decomposition scheme (see box 9.D). By comparison, the preprocessing approach embeds
some of the structure within deterministic CPDs, a structure that most variable elimination
algorithms do not fully exploit.
On the other hand, specialized variable elimination schemes such as those for rules require
the use of special-purpose variable elimination algorithms rather than oﬀ-the-shelf packages.
Furthermore, the data structures for tables are signiﬁcantly more eﬃcient than those for other
types of factors such as rules. Although this diﬀerence seems to be an implementation issue,
it turns out to be quite signiﬁcant in practice. One can somewhat address this limitation by
the use of more sophisticated algorithms that exploit eﬃcient table-based operations whenever
possible (see exercise 9.18).

Although the trade-oﬀs between these two approaches is not always clear, it is generally
the case that, in networks with signiﬁcant amounts of local structure, it is valuable
to design an inference scheme that exploits this structure for increased computational
eﬃciency.
Box 9.D — Case Study: Inference with Local Structure. A natural question is the extent to
which local structure can actually help speed up inference.
In one experimental comparison by Zhang and Poole (1996), four algorithms were applied to frag-
ments of the CPCS network (see box 5.D): standard variable elimination (with table representation
of factors), the two decompositions illustrated in ﬁgure 9.14 for the case of noisy-or, and a special-
purpose elimination algorithm that uses a heterogeneous factorization. The results show that in a
network such as CPCS, which uses predominantly noisy-or and noisy-max CPDs, signiﬁcant gains
in performance can be obtained. They results also showed that the two decomposition schemes
(tree-based and chain-based) are largely equivalent in their performance, and the heterogeneous
factorization outperforms both of them, due to its greater ﬂexibility in dynamically determining the
elimination ordering during the course of the algorithm.
For rule-based variable elimination, no large networks with extensive rule-based structure had
been constructed.
So, Poole and Zhang (2003) used a standard benchmark network, with 32
variables and 11,018 entries. Entries that were within 0.05 of each other were collaped, to construct
a more compact rule-based representation, with a total of 5,834 distinct entries. As expected, there
are a large number of cases where the use of rule-based inference provided signiﬁcant savings.
However, there were also many cases where contextual independence does not provide signiﬁcant
help, in which case the increased overhead of the rule-based inference dominates, and standard VE
performs better.
At a high level, the main conclusion is that table-based approaches are amenable to numerous
optimizations, such as those described in box 10.A, which can improve the performance by an

336
Chapter 9. Variable Elimination
order of magnitude or even more. Such optimizations are harder to deﬁne for more complex data
structures. Thus, it is only useful to consider algorithms that exploit local structure either when it is
extensively present in the model, or when it has speciﬁc structure that can, itself, be exploited using
specialized algorithms.
9.7
Summary and Discussion
In this chapter, we described the basic algorithms for exact inference in graphical models. As
we saw, probability queries essentially require that we sum out an exponentially large joint
distribution. The fundamental idea that allows us to avoid the exponential blowup in this task
is the use of dynamic programming, where we perform the summation of the joint distribution
from the inside out rather than from the outside in, and cache the intermediate results, thereby
avoiding repeated computation.
We presented an algorithm based on this insight, called variable elimination. The algorithm
works using two fundamental operations over factors — multiplying factors and summing out
variables in factors.
We analyzed the computational complexity of this algorithm using the
structural properties of the graph, showing that the key computational metric was the induced
width of the graph.
We also presented another algorithm, called conditioning, which performs some of the sum-
mation operations from the outside in rather than from the inside out, and then uses variable
elimination for the rest of the computation. Although the conditioning algorithm is never less
expensive than variable elimination in terms of running time, it requires less storage space and
hence provides a time-space trade-oﬀfor variable elimination.
We showed that both variable elimination and conditioning can take advantage of local
structure within the CPDs. Speciﬁcally, we presented methods for making use of CPDs with
independence of causal inﬂuence, and of CPDs with context-speciﬁc independence. In both
cases, techniques tend to fall into two categories: In one class of methods, we modify the
network structure, adding auxiliary variables that reveal some of the structure inside the CPD
and break up large factors. In the other, we modify the variable elimination algorithm directly
to use structured factors rather than tables.
Although exact inference is tractable for surprisingly many real-world graphical models, it is
still limited by its worst-case exponential performance. There are many models that are simply
too complex for exact inference. As one example, consider the n × n grid-structured pairwise
Markov networks of box 4.A. It is not diﬃcult to show that the minimal tree-width of this network
is n. Because these networks are often used to model pixels in an image, where n = 1, 000
is quite common, it is clear that exact inference is intractable for such networks.
Another
example is the family of networks that we obtain from the template model of example 6.11.
Here, the moralized network, given the evidence, is a fully connected bipartite graph; if we
have n variables on one side and m on the other, the minimal tree-width is min(n, m), which
can be very large for many practical models. Although this example is obviously a toy domain,
examples of similar structure arise often in practice. In later chapters, we will see many other
examples where exact inference fails to scale up. Therefore, in chapter 11 and chapter 12 we

9.8. Relevant Literature
337
discuss approximate inference methods that trade oﬀthe accuracy of the results for the ability
to scale up to much larger models.
One class of networks that poses great challenges to inference is the class of networks
induced by template-based representations. These languages allow us to specify (or learn) very
small, compact models, yet use them to construct arbitrarily large, and often densely connected,
networks. Chapter 15 discusses some of the techniques that have been used to deal with dynamic
Bayesian networks.
Our focus in this chapter has been on inference in networks involving only discrete variables.
The introduction of continuous variables into the network also adds a signiﬁcant challenge.
Although the ideas that we described here are instrumental in constructing algorithms for this
richer class of models, many additional ideas are required. We discuss the problems and the
solutions in chapter 14.
9.8
Relevant Literature
The ﬁrst formal analysis of the computational complexity of probabilistic inference in Bayesian
networks is due to Cooper (1990).
Variants of the variable elimination algorithm were invented independently in multiple com-
munities. One early variant is the peeling algorithm of Cannings et al. (1976, 1978), formulated for
peeling
the analysis of genetic pedigrees. Another early variant is the forward-backward algorithm, which
forward-backward
algorithm
performs inference in hidden Markov models (Rabiner and Juang 1986). An even earlier variant
of this algorithm was proposed as early as 1880, in the context of continuous models (Thiele
1880). Interestingly, the ﬁrst variable elimination algorithm for fully general models was invented
as early as 1972 by Bertelé and Brioschi (1972), under the name nonserial dynamic programming.
nonserial
dynamic
programming
However, they did not present the algorithm in the setting of probabilistic inference in graph-
structured models, and therefore it was many years before the connection to their work was
recognized. Other early work with similar ideas but a very diﬀerent application was done in the
database community (Beeri et al. 1983).
The general problem of probabilistic inference in graphical models was ﬁrst tackled by Kim
and Pearl (1983), who proposed a local message passing algorithm in polytree-structured Bayesian
networks. These ideas motivated the development of a wide variety of more general algorithms.
One such trajectory includes the clique tree methods that we discuss at length in the next chapter
(see also section 10.6). A second includes a specrum of other methods (for example, Shachter
1988; Shachter et al. 1990), culminating in the variable elimination algorithm, as presented here,
ﬁrst described by Zhang and Poole (1994) and subsequently by Dechter (1999).
Huang and
Darwiche (1996) provide some useful tips on an eﬃcient implementation of algorithms of this
type.
Dechter (1999) presents interesting connections between these algorithms and constraint-
satisfaction algorithms, connections that have led to fruitful work in both communities. Other
generalizations of the algorithm to settings other than pure probabilistic inference were described
by Shenoy and Shafer (1990); Shafer and Shenoy (1990) and by Dawid (1992). The construction
of the network polynomial was proposed by Darwiche (2003).
The complexity analysis of the variable elimination algorithm is described by Bertelé and
Brioschi (1972); Dechter (1999). The analysis is based on core concepts in graph theory that have

338
Chapter 9. Variable Elimination
been the subject of extensive theoretical analysis; see Golumbic (1980); Tarjan and Yannakakis
(1984); Arnborg (1985) for an introduction to some of the key concepts and algorithms.
Much work has been done on the problem of ﬁnding low-tree-width triangulations or (equiv-
alently) elimination orderings. One of the earliest algorithms is the maximum cardinality search
of Tarjan and Yannakakis (1984). Arnborg, Corneil, and Proskurowski (1987) show that the prob-
lem of ﬁnding the minimal tree-width elimination ordering is NP-hard. Shoikhet and Geiger
(1997) describe a relatively eﬃcient algorithm for ﬁnding this optimal elimination ordering —
one whose cost is approximately the same as the cost of inference with the resulting ordering.
Becker and Geiger (2001) present an algorithm that ﬁnds a close-to-optimal ordering. Neverthe-
less, most implementations use one of the standard heuristics. A good survey of these heuristic
methods is presented by Kjærulﬀ(1990), who also provides an extensive empirical comparison.
Fishelson and Geiger (2003) suggest the use of stochastic search as a heuristic and provide
another set of comprehensive experimental comparisons, focusing on the problem of genetic
linkage analysis. Bodlaender, Koster, van den Eijkhof, and van der Gaag (2001) provide a series
of simple preprocessing steps that can greatly reduce the cost of triangulation.
The ﬁrst incarnation of the conditioning algorithm was presented by Pearl (1986a), in the
context of cutset conditioning, where the conditioning variables cut all loops in the network,
forming a polytree. Becker and Geiger (1994); Becker, Bar-Yehuda, and Geiger (1999) present a va-
riety of algorithms for ﬁnding a small loop cutset. The general algorithm, under the name global
conditioning, was presented by Shachter et al. (1994). They also demonstrated the equivalence of
conditioning and variable elimination (or rather, the clique tree algorithm) in terms of the under-
lying computations, and pointed out the time-space trade-oﬀs between these two approaches.
These time-space trade-oﬀs were then placed in a comprehensive computational framework in
the recursive conditioning method of Darwiche (2001b); Allen and Darwiche (2003a,b). Cutset
algorithms have made a signiﬁcant impact on the application of genetic linkage analysis Schäﬀer
(1996); Becker et al. (1998), which is particularly well suited to this type of method.
The two noisy-or decomposition methods were described by Olesen, Kjærulﬀ, Jensen, Falck,
Andreassen, and Andersen (1989) and Heckerman and Breese (1996). An alternative approach that
utilizes a heterogeneous factorization was described by Zhang and Poole (1996); this approach
is more ﬂexible, but requires the use of a special-purpose inference algorithm. For the case
of CPDs with context-speciﬁc independence, the decomposition approach was proposed by
Boutilier, Friedman, Goldszmidt, and Koller (1996). The rule-based variable elimination algorithm
was proposed by Poole and Zhang (2003). The trade-oﬀs here are similar to the case of the
noisy-or methods.
9.9
Exercises
Exercise 9.1⋆
Prove theorem 9.2.
Exercise 9.2⋆
Consider a factor produced as a product of some of the CPDs in a Bayesian network B:
τ(W ) =
k
Y
i=1
P(Yi | PaYi)

9.9. Exercises
339
where W = ∪k
i=1({Yi} ∪PaYi).
a. Show that τ is a conditional probability in some network. More precisely, construct another Bayesian
network B′ and a disjoint partition W = Y ∪Z such that τ(W ) = PB′(Y | Z).
b. Conclude that all of the intermediate factors produced by the variable elimination algorithm are also
conditional probabilities in some network.
Exercise 9.3
Consider a modiﬁed variable elimination algorithm that is allowed to multiply all of the entries in a single
factor by some arbitrary constant. (For example, it may choose to renormalize a factor to sum to 1.) If we
run this algorithm on the factors resulting from a Bayesian network with evidence, which types of queries
can we still obtain the right answer to, and which not?
Exercise 9.4⋆
This exercise shows basic properties of the network polynomial and its derivatives:
a. Prove equation (9.8).
b. Prove equation (9.9).
c. Let Y = y be some assignment. For Yi ∈Y , we now consider what happens if we retract the
evidence
retraction
observation Yi = yi. More precisely, let y−i be the assignment in y to all variables other than Yi.
Show that
P(y−i, Yi = y′
i | θ)
=
∂fΦ(θ, λy)
λy′
i
P(y−i | θ)
=
X
y′
i
∂fΦ(θ, λy)
λy′
i
.
Exercise 9.5⋆
In this exercise, you will show how you can use the gradient of the probability of a Bayesian network
to perform sensitivity analysis, that is, to compute the eﬀect on a probability query of changing the
sensitivity
analysis
parameters in a single CPD P(X | U). More precisely, let θ be one set of parameters for a network G,
where we have that θx|u is the parameter associated with the conditional probability entry P(X | U).
Let θ′ be another parameter assignment that is the same except that we replace the parameters θx|u with
θ′
x|u = θx|u + ∆x|u.
For an assignment e (which may or may not involve variables in X, U, compute the change P(e :
θ) −P(e : θ′) in terms of ∆x|u, and the network derivatives.
Exercise 9.6⋆
Consider some run of variable elimination over the factors Φ, where all variables are eliminated. This run
generates some set of intermediate factors τi(W i). We can deﬁne a set of intermediate (arithmetic, not
random) variables vik corresponding to the diﬀerent entries τi(wk
i ).
a. Show how, for each variable vij, we can write down an algebraic expression that deﬁnes vij in terms
of: the parameters λxi; the parameters θxc; and variables vjl for j < i.
b. Use your answer to the previous part to deﬁne an alternative representation whose complexity is linear
in the total size of the intermediate factors in the VE run.
c. Show how the same representation can be used to compute all of the derivatives of the network
polynomial; the complexity of your algorithm should be linear in the compact representation of the
network polynomial that you derived in the previous part. (Hint: Consider the partial derivatives of the
network polynomial relative to each vij, and use the chain rule for derivatives.)

340
Chapter 9. Variable Elimination
Exercise 9.7
Prove proposition 9.1.
Exercise 9.8⋆
Prove theorem 9.10, by showing that any ordering produced by the maximum cardinality search algorithm
eliminates cliques one by one, starting from the leaves of the clique tree.
Exercise 9.9
a. Show that variable elimination on polytrees can be performed in linear time, assuming that the local
probability models are represented as full tables. Speciﬁcally, for any polytree, describe an elimination
ordering, and show that the complexity of variable elimination with your ordering is linear in the size
of the network. Note that the linear time bound here is in terms of the size of the CPTs in the network,
so that the cost of the algorithm grows exponentially with the number of parents of a node.
b. Extend your result from (1) to apply to cases where the CPDs satisfy independence of causal inﬂuence.
Note that, in this case, the network representation is linear in the number of variables in the network,
and the algorithm should be linear in that number.
c. Now extend your result from (1) to apply to cases where the CPDs are tree-structured. In this case, the
network representation is the sum of the sizes of the trees in the individual CPDs, and the algorithm
should be linear in that number.
Exercise 9.10⋆
Consider the four criteria described in connection with Greedy-Ordering of algorithm 9.4: Min-Neighbors,
Min-Weight, Min-Fill, and Weighted-Min-Fill. Show that none of these criteria dominate the others; that
is, for any pair, there is always a graph where the ordering produced by one of them is better than
that produced by the other. As our measure of performance, use the computational cost of full variable
elimination (that is, for computing the partition function). For each counterexample, deﬁne the structure
of the graph and the cardinality of the variables, and show the ordering produced by each member of the
pair.
Exercise 9.11⋆
Let H be an undirected graph, and ≺an elimination ordering. Prove that X—Y is a ﬁll edge in the
induced graph if and only if there is a path X—Z1— . . . Zk—Y in H such that Zi ≺X and Zi ≺Y
for all i = 1, . . . , k.
Exercise 9.12⋆
Prove theorem 9.12.
Exercise 9.13⋆
Prove theorem 9.13.
Exercise 9.14⋆
The standard conditioning algorithm ﬁrst conditions the network on the conditioning variables U, splitting
the computation into a set of computations, one for every instantiation u to U; it then performs variable
elimination on the remaining network. As we discussed in section 9.5.4.1, we can generalize conditioning
so that it alternates conditioning steps and elimination in an arbitrary way. In this question, you will
formulate such an algorithm and provide a graph-theoretic analysis of its complexity.
Let Φ be a set of factors over X, and let X be a set of nonquery variables. Deﬁne a summation procedure
σ to be a sequence of operations, each of which is either elim(X) or cond(X) for some X ∈X, such
that each X ∈X appears in the sequence σ precisely once. The semantics of this procedure is that,
going from left to right, we perform the operation described on the variables in sequence. For example,
the summation procedure of example 9.5 would be written as:
elim(Ak−1), elim(Ak−2), . . . elim(A1), cond(Ak), elim(C), elim(B).

9.9. Exercises
341
a. Deﬁne an algorithm that takes a summation sequence as input and performs the operations in the
order stated. Provide precise pseudo-code for the algorithm.
b. Deﬁne the notion of an induced graph for this algorithm, and deﬁne the time and space complexity of
the algorithm in terms of the induced graph.
Exercise 9.15⋆
In section 9.6.1.1, we described an approach to decomposing noisy-or CPDs, aimed at reducing the cost of
variable elimination. In this exercise, we derive a construction for CPD-trees in a similar spirit.
a. Consider a variable Y that has a binary-valued parent A and four additional parents X1, . . . , X4.
Assume that the CPD of Y is structured as a tree whose ﬁrst split is A, and where Y depends only on
X1, X2 in the A = a1 branch, and only on X3, X4 in the A = a0 branch. Deﬁne two new variables,
Ya1 and Ya0, which represent the value that Y would take if A were to have the value a1, and the
value that Y would take if A were to have the value a0. Deﬁne a new model for Y that is deﬁned in
terms of these new variables. Your model should precisely specify the CPDs for Ya1, Ya0, and Y in
terms of Y ’s original CPD.
b. Deﬁne a general procedure that recursively decomposes a tree-CPD using the same principles.
Exercise 9.16
In this exercise, we show that rule-based variable elimination performs exactly the same operations as
table-based variable elimination, when applied to rules generated from table-CPDs. Consider two table
factors φ(X), φ′(Y ). Let R be the set of constituent rules for φ(X) and R′ the set of constituent rules
for φ(Y ).
a. Show that the operation of multiplying φ · φ′ can be implemented as a series of rule splits on R ∪R′,
followed by a series of rule products.
b. Show that the operation of summing out Y ∈X in φ can be implemented as a series of rule sums in
R.
Exercise 9.17⋆
Prove that each step in the algorithm of algorithm 9.7 maintains the program-correctness invariant de-
scribed in the text: Let R be the current set of rules maintained by the algorithm, and W be the variables
that have not yet been eliminated. The invariant is that:
The probability of a context c such that Scope[c] ⊆W can be obtained by multiplying all rules
⟨c′; p⟩∈R whose context is compatible with c.
Exercise 9.18⋆⋆
Consider an alternative factorization of a Bayesian network where each factor is a hybrid between a rule
and a table, called a confactor. Like a rule, a confactor associated with a context c; however, rather than
a single number, each confactor contains not a single number, but a standard table-based factor. For
example, the CPD of ﬁgure 5.4a would have a confactor, associated with the middle branch, whose context
is a1, s0, and whose associated table is
l0, j0
0.9
l0, j1
0.1
l1, j0
0.4
l1, j1
0.6
Extend the rule splitting algorithm of algorithm 9.6 and the rule-based variable elimination algorithm of
algorithm 9.7 to operate on confactors rather than rules. Your algorithm should use the eﬃcient table-based
data structures and operations when possible, resorting to the explicit partition of tables into rules only
when absolutely necessary.

342
Chapter 9. Variable Elimination
Exercise 9.19⋆⋆
We have shown that the sum-product variable elimination algorithm is sound, in that it returns the same
answer as ﬁrst multiplying all the factors, and then summing out the nonquery variables. Exercise 13.3 asks
for a similar argument for max-product. One can prove similar results for other pairs of operations, such as
max-sum. Rather than prove the same result for each pair of operations we encounter, we now provide a
generalized variable elimination algorithm from which these special cases, as well as others, follow directly.
generalized
variable
elimination
This general algorithm is based on the following result, which is stated in terms of a pair of abstract
operators: generalized combination of two factors, denoted φ1
N φ2; and generalized marginalization of
a factor φ over a subset W , denoted ΛW (φ). We deﬁne our generalized variable elimination algorithm
in direct analogy to the sum-product algorithm of algorithm 9.1, replacing factor product with N and
summation for variable elimination with Λ.
We now show that if these two operators satisfy certain conditions, the variable elimination algorithm for
these two operations is sound:
Commutativity of combination: For any factors φ1, φ2:
φ1
O
φ2 = φ2
O
φ1.
(9.12)
Associativity of combination: For any factors φ1, φ2, φ3:
φ1
O
(φ2
O
φ3) = (φ1
O
φ2)
O
φ3.
(9.13)
Consonance of marginalization: If φ is a factor of scope W , and Y , Z are disjoint subsets of W , then:
ΛY (ΛZ(φ)) = Λ(Y ∪Z)(φ).
(9.14)
Marginalization over combination: If φ1 is a factor of scope W and Y ∩W = ∅, then:
ΛY (φ1
O
φ2) = φ1
O
ΛY (φ2).
(9.15)
Show that if N and Λ satisfy the preceding axioms, then we obtain a theorem analogous to theorem 9.5.
That is, the algorithm, when applied to a set of factors Φ and a set of variables to be eliminated Z, returns
a factor
φ∗(Y ) = ΛZ(
O
φ∈Φ
φ).
Exercise 9.20⋆⋆
You are taking the ﬁnal exam for a course on computational complexity theory. Being somewhat too
theoretical, your professor has insidiously sneaked in some unsolvable problems and has told you that
exactly K of the N problems have a solution. Out of generosity, the professor has also given you a
probability distribution over the solvability of the N problems.
To formalize the scenario, let X = {X1, . . . , XN} be binary-valued random variables corresponding to
the N questions in the exam where Val(Xi) = {0(unsolvable), 1(solvable)}. Furthermore, let B be a
Bayesian network parameterizing a probability distribution over X (that is, problem i may be easily used
to solve problem j so that the probabilities that i and j are solvable are not independent in general).
a. We begin by describing a method for computing the probability of a question being solvable. That is
we want to compute P(Xi = 1, Possible(X) = K) where
Possible(X) =
X
i
1{Xi = 1}
is the number of solvable problems assigned by the professor.

9.9. Exercises
343
To this end, we deﬁne an extended factor φ as a “regular” factor ψ and an index so that it deﬁnes a
function φ(X, L) : V al(X) × {0, . . . , N} 7→IR where X = Scope[φ]. A projection of such a factor
[φ]l is a regular factor ψ : V al(X) 7→IR, such that ψ(X) = φ(X, l).
Provide a deﬁnition of factor combination and factor marginalization for these extended factors such
that
P(Xi, Possible(X) = K) =


X
X−{Xi}
Y
φ∈Φ
φ


K
,
(9.16)
where each φ ∈Φ is an extended factor corresponding to some CPD of the Bayesian network, deﬁned
as follows:
φXi({Xi} ∪PaXi, k) =
 P(Xi | PaXi)
if Xi = k
0
otherwise
b. Show that your operations satisfy the condition of exercise 9.19 so that you can compute equation (9.16)
use the generalized variable elimination algorithm.
c. Realistically, you will have time to work on exactly M problems (1 ≤M ≤N). Obviously, your goal
is to maximize the expected number of solvable problems that you attempt. (Luckily for you, every
solvable problem that you attempt you will solve correctly, and you neither gain nor lose credit for
working on an unsolvable problem.) Let Y be a subset of X indicating exactly M problems you
choose to work on, and let
Correct(X, Y ) =
X
Xi∈Y
Xi
be the number of solvable problems that you attempt. The expected number of problems you solve is
IEPB[Correct(X, Y ) | Possible(X) = K].
(9.17)
Using your generalized variable elimination algorithm, provide an eﬃcient algorithm for computing this
expectation.
d. Your goal is to ﬁnd Y that optimizes equation (9.17). Provide a simple example showing that:
arg
max
Y :|Y |=M IEPB[Correct(X, Y )] ̸= arg
max
Y :|Y |=M IEPB[Correct(X, Y ) | Possible(X) = K].
e. Give an eﬃcient algorithm for ﬁnding
arg
max
Y :|Y |=M IEPB[Correct(X, Y ) | Possible(X) = K].
(Hint: Use linearity of expectations.)


10
Exact Inference: Clique Trees
In the previous chapter, we showed how we can exploit the structure of a graphical model
to perform exact inference eﬀectively.
The fundamental insight in this process is that the
factorization of the distribution allows us to perform local operations on the factors deﬁning
the distribution, rather than simply generate the entire joint distribution. We implemented this
insight in the context of the variable elimination algorithm, which sums out variables one at a
time, multiplying the factors necessary for that operation.
In this chapter, we present an alternative implementation of the same insight. As in the case
of variable elimination, the algorithm uses manipulation of factors as its basic computational
step. However, the algorithm uses a more global data structure for scheduling these operations,
with surprising computational beneﬁts.
Throughout this chapter, we will assume that we are dealing with a set of factors Φ over a
set of variables X, where each factor φi has a scope Xi. This set of factors deﬁnes a (usually)
unnormalized measure
˜PΦ(X) =
Y
φi∈Φ
φi(Xi).
(10.1)
For a Bayesian network without evidence, the factors are simply the CPDs, and the measure ˜PΦ
is a normalized distribution. For a Bayesian network B with evidence E = e, the factors are
the CPDs restricted to e, and ˜PΦ(X) = PB(X, e). For a Gibbs distribution (with or without
evidence), the factors are the (restricted) potentials, and ˜PΦ is the unnormalized Gibbs measure.
It is important to note that all of the operations that one can perform on a normalized distri-
bution can also be performed on an unnormalized measure. In particular, we can marginalize
˜PΦ on a subset of the variables by summing out the others. We can also consider a conditional
measure, ˜PΦ(X | Y ) = ˜PΦ(X, Y )/ ˜PΦ(Y ) (which, in fact, is the same as PΦ(X | Y )).
10.1
Variable Elimination and Clique Trees
Recall that the basic operation of the variable elimination algorithm is the manipulation of
factors. Each step in the computation creates a factor ψi by multiplying existing factors. A
variable is then eliminated in ψi to generate a new factor τi, which is then used to create
another factor. In this section, we present another view of this computation. We consider a
factor ψi to be a computational data structure, which takes “messages” τj generated by other
message
factors ψj, and generates a message τi that is used by another factor ψl.

346
Chapter 10. Clique Trees
5: G,J,L,S
4: G,H,J
3: G,I,S
7: J,L
J,L
6: J,L,S
J,S,L
2: D,I,G
1: C,D
G,J
G,S
G,I
D
Figure 10.1
Cluster tree for the VE execution in table 9.1
10.1.1
Cluster Graphs
We begin by deﬁning a cluster graph — a data structure that provides a graphical ﬂowchart of
the factor-manipulation process. Each node in the cluster graph is a cluster, which is associated
with a subset of variables; the graph contains undirected edges that connect clusters whose
scopes have some non-empty intersection. We note that this deﬁnition is more general than the
data structures we use in this chapter, but this generality will be important in the next chapter,
where we signiﬁcantly extend the algorithms of this chapter.
Deﬁnition 10.1
A cluster graph U for a set of factors Φ over X is an undirected graph, each of whose nodes i is
cluster graph
associated with a subset Ci ⊆X. A cluster graph must be family-preserving — each factor φ ∈Φ
family
preservation
must be associated with a cluster Ci, denoted α(φ), such that Scope[φ] ⊆Ci. Each edge between
a pair of clusters Ci and Cj is associated with a sepset Si,j ⊆Ci ∩Cj.
sepset
An execution of variable elimination deﬁnes a cluster graph: We have a cluster for each factor
ψi used in the computation, which is associated with the set of variables Ci = Scope[ψi]. We
draw an edge between two clusters Ci and Cj if the message τi, produced by eliminating a
variable in ψi, is used in the computation of τj.
Example 10.1
Consider the elimination process of table 9.1. In this case, we have seven factors ψ1, . . . , ψ7, whose
scope is shown in the table. The message τ1(D), generated from ψ1(C, D), participates in the
computation of ψ2. Thus, we would have an edge from C1 to C2. Similarly, the message τ3(G, S)
is generated from ψ3 and used in the computation of ψ5. Hence, we introduce an edge between
C3 and C5. The entire graph is shown in ﬁgure 10.1. The edges in the graph are annotated
with directions, indicating the ﬂow of messages between clusters in the execution of the variable
elimination algorithm. Each of the factors in the initial set of factors Φ is also associated with a
cluster Ci. For example, the cluster φD(D, C) (corresponding to the CPD P(D | C)) is associated
with C1, and the cluster φH(H, G, J) (corresponding to the CPD P(H | G, J)) is associated with
C4.
10.1.2
Clique Trees
The cluster graph associated with an execution of variable elimination is guaranteed to have
certain properties that turn out to be very important.

10.1. Variable Elimination and Clique Trees
347
First, recall that the variable elimination algorithm uses each intermediate factor τi at most
once: when φi is used in Sum-Product-Eliminate-Var to create ψj, it is removed from the set of
factors Φ, and thus cannot be used again. Hence, the cluster graph induced by an execution of
variable elimination is necessarily a tree.
We note that although a cluster graph is deﬁned to be an undirected graph, an execution of
variable elimination does deﬁne a direction for the edges, as induced by the ﬂow of messages
between the clusters. The directed graph induced by the messages is a directed tree, with all
the messages ﬂowing toward a single cluster where the ﬁnal result is computed. This cluster is
called the root of the directed tree. Using standard conventions in computer science, we assume
that the root of the tree is “up,” so that messages sent toward the root are sent upward. If Ci is
on the path from Cj to the root we say that Ci is upstream from Cj, and Cj is downstream
upstream clique
downstream
clique
from Ci. We note that, for reasons that will become clear later on, the directions of the edges
and the root are not part of the deﬁnition of a cluster graph.
The cluster tree deﬁned by variable elimination satisﬁes an important structural constraint:
Deﬁnition 10.2
Let T be a cluster tree over a set of factors Φ. We denote by VT the vertices of T and by ET its
edges. We say that T has the running intersection property if, whenever there is a variable X such
running
intersection
property
that X ∈Ci and X ∈Cj, then X is also in every cluster in the (unique) path in T between Ci
and Cj.
Note that the running intersection property implies that Si,j = Ci ∩Cj.
Example 10.2
We can easily check that the running intersection property holds for the cluster tree of ﬁgure 10.1.
For example, G is present in C2 and in C4, so it is also present in the cliques on the path between
them: C3 and C5.
Intuitively, the running intersection property must hold for cluster trees induced by variable
elimination because a variable appears in every factor from the moment it is introduced (by
multiplying in a factor that mentions it) until it is summed out. We now prove that this property
holds in general.
Theorem 10.1
Let T be a cluster tree induced by a variable elimination algorithm over some set of factors Φ. Then
T satisﬁes the running intersection property.
Proof Let C and C′ be two clusters that contain X.
Let CX be the cluster where X is
eliminated. (If X is a query variable, we assume that it is eliminated in the last cluster.) We will
prove that X must be present in every cluster on the path between C and CX, and analogously
for C′, thereby proving the result.
First, we observe that the computation at CX must take place later in the algorithm’s execu-
tion than the computation at C: When X is eliminated in CX, all of the factors involving X are
multiplied into CX; the result of the summation does not have X in its domain. Hence, after
this elimination, Φ no longer has any factors containing X, so no factor generated afterward
will contain X in its domain.
By assumption, X is in the domain of the factor in C. We also know that X is not eliminated
in C. Therefore, the message computed in C must have X in its domain. By deﬁnition, the
recipient of X’s message, which is C’s upstream neighbor in the tree, multiplies in the message

348
Chapter 10. Clique Trees
from C. Hence, it will also have X in its scope. The same argument applies to show that all
cliques upstream from C will have X in their scope, until X is eliminated, which happens only
in CX. Thus, X must appear in all cliques between C and CX, as required.
A very similar proof can be used to show the following result:
Proposition 10.1
Let T be a cluster tree induced by a variable elimination algorithm over some set of factors Φ. Let
Ci and Cj be two neighboring clusters, such that Ci passes the message τi to Cj. Then the scope
of the message τi is precisely Ci ∩Cj.
The proof is left as an exercise (exercise 10.1).
It turns out that a cluster tree that satisﬁes the running intersection property is an extremely
useful data structure for exact inference in graphical models. We therefore deﬁne:
Deﬁnition 10.3
Let Φ be a set of factors over X. A cluster tree over Φ that satisﬁes the running intersection property
is called a clique tree (sometimes also called a junction tree or a join tree). In the case of a clique
clique tree
tree, the clusters are also called cliques.
clique
Note that we have already deﬁned one notion of a clique tree in deﬁnition 4.17. This double
deﬁnition is not an overload of terminology, because the two deﬁnitions are actually equivalent:
It follows from the results of this chapter that T is a clique tree for Φ (in the sense of deﬁni-
tion 10.3) if and only if it is a clique tree for a chordal graph containing HΦ (in the sense of
deﬁnition 4.17), and these properties are true if and only if the clique-tree data structure admits
variable elimination by passing messages over the tree.
We ﬁrst show that the running intersection property implies the independence statement,
which is at the heart of our ﬁrst deﬁnition of clique trees. Let T be a cluster tree over Φ,
and let HΦ be the undirected graph associated with this set of factors. For any sepset Si,j,
let W <(i,j) be the set of all variables in the scope of clusters in the Ci side of the tree, and
W <(j,i) be the set of all variables in the scope of clusters in the Cj side of the tree.
Theorem 10.2
T satisﬁes the running intersection property if and only if, for every sepset Si,j, we have that
W <(i,j) and W <(j,i) are separated in HΦ given Si,j.
The proof is left as an exercise (exercise 10.2).
To conclude the proof of the equivalence of the two deﬁnitions, it remains only to show that
the running intersection property for a tree T implies that each node in T corresponds to a
clique in a chordal graph H′ containing H, and that each maximal clique in H′ is represented in
T . This result follows from our ability to use any clique tree satisfying the running intersection
property to perform inference, as shown in this chapter.
10.2
Message Passing: Sum Product
In the previous section, we started out with an execution of the variable elimination algorithm,
and showed that it induces a clique tree. In this section, we go in the opposite direction. We
assume that we are given a clique tree as a starting point, and we will show how this data
structure can be used to perform variable elimination. As we will see, the clique tree is a very

10.2. Message Passing: Sum Product
349
1: C,D
P(D|C)
P(C)
P(G |I,D)
P(I)
P(S |I)
P(L|G)
P(J |L,S)
P(H |G,J)
4: G,H,J
2: D,I,G
3: G,I,S
5: G,J,L,S
D
G,I
G,S
G,J
Figure 10.2
Simpliﬁed clique tree T for the Extended Student network
useful and versatile data structure. For one, the same clique tree can be used as the basis for
many diﬀerent executions of variable elimination. More importantly, the clique tree provides a
data structure for caching computations, allowing multiple executions of variable elimination to
be performed much more eﬃciently than simply performing each one separately.
Consider some set of factors Φ over X, and assume that we are given a clique tree T over Φ,
as deﬁned in deﬁnition 4.17. In particular, T is guaranteed to satisfy the family preservation and
running intersection properties. As we now show, we can use the clique tree in several diﬀerent
ways to perform exact inference in graphical models.
10.2.1
Variable Elimination in a Clique Tree
One way of using a clique tree is simply as guidance for the operations of variable elimination.
The factors ψ are computed in the cliques, and messages are sent along the edges. Each clique
takes the incoming messages (factors), multiplies them, sums out one or more variables, and
sends an outgoing message to another clique. As we will see, the clique-tree data structure
dictates the operations that are performed on factors in the clique tree and a partial order over
these operations. In particular, if clique C′ requires a message from C, then C′ must wait with
its computation until C performs its computation and sends the appropriate message to C′.
We begin with an example and then describe the general algorithm.
10.2.1.1
An Example
Figure 10.2 shows one possible clique tree T for the Student network. Note that it is diﬀerent
from the clique tree of ﬁgure 10.1, in that nonmaximal cliques (C6 and C7) are absent. Nev-
ertheless, it is straightforward to verify that T satisﬁes both the family preservation and the
running intersection property. The ﬁgure also speciﬁes the assignment α of the initial factors
(CPDs) to cliques. Note that, in some cases (for example, the CPD P(I)), we have more than
one possible clique into which the factor can legally be assigned; as we will see, the algorithm
applies for any legal choice.
Our ﬁrst step is to generate a set of initial potentials associated with the diﬀerent cliques. The
initial potential ψi(Ci) is computed by multiplying the initial factors assigned to the clique Ci.
For example, ψ5(J, L, G, S) = φL(L, G) · φJ(J, L, S).
Now, assume that our task is to compute the probability P(J). We want to do the variable
elimination process so that J is not eliminated. Thus, we select as our root clique some clique
that contains J, for example, C5. We then execute the following steps:

350
Chapter 10. Clique Trees
d1→2(D):
∑Cy1(C1)
d2→3(G,I):
∑Dy2(C2) × d1→2
d1→2(D):
∑Cy1(C1)
d2→3(G,I):
∑Dy2(C2) × d1→2
d3→5(G,S):
∑Iy3(C3) × d2→3
d5→3(G,S):
∑J,Ly5(C5) × d4→5
d4→5(G,J):
∑Hy4(C4)
d4→5(G,J):
∑Hy4(C4)
(a)
(b)
1: C,D
4: G,H,J
2: D,I,G
3: G,I,S
5: G,J,L,S
1: C,D
4: G,H,J
2: D,I,G
3: G,I,S
5: G,J,L,S
Figure 10.3
Two diﬀerent message propagations with diﬀerent root cliques in the Student clique
tree: (a) C5 is the root; (b) C3 is the root.
1. In C1: We eliminate C by performing 
C ψ1(C, D). The resulting factor has scope D. We
send it as a message δ1→2(D) to C2.
2. In C2: We deﬁne β2(G, I, D) = δ1→2(D) · ψ2(G, I, D). We then eliminate D to get a
factor over G, I. The resulting factor is δ2→3(G, I), which is sent to C3.
3. In C3: We deﬁne β3(G, S, I) = δ2→3(G, I) · ψ3(G, S, I) and eliminate I to get a factor
over G, S, which is δ3→5(G, S).
4. In C4: We eliminate H by performing 
H ψ4(H, G, J) and send out the resulting factor
as δ4→5(G, J) to C5.
5. In C5: We deﬁne β5(G, J, S, L) = δ3→5(G, S) · δ4→5(G, J) · ψ5(G, J, S, L).
The factor β5 is a factor over G, J, S, L that encodes the joint distribution P(G, J, L, S): all
the CPDs have been multiplied in, and all the other variables have been eliminated. If we now
want to obtain P(J), we simply sum out G, L, and S.
We note that the operations in the elimination process could also have been done in another
order. The only constraint is that a clique get all of its incoming messages from its downstream
neighbors before it sends its outgoing message toward its upstream neighbor. We say that a
clique is ready when it has received all of its incoming messages. Thus, for example, C4 is ready
ready clique

10.2. Message Passing: Sum Product
351
at the very start of the algorithm, and the computation associated with it can be performed at
any point in the execution.
However, C2 is ready only after it receives its message from
C1. Thus, C1, C4, C2, C3, C5 is a legal execution ordering for a tree rooted at C5, whereas
C2, C1, C4, C3, C5 is not. Overall, the set of messages transmitted throughout the execution
of the algorithm is shown in ﬁgure 10.3a.
As we mentioned, the choice of root clique is not fully determined. To derive P(J), we could
have chosen C4 as the root. Let us see how the algorithm would have changed in that case:
1. In C1: The computation and message are unchanged.
2. In C2: The computation and message are unchanged.
3. In C3: The computation and message are unchanged.
4. In C5: We deﬁne β5(G, J, S, L) = δ3→5(G, S) · ψ5(G, J, S, L) and eliminate S and L. We
send out the resulting factor as δ5→4(G, J) to C4.
5. In C4: We deﬁne β4(H, G, J) = δ5→4(G, S) · ψ4(H, G, J).
We can now extract P(J) by eliminating H and G from β4(H, G, J).
In a similar way, we can apply exactly the same process to computing the distribution over
any other variable. For example, if we want to compute the probability P(G), we could choose
any of the cliques where it appears. If we use C3, for example, the computation in C1 and C2
is identical. The computation in C4 is the same as in the ﬁrst of our two executions: a message
is computed and sent to C5. In C5, we compute β5(G, J, S, L) = δ4→5(G, J)·ψ5(G, J, S, L),
and we eliminate J and L to produce a message δ5→3(G, S), which can then be sent to C3
and used in the operation:
β3(G, S, I) = δ2→3(G, I) · δ5→3(G, S) · ψ3(G, S, I).
Overall, the set of messages transmitted throughout this execution of the algorithm is shown in
ﬁgure 10.3b.
10.2.1.2
Clique-Tree Message Passing
We can now specify a general variable elimination algorithm that can be implemented via
message passing in a clique tree. Let T be a clique tree with the cliques C1, . . . , Ck. We begin
message passing
by multiplying the factors assigned to each clique, resulting in our initial potentials. We then
use the clique-tree data structure to pass messages between neighboring cliques, sending all
messages toward the root clique. We describe the algorithm in abstract terms; box 10.A provides
some important tips for eﬃcient implementation.
Recall that each factor φ ∈Φ is assigned to some clique α(φ). We deﬁne the initial potential
initial potential
of Cj to be:
ψj(Cj) =
Y
φ : α(φ)=j
φ.
Because each factor is assigned to exactly one clique, we have that
Y
φ
φ =
Y
j
ψj.

352
Chapter 10. Clique Trees
C1
C4
C5
C3
C2
C6
Figure 10.4
An abstract clique tree that is not chain-structured
Let Cr be the selected root clique. We now perform sum-product variable elimination over
the cliques, starting from the leaves of the clique tree and moving inward. More precisely, for
each clique Ci, we deﬁne Nbi to be the set of indexes of cliques that are neighbors of Ci. Let
pr(i) be the upstream neighbor of i (the one on the path to the root clique r). Each clique
Ci, except for the root, performs a message passing computation and sends a message to its
upstream neighbor Cpr(i).
The message from Ci to another clique Cj is computed using the following sum-product
sum-product
message passing
message passing computation:
δi→j =
X
Ci−Si,j
ψi ·
Y
k∈(Nbi−{j})
δk→i.
(10.2)
In words, the clique Ci multiplies all incoming messages from its other neighbors with its initial
clique potential, resulting in a factor ψ whose scope is the clique. It then sums out all variables
except those in the sepset between Ci and Cj, and sends the resulting factor as a message to
Cj.
This message passing process proceeds up the tree, culminating at the root clique. When the
root clique has received all messages, it multiplies them with its own initial potential. The result
is a factor called the beliefs, denoted βr(Cr). It represents, as we show,
beliefs
˜PΦ(Cr) =
X
X−Cr
Y
φ
φ.
The complete algorithm is shown in algorithm 10.1.
Example 10.3
Consider the abstract clique tree of ﬁgure 10.4, and assume that we have selected C6 as our root
clique. The numbering of the cliques denotes one possible ordering of the operations, with C1 being
the ﬁrst to compute its message. However, multiple other orderings are legitimate, for example,
2, 5, 1, 3, 4, 6; in general, any ordering that respects the ordering constraints {(2 ≺3), (3 ≺
4), (1 ≺4), (4 ≺6), (5 ≺6)} is a legal ordering for the message passing process.
We can use this algorithm to compute the marginal probability of any set of query nodes
Y which is fully contained in some clique. We select one such clique Cr to be the root, and
perform the clique-tree message passing toward that root. We then extract ˜PΦ(Y ) from the
ﬁnal potential at Cr by summing out the other variables Cr −Y .

10.2. Message Passing: Sum Product
353
Algorithm 10.1 Upward pass of variable elimination in clique tree
Procedure CTree-SP-Upward (
Φ,
// Set of factors
T ,
// Clique tree over Φ
α,
// Initial assignment of factors to cliques
Cr
// Some selected root clique
)
1
Initialize-Cliques
2
while Cr is not ready
3
Let Ci be a ready clique
4
δi→pr(i)(Si,pr(i)) ←SP-Message(i, pr(i))
5
βr ←ψr · Q
k∈NbCr δk→r
6
return βr
Procedure Initialize-Cliques (
)
1
for each clique Ci
2
ψi(Ci) ←Q
φj : α(φj)=i φj
3
Procedure SP-Message (
i,
// sending clique
j
// receiving clique
)
1
ψ(Ci) ←ψi · Q
k∈(Nbi−{j}) δk→i
2
τ(Si,j) ←P
Ci−Si,j ψ(Ci)
3
return τ(Si,j)
10.2.1.3
Correctness
We now prove that this algorithm, when applied to a clique tree that satisﬁes the family preser-
vation and running intersection property, computes the desired expressions over the messages
and the cliques.
In our algorithm, a variable X is eliminated only when a message is sent from Ci to a
neighboring Cj such that X ∈Ci and X ̸∈Cj. We ﬁrst prove the following result:
Proposition 10.2
Assume that X is eliminated when a message is sent from Ci to Cj. Then X does not appear
anywhere in the tree on the Cj side of the edge (i–j).
Proof The proof is a simple consequence of the running intersection property.
Assume by
contradiction that X appears in some other clique Ck that is on the Cj side of the tree. Then
Cj is on the path from Ci to Ck. But we know that X appears in both Ci and Ck but not
in Cj, violating the running intersection property.

354
Chapter 10. Clique Trees
Based on this result, we can provide a semantic interpretation for the messages used in the
clique tree. Let (i–j) be some edge in the clique tree. We use F≺(i→j) to denote the set of
factors in the cliques on the Ci-side of the edge and V≺(i→j) to denote the set of variables
that appear on the Ci-side but are not in the sepset.
For example, in the clique tree of
ﬁgure 10.2, we have that F≺(3→5) = {P(C), P(D | C), P(G | I, D), P(I), P(S | I)} and
V≺(3→5) = {C, D, I}. Intuitively, the message passed between the cliques Ci and Cj is the
product of all the factors in F≺(i→j), marginalized over the variables in the sepset (that is,
summing out all the others).
Theorem 10.3
Let δi→j be a message from Ci to Cj. Then:
δi→j(Si,j) =
X
V≺(i→j)
Y
φ∈F≺(i→j)
φ.
Proof The proof proceeds by induction on the length of the path from the leaves. For the
base case, the clique Ci is a leaf in the tree. In this case, the result follows from a simple
examination of the operations executed at the clique.
Now, consider a clique Ci that is not a leaf, and consider the expression
X
V≺(i→j)
Y
φ∈F≺(i→j)
φ.
(10.3)
Let i1, . . . , im be the neighboring cliques of Ci other than Cj. It follows immediately from
proposition 10.2 that V≺(i→j) is the disjoint union of V≺(ik→i) for k = 1, . . . , m and the
variables Y i eliminated at Ci itself. Similarly, F≺(i→j) is the disjoint union of the F≺(ik→i)
and the factors Fi from which ψi was computed. Thus equation (10.3) is equal to
X
Y i
X
V≺(i1→i)
. . .
X
V≺(im→i)


Y
φ∈F≺(i1→i)
φ

· · · · ·


Y
φ∈F≺(im→i)
φ

·

Y
φ∈Fi
φ

.
(10.4)
As we just showed, for each k, none of the variables in V≺(ik→i) appear in any of the other
factors. Thus, we can use equation (9.6) and push in the summation over V≺(ik→i) in equa-
tion (10.4), and obtain:
X
Y i

Y
φ∈Fi
φ

·
X
V≺(i1→i)


Y
φ∈F≺(i1→i)
φ

· · · · ·
X
V≺(im→i)


Y
φ∈F≺(im→i)
φ

.
(10.5)
Using the inductive hypothesis and the deﬁnition of ψi, this expression is equal to
X
Y i
ψi · δi1→i · · · · · δim→i,
(10.6)
which is precisely the operation used to compute the message δi→j.
This theorem is closely related to theorem 10.2, which tells us that a sepset divides the graph
into conditionally independent pieces. It is this conditional independence property that allows

10.2. Message Passing: Sum Product
355
the message over the sepset to summarize completely the information in one side of the clique
tree that is necessary for the computation in the other.
Based on this analysis, we can show that:
Corollary 10.1
Let Cr be the root clique in a clique tree, and assume that βr is computed as in the algorithm of
algorithm 10.1. Then
βr(Cr) =
X
X−Cr
˜PΦ(X).
As we discussed earlier, this algorithm applies both to Bayesian network and Markov network
inference. For a Bayesian network B, if Φ consists of the CPDs in B, reduced with some evidence
e, then βr(Cr) = PB(Cr, e). For a Markov network H, if Φ consists of the compatibility
functions deﬁning the network, then βr(Cr) = ˜PΦ(Cr). In both cases, we can obtain the
probability over the variables in Cr as usual, by normalizing the resulting factor to sum to 1. In
the Markov network, we can also obtain the value of the partition function simply by summing
up all of the entries in the potential of the root clique βr(Cr).
10.2.2
Clique Tree Calibration
We have shown that we can use the same clique tree to compute the probability of any variable
in X. In many applications, we often wish to estimate the probability of a large number of
variables.
For example, in a medical-diagnosis setting, we generally want the probability of
several possible diseases. Furthermore, as we will see, when learning Bayesian networks from
partially observed data, we always want the probability distributions over each of the unobserved
variables in the domain (and their parents).
Therefore, let us consider the task of computing the posterior distribution over every random
variable in the network. The most naive approach is to do inference separately for each variable.
Letting c be the cost of a single execution of clique tree inference, the total cost of this algorithm
is nc. An approach that is slightly less naive is to run the algorithm once for every clique, making
it the root. The total cost of this variant is Kc, where K is the number of cliques. However, it
turns out that we can do substantially better than either of these approaches.
Let us revisit our clique tree of ﬁgure 10.2 and consider the three diﬀerent executions of the
clique tree algorithm that we described: one where C5 is the root, one where C4 is the root,
and one where C3 is the root. As we pointed out, the messages sent from C1 to C2 and from
C2 to C3 are the same in all three executions. The message sent from C4 to C5 is the same
in both of the executions where it appears. In the second of the three executions, there simply
is no message from C4 to C5 — the message goes the other way, from C5 to C4.
More generally, consider two neighboring cliques Ci and Cj in some clique tree. It follows
from theorem 10.3 that the value of the message sent from Ci to Cj does not depend on
speciﬁc choice of root clique: As long as the root clique is on the Cj-side, exactly the same
message is sent from Ci to Cj. The same argument applies if the root is on the Ci-side. Thus,
in all executions of the clique tree algorithm, whenever a message is sent between two cliques
in the same direction, it is necessarily the same. Thus, for any given clique tree, each edge has
two messages associated with it: one for each direction of the edge. If we have a total of c
cliques, there are c −1 edges in the tree; therefore, we have 2(c −1) messages to compute.

356
Chapter 10. Clique Trees
1: C,D
4: G,H,J
2: D,I,G
3: G,I,S
5: G,J,L,S
d1→2(D):
∑Cy1(C1)
d2→3(G,I):
∑Dy2(C2) × d1→2
d1→2(D):
∑Cy1(C1)
d2→3(G,I):
∑Dy2(C2) × d1→2
d3→5(G,S):
∑J,Ly3(C3) × d2→3
d3→5(G,S):
∑J,Ly3(C3) × d2→3
d5→3(G,S):
∑J,Ly5(C5) × d4→5
d3→2(G,I):
∑Sy3(C3) × d5→3
d5→3(G,S):
∑J,Ly5(C5) × d4→5
d4→5(G,J):
∑Hy4(C4)
d4 →5(G,J):
∑Hy4(C4)
1: C,D
4: G,H,J
2: D,I,G
3: G,I,S
5: G,J,L,S
(a)
(b)
Figure 10.5
Two steps in a downward pass in the Student network
We can compute both messages for each edge by the following simple asynchronous algorithm.
Recall that a clique can transmit a message upstream toward the root when it has all of the
messages from its downstream neighbors. We can generalize this concept as follows:
Deﬁnition 10.4
Let T be a clique tree. We say that Ci is ready to transmit to a neighbor Cj when Ci has
ready clique
messages from all of its neighbors except from Cj.
When Ci is ready to transmit to Cj, it can compute the message δi→j(Si,j) by multiplying its
initial potential with all of its incoming messages except the one from Cj, and then eliminate the
variables in Ci −Si,j. In eﬀect, this algorithm uses yet another layer of dynamic programming
dynamic
programming
to avoid recomputing the same message multiple times.
Algorithm 10.2 shows the full procedure, often called sum-product belief propagation.
As
sum-product
belief
propagation
written, the algorithm is deﬁned asynchronously, with each clique sending a message as soon
as it is ready. One might wonder why this process is guaranteed to terminate, that is, why
there is always a clique that is ready to transmit to some other clique. In fact, the message
passing process performed by the algorithm is equivalent to a much more systematic process
that consists of an upward pass and a downward pass. In the upward pass, we ﬁrst pick a
upward pass
downward pass
root and send all messages toward the root. When this process is complete, the root has all
messages.
Therefore, it can now send the appropriate message to all of its children.
This

10.2. Message Passing: Sum Product
357
Algorithm 10.2 Calibration using sum-product message passing in a clique tree
Procedure CTree-SP-Calibrate (
Φ,
// Set of factors
T
// Clique tree over Φ
)
1
Initialize-Cliques
2
while exist i, j such that i is ready to transmit to j
3
δi→j(Si,j) ←SP-Message(i, j)
4
for each clique i
5
βi ←ψi · Q
k∈Nbi δk→i
6
return {βi}
algorithm continues until the leaves of the tree are reached, at which point no more messages
need to be sent. This second phase is called the downward pass. The asynchronous algorithm
is equivalent to this systematic algorithm, except that the root is simply the ﬁrst clique that
happens to obtain messages from all of its neighbors. In an actual implementation, we might
want to schedule this process more explicitly. (At the very least, the algorithm would check in
message
scheduling
line 2 that a message is not computed more than once.)
Example 10.4
Figure 10.3a shows the upward pass of the clique tree algorithm when C5 is the root. Figure 10.5a
shows a possible ﬁrst step in a downward pass, where C5 sends a message to its child C3, based
on the message from C4 and its initial potential. As soon as a child of the root receives a message,
it has all of the information it needs to send a message to its own children. Figure 10.5b shows C3
sending the downward message to C2.
At the end of this process, we compute the beliefs for all cliques in the tree by multiplying
beliefs
the initial potential with each of the incoming messages. The key is to note that the messages
used in the computation of βi are precisely the same messages that would have been used in a
standard upward pass of the algorithm with Ci as the root. Thus, we conclude:
Corollary 10.2
Assume that, for each clique i, βi is computed as in the algorithm of algorithm 10.2. Then
βi(Ci) =
X
X−Ci
˜PΦ(X).
Note that it is important that Ci compute the message to a neighboring clique Cj based
on its initial potential ψi and not its modiﬁed potential βi.
The latter already integrates
information from j. If the message were computed based on this latter potential, we would be
double-counting the factors assigned to Cj (multiplying them twice into the joint).
When this process concludes, each clique contains the marginal (unnormalized) probability
over the variables in its scope.
As we discussed, we can compute the marginal probability
over a particular variable X by selecting a clique whose scope contains X, and eliminating the
redundant variables in the clique. A key point is that the result of this process does not depend
on the clique we selected. That is, if X appears in two cliques, they must agree on its marginal.

358
Chapter 10. Clique Trees
Deﬁnition 10.5
Two adjacent cliques Ci and Cj are said to be calibrated if
calibrated
X
Ci−Si,j
βi(Ci) =
X
Cj−Si,j
βj(Cj).
A clique tree T is calibrated if all pairs of adjacent cliques are calibrated. For a calibrated clique
tree, we use the term clique beliefs for βi(Ci) and sepset beliefs for
beliefs
µi,j(Si,j) =
X
Ci−Si,j
βi(Ci) =
X
Cj−Si,j
βj(Cj).
(10.7)

The main advantage of the clique tree algorithm is that it computes the posterior
probability of all variables in a graphical model using only twice the computation of
the upward pass in the same tree. Letting c once again be the execution cost of message
passing in a clique tree to one root, the cost of this algorithm is 2c. By comparison, recall that
the cost of doing a separate computation for each variable is nc and a separate computation
for each root clique is Kc, where K is the number of cliques. In most cases, the savings are
considerable, making the clique tree algorithm the algorithm of choice in situations where we
want to compute the posterior of multiple query variables.
Box 10.A — Skill: Eﬃcient Implementation of Factor Manipulation Algorithms. While sim-
ple conceptually, the implementation of algorithms involving manipulation of factors can be sur-
prisingly subtle. In particular, diﬀerent design decisions can lead to orders-of-magnitude

diﬀerences in performance, as well as diﬀerences in the accuracy of the results. We now
discuss some of the key design decisions in these algorithms. We note that the methods we describe
here are equally applicable to the algorithms in many of the other chapters in the book, including
the variable elimination algorithm of chapter 9, the exact and approximate sum-product message
passing algorithms of chapters 10 and 11, and many of the MAP algorithms of chapter 13.
The ﬁrst key decision is the representation of our basic data structure: a factor, or a multidimen-
sional table, with an entry for each possible assignment to the variables. One standard technique
for storing multidimensional tables is to ﬂatten them into a single array in computer memory. For
each variable, we also store its cardinality, and its stride, or step size in the factor. For example,
stride
given a factor φ(A, B, C) over variables A, B, and C, with cardinalities 2, 2, and 3, respectively,
we can represent the factor in memory by the array
phi[0 . . . 11] =

φ(a1, b1, c1), φ(a2, b1, c1), φ(a1, b2, c1), . . . , φ(a2, b2, c3)
	
.
Here the stride for variable A is 1, for B is 2 and for C is 4. If we add a fourth variable, D, its stride
would be 12, since we would need to step over twelve entries before reaching the next assignment
to D. Notice how, using each variable’s stride, we can easily go from a variable assignment to a
corresponding index into the factor array
index =
X
i
assignment[i] · phi.stride[i]

10.2. Message Passing: Sum Product
359
Algorithm 10.A.1 — Eﬃcient implementation of a factor product operation.
Procedure Factor-Product (
phi1 over scope X1,
phi2 over scope X2
// Factors represented as a ﬂat array with strides for the vari-
ables
)
1
j ←0, k ←0
2
for l = 0, . . . , |X1 ∪X2|
3
assignment[l] ←0
4
for i = 0, . . . , |Val(X1 ∪X2)| −1
5
psi[i] ←phi1[j] · phi2[k]
6
for l = 0, . . . , |X1 ∪X2|
7
assignment[l] ←assignment[l] + 1
8
if assignment[l] = card[l] then
9
assignment[l] ←0
10
j ←j −(card[l] −1) · phi1.stride[l]
11
k ←k −(card[l] −1) · phi2.stride[l]
12
else
13
j ←j + phi1.stride[l]
14
k ←k + phi2.stride[l]
15
break
16
return (psi)
and vice versa
assignment[i] = ⌊index/phi.stride[i]⌋mod card[i]
With this factor representation, we can now design a library of operations: product, marginal-
ization, maximization, reduction, and so forth. Since many inference algorithms involve multiple
iterations over a series of factor operations, it is important that these be high-performance. One of
the key design decisions is indexing the appropriate entries in each factor for the operations that we
wish to perform. (In fact, when one uses a naive implementation of index computations, one often
discovers that 90 percent of the running time is spent on that task.)
Algorithm 10.A.1 provides an example for the product between two arbitrary factors. Here we
factor product
deﬁne phi.stride[X] = 0 if X ̸∈Scope[φ].
The inner loop (over l) advances to the next
assignment to the variables in ψ and calculates indexes into each other factor array on the ﬂy. It
can be understood by considering the equation for computing index shown earlier. Similar on-
the-ﬂy index calculations can be applied for other factor operations. We leave these as an exercise
(exercise 10.3).
For iterative algorithms or multiple queries, where the same operation (on diﬀerent data) is
performed a large number of times, it may be beneﬁcial to cache these index mappings for later
use. Note, however, that the index mappings require the same amount of storage as the factors
themselves, that is, are exponential in the number of variables. Thus, this design choice oﬀers a

360
Chapter 10. Clique Trees
direct trade-oﬀbetween memory usage and speed, especially in view of the fact that the index
computations require approximately the same amount of work as the factor operation itself. Since
performance of main memory is orders of magnitudes faster than secondary storage (disk), when
memory limitations are an issue, it is better not to cache index mappings for large problems. One
exception is template models, where savings can be made by reusing the same indexes for diﬀerent
instantiations of the factor templates.
An additional trick in reducing the computational burden is to preallocate and reuse memory
for factor storage. Allocating memory is a relatively expensive procedure, and one does not want
to waste time on this task inside a tight loop. To illustrate this point, we consider the example of
variable elimination for computing ψ(A, D) as
ψ(A, D) =
X
B,C
φ1(A, B)φ2(B, C)φ3(C, D) =
X
B
φ1(A, B)
X
C
φ2(B, C)φ3(C, D).
Here we need to compute three intermediate factors: τ1(B, C, D) = φ2(B, C)φ3(C, D); τ2(B, D) =
P
C τ1(B, C, D); and τ3(A, B, D) = φ1(A, B)τ2(B, D). Notice that, once τ2(B, D) has been
calculated, we no longer need the values in τ1(B, C, D). By initially allocating memory large
enough to hold the larger of τ1(B, C, D) and τ3(A, B, D), we can use the same memory for
both. Because every operation in a variable elimination or message passing algorithm requires the
computation of one or more intermediate factors, some of which are much larger than the desired
end product, the savings in both time (preallocation) and memory (reusage) can be signiﬁcant.
We now turn our attention to numerical considerations.
Operations such as factor product
involve multiplying many small numbers together, which can lead to underﬂow problems due to
ﬁnite precision arithmetic. The problem can be alleviated somewhat by renormalizing the factor
after each operation (so that the maximum entry in the factor is one); this operation leaves the
results to most queries unchanged (see exercise 9.3). However, if each entry in the factor is computed
as the product of many terms, underﬂow can still occur. An alternative solution is to perform
the computation in log-space, replacing multiplications with additions; this transformation allows
log-space
for greater machine precision to be utilized. Note that marginalization, which requires that we
factor
marginalization
sum entries, cannot be performed in log-space; it requires exponentiating each entry in the factor,
performing the marginalization, and taking the log of the result. Since moving from log-space
to probability-space incurs a signiﬁcant decrease in dynamic range, factors should be normalized
before applying this transform. One standard trick is to shift every entry by the maximum entry
phi[i] ←exp {logPhi[i] −c} ,
where c = maxi logPhi[i]; this transformation ensures that the resulting factor has a maximum
entry of one and prevents overﬂow.
We note that there are some caveats to operating in log-space. First, one may incur a performance
hit: Floating point multiplication is no slower than ﬂoating point addition, but the transformation
to and from log-space, as required for marginalization, can take a signiﬁcant proportion of the total
processing time. This caveat does not apply to algorithms such as max-product, where maximization
can be performed in log-space; indeed, these algorithms are almost always implemented as max-
sum. Moreover, log-space operations require care in handling nonpositive factors (that is, factors
with some zero entries).
Finally, at a higher level, as with any software implementation, there is always a trade-oﬀbetween
speed, memory consumption, and reusability of the code. For example, software specialized for the

10.2. Message Passing: Sum Product
361
case of pairwise potentials over a grid will almost certainly outperform code written for general
graphs with arbitrary potentials. However, the small performance hit in using well designed general
purpose code often outweighs the development eﬀort required to reimplement algorithms for each
specialized application. However, as always, it is also important not to try to optimize code too
early. It is more beneﬁcial to write and proﬁle the code, on real examples, to determine what
operations are causing bottlenecks. This allows the development eﬀort to be targeted to areas that
can yield the most gain.
10.2.3
A Calibrated Clique Tree as a Distribution
A calibrated clique tree is more than simply a data structure that stores the results of probabilistic
inference for all of the cliques in the tree. As we now show, it can also be viewed as an alternative
representation of the measure ˜PΦ.
At calibration, we have that:
βi = ψi ·
Y
k∈Nbi
δk→i.
(10.8)
We also have that:
µi,j(Si,j)
=
X
Ci−Si,j
βi(Ci)
=
X
Ci−Si,j
ψi ·
Y
k∈Nbi
δk→i
=
X
Ci−Si,j
ψi · δj→i
Y
k∈(Nbi−{j})
δk→i
=
δj→i
X
Ci−Si,j
ψi ·
Y
k∈(Nbi−{j})
δk→i
=
δj→iδi→j,
(10.9)
where the fourth equality holds because no variable in the scope of δj→i is involved in the
summation.
We can now show the following important result:
Proposition 10.3
At convergence of the clique tree calibration algorithm, we have that:
˜PΦ(X) =
Q
i∈VT βi(Ci)
Q
(i–j)∈ET µi,j(Si,j).
(10.10)
Proof Using equation (10.8), the numerator in the right-hand side of equation (10.10) can be
rewritten as:
Y
i∈VT
ψi(Ci)
Y
k∈Nbi
δk→i.

362
Chapter 10. Clique Trees
Assignment
MargC
a0
b0
d0
600, 000
a0
b0
d1
300, 030
a0
b1
d0
5, 000, 500
a0
b1
d1
1, 000
a1
b0
d0
200
a1
b0
d1
1, 000, 100
a1
b1
d0
100, 010
a1
b1
d1
200, 000
Assignment
MargA,C
b0
d0
600, 200
b0
d1
1, 300, 130
b1
d0
5, 100, 510
b1
d1
201, 000
Assignment
MargA
b0
c0
d0
300, 100
b0
c0
d1
1, 300, 000
b0
c1
d0
300, 100
b0
c1
d1
130
b1
c0
d0
510
b1
c0
d1
100, 500
b1
c1
d0
5, 100, 000
b1
c1
d1
100, 500
β1(A, B, D)
µ1,2(B, D)
β2(B, C, D)
Figure 10.6
The clique and sepset beliefs for the Misconception example.
Using equation (10.9), the denominator can be rewritten as:
Y
(i–j)∈ET
δi→jδj→i.
Each message δi→j appears exactly once in the numerator and exactly once in the denominator,
so that all messages cancel. The remaining expression is simply:
Y
i∈VT
ψi(Ci) = ˜PΦ.
Thus, via equation (10.10), the clique and sepsets beliefs provide a reparameterization of the
reparameteriza-
tion
unnormalized measure. This property is called the clique tree invariant, for reasons which will
clique tree
invariant
become clear later on in this chapter.
Another intuition for this result can be obtained from the following example:
Example 10.5
Consider a clique tree obtained from Markov network A—B—C—D, with an appropriate set of
factors Φ. Our clique tree in this case would have three cliques C1 = {A, B}, C2 = {B, C},
and C3 = {C, D}. When the clique tree is calibrated, we have that β1(A, B) = ˜PΦ(A, B) and
β2(B, C) = ˜PΦ(B, C). From the conditional independence properties of this distribution, we
have that
˜PΦ(A, B, C) = ˜PΦ(A, B) ˜PΦ(C | B),
and
˜PΦ(C | B) = β2(B, C)
˜PΦ(B)
.
As β2(B, C) = ˜PΦ(B, C), we can obtain ˜PΦ(B) by marginalizing β2(B, C). Thus, we can write:
˜PΦ(A, B, C)
=
β1(A, B)
β2(B, C)
P
C β2(B, C)
=
β1(A, B)β2(B, C)
P
C β2(B, C)
.

10.2. Message Passing: Sum Product
363
In fact, when the two cliques are calibrated, they must agree on the marginal of B. Thus, the
expression in the denominator can equivalently be replaced by P
A β1(A, B).
Based on this analysis, we now formally deﬁne the distribution represented by a clique tree:
Deﬁnition 10.6
We deﬁne the measure induced by a calibrated tree T to be:
clique tree
measure
QT =
Q
i∈VT βi(Ci)
Q
(i–j)∈ET µi,j(Si,j),
(10.11)
where
µi,j =
X
Ci−Si,j
βi(Ci) =
X
Cj−Si,j
βj(Cj).
Example 10.6
Consider, for example, the Markov network of example 3.8, whose joint distribution is shown in
ﬁgure 4.2. One clique tree for this network consists of the two cliques {A, B, D} and {B, C, D},
with the sepset {B, D}. The ﬁnal potentials and sepset for this example are shown in ﬁgure 10.6.
It is straightforward to conﬁrm that the clique tree is indeed calibrated. One can also verify that
this clique tree provides a reparameterization of the original distribution. For example, consider the
entry ˜PΦ(a1, b0, c1, d0) = 100. According to equation (10.10), the clique tree measure is:
β1(a1, b0, d0)β2(b0, c1, d0)
µ1,2(b0, d0)
= 200 · 300, 100
600, 200
= 100,
as required.
Our analysis so far shows that for a set of calibrated potentials derived from clique tree
inference, we have two properties: the clique tree measure is ˜PΦ and the ﬁnal beliefs are the
marginals of ˜PΦ. As we now show, these two properties coincide for any calibrated clique tree.
Theorem 10.4
Let T be a clique tree over Φ, and let βi(Ci) be a set of calibrated potentials for T . Then,
˜PΦ(X) ∝QT if and only if, for each i ∈VT , we have that βi(Ci) ∝˜PΦ(Ci).
Proof Let r be any clique in T , which we choose to be the root. Deﬁne the descendant cliques
of a clique Ci to be the cliques that are downstream from Ci relative to Cr; the nondescendant
cliques are then the remaining cliques (other than Ci). Let X be the variables in the scope of
the nondescendant cliques. It follows immediately from theorem 10.2 that
˜PΦ |= (Ci ⊥X | Si,pr(i)).
From this, we obtain, using the standard chain-rule argument, that:
˜PΦ(X) = ˜PΦ(Cr) ·
Y
i̸=r
˜PΦ(Ci | Si,pr(i)).
We can rewrite equation (10.11) as a similar product, using the same root:
QT (X) = βr(Cr) ·
Y
i̸=r
βi(Ci | Si,pr(i)).

364
Chapter 10. Clique Trees
The “if” direction now follows from direct substitution of βi for each ˜PΦ(Ci).
To prove the “only if” direction, we note that each of the terms βi(Ci | Si,pr(i)) is a
conditional distribution; hence, if we marginalize out the variables not in Cr in the distribution
QT , each of these conditional distributions marginalizes to 1, and so we are left with QT (Cr) =
βr(Cr). It now follows that if ˜PΦ ∝QT , then ˜PΦ(Cr) ∝QT (Cr) = βr(Cr). Because this
argument applies to any choice of root clique, we have proved that this equality holds for every
clique.

Thus, we can view the clique tree as an alternative representation of the joint measure,
one that directly reveals the clique marginals. As we will see, this view turns out to be very
useful, both in the next section and in chapter 11.
10.3
Message Passing: Belief Update
The previous section showed one approach to message passing in clique trees, based on the
same ideas of variable elimination that we discussed in chapter 9. In this section, we present a
related approach, but one that is based on very diﬀerent intuitions. We begin by describing an
alternative message passing scheme that is diﬀerent from but mathematically equivalent to that
of the previous section. We then show how this new approach can be viewed as operations on
the reparameterization of the distribution in terms of the clique and sepset beliefs {βi(Ci)}i∈VT
and {µi,j(Si,j)}(i–j)∈ET .
Each message passing step will change this representation while
leaving it a reparameterization of ˜PΦ.
10.3.1
Message Passing with Division
Consider again the message passing process used in CTree-SP-Calibrate (algorithm 10.2). There,
two messages are passed along each link (i–j). Assume, without loss of generality, that the ﬁrst
message is passed from Cj to Ci. A return message from Ci to Cj is passed when Ci has
received messages from all of its other neighbors.
At this point, Ci has all of the necessary information to compute its ﬁnal potential.
It
multiplies the initial potential with the incoming messages from all of its neighbors:
βi = ψi ·
Y
k∈Nbi
δk→i.
(10.12)
As we discussed, this ﬁnal potential is not used in computing the message to Cj: this potential
already incorporates the information (message) passed from Cj; if we used it when computing
the message to Cj, this information would be double-counted. Thus, the message from Ci to
Cj is computed in a way that omits the information obtained from Cj: we multiply the initial
potential with all of the messages except for the message from Ci, and then marginalize over
the sepset (equation (10.2)).
A diﬀerent approach to computing the same expression is to multiply in all of the messages,
and then divide the resulting factor by δj→i. To make this notion precise, we must deﬁne a
factor-division operation:

10.3. Message Passing: Belief Update
365
0.625
0.25
0
0
0.5
0.75
0.5
0.2
0
0.8
0
0.6
0
0.3
0.45
a1
a1
a2
a2
a3
a3
a1
a2
a3
b1
b2
b1
b2
b1
b2
a1
a1
a2
a2
a3
a3
b1
b2
b1
b2
b1
b2
Figure 10.7
An example of factor division
Deﬁnition 10.7
Let X and Y be disjoint sets of variables, and let φ1(X, Y ) and φ2(Y ) be two factors. We deﬁne
the division φ1
φ2 to be a factor ψ of scope X, Y deﬁned as follows:
factor division
ψ(X, Y ) = φ1(X, Y )
φ2(Y )
,
where we deﬁne 0/0 = 0.
Note that, as in the case of other factor operations, factor division is done component by
component. Figure 10.7 shows an example. Also note that the operation is not well deﬁned if
the denominator is zero and the numerator is not.
We now see that we can compute the expression of equation (10.2) by computing the beliefs
as in equation (10.12), and then dividing by the remaining message:
δi→j =
P
Ci−Si,j βi
δj→i
.
(10.13)
Example 10.7
Let us return to the simple clique tree in example 10.5, and assume that C2 serves as the (de
facto) root, so that we ﬁrst pass messages from C1 to C2 and from C3 to C2. The message
δ1→2 is computed as P
A ψ1(A, B). Using the variable elimination message (CTree-SP-Calibrate),
we pass a return message δ2→1(B) = P
C ψ2(B, C)δ3→2(C). Alternatively, we can compute
β2(B, C) = δ1→2(B) · δ3→2(C) · ψ2(B, C), and then send a message
P
C β2(B, C)
δ1→2(B)
=
X
C
β2(B, C)
δ1→2(B) =
X
C
ψ2(B, C) · δ3→2(C).
Thus, the two approaches are equivalent.
Based on this insight, we can deﬁne the sum-product-divide message passing scheme, where
sum-product-
divide
each clique Ci maintains its fully updated current beliefs βi, which are deﬁned as in equa-
beliefs
tion (10.8). Each sepset also maintains its beliefs µi,j deﬁned as the product of the messages
in both directions, as in equation (10.9). We now show that the entire message passing process

366
Chapter 10. Clique Trees
can be executed in an equivalent way in terms of the clique and sepset beliefs, without having
to remember the initial potentials ψi or to compute explicitly the messages δi→j.
The message passing process follows the lines of example 10.7. Each clique Ci initializes βi
as ψi and then updates it by multiplying with message updates received from its neighbors.
Each sepset Si,j maintains µi,j as the previous message passed along the edge (i–j), regardless
of the direction. This message is used to ensure that we do not double-count: Whenever a new
message is passed along the edge, it is divided by the old message, eliminating the previous
message from the update to the clique. Somewhat surprisingly, as we will show, the message
passing operation is correct regardless of the clique that sent the last message on the edge.
Intuitively, once the message is passed, its information is incorporated into both cliques; thus,
each needs to divide by it when passing a message to the other. We can view this algorithm
as maintaining a set of belief over the cliques in the tree.
The message passing operation
takes the beliefs of one clique and uses them to update the beliefs of a neighbor. Thus, we
call this algorithm belief-update message passing; it is also known as the Lauritzen-Spiegelhalter
belief-update
algorithm.
Example 10.8
Continuing with example 10.7, assume that C2 initially passes an uninformed message to C3:
σ2→3 = P
B ψ2(B, C). This message multiplies the beliefs about C3, so that, at this point:
β3(C, D) = ψ3(C, D)
X
B
ψ2(B, C).
This message is also stored in the sepset as µ2,3. Now, assume that C3 sends a message to C2:
σ3→2(C) = P
D β3(C, D). This message is divided by µ2,3, so the actual update for C2 is:
σ3→2(C)
µ2,3(C)
=
P
D β3(C, D)
µ2,3(C)
=
P
D ψ3(C, D)µ2,3(C)
µ2,3(C)
=
X
D
ψ3(C, D).
This expression is precisely the update that C2 would have received from C3 in the case where C2
does not ﬁrst send an uninformed message. At this point, the message stored in the sepset is
X
D
β3(C, D) =
X
D
 
ψ3(C, D) ·
X
B
ψ2(B, C)
!
.
Assume that at the next step C2 receives a message from C1, containing P
A ψ1(A, B). The
sepset S1,2 contains a message that is identically 1, so that this message is transmitted as is. At
this point, C2 has received informed messages from both sides and is therefore informed. Indeed,
we have shown that:
β2(B, C) = ψ2(B, C) ·
X
A
ψ1(A, B) ·
X
D
ψ3(C, D).
as required.

10.3. Message Passing: Belief Update
367
Algorithm 10.3 Calibration using belief propagation in clique tree
Procedure CTree-BU-Calibrate (
Φ,
// Set of factors
T
// Clique tree over Φ
)
1
Initialize-CTree
2
while exists an uninformed clique in T
3
Select (i–j) ∈ET
4
BU-Message(i, j)
5
return {βi}
Procedure Initialize-CTree (
)
1
for each clique Ci
2
βi ←Q
φ : α(φ)=i φ
3
for each edge (i–j) ∈ET
4
µi,j ←1
Procedure BU-Message (
i,
// sending clique
j
// receiving clique
)
1
σi→j ←P
Ci−Si,j βi
2
// marginalize the clique over the sepset
3
βj ←βj · σi→j
µi,j
4
µi,j ←σi→j
The precise algorithm is shown in algorithm 10.3. Note that, as written, the message passing
algorithm is underspeciﬁed: in line 3, we can select any pair of cliques Ci and Cj between
which we will pass a message.
Interestingly, we can make this choice arbitrarily, without
damaging the correctness of the algorithm. For example, if Ci (for some reason) passes the
same message to Cj a second time, the process of dividing out by the stored message reduces
the message actually passed to 1, so that it has no inﬂuence.
Furthermore, if Ci passes
a message to Cj based on partial information (that is, without taking into consideration all
of its incoming messages), and then resends a more updated message later on, the eﬀect is
identical to simply sending the updated message once. Moreover, at convergence, regardless
of the message passing steps used, we necessarily have a calibrated clique tree. This property
follows from the fact that, in order for all message updates to have no eﬀect, we need to have

368
Chapter 10. Clique Trees
σi→j = µi,j = σj→i for all i, j, and so:
X
Ci−Si,j
βi = µi,j =
X
Cj−Si,j
βj.
Thus, at convergence, each pair of neighboring cliques i, j must agree on the variables in sepset,
and the message µi,j is precisely the sepset marginal. These properties also follow from the
equivalence between belief-update message passing and sum-product message passing, which
we show next.
10.3.2
Equivalence of Sum-Product and Belief Update Messages
So far, although we used sum-product message propagation to motivate the deﬁnition of the
belief update steps, we have not shown a direct connection between them. We now show a
simple and elegant equivalence between the two types of message passing operations. From
this result, it immediately follows that belief-update message passing is guaranteed to converge
to the correct marginals.
Our proof is based on equation (10.8) and equation (10.9), which provide a mapping between
the sum-product and belief-update representations. We consider corresponding runs of the two
algorithms in which an identical sequence of message passing steps is executed. We show that
these two properties hold as an invariant between the data structures maintained by the two
algorithms. The invariant holds initially, and it is maintained throughout the corresponding
runs.
Theorem 10.5
Consider a set of sum-product initial potentials {ψi
:
i ∈VT } and messages {δi→j, δj→i
:
(i–j) ∈ET }, and a set of belief-update beliefs {βi : i ∈VT } and messages {µi,j : (i–j) ∈ET },
for which equation (10.8) and equation (10.9) hold. For any pair of neighboring cliques Ci, Cj,
let {δ′
i→j, δ′
j→i : (i–j) ∈ET } be the set of sum-product messages following an application of
SP-Message(i, j), and {β′
i : Ci ∈T }, {µ′
i,j : (i–j) ∈ET }, be the set of belief-update beliefs
following an application of BU-Message(i, j). Then equation (10.8) and equation (10.9) also hold
for the new beliefs δ′
i→j, β′
i, µ′
i,j.
The proof uses simple algebraic manipulation, and it is left as an exercise (exercise 10.4).
This equivalence implies another result that will prove important in subsequent developments:
Corollary 10.3
In an execution of belief-update message passing, the clique tree invariant equation (10.10) holds
initially and after every message passing step.
Proof The proof of proposition 10.3 relied only on equation (10.8) and equation (10.9). Because
these two equalities hold in every step of the belief-update message passing algorithm, we have
that the clique tree invariant also holds continuously.
This equivalence also allows us to deﬁne a message schedule that guarantees convergence to
the correct clique marginals in two passes: We simply follow the same upward-downward-pass
schedule used in CTree-SP-Calibrate, using any (arbitrarily chosen) root clique Cr.

10.3. Message Passing: Belief Update
369
10.3.3
Answering Queries
As we have seen, a calibrated clique tree contains the answer to multiple queries at once: the
posterior probability of any set of variables that are present together in a single clique.
A
particular type of query that turns out to be important in this setting is the computation of
the posterior for families of variables in a probabilistic network: a node and its parents in the
context of Bayesian networks, or a clique in a Markov network. The family preservation property
for cluster graphs (and hence for clique trees) implies that a family must be a subset of some
cluster in the cluster graph.
In addition to these queries, which we get immediately as a by-product of calibration, we can
also use a clique tree for other queries. We describe the algorithm for these queries in terms
of a calibrated clique tree that satisﬁes the clique tree invariant. Due to the equivalence of
sum-product and belief-update message passing, we can obtain such a clique tree using either
method.
10.3.3.1
Incremental Updates
Consider a situation where, at some point in time, we have a certain set of observations, which
we use to condition our distribution and reach conclusions. At some later time, we obtain
additional evidence, and want to update our conclusions accordingly. This type of situation,
where we want to perform incremental update is very common in a wide variety of settings. For
incremental
update
example, in a medical setting, we often perform diagnosis on the basis of limited evidence; the
initial diagnosis helps us decide which tests to perform, and the results need to be incorporated
into our diagnosis.
The most naive approach to dealing with this task is simply to condition the initial factors
(for example, the CPDs) on all of the evidence, and then redo the calibration process from the
beginning, starting from these factors. A somewhat more eﬃcient approach is based on the
view of the clique tree as representing the distribution ˜PΦ.
Assume that our initial distribution ˜PΦ (prior to the new information) is represented via a set
of factors Φ, as in equation (10.1). Given some evidence Z = z, we can obtain ˜PΦ(X, Z = z) by
zeroing out the entries in the unnormalized distribution that are inconsistent with the evidence
Z = z. We can accomplish this eﬀect by multiplying ˜PΦ with an additional factor which is
the indicator function 11{Z = z}. More precisely, assume that our current distribution over X
is deﬁned by a set of factors Φ, so that
˜PΦ(X) =
Y
φ∈Φ
φ.
Then,
˜PΦ(X, Z = z) = 11{Z = z} ·
Y
φ∈Φ
φ.
Let ˜P ′
Φ(X) = ˜PΦ(X, Z = z).
Now, assume that we have a clique tree (calibrated or not) that represents this distribution
using the clique tree invariant. That is:
˜PΦ(X) = QT =
Q
i∈VT βi(Ci)
Q
(i–j)∈ET µi,j(Si,j).

370
Chapter 10. Clique Trees
We can represent the distribution ˜P ′
Φ(X) as
˜P ′
Φ(X) = 11{Z = z} ·
Q
i∈VT βi(Ci)
Q
(i–j)∈ET µi,j(Si,j).
Thus, we obtain a representation of ˜P ′
Φ in the clique tree simply by multiplying in the new
factor 11{Z = z} into some clique Ci containing the variable Z.
If the clique tree is calibrated before this new factor is introduced, then the clique Ci has
already assimilated all of the other information in the graph. Thus, the clique Ci itself is now
fully informed, and no additional message passing is required in order to obtain ˜P ′
Φ(Ci). Other
cliques, however, still need to be updated with the new information. To obtain ˜P ′
Φ(Cj) for
another clique Cj, we need only transmit messages from Ci to Cj, via the intervening cliques
on the path between them.
(See exercise 10.10.)
As a consequence, the entire tree can be
recalibrated to account for the new evidence using a single pass. Note that retracting evidence
is not as simple: Once we multiply parts of the distribution by zero, these parts are lost, and
they cannot be recovered. Thus, if we want to reserve the ability to retract evidence, we must
store the beliefs prior to the conditioning step (see exercise 10.12).
Interestingly, the same incremental-update approach applies to other forms of updating the
distribution. In particular, we can multiply the distribution with a factor that is not an indicator
function for some variable, an operation that is useful in various applications. The same analysis
holds unchanged.
10.3.3.2
Queries Outside a Clique
Consider a query P(Y | e) where the variables Y are not present together in a single clique.
One naive approach is to construct a clique tree where we force one of the cliques to contain Y
(see exercise 10.13). However, this approach forces us to tailor our clique tree to diﬀerent queries,
negating many of its advantages. An alternative approach is to perform variable elimination over
a calibrated clique tree.
Example 10.9
Consider the simple clique tree of example 10.7, and assume that we have calibrated the clique tree,
so that the beliefs represent the joint distribution as in equation (10.10). Assume that we now want
to compute the probability ˜PΦ(B, D). If the entire clique tree is calibrated, so is any (connected)
subtree T ′. Letting T ′ consist of the two cliques C2 and C3, it follows from theorem 10.4 that:
˜PΦ(B, C, D) = QT ′.
By the clique tree invariant (equation (10.10)), we have that:
˜PΦ(B, D)
=
X
C
˜PΦ(B, C, D)
=
X
C
β2(B, C)β3(C, D)
µ2,3(C)
=
X
C
˜PΦ(B | C) ˜PΦ(C, D),

10.3. Message Passing: Belief Update
371
where the last equality follows from calibration. Each of these probability expressions corresponds
to a set of clique beliefs divided by a message. We can now perform variable elimination, using
these factors in the usual way.
Algorithm 10.4 Out-of-clique inference in clique tree
Procedure CTree-Query (
T ,
// Clique tree over Φ
{βi}, {µi,j},
// Calibrated clique and sepset beliefs for T
Y
// A query
)
1
Let T ′ be a subtree of T such that Y ⊆Scope[T ′]
2
Select a clique r ∈VT ′ to be the root
3
Φ ←βr
4
for each i ∈VT ′ −{r}
5
φ ←
βi
µi,pr(i)
6
Φ ←Φ ∪{φ}
7
Z ←Scope[T ′] −Y
8
Let ≺be some ordering over Z
9
return Sum-Product-VE(Φ, Z, ≺)
More generally, we can compute the joint probability ˜PΦ(Y ) for an arbitrary subset Y
by using the beliefs in a calibrated clique tree to deﬁne factors corresponding to conditional
probabilities in ˜PΦ, and then performing variable elimination over the resulting set of factors.
The precise algorithm is shown in algorithm 10.4. The savings over simple variable elimination
arise because we do not have to perform inference over the entire clique tree, but only over a
portion of the tree that contains the variables Y that constitute our query. In cases where we
have a very large clique tree, the savings can be signiﬁcant.
10.3.3.3
Multiple Queries
Now, assume that we want to compute the probabilities of an entire set of queries where the
variables are not together in a clique. For example, we might wish to compute ˜PΦ(X, Y ) for
every pair of variables X, Y ∈X −E. Clearly, the approach of constructing a clique tree to
ensure that our query variables are present in a single clique breaks down in this case: If every
pair of variables is present in some clique, there must be some clique that contains all of the
variables (see exercise 10.14).
A somewhat less naive approach is simply to run the variable elimination algorithm of algo-
rithm 10.4
 n
2

times, once for each pair of variables X, Y . However, because pairs of variables,
on average, are fairly far from each other in the clique tree, this approach requires fairly sub-
stantial running time (see exercise 10.15). An even better approach can be obtained by using
dynamic programming.
dynamic
programming
Consider a calibrated clique tree T over Φ, and assume we want to compute the probability
˜PΦ(X, Y ) for every pair of variables X, Y . We execute this process by gradually constructing a

372
Chapter 10. Clique Trees
table for each Ci, Cj that contains ˜PΦ(Ci, Cj). We construct the table for i, j in order of the
distance between Ci and Cj in the tree.
The base case is when i, j are neighboring cliques. In this case, we simply extract ˜PΦ(Ci)
from its clique beliefs, and compute
˜PΦ(Cj | Ci) =
βj(Cj)
µi,j(Ci ∩Cj).
From these, we can compute ˜PΦ(Ci, Cj).
Now, consider a pair of cliques Ci, Cj that are not neighbors, and let Cl be the neighbor of
Cj that is one step closer in the clique tree to Ci. By construction, we have already computed
˜PΦ(Ci, Cl) and ˜PΦ(Cl, Cj). The key now, is to observe that
˜PΦ |= (Ci ⊥Cj | Cl).
Thus, we can compute
˜PΦ(Ci, Cj) =
X
Cl−Cj
˜PΦ(Ci, Cl) ˜PΦ(Cj | Cl),
where ˜PΦ(Cj | Cl) can be easily computed from the marginal ˜PΦ(Cj, Cl).
The cost of this computation is signiﬁcantly lower than that of running variable elimination
in the clique tree
 n
2

times (see exercise 10.15).
10.4
Constructing a Clique Tree
So far, we have assumed that a clique tree is given to us. How do we construct a clique tree for
a set of factors, or, equivalently, for its underlying undirected graph HΦ? There are two basic
approaches, the ﬁrst based on variable elimination and the second on direct graph manipulation.
10.4.1
Clique Trees from Variable Elimination
The ﬁrst approach is based on variable elimination.
As we discussed in section 10.1.1, the
execution of a variable elimination algorithm can be associated with a cluster graph: A cluster
Ci corresponds to the factor ψi generated during the execution of the algorithm, and an
undirected edge connects Ci and Cj when τi is used (directly) in the computation of ψj (or
vice versa). As we showed in section 10.1.1, this cluster graph is a tree, and it satisﬁes the running
intersection property; hence, it is a clique tree.
As we showed in theorem 9.6, each factor in an execution of variable elimination with the
ordering ≺is a subset of a clique in the induced graph IΦ,≺. Furthermore, every maximal
clique is a factor in the computation. Based on this result, we can conclude that, in the clique
tree T induced by variable elimination using the ordering ≺, each clique is also a clique in the
induced graph IΦ,≺, and each clique in IΦ,≺is a clique in T . This equivalence is the reason
for the use of term clique in this context.
In the context of clique tree inference, it is standard to reduce the tree to contain only clusters
that are (maximal) cliques in IΦ,≺. Speciﬁcally, we eliminate from the tree a cluster Cj which
is a strict subset of some other cluster Ci:

10.4. Constructing a Clique Tree
373
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
Figure 10.8
A modiﬁed Student BN with an unambitious student
G,H
G,I,S
J,L,S
G,L
G,L,S
D,I,G
C,D
G
G,S
G,I
D
Figure 10.9
A clique tree for the modiﬁed Student BN of ﬁgure 10.8
Theorem 10.6
Let T be a clique tree for a set of factors Φ. Then there exists a clique tree T ′ such that:
• each clique in T ′ is also a clique in T ;
• there is no pair of cliques Ci, Cj in T ′ such that Cj ⊂Ci.
Proof The proof is constructive, eliminating redundant cliques one by one.
We begin with
T ′ = T . Let Cj, Ci be a pair of cliques in T ′ such that Cj ⊂Ci. By the running intersection
property, Cj is a subset of all cliques on the path between Cj and Ci. Let Cl be some
neighbor clique of Cj such that Cj ⊆Cl. We simply remove Cj from the tree, and connect
all of the neighbors of Cj, except for Cl itself, directly to Cl. The proof that the resulting
structure is a valid clique tree is not diﬃcult, and it is left as an exercise (exercise 10.16).
As each of the clique elimination steps reduces the number of cliques in the tree, the process
must terminate. When it does, we have a valid clique tree that does not contain a pair of cliques
where one subsumes the other.
The reduction used in this theorem is precisely the one we used to transform the tree in
ﬁgure 10.1 to the one in ﬁgure 10.2. Consider also the following slightly more complex example
(one that does not result in a clique tree that has the form of a chain):

374
Chapter 10. Clique Trees
Example 10.10
Assume that our student plans to be a beach bum upon graduation, so his happiness does not
depend on getting a job. On the other hand, his happiness still depends on his grade. The network
is shown in ﬁgure 10.8. Variable elimination with the ordering J, L, S, H, C, D, I, G, followed by
a pruning of the nonmaximal clusters from the tree (as in theorem 10.6), we obtain the clique tree
shown in ﬁgure 10.9.
10.4.2
Clique Trees from Chordal Graphs
Theorem 10.6 shows that there exists a clique tree for Φ whose cliques are precisely the maximal
cliques in IΦ,≺. This observation leads us to an alternative approach to constructing a clique
tree. As we discussed in section 9.4.3.1, the induced graph I≺,Φ is necessarily a chordal graph.
In fact, the converse also holds: any chordal graph can be used as the basis for inference. To see
that, recall that theorem 4.12 states that any chordal graph is associated with a clique tree. The
algorithms presented in this chapter show that any clique tree can be used for inference. Thus,
for any set of factors Φ, we can construct a clique tree for inference over Φ by constructing
a chordal graph H∗that contains the edges in HΦ, ﬁnding the maximal cliques in it, and
connecting them appropriately. We now discuss each of these steps.
The process of constructing a chordal graph that subsumes an existing graph H is called
triangulation. Not surprisingly, ﬁnding a minimum triangulation, that is, one where the largest
triangulation
clique in the resulting chordal graph has minimum size, is NP-hard. There are exact algorithms
for ﬁnding an optimal triangulation of a graph, which are exponential in the size of the largest
clique in the graph. In practice, these algorithms are too expensive, and one typically resorts to
heuristic algorithms. Other triangulation methods provide a guarantee that the size of the largest
clique is within a certain constant factor of optimal. These algorithms are less expensive, but
they are still typically too costly in most applications. In practice, the most common approach
to triangulation in graphical models uses the node-elimination techniques that we discussed in
section 9.4.3.2.
Given a chordal graph H, we now must ﬁnd the maximal cliques in the graph. In general,
ﬁnding the maximal cliques in a graph is also an NP-hard problem. However, for chordal
graphs the problem is quite easy. There are several methods. One of the simplest is to run
maximum cardinality search on the resulting chordal graph and collect the maximal cliques
generated in the process. By theorem 9.10, this process introduces no ﬁll edges. It follows from
theorem 9.6 that this process therefore generates all of the maximal cliques in H. Another
method is to begin with a family, each member of which is guaranteed to be a clique, and then
use a greedy algorithm that adds nodes to the clique until it no longer induces a fully connected
subgraph. This algorithm can be performed eﬃciently, because the number of maximal cliques
in a chordal graph is at most n.
Finally, we must determine the edges in the clique tree. Again, one approach for achieving
this task is maximum cardinality search, which also dictates which clique transmits information
to which other clique. A somewhat more eﬃcient approach that achieves the same eﬀect is
via a maximum spanning tree algorithm. More speciﬁcally, we build an undirected graph whose
maximum
spanning tree
nodes are the maximal cliques in H, and where every pair of nodes Ci, Cj is connected by
an edge whose weight is |Ci ∩Cj|. We then use a standard algorithm to ﬁnd a tree in this
graph whose weight — that is, the sum of the weights of the edges in the graph — is maximal.

10.4. Constructing a Clique Tree
375
G,H
G,I,S
J,L,S
2
G,L,S
D,I,G
C,D
1
1
1
1
1
2
2
1
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
(a)
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
(b)
(c)
Figure 10.10
Example of a clique-tree construction algorithm: (a) Undirected factor graph (moralized
graph for the network of ﬁgure 10.8). (b) One possible triangulation for this graph. (c) Cluster graph with
edge weights where clusters are the cliques.
One such algorithm is shown in algorithm A.2. We can show that this approach results in the
same structure as the one constructed by maximal cardinality search, and therefore it satisﬁes
the running intersection property (see exercise 10.17).
To summarize, we can construct a clique tree as follows:
1. Given a set of factors, construct the undirected graph HΦ.
2. Triangulate HΦ to construct a chordal graph H∗.
3. Find cliques in H∗, and make each one a node in a cluster graph.
4. Run the maximum spanning tree algorithm on the cluster graph to construct a tree.
Example 10.11
Consider again the network of ﬁgure 10.8. The undirected graph HΦ for this example is simply
the moralized graph, shown in ﬁgure 10.10a. One possible triangulation for this graph, shown in

376
Chapter 10. Clique Trees
ﬁgure 10.10b, is obtained by introducing the ﬁll edge G—S. This triangulation is a minimal one, as
is the one that introduces the edge L, I. There are also, of course, other nonminimal triangulations.
The graph of the maximal cliques for the chordal graph of ﬁgure 10.10b, with the associated weights
on the edges, is shown in ﬁgure 10.10c. It is easy to verify that the maximum weight spanning tree
is the clique tree shown in ﬁgure 10.9.
Once a clique tree is constructed, it can be used for inference using either of the two message
passing algorithms described earlier.
10.5
Summary
In this chapter, we have described a somewhat diﬀerent perspective on the basic task of exact
inference. This approach uses a preconstructed clique tree as a data structure for exact inference.
Messages are passed between the cliques in the clique tree, with the end result that the cliques
are calibrated — all cliques agree on the same marginal beliefs of any variable they share. We
showed two diﬀerent approaches to message passing in clique trees. The ﬁrst uses the same
operations as variable elimination, using dynamic programming to cache messages in order to
avoid repeated computation. The second uses belief propagation messages, which propagate
marginal beliefs between cliques in an attempt to make them agree with each other.
Both
approaches allow calibration of the entire clique tree within two passes over the tree.
It is instructive to compare the standard variable elimination algorithm of chapter 9 and the
algorithm obtained by variable elimination in a clique tree. In principle, they are equivalent, in
that they both use the same basic operations of multiplying factors and summing out variables.
Furthermore, the cliques in the clique tree are basically the factors in variable elimination.
Thus, we can use any variable elimination algorithm to ﬁnd a clique tree, and any clique tree
to deﬁne an elimination ordering. It follows that the two approaches have basically the same
computational complexity.
In practice, however, the two algorithms oﬀer diﬀerent trade-oﬀs. On one hand, clique trees
have several advantages. Most importantly, through the use of dynamic programming, the

clique tree provides answers to multiple cliques using a single computation. Additional
layers of dynamic programming allow the same data structure to answer an even broader
range of queries, and to dynamically introduce and retract evidence. Moreover, the clique
tree approach executes a nontrivial number of the required operations in advance, including the
construction of the basic data structures, the choice of elimination ordering (which is almost
determined), and the product of the CPDs assigned to a single clique.
On the other hand, clique trees, as typically implemented, also have disadvantages. First,
clique trees are more expensive in terms of space. In a clique tree, we keep all intermediate
factors, whereas in variable elimination we can throw them out. If there are c cliques, the cost
of the clique tree algorithm can be as much as 2c times as expensive. More importantly, in a

clique tree, the structure of the computation is ﬁxed and predetermined. We therefore
have less ﬂexibility to take advantage of computational eﬃciencies that arise because
of speciﬁc features of the evidence and query. For example, in the Student network with
evidence i1, the variable elimination algorithm could avoid introducing a dependence between G
and S, resulting in substantially smaller factors. In the clique tree algorithm, the clique structure
is usually predetermined, precluding these online optimizations. The diﬀerence in cost can be

10.6. Relevant Literature
377
quite dramatic in situations where there is a lot of evidence. This type of situation-speciﬁc
simpliﬁcation occurs even more often in networks that exhibit context-speciﬁc independence.
Finally, in standard implementaitons, the cliques in a clique tree are typically the maximal cliques
in a triangulated graph. Furthermore, the operations performed in the clique tree computation
are typically implemented in a fairly standard way, where the incoming messages are multiplied
with the clique beliefs, and the outgoing message is generated. This approach is not always
optimal (see exercise 10.7).
We can modify each of these algorithms to have some of the advantages of the other. For
example, we can choose to deﬁne a clique tree online, after the evidence is obtained. In this
case, the clique tree structure can take advantage of simpliﬁcations resulting from the evidence.
However, we lose the advantage of precomputing the clique tree oﬄine. As another example,
we can store intermediate results in a variable elimination execution, and then do a downward
pass to obtain the marginal posteriors of all variables. Here, we gain the advantage of reusing
computation, at the cost of additional space. In general, we can view these two algorithms
as two examples in a space of variable elimination algorithms. There are many other variants
that make somewhat diﬀerent trade-oﬀs, but, fundamentally, they are performing essentially
equivalent computations.
10.6
Relevant Literature
The content of this chapter is closely related to that of chapter 9; hence, many of the citations
in section 9.8 are highly relevant to the discussion in this chapter, and the reader is encouraged
to explore those as well.
Following the lines of the polytree algorithm, Pearl (1988) proposed a simple approach that
clustered nodes so as to produce a polytree; this approach, however, produced very ineﬃcient
trees. The sum-product message passing algorithm in clique trees was developed by Shenoy and
Shafer (1990); Shafer and Shenoy (1990), who described it in a much broader form that applies
to many factored models other than probabilistic graphical models. The sum-product-divide
approach was developed in parallel, in a series of papers by Lauritzen and Spiegelhalter (1988)
and Jensen, Olesen, and Andersen (1990). This line of work also generated the perspective of
message passing operations as performing a reparameterization of the original distribution, an
intuition that has been very inﬂuential in some of the work of chapter 11. The sum-product-
divide algorithm formed the basis for the Hugin Bayesian network system, described by Andersen,
Hugin
Olesen, Jensen, and Jensen (1989), leading to the common use of the name “Hugin algorithm”
for this method.
Some simple modiﬁcations to the clique tree algorithm can greatly improve its eﬃciency.
For example, Kjaerulﬀ(1997) describes a method for improving the in-clique computations by
using a nested clique tree data structure. Jensen (1995) provides a method for eﬃciently doing
nested clique tree
incremental update and retraction of evidence in a clique tree.
Park and Darwiche (2004b)
provide a derivation of the clique tree algorithm in terms of gradients of the network polynomial
(see box 9.A). This approach can also be used as the basis for incremental update and evidence
retraction.
Clique trees and their variants have been extended in many ways, and used for many tasks
other than simple probabilistic inference. A complete survey is outside the scope of this book,

378
Chapter 10. Clique Trees
but some of these applications and generalizations are mentioned in later chapters.
10.7
Exercises
Exercise 10.1
Prove proposition 10.1.
Exercise 10.2
Prove theorem 10.2.
Exercise 10.3
Show how to perform eﬃcient index computations for factor marginalization along the lines discussed in
factor
marginalization
box 10.A.
Exercise 10.4
Prove theorem 10.5.
Exercise 10.5⋆
Let T be a clique tree and Cr be a root. Let Cj be a clique in the tree and Ci its upward neighbor.
Let βj be the potential at Cj after the upward pass of CTree-SP-Upward (algorithm 10.1). Show that βj
represents the correct conditional probability ˜PΦ(Cj | Si,j). In other words, letting X = Cj −Si,j and
S = Si,j, we have that:
βj(X, S)
βj(S)
= ˜PΦ(X | S).
Exercise 10.6⋆
Assume that we have constructed a clique tree T for a given Bayesian network graph G, and that each of
the cliques in T contains at most k nodes. Now, the user decides to add a single edge to the Bayesian
network, resulting in a network G′. (The edge can be added between any pair of nodes in the network, so
long as it maintains acyclicity.) What is the tightest bound you can provide on the maximum clique size
in a clique tree T ′ for G′? Justify your response by explaining how to construct such a clique tree. (Note:
You do not need to provide the optimal clique tree T ′. The question asks for the tightest clique tree that
you can construct, using only the fact that T is a clique tree for G.)
Exercise 10.7
The algorithm for performing variable elimination in a clique tree (algorithm 10.1 and algorithm 10.2)
speciﬁes a particular approach for sending a variable elimination message: First (in a preprocessing step),
the initial clique potentials are generated by multiplying all the factors assigned to a clique. Then, in the
message passing operation (SP-Message), the initial potential is multiplied by all of the incoming messages,
and the variables not on the sepset are summed out.
Is this the most computationally eﬃcient procedure for executing the message passing step? Either explain
why or provide a counterexample.
Exercise 10.8⋆
Consider again the network polynomial construction of box 9.A and the algorithm of exercise 9.6 for
network
polynomial
eﬃciently computing all the polynomial’s derivatives. Show that this algorithm provides an alternative
derivation of the up/down clique-tree calibration procedure for sum-product clique trees. In other words,
ﬁnd a correspondence between the computation of the partial derivatives in exercise 9.6 and the message
passing operations in the clique tree algorithm.

10.7. Exercises
379
Exercise 10.9⋆⋆
Use your answer to exercise 10.8 to come up with a rule-based clique tree algorithm, based on the rule-
rule-based clique
tree
based variable elimination procedure of section 9.6.2.1. Your algorithm should compute in a single upward-
downward pass all of the marginal probabilities for all variables in the network; its complexity should be
twice the cost of the simple rule-based variable elimination of section 9.6.2.1.
Exercise 10.10⋆
Let T be a calibrated clique tree representing the unnormalized distribution ˜PΦ = Q
φ∈Φ φ. Let φ′ be a
new factor, and let ˜P ′
Φ = ˜PΦ · φ′. Let Ci be some clique such that Scope[φ] ⊆Ci. Show that we can
perform an incremental update to obtain ˜P ′
Φ(Cj) for any clique Cj by multiplying φ′ into βi and then
incremental
update
propagating messages from Ci to Cj along the path between them.
Exercise 10.11⋆
Consider the problem of eliminating extraneous variables from a clique tree.
More precisely, given a
calibrated clique tree T over X, we want to generate a (calibrated) clique tree T ′ whose scope is some
subset Y ⊂X. Clearly, we want to make T ′ as small as possible (in terms of the size of the resulting
cliques). However, we do not want to construct and calibrate a clique tree from scratch; rather, we want to
reuse our previous computation.
a. Suppose T consists of two cliques C1 and C2 over variables {A, B, C} and {C, D}, respectively.
What is the resulting T ′ if Y = {B, C}?
b. For the clique tree T deﬁned in part 1, what is T ′ if Y = {B, D}?
c. Now consider an arbitrary clique tree T over X and an arbitrary Y ⊆X. Provide an algorithm to
transform a calibrated tree T into a calibrated tree T ′ over Y . Your algorithm should not resort to
manipulating the underlying network or factors; all operations should be performed directly on the
clique tree.
d. Give an example where the resulting tree T ′ is larger than the original clique tree T .
Exercise 10.12⋆⋆
Let T be a clique tree over X, deﬁned via a set of initial factors Φ. Let Y = y be some observed
assignment to a subset of the variables. Consider a setting where we might be unsure about a particular
observation Yi = yj
i for Yi ∈Y , and that we want to compute the eﬀect on the unnormalized probability
of the other possible values yk
i . More precisely, let Y −i = Y −{Yi} and y−i be the assignment in y
to Y −i. We want to compute ˜PΦ(yk
i , Y −i = y−i) for every Yi and every yk
i . Describe a variant of
sum-product message passing (algorithm 10.2) that can perform this task without requiring more messages
than the standard two-pass calibration. (Hint: Rather than reducing the factors prior to message passing,
consider reducing factors during the message passing process.)
Exercise 10.13
Provide a simple method for constructing a clique tree such that a given set of variables Y is guaranteed
to be to together in some clique. Your algorithm should use a standard clique-tree construction algorithm
as a black box.
Exercise 10.14
Assume that we have a clique tree T over X such that, for every pair of nodes X, Y ∈X, there exists a
clique that contains both X and Y . Prove that T must contain a single clique that contains all of X.
Exercise 10.15⋆
Consider the task of using a calibrated clique tree T over Φ to compute all of the pairwise marginals of vari-
ables, ˜PΦ(X, Y ) for all X, Y . Assume that our probabilistic network consists of a chain X1—X2— . . . —Xn,
and that our clique tree has the form 1— . . . —n −1 where Scope[Ci] = {Xi, Xi+1}. Also assume that
each variable Xi has |Val(Xi)| = d.

380
Chapter 10. Clique Trees
a. What is the total cost (number of multiplication steps and number of addition steps) of doing variable
elimination over this chain-structured clique tree, as described in the algorithm of algorithm 10.4, for
all
 n
2

variable pairs?
b. What is the total cost of the algorithm described in section 10.3.3.3? Provide a precise expression, not
merely an asymptotic upper bound.
c. Since we are computing marginals for all variable pairs, we may store any computations done for the
previous pairs and use them to save time for the remaining pairs. Construct a dynamic programming
algorithm for this speciﬁc chain structure that reduces the complexity of this task by a factor of d2
over the algorithm described in section 10.3.3.3? Explain why this approach idea would not work for a
general clique tree.
Exercise 10.16
Complete the proof of theorem 10.6 by showing that, if we have a valid clique tree, then a clique elimination
step, as described, results in a valid clique tree. In particular, show that it is a tree, that it satisﬁes the
family preservation property, and that it satisﬁes the running intersection property.
Exercise 10.17⋆
Show that the clique tree constructed using the maximum-weight-spanning tree procedure of section 10.4.2
satisﬁes the running intersection property. (Hint: Show that this tree structure can also be constructed
using the maximum-cardinality algorithm.)

11
Inference as Optimization
11.1
Introduction
In the previous chapters we examined exact inference. We have seen that for many networks
we can perform exact inference eﬃciently.
As we have seen, the computational and space
complexity of the clique tree is exponential in the tree-width of the network. This means that the
exact algorithms we examined become infeasible for networks with a large tree-width. In many
real-life applications, we encounter such networks. This motivates examination of approximate
inference methods that are applicable to networks where exact inference is intractable.

In this chapter we consider a class of approximate inference methods, where the ap-
proximation arises from constructing an approximation to the target distribution PΦ.
This approximation takes a simpler form that allows for inference. In general, the sim-
pler approximating form exploits a local factorization structure that is similar in nature to the
structure exploited by graphical models.
The speciﬁc algorithms we consider diﬀer in many details, and yet they share some common
conceptual principles. We now review these principles to provide a common framework for the
remaining presentation. In each method, we deﬁne a target class Q of “easy” distributions Q
and then search for an instance within that class that is the “best” approximation to PΦ. Queries
can then be answered using inference on Q rather than on PΦ. All of the methods we describe
optimize (roughly) the same target function for measuring the similarity between Q and PΦ.
This approach reformulates the inference task as one of optimizing an objective function over
the class Q. This problem falls into the category of constrained optimization. Such problems
constrained
optimization
can be solved using a variety of diﬀerent methods. Thus, the formulation of inference from
this perspective opens the door to the application of a range of techniques developed in the
optimization literature. Currently, the technique most often used in the setting of graphical
models is one based on the use of Lagrange multipliers, which we review in appendix A.5.3. This
method produce a set of equations that characterize the optima of the objective. In our setting,
this characterization takes the form of a set of ﬁxed-point equations that deﬁne each variable in
terms of others. A particularly compelling and elegant result is that the ﬁxed-point equations

derived from the constrained energy optimization, for any of the methods we describe,
can be viewed as passing messages over a graph object. Indeed, as we will show, even
the standard sum-product algorithm for clique trees (algorithm 10.2) can be rederived from this
perspective. Moreover, many other message passing algorithms follow from the same derivation.
Methods in this class fall into three main categories. The ﬁrst category includes methods that

382
Chapter 11. Inference as Optimization
use clique-tree message passing schemes on structures other than trees. This class of methods,
which includes the famous loopy belief propagation algorithm, can be understood as optimizing
approximate versions of the energy functional. The second category includes methods that use
message propagation on clique trees with approximate messages. This class of methods, often
known as the expectation propagation algorithm, maximize the exact energy functional, but with
relaxed consistency constraints on the representation Q. Finally, in the third category there are
methods that generalize the mean ﬁeld method originating in statistical physics. These methods
use the exact energy functional, but they restrict attention to a class Q consisting of distributions
Q that have a particular simple factorization. This factorization is chosen to be simple enough
to ensure that we can perform inference with Q.
More broadly, each of these algorithms can be described from two perspectives: as a proce-
dural description of a message passing algorithm, or as an optimization problem consisting of
an objective and a constraint space. Historically, the message passing algorithm generally origi-
nated ﬁrst, sometimes long before the optimization interpretation was understood. However, the
optimization perspective provides a much deeper understanding of these methods, and it shows
that message passing is only one way of performing the optimization; it also helps point the
way toward useful generalizations. In the ensuing discussion, we usually begin the presentation
of each class of methods by describing a simple variant of the algorithm, providing a concrete
manifestation to ground the concepts. We then present the optimization perspective on the
algorithm, allowing a deeper understanding of the algorithm. Finally, we discuss generalizations
of the simple algorithm, often ones that are derived directly from the optimization perspective.
11.1.1
Exact Inference Revisited ⋆
Before considering approximate inference methods, we start by casting exact inference as an
optimization problem.
The concepts we introduce here will serve in the discussion of the
following approximate inference methods.
Assume we have a factorized distribution of the form
PΦ(X) = 1
Z
Y
φ∈Φ
φ(U φ),
(11.1)
where the factors φ in Φ comprise the distribution, and the variables U φ = Scope[φ] ⊆X
are the scope of each factor. For example, the factors might be CPDs in a Bayesian network,
generally restricted by an evidence set, or they might be potentials in a Markov network. We are
interested in answering queries about the distribution PΦ. These include queries about marginal
probabilities of variables and queries about the partition function Z. As we discussed, if PΦ is
a Bayesian network with instantiated evidence on some variables, then the partition function Z
is the probability of the evidence.
Recall that the end product of belief propagation is a calibrated cluster tree. Also recall that
a calibrated set of beliefs for the cluster tree represents a distribution. In exact inference we
ﬁnd a set of calibrated beliefs that represent PΦ(X). That is, we ﬁnd beliefs that match the
distribution represented by given set of initial potentials. Thus, we can view exact inference
as searching over the set of distributions Q that are representable by the cluster tree to ﬁnd a
distribution Q∗that matches PΦ.

11.1. Introduction
383
Intuitively, we can rephrase this question as searching for a calibrated distribution that is as
close as possible to PΦ. There are many possible ways of measuring the distance between two
distributions, such as the Euclidean distance (L2), or the L1 distance and the related variational
distance (see appendix A.1.3.3). As we will see, our main challenge, however, is our aim to avoid
performing inference with the distribution PΦ; in particular, we cannot eﬀectively compute
marginal distributions in PΦ. Hence, we need methods that allow us to optimize the distance
between Q and PΦ without answering hard queries about PΦ. A priori, this requirement may
seem impossible to satisfy. However, it turns out that there exists a distance measure — the
relative entropy (or KL-divergence) — that allows us to exploit the structure of PΦ without
performing reasoning with it.
Recall that the relative entropy between P1 and P2 is deﬁned as1
ID(P1||P2) = IEP1

ln P1(X)
P2(X)

.
Also recall that the relative entropy is always nonnegative, and equal to 0 if and only if P1 = P2.
Thus, we can use it as a distance measure, and choose to ﬁnd an approximation Q to PΦ that
minimizes the relative entropy.
However, as we discussed, the relative entropy is not symmetric — ID(P1||P2) ̸= ID(P2||P1).
In section 8.5, we discussed the use of relative entropy for projecting a distribution into a
restricted class; this projection can aim to minimize either ID(PΦ||Q), via the M-projection,
M-projection
or ID(Q||PΦ), via the I-projection.
A priori, it might appear that the M-projection is more
I-projection
appropriate, since one of the main information-theoretic justiﬁcations for the relative entropy
ID(PΦ||Q) is the number of bits lost when coding a true message distribution PΦ using an
(approximate) estimate Q. However, as the discussion of section 8.5.2 shows, computing the
M-projection Q — arg minQ ID(PΦ||Q) — requires that we compute marginals of PΦ and is
therefore equivalent to running inference in PΦ. Somewhat surprisingly, as we show in the
subsequent discussion, this does not apply to I-projection: we can exploit the structure of PΦ
to optimize arg minQ ID(Q||PΦ) eﬃciently, without running inference in PΦ.
To summarize this discussion, we want to search for a distribution Q that minimizes
ID(Q||PΦ). To deﬁne and analyze this optimization problem formally, we also need to spec-
ify the objects we optimize over. Suppose we are given a clique tree structure T for PΦ; that
is, T satisﬁes the running intersection property and the family preservation property. Moreover,
suppose we are given a set of beliefs
Q = {βi : i ∈VT } ∪{µi,j : (i–j) ∈ET , }
where Ci denotes clusters in T , βi denotes beliefs over Ci, and µi,j denotes beliefs over Si,j
of edges in T .
As in deﬁnition 10.6, the set of beliefs in T deﬁnes a distribution Q by the formula
Q(X) =
Q
i∈VT βi
Q
(i–j)∈ET µi,j
.
(11.2)
1. Note that, until now, we deﬁned the relative entropy and other information-theoretic terms, such as mutual informa-
tion, using logarithms to base 2. As will become apparent, in the context of the discussion in this chapter, the natural
logarithm (base e) is more suitable. This change is a simple rescaling of the relevant information-theoretic quantities
and does not change their basic properties.

384
Chapter 11. Inference as Optimization
(See section 10.2.3.) Due to the calibration requirement, the set of beliefs Q satisﬁes the marginal
calibration
marginal
consistency
consistency constraints if, for each (i–j) ∈ET , the beliefs on Si,j are the marginal of βi (and
βj). Recall that theorem 10.4 shows that if Q is a set of calibrated beliefs for T and Q is the
distribution deﬁned by equation (11.2), then
βi[ci]
=
Q(ci)
µi,j[si,j]
=
Q(si,j).
Thus, the beliefs correspond to marginals of the distribution Q deﬁned by equation (11.2).
Thus, we are now searching over a set of distributions Q that are representable by a set of
beliefs Q over the cliques and sepsets in a particular clique tree structure T . Note that when
deciding on the representation of Q we are actually making two decisions: We are deciding both
on the space of distributions that we are considering (all distributions for which T is an I-map),
and on the representation of these distributions (as a set of calibrated clique beliefs). Both of
these decisions are signiﬁcant components in the speciﬁcation of our optimization problem.
With these deﬁnitions in hand, we can now view exact inference as maximizing −ID(Q||PΦ)
over the space of calibrated sets Q.
CTree-Optimize-KL:
Find
Q = {βi : i ∈VT } ∪{µi,j : (i–j) ∈ET }
maximizing
−ID(Q||PΦ)
subject to
µi,j[si,j]
=
X
Ci−Si,j
βi(ci)
∀(i–j) ∈ET , ∀si,j ∈Val(Si,j)
X
ci
βi(ci)
=
1
∀i ∈VT .
In solving this optimization problem, we conceptually examine diﬀerent conﬁgurations of
beliefs that satisfy the marginal consistency constraints, and we select the conﬁguration that
maximizes the objective. Such an exhaustive examination, of course, is impossible to perform
in practice. However, there are eﬀective solutions to this problem that ﬁnd the maximum point.
We have already seen that, if T is a proper cluster tree for the set of original potentials Φ, we
know that there is a set Q that induces, via equation (11.2), a distribution Q = PΦ. Because this
solution achieves a relative entropy of 0, which is the highest value possible, it is the unique
global optimum of this optimization.
Theorem 11.1
If T is an I-map of PΦ, then there is a unique solution to CTree-Optimize-KL.
This optimum can be found using the exact inference algorithms we developed in chapter 10.
11.1.2
The Energy Functional
The preceding discussion suggests a strategy for constructing approximations of PΦ. Instead of
searching over the space of all calibrated cluster trees, we can search over a space of “simpler”
distributions. In this search we will not ﬁnd a distribution equivalent to PΦ, yet we might

11.1. Introduction
385
ﬁnd one that is reasonably close to PΦ. Moreover, as part of the design of the target set of
distributions, we can ensure that these distributions are ones in which we can perform inference
eﬃciently.
One problem that we will face is that the target of the optimization ID(Q||PΦ) is unwieldy for
direct optimization. The relative entropy term contains an explicit summation over all possible
instantiations of X, an operation that is infeasible in practice. However, since we know the form
of ln PΦ(ξ) from equation (11.1), we can exploit its structure to rewrite the relative entropy in a
simpler form, as shown in the following theorem.
Theorem 11.2
ID(Q||PΦ) = ln Z −F[ ˜PΦ, Q]
where F[ ˜PΦ, Q] is the energy functional
energy functional
F[ ˜PΦ, Q] = IEQ
h
ln ˜P(X)
i
+ IHQ(X) =
X
φ∈Φ
IEQ[ln φ] + IHQ(X).
(11.3)
Proof
ID(Q||PΦ) = IEQ[ln Q(X)] −IEQ[ln PΦ(X)].
(11.4)
Using the product form of PΦ, we have that
ln PΦ(X) =
X
φ∈Φ
ln φ(U φ) −ln Z.
Moreover, recall that IHQ(X) = −IEQ[ln Q(X)]. Plugging these into equation (11.4), we get
ID(Q||PΦ)
=
−IHQ(X) −IEQ

X
φ∈Φ
ln φ(U φ)

+ IEQ[ln Z]
=
−F[ ˜PΦ, Q] + ln Z.
Importantly, the term ln Z does not depend on Q. Hence, minimizing the relative entropy
ID(Q||PΦ) is equivalent to maximizing the energy functional F[ ˜PΦ, Q].
This latter term relates to concepts from statistical physics, and it is the negative of what
is referred to in that ﬁeld as the (Helmholtz) free energy. While explaining the physics-based
free energy
motivation for this term is out of the scope of this book, we continue to use the standard
terminology of energy functional.
The energy functional contains two terms. The ﬁrst, called the energy term, involves expecta-
energy term
tions of the logarithms of factors in Φ. Here, each factor in Φ appears as a separate term. Thus,
if the factors that comprise Φ are small, each expectation deals with relatively few variables.
The diﬃculties in dealing with these expectations depends on the properties of the distribution
Q. Assuming that inference is “easy” in Q, we should be able to evaluate such expectations
relatively easily. The second term, called the entropy term, is the entropy of Q. Again, the choice
entropy term
of Q determines whether we can evaluate this term. However, we will see that, for the choices
we make, this term will also be tractable.

386
Chapter 11. Inference as Optimization
11.1.3
Optimizing the Energy Functional
In the remainder of this chapter, we pose the problem of ﬁnding a good approximation

Q as one of maximizing the energy functional, or, equivalently, minimizing the relative
entropy. Importantly, the energy functional involves expectations in Q. As we show, by choosing
approximations Q that allow for eﬃcient inference, we can both evaluate the energy functional
and optimize it eﬀectively.
Moreover, since ID(Q||PΦ) ≥0, we have that
ln Z ≥F[ ˜PΦ, Q].
(11.5)
That is, the energy functional is a lower bound on the logarithm of the partition function Z,
lower bound
for any choice of Q. Why is this fact signiﬁcant? Recall that, in directed models, the partition
function Z is the probability of the evidence. Computing the partition function is often the
hardest part of inference. And so, this theorem shows that if we have a good approximation (that
is, ID(Q||PΦ) is small), then we can get a good lower-bound approximation to Z. The fact that
this approximation is a lower bound will play an important role in later chapters on learning.
In this chapter, we explore inference methods that can be viewed as strategies for optimizing
the energy functional. These kinds of methods are often referred to as variational methods.
variational
method
The name refers to a general strategy in which we want to solve a problem by introducing new
variational parameters that increase the degrees of freedom over which we optimize. Each choice
of these parameters gives an approximate answer. We then attempt to optimize the variational
parameters to get the best approximation. In our case, the task is to answer queries about PΦ,
and the variational parameters describe the distribution Q. In the methods we consider, we
vary these parameters to try to ﬁnd a good approximation to the target query.
11.2
Exact Inference as Optimization
Before considering approximate inference methods, we illustrate the the use of a variational
approach to rederive an exact inference procedure. The concepts we introduce here will serve
in discussion of the following approximate inference methods.
As we have already seen, the optimization problem CTree-Optimize-KL has a unique solution.
We start by reformulating the optimization problem in terms of the energy functional. As we
have seen, maximizing the energy functional is equivalent to minimizing the relative entropy
between Q and PΦ.
Once we restrict attention to calibrated cluster trees, we can further simplify the objective
function. More precisely, we can rewrite the energy functional in a factored form as a sum of
terms each of which depends directly only on one of the beliefs in Q. This form reveals the
structure in the distribution, and it is therefore a much better starting point for further analysis.
As we will see, this form is also the basis for our approximations in subsequent sections.
Deﬁnition 11.1
Given a cluster tree T with a set of beliefs Q and an assignment α that maps factors in PΦ to
clusters in T , we deﬁne the factored energy functional:
factored energy
functional
˜F[ ˜PΦ, Q] =
X
i∈VT
IECi∼βi[ln ψi] +
X
i∈VT
IHβi(Ci) −
X
(i–j)∈ET
IHµi,j(Si,j),
(11.6)

11.2. Exact Inference as Optimization
387
where ψi is the initial potential assigned to Ci:
ψi =
Y
φ,α(φ)=i
φ,
and IECi∼βi[·] denotes expectation on the value Ci given the beliefs βi
Before we prove that the energy functional is equivalent to its factored variant, let us ﬁrst
study its components. The ﬁrst term is a sum of terms of the form IECi∼βi[ln ψi]. Recall that ψi
is a factor (not necessarily a distribution) over the scope Ci, that is, a function from Val(Ci) to
IR+. Its logarithm is therefore a function from Val(Ci) to IR. The beliefs βi are a distribution
over Val(Ci). We can therefore compute the expectation P
ci βi(ci) ln ψi. The last two terms
are entropies of the beliefs associated with the clusters and sepsets in the tree. The important
beneﬁt of this reformulation is that all the terms are local, in the sense that they refer to a
speciﬁc belief factor. As we will see, this will make our tasks much simpler.
Proposition 11.1
If Q is a set of calibrated beliefs for T , and Q is deﬁned by equation (11.2), then
˜F[ ˜PΦ, Q] = F[ ˜PΦ, Q].
Proof Note that ln ψi = P
φ,α(φ)=i ln φ. Moreover, since βi(ci) = Q(ci), we conclude that
X
i
IECi∼βi[ln ψi] =
X
φ
IECi∼Q[ln φ].
It remains to show that
IHQ(X) =
X
i∈VT
IHβi(Ci) −
X
(i–j)∈ET
IHµi,j(Si,j).
This equality follows directly from equation (11.2) and theorem 10.4.
Using this form of the energy, we can now deﬁne the optimization problem. We ﬁrst need
to deﬁne the space over which we are optimizing. If Q is factorized according to T , we can
represent it by a set of calibrated beliefs. Marginal consistency is a constraint on the beliefs
that requires neighboring beliefs to agree on the marginal distribution on their joint subset. It
is equivalent to requiring that the beliefs be calibrated. Thus, we pose the following constrained
optimization procedure:
CTree-Optimize:
Find
Q = {βi : i ∈VT } ∪{µi,j : (i–j) ∈ET }
maximizing
˜F[ ˜PΦ, Q]
subject to
µi,j[si,j]
=
X
Ci−Si,j
βi(ci)
(11.7)
∀(i–j) ∈ET , ∀si,j ∈Val(Si,j)
X
ci
βi(ci)
=
1
∀i ∈VT
(11.8)
βi(ci)
≥
0
∀i ∈VT , ci ∈Val(Ci).
(11.9)

388
Chapter 11. Inference as Optimization
The constraints equation (11.7), equation (11.8), and equation (11.9) ensure that the beliefs in Q
are calibrated and represent legal distributions (exercise 11.2).
11.2.1
Fixed-Point Characterization
We can now prove that the stationary points of this constrained optimization function — the
points at which the gradient is orthogonal to all the constraints — can be characterized by a
set of ﬁxed-point equations. As we show, these equations turn out to be the update equations
in the sum-product belief-propagation procedure (CTree-SP-calibrate in algorithm 10.2). Thus,
if we turn these equations into an iterative algorithm, as we will describe, we obtain precisely
the belief propagation algorithm in clique trees. We note that for this derivation and other
similar ones later in the chapter, we restrict attention to models where all of the potentials are
strictly positive (contain no zero entries). Although the results generally hold also for the case of
deterministic potentials (zero entries), the proofs are considerably more complex and are outside
the scope of this book.
Recall that a stationary point of a function is either a local maximum, a local minimum, or
a saddle point. In the optimization problem CTree-Optimize, there is a single global maximum
(see theorem 11.1). Although we do not show it here, one can show that it is also the only
stationary point (see exercise 11.3), and thus once we ﬁnd a stationary point, we know that we
have found the maximum.
We want to characterize this stationary point by a set of equations that must hold when the
choice of beliefs in Q is at the stationary point. Recall that our aim is to maximize the function
˜F[ ˜PΦ, Q] under the consistency constraints.
The method of Lagrange multipliers, reviewed
Lagrange
multipliers
in appendix A.5.3, provides us with tools for dealing with constrained optimization. Because
the characterization of the stationary point is of central importance to later developments, we
examine how to construct such a characterization using the method of Lagrange multipliers.
When using the method of Lagrange multipliers, we start by deﬁning a Lagrangian with a
Lagrange multiplier for each of the constraints on the function we want to optimize. In our
case, we have the constraints in equation (11.7) and equation (11.8). We note that, in principle,
we also need to introduce a Lagrange multiplier for the inequality constraint that ensures that
all beliefs are nonnegative. However, as we will see, the assumption that factors are strictly
positive implies that the beliefs we construct in the solution to the optimization problem will be
nonnegative, and thus we do not need to enforce these constraints actively. We therefore obtain
the following Lagrangian:
J
=
˜F[ ˜PΦ, Q]
−
X
i∈VT
λi
 X
ci
βi(ci) −1
!
−
X
i
X
j∈Nbi
X
si,j
λj→i[si,j]

X
ci∼si,j
βi(ci) −µi,j[si,j]

,
where Nbi is the neighbors of Ci in the clique tree. We introduce Lagrange multipliers λi for
each beliefs factor βi to ensure that it sums to 1. We also introduce, for each pair of neighboring
cliques i and j and assignment to their sepset si,j, a Lagrange multiplier λj→i[si,j] to ensure

11.2. Exact Inference as Optimization
389
that the marginal distribution of si,j in βj is consistent with its value in the sepset beliefs µi,j.
(Note that we also introduce another Lagrange multiplier for the direction i →j.)
Remember that J is a function of the clique beliefs {βi}, the sepset beliefs {µi,j}, and the
Lagrange multipliers. To ﬁnd the maximum of the Lagrangian, we take its partial derivatives
with respect to βi(ci), µi,j[si,j], and the Lagrange multipliers. These last derivatives reconstruct
the original constraints. The ﬁrst two types of derivatives require some work. Diﬀerentiating the
Lagrangian (see exercise 11.1), we get that
∂
∂βi(ci)J
=
ln ψi[ci] −ln βi(ci) −1 −λi −
X
j∈Nbi
λj→i[si,j]
∂
∂µi,j[si,j]J
=
ln µi,j[si,j] + 1 + λi→j[si,j] + λj→i[si,j].
At the stationary point, these derivatives are zero. Equating each derivative to 0, rearranging
terms, and exponentiating, we get
βi(ci)
=
exp {−1 −λi} ψi[ci]
Y
j∈Nbi
exp {−λj→i[si,j]}
µi,j[si,j]
=
exp {−1} exp {−λi→j[si,j]} exp {−λj→i[si,j]} .
These equations describe beliefs as functions of terms of the form exp {−λi→j[si,j]}. In
fact, µi,j is a product of two such terms (and a constant). This suggests that these terms play
the role of a message δi→j. To make this more explicit, we deﬁne
δi→j[si,j] = exp

−λi→j[si,j] −1
2

.
(We add the term −1
2 to deal with the additional exp {−1} term, but since this is a multiplicative
constant, it is not that crucial.) We can now rewrite the resulting system of equations as
βi(ci)
=
exp

−λi −1 + 1
2|Nbi|

ψi(ci)
Y
j∈Nbi
δj→i[si,j]
µi,j[si,j]
=
δi→j[si,j]δj→i[si,j].
Combining these equations with equation (11.7), we now rewrite the message δi→j as a
function of other messages:
δi→j[si,j]
=
µi,j[si,j]
δj→i[si,j]
=
P
ci∼si,j βi(ci)
δj→i[si,j]
=
exp

−λi −1 + 1
2|Nbi|
 X
ci∼si,j
ψi(ci)
Y
k∈Nbi−{j}
δk→i[si,k].
Note that the term exp

−λi −1 + 1
2|Nbi|
	
is a constant (since it does not depend on ci),
and when we combine these equations with equation (11.8), we can solve for λi to ensure that

390
Chapter 11. Inference as Optimization
this constant normalizes the clique beliefs βi. We note that if the original factors deﬁne a
distribution that sums to 1, then the solution for λi that satisﬁes equation (11.8) will be one
where λi = 1
2(|Nbi| −1), that is, the normalizing constant is 1.
This derivation proves the following result.
Theorem 11.3
A set of beliefs Q is a stationary point of CTree-Optimize if and only if there exists a set of factors
{δi→j[Si,j] : (i–j) ∈ET } such that
δi→j ∝
X
Ci−Si,j
ψi


Y
k∈Nbi−{j}
δk→i


(11.10)
and moreover, we have that
βi
∝
ψi

Y
j∈Nbi
δj→i


µi,j
=
δj→i · δi→j.

This theorem characterizes the solution of the optimization problem in terms of ﬁxed-
ﬁxed-point
equations
point equations that must hold when we ﬁnd a maximal Q. These ﬁxed-point equations
deﬁne the relationships that must hold between the diﬀerent parameters involved in the
optimization problem. Most importantly, equation (11.10) deﬁnes each message in terms of
other messages, allowing an easy iterative approach to solving the ﬁxed point equations.
These same themes appear in all the approaches we will discuss later in this chapter.
11.2.2
Inference as Optimization
The ﬁxed-point characterization of theorem 11.3 focuses on the relationships that hold at the
maximum point (or points). However, they also hint at a way of achieving these relationships.
Intuitively, a change in Q that reduces the diﬀerences between the left-hand and right-hand
side of these equations will get us closer to a maximum point. The most direct way of reducing
such discrepancies is to apply the equations as assignments and iteratively apply equations to
the current values of the right-hand side to deﬁne a new value for the left-hand side.
More precisely, we initialize all of the δi→j’s to 1 and then iteratively apply equation (11.10),
computing the left-hand side δi→j of each equality in terms of the right-hand side (essentially
converting each equality sign to an assignment). Clearly, a single iteration of this process does
not usually suﬃce to make the equalities hold; however, under certain conditions (which hold
in a clique tree), we can guarantee that this process converges to a solution satisfying all of the
equations in equation (11.10); the other equations are now easy to satisfy.
Each assignment step deﬁned by a ﬁxed-point equation corresponds to a message passing
step, where an outgoing message δi→j is deﬁned in terms of incoming messages δk→i. The
fact that the process requires multiple assignments to converge corresponds to the fact that
inference requires multiple message passing steps. In this speciﬁc example, a particular order of
applying the ﬁxed-point equation reconstructs the sum-product message passing algorithm in
cluster trees shown in algorithm 10.2. As we will see, however, when we consider other variants
of the optimization problem, the associated ﬁxed-point equations result in new algorithms.

11.3. Propagation-Based Approximation
391
(b)
(c)
(a)
D
B
A
C
A,B,D
B,C,D
B,D
1: A,B
2: B,C
B
A
C
4: A,D
3: C,D
D
Figure 11.1
An example of a cluster graph. (a) A simple network. (b) A clique tree for the network in
(a). (c) A cluster graph for the same network.
11.3
Propagation-Based Approximation
In this section, we consider approximation methods that use exactly the same message propa-
gation as in exact inference. However, these propagation schemes use a general-purpose cluster
graph, as in deﬁnition 10.1, rather than a clique tree. Since the constraints deﬁning a clique
tree were crucial in ensuring exact inference, the message-propagation schemes that use cluster
graphs will generally not provide the correct answers.
We begin by deﬁning the general message passing algorithm in a cluster graph. We then
show that it can be derived, using the same process as in the previous section, from a set of
ﬁxed-point equations induced by the stationary points of an approximate energy functional.
11.3.1
A Simple Example
Consider the simple Markov network of ﬁgure 11.1a. Recall that, to perform exact inference within
this network, we must ﬁrst reduce it to a tree, such as the tree of ﬁgure 11.1b. Inference in this
simple tree involves passing messages over the sepset, which consists of the variables {B, D}.
Now suppose that, instead, we perform inference as follows. We set up four clusters, which
correspond to the four initial potentials: C1 = {A, B}, C2 = {B, C}, C3 = {C, D}, C4 =
{A, D}. We connect these clusters to each other as shown in the cluster graph of ﬁgure 11.1c.
Note that this cluster graph contains loops (undirected cycles), and is therefore not a tree;
such graphs are often called loopy. Nevertheless, we can apply the belief-update propagation
algorithm CTree-BU-calibrate (algorithm 10.3). Although in our discussion of that algorithm we
assumed that the input is a tree, there is nothing in the algorithm itself that relies on that fact.
In each step of the algorithm we propagate a message between neighboring clusters. Thus, it is
perfectly applicable to a general cluster graph that may not necessarily be a tree.
The clusters in this cluster graph are smaller than those in the clique tree of ﬁgure 11.1b;
therefore, the message passing steps are less expensive. But what is the result of this procedure?
Suppose we propagate messages in the following order µ1,2, µ2,3, µ3,4, and then µ4,1. In the
ﬁrst message, the {A, B} cluster passes information to the {B, C} cluster through a marginal
distribution on B. This information is then propagated to next cluster, and so on. However, in
the ﬁnal message µ4,1, this information reaches the original cluster, but this time as observation

392
Chapter 11. Inference as Optimization
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0
5
10
15
20
Iteration #
P(a1)
True posterior
Figure 11.2
An example run of loopy belief propagation in the simple network of ﬁgure 11.1a. In this
run, all potentials prefer consensus assignments over nonconsensus ones. In each iteration, we perform
message passing for all the edges in the cluster graph of ﬁgure 11.1b.
about the values of A. As an example, suppose all clusters favor consensus joint assignments;
that is, β1(a0, b0) and β1(a1, b1) are much larger than β1(a1, b0) and β1(a0, b1), and similarly
for the other beliefs. Thus, if the message µ1,2 strengthens the belief that B = b1, then the
message µ2,3 will increase the belief in C = c1 and so on. Once we get around the loop, the
message µ4,1 will strengthen the support in A = a1. This message will be incorporated into the
cluster as though it were independent evidence that did not depend on the initial propagation.
Now, if we continue to apply the same sequence of propagations again, we will keep increasing
the beliefs in the assignment of A = a1. This behavior is illustrated in ﬁgure 11.2. As we can
see, in later iterations the procedure overestimates the marginal probability of A. However, the
eﬀect of the “feedback” decays until the iterations converge.
This simple experiment already suggests several important issues we need to consider:
•
In the case of cluster trees, we described a sequence of message propagations that calibrate
the tree in two passes. Once the tree is calibrated, additional message propagations do not
change any of the beliefs. Thus, we can say that the propagation process has converged.
convergence
When we consider our example, it seems clear that the process may not converge in

two passes, since information from one pass will circulate and aﬀect the next round.
Indeed, it is far from clear that the propagation of beliefs necessarily converges at all.
•
In the case of cluster trees, we saw that, in a calibrated tree, each cluster of beliefs is the joint
marginal of the cluster variables. As our example suggests, for cluster graph propagation, the
beliefs on A are not necessarily the marginal probability in PΦ. Thus, the question is the
relationship between the calibrated cluster graph and the actual probability distribution.
Before we address these questions, we present the algorithm in more general terms.

11.3. Propagation-Based Approximation
393
1: A, B, C
B
B
D
B
C
E
D
4: B, E
2: B, C, D
5: D, E
3: B, D, F
1: A, B, C
B
B
D
B,C
E
D
4: B, E
2: B, C, D
5: D, E
3: B, D, F
Figure 11.3
Two examples of generalized cluster graph for an MRF with potentials over {A, B, C},
{B, C, D}, {B, D, F}, {B, E} and {D, E}.
Box 11.A — Case Study: Turbocodes and loopy belief propagation. The idea of propagating
messages in loopy graphs was ﬁrst proposed in the early days of the ﬁeld, in parallel with the
introduction of the ﬁrst exact inference algorithms. As we discussed in box 9.B, one of the ﬁrst
inference algorithms was Pearl’s message passing for singly connected Bayesian networks (polytrees).
In his 1988 book, Pearl says:
When loops are present, the network is no longer singly connected and local propagation
schemes will invariably run into trouble . . . If we ignore the existence of loops and permit the
nodes to continue communicating with each other as if the network were singly connected,
messages may circulate indeﬁnitely around the loops and the process may not converge
to a stable equilibrium . . . Such oscillations do not normally occur in probabilistic networks
. . . which tend to bring all messages to some stable equilibrium as time goes on. However, this
asymptotic equilibrium is not coherent, in the sense that it does not represent the posterior
probabilities of all nodes of the networks.
As a consequence of these problems, the idea of loopy belief propagation was largely abandoned
loopy belief
propagation
for many years.
Surprisingly, the revival of loopy belief propagation is due to a seemingly unrelated advance
in coding theory. The area of coding addresses the problem of sending messages over a noisy
channel, and recovering it from the garbled result. Formally, the coding task can be deﬁned as
follows. We wish to send a k-bit message u1, . . . , uk. We code the message using a number of bits
x1, . . . , xn, which are then sent over the noisy channel, resulting in a set of (possibly corrupted)
outputs y1, . . . , yn, which can be either discrete or continuous. Diﬀerent channels introduce noise
in diﬀerent ways: In a simple Gaussian noise model, each bit sent is corrupted independently
by the addition of some Gaussian noise; another simple model ﬂips each bit independently with
some probability; more complex channel models, where noise is added in a correlated way to
consecutive bits, are also used. The message decoding task is to recover an estimate ˆu1, . . . , ˆuk
message
decoding
from y1, . . . , yn. The bit error rate is the probability that a bit is ultimately decoded incorrectly.
This error rate depends on the code and decoding algorithm used and on the amount of noise in
the channel. The rate of a code is k/n — the ratio between the number of bits in the message and
the number of bits used to transmit it.
For example, a very simple repetition code takes each bit and transmits it three times, then
decodes the bit by majority voting on the three (noisy) copies received. If the channel corrupts each
bit with probability p, the bit error rate of this algorithm is p3 + 3p2, which, for reasonable values

394
Chapter 11. Inference as Optimization
(b)
(a)
Y1
Y2
Y3
Y4
X1
X2
X3
X4
U1
U2
U3
U4
W1
U1
Y1
Y7
Y3
Y5
Z1
X2
W2
U2
Z2
W3
Permuter
U3
Z3
X6
Y2
Y6
W4
U4
Y4
Y8
X4
X8
Z4
X5
X6
X7
Y5
Y6
Y7
Figure 11.A.1 — Two examples of codes (a) A k = 4, n = 7 parity check code, where every four
message bits are sent along with three bits that encode parity checks. (b) A k = 4, n = 8 turbocode. Here,
the Xa bits X1, X3, X5, X7 are simply the original bits U1, U2, U3, U4 and are omitted for clarity of the
diagram; the Xb bits use a shift register — a state bit that changes with each bit of the message, where
the ith state bit depends on the (i −1)st state bit and on the ith message bit. The code uses two shift
registers, one applied to the original message bits and one to a set of permuted message bits (using some
predetermined permutations). The sent bits contain both the original message bits and some number of
the state bits.
of p, is much lower than p. The rate of this code is 1/3, because for every message bit, three bits
are transmitted. In general, we can get better bit error rates by increasing the redundancy of the
code, so we want to compare the bit error rate of diﬀerent codes that have the same rate. Repetition
codes are some of the least eﬃcient codes designed. Figure 11.A.1a shows a simple rate 4/7 parity
check code, where every four message bits are sent along with three bits that encode parity checks
(exclusive ORs) of diﬀerent subsets of the four bits.
In 1948, Claude Shannon provided a theoretical analysis of the coding problem (Shannon 1948).
For a given rate, Shannon provided an upper bound on the maximum noise level that can be
tolerated while still achieving a certain bit error rate, no matter which code is used. Shannon also
showed that there exist channel codes that achieve this limit, but his proof was nonconstructive —

11.3. Propagation-Based Approximation
395
he did not present practical encoders and decoders that achieve this limit.
Since Shannon’s landmark result, multiple codes were suggested. However, despite a gradual
improvement in the quality of the code (bit-error rate for a given noise level), none of the codes even
came close to the Shannon limit. The big breakthrough came in the early 1990s, when Berrou et al.
(1993) came up with a new scheme that they called a turbocode, which, empirically, came much
turbocode
closer to achieving the Shannon limit than any other code proposed up to that point. However,
their decoding algorithm had no theoretical justiﬁcation, and, while it seemed to work well in real
examples, could be made to diverge or converge to the wrong answer. The second big breakthrough
was the subsequent realization that turbocodes were simply performing belief propagation on a
Bayesian network representing the probability model for the code and the channel noise.
To understand this, we ﬁrst observe that message decoding can easily be reformulated as a
probabilistic inference task: We have a prior over the message bits U = ⟨U1, . . . , Uk⟩, a (usually
deterministic) function that deﬁnes how a message is converted into a sequence of transmitted
bits X1, . . . , Xn, and another (stochastic) model that deﬁnes how the channel randomly corrupts
the Xi’s to produce Yi’s. The decoding task can then be viewed as ﬁnding the most likely joint
assignment to U given the observed message bits y = ⟨y1, . . . , yn⟩, or (alternatively) as ﬁnding
the posterior P(Ui | y) for each bit Ui. The ﬁrst task is a MAP inference task, and the second
task one of computing posterior probabilities. Unfortunately, the probability distribution is of high
dimension, and the network structure of the associated graphical model is quite densely connected
and with many loops.
The turbocode approach, as ﬁrst proposed, comprised both a particular coding scheme, and the
use of a message passing algorithm to decode it. The coding scheme transmits two sets of bits:
one set comprises the original message bits Xa = ⟨Xa
1 , . . . , Xa
k⟩= u, and the second some
set Xb = ⟨Xb
1, . . . , Xb
k⟩of transformed bits (like the parity check bits, but more complicated).
The received bits then can also be partitioned into the noisy ya, yb.
Importantly, the code is
designed so that the message can be decoded (albeit with errors) using either ya or yb.
The
turbocoding algorithm then works as follows: It uses the model of Xa (trivial in this case) and of
the channel noise to compute a posterior probability over U given ya. It then uses that posterior
πa(U1), . . . , πa(Uk) as a prior over U and computes a new posterior over U, using the model
for Xb and the channel, and yb as the evidence, to compute a new posterior πb(U1), . . . , πb(Uk).
The “new information,” which is πb(Ui)/πa(Ui), is then transmitted back to the ﬁrst decoder, and
the process repeats until a stopping criterion is reached. In eﬀect, the turbocoding idea was to use
two weak coding schemes, but to “turbocharge” them using a feedback loop. Each decoder is used to
decode one subset of received bits, generating a more informed distribution over the message bits to
be subsequently updated by the other. The speciﬁc method proposed used particular coding scheme
for the Xb bits, illustrated in ﬁgure 11.A.1b.
This process looked a lot like black magic, and in the beginning, many people did not even
believe that the algorithm worked. However, when the empirical success of these properties was
demonstrated conclusively, an attempt was made to understand its theoretical properties. McEliece
et al. (1998) subsequently showed that the speciﬁc message passing procedure proposed by Berrou et
al. is precisely an application of belief propagation (with a particular message passing schedule) to
the Bayesian network representing the turbocode (as in ﬁgure 11.A.1b).

This revelation had a tremendous impact on both the coding theory community and
the graphical models community.
For the former, loopy belief propagation provides
a general-purpose algorithm for decoding a large family of codes.
By separating the

396
Chapter 11. Inference as Optimization
algorithmic question of decoding from the question of the code design, it allowed the
development of many new coding schemes with improved properties. These codes have
come much, much closer to the Shannon limit than any previous codes, and they have
revolutionized both the theory and the practice of coding.
For the graphical models
community, it was the astounding success of loopy belief propagation for this application
that led to the resurgence of interest in these approaches, and subsequently to much of
the work described in this chapter.
11.3.2
Cluster-Graph Belief Propagation
The basis for our message passing algorithm is the cluster graph of deﬁnition 10.1, ﬁrst deﬁned in
cluster graph
section 10.1.1. In that section, we required that cluster graphs be trees and that they respect the
running intersection property. Those requirements led us to the deﬁnition of a clique tree. Here,
we remove the ﬁrst of these two assumptions, allowing inference to be performed on a loopy
cluster graph. However, we still wish to require a variant of the running intersection property
that is generalized to this case: for any two clusters containing X, there is precisely one path
between them over which information about X can be propagated.
Deﬁnition 11.2
We say that U satisﬁes the running intersection property if, whenever there is a variable X such
running
intersection
property
that X ∈Ci and X ∈Cj, then there is a single path between Ci and Cj for which X ∈Se for
all edges e in the path.
This generalized running intersection property implies that all edges associated with X form a
tree that spans all the clusters that contain X. Thus, intuitively, there is only a single path by
which information that is directly about X can ﬂow in the graph. Both parts of this assumption
are signiﬁcant. The fact that some path must exist forces information about X to ﬂow between
all clusters that contain it, so that, in a calibrated cluster graph, all clusters must agree about the
marginal distribution of X. The fact that there is at most one path prevents information about
X from cycling endlessly in a loop, making our beliefs more extreme due to “cyclic arguments.”
Importantly, however, since the graph is not necessarily a tree, the same pair of clusters
might also be connected by other paths. For example, in the cluster graph of ﬁgure 11.3a, we
see that the edges labeled with B form a subtree that spans all the clusters that contain B.
However, there are loops in the graph. For example, there are two paths from C3 = {B, D, F}
to C2 = {B, C, D}. The ﬁrst, through C4, propagates information about B, and the second,
through C5, propagates information about D. Thus, we can still get circular reasoning, albeit
less directly than we would in a graph that did not satisfy the running intersection property;
we return to this point in section 11.3.8. Note that while in the case of trees the deﬁnition of
running intersection implied that Si,j = Ci ∩Cj, in a graph this equality is no longer enforced
by the running intersection property. For example, cliques C1 and C2 in ﬁgure 11.3a have B in
common, but S1,2 = {C}.
In clique trees, inference is performed by calibrating beliefs. In a cluster graph, we can also
associate cluster Ci with beliefs βi. We now say that a cluster graph is calibrated if for each
beliefs
calibrated cluster
graph

11.3. Propagation-Based Approximation
397
edge (i–j), connecting the clusters Ci and Cj, we have that
X
Ci−Si,j
βi =
X
Cj−Si,j
βj;
that is, the two clusters agree on the marginal of variables in Si,j.
Note that this deﬁni-
tion is weaker than cluster tree calibration, since the clusters do not necessarily agree on the
joint marginal of all the variables they have in common, but only on those variables in the
sepset. However, if a calibrated cluster graph satisﬁes the running intersection property, then
the marginal of a variable X is identical in all the clusters that contain it.
Algorithm 11.1 Calibration using sum-product belief propagation in a cluster graph
Procedure CGraph-SP-Calibrate (
Φ,
// Set of factors
U
// Generalized cluster graph Φ
)
1
Initialize-CGraph
2
while graph is not calibrated
3
Select (i–j) ∈EU
4
δi→j(Si,j) ←SP-Message(i, j)
5
for each clique i
6
βi ←ψi · Q
k∈Nbi δk→i
7
return {βi}
Procedure Initialize-CGraph (
U
)
1
for each cluster Ci
2
βi ←Q
φ : α(φ)=i φ
3
for each edge (i–j) ∈EU
4
δi→j ←1
5
δj→i ←1
6
Procedure SP-Message (
i,
// sending clique
j
// receiving clique
)
1
ψ(Ci) ←ψi · Q
k∈(Nbi−{j}) δk→i
2
τ(Si,j) ←P
Ci−Si,j ψ(Ci)
3
return τ(Si,j)
How do we calibrate a cluster graph?
Because calibration is a local property that relates
adjoining clusters, we want to try to ensure that each cluster is sharing information with its

398
Chapter 11. Inference as Optimization
A1,1
A1,4
A1,3
A1,2
A2,1
A2,4
A2,3
A2,2
A3,1
A3,4
A3,3
A3,2
A4,1
A4,4
A4,3
A4,2
Figure 11.4
An example of a 4 × 4 two-dimensional grid network
neighbors. From the perspective of a single cluster Ci, there is not much diﬀerence between a
cluster graph and a cluster tree. The cluster is related to each neighboring cluster through an
edge that conveys information on variables in the sepset. Thus, we can transmit information by
simply having one cluster pass a message to the other.
However, a priori, it is not clear how we can execute a message passing algorithm over a
loopy clustergraph. In particular, the sum-product calibration of algorithm 10.2 sends a message
only when the sending clique is ready to transmit, that is, when all other incoming messages
have been received. In the loopy cluster graph, initially, there is no cluster that has received
any incoming messages. Thus, no cluster is ready to transmit, and the algorithm is deadlocked.
However, in section 10.3, we showed that the two algorithms are actually equivalent; that is, any
sequence of sum-product propagation steps can be emulated by the same sequence of belief-
update propagation steps and leads to the same beliefs. In this transformation, we have that
µi,j = δi→jδj→i. Thus, we can construct a “deadlock-free” variant of the sum-product message
passing algorithm simply by initializing all messages δi→j = 1. This initialization of the sum-
product algorithm is equivalent to the standard initialization of the belief update algorithm, in
which µi,j = 1. Importantly, in this variant of the sum-product algorithm, each cluster begins
with all of the incoming messages initialized, and therefore it can send any of the outgoing
messages at any time, without waiting for any other cluster.
Algorithm 11.1 shows the sum-product message passing algorithm for cluster graphs; other
than the fact that the algorithm is applied to graphs rather than trees, the algorithm is identical
to CTree-SP-Calibrate. In much the same manner, we can adapt CTree-BU-Calibrate to deﬁne
a procedure CGraph-BU-Calibrate that operates over cluster graphs using belief-update message
passing steps. Both of these algorithms are instances of a general class of algorithms called
cluster-graph belief propagation, which passes messages over cluster graphs.
cluster-graph
belief
propagation
Before we continue, we note that cluster-graph belief propagation can be signiﬁcantly cheaper
than performing exact inference. A canonical example of a class of networks that is compactly
representable yet hard for inference is the class of grid-structured Markov networks (such as the
ones used in image analysis; see box 4.B). In these networks, each variable Ai,j corresponds
to a point on a two-dimensional grid. Each edge in this network corresponds to a potential
between adjacent points on the grid, with Ai,j connected to the four nodes Ai−1,j, Ai+1,j,

11.3. Propagation-Based Approximation
399
A3,2 , A3,3
A3,1 , A3,2
A2,2 , A2,3
A2,1 , A2,2
A2,1
A2,2
A2,2
A2,3
A3,1
A3,2
A3,2
A3,3
A2,1
A2,2
A2,3
A1,3 , A2,3
A1,1 , A2,1
A1,2 , A2,2
A2,3 , A3,3
A2,1 , A3,1
A2,2 , A3,2
A1,2 , A1,3
A1,1 , A1,2
A1,1
A1,2
A1,2
A1,3
Figure 11.5
An example of generalized cluster graph for a 3 × 3 grid network
Ai,j−1, Ai,j+1 (except for nodes Ai,j on the boundary of the grid); see ﬁgure 11.4. Such a
network has only pairwise potentials, and hence it is very compactly represented. Yet, exact
inference requires separating sets, which are as large as cutsets in the grid. Hence, in an n × n
grid, exact computation is exponential in n.
However, we can easily create a generalized cluster graph for grid networks that directly
corresponds to the factors in the network. In this cluster graph, each cluster represents beliefs
over two neighboring grid variables, and each cluster has a small number of adjoining edges that
connect it to other clusters that share one of the two variables. See ﬁgure 11.5 for an example
for a small 3 × 3 grid. (Note that there are several ways of constructing such a cluster graph;
this ﬁgure represents one reasonable choice.) A round of propagations in the generalized cluster
graph is linear in the size of the grid (quadratic in n).
11.3.3
Properties of Cluster-Graph Belief Propagation
What can we say about the properties and guarantees provided by cluster-graph belief propa-
gation? We now consider some of the ramiﬁcations of the “mechanical” operation of message
passing in the graph. Later, when we discuss cluster-graph belief propagation as an optimization
procedure, we will revisit this question from a diﬀerent perspective.
11.3.3.1
Reparameterization
Recall that in section 10.2.3 we showed that belief propagation maintains an invariant property.
This allowed us to show that the convergence point represents a reparameterization of the
reparameteriza-
tion
original distribution. We can directly extend this property to cluster graphs, resulting in a cluster
cluster graph
invariant
graph invariant.

400
Chapter 11. Inference as Optimization
Theorem 11.4
Let U be a generalized cluster graph over a set of factors Φ. Consider the set of beliefs {βi} and
sepsets {µi,j} at any iteration of CGraph-BU-Calibrate; then
˜PΦ(X) =
Q
i∈VU βi[Ci]
Q
(i–j)∈EU µi,j[Si,j].
where ˜PΦ(X) = Q
φ∈Φ φ is the unnormalized distribution deﬁned by Φ.
Proof Recall that βi = ψi
Q
j∈Nbi δj→i and that µi,j = δj→iδi→j. We now have
Q
i∈VU βi[Ci]
Q
(i–j)∈EU µi,j[Si,j]
=
Q
i∈VU ψi[Ci] Q
j∈Nbi δj→i[Si,j]
Q
(i–j)∈EU δj→i[Si,j]δi→j[Si,j]
=
Y
i∈VU
ψi[Ci]
=
Y
φ∈Φ
φ(U φ) = ˜PΦ(X).
Note that the second step is based on the fact that each message δi→j appears exactly once in
the numerator and the denominator and thus can be canceled.

This property shows that cluster-graph belief propagation preserves all of the informa-
tion about the original distribution. In particular, it does not “dilute” the original factors
by performing propagation along loops. Hence, we can view the process as trying to
represent the original factors anew in a more useful form.
11.3.3.2
Tree Consistency
Recall that theorem 10.4 implies that, in a calibrated cluster tree, the belief over a cluster is the
marginal of the distribution. Thus, in a calibrated cluster tree, we can “read oﬀ” the marginals
of PΦ locally from clusters that contain them. More precisely, by normalizing the beliefs factor
βi (so that it sums to 1), we get the marginal distribution over Ci.
An obvious question
is whether a corresponding property holds for cluster-graph belief propagation. Suppose we
manage to calibrate a generalized cluster graph and normalize the resulting beliefs; do we have
an interpretation for the beliefs in each cluster?
As we saw in our simple example (ﬁgure 11.2), the beliefs we compute by BU-message are
not necessarily marginals of PΦ, but rather an approximation. Can we say anything about the
quality of this approximation? To characterize the beliefs we get at the end of the process, we
can use the cluster tree invariant property applied to subtrees of a cluster graph.
Consider a subtree T of U; that is, a subset of clusters and edges that together form a
tree that satisﬁes the running intersection property. For example, consider the cluster graph of
ﬁgure 11.1c. If we remove one of the clusters and its incident edges, we are left with a proper
cluster tree. Note that the running intersection property is not necessarily as easy to achieve in
general, since removing some edges from the cluster graph may result in a graph that violates
the running intersection property relative to a variable, necessitating the removal of additional
edges, and so on.

11.3. Propagation-Based Approximation
401
Once we select a tree T , we can think of it as deﬁning a distribution
PT (X) =
Q
i∈VT βi(Ci)
Q
(i–j)∈ET µi,j[Si,j].
If the cluster graph is calibrated, then by deﬁnition so is T . And so, because T is a tree that
satisﬁes the running intersection property, we can apply theorem 10.4, and we conclude that
βi(Ci) = PT (Ci).
(11.11)
That is, the beliefs over Ci in the tree are the marginal of PT , a property called tree consistency.
tree consistency
As a concrete example, consider the cluster graph of ﬁgure 11.1c.
Removing the cluster
C4 = {A, D}, we are left with a proper cluster tree T . The preceding argument implies that
once we have calibrated the cluster graph, we have β1(A, B) = PT (A, B). This result suggests
that β1(A, B) ̸= PΦ(A, B); to show this formally, contrast equation (11.11) with theorem 11.4.
We see that the tree distribution involves some of the terms that deﬁne the joint distribution.
Thus, we can conclude that
PT (A, B, C, D) = PΦ(A, B, C, D)µ3,4[D]µ1,4[A]
β4(A, D)
.
We see that unless β4(A, D) = µ3,4[D]µ1,4[A], PT will be diﬀerent from PΦ. This conclusion
suggests that, in this example, the beliefs β1(A, B) in the calibrated cluster graph are not the
marginal PΦ(A, B).
Clearly, we can apply the same type of reasoning using other subtrees of U. And so we
reach the surprising conclusion that equation (11.11) must hold with respect to every cluster
tree embedded in U. In our example, we can see that by removing a single cluster, we can
construct three diﬀerent trees that contain C1. The same beliefs β1(A, B) are the marginal of
the three distributions deﬁned by each of these trees. While these three distributions agree on
the joint marginal of A and B, they can diﬀer on the joint marginal distributions of other pairs
of variables.
Moreover, these subtrees allow us to get insight about the quality of the marginal distributions
we read from the calibrated cluster graph. Consider our example again: we can use the residual
cluster graph
residual
term µ3,4[D]µ1,4[A]
β4(A,D)
to analyze the error in the marginal distribution. In this simple example, this
analysis is fairly straightforward (see exercise 11.4).
In other cases, the analysis can be more complex. For example, suppose we want to ﬁnd a
subtree in the cluster graph for a grid (e.g., ﬁgure 11.5). To construct a tree, we must remove a
nontrivial number of clusters. More precisely, because each cluster corresponds to an edge in
the grid, a cluster tree corresponds to a subtree of the grid. For an n × n grid, such a tree will
have at most n2 −1 edges of the 2n(n −1) edges in the grid. Thus, each cluster tree contains
about half of the clusters in the original cluster graph. In such a situation the residual term is
more complex, and we cannot necessarily evaluate it.
11.3.4
Analyzing Convergence ⋆
A key question regarding the belief propagation algorithm is whether and when it converges.
Indeed, there are many networks for which belief propagation does not converge; see box 11.C.

402
Chapter 11. Inference as Optimization
Although we cannot hope for convergence in all cases, it is important to understand when this
algorithm does converge. We know that if the cluster graph is a tree then the algorithm will
converge. Can we ﬁnd other classes of cluster graphs for which we can prove convergence?
One method of analyzing convergence is based on the following important perspective on
belief propagation. This analysis is easier to perform on a variant of BP called synchronous BP
synchronous BP
that performs all of the message updates simultaneously. Consider the update step that takes
all of the messages δt at a particular iteration t and produces a new set of messages δt+1 for
the next step. Letting ∆be the space of all possible messages in the cluster graph, we can view
the belief-propagation update operator as a function GBP
: ∆7→∆. Consider the standard
sum-product message update:
δ′
i→j ∝
X
Ci−Si,j
ψi ·
Y
k∈(Nbi−{j})
δk→i,
where we normalize each message to sum to 1; this renormalization step is essential to avoid a
degenerate convergence to the 0 message. We can now deﬁne the BP operator as the function
BP operator
that simultaneously takes one set of messages and computes a new one:
GBP ({δi→j}) = {δ′
i→j}.
The question of convergence of the algorithm now reduces to one of asking whether repeated
applications of the operator GBP are guaranteed to converge.
One interesting, albeit strong, condition that guarantees convergence is the contraction prop-
erty:
Deﬁnition 11.3
For a number α ∈[0, 1), an operator G over a metric space (∆, ID(; )) is an α-contraction relative
contraction
to the distance function ID(; ) if, for any δ, δ′ ∈∆, we have that:
ID(G(δ); G(δ′)) ≤αID(δ; δ′).
(11.12)
In other words, an operator is a contraction if its application to two points in the space is
guaranteed to decrease the distance between them by at least some constant factor α < 1.
A basic result in analysis shows that, under fairly weak conditions, if an operator G is a
contraction, we have that repeated applications of G are guaranteed to converge to a unique
ﬁxed point:
Proposition 11.2
Let G be an α-contraction of a complete metric space (∆, ID(; )). Then there is a unique ﬁxed-point
ﬁxed-point
δ∗for which G(δ∗) = δ∗. Moreover, for any δ, we have that
lim
n→∞Gn(δ) = δ∗.
The proof is left as an exercise (exercise 11.5).
Indeed, the contraction rate α can be used to provide bounds on the rate of convergence of
the algorithm to its unique ﬁxed point: To reach a point that is guaranteed to be within ϵ of δ∗,
it suﬃces to apply G the following number of times:
logα
ϵ
diameter(∆),

11.3. Propagation-Based Approximation
403
where diameter(∆) = maxδ,δ′∈∆ID(δ; δ′).
Applying this analysis to the operator G induced by the belief-propagation message update
is far from trivial. This operator is complex and nonlinear, because it involves both multiplying
messages and a renormalization step. A review of these analyses is outside the scope of this
book. At a high level, these results show that if the factors in the network are fairly “smooth,”
one can guarantee that the synchronous BP operator is a contraction and hence converges to a
unique ﬁxed point. We describe one of the simplest of these results, in order to give a ﬂavor for
this type of analysis.
This analysis applies to synchronous loopy belief propagation over a pairwise Markov network
with two-valued random variables Xi ∈{−1, +1}. Speciﬁcally, we assume that the network
model is parameterized as follows:
P(x1, . . . , xn) = 1
Z exp

X
(i,j)
ϵi,j(xi, xj) +
X
i
ϵi(xi),

,
where we assume for simplicity of notation that ϵi,j = 0 when Xi and Xj are not neighbors in
the network.
We begin by introducing some notation. The hyperbolic tangent function is deﬁned as:
hyperbolic
tangent
tanh(w) = ew −e−w
ew + e−w = e2w −1
e2w + 1.
The hyperbolic tangent has a shape very similar to the sigmoid function of ﬁgure 5.11a. The
following condition can be shown to suﬃce for GBP to be a contraction, and hence for the
convergence of belief propagation to a unique ﬁxed point:
max
i
max
j∈Nbi
X
k∈Nbi−{j}
tanh |ϵk,i| < 1.
(11.13)
Intuitively, this expression measures the total extent to which i’s neighbors other than j can
inﬂuence the message from i to j. The larger the magnitude of the parameters in the network,
the larger this sum.
The analysis of the more general case is signiﬁcantly more complex but shares the same
intuitions. At a very high level, if we can place strong bounds on the skew of the parameters in
a factor:
max
x,x′ φ(x)/φ(x′),
we can guarantee convergence of belief propagation. Intuitively, the lower the skew of the factors
in our network, the more each message update “smoothes out” diﬀerences between entries in
the messages, and therefore also makes diﬀerent messages more similar to each other.
While the conditions that underlie these theorems are usually too stringent to hold in practice,
this analysis does provide useful insight.
First, it suggests that networks with potentials

that are closer to deterministic are more likely to have problems with convergence, an
observation that certainly holds in practice. Second, although global contraction throughout
the space is a very strong assumption, a contraction property in a region of the space may
be plausible, guaranteeing convergence of the algorithm if it winds up (or is initialized) in this
region. These results and their ramiﬁcations are only now being explored.

404
Chapter 11. Inference as Optimization
11.3.5
Constructing Cluster Graphs
So far, we have taken the cluster graph to be given. However, the choice of cluster graph is
generally far from obvious, and it can make a signiﬁcant diﬀerence to the algorithm. Recall that,
even in exact inference, more than one clique tree can be used to perform inference for a given
distribution. However, while these diﬀerent trees can vary in their computational cost, they
all give rise to the same answers.
In the case of cluster graph approximations, diﬀerent

graphs can lead to very diﬀerent answers. Thus, when selecting a cluster graph, we have
to consider trade-oﬀs between cost and accuracy, since cluster graphs that allow fast
propagation might result in a poor approximation.
It is important to keep in mind that the structure of the cluster graph determines the prop-
agation steps the algorithm can perform, and thus dictate what type of information is passed
during the propagations. These choices directly inﬂuence the quality of the results.
Example 11.1
Consider, for example, the cluster graphs U1 and U2 of ﬁgure 11.3a and ﬁgure 11.3b. Both are
fairly similar, yet in U2 the edge between C1 and C2 involves the marginal distribution over B
and C. On the other hand, in U1, we propagate the marginal only over C. Intuitively, we expect
inference in U2 to better capture the dependencies between B and C. For example, assume that
the potential of C1 introduces strong correlations between B and C (say B = C). In U2, this
correlation is conveyed to C2 directly. In U1, the marginal on C is conveyed on the edge (1–2),
while the marginal on B is conveyed through C4. In this case, the strong dependency between
the two variables is lost. In particular, if the marginal on C is diﬀuse (close to uniform), then the
message C1 sends to C4 will also have a uniform distribution on B, and from C2’s perspective
the messages on B and C will appear as two independent variables.
On the other hand, if we introduce many messages between clusters or increase the scope
of these messages, we run the risk of constructing a tree that violates the running intersection
property. And so, we have to worry about methods that ensure that the resulting structure is a
proper cluster graph. We now consider several approaches for constructing cluster graphs.
11.3.5.1
Pairwise Markov Networks
We start with the class of pairwise Markov networks. In these networks, we have a univariate
pairwise Markov
networks
potential φi[Xi] over each variable Xi, and in addition a pairwise potential φ(i,j)[Xi, Xj]
over some pairs of variables.
These pairwise potentials correspond to edges in the Markov
network. Many problems are naturally formulated as pairwise Markov networks, including the
grid networks we discussed earlier and Boltzmann distributions (see box 4.C). Indeed, if we are
willing to transform our variables, any distribution can be reformulated as a pairwise Markov
network (see exercise 11.10).
One straightforward transformation of such a network into a cluster graph is as follows:
For each potential, we introduce a corresponding cluster, and put edges between the clusters
that have overlapping scope. In other words, there is an edge between the cluster C(i,j) that
corresponds to the edge Xi—Xj and the clusters Ci and Cj that correspond to the univariate
factors over Xi and Xj. Figure 11.6 illustrates this construction in the case of a 3 by 3 grid
network.
Because there is a direct correspondence between the clusters in the cluster graphs and vari-

11.3. Propagation-Based Approximation
405
A3,2 , A3,3
A3,1 , A3,2
A3,1
A3,2
A3,3
A2,2 , A2,3
A2,1 , A2,2
A2,1
A2,2
A2,3
A1,3 , A2,3
A1,1 , A2,1
A1,2 , A2,2
A2,3 , A3,3
A2,1 , A3,1
A2,2 , A3,2
A1,2 , A1,3
A1,1 , A1,2
A1,1
A1,2
A1,3
Figure 11.6
A generalized cluster graph for the 3 × 3 grid when viewed as pairwise MRF
ables or edges in the original Markov network, it is often convenient to think of the propagation
steps as operations on the original network. Moreover, since each pairwise cluster has only two
neighbors, we consider two propagation steps along the path Ci—C(i,j)—Cj as propagating
information between Xi and Xj. (See exercise 11.9.) Indeed, early versions of cluster-graph be-
lief propagation were stated in these terms. This algorithm is known as loopy belief propagation,
loopy belief
propagation
since it uses propagation steps used by algorithms for Markov trees, except that it was applied
to networks with loops.
11.3.5.2
Bethe Cluster Graph
A natural question is how we can extend this idea to networks that are more complex than
pairwise Markov networks. Once we have larger potentials, they may overlap in ways that result
in complex interactions among them.
One simple construction, called the Bethe cluster graph, uses a bipartite graph. The ﬁrst layer
Bethe cluster
graph
consists of “large” clusters, with one cluster for each factor φ in Φ, whose scope is Scope[φ].
These clusters ensure that we satisfy the family-preservation property. The second layer consists
of “small” univariate clusters, one for each random variable. Finally, we place an edge between
each univariate cluster X on the second layer and each cluster in the ﬁrst layer that includes
X; the scope of this edge is X itself. For a concrete example, see ﬁgure 11.7a.
We can easily verify that this cluster graph is a proper one. First, by construction, it satisﬁes
the family preservation property. Second, the edges that mention a variable X form a star-
shaped subgraph with edges from the univariate cluster for X to all the large clusters that
contain X.
It is also easy to check that, if we apply this procedure to a pairwise Markov
network, it results in the “natural” cluster graph for the pairwise network that we discussed. The
construction of this cluster graph is simple and can easily be automated.

406
Chapter 11. Inference as Optimization
12: B, C
1: A, B, C
2: B, C, D
6: A
11: F
10: E
9: D
8: C
7: B
11: F
10: E
9: D
8: C
7: B
6: A
1: A, B, C
2: B, C, D
4: B, E
5: D, E
3: B, D, F
4: B, E
5: D, E
3: B, D, F
(a)
3
(b)
4
Figure 11.7
Examples of generalized cluster graphs for network with potentials over {A, B, C},
{B, C, D}, {B, D, F}, {B, E} and {D, E}. For visual clarity, sepsets have been omitted — the sepset
between any pair of clusters is the intersection of their scopes. (a) Bethe factorization. (b) Capturing
interactions between {A, B, C} and {B, C, D}.
11.3.5.3
Beyond Marginal Probabilities
The main limitation of using the Bethe cluster graph is that information between diﬀerent
clusters in the top level is passed through univariate marginal distributions. Thus, interactions
between variables are lost during propagations. Consider the example of ﬁgure 11.7a. Suppose
that C1 creates a strong dependency between B and C. These two variables are shared with
C2. However, the messages between two clusters are mediated through the univariate factors.
And thus, interactions introduced by one cluster are not directly propagated to the other.
One possible solution is to merge some of the large clusters. For example, if we want to
capture the interactions between C1 and C2 in ﬁgure 11.7a, we can replace both of them by
a cluster with the score A, B, C, D. This new cluster will allow us to capture the interactions
between the factors involved in these two clusters. This modiﬁcation, however, comes at a price,
since the cost of manipulating a cluster grows exponentially with this scope. Moreover, this
approach seems excessive in this case, since we can summarize these interactions simply using
a distribution over B and C. This intuition suggests the construction of ﬁgure 11.7b. Note that
this cluster graph is equivalent to ﬁgure 11.3b; see exercise 11.6.
Can we generalize this construction?
A reasonable goal might be to capture all pairwise
interactions.
We can try to use a construction similar to the Bethe approximation, but in-
troducing an intermediate level that includes pairwise clusters.
In the same manner as we
introduced C12 in ﬁgure 11.7b, we can introduce other pairs that are shared by more than two
clusters. As a concrete example, consider the factors C1 = {A, B, C}, C2 = {B, C, D}, and
C3 = {A, C, D}. The relevant pairwise factors that capture interactions among these clusters

11.3. Propagation-Based Approximation
407
1: A, B, C
2: B, C, D
4: B, C
5: A, C
(a)
(b)
6: C, D
3: A, C, D
1: A, B, C
2: B, C, D
4: B, C
5: A
6: C, D
3: A, C, D
Figure 11.8
Examples of generalized cluster graph for network with potentials over {A, B, C},
{B, C, D}, and {A, C, D}. For visual clarity, sepsets have been omitted — the sepset between any pair
of clusters is the intersection of their scopes. (a) A Bethe-like factorization with pairwise marginals that
leads to an illegal cluster graph. (b) One possible way to make this graph legal.
are {B, C} = C1 ∩C2, {C, D} = C2 ∩C3, and {A, C} = C1 ∩C3. The resulting cluster
graph appears in ﬁgure 11.8a. Unfortunately, a quick check shows that this cluster graph does
not satisfy the running intersection property — all the edges in this graph are labeled by C,
and together they form a loop. As a result, information concerning C can propagate indeﬁnitely
around the loop, “overcounting” the eﬀect of C in the result.
How do we avoid this problem? In this speciﬁc example, we can consider a weaker approx-
imation by removing C from one of the intersection sets. For example, if we remove C from
C5, we get the cluster graph of ﬁgure 11.8b. This cluster graph satisﬁes the running intersection
property. An alternative approach tries to “compensate” somehow for the violation of the run-
ning intersection property using a more complex message passing algorithm; see section 11.3.7.3.
Box 11.B — Skill: Making loopy belief propagation work in practice. One of the main prob-
lems with loopy belief propagation is nonconvergence. This problem is particularly serious when
belief
propagation
nonconvergence
we build systems that use inference as a subroutine within other tasks, for example, as the inner
loop of a learning algorithm (see, for example, section 20.5.1). Several approaches have been used
for addressing this nonconvergence issue. Some are fairly simple heuristics. Others are more so-
phisticated, and typically are based on the characterization of cluster-graph belief propagation as
optimizing the approximate free-energy functional.
A ﬁrst observation is that, often, nonconvergence is a local problem. In many practical cases,
most of the beliefs in the network do converge, and only a small portion of the network remains
problematic. In such cases, it is often quite reasonable simply to stop the algorithm at some point
(for example, when some predetermined amount of time has elapsed) and use the beliefs at that
point, or a running average of the beliefs over some time window. This heuristic is particularly
reasonable when we are not interested in individual beliefs, but rather in some aggregate over the
entire network, for example, in a learning setting.
A second observation is that nonconvergence is often due to oscillations in the beliefs (see sec-
tion 11.3.1). This observation suggests that we dampen the oscillations by reducing the diﬀerence
between two subsequent updates. Consider the belief-propagation update rule in SP-Message(i, j):
δi→j ←
X
Ci−Si,j
ψi
Y
k̸=j
δk→i.

408
Chapter 11. Inference as Optimization
We can replace this line by a damped (or smoothed) version that averages the update δi→j with
damping
the previous message between the two cliques:
δi→j ←λ


X
Ci−Si,j
ψi
Y
k̸=j
δk→i

+ (1 −λ)δold
i→j,
(11.14)
where λ is the damping weight and δold
i→j is the previous value of the message. When λ = 1,
this update is equivalent to standard belief propagation. For 0 < λ < 1, the update is partial
and although it shifts βj toward agreement with βi, it leaves some momentum for the old value
of the belief, a dampening eﬀect that in turn reduces the ﬂuctuations in the beliefs. It turns out
that this damped update rule is “equivalent” to the original update rule, in that a set of beliefs
is a convergence point of the damped update if and only if it is a convergence point of standard
updates (see exercise 11.13). Moreover, one can show that, if run from a point close enough to a
stable convergence point of the algorithm, with a suﬃciently small λ, this damped update rule is
stable
convergence
point
guaranteed to converge. Of course, this guarantee is not very useful in practice, but there are indeed
many cases where the damped update rule is convergent, whereas the original update rule oscillates
indeﬁnitely.
A broader-spectrum heuristic, which plays an important role not only in ensuring convergence but
also in speeding it up considerably, is intelligent message scheduling. It is tempting to implement
message
scheduling
BP message passing as a synchronous algorithm, where all messages are updated at once. It turns
out that, in most cases, this schedule is far from optimal, both in terms of reaching convergence,
and in the number of messages required for convergence. The latter problem is easy to understand:
In a cluster graph with m edges, and diameter d, synchronous message passing requires m(d −1)
messages to pass information from one side of the graph to the other. By contrast, asynchronous
message passing, appropriately scheduled, can pass information between two clusters at opposite
ends of the graph using d −1 messages. Moreover, the fact that, in synchronous message passing,
each cluster uses messages from its neighbors that are based on their previous beliefs appears to
increase the chances of oscillatory behavior and nonconvergence in general.

In practice, an asynchronous message passing schedule works signiﬁcantly better than
asynchronous BP
the synchronous approach. Moreover, even greater improvements can be obtained by
scheduling messages in a guided way. One approach, called tree reparameterization (TRP),
tree reparameter-
ization
selects a set of trees, each of which spans a large number of the clusters, and whose union covers
all of the edges in the network. The TRP algorithm then iteratively selects a tree and does an
upward-downward calibration of the tree, keeping all other messages ﬁxed. Of course, calibrating
this tree has the eﬀect of “uncalibrating” other trees, and so this process repeats. This approach has
the advantage of passing information more globally within the graph. It therefore converges more
often, and more quickly, than other asynchronous schedules, particularly if the trees are selected
using a careful design that accounts for the properties of the problem.
An even more ﬂexible approach attempts to detect dynamically in which parts of the network
messages would be most useful.
Speciﬁcally, as we observed, often some parts of the network
converge fairly quickly, whereas others require more messages. We can schedule messages in a
way that accounts for their potential usefulness; for example, we can pass a message between
clusters where the beliefs disagree most strongly on the sepset. This approach, called residual belief
residual belief
propagation
propagation is convenient, since it is fully general and does not require a deep understanding of the
properties of the network. It also works well across a range of diﬀerent real-world networks.

11.3. Propagation-Based Approximation
409
An alternative general-purpose approach to avoiding nonconvergence is to directly optimize the
energy functional. Here, several methods have been proposed. The simplest is to use standard
optimization methods such as gradient ascent to optimize ˜F[ ˜PΦ, Q] (see appendix A.5.2 and
exercise 11.12). Other methods are more specialized to the form of the energy functional, and they
often turn out to be more eﬃcient (see section 11.7). Although these methods do improve convergence,
they are somewhat complex to implement, and have not (at this time) been used extensively in
practice.
It turns out that many of the parameter settings encountered during a learning algorithm are
problematic, and cause cluster-graph belief propagation to diverge. Intuitively, in many real-world
problems, “appropriate” parameters encode strong constraints that tend to drive the algorithm
toward well-behaved regions of the space. However, the parameters encountered during an iterative
learning procedure have no such properties, and often allow the algorithm to end up in diﬃcult
regions. One approach is to train some parameters of the model separately, using a simpler network.
We then use these parameters as our starting point in the general learning procedure. The use of
“reasonable” parameters in the model can stabilize BP, allowing it to converge within the context of
the general learning algorithm.
A ﬁnal problem with cluster-graph belief propagation is the fact that the energy functional
objective is multimodal, and so there are many local maxima to which a cluster-graph belief
local maxima
propagation algorithm might converge (if it converges).
One can, of course, apply any of the
standard approaches for addressing optimization of multimodal functions, such as initializing the
algorithm heuristically, or using multiple restarts with diﬀerent initializations. In the setting of BP,
initialization must be done with care, so as not to lose the connection to the correct underlying
distribution PΦ, as reﬂected by the invariant of theorem 11.4. In sum-product belief propagation,
we can simply initialize the messages to something other than 1. In belief update propagation, care
must be taken to initialize messages and beliefs in a coordinate way, to preserve PΦ.
Box 11.C — Case Study: BP in practice. To convey the behavior of belief propagation in practice,
we demonstrate its performance on an 11 × 11 (121 binary variables) Ising grid (see box 4.C). The
potentials of the network were randomly sampled as follows: Each univariate potential was sampled
uniformly in the interval [0, 1]; for each pair of variables Xi, Zj, wi,j is sampled uniformly in the
range [−C, C] (recall that in an Ising model, we deﬁne the negative log potential ϵi,j(xi, xj) =
−wi,jxixj). This sampling process creates an energy function where some potentials are attractive
(wi,j > 0) and some are repulsive (wi,j < 0), resulting in a nontrivial inference problem. The
magnitude of C (11 in this example) controls the magnitude of the energy forces and higher values
correspond, on average, to more challenging inference problems.
Figure 11.C.1 illustrates the convergence behavior on this problem. Panel (a) shows the percentage
of messages converged as a function of time for three variants of the belief propagation algorithm:
synchronous BP with damping (dashed line), where only a small fraction of the messages ever
converge; asynchronous BP with damping (smoothing) that converges (solid line); asynchronous BP
with no damping (dash-dot line) that does not fully converge. The beneﬁt of using asynchronous
propagation over synchronous updating is obvious. At ﬁrst, it appears as if smoothing messages is
not beneﬁcial. This is because some percentage of messages can converge quickly when updates are

410
Chapter 11. Inference as Optimization
Synchronous
Asynchronous
No smoothing
True
0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10
20
30
40
50
60
70
80
90 100
Time (seconds)
% of messages converged
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Time (seconds)
P (X10 = 0)
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.1
0.2
0.3
0.4
0.5
Time (seconds)
P(X115 = 0)
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Time (seconds)
P (X61 = 0)
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Time (seconds)
P (X17 = 0)
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Time (seconds)
P (X7 = 0)
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
0.5
(a)
(d)
(b)
(e)
(c)
(f)
Figure 11.C.1 — Example of behavior of BP in practice on an 11 × 11 Ising grid. (a)
Percentage
of messages converged as a function of time for three diﬀerent BP variants.
(b) A marginal where
both variants converge rapidly.
(c–e) Marginals where the synchronous BP marginals oscillate around
the asynchronous BP marginals. (f) A marginal where both variants are inaccurate.
not slowed down by smoothing. However, the overall beneﬁt of damping is evident, and without it
the algorithm never converges.
The remaining panels illustrate the progression of the marginal beliefs over the course of the
algorithm. (b) shows a marginal where both the synchronous and asynchronous updates converge
quite rapidly and are close to the true marginal (thin solid black). Such behavior is atypical, and
it comprises only around 10 percent of the marginals in this example. In the vast majority of the
cases (almost 80 percent in this example), the synchronous beliefs oscillate around the asynchronous
ones ((c)–(e)). In many cases, such as the ones shown in (e), the entropy of the synchronous beliefs
is quite signiﬁcant. For about 10 percent of the marginals (for example (f)), both the asynchronous
and synchronous marginals are inaccurate. In these cases, using more informed message schedules
can signiﬁcantly improve the algorithms performance.
These qualitative diﬀerences between the BP variants are quite consistent across many random

11.3. Propagation-Based Approximation
411
and real-life models. Typically, the more complex the inference problem, the larger the gaps in
performance. For very complex real-life networks involving tens of thousands of variables and
multiple cycles, even asynchronous BP is not very useful and more elaborate propagation methods
or convergent alternatives must be adopted.
11.3.6
Variational Analysis
So far, our discussion of cluster-graph belief propagation has been procedural, motivated purely
by similarity to message passing algorithms for cluster trees. Is there any formal justiﬁcation for
this approach? Is there a sense in which we can view this algorithm as providing an approxima-
tion to the exact inference task? In this section, we show that cluster-graph belief propagation
can be justiﬁed using the energy functional formulation of section 11.1. Speciﬁcally, the mes-
sages passed by cluster-graph belief propagation can be derived from ﬁxed-point equations for
the stationary points of an approximate version of the energy functional of equation (11.3). As
we will see, this formulation provides signiﬁcant insight into the generalized belief propagation
algorithm. It allows us to understand better the convergence properties of cluster-graph belief
propagation and to characterize its convergence points. It also suggests generalizations of the
algorithm that have better convergence properties, or that optimize a better approximation to
the energy functional.
Our construction will be similar to the one in section 11.2 for exact inference.
However,
there are important diﬀerences that underlie the fact that this algorithm is only an approximate
inference algorithm.
First, the exact energy functional F[ ˜PΦ, Q] has terms involving the entropy of an entire
joint distribution; thus, it cannot be tractably optimized. However, the factored energy functional
factored energy
functional
˜F[ ˜PΦ, Q] is deﬁned in terms of entropies of clusters and sepsets, each of which can be computed
eﬃciently based purely on local information at the clusters. Importantly, however, unlike for
clique trees, ˜F[ ˜PΦ, Q] is no longer simply a reformulation of the energy functional, but rather
an approximation of it.
However, even the factored energy functional cannot be optimized over the space of all
marginals Q that correspond to some actual distribution PΦ. More precisely, consider some
cluster graph U; for a distribution P, we deﬁne QP = {P(Ci)}i∈VU ∪{P(Si,j)}(i–j)∈EU . We
now deﬁne the marginal polytope of U to be
marginal
polytope
Marg[U] = {QP : P is a distribution over X}
(11.15)
That is, the marginal polytope is the set of all cluster (and sepset) beliefs that can be obtained
from marginalizing an actual distribution P. It is called the marginal polytope because it is the
set of marginals obtained from the polytope of all probability distributions over X. Unfortunately,
not every set of beliefs that correspond to clusters in U is in the marginal polytope; that is, there
are calibrated cluster graph beliefs that do not represent the marginals of any single coherent
joint distribution over X (see exercise 11.2). However, the marginal polytope is a complex object
with exponentially many facets. (In fact, the problem of determining whether a set of beliefs is
in the marginal polytope can be shown to be NP-hard.) Thus, optimizing a function over the

412
Chapter 11. Inference as Optimization
marginal polytope is a computationally diﬃcult task that is generally as hard as exact inference
over the cluster graph. To circumvent these problems, we perform our optimization over the
local consistency polytope:
local consistency
polytope
Local[U] =
(11.16)



{βi : i ∈VU}∪
{µi,j : (i–j) ∈EU}

µi,j[si,j]
=
P
Ci−Si,j βi(ci)
∀(i–j) ∈EU, ∀si,j ∈Val(Si,j)
1
=
P
ci βi(ci)
∀i ∈VU
βi(ci)
≥
0
∀i ∈VU, ci ∈Val(Ci).



We can think of the local consistency polytope as deﬁning a set of pseudo-marginal distri-
pseudo-marginals
butions, each one over the variables in one cluster. The constraints imply that these pseudo-
marginals must be calibrated and therefore locally consistent with each other. However, they are
not necessarily marginals of a single underlying joint distribution.
Overall, we can write down an optimization problem as follows:
CGraph-Optimize:
Find
Q
maximizing
˜F[ ˜PΦ, Q]
subject to
Q ∈Local[U]
(11.17)

Thus, our optimization problem contains two approximations: We are using an approx-
imation, rather than an exact, energy functional; and we are optimizing it over the space
of pseudo-marginals, which is a relaxation (a superspace) of the space of all coherent
probability distributions that factorize over the cluster graph.
In section 11.1, we noted that the energy functional is a lower bound on the log-partition
function; thus, by maximizing it, we get better approximations of PΦ.
Unfortunately, the
factored energy functional, which is only an approximation to the true energy functional, is not
necessarily also a lower bound. Nonetheless, it is still a reasonable strategy to maximize the
approximate energy functional, since it may lead to a good approximation of the log-partition
function.
This maximization problem directly generalizes CTree-Optimize to the case of cluster graphs.
Not surprisingly, we can derive a similar analogue to theorem 11.3, where we characterize the
stationary points of this optimization problem as solutions to a set of ﬁxed-point equations.
ﬁxed-point
equations
Theorem 11.5
A set of beliefs Q is a stationary point of CGraph-Optimize if and only if for every edge (i–j) ∈EU
there are auxiliary factors δi→j(Si,j) and δj→i(Sj,i) so that
δi→j ∝
X
Ci−Si,j
ψi ·
Y
k∈Nbi−{j}
δk→i.
(11.18)
and moreover, we have that
βi
∝
ψi ·
Y
j∈Nbi
δj→i
µi,j
=
δj→i · δi→j.

11.3. Propagation-Based Approximation
413
The proof is identical to the proof of theorem 11.3.
This theorem shows that we can characterize convergence points of the energy function in
terms of the original potentials and messages between clusters. We can, once again, deﬁne
a procedural variant, in which we initialize δi→j, and then iteratively use equation (11.18) to
redeﬁne each δi→j in terms of the current values of other δk→i. This process is identical (up
to a renormalization step) to the update formula we use in CTree-SP-calibrate (algorithm 10.2).
Indeed, we deﬁned CGraph-SP-Calibrate, a cluster graph version of CTree-SP-Calibrate, the mes-
sage passing steps are simply executing this iterative process using the ﬁxed-point equation.
Theorem 11.5 shows that convergence points of this procedure are related to stationary points of
˜F[ ˜PΦ, Q].
Corollary 11.1
Q is the convergence point of applying CGraph-SP-Calibrate(Φ, U) if and only if Q is a stationary
point of ˜F[ ˜PΦ, Q].
Due to the equivalence between sum-product and belief update messages, it follows that
convergence points of CGraph-BU-Calibrate are also convergence points of CGraph-SP-Calibrate.
Corollary 11.2
At convergence of CGraph-BU-Calibrate, the set of beliefs is a stationary point of ˜F[ ˜PΦ, Q].
It is tempting to interpret this result as stating that the convergence points of belief propa-
gation are maxima of the factored energy functional. However, there are several gaps between
the theorem and this idealized interpretation, which it is important to understand. First, we
note that maxima of a function are not necessarily ﬁxed points. In this case, we can verify that
˜F[ ˜PΦ, Q] is bounded from above, and thus must have a maximum. However, if the maximum
is a boundary point (where some of the probabilities in Q are 0), it may not be a ﬁxed point.
Fortunately, this situation is rare in practice, and it can be guaranteed not to arise under fairly
benign assumptions.
Second, we note that maxima are not the only ﬁxed points of the belief propagation algorithm;
minima and saddle points are also ﬁxed points. Intuitively, however, such solutions are not likely
to be stable, in the sense that slight perturbations to the messages will drive the process away
from them. Indeed, it is possible to show (although this result is outside the scope of this book)
that stable convergence points of belief propagation are always local maxima of the function.
stable
convergence
point
The most important limitation of this result, however, is that it does not show that we can
reach these maxima by applying belief propagation steps. There is no guarantee that the message
passing steps of cluster-graph belief propagation necessarily improve the energy functional: a
message passing step may increase or decrease the energy functional. Indeed, as we showed,
there are examples where the belief propagation procedure oscillates indeﬁnitely and fails to
converge. Even more surprisingly, this problem is not simply a matter of the algorithm being
unable to “ﬁnd” the maximum. One can show examples where the global maximum is not a
stable convergence point of belief propagation. That is, while it is, in principle, a ﬁxed point of
the algorithm, it will never be reached in practice, since even a slight perturbation will give rise
to oscillatory behavior.
Nevertheless, this result is of signiﬁcant importance in several ways. First, it provides us with
a declarative semantics for cluster-graph belief propagation in terms of optimization of a target
functional.
The success of the belief propagation algorithm, when it converges, leads us to
hope that the development of new, possibly more convergent, methods to solve the optimization

414
Chapter 11. Inference as Optimization
problem may give rise to good solutions. Second, the declarative view deﬁnes the problem in
terms of an objective — the factored energy functional — and a set of constraints — the set
of locally consistent pseudo-marginals. Both of these are approximations to the ones used in
the optimization problem for exact inference. When we view the task from this perspective,
some potential directions for improvements become obvious: We can perhaps achieve a better
approximation by making our objective a better approximation to the true energy functional, or
by tightening our constraints so as to make the constraint space closer to the exact marginal
polytope. We will describe some of the extensions based on these ideas; others are mentioned
in section 11.7.
11.3.7
Other Entropy Approximations ⋆
The variational analysis of the previous section provides us with a framework for understanding
the properties of this type of approximation, and for providing signiﬁcant generalizations.
11.3.7.1
Motivation
To understand this general framework, consider ﬁrst the form of the factored energy functional
when our cluster graph U has the form of the Bethe approximation. Recall that in the Bethe
approximation graph there are two layers: one consisting of clusters that correspond to factors
in Φ, and the other consisting of univariate clusters. When the cluster graph is calibrated, these
univariate clusters have the same distribution as the sepsets between them and the factors in
the ﬁrst layer. As such, we can combine together the entropy terms for all the sepsets labeled
by X and the associated univariate cluster and rewrite the energy functional, as follows:
Proposition 11.3
If Q = {βφ : φ ∈Φ} ∪{βi(Xi)} is a calibrated set of beliefs for a Bethe cluster graph U with
clusters {Cφ : φ ∈Φ} ∪{Xi : Xi ∈X}, then
˜F[ ˜PΦ, Q] =
X
φ∈Φ
IEScope[φ]∼βφ[ln φ] +
X
φ∈Φ
IHβφ(Cφ) −
X
i
(di −1)IHβi(Xi),
(11.19)
where di = |{φ : Xi ∈Scope[φ]}| is the number of factors that contain Xi.
Note that equation (11.19) is equivalent to the factored energy functional only when Q is cali-
brated. However, because we are interested only in such cases, we can freely alternate between
the two forms for the purpose of ﬁnding ﬁxed points of the factored energy functional. Equa-
tion (11.19) is the negation of a term known as the Bethe free energy in statistical mechanics. The
Bethe free energy
Bethe cluster graph we discussed earlier is a construction that is designed to match the Bethe
free energy functional.
Why is this reformulation useful? Recall that, in our discussion of generalized cluster graphs,
we required the running intersection property. This property has two important implications.
First is that the set of clusters that contain some variable X are connected; hence, the marginal
over X will be the same in all of these clusters at the calibration point. Second is that there
is no cycle of clusters and sepsets all of which contain X. We motivated this assumption by
noting that it prevents us from allowing information about X to cycle endlessly through a loop.
This new formulation provides a more formal justiﬁcation. As we can see, if the variable Xi

11.3. Propagation-Based Approximation
415
appears in di clusters in the cluster graph, then it appears in an entropy term with a positive
sign exactly di times. Owing to the running intersection property, the number of sepsets that
contain Xi is di −1 (the number of edges in a tree with k vertices is k −1), so that Xi appears
in an entropy term with a negative sign exactly di −1 times. In this case, the entropy of Xi
appears with positive sign di times, and with negative sign di −1 times, so overall it is counted
exactly once.
This reformulation suggests a much more general class of entropy approximations. We can
construct deﬁne a set of regions R, each with its own scope Cr and its own counting number
counting number
κr. We can now deﬁne the following weighted approximate entropy:
weighted
approximate
entropy
˜IH
κ
Q(X) =
X
r
κrIHβr(Cr).
(11.20)
Example 11.2
The simple Bethe cluster graph of section 11.3.5.2 ﬁts easily into this new framework. The construc-
Bethe cluster
graph
tion has two levels of regions: a set of “large” R+, where each r ∈R+ contains multiple variables,
and singleton regions containing the individual variables Xi ∈X. Both types of regions have
counting numbers: κr for r ∈R+ and κi for Xi ∈X. All factors in Φ are assigned only to large
regions, so that ψi = 1 for all i. We use Nbr to denote the set {Xi ∈Cr}, and Nbi to denote the
set {r : Xi ∈Cr}.
To capture exactly the Bethe free energy, we set each large region to have a counting number of
1, and each singleton region corresponding to Xi to have a counting number of 1 −di where di
is the number of large regions that contain Xi in their scope. We see that in this construction the
region graph energy functional is identical to the Bethe free energy of equation (11.19).
However, this framework also allows us to capture much richer constructions.
Example 11.3
Consider again the example of ﬁgure 11.8a. As we discussed in section 11.3.5.3, this cluster graph
has the beneﬁt of maintaining the pairwise correlations between all pairs of variables when passing
messages between clusters. Unfortunately, it is not a legal cluster graph, since it does not satisfy the
running intersection property. We can obtain another perspective on the problem with this cluster
graph by examining the energy functional associated with it:
˜F[ ˜PΦ, Q]
=
IEβ1[ln φ1(A, B, C)] + IEβ2[ln φ2(B, C, D)] + IEβ3[ln φ3(A, C, D)]
+IHβ1(A, B, C) + IHβ2(B, C, D) + IHβ3(A, C, D)
−IHβ4(B, C) −IHβ5(A, C) −IHβ6(C, D).
As we can see, the variable C appears in three clusters and three sepsets. As a consequence, the
counting number of C in the energy functional is 0. This means that we are undercounting the
entropy of C in the approximation. Indeed, as we discussed, this cluster graph does not satisfy the
running intersection property. Thus, we considered modifying the graph by removing C from one
of the sepsets. However, if we consider this problem from the perspective of the energy functional,
we can deal with the problem by adding another factor β7 that has C as its scope. If we add
IHβ7(C) to the energy functional we solve the undercounting problem. This results in a modiﬁed
energy functional
˜F[ ˜PΦ, Q]
=
IEβ1[ln φ1(A, B, C)] + IEβ2[ln φ2(B, C, D)] + IEβ3[ln φ3(A, C, D)]
+IHβ1(A, B, C) + IHβ2(B, C, D) + IHβ3(A, C, D) + IHβ7(C)
−IHβ4(B, C) −IHβ5(A, C) −IHβ6(C, D).

416
Chapter 11. Inference as Optimization
This is simply an instance of our weighted entropy approximation, with seven regions: the three
triplets, the three pairs, and the singleton C.
This perspective provides a clean and simple framework for proposing generalizations to the
class of approximations deﬁned by the cluster graph framework. Of course, to formulate our
optimization problem fully, we need to deﬁne the constraints and construct algorithms that
solve the resulting optimization problems. We now address these issues in the context of two
diﬀerent classes of weighted entropy approximations.
11.3.7.2
Convex Approximations
One of the biggest problems with the objective used in standard loopy BP is that it gives rise
to a nonconvex optimization problem. In fact, the objective often has multiple local optima.
These properties make the optimization hard and the answers nonrobust. However, a diﬀerent
choice of counting numbers can lead to a concave optimization objective, and hence to a convex
optimization problem. Such problems are much easier to solve using a range of algorithms, and
the solutions oﬀer a satisfying guarantee of optimality. We ﬁrst deﬁne the class of convex BP
objectives and then describe one solution algorithm.
We focus our discussion on energy functionals whose structure uses the two-layer Bethe
cluster graph structure of example 11.2, but where the counting numbers are diﬀerent.
To
preserve the desired semantics of the counting numbers, we require:
κi = 1 −
X
r∈Nbi
κr,
(11.21)
ensuring that the total counting number of terms involving the entropy of Xi is precisely 1.
When we deﬁne κr = 1 for all r ∈R, this constraint implies the counting numbers in the
Bethe free energy.
We now introduce the following condition on the counting numbers:
Deﬁnition 11.4
We say that a vector of counting numbers κr is convex if there exist nonnegative numbers νr, νi,
convex counting
numbers
and νr,i such that:
κr
=
νr + P
i : Xi∈Cr νr,i
for all r
κi
=
νi −P
r : Xi∈Cr νr,i
for all i
(11.22)
Assuming that we have a set of convex counting numbers, we can rewrite the weighted
approximate entropy of equation (11.20) as:
X
r
κrIHβr(Cr) +
X
i
κiIHβi(Xi) =
(11.23)
X
r
νrIHβr(Cr) +
X
r,Xi∈Cr
νr,i(IHβr(Cr) −IHβi(Xi)) +
X
i
νiIHβi(Xi).
Importantly, when the beliefs satisfy the marginal-consistency constraints, the terms in the
second summation can be rewritten as conditional entropies:
IHβr(Cr) −IHβi(Xi) = IHβr(Cr | Xi).

11.3. Propagation-Based Approximation
417
Plugging this result back into equation (11.23), we obtain an objective that is a summation of
terms each of which is either an entropy or a conditional entropy, all with positive coeﬃcients.
Because both entropies and conditional entropies are convex, we obtain the following result:
Proposition 11.4
The function in equation (11.23) is a concave function for any set of beliefs Q that satisﬁes the
marginal consistency constraints.
This type of objective function is called concave over the constraints, since it is not generally
concave over
constraints
concave, but it is concave over the subspace that satisﬁes the constraints of our optimization
problem. An entropy as in equation (11.23) that uses convex counting numbers is called a convex
convex entropy
entropy.
Assuming that the potentials are all strictly positive, we can now conclude that the optimiza-
tion problem CGraph-Optimize with convex counting numbers is a convex optimization problem
that has a unique global optimum.
Convex optimization problems can, in principle, be solved by a range of diﬀerent algorithms,
all of which guarantee convergence to the unique global optimum. However, the basic optimiza-
tion problem can easily get intractably large. Recall that to formulate our optimization space,
we need to introduce an optimization variable for each assignment of values to each cluster in
our cluster graph, and a constraint for each assignment of values to each sepset in the graph.
Example 11.4
Consider a grid-structured network corresponding to a modestly sized 500×500 image, where each
pixel can take 100 values. The structure of the graph is a pairwise network, with approximately
2 × 250, 000 clusters (pairwise edges), each of which can take 100 × 100 = 10, 000 values. The
total number of variables is therefore 500, 000 × 10, 000 = 5 × 109, an unmanageable number for
most optimizers.
Fortunately, due to the convexity of this problem, we have that strong duality holds (see
appendix A.5.4), and therefore we can ﬁnd a solution to this problem by solving its dual. The
message passing algorithms that we derive from the Lagrange multipliers are one method that
we can use for solving the dual. (For example, exercise 11.17 provides one message passing
algorithm for a Bethe cluster graph with general counting numbers.) However, the message
passing algorithms are not directly optimizing the objective.
Rather, they characterize the
optimum using a set of ﬁxed-point equations, and attempt to converge to the optimum by
iterating through these equations.
This process is generally not guaranteed to achieve the
optimum, even when the problem is convex. Again, we can consider using other optimization
algorithms over the dual problem. However, a message passing approach has some important
advantages, such as modularity and eﬃciency.
Fortunately, a careful reformulation of the message passing scheme can be shown to guar-
antee convergence to the global optimum. This reformulation is diﬀerent for synchronous and
asynchronous message passing. We present the asynchronous version, which is simpler and also
likely to be more eﬃcient in practice.
The algorithm, shown in algorithm 11.2, uses the following quantities in its computations:
ˆνi = νi +
X
r∈Nbi
νr;
ˆνi,r = νr + νi,r.
(11.24)
In each message passing iteration, it traverses the variables Xi in a round-robin fashion; for each
Xi, it computes two sets of messages: incoming messages δr→i(Xi) from regions to variables,

418
Chapter 11. Inference as Optimization
Algorithm 11.2 Convergent message passing for Bethe cluster graph with convex counting
numbers
Procedure Convex-BP-Msg (
ψr(Cr)
// set of initial potentials
σi→r(Cr)
// Current node-to-region messages
)
1
for i = 1, . . . , n
2
// Compute incoming messages from neighboring regions to
Xi
3
for r ∈Nbi
4
δr→i(Xi) ←P
Cr−Xi

ψr(Cr) Q
j∈Nbr−{i} σj→r(Cr)

1
ˆνi,r
5
// Compute beliefs for Xi, renormalizing to avoid numerical
underﬂows
6
βi(Xi) ←
1
ZXi
Q
r∈Nbi(δr→i(Xi))ˆνi,r/ˆνi
7
// Compute outgoing messages from Xi to neighboring re-
gions
8
for r ∈Nbi
9
σi→r(Cr) ←

ψr(Cr) Q
j∈Nbr−{i} σj→r(Cr)
−
νi,r
ˆνi,r 
βi(Xi)
δr→i(Xi)
νr
10
return {σi→r(Cr)}i,r∈Nbi
and outgoing messages σi→r(Cr) from variables to regions (essentially passing messages over
the factor graph). The overall process is initialized (in the ﬁrst message passing iteration) by
factor graph
setting σi→r = 1. This algorithm is guaranteed to converge to the global maximum of our
convex energy functional.
This derivation applies to any set of convex counting numbers, leaving open the question of
which counting numbers are likely to be give the best approximation. Although there is currently
no theoretical analysis answering this question, intuitively, we might argue that we want the
counting numbers for diﬀerent regions to be as close as possible to uniform. This intuition is
also supported by the fact that the Bethe approximation, which sets all κr = 1, obtains very high-
quality approximations when it converges. Thus, we can try to select nonnegative coeﬃcients
νi, νr, and νi,r for which κr and κi, deﬁned via equation (11.22), satisfy equation (11.21) and
minimize
X
r∈R+
(κr −1)2.
(11.25)
Other choices are also possible. For example, the tree-reweighted belief propagation (TRW)
TRW
algorithm computes convex counting numbers for a pairwise Markov network using the following
process: We ﬁrst deﬁne a probability distribution ρ over trees T in the network, such that each
edge in the pairwise network is present in at least one tree. This distribution deﬁnes a set of
weights:
κi
= −P
T ∋Xi ρ(T )
κi,j
= P
T ∋(Xi,Xj) ρ(T )
(11.26)

11.3. Propagation-Based Approximation
419
This computation results in a set of convex counting numbers (see exercise 11.18). Preliminary
convex counting
numbers
results suggest that the TRW counting numbers and the ones derived from optimizing equa-
tion (11.25) appear to achieve similar performance in practice.
However, the comparison to standard (Bethe-approximation) BP is less clear. When standard

BP converges, it generally tends to produce better results than the convex counterparts,
and almost universally it converges much faster. Conversely, when standard BP does not
converge, the convex algorithms have an advantage; but, as we discuss in box 11.B, there
are many tricks we can use to improve the convergence of BP, so it is not clear how often
nonconvergence is a problem. One setting where a convergent algorithm can have important
beneﬁts is in those settings (chapter 19 and chapter 20) where we generally learn the model using
iterative, hill-climbing methods that use inference in the inner loop for tasks such as gradient
computations. There, the use of a nonconvergent algorithm for computing the gradient can
severely destabilize the learning algorithm. In other settings, however, the decision of whether
to use standard or convex BP is one of approximate optimization of a pretty good (although still
approximate) objective, versus exact optimization of an objective that is generally not as good.
The right decision in this trade-oﬀis not clear, and needs to be made speciﬁcally for the target
application.
11.3.7.3
Region Graph Approximations
As illustrated in example 11.3, a very diﬀerent motivation for using an objective based on diﬀerent
counting numbers is to improve the quality of the approximation by better capturing interactions
between variables. As we showed in this example, we can use the notion of a weighted entropy
approximation to deﬁne a (hopefully) better approximation to the entropy. Of course, to specify
the optimization problem fully, we also need to specify the constraints. In this example, it is
fairly straightforward to do so: we want β7(C) to be consistent with the marginal probability
of C in one of the other beliefs that mention C. Now, we have an optimization problem that
seems to solve the problem we set out to solve: It can compute beliefs on each of the original
factors while maintaining consistency at the level of each pairwise marginal shared among these
factors.
However, the new optimization problem we deﬁned is not one that corresponds to a cluster
graph. To see this, notice that β7 appears in the role of a cluster. But, if it is a cluster, it would
have to be connected to one of the other factors by a sepset with scope C, which would require
an additional term in the energy functional associated with this cluster graph. Thus, it is not
immediately clear how we would go about optimizing the new modiﬁed functional.
We now discuss a general framework that deﬁnes the form of the optimization objective and
the constraints for constructions that capture higher-level interactions between the variables.
We also describe a message passing algorithm that can be used to ﬁnd ﬁxed points of this
optimization problem.
Region Graphs
The basic structure we consider is similar to a cluster graph, but unlike cluster
graphs we no longer distinguish two types of vertices (clusters and sepsets). Rather, we can have
a more deeply nested hierarchy of regions, related by containment.
Deﬁnition 11.5
A region graph R is a directed graph over a set of vertices R. Each vertex r is called a region and
region graph

420
Chapter 11. Inference as Optimization
1: A, B, C
2: B, C, D
3: A, C, D
4: B, C
5: A, C
7: C
6: C, D
Figure 11.9
An example of simple region graph
is associated with a distinct set of random variables Cr. Whenever there is an arc from a region
ri to a region rj we require that Scope[ri] ⊃Scope[rj]. Regions that have no incoming edges are
called top regions.
Each region is associated with a counting number κr ∈IR. Note that the counting number may
counting
numbers
be negative, positive, or zero.
Because containment is transitive, we have that if there is a directed path from r to r′, then
Scope[r′] ⊂Scope[r]. Thus, a region graph is acyclic.
To deﬁne the energy term in the free energy, we must assign our original factors in Φ to
regions in the region graph. Here, because diﬀerent regions are counted to diﬀerent extents, it
is useful to allow a factor to be assigned to more than one region. Thus, for each φ ∈Φ we
have a set of regions α(φ) ⊂R such that Scope[φ] ⊆Cr. This property is analogous to the
family-preservation property for cluster graphs. Throughout this book, we assume without loss of
family
preservation
generality that any r ∈α(φ) is a top region.
We are now ready to deﬁne the energy functional associated with a region graph:
˜F[ ˜PΦ, Q] =
X
r
κrIECr∼βr[ln ψr] + ˜IH
κ
Q(X),
(11.27)
where ψr is deﬁned as:
ψr =
Y
φ∈Φ : r∈α(φ)
φ.
As with cluster graphs, a region graph deﬁnes a set of beliefs, one per region. We use the
notation βr(Cr) to denote the belief associated with the region r over the set of variables
Cr = Scope[r].
Figure 11.9 demonstrates the region graph construction for the approximation of example 11.3.
This example contains three regions that correspond to the initial factors in the distribution we
want to approximate. The lower set of regions are the pairwise intersections between the three
factors. The lowest region is associated with the variable C.
Whereas the counting numbers specify the energy functional, the graph structure speciﬁes
the constraints over the beliefs that we wish to associate with the regions. In particular, we

11.3. Propagation-Based Approximation
421
6: A
11: F
10: E
9: D
8: C
7: B
1: A, B, C
2: B, C, D
4: B, E
5: D, E
3: B, D, F
Figure 11.10
The region graph corresponding to the Bethe cluster graph of ﬁgure 11.7a
want the beliefs to satisfy the calibration constraints that are implied by the edges in the region
graph structure.
Deﬁnition 11.6
Given a region graph, a set of region beliefs is calibrated if whenever r →r′ appears in the region
region graph
calibration
graph then
X
Cr−Cr′
βr(Cr) = βr′(C′
r)
(11.28)
The region graph structure provides a very general framework for specifying energy-functional
optimization problems. The set of regions and their counting numbers tell us which components
appear in the energy functional, and with what weight. The arcs in the region graph tell us
which consistency constraints we wish to enforce. We can choose which consistency constraints
we want to enforce by adding or removing regions or edges between them; we note that this
can be done without aﬀecting the energy functional by simply giving the regions introduced a
counting number of 0. The more edges we have, the more constraints we require regarding the
calibration of diﬀerent beliefs.
The region graph framework is very general, and it can encode a broad spectrum of opti-
mization objectives and constraints. However, not all such formulations are equally reasonable
as approximations to our true objective, which is the exact energy functional of equation (11.3).
The following requirement captures some of the essential properties that make a region graph
construction suitable for this purpose.
Deﬁnition 11.7
For each variable Xi, let Ri = {r : Xi ∈Scope[r]}; for each factor φ ∈Φ, let Rφ = {r :
Scope[φ] ⊆Cr}. We now deﬁne the following conditions for a region graph:
• variable connectedness: for every variable Xi, the set Ri forms a single connected component;
• factor connectedness: for every factor φ, the set Rφ forms a single connected component;
• factor preservation:
X
r∈α(φ)
κr = 1.
• running intersection: for every variable Xi,
X
r∈Ri
κr = 1.

422
Chapter 11. Inference as Optimization
The connectedness requirement for variables ensures that the beliefs about any individual vari-
able Xi in any calibrated region graph will be consistent for all beliefs that contain Xi. The
counting number condition for variables ensures that we are not not overcounting or under-
counting the entropy of Xi. Together, these conditions extend the running intersection property,
ensuring that the subgraph containing Xi is connected and that Xi is “counted” once in total.
We note that, like the running intersection property, this requirement does not address the ex-
tent to which we count the contribution associated with the interactions between pairs or larger
subsets of variables.
The factor-preservation condition for factors ensures that, when we sum up the energy terms
of the diﬀerent regions, each factor is counted exactly once in total.
As we will see, this
ensures that a calibrated region graph will still encode our original distribution PΦ. Finally, the
connectedness condition for factors ensures that we cannot double-count contributions of our
initial factors: that is, a factor cannot “ﬂow” around a loop.
An examination conﬁrms that all parts of the region graph condition hold both for the Bethe
region graph and for the region graph of ﬁgure 11.9.
How do we construct a valid region graph? One approach is based on the following simple
recursive construction for the counting numbers. We ﬁrst deﬁne, for a region r,
Up(r) = {r′ : (r′ →r) ∈ER}
to be the set of regions that are directly upwards of r; similarly, we deﬁne
Down(r) = {r′ : (r →r′) ∈ER}.
We also deﬁne the upward closure of r to be the set Up∗(r) of all the regions from which there
is a directed path to r, and the downward closure Down∗(r) to be all the regions that can be
reached by a directed path from r; ﬁnally, we deﬁne Down+(r) = {r} ∪Down∗(r).2
We can now deﬁne the counting numbers recursively, using the following formula:
κr = 1 −
X
r′∈Up∗(r)
κr′.
(11.29)
This condition ensures that the sum of the counting numbers of r and all of the regions above
it will be 1. Intuitively, we can think of the counting number of the region r as correcting for
overcounting or undercounting of the weight of the scope of r by regions above it. Now, assume
that our region graph is structured so that, for each variable Xi, there is a unique region ri such
that every other region whose scope contains Xi is an ancestor of ri. Then, we are guaranteed
that both the connectedness and the counting number condition for Xi hold. We can similarly
require that for any factor φ, there is a unique region rφ such that any other region whose scope
contains Scope[φ] is an ancestor of rφ. This construction guarantees the requirements for factor
connectedness and counting numbers.
It is easy to see that the Bethe region graph of example 11.2 satisﬁes both of these conditions.
Moreover, this process of guaranteeing that a unique minimal region exists for each Xi is
essentially what we did in example 11.3 to produce a valid region graph.
These conditions provide us with a simple strategy for constructing a saturated region graph.
saturated region
graph
2. Some of the literature on region graphs use the terminology of parents and children of regions. To avoid the confusion
with similar terminology in Bayesian networks, we use the terms up and down here.

11.3. Propagation-Based Approximation
423
Algorithm 11.3 Algorithm to construct a saturated region graph
Procedure Build-Saturated-Region-Graph (
R
// a set of initial regions
)
1
repeat
2
For any r1, r2 ∈R
3
Z ←Scope[r1] ∩Scope[r2]
4
if Z ̸= ∅
5
and R does not contain a region with scope Z then
6
create region r with Scope[r] = Z
7
R ←R ∪{r}
8
Until convergence
9
Initialize R as an empty graph with R as vertices
10
for each r1 ̸= r2 ∈R
11
if Scope[r2] ⊂Scope[r1] and
12
not exist r3 ∈R such that Scope[r2] ⊂Scope[r3] ⊂Scope[r1] then
13
add an arc r1 →r2 to the region graph R
14
return R
We start with initial set of regions. Often, these regions will be the initial factors in PΦ, although
we can decide to work with bigger regions that capture some more global interactions. We then
extend this set of regions into a valid region graph, where our goal is to represent appropriately
any subset of variables that is shared by some of the regions. We therefore expand the set of
regions to be closed under intersections. We connect these regions so that the upward closure
of each region contains all of its supersets. The full procedure is shown in algorithm 11.3. Unlike
the Bethe approximation, this region graph maintains the consistency of higher-order marginals.
The example of ﬁgure 11.9 is an example of running this procedure on the original set of regions
{A, B, C}, {B, C, D}, and {A, C, D}. As our previous discussion suggests, this procedure
guarantees a region graph that satisﬁes the region graph condition.
Belief Propagation in Region Graphs
Given a region graph, we are faced with the task of
optimizing the free energy associated with its structure:

424
Chapter 11. Inference as Optimization
RegionGraph-Optimize:
Find
Q = {βr : i ∈VR}
maximizing
˜F[ ˜PΦ, Q]
subject to
X
Cr′−Cr
βr′(cr′)
=
βr(cr)
(11.30)
∀r ∈VR, ∀r′ ∈Up(r), ∀cr ∈Val(Cr)
X
Cr
βr(cr)
=
1
∀r ∈VR
(11.31)
βr(cr)
≥
0
∀r ∈VR, cr ∈Val(Cr).
(11.32)
Our strategy for devising algorithms for solving this optimization problem is similar to the
approach we took in section 11.3.6. Using the method of Lagrange multipliers, we characterize the
stationary points of the target function (given the constraints) as a set of ﬁxed-point equations.
We then ﬁnd an iterative algorithm that attempts to reach such a stationary point.
We ﬁrst characterize the ﬁxed point via the Lagrange multipliers.
As before, we form a
Lagrangian by introducing terms for each of the constraints: from equation (11.30), we obtain
a Lagrange multiplier λr,r′(cr) for every pair r′ ∈Up(r) and every cr ∈Val(Cr); from
equation (11.31), we obtain a Lagrange multiplier λr for every r and every cr ∈Val(Cr); as
before, we assume that we are dealing with interior ﬁxed points only, and so do not have to
worry about the inequality constraint. We diﬀerentiate the Lagrangian relative to each of the
region beliefs βr(cr), and obtain the following set of ﬁxed-point equations:
ﬁxed-point
equations
κr ln βr(cr) = λr + κr ln ψr(cr) −
X
r′∈Up(r)
λr,r′(cr) +
X
r′∈Down(r)
λr′,r(cr′) −κr. (11.33)
For regions for which κr ̸= 0, we can rewrite this equation to conclude that:
βr(cr) = 1
Zr
ψr(cr)
 Q
r′∈Down(r) δr→r′(cr′)
Q
r′∈Up(r) δr′→r(cr)
!1/κr
.
(11.34)
From this equality, one can conclude the following result:
Theorem 11.6
Assume that our region graph satisﬁes the family preservation property. Then, at ﬁxed points of the
RegionGraph-Optimize optimization problem, we have that:
PΦ(X) ∝
Y
r
(βr)κr.
(11.35)
The proof is derived from a simple cancellation of messages in the diﬀerent terms (see exer-
cise 11.16).
This result tells us that we can reparameterize the initial distribution PΦ in terms of the ﬁnal
beliefs obtained as ﬁxed points of the region graph optimization problem. It tells us that we can
represent the distribution in terms of a calibrated set of beliefs for the individual regions. This

11.3. Propagation-Based Approximation
425
Down+(ru)
Down+(rd)
N(ru, rd)
rd
ru
rd
ru
rd
ru
Down+(ru)
Down+(rd)
D(ru, rd)
Down+(r)
(b)
(c)
(a)
Up(r)
r
Figure 11.11
The messages participating in diﬀerent region graph computations. Participating mes-
sages are marked with thicker arrows. (a) The computation of the beliefs βr(Cr); (b) The set N(ru, rd)
participating in the computation of the message δru→rd; (c) The set D(ru, rd) participating in the com-
putation of the message δru→rd.
result is a very powerful one, because it holds for any set of counting numbers that satisﬁes the
family preservation property — a very large class. Of course, this result only shows that any
ﬁxed point is a reparameterization of the distribution, but not that such a reparameterization
exists. However, under our assumption that all of the initial factors in Φ are strictly positive, one
can show that such a ﬁxed point, and hence a corresponding reparameterization, necessarily
exists.
As we can see, unlike the case of cluster graphs, the ﬁxed-point equations for region graphs
are more involved, and do not lead directly to an elegant message passing algorithm. Indeed,
message passing
algorithm
the derivation of the update rules from the Lagrange multipliers often involves multiple steps
of algebraic manipulation.
(Although such derivations are possible in restricted cases; see
exercise 11.17 and exercise 11.19.) In the remainder of this section, we present, without derivation,
one set of update rules that can be derived from equation (11.34), in the speciﬁc case where the
counting numbers are as presented in equation (11.29).
The basic idea is similar to the message update used in cluster graphs. There, each sepset
carried messages sent by one of the clusters through the sepset to the neighboring cluster. We
can think of that as a message from the cluster to the sepset. The analog of this in region graph
is a message from a region to a region below it. Thus, for each pair r1 →r2 in the region
graph we will have a message δr1→r2 whose scope is Scope[r2]. All messages are associated with
“downward” edges of the form r1 →r2, but they are used to deﬁne the beliefs and messages of
regions that contain them.
The deﬁnitions of the messages and the beliefs are somewhat involved. We begin by deﬁning

426
Chapter 11. Inference as Optimization
the beliefs of a region as a function of these messages, which is somewhat simpler:
βr(Cr)
=
ψr(Cr)


Y
ru∈Up(r)
δru→r(Cr)


(11.36)


Y
rd∈Down∗(r)
Y
ru∈Up(rd)−Down+(r)
δru→rd(Crd)

.
In other words, the belief of a region is the product of three groups of terms. The ﬁrst two are
very natural: the initial beliefs ψr (1 for all regions except the top ones) and the messages from
its upward regions. The last group contain all messages sent to regions below the region r from
regions other than the region r itself and regions below it. In other words, these are messages
from “external” sources to regions below the region r; see ﬁgure 11.11a. Thus, the beliefs of the
region are not inﬂuenced by messages it sends down, but only by messages sent to it or its
subsets from other regions.
Again, it is instructive to compare this deﬁnition to our deﬁnition of beliefs in cluster graphs.
In cluster graphs, the belief over a sepset is the product of messages from the neighboring
clusters. These messages correspond in our case to messages from upward regions. The belief
over a cluster C is the product of its initial potential and messages sent from neighboring
clusters to the sepsets adjacent to C. The sepsets correspond to the regions in Down+(C);
the message sent by another clique C′ to this sepset corresponds to messages sent by an
“external” source to a region in Down+(C).
We now move to deﬁning the computation of a message from ru to rd, also illustrated in
ﬁgure 11.11b,c:
δru→rd(Crd) =
P
Cru−Crd ψru(Cru) Q
r1→r2∈N(ru,rd) δr1→r2(Cr2)
Q
r1→r2∈D(ru,rd) δr1→r2(Cr2).
(11.37)
The numerator involves the initial factor assigned to the region, and a product of messages
associated with the set of edges
N(ru, rd) = {(r1 →r2) ∈ER : r1 ̸∈Down+(ru), r2 ∈Down+(ru) −Down+(rd)}.
This set contains edges from sources “external” to ru that are outside the scope of inﬂuence of
rd; that is, they either enter ru directly, or enter regions below ru that are not below rd. The
denominator involves a product of the messages in the set:
D(ru, rd) = {(r1 →r2) ∈ER : r1 ∈Down+(ru) −Down+(rd), r2 ∈Down+(rd)}.
This set counts information that would be passed from ru to regions below rd indirectly — not
through rd. We want to divide by these messages, since otherwise the same information would
be incorporated multiple times into the beliefs and the messages.
Example 11.5
We now return to the region graph of ﬁgure 11.9 that corresponds to the potential function we
discussed in example 11.3.

11.3. Propagation-Based Approximation
427
Applying equation (11.36) to this example, we get a set of equation representing the potentials as
function of the initial factors and the messages:
β1
=
ψ1δ2→4δ3→5δ6→7
β2
=
ψ2δ1→4δ3→6δ5→7
β3
=
ψ3δ1→5δ2→6δ4→7
β4
=
δ1→4δ2→4δ5→7δ6→7
β5
=
δ1→5δ3→5δ4→7δ6→7
β6
=
δ2→6δ3→6δ4→7δ5→7
β7
=
δ4→7δ5→7δ6→7.
Applying equation (11.37), we can construct the messages. For example,
δ4→7 =
X
B
δ1→4δ2→4.
One easy way to derive this message directly is to use the marginal consistency constraint:
β7 =
X
B
β4.
Plugging in the expanded form of the two beliefs, we get
δ4→7δ5→7δ6→7 =
X
B
δ1→4δ2→4δ5→7δ6→7.
If we now isolate δ4→7 we get
δ4→7 =
P
b δ1→4δ2→4δ5→7δ6→7
δ5→7δ6→7
.
After we cancel out the common terms δ5→7 and δ6→7, we get the desired form.
This message is essentially identical to the message in a cluster graph where we marginalize the
other incoming messages in the cluster to send a message to a particular sepset. Here region 4
behaves as a cluster and region 7 as a sepset. The other messages incoming to region 7 have a
similar form.
Messages into the middle layer regions have more complex form. For example
δ1→4 =
P
A ψ1δ3→5
δ5→7
.
Again, we can use the marginal consistency constraint
β4 =
X
A
β1
to reconstruct the message. Plugging in the expanded form of the two beliefs, and isolating δ1→4
we get
δ1→4 =
P
A ψ1δ2→4δ3→5δ6→7
δ2→4δ5→7δ6→7
.
After we cancel out δ2→4 and δ6→7, we get the desired form.

428
Chapter 11. Inference as Optimization
These deﬁnitions set up a message passing algorithm similar to CGraph-SP-Calibrate, except
that we use the messages as formulated in equation (11.37). As with belief propagation on cluster
graphs, we can prove that convergence points of such propagations are stationary points of the
RegionGraph-Optimize optimization problem.
Theorem 11.7
A set of beliefs Q is a stationary point of RegionGraph-Optimize for region graph R if and only if
for every edge (i–j) ∈ER there are auxiliary factors δu→d(Cd) that satisfy equation (11.36) and
equation (11.37).
This result is a direct generalization of theorem 11.5, and is proved in a similar way. We leave
the detail as an exercise (see exercise 11.14). Much of the discussion following theorem 11.5 applies
here. In particular, we do not have guarantees that iterations of message passing will converge.
However, if they do, we have reached a stationary point of the energy functional. In practice, the
experience is that when we consider moving from the Bethe approximation to “richer” region
graphs that contain intermediate regions with larger subsets, problems of nonconverging runs
are less common. For example, a region graph construction for grids is much more convergent
than the corresponding cluster graph (see exercise 11.15). However, except for special cases (for
example, region graphs that correspond to cluster trees), we do not know how to characterize
region graphs where belief propagation converges.
11.3.8
Discussion
Cluster-graph belief propagation methods such as the ones we have described in this chapter pro-
vide a general-purpose mechanism for approximate inference in graphical models. In principle,
they apply to any network, including networks with high tree-width, for which exact inference
is intractable. They have been applied successfully to a large number of dramatically diﬀerent
applications, including (among many others) message decoding in communication over a noisy
channel (see box 11.A), predicting protein structure (see box 20.B), and image segmentation (see
box 4.B).
However, it is important to keep in mind that cluster-graph belief propagation is not

a global panacea to the problem of inference in graphical models. The algorithm may
not converge, and when it does converge, there may be multiple diﬀerent convergence
points. Although there are currently no conditions characterizing precisely when cluster-graph
belief propagation converges, several factors seem to play a role.
The ﬁrst is the topology of the network: A network containing a large number of short loops
is more likely to be nonconvergent. Although this notion has been elusive to characterize in
practice, it has been shown that cluster-graph belief propagation is guaranteed to converge on
networks with a single loop.
An even more signiﬁcant factor is the extent to which the factors parameterizing the network
are skewed, or close to deterministic. Intuitively, deterministic factors can cause diﬃculties in
several ways. First, they often induce strong correlations between variables, which cluster-graph
belief propagation (depending on the approximation chosen) can lose. This error can have an
eﬀect not only for the correlated variables, but also for marginals of variables that interact with
both. Second, close-to-deterministic factors allow information to be propagated reliably through
long paths in the network.
Recall that part of our motivation for the running intersection
property was to prevent information about some variable to be propagated inﬁnitely through a

11.3. Propagation-Based Approximation
429
loop. While the running intersection property prevents such loops from occurring structurally,
deterministic potentials allow us to recreate them using an appropriate choice of parameters.
For example, if A is deterministically equal to B, then we can have a cycle of clusters where A
appears in some of the clusters and B in others. Although this cluster graph may satisfy the
running intersection property relative to A, eﬀectively there is a cycle in which the same variable
appears in all clusters. Finally, as we discussed in section 11.3.4, factors that are less skewed
provide smoothing of the messages, reducing oscillations; indeed, one can even prove that, if
the skew of the factors in the network is suﬃciently bounded, it can give rise to a contraction
property that guarantees convergence.

In summary, the key factor relating to convergence of belief propagation appears to
be the extent to which the network contains strong inﬂuences that “pull” a variable in
diﬀerent directions. Owing to its local nature, the algorithm is incapable of reconciling
these diﬀerent constraints, and it can therefore oscillate as diﬀerent messages arrive that
pull it in one direction or another.
A second problem relates to the quality of the results obtained. Despite the appeal and im-
portance of the energy-based analysis, it does not (except in a few rare cases — see section 11.7)
provide any guarantees on the accuracy of the marginals obtained by cluster-graph belief prop-
agation. This is in contrast to the sampling-based methods of chapter 12, where we are at least
assured that, if we run the algorithm for long enough, we will obtain accurate estimates of the
posteriors. (Of course, the key question of “how long is long enough” does not usually have an
answer, so it is not clear how important this distinction is in practice.) Empirical results show
that, in the settings where cluster-graph belief propagation convergence is more likely (not too
many tight loops, no highly skewed factors), one also often obtains reasonable answers.
Importantly, these answers are often good but overconﬁdent: The value x ∈Val(X) to
which cluster-graph belief propagation gives the highest probability is often the value for which
PΦ(X = x) is indeed the highest, but the probability assigned to x by the approximation is often
too high. This phenomenon arises (partly) from the fact the cluster-graph belief propagation
ignores correlations between messages and can therefore count the same piece of evidence
multiple times as it arrives along diﬀerent paths, leading to overly strong conclusions. In other
cases, however, the answers obtained by cluster-graph belief propagation are simply wrong (see
section 11.3.1); unfortunately, there is currently no way of determining when the answers returned
by a run of cluster-graph belief propagation are reasonable approximations to the true marginals.
The intuitions described previously do help us, however, to design approximations that are
more likely to produce good answers. In general, we cannot construct a cluster graph that pre-
serves all of the higher-order interactions among the factors. Hence, we need to decide which
factors to include in the cluster graph and how to relate them. As the preceding discussion
suggests, we do better if we construct approximations that incorporate tight loops and maintain
the strongest factors within clusters as much as possible. While these intuitions provide reason-
able rules of thumb on how to construct approximations, it is not obvious how to capture them
within a general-purpose automated cluster-graph construction procedure.

430
Chapter 11. Inference as Optimization
A1,4
A2,4
A3,4
A4,4
A1,3
A2,3
A3,3
A4,3
4
A1,3
A2,3
A3,3
A4,3
A1,2
A2,2
A3,2
A4,2
3
A1,1
A2,1
A3,1
A4,1
A1,2
A2,2
A3,2
A4,2
2
A1,1
A2,1
A3,1
A4,1
1
Figure 11.12
A cluster for a 4 × 4 grid network. The structure within each cluster represents the arcs
whose factors are assigned to that cluster.
11.4
Propagation with Approximate Messages ⋆
The cluster-graph belief propagation methods achieved approximation by “relaxing” the require-
ment of having a cluster tree. Instead, we used a cluster graph and constructed an approximation
by a set of pseudo-marginals. This approximation avoided the need to construct large clusters,
which incur an exponential cost in terms of memory and running time. In this section, we

consider an approach in which the simpliﬁcation occurs within a given cluster structure;
rather than simplify this structure, we perform approximate propagation steps within it.
This allows us to keep the correct clusters and gain eﬃciency by using more compact
representations and operations on these clusters (at the cost of introducing approxima-
tions). Importantly, this approach is orthogonal to the methods we described in the previous
section, in that approximate message passing can occur both within a clique tree or a cluster
graph approximation. For ease of presentation, we focus on the case of clique trees in this
section, but the ideas easily carry through to the more general setting.
The basic concept behind the methods described in this section is the use of approximate
messages in clique tree (or cluster graph) propagation. Instead of representing messages in the
clique trees as factors, we use more compact representations of approximate factors.
There
are many diﬀerent schemes that we can use for approximating messages.
To ground the
discussion, we begin in section 11.4.1 by describing one important instantiation of this general
framework, which is very natural in our setting — message approximation using a factored form
(for example, as a product of marginals). In section 11.4.2 and section 11.4.3 we then discuss
algorithms that perform message passing using approximate messages.
In section 11.4.4 we
describe a general algorithm, called expectation propagation, that applies to any approximation
expectation
propagation
in the exponential family. Finally, in section 11.4.5, we show that the expectation propagation
algorithm for the exponential family can be derived from a variational analysis, similar to the
ones we discussed in the previous section.

11.4. Propagation with Approximate Messages ⋆
431
A1,1
A2,1
A3,1
A4,1
A1,2
A2,2
A3,2
A4,2
2
A1,1
A2,1
A3,1
A4,1
(a)
(b)
A1,2
A2,2
A3,2
A4,2
A1,1
A2,1
A3,1
A4,1
A1,2
A2,2
A3,2
A4,2
2
A1,1
A2,1
A3,1
A4,1
A1,2
A2,2
A3,2
A4,2
d1→2
~
~d3→2
~
~
b2 = y2 ⋅ d1→2 ⋅ d3→2
~
d1→2
~
~d3→2
~
~
b2 = y2 ⋅ d1→2 ⋅ d3→2
~
Figure 11.13
Eﬀect of diﬀerent message factorizations on the structure of the belief factor ˜β2 in
the example of ﬁgure 11.12.
11.4.1
Factorized Messages
We begin by considering a concrete example where message approximation may be useful.
Consider again the 4 × 4 grid network of ﬁgure 11.4. We can construct a cluster tree for this
network as shown in ﬁgure 11.12. (Note that this cluster tree is not the optimal one for this
network.) In our discussion of inference until now, we ignored the inner structure of clusters
and treated the cluster as being associated with a single factor. Now, however, we keep track of
the structure of the original factors associated with the cluster. We will see shortly why keeping
this structure will help us. In ﬁgure 11.12 this structure is portrayed by “subnetworks” within
each cluster.
Calibration in this cluster tree involves sending messages that are joint measures over four
variables. An intuitive idea is to simplify the messages by using a factored representation. For
example, consider the message δ1→2, and suppose that we approximate it by a factored form
˜δ1→2[A1,1, A2,1, A3,1, A4,1] = ˜δ1→2[A1,1]˜δ1→2[A2,1]˜δ1→2[A3,1]˜δ1→2[A4,1].
What can we gain from such an approximation? Clearly, this form provides a more concise
representation of the message. Instead of representing the message with values for the sixteen
possible joint assignments, we send a message with two values for each variables (leading to a
total of eight “parameters”). If we consider bigger grids, then the saving will be more substantial.
Does this saving gain us eﬃciency? Naively, the answer is no. Recall that the main compu-
tational cost of exact inference is generating the message in a cluster. This requires multiplying
incoming messages with the original factor of the cluster and marginalization. In our example,
C2 involves eight variables, and so this operation should involve operations over the 28 = 256
joint values of these variables. Thus, one might argue that the saving of eight parameters in
representing the message does not deal with the core computational problem we have at hand

432
Chapter 11. Inference as Optimization
and leads to negligible improvement.
It turns out, however, that if we consider the internal structure of the cluster we can exploit
the factored form of the message. Consider computing the beliefs over C2 given messages from
C1 and C3. In exact computation, we multiply the potential ψ2 with δ1→2 and δ3→2 and
normalize to get the beliefs about C2. However, if we approximate both messages by a product
of univariate terms, we notice that the product of the messages with the factors in C2 forms a
network structure that we can exploit. In our example, this network is a tree-structured network
shown in ﬁgure 11.13a. We can easily calibrate this network and answer queries about the beliefs
over C2 without enumerating all joint assignments to variables in this cluster.
We can get similar savings even if we use a richer approximation that can better capture
dependencies among variables in the message. Suppose we approximate δ1→2 by a factored
form that corresponds to the chain structure A1,1—A2,1—A3,1—A4,1.
This approximation
makes conditional independence assumptions about the variables, but it captures some of the
dependencies among them. In this example, this representation actually captures the message
δ1→2. However, we can check that δ3→2 does not satisfy the conditional independence of A1,2
and A3,2 given A2,2. Thus, in this case, a chain representation is an approximation. If we
multiply these two approximations with the cluster C2, we get a set of factors that has the
structure shown in ﬁgure 11.13b. Although not a tree, this graph has a tree-width of 2, regardless
of the grid size. Thus, once again, we can use exact inference methods on the resulting product
of factors.
We can exploit this intuition by maintaining both the initial potentials and the messages as
factor sets. For the initial potential, these factors are the parameterization of the original network;
factor set
for messages, these factors are introduced by the approximation. A factor set ⃗φ = {φ1, . . . , φk}
provides a compact representation for the higher-dimensional factor φ1 · φ2 · · · · · φk.
Recall that belief propagation involves two main operations: product and marginalization.
The product of factor sets is easy. Suppose we have the factor sets ⃗φ1 and ⃗φ2. The factor set
factor set product
⃗φ1 · ⃗φ2 is simply the factor set that contains the union of the two factor sets (we assume that
the components of the factor sets are distinct).
The diﬃcult operation is marginalization. Suppose we have a factor set ⃗φ = {φ1, . . . , φk},
factor set
marginalization
and we consider the marginalization P
X ⃗φ. This operation couples all the factors that contain
X. In general, for a well-constructed clique tree, the message that results from marginalizing
a clique will not satisfy any conditional independence statements and therefore cannot be
factorized (see exercise 11.21).
Returning to our example of ﬁgure 11.12, one of our inference steps is to compute:
δ2→3 =
X
A1,1,A2,1,A3,1,A4,1
ψ2 · δ1→2.
To achieve the eﬃcient inference steps we discussed earlier, we want to approximate δ2→3 by
a product of simpler terms. This is an instance of a problem we encountered in section 8.5 —
approximating a given distribution by another distribution from a given family of distributions.
In our case, the approximating family is the set of distributions that take a particular product
form. As we discussed in section 8.5, there are several ways of projecting a distribution P into
some constrained class of distributions Q. Indeed, throughout our discussion in this chapter
so far, we have been searching for a distribution Q which is the I-projection of PΦ — the one

11.4. Propagation with Approximate Messages ⋆
433
A1,1
A2,1
A3,1
A4,1
A1,2
A2,2
A3,2
A4,2
2
A1,1
A2,1
A3,1
A4,1
A1,2
A2,2
A3,2
A4,2
d1→2
~
~
y2 ⋅ d1→2)
~d2→3 = r(∑
y2
Figure 11.14
Example of propagation in cluster tree with factorized messages
that minimizes ID(Q||PΦ) . However, as we discussed in section 8.5, we cannot compute the
I-projection of a distribution in closed form. Conversely, the M-projection for many classes Q
M-projection
of factored distributions has an easy closed form expression. For example, the M-projection
of P onto the class of factored distributions is simply the product of marginals of P (see
proposition 8.3). We can compute these marginals by computing the marginals of the individual
variables in the message. We note that the message is often not normalized, and is therefore
not a distribution; however, we can normalize the message and treat it as a distribution without
changing the ﬁnal posterior obtained by the inference algorithm (see exercise 9.3).
Given the simplicity of M-projections, one might wonder why we used I-projections in our
discussion so far. There are two main reasons. First, the theory of energy functionals allows
us to use I-projections to lower-bound the partition function. Second, and more importantly,
computing marginal probabilities for our distribution PΦ is generally intractable. Thus, although
the M-projection has a simple form, it is infeasible to apply when we have an intractable
network structure. In our current setting, the situation is diﬀerent. Here, our projection task is
to approximate the outgoing message from a cluster C. As discussed earlier, we assume that
the product of the beliefs and the approximated messages for C is a factor set with tractable
structure. This assumption allows us to use exact inference within the cluster C to compute the
marginal probabilities needed for M-projections.
11.4.2
Approximate Message Computation
We now discuss in greater detail the task of computing factorized messages. To begin, consider
the projection of δ2→3 into a fully factorized representation. According to proposition 8.3, to
build the M-projection of δ2→3 onto a factored distribution, we need to compute the marginals
of A1,2, A2,2, A3,2, and A4,2 in δ2→3. Since δ2→3 is deﬁned to be the marginal of the factor
set ψ2 · δ1→2, we need to compute terms such as Pψ2·δ1→2(A1,2), where we use Pψ2·δ1→2 to
denote the measure represented by the factor set. At an abstract level, this operation involves
computing δ2→3 and then projecting it onto the simpler representation. However, if we examine

434
Chapter 11. Inference as Optimization
the structure of this factor set (see ﬁgure 11.14), we see that this computation can be done using
standard exact inference on the factor set. In this particular case, the factor set is a tree network,
and so inference is cheap. Similarly, we can compute the marginals over the variables in δ2→3.
Thus, we can use the properties of M-projections and exact inference to compute the resulting
projected message without ever explicitly enumerating the joint over the four variables in δ2→3.
In an n × n grid, for example, we can perform these operations in time that is linear in n,
whereas the explicit computation would have constructed a factor of exponential size. As we
discussed, the more complex approximation of ﬁgure 11.13b also has a bounded tree-width of 2,
and therefore also allows linear time inference.
Algorithm 11.4 Projecting a factor set to produce a set of marginals over a given set of
scopes
Procedure Factored-Project (
⃗φ,
// an input factor set
Y,
// A set of desired output scopes
)
1
Build an inference data structure U
2
Initialize U with factors in ⃗φ
3
Perform inference in U
4
⃗φo ←∅
5
for all Y j ∈Y
6
Find a clique Cj in U so that Y j ⊆Cj
7
ψj ←βU(Y j)
// marginal of Y j in U
8
⃗φo ←⃗φo ∪{ψj}
9
return ⃗φo
The overall structure of the algorithm is shown in algorithm 11.4. At a high level, we can
view exact inference in the factor set as a “black box” and not concern ourselves with the
exact implementation. However, it is also useful to consider how this type of projected message
may be computed eﬃciently. Most often, the inference data structure U is a cluster graph or
a clique tree, into which the initial factors ⃗φ can easily be incorporated, and from which the
target factors, of the appropriate scopes, can be easily extracted. To allow for that, we typically
design the cluster graph to be family-preserving with respect to both sets of factors. Under
that assumption, we can extract a factor ψj over the scope Y j by identifying a cluster Cj in
U whose scope contains Y j, and marginalizing it over Y j. As an alternative approach, we
can construct an unconstrained clique tree, and use the out-of-clique inference algorithm of
section 10.3.3.2 to extract from the graph the joint marginals of subsets of variables that are not
together in a clique. (We note that out-of-clique inference is more challenging in the context
of cluster graphs, since the path used to relate the clusters containing the query variables can
aﬀect the outcome of the computation; see exercise 11.22.)
If our representation of a message is simply a product of marginals over disjoint subsets of
variables, this algorithm suﬃces to produce the output message: We produce a factor over each
(disjoint) scope Y j; the product of these factors is the M-projection of the distribution. But for

11.4. Propagation with Approximate Messages ⋆
435
richer approximations, the required operation is somewhat more complex.
Example 11.6
Consider again our grid example of ﬁgure 11.13b. Here, the clique needs to take in messages that
involve factors over pairs of variables Ai,1, Ai+1,1 and produce messages over pairs of variables
Ai,2, Ai+1,2. We can accomplish this goal by constructing, within this clique, a nested clique
tree that has at least one clique containing each of these pairs. For this purpose, we can use any
clique tree based on triangulating the structure inside the clique in ﬁgure 11.13b. For example,
we can have a tree with the cliques {A1,1, A1,2, A2,2}, {A1,1, A2,1, A2,2}, {A2,1, A2,2, A3,2},
{A2,1, A3,1, A3,2}, {A3,1, A3,2, A4,2}, {A3,1, A4,1, A4,2}.
Our goal now is to produce a set of factors that is the M-projection of the true message onto
the chain. Assume that we extract from the clique tree the pairwise marginals P(A1,2, A2,2),
P(A2,2, A3,2), and P(A3,2, A4,2). However, we cannot directly encode the distribution using
these factors as a factor set, since this approach would double-count the probability of the singletons
A2,2 and A3,2, each of which appears in two factors. To produce the true M-projected distribution,
we need to divide by the double-counted marginals A2,2 and A3,2.
We can achieve this correction easily by computing these node marginals and adding to our
factor set representation two factors
1
˜δ2→3(A2,2)
,
1
˜δ2→3(A3,2)
that compensate for the double-counting. This factor set represents the M-projection of the distribu-
tion, and it can be used in subsequent message passing steps. Equivalently, we are representing the
distribution as a calibrated clique tree over A1,2, A2,2, A3,2, A4,2, where the factors derived from
the pairwise marginals are the clique beliefs, the factors derived from inverted node marginals are
the sepset messages, and the overall distribution is encoded as in equation (10.11).
We can easily generalize this approach to more complex message representations.
Most
generally, assume that we choose to encode our message between cluster i and cluster j using
a representation as in equation (11.35); more precisely, we have some set of factors {βr(Xr) :
Xr ∈Ri,j}, each raised to some power:
˜δi→j =
Y
r∈Ri,j
(βr(Xr))κr.
(11.38)
We can compute this approximation in our procedure using the Factored-Project algorithm,
using the set {Xr ∈Ri,j} as Y. The output of this procedure is a set of calibrated marginals
over these scopes, and can therefore be plugged into equation (11.38) to produce a message in
the appropriate factored form. The same factors, each raised to its appropriate power, can be
used as the factorized message passed to cluster j.
Importantly, we note that this approach does not always provide an exact M-projection of
the true message. This property is guaranteed only when the set of regions forms a clique
tree, allowing the M-projection to be calculated in closed form using equation (11.38); in other
cases, we obtain only an approximation to the M-projection. In practice, we often choose some
simple clique tree, as in example 11.6, to encode our distribution, allowing the M-projection
to be performed easily and exactly.
However, in some cases, a clique tree approximation,
to stay tractable, is forced to make too many independence assumptions, leading to a poor

436
Chapter 11. Inference as Optimization
approximation of the message. Hence, the approach of an approximation M-projection into a
richer class of distributions may provide a useful alternative in some cases.
In summary, we have shown how we can perform an approximate message passing step with
structured messages. The algorithm maintains messages and beliefs in factored form. Factors are
entered into a nested clique tree or cluster graph, and message propagation is used to compute
the factors describing the output message. In the fully factored case, this representation is simply
a set of factors, and the multiplication operation used in message passing is simply a union of
factor sets. In the more complex setting, we may need to postprocess the set of marginals —
exponentiating them by appropriate counting numbers — to eliminate double-counting. The
resulting set of factors is the compact representation of the outgoing message.
11.4.3
Inference with Approximate Messages
Equipped with these operations on factor sets, we can consider how to use these tools within
cluster tree propagation. Our algorithm will maintain the beliefs at each cluster i using a factor
set ⃗φi. Initially, we are given a cluster tree T with an assignment of the original factors to
clusters. We have also determined the factorized form for each sepset, in terms of a network
structure Gi,j that describes the desired factorization for ˜δi→j and ˜δj→i.
There are two main strategies for applying the ideas described in the previous section to
deﬁne an approximate message passing algorithms in clique trees. One is based on a sum-
product message passing scheme, and the other on belief update messages. As we will see,
although these two strategies are equivalent in the exact inference setting, they lead to fairly
diﬀerent algorithms when we consider approximations.
For notational simplicity, we introduce the notation M-project-distri,j to be the combined
operation of marginalizing variables that do not appear in Si,j and performing the M-projection
onto the set of distributions that can be factorized according to Gi,j.
11.4.3.1
Sum-Product Propagation
Consider the application of sum-product propagation (CTree-SP-Calibrate, algorithm 10.2), to our
grid example. In this case, we need to perform the following operations:
δ1→2
=
ψ1
δ2→3
=
X
A1,1,...,A4,1
ψ2 · δ1→2
δ3→4
=
X
A1,2,...,A4,2
ψ3 · δ2→3
δ4→3
=
X
A1,4,...,A4,4
ψ4
δ3→2
=
X
A1,3,...,A4,3
ψ3 · δ4→3
δ2→1
=
X
A1,2,...,A4,2
ψ2 · δ3→2.

11.4. Propagation with Approximate Messages ⋆
437
A straightforward application of approximation is to replace each of the messages by the
M-projected version:
˜δ1→2
=
M-project-distr1,2(ψ1)
˜δ2→3
=
M-project-distr2,3(ψ2 · ˜δ1→2)
˜δ3→4
=
M-project-distr3,4(ψ3 · ˜δ2→3)
˜δ4→3
=
M-project-distr3,4(ψ4)
˜δ3→2
=
M-project-distr2,3(ψ3 · ˜δ4→3)
˜δ2→1
=
M-project-distr1,2(ψ2 · ˜δ3→2).
Each of these projection operations can be performed using the procedure described in the
previous section.
More generally, our sum-product expectation propagation procedure is identical to sum-product
sum-product
expectation
propagation
propagation of section 10.2, except that we modify the basic propagation procedure SP-Message
so that, rather than simply marginalizing the product of factors, it computes their M-projection.
Otherwise, the general structure of the propagation procedure is maintained exactly as before.
Each cluster collects the messages from its neighbors and when possible sends outgoing mes-
sages. As for the original sum-product message passing, this process converges after a single
upward-and-downward pass over the clique tree structure.
Thus, unlike most of the other
approximations we discuss in this chapter, this procedure terminates in a ﬁxed number of steps.
Note that, after performing the propagation, the ﬁnal beliefs over the individual clusters in
the tree are not an explicit representation of a joint distribution over the variables in the cluster.
In this case, the ﬁnal beliefs over a cluster C are represented in a factorized form, as a set of
beliefs. In our running example, the computed beliefs over C2 have the form
˜β2 = ψ2 · ˜δ1→2˜δ3→2,
and the structure shown in ﬁgure 11.13a. Because this structure allows tractable inference, we
can answer queries about the posterior distribution of these variables using standard inference
methods, such as variable elimination or clique tree inference.
These computational beneﬁts come at a price. The exact beliefs over C2 are not decompos-
able (see exercise 11.20). And thus, although forcing a particular independence structure on the
marginal distribution has computational advantages, it does lose information. With this caveat in
mind, we can still question whether the algorithm ﬁnds the best possible approximation within
the set of constraints we are considering.
Example 11.7
To get a sense of the issues, consider the simple example shown in ﬁgure 11.15a. In this small
network, the potential φ1(A, B) introduces a strong coupling between A and B, with a strong
preference for A = B. The potentials φ2(A, C), φ3(B, D) incur weaker coupling between A and
C and B and D. Finally, the potential φ4(C, D) has a strong preference for C = c1, with a
weaker preference for C ̸= D.

438
Chapter 11. Inference as Optimization
2
1
A
C
B
D
A
B
A
C
B
D
f1
b0
b1
a0
10
0.1
a1
0.1
10
f2
c0
c1
a0
5
0.2
a1
0.2
5
f1
f2
f3
f4
f4
d0
d1
c0
0.5
1
c1
20
2.5
f3
d0
d1
b0
5
0.2
b1
0.2
5
(a) Markov network
(b) cluster tree
Figure 11.15
Markov network used to demonstrate approximate message passing.
(a) A simple
Markov network with four pairwise potentials. (b) A clique tree for this network.
If we perform exact inference in this network, we ﬁnd the following marginal posteriors:
P(a0, b0)
=
0.274
P(c0, d0)
=
0.102
P(a0, b1)
=
0.002
P(c0, d1)
=
0.018
P(a1, b0)
=
0.041
P(c1, d0)
=
0.368
P(a1, b1)
=
0.682
P(c1, d1)
=
0.512.
We see that the preference for C = c1 is reﬂected in this distribution (with the marginal distribution
P(c1) = 0.88). In addition, the strong coupling between A and B is propagated through the
network, which results in making D = d1 more probable when C = c1.
What happens when we perform inference using the cluster tree of ﬁgure 11.15b and use approx-
imate messages that are products of marginals? It is easy to see that, because φ1 is symmetric,
we get that ˜δ1→2[a1] = 0.5, and ˜δ1→2[b1] = 0.5. We can compare the exact message and the
approximate one
δ1→2(a0, b0)
=
0.495
˜δ1→2(a0, b0)
=
0.5 ∗0.5 = 0.25
δ1→2(a0, b1)
=
0.005
˜δ1→2(a0, b1)
=
0.5 ∗0.5 = 0.25
δ1→2(a1, b0)
=
0.005
˜δ1→2(a1, b0)
=
0.5 ∗0.5 = 0.25
δ1→2(a1, b1)
=
0.495
˜δ1→2(a1, b1)
=
0.5 ∗0.5 = 0.25.
Thus, the approximate message is one where each joint assignment to A and B is equiprobable.
This approximation loses the coupling that φ1 introduces between A and B, and therefore it is a
poor approximation to the exact message.
Next, we multiply this approximate message into the clique C2. The initial factor here is ψ2 =

11.4. Propagation with Approximate Messages ⋆
439
φ2 · φ3 · φ4, and after multiplying it with ˜δ1→2 we get the beliefs
˜β2(c0, d0)
=
0.021
˜β2(c0, d1)
=
0.042
˜β2(c1, d0)
=
0.833
˜β2(c1, d1)
=
0.104.
Note that this factor is essentially a normalization of φ4, since the message ˜δ1→2 puts a uniform
distribution of A and B, and since φ2 and φ3 are symmetric. Because the information about the
coupling between A and B is not propagated into this cluster, we lose the consequent coupling
between C and D, and the resulting approximation to P(C, D) is also quite poor.
By contrast, if we now compute ˜δ2→1, we get that
˜δ2→1[a1]
=
0.904
˜δ2→1[b1]
=
0.173.
This message ascribes high probability to a1 and a low one to b1. This is quite diﬀerent from
the original coupling introduced by φ1. Thus, when we combine the two to get the approximated
posterior over A and B, we get the following beliefs factor:
˜β1(a0, b0)
=
0.326
˜β1(a0, b1)
=
0.001
˜β1(a1, b0)
=
0.031
˜β1(a1, b1)
=
0.642.
This approximation is fairly close to the exact marginal over A and B.
The problem in this example is that the approximation of ˜δ1→2 is done blindly, not taking
into account its eﬀect on approximations on computations in downstream clusters. Thus, the
message did not place suﬃcient emphasis on obtaining a correct approximation for the case
A = a1, which roughly corresponds to the “important” (high-probability) case C = c1.
11.4.3.2
Belief Update Propagation
Example 11.7 shows a case where factorizing messages leads to a big error in the approximated
beliefs. Let us examine this example in somewhat more detail. Given the update of ˜δ2→1, the
approximation of P(A, B) is more or less on target. Can we use this information to improve
our approximation of P(C, D)? To do this, we would need to revise ˜δ1→2. The posterior over
A and B informs us that most of the mass of the probability distribution is on a1, b1. With this
information, we might want to change ˜δ1→2 to reﬂect preferences for a1 and b1.
A priori, it appears that this idea is inherently problematic; after all, a key constraint for exact
inference is to avoid feedback from δ2→1 to δ1→2, so as not to double-count evidence. For
this reason, we took care, in the sum-product message-passing algorithm, not to multiply in the
message δ2→1 when passing messages from C1 to C2.
However, recall that in section 10.3, we presented the sum-product-divide update rule and
showed its equivalence to the sum-product rule. Brieﬂy recapping, we can take the sum-product

440
Chapter 11. Inference as Optimization
update rule:
δi→j =
X
Ci−Si,j
ψi


Y
k∈Nbi−{j}
δk→i,


and multiply and divide by δj→i, resulting in the rule:
δi→j ←
P
Ci−Si,j βi
δj→i
.
These two rules are therefore equivalent in the exact case. However, when we consider approxi-
mate inference, the situation is more complex.
Consider now performing belief-update expectation propagation message passing. Assume, as
belief-update
expectation
propagation
before, that the algorithm is maintaining a set of approximate beliefs ˜βi. We now have two
possible stages in which to do the project. The ﬁrst is:
˜δi→j ←M-project-distri,j
 ˜βi
˜δj→i
!
.
This version is identical to the approximate sum-product we discussed before: The beliefs factor
βi accounts for all of the incoming messages; when we divide by the message ˜δj→i before
projecting, we are projecting the product of all the other incoming message, which is precisely
the sum-product message.
Alternatively, we can do the projection before we divide by ˜δj→i. In this second approach,
we ﬁrst project ˜βi, and then divide by ˜δj→i:
˜σi→j
←
M-project-distri,j(˜βi)
˜δi→j
←
˜σi→j
˜δj→i
.
(11.39)
In this update, we ﬁrst collect all messages into Ci; we then compute the beliefs about Ci and
project this to the required form of the message. As in the exact belief update algorithm, this
term accounts for information sent from Cj. Note that both ˜σi→j and ˜δj→i have the same
factorization, and hence so does their quotient ˜δi→j.
This message, which is subsequently used to update Cj, is very diﬀerent from the sum-
product update. This is because the incoming message was used in determining the approxima-
tion. The approximation process is invariably a trade-oﬀ, in that a better approximation

of some regions of the probability space results in a worse approximation in others. In
the belief-update form of message passing, we can take into account the current ap-
proximation of the message from the target clique when deciding on our approximation,
potentially focusing more of our attention on “more relevant” parts of the space.
To integrate this update rule into the standard belief-update message passing algorithm, we
simply replace BU-Message of algorithm 10.3 with EP-Message, shown in algorithm 11.5. We
note that the data structures in this procedure are slightly diﬀerent from those in the original
algorithm. First, we maintain the cluster beliefs implicitly, as a factor set, and consider the
product of these factors only as part of the M-projection operation. Second, we do not only

11.4. Propagation with Approximate Messages ⋆
441
Algorithm 11.5 Modiﬁed version of BU-Message that incorporates message projection
Procedure EP-Message (
i,
// sending clique
j
// receiving clique
)
1
˜σi→j ←M-project-distrQi,j(⃗φi)
2
// marginalize and project the clique over the sepset
3
Remove old ˜δi→j from ⃗φj
4
˜δi→j ←
˜σi→j
˜δj→i
5
// divide by the message from from Cj to Ci
6
Insert new ˜δi→j into ⃗φj
keep the previous message sent over the edge, but rather keep the messages ˜δi→j sent in
both directions; this more reﬁned bookkeeping is necessary for dividing by the correct message
following the approximation. This algorithm is called expectation propagation (EP) for reasons
expectation
propagation
that are explained later.
Example 11.8
To understand the behavior of this algorithm, consider the application of belief-update message
propagation to example 11.7. Suppose we initialize all messages to 1 and use updates of the form
equation (11.39). We start by propagating a message ˜σ1→2 = M-project-distr1,2(˜β1) from C1
to C2. Because ˜δ1→2 at this stage is 1, the resulting update is exactly the one we discussed in
example 11.7. If we now perform propagation from C2 to C1, we get the message ˜δ2→1 derived in
example 11.7, multiplied by a constant (since ˜δ1→2 is uniform). At this point, as we discussed, the
clique beliefs β1 are a fairly reasonable approximation to the posterior.
Using the revised update rule, we now project ˜β1, and then divide by ˜δ2→1:
˜δ1→2 ←
M-project-distr1,2(˜β1)
˜δ2→1
.
This quotient, which is thensubsequently used to update C2, is very diﬀerent from the previous
update ˜δ2→1. Speciﬁcally, The marginal ˜β1 at this stage puts a posterior of 0.642 + 0.031 = 0.673
on a1, and 0.642 on b1. To avoid double-counting the contribution of ˜δ2→1, we need to divide this
marginal by this message. After we normalize messages, we obtain:
˜δ1→2[a1]
←
0.673
0.904
0.673
0.904 + 0.327
0.086
= 0.744
4.15 = 0.179
˜δ1→2[a0]
←
0.327
0.086
0.673
0.904 + 0.327
0.086
= 3.406
4.15 = 0.821
˜δ1→2[b1]
←
0.642
0.173
0.642
0.173 + 0.358
0.827
= 3.710
4.413 = 0.895
˜δ1→2[b0]
←
0.358
0.827
0.642
0.173 + 0.358
0.827
= 0.433
4.144 = 0.105.

442
Chapter 11. Inference as Optimization
Recall that this message can be viewed as a “correction term” for the sepset marginals, relative to
the message ˜δ2→1. Thus, its eﬀect is to reduce the support for a1 (which was very high in ˜β2), and
at the same time to increase the support for b1.
Propagating this message to C2 and updating the clique beliefs ˜β2, we get a normalized factor
of:
˜β2(c0, d0)
←
0.031
˜β2(c0, d1)
←
0.397
˜β2(c1, d0)
←
0.317
˜β2(c1, d1)
←
0.254.
These beliefs are closer to the exact marginals in that they distribute some of the mass that was
previously assigned to c1, d0 to two other cases. However, they are still quite far from the exact
marginal.
This example demonstrates several issues. First, there is signiﬁcant diﬀerence between the two
update rules. After incorporating ˜δ2→1, the message from C1 to C2 is readjusted to account for
the new information, leading to a diﬀerent approximation and hence a diﬀerent update ˜δ1→2.
Second, unlike sum-product propagation, belief update propagation does not generally converge
within two rounds, even in a clique tree.
In fact, an immediate question is whether these
iterations converge at all. And if they do, is there anything we can say about the convergence
points? As we show in section 11.4.5, the answers to these questions are very similar to the
answers we got in the case of cluster-graph belief propagation.
11.4.4
Expectation Propagation
So far, our discussion of approximate message passing has focused on a particular type of ap-
proximation: approximating a complex joint distribution as a product of small factors. However,
the same ideas are applicable to a broad range of approximations. We now consider this process
from a more general perspective, which will allow us to use a wider range of structured approx-
imation in messages. Moreover, as we will see, this generalized form simpliﬁes the variational
analysis of this approach.
The framework we use is based on the idea of the exponential family, as presented in
exponential
family
chapter 8. As we discussed there, the exponential families are a general class of distributions,
that contains many of the distributions of interest. Recall that a family of distributions Q is
in the exponential family if it can be deﬁned by two functions: the suﬃcient statistic function
τ(x), and the natural parameter function t(θ), so that any distribution Q in the family can be
written as
Q(x) =
1
Z(θ) exp {⟨τ(x), t(θ)⟩} ,
where θ is a set of parameters that specify the particular member of the family.
To simplify the discussion we will focus on linear exponential families, where t(θ) = θ. Recall
that linear exponential families include Markov networks (and consequently chordal Bayesian
networks).
As we now show, the approximate message passing approach described earlier applies when
exponential
family messages

11.4. Propagation with Approximate Messages ⋆
443
Algorithm 11.6 The message passing step in the expectation propagation algorithm. The
algorithm performs approximate message propagation by projecting expected suﬃcient statistics.
Procedure M-Project-Distr (
Q,
// target exponential family for projection
⃗φ
// Factor set
)
1
X ←Scope[⃗φ]
// Variables in factor set
2
¯τ ←IEx∼Q
φ∈⃗
φ

τQi,j(x)

3
// Compute expectation of suﬃcient statistics relative to dis-
tribution deﬁned by product of factors
4
θ ←M-project(¯τ)
5
return (θ)
we choose to approximate messages by distributions from (linear) exponential families. Specif-
ically, assume that we restrict each sepset Si,j to be represented within an exponential family
Qi,j deﬁned by a suﬃcient statistics function τi,j. When performing message passing from Ci
to Cj, we compute the marginal of ˜βi, usually represented as a factor set ⃗φi, and project it into
Qi,j using the M-projection operator M-project-distri,j. This computation is often done using
M-projection
inference procedure that takes into account the structure of ˜βi as a factor set.
It turns out that the entire message passing operation can be formulated cleanly within this
framework. If we are using an exponential family to represent our messages, then both the
approximate clique marginal ˜σi→j and the previous message ˜δj→i can be represented in the
exponential form. Thus, if we ignore normalization factors, we have:
˜σi→j
∝
exp

⟨θ˜σi→j, τi,j(si,j)⟩
	
˜δj→i
∝
exp
n
⟨θ˜δj→i, τi,j(si,j)⟩
o
˜δi→j
=
˜σi→j
˜δj→i
∝exp
n
⟨(θ˜σi→j −θ˜δj→i), τi,j(si,j)⟩
o
,
where θ˜σi→j and θ˜δi→j are the parameters of the messages ˜σi→j and ˜δi→j respectively.
Since these messages are in an exponential family, it suﬃces to represent each of them by
the parameters that describe them.
We can then view propagation steps as updating these
parameters. Speciﬁcally, we can rewrite the update step in line 4 of EP-Message as
θ˜δi→j ←(θ˜σi→j −θ˜δj→i).
(11.40)
Note that, in the case of exact inference in discrete networks, the original update and the one
using the exponential family representation are essentially identical, since the exponential family
representation of factors is of the same size as the factor. Indeed, the standard update is often
performed in a logarithmic representation (for reasons of numerical stability; see box 10.A), which
gives rise precisely to the exponential family update.
The ﬁnal issue we must address is the construction of the exponential-family representation
of ˜σi→j in line 1 of the algorithm. Recall that this process involves the M-projection of ˜βi

444
Chapter 11. Inference as Optimization
onto Qi,j.
As we discussed in section 8.5, the M-projection of a distribution P within an
exponential family Q is the distribution Q ∈Q, which deﬁnes the same expectation over the
suﬃcient statistics as deﬁned by P. In that section, we described a two-phase procedure for
computing this approximating distribution: We compute the expected suﬃcient statistics induced
by P, and then ﬁnd the parameters for a distribution Q ∈Q that induces the same expected
suﬃcient statistics. We can apply this approach to deﬁne a general procedure for performing the
operation M-project-distri,j(⃗φi) for a general exponential family. We ﬁrst compute the expected
expectation of τi,j according to ˜βi. We then ﬁnd the distribution within Qi,j that gives rise
to the same expected suﬃcient statistics. This step is accomplished by the application of the
function M-project that takes a vector of suﬃcient statistics and returns a parameter vector in
the exponential family that induces precisely the expected suﬃcient statistics ¯τi,j. This function
is shown in algorithm 11.6.
The use of the expectation step in computing the messages is
the basis for the name expectation propagation, which describes the general procedure for any
expectation
propagation
member of the exponential family.
As we have already discussed, dividing by the previous message corresponds to subtraction
of the parameters. Thus, overall, we obtain the following EP message passing step:
EP message
passing
θ˜δi→j ←M-projecti,j(IESi,j∼˜βi[τi,j(Si,j)]) −θ˜δj→i.
(11.41)
How expensive are the two key steps in the expectation-propagation message passing proce-
dure? The ﬁrst step is computing the expectation IESi,j∼˜βi[τi,j]. In the case of discrete networks,
this step might be as expensive as the number of possible values of Ci. However, as we saw
in the previous section, in many cases we can use the structure of the factors that make up ˜βi
to perform this expectation much more eﬃciently. The second step is computing M-project on
these factors. For some exponential families, this step is trivial, and can be done using a plug-in
formula. In other families, this is a complex problem by itself. We will return to these issues in
much greater detail in later chapters (particularly chapter 19 and chapter 20). Usually, when we
design an approximation algorithm, we choose an exponential family for which this second step
is easy.
The factored distributions we discussed earlier are perhaps the simplest example of an expo-
nential family that we can use in this algorithm. However, other representations also fall into
this class.
Example 11.9
Consider using a chain network to approximate each message, as in ﬁgure 11.15b. In example 8.16
and exercise 8.6 we showed that this class of distributions is a linear family and constructed
the function M-project for it. Following the derivation, suppose the variables in the sepset are
X1, . . . , Xk and we want to represent messages using the network structure X1 →X2 →. . . →
Xk.
In example 8.16, we showed that the expected suﬃcient statistics are summarized in the
vector of indicators comprising: 11{xj
i} for i = 1, . . . , k and xj
i ∈Val(Xi); and 11{xj
i, xℓ
i+1} for
i = 1, . . . , k −1, xj
i ∈Val(Xi), xℓ
i ∈Val(Xi+1). Once we have the expected value of these
statistics, we can reconstruct the distribution Q(Xi+1 | Xi) as described in exercise 8.6. Given
these subprocedures, the remaining propagation steps have been described earlier.
If we consider a chain approximation to grid network, we can use the chains as in ﬁgure 11.13b.
In this case, the main cost of a propagation step is the projection. The messages incoming to a
cluster consists of univariate beliefs and pairwise beliefs along the column. When combined with

11.4. Propagation with Approximate Messages ⋆
445
the clique factors, these result in a ladder-like network (shown in ﬁgure 11.13b). As we discussed, we
can a build a (nested) clique tree for this network that involves clusters of at most three variables.
Thus, we can perform inference eﬃciently on this nested clique tree to compute the expectations on
pairwise beliefs in the outgoing column, and then use those expectations to reconstruct parameters.
The view of the expectation propagation algorithm in these terms allows us to understand
the scope of this approach. We can apply this approach to any class of distributions in the
linear exponential family for which the computation of expected suﬃcient statistics and the M-
projection operation can be implemented eﬀectively. Note that not every class of distributions
we have discussed obeys these two restrictions.
In particular, Markov networks are in the
linear exponential family, but they do not have an eﬀective M-projection procedure. Thus, a
general Markov network structure is not often used to represent messages in the expectation
propagation algorithm. Conversely, Bayesian networks are not in the linear exponential family
(see section 8.3.2). One might argue that Bayesian networks should be usable, since the M-
projection operation can be implemented analytically (see theorem 8.7), allowing the expectation
propagation algorithm to be applied eﬀectively.
This argument, however, brings up one important caveat regarding the expectation propaga-
tion update rule. In the rule, we subtract two sets of parameters: the parameters associated
with the message from j to i are subtracted from the parameters obtained from M-projection
operation for Ci. For the classes of distributions that we considered earlier, the space of legal
parameters Θ was the entire real space IRK; this space is closed under subtraction, guarantee-
ing that the result of this update rule is a valid distribution in our space. This property does
not hold for every exponential family; in particular, it is not the case for Bayesian networks with
immoralities. Thus, we may end up in situations where the resulting parameters do not actually
deﬁne a legal distribution in our space. We return to this issue when we discuss the applica-
tion of expectation propagation to Gaussians in section 14.3.3, where it can give rise to severe
problems. Thus, the most commonly used class of distributions in the expectation propagation
algorithm is the class of low tree-width chordal graphs. Because these graphs are both Bayesian
networks and Markov networks, they are both in the linear exponential family and admit an
eﬀective M-projection operation. The example of the chain distribution we used earlier falls into
this category. In more general cases, these distributions are represented as clique trees, allowing
them to be represented using a smaller set of factors: clique beliefs and sepset messages.
11.4.5
Variational Analysis
To deﬁne a variational principle for expectation propagation, we take an approach similar to
the one we discussed in the context of cluster-graph belief propagation. Again, we consider an
approximation Q that consists of a set of pseudo marginals. In fact, we use the same energy
functional ˜F[ ˜PΦ, Q].
The main diﬀerence from the case of cluster-graph belief propagation is that, in the current
approximation, the cluster tree is not calibrated. As we project messages into an approximate
form, we no longer ensure that beliefs of neighboring clusters agree on the joint distribution of
the variables they share. Instead, we maintain a weaker property that depends on the nature
of the approximation. For example, in the case where we use messages that are product of
marginal distributions, intuition suggests that neighboring clusters will eventually agree on the
marginal distributions of the variables in the sepset.

446
Chapter 11. Inference as Optimization
To gain better insight into the constraints we need, we start with reasoning about properties of
convergence points of the algorithm. Suppose we iterate expectation-propagation belief update
propagations until convergence. Now, consider two neighboring clusters i and j. Since the
algorithm has converged, it follows that further updates do not change the cluster beliefs.
Thus, the assignment of the expectation-propagation update rule of equation (11.41) becomes an
equality for all clusters. We can then reason that
M-projecti,j(IESi,j∼˜βi[τi,j]) = θ˜δi→j + θ˜δj→i.
A similar argument holds for the M-projection of the j cluster beliefs, M-projecti,j(IESi,j∼˜βj[τi,j]).
It follows that the projection of the two beliefs onto Qi,j result in the same distribution.
This discussion suggests that we can pose the problem as optimizing the same objective as
CTree-Optimize, except that we now replace the constraint equation (11.7) with an expectation
expectation
consistency
constraint
consistency constraint:
IESi,j∼µi,j[τi,j] = IESi,j∼βj[τi,j].
(11.42)
In general, τi,j is a vector, and so this equation deﬁnes a vector of constraints.
We now can deﬁne the optimization problem. It is identical to the optimization problems we
already encountered, except that we relax the marginal consistency constraints.
EP-Optimize:
Find
Q
maximizing
˜F[ ˜PΦ, Q]
subject to
IESi,j∼µi,j[τi,j]
=
IESi,j∼βj[τi,j]
∀(i–j) ∈ET
(11.43)
X
ci
βi(ci)
=
1
∀i ∈VT
(11.44)
X
si,j
µi,j[si,j]
=
1
∀(i–j) ∈ET
(11.45)
βi(ci)
≥
0
∀i ∈VT , ci ∈Val(Ci).
(11.46)
Like CGraph-Optimize, this optimization problem is an approximation to the problem of
optimizing the energy functional in two ways. First, the space over which we are optimizing
is the set of pseudo-marginals Q. Because our marginalization constraint only requires that
two neighboring clusters agree on their expectations, they will generally not agree on the full
marginals. Thus, in general, a solution to this problem will generally not correspond to the
marginals of an actual distribution Q, even in the context of a clique tree. Second, because
we can deﬁne the true energy function F[ ˜PΦ, Q] only for coherent joint distributions, we must
resort here to optimizing its factored form ˜F[ ˜PΦ, Q]. Although the two forms are equivalent
for distributions over clique trees, they are not equivalent in this setting (since the exact energy
functional is not even deﬁned outside the space of coherent distributions Q). Thus, as in the
case of the CGBP algorithms in the previous section, we are approximating both the objective
and the optimization space.

11.4. Propagation with Approximate Messages ⋆
447
The generalization of theorem 11.3 for this relaxed optimization problem follows using more
or less the same proof structure, where we characterize the stationary points of the constrained
objective using a set of ﬁxed-point equations that deﬁne a message passing algorithm.
ﬁxed-point
equations
Theorem 11.8
Let Q be a set of beliefs such that µi,j is in the exponential family Qi,j for all (i–j) ∈ET . Let
M-project-distri,j be the M-projection operation into the family Qi,j. Then Q is a stationary point
of EP-Optimize if and only if for every edge (i–j) ∈ET there are auxiliary beliefs δi→j(Si,j) and
δj→i(Si,j) so that
δi→j
=
M-project-distri,j(βi)
δj→i
(11.47)
βi
∝
ψi ·
Y
j∈Nbi
δj→i
µi,j
∝
δj→i · δi→j.
Proof As in previous proofs of this kind, we deﬁne a Lagrangian that consists of the (ap-
proximate) energy functional ˜F[ ˜PΦ, Q] as well as Lagrange multiplier terms for each of the
constraints. In our case, we have a vector of Lagrange multipliers for each of the constraints in
equation (11.43).
J
=
X
i∈VT
IECi∼βi[ln ψi] +
X
i∈VT
IHβi(Ci) −
X
(i–j)∈ET
IHµi,j(Si,j)
−
X
i
X
j∈Nbi
⃗λj→i ·
 IESi,j∼µi,j[τi,j] −IESi,j∼βj[τi,j]

−
X
i∈VT
λi
 X
ci
βi(ci) −1
!
−
X
(i–j)∈ET
λi,j

X
si,j
µi,j[si,j] −1

.
Taking partial derivatives of J with respect to βi(ci) and µi,j[si,j] and equating these deriva-
tives to zero, we get the following equalities that must hold at a stationary point:
βi(ci)
∝
ψi(ci)
Y
j∈Nbi
exp
n
⃗λj→i · τi,j(si,j)
o
µi,j[si,j]
∝
exp
n
(⃗λj→i + ⃗λi→j) · τi,j(si,j)
o
.
Note that (⃗λj→i + ⃗λi→j) serves as the natural parameters of µi,j in its exponential form
representation.
Moreover, the constraint of equation (11.43) implies that
IESi,j∼µi,j[τi,j] = IESi,j∼βj[τi,j].
Thus, using theorem 8.6, we conclude that µi,j = M-project-distri,j(βi). By deﬁning
δi→j[si,j] ∝exp
n
⃗λi→j · τi,(si,j)
o
,
we can verify that the statement of the theorem is satisﬁed.

448
Chapter 11. Inference as Optimization
Theorem 11.8 shows that, if we perform EP belief update propagation until convergence, then
we reach a stationary point of EP-Optimize. Thus, this result provides an optimization semantics
for expectation-propagation message passing.
Our discussion of expectation propagation and the proof were posed in the context of linear
exponential families. The same ideas can be extended to nonlinear families but require additional
subtleties that we do not discuss.
11.4.6
Discussion
In this section, we presented an alternative approach for inference in large graphical models.
Rather than modifying the global structure of the inference object, we modify the structure of
the messages and how they are computed. Although we focused on the application of this
approximation to clique trees, it is equally applicable in the context of cluster graphs.
The
message passing algorithms (both the sum-product algorithm and the belief update algorithm)
can be used for passing messages between clusters in general graphs. Moreover, the variational
analysis of section 11.4.5 also applies, essentially without change, to cluster graphs, using the
same derivation as in section 11.3.
Note that the expectation propagation algorithm suﬀers from the same caveats we discussed
in the previous section: Iterations of EP message propagation are not guaranteed to in-

duce monotonic improvements to the objective function, and the algorithm does not
always converge.
Moreover, even when the algorithm does converge, the clusters are
only approximately calibrated: their marginals agree on the expectations of the suﬃcient
statistics (say the individual marginals), but not on other aspects of the distribution (say
marginals over pairs of variables). As a consequence, if we want to answer a query using
the network, it may make a diﬀerence from which cluster we extract the answer.
We presented expectation propagation in the context of its application to factored messages.
In the simplest case, of fully factored messages, the messages are simply cluster marginals
over individual variables. The similarity between this variant of expectation propagation and
belief propagation is quite striking; indeed, one can simulate expectation propagation with fully
factored messages using cluster-graph belief propagation with a particular factor graph structure
(see exercise 11.23). The more general case of messages that are not fully factored (for example,
ﬁgure 11.13b) is more complex, and they cannot be mapped directly to belief propagation in
cluster graphs. However, a mapping does exist between expectation propagation in discrete
networks with factorized messages and cluster-graph belief propagation with region graphs.
More important, however, is the fact that expectation propagation provides a general approach
for dealing with distributions in the exponential family. It therefore provides message passing
algorithms for a broad class of models. For example, we will see an application of expectation
propagation to hybrid (continuous/discrete) graphical models in section 14.3.3. Its broad appli-
cability makes expectation propagation an important component in the approximate inference
toolbox.
11.5
Structured Variational Approximations
In the previous two sections, we examined approximations based on belief propagation. As
we saw, both methods can be viewed as optimizing an approximate energy functional over the

11.5. Structured Variational Approximations
449
class of pseudo-marginals. These pseudo-marginals generally do not correspond to a globally
coherent joint distribution Q. The structured variational approach aims to optimize the

structured
variational
energy functional over a family Q of coherent distributions Q. This family is chosen
to be computationally tractable, and hence it is generally not suﬃciently expressive to
capture all of the information in PΦ.
More precisely, we aim to address the following maximization problem:
Structured-Variational:
Find
Q ∈Q
maximizing
F[ ˜PΦ, Q]
where Q is a given family of distributions. In these methods, we are using the exact energy
functional F[ ˜PΦ, Q], which satisﬁes theorem 11.2.
Thus, maximizing the energy functional
corresponds directly to obtaining a better approximation to PΦ (in terms of ID(Q||PΦ)).
The main parameter in this maximization problem is the choice of family Q. This choice
induces a trade-oﬀ. On one hand, families that are “simpler,” that is, that can be described by
a Bayesian network or a Markov network with small tree-width, allow more eﬃcient inference.
As we will see, simpler families also allow us to solve the maximization problem eﬃciently. On
the other hand, if the family Q is too restrictive, then it cannot represent distributions that are
good approximations of PΦ, giving rise to a poor approximation Q. In either case, this family
is generally chosen to have enough structure that allows inference to be tractable, giving rise to
the name structured variational approximation.
structured
variational
As we will see, the methods of this type diﬀer from generalized belief propagation in sev-
eral ways. They are guaranteed to lower-bound the log-partition function, and they also are
guaranteed to converge.
11.5.1
The Mean Field Approximation
The ﬁrst approach we consider is called the mean ﬁeld approximation.
As we will see, in
mean ﬁeld
many respects, it resembles the algorithm obtained using the Bethe approximation to the energy
functional. In particular, the resulting algorithm performs message passing where the messages
are distributions over single variables.
As we will see, however, the form of the updates is
somewhat diﬀerent.
11.5.1.1
The Mean Field Energy
Unlike our presentation in earlier sections, we begin our discussion with the energy functional,
and we derive the algorithm directly from analyzing it. The mean ﬁeld algorithm ﬁnds the
distribution Q, which is closest to PΦ in terms of ID(Q||PΦ) within the class of distributions
representable as a product of independent marginals:
Q(X) =
Y
i
Q(Xi).
(11.48)
On the one hand, the approximation of PΦ as a fully factored distribution is likely to lose a
lot of information in the distribution. On the other hand, this approximation is computationally

450
Chapter 11. Inference as Optimization
attractive, since we can easily evaluate any query on Q by a product over terms that involve
the variables in the scope of the query. Moreover, to represent Q, we need only to describe the
marginal probabilities of each of the variables.
As in previous sections, the mean ﬁeld algorithm is derived by considering ﬁxed points of
the energy functional.
We thus begin by considering the form of the energy functional in
energy functional
equation (11.3) when Q has the form of a product distribution as in equation (11.48). We can then
characterize its ﬁxed points and thereby derive an iterative algorithm to ﬁnd such ﬁxed points.
The functional contains two terms. The ﬁrst is a sum of terms of the form IEU φ∼Q[ln φ],
where we need to evaluate
IEU φ∼Q[ln φ]
=
X
uφ
Q(uφ) ln φ(uφ)
=
X
uφ

Y
Xi∈U φ
Q(xi)

ln φ(uφ).
As shown, we can use the form of Q to compute Q(uφ) as a product of marginals, allowing the
evaluation of this term to be performed in time linear in the number of values of U φ. Because
this cost is linear in the description size of the factors of PΦ, we cannot expect to do much
better.
As we saw in section 8.4.1, the term IHQ(X) also decomposes in this case.
Corollary 11.3
If Q(X) = Q
i Q(Xi), then
IHQ(X) =
X
i
IHQ(Xi).
(11.49)
Thus, the energy functional for a fully factored distribution Q can be rewritten simply as a
sum of expectations, each one over a small set of variables. Importantly, the complexity of this
expression depends on the size of the factors in PΦ, and not on the topology of the network.
Thus, the energy functional in this case can be represented and manipulated eﬀectively, even in
networks that would require exponential time for exact inference.
Example 11.10
Continuing our running example, consider the form of the mean ﬁeld energy for a 4 × 4 grid
network. Based on our discussion, we see that it has the form
F[ ˜PΦ, Q]
=
X
i∈{1,2,3},j∈{1,2,3,4}
IEQ[ln φ(Ai,j, Ai+1,j)]
+
X
i∈{1,2,3,4},j∈{1,2,3}
IEQ[ln φ(Ai,j, Ai,j+1)]
+
X
i∈{1,2,3,4},j∈{1,2,3,4}
IHQ(Ai,j).
We see that the energy functional involves only expectations over single variables and pairs of
neighboring variables. The expression has the same general form for an n × n grid. Thus, although
the tree-width of an n × n grid is exponential in n, the energy functional can be represented and
computed in cost O(n2); that is, in a time linear in the number of variables.

11.5. Structured Variational Approximations
451
11.5.1.2
Maximizing the Energy Functional: Fixed-point Characterization
The next step is to consider the task of optimizing the energy function: ﬁnding the distribution
Q for which this energy functional is maximized:
Mean-Field:
Find
{Q(Xi)}
maximizing
F[ ˜PΦ, Q]
subject to
Q(X)
=
Y
i
Q(Xi)
(11.50)
X
xi
Q(xi)
=
1
∀i.
(11.51)
To simplify notation, from now on we use X−i to denote X −{Xi}.
Note that, unlike the cluster-graph belief propagation algorithms of section 11.3 and the ex-
pectation propagation algorithm of section 11.4, here we are not approximating the objective.
We are approximating only the optimization space by selecting a space of distributions Q that
generally does not contain our original distribution PΦ.
As with the previous optimization problems in this chapter, we use the method of Lagrange
multipliers to derive a characterization of the stationary points of F[ ˜PΦ, Q].
However, the
structure of Q allows us to consider the optimal value of each component (that is, marginal
distribution) given the rest. (This iterative optimization procedure was not feasible in cluster
trees and graphs due to constraints that relate diﬀerent beliefs.)
We now provide a set of ﬁxed-point equations that characterize the stationary points of the
ﬁxed-point
equations
mean ﬁeld optimization problem:
Theorem 11.9
The distribution Q(Xi) is a local maximum of Mean-Field given {Q(Xj)}j̸=i if and only if
Q(xi) = 1
Zi
exp



X
φ∈Φ
IEX∼Q[ln φ | xi]


,
(11.52)
where Zi is a local normalizing constant and IEX∼Q[ln φ | xi] is the conditional expectation given
conditional
expectation
the value xi
IEX∼Q[ln φ | xi] =
X
uφ
Q(uφ | xi) ln φ(uφ).
Proof The proof of this theorem relies on proving the ﬁxed-point characterization of the indi-
vidual marginal Q(Xi) in terms of the other components, Q(X1), . . . , Q(Xi−1), Q(Xi+1), . . . ,
Q(Xn), as speciﬁed in equation (11.52).
We ﬁrst consider the restriction of our objective F[ ˜PΦ, Q] to those terms that involve Q(Xi):
Fi[Q] =
X
φ∈Φ
IEU φ∼Q[ln φ] + IHQ(Xi).
(11.53)

452
Chapter 11. Inference as Optimization
To optimize Q(Xi), we deﬁne the Lagrangian that consists of all terms in F[ ˜PΦ, Q] that
involve Q(Xi)
Li[Q] =
X
φ∈Φ
IEU φ∼Q[ln φ] + IHQ(Xi) + λ
 X
xi
Q(xi) −1
!
.
The Lagrange multiplier λ corresponds to the constraint that Q(Xi) is a distribution. We now
take derivatives with respect to Q(xi).
The following result plays an important role in the
remainder of the derivation:
Lemma 11.1
If Q(X) = Q
i Q(Xi) then, for any function f with scope U,
∂
∂Q(xi)IEU∼Q[f(U)] = IEU∼Q[f(U) | xi].
The proof of this lemma is left as an exercise (see exercise 11.24).
Using this lemma, and standard derivatives of entropies, we see that
∂
∂Q(xi)Li =
X
φ∈Φ
IEX∼Q[ln φ | xi] −ln Q(xi) −1 + λ.
Setting the derivative to 0, and rearranging terms, we get that
ln Q(xi) = λ −1 +
X
φ∈Φ
IEX∼Q[ln φ | xi].
We take exponents of both sides and renormalize; because λ is constant relative to xi, it drops
out in the renormalization, so that we obtain the formula in equation (11.52).
This derivation, by itself, shows only that the solution of equation (11.52) is a stationary point
of equation (11.53). To prove that it is a maximum, we note that equation (11.53) is a sum of two
terms: P
φ∈Φ IEU φ∼Q[ln φ] is linear in Q(Xi), given all the other components Q(Xj); IHQ(Xi)
is a concave function in Q(Xi). As a whole, given the other components of Q, the function Fi
is concave in Q(Xi), and therefore has a unique global optimum, which is easily veriﬁed to be
equation (11.52) rather than any of the extremal points.
From this it follows that:
Corollary 11.4
The distribution Q is a stationary point of Mean-Field if and only if, for each Xi, equation (11.52)
holds.
In contrast to theorem 11.9, this result only provides a characterization of stationary points of the
objective, and not necessarily of its optima. The stationary points include local maxima, local
minima, and saddle points. The reason for the diﬀerence is that, although each “coordinate”
Q(Xi) is guaranteed to be locally maximal given the others, the direction that locally improves
the objective may require a coordinated change in several components. We return to this point
in section 11.5.1.3.
We now move to interpreting this characterization. The key term in equation (11.52) is the
argument in the expectation. We can prove the following property.

11.5. Structured Variational Approximations
453
Corollary 11.5
In the mean ﬁeld approximation, Q(Xi) is locally optimal only if
Q(xi) = 1
Zi
exp

IEX−i∼Q[ln PΦ(xi | X−i)]
	
(11.54)
where Zi is a normalizing constant.
Proof Recall that ˜PΦ = Q
φ∈Φ φ is the unnormalized measure deﬁned by Φ.
Due to the
linearity of expectation:
X
φ∈Φ
IEX∼Q[ln φ | xi] = IEX∼Q
h
ln ˜PΦ(Xi, X−i) | xi
i
.
Because Q is a product of marginals, we can rewrite Q(X−i | xi) = Q(X−i), and get that:
IEX∼Q
h
ln ˜PΦ(Xi, X−i) | xi
i
= IEX−i∼Q
h
ln ˜PΦ(xi, X−i)
i
.
Using properties of conditional distributions, it follows that:
˜PΦ(xi, X−i) = ZPΦ(xi, X−i) = ZPΦ(X−i)PΦ(xi | X−i).
We conclude that
X
φ∈Φ
IEX∼Q[ln φ | xi] = IEX−i∼Q[ln PΦ(xi | X−i)] + IEX−i∼Q[ln PΦ(X−i)Z].
Plugging this equality into the update equation equation (11.52), we get that
Q(xi) = 1
Zi
exp

IEX−i∼Q[ln PΦ(xi | X−i)]
	
exp

IEX−i∼Q[ln PΦ(X−i)Z]
	
.
The term ln PΦ(X−i)Z does not depend on the value of xi. Recall that when we multiply a
belief by a constant factor, it does not change the distribution Q; in fact, as we renormalize
the distribution at the end to sum to 1, this constant will be “absorbed” into the normalizing
function, to achieve normalization. Thus, we can simply ignore this term, thereby achieving the
desired conclusion. We note that this type of algebraic manipulation will prove useful multiple
times throughout this section.
This corollary shows that Q(xi) is the geometric average of the conditional probability of
xi given all other variables in the domain. The average is based on the probability that Q
assigns to all possible assignments to the variables in the domain. In this sense, the mean
ﬁeld approximation requires that the marginal of Xi be “consistent” with the marginals of other
variables.
Note that, in PΦ, we can also represent the marginal of Xi as an average:
PΦ(xi) =
X
x−i
PΦ(x−i)PΦ(xi | x−i) = IEX−i∼PΦ[PΦ(xi | X−i)].
(11.55)
This average is an arithmetic average, whereas the one used in the mean ﬁeld approximation is a
geometric average. In general, the latter tends to lead to marginals that are more sharply peaked
than the original marginals in PΦ. More signiﬁcant, however, is the fact that the expectations
in equation (11.55) are taken relative to PΦ, whereas the ones in equation (11.54) are taken
relative to the approximation Q. Thus, this similarity does not imply as a consequence that our
approximation in Q to the marginals in PΦ is a good one.

454
Chapter 11. Inference as Optimization
11.5.1.3
Maximizing the Energy Functional: The Mean Field Algorithm
How do we convert the ﬁxed-point equation of equation (11.52) into an update algorithm? We
start by observing that if Xi ̸∈Scope[φ] then IEU φ∼Q[ln φ | xi] = IEU φ∼Q[ln φ]. Thus, expec-
tation terms on such factors are independent of the value of Xi. Consequently, we can absorb
them into the normalization constant Zi and get the following simpliﬁcation.
Corollary 11.6
In the mean ﬁeld approximation, Q(Xi) is locally optimal only if
Q(xi) = 1
Zi
exp



X
φ:Xi∈Scope[φ]
IE(U φ−{Xi})∼Q[ln φ(U φ, xi)]


.
(11.56)
where Zi is a normalizing constant.
This representation shows that Q(Xi) has to be consistent with the expectation of the
potentials in which it appears.
In our grid network example, this characterization implies
that Q(Ai,j) is a product of four terms measuring its interaction with each of its four neighbors:
Q(ai,j) =
1
Zi,j
exp









P
ai−1,j Q(ai−1,j) ln(φ(ai−1,j, ai,j))+
P
ai,j−1 Q(ai,j−1) ln(φ(ai,j−1, ai,j))+
P
ai+1,j Q(ai+1,j) ln(φ(ai,j, ai+1,j))+
P
ai,j+1 Q(ai,j+1) ln(φ(ai,j, ai,j+1))









.
(11.57)
Each term is a (geometric) average of one of the potentials involving Ai,j. For example, the ﬁnal
term in the exponent represents a geometric average of the potential between Ai,j and Ai,j+1,
averaged using the distribution Q(Ai,j+1).
The characterization of corollary 11.6 provides tools for developing an algorithm to maximize
F[ ˜PΦ, Q]. For example, examining equation (11.57), we see that we can easily evaluate the term
within the exponential by considering each of Ai,j’s neighbors and computing the interaction
between the values that neighbor can take and possible values of Ai,j.
Moreover, in this
example, we see that Q(Ai,j) does not appear on the right-hand side of the update rule. Thus,
we can choose Q(Ai,j), which satisﬁes the required equality by assigning it to the term denoted
by the right-hand side of the equation.
This last observation is true in general. All the terms on the right-hand side of equation (11.56)
involve expectations of variables other than Xi, and do not depend on the choice of Q(Xi). We
can achieve equality simply by evaluating the exponential terms for each value xi, normalizing
the results to sum to 1, and then assigning them to Q(Xi). As a consequence, we reach the
optimal value of Q(Xi) in one easy step.
This last statement must be interpreted with some care. The resulting value for Q(Xi) is its
optimal value given the choice of all other marginals. Thus, this step optimizes our function
relative only to a single coordinate in the space — the marginal of Q(Xi). To optimize the
function in its entirety, we need to optimize relative to all of the coordinates. We can embed this
step in an iterated coordinate ascent algorithm, which repeatedly optimizes a single marginal at a
time, given ﬁxed choices to all of the others. The resulting algorithm is shown in algorithm 11.7.
Importantly, a single optimization of Q(Xi) does not usually suﬃce: a subsequent modiﬁcation

11.5. Structured Variational Approximations
455
Algorithm 11.7 The Mean-Field approximation algorithm
Procedure Mean-Field (
Φ,
// factors that deﬁne PΦ
Q0
// Initial choice of Q
)
1
Q ←Q0
2
Unprocessed ←X
3
while Unprocessed ̸= ∅
4
Choose Xi from Unprocessed
5
Qold(Xi) ←Q(Xi)
6
for xi ∈Val(Xi) do
7
Q(xi) ←exp
nP
φ:Xi∈Scope[φ] IE(U φ−{Xi})∼Q[ln φ[U φ, xi]]
o
8
Normalize Q(Xi) to sum to one
9
if Qold(Xi) ̸= Q(Xi) then
10
Unprocessed ←Unprocessed ∪
 ∪φ:Xi∈Scope[φ]Scope[φ]

11
Unprocessed ←Unprocessed −{Xi}
12
return Q
to another marginal Q(Xj) may result in a diﬀerent optimal parameterization for Q(Xi). Thus,
the algorithm repeats these steps until convergence. Note that, in practice, we do not test for
equality in line 9, but rather for equality up to some ﬁxed small-error tolerance.
A key property of the coordinate ascent procedure is that each step leads to an increase in
the energy functional. Thus, each iteration of Mean-Field results in a better approximation

Q to the target density PΦ, guaranteeing convergence.
Theorem 11.10
The Mean-Field iterations are guaranteed to converge. Moreover, the distribution Q∗returned by
Mean-Field is a stationary point of F[ ˜PΦ, Q], subject to the constraint that Q(X) = Q
i Q(Xi) is
a distribution.
Proof We showed earlier that each iteration of Mean-Field is monotonically nondecreasing in
F[ ˜PΦ, Q]. Because the energy functional is bounded, the sequence of distributions represented
by successive iterations of Mean-Field must converge. At the convergence point the ﬁxed-point
equations of theorem 11.9 hold for all the variables in the domain.
As a consequence, the
convergence point is a stationary point of the energy functional.
As we discussed, the distribution Q∗returned by Mean-Field is not necessarily a local op-
timum of the algorithm. However, local minima and saddle points are not stable convergence
points of the algorithm, in the sense that a small perturbation of Q followed by optimization
will lead to a better convergence point. Because the algorithm is unlikely to accidentally land
precisely on the unstable point and get stuck there, in practice, the convergence points of the
algorithm are local maxima.
In general, however, the result of the mean ﬁeld approximation is a local maximum, and not
necessarily a global one.

456
Chapter 11. Inference as Optimization
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Q(a1)
Q(b1)
Figure 11.16
An example of a multimodal mean ﬁeld energy functional landscape. In this network,
P(a, b) = 0.5 −ϵ if a ̸= b and ϵ if a = b. The axes correspond to the mean ﬁeld marginal for Q(a1) and
Q(b1) and the contours show equi-values of the energy functional for diﬀerent choices of these variational
parameters. As we can see, there are two modes to the energy functional, one roughly corresponds to
a1, b0 and the other one to a0, b1. In addition, there is a saddle point at (0.5, 0.5).
Example 11.11
Consider a distribution PΦ that is an approximate XOR (exclusive or) of two variables A and B,
so that PΦ(a, b) = 0.5 −ϵ if a ̸= b and PΦ(a, b) = ϵ if a = b. Clearly, we cannot approximate
PΦ by a product of marginals, since such a product cannot capture the relationship between A
and B. It turns out that if ϵ is suﬃciently small, say 0.01, then the energy potential surface has
two local maxima that correspond to the two cases where a ̸= b. See ﬁgure 11.16. (For suﬃciently
large ϵ, such as 0.1, the mean ﬁeld approximation has a single maximum point at the uniform
distribution.)
We can use standard strategies, such as multiple random restarts, to try to avoid getting stuck
in local maxima. However, these do not overcome the basic shortcoming of the mean ﬁeld
approximation, which is apparent in this example. The approximation cannot describe complex
posteriors, such as the XOR posterior we discussed.
And thus, we cannot expect it to give
satisfactory approximations in these situations. To provide better approximations, we must use
a richer class of distributions Q, which has greater expressive power.
11.5.2
Structured Approximations
The mean ﬁeld algorithm provides an easy approximation method. However, it is limited by
forcing Q to be a very simple distribution.
As we just saw, the fact that all variables are
independent of each other in Q can lead to very poor approximations.
Intuitively, if we

11.5. Structured Variational Approximations
457
(b)
(a)
A1,1
A1,4
A1,3
A1,2
A2,1
A2,4
A2,3
A2,2
A3,1
A3,4
A3,3
A3,2
A4,1
A4,4
A4,3
A4,2
A1,1
A1,4
A1,3
A1,2
A2,1
A2,4
A2,3
A2,2
A3,1
A3,4
A3,3
A3,2
A4,1
A4,4
A4,3
A4,2
Figure 11.17
Two structures for variational approximation of a 4 × 4 grid network
use a distribution Q that can capture some of the dependencies in PΦ, we can get a better
approximation. Thus, we would like to explore the spectrum of approximations between the
mean ﬁeld approximation and exact inference.
A natural approach to get richer approximations that capture some of the dependencies in
PΦ is to use network structures of diﬀerent complexity. By adding and removing edges from
the network we can control the cost of inference in the approximating distribution and how
well it captures dependencies in the target distribution. We can achieve this type of ﬂexibility
by using either Bayesian networks or Markov networks. Both types of networks lead to similar
approximations, and so we focus on the undirected case, parameterized as Gibbs distributions
(so that we are not restricted to factors over maximal cliques). Exercise 11.34 develops similar
ideas using a Bayesian network approximation.
11.5.2.1
Fixed-Point Characterization
We now consider the form of the variational approximation when we are given a general form
of Q as a Gibbs parametric family. Formally, we assume we are given a set of potential scopes
{Cj ⊆X : j = 1, . . . , J}. We can then choose an approximation Q that has the form:
Q(X) =
1
ZQ
J
Y
j=1
ψj,
(11.58)
where ψj is a factor with Scope[ψj] = Cj.
Example 11.12
Consider again the grid network example. There are many possible approximating network struc-
tures we can choose that allow for eﬃcient inference. As a concrete example, we might choose
potential scopes {A1,1, A1,2}, {A1,2, A1,3}, . . . , {A2,1, A2,2}, {A2,2, A2,3}, . . .. That is, we pre-
serve the dependencies between variables in the same row, but ignore the ones that relate diﬀer-
ent columns. Alternatively, we can consider an approximation that preserves dependencies along
columns and ignores the dependencies between rows. As we can see in ﬁgure 11.17, in both cases,

458
Chapter 11. Inference as Optimization
the structure we use is a collection of independent chain structures. Exact inference with such
structures is linear, and so the cost of inference is not much worse than in the mean ﬁeld approx-
imation. Clearly, we can also consider many other structures for the approximating distributions.
These might introduce additional dependencies and can have higher cost in terms of inference. We
will return to the question of what structure to use.
Assume that we decide on the form of the potentials for the approximating family Q. As
before, we consider the form of the energy functional for a distribution Q in this family. We
then characterize the stationary points of the functional, and we use those to derive an iterative
optimization algorithm.
As before, evaluating the terms that involve IEU φ∼Q[ln φ] requires performing expectations
with respect to the variables in Scope[φ]. Unlike the case of mean ﬁeld approximation, the
complexity of computing this expectation depends on the structure of the approximating distri-
bution. However, we assume that we can solve this problem by exact inference (in the network
corresponding to Q), using the methods we discussed in previous chapters.
As discussed in section 8.4.1, the entropy term in the energy functional also reduces to
computing a similar set of expectation terms:
Proposition 11.5
If Q(X) =
1
ZQ
Q
j ψj, then
IHQ(X) = −
J
X
j=1
IECj∼Q[ln ψj(Cj)] + ln ZQ.
Overall, we obtain the following form for the energy functional, for distributions Q in the
family Q:
F[ ˜PΦ, Q] =
K
X
k=1
IEQ[ln φk] −
J
X
j=1
IEQ[ln ψj] + ln ZQ.
(11.59)
As before, the hard question is how to optimize the potential to get the best approximation.
We solve this problem using the same general strategy we discussed in the context of the mean
ﬁeld approximation. First, we derive the ﬁxed-point equations that hold when the approximation
is a local maximum (or, more precisely, a stationary point) of the energy functional. We then
use these ﬁxed-point equations to help derive an optimization algorithm.
We derive the ﬁxed-point equations by taking derivatives of F[ ˜PΦ, Q] with respect to param-
ﬁxed-point
equations
eters of the distribution Q. In our case, the parameters will be an entry ψj(cj) in each of the
factors that deﬁne the distribution. We then set those equations to zero, obtaining the following
result:
Theorem 11.11
If Q(X) =
1
ZQ
Q
j ψj, then the potential ψj is a stationary point of the energy functional if and
only if
ψj(cj) ∝exp


IEQ
h
ln ˜PΦ | cj
i
−
X
k̸=j
IEQ[ln ψk | cj] −F[ ˜PΦ, Q]


.
(11.60)

11.5. Structured Variational Approximations
459
The proof is straightforward algebraic manipulation and is left as an exercise (exercise 11.26).
This theorem establishes a characterization of the ﬁxed point as the diﬀerence between the
expected value of logarithm of the original potentials and the expected value of the logarithm of
the approximating potentials. The last term in equation (11.60) is the energy functional F[ ˜PΦ, Q],
which is independent of the assignment cj; thus, as we discussed in the proof of corollary 11.5,
we can absorb this term into the normalization constant of the distribution and ignore it.
Corollary 11.7
If Q(X) =
1
ZQ
Q
j ψj, then the potential ψj is a stationary point of the energy functional if and
only if:
ψj(cj) ∝exp


IEQ
h
ln ˜PΦ | cj
i
−
X
k̸=j
IEQ[ln ψk | cj]


.
(11.61)
As we show in section 11.5.2.3 and section 11.5.2.4, we can often exploit additional structure
in Q to reduce further the complexity of the ﬁxed-point equations, and hence of the resulting
update steps. The following discussion, which describes the procedure of applying the ﬁxed-
point equations to ﬁnd a stationary point of the energy functional, is orthogonal to these
simpliﬁcations.
11.5.2.2
Optimization
Given a set of ﬁxed-point equations as in equation (11.61), our task is to ﬁnd a distribution Q
that satisﬁes them. As in section 11.5.1, our strategy is based on the key observation that the
factor ψj does not aﬀect the right-hand side of the ﬁxed-point equations deﬁning its value: The
ﬁrst expectation, IEQ
h
ln ˜PΦ | cj
i
, is conditioned on cj and therefore does not depend on the
parameterization of ψj. The same observation holds for the second expectation, IEQ[ln ψk | cj],
for any k ̸= j. (Importantly, there is no such term for k = j in the right-hand side.) Thus,
we can use the same general approach as in Mean-Field: We can optimize each potential ψj,
given values for the other potentials, by simply selecting ψj to satisfy the ﬁxed-point equation.
As for the case of mean ﬁeld, this step is guaranteed to increase (or not decrease) the value of
the objective; thus, the overall process is guaranteed to converge to a stationary point of the
objective.
This last step requires that we perform inference in the approximating distribution Q to
compute the requisite expectations.
Although this step was also present (implicitly) in the
mean ﬁeld approximation, there the structure of the approximating distribution was trivial,
and so the inference step involved only individual marginals.
Here, we need to collect the
expectation of several factors, and each of these requires that we compute expectations given
diﬀerent assignments to the factor of interest. (See exercise 11.27 for a discussion of how these
expectations can be computed eﬃciently.) For a general distribution Q, even one with tractable
structure, running inference in the corresponding network HQ can be costly, and we may want
to reduce the number of calls to the inference subroutine.
This observation leads to a question of how best to perform updates for several factors in
Q. We can consider two strategies. The sequential update strategy is similar to our strategy
in the mean ﬁeld algorithm: We choose a factor ψj, apply the ﬁxed-point equation to that

460
Chapter 11. Inference as Optimization
factor by running inference in HQ, update the distribution, and then repeat this process with
another factor until convergence. The problematic aspect of this approach is that we need to
perform inference after each update step. For example, if we are using cluster tree inference
in the network HQ, the network parameterization changes after each update step, so we need
to recalibrate the clique tree every time. Some of these steps can be made more eﬃcient by
selecting an appropriate order of updates and using dynamic programming (see exercise 11.27),
but the process can still be quite expensive.
An alternative approach is the parallel update strategy, where we compute the right-hand side
of our ﬁxed-point equations (for example, equation (11.61)) simultaneously for each of the factors
in Q. If we are using a cluster tree for inference, this process involves multiple queries from the
same calibrated cluster tree. Thus, we can perform a single calibration step and use the resulting
tree to reestimate all of our potentials. However, the diﬀerent queries required all have diﬀerent
evidence; hence, it is not easy to obtain signiﬁcant computational savings, and the algorithms
needed are fairly tricky. Nevertheless, this approach might be less costly than recalibrating the
clique tree J times.
On the other hand, the guarantees provided by these two update steps are diﬀerent. For
the sequential update strategy, we can prove that each update step is monotonic in the energy
functional: each step maximizes the value of one potential given the values of all the others,
and therefore is guaranteed not to decrease (and generally to increase) the energy functional.
This monotonic improvement implies that iterations of sequential updates necessarily converge,
generally to a local maximum. The issue of convergence is more complicated in the parallel
update strategy.
Because we update all the potentials at once, we have no guarantees that
any ﬁxed-point equation holds after the update; a value that was optimal for ψj with respect
to the values of all other factors before the parallel update step is not necessarily optimal
given their new values. As such, it is conceivable that parallel updates will not converge (for
example, oscillate between two sets of values for the potentials). Such oscillations can generally
be avoided using damped update steps, similar to these we discussed in the case of cluster-
graph belief propagation (see box 11.B), but this modiﬁed procedure still does not guarantee
convergence.
At this point, there is no generally accepted procedure for scheduling updates in variational
methods, and diﬀerent approaches are likely to be best for diﬀerent applications.
11.5.2.3
Simplifying the Update Equations
Equation (11.61) provides a general characterization of the ﬁxed points of the energy functional,
for any approximating class of distributions Q obeying a particular factorization, as in equa-
tion (11.58). In many cases, we can exploit additional structure of the approximating class Q
and of the distribution PΦ to simplify signiﬁcantly the form of these ﬁxed-point equations and
thereby make the update step more eﬃcient.
The simpliﬁcations we describe take two forms. The ﬁrst utilizes marginal independencies
in Q to simplify the right-hand side of the ﬁxed-point equation, equation (11.61), eliminating
irrelevant terms. The second exploits interactions between the form of Q and the form of PΦ to
simplify the factorization of Q, without loss in expressive power. Both simpliﬁcations allow the
ﬁxed-point updates to be performed more eﬃciently. We motivate each of the simpliﬁcations
using an example, and then we present the general result.

11.5. Structured Variational Approximations
461
Example 11.13
Once again, consider the 4×4 grid network. Assume that we approximate it by a “row” network that
has the structure shown in ﬁgure 11.17a. This approximating network consists of four independent
chains. Now we can apply the general form of the ﬁxed-point equation equation (11.61) for a speciﬁc
entry in our approximation, say:
ψ1,1(a1,1, a1,2) ∝exp











IEQ
h
ln ˜PΦ | a1,1, a1,2
i
−P
i=1,...,4
j=1,...,3
(i,j)̸=(1,1)
IEQ

ln ψ(i,j)(Ai,j, Ai,j+1) | a1,1, a1,2












.
As in the proof of corollary 11.5, the expectation of ln ˜PΦ is the sum of expectations of the logarithm
of each of the potentials in Φ. Some of these terms, however, do not depend on the choice of value of
A1,1, A1,2 we are evaluating. For example, because A2,1 and A2,2 are independent of A1,1, A1,2
in Q, we conclude that
IE{A2,1,A2,2}∼Q[ln φ(A2,1, A2,2) | a1,1, a1,2] = IE{A2,1,A2,2}∼Q[ln φ(A2,1, A2,2)].
Thus, this term will contribute the same value to each of the entries of ψ(A1,1, A1,2), and
can therefore be absorbed into the corresponding normalizing term.
We can continue in this
manner and remove all terms that are not dependent on the context of the factor we are in-
terested in. Overall, we can remove any term IEQ[ln φ(Ai,j, Ai,j+1) | a1,1, a1,2] and any term
IEQ[ln φ(Ai,j, Ai+1,j) | a1,1, a1,2] except those where i = 1. Similarly, we can remove any term
IEQ

ln ψ(i,j)(Ai,j, Ai,j+1) | a1,1, a1,2

except those where i = 1. These simpliﬁcations result in
the following update rule:
ψ1,1(a1,1, a1,2) ∝
exp



P
j=1,...,3 IE{A1,j,A1,j+1}∼Q

ln φ(1,j)(A1,j, A1,j+1) | a1,1, a1,2

+ P
j=1,...,4 IE{A1,j,A2,j}∼Q

ln φ(1,j)(A1,j, A2,j) | a1,1, a1,2

−P
j=2,3 IE{A1,j,A1,j+1}∼Q

ln ψ(1,j)(A1,j, A1,j+1) | a1,1, a1,2



.
We can generalize this analysis to arbitrary sets of factors:
Theorem 11.12
If Q(X) =
1
ZQ
Q
j ψj, then the potential ψj is locally optimal only if
ψj(cj) ∝exp



X
φ∈Aj
IEX∼Q[ln φ | cj] −
X
ψk∈Bj
IEX∼Q[ln ψk | cj]


,
(11.62)
where
Aj = {φ ∈Φ : Q ̸|= (U φ ⊥Cj)}
and
Bj = {ψk : Q ̸|= (Ck ⊥Cj)} −{Cj}.

462
Chapter 11. Inference as Optimization
(b)
D
B
A
C
1
(c)
D
B
A
C
2
(d)
D
B
A
C
3
(a)
D
B
A
C
Figure 11.18
A diamond network and three possible approximating structures
.
Stated in words, this result shows that the parameterization of a factor ψj(Cj) depends only
on factors in PΦ and in Q whose scopes are not independent of Cj in Q. This result, applied
to example 11.13, provides us precisely with the simpliﬁcation shown: only factors whose scopes
intersect with the ﬁrst row are relevant to ψ1,1(A1,1, A1,2). Thus, we can use independence
properties of the approximating family Q to simplify the right-hand side of equation (11.61) by
removing irrelevant terms.
11.5.2.4
Simplifying the Family Q
It turns out that a similar analysis allows us to simplify the form of the approximating family Q
without loss in the quality of the approximation.
We start by considering a simple example.
Example 11.14
Consider again the four-variable pairwise Markov network of ﬁgure 11.18a, which is parameterized
by the pairwise factors:
PΦ(A, B, C, D) ∝φAB(A, B) · φBC(B, C) · φCD(C, D) · φAD(A, D).
Consider applying the variational approximation with the distribution
Q(A, B, C, D) =
1
ZQ
ψ1(A, B) · ψ2(C, D)
(11.63)
that has the structure shown in ﬁgure 11.18b. Using equation (11.62), we conclude that the ﬁxed-point
characterization of ψ1 is
ψ1(a, b) ∝exp {IEQ[ln φAB(A, B) | a, b] + IEQ[ln φBC(B, C) | a, b] + IEQ[ln φAD(A, D) | a, b]} .
Can we further simplify this equation? Consider the ﬁrst term. Clearly, IEQ[ln φAB(A, B) | a, b] =
ln φAB(a, b). What about the second term, IEQ[ln φBC(B, C) | a, b]? To compute this expectation,
we need to compute Q(B, C | a, b). According to the structure of Q, we can see that
Q(B, C | a, b) =
 Q(C)
If B = b
0
otherwise.

11.5. Structured Variational Approximations
463
Thus, we conclude that
IEA,B,C∼Q[ln φBC(B, C) | a, b] = IEC∼Q[ln φBC(b, C)].
We can simplify the third term in exactly the same way, concluding that:
ψ1(a, b) ∝exp {ln φAB(a, b) + IEC∼Q[ln φBC(b, C)] + IED∼Q[ln φAD(a, D)]} .
Setting ψ′
1(a) = exp{IED∼Q[ln φAD(a, D)]} and ψ′′
1(b) = exp{IEC∼Q[ln φBC(b, C)]}, we con-
clude that the optimal ψ1 factorizes as a product of three factors:
ψ1(A, B) = φAB(A, B) · ψ′
1(A) · ψ′′
1(B).
Have we gained anything from this decomposition? First, we see that Q preserves the original
pairwise interaction term φ(A, B) from PΦ. Moreover, the eﬀect of the interactions between these
variables and the rest of the network (C and D in this example) is summarized by a univariate
factor for each of the variables. Thus, Q does not change the interaction between A and B.
Applying the same set of arguments to ψ2, we conclude that we can rewrite Q as
Q′(A, B, C, D) =
1
ZQ
φAB(A, B) · φCD(C, D) · ψ′
1(A) · ψ′′
1(B) · ψ′
2(C) · ψ′′
2(D).
(11.64)
The preceding discussion shows that the best approximation to PΦ within Q can be rewritten in the
form of Q′. Thus, there is no point in using the more complicated form of the approximating family
of equation (11.63); we may as well use the form of Q′ in equation (11.64). Note that the form of Q′
involves a product of a subset of the original factors, which we keep intact without change, and a
set of new factors, which we need to optimize. In this example, instead of estimating two pairwise
potentials, we estimate four univariate potentials, which utilize a smaller number of parameters.
Moreover, the update equations for Q′ are simpler.
Consider, for example, applying equa-
tion (11.62) for ψ′
1:
ln ψ′
1(a) ∝IEB∼Q′[ln φAB(a, B) | a] + IED∼Q′[ln φAD(a, D) | a] + IEB,C∼Q′[ln φBC(B, C) | a]
−IEB∼Q′[ln φAB(a, B) | a] −IEB∼Q′[ln ψ′′
1(B) | a]
= IED∼Q′[ln φAD(a, D) | a] + IEB,C∼Q′[ln φBC(B, C) | a] −IEB∼Q′[ln ψ′′
1(B) | a].
The terms involving IEQ′[ln φAB | a] appear twice, once as a factor in PΦ and once as a factor in
Q′. These two terms cancel out, and we are left with the simpler update equation. Although this
equation does not explicitly mention φAB, this factor participates in the computation of Q′(B | a)
that implicitly appears in IEB∼Q′[ln ψ′′
1(B) | a].
Note that this result is somewhat counterintuitive, since it shows that the interactions between
A and B are captured by the original potential in PΦ. Intuitively, we would expect the chain of
inﬂuence A—D—C—B to introduce additional interactions between A and B that should be
represented in Q. This is not the only counterintuitive result.

464
Chapter 11. Inference as Optimization
Example 11.15
Consider another approximating family for the same network, using the network structure shown
in ﬁgure 11.18c. In this approximation, we have two pairwise factors, ψ1(A, C), and ψ2(B, D).
Applying the same set of arguments as before, we can show that the update equation can be written
as
ln ψ1(a, c)
∝
IEB∼Q[ln φAB(a, B)] + IED∼Q[ln φAD(a, D)]
+IEB∼Q[ln φBC(B, c)] + IED∼Q[ln φCD(c, D)].
Thus, we can factorize ψ1 into two factors, one with a scope of A and the other with C
ψ1(A, C) = ψ′
1(A) · ψ′′
1(C).
In other words, the approximation in this case is equivalent to the mean ﬁeld approximation.
This result shows that, in some cases, we can remove spurious dependencies in the approximating
distribution. However, this result is surprising, since it holds regardless of the actual values of the
potentials in PΦ. And so, we can imagine a network where there are very strong interactions
between A and C and between B and D in PΦ, and yet the variational approximation with
a network structure of ﬁgure 11.18c will not capture these dependencies. This is a consequence of
using I-projections. Had we used an M-projection that minimizes ID(PΦ||Q), then we would have
represented the dependencies between A and C; see exercise 11.30.
These two examples suggest that we can use the ﬁxed-point characterization to reﬁne an
initial approximating network by factorizing its factors into a product of, possibly smaller, factors
and potentials from PΦ. We now consider the general theory of such factorizations and then
discuss its implications.
We start with a simple deﬁnition and a proposition that form the basis of the simpliﬁcations
we consider.
Deﬁnition 11.8
Let H be a Markov network structure and let X, Y ⊆X. We deﬁne the Y -interface of X, denoted
interface
InterfaceH(X; Y ), to be the minimal subset of X such that sepH(X; Y | InterfaceH(X; Y )).
That is, the Y -interface of X is the subset of X that suﬃces to separate it from Y .
Example 11.16
The {A, D}-interface of {A, B} in HPΦ of ﬁgure 11.18 is {A, B}, since neither A is separated from
{A, D} given B, nor is B is separated from {A, D} given A. In HQ1, we have that B is separated
from {A, D} given A, so that InterfaceHQ1({A, B}; {A, D}) is {A}. The same holds in HQ3. In
HQ2, we have that, again, neither A nor B suﬃces to separate the other from {A, D}, and hence,
InterfaceHQ2({A, B}; {A, D}) = {A, B}.
The deﬁnition of interface can be used to reduce the scope of the conditional expectations in
the ﬁxed-point equations:
Proposition 11.6
If H is an I-map of Q(X) =
1
ZQ
Q
j ψj and φ is a potential with scope U φ. Then,
IEU φ∼Q[φ | cj] = IEU φ∼Q[φ | cj⟨InterfaceH(Cj; U φ)⟩].

11.5. Structured Variational Approximations
465
The proof follows immediately from the deﬁnition of conditional independence.
This proposition provides a principled approach for reformulating terms on the right-hand
side of the ﬁxed-point equation.

We can use this simpliﬁcation result to deﬁne a two-phase strategy for designing
approximation. First, we deﬁne a “rough” outline for approximation by deﬁning Q over
factors with a fairly large scope. We use this outline to obtain a set of update equations,
as implied by equation (11.62) on Q. We then derive a ﬁner-grained representation by
factorizing each of these factors using proposition 11.6. This process results in a ﬁner-
grained approximation that is provably equivalent to the one with which we started.
Theorem 11.13
(Factorization) Let Q be an approximating family deﬁned in terms of factors {ψj(Ck)}, which
induce a Markov network structure HQ. Let Q ∈Q be a stationary point of the energy functional
F[ ˜PΦ, Q] subject to the given factorization. Then, factors in Q are factorized as
ψj(Cj) =
Y
φ∈Φj
φ
Y
Dl∈Dj
ψj,l(Dl),
(11.65)
where
Φj = {φ ∈Φ : Scope[φ] ⊆Cj}
and
Dj = {InterfaceHQ(Cj; X) : X ∈{Scope[φ] : φ ∈Φ −Φj} ∪{Scope[ψk] : k ̸= j}}.
This theorem states that ψj can be written as the product of two sets of factors. The ﬁrst set
contains factors in the original distribution PΦ whose scope is a subset of the scope of ψj. The
factors in the second set are the interfaces of ψj with other factors that appear in the update
equation. These include factors in PΦ that are partially “covered” by the scope of ψk, and other
factors in Q. The set Dk deﬁnes the set of interfaces between ψk and these factors.
To gain a better understanding of this theorem, let us consider various approximations in two
concrete examples. The ﬁrst example serves to demonstrate the ease with which this theorem
allows us to determine the form of the factorization of Q.
Example 11.17
Let us return to example 11.14. In example 11.16, we have already shown the interfaces of {A, B}
with {A, D} in H1. This analysis, together with theorem 11.13, directly imply the reduced factor-
ization of example 11.14 In particular, for ψ1({A, B}), we have that Φ1 contains only the factor
φ({A, B}) in PΦ, which therefore constitutes the ﬁrst term in the factorization of equation (11.65).
The second set of terms in the equation corresponds to the interfaces of {A, B} with other factors in
both HPΦ and in HQ1. We get two such interfaces: one with scope {A} from the factor φ({A, D})
in PΦ, and one with scope {B} from the factor φ({B, C}).
Assume that we add the edge A—C, as in ﬁgure 11.18d. Now, InterfaceHQ3 ({A, B}; {B, C})
is the entire set {A, B}, since B no longer separates C from A. Thus, in this case, the second set
of terms in the factorization of ψ also contains a new pairwise interaction factor ψ1,{A,B}. As a
consequence, the pairwise interaction of A, B is no longer the same in Q and in PΦ. This result is
somewhat counterintuitive: In the simpler network HQ1, which contained no factors allowing any
interaction between the A, B pair and the C, D pair, the A, B interaction was the same in PΦ and

466
Chapter 11. Inference as Optimization
in Q. But if we enrich our approximation (presumably allowing a better ﬁt via the introduction of
the A, C factor), the pairwise interaction term does change.
Finally, HQ2 does not contain an {A, B} factor. Here, Φj = ∅for both factors in HQ2, and
each Dj consists solely of singleton scopes; for example, InterfaceHQ2 ({A, C}; {A, D}) = {A}.
Our second example serves to illustrate the two-phase strategy described earlier, where we
ﬁrst select a “rough” approximation containing a few large factors and then use the theorem to
reﬁne them.
Example 11.18
Consider again our running example of the 4 × 4 grid.
Suppose we select an approximation
where each factor consists of the variables in a single row in the grid. Thus, for example, C1 =
{A1,1, . . . , A1,4}. Note that this approximation is not the one shown in ﬁgure 11.17a, since the
structure in our approximation here is a full clique over each row. We now apply theorem 11.13.
What is the factorization of C1?
First, we search for factors in Φ1.
We see that the factors
φ(A1,1, A1,2), φ(A1,2, A1,3), and φ(A1,3, A1,4) have a scope that is a subset of C1. Next, we
consider the interfaces between C1 and other factors in PΦ and Q. For example, the interface with
φ(A1,1, A2,1) is {A1,1}. Similarly, {A1,2}, {A1,3}, and {A1,4} are interfaces with other factors
in PΦ. It is easy to convince ourselves that these are the only non-empty interfaces in I1. Thus, by
applying theorem 11.13, we get the following factorization:
ψ1(A1,1, . . . , A1,4)
=
φ(A1,1, A1,2) · φ(A1,2, A1,3) · φ(A1,3, A1,4)
ψ1,1(A1,1) · ψ1,2(A1,2) · ψ1,3(A1,3) · ψ1,4(A1,4).
We conclude that, once we decide that the approximation should decouple the rows in the group,
we might as well work with an approximation where we keep all original potentials along each
row and introduce univariate potentials only to capture interactions along columns. Additional
potentials, such as a potential between A1,1 and A1,3, would not improve the approximation.
Thus, while we started with an approximation containing full cliques on each of the rows, we ended
up with an approximation whose structure is that of ﬁgure 11.17a, and where we have only the
original factors and new factors over single variables.
We can work directly with this new factorized form of Q, ignoring our original factorization
entirely. More precisely, we deﬁne Q′ to be
Q′(X)
=
φ(A1,1, A1,2) · φ(A1,2, A1,3) · φ(A1,3, A1,4)
. . .
φ(A4,1, A4,2) · φ(A4,2, A4,3) · φ(A4,3, A4,4)
ψ1,1(A1,1) · ·ψ4,4(A4,4).
In this new form, we ﬁx the value of all the pairwise potentials, and so we have to deﬁne an
update rule only for the new singleton potentials. For example, consider the ﬁxed-point equation
for ψ1,1(A1,1). Applying theorem 11.12 we get that
ln ψ1,1(a1,1) ∝
+IEQ′[ln φ(A1,1, A2,1) | a1,1] + IEQ′[ln φ(A1,2, A2,2) | a1,1]
+IEQ′[ln φ(A1,3, A2,3) | a1,1] + IEQ′[ln φ(A1,4, A2,4) | a1,1]
−IEQ′[ln ψ1,2(A1,2) | a1,1] −IEQ′[ln ψ1,3(A1,3) | a1,1] −IEQ′[ln ψ1,4(A1,4) | a1,1]

11.5. Structured Variational Approximations
467
where we have exploited the fact that the terms involving factors such as φ(A1,1, A1,2) appear
in both PΦ and Q, and so cancel out of the equation.
Note that to compute terms such as
IEQ′[ln φ(A1,2, A2,2) | a1,1] we need to evaluate Q′(A1,2, A2,2 | a1,1) = Q′(A1,2 | a1,1) ·
Q′(A2,2) (where we used the independencies in Q′ to simplify the joint marginal). Note that
Q′(A2,2) does not change when we update factors in the ﬁrst row, such as ψ1,1(A1,1). Thus, we
can cache the computation of this marginal when updating the factors ψ1,1(A1,1), . . . , ψ1,4(A1,4).
When performing inference in a large model this can result in dramatic eﬀect.
This example is a special case of an approximation approach called cluster mean ﬁeld. In this
cluster mean ﬁeld
case, our initial approximation has the form
Q(X) =
1
ZQ
Y
j
ψj(Cj),
where the scopes C1, . . . , CK are partition of X. That is, each pair of factors have disjoint
scopes, and each variable in X appears in one factor. This approximation resembles the mean
ﬁeld approximation, except that it is clusters, rather than individual variables, that are marginally
independent. We can now apply theorem 11.13 to reﬁne the approximation. Because the factors
are all disjoint, there are no chains of inﬂuence, and so the interfaces take a particularly simple
form:
Proposition 11.7
Let Q(X) =
1
ZQ
Q
j ψj(Cj) be a cluster mean ﬁeld approximation to a set of factors PΦ, and let
ψj be a factor of Q. Then, the set Dj of theorem 11.13 can be written as
Dj = {Cj ∩Scope[φ] : φ ∈Φ −Φj} −{∅}.
The proof follows directly from the independence properties in Q, and is left as an exercise
(exercise 11.31).
In words, this result states that the interfaces of a cluster are simply the places where the
cluster scope intersects potentials in Φ that are not fully contained in the cluster.
In our
grid example, when we choose the clusters to be the individual columns, the interfaces are
the intersections with the row potentials, which are precisely the singleton variables that we
discussed in example 11.18.
We conclude this discussion with a slightly more elaborate example, demonstrating again the
strength of this result:
Example 11.19
Consider again our 4 × 4 grid, and the “comb” approximation whose structure is shown in ﬁg-
ure 11.19a. In this structure, we have a fully connected clique over each of the columns, and a
“backbone” connecting the columns to each other. Consider again the factorization of the potential
over C1 = {A1,1, . . . , A4,1}. As in the previous example, the ﬁrst term in the new factorization
contains the pairwise factors φ(A1,1, A2,1), φ(A2,1, A3,1), and φ(A3,1, A4,1). The second set of
terms contains the interfaces with other factors in PΦ and Q. Due to the structure of the approxi-
mation, the Q interfaces introduce only singleton potentials. The factors in PΦ, however, are more
interesting. Consider, for example, the factor φ(A4,1, A4,2). The interface of C1 with {A4,1, A4,2}
is A1,1, A4,1 — the variable A4,1 separates C1 from itself, and the variable A1,1 from A4,2. Now,
consider a factor φ(A2,3, A3,3); in this case, the interface is simply A1,1, which separates the ﬁrst
column from both of these variables. Continuing this argument, it follows that all other factors in

468
Chapter 11. Inference as Optimization
(b)
(a)
A1,1
A1,4
A1,3
A1,2
A2,1
A2,4
A2,3
A2,2
A3,1
A3,4
A3,3
A3,2
A4,1
A4,4
A4,3
A4,2
A1,1
A1,4
A1,3
A1,2
A2,1
A2,4
A2,3
A2,2
A3,1
A3,4
A3,3
A3,2
A4,1
A4,4
A4,3
A4,2
Figure 11.19
Simpliﬁcation of approximating structure in cluster mean ﬁeld.
(a) Example of an
approximating structure we can use in a variational approximation of a 4×4 grid network. (b) Simpliﬁcation
of that network using theorem 11.13.
PΦ induce an interface containing a variable at the head of the column and (possibly) another
variable in the column. Thus, we can eliminate any (new) pairwise interaction terms between any
other pair of variables. For a general n × n grid, this reduces the overall number of (new) pairwise
potentials from n ·
 n
2

to n × (n −1).
11.5.2.5
Selecting the Approximation
In general, both the quality and the computational complexity of the variational approximation
depend on the structure of PΦ and the structure of the approximating family Q. There are
several guiding intuitions.
First, we want to be able to perform eﬃcient inference in the
approximating network. In example 11.18, the approximating structure was a chain of variables,
where we can perform inference in linear time (as a function of the number of variables in the
chain). In general, we often select our network so that the resulting factorization leads to a
tractable network (that is, one of low tree-width).
It is important to note, however, that the structure of the original distribution is not the only
aspect in determining the complexity of inference in Q. We also need to take into account
factors that correspond to the interfaces of the cluster. In our grid example, these interfaces
involved a single variable at time, and so they did not add to the network complexity. However,
in more complex networks, these factors can have a signiﬁcant eﬀect.
Another consideration besides computational complexity is the quality of our approximation.
Intuitively, we should design Q so as to preserve the strong dependencies in PΦ. By preserving
such dependencies we maintain the main eﬀects in the distribution we want to apply.
These intuitions provide some guidelines in choosing the approximating distribution. How-
ever, these choices are far from an exact science at this stage. The theory we described here
allows to automate two parts of the process: deﬁning the form of the approximation given some
initial rough set of (disjoint or overlapping) clusters; and deﬁning the ﬁxed-point iterations to

11.5. Structured Variational Approximations
469
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
-ln(x)
l=8
l=5
l=3
l=1.2
Figure 11.20
Illustration of the variational bound −ln(x) ≥−λx + ln(λ) + 1. For any x, the bound
holds for all λ, and is tight for some value of λ.
optimize such an approximation. The current tools do not provide for an automated way for
determining what are reasonable sets of clusters to achieve a desired degree of approximation.
11.5.3
Local Variational Methods ⋆
The general method that we used throughout this chapter is an instance of a general class of
methods known as variational methods. In this class of methods, we take a complex objective
variational
method
function fobj(x), and lower or upper bound it using a parameterized family of functions
gg(x, λ). Focusing, for concreteness, on the case of a lower bound, this family has the property
lower bound
that fobj(x) ≥gg(x, λ) for any value of λ, and that, for any x, the bound is tight for some
value of λ (a diﬀerent one for every x).
As an example, we can show the variational lower bound:
variational lower
bound
Lemma 11.2
For any choice of λ and x
−ln(x) ≥−λx + ln(λ) + 1,
and, for any x, this bound is tight for some value of λ.
Proof Consider the tangent of ln(x) at the point x0
fobj(x : x0) = ln(x0) + (x −x0) 1
x0
= x
x0
+ ln(x0) −1.
Since ln(x) is a concave function, it is upper bounded by each of its tangents.
And so,
−ln(x) ≥−fobj(x : x0) for any choice of x and x0. Setting x0 = λ−1 leads to the desired
result.

470
Chapter 11. Inference as Optimization
This result is illustrated in ﬁgure 11.20. It is a special case of a general result in the ﬁeld of
convex duality, which guarantees the existence of such bounds for a broad class of functions.
convex duality
This lower bound allows to approximate a nonlinear function −ln(x) with a term that is
linear in x. This simpliﬁcation comes at the price of introducing a new variational parameter λ,
variational
parameter
whose value is undetermined. If we optimize λ exactly for each value of x, we obtain a tight
lower bound, but a bound is obtained for any value of λ.
The techniques we have used in this chapter so far also fall into this category. Equation (11.5)
shows that the energy functional is a lower bound on the log-partition function for any distribu-
tion Q. Thus, we can take fobj to be the partition function, x to correspond to the parameters
of the true distribution PΦ, and λ to correspond to the parameters of the approximating dis-
tribution Q. Although the lower bound is tight when Q precisely represents PΦ, for reasons of
eﬃciency, we generally optimize Q in a restricted space that provides a bound, but not a tight
one, on the log-partition function.
This general approach of introducing auxiliary variational parameters that help in simplifying
a complex objective function appears in many other domains. While it is beyond our scope to
introduce a general theory of variational methods, we now brieﬂy describe one other application
of variational methods that is relevant to probabilistic inference and does not fall directly within
the scope of optimizing the energy functional. This application arises in the context of exact
inference using an algorithm such as variable elimination. Here, we use variational bounds to
avoid creating large factors that can lead to exponential complexity in the algorithm, giving
rise to an approximate variational variable elimination algorithm. Such simpliﬁcations can be
variational
variable
elimination
achieved in several ways; we describe two.
11.5.3.1
Variational Bounds
Consider, for example, the diamond network of ﬁgure 11.18a.
Assume that we run variable
elimination to sum out the variable B, which we assume for convenience is binary-valued. The
elimination of B introduces a new factor:
φB(A, C) =
X
b
φ1(A, b)φ2(b, C)
Coupling A and C in a single factor may be expensive, for example, if A and C have many
values. In more complex networks, this type of coupling can induce complexity if an elimination
step couples a larger set of variables, or if the local coupling leads to additional cost later in the
computation, when we eliminate A or C.
As we now show, we can use a variational bound to avoid this coupling.
Consider the
following bound:
Proposition 11.8
If 0 ≤λ ≤1, then
ln(1 + ex) ≥λx + IH(λ),
where IH(λ) = −λ ln λ −(1 −λ) ln(1 −λ).
This bound implies that
1 + ex ≥eλx+IH(λ).

11.5. Structured Variational Approximations
471
Why is this useful? Using some algebraic manipulation, we can bound each of the entries in
our newly generated factor:
φB(a, c)
=
φ1(b0, a)φ2(b0, c) + φ1(b1, a)φ2(b1, c)
=
φ1(b0, a)φ2(b0, c)

1 + exp

ln φ1(b1, a)φ2(b1, c)
φ1(b0, a)φ2(b0, c)

≥
φ1(b0, a)φ2(b0, c) exp

λa,c ln φ1(b1, a)φ2(b1, c)
φ1(b0, a)φ2(b0, c) + IH(λa,c)

=
 φ1(b0, a)1−λa,cφ1(b1, a)λa,c
·
 φ2(b0, c)1−λa,cφ2(b1, c)λa,c
· eIH(λa,c).
(11.66)
Thus, we can replace a factor that couples A and C by a product of three terms: an expression
involving only factors of A, an expression involving only factors of C, and the ﬁnal factor
eIH(λa,c). However, all three terms also involve the variational parameter λa,c and therefore also
depend on both A and C. At this point, it is unclear what we gain from the transformation.
However, we can choose the same λ for all joint assignments to A, C. In doing so, we replace
four variational parameters by a single parameter λ. This operation relaxes the bound, which is
no longer tight. On the other hand, it also decouples A and C, leading to a product of terms
none of which depends on both variables:
 φ1(b0, a)1−λφ1(b1, a)λ
·
 φ2(b0, c)1−λφ2(b1, c)λ
· eIH(λ) = ˜φ1(a, λ)˜φ2(c, λ)eIH(λ). (11.67)
Thus, if we use this approximation, we have eﬀectively eliminated B without coupling A and
C. As we saw in chapter 9, this type of simpliﬁcation can circumvent the need for coupling yet
more variables in later stages in variable elimination, potentially leading to signiﬁcant savings.
It is interesting to observe how λ decouples the two factors. Each factor is replaced by a
geometric average over the values of B. The variational parameter speciﬁes the weight we assign
to each of the two cases. Note that the original bound in equation (11.66) is tight; thus, if we pick
the “right” variational parameter λa,c for each assignment a, c, we reproduce the correct factor
φB(A, C). However, these variational parameters are generally diﬀerent for each assignment
a, c, and hence, a single variational parameter cannot optimize all of the terms. Our choice of
λ eﬀectively determines the quality of our approximation for each of the terms φB(a, c). Thus,
the overall quality of our approximation for a particular choice of λ depends on the importance
of these diﬀerent terms in the variable elimination computation as a whole.
Other variational approximations exploit speciﬁc parametric forms of CPDs in the network.
For example, consider networks with sigmoid CPDs (see section 5.4.2). Recall that a logistic CPD
P(X | U) has the parametric form:
P(x1 | u) = sigmoid(
X
i
wiui + w0),
where sigmoid(x) =
1
1+e−x . The observation of X couples the parents U. Can we decouple
these parents using an approximation? Using proposition 11.8 we can ﬁnd an upper bound:
ln sigmoid(
X
i
wiui + w0) ≤λ
 X
i
wiui + w0
!
−IH(λ).
(11.68)

472
Chapter 11. Inference as Optimization
Similarly to our earlier example, such an approximation allows us to replace a factor over several
variables by a product of smaller factors. In this case, all the parents of X are decoupled by the
approximate form.
11.5.3.2
Variational Variable Elimination
How do we use this approximation in the course of inference? Note that equation (11.67) provides
a lower bound to φB(a, c) for every value of a, c. Assume that, in the course of running variable
elimination, rather than generating φB(A, C), we introduce the expression in equation (11.67)
and continue the variable elimination process with these decoupled factors. From a graph-
theoretic perspective, the result of this approximation when applied to a variable B is analogous
to the eﬀect of conditioning on B, as described in section 9.5.3: it removes from the graph
B and all its adjacent edges. However, unlike conditioning, we do not enumerate and perform
inference for all values of B (of course, at the cost of obtaining an approximate result).
More generally, in an execution of variable elimination, there may be some set of elimination
steps that create large factors that couple many variables. This variational approximation can
allow us to avoid this coupling. Like conditioning (see section 9.5.4.1), we can perform such
an approximation step not only at the very beginning, but also in a way that is interleaved
with variable elimination, allowing us to reuse computation. This class of algorithms is called
variational variable elimination.
variational
variable
elimination
What is the result of this approximation? Each of the entries in the approximated factor
is replaced with a lower bound; thus, each entry in every subsequent factor produced by the
algorithm is also a lower bound to the original entry. If we proceed to eliminate all variables,
either exactly or using additional variational approximation steps for other intermediate factors,
the outcome of this process is a lower bound to the partition function. If we do not eliminate
all of the variables, the result is an approximate factor in which every entry is a lower bound to
the original. Of course, once we renormalize the factor to produce a distribution, we can make
no guarantees about the direction of the approximation for any given entry. Nevertheless, the
resulting factor might be a reasonable approximation to the original.
The quality of our approximation depends on the choice of variational parameters introduced
during the course of the variable elimination algorithm. How do we select them? One approach
is simply to select the variational parameter at each step to optimize the quality of our ap-
proximation at that step. However, this high-level goal is not fully deﬁned. For example, we
can choose λ so as to make ˜φ1(a1, λ)˜φ2(b1, λ)eIH(λ) as close as possible to φB(a1, c1); or,
we can focus on a1, c0. The decision of where to focus our “approximation eﬀort” depends on
the impact of these components of the factor on the ﬁnal outcome of the computation. Thus,
a more correct approach is to identify the actual expression that we are trying to estimate —
for example, the partition function — and to try to maximize our bound to that expression. In
our simple example, we can write down the partition function as a function of the variational
parameter λ introduced when eliminating B:
˜Z(λ) = eIH(λ) X
a
X
c
φ3(a, d)φ4(c, d)
X
d
˜φ1(a, λ)˜φ2(c, λ).
This expression is a function of λ; we can then try to identify the best bound by maximizing
maxλ ˜Z(λ), say, using gradient ascent or another numerical optimization method.

11.6. Summary and Discussion
473
However, as we discussed, in most cases we would use several approximate elimination steps
within the variable elimination algorithm. In our example, the elimination of D also couples A
and C, a situation we may wish to avoid. Thus, we could apply the same type of variational
bound to the internal summation:
φD(A, C) =
X
d
˜φ1(a, λ)˜φ2(c, λ),
giving rise to the bound ˜φ3(a, λ′)˜φ4(c, λ′)eIH(λ′). The resulting approximate partition function
now has the form
˜Z(λ, λ′) = eIH(λ)eIH(λ′) X
a
˜φ1(a, λ)˜φ3(a, λ′)
X
c
˜φ2(c, λ)˜φ4(c, λ′).
We can now maximize maxλ,λ′ ˜Z(λ, λ′); the higher the value we ﬁnd, the better our approxi-
mation to the true partition function.
In general, our approximate partition function will be a function ˜Z(λ), where λ is the
vector of all variational parameters λ produced during the diﬀerent approximation steps in the
algorithm. One approach is to reformulate the variable elimination algorithm so that it produces
factors that are not purely numerical, but rather symbolic expressions in the variables λ; see
exercise 11.32. Given the multivariate function ˜Z(λ), we can optimize it numerically to produce
the best possible bound. A similar principle applies when we use the variational bound for
sigmoid CPDs; see exercise 11.36 for an example. While we only sketch the basic idea here, this
approach can be used as the basis for an algorithm that interleaves variational approximation
steps with exact elimination steps to form a variational variable elimination algorithm.
11.6
Summary and Discussion
In this chapter, we have described a general class of methods for performing approximate
inference in a distribution PΦ deﬁned by a graphical model. These methods all attempt to
construct a representation Q within some approximation class Q that best approximates PΦ.
The key issue that must be tackled in this task is the construction of such an approximation
without performing inference on the (intractable) PΦ.
The methods that we described all
take a similar approach of using the I-projection framework, whereby we minimize the KL-
divergence ID(Q||PΦ); these methods all reformulate this problem as one of maximizing the
energy functional.
The diﬀerent methods that we described all follow a similar template.
We optimize the
free energy, or an approximation thereof, over a class of representations Q. We provided an
optimization-based view for four diﬀerent methods, each of which makes a particular choice
regarding the objective and the constraints. We now recap these choices and their repercussions.
Clique tree calibration optimizes the factored energy functional, which is exact for clique trees.
The optimization is performed over the space of calibrated clique potentials. For clique trees,
any set of calibrated clique potentials must arise from a real distribution, and so this space is
precisely the marginal polytope. As a consequence, both our objective and our constraint space
are exact, so our solution represents the exact posterior.
Cluster-graph (loopy) belief propagation optimizes the factored energy functional, which is
approximate for loopy graphs. The optimization is performed over the space of locally consistent

474
Chapter 11. Inference as Optimization
pseudo-marginals, which is a relaxation of the marginal polytope. Thus, both the objective and
the constraint space are approximate.
Expectation propagation over clique trees optimizes the factored energy functional, which is
the exact objective in this case. However, the constraints deﬁne a space of pseudo-marginals
that are not even entirely consistent — only their moments are required to match. Thus, the
constraint space here is a relaxation of our constraints, even when the structure is a tree.
Expectation propagation over cluster graphs adds, on top of these, all of the approximations
induced by belief propagation.
Finally, structured variational methods optimize the exact factored energy functional, for a
class of distributions that is (generally) less expressive than necessary to encode PΦ.
As a
consequence, the constraint space is actually a tightening of our original constraint space.
Because the objective function is exact, the result of this optimization provides a lower bound
on the value of the exact optimization problem.
As we will see, such bounds can play an
important role in the context of learning.
In the approaches we discussed, the optimization method was based on the method of
Lagrange multipliers.
This derivation gave rise to a series of ﬁxed-point equations for the
solution, where one variable is deﬁned in terms of the others.
As we showed, an iterative
solution for these ﬁxed-point equations gives rise to a suite of message passing algorithms that
generalize the clique-tree message passing algorithms of chapter 10.
This general framework opens the door to the development of many other approaches. Each

of the methods that we described here involves a design choice in three dimensions:
the objective function that we aim to optimize, the space of (pseudo-)distributions over
which we perform our optimization, and the algorithm that we choose to use in order
to perform the optimization.
Although these three decisions aﬀect each other, these
dimensions are suﬃciently independent that we can improve each one separately. Some
recent work takes exactly this approach. For example, we have already seen some work that
focuses on better approximations to the energy functional; other work (see section 11.7) focuses
on identifying constraints over the space of pseudo-marginals that make it a tighter relaxation to
the (exact) marginal polytope; yet other work aims to ﬁnd better (for example, more convergent)
algorithms for a general class of optimization problems.
Many of these improvements aimed to address a fundamental problem arising in these al-
gorithms: the possible lack of convergence to a stable solution. The issue of convergence is
one on which some progress has been made. Some recently developed methods have better
convergence properties in practice, and others are even guaranteed to converge. There are also
theoretical analyses that can help determine when the algorithms converge.
A second key question, and one on which relatively little progress has been made, is the quality
of the approximation. There is very little work that provides guarantees on the error made in the
answers (for example, individual marginals) by any of these methods. As a consequence, there
is almost no work that provides help in choosing a low-error approximation for a particular
problem. Thus, the problem of applying these methods in practice is still a combination of
manual tuning on one hand, and luck on the other. The development of automated methods
that can help guide the selection of an approximating class for a particular problem is an
important direction for future work.

11.7. Relevant Literature
475
11.7
Relevant Literature
Variational approximation methods are applicable in a wide variety of settings, including quan-
tum mechanics (Sakurai 1985), statistical mechanics (Parisi 1988), neural networks (Elfadel 1995),
and statistics (Rustagi 1976).
The literature on the use of these approximation methods for
inference problems in graphical models is heavily inﬂuenced by ideas in statistical mechanics,
although the connections are beyond the scope of this book.
The rules for belief propagation were developed in Pearl’s (1986b; 1988) analysis of inference
in singly connected networks. In his book, Pearl noted that these propagation rules lead to
wrong answers in multiconnected networks. Interest in “loopy” inference on pairwise networks
was raised in several publications (Frey 1998; MacKay and Neal 1996; Weiss 1996) that reported
good empirical results when running Pearl’s algorithm on networks with loops.
The most
impressive success was in the error-correcting code scheme known as turbocodes (Berrou et al.
1993; McEliece et al. 1995). These decoding algorithms were shown to be equivalent to belief
propagation in a network with loops (Kschischang and Frey 1998; McEliece et al. 1998; Frey and
MacKay 1997). Researchers using these ad hoc algorithms reported that although they did not
compute the correct posteriors, they often did compute the correct MAP assignment.
These empirical successes led to examination of both the reasons for the success of such
methods and evaluated their performance in other domains. Weiss (2000) showed that when a
network has a single loop, an analytic relationship exists between the belief computed and the
true marginals, and it used that relationship to characterize network topologies for which the
MAP solution is provably correct. Weiss and Freeman (2001a) then showed that if loopy belief
propagation converges in a linear Gaussian network, then it computes correct posterior means;
see section 14.7 for more references on results for the Gaussian case. These initial analytic results
were supplemented by promising empirical results (Murphy et al. 1999; Horn and McEliece 1997;
Weiss 2001).
Several alternative formulations of loopy belief propagation appeared in the literature. These
include factor graphs (Kschischang et al. 2001b), tree-based reparameterization (Wainwright et al.
2003a), and algebraic formulations (Aji and McEliece 2000). In parallel, several authors (Yedidia
et al. 2000; Dechter et al. 2002) proposed extending the idea of belief propagation to more
generalized cluster graphs (or region graphs). Welling (2004) examined methods for automatically
choosing the regions for this approximation.
A major advance in our understanding of this general class of approximation algorithms came
with the analysis of Yedidia et al. (2000, 2005), showing that these methods can be posed as
attempting to maximize an approximate energy functional.
This result connected the algo-
rithmic developments in the ﬁeld with literature on free-energy approximations developed in
statistical mechanics (Bethe 1935; Kikuchi 1951). This insight is the basis for our discussion of
the Bethe energy functional in section 11.3.6. This result also provided a connection between
belief propagation algorithms and other approximation procedures, such as structured varia-
tional methods. In addition, it led to the development of new types of approximate inference
procedures. These include direct optimization of the Bethe energy functional (Welling and Teh
2001) using gradient-based methods, as well as provably convergent “double loop” variants of
belief propagation (Yuille 2002; Heskes et al. 2003). Another class of algorithms combined exact
inference on subtrees of the cluster graph within cluster-graph belief propagation (Wainwright
et al. 2001, 2002a). This combination leads to faster convergence and introduces new directions

476
Chapter 11. Inference as Optimization
for characterizing the errors of the approximation. Wainwright and Jordan (2003) review the
connections between belief propagation and optimization of the energy functional.
An active area of research is improving and analyzing the convergence properties of gener-
alized belief propagation algorithms. In practice, the techniques most important for improving
convergence are damping (Murphy et al. 1999; Heskes 2002) and message scheduling algorithms,
such as the tree-based reparameterization algorithm of (Wainwright et al. 2003a) or the residual
belief propagation algorithm of Elidan et al. (2006). On the theoretical side, key directions include
the identiﬁcation of a set of suﬃcient conditions for the existence of unique local maxima of the
Bethe energy functional (Pakzad and Anantharam 2002; Heskes 2002), as well as conditions that
ensure a unique convergence point of belief propagation (Tatikonda and Jordan 2002; Ihler et al.
2003; Heskes 2004; Ihler et al. 2005; Mooij and Kappen 2007). Our discussion in section 11.3.4 is
based on that of Mooij and Kappen (2007). A related trajectory attempts to estimate the error of
the approximate marginal probabilities; see, for example, Leisink and Kappen (2003); Ihler (2007).
A diﬀerent generalizatiohn of belief-propagation algorithms develops variants of the energy
functional that have desired properties. For example, if the energy functional is convex, then
it is guaranteed to have a single maximum. An initial convexiﬁed free-energy functional was
introduced in Wainwright et al. (2002b). This functional has the additional property of providing
an upper bound on the partition function.
Recently, there has been signiﬁcant work that
provides a more detailed characterization of convex energy functionals (Wainwright and Jordan
2003; Heskes 2006; Wainwright and Jordan 2004; Sontag and Jaakkola 2007).
Although the
convexity of energy functional implies a unique maximum, it does not generally guarantee that
a message passing algorithm will converge. Recent alternative algorithms provide guaranteed
convergence for such energy functionals (Heskes 2006; Globerson and Jaakkola 2007a; Hazan
and Shashua 2008).
A recent extension is the combination of belief propagation with particle-based methods. The
basic idea is to use particle sampling to perform the basic belief propagation steps (Sudderth
et al. 2003; Isard 2003). This combination allows us to use cluster-graph belief propagation on
networks with continuous non-Gaussian variables, which appear in applications in vision and
signal processing; see also section 14.7.
The idea of factored messages ﬁrst appeared in several contexts (Dechter and Rish 1997;
Dechter 1997; Boyen and Koller 1998b; Murphy and Weiss 2001).
Similar ideas that involve
projection of messages onto “simple” representations (such as Gaussians) are common in the
control and tracking literature (Bar-Shalom 1992). The generalization of these ideas in the form of
expectation propagation was introduced by Minka (2001b). The connection between expectation
propagation and the Bethe energy functional was explored by (Minka 2001b; Heskes and Zoeter
2002; Heskes et al. 2005). The connection between expectation propagation and generalized
belief propagation in the case of discrete variables is explored by Welling et al. (2005).
One of the early applications of variational methods to probabilistic methods was the use of
mean ﬁeld approximation for Boltzmann machines (Peterson and Anderson 1987), an approach
then widely adopted in computer vision (see, for example, Li (2001)). This methodology was
introduced into the ﬁeld of directed graphical models by Saul et al. (1996), following ideas that
appeared in the context of Helmholtz machines (Hinton et al. 1995).
The mean ﬁeld approximation cannot capture strong dependencies in the posterior distribu-
tion. Saul and Jordan (1996) suggested to circumvent this problem by using structured varia-
tional methods. These ideas were extended for diﬀerent forms of approximating distributions

11.8. Exercises
477
and target networks. Ghahramani and Jordan (1997) used independent hidden Markov chains
to approximate factorial HMMs, a speciﬁc form of a dynamic Bayesian network. Barber and
Wiegerinck (1998) use a Boltzmann machine approximation. Wiegerinck (2000) uses Markov net-
works and cluster trees as approximate distribution. Xing et al. (2003) describe cluster mean ﬁeld
and suggest eﬃcient implementations. Geiger et al. (2006) further extend Wiegerinck’s proce-
dure and introduce eﬃcient propagation schemes to maximize parameters in the approximating
distribution.
The idea of using a mixture of mean ﬁeld approximation was developed by Jaakkola and
Jordan (1998). These ideas were extended to general networks with auxiliary variables by El-Hay
and Friedman (2001).
Jaakkola and Jordan (1996b,a) introduced local variational approximations to compute both
upper bounds and lower bounds of the log-likelihood function (that is, of log Z). They demon-
strated these methods by a large scale study of inference in the QMR-DT network (Jaakkola and
Jordan 1999), where they show that variational methods are more eﬀective than particle based
methods. Additional extension on these methods appeared in Ng and Jordan (2000).
Tutorials discussing the use of structured and local methods appear in Jordan et al. (1998);
Jaakkola (2001).
11.8
Exercises
Exercise 11.1
Show that the derivative of the entropy is:
entropy
∂
∂Q(x)IHQ(X) = −ln Q(x) −1.
Exercise 11.2⋆
Consider the set of locally consistent distributions, as in the local consistency polytope of equation (11.16).
local consistency
polytope
a. For a cluster graph U that is a clique tree T (satisfying the running intersection property), show that
the set of distributions satisfying these constraints is precisely the marginal polytope — the set of legal
marginal
polytope
distributions Q that can be represented over T .
b. Show that, for a cluster graph that is not a tree, there are parameterizations that satisfy these constraints
but are not marginals of any legal probability distribution.
Exercise 11.3⋆
In the text, we showed that CTree-Optimize has a unique global optimum. Show that it also has a unique
ﬁxed point.
Exercise 11.4⋆
Consider the network of ﬁgure 11.1c. We have shown that
PT (A, B, C, D) ∝PΦ(A, B, C, D)µ3,4[D]µ1,4[A]
β4(A, D)
,
where T is the cluster tree we get if we remove C4 = {A, D}. Show how to use this result on the
residual to bound the error in the estimation of marginals in this cluster graph.
cluster graph
residual
Exercise 11.5⋆
Prove proposition 11.2.

478
Chapter 11. Inference as Optimization
Exercise 11.6
Compare the cluster graphs in ﬁgure 11.3 and ﬁgure 11.7.
a. Show that U2 and U3 are equivalent in the following sense: Any set of calibrated potentials of one can
be transformed to a calibrated set of potentials of the other.
b. Show similar equivalence between U1 and U3.
Exercise 11.7
As we discussed in box 4.B, Markov networks are commonly used for image-processing tasks. We now
consider the application of Markov networks to foreground-background image segmentation. Here, we do
image
segmentation
not have a predetermined set of classes, each with its own model. Rather, for each pixel, we have a
binary-valued variable Xi, where x1
i means that Xi is a foreground pixel, and x0
i a background pixel.
In this case, we have no node potentials (say that features of an individual pixel cannot help distinguish
foreground from background). The network structure is a grid, where for each pair of adjacent pixels i, j,
we have an edge potential φi,j(Xi, Xj) = αi,j if Xi = Xj and 1 otherwise. A large value of αi,j makes
it more likely that Xi = Xj (that is, pixels i and j are assigned to the same segment), and a small value
makes it less likely.
Using the standard Bethe cluster graph, compute the ﬁrst message sent by loopy belief propagation from
any variable to any one of its neighbors. What is wrong with this approach?
Exercise 11.8
Let X be a node with parents U = {U1, . . . , Uk}, where P(X | U) is a tree-CPD. Assume that we
have a cluster consisting of X and U. In this question, you will show how to exploit the structure of the
tree-CPD to perform message passing more eﬃciently.
a. Consider a step where our cluster gets incoming messages about U1, . . . , Uk, and sends a message
about X. Show how this step can be executed in time linear in the size of the tree-CPD of P(X | U).
b. Now, consider the step where our clique gets incoming messages about U1, . . . , Uk−1 and X, and
send a message about Uk. Show how this step can also be executed in time linear in the size of the
tree-CPD.
Exercise 11.9
Recall that a pairwise Markov network consists of univariate potential φi[Xi] over each variable Xi,
and in addition a pairwise potential φ(i,j)[Xi, Xj] over some pairs of variables. In section 11.3.5.1 we
showed a simple transformation of such a network to a cluster graph where we introduce a cluster for each
potential. Write the update equations for cluster-graph belief propagation for such a network. Show that we
can formulate inference as the direct propagation of messages between variables by implicitly combining
messages through pairwise potentials.
Exercise 11.10⋆
Suppose we are given a set of factors Φ = {φ1, . . . , φK} over X = {X1, . . . , Xn}. Our aim is to convert
these factors into a pairwise Markov network by introducing new auxiliary variables Y = {Y1, . . . , Yk}
so that Yj denotes a joint assignment to Scope[φj]. Show how to construct a set of factors Φ′ that is a
pairwise Markov network over X ∪Y such that PΦ′(x) = PΦ(x) for each assignment to X.
Exercise 11.11⋆
In this question, we study how we can deﬁne a cluster graph for a Bayesian network.
BN cluster graph
a. Consider the following two schemes for converting a Bayesian network structure G to a cluster graph
U. For each of these two schemes, either show (by proving the necessary properties) that it produces a
valid cluster graph for a general Bayesian network, or disprove this result by showing a counterexample.
•
Scheme 1: For each node Xi in G, deﬁne a cluster Ci over Xi’s family. Connect Ci and Cj if
Xj is a parent of Xi in G, and deﬁne the sepset to be the intersection of the clusters.

11.8. Exercises
479
•
Scheme 2: For each node Xi in G, deﬁne a cluster Ci over Xi’s family. Connect Ci and Cj if
Xj is a parent of Xi in G, and deﬁne the sepset to be the {Xj}.
b. Construct an alternative scheme to the ones proposed earlier that uses a minimum spanning tree
algorithm to transform any Bayesian network into a valid cluster graph.
Exercise 11.12⋆
Suppose we want to use a gradient method to directly maximize ˜F[ ˜PΦ, Q] with respect to entries in Q.
For simplicity, assume that we are dealing with a pairwise network for both PΦ and Q, and so the entries
in Q are all univariate and pairwise potentials.
a. Derive ∂˜
F [ ˜
PΦ,Q]
∂Q(xi)
and ∂˜
F [ ˜
PΦ,Q]
∂Q(xi,yi) for the two types of potentials we have.
b. Recall that we cannot simply choose Q to maximize ˜F[ ˜PΦ, Q]. We need to ensure that every potential
in Q is nonnegative and sums to 1. In addition, we need to maintain the marginal consistency between
each pairwise potential and the associated univariate potential marginals in Q. A standard solution is
to write Q as a function of meta parameters η, so that the transformation from η to Q ensures the
consistency. Suggest such a reparameterization, and derive the gradient of ˜F[ ˜PΦ, Q] with respect to
this reparameterization. (Hint: use the chain law of partial derivatives.)
Exercise 11.13⋆
Prove that if the damped updates of equation (11.14) converge, then they converge to a stationary point of
˜F[ ˜PΦ, Q].
Exercise 11.14⋆
In this exercise you will prove theorem 11.7.
a. Start by deﬁning the Lagrangian
J
=
˜F[ ˜PΦ, Q]
−
X
i∈VR
λr
 X
cr
βr(cr) −1
!
−
X
(r′→r)∈ER
X
cr
λr′→r[cr]

X
cr′ ∼cr
βr′(cr′) −βr(cr)

,
and show that any maximum point satisﬁes
∂
∂βr(cr)J = κr ln ψr(cr) −κr ln βr(cr) −κr −λr −
X
r′∈Up(r)
λr′→r[cr] +
X
r′∈Down(r)
λr→r′[cr′],
where κr is the counting number of the region, as deﬁned in equation (11.20).
b. Deﬁne δru→rd in terms of the Lagrange multipliers and show that your solution satisﬁes equation (11.37)
and equation (11.36).
Exercise 11.15
Consider an n×n grid network. Construct a region-graph approximation for this network that would have
a region r for each small square Ai,j, Ai,j+1, Ai+1,j, Ai+1,j+1. Describe the structure of the graph and
the computation of the messages and beliefs.
Exercise 11.16
Prove theorem 11.6.

480
Chapter 11. Inference as Optimization
Exercise 11.17⋆
In this exercise, we will derive a message passing algorithm, called the child-parent algorithm, for Bethe
region graphs. Although the messages in this algorithm are somewhat convoluted, they have the advantage
of corresponding directly to the Lagrange multipliers for the region graph energy functional. Moreover,
although somewhat opaque, the message passing algorithm is a very slight variation on the scheme used
in standard belief propagation: the standard messages are simply raised to a power.
Consider the Bethe cluster graph of example 11.2, where we assume that all counting numbers are nonzero.
Let ρi = 1/κi and ρr = 1/κr. Starting from equation (11.34), derive the correctness of the following
update rules for the messages and the beliefs. For all r ∈R+:
βr(Cr) ←

˜ψr(Cr)
Y
Xi∈Cr
δi→r(Xi)


ρr
.
(11.69)
δi→r(Xi) ←


 Y
r′¬r
δi→r′(Xi)
!ρi 
X
Cr−Xi
ψr(Cr)


Y
Xj∈Cr,j̸=i
δj→r


ρr



−
1
ρi+ρr
.
(11.70)
Exercise 11.18⋆
Show that the counting numbers deﬁned by equation (11.26) are convex. (Hint: Show ﬁrst the convexity of
counting numbers obtained from this analysis for a tree-structured MRF.)
Exercise 11.19⋆
In this exercise, we will derive another message passing algorithm, called the two-way algorithm, for ﬁnding
ﬁxed points of region-based energy functionals; this algorithm allows for more general region graphs than
in exercise 11.17. It uses two messages along each link r →r′: one from r to r′ and one from r′ to r.
Consider a region-based free energy as in equation (11.27). For any region r, let pr = |Up(r)| be the
number of regions that are directly upward of r. Assume that for any top region (so that pr = 0), we have
that κr = 1. We now deﬁne qr = (1 −κr)/pregion, taking qr = 1 when pr = 0 (so that κr = 1, as per
our assumption). Assume furthermore that qr ̸= 2, and deﬁne βr = 1/(2 −qr).
The following equalities deﬁne the messages and the potentials in this algorithm:
˜ψr(Cr)
=
(ψr(Cr))κr)
(11.71)
˜δup
r→r′
=
˜ψr(Cr)
Y
r′′∈Up(r)−{r′}
˜δdown
r′′→r(Cr)
Y
r′′∈Down(r)
˜δup
r′′→r(Cr′′)
(11.72)
(ψr(Cr))κr)
˜δdown
r′→r
=
X
Cr′ −Cr
˜ψr(Cr)
Y
r′′∈Up(r)
˜δdown
r′′→r(Cr)
Y
r′′∈Down(r)−{r′}
˜δup
r′′→r(Cr′′)
(11.73)
δup
r→r′
=
(˜δup
r→r′(Cr))βr(˜δdown
r′→r(Cr))βr−1
(11.74)
δdown
r→r′
=
(˜δup
r→r′(Cr))βr−1(˜δdown
r′→r(Cr))βr
(11.75)
βr(Cr)
=
˜ψr(Cr)
Y
r′∈Up(r)
δdown
r′→r(Cr)
Y
r′′∈Down(r)
δup
r′′→r(Cr′′).
(11.76)
Note that the messages ˜δup
r→r′ (or ˜δdown
r→r′) are as we would expect: the message sent from r to r′ is simply
the product of r’s initial potential with all of its incoming messages except the one from r′ (and similarly for
the message sent from r′ to r). However, as we discussed in our derivation of the region graph algorithm,

11.8. Exercises
481
this computation will double-count the information that arrived at r from r′ via an indirect path. The ﬁnal
computation of the messages δup
r→r′ and δdown
r→r′ is intended to correct for that double-counting.
In this exercise, you will show that the ﬁxed points of equation (11.33) are precisely the same as the ﬁxed
points of the update equations equation (11.71)–equation (11.76).
a. We begin by deﬁning the messages in terms of the beliefs and the Lagrange multipliers:
˜δup
r→r′(cr)
=
exp{λr,r′(cr)}
(11.77)
˜δdown
r→r′(cr′)
=
βr′(cr′)qr′ exp{λr,r′(cr′)}.
(11.78)
Show that these messages satisfy the ﬁxed-point equations in equation (11.33).
b. Show that
(βr(cr))κr ∝(βr(cr))κr−1 ˜ψr(cr)
Y
r′∈Up(r)
δdown
r′→r(Cr)
Y
r′′∈Down(r)
δup
r′′→r(Cr′′).
(11.79)
Conclude that equation (11.76) holds.
c. Show that
˜δup
r→r′δdown
r′→r = βr = δup
r→r′ ˜δdown
r′→r,
(11.80)
and that
δup
r→r′δdown
r′→r = (βr)qr.
(11.81)
Show that the only solution to these equations is given by equation (11.73) and equation (11.74).
d. Show by direct derivation that theorem 11.6 holds for the potentials deﬁned by equation (11.71)–11.76.
(Hint: consider separately those regions that have no parents, and recall our previous assumptions.)
Exercise 11.20
Consider the marginal probability over C2 = {A1,2, A2,2, A3,2, A4,2} in the 4 × 4 grid network. Show
that, if we assume general potentials, this marginal probability does not satisfy any conditional indepen-
dencies.
Exercise 11.21⋆
Consider a clique tree T that is constructed from a run of variable elimination over a Gibbs distribution
P ′
Φ, as described in section 10.1.2. Let Si,j be a sepset in the clique tree. Show that, for general potentials,
P ′
Φ does not satisfy any conditional independencies relative to the marginal distribution over Si,j. (Hint:
consider the actual run of variable elimination that led to the construction of this sepset, and the eﬀect
on the graph of eliminating these variables.)
Exercise 11.22⋆
In section 10.3.3.2 we described an algorithm that uses a calibrated clique tree to compute a joint marginal
over a set of variables that is not a subset of any clique in the tree. We can consider using the same
procedure in a calibrated cluster graph U. For example, consider a pair of variables X, Y that are not
found together in any cluster in U.
a. Deﬁne an algorithm that, given clusters C1 ∋X and C2 ∋Y and a path between them in the cluster
graph computes a joint distribution P(X, Y ).
b. A cluster graph can contain more than one path between C1 and C2. Provide an example showing
that the answer returned by this algorithm can depend on the choice of path used.

482
Chapter 11. Inference as Optimization
Exercise 11.23⋆
Suppose we have a cluster T , and we consider two approximation schemes for inference in this cluster
tree.
a. Using expectation propagation with fully factored approximation to the messages (as in ﬁgure 11.13).
b. Using a cluster graph approach on the cluster graph U constructed in the following manner. Each
cluster Ci ∈T appears in U. In addition, for each variable Xk that appears in some sepset Si,j in U
we introduce a new cluster with scope {Xk} and connect it to both Ci and Cj. (This construction
is similar to the Bethe cluster graph, except that we use the clusters in T as the “big” clusters in the
construction.)
Show that both approximations are equivalent in the sense that their respective energy functionals (and
the constraints they satisfy) coincide.
Exercise 11.24
Prove lemma 11.1.
Exercise 11.25⋆
In this exercise, you will provide a simpler proof for a special case of theorem 11.9 and corollary 11.6.
Assume that each Xi ∈X is a binary-valued random variable, parameterized with a single parameter
qi = Q(x1
i ).
a. By considering the derivative of F[ ˜PΦ, Q] and using lemma 11.1, prove theorem 11.9 without using
Lagrange multipliers.
b. Now, prove corollary 11.6.
Exercise 11.26⋆
In this exercise, we will prove theorem 11.11. The proof relies on the following proposition, which charac-
terizes the derivatives of an expectation relative to a Gibbs distribution.
Proposition 11.9
If Q(X) =
1
ZQ
Q
j ψj, then, for any function f with scope U,
∂
∂ψj(cj)IEQ[f(U)] = Q(cj)
ψj(cj) (IEQ[f(U) | cj] −IEQ[f(U)]) + IEQ

∂
∂ψj(cj)f(U)

.
a. Prove proposition 11.9.
b. Apply this proposition to prove theorem 11.11.
Exercise 11.27⋆
Consider the structured variational approximation of equation (11.61). As we discussed, to execute this
update we need to collect the expectation of several factors, and each of these requires that we compute
expectations given diﬀerent assignments to the factor of interest. Assuming we use a clique tree to perform
our inference, show how we can use a dynamic programming algorithm to reuse computation so as to
evaluate these updates more eﬃciently.
Exercise 11.28
Consider the factorial HMM model shown in ﬁgure 6.3a. Find a variational update rule for the structured
factorial HMM
variational approximation that factorizes as a set of clusters corresponding to the individual chains, that is,
a cluster {X(0)
i
, . . . , X(T )
i
} for each i. Make sure that you simplify your clusters as much as possible, as
in section 11.5.2.3 and section 11.5.2.4.

11.8. Exercises
483
Exercise 11.29⋆
Now, consider the DBN whose structure is a set of variables Xi that evolve over time, where, at each time
point, the chains are correlated by a tree structure that is the same for all time slices. Let Pai be the
parents of Xi in the tree.
Such structures (or extensions along similar lines) arise in several applications,
such as those involving evolution of DNA or protein sequences; here, the chains encode spatial correlations
over the sequence, and the tree the evolutionary process of each letter (DNA base pair or protein amino
acid), whether across species, as in the phylogenetic HMM of box 6.A, or within a family tree (as in box 3.B).
phylogenetic
HMM
Consider an unrolled network over time 0, . . . , T, where the initial X(0)
i
are all independent. Find update
rules for the following cluster variational approximations. In both cases, make sure that you simplify your
clusters as much as possible, as in section 11.5.2.3 and section 11.5.2.4.
a. Find an update rule for the cluster variational approximation that has a cluster for each chain; that is,
a cluster {X(0)
i
, . . . , X(T )
i
} for each i.
b. Find an update rule for the cluster variational approximation that has a cluster for each time slice; that
is, a cluster {X(t)
1 , . . . , X(t)
n } for each t.
Exercise 11.30
Consider a distribution PΦ that consists of the pairwise Markov network of ﬁgure 11.18a, and consider
approximating it with distribution Q that is represented by a pairwise Markov network of ﬁgure 11.18c.
Derive the potentials ψ1(A, C) and ψ1(B, D) that maximize ID(PΦ||Q). If A and C are not independent
in PΦ, will they be independent in the Q with these potentials?
Exercise 11.31⋆
Prove proposition 11.7.
Exercise 11.32⋆⋆
Describe an algorithm that performs variational variable elimination with optimization of the variational
parameters. Your algorithm should take as input a set of factors Φ, an elimination ordering X1, . . . , Xn,
and a subset Xi1, . . . , Xik of steps at which variational approximations should be performed.
Your
algorithm should use the technique of section 11.5.3.2 to avoid coupling the factors in the elimination step
of each Xij, introducing a single variational parameter λj for each such step. The result of the variable
elimination procedure should be a symbolic expression in the λj’s. Explain precisely how to construct this
symbolic expression and how to optimize it to select the optimal set of variational parameters λ1, . . . , λk.
Exercise 11.33⋆
Show that if Q = Q
i Q(Xi | U i), then
∂
∂Q(xi | ui)IEQ[f(Y )] =
1
Q(xi | ui)IEQ[f(Y ) | xi, ui].
Exercise 11.34⋆⋆
Develop a variational approximation using Bayesian networks. Assume that Q is represented by a Bayesian
network of a given structure G. Derive the ﬁxed-point characterization of parameters that maximize the
energy functional (that is, the analog of theorem 11.11) for this type of approximation. (Hint: use theorem 8.2
and exercise 11.33 in your derivation.)
Exercise 11.35⋆
Prove proposition 11.8.
Exercise 11.36⋆
In this exercise we consider inference in two-layered Bayesian networks with logistic CPDs.
In these
networks, all variables are binary. The variables X1, . . . , Xn in the top layer are all independent of each

484
Chapter 11. Inference as Optimization
other. The variables Y1, . . . , Ym in the bottom layer depend on the variables in the top layer using a
logistic CPD:
P(y1
j | x1, . . . , xn) = sigmoid(
X
i
wi,jxi + w0,j).
Suppose we observe some of the variables on the bottom layer, and we want to estimate the probability of
the evidence.
a. Show that when computing the probability of evidence, we can remove variables Yi that are not
observed.
b. Given evidence y, use the bound of equation (11.68) to write a variational upper bound of P(y).
c. Develop a mean ﬁeld approximation to lower-bound the probability of the same evidence.
Exercise 11.37⋆⋆
a. Show that the following function fobj is convex:
fobj(a1, . . . , an) = ln
X
i
eai.
b. Prove the following bound
fobj(a1, . . . , an) ≥
X
i
λiai −
X
i
λi ln λi,
for any choice of {λi} so that P
i λi = 1 and λi ≥0 for all i. (Hint: use the convexity of fobj().)
c. Write
ln Z = fobj({ln ˜PΦ(ξ)}),
and use the preceding bound to write a lower bound for ln Z. Use this analysis to provide an alternative
derivation of the mean ﬁeld approximation.
Exercise 11.38⋆⋆
We now consider a very diﬀerent class of representations that we can use in a structure variational
approximation.
Here, our distribution Q is a mixture distribution.
More precisely, in addition to the
mixture
distribution
variables X in PΦ, we introduce a new variable T. We can now deﬁne an approximating family:
Q(X) =
X
t
Q(t, X) =
X
t
Q(t)Q(X | t),
(11.82)
which is a mixture of diﬀerent approximations Q(X | t). As one simple instantiation, we might have:
Q(X, T) = Q(T)
Y
i
Q(Xi | T),
(11.83)
where each mixture component, Q(X | t), is a mean ﬁeld distribution.
a. Assuming that Q is structured as in equation (11.82), prove that
F[ ˜PΦ, Q] =
X
t
Q(t)F[ ˜PΦ, Q(X | t)] + IIQ(T; X),
(11.84)
where IIQ(T; X) is the mutual information between T and X.

11.8. Exercises
485
This result quantiﬁes the gains that we can obtain by using a mixture distribution. The ﬁrst term is the
weighted average of the energy functional of the mixture components; this term is bounded by the best
of the components in isolation.
The improvement over using speciﬁc components is captured by the
second term, which measures the extent to which T inﬂuences the distribution over X. If the components
represent identical distributions, then this term is zero, and, as expected, the value of the energy functional
is identical to the one obtained using the component representation. By contrast, if the components are
diﬀerent, then T is informative on X, and the lower bound provided by Q can be better than that of
each of the components. We note that the “best scenario” for improvement is when the component terms
F[ ˜PΦ, Q(X | t)] are similar, and yet IIQ(T; X) is large. In other words, the best approximation is obtained
when each of the components provides a good approximation, but by using a diﬀerent distribution.
We now use this formulation of the energy functional to produce an update rule for this variational
approximation. This derivation uses a local variational approximation.
b. Let {λ(ξ, t) : ξ ∈Val(X), t ∈Val(T)} be a family of constants. Use lemma 11.2 to show that:
IIQ(T; X) ≥IHQ(T) −
X
ξ,t
λ(ξ, t)Q(t) + IEQ[ln λ(X, T)] + 1.
Show also that this bound is tight if we choose a diﬀerent value of λ(ξ, t) for each possible combination
of ξ, t.
c. Suppose that we use a factorized form for the variational parameters:
λ(ξ, t) = λ(t)
Y
i
λ(xi | t).
Show that
F[ ˜PΦ, Q]
≥
X
t
Q(t)F[ ˜PΦ, Q(X | t)] + IHQ(T) −IEQ
h
˜λ(T)
i
+
(11.85)
IEQ[λ(T)] +
X
i
IEQ[λ(Xi | T)],
(11.86)
where ˜λ(t) = P
ξ λ(ξ, t).
d. Now, assume concretely that our approximation Q has the form of equation (11.83), and that all of the
Xi variables are binary-valued. Use your lower bound in equation (11.85) as an approximation to the
free energy, and provide an update rule for all of the parameters in Q: P(t) and P(x1
i | t).
e. Analyze the computational complexity of applying these ﬁxed-point equations.


12
Particle-Based Approximate Inference
In the previous chapter, we discussed one class of approximate inference methods. The tech-
niques used in that chapter gave rise to algorithms that were very similar in ﬂavor to the
factor-manipulation methods underlying exact inference. In this chapter, we discuss a very dif-
ferent class of methods, ones that can be roughly classiﬁed as particle-based methods. In these
methods, we approximate the joint distribution as a set of instantiations to all or some of the
variables in the network. These instantiations, often called particles, are designed to provide a
particle
good representation of the overall probability distribution.
Particle-based methods can be roughly characterized along two axes. On one axis, approaches
vary on the process by which particles are generated.
There is a wide variety of possible
processes. At one extreme, we can generate particles using some deterministic process. At
another, we can sample particles from some distribution. Within each category, there are many
possible variations.
On the other axis, techniques use diﬀerent notions of particles. Most simply, we can consider
full particles — complete assignments to all of the network variables X. The disadvantage of
full particle
this approach is that each particle covers only a very small part of the space. A more eﬀective
notion is that of a collapsed particle. A collapsed particle speciﬁes an assignment w only to
collapsed particle
some subset of the variables W , associating with it the conditional distribution P(X | w) or
some “summary” of it.
The general framework for most of the discussion in this chapter is as follows. Consider
some distribution P(X), and assume we want to estimate the probability of some event Y = y
relative to P, for some Y ⊆X and y ∈Val(Y ). More generally, we might want to estimate
the expectation of some function f(X) relative to P; this task is a generalization, since we can
choose f(ξ) = 11{ξ⟨Y ⟩= y}, where we recall that ξ⟨Y ⟩is the assignment in ξ to the variables
Y . We approximate this expectation by generating a set of M particles, estimating the value of
the function or its expectation relative to each of the generated particles, and then aggregating
the results.
For most of this chapter, we focus on methods that generate particles using random sampling:
In section 12.1, we consider the simplest possible method, which simply generates samples
from the original network. In section 12.2, we present a signiﬁcantly improved method that
generates samples from a distribution that is closer to the posterior distribution. In section 12.3,
we discuss a method based on Markov chains that deﬁnes a sampling process that, as it
converges, generates samples from distributions arbitrarily close to the posterior. In section 12.5,
we consider a very diﬀerent type of method, one that generates particles deterministically by

488
Chapter 12. Particle-Based Approximate Inference
Grade
Letter
SAT
Intelligence
Difﬁculty
d1
d0
0.6
0.4
i1
i0
0.7
0.3
i0
i1
s1
s0
0.95
0.2
0.05
0.8
g1
g2
g3
l1
l 0
0.1
0.4
0.99
0.9
0.6
0.01
i0,d0
i0,d1
i1,d0
i1,d1
g2
g3
g1
0.3
0.05
0.9
0.5
0.4
0.25
0.08
0.3
0.3
0.7
0.02
0.2
Figure 12.1
The Student network Bstudent revisited
searching for high-probability instantiations in the joint distribution. Finally, in section 12.4, we
extend these methods to the case of collapsed particles. We note that, unlike our discussion
of exact inference, some of the methods presented in this chapter — forward sampling and
likelihood weighting — apply (at least in their simple form) only to Bayesian networks, and not
to Markov networks or chain graphs.
12.1
Forward Sampling
The simplest approach to the generation of particles is forward sampling. Here, we generate
forward sampling
random samples ξ[1], . . . , ξ[M] from the distribution P(X). We ﬁrst show how we can easily
generate particles from PB(X) by sampling from a Bayesian network. We then analyze the
number of particles needed in order to get a good approximation of the expectation of some
target function f. We ﬁnally discuss the diﬃculties in generating samples from the posterior
PB(X | e).
We note that, in undirected models, even generating a sample from the prior
distribution is a diﬃcult task.
12.1.1
Sampling from a Bayesian Network
Sampling from a Bayesian network is a very simple process.
Example 12.1
Consider the Student network, shown again in ﬁgure 12.1. We begin by sampling D with the
appropriate (unconditional) distribution; that is, we ﬁguratively toss a coin that lands heads (d1)
40 percent of the time and tails (d0) the remaining 60 percent. Let us assume that the coin landed

12.1. Forward Sampling
489
Algorithm 12.1 Forward Sampling in a Bayesian network
Procedure Forward-Sample (
B
// Bayesian network over X
)
1
Let X1, . . . , Xn be a topological ordering of X
2
for i = 1, . . . , n
3
ui ←x⟨PaXi⟩
// Assignment to PaXi in x1, . . . , xi−1
4
Sample xi from P(Xi | ui)
5
return (x1, . . . , xn)
heads, so that we pick the value d1 for D. Similarly, we sample I from its distribution; say that the
result is i0. Given those, we know the right distribution from which to sample G: P(G | i0, d1), as
deﬁned by G’s CPD; we therefore pick G to be g1 with probability 0.05, g2 with probability 0.25,
and g3 with probability 0.7. The process continues similarly for S and L.
As shown in algorithm 12.1, we sample the nodes in some order consistent with the partial
order of the BN, so that by the time we sample a node we have values for all of its parents.
We can then sample from the distribution deﬁned by the CPD and by the chosen values for
the node’s parents. Note that the algorithm requires that we have the ability to sample from
the distributions underlying our CPD. Such sampling is straightforward in the discrete case (see
box 12.A), but subtler when dealing with continuous measures (see section 14.5.1).
Box 12.A — Skill: Sampling from a Discrete Distribution. How do we generate a sample from
a distribution? For a uniform distribution, we can use any pseudo-random number generator on our
machine. Other distributions require more thought, and much work has been devoted in statistics
to the problem of sampling from a variety of parametric distributions. Most obviously, consider
a multinomial distribution P(X) for Val(X) = {x1, . . . , xk}, which is deﬁned by parameters
θ1, . . . , θk. This process can be done quite simply as follows: We generate a sample s uniformly
from the interval [0, 1]. We then partition the interval into k subintervals: [0, θ1), [θ1, θ1 +θ2), . . .;
that is, the ith interval is [Pi−1
j=1 θj, Pi
j=1 θj). If s is in the ith interval, then the sampled value is
xi. We can determine the interval for s using binary search in time O(log k).
This approach gives us a general-purpose solution for generating samples from the CPD of any
discrete-valued variable: given a parent assignment u, we can always generate the full conditional
distribution P(X | u) and sample from it. (Of course, more eﬃcient methods may exist if X has a
large value space or a CPD that requires an expensive computation.) As we discuss in section 14.5.1,
the problem of sampling from continuous CPDs is considerably more complex.
Using basic convergence bounds (see appendix A.2), we know that from a set of particles
convergence
bound
D = {ξ[1], . . . , ξ[M]} generated via this sampling process, we can estimate the expectation of

490
Chapter 12. Particle-Based Approximate Inference
any function f as:
ˆIED(f) = 1
M
M
X
m=1
f(ξ[m]).
(12.1)
In the case where our task is to compute P(y), this estimate is simply the fraction of particles
where we have seen the event y:
ˆPD(y) = 1
M
M
X
m=1
11{y[m] = y},
(12.2)
where we use y[m] to denote ξ[m]⟨Y ⟩— the assignment to Y in the particle ξ[m]. For
example, our estimate for the probability of an event such as i1, l0 (a smart student getting a
bad letter) is the fraction of particles in which I got the value i1 and L the value l0. Note that
the same set of particles can be used to estimate the probabilities of multiple events.
This sampling process requires one sampling operation for each random variable in the net-
work. For each variable X, we need to index into the CPD using the current partial instantiation
of the parents of X. Using an appropriate data structure, this indexing can be accomplishing in
time O(|PaX|). The actual sampling process, as we discussed, requires time O(log |Val(X)|)
(assuming appropriate preprocessing). Letting M be the total number of particles generated,
n = |X|, p = maxi |PaXi|, and d = maxi |Val(Xi)|, the overall cost is O(Mnp log d).
12.1.2
Analysis of Error
Of course, the quality of the estimate obtained depends heavily on the number of particles
generated. We now analyze the question of the number of particles required to obtain certain
performance guarantees. We focus on the analysis for the case where our goal is to estimate
P(y).
The techniques of appendix A.2 provide us with the necessary tools for this analysis. Consider
the quality of our estimate for a particular event Y = y. We deﬁne a new random variable
over the probability space of P, using the indicator function 11{Y = y}. This is a Bernoulli
random variable, and hence our M particles in D deﬁne M independent Bernoulli trials, each
with success probability P(y).
We can now apply the Hoeﬀding bound (theorem A.3) to show that this estimate is close to
Hoeﬀding bound
the truth with high probability:
PD( ˆPD(y) ̸∈[P(y) −ϵ, P(y) + ϵ]) ≤2e−2Mϵ2.
(12.3)
This analysis provides us with an estimate of how many samples are required to achieve an
estimate whose error is bounded by ϵ, with probability at least 1 −δ. Setting
2e−2Mϵ2 ≤δ
and doing simple algebraic manipulations, we get that the required sample size to get an
sample size
estimator with (ϵ, δ) reliability is:
estimator

12.1. Forward Sampling
491
M ≥ln(2/δ)
2ϵ2
.
We can similarly apply the Chernoﬀbound (theorem A.4) to conclude that ˆPD(y) is also
Chernoﬀbound
within a relative error ϵ of the true value P(y), with high probability. Speciﬁcally, we have that:
relative error
PD( ˆPD(y) ̸∈P(y)(1 ± ϵ)) ≤2e−MP (y)ϵ2/3.
(12.4)
Note that in this analysis, unlike the one based on Hoeﬀding’s bound, the error probability (the
chance of getting an estimate that is more than ϵ away from the true value) depends on the
actual target value P(y). This dependence is not surprising for a relative error bound. Assume
that we generate M samples, but we generate none where y[m] = y. Our estimate ˆPD(y) is
simply 0. However, if P(y) is very small, it is fairly likely that we simply have not generated
any samples where this event holds. In this case, our estimate of 0 is not going to be within
any relative error of P(y). Thus, for very small values of P(y), we need many more samples
in order to guarantee that our estimate is close with high probability.
Examining equation (12.4), we can see that, for a given ϵ, the number of samples needed to
guarantee a certain error probability δ is:
M ≥3ln(2/δ)
P(y)ϵ2 .
(12.5)
Thus, the number of required samples grows inversely with the probability P(y).
In summary, to guarantee an absolute error of ϵ with probability at least 1 −δ, we need a
number of samples that grows logarithmically in 1/δ and quadratically in 1/ϵ. To guarantee a
relative error of ϵ with error probability at most δ, we need a number of samples that grows
similarly in δ and ϵ, but that also grows linearly with 1/P(y). A signiﬁcant problem with using
this latter bound is that we do not know P(y). (If we did, we would not have to estimate it.)
Thus, we cannot determine how many samples we need in order to ensure a good estimate.
12.1.3
Conditional Probability Queries
So far, we have focused on the problem of estimating marginal probabilities, that is, probabilities
of events Y = y relative to the original joint distribution. In general, however, we are interested
in conditional probabilities of the form P(y | E = e). Unfortunately, it turns out that this
estimation task is signiﬁcantly harder.
One approach to this task is simply to generate samples from the posterior probability P(X |
e). We can do so by a process called rejection sampling: We generate samples x from P(X),
rejection
sampling
as in section 12.1.1. We then reject any sample that is not compatible with e. The resulting
samples are sampled from the posterior P(X | e). The results of the analysis in section 12.1.2
now apply unchanged.
The problem, of course, is that the number of unrejected particles can be quite small. In
general, the expected number of particles that are not rejected from an original sample set of
size M is MP(e). For example, if P(e) = 0.001, then even for M = 10, 000 samples, the
expected number of unrejected particles is 10. Conversely, to obtain at least M ∗unrejected
particles, we need to generate on average M = M ∗/P(e) samples from P(X).

492
Chapter 12. Particle-Based Approximate Inference
Unfortunately, in many applications, low-probability evidence is the rule rather than the
exception. For example, in medical diagnosis, any set of symptoms typically has low probability.
In general, as the number of observed variables k = |E| grows, the probability of the evidence
usually decreases exponentially with k.
An alternative approach to the problem is to use a separate estimator for P(e) and for
P(y, e) and then compute the ratio. We can show that if we have estimators of low relative
error for both of these quantities, then their ratio will also have a low relative error (exercise 12.2).
Unfortunately, this approach only moves the problem from one place to the other. As we said,
the number of samples required to achieve a low relative error also grows linearly with

1/P(e). The number of samples required to get low absolute error does not grow with
P(e). However, it is not hard to verify (exercise 12.2) that a bound on the absolute error
for P(e) does not suﬃce to get any type of bound (relative or absolute) for the ratio
P(y, e)/P(e).
12.2
Likelihood Weighting and Importance Sampling
The rejection sampling process seems very wasteful in the way it handles evidence. We generate
multiple samples that are inconsistent with our evidence and that are ultimately rejected without
contributing to our estimator. In this section, we consider an approach that makes our samples
more relevant to our evidence.
12.2.1
Likelihood Weighting: Intuition
Consider the network in example 12.1, and assume that our evidence is d1, s1. Our forward
sampling process might begin by generating a value of d0 for D. No matter how the sampling
process proceeds, this sample will always be rejected as being incompatible with the evidence.
It seems much more sensible to simply force the samples to take on the appropriate values at
observed nodes. That is, when we come to sampling a node Xi whose value has been observed,
we simply set it to its observed value.
In general, however, this simple approach can generate incorrect results:
Example 12.2
Consider the network of example 12.1, and assume that our evidence is s1 — a student who received
a high SAT score. Using the naive process, we sample D and I from their prior distribution, set
S = s1, and then sample G and L appropriately. All of our samples will have S = s1, as
desired. However, the expected number of samples that have I = i1 — an intelligent student —
is 30 percent, the same as in the prior distribution. Thus, this approach fails to conclude that the
posterior probability of i1 is higher when we observe s1.
The problem with this approach is that it fails to account for the fact that, in the standard
forward sampling process, the node S is more likely to take the value s1 when its parent I has
the value i1 than when I has the value i0. In particular, consider an imaginary process where
we run rejection sampling many times; samples where we generated the value I = i1 would
have generated S = s1 in 80 percent of the samples, whereas samples where we generated the
value I = i0 would have generated S = s1 in only 5 percent of the samples. To simulate this
long-run behavior within a single sample, we should conclude that a sample where we have

12.2. Likelihood Weighting and Importance Sampling
493
Algorithm 12.2 Likelihood-weighted particle generation
Procedure LW-Sample (
B,
// Bayesian network over X
Z = z
// Event in the network
)
1
Let X1, . . . , Xn be a topological ordering of X
2
w ←1
3
for i = 1, . . . , n
4
ui ←x⟨PaXi⟩
// Assignment to PaXi in x1, . . . , xi−1
5
if Xi ̸∈Z then
6
Sample xi from P(Xi | ui)
7
else
8
xi ←z⟨Xi⟩
// Assignment to Xi in z
9
w ←w · P(xi | ui)
// Multiply weight by probability of desired value
10
return (x1, . . . , xn), w
I = i1 and force S = s1 should be worth 80 percent of a sample, whereas one where we have
I = i0 and force S = s1 should only be worth 5 percent of a sample.
When we have multiple observations and we want our sampling process to set all of them to
their observed values, we need to consider the probability that each of the observation nodes,
had it been sampled using the standard forward sampling process, would have resulted in the
observed values.
The sampling events for each node in forward sampling are independent,
and hence the weight for each sample should be the product of the weights induced by each
evidence node separately.
Example 12.3
Consider the same network, where our evidence set now consists of l0, s1. Assume that we sample
D = d1, I = i0, set S = s1, sample G = g2, and set L = l0. The probability that, given
I = i0, forward sampling would have generated S = s1 is 0.05. The probability that, given
G = g2, forward sampling would have generated L = l0 is 0.4. If we consider the standard
forward sampling process, each of these events is the result of an independent coin toss. Hence, the
probability that both would have occurred is simply the product of their probabilities. Thus, the
weight required for this sample to compensate for the setting of the evidence is 0.05 · 0.4 = 0.02.
Generalizing this intuition results in an algorithm called likelihood weighting (LW), shown in
likelihood
weighting
algorithm 12.2. The name indicates that the weights of diﬀerent samples are derived from the
likelihood of the evidence accumulated throughout the sampling process.
This process generates a weighted particle.
We can now estimate a conditional proba-
weighted particle
bility P(y | e) by using LW-Sample M times to generate a set D of weighted particles
⟨ξ[1], w[1]⟩, . . . , ⟨ξ[M], w[M]⟩. We then estimate:
ˆPD(y | e) =
PM
m=1 w[m]11{y[m] = y}
PM
m=1 w[m]
.
(12.6)
This estimator is an obvious generalization of the one we used for unweighted particles in
estimator

494
Chapter 12. Particle-Based Approximate Inference
equation (12.2). There, each particle had weight 1; hence, the terms in the numerator were
unweighted, and the denominator, which is the sum of all the particle weights, was simply M.
It is also important to note that, as in forward sampling, the same set of samples can be used
to estimate the probability of any event y.
Aside from some intuition, we have provided no formal justiﬁcation for the correctness of
LW as yet. It turns out that LW is a special case of a very general approach called importance
sampling, which also provides us the basis for an analysis. We begin by providing a general
description and analysis of importance sampling, and then reformulate LW as a special case of
this framework.
12.2.2
Importance Sampling
Let X be a variable (or set of variables) that takes on values in some space Val(X). Importance
importance
sampling
sampling is a general approach for estimating the expectation of a function f(x) relative to some
distribution P(X), typically called the target distribution. As we discussed, we can estimate this
target
distribution
expectation by generating samples x[1], . . . , x[M] from P, and then estimating
IEP [f] ≈1
M
M
X
m=1
f(x[m]).
In some cases, however, we might prefer to generate samples from a diﬀerent distribution Q,
known as the proposal distribution or sampling distribution. There are several reasons why we
proposal
distribution
might wish to sample from a diﬀerent distribution. Most importantly for our purposes, it might
be impossible or computationally very expensive to generate samples from P. For example,
P might be a posterior distribution for a Bayesian network, or even a prior distribution for a
Markov network.
In this section, we discuss how we might obtain estimates of an expectation relative to P
by generating samples from a diﬀerent distribution Q. In general, the proposal distribution Q
can be arbitrary; we require only that Q(x) > 0 whenever P(x) > 0, so that Q does not
“ignore” any states that have nonzero probability relative to P. (More formally, the support of
support
a distribution P is the set of points x for which P(x) > 0; we require that the support of
Q contain the support of P.) However, as we will see, the computational performance of this
approach does depend strongly on the extent to which Q is similar to P.
12.2.2.1
Unnormalized Importance Sampling
If we generate samples from Q instead of P, we cannot simply average the f-value of the
samples generated. We need to adjust our estimator to compensate for the incorrect sampling
distribution. The most obvious way of adjusting our estimator is based on the observation that
IEP (X)[f(X)] = IEQ(X)

f(X)P(X)
Q(X)

.
(12.7)

12.2. Likelihood Weighting and Importance Sampling
495
This equality follows directly:1
IEQ(X)

f(X)P(X)
Q(X)

=
X
x
Q(x)f(x)P(x)
Q(x)
=
X
x
f(x)P(x)
=
IEP (X)[f(X)].
Based on this observation, we can use the standard estimator for expectations relative to Q. We
generate a set of samples D = {x[1], . . . , x[M]} from Q, and then estimate:
ˆIED(f) = 1
M
M
X
m=1
f(x[m])P(x[m])
Q(x[m]).
(12.8)
We call this estimator the unnormalized importance sampling estimator; this method is also
unnormalized
importance
sampling
estimator
often called unweighted importance sampling (this terminology is confusing, inasmuch as the
particles here are also associated with weights). The factor P(x[m])/Q(x[m]) can be viewed
as a correction weight to the term f(x[m]), which we would have used had Q been our target
distribution. We use w(x) to denote P(x)/Q(x).
Our analysis immediately implies that this estimator is unbiased, that is, its mean for any data
unbiased
estimator
set is precisely the desired value:
Proposition 12.1
For data sets D sampled from Q, we have that:
IED
h
ˆIED(f)
i
= IEQ(X)[f(X)w(X)] = IEP (X)[f(X)].
We can also estimate the distribution of this estimator around its mean.
Letting ϵD =
ˆIED(f) −IEP [f(x)], we have that, since M →∞:
IED[ϵD] ∼N
 0; σ2
Q/M

,
where
σ2
Q
=
IEQ(X)

(f(X)w(X))2
−IEQ(X)[(f(X)w(X))]2
=
IEQ(X)

(f(X)w(X))2
−(IEP (X)[f(X)])2.
(12.9)
As we discussed in appendix A.2, the variance of this type of estimator — an average of
estimator
variance
M independent random samples from a distribution — decreases linearly with the number
of samples. This point is important, since it allows us to provide a bound on the number of
samples required to obtain a reliable estimate.
To understand the constant term in this expression, consider the (uninteresting) case where
the function f is the constant function f(ξ) ≡1. In this case, equation (12.9) simpliﬁes to:
IEQ(X)

w(X)2
−IEP (X)[1]
=
IEQ(X)
"P(X)
Q(X)
2#
−

IEQ(X)
P(X)
Q(X)
2
,
1. We present the proof in terms of discrete state spaces, but it holds equally for continuous state spaces.

496
Chapter 12. Particle-Based Approximate Inference
which is simply the variance of the weighting function P(x)/Q(x). Thus, the more diﬀerent Q
is from P, the higher the variance of this estimator. When f is an indicator function over part
of the space, we obtain an identical expression restricted to the relevant subspace. In general,
one can show that the lowest variance is achieved when
Q(X) ∝|f(X)|P(X);
thus, for example, if f is an indicator function over part of the space, we want our sampling
distribution to be P conditioned on the subspace.
Note that we should avoid cases where our sampling probability Q(X) ≪P(X)f(X) in
any part of the space, since these cases can lead to very large or even inﬁnite variance. Thus,
care must be taken when using very skewed sampling distributions, to ensure that probabilities
in Q are close to zero only when P(X)f(X) is also very small.
12.2.2.2
Normalized Importance Sampling
One problem with the preceding discussion is that it assumes that P is known. A frequent
situation, and one of the most common reasons why we must resort to sampling from a
diﬀerent distribution Q, is that P is known only up to a normalizing constant Z. Speciﬁcally,
what we have access to is a function ˜P(X) such that ˜P is not a normalized distribution, but
˜P(X) = ZP(X). For example, in a Bayesian network B, we might have (for X = X) P(X)
be our posterior distribution PB(X | e), and ˜P(X) be the unnormalized distribution PB(X, e).
In a Markov network, P(X) might be PH(X), and ˜P might be the unnormalized distribution
obtained by multiplying together the clique potentials, but without normalizing by the partition
function.
In this context, we cannot deﬁne the weights relative to P, so we deﬁne:
w(X) =
˜P(X)
Q(X).
(12.10)
Unfortunately, with this deﬁnition of weights, the analysis justifying the use of equation (12.8)
breaks down. However, we can use a slightly diﬀerent estimator based on similar intuitions. As
before, the weight w(X) is a random variable. Its expected value is simply Z:
IEQ(X)[w(X)] =
X
x
Q(x)
˜P(x)
Q(x) =
X
x
˜P(x) = Z.
(12.11)
This quantity is the normalizing constant of the distribution ˜P, which is itself often of consid-
erable interest, as we will see in our discussion of learning algorithms.

12.2. Likelihood Weighting and Importance Sampling
497
We can now rewrite equation (12.7):
IEP (X)[f(X)]
=
X
x
P(x)f(x)
=
X
x
Q(x)f(x)P(x)
Q(x)
=
1
Z
X
x
Q(x)f(x)
˜P(x)
Q(x)
=
1
Z IEQ(X)[f(X)w(X)]
=
IEQ(X)[f(X)w(X)]
IEQ(X)[w(X)]
.
(12.12)
We can use an empirical estimator for both the numerator and denominator.
Given M
samples D = {x[1], . . . , x[M]} from Q, we can estimate:
ˆIED(f) =
PM
m=1 f(x[m])w(x[m])
PM
m=1 w(x[m])
.
(12.13)
We call this estimator the normalized importance sampling estimator; it is also known as the
normalized
importance
sampling
estimator
weighted importance sampling estimator.
The normalized estimator involves a quotient, and it is therefore much more diﬃcult to analyze
theoretically. However, unlike the unnormalized estimator of equation (12.8), the normalized
estimator is not unbiased. This bias is particularly immediate in the case M = 1. Here, the
estimator reduces to:
f(x[1])w(x[1])
w(x[1])
= f(x[1]).
Because x[1] is sampled from Q, the mean of the estimator in this case is IEQ(X)[f(X)] rather
than the desired IEP (X)[f(X)]. Conversely, when M goes to inﬁnity, we have that each of
the numerators and denominators converges to the expected value, and our analysis of the
expectation applies. In general, for ﬁnite M, the estimator is biased, and the bias goes down as
1/M.
One can show that the variance of the importance sampling estimator with M data instances
is approximately:
VVarP
h
ˆIED(f(X))
i
≈1
M VVarP [f(X)](1 + VVarQ[w(X)]),
(12.14)
which also goes down as 1/M. Theoretically, this variance and the variance of the unnormalized
estimator (equation (12.8)) are incomparable, and each of them can be larger than the other.
Indeed, it is possible to construct examples where each of them performs better than the other.
In practice, however, the variance of the normalized estimator is typically lower than that of the
unnormalized estimator. This reduction in variance often outweighs the bias term, so that the
normalized estimator is often used in place of the unnormalized estimator, even in cases where
P is known and we can sample from it eﬀectively.

498
Chapter 12. Particle-Based Approximate Inference
Note that equation (12.14) can be used to provide a rough estimate on the quality of a set
of samples generated using normalized importance sampling. Assume that we were to estimate
IEP [f] using a standard sampling method, where we generate M IID samples from P(X).
(Obviously, this is generally intractable, but it provides a useful benchmark for comparison.) This
approach would result in a variance VVarP [f(X)]/M. The ratio between these two variances is:
1
1 + VVarQ[w(x)].
Thus, we would expect M weighted samples generated by importance sampling to be “equiv-
alent” to M/(1 + VVarQ[w(x)]) samples generated by IID sampling from P. We can use this
observation to deﬁne a rule of thumb for the eﬀective sample size of a particular set D of M
eﬀective sample
size
samples resulting from a particular run of importance sampling:
Meﬀ
=
M
1 + VVar[D]
(12.15)
VVar[D]
=
M
X
m=1
w(x[m])2 −(
M
X
m=1
w(x[m]))2.
This estimate can tell us whether we should continue generating additional samples.
12.2.3
Importance Sampling for Bayesian Networks
With this theoretical foundation, we can now describe the application of importance sampling
to Bayesian networks. We begin by providing the proposal distribution most commonly used
for Bayesian networks. This distribution Q uses the network structure and its CPDs to focus
the sampling process on a particular part of the joint distribution — the one consistent with
a particular event Z = z. We show several ways in which this construction can be applied to
the Bayesian network inference task, dealing with various types of probability queries. Finally,
we brieﬂy discuss several other proposal distributions, which are somewhat more complicated
to implement but may perform better in practice.
12.2.3.1
The Mutilated Network Proposal Distribution
Assume that we are interested in a particular event Z = z, either because we wish to estimate
its probability, or because we have observed it as evidence. We wish to focus our sampling
process on the parts of the joint that are consistent with this event. In this section, we deﬁne
an importance sampling process that achieves this goal.
To gain some intuition, consider the network of ﬁgure 12.1 and assume that we are interested
in a particular event concerning a student’s grade: G = g2. We wish to bias our sampling
toward parts of the space where this event holds. It is easy to take this event into consideration
when sampling L: we simply sample L from P(L | g2). However, it is considerably more
diﬃcult to account for G’s inﬂuence on D, I, and S without doing inference in the network.
Our goal is to deﬁne a simple proposal distribution that allows for the eﬃcient generation
of particles.
We therefore avoid the problem of accounting for the eﬀect of the event on
nondescendants; we deﬁne a proposal distribution that “sets” the value of a Z ∈Z to take the

12.2. Likelihood Weighting and Importance Sampling
499
Grade
Letter
SAT
Intelligence
Difﬁculty
d1
d0
0.6
0.4
i1
i0
0
1
i0
i1
s1
s0
0.95
0.2
0.05
0.8
g1
g2
g3
l1
l 0
0.1
0.4
0.99
0.9
0.6
0.01
g2
g3
g1
0
1
0
Figure 12.2
The mutilated network Bstudent
I=i1,G=g2 used for likelihood weighting
prespeciﬁed value in a way that inﬂuences the sampling process for its descendants, but not for
the other nodes in the network. The proposal distribution is most easily described in terms of
a Bayesian network:
Deﬁnition 12.1
Let B be a network, and Z1 = z1, . . . , Zk = zk, abbreviated Z = z, an instantiation of variables.
We deﬁne the mutilated network BZ=z as follows:
mutilated
network
• Each node Zi ∈Z has no parents in BZ=z; the CPD of Zi in BZ=z gives probability 1 to
Zi = zi and probability 0 to all other values z′
i ∈Val(Zi).
• The parents and CPDs of all other nodes X ̸∈Z are unchanged.
For example, the network Bstudent
I=i1,G=g2 is shown in ﬁgure 12.2. As we can see, the node G is
decoupled from its parents, eliminating its dependence on them (the node I has no parents in
the original network, so its parent set remains empty). Furthermore, both I and G have CPDs
that are deterministic, ascribing probability 1 to their (respective) observed values.
Importance sampling with this proposal distribution is precisely equivalent to the LW algo-
rithm shown in algorithm 12.2, with ˜P(X) = PB(X, z) and the proposal distribution Q induced
by the mutilated network BZ=z. More formally, we can show the following proposition:
Proposition 12.2
Let ξ be a sample generated by algorithm 12.2 and w be its weight. Then the distribution over ξ is
as deﬁned by the network BZ=z, and
w(ξ) =
PB(ξ)
PBZ=z(ξ).

500
Chapter 12. Particle-Based Approximate Inference
The proof is not diﬃcult and is left as an exercise (exercise 12.4). It is important to note, however,
that the algorithm does not require the explicit construction of the mutilated network. It simply
traverses the original network, using the process shown in algorithm 12.2.
As we now show, this proposal distribution can be used for estimating a variety of Bayesian
network queries.
12.2.3.2
Unconditional Probability of an Event ⋆
We begin by considering the simple problem of computing the unconditional probability of an
event Z = z. Although we can clearly use forward sampling for estimating this probability, we
can also use unnormalized importance sampling, where the target distribution P is simply our
prior distribution PB(X), and the proposal distribution Q is the one deﬁned by the mutilated
network BZ=z. Our goal is to estimate the expectation of a function f, which is the indicator
function of the query z: f(ξ) = 11{ξ⟨Z⟩= z}.
The unnormalized importance-sampling estimator for this case is simply:
ˆPD(z)
=
1
M
M
X
m=1
11{ξ[m]⟨Z⟩= z}w(ξ[m])
=
1
M
M
X
m=1
w[m],
(12.16)
where the equality follows because, by deﬁnition of Q, our sampling process generates samples
ξ[m] only where z holds.
When trying to bound the relative error of an estimator, a key quantity is the variance of the
estimator relative to its mean. In the Chernoﬀbound, when we are estimating the probability
p of a very low-probability event, the variance of the estimator, which is p(1 −p), is very high
relative to the mean p. Importance sampling removes some of the variance associated with this
sampling process, and it can therefore achieve better performance in certain cases.
In this case, the samples are derived from our proposal distribution Q, and the value of the
function whose expectation we are computing is simply the weight. Thus, we need to bound the
variance of the function w(X) under our distribution Q. Let us consider the sampling process
in the algorithm. As we go through the variables in the network, we encounter the observed
variables Z1, . . . , Zk. At each point, we multiply our current weight w by some conditional
probability number PB(Zi = zi | PaZi).
One situation where we can bound the variance arises in a restricted class of networks, one
where the entries in the CPD of the variables Zi are bounded away from the extremes of 0 and
1. More precisely, we assume that there is some pair of numbers ℓ> 0 and u < 1 such that:
for each variable Z ∈Z, z ∈Val(Z), and u ∈Val(PaZ), we have that PB(Z = z | PaZ =
u) ∈[ℓ, u]. Next, we assume that |Z| = k for some small k. This assumption is not a trivial
one; while queries often involve only a small number of variables, we often have a fairly large
number of observations that we wish to incorporate.
Under these assumptions, the weight w generated through the LW process is necessarily in
the interval ℓk and uk. We can now redeﬁne our weights by dividing each w[m] by uk:
w′[m] = w[m]/uk.

12.2. Likelihood Weighting and Importance Sampling
501
Each weight w′[m] is now a real-valued random variable in the range [(ℓ/u)k, 1]. For a data set
D of weights w[1], . . . , w[M], we can now deﬁne:
ˆp′
D = 1
M
M
X
m=1
w′[m].
The key point is that the mean of this random variable, which is PB(z)/uk, is therefore also in
the range [(ℓ/u)k, 1], and its variance is, at worst, the variance of a Bernoulli random variable
with the same mean. Thus, we now have a random variable whose variance is not that small
relative to its mean.
A simple generalization of Chernoﬀ’s bound (theorem A.4) to the case of real-valued variables
can now be used to show that:
PD( ˆPD(z) ̸∈PB(z)(1 ± ϵ))
=
PD(ˆp′
D ̸∈1
uk PB(z)(1 ± ϵ))
≤
2e−M
1
uk PB(z)ϵ2/3.
We can use this equation, as in the case of Bernoulli random variables, to derive a suﬃcient
condition for the sample size that can guarantee that the estimator ˆPD(z) of equation (12.16)
sample size
has error at most ϵ with probability at least 1 −δ:
M ≥3 ln(2/δ)uk
PB(z)ϵ2
.
(12.17)
Since PB(z) ≥ℓk, a (stronger) suﬃcient condition is that:
M ≥3 ln(2/δ)
ϵ2
u
ℓ
k
.
(12.18)
It is instructive to compare this bound to the one we obtain from the Chernoﬀbound in
Chernoﬀbound
equation (12.5). The bound in equation (12.18) makes a weaker assumption about the probability
of the event z. Equation (12.5) requires that PB(z) not be too low. By contrast, equation (12.17)
assumes only that this probability is in a bounded range ℓk, uk; the actual probability of the
event z can still be very low — we have no guarantee on the actual magnitude of ℓ. Thus, for
example, if our event z corresponds to a rare medical condition — one that has low probability
given any instantiation of its parents — the estimator of equation (12.16) would give us a relative
error bound, whereas standard sampling would not.
We can use this bound to determine in advance the number of samples required for a certain
desired accuracy. A disadvantage of this approach is that it does not take into consideration
the speciﬁc samples we happened to generate during our sampling process. Intuitively, not all
samples contribute equally to the quality of the estimate. A sample whose weight is high is more
compatible with the evidence e, and it arguably provides us with more information. Conversely,
a low-weight sample is not as informative, and a data set that contains a large number of
low-weight samples might not be representative and might lead to a poor estimate. A somewhat
more sophisticated approach is to preselect not the number of particles, but a predeﬁned total
weight. We then stop sampling when the total weight of the generated particles reaches our
predeﬁned lower bound.

502
Chapter 12. Particle-Based Approximate Inference
Algorithm 12.3 Likelihood weighting with a data-dependent stopping rule
Procedure Data-Dependent-LW (
B,
// Bayesian network over X
Z = z,
// Instantiation of interest
u,
// Upper bound on CPD entries of Z
ϵ,
// Desired error bound
δ
// Desired probability of error
)
1
γ ←
4(1+ϵ)
ϵ2
ln 2
δ
2
k ←|Z|
3
W ←0
4
M ←0
5
while W < γuk
6
ξ, w ←LW-Sample(B, Z = z)
7
W ←W + w
8
M ←M + 1
9
return W/M
For this algorithm, we can provide a similar theoretical analysis with certain guarantees for
this data-dependent likelihood weighting approach. Algorithm 12.3 shows an algorithm that uses
data-dependent
likelihood
weighting
a data-dependent stopping rule to terminate the sampling process when enough weight has
been accumulated. We can show that:
Theorem 12.1
Data-Dependent-LW returns an estimate ˆp for PB(Z = z) which, with probability at least 1 −δ,
has a relative error of ϵ.
We can also place an upper bound on the expected sample size used by the algorithm:
expected sample
size
Theorem 12.2
The expected number of samples used by Data-Dependent-LW is
uk
PB(z)γ ≤
u
ℓ
k
γ,
where γ = 4(1+ϵ)
ϵ2
ln 2
δ .
The intuition behind this result is straightforward. The algorithm terminates when W ≥γuk.
The expected contribution of each sample is IEQ(X)[w(ξ)] = PB(z). Thus, the total number
of samples required to achieve a total weight of W ≥γuk is M ≥γuk/PB(z). Although this
bound on the expected number of samples is no better than our bound in equation (12.17), the
data-dependent bound allows us to stop early in cases where we were lucky in our random
choice of samples, and to continue sampling in cases where we were unlucky.
12.2.3.3
Ratio Likelihood Weighting
We now move to the problem of computing a conditional probability P(y | e) for a speciﬁc
event y.
One obvious approach is ratio likelihood weighting: we compute the conditional
ratio likelihood
weighting

12.2. Likelihood Weighting and Importance Sampling
503
probability as P(y, e)/P(e), and use unnormalized importance sampling (equation (12.16)) for
both the numerator and denominator.
We can therefore estimate the conditional probability P(y | e) in two phases: We use
the algorithm of algorithm 12.2 M times with the argument Y
= y, E = e, to gener-
ate one set D of weighted samples (ξ[1], w[1]), . . . , (ξ[M], w[M]).
We use the same algo-
rithm M ′ times with the argument E = e, to generate another set D′ of weighted samples
(ξ′[1], w′[1]), . . . , (ξ′[M ′], w′[M ′]). We can then estimate:
ˆPD(y | e) =
ˆPD(y, e)
ˆPD′(e)
= 1/M PM
m=1 w[m]
1/M ′ PM′
m=1 w′[m]
.
(12.19)
In ratio LW, the numerator and denominator are both using unnormalized importance sam-
pling, which admits a rigorous theoretical analysis. Thus, we can now provide bounds on the
number of samples M required to obtain a good estimate for both P(y, e) and P(e).
12.2.3.4
Normalized Likelihood Weighting
Ratio LW allows us to estimate the probability of a single query P(y | e). In many cases,
however, we are interested in estimating an entire joint distribution P(Y | e) for some variable
or subset of variables Y . We can answer such a query by running ratio LW for each y ∈Val(Y ),
but this approach is typically too computationally expensive to be practical.
An alternative approach is to use normalized likelihood weighting, which is based on the
normalized
likelihood
weighting
normalized importance sampling estimator of equation (12.13). In this application, our target
distribution is P(X) = PB(X | e). As we mentioned, we do not have access to P directly;
rather, we can evaluate ˜P(X) = PB(X, e), which is the probability of a full assignment and can
be easily computed via the chain rule. In this case, we are trying to estimate the expectation of
a function f which is the indicator function of the query y: f(ξ) = 11{ξ⟨Y ⟩= y}. Applying the
normalized importance sampling estimator of equation (12.13) to this setting, we obtain precisely
the estimator of equation (12.6).

The quality of the importance sampling estimator depends largely on how close the
proposal distribution Q is to the target distribution P. We can gain intuition for this
question by considering two extreme cases. If all of the evidence in our network is at
the roots, the proposal distribution is precisely the posterior, and there is no need to
compensate; indeed, no evidence is encountered along the way, and all samples will have
the same weight P(e). On the other side of the spectrum, if all of the evidence is at
the leaves, our proposal distribution Q(X) is the prior distribution PB(X), leaving the
correction purely to the weights. In this situation, LW will work reasonably only if the
prior is similar to the posterior. Otherwise, most of our samples will be irrelevant, a fact
that will be reﬂected by their low weight. For example, consider a medical-diagnosis setting,
and assume that our evidence is a very unusual combination of symptoms generated by only
one very rare disease. Most samples will not involve this disease and will give only very low
probability to this combination of symptoms. Indeed, the combinations sampled are likely to be
irrelevant and are not useful at all for understanding what disease the patient has. We return to
this issue in section 12.2.4.
To understand the relationship between the prior and the posterior, note that the prior is a

504
Chapter 12. Particle-Based Approximate Inference
weighted average of the posteriors, weighted over diﬀerent instantiations of the evidence:
P(X) =
X
e
P(e)P(X | e).
If the evidence is very likely, then it is a major component in this summation, and it is probably
not too far from the prior. For example, in the network Bstudent, the event S = s1 is fairly likely,
and the posterior distribution PBstudent(X | s1) is fairly similar to the prior. However, for unlikely
evidence, the weight of P(X | e) is negligible, and there is nothing constraining the posterior to
be similar to the prior. Indeed, our distribution PBstudent(X | l0) is very diﬀerent from the prior.
Unfortunately, there is currently no formal analysis for the number of particles required to
achieve a certain quality of estimate using normalized importance sampling. In many cases, we
simply preselect a number of particles that seems large enough, and we generate that number.
Alternatively, we can use a heuristic approach that uses the total weight of the particles generated
so far as guidance as to the extent to which they are representative. Thus, for example, we might
decide to generate samples until a certain minimum bound on the total weight has been reached,
as in Data-Dependent-LW. We note, however, that this approach is entirely heuristic in this case
(as in all cases where we do not have bounds [ℓ, u] on our CPDs). Furthermore, there are cases
where the evidence is simply unlikely in all conﬁgurations, and therefore all samples will have
low weights.
12.2.3.5
Conditional Probabilities: Comparison
We have seen two variants of likelihood weighting: normalized LW and ratio LW. Ratio LW has
two related advantages. The normalized LW process samples an assignment of the variables Y
(those not in E), whereas ratio LW simply sets the values of these variables. The additional sam-
pling step for Y introduces additional variance into the overall process, leading to a reduction
in the robustness of the estimate. Thus, in many cases, the variance of this estimator is lower
than that of equation (12.6), leading to more robust estimates.
A second advantage of ratio LW is that it is much easier to analyze, and therefore it is
associated with stronger guarantees regarding the number of samples required to get a good
estimate. However, these bounds are useful only under very strong conditions: a small number
of evidence variables, and a bound on the skew of the CPD entries in the network.
On the other hand, a signiﬁcant disadvantage of ratio LW is the fact that each query y
requires that we generate a new set of samples for the event y, e. It is often the case that we
want to evaluate the probability of multiple queries relative to the same set of evidence. The
normalized LW approach allows these multiple computations to be executed relative to the same
set of samples, whereas ratio LW requires a separate sample set for each query y. This cost is
particularly problematic when we are interested in computing the joint distribution over a subset
of variables. Probably due to this last point, normalized LW is used more often in practice.
12.2.4
Importance Sampling Revisited
The likelihood weighting algorithm uses, as its proposal distribution, the very simple distribution
obtained from mutilating the network by eliminating edges incoming to observed variables.
However, this proposal distribution can be far from optimal. For example, if the CPDs associated

12.3. Markov Chain Monte Carlo Methods
505
with these evidence variables are skewed, the importance weights are likely to be quite large,
resulting in estimators with high variance. Indeed, somewhat surprisingly, even in very simple
cases, the obvious proposal distribution may not be optimal. For example, if X is not a root
node in the network, the optimal proposal distribution for computing P(X = x) may not be
the distribution P, even without evidence! (See exercise 12.5.)
The importance sampling framework is very general, however, and several other proposal
distributions have been utilized. For example, backward importance sampling generates samples
backward
importance
sampling
for parents of evidence variables using the likelihood of their children. Most simply, if X is a
variable whose child Y is observed to be Y = y, we might generate some samples for X from
a renormalized distribution Q(X) ∝P(Y = y | X). We can continue this process, sampling
X’s parents from the likelihood of X’s sampled value. We can also propose more complex
schemes that sample the value of a variable given a combination of sampled or observed values
for some of its parents and/or children. One can also consider hybrid approaches that use some
global approximate inference algorithm (such as those in chapter 11) to construct a proposal
distribution, which is then used as the basis for sampling. As long as the importance weights

are computed correctly, we are guaranteed that this process is correct. (See exercise 12.7.)
This process can lead to signiﬁcant improvements in theory, and it does lead to improvements
in some cases in practice.
12.3
Markov Chain Monte Carlo Methods
One of the limitations of likelihood weighting is that an evidence node aﬀects the sampling only
for nodes that are its descendants. The eﬀect on nodes that are nondescendants is accounted
for only by the weights. As we discussed, in cases where much of the evidence is at the leaves
of the network, we are essentially sampling from the prior distribution, which is often very far
from the desired posterior. We now present an alternative sampling approach that generates
a sequence of samples. This sequence is constructed so that, although the ﬁrst sample may
be generated from the prior, successive samples are generated from distributions that provably
get closer and closer to the desired posterior. We note that, unlike forward sampling methods
(including likelihood weighting), Markov chain methods apply equally well to directed and to
undirected models. Indeed, the algorithm is easier to present in the context of a distribution PΦ
deﬁned in terms of a general set of factors Φ.
12.3.1
Gibbs Sampling Algorithm
One idea for addressing the problem with forward sampling approaches is to try to “ﬁx” the
sample we generated by resampling some of the variables we generated early in the process.
Perhaps the simplest method for doing this is presented in algorithm 12.4. This method, called
Gibbs sampling, starts out by generating a sample of the unobserved variables from some initial
Gibbs sampling
distribution; for example, we may use the mutilated network to generate a sample using forward
sampling. Starting from that sample, we then iterate over each of the unobserved variables,
sampling a new value for each variable given our current sample for all other variables. This
process allows information to “ﬂow” across the network as we sample each variable.
To apply this algorithm to a network with evidence, we ﬁrst reduce all of the factors by the
observations e, so that the distribution PΦ used in the algorithm corresponds to P(X | e).

506
Chapter 12. Particle-Based Approximate Inference
Algorithm 12.4 Generating a Gibbs chain trajectory
Procedure Gibbs-Sample (
X
// Set of variables to be sampled
Φ
// Set of factors deﬁning PΦ
P (0)(X),
// Initial state distribution
T
// Number of time steps
)
1
Sample x(0) from P (0)(X)
2
for t = 1, . . . , T
3
x(t) ←x(t−1)
4
for each Xi ∈X
5
Sample x(t)
i
from PΦ(Xi | x−i)
6
// Change Xi in x(t)
7
return x(0), . . . , x(T )
Example 12.4
Let us revisit example 12.3, recalling that we have the observations s1, l0. In this case, our algo-
rithm will generate samples over the variables D, I, G. The set of reduced factors Φ is therefore:
P(I), P(D), P(G | I, D), P(s1 | I), P(l0 | G). Our algorithm begins by generating one sam-
ple, say by forward sampling. Assume that this sample is d(0) = d1, i(0) = i0, g(0) = g2. In
the ﬁrst iteration, it would now resample all of the unobserved variables, one at a time, in some
predetermined order, say G, I, D. Thus, we ﬁrst sample g(1) from the distribution PΦ(G | d1, i0).
Note that because we are computing the distribution over a single variable given all the others,
this computation can be performed very eﬃciently:
PΦ(G | d1, i0)
=
P(i0)P(d1)P(G | i0, d1)P(l0 | G)P(s1 | i0)
P
g P(i0)P(d1)P(g | i0, d1)P(l0 | g)P(s1 | i0)
=
P(G | i0, d1)P(l0 | G)
P
g P(g | i0, d1)P(l0 | g).
Thus, we can compute the distribution simply by multiplying all factors that contain G, with all
other variables instantiated, and renormalizing to obtain a distribution over G.
Having sampled g(1) = g3, we now continue to resampling i(1) from the distribution PΦ(I |
d1, g3), obtaining, for example, i(1) = i1; note that the distribution for I is conditioned on the
newly sampled value g(1). Finally, we sample d(1) from PΦ(D | g3, i1), obtaining d1. The result of
the ﬁrst iteration of sampling is, then, the sample (i1, d1, g3). The process now repeats.
Note that, unlike forward sampling, the sampling process for G takes into consideration the
downstream evidence at its child L. Thus, its sampling distribution is arguably closer to the
posterior. Of course, it is not the true posterior, since it still conditions on the originally sampled
values for I, D, which were sampled from the prior distribution. However, we now resample I
and D from a distribution that conditions on the new value of G, so one can imagine that their
sampling distribution may also be closer to the posterior. Thus, perhaps the next sample of G,

12.3. Markov Chain Monte Carlo Methods
507
0.25
0.25
0.5
0.5
0.25
0.5
0.25
0.5
0.25
0.5
0.25
0.25
0.5
0.25
0.5
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.5
–3
–4
–2
–1
+1
+2
+3
+4
Figure 12.3
The Grasshopper Markov chain
which uses these new values for I, D (and conditions on the evidence l0), will be sampled from
a distribution even closer to the posterior. Indeed, this intuition is correct. One can show that,
as we repeat this sampling process, the distribution from which we generate each sample gets
closer and closer to the posterior PΦ(X) = P(X | e).
In the subsequent sections, we formalize this intuitive argument using a framework called
Markov chain Monte Carlo (MCMC). This framework provides a general approach for generating
Markov chain
Monte Carlo
samples from the posterior distribution, in cases where we cannot eﬃciently sample from the
posterior directly.
In MCMC, we construct an iterative process that gradually samples from
distributions that are closer and closer to the posterior.
A key question is, of course, how
many iterations we should perform before we can collect a sample as being (almost) generated
from the posterior. In the following discussion, we provide the formal foundations for MCMC
algorithms, and we try to address this and other important questions. We also present several
valuable generalizations.
12.3.2
Markov Chains
12.3.2.1
Basic Deﬁnition
At a high level, a Markov chain is deﬁned in terms of a graph of states over which the sampling
algorithm takes a random walk. In the case of graphical models, this graph is not the original
graph, but rather a graph whose nodes are the possible assignments to our variables X.
Deﬁnition 12.2
A Markov chain is deﬁned via a state space Val(X) and a model that deﬁnes, for every state
Markov chain
x ∈Val(X) a next-state distribution over Val(X).
More precisely, the transition model T
transition model
speciﬁes for each pair of state x, x′ the probability T (x →x′) of going from x to x′. This
transition probability applies whenever the chain is in state x.
We note that, in this deﬁnition and in the subsequent discussion, we restrict attention to
homogeneous, where the system dynamics do not change over time.
homogeneous
Markov chain
We illustrate this concept with a simple example.
Example 12.5
Consider a Markov chain whose states consist of the nine integers −4, . . . , +4, arranged as points
on a line. Assume that a drunken grasshopper starts out in position 0 on the line. At each point
in time, it stays where it is with probability 0.5, or it jumps left and right with equal probability.
Thus, T (i →i) = 0.5, T (i →i + 1) = 0.25, and T (i →i −1) = 0.25. However, the two end
positions are blocked by walls; hence, if the grasshopper is in position +4 and tries to jump right, it

508
Chapter 12. Particle-Based Approximate Inference
remains in position +4. Thus, for example, T (+4 →+4) = 0.75. We can visualize the state space
as a graph, with probability-weighted directed edges corresponding to transitions between diﬀerent
states. The graph for our example is shown in ﬁgure 12.3.
We can imagine a random sampling process, that deﬁnes a random sequence of states
x(0), x(1), x(2), . . .. Because the transition model is random, the state of the process at step t
can be viewed as a random variable X(t). We assume that the initial state X(0) is distributed
according to some initial state distribution P (0)(X(0)). We can now deﬁne distributions over
the subsequent states P (1)(X(1)), P (2)(X(2)), . . . using the chain dynamics:
P (t+1)(X(t+1) = x′) =
X
x∈Val(X)
P (t)(X(t) = x)T (x →x′).
(12.20)
Intuitively, the probability of being at state x′ at time t + 1 is the sum over all possible states
x that the chain could have been in at time t of the probability being in state x times the
probability that the chain took a transition from x to x′.
12.3.2.2
Asymptotic Behavior
For our purposes, the most important aspect of a Markov chain is its long-term behavior.
Example 12.6
Because the grasshopper’s motion is random, we can consider its location at time t to be a random
variable, which we denote X(t). Consider the distribution over X(t). Initially, the grasshopper
is at 0, so that P(X(0) = 0) = 1. At time 1, we have that X(1) is 0 with probability 0.5,
and +1 or −1, each with probability 0.25. At time 2, we have that X(2) is 0 with probability
0.52 + 2 · 0.252 = 0.375, +1 and −1 each with probability 2(0.5 · 0.25) = 0.25, and +2 and
−2 each with probability 0.252 = 0.0625. As the process continues, the probability gets spread
out over more and more of the states. For example, at time t = 10, the probabilities of the diﬀerent
states range from 0.1762 for the value 0, and 0.0518 for the values ±4. At t = 50, the distribution
is almost uniform, with a range of 0.1107–0.1116.
Thus, one approach for sampling from the uniform distribution over the set −4, . . . , +4 is to
start oﬀat 0 and then randomly choose the next state from the transition model for this chain.
After some number of such steps t, our state X(t) would be sampled from a distribution that
is very close to uniform over this space. We note that this approach is not a very good one for
sampling from a uniform distribution; indeed, the expected time required for such a chain even
to reach the boundaries of the interval [−K, K] is K2 steps. However, this general approach
applies much more broadly, including in cases where our “long-term” distribution is not one
from which we can easily sample.
Markov chain Monte carlo (MCMC) sampling is a process that mirrors the dynamics of the
MCMC sampling
Markov chain; the process of generating an MCMC trajectory is shown in algorithm 12.5. The
sample x(t) is drawn from the distribution P (t). We are interested in the limit of this process,
that is, whether P (t) converges, and if so, to what limit.

12.3. Markov Chain Monte Carlo Methods
509
Algorithm 12.5 Generating a Markov chain trajectory
Procedure MCMC-Sample (
P (0)(X),
// Initial state distribution
T ,
// Markov chain transition model
T
// Number of time steps
)
1
Sample x(0) from P (0)(X)
2
for t = 1, . . . , T
3
Sample x(t) from T (x(t−1) →X)
4
return x(0), . . . , x(T )
0.25
0.75
0.3
0.5
0.5
0.7
x2
x1
x3
Figure 12.4
A simple Markov chain
12.3.2.3
Stationary Distributions
Intuitively, as the process converges, we would expect P (t+1) to be close to P (t).
Using
equation (12.20), we obtain:
P (t)(x′) ≈P (t+1)(x′) =
X
x∈Val(X)
P (t)(x)T (x →x′).
At convergence, we would expect the resulting distribution π(X) to be an equilibrium relative
to the transition model; that is, the probability of being in a state is the same as the probability
of transitioning into it from a randomly sampled predecessor. Formally:
Deﬁnition 12.3
A distribution π(X) is a stationary distribution for a Markov chain T if it satisﬁes:
stationary
distribution
π(X = x′) =
X
x∈Val(X)
π(X = x)T (x →x′).
(12.21)
A stationary distribution is also called an invariant distribution.2
2. If we view the transition model as a matrix deﬁned as Ai,j = T (xi →xj), then a stationary distribution is an
eigen-vector of the matrix, corresponding to the eigen-value 1. In general, many aspects of the theory of Markov chains
have an algebraic interpretation in terms of matrices and vectors.

510
Chapter 12. Particle-Based Approximate Inference
As we have already discussed, the uniform distribution is a stationary distribution for the
Markov chain of example 12.5. To take a slightly diﬀerent example:
Example 12.7
Figure 12.4 shows an example of a diﬀerent simple Markov chain where the transition probabilities
are less uniform.
By deﬁnition, the stationary distribution π must satisfy the following three
equations:
π(x1)
=
0.25π(x1) + 0.5π(x3)
π(x2)
=
0.7π(x2) + 0.5π(x3)
π(x3)
=
0.75π(x1) + 0.3π(x2),
as well as the one asserting that it is a legal distribution:
π(x1) + π(x2) + π(x3) = 1.
It is straightforward to verify that this system has a unique solution: π(x1) = 0.2, π(x2) = 0.5,
π(x3) = 0.3. For example, the ﬁrst equation asserts that
0.2 = 0.25 · 0.2 + 0.5 · 0.3,
which clearly holds.
In general, there is no guarantee that our MCMC sampling process converges to a stationary
distribution.
Example 12.8
Consider the Markov chain over two states x1 and x2, such that T (x1 →x2) = 1 and T (x2 →
x1) = 1. If P (0) is such that P (0)(x1) = 1, then the step t distribution P (t) has P (t)(x1) = 1 if
t is even, and P (t)(x2) = 1 if t is odd. Thus, there is no convergence to a stationary distribution.
Markov chains such as this, which exhibit a ﬁxed cyclic behavior, are called periodic Markov
periodic Markov
chain
chains.
There is also no guarantee that the stationary distribution is unique: In some chains, the
stationary distribution reached depends on our starting distribution P (0). Situations like this
occur when the chain has several distinct regions that are not reachable from each other. Chains
such as this are called reducible Markov chains.
reducible Markov
chain
We wish to restrict attention to Markov chains that have a unique stationary distribution,
which is reached from any starting distribution P (0). There are various conditions that suﬃce
to guarantee this property. The condition most commonly used is a fairly technical one: that
the chain be ergodic. In the context of Markov chains where the state space Val(X) is ﬁnite,
ergodic Markov
chain
the following condition is equivalent to this requirement:
Deﬁnition 12.4
A Markov chain is said to be regular if there exists some number k such that, for every x, x′ ∈
regular Markov
chain
Val(X), the probability of getting from x to x′ in exactly k steps is > 0.
In our Markov chain of example 12.5, the probability of getting from any state to any state in
exactly 9 steps is greater than 0. Thus, this Markov chain is regular. Similarly, in the Markov
chain of example 12.7, we can get from any state to any state in exactly two steps.
The following result can be shown to hold:

12.3. Markov Chain Monte Carlo Methods
511
Theorem 12.3
If a ﬁnite state Markov chain T is regular, then it has a unique stationary distribution.
Ensuring regularity is usually straightforward. Two simple conditions that together guarantee
regularity in ﬁnite-state Markov chains are as follows. First, it is possible to get from any state to
any state using a positive probability path in the state graph. Second, for each state x, there is a
positive probability of transitioning from x to x in one step (a self-loop). These two conditions
together are suﬃcient but not necessary to guarantee regularity (see exercise 12.12). However,
they often hold in the chains used in practice.
12.3.2.4
Multiple Transition Models
In the case of graphical models, our state space has a factorized structure — each state is an
assignment to several variables. When deﬁning a transition model over this state space, we can
consider a fully general case, where a transition can go from any state to any state. However, it
is often convenient to decompose the transition model, considering transitions that update only
a single component of the state vector at a time, that is, only a value for a single variable.
Example 12.9
Consider an extension to our Grasshopper chain, where the grasshopper lives, not on a line, but in a
two-dimensional plane. In this case, the state of the system is deﬁned via a pair of random variables
X, Y . Although we could deﬁne a joint transition model over both dimensions simultaneously, it
might be easier to have separate transition models for the X and Y coordinate.
In this case, as in several other settings, we often deﬁne a set of transition models, each
with its own dynamics. Each such transition model Ti is called a kernel. In certain cases, the
kernel
diﬀerent kernels are necessary, because no single kernel on its own suﬃces to ensure regularity.
This is the case in example 12.9. In other cases, having multiple kernels simply makes the state
space more “connected” and therefore speeds the convergence to a stationary distribution.
There are several ways of constructing a single Markov chain from multiple kernels. One com-
multi-kernel
Markov chain
mon approach is simply to select randomly between them at each step, using any distribution.
Thus, for example, at each step, we might select one of T1, . . . , Tk, each with probability 1/k.
Alternatively, we can simply cycle over the diﬀerent kernels, taking each one in turn. Clearly,
this approach does not deﬁne a homogeneous chain, since the kernel used in step i is diﬀerent
from the one used in step i + 1. However, we can simply view the process as deﬁning a single
chain T , each of whose steps is an aggregate step, consisting of ﬁrst taking T1, then T2, . . . ,
through Tk.
In the case of graphical models, one approach is to deﬁne a multikernel chain, where we have
a kernel Ti for each variable Xi ∈X. Let X−i = X −{Xi}, and let xi denote an instantiation
to X i. The model Ti takes a state (x−i, xi) and transitions to a state of the form (x−i, x′
i). As
we discussed, we can combine the diﬀerent kernels into a single global model in various ways.
Regardless of the structure of the diﬀerent kernels, we can prove that a distribution is a

stationary distribution for the multiple kernel chain by proving that it is a stationary dis-
tribution (satisﬁes equation (12.21)) for each of individual kernels Ti. Note that each kernel
by itself is generally not ergodic; but as long as each kernel satisﬁes certain conditions
(speciﬁed in deﬁnition 12.5) that imply that it has the desired stationary distribution, we
can combine them to produce a coherent chain, which may be ergodic as a whole. This

512
Chapter 12. Particle-Based Approximate Inference
ability to add new types of transitions to our chain is an important asset in dealing with
the issue of local maxima, as we will discuss.
12.3.3
Gibbs Sampling Revisited
The theory of Markov chains provides a general framework for generating samples from a target
distribution π. In this section, we discuss the application of this framework to the sampling
tasks encountered in probabilistic graphical models. In this case, we typically wish to generate
samples from the posterior distribution P(X | E = e), where X = X −E. Thus, we wish
to deﬁne a chain for which P(X | e) is the stationary distribution. Thus, we deﬁne the states
of the Markov chain to be instantiations x to X −E. In order to deﬁne a Markov chain, we
need to deﬁne a process that transitions from one state to the other, converging to a stationary
distribution π(X), which is the desired posterior distribution P(X | e). As in our earlier
example, we assume that P(X | e) = PΦ for some set of factors Φ that are deﬁned by
reducing the original factors in our graphical model by the evidence e. This reduction allows
us to simplify notation and to discuss the methods in a way that applies both to directed and
undirected graphical models.
Gibbs sampling is based on one yet eﬀective Markov chain for factored state spaces, which
Gibbs chain
is particularly eﬃcient for graphical models. We deﬁne the kernel Ti as follows. Intuitively, we
simply “forget” the value of Xi in the current state and sample a new value for Xi from its
posterior given the rest of the current state. More precisely, let (x−i, xi) be a state in the chain.
We deﬁne:
Ti((x−i, xi) →(x−i, x′
i)) = P(x′
i | x−i).
(12.22)
Note that the transition probability does not depend on the current value xi of Xi, but only
on the remaining state x−i. It is not diﬃcult to show that the posterior distribution PΦ(X) =
P(X | e) is a stationary distribution of this process. (See exercise 12.13.)
Gibbs stationary
distribution
The sampling algorithm for a single trajectory of the Gibbs chain was shown earlier in this
section, in algorithm 12.4. Recall that the Gibbs chain is deﬁned via a set of kernels; we use the
multistep approach to combine them. Thus, the diﬀerent local kernels are taken consecutively;
having changed the value for a variable X1, the value for X2 is sampled based on the new
value. Note that a step in the aggregate chain occurs only once we have executed every local
transition once.
Gibbs sampling is particularly easy to implement in the many graphical models where we
can compute the transition probability P(Xi | x−i) (in line 5 of the algorithm) very eﬃciently.
In particular, as we now show, this distribution can be done based only on the Markov blanket
Markov blanket
of Xi. We show this analysis for a Markov network; the application to Bayesian networks is
straightforward. Recalling deﬁnition 4.4, we have that:
PΦ(X)
=
1
Z
Y
j
φj(Dj)
=
1
Z
Y
j : Xi∈Dj
φj(Dj)
Y
j : Xi̸∈Dj
φj(Dj).
Let xj,−i denote the assignment in x−i to Dj −{Xi}, noting that when Xi ̸∈Dj, xj,−i is a

12.3. Markov Chain Monte Carlo Methods
513
full assignment to Dj. We can now derive:
P(x′
i | x−i)
=
P(x′
i, x−i)
P
x′′
i P(x′′
i , x−i)
=
1
Z
Q
Dj∋Xi φj(x′
i, xj,−i) Q
Dj̸∋Xi φj(x′
i, xj,−i)
1
Z
P
x′′
i
Q
Dj∋Xi φj(x′′
i , xj,−i) Q
Dj̸∋Xi φj(x′′
i , xj,−i)
=
Q
Dj∋Xi φj(x′
i, xj,−i) Q
Dj̸∋Xi φj(xj,−i)
P
x′′
i
Q
Dj∋Xi φj(x′′
i , xj,−i) Q
Dj̸∋Xi φj(xj,−i)
=
Q
Dj∋Xi φj(x′
i, xj,−i)
P
x′′
i
Q
Dj∋Xi φj(x′′
i , xj,−i).
(12.23)
This last expression uses only the factors involving Xi, and depends only on the instantiation
in x−i of Xi’s Markov blanket. In the case of Bayesian networks, this expression reduces to a
formula involving only the CPDs of Xi and its children, and its value, again, depends only on
the assignment in x−i to the Markov blanket of Xi.
Example 12.10
Consider again the Student network of ﬁgure 12.1, with the evidence s1, l0. The kernel for the
variable G is deﬁned as follows. Given a state (i, d, g, s1, l0), we deﬁne T ((i, g, d, s1, l0) →
(i, g′, d, s1, l0)) = P(g′ | i, d, s1, l0). This value can be computed locally, using only the CPDs
that involve G, that is, the CPDs of G and L:
P(g′ | i, d, s1, l0) =
P(g′ | i, d)P(l0 | g′)
P
g′′ P(g′′ | i, d)P(l0 | g′′).
Similarly, the kernel for the variable I is deﬁned to be T ((i, g, d, s1, l0) →(i′, g, d, s1, l0)) =
P(i′ | g, d, s1, l0), which simpliﬁes as follows:
P(i′ | g, d, s1, l0) =
P(i′)P(g | i′, d)P(s1 | i′)
P
i′′ P(i′′)P(g | i′′, d)P(s1 | i′′).
As presented, the algorithm is deﬁned via a sequence of local kernels, where each samples a
single variable conditioned on all the rest. The reason for this approach is computational. As we
showed, we can easily compute the transition model for a single variable given the rest. However,
there are cases where we can simultaneously sample several variables eﬃciently. Speciﬁcally,
assume we can partition the variables X into several disjoint blocks of variables X1, . . . , Xk,
such that we can eﬃciently sample xi from PΦ(Xi | x1, . . . , xi−1, xi+1, . . . , xk). In this case,
we can modify our Gibbs sampling algorithm to iteratively sample blocks of variables, rather
than individual variables, thereby taking much “longer-range” transitions in the state space in
a single sampling step. Here, like in Gibbs sampling, we deﬁne the algorithm to be producing
a new sample only once all blocks have been resampled. This algorithm is called block Gibbs.
block Gibbs
sampling
Note that standard Gibbs sampling is a special case of block Gibbs sampling, with the blocks
corresponding to individual variables.
Example 12.11
Consider the Bayesian network induced by the plate model of example 6.11. Here, we generally
have n students, each with a variable representing his or her intelligence, and m courses, each

514
Chapter 12. Particle-Based Approximate Inference
G1,1
I1
D1
I3
I4
I2
D2
G2,2
G3,1
G3,2
G4,2
Figure 12.5
A Bayesian network with four students, two courses, and ﬁve grades
with a variable representing its diﬃculty. We also have a set of grades for students in classes (not
necessarily a grade for each student in every class). Using an abbreviated notation, we have a
set of variables I1, . . . , In for the students (where each Ij = I(sj)), D = {D1, . . . , Dℓ} for the
courses, and G = {Gj,k} for the grades, where each variable Gj,k has the parents Ij and Dk. See
ﬁgure 12.5 for an example with n = 4 and ℓ= 2. Let us assume that we observe the grades, so
that we have evidence G = g. An examination of active paths shows that the diﬀerent variables Ij
are conditionally independent given an assignment d to D. Thus, given D = d, G = g, we can
eﬃciently sample all of the I variables as a block by sampling each Ij independently of the others.
Similarly, we can sample all of the D variables as a block given an assignment I = i, G = g.
Thus, we can alternate steps where in one we sample i[m] given g and d[m], and in the other we
sample d[m + 1] given g and i[m].
In this example, we can easily apply block Gibbs because the variables in each block are
marginally independent given the variables outside the block. This independence property allows
us to compute eﬃciently the conditional distribution PΦ(Xi | x1, . . . , xi−1, xi+1, . . . , xk), and
to sample from it. Importantly, however, full independence is not essential: we need only have
the property that the block-conditional distribution can be eﬃciently manipulated. For example,
in a grid-structured network, we can easily deﬁne our blocks to consist of separate rows or of
separate columns. In this case, the structure of each block is a simple chain-structured network;
we can easily compute the conditional distribution of one row given all the others, and sample
from it (see exercise 12.3).
We note that the Gibbs chain is not necessarily regular, and might not converge to a unique
stationary distribution.
Example 12.12
Consider a simple network that consists of a single v-structure X →Z ←Y , where the variables
are all binary, X and Y are both uniformly distributed, and Z is the deterministic exclusive or of
X and Y (that is, Z = z1 iﬀX ̸= Y ). Consider applying Gibbs sampling to this network with
the evidence z1. The true posterior assigns probability 1/2 to each of the two states x1, y0, z1 and
x0, y1, z1. Assume that we start in the ﬁrst of these two states. In this case, P(X | y0, z1) assigns
probability 1 to x1, so that the X transition leaves the value of X unchanged. Similarly, the Y
transition leaves the value of Y unchanged. Therefore, the chain will simply stay at the initial state
forever, and it will never sample from the other state. The analogous phenomenon occurs for the
other starting state. This chain is an example of a reducible Markov chain.
However, this chain is guaranteed to be regular whenever the distribution is positive, so that
every value of Xi has positive probability given an assignment x−i to the remaining variables.

12.3. Markov Chain Monte Carlo Methods
515
Theorem 12.4
Let H be a Markov network such that all of the clique potentials are strictly positive. Then the
Gibbs-sampling Markov chain is regular.
The proof is not diﬃcult, and is left as an exercise (exercise 12.20).
Positivity is, however, not necessary; there are many examples of nonpositive distributions
where the Gibbs chain is regular.
Importantly, however, even chains that are regular may require a long time to mix, that is, get
mixing
close to the stationary distribution. In this case, instances generated from early in the sampling
process will not be representative of the desired stationary distribution.
12.3.4
A Broader Class of Markov Chains ⋆
As we discussed, the use of MCMC methods relies on the construction of a Markov chain that
has the desired properties: regularity, and the target stationary distribution. In the previous
section, we described the Gibbs chain, a simple Markov chain that is guaranteed to have these
properties under certain assumptions. However, Gibbs sampling is applicable only in certain
circumstances; in particular, we must be able to sample from the distribution P(Xi | x−i).
Although this sampling step is easy for discrete graphical models, in continuous models, the
conditional distribution may not be one that has a parametric form that allows sampling, so
that Gibbs is not applicable.
Even more important, the Gibbs chain uses only very local moves over the state space:

moves that change one variable at a time. In models where variables are tightly cor-
related, such moves often lead from states whose probability is high to states whose
probability is very low. In this case, the high-probability states will form strong basins
of attraction, and the chain will be very unlikely to move away from such a state; that is,
the chain will mix very slowly. In this case, we often want to consider chains that allow
a broader range of moves, including much larger steps in the space. The framework we
develop in this section allows us to construct a broad family of chains in a way that guarantees
the desired stationary distribution.
12.3.4.1
Detailed Balance
Before we address the question of how to construct a Markov chain with a particular stationary
distribution, we address the question of how to verify easily that our Markov chain has the
desired stationary distribution. Fortunately, we can deﬁne a test that is local and easy to check,
and that suﬃces to characterize the stationary distribution. As we will see, this test also provides
us with a simple method for constructing an appropriate chain.
Deﬁnition 12.5
A ﬁnite-state Markov chain T is reversible if there exists a unique distribution π such that, for all
reversible Markov
chain
x, x′ ∈Val(X):
π(x)T (x →x′) = π(x′)T (x′ →x).
(12.24)
This equation is called the detailed balance.
detailed balance

516
Chapter 12. Particle-Based Approximate Inference
The product π(x)T (x →x′) represents a process where we pick a starting state at random
according to π, and then take a random transition from the chosen state according to the
transition model. The detailed balance equation asserts that, using this process, the probability
of a transition from x to x′ is the same as the probability of a transition for x′ to x.
Reversibility implies that π is a stationary distribution of T , but not necessarily that the chain
will converge to π (see example 12.8). However, if T is regular, then convergence is guaranteed,
and the reversibility condition provides a simple characterization of its stationary distribution:
Proposition 12.3
If T is regular and it satisﬁes the detailed balance equation relative to π, then π is the unique
stationary distribution of T .
The proof is left as an exercise (exercise 12.14).
Example 12.13
We can test this proposition on the Markov chain of ﬁgure 12.4. Our detailed balance equation for
the two states x1 and x3 asserts that
π(x1)T (x1 →x3) = π(x3)T (x3 →x1).
Testing this equation for the stationary distribution π described in example 12.7, we have:
0.2 · 0.75 = 0.3 · 0.5 = 0.15.
The detailed balance equation can also be applied to multiple kernels.
If each kernel Ti
satisﬁes the detailed balance equation relative to some stationary distribution π, then so does
the mixture transition model T (see exercise 12.16). The application to the multistep transition
model T is also possible, but requires some care (see exercise 12.17).
12.3.4.2
Metropolis-Hastings Algorithm
The reversibility condition gives us a condition for verifying that our Markov chain has the
desired stationary distribution. However, it does not provide us with a constructive approach
for producing such a Markov chain. The Metropolis-Hastings algorithm is a general construction
Metropolis-
Hastings
algorithm
that allows us to build a reversible Markov chain with a particular stationary distribution.
Unlike the Gibbs chain, the algorithm does not assume that we can generate next-state
samples from a particular target distribution. Rather, it uses the idea of a proposal distribution
proposal
distribution
that we have already seen in the case of importance sampling.
As for importance sampling, the proposal distribution in the Metropolis-Hastings algorithm
is intended to deal with cases where we cannot sample directly from a desired distribution.
In the case of a Markov chain, the target distribution is our next-state sampling distribution
at a given state. We would like to deal with cases where we cannot sample directly from this
target. Therefore, we sample from a diﬀerent distribution — the proposal distribution — and
then correct for the resulting error. However, unlike importance sampling, we do not want to
keep track of importance weights, which are going to decay exponentially with the number
of transitions, leading to a whole slew of problems. Therefore, we instead randomly choose
whether to accept the proposed transition, with a probability that corrects for the discrepancy
between the proposal distribution and the target.
More precisely, our proposal distribution T Q deﬁnes a transition model over our state space:
For each state x, T Q deﬁnes a distribution over possible successor states in Val(X), from

12.3. Markov Chain Monte Carlo Methods
517
which we select randomly a candidate next state x′. We can either accept the proposal and
transition to x′, or reject it and stay at x. Thus, for each pair of states x, x′ we have an
acceptance probability A(x →x′). The actual transition model of the Markov chain is then:
acceptance
probability
T (x →x′)
=
T Q(x →x′)A(x →x′)
x ̸= x′
T (x →x)
=
T Q(x →x) + P
x′̸=x T Q(x →x′)(1 −A(x →x′)).
(12.25)
By using a proposal distribution, we allow the Metropolis-Hastings algorithm to be applied even
in cases where we cannot directly sample from the desired next-state distribution; for example,
where the distribution in equation (12.22) is too complex to represent. The choice of proposal
distribution can be arbitrary, so long as it induces a regular chain. One simple choice in discrete
factored state spaces is to use a multiple transition model, where T Q
i
is a uniform distribution
over the values of the variable Xi.
Given a proposal distribution, we can use the detailed balance equation to select the accep-
tance probabilities so as to obtain the desired stationary distribution. For this Markov chain, the
detailed balance equations assert that, for all x ̸= x′,
π(x)T Q(x →x′)A(x →x′) = π(x′)T Q(x′ →x)A(x′ →x).
We can verify that the following acceptance probabilities satisfy these equations:
A(x →x′) = min

1, π(x′)T Q(x′ →x)
π(x)T Q(x →x′)

,
(12.26)
and hence that the chain has the desired stationary distribution:
Theorem 12.5
Let T Q be any proposal distribution, and consider the Markov chain deﬁned by equation (12.25)
and equation (12.26). If this Markov chain is regular, then it has the stationary distribution π.
The proof is not diﬃcult, and is left as an exercise (exercise 12.15).
Let us see how this construction process works.
Example 12.14
Assume that our proposal distribution T Q is given by the chain of ﬁgure 12.4, but that we want to
sample from a stationary distribution π′ where: π′(x1) = 0.6, π′(x2) = 0.3, and π′(x3) = 0.1.
To deﬁne the chain, we need to compute the acceptance probabilities. Applying equation (12.26), we
obtain, for example, that:
A(x1 →x3)
=
min

1, π′(x3)T Q(x3 →x1)
π′(x1)T Q(x1 →x3)

= min

1, 0.1 · 0.5
0.6 · 0.75

= 0.11
A(x3 →x1)
=
min

1, π′(x1)T Q(x1 →x3)
π′(x3)T Q(x3 →x1)

= min

1, 0.6 · 0.75
0.1 · 0.5

= 1.
We can now easily verify that the stationary distribution of the chain resulting from equation (12.25)
and these acceptance probabilities gives the desired stationary distribution π′.
The Metropolis-Hastings algorithm has a particularly natural implementation in the context
of graphical models.
Each local transition model Ti is deﬁned via an associated proposal

518
Chapter 12. Particle-Based Approximate Inference
distribution T Qi
i
. The acceptance probability for this chain has the form
A(x−i, xi →x−i, x′
i)
=
min
"
1, π(x−i, x′
i)T Qi
i
(x−i, x′
i →x−i, xi)
π(x−i, xi)T Qi
i
(x−i, xi →x−i, x′
i)
#
=
min
"
1, PΦ(x′
i, x−i)
PΦ(xi, x−i)
T Qi
i
(x−i, x′
i →x−i, xi)
T Qi
i
(x−i, xi →x−i, x′
i)
#
.
The proposal distributions are usually fairly simple, so it is easy to compute their ratios. In
the case of graphical models, the ﬁrst ratio can also be computed easily:
PΦ(x′
i, x−i)
PΦ(xi, x−i)
=
PΦ(x′
i | x−i)PΦ(x−i)
PΦ(xi | x−i)PΦ(x−i)
=
PΦ(x′
i | x−i)
PΦ(xi | x−i).
As for Gibbs sampling, we can use the observation that each variable Xi is conditionally
independent of the remaining variables in the network given its Markov blanket. Letting U i
denote MBK(Xi), and ui = (x−i)⟨U i⟩, we have that:
PΦ(x′
i | x−i)
PΦ(xi | x−i)
=
PΦ(x′
i | ui)
PΦ(xi | ui).
This expression can be computed locally and eﬃciently, based only on the local parameterization
of Xi and its Markov blanket (exercise 12.18).
The similarity to the derivation of Gibbs sampling is not accidental. Indeed, it is not diﬃcult to
show that Gibbs sampling is simply a special case of Metropolis-Hastings, one with a particular
choice of proposal distribution (exercise 12.19).
The Metropolis-Hastings construction allows us to produce a Markov chain for an arbitrary
stationary distribution. Importantly, however, we point out that the key theorem still requires that
the constructed chain be regular. This property does not follow directly from the construction.
In particular, the exclusive-or network of example 12.12 induces a nonregular Markov chain
for any Metropolis-Hastings construction that uses a local proposal distribution — one that
proposes changes to only a single variable at a time. In order to obtain a regular chain for this
example, we would need a proposal distribution that allows simultaneous changes to both X
and Y at a single step.
12.3.5
Using a Markov Chain
So far, we have discussed methods for deﬁning Markov chains that induce the desired stationary
distribution. Assume that we have constructed a chain that has a unique stationary distribution
π, which is the one from which we wish to sample. How do we use this chain to answer queries?
A naive answer is straightforward. We run the chain using the algorithm of algorithm 12.5 until
it converges to the stationary distribution (or close to it). We then collect a sample from π.
We repeat this process once for each particle we want to collect. The result is a data set D
consisting of independent particles, each of which is sampled (approximately) from the stationary
distribution π. The analysis of section 12.1 is applicable to this setting, so we can provide tight

12.3. Markov Chain Monte Carlo Methods
519
bounds on the number of samples required to get estimators of a certain quality. Unfortunately,
matters are not so straightforward, as we now discuss.
12.3.5.1
Mixing Time
A critical gap in this description of the MCMC algorithm is a speciﬁcation of the burn-in time
burn-in time
T — the number of steps we take until we collect a sample from the chain. Clearly, we want
to wait until the state distribution is reasonably close to π. More precisely, we want to ﬁnd a
T that guarantees that, regardless of our starting distribution P (0), P (T ) is within some small
ϵ of π. In this context, we usually use variational distance (see section A.1.3.3) as our notion of
“within ϵ.”
Deﬁnition 12.6
Let T be a Markov chain. Let Tϵ be the minimal T such that, for any starting distribution P (0),
we have that:
IDvar(P (T ); π) ≤ϵ.
Then Tϵ is called the ϵ-mixing time of T .
mixing time
In certain cases, the mixing time can be extremely long. This situation arises in chains where
the state space has several distinct regions each of which is well connected, but where transitions
between regions are low probability. In particular, we can estimate the extent to which the chain
allows mixing using the following quantity:
Deﬁnition 12.7
Let T be a Markov chain transition model and π its stationary distribution. The conductance of
conductance
T is deﬁned as follows:
min
S⊂Val(X)
0 < π(S) ≤1/2
P(S ; Sc)
π(S)
,
where π(S) is the probability assigned by the stationary distribution to the set of states S, Sc =
Val(X) −S, and
P(S ; Sc) =
X
x∈S,x′∈Sc
T (x →x′).
Intuitively, P(S ; Sc) is the total “bandwidth” for transitioning from S to its complement.
In cases where the conductance is low, there is some set of states S where, once in S, it is
very diﬃcult to transition out of it. Figure 12.6 visualizes this type of situation, where the only
transition between S = {x1, x2, x3} and its complement is the dashed transition between x2
and x4, which has a very low probability. In cases such as this, if we start in a state within S,
the chain is likely to stay in S and to take a very long time before exploring other regions of
the state space. Indeed, it is possible to provide both upper and lower bounds on the mixing
rate of a Markov chain in terms of its conductance.
In the context of Markov chains corresponding to graphical models, chains with low conduc-
tance are most common in networks that have deterministic or highly skewed parameterization.

520
Chapter 12. Particle-Based Approximate Inference
x2
x4
x7
x5
x6
x1
x3
Figure 12.6
Visualization of a Markov chain with low conductance
In fact, as we saw in example 12.12, networks with deterministic CPDs might even lead to
reducible chains, where diﬀerent regions are entirely disconnected. However, even when the dis-
tribution is positive, we might still have regions that are connected only by very low-probability
transitions. (See exercise 12.21.)
There are methods for providing tight bounds on the ϵ-mixing time of a given Markov chain.
These methods are based on an analysis of the transition matrix between the states in the
Markov chain.3 Unfortunately, in the case of graphical models, an exhaustive enumeration of
the exponentially many states is precisely what we wish to avoid. (If this enumeration were
feasible, we would not have to resort to approximate inference techniques in the ﬁrst place.)
Alternatively, there is a suite of indirect techniques that allow us to provide bounds on the
mixing time for some general class of chains.
However, the application of these methods
to each new class of chains requires a separate and usually quite sophisticated mathematical
analysis. As of yet, there is no such analysis for the chains that are useful in the setting of
graphical models. A more common approach is to use a variety of heuristics to try to evaluate
the extent to which a sample trajectory has “mixed.” See box 12.B for some further discussion.
12.3.5.2
Collecting Samples
The burn-in time for a large Markov chain is often quite large.
Thus, the naive algorithm
described above has to execute a large number of sampling steps for every usable sample.
However, a key observation is that, if x(t) is sampled from π, then x(t+1) is also sampled from
π. Thus, once we have run the chain long enough that we are sampling from the stationary
distribution (or a distribution close to it), we can continue generating samples from the same
trajectory and obtain a large number of samples from the stationary distribution.
More formally, assume that we use x(0), . . . , x(T ) as our burn-in phase, and then collect M
samples D = {x[1], . . . , x[M]} from the stationary distribution. Most simply, we might collect
M consecutive samples, so that x[m] = x(T +m), for m = 1, . . . , M. If x(T +1) is sampled
from π, then so are all of the samples in D. Thus, if our chain has mixed by the time we collect
3. Speciﬁcally, they involve computing the second largest eigen-value of the matrix.

12.3. Markov Chain Monte Carlo Methods
521
our ﬁrst sample, then for any function f,
ˆIED(f) = 1
M
M
X
m=1
f(x[m], e)
is an unbiased estimator for IEπ(X)[f(X, e)].
estimator
How good is this estimator? As we discussed in appendix A.2.1, the quality of an unbiased
estimator is measured by its variance: the lower the variance, the higher the probability that
the estimator is close to its mean. In theorem A.2, we showed an analysis of the variance of an
estimator obtained from M independent samples. Unfortunately, we cannot apply that analysis
in this setting. The key problem, of course, is that consecutive samples from the same trajectory
are correlated. Thus, we cannot expect the same performance as we would from M independent
samples from π. More formally, the variance of the estimator is signiﬁcantly higher than that of
an estimator generated by M independent samples from π, as discussed before.
Example 12.15
Consider the Gibbs chain for the deterministic exclusive-or network of example 12.12, and assume
we compute, for a given run of the chain, the fraction of states in which x1 holds in the last 100
states traversed by the chain. A chain started in the state x1, y0 would have that 100/100 of the
states have x1, whereas a chain started in the state x0, y1 would have that 0/100 of the states have
x1. Thus, the variance of the estimator is very high in this case.
One can formalize this intuition by the following generalization of the central limit theorem
central limit
theorem
that applies to samples collected from a Markov chain:
Theorem 12.6
Let T be a Markov chain and X[1], . . . , X[M] a set of samples collected from T at its stationary
distribution P. Then, since M −→∞:

ˆIED(f) −IEX∼P [f(X)]

−→N
 0; σ2
f

where
σ2
f = VVarX∼T [f(X)] + 2
∞
X
ℓ=1
CCovT [f(X[m]); f(X[m + ℓ])] < ∞.
The terms in the summation are called autocovariance terms, since they measure the covariance
autocovariance
between samples from the chain, taken at diﬀerent lags. The stronger the correlations between
diﬀerent samples, the larger the autocovariance terms, the higher the variance of our estimator.
This result is consistent with the behavior we discussed in example 12.12.
We want to use theorem 12.6 in order to assess the quality of our estimator. In order to do
so, we need to estimate the quantity σ2
f. We can estimate the variance from our empirical data
using the standard estimator:
VVarX∼T [f(X)] ≈
1
M −1
" M
X
m=1

f(X) −ˆIED(f)
2
#
.
(12.27)
To estimate the autocovariance terms from the empirical data, we compute:
CCovT [f(X[m]); f(X[m + ℓ])] ≈
1
M −ℓ
M−ℓ
X
m=1
(f(X[m] −ˆIED(f))(f(X[m + ℓ] −ˆIED(f)).

522
Chapter 12. Particle-Based Approximate Inference
(12.28)
At ﬁrst glance, theorem 12.6 suggests that the variance of the estimate could be reduced if
the chain is allowed a suﬃcient number of iterations between sample collections. Thus, having
collected a particle x(T ), we can let the chain run for a while, and collect a second particle
x(T +d) for some appropriate choice of d. For d large enough, x(T ) and x(T +d) are only slightly
correlated, reducing the correlation in the preceding theorem.
However, this approach is suboptimal for various reasons.
First, the time d required for
“forgetting” the correlation is clearly related to the mixing time of the chain. Thus, chains that
are slow to mix initially also require larger d in order to produce close-to-independent particles.
Nevertheless, the samples do come from the correct distribution for any value of d, and hence it
is often better to compromise and use a shorter d than it is to use a shorter burn-in time T. This
method thus allows us to collect a larger number of usable particles with fewer transitions of the
Markov chain. Indeed, although the samples between x(T ) and x(T +d) are not independent

samples, there is no reason to discard them. That is, one can show that using all of the
samples x(T ), x(T +1), . . . , x(T +d) produces a provably better estimator than using just
the two samples x(T ) and x(T +d): our variance is always no higher if we use all of the
samples we generated rather than a subset. Thus, the strategy of picking only a subset
of the samples is useful primarily in settings where there is a signiﬁcant cost associated
with using each sample (for example, the evaluation of f is costly), so that we might want
to reduce the overall number of particles used.
Box 12.B — Skill: MCMC in Practice. A key question when using a Markov chain is evaluating
the time required for the chain to “mix” — that is, approach the stationary distribution. As we
discussed, no general-purpose theoretical analysis exists for the mixing time of graphical models.
However, we can still hope to estimate the extent to which a sample trajectory has “forgotten” its
origin. Recall that, as we discussed, the most common problem with mixing arises when the state
space consists of several regions that are connected only by low-probability transitions. If we start
the chain in a state in one of these regions, it is likely to spend some amount of time in that same
region before transitioning to another region. Intuitively, the states sampled in the initial phase are
clearly not from the stationary distribution, since they are strongly correlated with our initial state,
which is arbitrary. However, later in the trajectory, we might reach a state where the current state
is as likely to have originated in any initial state. In this case, we might consider the chain to have
mixed.
Diagnosing convergence of a Markov chain Monte Carlo method is a notoriously hard problem.
The chain may appear to have converged simply by spending a large number of iterations in a
particular mode due to low conductance between modes. However, there are approaches that can
tell us if a chain has not converged.
One technique is based directly on theorem 12.6. In particular, we can compute the ratio ρℓof the
estimated autocovariance in equation (12.28) to the estimated variance in equation (12.27). This ratio
is known as the autocorrelation of lag ℓ; it provides a normalized estimate of the extent to which the
chain has mixed in ℓsteps. In practice, the autocorrelation should drop oﬀexponentially with the
length of the lag, and one way to diagnose a poorly mixing chain is to observe high autocorrelation
at distant lags. Note, however, that the number of samples available for computing autocorrelation
decreases with lag, leading to large variance in the autocorrelation estimates at large lags.

12.3. Markov Chain Monte Carlo Methods
523
A diﬀerent technique uses the observation that multiple chains sampling the same distribution
should, upon convergence, all yield similar estimates. In addition, estimates based on a complete
set of samples collected from all of the chains should have variance comparable to variance in
each of the chains. More formally, assume that K separate chains are each run for T + M steps
starting from a diverse set of starting points. After discarding the ﬁrst T samples from each chain,
let Xk[m] denote a sample from chain k after iteration T + m. We can now compute the B
(between-chains) and W (within-chain) variances:
¯fk
=
1
M
M
X
m=1
f(Xk[m])
¯f
=
1
K
K
X
k=1
¯fk
B
=
M
K −1
K
X
k=1
( ¯fk −¯f)2
W
=
1
K
1
M −1
K
X
k=1
M
X
m=1
 f(Xk[m]) −¯fk
2 .
The expression V = M−1
M W + 1
M B can now be shown to overestimate the variance of our estimate
of f based on the collected samples. In the limit of M −→∞, both W and V converge to the
true variance of the estimate. One measure of disagreement between chains is given by ˆR =
q
V
W .
If the chains have not all converged to the stationary distribution, this estimate will be high. If
this value is close to 1, either the chains have all converged to the true distribution, or the starting
points were not suﬃciently dispersed and all of the chains have converged to the same mode or
a set of modes. We can use this strategy with multiple diﬀerent functions f in order to increase
our conﬁdence that our chain has mixed. We can, for example, use indicator functions of various
events, as well as more complex functions of multiple variables.
Overall, although the strategy of using only a single chain produces more viable particles using
lower computational cost, there are still signiﬁcant advantages to the multichain approach. First, by
starting out in very diﬀerent regions of the space, we are more likely to explore a more representative
subset of states. Second, the use of multiple chains allows us to evaluate the extent to which our
chains are mixing. Thus, to summarize, a good strategy for using a Markov chain in practice

is a hybrid approach, where we run a small number of chains in parallel for a reasonably
long time, using their behavior to evaluate mixing. After the burn-in phase, we then
use the existence of multiple chains to estimate convergence. If mixing appears to have
occurred, we can use each of our chains to generate multiple particles, remembering that
the particles generated in this fashion are not independent.
12.3.5.3
Discussion
MCMC methods have many advantages over other methods.
Unlike the global approximate
inference methods of the previous chapter, they can, at least in principle, get arbitrarily close

524
Chapter 12. Particle-Based Approximate Inference
to the true posterior. Unlike forward sampling methods, these methods do not degrade when
the probability of the evidence is low, or when the posterior is very diﬀerent from the prior.
Furthermore, unlike forward sampling, MCMC methods apply to undirected models as well as
to directed models. As such, they are an important component in the suite of approximate
inference techniques.
However, MCMC methods are not generally an out-of-the-box solution for dealing with in-
ference in complex models. First, the application of MCMC methods leaves many options that
need to be speciﬁed: the proposal distribution, the number of chains to run, the metrics for
evaluating mixing, techniques for determining the delay between samples that would allow them
to be considered independent, and more. Unfortunately, at this point, there is little theoretical
analysis that can help answer these questions for the chains that are of interest to us. Thus, the
application of Markov chains is more of an art than a science, and it often requires signiﬁcant
experimentation and hand-tuning of parameters.
Second, MCMC methods are only viable if the chain we are using mixes reasonably quickly.
Unfortunately, many of the chains derived from real-world graphical models frequently have
multimodal posterior distributions, with slow mixing between the modes.
For such chains,
the straightforward MCMC methods described in this chapter are unlikely to work. In such
cases, diagnostics such as the ones described in box 12.B can be used to determine that the
chain is not mixing, and better methods must then be applied. The key to improving the

convergence of a Markov chain is to introduce transitions that take larger steps in the
space, allowing the chain to move more rapidly between modes, and thereby to better
explore the space. The best strategy is often to analyze the properties of the posterior
landscape of interest, and to construct moves that are tailored for this speciﬁc space. (See,
for example, exercise 12.23.) Fortunately, the ability to mix diﬀerent reversible kernels
within a single chain (as discussed in section 12.3.4) allows us to introduce a variety of
long-range moves while still maintaining the same target posterior.
In addition to the use of long-range steps that are speciﬁcally designed for particular (classes
of) chains, there are also some general-purpose methods that try to achieve that goal. The block
Gibbs approach (section 12.3.3) is an instance of this general class of methods. Another strategy
uses the same ideas in simulated annealing to improve convergence of local search to a better
simulated
annealing
optimum.
Here, we can deﬁne an intermediate distribution parameterized by a temperature
temperature
parameter
parameter T: T:
˜PT (X) ∝exp{−1
T log ˜P(X)}.
This distribution is similar to our original target distribution ˜P. At a low temperature of T = 1,
this equation yields the original target distribution. But as the temperature increases, modes
become broader and merge, reducing the multimodality of the distribution and increasing its
mixing rate. We can now deﬁne various methods that use a combination of related chains
running at diﬀerent temperatures. At a high level, the higher-temperature chain can be viewed
as proposing a step, which we can accept or reject using the acceptance probability of our true
target distribution. (See section 12.7 for references to some of these more advanced methods.) In
eﬀect, these approaches use the higher-temperature chains to deﬁne a set of larger steps in the
space, thereby providing a general-purpose method for achieving more rapid movement between
multiple modes. However, this generality comes at the computational cost of running parallel

12.3. Markov Chain Monte Carlo Methods
525
B
A
Y
C
X
var
A, B, C, X, Y, mu, tau, p[2,3], q;
p = . . .
A ∼dbern(0.3)
B ∼dcat(p[A,1:3])
X ∼dnorm(-1,0.25)
mu <- 3*X+B∧2
tau <- 1/X∧2
Y ∼dnorm(mu,tau)
logit(q) <- 4*X + 2
C ∼dbern(q)
(a)
(b)
Figure 12.C.1 — Example of bugs model speciﬁcation (a) A simple hybrid Bayesian network. (b) A
bugs deﬁnition of a probabilistic model over this network.
chains; thus, if we can understand our speciﬁc posterior well enough to construct specialized
operators that move between modes, that often provides a more eﬀective solution.
Box 12.C — Case Study: The bugs System. One of the main advantages of MCMC methods is
their broad applicability to a very general class of networks. Not only do they apply (at least in
principle) to any discrete network, regardless of its complexity, they also generalize fairly simply to
continuous variables (see section 14.5.3). One very useful system that exploits this generality is the
bugs system, developed by Thomas et al. (1992). This system provides a general-purpose language
for representing a broad range of probabilistic models and uses MCMC to run inference over these
models.
The bugs system provides a programming-language-based representation of a probabilistic model.
BUGS system
The model deﬁnes a joint distribution over a set of random variables. Variables can be deﬁned as
functions of each other; these functions can be deterministic functions, or stochastic functions utiliz-
ing a rich set of predeﬁned distributions. For example, consider the simple Bayesian network shown
in ﬁgure 12.C.1a, where A, B, C are discrete and X, Y are continuous. One possible probabilistic
model can be written in bugs using the commands shown in ﬁgure 12.C.1b. This model deﬁnes:
A to be a binary-valued variable, with P(a1) = 0.3; B is a 3-valued variable that depends on
A, whose CPT is deﬁned in the matrix P; X is a Gaussian random variable with mean −1 and
precision (inverse variance) 0.25; Y is a conditional Gaussian whose mean depends on X and B
and whose precision also depends on X; and C is a logistic function of 4X + 2. Even in this very
simple example, we can see that the bugs language provides a rich language for encoding diﬀerent
families of functional and stochastic dependencies between variables.
Given a probabilistic model deﬁned in this way, the bugs system can instantiate evidence for some

526
Chapter 12. Particle-Based Approximate Inference
of the variables (for example, by reading their values from a ﬁle) and then perform inference over the
model by running various MCMC algorithms. The system analyzes the parametric form specifying
the distribution of the diﬀerent variables, and it selects an appropriate sampling algorithm to use.
The user speciﬁes the number of sampling iterations to perform, and which variables are to be
monitored — their values are to be stored during the MCMC iterations. We can then compute such
values as the mean and standard deviation of these monitored variables. The system also provides
various methods to help detect convergence of the MCMC runs (see also box 12.B).
Overall, the bugs tool provides a general-purpose and highly ﬂexible framework for specifying
and reasoning with probabilistic models. Its ability to provide such a high level of expression power
rests on the generality of MCMC as an inference method, and its applicability to a very broad range
of distributions (broader than any other inference method currently available).
12.4
Collapsed Particles
So far, we have restricted our attention to methods that use as their particles only instantiations
ξ to all the network variables. Clearly, covering an exponentially large state space with a small
number of instantiations is diﬃcult, and it often takes a large number of full particles to obtain
reasonable estimates. One approach for improving the performance of particle-based methods
is to use as particles partial assignments to some subset of the network variables, combined
with some type of closed-form representation of a distribution over the rest.
More precisely, assume that we partition X into two subsets: Xp — the variables whose
assignment deﬁnes the particle, and Xd — the variables over which we will maintain a closed-
form distribution. Then collapsed particles consist of an instantiation xp ∈Val(Xp), coupled
collapsed
particles
with some representation of the distribution P(Xd | xp, e). The particle is “collapsed” because
some of the variables are not assigned but rather summarized using a distribution. Collapsed
particles are also known as Rao-Blackwellized particles.
Assume that we want to estimate an expectation of some function f(ξ) relative to our
posterior distribution P(Xp, Xd | e). We now have:
IEP (ξ|e)[f(ξ)]
=
X
xp,xd
P(xp, xd | e)f(xp, xd, e)
=
X
xp
P(xp | e)
X
xd
P(xd | xp, e)f(xp, xd, e)
=
X
xp
P(xp | e)
 IEP (Xd|xp,e)[f(xp, Xd, e)]

.
(12.29)
We can use the samples xp[m] to approximate any expectation relative to the distribution
P(Xp | e), using the techniques described above.
In particular, we can approximate the
expectation of the expression in parentheses — an expression that is itself an expectation.
In the case of collapsed particles, we assume that the internal expectation can be computed
(or approximated) eﬃciently. As we will discuss, we can explicitly represent the distribution
P(Xd | xp, e) as a graphical model, using constructions such as the reduced Markov network
of section 4.2.3. We thus have a hybrid approach where we generate samples xp from Xp

12.4. Collapsed Particles
527
and perform exact inference on Xd given xp. Thus, this approach deﬁnes a spectrum: When
Xp = X, collapsed particles are simply full particles, and we are simply applying the methods
deﬁned earlier in this chapter; when Xp = ∅, we have a single particle whose associated
distribution is our original network, so that we are back in the regime of exact inference. We
note that, in some cases, the network might not be suﬃciently simple for exact inference.
However, it might be amenable to some other form of approximation — for example, one of the
methods we discuss in chapter 11. Intuitively, the fewer variables we sample (keep in Xp),

the larger the part of the probability mass that we cover using each collapsed particle
xp[m]. From an alternative perspective, we are performing an exact computation for the
expectation relative to Xd, thereby eliminating any contribution it makes to the bias or
the variance of the estimator. Thus, if |Xp| is fairly small, we can obtain much better
estimates for the distribution using signiﬁcantly fewer particles.
In this section, we describe extensions of the approaches discussed earlier in this chapter to
the case of collapsed particles.
12.4.1
Collapsed Likelihood Weighting ⋆
We begin by describing a collapsed extension to likelihood weighting. We ﬁrst describe the
algorithm generally, from the perspective of normalized importance sampling. We then consider
a speciﬁc application that is a direct extension to the full-particle version of likelihood weighting.
12.4.1.1
Collapsed Importance Sampling
Recall that, in importance sampling, we generate our samples from an alternative proposal
distribution Q, and we compensate for the discrepancy by associating with each particle a
weight w[m]. In the case of collapsed particles, we are generating speciﬁc particles only for the
variables in Xp, so that Q would be a distribution over xp. We thus generate a data set
D = {(xp[m], w[m], P(Xd | xp[m], e))}M
m=1,
where each xp[m] is sampled from Q.
Here we will discuss both the choice of proposal
distribution Q and the computation of the weights w[m].
Our goal is to estimate the expectation of equation (12.29).
In eﬀect, we are estimat-
ing the expectation of a new function g, which represents the internal expectation: g =
IEP (Xd|xp,e)[f(xp, Xd, e)]. Using normalized importance sampling, we estimate this expec-
tation as:
ˆIED(f) =
PM
m=1 w[m]
 IEP (Xd|xp[m],e)[f(xp[m], Xd, e)]

PM
m=1 w[m]
.
(12.30)
Example 12.16
Consider the Extended Student network, repeated in ﬁgure 12.7a, with the evidence d1, h0. Assume
that we choose to partition the variables as follows: Xp = {D, G}, and Xd = {C, I, S, L, J, H}.
In this case, each particle deﬁnes an assignment (d, g) to the variables D, G.
Assuming that
our algorithm follows the template of full-particle likelihood weighting, we would ensure that our
proposal distribution Q ascribes only positive probability to particles (d1, g) that are compatible
with our evidence d1. Each such particle is also associated with a distribution P(C, I, S, L, H |
g, d1, h0). The reduced Markov network shown in ﬁgure 12.7b represents this distribution.

528
Chapter 12. Particle-Based Approximate Inference
(a)
Letter
Job
Happy
Coherence
SAT
Intelligence
(b)
Grade
Letter
Job
Happy
Coherence
SAT
Intelligence
Difﬁculty
Figure 12.7
Networks illustrating collapsed importance sampling: (a) The Extended-Student Bayesian
network Bstudent; (b) The network Bstudent
G=g,D=d reduced by G = g, D = d.
Now, assume that our query is P(j1 | d1, h0), so that our function f is the indicator function
11{ξ⟨J⟩= j1}. For each particle, we evaluate
IEP (C,I,S,L,J,H|g,d1,h0)

11{ξ⟨J⟩= j1}

= P(j1 | g, d1, h0).
We then compute an average of these probabilities, weighted by the importance weights (which we
will discuss). The computation of the probabilities P(J | g, d1, h0) can be done using inference in
the reduced network, which is simpler than inference in the original network. (Although, in this
very simple network, the savings are not signiﬁcant.)
Note that the extent to which the reduced network allows more eﬀective inference than the
original one depends on our choice of variables Xp. For example, if (for some reason) we choose
Xp = {H}, the resulting conditioned network is no simpler than the original, and the use of
particle-based methods has no computational beneﬁts over the use of exact inference.
12.4.1.2
Formal Description
To specify the algorithm formally, we must deﬁne the proposal distribution Q and the associated
importance weights. We begin by partitioning our evidence set E into two subsets: Ep =
E ∩Xp, and Ed = E ∩Xd, with ep and ed deﬁned accordingly. This partition determines
how we handle each of the observed variables: evidence in Ep will be treated as in likelihood
weighting, modifying our sampling process and the importance weights; evidence in Ed will be
accounted for as part of the exact inference process.
Now, consider an arbitrary proposal distribution Q. We can go through an analysis similar to

12.4. Collapsed Particles
529
the one that allowed us to derive equation (12.12):
IEP (ξ|e)[f(ξ)]
=
X
xp,xd
P(xp, xd | e)f(xp, xd, e)
=
X
xp
Q(xp)P(xp | e)
Q(xp)
X
xd
P(xd | xp, e)f(xp, xd, e).
We can now reformulate the term P(xp | e) as:
P(xp | e)
=
P(xp, e)
P(e)
=
P(xp, ep, ed)
P(e)
=
1
P(e)P(xp, ep)P(ed | xp, ep).
Plugging this result back into our derivation, we obtain that:
IEP (ξ|e)[f(ξ)] =
1
P(e)
X
xp
Q(xp)P(xp, ep)
Q(xp)
P(ed | xp, ep)
X
xd
P(xd | xp, e)f(xp, xd, e)
=
1
P(e)IEQ(Xp)
P(xp, ep)
Q(xp)
P(ed | xp, ep)IEP (xd|xp,e)[f(xp, xd, e)]

.
(12.31)
This analysis suggests that the appropriate importance weights should be deﬁned as:
w(xp) = P(xp, ep)
Q(xp)
P(ed | xp, ep).
(12.32)
Indeed, if we compute the mean of our importance weights, as in equation (12.11), we obtain the
following formula for the normalized importance sampling estimator:
IEQ(Xp)[w(Xp)]
=
X
xp
Q(xp)P(xp, ep)
Q(xp)
P(ed | xp, ep)
=
X
xp
P(xp, ep)P(ed | xp, ep)
=
X
xp
P(ed, xp, ep) = P(ed, ep).
Thus, if we select our importance weights as in equation (12.32), we have that:
IEP (ξ|e)[f(ξ)] = IEQ(Xp)

w(Xp)IEP (xd|xp,e)[f(xp, xd, e)]

IEQ(Xp)[w(Xp)]
,
as desired.

530
Chapter 12. Particle-Based Approximate Inference
12.4.1.3
Proposal Distribution
Our preceding analysis does not place any restrictions on the proposal distribution; we can
choose any proposal distribution Q that seems appropriate (as long as it dominates P). However,
it is important to remember our two main desiderata for a proposal distribution: easy generation
of samples from Q and similarity between Q and our target distribution P(Xp | e).
The
proposal distribution we used for the full particle case attempted to address both of these
desiderata, at least to some extent. In this section, we describe a generalization of that proposal
distribution for the case of collapsed particles.
Our goal is to generate particles from a distribution Q(Xp). Following the template for the
full particle case, we would sample each unobserved variable X ∈Xp from its CPD. The reason
we can execute this process is that we were careful to sample the parents of Xi before that,
so that the distribution from which we should sample X is uniquely deﬁned. In the collapsed
case, however, PaX might not be within the set Xp, in which case we would not have values
for them when sampling X. For example, returning to the setting of example 12.16, it is not
clear how we would deﬁne a sampling distribution for the variable G.
Most simply, we can select as our subset Xp an upwardly closed subset of nodes in the
network. In our example, we might select Xp to consist of all of the variables C, D, I, G. This
variant of the algorithm is very close to full-particle likelihood weighting. Here, we are back in a
situation where we can order the variables in such a way that each unobserved variable can be
sampled using its original CPD, using the previously sampled assignment to its parents. Each
observed variable Xi is “sampled,” as in standard likelihood weighting, from a mutilated network
that ascribes probability 1 to its observed value ep⟨Xi⟩. The computation of the importance
weights for this case is straightforward: We ﬁrst compute the part of the importance weight
corresponding to P(xp, ep)/Q(xp), using precisely the same incremental computation as in
standard likelihood weighting. We then compute P(ed | xp, ep) in the network conditioned on
xp, ep, and we multiply the importance weight by this additional factor.
Example 12.17
Continuing example 12.16, assume we choose Xp to be the upwardly closed set C, D, I, G. We
would sample C, I, and G from their CPD; D would be sampled from a CPD that has no parents,
and ascribes probability 1 to d1. The importance weight for a particle (c, d1, i, g) is then computed
as P(d1 | c) · P(h0 | c, d1, i, g).
Note that this last term requires inference in the network,
speciﬁcally, the marginalization of L, J, S.
More generally, we can deﬁne a fully general proposal distribution Q by specifying a topo-
logical ordering X1, . . . , Xk over Xp, and a proposal distribution deﬁned in terms of a
Bayesian network GQ over Xp that speciﬁes, for each variable Xi, i = 1, . . . , k, a parent
set PaGQ
Xi ⊆{X1, . . . , Xi−1}, and a CPD Q(Xi | PaGQ
Xi ). This approach allows us to represent
an arbitrary proposal distribution.
We can now consider the computation of each of the three terms in the deﬁnition of the
importance weights in equation (12.32). The term Q(xp) can be computed very simply via the
chain rule for the proposal network GQ. The terms P(ed | xp, ep) can be computed using
inference in the conditioned network BEp=ep,Xp=xp — we compute the probability of the
query P(ed | ep, xp) in this network. As we stated, the whole approach of collapsed particles
is based on the premise that (exact or approximate) inference in this conditioned network is
feasible. Similarly, we can compute P(xp, ep) in the same network.

12.4. Collapsed Particles
531
12.4.2
Collapsed MCMC
The collapsed MCMC algorithm is also based on equation (12.29); however, as in section 12.3, we
simplify our notation by deﬁning Φ to be the set of factors reduced by the evidence e, so that
PΦ(X) = P(X | e) (for X = X −E). As in collapsed likelihood weighting, we approximate
the outer expectation by generating particles that are instantiations of Xp. Here, we generate
the particles xp[m] using a Markov chain process; at the limit of the chain, these particles will
be sampled from the marginal posterior PΦ(Xp). For each such particle xp, we maintain some
representation of the distribution PΦ(Xd | xp), and perform (exact or approximate) inference to
compute the expectation of f relative to this distribution. For simplicity, we focus our discussion
on Gibbs sampling; the extension to a Metropolis-Hastings algorithm with a general proposal
distribution is straightforward.
We deﬁne the collapsed Gibbs sampling algorithm via a Markov chain whose states are
instantiations to Xp.
As we discuss, to provide an unbiased estimator for the expectation
over PΦ(Xp) in equation (12.29), we want the stationary distribution of this Markov chain
to be PΦ(Xp).
We thus modify our Gibbs sampling algorithm as follows: As in standard
Gibbs sampling, we deﬁne a kernel for each variable Xi ∈Xp. Let x−i be an assignment to
Xp −{Xi}. The kernel for Xi is deﬁned as follows:
Ti((x−i, xi) →(x−i, x′
i)) = PΦ(x′
i | x−i).
(12.33)
This equation is very similar to equation (12.22). The only diﬀerence is that the event x−i on
which we condition the distribution over Xi is not a full assignment to all the other variables in
the network, but rather only to the remaining variables in Xp. Thus, the eﬃcient computation
of equation (12.23), where we simply compute the distribution over Xi given its Markov blanket,
may not apply.
However, we can compute this probability using inference in the network
HΦ[xp] — the Markov network reduced over the assignment xp — a model that we assume to
be tractable. Indeed, the sampling approach can be well integrated with a clique tree over the
variables Xp to allow the sampling process to be executed eﬃciently (see exercise 12.27).
Having deﬁned the chain, we can use it in any of the ways described in section 12.3.5 to
collect a data set D of particles x(m)
p
, each of which is associated with a distribution over Xd.
Using these particles, we can estimate:
ˆIED(f) = 1
M
M
X
m=1

IEPΦ[xp[m]](Xd)[f(xp, xd, e)]

.
Example 12.18
Consider the Markov network deﬁned by the Bayesian network of example 12.11, reduced over the
evidence G = g. This Markov network is a bipartite graph, where we have two sets of variables
I, D such that the only edges in the network are of the form Ij, Dk. Let φj,k(Ij, Dk) be the factor
associated with each such edge. (For simplicity of notation, we assume that there is a factor for
every such edge; the factors corresponding to grades that were not in the model will be vacuous.)
We can now apply collapsed Gibbs sampling, selecting to sample the variables D and maintain a
distribution over I; this choice will (in most cases) be better, since there are usually more students
than courses, and it is generally better to sample in a lower-dimensional space. Thus, our Markov

532
Chapter 12. Particle-Based Approximate Inference
chain needs to be able to sample Dk from PΦ(Dk | d−k):
PΦ(dk | d−k) = PΦ(d1, d2, . . . , dm)
P
Dk PΦ(Dk, d−k).
The expression in the numerator, and each term in the sum in the denominator, is the probability
of a full assignment to D. This type of expression can be computed as the partition function of the
reduced Markov network that we obtain by setting D = d:
PΦ[d](I1, . . . , In) =
1
Z(d)
Y
j,k
φj,k(Ij, dk).
Each of the factors in the product is a singleton over an individual Ij. Thus, we can compute
the partition function or marginals of this distribution in linear time. In particular, we can easily
compute the Gibbs transition probability.
Using the same analysis, we can easily compute an
expression for PΦ[d](I) in closed form, as a product of individual factors over the Ij’s. Thus, we
can also easily compute expectations of any function over these variables, such as the expected value
of the number of smart students who got a grade of a C in an easy class (see exercise 12.28).
Box 12.D — Concept: Correspondence and Data Association. One simple but important prob-
lem that arises in many settings is the correspondence problem: mapping between one set of objects
correspondence
problem
U = {u1, . . . , uk} and another V = {v1, . . . , vm}. This problem arises across multiple diverse
application domains. Perhaps the most familiar are physical sensing applications, where U are
sensor measurements and V objects that can generate such measurements; we want to know which
object generated which measurement. For example, the objects may be airplanes, and the sensor
measurements blips on a radar screen; or the objects may be obstacles in a robot’s environments,
and the sensor measurements readings from a laser range ﬁnder. (See box 15.A.) In this incarnation,
this task is known as data association. However, there are also many other applications, of which
data association
we list only a few examples:
• Matching citations in text to the entities to which they refer (see, for example, box 6.D); this
problem has been called identity resolution and record matching.
identity
resolution
• Image registration, or matching features in an image representing one view of an object to
image
registration
features in an image representing a diﬀerent view; this problem arises in applications such as
stereo reconstruction or structure from motion.
• Matching words in a sentence in one language to words in the same sentence, translated to a
diﬀerent language; this problem is called word alignment.
• Matching genes in a DNA sequence of one organism to orthologous genes in the DNA sequence
of another organism.
This problem has been tackled using a range of models and a variety of inference methods. In this
case study, we describe some of these approaches and the design decisions they made.

12.4. Collapsed Particles
533
Probabilistic Correspondence
To formulate the correspondence problem as a probabilistic
model, we can introduce a set of correspondence variables that indicate the correspondence be-
correspondence
variable
tween one set of objects and the other. One approach is to have binary-valued variables Cij, such
that Cij = true when ui matches vj, and false otherwise. While this approach is simple, it places
no constraints on the number of matches for each i or for each j. Typically, we want to restrict
our model so that, at least on one side, the match is unique. For example, in the radar tracking
application, we typically want to assume that each measurement was derived from only a single
object. In order to accommodate that in this model, we would need to add (hard) mutual exclusion
mutual exclusion
constraints
constraints (mutex) that Cij = true implies that Cij′ = false for all j′ ̸= j. The resulting model
is very densely connected, and it can be challenging for inference algorithms to deal with.
A more parsimonious representation uses nonbinary variables Ci such that Val(Ci) = {1, . . . , m};
here, Ci = j indicates that ui is matched to vj. The mutex constraints for the matches to ui are
then forced by the fact that each variable Ci takes on only a single value. Of course, we might also
have mutex constraints in the other direction, where we want to assume that each vj matches only
a single ui. We will return to this setting.
The probabilistic model and the evidence generally combine to produce a set of aﬃnities wij
that specify how likely ui is to match to vj. For convenience, we assume that these aﬃnities
are represented in log-space. These aﬃnities can be derived from a generative probability — how
likely is vj to have generated ui, or from an undirected potential that measures the quality of the
match. (See, for example, box 6.D for an application where both approaches have been utilized.)
For example, in the robot obstacle matching task, exp(wij) may evaluate how likely it is that
an obstacle vj,at location Lj, generated a measurement ui at sensed location Si. In the citation
matching problem, we may have a model that tells us how likely it is that an individual with the
name “John X. Smyth” generated a citation “J. Smith.” The aﬃnities wij deﬁne a node potential
over the variables Ci: φi(Ci = j) = exp(wij). In addition, there may well be other components
to the model, as we will discuss.
The general inference task is then to compute the distribution over the correspondence variables
or (as a fallback) to ﬁnd the most likely assignment. We now discuss the challenges posed by this
task in diﬀerent variants of the correspondence problem, and some of the solutions.
The simplest case (which rarely arises in practice) is the one just described: We have only a
set of node potentials φi(Ci = j) that specify the aﬃnity of each ui to each vj. In this case,
we have only a set of unrelated node potentials over the variables Ci, making the model one
comprising independent random variables. We can then easily ﬁnd the most likely assignment
c∗
i = arg maxj φi(Ci = j), or even compute the full posterior P(Ci) ∝φi(Ci).
The inference task becomes much more challenging when we extend the model in a way that
induces correlations over the correspondence variables. We now describe three settings where such
complications arise, noting that these are largely orthogonal, so that more than one of these com-
plicating factors may arise in any given application.
Two-Sided Mutex Constraints
The ﬁrst complication arises if we want to impose mutex con-
straints not just on the correspondence of the ui’s to the vj’s, but also in the reverse direction; that
is, we want each vj to be assigned to exactly one ui. (We note that, unless k = m, a perfect match
is not possible; however, we can circumvent this detail by adding “dummy” objects that are equally
good matches to anything.) This requirement induces a model where all of the Ci’s are connected
to each other with potentials that impose mutex constraints, making it clearly intractable for a

534
Chapter 12. Particle-Based Approximate Inference
variable elimination algorithm. Nevertheless, several techniques have been eﬀectively applied to this
problem.
First, we note that we can view the correspondence problem, in this case, as a bipartite graph,
where the ui’s are one set of vertices, the vj’s are a second set, and an edge of weight wij connects
ui to each vj. An assignment satisfying the mutex constraints is a bipartite matching in this graph
bipartite
matching
— a subset of edges such that each node has exactly one adjacent edge in the matching. Finding the
single highest-probability assignment — the MAP assignment — is equivalent to one of ﬁnding the
MAP assignment
bipartite matching whose weight (sum of the edge weights in the matching) is largest. This problem
can be solved very eﬃciently using combinatorial optimization algorithms: If we can match any ui
to any vj, the total running time is O(k3); if we have only ℓpossible edges, the running time is
O(k2 log(k) + kℓ).
Of course, in many cases, a single assignment is not an adequate solution for our problem, since
there may be many distinct solutions that have high probability. For computing posteriors in this
setting, two main approaches have been used.
The ﬁrst is an MCMC sampler over the space of possible matchings. Here, Gibbs sampling is
not a viable approach, since any change to the value of a single variable would give rise to an
assignment that violates the mutex constraints. A Metropolis-Hastings chain is a good solution,
but for good performance, the proposal distribution must be carefully chosen. Most obvious is to
pick two variables such that Ci1 = j1 and Ci2 = j2, and propose a move that ﬂips them, setting
Ci1 = j2 and Ci2 = j1. While this approach is a legal chain, it can be slow to mix: in most local
maxima, a single ﬂip of this type will often produce a very poor solution. Thus, some work has
been done on constructing proposal distributions with larger steps that ﬂip multiple variables at a
time while still maintaining a legal matching.
The second solution uses loopy belief propagation over the Markov network with the Ci variables.
As we mentioned, however, the mutex constraints give rise to a fully connected network, which is a
very challenging setup for belief propagation. Here, however, we can adopt a simple heuristic: We
begin without including the mutex constraints and run inference. We then examine the resulting
posterior and identify those mutex constraints that are violated with high probability (those where
P(Ci1 = Ci2) is high). We then add those violated constraints and repeat the inference. In
practice, this process usually converges long before all k2 mutex constraints are added.
Unobserved Attributes
The second type of complication arises when the aﬃnities wij depend
on properties Aj of the objects that are unobserved, and need to be inferred. For example, in the
citation matching problem, we generally do not know the correct name of an author or a paper;
given the name of an author vj, we can determine the aﬃnity wij of any citation ui. Conversely,
given a set of citations ui that match vj, we can infer the most likely name to have generated
the observed names in these citations.
The same situation arises in many other applications.
For example, in the airplane tracking application, the plane’s location at a given point in time
is generally unknown, although we may have a prior on this location based on observations at
previous time points. Given the location of airplane vj, we can determine how likely it was to have
generate the blip ui. Conversely, once we assign blip ui to vj, we can update our posterior on vj’s
location.
More formally, here we have a set of observed features Bi for each object ui, and a set of hidden
attributes Aj for each vj. We have a prior P(Aj), and a set of factors φi(Aj, Bi, Ci) that
are vacuous (uniform) if Ci ̸= j. We want to compute the posterior over Aj. For this problem,

12.4. Collapsed Particles
535
3
5
7
8
6
9
10
12
13
14
11
4
2
1
3
5
7
8
6
9
10
12
13
14
11
4
2
1
3
5
7
8
6
9
10
12
13
14
11
4
2
1
Figure 12.D.1 — Results of a correspondence algorithm for 3D human body scans The model ex-
plicitly captures correlations between diﬀerent correspondence variables.
one obvious extension of the MCMC approach we described is to use a collapsed MCMC approach,
where we sample the correspondence variables but maintain a closed-form distribution over Aj;
see exercise 12.29. When the features Ak are such that maintaining a closed-form posterior is
challenging (for example, they are continuous and do not have a simple parametric form), we
often adopt an approach where we pick a single assignment to each Aj; this solution can be
implemented in the framework of an EM algorithm (see box 19.D for one example).
EM
Directly Correlated Correspondences
A ﬁnal complication arises when we wish to model direct
correlations between the values of diﬀerent correspondence variables. This type of correlation would
correlated
correspondence
arise naturally, for example, in an application where we match visual features (small patches) in two
images of the same real-world object (say a person). Here, we not only want the local appearance
of a feature to be reasonably similar in the two images; we also want the relative geometry of the
features to be consistent. For example, if a patch containing the eye is next to a patch containing the
nose in one image, they should also be close in the other. These (soft) constraints can be represented
as potentials on pairs of variables Ci1, Ci2 (or even on larger subsets). Similar situations arise in
sequence alignment, where we often prefer to match adjacent sequence elements (whether words or
genes) on one side to adjacent sequence elements on the other.
Here, we can often exploit the fact that the spatial structure we are trying to encode is local,
so that the resulting Markov network over the Ci variables is often reasonably structured. As a
consequence, techniques such as loopy belief propagation, if carefully designed, can be surprisingly
eﬀective in this setting.
Figure 12.D.1 demonstrates the results of one model along these lines. The model, due to Anguelov
et al. (2004), aims to ﬁnd a correspondence between a set of (automatically selected) landmarks

536
Chapter 12. Particle-Based Approximate Inference
on diﬀerent three-dimensional scans of human bodies. Here, φi(Ci = j) represents the extent to
which the local appearance around the ith landmark in one scan is similar to the local appearance
of the jth landmark on the other. This task is very challenging for two reasons. First, the local
appearance of corresponding patches on two scans can be very diﬀerent, so that the node potentials
are only somewhat informative. Second, the two scans exhibit signiﬁcant deformation, both of
shape and of pose, so that standard parametric models of deformation do not work.
In this
task, modeling correlations between the diﬀerent correspondence variables allows us to capture
constraints regarding the preservation of the object geometry, speciﬁcally, the fact that distances
between corresponding points should be roughly preserved. This feature was essential for obtaining
reasonable correspondences. Most of the constraints regarding preservation of distances relate to
pairs of nearby points, so that the resulting Markov network was not too densely connected, allowing
a judicious application of loopy belief propagation to work well.
12.5
Deterministic Search Methods ⋆
So far, we have focused on particles generated by random sampling. Random sampling tries to
explore the state space of the distribution “uniformly,” generating each state proportionately to
its probability. A sampling-based approach can, however, be problematic when we have

a highly skewed distribution, where only a small number of states have nonnegligible
probability. In this case, sampling methods will tend to sample the same small set of
states repeatedly, wasting computational resources to no gain. An alternative approach,
designed for settings such as this, is to use a deterministic method that explicitly searches
for high-probability states.
Intuitively, in these search methods, we deterministically generate some set of distinct assign-
ments D = {ξ[1], . . . , ξ[M]}. We then approximate the joint distribution P by considering only
these instantiations, ignoring the rest. Intuitively, if our particles account for a large proportion
of the probability mass, we have a reasonable approximation to the joint.
Example 12.19
In the Student network of ﬁgure 12.1, the most likely ten instantiations (of 48), in decreasing order,
are:
d1
i0
g3
s0
l0
0.184
d0
i0
g3
s0
l0
0.119
d0
i1
g1
s1
l1
0.117
d0
i0
g1
s0
l1
0.108
d0
i0
g2
s0
l1
0.096
d0
i0
g2
s0
l0
0.064
d1
i1
g1
s1
l1
0.043
d1
i0
g2
s0
l1
0.04
d0
i1
g1
s0
l1
0.029
d1
i0
g2
s0
l0
0.027.
Together, they account for about 82.6 percent of the probability mass. To account for 95 percent of
the probability mass, we would need twenty-two instantiations, and to account for 99 percent of
the mass, we would need thirty-three instantiations.

12.5. Deterministic Search Methods ⋆
537
Intuitively, the quality of our particle-based approximation to the joint distribution improves
with the amount of probability mass accounted for in our particles. Thus, our goal is to enu-
merate instantiations that have high probability in some unnormalized measure ˜P. Speciﬁcally,
in the case of Bayesian networks, we want to enumerate instantiations that are likely given e.
We can formalize this goal as one of ﬁnding the highest-probability instantiations in ˜P,
until we reach some upper bound K on the number of particles. (One might be tempted to
use, as a stopping criterion, a lower bound on the total mass accumulated in the enumerated
particles; however, because we generally do not know the normalizing constant, the absolute
mass accumulated has no real interpretation.) Clearly, this problem encompasses within it the
task of ﬁnding the single most likely instantiation, also known as the maximum a posteriori
MAP assignment
(MAP) assignment. The problem of ﬁnding the MAP assignment is the focus of chapter 13. Not
surprisingly, many of the successful methods for enumerating high-probability assignments are
extensions of methods for ﬁnding the MAP assignment. Thus, we largely defer discussion of
methods for ﬁnding high-probability particles to that chapter. (See, for example, exercise 15.10.)
In this section, we focus primarily on the question of using a set of deterministically selected
particles to approximate the answers to conditional probability queries.
Consider any event Z = z. Approximating the joint using the set of high-probability assign-
ments D, we have that one natural estimate for ˜P(z) is:
M
X
m=1
11{z[m] = z} ˜P(ξ[m]),
(12.34)
where z[m] = ξ[m]⟨Z⟩= z. It is important to note a key diﬀerence between this estimate and
the one of equation (12.2). There, we merely counted particles, whereas in this case, the particles
are weighted by their probability. Intuitively, the diﬀerence is due to the fact that, in sampling
methods, particles are generated proportionately to their probability. Thus, higher- probability
instantiations are generated more often.
If we then also weighted the sampled particles by
their probability, we would be “double-counting” the probability. Alternatively, we can view this
formula as an instance of the importance sampling estimator, equation (12.8). In this case, each
particle ξ[m] is generated from a (diﬀerent) deterministic proposal distribution Q, that ascribes
it probability 1.4 Hence, we must weight the particle by ˜P(ξ[m])/Q(ξ[m]) = ˜P(ξ[m]).
Here, however, we have no guarantees about the bias of the estimator. Depending on our
search procedure, we might end up with estimates that are arbitrarily bad. At one extreme, for
example, we might have a search procedure that avoids (for as many particles as possible) any
instantiation ξ where ξ⟨Z⟩= z. Clearly, our estimate will then be biased in favor of low values.
A more correct approach is to use our particles to provide both an upper and lower bound to the
unnormalized probability of any event z. Some of our particles ξ[m] have z[m] = z. The total
probability mass of these particles is a lower bound on the probability mass of all instantiations
lower bound
ξ where Z = z. Similarly, the total probability mass of particles that have z[m] ̸= z is a lower
bound on the complementary mass, and hence provides an upper bound on the probability
4. We cannot, of course, actually apply importance sampling in this way, since this deterministic proposal distribution
violates our assumption that Q’s support contains that of P. However, this perspective provides intuition for our choice
of weights.

538
Chapter 12. Particle-Based Approximate Inference
mass of the assignments where Z = z:
M
X
m=1
11{z[m] = z} ˜P(ξ[m]) ≤˜P(Z = z) ≤
 
1 −
M
X
m=1
11{z[m] ̸= z} ˜P(ξ[m])
!
.
(12.35)
Equivalently, we can deﬁne ρ = 1−PM
m=1 ˜P(ξ[m]) to be the probability mass not accounted
for by our particles. The bound can then be rewritten as:
M
X
m=1
11{z[m] = z} ˜P(ξ[m]) ≤˜P(Z = z) ≤ρ +
M
X
m=1
11{z[m] = z} ˜P(ξ[m]).
This reformulation reﬂects the fact that the unaccounted probability mass can be associated
either with the event Z = z or with its complement. If all of the unaccounted probability mass
is associated with z, we get the upper bound, and if all of it is associated with the complement,
we get the lower bound.
Example 12.20
Consider the Student network of ﬁgure 12.1, and the event l1 — a strong letter. Within the ten
particles of example 12.19, the particles compatible with this event are: ξ[3], ξ[4], ξ[5], ξ[7], ξ[8],
and ξ[9]. Together, they account for 0.433 of the probability mass in the joint. The complementary
event accounts for 0.393 of the mass. The total unaccounted mass is 1 −0.826 = 0.174. From
these numbers, we obtain the following bounds:
0.433 ≤P(l1) ≤1 −0.393 = 0.433 + 0.174 = 0.607.
The true probability of this event is 0.502.
Now, consider the event i0, l1 — a weak student getting a strong letter. In this case, we have
three instantiations compatible with this assignment: ξ[4], ξ[5], and ξ[8]. Together, they account for
0.244 of the probability mass. The remaining assignments, which are incompatible with the event,
account for 0.582 of the mass. Altogether, we obtain the following bounds for the probability of this
event:
0.244 ≤P(i0, l1) ≤(1 −0.582) = 0.418.
The true probability of this event is 0.272.
When evaluating these results, it is important to note that they are based on the use of only ten
particles. The results from any sampling-based method that uses only ten particles would probably
be signiﬁcantly worse.
We can see that the true probability can lie anywhere within the interval speciﬁed by equa-
tion (12.35), and there is no particular justiﬁcation for choosing any particular point within the
range (for example, the point speciﬁed by equation (12.34)). If our search happens to ﬁrst ﬁnd
instantiations that are compatible with z, the lower bound is likely to be a better estimate; if it
ﬁrst ﬁnds instantiations that are incompatible with z, the upper bound is likely to be closer.
The fact that we obtain both lower and upper bounds on the mass in the unnormalized
measure also allows us to provide bounds on the value of probability (or conditional) probability
queries. Assume we are trying to compute P(y | e) = ˜P(y, e)/ ˜P(e). We can now obtain
upper and lower bounds for both the numerator and denominator, using equation (12.35). A lower

12.5. Deterministic Search Methods ⋆
539
bound for the numerator and an upper bound for the denominator provide a lower bound for
the ratio. Similarly, an upper bound for the numerator and a lower bound for the denominator
provide an upper bound for the ratio. Analogously, we can obtain bounds on the marginal
probability P(y) by normalizing ˜P(y) by ˜P(true).
More precisely, assume that we have bounds:
ℓy,e
≤
P(y, e)
≤
uy,e
ℓe
≤
P(e)
≤
ue.
Then we can bound:
ℓy,e
ue
≤
P(y | e)
≤
uy,e
ℓe .
(12.36)
Example 12.21
Assume we want to compute P(i0 | l1) — the probability that a student with a strong letter is not
intelligent — using our ten samples from example 12.19. We have already shown the bounds for
both the numerator and the denominator. We obtain:
0.265/0.607
≤
P(i0 | l1)
≤
0.439/0.433
0.437
≤
P(i0 | l1)
≤
1.014.
The true probability of this event is 0.542, which is fairly far away from any of the bounds.
Interestingly, the upper bound in this case is greater than 1. Although clearly valid, this con-
clusion is not particularly interesting. In general, the deterministic approximations are of value
only if we can cover a very large fraction of our probability mass with a small number of
particles. While this constraint might seem very restrictive (as it often is), there are nevertheless
applications where the probabilities are very skewed, and this assumption is a very good one;
we return to this point in section 12.6.
A very similar discussion applies to the extension of deterministic search methods to collapsed
particles. In this case, we approximate the outer expectation in equation (12.29) using a set D of
particles xp[1], . . . , xp[M], which are selected using some deterministic search procedure. As
usual, each particle is associated with a distribution P(Xd | xp[m], e).
Consider the task of computing the probability P(z). As in the case of full particles, each of
the generated particles xp[m] accounts for a certain part of the probability mass. However, in
this case, we cannot simply test whether the event z is compatible with the particle, since the
particle might not specify a full assignment to the variables Z. Rather, we have to compute the
probability of z relative to the distribution over all of the variables deﬁned by the particle. (See
exercise 12.26.)
Thus, in particular, the lower bound on P(z) deﬁned by our particles is:
M
X
m=1
P(xp[m])
 IEP (Xd|xp[m])[11{xp, Xd⟨Z⟩= z}]

=
M
X
m=1
P(xp[m])P(z | xp[m]).

540
Chapter 12. Particle-Based Approximate Inference
The lower bound assumes that none of the unaccounted probability mass is compatible with
z. Similarly, the upper bound assumes that all of this unaccounted mass is compatible with z,
leading to:
 M
X
m=1
P(z, xp[m])
!
+
 
1 −
M
X
m=1
P(xp[m])
!
.
Once again, we can compute both the probability P(z | xp) and the weight P(xp) by using
inference in the conditioned network.
This method is simply an incremental version of the conditioning algorithm of section 9.5,
incremental
conditioning
using xp as a conditioning set. However, rather than enumerating all possible assignments to
the conditioning set, we enumerate only a subset of them, attempting to cover most of the
probability mass. This algorithm is also called bounded conditioning.
bounded
conditioning
12.6
Summary
This chapter presents a series of methods that attempt to approximate a joint distribution using
a set of particles. We discussed three main classes of techniques for generating particles.
Importance sampling, and speciﬁcally likelihood weighting, generates particles by random
sampling from a distribution. Because we cannot, in general, sample from the posterior dis-
tribution, we generate particles from a diﬀerent distribution, called the proposal distribution,
and then adjust their weights so as to get an unbiased estimator. The proposal distribution is
a mutilated Bayesian network, and the samples are generated using forward sampling. Owing
to the use of forward sampling, likelihood weighting applies only to directed graphical mod-
els. However, if we somehow choose a proposal distribution, the more general framework of
importance sampling can also be applied to undirected graphical models (see exercise 12.9).
Markov chain Monte Carlo techniques attempt to generate samples from the posterior distri-
bution. We deﬁne a Markov chain, or a stochastic sampling process, whose stationary distri-
bution (the asymptotic result of the sampling process) is the correct posterior distribution. The
Metropolis-Hastings algorithm is a general scheme for specifying a Markov chain that induces a
particular posterior distribution. We showed how we can use the Metropolis-Hastings algorithm
for graphical models. We also showed a particular instantiation, called Gibbs sampling, that is
speciﬁcally designed for graphical models.
Finally, search methods take a diﬀerent approach, where particles are generated determin-
istically, trying to focus on instantiations that have high probability. Unlike random sampling
methods, deterministic search methods do not provide an unbiased estimator for the target
query. However, they do provide sound upper and lower bounds. When the probability distribu-
tion is highly diﬀuse, so that many particles are necessary to cover most of the probability mass,
these bounds will be very loose, and generally without value. However, when a small number
of instantiations account for a large fraction of the probability mass, deterministic search tech-
niques can obtain very accurate results with a very small number of particles, often much more
accurate results than sampling-based methods with a comparable number of particles. There
are several applications that have this property. For example, when performing fault diagnosis
(see, for example, box 5.A), where faults are very rare, it can be very eﬃcient to enumerate all
hypotheses where the system has up to K faults. Because multiple faults are highly unlikely,

12.7. Relevant Literature
541
even a small value of K (2 or 3) will likely suﬃce to cover most of the probability mass —
even the mass consistent with our evidence. Another example is speech recognition (box 6.B),
where only very few trajectories through the HMM are likely. In both of these applications,
deterministic search methods have been successfully applied.
From a high level, it appears that sampling methods are the ultimate general-purpose inference
algorithm. They are the only method that can be applied to arbitrary probabilistic models and
that is guaranteed to achieve the correct results at the large sample limit. Indeed, when faced
with a complex probabilistic model that involves continuous variables or a nonparametric model,
there are often very few other choices available to us. While optimization-based methods, such
as those of chapter 11, can sometimes be applied, the application often requires a nontrivial
derivation, speciﬁc to the problem at hand.
Moreover, these methods provide no accuracy
guarantee. Conversely, it seems that sampling-based methods can be applied easily, oﬀ-the-
shelf, to virtually any model.
This impression, however, is somewhat misleading. While it is true that sampling methods

provide asymptotic guarantees, their performance for reasonable sample sizes is very
diﬃcult to predict. In practice, a naive application of sampling methods to a complex
probabilistic model often fails dismally, in that the estimates obtained from any rea-
sonable number of samples are highly inaccurate. Thus, the success of these methods
depends heavily on the properties of the distribution, and on a careful design of our
sampling algorithm. Moreover, there is little theoretic basis for this design, so that the
process of getting sampling methods to work is largely a matter of intuition and intensive
experimentation.
Nevertheless, the methods described in this chapter do provide an important component in
our arsenal of methods for inference in complex models. Moreover, they are often used very
successfully in combination with exact or approximate global inference methods. Standard com-
binations include the use of global inference for providing more informed proposal distributions,
and for manipulating collapsed particles. Such combinations are highly successful in practice,
and they often lead to much better results than any of the two types of inference methods in
isolation.
Having described these basic methods, we showed how they can be extended to the case
of collapsed particles, which consist of an assignment to a subset of network variables, associ-
ated with a closed-form distribution over the remaining ones. The answer to a query is then a
(possibly weighted) sum over the particles, of the answer to the query within each associated dis-
tribution. This approach approximates part of the inference task via particles, while performing
exact inference on a subnetwork, which may be simpler than the original network.
12.7
Relevant Literature
The NP-hardness of approximate probabilistic inference in Bayesian networks was shown by
Dagum and Luby (1993).
There is a vast literature on the use of Monte Carlo methods for estimating integrals in
general, and the expectation of functions in particular. See Robert and Casella (2005) for one
good review.
Geweke (1989) proves some of the basic results regarding the accuracy of the
importance sampling estimates.

542
Chapter 12. Particle-Based Approximate Inference
Probabilistic logic sampling was ﬁrst proposed by Henrion (1986). The improvement to likeli-
hood weighting was proposed independently by Fung and Chang (1989) and by Shachter and Peot
(1989). Cano et al. (2006) and Dagum and Luby (1997) proposed a variant of likelihood weighting
based on unnormalized importance sampling, which separately estimates P(y, e) and P(e),
as described in section 12.2.3.2. Dagum and Luby also proposed the use of a data-dependent
stopping rule for the case where the CPD entries are bounded away from 0 or 1. For this
case, they provide guaranteed bounds on the expected number of samples required to achieve
a certain error rate, as discussed in section 12.2.3.2. Pradhan and Dagum (1996) provide some
empirical validation of this algorithm, applied to a large medical diagnosis network.
Various heuristic approaches for improving the proposal distribution have been proposed.
Fung and del Favero (1994) proposed the backward sampling algorithm, that allows for generating
samples from evidence nodes in the direction that is opposite to the topological order of nodes
in the network, combining their likelihood function with the CPDs of some previously sampled
variables. Other variants use some alternative form of approximate inference over the network to
produce an approximation Q(X) to P(X | E = e), and then use Q as a proposal distribution
for importance sampling. For example, de Freitas et al. (2001) use variational methods (described
in chapter 11) in this way.
Shachter and Peot (1989) proposed an adaptive approach, called self-importance sampling,
which adapts the proposal distribution to the samples obtained, attempting to increase the
probability of sampling in higher-probability regions of the posterior distribution P(X | e).
This approach was subsequently improved by Cheng and Druzdzel (2000) and by Ortiz and
Kaelbling (2000), who proposed an adaptive importance sampling method that uses the variance
adaptive
importance
sampling
of the estimator in a more principled way.
Shwe and Cooper (1991) applied importance sampling to the QMR-DT network for medical
diagnosis (Shwe et al. 1991). Their variant of the algorithm combined self-importance sampling
and an improved proposal distribution called Markov blanket scoring. This proposal distribution
was designed to be computed eﬃciently in the context of BN2O networks.
Sampling methods based on Markov chains were ﬁrst proposed for models arising in statistical
physics. In particular, the Metropolis-Hastings algorithm was ﬁrst proposed by Metropolis et al.
(1953). Geman and Geman (1984) applied Gibbs sampling to image restoration in a paper that was
very inﬂuential in popularizing this method within computer vision, and subsequently in related
communities. Ackley, Hinton, and Sejnowski (1985) propose the use of Gibbs sampling within
Boltzmann machines, for both inference and parameter estimation.
Pearl (1987) introduced
Gibbs sampling for Bayesian networks. York (1992) continues this work, speciﬁcally addressing
the problem of networks where some states have low (or even zero) probability.
Over the years, extensive work has been done on the topic of MCMC methods, addressing
a broad range of topics including: theoretical analyses, application to new classes of models,
improved algorithms that are faster or have better convergence properties, speciﬁc applications,
and many more.
Works reviewing some of these developments include Neal (1993); Smith
and Roberts (1993); Gilks et al. (1996); Gamerman and Lopes (2006). Neal (1993), in particular,
provides both an excellent tutorial, guidelines for practitioners, and a comprehensive annotated
bibliography for relevant papers (up to 1993).
Tierney (1994) discusses the conditions under
which we can use multiple kernels within a single chain. Nummelin (1984, 2002) shows the
central limit theorem for samples from a Markov chain. MacEachern and Berliner (1994) show
that subsampling the samples derived from a Markov chain is suboptimal. Gelman and Rubin

12.7. Relevant Literature
543
(1992) provide a speciﬁc strategy for applying MCMC: they specify the number of chains, the
burn-in time, and the intervals at which samples should be selected. Their strategy is applicable
mostly for problems for which a good initial distribution is available, but provides insights more
broadly.
Algorithms that improve convergence are particularly relevant for the high-dimensional, mul-
timodal distributions that often arise in the setting of graphical models. Some methods for
addressing this issue use larger, nonlocal steps in the search space, which are helpful in break-
ing out of local optima; for example, for pairwise MRFs where all variables have a uniform set
of values, Swendsen and Wang (1987) and Barbu and Zhu (2005) propose moves that simulta-
neously ﬂip an entire subgraph from one value to another. Higdon (1998) discusses the general
idea of introducing auxiliary variables as a mechanism for taking larger steps in the space. The
temperature-based methods draw on the idea of simulated annealing (Kirkpatrick et al. 1983).
These methods include simulated tempering (Marinari and Parisi 1992; Geyer and Thompson
1995) in which the state of the model is augmented with a temperature variable for purposes
of sampling; parallel tempering (Swendsen and Wang 1986; Geyer 1991) runs multiple chains at
diﬀerent temperatures at the same time and allows chains to exchange datapoints; tempered
transitions (Neal 1996) proposes a new sample by moving up and down the temperature sched-
ules; and annealed importance sampling Neal (2001) uses a similar approach in combination
annealed
importance
sampling
with an importance sampling reweighting scheme.
The bugs system is an invaluable tool for the application of MCMC methods, inasmuch as it
encompasses a large class of models and implements a number of fairly advanced techniques
for improving the mixing rate. The system itself, and some of the ideas in it, are described by
Thomas et al. (1992) and Gilks et al. (1994).
The task of ﬁnding high-probability assignments in a probabilistic model is very closely related
to the problem of ﬁnding a MAP assignment. We refer the reader to section 13.9 for many of
the relevant references on that topic.
Techniques based on deterministic search were popular in the early days of the Bayesian
network community, often motivated by connections with search algorithms and constraint-
satisfaction algorithms in AI. As a few examples, Henrion (1991) proposes a search-based algo-
rithm aimed speciﬁcally at BN2O networks, whereas Poole (1993b, 1989) describes an algorithm
aimed at more general network structures. Horvitz et al. (1989) propose an algorithm that com-
bines conditioning with search, to obtain some of the beneﬁts of exact inference in a search-
based algorithm. More recently, Lerner et al. (2000) use a collapsed search-based approach for
the problem of fault diagnosis, where probabilities are also highly skewed.
The use of collapsed particles, and speciﬁcally the Rao-Blackwell theorem, for sampling-based
estimation was proposed by Gelfand and Smith (1990) and further developed by Liu et al. (1994)
and Robert and Casella (1996). This idea has since been used in many applications of sampling
techniques to graphical models, where sampling some of the variables can often allow us to
perform tractable inference over the others. The idea of sampling some of the variables in a
Bayesian network and performing exact inference over the others was explored by Bidyuk and
Dechter (2007).
We have focused our presentation here on the problem of generating samples from a distri-
bution so as to estimate the posterior. However, another task of signiﬁcant practical importance
is that of computing the partition function of an unnormalized measure. As we will see, this
partition function
task is of particular importance in learning, since the normalizing constant is often used as

544
Chapter 12. Particle-Based Approximate Inference
a measure of the overall quality of a learned model. Computation of a partition function by
directly sampling the distribution leads to estimates of high variance, since such estimates are
usually dominated by a small number of samples with high unnormalized probabilities. In order
to avoid this problem, a ratio of partition functions is computed; see exercise 12.8. An equivalent
problem of free energy diﬀerence, or partition function ratio, has been tackled in computa-
tional physics and computation chemistry communities with a range of methods, including
free energy perturbation, thermodynamic integration, bridge sampling, umbrella sampling, and
Jarzynski equation; these methods are in essence importance-sampling algorithms. See Gelman
and Meng (1998); Jarzynski (1997); Neal (2001) for some examples.
Extensive work has been done on the correspondence problem in its various incarnations:
correspondence
data association, record matching, identity resolution, and more. See, for example, Bar-Shalom
and Fortmann (1988); Bar-Shalom (1992) for a review of some of the key ideas. Pasula et al.
(1999) ﬁrst proposed the use of MCMC methods to sample the space of possible associations in
target tracking, based on the analysis of Jerrum and Sinclair (1997) for sampling over matchings.
Dellaert et al. (2003) also use MCMC for the data-association problem in a structure-from-
motion task in computer vision; they propose a more sophisticated proposal distribution that
allows more rapid mixing. Anguelov et al. (2004) suggest the use of belief propagation for solving
the correspondence problem, in settings where the correspondences are correlated with each
other. Their results form part of box 12.D. Some work has also been done on ﬁnding the MAP
assignment to a data-association problem using variants of belief propagation (Chen et al. 2003;
Duchi et al. 2006) or other methods (Lacoste-Julien et al. 2006).
12.8
Exercises
Exercise 12.1⋆
Consider the sequence of variables Tn deﬁned in appendix A.2. Given ϵ and δ, we deﬁne m(ϵ, δ) =
minn P(|Tn −p| ≥ϵ) ≤δ to be smallest number of trials we need to ensure that the probability of
deviating ϵ standard deviations from p is less than δ. Although we cannot compute m(ϵ, δ) exactly, we
can upper-bound it using the bounds we discuss earlier.
a. Use Chebyshev’s bound to give an upper bound on m(ϵ, δ). (You can use the fact that VVar[Tn] ≤
1
4n.)
b. Use the Chernoﬀbounds discussed above to give an upper bound on m(ϵ, δ).
c. What is the diﬀerence between these two bounds? How does each of these depend on ϵ and δ? What
are the implications if we want design a test to estimate p?
Exercise 12.2
Consider the problem of providing a reliable estimate for a ratio p/q, where we have estimators ˆp for p
and ˆq for q.
a. Assume that ˆp has ϵ relative error to p and ˆq has ϵ relative error to q. More precisely, ˆp ∈[(1 −
ϵ)p, (1 + ϵ)p], and ˆq ∈[(1 −ϵ)q, (1 + ϵ)q]. Provide a relative error bound on ˆp/ˆq relative to p/q.
b. Now, assume that we have only an absolute error bound for ˆp and ˆq: ˆp ∈[p−ϵ, p+ϵ], ˆq ∈[q−ϵ, q+ϵ].
Show by example that ˆp/ˆq can be an arbitrarily bad estimate for p/q.
c. What type of guarantee (if any) can you provide if ˆp has low absolute error but ˆq has low relative error?
Exercise 12.3
Assume we have a calibrated clique tree for a distribution PΦ. Show how we can use the clique tree to
generate samples that are sampled exactly from PΦ.

12.8. Exercises
545
Exercise 12.4
Prove proposition 12.2.
Exercise 12.5⋆
Let B be a Bayesian network, and X a variable that is not a root of the network. Show that PB may not
be the optimal proposal distribution for estimating PB(X = x).
Exercise 12.6
In exercise 3.12, we deﬁned the edge reversal transformation, which reverses the directionality of an edge
edge reversal
X →Y . How would you apply such a transformation in the context of likelihood weighting, and what
would be the beneﬁts?
Exercise 12.7⋆
Let B be a network, E = e an observation, and X1, . . . , Xk be some ordering (not necessarily topological)
of the unobserved variables in B. Consider a sampling algorithm that, for each X1, . . . , Xk in order,
randomly selects a value xi[m] for Xi using some distribution Q(Xi | x1[m], . . . , xi−1[m], e).
a. Write the formula for the importance weights in normalized importance sampling using this sampling
process.
b. Using your answer in part 1, deﬁne an improved likelihood weighting algorithm that samples variables
in the network in topological order, but, for a variable Xi with parents U i, samples Xi using a
distribution that uses both x−i[m] and the evidence in Xi’s Markov blanket.
c. Does your approach from part 2 generate samples from the posterior distribution P(X1, . . . , Xk | e)?
Explain.
Exercise 12.8⋆
Consider the normalized importance sampling algorithm, but now assume that, in equation (12.10), an
unnormalized measure ˜Q(X) is used in place of Q(X). Show that the average of the weights converges
to
P
X ˜
P (x)
P
X ˜
Q(X), which is the ratio of the normalizing constants of the two distributions.
Exercise 12.9
In this question, we consider the application of importance sampling to Markov networks.
a. Explain intuitively why we cannot simply apply likelihood weighting to Markov networks.
b. Show how likelihood weighting can be applied to chordal Markov networks. Is this approach interesting?
Explain.
c. Provide a technique by which the more general framework of importance sampling can be applied
to Markov networks. Be sure to deﬁne both a reasonable proposal distribution and an algorithmic
technique for computing the weights.
Exercise 12.10⋆
Consider the Grasshopper example of ﬁgure 12.3
a. Assume that the probability space is the full set of (positive and negative) integers; the transition model
is now the same for all i (and not diﬀerent for i = ±4). Assuming that the grasshopper starts from 0,
use the central limit theorem (theorem A.2) to bound the probability that the grasshopper reaches an
integer larger than
l√
2T
m
after T steps.
b. Returning to the setting where the grasshopper moves over the integers in the range ±B, try con-
structing a chain that reaches every state in time linear in B (Hint: Your chain will be nonreversible,
and it will require the addition of another variable to the state description.)

546
Chapter 12. Particle-Based Approximate Inference
Exercise 12.11
Show an example of a Markov chain where the limiting distribution reached via repeated applications of
equation (12.20) depends on the initial distribution P (0).
Exercise 12.12⋆
Consider the following two conditions on a Markov chain T :
a. It is possible to get from any state to any state using a positive probability path in the state graph.
b. For each state x, there is a positive probability of transitioning directly from x to x (a self-loop).
a. Show that, for a ﬁnite-state Markov chain, these two conditions together imply that T is regular.
b. Show that regularity of the Markov chain implies condition 1.
c. Show an example of a regular Markov chain that does not satisfy the condition 2.
d. Now let us weaken condition 2, requiring only that there exists a state x with a positive probability of
transitioning directly from x to x. Show that this weaker condition and condition 1 together still suﬃce
to ensure regularity.
Exercise 12.13
Show directly from equation (12.21) (without using the detailed balance equation) that the posterior distri-
bution P(X | e) is a stationary distribution of the Gibbs chain (equation (12.22)).
Exercise 12.14
Show that any distribution π that satisﬁes the detailed balance equation, equation (12.24), must be a
stationary distribution of T .
Exercise 12.15
Prove theorem 12.5.
Exercise 12.16
Let Val(X) be a set of states, and let T1, . . . , Tk be a set of kernels, each of which satisﬁes the detailed
balance equation relative to some stationary distribution π. Let p1, . . . , pk be any distribution over the
indexes 1, . . . , k. Prove that the mixture Markov chain T , which at each step takes a step sampled from
Ti with probability pi, also satisﬁes the detailed balance equation relative to π.
Exercise 12.17⋆
Let T1, . . . , Tk be as in exercise 12.16. Consider the aggregate Markov chain T , where each step consists
of a sequence of k steps, with step i being sampled from Ti.
a. Provide an example demonstrating that T may not satisfy the detailed balance equation relative to π.
b. Show that, nevertheless, T has the stationary distribution π.
c. Provide a multistep kernel using T1, . . . , Tk that satisﬁes the detailed balance equation relative to π.
Exercise 12.18
a. Let X be a node in a Markov network H, and let y be an assignment of values to X’s Markov blanket
Y = MBH(X). Provide an eﬃcient algorithm for computing
P(x′ | y)
P(x | y)
for x, x′ ∈Val(X). Your algorithm should involve only local parameters — potentials for cliques
involving X.
b. Show how this algorithm applies to Bayesian networks.

12.8. Exercises
547
Exercise 12.19
Show that Gibbs sampling is a special case of the Metropolis-Hastings algorithm. More precisely, pro-
vide a particular proposal distribution Qi for each local transition T Qi that induces precisely the same
distribution over the transitions taken as the associated Gibbs transition distribution Ti.
Exercise 12.20⋆
Prove theorem 12.4.
Exercise 12.21
Consider the network of example 12.12, but now assume that we make Z a slightly noisy exclusive or of
its parents. That is, with some small probability q, Z is chosen uniformly at random regardless of the
values of X and Y , and with probability 1 −q, Z is the exclusive or of X and Y . Analyze the transitions
between states in the Gibbs chain for this network, with the evidence z1, and particularly the expected
time required to transition between diﬀerent regions.
Exercise 12.22
Consider the same situation as in importance sampling, where we have an unnormalized measure ˜P(X)
from which it is hard to sample, and a proposal distribution Q which is (hopefully) close to the normalized
distribution P ∝˜P, from which we can draw independent samples. Consider a Markov chain where we
deﬁne
T (x →x′) = Q(x′) min

1, w(x′)
w(x)

for x′ ̸= x, where w(x) is as deﬁned in equation (12.10); we deﬁne T (x →x) = 1−P
x′̸=x T (x →x′).
Intuitively, the transition from x to x′ selects an independent sample x′ from Q and then moves toward
it, depending on whether its importance weight is better than that of our current point x.
a. Show that T deﬁnes a legal transition model.
b. Show that P is the stationary distribution of T .
Exercise 12.23⋆⋆
The Swendson-Wang algorithm is a Metropolis-Hastings algorithm for labeling MRFs, such as those used
Swendson-Wang
algorithm
labeling MRF
in image segmentation (box 4.B), where all variables take values in the same set of labels l = 1, . . . , L.
Unlike the standard application of Metropolis-Hastings, this variant takes large steps in the space, which
simultaneously ﬂip the values of multiple variables together.
The algorithm proceeds as follows. Let X be the variables in the network, and E the set of edges in the
pairwise MRF. Let ξ be our current state in the sampling process. We begin by partitioning the variables
X into L disjoint subsets, based on their value in ξ: Xl = {Xi : xi = l}. We now randomly select a
subset of the edges in the graph to produce a new set E′: each edge (i, j) ∈E is selected independently,
with probability qi,j. We now use E′ to partition the graph into connected components. Each set Xl is
partitioned into Kl connected components, where a connected component Xlk is a maximal subset of
nodes Xlk ⊆Xl such that each Xi, Xj ∈Xlk is connected by a path within E′. The result is a set of
connected components:
C = {Xlk : l = 1, . . . , L; k = 1, . . . , Kl}.
We now select a connected component Xlk ∈C uniformly at random. Let q(Y | ξ) be the probability
with which a set Y is selected using this procedure.
We now propose to assign Xlk a new label l′ with probability q(l′ | ξ, Xlk); note that this probability
can depend in arbitrary ways on the current state. This proposed move, if accepted, deﬁnes a new state
ξ′, where X′
i = l′ for any Xi ∈Xlk, and X′
i = Xi otherwise. Note that this proposed move ﬂips a large
number of variables at the same time, and thus it takes much larger steps in the space than a local Gibbs
or Metropolis-Hastings sampler for this MRF.
In this exercise, we show how to use this proposal distribution within the Metropolis-Hastings algorithm.

548
Chapter 12. Particle-Based Approximate Inference
a. Let ξ and ξ′ be a pair of states such that:
•
Y forms a connected component in E;
•
the variables in Y all take the value l in ξ;
•
the variables in Y all take the value l′ in ξ′;
•
all other variables in X −Y take the same value in ξ and ξ′.
Show that
q(Y | ξ)
q(Y | ξ′) =
Q
(i,j)∈E(Y ,Xl−Y )(1 −qi,j)
Q
(i,j)∈E(Y ,X′
l′ −Y )(1 −qi,j)
where: Xl is the set of vertices with label l in ξ, X′
l′ the set of vertices with label l′ in ξ′; and where
E(Y , Z) (between two disjoint sets Y , Z) is the set of edges connecting nodes in Y to nodes in Z.
b. Use this fact to obtain an acceptance probability for the proposed move that respects the detailed-
balance equation.
Exercise 12.24⋆⋆
Let us return to the setting of exercise 9.20.
a. Suppose now that you want to sample from P(X | Possible(X) = K) using a Gibbs sampling strategy.
Why is this a bad idea?
b. Can you devise a Metropolis-Hastings MCMC strategy that generates samples from the correct posterior?
Describe the proposal distribution and the acceptance probabilities, and prove that your scheme does
sample from the correct posterior. Explain intuitively why your chain is likely to work better than the
Gibbs chain. You may assume that 1 < K < N.
Exercise 12.25⋆⋆
In this exercise we develop the annealed importance sampling procedure. Assume that we want to generate
annealed
importance
sampling
samples from a distribution p(x) ∝f(x) from which sampling is hard.
Assume also that we have
a distribution q(x) ∝g(x) from which sampling is easy.
In principle, we can use q as a proposal
distribution for p, and apply importance sampling. However, if q and p are very diﬀerent, the results are
likely to be quite poor. We now construct a sequence of Markov chains that will allow us to incrementally
produce a lower-variance importance-sampling estimator.
The technique is as follows. We deﬁne a sequence of distributions, p0, . . . , pk, where pi(x) ∝fi(x), and
fi is deﬁned as:
fi(x) = p(x)βiq(x)1−βi,
where 1 = β0 > β1 > . . . > βk = 0. Note that p0 = p and pk = q. We assume that we can generate
samples from pk, and that, for each pi, i = 1, . . . , k −1, we have a Markov chain Ti whose stationary
distribution is pi. To generate a weighted sample x, w relative to our target distribution p, we follow the
following algorithm:
xk
∼
pk(X)
xi
∼
Ti(xi+1 →X)
i = (k −1), . . . , 1.
(12.37)
Finally, we deﬁne our sample to be x = x1, with weight
w =
k
Y
i=1
fi−1(xi)
fi(xi) .
(12.38)
To prove that these importance weights are correct, we deﬁne both a target distribution and a proposal
distribution over the larger state space (x1, . . . , xk). We then show that the importance weights deﬁned
in equation (12.38) are correct relative to these distributions over the larger space.

12.8. Exercises
549
a. Let
T −1
i
(x →x′) = Ti(x′ →x)fi(x′)
fi(x)
deﬁne the reversal of the transition model deﬁned by Ti.
Show that T −1
i
(X →X′) is a valid
transition model.
b. Deﬁne
f ∗(x1, . . . , xk) = f0(x1)
k−1
Y
i=1
T −1
i
(xi →xi+1),
and deﬁne p∗(x1, . . . , xk) ∝f ∗(x1, . . . , xk).
Use your answer from above to conclude that
p∗(x1) = p(x1).
c. Let g∗be the function encoding the joint distribution from which x1, . . . , xk are sampled in the
annealed importance sampling procedure equation (12.37). Show that the weight in equation (12.38) can
be obtained as
f ∗(x1, . . . , xk)
g∗(x1, . . . , xk) .
One can show, under certain assumptions, that the variance of the weights obtained by this procedure
grows linearly in the dimension n of the number of variables X, whereas the variance in a traditional
importance sampling procedure grows exponentially in n.
Exercise 12.26
This exercise explores one heuristic approach for deterministic search in a Bayesian network. It is an
intermediate method between full-particle search and collapsed-particle search: It uses partial instantiations
as particles but does not perform inference on the resulting conditional distribution.
Assume that our goal is to provide upper and lower bounds on the probability of some event y in a
Bayesian network B over X. Let X1, . . . , Xn be some topological ordering of X. We enumerate particles
that are partial assignments to X, where each partial assignment instantiates some subset X1, . . . , Xk;
note that the set X1, . . . , Xk is not an arbitrary subset of X1, . . . , Xn, but rather the ﬁrst k variables in
the ordering. Diﬀerent partial assignments may instantiate diﬀerent preﬁxes of the variables. We organize
these partial assignments in a tree, where each node is labeled with some partial assignment (x1, . . . , xk).
The children of a node labeled (x1, . . . , xk) are (x1, . . . , xk, xk+1), for each xk+1 ∈Val(Xk+1). We can
iteratively grow the tree by choosing some leaf in the tree, corresponding to an assignment (x1, . . . , xk),
and expanding the tree to include its children (x1, . . . , xj, xk+1) for all possible values xk+1.
Consider a particular tree, with a set of leaves L = {ℓ[1], . . . , ℓ[M]}, where each leaf ℓ[m] ∈L is
associated with the assignment x[m] to some subset of variables X[m].
a. Each leaf ℓ[m] in the tree deﬁnes a particle. Specify the assignment and probability associated with
this particle, and describe how we would compute its probability eﬃciently.
b. Show how to use your probability estimates from part 1 (a) to provide both a lower and an upper bound
for P(y).
c. Based on your answer from part 1, provide a simple heuristic for choosing the next leaf to expand in
the partial search tree.
Exercise 12.27⋆⋆
Consider the application of collapsed Gibbs sampling, where we use a clique tree to manipulate the
conditional distribution ˜P(Xd | Xp). Develop an algorithm in which, after an initial calibration step, all
of the variables Xi ∈Xp in can be resampled using a single pass over the clique tree. (Hint: Use the
algorithm developed in exercise 10.12.)

550
Chapter 12. Particle-Based Approximate Inference
Exercise 12.28
Consider the setting of example 12.18, where we assume that all grades are observed but none of the Ij
or Dk variables are observed. Show how you would use the set of collapsed samples generated in this
example to compute the expected value of the number of smart students (i1) who got a grade of a C (g3)
in an easy class (d0).
Exercise 12.29⋆
Consider the data-association problem described in box 12.D: We have two sets of objects U = {u1, . . . , uk}
and another V = {v1, . . . , vm}, and we wish to map U’s to V’s. We have a set of observed features Bi
for each object ui, and a set of hidden attributes Aj for each vj. We have a prior P(Aj), and a set
of factors φi(Aj, Bi, Ci) such that φi(aj, bi, Ci) = 1 for all aj, bi if Ci ̸= j. The model contains no
other potentials.
We wish to compute the posterior over Aj using collapsed Gibbs sampling, where we sample the Ci’s but
maintain a closed-form posterior over the Aj’s. Provide a sampling scheme for this task, showing clearly
both the sampling distribution for the Ci variables and the computation of the closed form over the Ai
variables given the assignment to the Ci’s.

13
MAP Inference
13.1
Overview
So far, we have dealt solely with conditional probability queries. However, MAP queries, which
we deﬁned in section 2.1.5, are also very useful in a variety of applications. As a reminder, a MAP
query aims to ﬁnd the most likely assignment to all of the (non-evidence) variables. A marginal
MAP query aims to ﬁnd the most likely assignment to a subset of the variables, marginalizing
out over the rest.
MAP queries are often used as a way of “ﬁlling in” unknown information. For example, we
might be trying to diagnose a complex device, and we want to ﬁnd a single consistent hypothesis
about failures in diﬀerent components that explains the observed behavior. Another example
arises when we are trying to decode messages transmitted over a noisy channel. In such cases,
the receiver observes a sequence of bits received over the channel, and then it attempts to ﬁnd
the most likely assignment of input bits that could have generated this observation (taking into
account the code used and a model of the channel noise). This type of query is much better
viewed as a MAP query than as a standard probability query, because we are not interested
in the most likely values for the individual bits sent, but rather in the message whose overall
probability is highest. A similar phenomenon arises in speech recognition, where we are trying
to decode the most likely utterance given the (noisy) acoustic signal; here also we are not
interested in the most likely value of individual phonemes uttered.
13.1.1
Computational Complexity
As for the case of conditional probability queries, it is instructive to analyze the computational
complexity of the problem. There are many possible ways of formulating the MAP problem as a
decision problem. One that is convenient for our purposes is the problem BN-MAP-DP, deﬁned
as follows:
Given a Bayesian network B over X and a number τ, decide whether there exists an
assignment x to X such that P(x) > τ.
It turns out that a very similar construction to theorem 9.1 can be used to show that the
BN-MAP-DP problem is also NP-complete.
Theorem 13.1
The decision problem BN-MAP-DP is NP-complete

552
Chapter 13. MAP Inference
The proof is left as an exercise (exercise 13.1).
We can also deﬁne an analogous decision problem BN-margMAP-DP for marginal MAP:
Given a Bayesian network B over X, a number τ, and a subset Y ⊂X, decide whether
there exists an assignment y to Y such that P(y) > τ.
Because marginal MAP is a generalization of MAP, we immediately conclude the following:
Corollary 13.1
The decision problem BN-margMAP-DP is NP-hard.
However, for the case of marginal MAP, we cannot conclude that BN-margMAP-DP is in NP.
Intuitively, as we said, the marginal MAP problem involves elements of both maximization and
summation, a combination that is signiﬁcantly harder than either subtask in isolation. In fact, it
is possible to show that BN-margMAP-DP is complete for a much harder complexity class:
Theorem 13.2
The decision problem BN-margMAP-DP is complete for NPPP.
Deﬁning the complexity class NPPP is outside the scope of this book (see section 9.8), but it
is generally considered very hard, since it is known to contain the entire polynomial hierarchy,
of which NP is only the ﬁrst level.
While the “harder” complexity class of the marginal MAP problem indicates that it is more
diﬃcult, the implications of this formulation may be somewhat abstract.
A more concrete
ramiﬁcation is the following result, which states that the marginal MAP problem is NP-hard
even for polytree networks:
Theorem 13.3
The following decision problem is NP-hard:
Given a polytree Bayesian network B over X, a subset Y ⊂X, and a number τ, decide
polytree
whether there exists an assignment y to Y such that P(y) > τ.
We defer the justiﬁcation for this result to section 13.2.3.
13.1.2
Overview of Solution Methods
As for conditional probability queries, when addressing MAP queries, it is useful to reformulate
the joint distribution somewhat more abstractly, as a product of factors. Consider a distribution
PΦ(X) deﬁned via a set of factors Φ and an unnormalized density ˜PΦ. We need to compute:
ξmap = arg max
ξ
PΦ(ξ) = arg max
ξ
1
Z
˜PΦ(ξ) = arg max
ξ
˜PΦ(ξ).
(13.1)
In particular, if PΦ(X) = P(X | e), then we aim to maximize P(X, e).
The MAP task goes hand in hand with ﬁnding the value of the unnormalized probability of
the most likely assignment: maxξ ˜PΦ(ξ). We note that, given an assignment ξ, we can easily
compute its unnormalized probability simply by multiplying all of the factors in Φ, evaluated
at ξ. However, we cannot retrieve the actual probability of ξ without computing the partition
function, a problem that requires that we also solve the sum-product task.
Because ˜PΦ is a product of factors, tasks that involve maximizing ˜PΦ are often called max-
max-product

13.1. Overview
553
product inference tasks. Note that we often convert the max-product problem into log-space
and maximize log ˜PΦ. This logarithm is a sum of factors that correspond to negative energies
(see section 4.4.1.2), and hence this version of the problem is often called the max-sum problem.
max-sum
It is also common to negate the factors and minimize the sum of the energies for the diﬀerent
potentials; this version is generally called an energy minimization problem. The transformation
energy
minimization
into log-space has several signiﬁcant advantages. First, it avoids the numerical issues associated
with multiplying many small numbers together. More importantly, it transforms the problem
into a linear one; as we will see, this transformation allows certain valuable tools to be brought
to bear. For consistency with the rest of the book, we mostly use the max-product variant
of the problem in the remainder of this chapter. However, all of our discussion carries over
with minimal changes to the analogous max-sum (or min-sum) problem: we simply take the
logarithm of all factors, and replace factor product steps with factor additions.
Many diﬀerent algorithms, both exact and approximate, have been proposed for addressing
the MAP problem. Most obviously, the goal of the MAP task is ﬁnd an assignment to a set
of variables whose score (unnormalized probability) is maximal. Thus, it is an instance of an
optimization problem (see appendix A.4.1), a class of problems for which many general-purpose
solutions have been developed.
These methods include heuristic hill-climbing methods (see
appendix A.4.2), as well as more specialized optimization methods. Some of these solutions
have also been usefully applied to the MAP problem.
There are also many algorithms that are speciﬁcally targeted at the max-product (or min-
sum) task, and exploit some of its special structure, most notably the connection to the graph
representation. A large subset of algorithms operate by ﬁrst computing a set of factors that are
max-marginals. Max-marginals are a general notion that can be deﬁned for any function:
Deﬁnition 13.1
The max-marginal of a function f relative to a set of variables Y is:
max-marginal
MaxMargf(y) = max
ξ⟨Y ⟩=y f(ξ),
(13.2)
for any assignment y ∈Val(Y ).
For example, the max-marginal MaxMarg ˜
PΦ(Y ) is a factor that determines a value for each
assignment y to Y ; this value is the unnormalized probability of the most likely joint assignment
consistent with y.
A large class of MAP algorithms proceed by ﬁrst computing an exact or approximate set of
max-marginals for all of the variables in X, and then attempting to extract an exact or approx-
imate MAP assignment from these max-marginals. The ﬁrst phase generally uses techniques
such as variable elimination or message passing in clique trees or cluster graphs, algorithms
similar to those we applied in the context of sum-product inference. Now, assume we have a
set of (exact or approximate) max-marginals {MaxMargf(Xi)}Xi∈X . A key question is how we
use those max-marginals to construct an overall assignment. As we show, the computation of

(approximate) max-marginals allows us to solve a global optimization problem as a set of
local optimization problems for individual variables. This task, known as decoding, is to
decoding
max-marginals
construct a joint assignment that locally optimizes each of the beliefs. If we can construct
such an assignment, we will see that we can provide guarantees on its (strong local or
even global) optimality. One such setting is when the max-marginals are unambiguous: For
unambiguous

554
Chapter 13. MAP Inference
each variable Xi, there is a unique x∗
i that maximizes:
x∗
i = arg
max
xi∈Val(Xi) MaxMargf(xi).
(13.3)
When the max-marginals are unambiguous, identifying the locally optimizing assignment is
easy. When they are ambiguous, the solution is nontrivial even for exact max-marginals, and
can require an expensive computational procedure in its own right.
The marginal MAP problem appears deceptively similar to the MAP task. Here, we aim to ﬁnd
the assignment whose (conditional) marginal probability is maximal. Here, we partition X into
two disjoint subsets, X = Y ∪W , and aim to compute:
ym-map = arg max
y
PΦ(y) = arg max
y
X
W
˜PΦ(y, W ).
(13.4)
Thus, the marginal MAP problem involves both multiplication and summation, a com-

bination that makes the task much more diﬃcult, both theoretically and in practice. In
particular, exact inference methods such as variable elimination can be intractable, even
in simple networks. And many of the approximate methods that have been developed for MAP
queries do not extend easily to marginal MAP. So far, the only eﬀective approximation technique
for the marginal MAP task uses a heuristic search over the assignments y, while employing
some (exact or approximate) sum-product inference over W in the inner loop.
13.2
Variable Elimination for (Marginal) MAP
We begin our discussion with the most basic inference algorithm: variable elimination. We ﬁrst
present the simpler case of pure MAP queries, which turns out to be quite straightforward. We
then discuss the issues that arise in marginal MAP queries.
13.2.1
Max-Product Variable Elimination
To gain some intuition for the MAP problem, let us begin with a very simple example.
Example 13.1
Consider the Bayesian network A →B. Assume we have no evidence, so that our goal is to
compute:
max
a,b P(a, b)
=
max
a,b P(a)P(b | a)
=
max
a
max
b
P(a)P(b | a).
Consider any particular value a of A, and let us consider possible completions of that assignment.
Among all possible completions, we want to pick one that maximizes the probability:
max
b
P(a)P(b | a) = P(a) max
b
P(b | a).
Thus, a necessary condition for our assignment a, b to have the maximum probability is that B
must be chosen so as to maximize P(b | a). Note that this condition is not suﬃcient: we must also
choose the value of A appropriately; but for any choice of A, we must choose B as described.

13.2. Variable Elimination for (Marginal) MAP
555
a1
a1
a1
a1
a2
a2
a2
a2
a3
a3
a3
a3
b1
b1
b2
b2
b1
b1
b2
b2
b1
b1
b2
b2
c1
c2
c1
c2
c1
c2
c1
c2
c1
c2
c1
c2
0.25
0.35
0.08
0.16
0.05
0.07
  0
  0
0.15
0.21
0.09
0.18
a1
a1
a2
a2
a3
a3
c1
c2
c1
c2
c1
c2
0.25
0.35
0.05
0.07
0.15
0.21
Figure 13.1
Example of the max-marginalization factor operation for variable B
Let φ(a) denote the internal expression maxb P(b | a). For example, consider the following
assignment of parameters:
a0
a1
0.4
0.6
A
b0
b1
a0
0.1
0.9
a1
0.55
0.45.
(13.5)
In this case, we have that φ(a1) = maxb P(b | a1) = 0.55 and φ(a0) = maxb P(b | a0) = 0.9.
To compute the max-marginal over A, we now compute:
max
a
P(a)φ(a) = max [0.4 · 0.9, 0.6 · 0.55] = 0.36.
As in the case of sum-product queries, we can reinterpret the computation in this example in
terms of factors. We deﬁne a new operation on factors, as follows:
Deﬁnition 13.2
Let X be a set of variables, and Y ̸∈X a variable. Let φ(X, Y ) be a factor. We deﬁne the factor
factor
maximization
maximization of Y in φ to be factor ψ over X such that:
ψ(X) = max
Y
φ(X, Y ).
The operation over the factor P(B | A) in example 13.1 is performing φ(A) = maxB P(B | A).
Figure 13.1 presents a somewhat larger example.
The key observation is that, like equation (9.6), we can sometimes exchange the order of
maximization and product operations: If X ̸∈Scope[φ1], then
max
X (φ1 · φ2) = φ1 · max
X φ2.
(13.6)
In other words, we can “push in” a maximization operation over factors that do not involve the
variable being maximized. A similar property holds for exchanging a maximization with a factor

556
Chapter 13. MAP Inference
Step
Variable
Factors
Intermediate
New
eliminated
used
factor
factor
1
S
φS(I, S)
ψ1(I, S)
τ1(I)
2
I
φI(I), φG(G, I, D), τ1(I)
ψ2(G, I, D)
τ2(G, D)
3
D
φD(D), τ2(G, D)
ψ3(G, D)
τ3(G)
4
L
φL(L, G)
ψ4(L, G)
τ4(G)
5
G
τ4(G), τ3(G)
ψ5(G)
τ5(∅)
Table 13.1
A run of max-product variable elimination
summation operation: If X ̸∈Scope[φ1], then
max
X (φ1 + φ2) = φ1 + max
X φ2.
(13.7)
This insight leads directly to a max-product variable elimination algorithm, which is directly
max-product
variable
elimination
analogous to the algorithm in algorithm 9.1. The diﬀerence is that in line 4, we replace the
expression P
Z ψ with the expression maxZ ψ. The algorithm is shown in algorithm 13.1. The
same template also covers max-sum, if we replace product of factors with addition of factors.
If Xi is the ﬁnal variable in this elimination process, we have maximized all variables other
than Xi, so that the resulting factor φXi is the max-marginal over Xi.
Example 13.2
Consider again our very simple Student network, shown in ﬁgure 3.4. Our goal is to compute
the most likely instantiation to the entire network, without evidence. We will use the elimination
ordering S, I, D, L, G.
Note that, unlike the case of sum-product queries, we have no query
variables, so that all variables are eliminated.
The computation generates the factors shown in table 13.1. For example, the ﬁrst step would
compute τ1(I) = maxs φS(I, s). Speciﬁcally, we would get τ1(i0) = 0.95 and τ1(i1) = 0.8.
Note, by contrast, that the same factor computed with summation instead of maximization would
give τ1(I) ≡1, as we discussed.
The ﬁnal factor, τ5(∅), is simply a number, whose value is
max
S,I,D,L,G P(S, I, D, L, G).
For this network, we can verify that the value is 0.184.
The factors generated by max-product variable elimination have an identical structure to those
generated by the sum-product algorithm using the same ordering. Thus, our entire analysis

of the computational complexity of variable elimination, which we performed for sum-
product in section 9.4, applies unchanged. In particular, we can use the same algorithms
for ﬁnding elimination orderings, and the complexity of the execution is precisely the
same induced width as in the sum-product case. We can also use similar ideas to exploit
structure in the CPDs; see, for example, exercise 13.2.
13.2.2
Finding the Most Probable Assignment
We now tackle the original MAP problem: decoding, or ﬁnding the most likely assignment itself.
decoding

13.2. Variable Elimination for (Marginal) MAP
557
Algorithm 13.1 Variable elimination algorithm for MAP. The algorithm can be used both in
its max-product form, as shown, or in its max-sum form, replacing factor product with factor
addition.
Procedure Max-Product-VE (
Φ,
// Set of factors over X
≺
// Ordering on X
)
1
Let X1, . . . , Xk be an ordering of X such that
2
Xi ≺Xj iﬀi < j
3
for i = 1, . . . , k
4
(Φ, φXi) ←Max-Product-Eliminate-Var(Φ, Xi)
5
x∗←Traceback-MAP({φXi : i = 1, . . . , k})
6
return x∗, Φ
// Φ contains the probability of the MAP
Procedure Max-Product-Eliminate-Var (
Φ,
// Set of factors
Z
// Variable to be eliminated
)
1
Φ′ ←{φ ∈Φ : Z ∈Scope[φ]}
2
Φ′′ ←Φ −Φ′
3
ψ ←Q
φ∈Φ′ φ
4
τ ←maxZ ψ
5
return (Φ′′ ∪{τ}, ψ)
Procedure Traceback-MAP (
{φXi : i = 1, . . . , k}
)
1
for i = k, . . . , 1
2
ui ←(x∗
i+1, . . . , x∗
k)⟨Scope[φXi] −{Xi}⟩
3
// The maximizing assignment to the variables eliminated after
Xi
4
x∗
i ←arg maxxi φXi(xi, ui)
5
// x∗
i is chosen so as to maximize the corresponding entry in
the factor, relative to the previous choices ui
6
return x∗
As we have discussed, the result of the computation is a max-marginal MaxMarg ˜
PΦ(Xi) over
the ﬁnal uneliminated variable, Xi.
We can now choose the maximizing value x∗
i for Xi.
Importantly, from the deﬁnition of max-marginals, we are guaranteed that there exists some
assignment ξ∗consistent with x∗
i . But how do we construct such an assignment?
We return once again to our simple example:
Example 13.3
Consider the network of example 13.1, but now assume that we wish to ﬁnd the actual assignment
a∗, b∗= arg maxA,B P(A, B). As we discussed, we ﬁrst compute the internal maximization

558
Chapter 13. MAP Inference
maxb P(a, b). This computation tells us, for each value of a, which value of b we must choose to
complete the assignment in a way that maximizes the probability. In our example, the maximizing
value of B for a1 is b0, and the maximizing value of B for a0 is b1.
However, we cannot
actually select the value of B at this point, since we do not yet know the correct (maximizing)
value of A. We therefore proceed with the computation of example 13.1, and compute both the
max-marginal over A, maxa P(a)φ(a), and the value a that maximizes this expression. In this
case, P(a1)φ(a1) = 0.6 · 0.55 = 0.33, and P(a0)φ(a0) = 0.4 · 0.9 = 0.36. The maximizing
value a∗of A is therefore a0. The key insight is that, given this value of A, we can now go back
and select the corresponding value of B — the one that maximizes φ(a∗). Thus, we obtain that
our maximizing assignment is a0, b1, as expected.
The key intuition in this computation is that, as we eliminate variables, we cannot determine
their maximizing value. However, we can determine a “conditional” maximizing value — their
maximizing value given the values of the variables that have not yet been eliminated. When
we pick the value of the ﬁnal variable, we can then go back and pick the values of the other
variables accordingly. For the last variable eliminated, say X, the factor for the value x contains
the probability of the most likely assignment that contains X = x. Thus, we correctly select
the most likely assignment to X, and therefore to all the other variables. This process is called
traceback of the solution.
traceback
The algorithm implementing this intuition is shown in algorithm 13.1. Note that the operation
in line 2 of Traceback-MAP is well deﬁned, since all of the variables remaining in Scope[φXi]
were eliminated after Xi, and hence must be within the set {Xi+1, . . . , Xk}. We can show that
the algorithm returns the MAP:
Theorem 13.4
The algorithm of algorithm 13.1 returns
x∗= arg max
x
Y
φ∈Φ
φ,
and Φ, which contains a single factor of empty scope whose value is:
max
x
Y
φ∈Φ
φ.
The proof follows in a straightforward way from the preceding intuitions, and we leave it as an
exercise (exercise 13.3).
We note that the traceback procedure is not an expensive one, since it simply involves a
linear traversal over the factors deﬁned by variable elimination. In each case, when we select
a value x∗
i for a variable Xi in line 2, we are guaranteed that x∗
i is, indeed, a part of a jointly
coherent MAP assignment. Thus, we will never need to backtrack and revisit this decision, trying
a diﬀerent value for Xi.
Example 13.4
Returning to example 13.2, we now consider the traceback phase. We begin by computing g∗=
arg maxg ψ5(g). It is important to remember that g∗is not the value that maximizes P(G). It is
the value of G that participates in the most likely complete assignment to all the network variables
X = {S, I, D, L, G}. Given g∗, we can now compute l∗= arg maxl ψ4(g∗, l). The value l∗is

13.2. Variable Elimination for (Marginal) MAP
559
the value of L in the most likely complete assignment to X. We use the same procedure for the
remaining variables. Thus,
d∗
=
arg max
d
ψ3(g∗, d)
i∗
=
arg max
i
ψ2(g∗, i, d∗)
s∗
=
arg max
s
ψ1(i∗, s).
It is straightforward (albeit somewhat tedious) to verify that the most likely assignment is d1, i0, g3,
s0, l0, and its probability is (approximately) the value 0.184 that we obtained in the ﬁrst part of
the computation.
The additional step of computing the actual assignment does not add signiﬁcant time com-
plexity to the basic max-product task, since it simply does a second pass over the same set of
factors computed in the max-product pass. With an appropriate choice of data structures, this
cost can be linear in the number n of variables in the network. The cost in terms of space is
a little greater, inasmuch as the MAP pass requires that we store the intermediate results in the
max-product computation. However, the total cost is at most a factor of n greater than the cost
of the computation without this additional storage.
The algorithm of algorithm 13.1 ﬁnds the one assignment of highest probability. This assign-
ment gives us the single most likely explanation of the situation. In many cases, however, we
want to consider more than one possible explanation. Thus, a common task is to ﬁnd the set
of the K most likely assignments. This computation can also be performed using the output of
a run of variable elimination, but the algorithm is signiﬁcantly more intricate. (See exercise 13.5
for one simpler case.) An alternative approach is to use one of the search-based algorithms that
we discuss in section 13.7.
13.2.3
Variable Elimination for Marginal MAP ⋆
We now turn our attention to the application of variable elimination algorithms to the marginal
MAP problem. Recall that our marginal MAP problem can be written as arg maxy
P
W ˜PΦ(y, W ),
where y ∪W = X, so that ˜PΦ(y, W ) is a product of factors in some set Φ. Thus, our com-
putation has the following max-sum-product form:
max-sum-product
max
Y
X
W
Y
φ∈Φ
φ.
(13.8)
This form immediately suggests a variable elimination algorithm, along the lines of similar
algorithms for sum-product and max-product. This algorithm simply puts together the ideas
we used for probability queries on one hand and MAP queries on the other. Speciﬁcally, the
summations and maximizations outside the product can be viewed as operations on factors.
Thus, to compute the value of this expression, we simply have to eliminate the variables W
by summing them out, and the variables in Y by maximizing them out. When eliminating a
variable X, whether by summation or by maximization, we simply multiply all the factors whose
scope involves X, and then eliminate X to produce the resulting factor. Our ability to perform
this step is justiﬁed by the exchangeability of factor summation/maximization and factor product
(equation (9.6) and equation (13.6)).

560
Chapter 13. MAP Inference
Example 13.5
Consider again the network of ﬁgure 3.4, and assume that we wish to ﬁnd the probability of the
most likely instantiation of SAT result and letter quality:
max
S,L
X
G,I,D
P(I, D, G, S, L).
We can perform this computation by eliminating the variables one at a time, as appropriate.
Speciﬁcally, we perform the following operations:
ψ1(I, G, D)
=
φD(D) · φG(G, I, D)
τ1(I, G)
=
X
D
ψ1(I, G, D)
ψ2(S, G, I)
=
φI(I) · φS(S, I) · τ1(I, G)
τ2(S, G)
=
X
I
ψ2(S, G, I)
ψ3(S, G, L)
=
τ2(S, G) · φL(L, G)
τ3(S, L)
=
X
G
ψ3(S, G, L)
ψ4(S, L)
=
τ3(S, L)
τ4(L)
=
max
S
ψ4(S, L)
ψ5(L)
=
τ4(L)
τ5(∅)
=
max
L
ψ5(L).
Note that the ﬁrst three factors τ1, τ2, τ3 are generated via the operation of summing out, whereas
the last two are generated via the operation of maxing out.
This process computes the unnormalized probability of the marginal MAP assignment. We
can ﬁnd the most likely values to the max-variables exactly as we did in the case of MAP: We
simply keep track of the factors associated with them, and then we work our way backward to
compute the most likely assignment; see exercise 13.4.
Example 13.6
Continuing our example, after completing the diﬀerent elimination steps, we compute the value
l∗= arg maxl ψ5(L). We then compute s∗= arg maxs ψ4(s, l∗).
The similarity between this algorithm and the previous variable elimination algorithms we
described may naturally lead one to conclude that the computational complexity is also similar.
Unfortunately, that is not the case: this process is computationally much more expensive than
the corresponding variable elimination process for pure sum-product or pure max-product. The
diﬃculty stems from the fact that we are not free to choose an arbitrary elimination ordering.
When summing out variables, we can utilize the fact that the operations of summing out diﬀerent
variables commute. Thus, when performing summing-out operations for sum-product variable

13.2. Variable Elimination for (Marginal) MAP
561
X2
X1
Xn
Y2
Y1
Yn
. . .
Figure 13.2
A network where a marginal MAP query requires exponential time
elimination, we could sum out the variables in any order. Similarly, we could use the same
freedom in the case of max-product elimination. Unfortunately, the max and sum operations do
not commute (exercise 13.19). Thus, in order to maintain the correct semantics of marginal MAP
queries, as speciﬁed in equation (13.4), we must perform all the variable summations before we
can perform any of the variable maximizations.
As we saw in example 9.1, diﬀerent elimination orderings can induce very diﬀerent widths.
When we constrain the set of legal elimination orderings, we have a smaller range of

constrained
elimination
ordering
possibilities, and even the best elimination ordering consistent with the constraint might
have signiﬁcantly larger width than a good unconstrained ordering.
Example 13.7
Consider the network shown in ﬁgure 13.2, and assume that we wish to compute
ym-map = arg max
Y1,...,Yn
X
X1,...,Xn
P(Y1, . . . , Yn, X1, . . . , Xn).
As we discussed, we must ﬁrst sum out X1, . . . , Xn, and only then deal with the maximization
over the Yi’s. Unfortunately, the factor generated after summing out all of the Xi’s contains all of
their neighbors, that is, all of the Yi’s. This factor is exponential in n. By contrast, the minimal
induced width of this network is 2, so that any probability query (assuming a small number of
query variables) or MAP query can be performed on this network in linear time.
As we can see, even on very simple polytree networks, elimination algorithms can

require exponential time to solve a marginal MAP query. One might hope that this blowup
is a consequence of the algorithm we use, and that perhaps a more clever algorithm would
avoid this problem. Unfortunately, theorem 13.3 shows that this diﬃculty is unavoidable, and
unless P = NP, some exact marginal MAP computation require exponential time, even in
very simple networks. Importantly, however, we must keep in mind that this result does not
aﬀect every marginal MAP query. Depending on the structure of the network and the choice
of maximization variables, the additional cost induced by the constrained elimination ordering
may or may not be prohibitive.
Putting aside the issue of computational cost, once we have executed a run of variable elimi-
nation for the marginal MAP problem, the task of ﬁnding the actual marginal MAP assignment
can be addressed using a traceback procedure that is directly analogous to Traceback-MAP of
traceback
algorithm 13.1; we leave the details as an exercise (exercise 13.4).

562
Chapter 13. MAP Inference
Algorithm 13.2 Max-product message computation for MAP
Procedure Max-Message (
i,
// sending clique
j
// receiving clique
)
1
ψ(Ci) ←ψi · Q
k∈(Nbi−{j}) δk→i
2
τ(Si,j) ←maxCi−Si,j ψ(Ci)
3
return τ(Si,j)
13.3
Max-Product in Clique Trees
We now extend the ideas used in the MAP variable elimination algorithm to the case of clique
trees. As for the case of sum-product, the beneﬁt of the clique tree algorithm is that it uses
dynamic programming to compute an entire set of marginals simultaneously. For sum-product,
we used clique trees to compute the sum-marginals over each of the cliques in our tree. Here,
we compute a set of max-marginals over each of those cliques.
At this point, one might ask why we want to compute an entire set of max-marginals si-
multaneously. After all, if our only task is to compute a single MAP assignment, the variable
elimination algorithm provides us with a method for doing so.
There are two reasons for
considering this extension.
First, a set of max-marginals can be a useful indicator for how conﬁdent we are in particular
components of the MAP assignment. Assume, for example, that our variables are binary-valued,
and that the max-marginal for X1 has MaxMarg(x1
1) = 3 and MaxMarg(x0
1) = 2.95, whereas
the max-marginal for X2 has MaxMarg(x1
2) = 3 and MaxMarg(x0
2) = 1. In this case, we know
that there is an alternative joint assignment whose probability is very close to the optimum,
in which X1 takes a diﬀerent value; by contrast, the best alternative assignment in which X2
takes a diﬀerent value has a much lower probability. Note that, without knowing the partition
function, we cannot determine the actual magnitude of these diﬀerences in terms of probability.
But we can determine the relative diﬀerence between the change in X1 and the change in X2.
Second, in many cases, an exact solution to the MAP problem via a variable elimination
procedure is intractable. In this case, we can use message passing procedures in cluster graphs,
similar to the clique tree procedure, to compute approximate max-marginals. These pseudo-
pseudo-max-
marginal
max-marginals can be used for selecting an assignment; while this assignment is not generally
the MAP assignment, we can nevertheless provide some guarantees in certain cases. As before,
our task has two parts: computing the max-marginals and decoding them to extract a MAP
assignment. We describe each of those steps in turn.
13.3.1
Computing Max-Marginals
In the same way that we used dynamic programming to modify the sum-product variable
elimination algorithm to the case of clique trees, we can also modify the max-product algorithm
to deﬁne a max-product belief propagation algorithm in clique trees. The resulting algorithm
max-product
belief
propagation
executes precisely the same initialization and overall message scheduling as in the sum-product

13.3. Max-Product in Clique Trees
563
belief propagation algorithm of algorithm 10.2; the only diﬀerence is the use of max-product
max-product
message passing
rather than sum-product message passing, as shown in algorithm 13.2; as for variable elimination,
the procedure has both a max-product and a max-sum variant.
As for sum-product message passing, the algorithm will converge after a single upward and
downward pass. After those steps, the resulting clique tree T will contain the appropriate
max-marginal in every clique.
max-marginal
Proposition 13.1
Consider a run of the max-product clique tree algorithm, where we initialize with a set of factors
Φ. Let βi be a set of beliefs arising from an upward and downward pass of this algorithm. Then
for each clique Ci and each assignment ci to Ci, we have that
βi(ci) = MaxMarg ˜
PΦ(ci).
(13.9)
That is, the clique belief contains, for each assignment ci to the clique variables, the (unnormal-
ized) measure ˜PΦ(ξ) of the most likely assignment ξ consistent with ci. The proof is exactly
the same as the proof of theorem 10.3 and corollary 10.2 for sum-product clique trees, and
so we do not repeat the proof. Note that, because the max-product message passing process
does not compute the partition function, we cannot derive from these max-marginals the ac-
tual probability of any assignment; however, because the partition function is a constant, we
can still compare the values associated with diﬀerent assignments, and therefore compute the
assignment ξ that maximizes ˜PΦ(ξ).
Because max-product message passing over a clique tree produces max-marginals in every
clique, and because max-marginals must agree, it follows that any two adjacent cliques must
agree on their sepset:
max
Ci−Si,j βi =
max
Cj−Si,j βj = µi,j(Si,j).
(13.10)
In this case, the clusters are said to be max-calibrated. We say that a clique tree is max-calibrated
max-calibrated
if all pairs of adjacent cliques are max-calibrated.
Corollary 13.2
The beliefs in a clique tree resulting from an upward and downward pass of the max-product clique
tree algorithm are max-calibrated.
Example 13.8
Consider, for example, the Markov network of example 3.8, whose joint distribution is shown in
ﬁgure 4.2. One clique tree for this network consists of the two cliques {A, B, D} and {B, C, D},
with the sepset {B, D}. The max-marginal beliefs for the clique and sepset for this example are
shown in ﬁgure 13.3. We can easily conﬁrm that the clique tree is calibrated.
We can also deﬁne a max-product belief update message passing algorithm that is entirely
max-product
belief update
analogous to the belief update variant of sum-product message passing. In particular, in line 1
of algorithm 10.3, we simply replace the summation with the maximization operation:
σi→j ←
max
Ci−Si,j βi.
The remainder of the algorithm remains completely unchanged. As in the sum-product case,
the max-product belief propagation algorithm and the max-product belief update algorithm

564
Chapter 13. MAP Inference
Assignment
maxC
a0
b0
d0
300, 000
a0
b0
d1
300, 000
a0
b1
d0
5, 000, 000
a0
b1
d1
500
a1
b0
d0
100
a1
b0
d1
1, 000, 000
a1
b1
d0
100, 000
a1
b1
d1
100, 000
Assignment
maxA,C
b0
d0
300, 000
b0
d1
1, 000, 000
b1
d0
5, 000, 000
b1
d1
100, 000
Assignment
maxA
b0
c0
d0
300, 000
b0
c0
d1
1, 000, 000
b0
c1
d0
300, 000
b0
c1
d1
100
b1
c0
d0
500
b1
c0
d1
100, 000
b1
c1
d0
5, 000, 000
b1
c1
d1
100, 000
β1(A, B, D)
µ1,2(B, D)
β2(B, C, D)
Figure 13.3
The max-marginals for the Misconception example. Listed are the beliefs for the two
cliques and the sepset.
are exactly equivalent. Thus, we can show that the analogue to equation (10.9) holds also for
max-product:
µi,j(Si,j) = δj→i(Si,j) · δi→j(Si,j).
(13.11)
In particular, this equivalence holds at convergence, so that a clique’s max-marginal over a sepset
can be computed from the max-product messages.
13.3.2
Message Passing as Reparameterization
Somewhat surprisingly, as for the sum-product case, we can view the max-product message
passing steps as reparameterizing the original distribution, in a way that leaves the distribution
reparameteriza-
tion
invariant. More precisely, we view a set of beliefs βi and sepset messages µi,j in a max-product
clique tree as deﬁning a measure using equation (10.11), precisely as for sum-product trees:
clique tree
measure
QT =
Q
i∈VT βi(Ci)
Q
(i–j)∈ET µi,j(Si,j).
(13.12)
When we begin a run of max-product belief propagation, the initial potentials are simply the
initial potentials in Φ, and the messages are all 1, so that QT is precisely ˜PΦ. Examining the
proof of corollary 10.3, we can see that it does not depend on the deﬁnition of the messages
in terms of summing out the beliefs, but only on the way in which the messages are then
used to update the receiving beliefs. Therefore, the proof of the theorem holds unchanged for
max-product message passing, proving the following result:
Proposition 13.2
In an execution of max-product message passing (whether belief propagation or belief update) in a
clique tree, equation (13.12) holds throughout the algorithm.
We can now directly conclude the following result:

13.3. Max-Product in Clique Trees
565
Theorem 13.5
Let {βi} and {µi,j} be the max-calibrated set of beliefs obtained from executing max-product mes-
sage passing, and let QT be the distribution induced by these beliefs. Then QT is a representation
of the distribution ˜PΦ that also satisﬁes the max-product calibration constraints of equation (13.10).
Example 13.9
Continuing with example 13.8, it is straightforward to conﬁrm that the original measure ˜PΦ can be
reconstructed directly from the max-marginals and the sepset message. For example, consider the
entry ˜PΦ(a1, b0, c1, d0) = 100. According to equation (10.10), the clique tree measure is:
β1(a1, b0, d0)β2(b0, c1, d0)
µ1,2(b0, d0)
= 100 · 300, 000
300, 000
= 100,
as required. The equivalence for other entries can be veriﬁed similarly.
Comparing this computation to example 10.6, we see that the sum-product clique tree and the
max-product clique tree both induce reparameterizations of the original measure ˜PΦ, but these
two reparameterizations are diﬀerent, since they must satisfy diﬀerent constraints.
13.3.3
Decoding Max-Marginals
Given the max-marginals, can we ﬁnd the actual MAP assignment?
In the case of variable
elimination, we had the max-marginal only for a single variable Xi (the last to be eliminated).
Therefore, although we could identify the assignment for Xi in the MAP assignment, we had
to perform a traceback procedure to compute the assignments to the other variables. Now the
situation appears diﬀerent: we have max-marginals for all of the variables in the network. Can
we use this property to simplify this process?
One obvious solution is to use the max-marginal for each variable Xi to compute its own
optimal assignment, and thereby compose a full joint assignment to all variables. However, this
simplistic approach may not always work.
Example 13.10
Consider a simple XOR-like distribution P(X1, X2) that gives probability 0.1 to the assignments
where X1 = X2 and 0.4 to the assignments where X1 ̸= X2. In this case, for each assignment to
X1, there is a corresponding assignment to X2 whose probability is 0.4. Thus, the max-marginal
of X1 is the symmetric factor (0.4, 0.4), and similarly for X2. Indeed, we can choose either of
the two values for X1 and complete it to a MAP assignment, and similarly for X2. However, if
we choose the values for X1 and X2 in an inconsistent way, we may get an assignment whose
probability is much lower. Thus, our joint assignment cannot be chosen by separately optimizing
the individual max-marginals.
Recall that we deﬁned a set of node beliefs to be unambiguous if each belief has a unique
maximal value. This condition prevents symmetric cases like the one in the preceding example.
Indeed, it is not diﬃcult to show the following result:
Proposition 13.3
The following two conditions are equivalent:
• The set of node beliefs {MaxMarg ˜
PΦ(Xi) : Xi ∈X} is unambiguous, with
x∗
i = arg max
xi MaxMarg ˜
PΦ(Xi)

566
Chapter 13. MAP Inference
the unique optimizing value for Xi;
• ˜PΦ has a unique MAP assignment (x∗
1, . . . , x∗
n).
See exercise 13.8. For generic probability measures, the assumption of unambiguity is not overly
stringent, since we can always break ties by introducing a slight random perturbation into
all of the factors, making all of the elements in the joint distribution have slightly diﬀerent
probabilities. However, if the distribution has special structure — deterministic relationships or
shared parameters — that we want to preserve, this type of ambiguity may be unavoidable.
Thus, if there are no ties in any of the calibrated node beliefs, we can ﬁnd the unique MAP
assignment by locally optimizing the assignment to each variable separately.
If there are ties in the node beliefs, our task can be reformulated as follows:
Deﬁnition 13.3
Let βi(Ci) be a belief in a max-calibrated clique tree. We say that an assignment ξ∗has the local
local optimality
optimality property if, for each clique Ci in the tree, we have that
ξ∗⟨Ci⟩∈arg max
ci βi(ci),
(13.13)
that is, the assignment to Ci in ξ∗optimizes the Ci belief. The task of ﬁnding a locally optimal
assignment ξ∗given a max-calibrated set of beliefs is called the decoding task.
decoding
Solving the decoding task in the ambiguous case can be done using a traceback procedure as
traceback
in algorithm 13.1. However, local optimality provides us with a simple, local test for verifying
whether a given assignment is the MAP assignment:
Theorem 13.6
Let βi(Ci) be a set of max-calibrated beliefs in a clique tree T , with µi,j the associated sepset
beliefs. Let QT be the clique tree measure deﬁned as in equation (13.12). Then an assignment ξ∗
satisﬁes the local optimality property relative to the beliefs {βi(Ci)}i∈VT if and only if it is the
global MAP assignment relative to QT .
Proof The proof of the “if” direction follows directly from our previous results. We have that
QT is max-calibrated, and hence is a ﬁxed point of the max-product algorithm. (In other words,
if we run max-product inference on the distribution deﬁned by QT , we would get precisely the
beliefs βi(Ci).) Thus, these beliefs are max-marginals of QT . If ξ∗is the MAP assignment to
QT , it must maximize each one of its max-marginals, proving the desired result.
The proof of the only if direction requires the following lemma, which plays an even more
signiﬁcant role in later analyses.
Lemma 13.1
Let φ be a factor over scope Y and ψ be a factor over scope Z ⊂Y such that ψ is a max-marginal
of φ over Z; that is, for any z:
ψ(z) = max
y∼z φ(y).
Let y∗= arg maxy φ(y). Then y∗is also an optimal assignment for the factor φ/ψ, where, as
usual, we take ψ(y∗) = ψ(y∗⟨Z⟩).
Proof Recall that, due to the properties of max-marginalization, each entry ψ(z) arises from
some entry φ(y) such that y ∼z. Because y∗achieves the optimal value in φ, and ψ is the

13.4. Max-Product Belief Propagation in Loopy Cluster Graphs
567
max-marginal of φ, we have that z∗achieves the optimal value in ψ. Hence, φ(y∗) = ψ(z∗),
so that

φ
ψ

(y∗) = 1. Now, consider any other assignment y and the assignment z = y⟨Z⟩.
Either the value of z is obtained from y, or it is obtained from some other y′ whose value is
larger. In the ﬁrst case, we have that φ(y) = ψ(z), so that

φ
ψ

(y) = 1. In the second case,
we have that φ(y) < ψ(z) and

φ
ψ

(y) < 1. In either case,
 φ
ψ

(y) ≤
 φ
ψ

(y∗),
as required.
To prove the only-if direction, we ﬁrst rewrite the clique tree distribution of equation (13.12)
in a directed way. We select a root clique Cr; for each clique i ̸= r, let π(i) be the parent
clique of i in this rooted tree. We then assign each sepset Si,π(i) to the child clique i. Note
that, because each clique has at most one parent, each clique is assigned at most one sepset.
Thus, we obtain the following rewrite of equation (13.12):
βr(Cr)
Y
i̸=r
βi(Ci)
µi,π(i)(Si,π(i)).
(13.14)
Now, let ξ∗be an assignment that satisﬁes the local optimality property. By assumption, it
optimizes every one of the beliefs. Thus, the conditions of lemma 13.1 hold for each of the ratios
in this product, and for the ﬁrst term involving the root clique. Thus, ξ∗also optimizes each
one of the terms in this product, and therefore it optimizes the product as a whole. It must
therefore be the MAP assignment.
As we will see, these concepts and related results have important implications in some of our
later derivations.
13.4
Max-Product Belief Propagation in Loopy Cluster Graphs
In section 11.3 we applied the sum-product message passing using the clique tree algorithm to
a loopy cluster graph, obtaining an approximate inference algorithm. In the same way, we can
generalize max-product message passing to the case of cluster graphs. The algorithms that we
present in this section are directly analogous to their sum-product counterparts in section 11.3.
However, as we discuss, the guarantees that we can provide are much stronger in this case.
13.4.1
Standard Max-Product Message Passing
As for the case of clique trees, the algorithm divides into two phases: computing the beliefs
using message passing and using those beliefs to identify a single joint assignment.
13.4.1.1
Message Passing Algorithm
The message passing algorithm is straightforward: it is precisely the same as the algorithm of
algorithm 11.1, except that we use the procedure of algorithm 13.2 in place of the SP-Message

568
Chapter 13. MAP Inference
procedure. As for sum-product, there are no guarantees that this algorithm will converge. Indeed,
in practice, it tends to converge somewhat less often than the sum-product algorithm, perhaps
because the averaging eﬀect of the summation operation tends to smooth out messages, and
reduce oscillations. Many of the same ideas that we discussed in box 11.B can be used to improve
convergence in this algorithm as well.
At convergence, the result will be a set of calibrated clusters: As for sum-product, if the
clusters are not calibrated, convergence has not been achieved, and the algorithm will continue
iterating. However, the resulting beliefs will not generally be the exact max-marginals; these
resulting beliefs are often called pseudo-max-marginals.
pseudo-max-
marginal
As we saw in section 11.3.3.1 for sum-product, the distribution invariance property that holds
for clique trees is a consequence only of the message passing procedure, and does not depend
on the assumption that the cluster graph is a tree.
The same argument holds here; thus,
proposition 13.2 can be used to show that max-product message passing in a cluster graph is
also simply reparameterizing the distribution:
Corollary 13.3
In an execution of max-product message passing (whether belief propagation or belief update) in a
cluster graph, the invariant equation (10.10) holds initially, and after every message passing step.
13.4.1.2
Decoding the Pseudo-Max-Marginals
Given a set of pseudo-max-marginals, we now have to solve the decoding problem in order
to identify a joint assignment. In general, we cannot expect this assignment to be the exact
MAP, but we can hope for some reasonable approximation. But how do we identify such an
assignment? It turns out that our ability to do so depends strongly on whether there exists some
assignment that satisﬁes the local optimality property of deﬁnition 13.3 for the max-calibrated
beliefs in the cluster graph. Unlike in the case of clique trees, such a joint assignment does not
necessarily exist:
Example 13.11
Consider a cluster graph with the three clusters {A, B}, {B, C}, {A, C} and the beliefs
a1
a0
b1
1
2
b0
2
1
b1
b0
c1
1
2
c0
2
1
a1
a0
c1
1
2
c0
2
1
These beliefs are max-calibrated, in that all messages are (2, 2). However, there is no joint assign-
ment that maximizes all of the cluster beliefs simultaneously. For example, if we select a0, b1, we
maximize the value in the A, B belief. We can now select c0 to maximize the value in the B, C
belief. However, we now have a nonmaximizing assignment a0, c0 in the A, C belief. No matter
which assignment of values we select in this example, we do not obtain a single joint assignment
that maximizes all three beliefs.
Loops such as this are often called frustrated.
frustrated loop

13.4. Max-Product Belief Propagation in Loopy Cluster Graphs
569
In other cases, a locally optimal joint assignment does exist. In particular, when all the

node beliefs are all unambiguous, it is not diﬃcult to show that all of the cluster beliefs
also have a unique maximizing assignment, and that these local cluster-maximizing as-
signments are necessarily consistent with each other (exercise 13.9). However, there are also
other cases where the node beliefs are ambiguous, and yet a locally optimal joint assignment
exists:
Example 13.12
Consider a cluster graph of the same structure as in example 13.11, but with the beliefs:
a1
a0
b1
2
1
b0
1
2
b1
b0
c1
2
1
c0
1
2
a1
a0
c1
2
1
c0
1
2
In this case, the beliefs are ambiguous, yet a locally optimal joint assignment exists (both a1, b1, c1
and a0, b0, c0 are locally optimal).
In general, the decoding problem in a loopy cluster graph is not a trivial task. Recall that,
in clique trees, we could simply choose any of the maximizing assignments for the beliefs at a
clique, and be assured that it could be extended into a joint MAP assignment. Here, as illustrated
by example 13.11, we may make a choice for one cluster that cannot be extended into a consistent
joint assignment. In that example, of course, there is no assignment that works. However, it is
not diﬃcult to construct examples where one choice of locally optimal assignments would give
rise to a consistent joint assignment, whereas another would not (exercise 13.10).
How do we ﬁnd a locally optimal joint assignment, if one exists? Recall from the deﬁnition
that an assignment is locally optimal if and only if it selects one of the optimizing assignments
in every single cluster. Thus, we can essentially label the assignments in each cluster as either
“legal” if they optimize the belief or “illegal” if they do not.
We now must search for an
assignment to X that results in a legal value for each cluster. This problem is precisely a
constraint satisfaction problem (CSP), where the constraints are derived from the local optimality
constraint
satisfaction
problem
condition. More precisely, a constraint satisfaction problem can be deﬁned in terms of a Markov
network (or factor graph) where all of the entries in the beliefs are either 0 or 1. The CSP
problem is now one of ﬁnding an assignment whose (unnormalized) measure is 1, if one exists,
and otherwise reporting failure. In other words, the CSP problem is simply that of ﬁnding the
MAP assignment in this model with {0, 1}-valued beliefs. The ﬁeld of CSP algorithms is a large
one, and a detailed survey is outside the scope of the book; see section 13.9 for some background
reading. We note, however, that the CSP problem is itself NP-hard, and therefore we have no
guarantees that a locally optimal assignment, even if one exists, can be found eﬃciently.
Thus, given a max-product calibrated cluster graph, we can convert it to a discrete-valued
CSP by simply taking the belief in each cluster, changing each assignment that locally optimizes
the belief to 1 and all other assignments to 0. We then run some CSP solution method. If
the outcome is an assignment that achieves 1 in every belief, this assignment is guaranteed
to be a locally optimal assignment.
Otherwise, there is no locally optimal assignment.
In
this case, we must resort to the use of alternative solution methods. One heuristic in this latter
situation is to use information obtained from the max-product propagation to construct a partial
assignment. For example, assume that a variable Xi is unambiguous in the calibrated cluster
graph, so that the only value that locally optimizes its node marginal is xi. In this case, we may

570
Chapter 13. MAP Inference
1: A, B, C
4: B, E
(a)
(b)
2: B, C, D
B
B
B
C
C
E
1: A, B, C
4: B, E
2: B, C, D
5: D, E
3: B, D, F
Figure 13.4
Two induced subgraphs derived from ﬁgure 11.3a. (a) Graph over {B, C}; (b) Graph over
{C, E}.
decide to restrict attention only to assignments where Xi = xi. In many real-world problems,
a large fraction of the variables in the network are unambiguous in the calibrated max-product
cluster graph. Thus, this heuristic can greatly simplify the model, potentially even allowing
exact methods (such as clique tree inference) to be used for the resulting restricted model. We
note, however, that the resulting assignment would not necessarily satisfy the local optimality
condition, and all of the guarantees we will present hold only under that assumption.
13.4.1.3
Strong Local Maximum
What type of guarantee can we provide for a decoded assignment from the pseudo-max-
marginals produced by the max-product belief propagation algorithm? It is certainly not the
case that this assignment is the MAP assignment; nor is it even the case that we can guarantee
that the probability of this assignment is “close” in any sense to that of the true MAP assignment.
However, if we can construct a locally optimal assignment ξ∗relative to the beliefs produced
by max-product BP, we can prove that ξ∗is a strong local maximum, in the following sense: For
strong local
maximum
certain subsets of variables Y ⊂X, there is no assignment ξ′ that is higher-scoring than ξ∗
and diﬀers from it only in the assignment to Y . These subsets Y are those that induce any
disjoint union of subgraphs each of which contains at most a single loop (including trees, which
contain no loops).
Deﬁnition 13.4
Let U be a cluster graph over X, and Y ⊂X be some set of variables. We deﬁne the induced
induced
subgraph
subgraph UY to be the subgraph of clusters and sepsets in U that contain some variable in Y .
This deﬁnition is most easily understood in the context of a pairwise Markov network, where
the cluster graph is simply the set of edges in the MRF and the sepsets are the individual
variables. In this case, the induced subgraph for a set Y is simply the set of nodes corresponding
to Y and any edges that contain them. In a more general cluster graph, the result is somewhat
more complex:
Example 13.13
Consider the cluster graph of ﬁgure 11.3a. Figure 13.4a shows the induced subgraph over {B, C};
this subgraph contains at exactly one loop, which is connected to an additional cluster. Figure 13.4b
shows the induced subgraph over {C, E}; this subgraph is a union of two disjoint trees.
We can now state the following important theorem:

13.4. Max-Product Belief Propagation in Loopy Cluster Graphs
571
Theorem 13.7
Let U be a max-product calibrated cluster graph for ˜PΦ, and let ξ∗be a locally optimal assignment
for U. Let Z be any set of variables for which UZ is a collection of disjoint subgraphs each of
which contains at most a single loop. Then for any assignment ξ′ which is the same as ξ∗except
for the assignment to the variables in Z, we have that
˜PΦ(ξ′) ≤˜PΦ(ξ∗).
(13.15)
Proof We prove the theorem under the assumption that UZ is a single tree, leaving the rest of
the proof as an exercise (exercise 13.12). Owing to the recalibration property, we can rewrite the
joint probability ˜PΦ as in equation (13.12). We can partition the terms in this expression into
two groups: those that involve variables in Z and those that do not. Let Y = X −Z and y∗
be the locally optimal assignment to Y . We now consider the unnormalized measure obtained
over Z when we restrict the distribution to the event Y = y∗(as in deﬁnition 4.5). Since we
set Y = y∗, the terms corresponding to beliefs that do not involve Z are constant, and hence
they do not aﬀect the comparison between ξ′ and ξ∗.
We can now deﬁne ˜P ′
y∗(Z) to be the measure obtained by restricting equation (13.12) only to
the terms in the beliefs (at both clusters and sepsets) that involve variables in Z. It follows that
an assignment z optimizes ˜PΦ(z, y∗) if and only if it optimizes ˜P ′
y∗. This measure precisely
corresponds to a clique tree whose structure is UZ and whose beliefs are the beliefs in our
original calibrated cluster graph U, but restricted to Y = y∗. Let Ty∗represent this clique tree
and its associated beliefs.
Because U is max-product calibrated, so is its subgraph Ty∗.
Moreover, if an assign-
ment (y∗, z∗) is optimal for some belief βi, then z∗is also optimal for the restricted belief
βi[Y = y∗]. We therefore have a max-product calibrated clique tree Ty∗and z∗is a locally
optimal assignment for it. Because this is a clique tree, local optimality implies MAP, and so z∗
must be a MAP assignment in this clique tree. As a consequence, there is no assignment z′ that
has a higher probability in ˜P ′
y∗, proving the desired result.
To illustrate the power of this theorem, consider the following example:
Example 13.14
Consider the 4 × 4 grid network in ﬁgure 11.4, and assume that we use the pairwise cluster graph
construction of ﬁgure 11.6 (shown there for a 3 × 3 grid). This result implies that the MAP solution
found by max-product belief propagation has higher probability than any assignment obtained by
changing the assignment to any of the following subsets of variables Y :
• a set of variables in any single row, such as Y = {A1,1, A1,2, A1,3, A1,4};
• a set of variables in any single column;
• a “comb” structure such as the variables in row 1, column 2 and column 4;
• a single loop, such as Y = {A1,1, A1,2, A2,2, A2,1};
• a collection of disconnected subsets of the preceding form, for example: the union of the
variables in rows 1 and 3; or the loop above union with the L-structure consisting of the
variables in row 4 and the variables in column 4.

This result is a powerful one, inasmuch as it shows that the solution obtained from
max-product belief propagation is robust against large perturbations.
Thus, although

572
Chapter 13. MAP Inference
one can construct examples where max-product belief propagation obtains the wrong
solutions, these solutions are strong local maxima, and therefore they often have high
probability.
13.4.2
Max-Product BP with Counting Numbers ⋆
The preceding algorithm performs max-product message passing that is analogous to the sum-
product message passing with the Bethe free-energy approximation.
We can also construct
analogues of the various generalizations of sum-product message passing, as deﬁned in sec-
tion 11.3.7. We can derive max-product variants based both on the region-graph methods, which
allow us to introduce larger clusters, and based on the notion of alternative counting numbers.
From an algorithmic perspective, the transformation of sum-product to max-product algorithms
is straightforward: we simply replace summation with maximization. The key question is the
extent to which we can provide a formal justiﬁcation for these methods.
Recall that, in our discussion of sum-product algorithms, we derived the belief propagation
algorithms in two diﬀerent ways. The ﬁrst was simply by taking the message passing algorithm
on clique trees and running it on loopy cluster graphs, ignoring the presence of loops. The
second derivation was obtained by a variational analysis, where the algorithm arose naturally as
the ﬁxed points of an approximate energy functional. This view was compelling both because it
suggested some theoretical justiﬁcation for the algorithm and, even more important, because it
immediately gave rise to a variety of generalizations, obtained from diﬀerent approximations to
the energy functional, diﬀerent methods for optimizing the objective, and more.
For the case of max-product, our discussion so far follows the ﬁrst approach, viewing the
message passing algorithm as a simple generalization of the max-product clique tree algorithm.
Given the similarity between the sum-product and max-product algorithms presented so far,
one may assume that we can analogously provide a variational justiﬁcation for max-product,
for example, as optimizing the same energy functional, but with max-calibration rather than
sum-calibration constraints on adjacent clusters. For example, in a variational derivation of the
max-product clique tree algorithm, we would replace the sum-calibration constraint of equa-
tion (11.7) with the analogous max-calibration constraint of equation (13.10). Although plausible,
this analogy turns out to be incorrect. The key problem is that, whereas the sum-marginalization
constraint of equation (11.7) is a simple linear equality, the constraint of equation (13.10) is not.
Indeed, the max function involved in the constraint is not even smoothly diﬀerentiable, so that
the framework of Lagrange multipliers cannot be applied.
However, as we now show, we can provide an optimization-based derivation and more formal
justiﬁcation for max-product BP with convex counting numbers. For these variants, we can
even show conditions under which these algorithms are guaranteed to produce the correct MAP
assignment.
We begin this section by describing the basic algorithm, and proving the key
optimality result: that any locally optimal assignment for convex max-product BP is guaranteed
to be the MAP assignment.
Then, in section 13.5, we provide an alternative view of this
approach in terms of its relationship to two other classes of algorithms. This perspective will
shed additional insight on the properties of this algorithm and on the cases in which it provides
a useful guarantee.

13.4. Max-Product Belief Propagation in Loopy Cluster Graphs
573
Algorithm 13.3 Calibration using max-product BP in a Bethe-structured cluster graph
Procedure Generalized-MP-BP (
Φ,
// Set of factors
R,
// Set of regions
{κr}r∈R, {κi}Xi∈X
// Counting numbers
)
1
ρi ←1/κi
2
ρr ←1/κr
3
Initialize-CGraph
4
while region graph is not max-calibrated
5
Select Cr and Xi ∈Cr
6
δi→r(Xi) ←
hQ
r′̸=r δi→r′(Xi)
ρi 
maxCr−Xi ψr(Cr)
Q
Xj∈Cr,j̸=i δj→r
ρri−
1
ρi+ρr
7
for each region r ∈R ∪{1, . . . , n}
8
βr(Cr) ←
 ψr(Cr) Q
Xi∈Cr δi→r(Xi)
ρr
9
return {βr}r∈R
13.4.2.1
Max-Product with Counting Numbers
We begin with a reminder of the notion of belief propagation with counting numbers.
For
concreteness, we also provide the max-product variant of a message passing algorithm for this
case, although (as we mentioned) the max-product variant can be obtained from the sum-
product algorithm using a simple syntactic substitution.
In section 11.3.7, we deﬁned a set of sum-product message passing algorithms; these algorithms
were deﬁned in terms of a set of counting numbers that specify the extent to which entropy
counting
numbers
terms for diﬀerent subsets of variables are counted in the entropy approximation used in the
energy functional. For a given set of counting numbers, one can derive a message passing
algorithm by using the ﬁxed point equations obtained by diﬀerentiating the Lagrangian for the
energy functional, with its sum-product calibration constraints. The standard belief propagation
algorithm is obtained from the Bethe energy approximation; other sets of counting numbers give
rise to other message passing algorithms.
As we discussed, one can take these sum-product message passing algorithms (for example,
those in exercise 11.17 and exercise 11.19) and convert them to produce a max-product variant by
simply replacing each summation operation as maximization. For concreteness, in algorithm 13.3,
we repeat the algorithm of exercise 11.17, instantiated to the max-product setting. Recall that
this algorithm applies only to Bethe cluster graphs, that is, graphs that have two levels of regions:
Bethe cluster
graphs
“large” regions r containing multiple variables with counting numbers κr, and singleton regions
containing individual variables Xi with counting numbers κi; all factors in Φ are assigned only
to large regions, so that ψi = 1 for all i.

574
Chapter 13. MAP Inference
A critical observation is that, like the sum-product algorithms, and like the max-product

clique tree algorithm (see theorem 13.5), these new message passing algorithms are a
reparameterization of the original distribution.
In other words, their ﬁxed points are a
reparameteriza-
tion
diﬀerent representation of the same distribution, in terms of a set of max-calibrated beliefs. This
property, which is stated for sum-product in theorem 11.6, asserts that, at ﬁxed points of the
message passing algorithm, we have that:
˜PΦ(X) =
Y
r
(βr)κr.
(13.16)
The proof of this equality (see exercise 11.16 and exercise 11.19) is a consequence only of the way
in which we deﬁne region beliefs in terms of the messages. Therefore, the reparameterization
property applies equally to ﬁxed points of the max-product algorithms. It is this property that
will be critical in our derivation.
13.4.2.2
Optimality Guarantee
As in the case of standard max-product belief propagation algorithms, given a set of max-product
calibrated beliefs that reparameterize the distribution, we now search for an assignment that is
locally optimal for this set of beliefs. However, as we now show, under certain assumptions,
such an assignment is guaranteed to be the MAP assignment.
Although more general variants of this theorem exist, we focus on the case of a Bethe-
structured region graph, as described. Here, we also assume that our large regions in R have
counting number 1. We assume also that factors in the network are assigned only to large
regions, so that ψi = 1 for all i. Finally, in a property that is critical to the upcoming derivation,
we assume that the counting numbers κr are convex, as deﬁned in deﬁnition 11.4. Recall that
a vector of counting numbers κr is convex if there exist nonnegative numbers νr, νi, and νr,i
convex counting
numbers
such that:
κr
=
νr + P
i : Xi∈Cr νr,i
for all r
κi
=
νi −P
r : Xi∈Cr νr,i
for all i.
This is the same assumption used to guarantee that the region-graph energy functional in
equation (11.27) is a concave function. Although here we have no energy functional, the purpose
of this assumption is similar: As we will see, it allows us to redistribute the terms in the
reparameterization of the probability distribution, so as to guarantee that all terms have a
positive coeﬃcient.
From these assumptions, we can now prove the following theorem:
Theorem 13.8
Let PΦ be a distribution, and consider a Bethe-structured region graph with large regions and
singleton regions, where the counting numbers are convex. Assume that we have a set of max-
calibrated beliefs βr(Cr) and βi(Xi) such that equation (13.16) holds. If there exists an assignment
ξ∗that is locally optimal relative to each of the beliefs βr, then ξ∗is the optimal MAP assignment.
Proof Applying equation (13.16) to our Bethe-structured graph, we have that:
˜PΦ(X) =
Y
r∈R
βr
Y
i
βκi
i .

13.4. Max-Product Belief Propagation in Loopy Cluster Graphs
575
Owing to the convexity of the counting numbers, we can rewrite the right-hand side as:
Y
r
(βr)νr Y
i
(βi)νi
Y
i,r : Xi∈Cr
βr
βi
νr,i
.
Owing to the nonnegativity of the coeﬃcients ν, we have that:
max
ξ
˜PΦ(ξ)
=
max
ξ
Y
r
(βr(cr))νr Y
i
(βi(xi))νi
Y
i,r : Xi∈Cr
βr
βi
(cr)
νr,i
≤
Y
r
(max
cr βr(cr))νr Y
i
(max
xi βi(xi))νi
Y
i,r : Xi∈Cr

max
cr
βr
βi
(cr)
νr,i
.
We have now reduced this expression to a product of terms, each raised to the power of a
positive exponent. Some of these terms are factors in the max-product calibrated network, and
others are ratios of factors and their max-product marginal over an individual variable. The proof
now is exactly the same as the proof of theorem 13.6. Let ξ∗be an assignment that satisﬁes the
local optimality property. By assumption, it optimizes every one of the region beliefs. Because
the ratios involve a factor and its max-marginal, the conditions of lemma 13.1 hold for each of
the ratios in this expression. Thus, ξ∗optimizes each one of the terms in this product, and
therefore it optimizes the product as a whole. It therefore optimizes ˜PΦ(ξ), and must therefore
be the MAP assignment.
We can also derive the following useful corollary, which allows us, in certain cases, to char-
acterize parts of the MAP solution even if the local optimality property does not hold:
Corollary 13.4
Under the setting of theorem 13.8, if a variable Xi takes a particular value x∗
i in all locally optimal
assignments ξ∗then xmap
i
= x∗
i in the MAP assignment. More generally, if there is some set Si
such that, in any locally optimal assignment ξ∗we have that x∗
i ∈Si, then xmap
i
∈Si.
At ﬁrst glance, the application of this result seems deceptively easy. After all, in order to be
locally optimal, an assignment must assign to Xi one of the values that maximizes its individual
node marginal. Thus, it appears that we can easily extract, for each Xi, some set Si (perhaps
an overestimate) to which corollary 13.4 applies. Unfortunately, when we use this procedure, we
cannot guarantee that xmap
i
is actually in the set Si. The corollary applies only if there exists a
locally optimal assignment to the entire set of beliefs. If no such assignment exists, the set of
locally maximizing values in Xi’s node belief may have no relation to the true MAP assignment.
13.4.3
Discussion

In this section, we have shown that max-product message passing algorithms, if they
converge, provide a max-calibrated reparameterization of the distribution ˜PΦ. This repa-
rameterization essentially converts the global optimization problem of ﬁnding a single
joint MAP assignment to a local optimization problem: ﬁnding a set of locally optimal
assignments to the individual cliques that are consistent with each other. Importantly,
we can show that this locally consistent assignment, if it exists, satisﬁes strong opti-
mality properties: In the case of the standard (Bethe-approximation) reparameterization,

576
Chapter 13. MAP Inference
the joint assignment satisﬁes strong local optimality; in the case of reparameterizations
based on convex counting numbers, it is actually guaranteed to be the MAP assignment.
Although these guarantees are very satisfying, their usefulness relies on several important
questions that we have not yet addressed. The ﬁrst two relate to the max-product calibrated
reparameterization of the distribution: does one exist, and can we ﬁnd it? First, for a given set of
counting numbers, does there always exist a max-calibrated reparameterization of ˜PΦ in terms
of these counting numbers? Somewhat surprisingly, as we show in section 13.5.3, the answer to
that question is yes for convex counting numbers; it turns out to hold also for the Bethe counting
numbers, although we do not show this result. Second, we must ask whether we can always
ﬁnd such a reparameterization. We know that if the max-product message passing algorithm
converges, it must converge to such a ﬁxed point. But unfortunately, there is no guarantee that
the algorithm will converge. In practice, standard max-product message passing often does not
converge. For certain speciﬁc choices of convex counting numbers (see, for example, box 13.A),
one can design algorithms that are guaranteed to be convergent.
However, even if we ﬁnd an appropriate reparameterization, we are still left with the problem
of extracting a joint assignment that satisﬁes the local optimality property. Indeed, such as
assignment may not even exist. In section 13.5.3.3, we present a necessary condition for the
existence of such an assignment.
It is currently not known how the choice of counting numbers aﬀects either of these two
issues: our ability to ﬁnd eﬀectively a max-product calibrated reparameterization, and our ability
to use the result to ﬁnd a locally consistent joint assignment. Empirically, preliminary results
suggest that nonconvex counting numbers (such as those obtained from the Bethe approxima-
tion) converge less often than the the convex variants, but converge more quickly when they do
converge. The diﬀerent convex variants converge at diﬀerent rates, but tend to converge to ﬁxed
points that have a similar set of ambiguities in the beliefs. Moreover, in cases where convex
max-product BP converges whereas standard max-product does not, the resulting beliefs often
contain many ambiguities (beliefs with equal values), making it diﬃcult to determine whether
the local optimality property holds, and to identify such an assignment if it exists.
Box 13.A — Concept: Tree-Reweighted Belief Propagation. One algorithm that is worthy of
special mention, both because of historical precedence and because of its popularity, is the tree-
tree-reweighted
belief
propagation
reweighted belief propagation algorithm (often known as TRW). This algorithm was the ﬁrst mes-
sage passing algorithm to use convex counting numbers; it was also the context in which message
passing algorithms were ﬁrst shown to be related to the linear program relaxation of the MAP
LP relaxation
optimization problem that we discuss in section 13.5. This algorithm, developed in the context
of a pairwise Markov network, utilizes the same approach as in the TRW variant of sum-product
message passing: It deﬁnes a probability distribution over trees T in the network, so that each edge
in the pairwise network appears in at least one tree, and it then deﬁnes the counting numbers to
be the edge and negative node appearance probabilities, as deﬁned in equation (11.26). Note that,
unlike the algorithms of section 13.4.2.1, here the factors do not have a counting number of 1, so
that the algorithms we presented there require some modiﬁcation. Brieﬂy, the max-product TRW

13.5. MAP as a Linear Optimization Problem ⋆
577
algorithm uses the following update rule:
δi→j = max
xi


 
ψi(xi)
Y
k∈Nbi
δk→i(xi)
! κi,j
κi
1
δj→i(xi)ψi,j(xi, xj)

.
(13.17)
One particular variant of the TRW algorithm, called TRW-S, provides some particularly satisfying
guarantees. Assume that we order the nodes in the network in some ﬁxed ordering X1, . . . , Xn,
and consider a set of trees each of which is a subsequence of this ordering, that is, of the form
Xi1, . . . , Xik for i1 < . . . ik. We now pass messages in the network by repeating two phases,
where in one phase we pass messages from X1 towards Xn, and in the other from Xn towards
X1. With this message passing scheme, it is possible to guarantee that the algorithm continuously
increases the dual objective, and hence it is convergent.
13.5
MAP as a Linear Optimization Problem ⋆
A very important and useful insight on the MAP problem is derived from viewing it directly
as an optimization problem. This perspective allows us to draw upon the vast literature on
optimization algorithms and apply some of these ideas to the speciﬁc case of MAP inference.
Somewhat surprisingly, some of the algorithms that we describe elsewhere in this chapter turn
out to be related to the optimization perspective; the insights obtained from understanding the
connections can provide the basis for a theoretical analysis of these methods, and they can
suggest improvements.
For the purposes of this section, we assume that the distribution speciﬁed in the MRF is
positive, so that all of the entries in all of the factors are positive. This assumption turns out to
be critical for some of the derivations in this section, and facilitates many others.
13.5.1
The Integer Program Formulation
The basic MAP problem can be viewed as an integer linear program — an optimization problem
integer linear
program
(see appendix A.4.1) over a set of integer valued variables, where both the objective and the
constraints are linear. To deﬁne a linear optimization problem, we must ﬁrst turn all of our
products into summations. This transformation gives rise to the following max-sum form:
max-sum
arg max
ξ
log ˜PΦ(ξ) = arg max
ξ
X
r∈R
log(φr(cr)),
(13.18)
where Φ = {φr : r ∈R}, and Cr is the scope of φr.
For r ∈R, we deﬁne nr = |Val(Cr)|. For any joint assignment ξ, if ξ⟨Cr⟩= cj
r, the factor
log(φr) makes a contribution to the objective of log(φr(cj
r)), a quantity that we denote as ηj
r.
We introduce optimization variables q(xj
r), where r ∈R enumerates the diﬀerent factors, and
optimization
variables
j = 1, . . . , nr enumerates the diﬀerent possible assignments to the variables Cr that comprise
the factor Cr. These variables take binary values, so that q(xj
r) = 1 if and only if Cr = cj
r,
and 0 otherwise. It is important to distinguish the optimization variables from the random

578
Chapter 13. MAP Inference
variables in our original graphical model; here we have an optimization variable q(xj
r) for each
joint assignment cj
r to the model variables Cr.
Let q denote a vector of the optimization variables {q(xj
r) : r ∈R; j = 1, . . . , nr}, and
η denote a vector of the coeﬃcient ηj
r sorted in the same order. Both of these are vectors of
dimension N = PK
k=1 nr. With this interpretation, the MAP objective can be rewritten as:
maximizeq
X
r∈R
nr
X
j=1
ηj
rq(xj
r),
(13.19)
or, in shorthand, maximizeqηT q.
Example 13.15
Assume that we have a pairwise MRF shaped like a triangle A—B—C—A, so that we have
three factors over pairs of connected random variables: φ1(A, B), φ2(B, C), φ3(A, C). Assume
that A, B are binary-valued, whereas C takes three values. Here, we would have the optimization
variables q(x1
1), . . . , q(x4
1), q(x1
2), . . . , q(x6
2), q(x1
3), . . . , q(x6
3). We assume that the values of
the variables are enumerated lexicographically, so that q(x4
3), for example, corresponds to a2, c1.
We can view our MAP inference problem as optimizing this linear objective over the space
of assignments to q ∈{0, 1}N that correspond to legal assignments to X. What constraints
on q do we need to impose in order to guarantee that it corresponds to some assignment to
X? Most obviously, we need to ensure that, in each factor, only a single assignment is selected.
Thus, in our example, we cannot have both q(x1
1) = 1 and q(x2
1) = 1. Slightly subtler are the
cross-factor consistency constraints: If two factors share a variable, we need to ensure that the
assignment to this variable according to q is consistent in those two factors. In our example,
for instance, if we have that q(x1
1) = 1, so that B = b1, we would need to have q(x1
2) = 1,
q(x2
2) = 1, or q(x3
2) = 1.
There are several ways of encoding these consistency constraints. First, we require that we
restrict attention to integer solutions:
q(xj
r) ∈{0, 1}
For all r ∈R; j ∈{1, . . . , nr}.
(13.20)
We can now utilize two linear equalities to enforce the consistency of these integer solutions.
The ﬁrst constraint enforces the mutual exclusivity within a factor:
nr
X
j=1
q(xj
r) = 1
For all r ∈R.
(13.21)
The second constraint implies that factors in our MRF agree on the variables in the intersection
of their scopes:
X
j : cj
r∼sr,r′
q(xj
r) =
X
l : cl
r′∼sr,r′
q(xl
r′)
(13.22)
For all r, r′ ∈R and all sr,r′ ∈Val(Cr ∩Cr′).
Note that this constraint is vacuous for pairs of clusters whose intersection is empty, since there
are no assignments sr,r′ ∈Val(Cr ∩Cr′).

13.5. MAP as a Linear Optimization Problem ⋆
579
Example 13.16
Returning to example 13.15, the mutual exclusivity constraints for φ1 would assert that P4
j=1 q(xj
1) =
1. Altogether, we would have three such constraints — one for each factor. The consistency con-
straints associated with φ1(A, B) and φ2(B, C) assert that:
q(x1
1) + q(x3
1)
=
q(x1
2) + q(x2
2) + q(x3
2)
q(x2
1) + q(x4
1)
=
q(x4
2) + q(x5
2) + q(x6
2),
where the ﬁrst constraint ensures consistency when B = b1 and the second when B = b2. Overall,
we would have three such constraints for φ2(B, C), φ3(A, C), corresponding to the three values of
C, and two constraints for φ1(A, B), φ3(A, C), corresponding to the two values of A.
Together, these constraints imply that there is a one-to-one mapping between possible assignments
to the q(xj
r) optimization variables and legal assignments to A, B, C.
In general, equation (13.20), equation (13.21), and equation (13.22) together imply that the
assignment q(xj
r)’s correspond to a single legal assignment:
Proposition 13.4
Any assignment to the optimization variables q that satisﬁes equation (13.20), equation (13.21), and
equation (13.22) corresponds to a single legal assignment to X1, . . . , Xn.
The proof is left as an exercise (see exercise 13.13).
Thus, we have now reformulated the MAP task as an integer linear program, where we optimize
the linear objective of equation (13.19) subject to the constraints equation (13.20), equation (13.21),
and equation (13.22). We note that the problem of solving integer linear programs is itself NP-
hard, so that (not surprisingly) we have not avoided the basic hardness of the MAP problem.
However, there are several techniques that have been developed for this class of problems, which
can be usefully applied to integer programs arising from MAP problems. One of the most useful
is described in the next section.
13.5.2
Linear Programming Relaxation
One of the methods most often used for tackling integer linear programs is the method of linear
LP relaxation
program relaxation. In this approach, we turn a discrete, combinatorial optimization problem
into a continuous problem.
This problem is a linear program (LP), which can be solved in
linear program
polynomial time, and for which a range of very eﬃcient algorithms exist. One can then use
the solutions to this LP to obtain approximate solutions to the MAP problem. To perform this
relaxation, we substitute the constraint equation (13.20) with a relaxed constraint:
q(xj
r) ≥0
For all r ∈R, j ∈{1, . . . , nr}.
(13.23)
This constraint and equation (13.21) together imply that each q(xr
j) ∈[0, 1]; thus, we have
relaxed the combinatorial constraint into a continuous one. This relaxation gives rise to the
following linear program (LP):
linear program

580
Chapter 13. MAP Inference
MAP-LP:
Find
{q(xj
r) : r ∈R; j = 1, . . . , nr}
maximizing
η⊤q
subject to
nr
X
j=1
q(xj
r)
=
1
r ∈R
X
j : cj
r∼sr,r′
q(xj
r)
=
X
l : cl
r′∼sr,r′
q(xl
r′)
r, r′ ∈R
sr,r′ ∈Val(Cr ∩Cr′)
q
≥
0
This linear program is a relaxation of our original integer program, since every assignment to
q that satisﬁes the constraints of the integer problem also satisﬁes the constraints of the linear
program, but not the other way around. Thus, the optimal value of the objective of the relaxed
version will be no less than the value of the (same) objective in the exact version, and it can be
greater when the optimal value is achieved at an assignment to q that does not correspond to
a legal assignment ξ.
A closer examination shows that the space of assignments to q that satisﬁes the constraints
of MAP-LP corresponds exactly to the locally consistent pseudo-marginals for our cluster graph
pseudo-marginals
U, which comprise the local consistency polytope Local[U], deﬁned in equation (11.16). To see
local consistency
polytope
this equivalence, we note that equation (13.23) and equation (13.21) imply that any assignment
to q deﬁnes a set of locally normalized distributions over the clusters in the cluster graph —
nonnegative factors that sum to 1; by equation (13.22), these factors must be sum-calibrated. Thus,
there is a one-to-one mapping between consistent pseudo-marginals and possible solutions to
the LP.
We can use this observation to answer the following important question: Given a non-integer
solution to the relaxed LP, how can we derive a concrete assignment? One obvious approach is
a greedy assignment process, which assigns values to the variables Xi one at a time. For each
variable, and for each possible assignment xi, it considers the set of reduced pseudo-marginals
that would result by setting Xi = xi. We can now compute the energy term (or, equivalently,
the LP objective) for each such assignment, and select the value xi that gives the maximum
value. We then permanently reduce each of the pseudo-marginals with the assignment Xi = xi,
and continue. We note that, at the point when we assign Xi, some of the variables have already
been assigned, whereas others are still undetermined. At the end of the process, all of the
variables have been assigned a speciﬁc value, and we have a single joint assignment.
To understand the result obtained by this algorithm, recall that Local[U] is a superset of
the marginal polytope Marg[U] — the space of legal distributions that factorize over U (see
marginal
polytope
equation (11.15)). Because our objective in equation (13.19) is linear, it has the same optimum
over the marginal polytope as over the original space of {0, 1} solutions: The value of the
objective at a point corresponding to a distribution P(X) is the expectation of its value at the
assignments ξ that receive positive probability in P; therefore, one cannot achieve a higher
value of the objective with respect to P than with respect to the highest-value assignment ξ.
Thus, if we could perform our optimization over the continuous space Marg[U], we would ﬁnd
the optimal solution to our MAP objective. However, as we have already discussed, the marginal

13.5. MAP as a Linear Optimization Problem ⋆
581
polytope is a complex object, which can be speciﬁed only using exponentially many constraints.
Thus, we cannot feasibly perform this optimization.
By contrast, the optimization problem obtained by this relaxed version has a linear objective
with linear constraints, and both involve a number of terms which is linear in the size of the
cluster graph. Thus, this linear program admits a range of eﬃcient solutions, including ones
with polynomial time guarantees. We can thus apply oﬀ-the-shelf methods for solving such
problems. Of course, the result is often fractional, in which case it is clearly not an optimal
solution to the MAP problem.
The LP formulation has advantages and disadvantages. By formulating our problem as

a linear program, we obtain a very ﬂexible framework for solving it; in particular, we
can easily incorporate additional constraints into the LP, which reduce the space of
possible assignments to q, eliminating some solutions that do not correspond to actual
distributions over X. (See section 13.9 for some references.) On the other hand, as we discussed
in example 11.4, the optimization problems deﬁned over this space of constraints are very large,
making standard optimization methods very expensive. Of course, the LP has special structure:
For example, when viewed as a matrix, the equality constraints in this LP all have a particular
block structure that corresponds to the structure of adjacent clusters; moreover, when the MRF
is not densely connected, the constraint matrix is also sparse. However, standard LP solvers may
not be ideally suited for exploiting this special structure. Thus, empirical evidence suggests that
the more specialized solution methods for the MAP problems are often more eﬀective than using
oﬀ-the-shelf LP solvers. As we now discuss, the convex message passing algorithms described in
section 13.4.2 can be viewed as specialized solution methods to the dual of this LP. More recent
work explicitly aims to solve this dual using general-purpose optimization techniques that do
take advantage of the structure; see section 13.9 for some references.
13.5.3
Low-Temperature Limits
In this section, we show how we can use a limit process to understand the connection between
the relaxed MAP-LP and both sum-product and max-product algorithms with convex counting
numbers. As we show, this connection provides signiﬁcant new insight on all three algorithms.
13.5.3.1
LP as Sum-Product Limit
More precisely, recall that the energy functional is deﬁned as:
F[PΦ, Q] =
X
φr∈Φ
IECr∼Q[log φr(Cr)] + ˜IHQ(X),
where ˜IHQ(X) is some (exact or approximate) version of the entropy of Q. Consider the ﬁrst
term in this expression, also called the energy term. Let q(xj
r) denote the cluster marginal
βr(cj
r). Then we can rewrite the energy term as:
X
r∈R
nr
X
j=1
q(xj
r) log(φr(cj
r),
which is precisely the objective in our LP relaxation of the MAP problem. Thus, the energy
functional is simply a sum of two terms: the LP relaxation objective, and an entropy term. In

582
Chapter 13. MAP Inference
the energy functional, both of these terms receive equal weight. Now, however, consider an
alternative objective, called the temperature-weighted energy function. This objective is deﬁned
temperature-
weighted energy
function
in terms of a temperature parameter T > 0:
temperature
˜F (T )[PΦ, Q] =
X
φr∈Φ
IECr∼Q[log φr(Cr)] + T ˜IHQ(X).
(13.24)
As usual in our derivations, we consider the task of maximizing this objective subject to the
sum-marginalization constraints, that is, that Q ∈Local[U].
The temperature-weighted energy functional reweights the importance of the two terms in
the objective. Since T −→0, we will place a greater emphasis on the linear energy term (the
ﬁrst term), which is precisely the objective of the relaxed LP. Thus, since T −→0, the objective
˜F (T )[PΦ, Q] tends to the LP objective. Can we then infer that the ﬁxed points of the objective
(say, those obtained from a message passing algorithm) are necessarily optima of the LP? The
answer to this question is positive for concave versions of the entropy, and negative otherwise.
In particular, assume that ˜IHQ(X) is a weighted entropy ˜IH
κ
Q(X), such that κ is a convex
set of counting numbers, as in equation (11.20).
From the assumption on convexity of the
counting numbers and the positivity of the distribution, it follows that the function ˜F (T )[PΦ, Q]
is strongly convex in the distribution Q. The space Local[U] is a convex space. Thus, there is
a unique global minimum Q∗(T ) for every T, and that optimum changes continuously in T.
Standard results now imply that the limit of Q∗(T ) is optimal for the limiting problem, which is
precisely the LP.
On the other hand, this result does not hold for nonconvex entropies, such as the one obtained
by the Bethe approximation, where the objective can have several distinct optima. In this case,
there are examples where a sequence of optima obtained for diﬀerent values of T converges to
a point that is not a solution to the LP. Thus, for the remainder of this section, we assume that
˜IHQ(X) is derived from a convex set of counting numbers.
13.5.3.2
Max-Product as Sum-Product Limit
What do we gain from this perspective? It does not appear practical to use this characterization
as a constructive solution method. For one thing, we do not want to solve multiple optimization
problems, for diﬀerent values of T. For another, the optimization problem becomes close to
degenerate as T grows small, making the problem hard to solve.
However, if we consider
the dual problem to each of the optimization problems of this sequence, we can analytically
characterize the limit of these duals. Surprisingly, this limit turns out to be a ﬁxed point of the
max-product belief propagation algorithm.
We ﬁrst note that the temperature-weighted energy functional is virtually identical in its form
to the original functional. Indeed, we can formalize this intuition if we divide the objective
through by T; since T > 0, this step does not change the optima. The resulting objective has
the form:
1
T
X
φr∈Φ
IECr∼Q[log φr(Cr)] + IHQ(X)
=
X
φr∈Φ
IECr∼Q
 1
T (log φr(Cr))

+ ˜IHQ(X)
=
X
φr∈Φ
IECr∼Q
h
log φ1/T
r
i
+ ˜IHQ(X).
(13.25)

13.5. MAP as a Linear Optimization Problem ⋆
583
This objective has precisely the same form as the standard approximate energy functional, but
for a diﬀerent set of factors: the original factors, raised to the power of 1/T. This set of factors
deﬁnes a new unnormalized density:
˜P (T )
Φ
(X) = ( ˜PΦ(X))1/T .
Because our entropy is concave, and using our assumption that the distribution is positive, the
approximate free energy ˜F[PΦ, Q] is strictly convex and hence has a unique global minimum
Q(T ) for each temperature T. We can now consider the Lagrangian dual of this new objective,
and characterize this unique optimum via its dual parameterization Q(T ). In particular, as we
have previously shown, Q(T ) is a reparameterization of the distribution ˜P (T )
Φ
(X):
˜P (T )
Φ
=
Y
r∈R
β(T )
r
Y
i
(β(T )
i
)κi =
Y
r∈R+
(β(T )
r
)κi,
(13.26)
where, for simplicity of notation, we deﬁne R+ = R ∪X and κr = 1 for r ∈R.
Our goal is now to understand what happens to Q(T ) as we take T −→0. We ﬁrst reformulate
these beliefs by deﬁning, for every region r ∈R+:
¯β(T )
r
=
max
x′
r
β(T )
r
(x′
r)
(13.27)
˜β(T )
r
(cr)
=
 
β(T )
r
(cr)
¯β(T )
r
!T
.
(13.28)
The entries in the new beliefs ˜β
(T ) = {˜β(T )
r
(Cr)} take values between 0 and 1, with the
maximal entry in each belief always having the value 1.
We now deﬁne the limiting value of these beliefs:
˜β(0)
r (cr) = lim
T −→0
˜β(T )
r
(cr).
(13.29)
Because the optimum changes continuously in T, and because the beliefs take values in a
convex space (all are in the range [0, 1]), the limit is well deﬁned. Our goal is to show that
the limit beliefs ˜β
(0) are a ﬁxed point of the max-product belief propagation algorithm for the
model ˜PΦ. We show this result in two parts. We ﬁrst show that the limit is max-calibrated, and
then that it provides a reparameterization of our distribution ˜PΦ.
Proposition 13.5
The limiting beliefs ˜β
(0) are max-calibrated.
Proof We wish to show that for any region r, any Xi ∈Cr, and any xi ∈Val(Xi), we have:
max
cr∼xi
˜β(0)
r (cr) = ˜β(0)
i
(xi).
(13.30)

584
Chapter 13. MAP Inference
Consider the left-hand side of this equality.
max
cr∼xi
˜β(0)
r (cr)
=
max
cr∼xi
h
lim
T −→0
˜β(T )
r
(cr)
i
(i)
=
lim
T −→0
" X
cr∼xi
(˜β(T )
r
(cr))1/T
#T
=
lim
T −→0
" X
cr∼xi
 
β(T )
r
(cr)
¯β(T )
r
!#T
=
lim
T −→0
"
1
¯β(T )
r
 X
cr∼xi
β(T )
r
(cr)
!#T
(ii)
=
lim
T −→0
"
1
¯β(T )
r
β(T )
i
(xi)
#T
(iii)
=
lim
T −→0
" ¯β(T )
i
¯β(T )
r
β(T )
i
(xi)
¯β(T )
i
#T
(iv)
=
lim
T −→0
" ¯β(T )
i
¯β(T )
r
(˜β(T )
i
(xi))1/T
#T
=
lim
T −→0


 ¯β(T )
i
¯β(T )
r
!T
˜β(T )
i
(xi)


(v)
=
lim
T −→0
˜β(T )
i
(xi)]
= ˜β(0)
i
(xi),
as required.
In this derivation, the step marked (i) is a general relationship between max-
imization and summation; see lemma 13.2.
The step marked (ii) is a consequence of the
sum-marginalization property of the region beliefs β(T )
r
(Cr) relative to the individual node
belief. The step marked (iii) is simply multiplying and dividing by the same expression. The
step marked (iv) is derived directly by substituting the deﬁnition of ˜β(T )
i
(xi). The step marked
(v) is a consequence of the fact that, because of sum-marginalization, ¯β(T )
i
/¯β(T )
r
(for Xi ∈Cr)
is bounded in the range [1, |Val(Cr −{Xi})|] for any T > 0, and therefore its limit, since
T −→0 is 1.
It remains to prove the following lemma:
Lemma 13.2
For i = 1, . . . , k, let ai(T) be a continuous function of T for T > 0. Then
max
i
lim
T −→0 ai(T) = lim
T −→0
 X
i
(ai(T))1/T
!T
.
(13.31)
Proof Because the functions are continuous, we have that, for some T0, there exists some j
such that, for any T < T0, aj(T) ≥ai(T) for all i ̸= j; assume, for simplicity, that this j

13.5. MAP as a Linear Optimization Problem ⋆
585
is unique. (The proof of the more general case is similar.) Let a∗
j = limT −→0 aj(T). The
left-hand side of equation (13.31) is then clearly a∗
j. The expression on the right-hand side can
be rewritten:
lim
T −→0 aj(T)
 X
i
 ai(T)
aj(T)
1/T !T
= a∗
j

lim
T −→0
 X
i
 ai(T)
aj(T)
T !1/T 
= a∗
j.
The ﬁrst equality follows from the fact that the aj(T) sequence is convergent. The second
follows from the fact that, because aj(T) > ai(T) for all i ̸= j and all T < T0, the ratio
ai(T)/aj(T) is bounded in [0, 1], with aj(T)/aj(T) = 1; therefore the limit is simply 1.
The proof of this lemma concludes the proof of the theorem.
We now wish to show the second important fact:
Theorem 13.9
The limit ˜β
(0) is a proportional reparameterization of ˜PΦ, that is:
˜PΦ(X) ∝
Y
r∈R
(˜β(0)
r (cr))κr.
Proof Due to equation (13.26), we have that
˜P (T )
Φ
(X) =
Y
r∈R
(β(T )
r
(cr))κr.
We can raise each side to the power T, and obtain that:
˜PΦ(X) =
 Y
r∈R
(β(T )
r
(cr))κr
!T
.
We can divide each side by
 Y
r∈R+
(¯β(T )
r
)κr
!T
,
to obtain the equality
˜PΦ(X)
Q
r∈R+(¯β(T )
r
)κr
T =
Y
r
(˜β(T )
r
(cr))κr.
This equality holds for every value of T > 0. Moreover, as we argued, the right-hand side is
bounded in [0, 1], and hence so is the left-hand side. As a consequence, we have an equality of
two bounded continuous functions of T, so that they must also be equal at the limit T −→0.
It follows that the limiting beliefs ˜β
(0) are proportional to a reparameterization of ˜PΦ.

586
Chapter 13. MAP Inference
13.5.3.3
Discussion
Overall, the analysis in this section reveals interesting connections between three separate al-
gorithms: the linear program relaxation of the MAP problem, the low-temperature limit of
sum-product belief propagation with convex counting numbers, and the max-product reparam-
eterization with (the same) convex counting numbers. These connections hold for any set of
convex counting numbers and any (positive) distribution ˜PΦ.
Speciﬁcally, we have characterized the solution to the relaxed LP as the limit of a sequence
of optimization problems, each deﬁned by a temperature-weighted convex energy functional.
Each of these optimization problems can be solved using an algorithm such as convex sum-
product BP, which (assuming convergence) produces optimal beliefs for that problem. We have
also shown that the beliefs obtained in this sequence can be reformulated to converge to a
new set of beliefs that are max-product calibrated. These beliefs are ﬁxed points of the convex
max-product BP algorithm. Thus, we can hope to use max-product BP to ﬁnd these limiting
beliefs.
Our earlier results show that the ﬁxed points of convex max-product BP, if they admit a locally
optimal assignment, are guaranteed to produce the MAP assignment. We can now make use of
the results in this section to shed new light on this analysis.
Theorem 13.10
Assume that we have a set of max-calibrated beliefs βr(Cr) and βi(Xi) such that equation (13.16)
holds. Assume furthermore that ξ∗is a locally consistent joint assignment relative to these beliefs.
Then the MAP-LP relaxation is tight.
Proof We ﬁrst observe that
max
ξ
log ˜PΦ(ξ) =
max
Q∈Marg[U] IEξ∼Q
h
log ˜PΦ(ξ)
i
≤
max
Q∈Local[U] IEξ∼Q
h
log ˜PΦ(ξ)
i
,
(13.32)
which is equal to the value of MAP-LP. Note that we are abusing notation in the expectation
used in the last expression, since Q ∈Local[U] is not a distribution but a set of pseudo-
marginals. However, because log ˜PΦ(ξ) factors according to the structure of the clusters in the
pseudo-marginals, we can use a set of pseudo-marginals to compute the expectation.
Next, we note that for any set of functions fr(Cr) whose scopes align with the clusters Cr,
we have that:
max
Q∈Local[U] IECr∼Q
"X
r
fr(Cr)
#
=
max
Q∈Local[U]
X
r
IECr∼Q[fr(Cr)]
≤
X
r
max
Cr fr(Cr),
because an expectation is smaller than the max.
We can now apply this derivation to the reformulation of ˜PΦ that we get from the reparame-
terization:
max
Q∈Local[U] IEQ
h
log ˜PΦ(ξ)
i
=
max
Q∈Local[U] IEQ
 P
r νr log(βr(cr)) + P
i νi log βi(xi)
+ P
i,r : Xi∈Cr νr,i (log βr(cr) −log βi(xi))

.

13.5. MAP as a Linear Optimization Problem ⋆
587
From the preceding derivation, it follows that:
≤
X
r
max
cr νr log(βr(cr)) +
X
i
max
xi νi log βi(xi)
+
X
i,r : Xi∈Cr
max
cr;xi=cr⟨Xi⟩νr,i (log βr(cr) −log βi(xi)) .
And from the positivity of the counting numbers, we get
=
X
r
νr max
cr log(βr(cr)) +
X
i
νi max
xi log βi(xi)
+
X
i,r : Xi∈Cr
νr,i
max
cr;xi=cr⟨Xi⟩(log βr(cr) −log βi(xi)) .
Now, due to lemma 13.1 (reformulated for log-factors), we have that ξ∗optimizes each of the
maximization expressions, so that we conclude:
=
X
r
νr log(βr(c∗
r)) +
X
i
νi log βi(x∗
i )
+
X
i,r : Xi∈Cr
νr,i (log βr(c∗
r) −log βi(x∗
i ))
= log ˜PΦ(ξ∗).
Putting this conclusion together with equation (13.32), we obtain:
max
ξ
log ˜PΦ(ξ) ≤
max
Q∈Local[U] IEξ∼Q
h
log ˜PΦ(ξ)
i
≤log ˜PΦ(ξ∗).
Because the right-hand side is clearly ≤the left-hand side, the entire inequality holds as an
equality, proving that
max
ξ
log ˜PΦ(ξ) =
max
Q∈Local[U] IEξ∼Q
h
log ˜PΦ(ξ)
i
,
that is, the value of the integer program optimization is the same as that of the relaxed LP.
This last fact has important repercussions. In particular, it shows that convex max-product

BP can be decoded only if the LP is tight; otherwise, there is no locally optimal joint
assignment, and no decoding is possible. It follows that convex max-product BP provides
provably useful results only in cases where MAP-LP itself provides the optimal answer
to the MAP problem. We note that a similar conclusion does not hold for nonconvex
variants such as those based on the standard Bethe counting numbers; in particular,
standard max-product BP is not an upper bound to MAP-LP, and therefore it can return
solutions in the interior of the polytope of MAP-LP. As a consequence, it may be decodable

588
Chapter 13. MAP Inference
even when the LP is not tight; in that case, the returned joint assignment may be the
MAP, or it may be a suboptimal assignment.
This result leaves several intriguing open questions. First, we note that this result shows a
connection between the results of max-product and the LP only when the LP is tight. It is an
open question whether we can show a general connection between the max-product beliefs and
the dual of the original LP. A second question is whether we can construct better techniques that
directly solve the LP or its dual; indeed, recent work (see section 13.9) explores a range of other
techniques for this task. A third question is whether this technique provides a useful heuristic:
Even if the reparameterization we derive does not have a locally consistent joint assignment, we
can still use it to construct an assignment using various heuristic methods, such as selecting for
each variable Xi the assignment x∗
i = arg maxxi βi(xi). While there are no guarantees about
this solution, it may still work well in practice.
13.6
Using Graph Cuts for MAP
In this section, we discuss the important class of metric and semi-metric MRFs, which we deﬁned
in box 4.D. This class has received considerable attention, largely owing to its importance in
computer- vision applications.
We show how this class of networks, although possibly very
densely connected, can admit an optimal or close-to-optimal solution, by virtue of structure in
the potentials.
13.6.1
Inference Using Graph Cuts
The basic graph construction is deﬁned for pairwise MRFs consisting solely of binary-valued
variables (V = {0, 1}). Although this case has restricted applicability, it forms the basis for the
general case. As we now show, the MAP problem for a certain class of binary-valued MRFs

can be solved optimally using a very simple and eﬃcient graph-cut algorithm. Perhaps
the most surprising aspect of this reduction is that this algorithm is guaranteed to
return the optimal solution in polynomial time, regardless of the structural complexity
of the underlying graph.
This result stands in contrast to most of the other results
presented in this book, where polynomial-time solutions were obtainable only for graphs
of low tree width.
Equally noteworthy is the fact that a similar result does not hold
for sum-product computations over this class of graphs; thus, we have an example of a
class of networks where sum-product inference and MAP inference have very diﬀerent
computational properties.
We ﬁrst deﬁne the min-cut problem for a graph, and then show how the MAP problem can
be reduced to it. The min-cut problem is deﬁned by a set of vertices Z, plus two distinguished
nodes generally known as s and t. We have a set of directed edges E over Z ∪{s, t}, where
each edge (z1, z2) ∈E is associated with a nonnegative cost cost(z1, z2). A graph cut is a
graph cut
disjoint partition of Z into Zs ∪Zt such that s ∈Zs and t ∈Zt. The cost of the cut is:
cost(Zs, Zt) =
X
z1∈Zs,z2∈Zt
cost(z1, z2).
In words, the cost is the total sum of the edges that cross from the Zs side of the partition
to the Zt side. The minimal cut is the partition Zs, Zt that achieves the minimal cost. While

13.6. Using Graph Cuts for MAP
589
presenting a min-cut algorithm is outside the scope of this book, such algorithms are standard,
have polynomial-time complexity, and are very fast in practice.
How do we reduce the MAP problem to one of computing cuts on a graph? Intuitively, we
need to design our graph so that a cut corresponds to an assignment to X, and its cost to the
value of the assignment. The construction follows straightforwardly from this intuition. Our
vertices (other than s, t) represent the variables in our MRF. We use the s side of the cut to
represent the label 0, and the t side to represent the label 1. Thus, we map a cut C = (Zs, Zt)
to the following assignment ξC:
xC
i = 0
if and only if
zi ∈Zs.
We begin by demonstrating the construction on the simple case of the generalized Ising
model of equation (4.6). Note that energy functions are invariant to additive changes in all of
the components, since these just serve to move all entries in E(x1, . . . , xn) by some additive
factor, leaving their relative order invariant. Thus, we can assume, without loss of generality,
that all components of the energy function are nonnegative. Moreover, we can assume that, for
every node i, either ϵi(1) = 0 or ϵi(0) = 0. We now construct the graph as follows:
•
If ϵi(1) = 0, we introduce an edge zi →t, with cost ϵi(0).
•
If ϵi(0) = 0, we introduce an edge s →zi, with cost ϵi(1).
•
For each pair of variables Xi, Xj that are connected by an edge in the MRF, we introduce
both an edge (zi, zj) and the edge (zj, zi), both with cost λi,j ≥0.
Now, consider the cost of a cut (Zs, Zt). If zi ∈Zs, then Xi is assigned a value of 0.
In this case, zi and t are on opposite sides of the cut, and so we will get a contribution of
ϵi(0) to the cost of the cut; this contribution is precisely the Xi node energy of the assignment
Xi = 0, as we would want. The analogous argument applies when zi ∈Zt. We now consider
the edge potential. The edge (zi, zj) only makes a contribution to the cut if we place zi and zj
on opposite sides of the cut; in this case, the contribution is λi,j. Conversely, the pair Xi, Xj
makes a contribution of λi,j to the energy function if Xi ̸= Xj, and otherwise it contributes
0. Thus, the contribution of the edge to the cut is precisely the same as the contribution of the
node pair to the energy function. Overall, we have shown that the cost of the cut is precisely the
same as the energy of the corresponding assignment. Thus, the min-cut algorithm is guaranteed
to ﬁnd the assignment to X that minimizes the energy function, that is, ξmap.
Example 13.17
Consider a simple example where we have four variables X1, X2, X3, X4 connected in a loop with
the edges X1—X2, X2—X3, X3—X4, X1—X4. Assume we have the following energies, where
we list only components that are nonzero:
ϵ1(0) = 7
ϵ2(1) = 2
ϵ3(1) = 1
ϵ4(1) = 6
λ1,2 = 6
λ2,3 = 6
λ3,4 = 2
λ1,4 = 1.
The graph construction and the minimum cut for this example are shown in ﬁgure 13.5.
Going by the node potentials alone, the optimal assignment is X1 = 1, X2 = 0, X3 = 0, X4 =
0. However, we also have interaction potentials that encourage agreement between neighboring
nodes. In particular, there are fairly strong potentials that induce X1 = X2 and X2 = X3. Thus,
the node-optimal assignment achieves a penalty of 7 from the contributions of λ1,2 and λ1,4.

590
Chapter 13. MAP Inference
z1
z4
z2
z3
s
t
1
1
2
2
6
6
6
7
Figure 13.5
Example graph construction for applying min-cut to the binary MAP problem, based
on example 13.17. Numbers on the edges represent their weight. The cut is represented by the set of
nodes in Zt. Dashed edges are ones that participate in the cut; note that only one of the two directions of
a bidirected edge contributes to the weight of the cut, which is 6 in this example.
Conversely, the assignment where X2 and X3 agree with X1 gets a penalty of only 6 from the X2
and X3 node contributions and from the weaker edge potentials λ3,4 and λ1,4. Thus, the overall
MAP assignment has X1 = 1, X2 = 1, X3 = 1, X4 = 0.
As we mentioned, the MAP problem in such graphs reduces to a minimum cut problem
regardless of the network connectivity. Thus, this approach allows us to ﬁnd MAP solution for a
class of MRFs for which probability computations are intractable.
We can easily extend this construction beyond the generalized Ising model:
Deﬁnition 13.5
A pairwise energy ϵi,j(·, ·) is said to be submodular if
submodular
energy function
ϵi,j(1, 1) + ϵi,j(0, 0) ≤ϵi,j(1, 0) + ϵi,j(0, 1).
(13.33)
The graph construction for submodular energies, which is shown in detail in algorithm 13.4,
is a little more elaborate. It ﬁrst normalizes each edge potential by subtracting ϵi,j(0, 0) from
all entries; this operation subtracts a constant amount from the energies of all assignments,
corresponding to a constant multiple in probability space, which only changes the (in this case
irrelevant) partition function. It then moves as much mass as possible to the individual node
potentials for i and j. These steps leave a single pairwise term that deﬁnes an energy only for
the assignment vi = 0, vj = 1:
ϵ′
i,j(0, 1) = ϵi,j(1, 0) + ϵi,j(0, 1) −ϵi,j(0, 0) −ϵi,j(1, 1).

13.6. Using Graph Cuts for MAP
591
Algorithm 13.4 Graph-cut algorithm for MAP in pairwise binary MRFs with submodular
potentials
Procedure MinCut-MAP (
ϵ
// Singleton and pairwise submodular energy factors
)
1
// Deﬁne the energy function
2
for all i
3
ϵ′
i ←ϵi
4
Initialize ϵ′
i,j to 0 for all i, j
5
for all pairs i < j
6
ϵ′
i(1) ←ϵ′
i(1) + (ϵi,j(1, 0) −ϵi,j(0, 0))
7
ϵ′
j(1) ←ϵ′
j(1) + (ϵi,j(1, 1) −ϵi,j(1, 0))
8
ϵ′
i,j(0, 1) ←ϵi,j(1, 0) + ϵi,j(0, 1) −ϵi,j(0, 0) −ϵi,j(1, 1)
9
10
// Construct the graph
11
for all i
12
if ϵ′
i(1) > ϵ′
i(0) then
13
E ←E ∪{(s, zi)}
14
cost(s, zi) ←ϵ′
i(1) −ϵ′
i(0)
15
else
16
E ←E ∪{(zi, t)}
17
cost(zi, t) ←ϵ′
i(0) −ϵ′
i(1)
18
for all pairs i < j such that ϵ′
i,j(0, 1) > 0
19
E ←E ∪{(zi, zj)}
20
cost(zi, zj) ←ϵ′
i,j(0, 1)
21
22
t ←MinCut({z1, . . . , zn}, E)
23
// MinCut returns ti = 1 iﬀzi ∈Zt
24
return t
Because of submodularity, this term satisﬁes ϵ′
i,j(0, 1) ≥0. The algorithm executes this trans-
formation for every pairwise potential i, j. The resulting energy function can easily be converted
into a graph using essentially the same construction that we used earlier; the only slight dif-
ference is that for our new energy function ϵ′
i,j(vi, vj) we need to introduce only the edge
(zi, zj), with cost ϵ′
i,j(0, 1); we do not introduce the opposite edge (zj, zi). We now use the
same mapping between s-t cuts in the graph and assignment to the variables X1, . . . , Xn.
It is not diﬃcult to verify that the cost of an s-t cut C in the resulting graph is precisely
E(ξC) + Const (see exercise 13.14). Thus, ﬁnding the minimum cut in this graph directly gives
us the cost-minimizing assignment ξmap.
Note that for pairwise submodular energy, there is an LP relaxation of the MAP integer
optimization, which is tight. Thus, this result provides another example where having a tight LP
relaxation allows us to ﬁnd the optimal MAP assignment.

592
Chapter 13. MAP Inference
13.6.2
Nonbinary Variables
In the case of nonbinary variables, we can no longer use a graph construction to solve the
MRF optimally. Indeed, the problem of optimizing the energy function, even if it is submodular,
is NP-hard in this case. Here, a very useful technique is to take greedy hill-climbing steps,
but where each step involves a globally optimal solution to a simpliﬁed problem. Two types of
steps have been utilized extensively: alpha-expansion and alpha-beta swap. As we will show,
under appropriate conditions on the energy function, both the alpha-expansion step and the
alpha-beta-swap steps can be performed optimally by applying the min-cut procedure to an
appropriately constructed MRF. Thus, the search procedure can take a global step in the space.
The alpha-expansion considers a particular value v; the step simultaneously considers all of
alpha-expansion
the variables Xi in the MRF, and allows each of them to take one of two values: it can keep its
current value xi, or change its value to v. Thus, the step expands the set of variables that take
the label v; the label v is often denoted α in the literature; hence the name alpha-expansion.
The alpha-expansion algorithm is shown in algorithm 13.5. It consists of repeated applications
of alpha-expansion steps, for diﬀerent labels v. Each alpha-expansion step is deﬁned relative to
our current assignment x and a target label v. Our goal is to select, for each variable Xi whose
current label xi is other than v, whether in the new assignment x′ its new label will remain xi
or move to v. We do so using a new MRF that has binary variables Ti for each variable Xi; we
then deﬁne a new assignment x′ so that x′
i = xi if Ti = t0
i , and x′
i = v if Ti = t1
i .
We deﬁne a new restricted energy function E′ using the following set of potentials:
restricted energy
function
ϵ′
i(t0
i )
=
ϵi(xi)
ϵ′
i(t1
i )
=
ϵi(v)
ϵ′
i,j(t0
i , t0
j)
=
ϵi,j(xi, xj)
ϵ′
i,j(t0
i , t1
j)
=
ϵi,j(xi, v)
ϵ′
i,j(t1
i , t0
j)
=
ϵi,j(v, xj)
ϵ′
i,j(t1
i , t1
j)
=
ϵi,j(v, v)
(13.34)
It is straightforward to see that for any assignment t, E′(t) = E(x′). Thus, ﬁnding the optimal
t corresponds to ﬁnding the optimal x′ in the restricted space of v-expansions of x.
In order to optimize t using graph cuts, the new energy E′ needs to be submodular, as in
equation (13.33). Plugging in the deﬁnition of the new potentials, we get the following constraint:
ϵi,j(xi, xj) + ϵi,j(v, v) ≤ϵi,j(xi, v) + ϵi,j(v, xj).
Now, if we have an MRF deﬁned by some distance function µ, then ϵi,j(v, v) = 0 by reﬂexivity,
and the remaining inequality is a direct consequence of the triangle inequality. Thus, we can
apply the alpha-expansion procedure to any metric MRF.
The second type of step is the alpha-beta swap. Here, we consider two labels: v1 and v2.
alpha-beta swap
The step allows each variable whose current label is v1 to keep its value or change it to v2, and
conversely for variables currently labeled v2. Like the alpha-expansion step, the alpha-beta swap
over a given assignment x can be deﬁned easily by constructing a new energy function, over
which min-cut can be performed. The details are left as an exercise (exercise 13.15). We note
that the alpha-beta-swap operation requires only that the energy function be a semimetric (that
is, the triangle inequality is not required).
These two steps allow us to use the min-cut procedure as a subroutine in solving the MAP
problem in metric or semimetric MRFs with nonbinary variables.

13.6. Using Graph Cuts for MAP
593
Algorithm 13.5 Alpha-expansion algorithm
Procedure Alpha-Expansion (
ϵ,
// Singleton and pairwise energies
x
// Some initial assignment
)
1
repeat
2
change ←false
3
for k = 1, . . . , K
4
t ←Alpha-Expand(ϵ,x, vk)
5
for i = 1, . . . , n
6
if ti = 1 then
7
xi ←vk
// If ti = 0, xi doesn’t change
8
change ←true
9
until change = false
10
return (x)
Procedure Alpha-Expand (
ϵ,
x
// Current assignment
v
// Expansion label
)
1
Deﬁne ϵ′ as in equation (13.34)
2
return MinCut-MAP(ϵ′)
Box 13.B — Case Study: Energy Minimization in Computer Vision. Over the past few years,
MRFs have become a standard tool for addressing a range of low-level vision tasks, some of which
we reviewed in box 4.B. As we discussed, the pairwise potentials in these models are often aimed
at penalizing discrepancies between the values of adjacent pixels, and hence they often naturally
satisfy the submodularity assumption that are necessary for the application of graph cut methods.
Also very popular is the TRW-S variant of the convex belief propagation algorithms, described in
box 13.A. Standard belief propagation has also been used in multiple applications.
Vision problems pose some signiﬁcant challenges. Although the grid structures associated with
images are not dense, they are very large, and they contain many tight loops, which can pose
diﬃculties for convergence of the message passing algorithm. Moreover, in some tasks, such as
stereo reconstruction, the value space of the variables is a discretization of a continuous space,
stereo
reconstruction
and therefore many values are required to get a reasonable approximation. As a consequence, the
representation of the pairwise potentials can get very large, leading to memory problems.
A number of fairly comprehensive empirical studies have been done comparing the various meth-
ods on a suite of computer-vision benchmark problems. By and large, it seems that for the grid-
structured networks that we described, graph-cut methods with the alpha-expansion step and TRW-
S are fairly comparable, with the graph-cut methods dominating in running time; both signiﬁcantly

594
Chapter 13. MAP Inference
Max-Product BP
a-Expansion
a-b Swap
TRW
Max-Product BP
a-Expansion
a-b Swap
TRW
2
1.9
1.8
1.7
1.6
1.5
1.4
1.3
Energy
Running Time (s)
100
×106
101
102
103
4.2
4.1
4
3.9
3.8
3.7
3.6
Energy
Running Time (s)
100
×105
101
102
Figure 13.B.1 — MAP inference for stereo reconstruction The top row contains a pair of stereo im-
ages for a problem known as Teddy and the target output (darker pixels denote a larger z value); the
images are taken from Scharstein and Szeliski (2003). The bottom row shows the best energy obtained
as a function of time by several diﬀerent MAP algorithms:max-product BP, the TRW variant of convex BP,
min-cut with alpha-expansion, and min-cut with alpha-beta swap. The left image is for Teddy, and the
right is for a diﬀerent stereo problem called Tsukuba.
outperform the other methods. Figure 13.B.1 shows some sample results on stereo-reconstruction
problems; here, the energies are close to submodular, allowing the application of a range of diﬀerent
methods.
The fact that convex BP is solving the dual problem to the relaxed LP allows it to provide a lower
bound on the energy of the true MAP assignment. Moreover, as we discussed, it can sometimes
provide optimality guarantees on the inferred solution. Thus, it is sometimes possible to compare the
results of these methods to the true global optimum of the energy function. Somewhat surprisingly,
it appears that both methods come very close to achieving optimal energies on a large fraction of
these benchmark problems, suggesting that the problem of energy minimization for these MRFs is

13.7. Local Search Algorithms ⋆
595
essentially solved.
In contrast to this optimistic viewpoint is the observation that the energy minimizing conﬁgura-
tion is often signiﬁcantly worse than the “target” assignment (for example, the true depth disparity
in a stereo reconstruction problem). In other words, the ground truth often has a worse energy
(lower probability) than the assignment that optimizes the energy function. This ﬁnding suggests
that a key problem is that of designing better energy functions, which better capture the structure
of our target assignments. This topic has been the focus of much recent work. In many cases, the
resulting energies involve nonlocal interactions between the pixels, and are therefore signiﬁcantly
more complex. Some evidence suggests that as the graph becomes more dense and less local, belief
propagation methods start to degrade. Conversely, as the potentials become less submodular, the
graph-cut methods become less applicable. Thus, the design of new energy-minimization methods
that are applicable to these richer energy functions is a topic of signiﬁcant current interest.
13.7
Local Search Algorithms ⋆
A ﬁnal class of methods that have been applied to MAP and marginal MAP queries are methods
that search over the space of assignments. The task of searching for a high-weight (or low-cost)
assignment of values to a set of variables is a central one in many applications, and it has
received attention in a number of communities. Methods for addressing this task come in many
ﬂavors.
Some of those methods are systematic: They search the space so as to ensure that assign-
systematic search
ments that are not considered are not optimal, and thereby guarantee an optimal solution. Such
methods generally search over the space of partial assignments, starting with the empty assign-
ment, and assigning variables one at a time. One such method, known as branch-and-bound, is
branch-and-
bound
described in appendix A.4.3.
Other methods are nonsystematic, and they come without performance guarantees. Here,
many of the methods search over the space of full assignments, usually by making local changes
to the assignment so as to improve its score. These local search methods generally provide
local search
no guarantees of optimality. Appendix A.4.2 describes some of the techniques that are most
commonly applied in practice.
The application of search techniques to the MAP problem is a fairly straightforward process:
The search space is deﬁned by the possible assignments ξ to X, and log ˜P(ξ) is the score; we
search space
omit details. Although generally less powerful than the methods we described earlier, these
methods do have some advantages. For example, the beam search method of appendix A.4.2
beam search
provides a useful alternative in cases where the complete model is too large to ﬁt into memory;
see exercise 15.10.
We also note that branch-and-bound does provide a simple method for
ﬁnding the K most likely assignment; see exercise 13.18. This algorithm requires at least as
much computation time as the clique tree–based algorithm, but signiﬁcantly less space.
These methods have much greater applicability in the context of marginal MAP problem,
marginal MAP
where most other methods are not (currently) applicable. Here, we search over the space of
assignments y to the max-variables Y . Here, we conduct the search so that we can ﬁx some
or all of the max-variables to have a concrete assignment. As we show, this allows us to remove
the constraint on the variable elimination ordering, allowing an unrestricted ordering to be used.

596
Chapter 13. MAP Inference
Here, we search over the space of assignments y for those that maximize
score(y) =
X
W
˜PΦ(y, W ).
(13.35)
Several search procedures are appropriate in this setting. In one approach, we use some local
search algorithm, as in appendix A.4.2. As usual in local search, the algorithm begins with
some complete assignment y0 to Y . We then consider applying diﬀerent search operators to
search operator
y; for each such operator o, we produce a new partial assignment y′ = o(y) as a successor
to the current state, which is evaluated by computing score(y′). Importantly, since we now
have a complete assignment to the max-variables y′ = o(y), the resulting score is simply a
sum-product expression, and it can be computed by standard sum-product elimination of the
variables W , with no constraints on the variable ordering. The tree-width in these cases is
usually much smaller than in the constrained case; for example, in the network of ﬁgure 13.2,
the network for a ﬁxed assignment y′ is simply a chain, and the computation of the score can
therefore be done in time linear in n.
While we can consider a variety of search operators, the most straightforward are operators
of the form do(Yi = yj
i ), which set a variable Yi ∈Y to the value yj
i . We can now apply any
greedy local-search algorithm, such as those described in appendix A.4.2. Empirical evidence
suggests that greedy hill climbing with tabu search performs very well on this task, especially
tabu search
if initialized intelligently. In particular, one simple yet good heuristic is to calibrate the clique
tree with no assignment to the max-variables; we then compute, for each Yi its unnormalized
probability ˜PΦ(Yi) (which can be extracted from any clique containing Yi), and initialize yi =
arg maxYi ˜PΦ(Yi).
While simple in principle, a naive implementation of this algorithm can be quite costly. Let
k = |Y | and assume for simplicity that |Val(Yi)| = d for all Yi ∈Y . Each step of the search
requires k × (d −1) evaluations of score, each of which involves a run of probabilistic inference
over the network. Even for simple networks, this cost can often be prohibitive.
Fortunately, we can greatly improve the computational performance of this algorithm using
the same type of dynamic programming tricks that we used in other parts of this book. Most
dynamic
programming
important is the observation that we can compute the score of all of the operators in our

search using a single run of clique tree propagation, in the clique tree corresponding
to an unconstrained elimination ordering.
Let T be an unconstrained clique tree over
X = Y ∪W , initialized with the original potentials of ˜PΦ. Let y be our current assignment
to Y . For any Yi, let Y −i = Y −{Yi} and y−i be the assignment in y to Y −i. We can use
the algorithm developed in exercise 10.12 to compute ˜PΦ(Yi, y−i) for every Yi ∈Y . Recall that
this algorithm requires only a single clique tree calibration that computes all of the messages;
with those messages, each clique that contains a variable Yi can locally compute ˜PΦ(Yi, y−i)
in time that is linear in the size of the clique. This idea reduces the cost of each step by a factor
of O(kd), an enormous saving. For example, in the network of ﬁgure 13.2, we can use a clique
tree whose cliques are of the form Xi, Yi+1, Xi+1, with sepsets Xi between cliques. Here, the
maximum clique size is 3, and the computation requires time linear in k.
We can also use search methods other than local hill climbing. One alternative is to utilize a
systematic search procedure that is guaranteed to ﬁnd the exact solution. Particularly well suited
to this task is the branch-and-bound search described in appendix A.4.3. Recall that branch-and-
bound systematically explores partial assignments to the variables Y ; it only discards a partial

13.8. Summary
597
assignment y′ if it already has a complete solution y that is provably better than the best
possible solution that one can obtain by extending y′ to a complete assignment. This pruning
relies on having a way of estimating the upper bound on a partial assignment y′. In our setting,
such an upper bound can be obtained by using variable elimination, ignoring the constraint on
the ordering whereby all summations occur before all maximizations. An algorithm based on
these ideas is developed further in exercise 13.20.
13.8
Summary
In this chapter, we have considered the problem of ﬁnding the MAP assignment and described
a number of methods for addressing it. The MAP problem has a broad range of applications,
in computer vision, computational biology, speech recognition, and more. Although the use of
MAP inference loses us the ability to measure our conﬁdence (or uncertainty) in our conclusions,
there are good reasons nevertheless for using a single MAP assignment rather than using the
marginal probabilities of the diﬀerent variables. One is the preference for obtaining a single
coherent joint assignment, whereas a set of individual marginals may not make sense as a
whole. The second is that there are inference methods that are applicable to the MAP problem
and not to the task of computing probabilities, so that the former may be tractable even when
the latter is not.
The methods we discussed fall into several major categories. The variable elimination method
is very similar to the approaches we discussed in chapter 9, where we replace summation with
maximization. The only slight extension is the traceback procedure, which allows us to identify
the MAP assignment once the variable elimination process is complete.
Although one can view the max-product clique tree algorithm as a dynamic programming
extension of variable elimination, it is more illuminating to view it as a method for reparame-
terizing the distribution to produce a max-calibrated set of beliefs. With this reparameterization,
we can convert the global optimization problem — ﬁnding a coherent joint assignment — to
a local optimization problem — ﬁnding a set of local assignments each of which optimizes its
(calibrated) belief. Importantly, the same view also characterizes the cluster-graph-based belief
propagation algorithms. The properties of max-calibrated beliefs allow us to prove strong (local
or global) optimality properties for the results of these diﬀerent message passing algorithms. In
particular, for message passing with convex counting numbers we can sometimes construct an
assignment that is the true MAP.
A seemingly very diﬀerent class of methods is based on considering the integer program
that directly encodes our optimization problem, and then constructing a relaxation as a linear
program. Somewhat surprisingly, there is a deep connection between the convex max-product
BP algorithm and the linear program relaxation. In particular, the solution to the dual problem
of this LP is a ﬁxed point of any convex max-product BP algorithm; thus, these algorithms can
be viewed as a computational method for solving this dual problem. The use of these message
passing methods oﬀers a trade-oﬀ: they are space-eﬃcient and easy to implement, but they
may not converge to the optimum of the dual problem.
Importantly, the ﬁxed point of a convex BP algorithm can be used to provide a MAP assign-
ment only if the MAP LP is a tight relaxation of the integer MAP optimization problem. Thus, it
appears that the LP relaxation is the fundamental construct in the application and analysis of

598
Chapter 13. MAP Inference
the convex BP algorithms. This conclusion motivates two recent lines of work in MAP inference:
One line attempts to construct tighter relaxations to the MAP optimization problem; importantly,
since the same relaxation is used for both the free energy optimization in section 11.3.6 and
for the MAP relaxations, progress made on improved relaxations for one task is directly useful
for the other. The second line of work attempts to solve the LP or its dual using techniques
other than message passing. While the problems are convex and hence can in principle be
solved directly using standard techniques, the size of the problems makes the cost of this sim-
ple approach prohibitive in many practical applications. However, the rich and well-developed
theory of convex optimization provides a wealth of potential tools, and some are already being
adapted to take advantage of the structure of the MAP problem. It is likely that eventually these
algorithms will replace convex BP as the method of choice for solving the dual. See section 13.9
for some references along those lines.
A diﬀerent class of algorithms is based on reducing the MAP problem in pairwise, binary MRFs
to one of ﬁnding the minimum cut in a graph. Although seemingly restrictive, this procedure
forms a basic building block for solving a much broader class of MRFs. These methods provide
an eﬀective solution method only for MRFs where the potentials satisfy (or almost satisfy) the
submodularity property. Conversely, their complexity depends fairly little on the complexity of
the graph (the number of edges); as such, they allow certain MRFs to be solved eﬃciently that are
not tractable to any other method. Empirically, for energies that are close to submodular,

the methods based on graph cuts are signiﬁcantly faster than those based on message
passing.
We note that in this case, also, there is an interesting connection to the linear
programming view: The case that admits an optimal solution using minimum cut (pairwise,
binary MRFs whose potentials are submodular) are also ones where there is a tight LP relaxation
to the MAP problem. Thus, one can view the minimum-cut algorithm as a specialized method
for exploiting special structure in the LP for solving it more eﬃciently.
In contrast to the huge volume of work on the MAP problem, relatively little work has been
done on the marginal MAP problem. This lack is, in some sense, not surprising: the intrinsic
diﬃculty of the problem is daunting and eliminates any hope of a general-purpose solution.
Nevertheless, it would be interesting to see whether some of the recent algorithmic techniques
developed for the MAP problem could be extended to apply to the marginal MAP case, leading
to new solutions to the marginal MAP problem for at least a subset of MRFs.
13.9
Relevant Literature
We begin by reminding the reader, before tackling the literature, that there is a conﬂict of
terminologies here: In some papers, the MAP problem is called MPE, whereas the marginal MAP
problem is called simply MAP.
The problem of ﬁnding the MAP assignment in a probabilistic model was ﬁrst addressed
by Viterbi (1967), in the context of hidden Markov models; this algorithm came to be called
the Viterbi algorithm. A generalization to other singly connected Bayesian networks was ﬁrst
Viterbi algorithm
proposed by Pearl (1988). The clique tree algorithm for this problem was described by Lauritzen
and Spiegelhalter (1988). Shimony (1994) showed that the MAP problem is NP-hard in general
networks.
The problem of ﬁnding a MAP assignment to an MRF is equivalent (up to a negative-logarithm

13.9. Relevant Literature
599
transformation) to the task of minimizing an energy function that is deﬁned as a sum of terms,
each involving a small number of variables. There is a considerably body of literature on the
energy minimization problem, in both continuous and discrete space. Extensive work on energy
energy
minimization
minimization in MRFs has been done in the computer-vision community, where the locality of
the spatial structure naturally deﬁnes a highly structured, often pairwise, MRF.
Early work on the energy minimization task focused on hill-climbing techniques, such as
simple coordinate ascent (known under the name iterated conditional modes (Besag 1986)) or
iterated
conditional
modes
simulated annealing (Barnard 1989). Many other search methods for the MAP problem have been
proposed, including systematic approaches such as branch-and-bound (Santos 1991; Marinescu
et al. 2003).
The interest in max-product belief propagation on a loopy graph ﬁrst arose in the context of
turbo-decoding. The ﬁrst general-purpose theoretical analysis for this approach was provided by
Weiss and Freeman (2001b), who showed optimality properties of an assignment derived from
an unambiguous set of beliefs reached at convergence of max-product BP. In particular, they
showed that the assignment is the global optimum for networks involving only a single loop,
and a strong local optimum (robust to changes in the assignments for a disjoint collection of
single loops and trees) in general.
Wainwright, Jaakkola, and Willsky (2004) ﬁrst proposed the view of message passing as repa-
rameterizing the distribution so as to get the local beliefs to correspond to max-marginals.
In subsequent work, Wainwright, Jaakkola, and Willsky (2005) developed the ﬁrst convexiﬁed
message passing algorithm for the MAP problem. The algorithm, known as TRW, used an ap-
proximation of the energy function based on a convex combination of trees. This paper was
the ﬁrst to show lemma 13.1. It also showed that if a ﬁxed point of the TRW algorithm satisﬁed
a stronger property than local optimality, it provided the MAP assignment. However, the TRW
algorithm did not monotonically improve its objective, and indeed the algorithm was generally
not convergent. Kolmogorov (2006) deﬁned TRW-S, a variant of TRW that passes message asyn-
chronously, in a particular order. TRW-S is guaranteed to increase the objective monotonically,
and hence is convergent. However, TRW-S is not guaranteed to converge to the global optimum
of the dual objective, since it can get stuck in local optima.
The connections between max-product BP, the lower-temperature limit of sum-product BP,
and the linear programming relaxation were studied by Weiss, Yanover, and Meltzer (2007). They
also showed results on the optimality of partial assignments extracted from unambiguous beliefs
derived from convex BP ﬁxed points, extending earlier results of Kolmogorov and Wainwright
(2005) for TRW-S.
Max ﬂow techniques to solve submodular binary problems were originally developed by Boros,
Hammer and collaborators (Hammer 1965; Boros and Hammer 2002). These techniques were
popularized in the vision-MRF community by Greig, Porteous, and Seheult (1989), who were the
ﬁrst to apply these techniques to images. Ishikawa (2003) extended this work to the nonbinary
case, but assuming that the interaction between variables is convex. Boykov, Veksler, and Zabih
(2001) were the ﬁrst to propose the alpha-expansion and alpha-beta swap steps, which allow
the application of graph-cut methods to nonbinary problems; they also prove certain guarantees
regarding the energy of the assignment found by these global steps, relative to the energy of
the optimal MAP assignment. Kolmogorov and Zabih (2004) generalized and analyzed the graph
constructions used in these methods, using techniques similar to those described by Boros
and Hammer (2002). Recent work extends the scope of the MRFs to which these techniques

600
Chapter 13. MAP Inference
can be applied, by introducing preprocessing steps that modify factors that do not satisfy the
submodularity assumptions. For example, Rother et al. (2005) consider a method that truncates
the potentials that do not conform to submodularity, as part of the iterative alpha-expansion
algorithm, and they show that this approach, although not making optimal alpha-expansion
steps, is still guaranteed to improve the objective at each iteration. We note that, for the case
of metric potentials, belief propagation algorithms such as TRW also do well (see box 13.B);
moreover, Felzenszwalb and Huttenlocher (2006) show how the computational cost of each
message passing step can be reduced from O(K2) to O(K), where K is the total number of
labels, reducing the cost of these algorithms in this setting.
Szeliski et al. (2008) perform an in-depth empirical comparison of the performance of diﬀerent
methods on an ensemble of computer vision benchmark problems. Other empirical comparisons
include Meltzer et al. (2005); Kolmogorov and Rother (2006); Yanover et al. (2006).
The LP relaxation for MRFs was ﬁrst proposed by Schlesinger (1976), and then subsequently
rediscovered independently by several researchers. Of these, the most relevant to our presen-
tation is the work of Wainwright, Jaakkola, and Willsky (2005), who also established the ﬁrst
connection between the LP dual and message passing algorithms, and proposed the TRW algo-
rithm. Various extensions were subsequently proposed by various authors, based on diﬀerent
relaxations that require more complex convex optimization algorithms (Muramatsu and Suzuki
2003; Kumar et al. 2006; Ravikumar and Laﬀerty 2006). Surprisingly, Kumar et al. (2007) subse-
quently showed that the simple LP relaxation was tighter (that is, better) relaxation than all of
those more sophisticated methods.
A spate of recent works (Komodakis et al. 2007; Schlesinger and Giginyak 2007a,b; Sontag
and Jaakkola 2007; Globerson and Jaakkola 2007b; Werner 2007; Sontag et al. 2008) make much
deeper use of the linear programming relaxation of the MAP problem and of its dual. Globerson
and Jaakkola (2007b); Komodakis et al. (2007) both demonstrate a message passing algorithm
derived from this dual. The algorithm of Komodakis et al. is based on a dual decomposition
algorithm, and is therefore guaranteed to converge to the optimum of the dual objective. Solving
the LP relaxation or its dual does not generally give rise to the optimal MAP assignment.
The work of Sontag and Jaakkola (2007); Sontag et al. (2008) shows how we can use the LP
formulation to gradually add local constraints that hold for any set of pseudo-marginals deﬁned
by a real distribution. These constraints make the optimization space a tighter relaxation of the
marginal polytope and thereby lead to improved approximations. Sontag et al. present empirical
results that show that a small number of constraints often suﬃce to deﬁne the optimal MAP
assignment.
Komodakis and colleagues 2005; 2007 also make use of LP duality in the context of graph cut
methods, where it corresponds to the well-known duality between min-cut and max-ﬂow. They
use this approach to derive primal-dual methods that speed up and extend the alpha-expansion
method in several ways.
Santos (1991, 1994) studied the question of ﬁnding the M most likely assignments. He pre-
sented an exact algorithm that uses the linear programming relaxation of the integer program,
augmented with a branch-and-bound search that uses the LP as the bound. Nilsson (1998) pro-
vides an alternative algorithm that uses propagation in clique trees. Yanover and Weiss (2003)
subsequently generalized this algorithm for the case of loopy BP.
Park and Darwiche extensively studied the marginal MAP problem, providing complexity re-
sults (Park 2002; Park and Darwiche 2001), local search algorithms (Park and Darwiche 2004a)

13.10. Exercises
601
(including an eﬃcient clique tree implementation), and a systematic branch-and-bound algo-
rithm (Park and Darwiche 2003) based on the bound obtained by exchanging summation and
maximization.
The study of constraint satisfaction problems, and related problems such as Boolean satisﬁ-
ability (see appendix A.3.4) is the focus of a thriving research community, and much progress
has been made. One recent overview can be found in the textbook of Dechter (2003). There has
been a growing interest recently in relating CSP methods to belief propagation techniques, most
notably the survey propagation (for example, (Maneva et al. 2007)).
survey
propagation
13.10
Exercises
Exercise 13.1⋆⋆
Prove theorem 13.1.
Exercise 13.2⋆
Provide a structured variable elimination algorithm that solves the MAP task for networks with rule-based
CPDs.
a. Modify the algorithm Rule-Sum-Product-Eliminate-Var in algorithm 9.7 to deal with the max-product
task.
b. Show how we can perform the backward phase that constructs the most likely assignment to X. Make
sure you describe which information needs to be stored in the forward phase so as to enable the
backward phase.
Exercise 13.3
Prove theorem 13.4.
Exercise 13.4
Show how to adapt Traceback-MAP of algorithm 13.1 to ﬁnd the marginal MAP assignment, given the factors
computed by a run of variable elimination for marginal MAP.
Exercise 13.5⋆
Consider the task of ﬁnding the second-most-likely assignment in a graphical model. Assume that we have
produced a max-calibrated clique tree.
a. Assume that the probabilistic model is unambiguous. Show how we can ﬁnd the second-best assign-
ment using a single pass over the clique tree.
b. Now answer the same question in the case where the probabilistic model is ambiguous. Your method
should use only the precomputed max-marginals.
Exercise 13.6⋆
Now, consider the task of ﬁnding the third-most-likely assignment in a graphical model.
Finding the
third-most-probable assignment is more complicated, since it cannot be computed from max-marginals
alone.
a. We deﬁne the notion of constrained max-marginal: a max-marginal in a distribution that has some
variable Xk constrained to take on only certain values. For Dk ⊂Val(Xk), we deﬁne the constrained
max-marginal of Xi to be:
MaxMarg ˜
PXk∈Dk (Xi = xi) =
max
{x:Xi=xi,Xk∈Dk}
˜P(x).
Explain how to compute the preceding constrained max-marginals for all i and xi using max-product
message passing.

602
Chapter 13. MAP Inference
b. Find the third-most-probable assignment by using two sets of constrained max-marginals.
Exercise 13.7
Prove proposition 13.1.
Exercise 13.8
Prove proposition 13.3.
Exercise 13.9
Assume that max-product belief propagation converges to a set of calibrated beliefs βi(Ci). Assume that
each belief is unambiguous, so that it has a unique maximizing assignment c∗
i . Prove that all of these
locally optimizing assignments are consistent with each other, in that if Xk = x∗
k in one assignment c∗
i ,
then Xk = x∗
k in every other assignment c∗
j for which Xk ∈Cj.
Exercise 13.10
Construct an example of a max-product calibrated cluster graph in which (at least) some beliefs have two
locally optimal assignments, such that one local assignment can be extended into a globally consistent
joint assignment (across all beliefs), and the other cannot.
Exercise 13.11⋆
Consider a cluster graph U that contains only a single loop, and assume that we have a set of max-product
calibrated beliefs {βi} for U and an assignment ξ∗that is locally optimal relative to {βi}. Prove that ξ∗
is the MAP assignment relative to the distribution PU. (Hint: Use lemma 13.1 and a proof similar to that of
theorem 13.6.)
Exercise 13.12
Using exercise 13.11, complete the proof of theorem 13.6. First prove the result for sets Z for which UZ
contains only a single loop. Then prove the result for any Z for which UZ is a combination of disconnected
trees and loops.
Exercise 13.13
Prove proposition 13.4.
Exercise 13.14
Show that the algorithm in algorithm 13.4 returns the correct MAP assignment. First show that for any cut
C = Zs, Zt, we have that
cost(C) = E(ξC) + Const.
Conclude the desired result.
Exercise 13.15⋆
Show how the optimal alpha-beta swap step can be found by running min-cut on an appropriately
constructed graph. More precisely:
a. Deﬁne a set of binary variables t1, . . . , tn, such that the value of the ti’s deﬁnes an alpha-beta-swap
transformation on the xi’s.
b. Deﬁne an energy function E′ over the T variables such that E′(t) = E(x′).
c. Show that the energy function E′ is submodular if the original energy function E is a semimetric.
Exercise 13.16⋆
As we discussed, many energy functions are not submodular. We now describe a method that allows
min-cut methods to be applied to energy functions where most of the terms are submodular, but some
small subset is not submodular. This method is based on the truncation of the nonsubmodular potentials,
truncation
so as to make them submodular.

13.10. Exercises
603
Algorithm 13.6 Eﬃcient min-sum message passing for untruncated 1-norm energies
Procedure Msg-Truncated-1-Norm (
c
// Parameters deﬁning the pairwise factor
hi(xi)
// Single-variable term in equation (13.36)
)
1
for xj = 1, . . . , K −1
2
r(xj) ←min[hi(xj), r(xj −1) + c]
3
for xj = K −2, . . . , 0
4
r(xj) ←min[r(xj), r(xj + 1) + c]
5
return (r)
a. Let E be an energy function over binary-valued variables that contains some number of pairwise
terms ϵi,j(vi, vj) that do not satisfy equation (13.33). Assume that we replace each such pairwise term
ϵi,j with a term ϵ′
i,j that satisﬁes this inequality, by decreasing ϵi,j(0, 0), by increasing ϵi,j(1, 0) or
ϵi,j(0, 1), or both. The node energies remain unchanged. Let E′ be the resulting energy.
Show that if ξ∗optimizes E′, then E(ξ∗) ≤E(0)
b. Describe how, in the multilabel case, this procedure can be used within the alpha-expansion algorithm
to ﬁnd a local optimum of the energy function.
Exercise 13.17⋆
Consider the task of passing a message over an edge Xi—Xj in a metric MRF; our goal is to make
the message passing step more eﬃcient by exploiting the metric structure. As usual in metric MRFs, we
consider the problem in terms of energies; thus, our message computation takes the form:
δi→j(xj) = min
xi (ϵi,j(xi, xj) + hi(xi)),
(13.36)
where hi(xi) = ϵi(xi) + P
k̸=j δi→j(xk). In general, this computation requires O(K2) steps. However,
we now consider two special cases where this computation can be done in O(K) steps.
a. Assume that ϵi,j(xi, xj) is an Ising energy function, as in equation (4.6). Show how the message can
be computed in O(K) steps.
b. Now assume that both Xi, Xj take on values in {0, . . . , K −1}.
Assume that ϵi,j(xi, xj) is a
nontruncated 1-norm, as in equation (4.7) with p = 1 and distmax = ∞. Show that the algorithm in
algorithm 13.6 computes the correct message in O(K) steps.
c. Extend the algorithm of algorithm 13.6 to the case of a truncated 1-norm (where distmax < ∞).
Exercise 13.18⋆
Consider the use of the branch-and-bound algorithm of appendix A.4.3 for ﬁnding the top K highest-
probability assignments in an (unnormalized) distribution ˜PΦ deﬁned by a set of factors Φ.
a. Consider a partial assignment y to some set of variables Y . Provide both an upper and a lower bound
to log ˜PΦ(y).
b. Describe how to use your bounds in the context of a branch-and-bound algorithm to ﬁnd the MAP
assignment for ˜PΦ. Can you use both the lower and upper bounds in your search?
c. Extend your algorithm to ﬁnd the K highest probability joint assignments in ˜PΦ. Hint: Your algorithm
should ﬁnd the assignments in order of decreasing probability, starting with the MAP. Be sure to reuse
as much of your previous computations as possible as you continue the search for the next assignment.

604
Chapter 13. MAP Inference
Exercise 13.19
Show that, for any function f,
max
x
X
y
f(x, y) ≤
X
y
max
x
f(x, y),
(13.37)
and provide necessary and suﬃcient conditions for when equation (13.37) holds as equality.
Exercise 13.20⋆
a. Use equation (13.37) to provide an eﬃcient algorithm for computing an upper bound
bound(y1...i) =
max
yi+1,...,yn score(y1...i, yi+1, . . . , yn),
where score(y) is deﬁned as in equation (13.35). Your computation of the bound should take no more
than a run of variable elimination in an unconstrained elimination ordering over all of the network
variables.
b. Use this bound to construct a branch-and-bound algorithm for the marginal-MAP problem.
Exercise 13.21⋆
In this question, we consider the application of conditioning to a marginal MAP query:
arg max
Y
X
Z
Y
φ∈Φ
φ.
Let U be a set of conditioning variables.
a. Consider ﬁrst the case of a simple MAP query, so that Z = ∅and Y = X. Show how you would
adapt Conditioning in algorithm 9.5 to deal with the max-product rather than the sum-product task.
b. Now, consider a max-sum-product task.
When is U a legal set of conditioning variables for this
query? Justify your response. (Hint: Recall that the order of the operations we perform must respect
the ordering constraint discussed in section 2.1.5, and that the elimination operations work from the
outside in, and the conditioning operations from the inside out.)
c. Now, assuming that U is a legal set of conditioning variables, specify a conditioning algorithm that
computes the value of the corresponding max-sum-product query, as in equation (13.8).
d. Extend your max-sum-product algorithm to compute the actual maximizing assignment to Y , as in the
MAP query. Your algorithm should work for any legal conditioning set U.

14
Inference in Hybrid Networks
In our discussion of inference so far, we have focused on the case of discrete probabilistic models.
However, many interesting domains also contain continuous variables such as temperature,
location, or distance. In this chapter, we address the task of inference in graphical models that
involve such variables.
For this chapter, let X = Γ∪∆, where Γ denotes the continuous variables and ∆the discrete
variables. In cases where we wish to distinguish discrete and continuous variables, we use the
convention that discrete variables are named with letters near the beginning of the alphabet
(A, B, C), whereas continuous ones are named with letters near the end (X, Y, Z).
14.1
Introduction
14.1.1
Challenges
At an abstract level, the introduction of continuous variables in a graphical model is not diﬃcult.
As we saw in section 5.5, we can use a range of diﬀerent representations for the CPDs or factors
in our network. We now have a set of factors, over which we can perform the same operations
that we utilize for inference in the discrete case: We can multiply factors, which in this case
corresponds to multiplying the multidimensional continuous functions representing the factors;
and we can marginalize out variables in a factor, which in this case is done using integration
rather than summation. It is not diﬃcult to show that, with these operations in hand, the sum-
product inference algorithms that we used in the discrete case can be applied without change,
and are guaranteed to lead to correct answers.
Unfortunately, a little more thought reveals that the correct implementation of these basic
operations poses a range of challenges, whose solution is far from obvious.
The ﬁrst challenge involves the representation of factors involving continuous variables. Unlike
discrete variables, there is no universal representation of a factor over continuous variables, and
so we must usually select a parametric family for each CPD or initial factor in our network. Even
if we pick the same parametric family for each of our initial factor in the network, it may not be
the case that multiplying factors or marginalizing a factor leaves it within the parametric family.
If not, then it is not even clear how we would represent the intermediate results in our inference
process. The situation becomes even more complex when factors in the original network call
for the use of diﬀerent parametric families. In this case, it is generally unlikely that we can

ﬁnd a single parametric family that can correctly encode all of the intermediate factors

606
Chapter 14. Inference in Hybrid Networks
in our network.
In fact, in some cases — most notably networks involving both discrete
and continuous variables — one can show that the intermediate factors cannot be represented
using any ﬁxed number of parameters; in fact, the representation size of those factors grows
exponentially with the size of the network.
A second challenge involves the marginalization step, which now requires integration rather
than summation. Integration introduces a new set of subtleties. First, not all functions are
integrable: in some cases, the integral may be inﬁnite or even ill deﬁned. Second, even functions
where the integral is well deﬁned may not have a closed-form integral, requiring the use of a
numerical integration method, which is usually approximate.
14.1.2
Discretization
An alternative approach to inference in hybrid models is to circumvent the entire problem of
dealing with continuous factors: We simply convert all continuous variable to discrete ones
by discretizing their domain into some ﬁnite set of intervals.
Once all variables have been
discretization
discretized, the result is a standard discrete probabilistic model, which we can handle using the
standard inference techniques described in the preceding chapters.
How do we convert a hybrid CPDs into a table? Assume that we have a variable Y with a
continuous parent X. Let A be the discrete variable that replaces X and B the discrete variable
that replaces Y . Let a ∈Val(A) correspond to the interval [x1, x2] for X, and b ∈Val(B)
correspond to the interval [y1, y2] for Y .
In principle, one approach for discretization is to deﬁne
P(b | a) =
Z x2
x1 p(Y ∈[y1, y2] | X = x)p(X = x | X ∈[x1, x2])dx.
This integral averages out the conditional probability that Y is in the interval [y1, y2] given X,
aggregating over the diﬀerent values of x in the relevant interval for X. The distribution used
in this formulation is the prior probability p(X), which has the eﬀect of weighting the average
more toward more likely values of X. While plausible, this computation is expensive, since it
requires that we perform inference in the model. Moreover, even if we perform our estimation
relative to the prior p(X), we have no guarantees of a good approximation, since our posterior
over X may be quite diﬀerent.
Therefore, for simplicity, we often use simpler approximations. In one, we simply select some
particular value x∗∈[x1, x2], and estimate P(b | a) as the total probability mass of the interval
[y1, y2] given x∗: :
P(Y ∈[y1, y2] | x∗) =
Z y2
y1 p(y | x∗)dy.
For some density functions p, we can compute this interval in closed form. In others, we might
have to resort to numerical integration methods. Alternatively, we can average the values over
the interval [x1, x2] using a predeﬁned distribution, such as the uniform distribution over the
interval.
Although discretization is used very often in practice, as we discussed in section 5.5, it has
several signiﬁcant limitations. The discretization is only an approximation of the true probability

14.1. Introduction
607
distribution. In order to get answers that do not lose most of the information, our discretization
scheme must have a ﬁne resolution where the posterior probability mass lies. Unfortunately,
before we actually perform the inference, we do not know the posterior distribution. Thus, we
must often resort to a discretization that is fairly ﬁne-grained over the entire space, leading to a
very large domain for the resulting discrete variable.
This problem is particularly serious when we need to approximate distributions over more
than a handful of discretized variables. As in any table-based CPD, the size of the resulting factor
is exponential in the number of variables. When this number is large and the discretization is
anything but trivial, the size of the factor can be huge. For example, if we need to represent
a distribution over d continuous variables, each of which is discretized into m values, the total
number of parameters required is O(md). By contrast, if the distribution is a d-dimensional
Gaussian, the number of parameters required is only O(d2). Thus, not only does the discretiza-
tion process introduce approximations into our joint probability distribution, but we also often
end up converting a polynomial parameterization into an exponential one.

Overall, discretization provides a trade-oﬀbetween accuracy of the approximation and
cost of computation. In certain cases, acceptable accuracy can be obtained at reasonable
cost. However, in many practical applications, the computational cost required to obtain
the accuracy needed for the task is prohibitive, making discretization a nonviable option.
14.1.3
Overview
Thus, we see that inference in continuous and hybrid models, although similar in principle to
discrete inference, brings forth a new set of challenges. In this chapter, we discuss some of these
issues and show how, in certain settings, these challenges can be addressed.
As for inference in discrete networks, the bulk of the work on inference in continuous and
hybrid networks falls largely into two categories: Approaches that are based on message passing
methods, and approaches that use one of the particle-based methods discussed in chapter 12.
However, unlike the discrete case, even the message passing algorithms are rarely exact.
The message passing inference methods have largely revolved around the use of the Gaussian
distribution. The easiest case is when the distribution is, in fact, a multivariate Gaussian. In this
case, many of the challenges described before disappear. In particular, the intermediate factors
in a Gaussian network can be described compactly using a simple parametric representation
called the canonical form. This representation is closed under the basic operations using in
inference: factor product, factor division, factor reduction, and marginalization. Thus, we can
deﬁne a set of simple data structures that allow the inference process to be performed. Moreover,
the integration operation required by marginalization is always well deﬁned, and it is guaranteed
to produce a ﬁnite integral under certain conditions; when it is well deﬁned, it has a simple
analytical solution.
As a consequence, a fairly straightforward modiﬁcation of the discrete sum-product algo-
rithms (whether variable elimination or clique tree) gives rise to an exact inference algorithm
for Gaussian networks. A similar extension results in a Gaussian version of the loopy belief
propagation algorithm; here, however, the conditions on integrability impose certain constraints
about the form of the distribution. Importantly, under these conditions, loopy belief propagation
for Gaussian distributions is guaranteed to return the correct means for the variables in the
network, although it can underestimate the variances, leading to overconﬁdent estimates.

608
Chapter 14. Inference in Hybrid Networks
There are two main extensions to the purely Gaussian case: non-Gaussian continuous densi-
ties, and hybrid models that involve both discrete and continuous variables. The most common
method for dealing with these extensions is the same: we approximate intermediate factors in
the computation as Gaussians; in eﬀect, these algorithms are an instance of the expectation
propagation algorithm discussed in section 11.4.4.
As we discuss, Gaussians provide a good
basis for the basic operations in these algorithms, including the key operation of approximate
marginalization. Interestingly, there is one class of inference tasks where this general algorithm is
guaranteed to produce exact answers to a certain subset of queries. This is the class of CLG net-
works in which we use a particular form of clique tree for the inference. Unfortunately, although
of conceptual interest, this “exact” variant is rarely useful except in fairly small problems.
An alternative approach is to use an approximation method that makes no parametric as-
sumptions about the distribution. Speciﬁcally, we can approximate the distribution as a set of
particles, as described in chapter 12. As we will see, particle-based methods often provide the
easiest approach to inference in a hybrid network. They make almost no assumptions about
the form of the CPDs, and can approximate an arbitrarily complex posterior. Their primary
disadvantage is, as usual, the fact that a very large number of particles might be required for a
good approximation.
14.2
Variable Elimination in Gaussian Networks
The ﬁrst class of networks we consider is the class of Gaussian networks, as described in
chapter 7: These are networks where all of the variables are continuous, and all of the local
factors encode linear dependencies. In the case of Bayesian networks, the CPDs take the form
of linear Gaussians (as in deﬁnition 5.15). In the case of Markov networks, they can take the
form of general log-quadratic form, as in equation (7.7).
As we showed in chapter 7, both of these representations are simply alternative parameter-
izations of a joint multivariate Gaussian distributions. This observation immediately suggests
one approach to performing exact inference in this class of networks: We simply convert the
LG network into the equivalent multivariate Gaussian, and perform the necessary operations
— marginalization and conditioning — on that representation. Speciﬁcally, as we discussed,
if we have a Gaussian distribution p(X, Y ) represented as a mean vector and a covariance
matrix, we can extract the marginal distribution p(Y ) simply by restricting attention to the
elements of the mean and the covariance matrix that correspond to the variables in Y . The
operation of conditioning a Gaussian on evidence Y = y is also easy: we simply instantiate
the variables Y to their observed values y in the joint density function, and renormalize the
resulting unnormalized measure over X to obtain a new Gaussian density.
This approach simply generates the joint distribution over the entire set of variables in the
network, and then manipulates it directly. However, unlike the case of discrete distributions, the
representation size of the joint density in the Gaussian case is quadratic, rather than exponential,
in the number of variables. Thus, these operations are often feasible in a Gaussian network in
cases that would not be feasible in a comparable discrete network.
Still, even quadratic cost might not be feasible in many cases, for example, when the network
is over thousands of variables. Furthermore, this approach does not exploit any of the structure
represented in the network distribution. An alternative approach to inference is to adapt the

14.2. Variable Elimination in Gaussian Networks
609
message passing algorithms, such as variable elimination (or clique trees) for exact inference, or
belief propagation for approximation inference, to the linear Gaussian case. We now describe
this approach. We begin with describing the basic representation used for these message passing
schemes, and then present these two classes of algorithms.
14.2.1
Canonical Forms
As we discussed, the key diﬀerence between inference in the continuous and the discrete case
is that the factors can no longer be represented as tables. Naively, we might think that we can
represent factors as Gaussians, but this is not the case. The reason is that linear Gaussian CPDs
are generally not Gaussians, but are rather a conditional distribution. Thus, we need to ﬁnd
a more general representation for factors, that accommodates both Gaussian distributions and
linear Gaussian models, as well as any combination of these models that might arise during the
course of inference.
14.2.1.1
The Canonical Form Representation
The simplest representation used in this setting is the canonical form, which represents the
intermediate result as a log-quadratic form exp(Q(x)) where Q is some quadratic function. In
the inference setting, it is useful to make the components of this representation more explicit:
Deﬁnition 14.1
A canonical form C (X; K, h, g) (or C (K, h, g) if we omit the explicit reference to X) is deﬁned
canonical form
as:
C (X; K, h, g) = exp

−1
2XT KX + hT X + g

.
(14.1)
We can represent every Gaussian as a canonical form. Rewriting equation (7.1), we obtain:
1
(2π)n/2|Σ|1/2 exp

−1
2(x −µ)T Σ−1(x −µ)

=
exp

−1
2xT Σ−1x + µT Σ−1x −1
2µT Σ−1µ −log

(2π)n/2|Σ|1/2
.
Thus, N (µ; Σ) = C (K, h, g) where:
K
=
Σ−1
h
=
Σ−1µ
g
=
−1
2µT Σ−1µ −log

(2π)n/2|Σ|1/2
.
However, canonical forms are more general than Gaussians: If K is not invertible, the canonical
form is well deﬁned, but it is not the inverse of a legal covariance matrix. In particular, we can
easily represent linear Gaussian CPDs as canonical forms (exercise 14.1).

610
Chapter 14. Inference in Hybrid Networks
14.2.1.2
Operations on Canonical Forms
It is possible to perform various operations on canonical forms. The product of two canonical
canonical form
product
form factors over the same scope X is simply:
C (K1, h1, g1) · C (K2, h2, g2) = C (K1 + K2, h1 + h2, g1 + g2) .
(14.2)
When we have two canonical factors over diﬀerent scopes X and Y , we simply extend the
scope of both to make their scopes match and then perform the operation of equation (14.2).
The extension of the scope is performed by simply adding zero entries to both the K matrices
and the h vectors.
Example 14.1
Consider the following two canonical forms:
φ1(X, Y )
=
C

X, Y ;

1
−1
−1
1

,

1
−1

, −3

φ2(Y, Z)
=
C

Y, Z;

3
−2
−2
4

,

5
−1

, 1

.
We can extend the scope of both of these by simply introducing zeros into the canonical form. For
example, we can reformulate:
φ1(X, Y, Z) = C

X, Y, Z;


1
−1
0
−1
1
0
0
0
0

,


1
−1
0

, −3

,
and similarly for φ2(X, Y, Z). The two canonical forms now have the same scope, and can be
multiplied using equation (14.2) to produce:
C

X, Y, Z;


1
−1
0
−1
4
−2
0
−2
4

,


1
4
−1

, −2

.
The division of canonical forms (which is required for message passing in the belief propaga-
canonical form
division
tion algorithm) is deﬁned analogously:
C (K1, h1, g1)
C (K2, h2, g2) = C (K1 −K2, h1 −h2, g1 −g2) .
(14.3)
Note that the vacuous canonical form, which is the analogue of the “all 1” factor in the discrete
vacuous
canonical form
case, is deﬁned as K = 0, h = 0, g = 0. Multiplying or dividing by this factor has no eﬀect.
Less obviously, we can marginalize a canonical form onto a subset of its variables.
Let
canonical form
marginalization
C (X, Y ; K, h, g) be some canonical form over {X, Y } where
K =
 KXX
KXY
KY X
KY Y

;
h =
 hX
hY

.
(14.4)
The marginalization of this function onto the variables X is, as usual, the integral over the
variables Y :
Z
C (X, Y ; K, h, g) dY .

14.2. Variable Elimination in Gaussian Networks
611
As we discussed, we have to guarantee that all of the integrals resulting from marginalization
operations are well deﬁned. In the case of canonical forms, the integral is ﬁnite if and only if
KY Y is positive deﬁnite, or equivalently, that it is the inverse of a legal covariance matrix. In
this case, the result of the integration operation is a canonical form C
 X; K′, h′, g′
given by:
K′
=
KXX −KXY K−1
Y Y KY X
h′
=
hX −KXY K−1
Y Y hY
g′
=
g + 1
2

log |2πK−1
Y Y | + hT
Y K−1
Y Y hY

.
(14.5)
Finally, it is possible to reduce a canonical form to a context representing evidence. Assume
canonical form
reduction
that the canonical form C (X, Y ; K, h, g) is given by equation (14.4). Then setting Y = y
results in the canonical form C
 X; K′, h′, g′
given by:
K′
=
KXX
h′
=
hX −KXY y
g′
=
g + hT
Y y −1
2yT KY Y y.
(14.6)
See exercise 14.3.
Importantly, all of the factor operations can be done in time that is polynomial in the scope
of the factor. In particular, the product or division of factors requires quadratic time; factor
marginalization, which requires matrix inversion, can be done naively in cubic time, and more
eﬃciently using advanced methods.
14.2.2
Sum-Product Algorithms
The operations described earlier are the basic building blocks for all of our sum-product exact
sum-product
inference algorithms: variable elimination and both types of clique tree algorithms. Thus, we
can adapt these algorithms to apply to linear Gaussian networks, using canonical forms as our
representation of factors. For example, in the Sum-Product-VE algorithm of algorithm 9.1, we
simply implement the factor product operation as in equation (14.2), and replace the summa-
tion operation in Sum-Product-Eliminate-Var with an integration operation, implemented as in
equation (14.5).
Care must be taken regarding the treatment of evidence. In discrete factors, when instantiating
a variable Z = z, we could leave the variable Z in the factors involving it, simply zeroing
the entries that are not consistent with Z = z.
In the case of continuous variables, our
representation of factors does not allow that option: when we instantiate Z = z, the variable
Z is no longer part of the canonical form.
Thus, it is necessary to reduce all the factors
participating in the inference process to a scope that no longer contains Z. This reduction
step is already part of the variable elimination algorithm of algorithm 9.2. It is straightforward
to ensure that the clique tree algorithms of chapter 10 similarly reduce all clique and sepset
potentials with the evidence prior to any message passing steps.
A more important problem that we must consider is that the marginalization operation may
not be well deﬁned for an arbitrary canonical form. In order to show the correctness of an
inference algorithm, we must show that it executes a marginalization step only on canonical
forms for which this operation is well deﬁned. We prove this result in the context of the sum-
well-deﬁned
marginalization

612
Chapter 14. Inference in Hybrid Networks
product clique tree algorithm; the proof for the other cases follows in a straightforward way, due
to the equivalence between the upward pass of the diﬀerent message passing algorithms.
Proposition 14.1
Whenever SP-Message is called, within the CTree-SP-Upward algorithm (algorithm 10.1) the marginal-
ization operation is well-deﬁned.
Proof Consider a call SP-Message(i, j), and let ψ(Ci) be the factor constructed in the clique
prior to sending the message. Let C (Ci; K, h, g) be the canonical form associated with ψ(Ci).
Let βi(Ci) = C
 Ci; K′, h′, g′
be the ﬁnal clique potential that would be obtained at Ci in
the case where Ci is the root of the clique tree computation. The only diﬀerence between these
two potentials is that the latter also incorporates the message δj→i from Cj.
Let Y = Cj −Si,j be the variables that are marginalized when the message is computed.
By the running intersection property, none of the variables Y appear in the scope of the sepset
Si,j. Thus, the message δj→i does not mention any of the variables Y . We can verify, by
examining equation (14.2), that multiplying a canonical form by a factor that does not mention
Y does not change the entries in the matrix K that are associated with the variables in Y . It
follows that KY Y = K′
Y Y , that is, the submatrices for Y in K and K′ are the same. Because
the ﬁnal clique potential βi(Ci) is its (unnormalized) marginal posterior, it is a normalizable
Gaussian distribution, and hence the matrix K′ is positive deﬁnite. As a consequence, the
submatrix K′
Y Y is also positive deﬁnite. It follows that KY Y is positive deﬁnite, and therefore
the marginalization operation is well deﬁned.

It follows that we can adapt any of our exact inference algorithms to the case of linear
Gaussian networks. The algorithms are essentially unchanged; only the representation of
factors and the implementation of the basic factor operations are diﬀerent. In particular,
since all factor operations can be done in polynomial time, inference in linear Gaussian
networks is linear in the number of cliques, and at most cubic in the size of the largest
clique. By comparison, recall that the representation of table factors is, by itself, exponential in
the scope, leading to the exponential complexity of inference in discrete networks.
It is interesting to compare the clique tree inference algorithm to the naive approach of simply
generating the joint Gaussian distribution and marginalizing it. The exact inference algorithm
requires multiple steps, each of which involves matrix product and inversion. By comparison,
the joint distribution can be computed, as discussed in theorem 7.3, by a set of vector-matrix
products, and the marginalization of a joint Gaussian over any subset of variables is trivial (as
in lemma 7.1). Thus, in cases where the Gaussian has suﬃciently low dimension, it may be
less computationally intensive to use the naive approach for inference. Conversely, in cases
where the distribution has high dimension and the network has reasonably low tree-width, the
message passing algorithms can oﬀer considerable savings.
14.2.3
Gaussian Belief Propagation
The Gaussian belief propagation algorithm utilizes the information form, or canonical form, of
Gaussian belief
propagation
the Gaussian distribution. As we discussed, a Gaussian network is encoded using a set of local
quadratic potentials, as in equation (14.1). Reducing a canonical-form factor on evidence also
results in a canonical-form factor (as in equation (14.6)), and so we can focus attention on a

14.2. Variable Elimination in Gaussian Networks
613
representation that consists of a product of canonical-form factors. This product results in an
overall quadratic form:
p(X1, . . . , Xn) ∝exp

−1
2XT JX + hT X

.
The measure p is normalizable and deﬁnes a legal Gaussian distribution if and only if J is
positive deﬁnite.
Note that we can obtain J by adding together the individual matrices K
deﬁned by the various potentials parameterizing the network.
In order to apply the belief propagation algorithm, we must deﬁne a cluster graph and assign
the components of this parameterization to the diﬀerent clusters in the graph. As in any belief
propagation algorithm, we need the cluster graph to respect the family preservation property. In
our setting, the only terms in the quadratic form involve single variables — the hiXi terms —
and pairs of variables Xi, Xj for which Jij ̸= 0. Thus, the minimal cluster graph that satisﬁes
the family preservation requirement would contain a cluster for each edge Xi, Xj (pairs for
which Jij ̸= 0). We choose to use a Bethe-structured cluster graph that has a cluster for each
variable Xi and a cluster for each edge Xi, Xj. While it is certainly possible to deﬁne a belief
propagation algorithm on a cluster graph with larger cliques, the standard application runs belief
propagation directly on this pairwise network.
We note that the parameterization of the cluster graph is not uniquely deﬁned. In particular,
a term of the form JiiX2
i can be partitioned in inﬁnitely many ways among the node’s own
cluster and among the edges that contain Xi. Each of these partitions deﬁnes a diﬀerent set of
potentials in the cluster graph, and hence will induce a diﬀerent execution of belief propagation.
We describe the algorithm in terms of the simplest partition, where each diagonal term Jii is
assigned to the corresponding Xi cluster, and the oﬀ-diagonal terms Jij are assigned to the
Xi, Xj cluster.
With this decision, the belief propagation algorithm for Gaussian networks is simply derived
from the standard message passing operations, implemented with the canonical-form operations
for the factor product and marginalization steps. For concreteness, we now provide the precise
message passing steps. The message from Xi to Xj has the form
δi→j(xj) = exp

−1
2Ji→jx2
j + hi→jxj

.
(14.7)
We compute the coeﬃcients in this expression via a two-stage process. The ﬁrst step corresponds
to the message sent from the Xi cluster to the Xi, Xj edge; in this step, Xi aggregates all of the
information from its own local potential and the messages sent from its other incident edges:
ˆJi\j
=
Jii + P
k∈Nbi−{j} Jk→i
ˆhi\j
=
hi + P
k∈Nbi−{j} hk→i.
(14.8)
In the second step, the Xi, Xj edge takes the message received from Xi and sends the ap-
propriate message to Xj. The form of the message can be computed (with some algebraic
manipulation) from the formulas for the conditional mean and conditional variance, that we
used in theorem 7.4, giving rise to the following update equations:
Ji→j
=
−Jji ˆJ−1
i\jJji
hi→j
=
−Jji ˆJ−1
i\j ˆhi\j.
(14.9)

614
Chapter 14. Inference in Hybrid Networks
These messages can be scheduled in various ways, either synchronously or asynchronously (see
box 11.B).
If and when the message passing process has converged, we can compute the Xi-entries of
the information form by combining the messages in the usual way:
ˆJi
=
Jii +
X
k∈Nbi
Jk→i
ˆhi
=
hi +
X
k∈Nbi
hk→i.
From this information-form representation, Xi’s approximate mean ˆµi and covariance ˆσ2
i can
be reconstructed as usual:
ˆµi
=
( ˆJi)−1ˆhi
ˆσ2
i
=
( ˆJi)−1
One can now show the following result, whose proof we omit:
Theorem 14.1
Let ˆµi, ˆσ2
i be a set of ﬁxed points of the message passing process deﬁned in equation (14.8), equa-
tion (14.9). Then ˆµi is the correct posterior mean for the variable Xi in the distribution p.

Thus, if the BP message passing process converges, the resulting beliefs encode the correct
mean of the joint distribution. The estimated variances ˆσ2
i are generally not correct;
rather, they are an underestimate of the true variances, so that the resulting posteriors
are “overconﬁdent.”
This correctness result is predicated on convergence. In general, this message passing process
may or may not converge. Moreover, their convergence may depend on the order in which
messages are sent. However, unlike the discrete case, one can provide a very detailed char-
acterization of the convergence properties of this process, as well as suﬃcient conditions for
convergence (see section 14.7 for some references). In particular, one can show that the pairwise
normalizability condition, as in deﬁnition 7.3, suﬃces to guarantee the convergence of the belief
propagation algorithm for any order of messages. Recall that this condition guarantees that
each edge can be associated with a potential that is a normalized Gaussian distribution. As a
consequence, when the Gaussian parameterizing the edge is multiplied with Gaussians encoding
the incoming message, the result is also a well-normalized Gaussian.
We note that pairwise normalizability is suﬃcient, but not necessary, for the convergence of
belief propagation.
Example 14.2
Consider the Gaussian MRF shown in ﬁgure 14.1. This model deﬁnes a frustrated loop, since three of
the edges in the loop are driving X1, X2 toward a positive correlation, but the edge between them
is driving in the opposite direction. The larger the value of r, the worse the frustration. This model
is diagonally dominant for any value of r < 1/3. It is pairwise normalizable for any r < 0.39030;
however, it deﬁnes a valid Gaussian distribution for values of r up to 0.5.
In practice, the Gaussian belief propagation algorithm often converges and provides an excel-
lent alternative for reasoning in Gaussian distributions that are too large for exact techniques.

14.3. Hybrid Networks
615
X1
X3
X4
X2
r
–r
r
r
r
Figure 14.1
A Gaussian MRF used to illustrate convergence properties of Gaussian belief propaga-
tion. In this model, Jii = 1, and Jij = r for all edges (i, j), except for J12 = −r.
X1
. . .
D1
X2
D2
Xn
Dn
Figure 14.2
Simple CLG network used to demonstrate hardness of inference.
D1, . . . , Dn are
discrete, and X1, . . . , Xn are continuous.
14.3
Hybrid Networks
So far, we have dealt with models that involve only continuous variables. We now begin our
discussion of hybrid networks — those that include both continuous and discrete variables. We
focus the bulk of our discussion on conditional linear Gaussian (CLG) networks (deﬁnition 5.16),
where there are no discrete variables with continuous parents, and where all the local probability
models of continuous variables are conditional linear Gaussian CPDs. In the next section, we
discuss inference for non-Gaussian dependencies, which will allow us to deal with non-CLG
dependencies.
Even for this restricted class of networks, we can show that inference is very challenging.
Indeed, we can show that inference in this class of networks is NP-hard, even when the network
structure is a polytree. We then show how the expectation propagation approach described in
the previous section can be applied in this setting. Somewhat surprisingly, we show that this
approach provides “exact” results in certain cases, albeit mostly ones of theoretical interest.
14.3.1
The Diﬃculties
As we discussed earlier, at an abstract level, variable elimination algorithms are all the same:
They perform operations over factors to produce new factors. In the case of discrete models,
factors can be represented as tables, and these operations can be performed eﬀectively as table
operations. In the case of Gaussian networks, the factors can be represented as canonical forms.
As we now show, in the hybrid case, the representation of the intermediate factors can grow
arbitrarily complex.
Consider the simple network shown in ﬁgure 14.2, where we assume that each Di is a discrete
binary variable, X1 is a conditional Gaussian, and each Xi for i > 1 is a conditional linear

616
Chapter 14. Inference in Hybrid Networks
P(x, y)
x
y
Figure 14.3
Joint marginal distribution p(X1, X2) for a network as in ﬁgure 14.2
Gaussian (CLG) (see deﬁnition 5.15):
p(Xi | Xi−1, Di) = N
 Xi | αi,dixi−1 + βi,di; σ2
i,di

,
where for simplicity we take α1,d1 = 0, so the same formula applies for all i.
Assume that our goal is to compute P(Xn). To do so, we marginalize the joint distribution:
p(Xn) =
X
D1,...,Dn
Z
p(D1, . . . , Dn, X1, . . . , Xn)dX1 . . . dXn−1.
Using the chain rule, the joint distribution is deﬁned as:
p(D1, . . . , Dn, X1, . . . , Xn) =
n
Y
i=1
P(Di)p(X1 | D1)
n
Y
i=2
p(Xi | Di, Xi−1).
We can reorder the sums and integrals and push each of them in over factors that do not involve
the variable to be marginalized. Thus, for example, we have that:
p(X2) =
X
D2
P(D2)
Z
p(X2 | X1, D2)
X
D1
p(X1 | D1)P(D1)dX1.
Using the same variable elimination approach that we used in the discrete case, we ﬁrst generate
a factor over X1 by multiplying P(D1)p(X1 | D1) and summing out D1. This factor is then
multiplied with p(X2 | X1, D2) to generate a factor over X1, X2, D2. We can now eliminate
X1 by integrating the function that corresponds to this factor, to generate a factor over X2, D2.
We can now sum out D2 to get p(X2). The process of computing p(Xn) is analogous.
Now, consider the marginal distribution p(Xi) for i = 1, . . . , n. For i = 1, this distribution is
a mixture of two Gaussians, one corresponding to the value D1 = d1
1 and the other to D1 = d0
1.
For i = 2, let us ﬁrst consider the distribution p(X1, X2). This distribution is a mixture of four

14.3. Hybrid Networks
617
Gaussians, for the four diﬀerent instantiations of D1, D2. For example, assume that we have:
p(X1 | d0
1)
=
N
 0; 0.72
p(X1 | d1
1)
=
N
 1.5; 0.62
p(X2 | X1, d0
2)
=
N
 −1.5X1; 0.62
p(X2 | X1, d1
2)
=
N
 1.5; 0.72
.
The joint marginal distribution p(X1, X2) is shown in ﬁgure 14.3.
Note that the mixture
contains two components where X1 and X2 are independent; these components correspond to
the instantiations where D2 = d1
2, in which we have α2,1 = 0. As shown in lemma 7.1, the
marginal distribution of a Gaussian is also a Gaussian, and the same applies to a mixture. Hence
the marginal distribution p(X2) is also a mixture of four Gaussians. We can easily extend this
argument, showing that p(Xi) is a mixture of 2i Gaussians. In general, even representing the

correct marginal distribution in a hybrid network can require space that is exponential
in the size of network.
Indeed, this type of example can be used as the basis for proving a result about the hardness of
inference in models of this type. Clearly, as CLG networks subsume standard discrete networks,
exact inference in such networks is necessarily NP-hard. More surprising, however, is the fact
that this task is NP-hard even in very simple network structures such as polytrees. In fact, the
problem of computing the probability of a single discrete variable, or even approximating this
probability with any absolute error strictly less than 1/2, is NP-hard.
To deﬁne the problem precisely, assume we are working with ﬁnite precision continuous
variables. We deﬁne the following decision problem CLG-DP:
Input: A CLG Bayesian network B over ∆∪Γ, evidence E = e, and a discrete variable A ∈∆.
Output: “Yes” if PB(A = a1 | E = e) > 0.5.
Theorem 14.2
The problem CLG-DP is NP-hard even if B is a polytree.
The fact that exact inference in polytree CLGs is NP-hard may not be very surprising by
itself. After all, the distribution of a continuous variable in a CLG distribution, even in a simple
polytree, can be a mixture of exponentially many Gaussians. Therefore, it might be expected
that tasks that require that we reason directly with such a distribution are hard. Somewhat more
surprisingly, this phenomenon arises even in networks where the prior distribution of every
continuous variable is a mixture of at most two Gaussians.
Theorem 14.3
The problem CLG-DP is NP-hard even if B is a polytree where all of the discrete variables are
binary-valued, and where every continuous variable has at most one discrete ancestor.
Intuitively, this proof relies on the use of activated v-structures to introduce, in the posterior,
dependencies where a continuous variable can have exponentially many modes.
Overall, these results show that even the easiest approximate inference task — inference over
a binary-valued variable that achieves absolute error less than 0.5 — is intractable in CLG
networks. This fact implies that one should not expect to ﬁnd a polynomial-time approximate
inference algorithm with a useful error bound without further restrictions on the structure or
the parameters of the CLGs.

618
Chapter 14. Inference in Hybrid Networks
14.3.2
Factor Operations for Hybrid Gaussian Networks
Despite the discouraging results in the previous section, one can try to produce useful algorithms
for hybrid networks in order to construct an approximate inference algorithm that has good
performance, at least in practice.
We now present the basic factor operations required for
message passing or variable elimination in hybrid networks. In subsequent sections, we describe
two algorithms that use these operations for inference.
14.3.2.1
Canonical Tables
As we discussed, the key decision in adapting an exact inference algorithm to a class of hybrid
networks is the representation of the factors involved in the process.
In section 14.2, when
doing inference for linear Gaussian networks, we used canonical forms to represent factors. This
representation is rich enough to capture both Gaussians and linear Gaussian CPDs, as well as
all of the intermediate expressions that arise during the course of inference. In the case of CLG
networks, we must contend with discrete variables as well as continuous ones. In particular, a
CLG CPD has a linear Gaussian model for each instantiation of the discrete parents.
Extending on the canonical form, we can represent this CPD as a table, with one entry for
each instantiation of the discrete variables, each associated with a canonical form over the
continuous ones:
Deﬁnition 14.2
A canonical table φ over D, X for D ⊆∆and X ⊆Γ is a table with an entry for each
canonical table
d ∈Val(D), where each entry contains a canonical form C (X; Kd, hd, gd) over X. We use
φ(d) to denote the canonical form over X associated with the instantiation d.
The canonical table representation subsumes both the canonical form and the table factors used
in the context of discrete networks. For the former, D = ∅, so we have only a single canonical
form over X. For the latter, X = ∅, so that the parameters Kd and hd are vacuous, and we
remain with a canonical form exp(gd) for each entry φ(d). Clearly, a standard table factor
φ(D) can be reformulated in this way by simply taking gd = ln(φ(d)). Therefore, we can
represent any of the original CPDs or Gaussian potentials in a CLG network as a canonical table.
Now, consider the operations on factors used by the various exact inference algorithms. Let us
ﬁrst consider the operations of factor product and factor division. As for the table representation
of discrete factors, these operations are performed between corresponding table entries, in the
usual way. The product or division operations for the individual entries are performed over the
associated canonical forms, as speciﬁed in equation (14.2) and equation (14.3).
Example 14.3
Assume we have two factors φ1(A, B, X, Y ) and φ2(B, C, Y, Z), which we want to multiply in
order to produce τ(A, B, C, X, Y, Z). The resulting factor τ has an entry for each instantiation of
the discrete variables A, B, C. The entry for a particular instantiation a, b, c is a canonical form,
which is derived as the product of the two canonical forms: the one associated with a, b in φ1
and the one associated with b, c in φ2. The product operation for two canonical forms of diﬀerent
scopes is illustrated in example 14.1.
Similarly, reducing a canonical table with evidence is straightforward. Let {d, x} be a set of
observations (where d is discrete and x is continuous). We instantiate d in a canonical table by

14.3. Hybrid Networks
619
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
–4
–2
0
(a)
(b)
2
4
6
8
–4
–2
0
2
4
6
8
Figure 14.4
Summing and collapsing a Gaussian mixture. (a) Two Gaussian measures and the measure
resulting from summing them. (b) The measure resulting from collapsing the same two measures into a
single Gaussian.
setting the entries which are not consistent with d to zero. We instantiate x by instantiating
every canonical form with this evidence, as in equation (14.6).
Finally, consider the marginalization operation. Here, we have two very diﬀerent cases: inte-
grating out a continuous variable and summing out a discrete one. The operation of continuous
marginalization (integration of of a continuous variable) is straightforward: we simply apply the
operation of equation (14.5) to each of the canonical forms in our table. More precisely, assume
that our canonical table consists of a set of canonical forms C (X, Y ; Kd, hd, gd), indexed by
d. Now, for each d separately, we can integrate out Y in the appropriate canonical form, as in
equation (14.5). This results in a new canonical table C
 X; K′
d, h′
d, g′
d

, indexed by d. Clearly,
this has the desired eﬀect: Before the operation, we have a mixture, where each mixture is a
function over a set of variables X, Y ; after the operation, we have a mixture with the same set
of components, but now each component only represents the function over the variables X.
The only important restriction, as we noted in the derivation of equation (14.5), that each of the
matrices Kd,Y Y be positive deﬁnite, so that the integral is well deﬁned.
14.3.2.2
Weak Marginalization
The task of discrete marginalization, however, is signiﬁcantly more complex. To understand the
diﬃculty, consider the following example:
Example 14.4
Assume that we have a canonical form φ(A, X), for a binary-valued variable A and a continuous
variable X. Furthermore, assume that the two canonical forms in the table (associated with a0
and a1) are both weighted Gaussians:
φ(a0)
=
0.4 × N (X | 0; 1)
φ(a1)
=
0.6 × N (X | 3; 4) .

620
Chapter 14. Inference in Hybrid Networks
Figure 14.4a shows the two canonical forms, as well as the marginal distribution over X. Clearly,
this distribution is not a Gaussian; in fact, it cannot be represented at all as a canonical form.
We see that the family of canonical tables is not closed under discrete marginalization: this
operation takes a canonical table and produces something that is not representable in this form.
We now have two alternatives. The ﬁrst is to enrich the family that we use for our represen-
tation of factors. Speciﬁcally, we would have to use a table where, for each instantiations of the
discrete variables, we have a mixture of canonical forms. In this case, the discrete marginaliza-
tion operation is trivial: In example 14.4, we would have a single entry that contains the marginal
distribution shown in ﬁgure 14.4a. While this family is closed under discrete marginalization, we
have only resurrected our original problem: As discrete variables are “eliminated,” they simply
induce more components in the mixture of canonical forms; the end result of this process is
simply the original exponentially large mixture that we were trying to avoid.
The second alternative is to approximate the result of the discrete marginalization operation.
In our example, when marginalizing D, we can approximate the resulting mixture of Gaussians
by collapsing it into a single Gaussian, as shown in ﬁgure 14.4b. An appropriate approximation
mixture
collapsing
to use in this setting is the M-projection operation introduced in deﬁnition 8.4. Here, we select
M-projection
the Gaussian distribution ˆp that minimizes ID(p||ˆp).
In example 8.15 we provided a precise
characterization of this operation:
Proposition 14.2
Let p be an arbitrary distribution over X1, . . . , Xk. Let µ be the mean vector of p, and Σ be the
matrix of covariances in p:
µi
=
IEp[Xi]
Σi,j
=
CCovp[Xi; Xj].
Then the Gaussian distribution ˆp = N (µ; Σ) is the one that minimizes ID(p||ˆp) among all Gaus-
sian distributions.
Using this result, we have:
Proposition 14.3
Let p be the density function of a mixture of k Gaussians {⟨wi, N (µi; Σi)⟩}k
i=1 for Pk
i=1 wi = 1.
Let q = N (µ; Σ) be a Gaussian distribution deﬁned as:
µ
=
k
X
i=1
wiµi
(14.10)
Σ
=
k
X
i=1
wiΣi +
k
X
i=1
wi(µi −µ)(µi −µ)T .
(14.11)
Then q has the same ﬁrst two moments (means and covariances) as p, and is therefore the Gaussian
distribution that minimizes ID(p||q) among all Gaussian distributions.
The proof is left as an exercise (exercise 14.2).
Note that the covariance matrix, as deﬁned by the collapsing operation, has two terms: one
term is the weighted average of the covariance matrices of the mixture components; the second
corresponds to the distances between the means of the mixture components — the larger these

14.3. Hybrid Networks
621
distances, the larger the “space” between the mixture components, and thus the larger the
variances in the new covariance matrix.
Example 14.5
Consider again the discrete marginalization problem in example 14.4. Using proposition 14.3, we
have that the mean and variance of the optimal Gaussian approximation to the mixture are:
µ
=
0.4 · 0 + 0.6 · 3 = 1.8
σ2
=
(0.4 · 1 + 0.6 · 4) + (0.4 · (1.8 −0)2 + 0.6 · (3 −1.8)2 = 4.96.
The resulting Gaussian approximation is shown in ﬁgure 14.4b.
Clearly, when approximating a mixture of Gaussians by one Gaussian, the quality of the
approximation depends on how close the mixture density is to a single multivariate Gaussian.
When the Gaussians are very diﬀerent, the approximation can be quite bad.
Using these tools, we can deﬁne the discrete marginalization operation.
Deﬁnition 14.3
Assume we have a canonical table deﬁned over {A, B, X}, where A, B ⊆∆and X ⊆Γ. Its
weak marginal is a canonical table over A, X, deﬁned as follows: For every value a ∈Val(A),
weak
marginalization
we select the table entries consistent with a and sum them together to obtain a single table entry.
The summation operation uses the collapsing operation of proposition 14.3.
One problem with this deﬁnition is that the collapsing operation was deﬁned in proposition 14.3
only for a mixture of Gaussians and not for a mixture of general canonical forms. Indeed, the
operation of combining canonical forms is well deﬁned if and only if the canonical forms have
ﬁnite ﬁrst two moments, which is the case only if they can be represented as Gaussians. This
restriction places a constraint on our inference algorithm: we can marginalize a discrete variable
only when the associated canonical forms represent Gaussians. We will return to this point.
14.3.3
EP for CLG Networks
The previous section described the basic data structure that we can use to encode factors
in a hybrid Gaussian network, and the basic factor operations needed to manipulate them.
Most important was the deﬁnition of the weak marginalization operation, which approximates a
mixture of Gaussians as a single Gaussian, using the concept of M-projection.
With our deﬁnition of weak marginalization and other operations on canonical tables, we can
now deﬁne a message passing algorithm based on the framework of the expectation propagation
Gaussian EP
described in section 11.4. As a reminder, to perform a message passing step in the EP algorithm,
a cluster multiplies all incoming messages, and then performs an approximate marginalization
on the resulting product factor. This last step, which can be viewed abstractly as a two-step
process — exact marginalization followed by M-projection — is generally performed in a single
approximate marginalization step.
For example, in section 11.4.2, the approximate marginals
were computed by calibrating a cluster graph (or clique tree) and then extracting from it a set
of required marginals.
To apply EP in our setting, we need only to deﬁne the implementation of the M-projection
operation M-Project-Distr, as needed in line 1 of algorithm 11.5. This operation can be performed
using the weak marginalization operation described in section 14.3.2.2, as shown in detail in
algorithm 14.1. The marginalization step uses two types of operation. The continuous variables

622
Chapter 14. Inference in Hybrid Networks
Algorithm 14.1 Expectation propagation message passing for CLG networks
Procedure CLG-M-Project-Distr (
Z,
// Scope to remain following projection
⃗φ
// Set of canonical tables
)
1
// Compute overall measure using product of canonical tables
2
˜β ←Q
φ∈⃗φ φ
3
// Variables to be preserved
4
A ←Z ∩∆
5
X ←Z ∩Γ
6
// Variables to be eliminated
7
B ←(Scope[⃗φ] −A) ∩∆
8
Y ←(Scope[⃗φ] −X) ∩Γ
9
for each a, b ∈Val(A, B)
10
τ(a, b) ←
R
βi(a, b)dY using equation (14.5)
11
˜σ ←P
B τ using deﬁnition 14.3
12
return ˜σ
are eliminated using the marginalization operation of equation (14.5) over each entry in the
canonical table separately. The discrete variables are then summed out. For this last step, there
are two cases. If the factor τ contains only discrete variables, then we use standard discrete
marginalization. If not, then we use the weak marginalization operation of deﬁnition 14.3.
In principle, this application of the EP framework is fairly straightforward. There are, however,
two important subtleties that arise in this setting.
14.3.3.1
Ordering Constraints
First, as we discussed, for weak marginalization to be well deﬁned, the canonical form being
marginalized needs to be a Gaussian distribution, and not merely a canonical form. In some
cases, this requirement is satisﬁed simply because of the form of the potentials in the original
network factorization. For example, in the Gaussian case, recall that our message passing process
is well deﬁned if our distribution is pairwise normalizable. In the conditional Gaussian case, we
can guarantee normalizability if for each cluster over scope X, the initial factor in that cluster
is a canonical table such that each canonical form entry C (X; Kd, hd, gd) is normalizable.
Because normalizability is closed under factor product (because the sum of two PSD matrices
is also PSD) and (both weak and strong) marginalization, this requirement guarantees us that all
factors produced by a sum-product algorithm will be normalizable.
However, this requirement is not always easy to satisfy:
Example 14.6
Consider a CLG network structured as in ﬁgure 14.5a, and the clique tree shown in ﬁgure 14.5b.
Note that the only clique where each canonical form is normalizable is C1 = {A, X}; in C2 =
{B, X, Y }, the canonical forms in the canonical table are all linear Gaussians whose integral is
inﬁnite, and hence cannot be collapsed in the operation of proposition 14.3.

14.3. Hybrid Networks
623
X
A
Y
B
(a)
(b)
B,X,Y
A,X
X
Figure 14.5
Example of unnormalizable potentials in a CLG clique tree. (a) A simple CLG (b) A clique
tree for it.
In this network, with an appropriate message passing order, one can guarantee that canonical
tables are normalizable at the appropriate time point. In particular, we can ﬁrst pass a message
from C1 to C2; this message is a Gaussian obtained by weak marginalization of p(A, X) onto X.
The resulting potential at C2 is now a product of a legal Gaussian density over X (derived from
the incoming message) multiplied by P(B) and the conditional linear Gaussian p(Y | X, B). The
resulting distribution is a standard mixture of Gaussians, where each component in the mixture is
normalizable. Thus, weak marginalization onto Y can be performed, allowing the message passing
process to continue.
This example illustrates that we can sometimes ﬁnd a legal message passing order even in cases
where the initial potentials are not normalizable. However, such a message passing order may
not always exist.
Example 14.7
Consider the network in ﬁgure 14.6a. After moralization, the graph, shown in ﬁgure 14.6b, is already
triangulated. If we now extract the maximum cliques and build the clique tree, we get the tree
shown in ﬁgure 14.6c. Unfortunately, at this point, neither of the two leaves in this clique tree can
send a message. For example, the clique {B, X, Y } contains the CPDs for P(B) and P(Y | B, X),
but not the CPD for X. Hence, the canonical forms over {X, Y } represent linear Gaussian CPDs
and not Gaussians. It follows that we cannot marginalize out B, and thus this clique cannot send
a message. For similar reasons, the clique {A, C, Y, Z} cannot send a message. Note, however, that
a diﬀerent clique tree can admit message passing. In particular, both the trees in (d) and (e) admit
message passing using weak marginalization.
As we can see, not every cluster graph allows message passing based on weak marginalization
to take place. More formally, we say that a variable Xi is order constrained at cluster Ck if
order-constrained
message passing
we require a normalizable probability distribution over Xi at Ck in order to send messages.
In a cluster graph for a CLG Bayesian network, if Ck requires weak marginalization, then any
continuous variable Xi in Ck is order constrained.
If Xi is order constrained in Ck, then we need to ensure that Xi has a well-deﬁned distri-
bution. In order for that to hold, Ck must have obtained a valid probability distribution over
Xi’s parents, whether from a factor within Ck or from a message sent by another cluster. Now,
consider a continuous parent Xj of Xi, and let Cl be the cluster from which Ck obtains the

624
Chapter 14. Inference in Hybrid Networks
X
Z
A
Y
B
C
X
Z
A
Y
B
C
(a)
(c)
(d)
(e)
(b)
B,X,Y
A,C,Y,Z A,C,Y
A,C,Y,Z
A,X,Y
A,Y
X,Y
A,B,C,Y A,B,Y
A,Y
A,B,X,Y
A,C,Y,Z
A,B,X,Y
Figure 14.6
A simple CLG and possible clique trees with diﬀerent correctness properties. (a) A
simple CLG. (b) The moralized (and triangulated) graph for the CLG. (c) Clique tree derived the moralized
graph in (b). (d) A clique tree with a strong root. (e) A clique tree without a strong root that allows message
passing using weak marginalization.
distribution over Xj. (In the ﬁrst case, we have k = l.) In order for Xj to have a well-deﬁned
distribution in Cl, we must have that Xj is order constrained in Cl. This process continues
until the roots of the networks are reached.
In the context of Markov networks, the situation is somewhat less complex. The use of a global
partition function allows us to specify models where individual factors are all normalizable,
ensuring a legal measure.
In particular, extending deﬁnition 7.3, we can require that all of
the entries in all of the canonical tables parameterizing the network be normalizable. In this
Gaussian
normalizability
case, the factors in all clusters in the network can be normalized to produce valid distributions,
avoiding any constraints on message passing. Of course, the factor normalizability constraint is
only a suﬃcient condition, in that there are models that do not satisfy this constraint and yet
allow weak marginalization to take place, if messages are carefully ordered.
As this discussion shows, the constraint on normalizability of the Gaussian in the context

of weak marginalization can impose signiﬁcant constraints on the structure of the cluster
graph and/or the order of the message passing.
14.3.3.2
Ill-Deﬁned Messages
The second subtlety arises when we apply the belief-update message passing algorithm rather
belief-update
than sum product. Recall that the belief update algorithm has important advantages, in that it
gradually tunes the approximation to the more relevant regions in the probability space.
Example 14.8
Consider the application of the sum-product belief propagation algorithm to the network of ﬁg-
ure 14.5. Assume furthermore that the distribution at C1 is as described in example 14.5, so that

14.3. Hybrid Networks
625
the result of weak marginalization is as shown in ﬁgure 14.4b. Now, assume that p(Y | x, b1) =
N (Y | x; 1), and that we observe b = b1 and Y = −2. The evidence Y = −2 is much more
consistent with the left-hand (mean 0) component in the mixture, which is the one derived from
p(X | a0). Given this evidence, the exact posterior over X would give much higher probability
to the a0 component in the mixture. However, our Gaussian approximation was the M-projection
of a mixture where this component had its prior weight of 0.4. To obtain a better approximation,
we would want to construct a new M-projection in C1 that gives more weight to the N (X | 0; 1)
component in the mixture when collapsing the two Gaussians.
This issue is precisely the motivation that we used in section 11.4.3.2 for the belief update algo-
rithm. However, the use of division in the Gaussian EP algorithm can create unexpected

complications, in that the messages passed can represent unnormalizable densities, with
negative variance terms; these can lead, in turn, to unnormalizable densities in the
clusters, and thereby to nonsensical results.
Example 14.9
Consider again the CLG in ﬁgure 14.5a, but where we now assume that the CPDs of the discrete
variables are uniform and the CPDs of the continuous nodes are deﬁned as follows:
p(X | A)
=
 N (0; 2)
A = a0
N (0; 6)
A = a1
p(Y | B, X)
=
 N (X; 0.01)
B = b0
N (−X; 0.01)
B = b1.
Consider the execution of message passing on the clique tree of ﬁgure 14.5b, with the evidence
Y = 4. To make the presentation easier to follow, we present some of the intermediate results in
moments form (means and covariances) rather than canonical form.
The message passing algorithm ﬁrst sends a message from the clique {A, X} to the clique
{B, X, Y }. Since the cliques share only the variable X, we collapse the prior distribution of X
using proposition 14.3 and get the message δ1→2 = N (X | 0; 4). This message is stored in the
sepset at µ1,2. We also multiply the potential of {B, X, Y } by this message, getting a mixture of
two Gaussians with equal weights:
β2(b0)
=
N

X, Y |

0
0

;

4
4
4
4.01

β2(b1)
=
N

X, Y |
 0
0

;

4
−4
−4
4.01

.
After instantiating the evidence Y = 4 we get a new mixture of two Gaussians:
β2(b0)
=
N (X | 3.99; 0.00998)
β2(b1)
=
N (X | −3.99; 0.00998) .
Note that, in the original mixture, Y has the same marginal — N (Y | 0; 4.01) — for both b0
and b1. Therefore, the evidence Y = 4 has the same likelihood in both cases, so that the posterior
weights of the two cases are still the same as each other.
We now need to send a message back to the clique {A, X}. To do so, we collapse the two
Gaussians, resulting in a message δ2→1 = N (X | 0; 15.93).
Note that, in this example, the
evidence causes the variance of X to increase.

626
Chapter 14. Inference in Hybrid Networks
To incorporate the message δ2→1 into the clique {A, X}, we divide it by the message µ1,2, and
multiply each entry in β1(A, X) by the resulting quotient δ2→1
µ1,2 . In particular, for A = a1, we
perform the operation:
N (0; 6) · N (0; 15.93)
N (0; 4)
.
This operation can be carried out using the canonical form C (K, h, g). Consider the operation
over the coeﬃcient K, which represents the inverse of the covariance matrix: K = Σ−1. In our
case, the Gaussians are all one-dimensional, so K =
1
σ2 . As in equation (14.2) and equation (14.3),
the product and division operation reduces to addition and subtraction of the coeﬃcient K. Thus,
the K for the resulting potential is:
1
6 +
1
15.93 −1
4 = −0.0206 < 0!
However, K < 0 does not represent a legal Gaussian: it corresponds to σ2 =
1
−0.0206, which is not
a legal variance.
In practice, this type of situation does occur, but not often. Therefore, despite this complica-
tion, the belief-update variant of the EP algorithm is often used in practice.
14.3.4
An “Exact” CLG Algorithm ⋆
There is one case where we can guarantee that the belief update algorithm is not only well
deﬁned but even returns “exact” answers.
Of course, truly exact answers are not generally
possible in a CLG network. Recall that a CLG distribution is a mixture of possibly exponentially
many Gaussian hypotheses. The marginal distribution over a single variable can similarly be
an exponentially large mixture (as in ﬁgure 14.2).
Thus, for a query regarding the marginal
distribution of a single continuous variable, even representing the answer might be intractable.
Lauritzen’s algorithm provides a compromise between correctness and computational eﬃciency.
Lauritzen’s
algorithm
It is exact for queries involving discrete variables, and it provides the exact ﬁrst and second
moments — means and (co)variances — for the continuous variables. More precisely, for a
query p(D, X | e) for D ⊆∆and X ⊆Γ, Lauritzen’s algorithm returns an answer ˆp such
that ˆp(D) = p(D | e) is correct, and for each d, ˆp(X | d) has the correct ﬁrst and second
moments. For many applications, queries of this type are suﬃcient.
The algorithm is a modiﬁcation of the clique tree algorithm for discrete networks. It uses
precisely the same type of message passing that we described, but over a highly restricted clique
tree data structure. As we will show, these restrictions can (and do) cause signiﬁcant blowup
in the size of the clique tree (much larger than the induced width of the network) and hence
do not violate the NP-hardness of inference in these graphs. Indeed, these restrictions restrict
the algorithm’s usefulness to a fairly narrow class of problems, and they render it primarily of
theoretical interest.
14.3.4.1
Strongly Rooted Trees
The ordering constraint we employ on the clique tree, as described in section 14.3.3.1, guarantees
that we can calibrate the clique tree without requiring weak marginalization on the upward pass.

14.3. Hybrid Networks
627
That is, the clique tree has a particular clique that is a strong root; when we pass messages from
the leaves of the clique tree toward this root, no weak marginalization is required.
This property has several implications.
First, weak marginalization is the only operation
that requires that the clique potential be normalizable. In the upward pass (from the leaves
toward the root), no such marginalization is required, and so we need not worry about this
constraint.
Intuitively, in the downward pass, each clique has already obtained all of the
necessary information to deﬁne a probability distribution over its scope.
This distribution
allows weak marginalization to be performed in a well-deﬁned way.
Second, because weak
marginalization is the only operation that involves approximation, this requirement guarantees
that the message passing in the upward pass is exact. As we will discuss, this property suﬃces
to guarantee that the weak marginals in the downward pass are all legal. Indeed, as we will
show, this property also allows us to guarantee that these marginals all possess the correct ﬁrst
and second moments (means and covariances).
When does our clique tree structure guarantee that no weak marginalization has to take place?
Consider two cliques C1 and C2, such that C2 is the upward neighbor of C1. There are two
cases. Either no marginalization of any discrete variable takes place between C1 and C2, or it
does. In the ﬁrst case, only continuous variables are eliminated, so that C2 −C1 ⊆Γ. In the
second case, in order to avoid weak marginalization, we must avoid collapsing any Gaussians.
Thus, we must have that the message from C2 to C1 contains no continuous variables, so that
C1 ∩C2 ⊆∆. Overall, we have:
Deﬁnition 14.4
A clique Cr in a clique tree T is a strong root if for every clique C1 and its upward neighbor C2,
strong root
we have that C2 −C1 ⊆Γ or that C1 ∩C2 ⊆∆. A clique tree is called strongly rooted if it
has a strong root.
Example 14.10
ﬁgure 14.6d shows a strongly rooted clique tree for the network of ﬁgure 14.6a. Here, the strong
root is the clique {A, B, C, Y }. For both of the two nonroot cliques, we have that no discrete
variables are marginalized between them and their upward neighbor {A, B, C, Y }, so that the
ﬁrst condition of the deﬁnition holds.
If we now apply the message passing procedure of algorithm 14.1, we note that the weak
marginalization can only occur in the downward pass. In the downward pass, Ci has received
messages from all of its neighbors, and therefore βi(Ci) represents a probability distribution
over Ci. Hence, τ(A, B, X) is a mixture of Gaussians over X, so that the weak marginalization
operation is well deﬁned.
14.3.4.2
Strong Roots and Correctness
So far, we have shown that the strongly rooted requirement ensures that the operations in the
clique tree are well deﬁned, speciﬁcally, that no collapsing is performed unless the canonical
forms represent Gaussians. However, as we have already shown, this condition is not necessary
for the message passing to be well deﬁned; for example, the clique tree of ﬁgure 14.6e is not
strongly rooted, but allows weak marginalization. However, as we now show, there are other
reasons for using strongly rooted trees for inference in hybrid networks. Speciﬁcally, the presence
of a strong root ensures not only that message passing is well deﬁned, but also that message
passing leads to exact results, in the sense that we described.

628
Chapter 14. Inference in Hybrid Networks
Theorem 14.4
Let T be a clique tree and Cr be a strong root in T . After instantiating the evidence and running
CTree-BU-Calibrate using EP-Message with CLG-M-Project-Distr for message passing, the tree T is
calibrated and every potential contains the correct (weak) marginal. In particular, every clique C
contains the correct probability distribution over the discrete variables C ∩∆and the correct mean
and covariances of the continuous variables C ∩Γ.
Proof Consider the steps in the algorithm CTree-BU-Calibrate. The clique initialization step is
exact: each CPD is multiplied into a clique that contains all of its variables, and the product
operation for canonical tables is exact. Similarly, evidence instantiation is also exact. It remains
only to show that the message passing phase is exact.
The upward pass is simple — as we discussed, all of the marginalization operations involved
are strong marginalizations, and therefore all the operations are exact. Thus, the upward pass is
equivalent to running the variable elimination algorithm for the variables in the strong root, and
its correctness follows from the correctness of the variable elimination algorithm. The result of
the upward pass is the correct (strong) marginal in the strong root Cr.
The downward pass involves weak marginalization, and therefore, it will generally not result
in the correct distribution. We wish to show that the resulting distributions in the cliques are
the correct weak marginals. The proof is by induction on the distance of the clique from the
strong root Cr. The base case is the root clique itself, where we have already shown that we
have exactly the correct marginal. Now, assume now that we have two cliques Ci and Cj
such that Ci is the upward neighbor of Cj. By the inductive hypothesis, after Ci receives the
message from its upward neighbor, it has the correct weak marginals. We need to show that,
after Cj receives the message from Ci, it also has the correct weak marginal.
Let βj and β′
j denote the potential of Cj before and after Ci sends the downward message
to Cj. Let µi,j denote the sepset message before the downward message, and δi→j denote the
actual message sent. Note that δi→j is the (weak) marginal of the clique potential βi, and is
therefore the correct weak marginal, by the inductive hypothesis.
We ﬁrst show that, after the message is sent, Cj agrees with Ci on the marginal distribution
of the sepset Si,j.
X
Cj−Si,j
β′
j =
X
Cj−Si,j
βj · δi→j
µi,j
= δi→j
µi,j
·
X
Cj−Si,j
βj
= δi→j
µi,j
· µi,j = δi→j,
(14.12)
where the marginalization P
Cj−Si,j also denotes integration, when appropriate. This deriva-
tion is correct because this marginalization P
Cj−Si,j is an exact operation: By the strong root
property, all marginalizations toward the strong root (that is, from j to i) are strong marginaliza-
tions. Thus, the marginal of Cj after the message is the same as the (weak) marginal of Ci, and
the two cliques are calibrated. Because the (weak) marginal of Ci is correct, so is the marginal
of Cj. Note that this property does not hold in a tree that is not strongly rooted. In general, in
such a clique tree, µi,j ̸= P
Cj−Si,j βj.
It remains to show that β′
j is the correct weak marginal for all the variables in Cj. As shown
in exercise 10.5, the premessage potential βj already encodes the correct posterior conditional
distribution P(Cj | Si,j).
(We use P to denote the posterior, after conditioning on the

14.3. Hybrid Networks
629
evidence.) In other words, letting X = Cj −Si,j and S = Si,j, we have that:
βj(X, S)
βj(S)
= P(X | S).
Now, since the last message along the edge Ci—Cj was sent from Cj to Ci, we have that
βj(X, S)
βj(S)
= βj(X, S)
µi,j(S) .
We therefore have that the potential of Cj after the message from Ci is:
β′
j = βjδi→j
µi,j
= δi→jP(X | S).
Thus, every entry in β′
j is a Gaussian, which is derived as a product of two terms: one is a
Gaussian over S that has the same ﬁrst and second moments as P, and the second is the
correct P(X | S). It follows easily that the resulting product βj(X, S) is a Gaussian that has
the same ﬁrst and second moments as P (see exercise 14.5). This concludes the proof of the
result.
Note that, if the tree is not strongly rooted, the proof breaks down in two places: the upward
pass is not exact, and equation (14.12) does not hold. Both of these arise from the fact that
P
Cj−Si,j βj is the exact marginal, whereas, if the tree is not strongly rooted, µi,j is computed
(in some cliques) using weak marginalization. Thus, the two are not, in general, equal. As
a consequence, although the weak marginal of βj is µi,j, the second equality fails in the
derivation: the weak marginal of a product βj · δi→j
µi,j is not generally equal to the product of
δi→j
µi,j and the weak marginal of βj. Thus, the strong root property is essential for the strong
correctness properties of this algorithm.
14.3.4.3
Strong Triangulation and Complexity
We see that strongly rooted trees are necessary for the correct execution of the clique tree
algorithm in CLG networks. Thus, we next consider the task of constructing a strongly rooted
tree for a given network. As we discussed, one of our main motivations for the strong root
requirement was the ability to perform the upward pass of the clique tree algorithm without
weak marginalization.
Intuitively, the requirement that discrete variables not be eliminated
before their continuous neighbors implies a constraint on the elimination ordering within the
constrained
elimination
ordering
clique tree. Unfortunately, constrained elimination orderings can give rise to clique trees that
are much larger — exponentially larger — than the optimal, unconstrained clique tree for the
same network. We can analyze the implications of the strong triangulation constraint in terms
of the network structure.
Deﬁnition 14.5
Let G be a hybrid network. A continuous connected component in G is a set of variables X ⊆Γ
such that: if X1, X2 ∈X then there exists a path between X1 and X2 in the moralized graph
M[G] such that all the nodes on the path are in Γ. A continuous connected component is maximal
if it is not a proper subset of any larger continuous connected component. The discrete neighbors
of a continuous connected component X are all the discrete variables that are adjacent to some
node X ∈X in the moralized graph M[G].

630
Chapter 14. Inference in Hybrid Networks
For example, in the network ﬁgure 14.6a, all of the continuous variables {X, Y, Z} are in a
single continuous connected component, and all the discrete variables are its neighbors.
We can understand the implications of the strong triangulation requirement in terms of
continuous connected components:
Theorem 14.5
Let G be a hybrid network, and let T be a strongly rooted clique tree for G. Then for any maximal
continuous connected component X in G, with D its discrete neighbors, T includes a clique that
contains (at least) all of the nodes in D and some node X ∈X.
The proof is left as an exercise (exercise 14.6).
In the CLG of ﬁgure 14.6a, all of the continuous variables are in one connected component,
and all of the discrete variables are its neighbors. Thus, a strongly rooted tree must have a
clique that contains all of the discrete variables and one of the continuous ones. Indeed, the
strongly rooted clique tree of ﬁgure 14.6d contains such a clique — the clique {A, B, C, Y }.
This analysis allows us to examine a CLG network, and immediately conclude lower bounds on
the computational complexity of clique tree inference in that network. For example, the polytree
CLG in ﬁgure 14.2 has a continuous connected component containing all of the continuous
variables {X1, . . . , Xn}, which has all of the discrete variables as neighbors. Thus, any strongly
rooted clique tree for this network necessarily has an exponentially large clique, which is as we
would expect, given that this network is the basis for our NP-hardness theorem.

Because many CLG networks have large continuous connected components that are
adjacent to many discrete variables, a strongly rooted clique tree is often far too large
to be useful. However, the algorithm presented in this section is of conceptual interest,
since it clearly illustrates when the EP message passing can lead to inaccurate or even
nonsensical answers.
14.4
Nonlinear Dependencies
In the previous sections, we dealt with a very narrow class of continuous networks: those
where all of the CPDs are parameterized as linear Gaussians.
Unfortunately, this class of
networks is inadequate as a model for many practical applications, even those involving only
continuous variables. For example, as we discussed, in modeling the car’s position as a function
of its previous position and its velocity (as in example 5.20), rather than assume that the
variance in the car’s position is constant, it might be more reasonable to assume that the
variance of the car is larger if the velocity is large. This dependence is nonlinear, and it cannot
be accommodated within the framework of linear Gaussians.
In this section, we relax the
assumption of linearity and present one approach for dealing with continuous networks that
include nonlinear dependencies. We note that the techniques in this section can be combined
with the ones described in section 14.3 to allow our algorithms to extend to networks that allow
discrete variables to depend on continuous parents.
Once again, the standard solution in this setting can be viewed as an instance of the general
expectation propagation framework described in section 11.4.4, and used before in the context of
the CLG models. Since we cannot tractably represent and manipulate the exact factors in this
setting, we need to use an approximation, by which intermediate factors in the computation are
approximated using a compact parametric family. Once again, we choose the family of Gaussian

14.4. Nonlinear Dependencies
631
measures as our representation.
At a high level, the algorithm proceeds as follows. As in expectation propagation (EP), each
cluster Ci maintains its potentials in a nonexplicit form, as a factor set ⃗φi; some of these factors
are from the initial factor set Φ, and others from the incoming messages into Ci. Importantly,
due to our use of a Gaussian representation for the EP factors, the messages are all Gaussian
measures.
To pass a message from Ci to Cj, Ci approximates the product of the factors in ⃗φi as a
Gaussian distribution, a process called linearization, for reasons we will explain. The resulting
linearization
Gaussian distribution can then be marginalized onto Si,j to produce the approximate cluster
marginal ˜σi→j. Essentially, the combination of the linearization step and the marginalization of
the resulting Gaussian over the sepset give rise to the weak marginalization operation that we
described.
The basic computational step in this algorithm is the linearization operation. We provide
several options for performing this operation, and discuss their trade-oﬀs. We then describe in
greater detail how this operation can be used within the EP algorithm, and the constraints that
it imposes on its detailed implementation.
14.4.1
Linearization
We ﬁrst consider the basic computational task of approximating a distribution p(X1, . . . , Xd) as
a Gaussian distribution ˆp(X). For the purposes of this section, we assume that our distribution
is deﬁned in terms of a Gaussian distribution p0(Z1, . . . , Zl) = N (Z | µ; Σ) and a set of
deterministic functions Xi = fi(Zi). Intuitively, the auxiliary Z variables encompass all of the
stochasticity in the distribution, and the functions fi serve to convert these auxiliary variables
into the variables of interest.
For a vector of functions ⃗f = (f1, . . . , fd) and a Gaussian
distribution p0, we can now deﬁne p(X, Z) to be the distribution that has p(Z) = p0(Z) and
Xi = fi(Z) with probability 1. (Note that this distribution has a point mass at the discrete
points where X = ⃗f(Z).) We use p(X) = p0(Z) L[X = ⃗f(Z)] to refer to the marginal of
this distribution over X.
Our goal is to compute a Gaussian distribution ˆp(X) that approximates this marginal distri-
bution p(X). We call the procedure of determining ˆp from p0 and ⃗f a linearization of ⃗f. We
linearization
now describe the two main approaches that have been proposed for the linearization operation.
14.4.1.1
Taylor Series Linearization
As we know, if p0(Z) is a Gaussian distribution and X = f(Z) is a linear function, then
p(X) = p(f(Z)) is also a Gaussian distribution. Thus, one very simple and commonly used
approach is to approximate f as a linear function ˆf, and then deﬁne ˆp in terms of ˆf.
The most standard linear approximation for f(Z) is the Taylor series expansion around the
Taylor series
mean of p0(Z):
ˆf(Z) = f(µ) + ∇f|µ Z.
(14.13)
The Taylor series is used as the basis for the famous extended Kalman ﬁlter (see section 15.4.1.2).
Extended Kalman
ﬁlter
Although the Taylor series expansion provides us with the optimal linear approximation to

632
Chapter 14. Inference in Hybrid Networks
f, the Gaussian ˆp(X) = p0(Z) L ˆf(Z) may not be the optimal Gaussian approximation to
p(X) = p0(Z) L f(Z).
Example 14.11
Consider the function X = Z2, and assume that p(Z) = N (Z | 0; 1). The mean of X is simply
IEp[X] = IEp

Z2
= 1. The variance of X is
VVarp[X] = IEp

X2
−IEp[X]2 = IEp

Z4
−IEp

Z22 = 3 −12 = 2.
On the other hand, the ﬁrst-order Taylor series approximation of f at the mean value Z = 0 is:
ˆf(Z) = 02 + (2Z)U=0Z ≡0.
Thus, ˆp(X) will simply be a delta function where all the mass is located at X = 0, a very poor
approximation to p.
This example illustrates a limitation of this simple approach. In general, the quality of the

Taylor series approximation depends on how well ˆf approximates f in the neighborhood
of the mean of Z, where the size of the neighborhood is determined by the variance
of p0(Z). The approximation is good only if the linear term in the Taylor expansion
of f dominates in this neighborhood, and the higher-order terms are small. In many
practical situations, this is not the case, for example, when f changes very rapidly relative to
the variance of p0(Z). In this case, using the simple Taylor series approach can lead to a very
poor approximation.
14.4.1.2
M-Projection Using Numerical Integration
The Taylor series approach uses what may be considered an indirect approach to approximating
p: we ﬁrst simplify the nonlinear function f and only then compute the resulting distribution.
Alternatively, we can directly approximate p using a Gaussian distribution ˆp, by using the M-
M-projection
projection operation introduced in deﬁnition 8.4. Here, we select the Gaussian distribution ˆp
that minimizes ID(p||ˆp).
In proposition 14.2 we provided a precise characterization of this operation. In particular, we
showed that we can obtain the M-projection of p by evaluating the following set of integrals,
corresponding to the moments of p:
IEp[Xi]
=
Z ∞
−∞
fi(z)p0(z)dz
(14.14)
IEp[XiXj]
=
Z ∞
−∞
fi(z)fj(z)p0(z)dz.
(14.15)
From these moments, we can derive the mean and covariance matrix for p, which gives us
precisely the M-projection.
Thus, the M-projection task reduces to one of computing the
expectation of some function f (which may be fi or a product fifj) relative to our distribution
p0. Before we discuss the solution of these integrals, it is important to inject a note of caution.
Even if p is a valid density, its moments may be inﬁnite, preventing it from being approximated
by a Gaussian.
In some cases, it is possible to solve the integrals in closed form, leading to an eﬃcient
and optimal way of computing the best Gaussian approximation. For instance, in the case of

14.4. Nonlinear Dependencies
633
example 14.11, equation (14.14) reduces to computing IEp

Z2
, where p is N (0; 1), an integral that
can be easily solved in closed form. Unfortunately, for many functions f, these integrals have
no closed-form solutions. However, because our goal is simply to estimate these quantities, we
can use numerical integration methods. There are many such methods, with various trade-oﬀs.
numerical
integration
In our setting, we can exploit the fact that our task is to integrate the product of a function
and a Gaussian. Two methods that are particularly eﬀective in this setting are described in the
following subsections.
Gaussian Quadrature
Gaussian quadrature is a method that was developed for the case of
Gaussian
quadrature
one-dimensional integrals. It approximates integrals of the form
R b
a W(z)f(z)dz where W(z)
is a known nonnegative function (in our case a Gaussian). Based on the function W, we choose
m points z1, . . . , zm and m weights w1, . . . , wm and approximate the integral as:
Z b
a
W(z)f(z)dz ≈
m
X
j=1
wjf(zj).
(14.16)
The points and weights are chosen such that the integral is exact if f is a polynomial of degree
2m −1 or less. Such rules are said to have precision 2m −1.
integration rule
precision
To understand this construction, assume that we have chosen m points z1, . . . , zm and m
weights w1, . . . , wm so that equation (14.16) holds with equality for any monomial zi for i =
1, . . . , 2m −1. Now, consider any polynomial of degree at most 2m −1: f(z) = P2m−1
i=0
αizi.
For such an f, we can show (exercise 14.8) that:
Z b
a
W(z)f(z)dz =
m
X
j=1
wjzi
j.
Thus, if our points are exact for any monomial of degree up to 2m −1, it is also exact for any
polynomial of this degree.
Example 14.12
Consider the case of m = 2. In order for the rule to be exact for f0, . . . , f3, it must be the case
that for i = 0, . . . , 3 we have
Z b
a
W(z)fi(z)dz = w1fi(z1) + w2fi(z2).
Assuming that W(z) = N (0; 1), a = −∞, and b = ∞, we get the following set of four nonlinear
equations
w1 + w2
=
Z ∞
−∞
N (z | 0; 1) dz = 1
w1z1 + w2z2
=
Z ∞
−∞
N (z | 0; 1) z dz = 0
w1z2
1 + w2z2
2
=
Z ∞
−∞
N (z | 0; 1) z2 dz = 1
w1z3
1 + w2z3
2
=
Z ∞
−∞
N (z | 0; 1) z3 dz = 0

634
Chapter 14. Inference in Hybrid Networks
The solution for these equations (up to swapping z1 and z2) is w1 = w2 = 0.5, z1 = −1, z2 = 1.
This solution gives rise to the following approximation, which we apply to any function f:
Z ∞
−∞
N (0; 1) f(z)dz ≈0.5f(−1) + 0.5f(1).
This approximation is exact for any polynomial of degree 3 or less, and approximate for other func-
tions. The error in the approximation depends on the extent to which f can be well approximated
by a polynomial of degree 3.
This basic idea generalizes to larger values of m.
Now, consider the more complex task of integrating a multidimensional function f(Z1, . . . , Zd).
One approach is to use the Gaussian quadrature grid in each dimension, giving rise to a d-
dimensional grid with md points. We can then evaluate the function at each of the grid points,
and combine the evaluations together using the appropriate weights. Viewed slightly diﬀerently,
this approach computes the d-dimensional integral recursively, computing, for each point in
dimension i, the Gaussian-quadrature approximation of the integral up to dimension i −1. This
integration rule is accurate for any polynomial which is a sum of monomial terms, each of the
form Qd
i=1 zai
i , where each ai ≤2m −1. Unfortunately, this grid grows exponentially with d,
which can be prohibitive in certain applications.
Unscented Transformation
An alternative approach, called the unscented transformation, is
unscented
transformation
based on the integration method of exact monomials. This approach uses grids designed specif-
exact monomials
ically for Gaussians over IRd. Intuitively, it uses the symmetry of the Gaussian around its axes
to reduce the density of the required grid.
The simplest instance of the exact monomials framework uses 2d + 1 points, as compared to
the 2d points required for the rule derived from Gaussian quadrature. To apply this transforma-
tion, it helps to assume that po(Z) is a standard Gaussian p0(Z) = N (Z | 0; I) where I is
the identity matrix. In cases where p0 is not of that form, so that p0(Z) = N (Z | µ; Σ), we
can do the following change of variable transformation: Let A be the matrix square root of Σ,
that is, A is a d × d matrix such that Σ = AT A. We deﬁne ˜p0(Z′) = N
 Z′ | 0; I

. We can
now show that
p0(Z) = ˜p0(Z′)
M
[Z = AZ′ + µ].
(14.17)
We can now perform a change of variables for each of our functions, deﬁning ˜fi(Z) = fi(AZ+
µ), and perform our moment computation relative to the functions ˜fi rather than fi.
Now, for i = 1, . . . , d, let z+
i be the point in IRd which has zi = +1 and zj = 0 for all j ̸= i.
Similarly, let z−
i = −z+
i . Let λ ̸= 0 be any number. We then use the following integration rule:
Z ∞
−∞
W(z)f(z)dz ≈

1 −d
λ2

f(0) +
d
X
i=1
1
2λ2 f(λz+
i ) +
d
X
i=1
1
2λ2 f(λz−
i ).
(14.18)
In other words, we evaluate f at the mean of the Gaussian, 0, and then at every point which is
±λ away from the mean for one of the variables Zi. We then take a weighted average of these

14.4. Nonlinear Dependencies
635
points, for appropriately chosen weights. Thus, this rule, like Gaussian quadrature, is deﬁned in
terms of a set of points z0, . . . , z2d and weights w0, . . . , w2d, so that
Z ∞
−∞
W(z)f(z)dz =
2d
X
i=0
wif(zi).
This integration rule is used as the basis for the unscented Kalman ﬁlter (see section 15.4.1.2).
unscented
Kalman ﬁlter
The method of exact monomials can be used to provide exact integration for all polynomials of
degree p or less, that is, to all polynomials where each monomial term Qd
i=1 zai
i
has Pd
i=1 ai ≤
p. Therefore, the method of exact monomials has precision p. In particular, equation (14.18)
provides us with a rule of precision 3. Similar rules exist that achieve higher precision. For
example, we can obtain a method of precision 5 by evaluating f at 0, at the 2d points that are
±λ away from the mean along one dimension, and at the 2d(d −1) points that are ±λ away
from the mean along two dimensions. The total number of points is therefore 2d2 + 1.
Note that the precision-3 rule is less precise than the one obtained by using Gaussian quadra-
ture separately for each dimension: For example, if we combine one-dimensional Gaussian
quadrature rules of precision 2, we will get a rule that is also exact for monomials such as z2
1z2
2
(but not for the degree 3 monomial z3
1). However, the number of grid points used in this method
is exponentially lower.
The parameter λ is a free parameter. Every choice of λ ̸= 0 results in a rule of precision
3, but diﬀerent choices lead to diﬀerent approximations. Small values of λ lead to more local
approximations, which are based on the behavior of f near the mean of the Gaussian and are
less aﬀected by the higher order terms of f.
14.4.1.3
Discussion
We have suggested several diﬀerent methods for approximating q as a Gaussian distribution.
What are the trade-oﬀs between them? We begin with two examples.
Example 14.13
Figure 14.7(top) illustrates the two diﬀerent approximations in comparison to the optimal approx-
imation (the correct mean and covariance) obtained by sampling. We can see that the unscented
transformation is almost exact, whereas the linearization method makes signiﬁcant errors in both
mean and covariance.
The bottom row provides a more quantitative analysis for the simple nonlinear function Y =
p
(σZ1)2 + (σZ2)2. The left panel presents results for σ = 2, showing the optimal Gaussian
M-projection and the approximations using three methods: Taylor series, exact monomials with
precision 3, and exact monomials with precision 5. The “optimal” approximation is estimated using
a very accurate Gaussian quadrature rule with a grid of 100 × 100 integration points. We can
see that the precision-5 rule is very accurate, but even the precision-3 rule is signiﬁcantly more
accurate than the Taylor series. The right panel shows the KL-divergence between the diﬀerent
approximations and the optimal approximation.
We see that the quality of approximation of
every method degrades as σ increases. This behavior is to be expected, since all of the methods
are accurate for low-order polynomials, and the larger the σ, the larger the contribution of the
higher-order terms. For small and medium variances, the Taylor series is the least exact of the three
methods. For large variances, the precision 3 rule becomes signiﬁcantly less accurate. The reason
is that for σ > 0.23, the covariance matrices returned by the numerical integration procedure

636
Chapter 14. Inference in Hybrid Networks
Taylor Series
precision 3 rule
precision 5 rule
optimal approximation
Taylor Series
precision 3 rule
precision 5 rule
Actual
(sampling)
Linearized
(Taylor)
Monomials
(unscented)
0
0.05
0.1
0.15
0.2
0.25
0.3
–2
0
2
4
6
8
Variance
0.00.0
0.2
0.4
0.6
0.8
1.0
KL−divergence
100.0
80.0
60.0
40.0
20.0
sigma points
convariance
mean
transformed sigma points
(a)
(b)
(c)
est. µ
est. µ
true µ
est. Σ
est. Σ
true Σ
Figure 14.7
Comparisons of diﬀerent Gaussian approximations for a nonlinear dependency. The
top row (adapted with permission from van der Merwe et al. (2000a)) illustrates the process of diﬀerent
approximation methods and the results they obtain; the function being linearized is a feed-forward neural
network with random weights.
The bottom row shows a more quantitative analysis for the function
f(Z1, Z2) =
p
(σZ1)2 + (σZ2)2. The left panel shows the diﬀerent approximations when σ2 = 4, and
the right panel the KL-divergence from optimal approximation as a function of σ2.

14.4. Nonlinear Dependencies
637
are illegal, and must be corrected. The correction produces reasonable answers for values of σ
up to σ = 4, and then degrades. However, it is important to note that, for high variances, the
Gaussian approximation is a poor approximation to p in any case, so that the whole approach of
using a Gaussian approximation in inference breaks down. For low variances, where the Gaussian
approximation is reasonable, even the corrected precision-3 rule signiﬁcantly dominates the Taylor
series approach.
From a computational perspective, the Gaussian quadrature method is the most precise, but
also the most expensive. In practice, one would only apply it in cases where precision was of
critical important and the dimension was very low. The cost of the other two methods depends
on the function f that we are trying to linearize. The linearization method (equation (14.13))
requires that we evaluate f and each of f’s d partial derivatives at the point 0.
In cases
where the partial derivative functions can be written in closed form, this process requires only
d + 1 function evaluations. By contrast, the precision 3 method requires 2d + 1 evaluations
of the function f, and the precision 5 method requires 2d2 + 1 function evaluations of f.
In addition, to use the numerical integration methods we need to convert our distribution to
the form of equation (14.17), which is not always required for the Taylor series linearization.
Finally, one subtle problem can arise when using numerical integration to perform the M-
projection operation: Here, the quantities of equation (14.14)–(14.15) are computed using an
approximate procedure. Thus, although for exact integration, the covariance matrix deﬁned by
these equations is guaranteed to be positive deﬁnite; this is not the case for the approximate
quantities, where the approximation may give rise to a matrix that is not positive deﬁnite. See
section 14.7 for references to a modiﬁed approach that avoids this problem.
Putting computational costs aside, the requirement in the linearization approach of com-

puting the gradient may be a signiﬁcant issue in some settings. Some functions may not
be diﬀerentiable (for example, the max function), preventing the use of the Taylor series
expansion. Furthermore, even if f is diﬀerentiable, computing its gradient may still be
diﬃcult. In some applications, f might not even be given in a parametric closed form;
rather, it might be implemented as a lookup table, or as a function in some program-
ming language. In such cases, there is no simple way to compute the derivatives of the
Taylor series expansion, but it is easy to evaluate f on a given point, as required for the
numerical integration approach.
14.4.2
Expectation Propagation with Gaussian Approximation
The preceding section developed the basic tool of approximating a distribution as a Gaussian.
We now show how these methods can be used to perform expectation propagation message
passing inference in nonlinear graphical models. Roughly speaking, at each step, we take all of
the factors that need to be multiplied, and we approximate as a Gaussian the measure derived
by multiplying all of these factors. We can then use this Gaussian distribution to form the
desired message.
For the application of this method, we assume that each of the factors in our original
distribution Φ deﬁnes a conditional distribution over a single random variable in terms of
others. This assumption certainly holds in the case of standard Bayesian networks, where all
CPDs are (by deﬁnition) of this form. It is also the case for many Markov networks, especially
in the continuous case. We further assume that each of the factors in Φ can be written as a

638
Chapter 14. Inference in Hybrid Networks
deterministic function:
Xi = fi(Y i, W i),
(14.19)
where Y i ⊆X are the model variables on which Xi’s CPD depends, and W i are “new”
standard Gaussian random variables that capture all of the stochasticity in the CPD of Xi. We
call these W’s exogenous variables, since they capture stochasticity that is outside the model.
(See also section 21.4.)
Although there are certainly factors that do not satisfy these assumptions, this representational
class is quite general, and encompasses many of the factors used in practical applications. Most
obviously, using the transformation of equation (14.17), this representation encompasses any
Gaussian distribution. However, it also allows us to represent many nonlinear dependencies:
Example 14.14
Consider the nonlinear CPD X ∼N
p
Y 2
1 + Y 2
2 ; σ2
. We can reformulate this CPD in terms
of a deterministic, nonlinear function, as follows: We introduce a new exogenous variable W that
captures the stochasticity in the CPD. We then deﬁne X = f(Y1, Y2, W) where f(Y1, Y2, W) =
p
Y 2
1 + Y 2
2 + σW.
In this example, as in many real-world CPDs, the dependence of X on the stochastic variable
W is linear. However, the same idea also applies in cases where the variance is a more complex
function:
Example 14.15
Returning again to example 5.20, assume we want the variance of the vehicle’s position at time
t + 1 to depend on its time t velocity, so that we have more uncertainty about the vehicle’s
next position if it is moving faster. Thus, for example, we might want to encode a distribution
N
 X′ | X + V ; ρV 2
. We can do so by introducing an exogenous standard Gaussian variable Z
and deﬁning
X′ = X + V + √ρV Z.
It is not diﬃcult to verify that X′ has the appropriate Gaussian distribution.
We now apply the EP framework of section 11.4.3.2. As there, we maintain the potential at
each cluster Ci as a factor set ⃗φi; some of those factors are initial factors, whereas others
are messages sent to cluster Ci. Our initial potentials are all of the form equation (14.19), and
since we project all messages into the Gaussian parametric family, all of the incoming messages
are Gaussians, which we can reformulate as a standard Gaussian and a set of deterministic
functions.
In principle, it now appears that we can take all of the incoming messages, along with all of
the exogenous Gaussian variables W i, to produce a single Gaussian distribution p0. We can
then apply the linearization procedures of section 14.4.1 to obtain a Gaussian approximation.
However, a closer examination reveals several important subtleties that must be addressed.
14.4.2.1
Dealing with Evidence
Our discussion so far has sidestepped the issue of evidence: nowhere did we discuss the place
at which evidence is instantiated into the factors. In the context of discrete variables, this issue

14.4. Nonlinear Dependencies
639
was resolved in a straightforward way by restricting the factors (as we discussed in section 9.3.2).
This restriction could be done at any time during the course of the message passing algorithm
(as long as the observed variable is never eliminated).
In the context of continuous variables, the situation is more complex, since any assignment
to a variable induces a density over a subspace of measure 0. Thus, when we observe X = x,
we must a priori restrict all factors to the space where X = x. This operation is straightforward
in the case of canonical forms, but it is somewhat subtler for nonlinear functions. For example,
consider the nonlinear dependence of example 14.14. If we have evidence Y1 = y1, we can easily
redeﬁne our model as X = g(Y2, W) where g(Y2, W) =
p
y2
1 + Y 2
2 + σW. However, it is not
so clear what to do with evidence on the dependent variable X.
The simplest solution to this problem, and one which is often used in practice, is to instantiate
“downstream” the evidence in a cluster after the cluster is linearized. That is, in the preceding
example, we would ﬁrst linearize the function f in its cluster, resulting in a Gaussian distribution
p(X, Y1, Y2); we would then instantiate the evidence X = x in the canonical form associated
with this distribution, to obtain a new canonical form that is proportional to p(Y1, Y2 | x).
This approach is simple, but it can be very inaccurate. In particular, the linearization operation
(no matter how it is executed) depends on the distribution p0 relative to which the linearization
is performed. Our posterior distribution, given the evidence, can be very diﬀerent from the
prior distribution p0, leading to a very diﬀerent linearization of f. Better methods for taking the
evidence into account during the linearization operation exist, but they are outside the scope of
this book.
14.4.2.2
Valid Linearization
A second key issue is that all of the linearization procedures described earlier require that po
be a Gaussian distribution, and not a general canonical form. This requirement is a signiﬁcant
one, and imposes constraints on one or more of: the structure of the cluster graph, the order in
which messages are passed, and even the probabilistic model itself.
Example 14.16
Most simply, consider a chain-structured Bayesian network X1 →X2 →X3, where all variables
have nonlinear CPDs. We thus have a function X1 = f1(W1) and functions X2 = f2(X1, W2)
and X3 = f3(X2, W3). In the obvious clique tree, we would have a clique C1 with scope X1, X2,
containing f1 and f2, and a clique C2 with scope X2, X3, containing f3. Further, assume that
we have evidence X3 = x3.
In the discrete setting, we can simply restrict the factor in C2 with the observation over X3,
producing a factor over X2. This factor can then be passed as a message to C1. In the nonlinear
setting, however, we must ﬁrst linearize f3, and for that, we must have a distribution over X2.
But we can only obtain this distribution by multiplying into the factor p(X1), p(X2 | X1), and
p(X3 | X2). In other words, in order to deal with C2, we must ﬁrst pass a message from C1.
In general, this requirement on order constrained message passing is precisely the same one
order-constrained
message passing
that we faced for CLG distributions in section 14.3.3.1, with the same consequences.
In a
Bayesian network, this requirement constrains us to pass messages in a topological order. In
other words, before we linearize a function Xi = fi(PaXi, W i), we must ﬁrst linearize and
obtain a Gaussian for every Yj ∈PaXi.

640
Chapter 14. Inference in Hybrid Networks
Example 14.17
Consider the network of ﬁgure 14.6, but where we now assume that all variables (including A, B, C)
are continuous and utilize nonlinear CPDs. As in example 14.7, the clique tree of ﬁgure 14.6c does
not allow messages to be passed at all, since none of the cliques respect the ordering constraint. The
clique tree of (e) does allow messages to be passed, but only if the {A, B, X, Y } clique is the ﬁrst
to act, passing a message over A, Y to the clique {A, C, Y, Z}; this message deﬁnes a distribution
over the parents of C and Z, allowing them to be linearized.
In the context of Markov networks, we again can partially circumvent the problem if we
assume that the factors in the network are all factor normalizable. However, this requirement
may not always hold in practice.
14.4.2.3
Linearization of Multiple Functions
A ﬁnal issue, related to the previous one, arises from our assumption in section 14.4.1 that all of
the functions in a cluster depend only on a set of standard Gaussian random variables Z. This
assumption is overly restrictive in almost all cluster graphs.
Example 14.18
Consider again our chain-structured Bayesian network X1 →X2 →X3 of example 14.16. Here,
C1 has scope X1, X2, and contains both f1 and f2. This structure does not satisfy the preceding
requirements, since X2 relies also on X1.
There are two ways to address the issue in this case.
The ﬁrst, which we call incremental
incremental
linearization
linearization ﬁrst linearizes f1, and subsequently linearize f2. This approach can be implemented
by making a separate clique containing just X1. In this clique, we have a Gaussian distribution
p1(Z1), and so we can compute a Gaussian approximation to f1(Z1), producing a Gaussian
message ˜δ1→2(X1).
We can then pass this message to a clique containing X1, X2, but only
the f2(X2, Z2) function; we now have a Gaussian distribution p2(X2, Z2), and can use the
techniques of section 14.4.1 to linearize f2(X2, Z2) into this Gaussian, to produce a Gaussian
distribution over X1, X2. (Note that p2(X2, Z2) is not a standard Gaussian, but we can use the
trick of equation (14.17) to change the coordinate system; see exercise 14.7.) We can then marginalize
the resulting Gaussian distribution onto X2, and send a message ˜δ2→3(X2) to C3, continuing this
process.
As a second alternative, called simultaneous linearization, we can linearize multiple nonlinear
simultaneous
linearization
functions into the same cluster together. We can implement this solution by substituting the variable
X1 with its functional deﬁnition; that is, we can deﬁne X2 = g2(Z1, Z2) = f2(f1(Z1), Z2), and
use g2 rather than f2 in the analysis of section 14.4.1.
Both of these solutions generalize beyond this simple example. In the incremental approach,
we simply deﬁne smaller clusters, each of which contains at most one nonlinear function. We
then linearize one such function at a time, producing each time a Gaussian distribution. This
Gaussian is passed on to another cluster, and used as the basis for linearizing another nonlinear
function.
The simultaneous approach linearizes several functions at once, substituting each
variable with the function that deﬁnes it, so as to make all the functions depend only on
Gaussian variables.
These two approaches make diﬀerent approximations: Going back to example 14.16, in the
incremental approach, the approximation of f2 uses a Gaussian approximation to the distribution

14.4. Nonlinear Dependencies
641
over X1. If this approximation is poor, it may lead to a poor approximation for the distribution
over X2.
Conversely, in the simultaneous approach, we are performing an integral for the
function g2, which may be more complicated than f2, possibly leading to a poor approximation
of its moments. In general, the errors made by each method are going to depend on the speciﬁc
case at hand, and neither necessarily dominates the other.
14.4.2.4
Iterated Message Passing
The general algorithm we described can be applied within the context of diﬀerent message
passing schemes. Most simply, we deﬁne a clique tree, and we then do a standard upward and
downward pass. However, as we discussed in the context of the general expectation propagation
algorithm, our approximation within a cluster depends on the contents of that factor. In our
setting, the approximation of p L f as a Gaussian depends on the distribution p. Depending
on the order in which CPDs are linearized and messages are passed, the resulting distribution
might be very diﬀerent.
Example 14.19
Consider a network consisting of a variable X and its two children Y and Z, so that our two
cliques are C1 = {X, Y } and C2 = {X, Z}. Assume that we observe Y = y. If we include the
prior p(X) within C1, then we ﬁrst compute C1’s message, producing a Gaussian approximation
to the joint posterior ˆp(X, Y ). This posterior is marginalized to produce ˆp(Y ), which is then used
as the basis for linearizing Z. Conversely, if we include p(X) within C2, then the linearization of
Z is done based on an approximation to X’s prior distribution, generally leading to a very diﬀerent
linearization for p(X, Z).
In general, the closer the distribution used for the approximation is to the true posterior, the
higher the quality of the approximation. Thus, in this example, we would prefer to ﬁrst linearize
Y and condition on its value, and only then to linearize Z. However, we cannot usually linearize
all CPDs using the posterior. For example, assume that Z has another child W whose value
is also observed; the constraint of using a topological ordering prevents us from linearizing W
before linearizing Z, so we are forced to use a distribution over X that does not take into
account the evidence on W.
The iterative EP algorithm helps address this limitation. In particular, once we linearize all
of the initial clusters (subject to the constraints we mentioned), we can now continue to pass
messages between the clusters using the standard belief update algorithm.
At any point in
the algorithm where cluster Ci must send a message (whether at every message or on a more
intermittent basis), it can use the Gaussian deﬁned by the incoming messages and the exogenous
Gaussian variables to relinearize the functions assigned to it. The revised messages arising from
this new linearization are then sent to adjacent clusters, using the standard EP update rule of
equation (11.41) (that is, by subtracting the suﬃcient statistics of the previously sent message).
This generally has the eﬀect of linearizing using a distribution that is closer to the posterior,
and hence leading to improved accuracy. However, it is important to remember that, as in
section 14.3.3.2, the messages that arise in belief-update message passing may not be positive
deﬁnite, and hence can give rise to canonical forms that are not legal Gaussians, and for which
integration is impossible. To avoid catastrophic failures in the algorithm, it is important to check
that any canonical form used for integration is, indeed, a normalizable Gaussian.

642
Chapter 14. Inference in Hybrid Networks
14.4.2.5
Augmented CLG Networks
One very important consequence of this algorithm is its ability to address one of the main
limitations of CLG networks — namely, that discrete variables cannot have continuous parents.
This limitation is a signiﬁcant one. For example, it prevents us from modeling simple objects
such as a thermostat, whose discrete on/oﬀstate depends on a continuous temperature variable.
As we showed, we can easily model such dependencies, using, for example, a linear sigmoid
model or its multinomial extension.
The algorithm described earlier, which accommodates
nonlinear CPDs, easily deals with this case.
Example 14.20
Consider the network X →A, where X has a Gaussian distribution given by p(X) = N (X | µ; σ)
and the CPD of A is a softmax given by P(A = a1 | X = x) = 1/(1 + ec1x+c0). The clique tree
has a single clique (X, A), whose potential should contain the product of these two CPDs. Thus,
it should contain two entries, one for a1 and one for a0, each of which is a continuous function:
p(x)P(a1 | x) and p(x)P(a0 | x). As before, we approximate this potential using a canonical
table, with an entry for every assignment to A, and where each entry is a Gaussian distribution.
Thus, we would approximate each entry p(x)P(a | x) as a Gaussian. Once again, we have a
product of a function — P(a | x) — with a Gaussian — p(x). We can therefore use any of the
methods in section 14.4.1 to approximate the result as a single Gaussian.
This simple extension forms the basis for a general algorithm that deals with hybrid networks
involving both continuous and discrete variables; see exercise 14.11.
14.5
Particle-Based Approximation Methods
All of the message passing schemes described before now utilize the Gaussian distribution as a
parametric representation for the messages and (in some cases) even the clique potentials. This
representation allows for truly exact inference only in the case of pure Gaussian networks, and it
is exact in a weak sense for a very restricted class of CLG networks. If our original factors are far
from being Gaussian, and, most particularly if they are multimodal, the Gaussian approximation
used in these algorithms can be a very poor one. Unfortunately, there is no general-purpose
parametric class that allows us to encode arbitrarily continuous densities.
An alternative approach is to use a semiparametric or nonparametric method, which allows us
to avoid parametric assumptions that may not be appropriate in our setting. Such approaches
are often applied in continuous settings, since there are many settings where the Gaussian
approximation is clearly inappropriate. In this section, we discuss how such methods can be
used in the context of inference in graphical models.
14.5.1
Sampling in Continuous Spaces
We begin by discussing the basic task of sampling from a continuous univariate measure. As
we discussed in box 12.A, sampling from a discrete distribution can be done using straightfor-
ward methods. Eﬃcient solutions, albeit somewhat more complex, are also available for other
parametric forms, including Gaussians, Gamma distributions, and others; see section 14.7. Un-
fortunately, such solutions do not exist for all continuous densities. In fact, even distributions
that can be characterized analytically may not have established sampling procedures.

14.5. Particle-Based Approximation Methods
643
A number of general-purpose approaches have been designed for sampling from arbitrary
continuous densities.
Perhaps the simplest is rejection sampling (or sometimes acceptance-
rejection
sampling
rejection sampling). The basic idea is to sample a value from a proxy distribution and then
“accept” it with probability that is proportional to the skew introduced by the proxy distribution.
Suppose that we want to sample a variable X with density p(x). Now suppose that we have
a function q(x) such that q(x) > 0 whenever p(x) > 0 and q(x) ≥p(x). Moreover, suppose
that we can sample from the distribution
Q(X) = 1
Z q(x),
where Z is a normalizing constant. (Note that Z > 1, as q(x) is an upper bound on a density
function whose integral is 1.)
In this case, we repeatedly perform the following steps, until
acceptance:
1. Sample x ∼Q(x), u ∼Unif([0, 1]).
2. If u < p(x)
q(x) return x.
The probability that this procedure returns the value x is exactly p(x). The proof is analogous
to the one we saw for importance sampling (see proposition 12.1). (The diﬀerence here is that
we need to return a single sample rather than weight multiple samples.) In practice, we can
speed up the procedure if we have also a lower bound b(x) for p(x) (that is b(x) ≤p(x)). In
that case, if u < b(x)
q(x) than we accept the sample x without evaluating p(x). By ﬁnding easy
to evaluate functions q(x) and b(x) that allow sampling from Q, and provide relatively tight
bounds on the density p(x), this method can accept samples within few iterations.
This straightforward approach, however, can be very slow if q(x) is not a good approximation
to p(x). Fortunately, more sophisticated methods have also been designed. One example is
described in exercise 12.22. See section 14.7 for other references.
14.5.2
Forward Sampling in Bayesian Networks
We now consider the application of forward sampling in a Bayesian network (algorithm 12.1).
Assuming that we have a method for generating samples from the continuous density of each
variable Xi given an assignment to its parents, we can use this method to generate samples
from a broad range of continuous and hybrid Bayesian networks — far greater than the set
of networks for which message passing algorithms are applicable.
This approach generates
random samples from the prior distribution p(X). As we discussed, these samples can be used
to estimate the expectation of any function f(X), as in equation (12.1). For example, we can
compute the mean IE[X] of any variable X by taking f(X) = X, or its variance by taking
f(X) = X2 and subtracting IE[X]2.
However, plain forward sampling is generally inapplicable to hybrid networks as soon as we
have evidence over some of the network variables. Consider a simple network X →Y , where
we have evidence Y = y. Forward sampling generates (x[m], y[m]) samples, and rejects those
in which y[m] ̸= y. Because the domain of Y is a continuum, the probability that our sampling
process will generate any particular value y is zero. Thus, none of our samples will match the
observations, and all will be rejected.

644
Chapter 14. Inference in Hybrid Networks
A more relevant approach in the case of hybrid networks is a form of importance sampling,
such as the likelihood weighting algorithm of algorithm 12.2. The likelihood weighting algorithm
generalizes to the hybrid case in the most obvious way: Unobserved variables are sampled, based
on the assignment to their parents, as described earlier. Observed variables are instantiated to
their observed values, and the sample weight is adjusted. We can verify, using the same analysis
as in section 12.2, that the appropriate adjustment to the importance weight for a continuous
variable Xi observed to be xi is the density p(xi | ui), where ui is the assignment, in the
sample, to PaXi. The only implication of this change is that, unlike in the discrete case, this
adjustment may be greater than 1 (because a density function is not bounded in the range
[0, 1]). Although this diﬀerence does not inﬂuence the algorithm, it can increase the variance of
the sample weights, potentially decreasing the overall quality of our estimator.
14.5.3
MCMC Methods
The second type of sampling algorithms is based on Markov chain Monte Carlo (MCMC) methods.
MCMC
In general, the theory of Markov chains for continuous spaces is quite complicated, and many
of the basic theorems that hold for discrete spaces do not hold, or hold only under certain
assumptions. A discussion of these issues is outside the scope of this book. However, simple
variants of the MCMC algorithms for graphical models continue to apply in this setting.
The Gibbs sampling algorithm (algorithm 12.4) can also be applied to the case of hybrid
Gibbs sampling
networks. As in the discrete case, we maintain a sample, which is a complete assignment to the
network variables. We then select variables Xi one at a time, and sample a new value for Xi
from p(Xi | ui), where ui is the current assignment to Xi’s Markov blanket. The only issue that
might arise relates to this sampling step. In equation (12.23), we showed a particularly simple
form for the probability of Xi = x′
i given the assignment ui to Xi’s. The same derivation
applies to the continuous case, simply replacing the summation with an integral:
p(x′
i | ui) =
Q
Cj∋Xi βj(x′
i, ui)
R
x′′
i
Q
Cj∋Xi βj((x′′
i , ui)),
where Cj are the diﬀerent potentials in the network.
In the discrete case, we could simply multiply the relevant potentials as tables, renormalize
the resulting factor, and use it as a sampling distribution. In the continuous case, matters may
not be so simple: the product of factors might not have a closed form that allows sampling.
In this case, we generally resort to the use of a Metropolis-Hastings algorithm.
Here, as in
Metropolis-
Hastings
the discrete case, changes to a variable Xi are proposed by sampling from a local proposal
distribution T Q
i . The proposed local change is either accepted or rejected using the appropriate
acceptance probability, as in equation (12.26):
A(ui, xi →ui, x′
i)
=
min

1, p(x′
i | ui)
p(xi | ui)
T Q(ui, x′
i →ui, xi)
T Q(ui, xi →ui, x′
i)

,
where (as usual) probabilities are replaced with densities.
The only question that needs to be addressed is the choice of the proposal distribution.
One common choice is a Gaussian or student-t distribution centered on the current value xi.
Another is a uniform distribution over a ﬁnite interval also centered on xi. These proposal

14.5. Particle-Based Approximation Methods
645
distributions deﬁne a random walk over the space, and so a Markov chain that uses such a
proposal distribution is often called a random-walk chain. These choices all guarantee that the
random-walk
chain
Markov chain converges to the correct posterior, as long as the posterior is positive: for all
z ∈Val(X −E), we have that p(z | e) > 0. However, the rate of convergence can depend
heavily on the window size: the variance of the proposal distribution. Diﬀerent distributions,
and even diﬀerent regions within the same distribution, can require radically diﬀerent window
sizes. Picking an appropriate window size and adjusting it dynamically are important questions
that can greatly impact performance.
14.5.4
Collapsed Particles
As we discussed in section 12.4, it is diﬃcult to cover a large state space using full instantiations
to the network variables. Much better estimates — often with much lower variance — can be
obtained if we use collapsed particles. Recall that, when using collapsed particles, the variables
X are partitioned into two subsets X = Xp ∪Xd.
A collapsed particle then consists of
an instantiation xp ∈Val(Xp), coupled with some representation of the distribution P(Xd |
xp, e). The use of such particles relies on our ability to do two things: to generate samples from
Xp eﬀectively, and to represent compactly and reason with the distribution P(Xd | xp, e).
The notion of collapsed particles carries over unchanged to the hybrid case, and virtually
every algorithm that applied in the discrete case also applies here. Indeed, collapsed particles
are often particularly suitable in the setting of continuous or hybrid networks: In many such
networks, if we select an assignment to some of the variables, the conditional distribution over
the remaining variables can be represented (or well approximated) as a Gaussian. Since we
can eﬃciently manipulate Gaussian distributions, it is generally much better, in terms of our
time/accuracy trade-oﬀ, to try and maintain a closed-form Gaussian representation for the parts
of the distribution for which such an approximation is appropriate.
Although this property can be usefully exploited in a variety of networks, one particularly
useful application of collapsed particles is motivated by the observation that inference in a
purely continuous network is fairly tractable, whereas inference in the simplest hybrid networks
— polytree CLGs — can be very expensive. Thus, if we can “erase” the discrete variables from
the network, the result is a much simpler, purely continuous network, which can be manipulated
using the methods of section 14.2 in the case of linear Gaussians, and the methods of section 14.4
in the more general case.

Thus, CLG networks are often eﬀectively tackled using a collapsed particles approach,
where the instantiated variables in each particular are the discrete variables, Xp = ∆,
and the variables maintained in a closed-form distribution are the continuous variables,
Xd = Γ. We can now apply any of the methods described in section 12.4, often with some
useful shortcuts. As one example, likelihood weighting can be easily applied, since discrete
variables cannot have continuous parents, so that the set Xp is upwardly closed, allowing for
easy sampling (see exercise 14.12). The application of MCMC methods is also compelling in this
case, and it can be made more eﬃcient using incremental update methods such as those of
exercise 12.27.

646
Chapter 14. Inference in Hybrid Networks
14.5.5
Nonparametric Message Passing
Yet another alternative is to use a hybrid approach that combines elements from both particle-
based and message passing algorithms. Here, the overall algorithm uses the structure of a mes-
sage passing (clique tree or cluster graph) approach. However, we use the ideas of particle-based
inference to address the limitations of using a parametric representation for the intermediate
factors in the computation. Speciﬁcally, rather than representing these factors using a single
parametric model, we encode them using a nonparametric representation that allows greater
ﬂexibility to capture the properties of the distribution. Thus, we are essentially still reparameter-
izing the distribution in terms of a product of cluster potentials (divided by messages), but each
cluster potential is now encoded using a nonparametric representation.
The advantage of this approach over the pure particle-based methods is that the samples are
generated in a much lower-dimensional space: single cluster, rather than the entire joint distri-
bution. This can alleviate many of the issues associated with sampling in a high-dimensional
space. On the other side, we might introduce additional sources of error. In particular, if we are
using a loopy cluster graph rather than a clique tree as the basis for this algorithm, we have all of
the same errors that arise from the representation of a distribution as a set of pseudo-marginals.
One can construct diﬀerent instantiations of this general approach, which can vary along
several axes. The ﬁrst is the message passing algorithm used: clique tree versus cluster graph,
sum-product versus belief update. The second is the form of the representation used for factors
and messages: plain particles; a nonparametric density representation; or a semiparametric
representation such as a histogram. Finally, we have the approach used to approximate the factor
in the chosen representation: importance sampling, MCMC, or a deterministic approximation.
14.6
Summary and Discussion
In this chapter, we have discussed some of the issues arising in applying inference to networks
involving continuous variables. While the semantics of such networks is easy to deﬁne, they
raise considerable challenges for the inference task.
The heart of the problem lies in the fact that we do not have a universal representation for
factors involving continuous variables — one that is closed under basic factor operations such
as multiplication, marginalization, and restriction with evidence. This limitation makes it very
diﬃcult to design algorithms based on variable elimination or message passing. Moreover, the
diﬃculty is not simply a matter of our inability to ﬁnd good algorithms. Theoretical analysis
shows that classes of network structures that are tractable in the discrete case (such as polytrees)
give rise to NP-hard inference problems in the hybrid case.
Despite the diﬃculties, continuous variables are ubiquitous in practice, and so signiﬁcant
work has been done on the inference problem for such models.
Most message passing algorithms developed in the continuous case use Gaussians as a lingua
franca for factors. This representation allows for exact inference only in a very limited class of
models: clique trees for linear Gaussian networks. However, the exact same factor operations
provide a basis for a belief propagation algorithm for Gaussian networks. This algorithm is easy
to implement and has several satisfying guarantees, such as guaranteed convergence under fairly
weak conditions, producing the exact mean if convergence is achieved. This technique allows us
to perform inference in Gaussians where manipulating the full covariance matrix is intractable.

14.7. Relevant Literature
647
In cases where not all the potentials in the network are Gaussians, the Gaussian representation
is generally used as an approximation. In particular, a standard approach uses an instantiation
of the expectation propagation algorithm, using M-projection to approximate each non-Gaussian
distribution as a Gaussian during message passing. In CLG networks, where factors represent
a mixture of (possibly exponentially many) Gaussians derived from diﬀerent instantiations of
discrete variables, the M-projection is used to collapse the mixture components into a single
Gaussian. In networks involving nonlinear dependencies between continuous variables, or be-
tween continuous and discrete variables, the M-projection involves linearization of the nonlinear
dependencies, using either a Taylor expansion or numerical integration. While simple in prin-
ciple, this application of EP raises several important subtleties. In particular, the M-projection
steps can only be done over intermediate factors that are legal distributions. This restriction
imposes signiﬁcant constraints on the structure of the cluster graph and on the order in which
messages can be passed. Finally, we note that the Gaussian approximation is good in some

cases, but can be very poor in others. For example, when the distribution is multimodal,
the Gaussian M-projection can be a very broad (perhaps even useless) agglomeration of
the diﬀerent peaks.
This observation often leads to the use of approaches that use a nonparametric approximation.
Commonly used approaches include standard sampling methods, such as importance sampling
or MCMC. Even more useful in some cases is the use of collapsed particles, which avoid sampling
a high-dimensional space in cases where parts of the distribution can be well approximated
as a Gaussian.
Finally, there are also useful methods that integrate message passing with
nonparametric approximations for the messages, allowing us to combine some of the advantages
(and some of the disadvantages) of both types of approaches.
Our presentation in this chapter has only brieﬂy surveyed a few of the key ideas related
to inference in continuous and hybrid models, focusing mostly on the techniques that are
speciﬁcally designed for graphical models. Manipulation of continuous densities is a staple of
statistical inference, and many of the techniques developed there could be applied in this setting
as well.
For example, one could easily imagine message passing techniques that use other
representations of continuous densities, or the use of other numerical integration techniques.
Moreover, the use of diﬀerent parametric forms in hybrid networks tends to give rise to a host of
“special-case” models where specialized techniques can be usefully applied. In particular, even
more so than for discrete models, it is likely that a good solution for a given hybrid model will
require a combination of diﬀerent techniques.
14.7
Relevant Literature
Perhaps the earliest variant of inference in Gaussian networks is presented by Thiele (1880), who
deﬁned what is the simplest special case of what is now known as the Kalman ﬁltering algorithm
(Kalman 1960; Kalman and Bucy 1961). Shachter and Kenley (1989) proposed the general idea
of network-based probabilistic models, in the context of Gaussian inﬂuence diagrams. The ﬁrst
presentation of the general elimination algorithm for Gaussian networks is due to Normand and
Tritchler (1992). However, the task of inference in pure Gaussian networks is highly related to the
basic mathematical problem of solving a system of linear equations, and the elimination-based
inference algorithms very similar to Gaussian elimination for solving such systems. Indeed, some

648
Chapter 14. Inference in Hybrid Networks
of the early incarnations of these algorithms were viewed from that perspective; see Parter (1961)
and Rose (1970) for some early work along those lines.
Iterative methods from linear algebra (Varga 2000) can also be used for solving systems of
linear equations; in eﬀect, these methods employ a form of local message passing to compute
the marginal means of a Gaussian distribution. Loopy belief propagation methods were ﬁrst
proposed as a way of also estimating the marginal variances.
Over the years, multiple au-
thors (Rusmevichientong and Van Roy 2001; Weiss and Freeman 2001a; Wainwright et al. 2003a;
Malioutov et al. 2006) have analyzed the convergence and correctness properties of Gaussian
belief propagation, for larger and larger classes of models. All of these papers provide conditions
that ensure convergence for the algorithm, and demonstrate that if the algorithm converges, the
means are guaranteed to be correct. The recent analysis of Malioutov et al. (2006) is the most
comprehensive; they show that their suﬃcient condition, called walk-summability, is equivalent
walk-summability
to pairwise normalizability, and encompasses all of the classes of Gaussian models that were
previously shown to be solvable via LBP (including attractive, nonfrustrated, and diagonally
dominant models). They also show that the variances at convergence are an underestimate of
the true variances, so that the LBP results are overconﬁdent; their results point the way to par-
tially correcting these inaccuracies. The results also analyze LBP for non-walksummable models,
relating convergence of the variance to validity of the LBP computation tree.
The properties of conditional Gaussian distributions were studied by Lauritzen and Wermuth
(1989). Lauritzen (1992) extended the clique tree algorithm to the task of inference in these
models, and showed, for strongly rooted clique trees, the correctness of the discrete marginals
and of the continuous means and variances. Lauritzen and Jensen (2001) and Cowell (2005)
provided alternative variants of this algorithm, somewhat diﬀerent representations, which are
numerically more stable and better able to handle deterministic linear relationships, where the
associated covariance matrix is not invertible. Lerner et al. (2001) extend Lauritzen’s algorithm
to CLG networks where continuous variables can have discrete children, and provide conditions
under which this algorithm also has the same correctness guarantees on the discrete marginals
and the moments of the continuous variables. The NP-hardness of inference in CLG networks
of simple structures (such as polytrees) was shown by Lerner and Parr (2001). Collapsed particles
have been proposed by several researchers as a successful alternative to full collapsing of the
potentials into a single Gaussian; methods include random sampling over particles (Doucet et al.
2000; Paskin 2003a), and deterministic search over the particle assignment (Lerner et al. 2000;
Lerner 2002), a method particularly suitable in applications such as fault diagnosis, when the
evidence is likely to be of low probability.
The idea of adaptively modifying the approximation of a continuous cluster potential during
the course of message passing was ﬁrst proposed by Kozlov and Koller (1997), who used a
variable-resolution discretization approach (a semiparametric approximation). Koller et al. (1999)
generalized this approach to other forms of approximate potentials. The expectation propagation
algorithm, which uses a parametric approximation, was ﬁrst proposed by Minka (2001b), who
also made the connection to optimizing the energy functional under expectation matching
constraints. Opper and Winther (2005) present an alternative algorithm based on a similar idea.
Heskes et al. (2005) provide a unifying view of these two works.
Heskes and Zoeter (2003)
discuss the use of weak marginalization within the generalized belief propagation in a network
involving both discrete and continuous variables.
The use of the Taylor series expansion to deal with nonlinearities in probabilistic models is

14.8. Exercises
649
a key component of the extended Kalman ﬁlter, which extends the Kalman ﬁltering method to
nonlinear systems; see Bar-Shalom, Li, and Kirubarajan (2001) for a more in-depth presentation
of these methods. The method of exact monomials, under the name unscented ﬁlter, was ﬁrst
proposed by Julier and Uhlmann (1997). Julier (2002) shows how this approach can be modiﬁed
to address the problem of producing approximations that are not positive deﬁnite.
Sampling from continuous distributions is a core problem in statistics, on which exten-
sive work has been done. Fishman (1996) provides a good overview of methods for various
parametric families. Methods for sampling from other distributions include adaptive rejection
sampling (Gilks and Wild 1992; Gilks 1992), adaptive rejection metropolis sampling (Gilks et al.
1995) and slice sampling (Neal 2003).
The bugs system supports sampling for many continuous families, within their general MCMC
framework. Several approaches combine sampling with message passing. Dawid et al. (1995);
Hernández and Moral (1997); Kjærulﬀ(1995b) propose methods that sample a continuous factor
to turn it into a discrete factor, on which standard message passing can be applied. These
methods vary on how the samples are generated, but the sampling is performed only once, in
the initial message passing step, so that no adaptation to subsequent information is possible.
Sudderth et al. (2003) propose nonparametric belief propagation, which uses a nonparametric
nonparametric
belief
propagation
approximation of the potentials and messages, as a mixture of Gaussians — a set of particles
each with a small Gaussian kernel. They use MCMC methods to regenerate the samples multiple
times during the course of message passing.
Many of the ideas and techniques involving inference in hybrid systems were ﬁrst developed
in a temporal setting; we therefore also refer the reader to the relevant references in section 15.6.
14.8
Exercises
Exercise 14.1
Let X and Y be two sets of continuous variables, with |X| = n and |Y | = m. Let
p(Y | X) = N (Y | a + BX; C)
where a is a vector of dimension m, B is an m × n matrix, and C is an m × m matrix. This dependence
is a multidimensional generalization of a linear Gaussian CPD. Show how p(Y | X) can be represented
as a canonical form.
Exercise 14.2
Prove proposition 14.3.
Exercise 14.3
Prove that setting evidence in a canonical form can be done as shown in equation (14.6).
Exercise 14.4⋆
Describe a method that eﬃciently computes the covariance of any pair of variables X, Y in a calibrated
Gaussian clique tree.
Exercise 14.5⋆
Let X and Y be two sets of continuous variables, with |X| = n and |Y | = m. Let p(X) be an arbitrary
density, and let
p(Y | X) = N (Y | a + BX; C)

650
Chapter 14. Inference in Hybrid Networks
where a is a vector of dimension m, B is an m × n matrix, and C is an m × m matrix. Show that the
ﬁrst two moments of p(X, Y ) = p(X)p(Y | X) depend only on the ﬁrst two moments of p(X) and
not on the distribution p(X) itself.
Exercise 14.6⋆
Prove theorem 14.5.
Exercise 14.7
Prove the equality in equation (14.17).
Exercise 14.8
Prove equation (14.17).
Exercise 14.9⋆
Our derivation in section 14.4.1 assumes that p and Y = f(U) have the same scope U. Now, assume
that Scope[f] = U, whereas our distribution p has scope U, Z. We can still use the same method if we
deﬁne g(u, u′) = f(u), and integrate g. This solution, however, requires that we perform integration in
dimension |U ∪Z|, which is often much higher than |U|. Since the cost of numerical integration grows
with the dimension of the integrals, we can gain considerable savings by using only U to compute our
approximation.
In this exercise, you will use the interchangeability of the Gaussian and linear Gaussian representations to
perform integration in higher dimension.
a. For Z ∈Z, show how we can write Z as a linear combination of variables in X, with Gaussian noise.
b. Use this expression to write CCov[Z; Y ] as a function of the covariances CCov[Xi; Y ].
c. Put these results together in order to show how we can obtain a Gaussian approximation to p(Y, Z).
Exercise 14.10
In some cases, it is possible to decompose a nonlinear dependency Y
= f(X) into ﬁner-grained
dependencies.
For example, we may be able to decompose the nonlinear function f as f(X) =
g(g1(X1), g2(X2)), where X1, X2 ⊂X are smaller subsets of variables.
Show how this decomposition can be used in the context of linearizing the function f in several steps
rather than in a single step. What are the trade-oﬀs for this approach versus linearizing f directly?
Exercise 14.11⋆⋆
Show how to combine the EP-based algorithms for CLGs and for nonlinear CPDs to address CLGs where
discrete variables can have continuous parents.
Your algorithm should specify any constraints on the
message passing derived from the need to allow for valid M-projection.
Exercise 14.12⋆
Assume we have a CLG network with discrete variables ∆and continuous variables Γ. In this exercise,
we consider collapsed methods that sample the discrete variables and perform exact inference over the
continuous variables. Let ed denote the discrete evidence and ep the continuous evidence.
a. Given a set of weighted particles such as those described earlier, show how we can estimate the
expectation of a function f(Xi) for some Xi ∈Γ. For what functions f do you expect this analysis
to give you drastically diﬀerent answers from the “exact” CLG algorithm of section 14.3.4. (Ignore issues
of inaccuracies arising from sampling noise or insuﬃcient number of samples.)
b. Show how we can eﬃciently apply collapsed likelihood weighting, and show precisely how the impor-
tance weights are computed.
c. Now, consider a combined algorithm that generates a clique tree over ∆to generate particles xp[1],
. . ., xp[M] sampled exactly from P(∆| ed). Show the computation of importance weights in this
case. Explain the computational beneﬁt of this approach over doing clique tree inference over the
entire network.

15
Inference in Temporal Models
In chapter 6, we presented several frameworks that provide a higher-level representation lan-
guage. We now consider the issue of performing probabilistic inference relative to these rep-
resentations. The obvious approach is based on the observation that a template-based model
can be viewed as a generator of ground graphical models: Given a skeleton, the template-based
model deﬁnes a distribution over a ground set of random variables induced by the skeleton.
We can then use any of our favorite inference algorithms to answer queries over this ground
network. This process is called knowledge-based model construction, often abbreviated as KBMC.
knowledge-based
model
construction
However, applying this simple idea is far from straightforward.
First, these models can easily produce models that are very large, or even inﬁnite. Several
approaches can be used to reduce the size of the network produced by KBMC; most obviously,
given a set of ground query variables Y and evidence E = e, we can produce only the part
of the network that is needed for answering the query P(Y | e). In other words, our ground
network is a dynamically generated object, and we can generate only the parts that we need
for our current query. While this approach can certainly give rise to considerable savings in
certain cases, in many applications the network generated is still very large. Thus, we have
to consider whether our various inference algorithms scale to this setting, and what additional
approximations we must introduce in order to achieve reasonable performance.
Second, the ground networks induced by a template-based model can often be densely con-
nected; most obviously, both aggregate dependencies on properties of multiple objects and
relational uncertainty can give rise to dense connectivity. Again, dense connectivity causes diﬃ-
culties for all exact and most approximate inference algorithms, requiring algorithmic treatment.
Finally, these models give rise to new types of queries that are not easily expressible as
standard probabilistic queries. For example, we may want to determine the probability that
every person in our family tree has at least one close relative (for some appropriate deﬁnition of
“close”) with a particular disease. This query involves both universal and existential quantiﬁers;
while it can be translated into a ground-level conjunction (over people in the family tree) of
disjunctions (over their close relatives), this translation is awkward and gives rise to a query over
a very large number of variables.
The development of methods that address these issues is very much an open area of research.
In the context of general template-based models, the great variability of the models expressed in
these languages limits our ability to provide general-purpose solutions; the existing approaches
oﬀer only partial solutions whose applicability at the moment is somewhat limited. We therefore
do not review these methods in this book; see section 15.6 for some references. In the more

652
Chapter 15. Inference in Temporal Models
restricted context of temporal models, the networks have a uniform structure, and the set of
relevant queries is better established. Thus, more work has been done on this setting. In the
remainder of this chapter, we describe some of the exact and approximate methods that have
been developed for inference in temporal models.
15.1
Inference Tasks
We now move to the question of inference in a dynamic Bayesian network (DBN). As we
discussed, we can view a DBN as a “generator” for Bayesian networks for diﬀerent time intervals.
Thus, one might think that the inference task is solved. Once we generate a speciﬁc Bayesian
network, we can simply run the inference algorithm of our choice to answer any queries.
However, this view is overly simplistic in two diﬀerent respects.
First, the Bayesian networks generated from a DBN can be arbitrarily large. Second, the type
of reasoning we want to perform in a temporal setting is often diﬀerent from the reasoning
applicable in static settings. In particular, many of the reasoning tasks in a temporal domain
are executed online as the system evolves.
For example, a common task in temporal settings is ﬁltering (also called tracking): at any time
ﬁltering
point t, we compute our most informed beliefs about the current system state, given all of the
evidence obtained so far. Formally, let o(t) denote the observation at time t; we want to keep
track of P(X (t) | o(1:t)) (or of some marginal of this distribution over some subset of variables).
As a probabilistic query, we can deﬁne the belief state at time t to be:
belief state
σ(t)(X (t)) = P(X (t) | o(1:t)).
Note that the belief state is exponentially large in the number of unobserved variables in X. We
therefore will not, in general, be interested in the belief state in its entirety. Rather, we must ﬁnd
an eﬀective way of encoding and maintaining the belief state, allowing us to query the current
probability of various events of interest (for example, marginal distributions over smaller subsets
of variables).
The tracking task is the task of maintaining the belief state over time. A related task is the
prediction task: at time t, given the observations o(1:t), predict the distribution over (some
prediction
subset of) the variables at time t′ > t.
A third task, often called smoothing, involves computing the posterior probability of X (t)
smoothing
given all of the evidence o(1:T ) in some longer trajectory.
The term “smoothing” refers to
the fact that, in tracking, the evidence accumulates gradually. In cases where new evidence
can have signiﬁcant impact, the belief state can change drastically from one time slice to the
next. By incorporating some future evidence, we reduce these temporary ﬂuctuations. This
process is particularly important when the lack of the relevant evidence can lead to temporary
“misconceptions” in our belief state.
Example 15.1
In cases of a sensor failure (such as example 6.5), a single anomalous observation may not be enough
to cause the system to realize that a failure has occurred. Thus, the ﬁrst anomalous sensor reading,
at time t1, may cause the system to conclude that the car did, in fact, move in an unexpected
direction. It may take several anomalous observations to reach the conclusion that the sensor has
failed. By passing these messages backward, we can conclude that the sensor was already broken

15.2. Exact Inference
653
at t1, allowing us to discount its observation and avoid reaching the incorrect conclusion about the
vehicle location.
We note that smoothing can be executed with diﬀerent time horizons of evidence going forward.
That is, we may want to use all of the available evidence, or perhaps just the evidence from
some window of a few time slices ahead.
A ﬁnal task is that of ﬁnding the most likely trajectory of the system, given the evidence —
arg maxξ(0:T ) P(ξ(0:T ) | o(1:T )). This task is an instance of the MAP problem.

In all of these tasks, we are trying to compute answers to standard probabilistic queries.
Thus, we can simply use one of the standard inference algorithms that we described
earlier in this book. However, this type of approach, applied naively, would require us to
run inference on larger and larger networks over time and to maintain our entire history
of observations indeﬁnitely. Both of these requirements can be prohibitive in practice.
Thus, alternative solutions are necessary to avoid this potentially unbounded blowup in
the network size.
In the remainder of our discussion of inference, we focus mainly on the tracking task, which
presents us with many of the challenges that arise in other tasks. The solutions that we present
for tracking can generally be extended in a fairly straightforward way to other inference tasks.
15.2
Exact Inference
We now consider the problem of exact inference in DBNs. We begin by focusing on the ﬁltering
problem, showing how the Markovian independence assumptions underlying our representation
provide a simple recursive rule that does not require maintaining an unboundedly large repre-
sentation. We then show how this recursive rule corresponds directly to the upward pass of
inference in the unrolled network.
15.2.1
Filtering in State-Observation Models
We begin by considering the ﬁltering task for state-observation models. Our goal here is to
maintain the belief state σ(t)(X(t)) = P(X(t) | o(1:t)). As we now show, we can provide a
simple recursive algorithm for propagating these belief states, computing σ(t+1) from σ(t).
Initially, P(X(0)) is precisely σ(0). Now, assume that we have already computed σ(t)(X(t)).
To compute σ(t+1) based on σ(t) and the evidence o(t+1), we ﬁrst propagate the state forward:
σ(·t+1)(X(t+1))
=
P(X(t+1) | o(1:t))
=
X
X(t)
P(X(t+1) | X(t), o(1:t))P(X(t) | o(1:t))
=
X
X(t)
P(X(t+1) | X(t))σ(t)(X(t)).
(15.1)
In words, this expression is the beliefs over the state variables at time t+1, given the observations
only up to time t (as indicated by the · in the superscript). We can call this expression the prior
prior belief state
belief state at time t + 1. In the next step, we condition this prior belief state to account for the

654
Chapter 15. Inference in Temporal Models
t (1)(S(1))=
∑P(S(0))P(S(1)|S(0))
S(0)
s (1)(S(1))∝
t(S(1))P(o(1)|S(1))
t (2)(S(2))=
∑s (1)(S(1))P(S(2)|S(1))
S(1)
s (2)(S1)∝
t(S(2))P(o(2)|S(2))
S(0), S(1)
S(1)
O(1)
S(2)
O(2)
S(1), S(2)
Figure 15.1
Clique tree for HMM
most recent observation o(t+1):
σ(t+1)(X(t+1))
=
P(X(t+1) | o(1:t), o(t+1))
=
P(o(t+1) | X(t+1), o(1:t))P(X(t+1) | o(1:t))
P(o(t+1) | o(1:t))
=
P(o(t+1) | X(t+1))σ(·t+1)(X(t+1))
P(o(t+1) | o(1:t))
.
(15.2)
This simple recursive ﬁltering procedure maintains the belief state over time, without keeping
recursive ﬁlter
track of a network or a sequence of observations of growing length. To analyze the cost of
this operation, let N be the number of states at each time point and T the total number of
time slices. The belief-state forward-propagation step considers every pair of states s, s′, and
therefore it has a cost of O(N 2). The conditioning step considers every state s′ (multiplying it
by the evidence likelihood and then renormalizing), and therefore it takes O(N) time. Thus, the
overall time cost of the message-passing algorithm is O(N 2T). The space cost of the algorithm
is O(N 2), which is needed to maintain the state transition model.
15.2.2
Filtering as Clique Tree Propagation
The simple recursive ﬁltering process is closely related to message passing in a clique tree. To
understand this relationship, we focus on the simplest state-observation model — the HMM.
Consider the process of exact clique tree inference, applied to the DBN for an HMM (as shown
in ﬁgure 6.2). One possible clique tree for this network is shown in ﬁgure 15.1. Let us examine
the messages passed in this tree in a sum-product clique tree algorithm, taking the last clique
in the chain to be the root. In this context, the upward pass is also called the forward pass.
forward pass
The ﬁrst message has the scope S(1) and represents P
S(0) P(S(0))P(S(1) | S(0)) =
P(S(1)).
The next message, sent from the S(1), O(1) clique, also has the scope S(1), and
represents P(S(1))P(o(1) | S(1)) = P(S(1), o(1)). Note that, if we renormalize this message to
sum to 1, we obtain P(S(1) | o(1)), which is precisely σ(1)(S(1)). Continuing, one can verify
that the message from the S(1), S(2) clique is P(S(2), o(1)), and the one from the S(2), O(2)
clique is P(S(2), o(1), o(2)). Once again, if we renormalize this last message to sum to 1, we
obtain exactly P(S(2) | o(1), o(2)) = σ(2)(S(2)).
We therefore see that the forward pass of the standard clique-tree message passing algorithm
provides us with a solution to the ﬁltering problem. A slight variant of the algorithm gives us

15.2. Exact Inference
655
precisely the recursive update equations of equation (15.1) and (15.2). Speciﬁcally, assume that we
normalize the messages from the S(1), O(1) clique as they are sent, resulting in a probability
distribution. (As we saw in exercise 9.3, such a normalization step has no eﬀect on the belief
state computed in later stages, and it is beneﬁcial in reducing underﬂow.)1
It is not hard to see that, with this slight modiﬁcation, the sum-product message passing
algorithm results in precisely the recursive update equations shown here: The message pass-
ing step executed by the S(t), S(t+1) clique is precisely equation (15.1), whereas the message
passing step executed by the S(t+1), O(t+1) clique is precisely equation (15.2), with the division
corresponding to the renormalization step.
Thus, the upward (forward) pass of the clique tree algorithm provides a solution to the
ﬁltering task, with no need for a downward pass. Similarly, for the prediction task, we can use
essentially the same message-passing algorithm, but without conditioning on the (unavailable
future) evidence. In terms of the clique tree formulation, the unobserved evidence nodes are
barren nodes, and they can thus be dropped from the network; thus, the S(t), O(t) cliques
would simply disappear.
When viewed in terms of the iterative algorithm, the operation of
equation (15.2) would be eliminated.
For the smoothing task, however, we also need to propagate messages backward in time. Once
again, this task is clearly an inference task in the unrolled DBN, which can be accomplished
using a clique tree algorithm. In this case, messages are passed in both directions in the clique
tree. The resulting algorithm is known as the forward-backward algorithm. In this algorithm,
forward-backward
algorithm
the backward messages also have semantics. Assume that our clique tree is over the time slices
0, . . . , T. If we use the variable-elimination message passing scheme (without renormalization),
the backward message sent to the clique S(t), S(t+1) represents P(o((t+1):T ) | S(t+1)). If we
use the belief propagation scheme, the backward message sent to this clique represents the fully
informed (smoothed) distribution P(S(t+1) | o(1:T )). (See exercise 15.1.)
For the smoothing task, we need to keep enough information to reconstruct a full belief
state at each time t. Naively, we might maintain the entire clique tree at space cost O(N 2T).
However, by more carefully analyzing the role of cliques and messages, we can reduce this cost
considerably. Consider variable-elimination message passing; in this case, the cliques contain
only the initial clique potentials, all of which can be read from the 2-TBN template. Thus, we
can cache only the messages and the evidence, at a total space cost of O(NT). Unfortunately,
space requirements that grow linearly in the length of the sequence can be computationally
prohibitive when we are tracking the system for extended periods.
(Of course, some linear
growth is unavoidable if we want to remember the observation sequence; however, the size of
the state space N is usually much larger than the space required to store the observations.)
Exercise 15.2 discusses one approach to reducing this computational burden using a time-space
trade-oﬀ.
15.2.3
Clique Tree Inference in DBNs
The clique tree perspective provides us with a general algorithm for tracking in DBNs. To derive
the algorithm, let us consider the clique tree algorithm for HMMs more closely.
1. Indeed, in this case, the probability P(S(t), o(1:t)) generally decays exponentially at t grows.
Thus, without a
renormalization step, numerical underﬂow would be inevitable.

656
Chapter 15. Inference in Temporal Models
Although we can view the ﬁltering algorithm as performing inference on an unboundedly
large clique tree, we never need to maintain more than a clique tree over two consecutive time
slices. Speciﬁcally, we can create a (tiny) clique tree over the variables S(t), S(t+1), O(t+1); we
then pass in the message σ(t) to the S(t), S(t+1) clique, pass the message to the S(t+1), O(t+1)
clique, and extract the outgoing message σ(t+1). We can now forget this time slice’s clique tree
and move on to the next time slice’s.
It is now apparent that the clique trees for all of the time slices are identical — only the
messages passed into them diﬀer. Thus, we can perform this propagation using a template
template clique
tree
clique tree Υ over the variables in the 2-TBN. In this setting, Υ would contain the two cliques
{S, S′} and {S′, O′}, initialized with the potentials P(S′ | S) and P(O′ | S′) respectively.
To propagate the belief state from time t to time t + 1, we pass the time t belief state into
Υ, by multiplying it into the clique P(S′ | S), taking S to represent S(t) and S′ to represent
S(t+1). We then run inference over this clique tree, including conditioning on O′ = o(t+1). We
can now extract the posterior distribution over S′, which is precisely the required belief state
P(S(t+1) | o(1:(t+1))). This belief state can be used as the input message for the next step of
propagation.
The generalization to arbitrary DBNs is now fairly straightforward. We maintain a belief state
σ(t)(X (t)) and propagate it forward from time t to time t + 1. We perform this propagation
step using clique tree inference as follows: We construct a template clique tree Υ, deﬁned over
the variables of the 2-TBN. We then pass a time t message into Υ, and we obtain as the result
of clique tree inference in Υ a time t + 1 message that can be used as the input message for
the next step. Most naively, the messages are full belief states, specifying the distribution over
all of the unobserved variables.
In general, however, we can often reduce the scope of the message passed: As can be seen
from equation (6.2), only the time t interface variables are relevant to the time t+1 distribution.
For example, consider the two generalized HMM structures of ﬁgure 6.3. In the factorial HMM
structure, all variables but the single observation variable are in the interface, providing little
savings. However, in the coupled HMM, all of the private observation variables are not in the
interface, leaving a much smaller belief state whose scope is X1, X2, X3. Observation variables
are not the only ones that can be omitted from the interface; for example, in the network of
ﬁgure 15.3a, the nonpersistent variable B is also not in the interface.
The algorithm is shown in algorithm 15.1. It passes messages corresponding to reduced belief
reduced belief
state
states σ(t)(X (t)
I ). At phase t, it passes a time t −1 reduced belief state into the template clique
tree, calibrates it, and extracts the time t reduced belief state to be used as input for the next
step. Note that the clique-tree calibration step is useful not only for the propagation step. It
also provides us with other useful conclusions, such as the marginal beliefs over all individual
variables X(t) (and some subsets of variables) given the observations o(1:t).
15.2.4
Entanglement
The use of a clique tree immediately suggests that we are exploiting structure in the algorithm,
and so can expect the inference process to be tractable, at least in a wide range of situations.
Unfortunately, that does not turn out to be the case.
The problem arises from the need
to represent and manipulate the (reduced) belief state σ(t) (a process not speciﬁed in the
algorithm). Semantically, this belief state is a joint distribution over X (t)
I ; if represented naively,

15.2. Exact Inference
657
Algorithm 15.1 Filtering in a DBN using a template clique tree
Procedure CTree-Filter-DBN (
⟨B0, B→⟩,
// DBN
o(1), o(2), . . .
// Observation sequence
)
1
Construct template clique tree Υ over XI ∪X ′
2
σ(0) ←PB0(X (0)
I
)
3
for t = 1, 2, . . .
4
T (t) ←Υ
5
Multiply σ(t−1)(X (t−1)
I
) into T (t)
6
Instantiate T (t) with o(t)
7
Calibrate T (t) using clique tree inference
8
Extract σ(t)(X (t)
I ) by marginalization
it would require an exponential number of entries in the joint. At ﬁrst glance, this argument
appears specious. After all, one of the key beneﬁts of graphical models is that high-dimensional
distributions can be represented compactly by using factorization. It certainly appears plausible
that we should be able to ﬁnd a compact representation for our belief state and use our
structured inference algorithms to manipulate it eﬃciently. As we now show, this very plausible
impression turns out to be false.
Example 15.2
Consider our car network of ﬁgure 6.1, and consider our belief state at some time t. Intuitively,
it seems as if there should be some conditional independence relations that hold in this network.
For example, it seems as if Weather(2) and Location(2) should be uncorrelated. Unfortunately,
they are not: if we examine the unrolled DBN, we see that there is an active trail between them
going through Velocity(1) and Weather(0), Weather(1). This path is not blocked by any of the time
2 variables; in particular, Weather(2) and Location(2) are not conditionally independent given
Velocity(2). In general, a similar analysis can be used to show that, for t ≥2, no conditional
independence assumptions hold in σ(t).
This phenomenon, known as entanglement, has signiﬁcant implications. As we discussed, there
entanglement
is a direct relationship between conditional independence properties of a distribution and our
ability to represent it as a product of factors. Thus, a distribution that has no independence
properties does not admit a compact representation in a factored form.
Unfortunately, the entanglement phenomenon is not speciﬁc to this example. Indeed, it holds
for a very broad class of DBNs. We demonstrate it for a large subclass of DBNs that exhibit a
very regular structure. We begin by introducing a few useful concepts.
Deﬁnition 15.1
For a DBN over X, and X, Y , Z ⊂X, we say that the independence (X ⊥Y | Z) is persistent
persistent
independence
if (X(t) ⊥Y (t) | Z(t)) holds for every t.
Persistent independencies are independence properties of the belief state, and are therefore
precisely what we need in order to provide a time-invariant factorization of the belief state.

658
Chapter 15. Inference in Temporal Models
The following concept turns out to be a useful one, in this setting and others.
Deﬁnition 15.2
Let B→be a 2-TBN over X. We deﬁne the inﬂuence graph for B→to be a directed cyclic graph
inﬂuence graph
I over X whose nodes correspond to X, and that contains a directed arc X →Y if X →Y ′ or
X′ →Y ′ appear in B→. Note that a persistence arc X →X′ induces a self-cycle in the inﬂuence
graph.
The inﬂuence graph corresponds to inﬂuence in the unrolled DBN:
Proposition 15.1
Let I be the inﬂuence graph for a 2-TBN B→. Then I contains a directed path from X to Y if and
only if, in the unrolled DBN, for every t, there exists a path from X(t) to Y (t′) for some t′ ≥t.
See exercise 15.3.
The following result demonstrates the inevitability of the entanglement phenomenon, by
proving that it holds in a broad class of networks. A DBN is called fully persistent if it encodes
fully persistent
a state-observation model, and, for each state variable X ∈X, the 2-TBN contains a persistence
persistence edge
edge X →X′.
Theorem 15.1
Let ⟨G0, G→⟩be a fully persistent DBN structure over X = X ∪O, where the state variables X(t)
are hidden in every time slice, and the observation variables O(t) are observed in every time slice.
Furthermore, assume that, in the inﬂuence graph for G→:
• there is a trail (not necessarily a directed path) between every pair of nodes, that is, the graph
is connected;
• every state variable X has some directed path to some evidence variable in O.
Then there is no persistent independence (X ⊥Y | Z) that holds for every DBN ⟨B0, B→⟩over
this DBN structure.
The proof is left as an exercise (see exercise 15.4). Note that, as in every other setting, there may
be spurious independencies that hold due to speciﬁc choices of the parameters. But, for almost
all choices of the parameters, there will be no independence that holds persistently.

In fully persistent DBNs, the tracking problem is precisely one of maintaining a belief
state — a distribution over X(t). The entanglement theorem shows that the only exact
representation for this belief state is as a full joint distribution, rendering any belief-
state-algorithm computationally infeasible except in very small networks.
More generally, if we want to track the system as it evolves, we need to maintain a repre-
sentation that summarizes all of our information about the past. Speciﬁcally, as we showed in
theorem 10.2, any sepset in a clique tree must render the two parts of the tree conditionally
independent. In a temporal setting, we cannot allow the sepsets to grow unboundedly with
the number of time slices. Therefore, there must exist some sepset over a scope Y that cuts
across the network, in that any path that starts from a time 0 variable and continues to inﬁnity
must intersect Y. In fully persistent networks, the set of state variables X(t) is a minimal set
satisfying this condition. The entanglement theorem states that this set exhibits no persistent in-
dependencies, and therefore the message over this sepset can only be represented as an explicit
joint distribution. The resulting sepsets are therefore very large — exponential in the number
of state variables. Moreover, as these large messages must be incorporated into some clique in
the clique tree, the cliques also become exponentially large.

15.2. Exact Inference
659
W, V, L, F
F'
(a)
(b)
W, V, L
L', F'
W, V
V', L', F'
W
W', V', L', F'
W, W'
W, F, F', L'
W, V,  V', L, L'
s(W', V', L', F')
P(F' |F,W)
P(o' |L',F')
P(L' |L,V)
P(o' |L',F')
P(L' |L,V)
P(V' |V, W)
P(V' |V, W)
P(W' |W)
P(W' |W)
P(F' |F,W)
s(W, V, L, F)
Figure 15.2
Diﬀerent clique trees for the Car DBN of ﬁgure 6.1. (a) A template clique tree that allows
exact ﬁltering for the Car DBN of ﬁgure 6.1. Because the variable O′ is always observed, it is not in the
scope of any clique (although the factor P(o′ | F ′, L′) is associated with a clique). (b) A clique tree for
the 2-TBN of the Car network, which does not allow exact ﬁltering.
Example 15.3
Consider again the Car network of ﬁgure 6.1. To support exact belief state propagation, our template
clique tree must include a clique containing W, V, L, F, where we can incorporate the previous
belief state σ(t)(W (t), V (t), L(t), F (t)). It must also contain a clique containing W ′, V ′, L′, F ′,
from which we can extract σ(t+1)(W (t+1), V (t+1), L(t+1), F (t+1)). A minimally sized clique tree
containing these cliques is shown in ﬁgure 15.2a. All of the cliques in the tree are of size 5. By
contrast, if we were simply to construct a template clique tree over the dependency structure deﬁned
by the 2-TBN, we could obtain a clique tree where the maximal clique size is 4, as illustrated in
ﬁgure 15.2b.
A clique size of 4 is the minimum we can hope for: In general, all of the cliques for a fully
persistent network over n variables contain at least n + 1 variables: one representative (either X or
X′) of each variable X in X, plus an additional variable that we are currently eliminating. (See
exercise 15.5.) In many cases, the minimal induced width would actually be higher. For example, if
we introduce an arc L →V ′ into our 2-TBN (for example, because diﬀerent locations have diﬀerent
speed limits), the smallest template clique tree allowing for exact ﬁltering has a clique size of 6.
Even in networks when not all variables are persistent, entanglement is still an issue: We still
need to represent a distribution that cuts across the entire width of the network. In most cases

660
Chapter 15. Inference in Temporal Models
A', B', C, C'
A, C, A', B'
(a)
Time slice t
Time slice t + 1
(b)
(c)
A
C
A'
B'
C'
D'
B', C, C'
A, A', B'
Figure 15.3
Nonpersistent 2-TBN and diﬀerent possible clique trees: (a) A 2-TBN where not all of the
unobserved variables are persistent. (b) A clique tree for this 2-TBN that allows exact ﬁltering; as before, D′
is always observed, and hence it is not in the scope of any clique. (c) A clique tree for the 2-TBN, which
does not allow exact ﬁltering.
— except for speciﬁc structures and observation patterns — these distributions do not exhibit
independence structure, and they must therefore be represented as an explicit joint distribution
over the interface variables. Because the “width” of the network is often fairly large, this results
in large messages and large cliques.
Example 15.4
Consider the 2-TBN shown in ﬁgure 15.3, where not all of the unobserved variables are persistent. In
this network, our interface variables are A, C. Thus, we can construct a template clique tree over
A, C, A′, B′, C′, D′ as the basis for our message passing step. To allow exact ﬁltering, we must
have a clique whose scope contains A, C and a clique whose scope contains A′, C′. A minimally
sized clique tree satisfying these constraints is shown in ﬁgure 15.3b. It has a maximum clique size
of 4; without these constraints, we can construct a clique tree where the maximal clique size is 3
(ﬁgure 15.3c).
We note that it is sometimes possible to ﬁnd better clique trees than those constrained to
use X (t)
I
as the message. In fact, in some cases, the best sepset actually spans variables in
multiple time slices. However, these improvements do not address the fundamental problem,
which is that the computational cost of exact inference in DBNs grows exponentially with the
“width” of the network. Speciﬁcally, we cannot avoid including in our messages at least the
set of persistent variables. In many applications, a large fraction of the variables are persistent,
rendering this approach intractable.

15.3. Approximate Inference
661
15.3
Approximate Inference
The computational problems with exact inference force us, in many cases, to fall back on
approximate inference. In principle, when viewing the DBN as a large unrolled BN, we can
apply any of the approximate inference algorithms that we discussed for BNs. Indeed, there
has been signiﬁcant success, for example, in using a variational approach for reasoning about
weakly coupled processes that evolve in parallel. (See exercise 15.6.)
However, several complications arise in this setting. First, as we discussed in section 15.1, the
types of tasks that we wish to address in the temporal setting often involve reasoning about
arbitrarily large, and possibly unbounded, trajectories. Although we can always address these
tasks using inference over the unrolled DBNs, as in the case of exact inference, algorithms that
require us to maintain the entire unrolled BN during the inference process may be impractical.
In the approximate case, we must also address an additional complication: An approximate
inference algorithm that achieves reasonable errors for static networks of bounded size may not
work well in arbitrarily large networks. Indeed, the quality of the approximation may degrade
with the size of the network.
For the remainder of this section, we focus on the ﬁltering task. As in the case of exact
inference, methods developed for ﬁltering extend directly to prediction, and (with a little work)
to smoothing with a bounded lookahead. There are many algorithms that have been proposed
for approximate tracking in dynamic systems, and one could come up with several others based
on the methods described earlier in this book. We begin by providing a high-level overview of a
general framework that encompasses these algorithms. We then describe two speciﬁc methods
that are commonly used in practice, one that uses a message passing approach and the other a
sampling approach.
15.3.1
Key Ideas
15.3.1.1
Bounded History Updates
A general approach to addressing the ﬁltering (or prediction) task without maintaining a full
history is related to the approach we used for exact ﬁltering. There, we passed messages in the
clique tree forward in time, which allowed us to throw away the observations and the cliques in
previous time slices once they have been processed.
In principle, the same idea can be applied in the case of approximate inference: We can
execute the appropriate inference steps for a time slice and then move forward to the next
time slice.
However, most approximate inference algorithms require that the same variable
in the network be visited multiple times during the course of inference. For example, belief
propagation algorithms (as in section 11.3 or section 11.4) send multiple messages through the
same cluster. Similarly, structured variational approximation methods (as in section 11.5) are
also iterative, running inference on the network multiple times, with diﬀerent values for the
variational parameters. Markov chain Monte Carlo algorithms also require that each node be
visited and sampled multiple times. Thus, we cannot just throw away the history and apply our
approximate inference to the current time slice alone.
One common solution is to use a form of “limited history” in the inference steps. The various
steps associated with the inference algorithm are executed not over the whole network, but
over a subnetwork covering only the recent history. Most simply, the subnetwork for time t

662
Chapter 15. Inference in Temporal Models
is simply a bounded window covering some predetermined number k of previous time slices
t −k, . . . , t −1, t. More generally, the subnetwork can be determined in a dynamic fashion,
using a variety of techniques.
We will describe the two methods most commonly used in practice, one based on importance
sampling, and the other on approximate message propagation. Both take k = 1, using only
the current approximate belief state ˆσ(t) and the current time slice in estimating the next
approximate belief state ˆσ(t+1). In eﬀect, these methods perform a type of approximate message
propagation, as in equation (15.1) and equation (15.2). Although this type of approximation is
clearly weak in some cases, it turns out to work fairly well in practice.
15.3.1.2
Analysis of Convergence
The idea of running approximate inference with some bounded history over networks of increas-
ing size immediately raises concerns about the quality of the approximation obtained. Consider,
for example, the simplest approximate belief-state ﬁltering process. Here, we maintain an ap-
proximate belief state ˆσ(t), which is (hopefully) similar to our true belief state σ(t). We use
ˆσ(t) to compute the subsequent belief state ˆσ(t+1). This step uses approximate inference and
therefore introduces some additional error into our approximation. Therefore, as time evolves,
our approximation appears to be accumulating more and more errors. In principle, it might be
the case that, at some point, our approximate belief state ˆσ(t) bears no resemblance to the true
belief state σ(t).

Although unbounded errors can occur, it turns out that such situations are rare in
practice (for algorithms that are carefully designed). The main reason is that the dynamic
system itself is typically stochastic. Thus, the eﬀect of approximations that occur far in
the past tends to diminish over time, and the overall error (for well-designed algorithms)
tends to remain bounded indeﬁnitely. For several algorithms (including the two described in
more detail later), one can prove a formal result along these lines. All of these results make
some assumptions about the stochasticity of the system — the rate at which it “forgets” the
past. Somewhat more formally, assume that propagating two distributions through the system
dynamics (equation (15.1) and 15.2) reduces some notion of distance between them. In this case,
discrepancies between ˆσ(t) and σ(t), which result from approximations in previous time slices,
decay over time. Of course, new errors are introduced by subsequent approximations, but, in
stochastic systems, we can show that they do not accumulate unboundedly.
Formal theorems proving a uniform bound on the distance between the approximate and true
belief state — a bound that holds for all time points t — exist for a few algorithms. These
theorems are quite technical, and the actual bounds obtained on the error are fairly large. For
this reason, we do not present them here. However, in practice, when the underlying system
is stochastic, we do see a bounded error for approximate propagation algorithms. Conversely,
when the system evolution includes a deterministic component — for example, when the state
contains a variable that (once chosen) does not evolve over time — the errors of the approximate
inference algorithms often do diverge over time. Thus, while the speciﬁc bounds obtain in the
theoretical analyses may not be directly useful, they do provide a theoretical explanation for the
behavior of the approximate inference algorithms in practice.

15.3. Approximate Inference
663
15.3.2
Factored Belief State Methods

The issue underlying the entanglement result is that, over time, all variables in a belief
state slice eventually become correlated via active trails through past time slices.
In
many cases, however, these trails can be fairly long, and, as a consequence, the resulting
correlations can be quite weak. This raises the idea of replacing the exact, fully correlated,
belief state, with an approximate, factorized belief state that imposes some independence
assumptions. For a carefully chosen factorization structure, these independence assumptions
may be a reasonable approximation to the structure in the belief state.
This idea gives rise to the following general structure for a ﬁltering algorithm: At each time
point t, we have a factored representation ˆσ(t) of our time t belief state. We then compute the
correct update of this time t belief state to produce a new time t + 1 belief state σ(·t+1). The
update step consists of propagating the belief state forward through the system dynamics and
conditioning on the time t + 1 observations. Owing to the correlations induced by the system
dynamics (as in section 15.2.4), σ(·t+1) has more correlations than ˆσ(t), and therefore it requires
larger factors to represent correctly. If we continue this process, we rapidly end up with a belief
state that has no independence structure and must be represented as a full joint distribution.
Therefore, we introduce a projection step, where we approximation σ(·t+1) using a more factored
belief-state
projection
representation, giving rise to a new ˆσ(t+1), with which we continue the process. This update-
project cycle ensures that our approximate belief state remains in a class of distributions that
we can tractably maintain and update.
Most simply, we can represent the approximate belief state ˆσ(t) in terms of a set of factors
Φ(t) = {β(t)
r (X(t)
r )}, where we assume (for simplicity) that the factorization of the messages
(that is, the choice of scopes Xr) does not change over time. Most simply, the scopes of the
diﬀerent factors are disjoint, in which case the belief state is simply a product of marginals over
disjoint variables or subsets of variables. As a richer but more complex representation, we can
represent ˆσ(t) using a calibrated cluster tree, or even a calibrated cluster graph U. Indeed, we
can even use a general representation that uses overlapping regions and associated counting
numbers:
ˆσ(t)(X (t)) =
Y
r
(β(t)
r (X(t)
r ))κr.
Example 15.5
Consider the task of monitoring a freeway with k cars. As we discussed, after a certain amount
of time, the states of the diﬀerent cars become entangled, so our only option for representing the
belief state is as a joint distribution over the states of all the cars. An obvious approximation is to
assume that the correlations between the diﬀerent cars are not very strong. Thus, although the cars
do inﬂuence each other, the current state of one car does not tell us too much about the current
state of another. Thus, we can choose to approximate the belief state over the entire system using
an approximate belief state that ignores or approximates these weak correlations. Speciﬁcally, let
Y i be the set of variables representing the state of car i, and let Z be a set of variables that encode
global conditions, such as the weather or the current traﬃc density. Most simply, we can represent
the belief state ˆσ(t) as a product of marginals
β(t)
g (Z(t))
k
Y
i=1
β(t)
i (Y (t)
i ).

664
Chapter 15. Inference in Temporal Models
In a better approximation, we might preserve the correlations between the state of each individual
vehicle and the global system state, by selecting as our factorization

β(t)
g (Z(t))
−(k−1)
k
Y
i=1
β(t)
i (Z(t), Y (t)
i ),
where the initial term compensates for the multiple counting of the probability of Z(t) in the other
factors. Here, the representation of the approximate belief state makes the assumption that the states
of the diﬀerent cars are conditionally independent given the global state variables.
We showed that exact ﬁltering is equivalent to a forward pass of message passing in a clique
tree, with the belief states playing the role of messages. Hence, ﬁltering with factored belief states
is simply a form of message passing with approximate messages. The use of an approximate
belief state in a particular parametric class is also known as assumed density ﬁltering. This
assumed density
ﬁlter
algorithm is a special case of the more general algorithm that we developed in the context of
the expectation propagation (EP) algorithm of section 11.4. Viewed abstractly, each slice-cluster in
expectation
propagation
our monolithic DBN clique tree (one that captures the entire trajectory) corresponds to a pair
of adjacent time slices t −1, t and contains the variables X (t)
I
∪X (t+1). As in the general EP
algorithm, the potential in a slice-cluster is never represented explicitly, but in a decomposed
form, as a product over factors. Each slice-cluster is connected to its predecessor and successor
slice-clusters. The messages between these slice-clusters correspond to approximate belief states
ˆσ(t)(X (t)
I ), which are represented in a factorized form. For uniformity of exposition, we assume
that the initial state distribution — the time 0 belief state — also takes (or is approximated
as) the same form. Thus, when propagating messages in this chain, each slice-cluster takes
messages in this factorized form and produces messages in this form.
As we discussed in section 11.4.2, the use of factorized messages allows us to perform the
operations in each cluster much more eﬃciently, by using a nested clique tree or cluster graph
that exploits the joint structure of the messages and cluster potential. For example, if the belief
state is fully factored as a product over the variables in the interface, the message structure
imposes no constraints on the nested data structure used for inference within a time slice. In
particular, we can use any clique tree over the 2-TBN structure; for instance, in example 15.3,
we can use the structure of ﬁgure 15.2b. By contrast, for exact ﬁltering, the messages are full
belief states over the interface variables, requiring the use of a nested clique tree with very large
cliques. Of course, a fully factorized belief state generally provides a fairly poor approximation to
the belief state. As we discussed in the context of the EP algorithm, we can also use much more
reﬁned approximations, which use a clique tree or even a general region-based approximation
to the belief state.
The algorithm used for the message passing is precisely as we described in section 11.4.2, and
we do not repeat it here. We make only three important observations. First, unlike a traditional
application of EP, when doing ﬁltering, we generally do only a single upward pass of message
propagation, starting at time 0 and propagating toward higher time slices. Because we do not
have a backward pass, the distinctions between the sum-product algorithm (section 11.4.3.1) and
the belief update algorithm (section 11.4.3.2) are irrelevant in this setting, since the diﬀerence
arises only in the backward pass. Second, without a backward pass, we do not need to keep
track of a clique once it has propagated its message forward. Thus, as in exact inference for

15.3. Approximate Inference
665
DBNs, we can keep only a single (factored) message and single (factored) slice-cluster in memory
at each point in time and perform the message propagation in space that is constant in the
number of time slices.
If we continue to assume that the belief state representation is the same for every time slice,
then the factorization structure used in each of the message passing steps is identical. In this
case, we can perform all the message passing steps using the same template cluster graph that
template cluster
graph
has a ﬁxed cluster structure and ﬁxed initial factors (those derived from the 2-TBN); at each time
t, the factors representing ˆσ(t) are introduced into the template cluster graph, which is then
calibrated and used to produce the factors representing ˆσ(t+1). The reuse of the template can
reduce the cost of the message propagation step. An alternative approach allows the structure
used in our approximation to change over time. This ﬂexibility allows us to adapt our structure
to reﬂect the strengths of the interactions between the variables in our domain. For example,
in example 15.5, we might expect the variables associated with cars that are directly adjacent
to be highly correlated; but the pairs of cars that are close to each other change over time.
Section 15.6 describes some methods for dynamically adapting the representation to the current
distribution.
15.3.3
Particle Filtering
Of the diﬀerent particle-based methods that we discussed, forward sampling appears best suited
to the temporal setting, since it generates samples incrementally, starting from the root of the
network. In the temporal setting, this would correspond to generating trajectories starting from
the beginning of time, and going forward.
This type of sampling, we might hope, is more
amenable to a setting where we do not have to keep sampled trajectories that go indeﬁnitely
far back. Obviously, rejection sampling is not an appropriate basis for a temporal sampling
algorithm. For an indeﬁnitely long trajectory, all samples will eventually be inconsistent with
our observations, so we will end up rejecting all samples. In this section, we present a family of
ﬁltering algorithms based on importance sampling and analyze their behavior.
15.3.3.1
Naive Likelihood Weighting
It is fairly straightforward to generalize likelihood weighting to the temporal setting.
Recall
that LW generates samples by sampling nodes that are not observed from their appropriate
distribution, and instantiating nodes that are observed to their observed values. Every node
that is instantiated in this way causes the weight of the sample to be changed. However, LW
generates samples one at a time, starting from the root and continuing until a full assignment
is generated. In an online setting, we can have arbitrarily many variables, so there is no natural
end to this sampling process. Moreover, in the ﬁltering problem, we want to be able to answer
queries online as the system evolves. Therefore, we ﬁrst adapt our sampling process to return
intermediate answers.
The LW algorithm for the temporal setting maintains a set of samples, each of which is
a trajectory up to the current time slice t: ξ(t)[1], . . . , ξ(t)[M].
Each sampled trajectory is
associated with a weight w[m]. At each time slice, the algorithm takes each of the samples,
propagates it forward to sample the variables at time t, and adjusts its weight to suit the new
evidence at time t.
The algorithm uses a likelihood-weighting algorithm as a subroutine to

666
Chapter 15. Inference in Temporal Models
propagate a time t sample to time t + 1. The version of the algorithm for 2-TBNs is almost
identical to the algorithm 12.2; it is shown in algorithm 15.2 primarily as a reminder.
Algorithm 15.2 Likelihood-weighted particle generation for a 2-TBN
Procedure LW-2TBN (
B→
// 2-TBN
ξ
// Instantiation to time t −1 variables
O(t) = o(t)
// time t evidence
)
1
Let X′
1, . . . , X′
n be a topological ordering of X ′ in B→
2
w ←1
3
for i = 1, . . . , n
4
ui ←(ξ, x′)⟨PaX′
i⟩
5
// Assignment to PaX′
i in x1, . . . , xn, x′
1, . . . , x′
i−1
6
if X′
i ̸∈O(t) then
7
Sample x′
i from P(X′
i | ui)
8
else
9
x′
i ←o(t)⟨X′
i⟩
// Assignment to X′
i in o(t)
10
w ←w · P(x′
i | ui)
// Multiply weight by probability of desired value
11
return (x′
1, . . . , x′
n), w
Algorithm 15.3 Likelihood weighting for ﬁltering in DBNs
Procedure LW-DBN (
⟨B0, B→⟩,
// DBN
M
// Number of samples
o(1), o(2), . . .
// Observation sequence
)
1
for m = 1, . . . , M
2
Sample ξ(0)[m] from B0
3
w[m] ←1
4
for t = 1, 2, . . .
5
for m = 1, . . . , M
6
(ξ(t)[m], w) ←LW-2TBN(B→, ξ(t−1)[m], o(t))
7
// Sample time t variables starting from time t −1 sample
8
w[m] ←w[m] · w
9
// Multiply weight of m’th sample with weight of time t evi-
dence
10
ˆσ(t)(ξ) ←
PM
m=1 w[m]11{ξ(t)[m]=ξ}
PM
m=1 w[m]
Unfortunately, this extension of the basic LW algorithm is generally a very poor algorithm for
DBNs. To understand why, consider the application of this algorithm to any state-observation

15.3. Approximate Inference
667
[t]
 25 samples
 100 samples
 1000 samples
 10000 samples
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Avg absolute error
Time step
0
Figure 15.4
Performance of likelihood weighting over time with diﬀerent numbers of samples, for a
state-observation model with one state and one observation variable.
model. In this case, we have a very long network, where all of the evidence is at the leaves.
Unfortunately, as we discussed, in such networks, LW generates samples according to the prior
distribution, with the evidence aﬀecting only the weights. In other words, the algorithm generates
completely random state trajectories, which “match” the evidence only by chance. For example,
in our Car example, the algorithm would generate completely random trajectories for the car, and
check whether one of them happens to match the observed sensor readings for the car’s location.
Clearly, the probability that such a match occurs — which is precisely the weight of the sample
— decreases exponentially (and quite quickly) with time. This problem can also arise in a static
BN, but it is particularly severe in this setting, where the network size grows unboundedly. In
this case, as time evolves, more and more evidence is ignored in the sample-generation process
(aﬀecting only the weight), so that the samples become less and less relevant. Indeed, we can
see in ﬁgure 15.4 that, in practice, the samples generated get increasingly irrelevant as t grows,
so that LW diverges rapidly as time goes by. From a technical perspective, this occurs because,
over time, the variance of the weights of the samples grows very quickly, and unboundedly.
Thus, the quality of the estimator obtained from this procedure — the probability that it returns
an answer within a certain error tolerance — gets increasingly worse.
One approach called particle ﬁltering (or sequential importance sampling) for addressing this
particle ﬁlter
sequential
importance
sampling
problem is based on the key observation that not all samples are equally “good.” In particular,
samples that have higher weight explain the evidence observed so far much better, and

are likely to be closer to the current state. Thus, rather than propagate all samples forward
to the next time step, we should preferentially select “good” samples for propagation,
where “good” samples are ones that have high weight. There are many ways of implementing
this basic intuition: We can select samples for propagation deterministically or stochastically.

668
Chapter 15. Inference in Temporal Models
We can use a ﬁxed number of samples, or vary the number of samples to achieve a certain
quality of approximation (estimated heuristically).
15.3.3.2
The Bootstrap Filter
The simplest and most common variant of particle ﬁltering is called the bootstrap ﬁlter.
It
bootstrap ﬁlter
maintains a set D(t) of M time t trajectories ¯x(0:t)[m], each associated with its own weight
w(t)[m]. When propagating samples to the next time slice, each sample is chosen randomly
for propagation, proportionately to its current weight. The higher the weight of the sample,
the more likely it is to be selected for propagation; thus, higher-weight samples may “spawn”
multiple copies, whereas lower-weight ones “die oﬀ” to make space for the others.
More formally, consider a data set D(t) consisting of M weighted sample trajectories (¯x(0:t)[m],
w(t)[m]). We can deﬁne the empirical distribution generated by the data set:
ˆPD(t)(x(0:t)) ∝
M
X
m=1
w(t)[m]11{¯x(0:t)[m] = x(0:t)}.
This distribution is a weighted sum of delta distributions, where the probability of each assign-
ment is its total weight in D(t), renormalized to sum to 1.
The algorithm then generates M new samples for time t + 1 as follows: For each sample m,
it selects a time t sample for propagation by randomly sampling from ˆPD(t). Each of the M
selected samples is used to generate a new time t + 1 sample using the transition model, which
is weighted using the observation model. Note that the weight of the sample w(t)[m] manifests
in the relative proportion with which the mth sample is propagated. Thus, we do not need to
account for its previous weight when determining the weight of the time t + 1 sample generated
from it. If we did include its weight, we would eﬀectively be double-counting it. The algorithm
is shown in algorithm 15.4 and illustrated in ﬁgure 15.5.
We can view ˆPD(t) as an approximation to the time t belief state (one where only the sampled
states have nonzero probability), and the sampling step as using it to generate an approximate
belief state for time t+1. Thus, this algorithm can be viewed as performing a stochastic version
of the belief-state ﬁltering process.
Note that we view the algorithm as maintaining entire trajectories ¯x(0:t), rather than simply
the current state.
In fact, each sample generated does correspond to an entire trajectory.
However, for the purpose of ﬁltering, the earlier parts of the trajectory are not relevant, and we
can throw out all but the current state ¯x(t).
The bootstrap particle ﬁlter works much better than likelihood weighting, as illustrated in
ﬁgure 15.6a. Indeed, the error seems to remain bounded indeﬁnitely over time (b).
We can generalize the basic bootstrap ﬁlter along two dimensions. The ﬁrst modiﬁes the
forward sampling procedure — the process by which we extend a partial trajectory ¯x(0:t−1)
to include a time t state variable assignment ¯x(t). The second modiﬁes the particle selection
scheme, by which we take a set of weighted time t samples D(t) and use their weights to select
a new set of time t samples. We will describe these two extensions in more detail.

15.3. Approximate Inference
669
Figure 15.5
Illustration of the particle ﬁltering algorithm. (Adapted with permission from van der
Merwe et al. (2000a).) At each time slice, we begin with a set of weighted samples (dark circles), we sample
from them to generate a set of unweighted samples (light circles). We propagate each sample forward
through the system dynamics, and we update the weight of each sample to reﬂect the likelihood of the
evidence (black line), producing a new set of weighted samples (some of which have weight so small as to
be invisible). The process then repeats for the next time slice.
15.3.3.3
Sequential Importance Sampling
We can generalize our forward sampling process by viewing it in terms of importance sampling,
as in section 12.2.2. Here, however, we are sampling entire trajectories rather than static states.
Our goal is to sample a trajectory ¯x(0:t) from the distribution P(x(0:t) | o(0:t)).
To use
importance sampling, we must construct a proposal distribution α for trajectories and then
use importance weights to correct for the diﬀerence between our proposal distribution and our
target distribution.
To maintain the ability to execute our ﬁltering algorithm in an online fashion, we must
construct our proposal distribution so that trajectories are constructed incrementally. Assume
that, at time t, we have sampled some set of partial trajectories D(t), each possibly associated
with some weight. If we want to avoid the need to maintain full trajectories and a full observation

670
Chapter 15. Inference in Temporal Models
Algorithm 15.4 Particle ﬁltering for DBNs
Procedure Particle-Filter-DBN (
⟨B0, B→⟩,
// DBN
M
// Number of samples
o(1), o(2), . . .
// Observation sequence
)
1
for m = 1, . . . , M
2
Sample ¯x(0)[m] from B0
3
w(0)[m] ←1/M
4
for t = 1, 2, . . .
5
for m = 1, . . . , M
6
Sample ¯x(0:t−1) from the distribution ˆPD(t−1).
7
// Select sample for propagation
8
(¯x(t)[m], w(t)[m]) ←LW-2TBN(B→, ¯x(t−1), o(t))
9
// Generate time t sample and weight from selected sample
¯x(t−1)
10
D(t) ←{(¯x(0:t)[m], w(t)[m]) : m = 1, . . . , M}
11
ˆσ(t)(x) ←
ˆPD(t)
[t]
LW
PF
(a)
(b)
0.0054
0.0056
0.0058
0.006
0.0062
0.0064
0.0066
0.0068
0.007
0.0072
0.0074
0.0076
0
500
1000
1500
2000
2500
3000
L1-norm error
Time step
0
0.2
0.4
0.6
0.8
1
0
10
20
30
40
50
Avg absolute error
Time step
Figure 15.6
Likelihood weighting and particle ﬁltering over time. (a) A comparison for 1,000 time
slices. (b) A very long run of particle ﬁltering.

15.3. Approximate Inference
671
history, each of our proposed sample trajectories for time t + 1 must be an extension of one of
our time t sample trajectories. More precisely, each proposed trajectory at time t + 1 must have
the form ¯x(0:t), x(t+1), for some ¯x(0:t)[m] ∈D(t).
Note that this requirement, while desirable from a computational perspective, does have
disadvantages. In certain cases, our set of time t sample trajectories might be unrepresentative
of the true underlying distribution; this might occur simply because of bad luck in sampling, or
because our evidence sequence up to time t was misleading, causing us to select for trajectories
that turn out to be a bad match to later observations. Thus, it might be desirable to rejuvenate
our sample trajectories, allowing the states prior to time t to be modiﬁed based on evidence
observed later on. However, this type of process is diﬃcult to execute eﬃciently, and is not often
done in practice.
If we proceed under the previous assumption, we can compute the appropriate importance
weights for our importance sampling process incrementally:
w(¯x(0:t))
=
P(x(0:t) | o(0:t))
α(t)(x(0:t))
=
P(x(0:t−1) | o(0:t))
α(t−1)(x(0:t−1))
P(¯x(t) | x(0:t−1), o(0:t))
α(t)(¯x(t) | x(0:t−1))
.
As we have discussed, the quality of an importance sampler is a function of the variance of the
weights: the lower the variance, the better the sampler. Thus, we aim to choose our proposal
distribution α(t)(¯x(t) | ¯x(0:t−1)) so as to reduce the variance of the preceding expression.
Note that only the second of the two terms in this product depends on our time t proposal
distribution α(t)(¯x(t) | ¯x(0:t−1)). By assumption, the samples x(0:t−1) are ﬁxed, and hence
so is the ﬁrst term. It is now not diﬃcult to show that the time t proposal distribution that
minimizes the overall variance is
α(t)(X(t) | x(0:t−1)) = P(X(t) | x(0:t−1), o(0:t)),
(15.3)
making the second term uniformly 1. In words, we should sample the time t state variable
assignment ¯x(t) from its posterior distribution given the chosen sample from the previous state
and the time t observations.
Using this proposal, the appropriate importance weight for our time t trajectory is
P(x(0:t−1) | o(0:t))
α(t−1)(x(0:t−1)) .
What is the proposal distribution we use for the time t −1 trajectories? If we use this idea
in combination with resampling, we can make the approximation that our uniformly sampled
particles at time t −1 are an approximation to P(x(0:t−1) | o(0:t−1)). In this case, we have
P(x(0:t−1) | o(0:t))
α(t−1)(¯x(0:t−1))
≈
P(x(0:t−1) | o(0:t))
P(x(0:t−1) | o(0:t−1))
∝
P(x(0:t−1) | o(0:t−1))P(o(t) | x(0:t−1), o(0:t−1))
P(x(0:t−1) | o(0:t−1))
=
P(o(t) | ¯x(t−1)),

672
Chapter 15. Inference in Temporal Models
where the last step uses the Markov independence properties. Thus, our importance weights
here are proportional to the probability of the time t observation given the time t −1 particle
x(t−1), marginalizing out the time t state variables. We call this approach posterior particle
posterior particle
ﬁlter
ﬁltering, because the samples are generated using our posterior over the time t state, given the
time t observation, rather than using our prior.
However, sampling from the posterior over the state variables given the time t observations
may not be tractable. Indeed, the whole purpose of the (static) likelihood-weighting algorithm is
to address this problem, deﬁning a proposal distribution that is (perhaps) closer to the posterior
while still allowing forward sampling according to the network structure. However, likelihood
weighting is only one of many importance distributions that can be used in this setting. In
many cases, signiﬁcant advantages can be gained by constructing proposal distributions that are
even closer to this posterior; we describe some ideas in section 15.3.3.5 below.
15.3.3.4
Sample Selection Scheme
We can also generalize the particle selection scheme. A general selection procedure associates
with each particle ¯x(0:t)[m] a number of oﬀspring K(t)
m . Each of the oﬀspring of this particle
is a (possibly weighted) copy of it, which is then propagated independently to the next step, as
discussed in the previous section. Let D(t) be the original sample set, and ˜D(t) be the new
sample set.
Assuming that we want to keep the total number of particles constant, we must have
PM
m=1 K(t)
m
= M.
There are many approaches to selecting the number of oﬀspring K(t)
m
for each particle ¯x(0:t)[m].
The bootstrap ﬁlter implicitly selects K(t)
m
using a multinomial
distribution, where one performs M IID trials, in each of which we obtain the outcome m
with probability w(¯x(0:t)[m]) (assuming the weights have been renormalized). This distribution
guarantees that the expectation of K(t)
m is M · w(¯x(0:t)[m]). Because each of the particles in
˜D(t) is weighed equally, this property guarantees that the expectation (relative to our random
resampling procedure) of ˆP ˜
D(t) is our original distribution ˆPD(t). Thus, the resampling proce-
dure does not introduce any bias into our algorithm, in the sense that the expectation of any
estimator relative to ˆPD(t) is the same as its expectation relative to ˆP ˜
D(t).
While the multinomial scheme is quite natural, there are many other selection schemes that
also satisfy this constraint. In general, we can use some other method to select the number of
oﬀspring K(t)
m for each sample m, so long as this number satisﬁes (perhaps approximately) the
constraint on the expectation. Assuming K(t)
m > 0, we then assign the weight of each of these
K(t)
m oﬀspring to be:
w(¯x(0:t)[m])
K(t)
m Pr(K(t)
m > 0)
;
intuitively, we divide the original weight of the mth sample between its K(t)
m oﬀspring. The
second term in the denominator accounts for the fact that the sample was not eliminated entirely.
To justify this expression, we observe that the total weight of the sample m oﬀspring conditioned
on the fact that K(t)
m > 0 is precisely w(¯x(0:t)[m]). Thus, the unconditional expectation of the
total of these weights is w(¯x(0:t)[m])P(K(t)
m > 0), causing us to divide by this latter probability

15.3. Approximate Inference
673
in the new weights.
There are many possible choices for generating the vector of oﬀspring (K(t)
1 , . . . , K(t)
M ), which
tells us how many copies (if any) of each of the M samples in D(t) we wish to propagate forward.
Although the diﬀerent schemes all have the same expectation, they can diﬀer signiﬁcantly
in terms of their variance. The higher the variance, the greater the probability of obtaining
unrepresentative distributions, leading to poor answers.
The multinomial sampling scheme
induced by the bootstrap ﬁlter tends to have a fairly high variance, and other schemes often
perform better in practice. Moreover, it is not necessarily optimal to perform a resampling step
at every time slice. For example, one can monitor the weights of the samples, and only resample
when the variance exceeds a certain threshold, or, equivalently, when the number of eﬀective
samples equation (12.15) goes below a certain minimal amount.
Finally, we note that one can also consider methods that vary the number of samples M over
time. In certain cases, such as tracking a system in real time, we may be forced to maintain
rigid constraints on the amount of time spent in each time slice. In other cases, however, it
may be possible to spend more computational resources in some time slices, at the expense of
using less in others. For example, if we have the ability to cache our observations a few time
slices back (which we may be doing in any case for the purposes of performing smoothing),
we can allow more samples in one time slice (perhaps falling a bit behind), and catch up in a
subsequent time slice. If so, we can determine whether additional samples are required for the
current time slice by using our estimate of the number of eﬀective samples in the current time
slice. Empirically, this ﬂexibility in the number of samples used per time slice can also improve
the quality of the results, since it helps reduce the variance of the estimator and thereby reduces
the harm done by a poor set of samples obtained at one time slice.
15.3.3.5
Other Extensions
As for importance sampling in the static case, there are multiple ways in which we can improve
particle ﬁltering by utilizing other inference methods. For example, a key problem in particle
ﬁltering is the fact that the diversity of particles often decreases over time, so that we are only
generating samples from a relatively small part of our space. In cases where there are multiple
reasonably likely hypotheses, this loss of diversity can result in bad situations, where a surprising
observation (surprising relative to our current sample population) can suddenly rule out all or
most of our samples.
There are several ways of addressing that problem. For example, we can use MCMC methods
within a time slice to obtain a more diverse set of samples.
While this cannot regenerate
hypotheses that are very far away from our current set of samples, it can build up and maintain
a broader set of hypotheses that is less likely to become depleted in subsequent steps. A related
approach is to generate a clique tree for the time slice in isolation, and then use forward sampling
to generate samples from the clique tree (see exercise 12.3). We note that exact inference for
a single time slice may be feasible, even if it is infeasible for the DBN as a whole due to the
entanglement phenomenon.
Another use for alternative inference methods is to reduce the variance of the generated
samples. Here also, multiple approaches are possible. For example, as we discussed in sec-
tion 15.3.3.3 (in equation (15.3)), we want to generate our time t state variable assignment from
its posterior distribution given the chosen sample from the previous state and the time t ob-

674
Chapter 15. Inference in Temporal Models
servations. We can generate this posterior using exact inference on the 2-TBN structure. Again,
this approach may be feasible even if exact inference on the DBN is not. If exact inference is
infeasible even for the 2-TBN, we may still be able to use some intermediate alternative. We
might be able to reverse some edges that point to observed variables (see exercise 3.12), making
the time t distribution closer to the optimal sampling distribution at time t. Alternatively, we
might use approximate inference method (for example, the EP-based approach of the previous
section) to generate a proposal distribution that is closer to the true posterior than the one
obtained by the simple likelihood-weighting sampling distribution.
Finally, we can also use collapsed particles rather than fully sampled states in particle ﬁltering.
This method is often known as Rao-Blackwellized particle ﬁltering, or RBPF. As we observed in
Rao-Blackwellized
particle ﬁlter
section 12.4, the use of collapsed particles reduces the bias of the estimator. The procedure
is based on the collapsed importance-sampling procedure for static networks, as described in
section 12.4.1. As there, we partition the state variables X into two disjoint sets: the sampled
variables Xp, and the variables Xd whose distribution we maintain in closed form.
Each
particle now consists of three components: (x(t)
p [m], w(t)[m], q(t)[m](X(t)
d )).
The particle
structure is generally chosen so as to allow q(t)[m](X(t)
d ) to be represented eﬀectively, for
example, in a factorized form.
At a high level, we use importance sampling from some appropriate proposal distribution
Q (as described earlier) to sample the variables X(t)
p
and exact inference to compute the
importance weights and the distribution q(t)[m](X(t)
d ). This process is described in detail in
section 12.4.1. When applying this procedure in the context of particle ﬁltering, we generate the
time t particle from a distribution deﬁned by a time t −1 particle and the 2-TBN.
More precisely, consider a time t −1 particle x(t−1)
p
[m], w(t−1)[m], q(t−1)[m](X(t−1)
d
). We
deﬁne a joint probability distribution P (t)
m (X(t−1) ∪X (t)) by taking the time t −1 particle
x(t−1)
p
[m], q(t−1)[m](X(t−1)
d
) as a distribution over X(t−1) (one that gives probability 1 to
x(t−1)
p
[m]), and then using the 2-TBN to deﬁne P(X (t) | X(t−1)). The distribution P (t)
m
is
represented in a factored form, which is derived from the factorization of q(t−1)[m](X(t−1)
d
)
and from the structure of the 2-TBN. We can now use P (t)
m , and the time t observation o(t),
as input to the procedure of section 12.4.1. The output is a new particle and weight w; the
particle is added to the time t data set D(t), with a weight w · w(t−1)[m]. The resulting data
set, consisting now of collapsed particles, is then utilized as in standard particle ﬁltering. In
particular, an additional sample selection step may be used to choose which particles are to be
propagated to the next time step.
As deﬁned, however, the collapsed importance-sampling procedure over P (t)
m
computes a
particle over all of the variables in this model: both time t and time t −1 variables.
For
our purpose, we need to extract a particle involving only time t variables.
This process is
fairly straightforward. The particle speciﬁes as assignment to X(t)
p ; the marginal distribution
over X(t)
d
can be extracted using standard probabilistic inference techniques. We must take
care, however: in general, the distribution over Xd can be subject to the same entanglement
phenomena as the distribution as a whole. Thus, we must select the factorization (if any) of
q(t)[m](X(t)
d ) so as to be sustainable over time; that is, q(t−1)[m](X(t−1)
d
) factorizes in a
certain way, then so does the marginal distribution q(t)[m](X(t)
d ) induced by P (t)
m . Box 15.A

15.4. Hybrid DBNs
675
provides an example of such a model for the application of collapsed particle ﬁltering to the
task of robot localization and mapping.
15.3.4
Deterministic Search Techniques
Random sampling methods such as particle ﬁltering are not always the best approach for
generating particles that search the space of possibilities. In particular, if the transition model is
discrete and highly skewed — some successor states have much higher probability than others
— then a random sampling of successor states is likely to generate many identical samples.
This greatly reduces sample diversity, wastes computational resources, and leads to a poor
representation of the space of possibilities. In this case, search-based methods may provide
search
a better alternative.
Here, we aim to ﬁnd a set of particles that span the high-probability
assignments and that we hope will allow us to keep track of the most likely trajectories through
the system.
These techniques are commonly used in applications such as speech recognition (see box 6.B),
speech
recognition
where the transitions between phonemes, and even between words, are often highly constrained,
with most transitions having probability (close to) 0. Here, we often formulate the problem as
that of ﬁnding the single highest-probability trajectory through the system. In this case, an exact
solution can be found by running a variable elimination algorithm such as that of section 13.2.
In the context of HMMs, this algorithm is known as the Viterbi algorithm.
Viterbi algorithm
In many cases, however, the HMM for speech recognition does not ﬁt into memory. Moreover,
if our task is continuous speech recognition, there is no natural end to the sequence. In such
settings, we often resort to approximate techniques that are more memory eﬃcient. A commonly
used technique is beam search, which has the advantage that it can be applied in an online
fashion as the data sequence is acquired. See exercise 15.10.
Finally, we note that deterministic search in temporal models is often used within the frame-
work of collapsed particles, combining search over a subset of the variables with marginalization
over others.
This type of approach provides an approximate solution to the marginal MAP
problem, which is often a more appropriate formulation of the problem. For example, in the
speech-recognition problem, the MAP solution ﬁnds the most likely trajectory through the speech
HMM. However, this complete trajectory tells us not only which words were spoken in a given
utterance, but also which phones and subphones were traversed; we are rarely interested in
the trajectory through these ﬁner-grained states. A more appropriate goal is to ﬁnd the most
likely sequence of words when we marginalize over the possible sequences of phonemes and
subphones. Methods such as beam search can also be adapted to the marginal MAP problem,
allowing it to be applied in this setting; see exercise 15.10.
15.4
Hybrid DBNs
So far, we have focused on inference in the context of discrete models. However, many (perhaps
even most) dynamical systems tend to include continuous, as well as discrete, variables. From
a representational perspective, there is no diﬃculty incorporating continuous variables into the
network model, using the techniques described in section 5.5. However, as usual, inference in
models incorporating continuous variables poses new challenges. In general, the techniques
developed in chapter 14 for the case of static networks also extend to the case of DBNs, in the

676
Chapter 15. Inference in Temporal Models
same way that we extended inference techniques for static discrete networks in earlier sections
in this chapter.
We now describe a few of the combinations, focusing on issues that are speciﬁc to the
combination between DBNs and hybrid models. We emphasize that many of the other techniques
described in this chapter and in chapter 14 can be successfully combined. For example, one of
the most popular combinations is the application of particle ﬁltering to continuous or hybrid
systems; however, the combination does not raise signiﬁcant new issues, and so we omit a
detailed presentation.
15.4.1
Continuous Models
We begin by considering systems composed solely of continuous variables.
15.4.1.1
The Kalman Filter
The simplest such system is the linear dynamical system (see section 6.2.3.2), where the variables
are related using linear Gaussian CPDs. These systems can be tracked very eﬃciently using a
set of update equations called the Kalman ﬁlter.
Recall that the key diﬃculty with tracking a dynamical system is the entanglement phe-
nomenon, which generally forces us to maintain, as our belief state, a full joint distribution
over the state variables at time t. For discrete systems, this distribution has size exponential
in the number of variables, which is generally intractably large. By contrast, as a linear Gaus-
sian network deﬁnes a joint Gaussian distribution, and Gaussian distributions are closed under
conditioning and marginalization, we know that the posterior distribution over any subset of
variables given any set of observations is a Gaussian. In particular, the belief state over the state
variables X(t) is a multivariate Gaussian. A Gaussian can be represented as a mean vector and
covariance matrix, which requires (at most) quadratic space in the number of state variables.
Thus, in a Kalman ﬁlter, we can represent the belief state fairly compactly.
As we now show, we can also maintain the belief state eﬃciently, using simple matrix
operations over the matrices corresponding to the belief state, the transition model, and the
observation model.
Speciﬁcally, consider a linear Gaussian DBN deﬁned over a set of state
variables X with n = |X| and a set of observation variables O with m = |O|.
Let the
probabilistic model be deﬁned as in equation (6.3) and equation (6.4), which we review for
convenience:
P(X(t) | X(t−1))
=
N

AX(t−1); Q

,
P(O(t) | X(t))
=
N

HX(t); R

.
We now show the Kalman ﬁlter update equations, which provide an eﬃcient implementation
Kalman ﬁlter
for equation (15.1) and equation (15.2). Assume that the Gaussian distribution encoding σ(t) is
maintained using a mean vector µ(t) and a covariance distribution Σ(t). The state transition
state transition
update
update equation is easy to implement:
µ(·t+1)
=
Aµ(t)
Σ(·t+1)
=
AΣ(t)AT + Q,
(15.4)

15.4. Hybrid DBNs
677
where µ(·t+1) and Σ(·t+1) are the mean and covariance matrix for the prior belief state σ(·t+1).
Intuitively, the new mean vector is simply the application of the linear transformation A to the
mean vector in the previous time step. The new covariance matrix is the transformation of the
previous covariance matrix via A, plus the covariance introduced by the noise.
The observation update is somewhat more elaborate:
observation
update
K(t+1)
=
Σ(·t+1)HT (HΣ(·t+1)HT + R)−1
µ(t+1)
=
µ(·t+1) + K(t+1)(o(t+1) −Hµ(·t+1))
Σ(t+1)
=
(I −K(t+1)H)Σ(·t+1).
(15.5)
This update rule can be obtained using tedious but straightforward algebraic manipulations, by
forming the joint Gaussian distribution over X(t+1), O(t+1) deﬁned by the prior belief state
σ(·t+1) and the observation model P(O(t+1) | X(t+1)), and then conditioning the resulting
joint Gaussian on the observation o(t+1).
To understand the intuition behind this rule, note ﬁrst that the mean of σ(t+1) is simply the
mean of σ(·t+1), plus a correction term arising from the evidence. The correction term involves
the observation residual — the diﬀerence between our expected observation Hµ(·t+1) and the
actual observation o(t+1). This residual is multiplied by a matrix called the Kalman gain K(t+1),
which dictates the importance that we ascribe to the observation. We can see, for example, that
when the measurement error covariance R approaches 0, the Kalman gain approaches H−1; in
this case, we are exactly “reverse engineering” the residual in the observation and using it to
correct the belief state mean. Thus, the actual observation is trusted more and more, and the
predicted observation Hµ(·t+1) is trusted less. We also then have that the covariance of the
new belief state approaches 0, corresponding to the fact that the observation tells us the current
state with close to certainty. Conversely, when the covariance in our belief state Σ(·t+1) tends
to 0, the Kalman gain approaches 0 as well. In this case, we trust our predicted distribution,
and pay less and less attention to the observation: both the mean and the covariance of the
posterior belief state σ(t+1) are the same as those of the prior belief state σ(·t+1). Finally, it can
be shown that the posterior covariance matrix of our estimate approaches some limiting value
as T −→∞, which reﬂects our “steady state” uncertainty about the system state. We note that
both the time t covariance and its limiting value do not depend on the data. On one hand, this
fact oﬀers computational savings, since it allows the covariance matrix to be computed oﬄine.
However, it also points to a fundamental weakness of linear-Gaussian models, since we would
naturally expect our uncertainty to depend on what we have seen.
The Kalman ﬁltering process maintains the belief state as a mean and covariance matrix.
An alternative is to maintain the belief state using information matrices (that is, a canonical
form representation, as in equation (14.1)). The resulting set of update equations, called the
information form of the Kalman ﬁlter, can be derived in a straightforward way from the basic
information form
operations on canonical forms described in section 14.2.1.2; the details are left as an exercise
(exercise 15.11). We note that, in the Kalman ﬁlter, which maintains covariance matrices, the state
transition update (equation (15.4)) is straightforward, and the observation update (equation (15.5))
is complex, requiring the inversion of an n×n matrix. In the information ﬁlter, which maintains
information matrices, the situation is precisely the reverse.

678
Chapter 15. Inference in Temporal Models
15.4.1.2
Nonlinear Systems
The Kalman ﬁlter can also be extended to deal with nonlinear continuous dynamics, using the
techniques described in section 14.4. In these methods, we maintain all of the intermediate fac-
tors arising in the course of inference as multivariate Gaussian distributions. When encountering
a nonlinear CPD, which would give rise to a non-Gaussian factor, we simply linearize the result
to produce a new Gaussian. We described two main methods for performing the linearization,
either by taking the Taylor series expansion of the nonlinear function, or by using one of several
numerical integration techniques. The same methods apply without change to the setting of
tracking nonlinear continuous DBNs. In this case, the application is particularly straightforward,
as the factors that we wish to manipulate in the course of tracking are all distributions; in a
general clique tree, some factors do not represent distributions, preventing us from applying
these linearization techniques and constraining the order in which messages are passed.
Concretely, assume that our nonlinear system has the model:
P(X(t) | X(t−1))
=
f(X(t−1), U (t−1))
P(O(t) | X(t))
=
g(X(t), W (t)),
where f and g are deterministic nonlinear (continuous) functions, and U (t), W (t) are Gaussian
random variables, which explicitly encode the noise in the transition and observation models,
respectively. (In other words, rather than modeling the system in terms of stochastic CPDs, we
use an equivalent representation that partitions the model into a deterministic function and a
noise component.)
To address the ﬁltering task here, we can apply either of the linearization methods described
earlier. The Taylor-series linearization of section 14.4.1.1 gives rise to a method called the extended
extended Kalman
ﬁlter
Kalman ﬁlter. The unscented transformation of section 14.4.1.2 gives rise to a method called the
unscented Kalman ﬁlter. In this latter approach, we maintain our belief state using the same
unscented
Kalman ﬁlter
representation as in the Kalman ﬁlter: σ(t) = N
 µ(t); Σ(t)
. To perform the transition update,
we construct a joint Gaussian distribution p(X(t), U (t)) by multiplying the Gaussians for σ(t)
and U (t). The result is a Gaussian distribution and a nonlinear function f, to which we can
now apply the unscented transformation of section 14.4.1.2. The result is a mean vector µ(·t+1)
and covariance matrix Σ(·t+1) for the prior belief state σ(·t+1).
To obtain the posterior belief state, we must perform the observation update. We construct
a joint Gaussian distribution p(X(t+1), W (t+1)) by multiplying the Gaussians for σ(·t+1) and
W (t+1). We then estimate a joint Gaussian distribution over X(t+1), O(t+1), using the un-
scented transformation of equation (14.18) to estimate the integrals required for computing the
mean and covariance matrix of this joint distribution. We now have a Gaussian joint distri-
bution over X(t+1), O(t+1), which we can condition on our observation o(t+1) in the usual
way. The resulting posterior over X(t+1) is the new belief state σ(t+1). Note that this approach
computes a full joint covariance matrix over X(t+1), O(t+1). When the dependency model of
the observation on the state is factored, where we have individual observation variables each
of which depends only on a few state variables, we can perform this computation in a more
structured way (see exercise 15.12).

15.4. Hybrid DBNs
679
observed
truth
observed
estimated
(a)
(b)
14
12
10
8
6
4
10
12
14
16
18
20
22
14
12
10
8
6
4
10
12
14
16
18
20
22
24
Figure 15.A.1 — Illustration of Kalman ﬁltering for tracking (a) Raw data (dots) generated by an ob-
ject moving to the right (line). (b) Estimated location of object: crosses are the posterior mean, circles are
95 percent conﬁdence ellipsoids.
Box 15.A — Case Study: Tracking, Localization, and Mapping. A key application of proba-
bilistic models is to the task of tracking moving objects from noisy measurements. One example is
target tracking, where we measure the location of an object, such as an airplane, using an external
target tracking
sensor. Another example is robot localization, where the moving object itself collects measurements,
robot localization
such as sonar or laser data, that can help it localize itself on a map.
Kalman ﬁltering was applied to this problem as early as the 1960s. Here, we give a simpliﬁed
example to illustrate the key ideas. Consider an object moving in a two-dimensional plane. Let
X(t)
1
and X(t)
2
be the horizontal and vertical locations of the object, and ˙X(t)
1
and ˙X(t)
2
be the
corresponding velocity. We can represent this as a state vector X(t) ∈IR4. Let us assume that the
object is moving at constant velocity, but is “perturbed” by random Gaussian noise (for example,
due to the wind). Thus we can deﬁne X′
i ∼N

Xi + ˙Xi; σ2
for i = 1, 2. Assume we can
obtain a (noisy) measurement of the location of the object but not its velocity. Let Y (t) ∈IR2
represent our observation, where Y (t) ∼N

(X(t)
1 , X(t)
2 ); Σo

, where Σo is the covariance matrix
that governs our observation noise model. Here, we do not necessarily assume that noise is added
separately to each dimension of the object location. Finally, we need to specify our initial (prior)
beliefs about the state of the object, p(X(0)). We assume that this distribution is also a Gaussian
p(X(0)) = N
 µ(0); Σ(0)
. We can represent prior ignorance by making Σ(0) suitably “broad.”
These parameters fully specify the model, allowing us to apply the Kalman ﬁlter, as described in
section 15.4.1.1.
Figure 15.A.1 gives an example. The object moves to the right and generates an observation at
each time step (such as a “blip” on a radar screen). We observe these blips and ﬁlter out the noise
by using the Kalman ﬁlter; the resulting posterior distribution is plotted on the right. Our best guess

680
Chapter 15. Inference in Temporal Models
about the location of the object is the posterior mean, IEσ(t)
h
X(t)
1:2 | y(1:t)i
, denoted as a cross. Our
uncertainty associated with this estimate is represented as an ellipse that contains 95 percent of
the probability mass. We see that our uncertainty goes down over time, as the eﬀects of the initial
uncertainty get “washed out.” As we discussed, the covariance converges to some steady-state value,
which will remain indeﬁnitely.
We have demonstrated this approach in the setting where the measurement is of the object’s
location, from an external measurement device. It is also applicable when the measurements are
collected by the moving object, and estimate, for example, the distance of the robot to various
landmarks on a map. If the error in the measured distance to the landmark has Gaussian noise,
the Kalman ﬁlter approach still applies.
In practice, it is rarely possible to apply the Kalman ﬁlter in its simplest form. First, the dynamics
and/or observation models are often nonlinear. A common example is when we get to observe
the range and bearing to an object, but not its X(t)
1
and X(t)
2
coordinates.
In this case, the
observation model contains some trigonometric functions, which are nonlinear. If the Gaussian
noise assumption is still reasonable, we apply either the extended Kalman ﬁlter or the unscented
Kalman ﬁlter to linearize the model. Another problem arises when the noise is non-Gaussian, for
example, when we have clutter or outliers. In this case, we might use the multivariate T distribution;
this solution gains robustness at the cost of computational tractability. An alternative is to assume
that each observation comes from a mixture model; one component corresponds to the observation
being generated by the object, and another corresponds to the observation being generated by the
background. However, this model is now an instance of a conditional linear Gaussian, raising all
of the computational issues associated with multiple modes (see section 15.4.2). The same diﬃculty
arises if we are tracking multiple objects, where we are uncertain which observation was generated
by which object; this problem is an instance of the data association problem (see box 12.D).
data association
In nonlinear settings, and particularly in those involving multiple modes, another very popular
alternative is to use particle ﬁltering. This approach is particularly appealing in an online setting
such as robot motion, where computations need to happen in real time, and using limited compu-
tational resources. As a simple example, assume that we have a known map, M. The map can
be encoded as a occupancy grid — a discretized grid of the environment, where each square is
1 if the environment contains an obstacle at that location, and 0 otherwise. Or we can encode it
using a more geometric representation, such as a set of line segments representing walls. We can
represent the robot’s location in the environment either in continuous coordinates, or in terms of the
discretized grid (if we use that representation). In addition, the robot needs to keep track of its pose,
or orientation, which we may also choose to discretize into an appropriate number of bins. The
measurement y(t) is a vector of measured distances to the nearest obstacles, as described in box 5.E.
Our goal is to maintain P(X(t) | y(1:t), M), which is our posterior over the robot location.
Note that our motion model is nonlinear. Moreover, although the error model on the measured
distance is a Gaussian around the true distance, the true distance to the nearest obstacle (in any
given direction) is not even a continuous function of the robot’s position. In fact, belief states can
easily become multimodal owing to perceptual aliasing, that is, when the robot’s percepts can match
two or more locations. Thus, a Gaussian model is a very poor approximation here.
Thrun et al. (2000) propose the use of particle ﬁltering for localization, giving rise to an algorithm
called Monte Carlo localization. Figure 15.A.2 demonstrates one sample trajectory of the particles
Monte Carlo
localization
over time. We see that, as the robot acquires more measurements, its belief state becomes more

15.4. Hybrid DBNs
681
Robot position
Robot position
Robot position
Robot position
Figure 15.A.2 — Sample trajectory of particle ﬁltering for robot localization
sharply peaked. More importantly, we see that the use of a particle-based belief state makes it easy
to model multimodal posteriors.
One important issue when implementing a particle ﬁlter is the choice of proposal distribution. The
simplest method is to use the standard bootstrap ﬁlter, where we propose directly from the dynamics
model, and weight the proposals by how closely they match the evidence. However, when the robot
is lost, so that our current belief state is very diﬀuse, this approach will not work very well, since the
proposals will literally be all over the map but will then get “killed oﬀ” (given essentially zero weight)
if they do not match the (high-quality) measurements. As we discussed, the ideal solution is to use
posterior particle ﬁltering, where we sample a particle x(t+1)[m] from the posterior distribution
P(X (t+1) | x(t)[m], y(t+1)). However, this solution requires that we be able to invert the evidence
model using Bayes rule, a process that is not always feasible for complex, nonlinear models. One
ad hoc ﬁx is to inﬂate the noise level in the measurement model artiﬁcially, giving particles an
artiﬁcially high chance of surviving until the belief state has the chance to adapt to the evidence.
A better approach is to use a proposal that takes the evidence into account; for example, we can
compare y(t) with the map and then use a proposal that is a mixture of the bootstrap proposal
P(X (t+1) | x(t)[m]) and some set of candidate locations that are most consistent with the recent
observations.
We now turn to the harder problem of localizing a robot in an unknown environment, while
mapping the environment at the same time. This problem is known as simultaneous localization
robot mapping
SLAM
and mapping (SLAM). In our previous terminology, this task corresponds to computing p(X(t), M |
y(1:t)). Here, again, our task is much easier in the linear setting, where we represent the map in
terms of K landmarks whose locations, denoted L1, . . . , Lk, are now unknown. Assume that we
have a Gaussian prior over the location of each landmark, and that our observations Y (t)
k
measure
the Euclidean distance between the robot position X(t) and the kth landmark location Lk, with
some Gaussian noise. It is not diﬃcult to see that P(Y (t)
k
| X(t), Lk) is a Gaussian distribution, so
that our entire model is now a linear dynamical system. Therefore, we can naturally apply a Kalman
ﬁlter to this task, where now our belief state represents P(X(t), L1, . . . , Lk | y(1:t)). Figure 15.A.3a
demonstrates this process for a simple two-dimensional map. We can see that the uncertainty
of the landmark locations is larger for landmarks encountered later in the process, owing to the
accumulation of uncertainty about the robot location. However, when the robot closes the loop
and reencounters the ﬁrst landmark, the uncertainty about its position reduces dramatically; the

682
Chapter 15. Inference in Temporal Models
Robot pose
(a)
(b)
Figure 15.A.3 — Kalman ﬁlters for the SLAM problem (a) Visualization of marginals in Gaussian
SLAM. (b) Visualization of information (inverse covariance) matrix in Gaussian SLAM, and its Markov
network structure; very few entries have high values.
landmark uncertainty reduces at the same point. If we were to use a smoothing algorithm, we
would also be able to reduce much of the uncertainty about the robot’s intermediate locations, and
hence also about the intermediate landmarks.
Gaussian inference is attractive in this setting because even representing a posterior over the
landmark positions would require exponential space in the discrete case. Here, because of our
ability to use Gaussians, the belief state grows quadratically only in the number of landmarks.
However, for large maps, even quadratic growth can be too ineﬃcient, particularly in an online
setting.
To address this computational burden, two classes of methods based on approximate
inference have been proposed.
The ﬁrst is based on factored belief-state idea of section 15.3.2. Here, we utilize the observation
that, although the variables representing the diﬀerent landmark locations become correlated due to
entanglement, the correlations between them are often rather weak. In particular, two landmarks
that were just observed at the same time become correlated due to uncertainty about the robot
position. But, as the robot moves, the direct correlation often decays, and two landmarks often
become almost conditionally independent given some subset of other landmarks; this conditional
independence manifests as sparsity in the information (inverse covariance) matrix, as illustrated in
ﬁgure 15.A.3b. Thus, these approaches approximate the belief state by using a sparser representation
that maintains only the strong correlations. We note that the set of strongly correlated landmark
pairs changes over time, and hence the structure of our approximation must be adaptive. We can
consider a range of sparser representations for a Gaussian distribution. One approach is to use a
clique tree, which admits exact M-projection operations but grows quadratically in the maximum
size of cliques in our clique tree. Another is to use the Markov network representation of the Gaussian

15.4. Hybrid DBNs
683
(or, equivalently, its inverse covariance). The two main challenges are to determine dynamically the
approximation to use at each step, and to perform the (approximate) M-projection in an eﬃcient
way, essential in this real-time setting.
A second approach, and one that is more generally applicable, is to use collapsed particles. This
approach is based on the important observation that the robot makes independent observations of
the diﬀerent landmarks. Thus, as we can see in ﬁgure 15.A.4a, the landmarks are conditionally
independent given the robot’s trajectory. Importantly, the landmarks are not independent given
the robot’s current location, owing to entanglement, but if we sample an entire robot trajectory
x(1:t), we can now maintain the conditional distribution over the landmark positions in a fully
factored form. In this approach, known as FastSLAM, each particle is associated with a (factorized)
distribution over maps. In this approach, rather than maintaining a Gaussian of dimension 2K +2
(two coordinates for each landmark and two for the robot), we maintain a set of particles, each
associated with K two-dimensional Gaussians. Because the Gaussian representation is quadratic
in the dimension and the matrix inversion operations are cubic in the dimension, this approach
provides considerable savings. (See exercise 15.13.) Experimental results show that, in the settings
where Kalman ﬁltering is suitable, this approximation achieves almost the same performance with
a small number of particles (as few as ten); see ﬁgure 15.A.4b.
This approach is far more general than the sparse Kalman-ﬁltering approach we have described,
since it also allows us to deal with other map representations, such as the occupancy-grid map de-
scribed earlier. Moreover, it also provides an integrated solution in cases where we have uncertainty
about data association; here, we can sample over data-association hypotheses and still maintain a
factored distribution over the map representation. Overall, by combining the diﬀerent techniques we
have described, we can eﬀectively address most instances of the SLAM problem, so that this problem
is now essentially considered solved.
15.4.2
Hybrid Models
We now move to considering a system that includes both discrete and continuous variables.
As in the static case, inference in such a model is very challenging, even in cases where the
continuous dynamics are simple.
Consider a conditional linear Gaussian DBN, where all of the continuous variables have
CLG dynamics, and the discrete variables have no continuous parents. This type of model is
CLG dynamics
known as a switching linear dynamical system, as it can be viewed as a dynamical system where
switching linear
dynamical system
changes in the discrete state cause the continuous (linear) dynamics to switch. The unrolled
DBN is a standard CLG network, and, as in any CLG network, given a full assignment to the
discrete variables, the distribution over the continuous variables (or any subset of them) is a
multivariate Gaussian. However, if we are not given an assignment to the discrete variables, the
marginal distribution over the continuous variables is a mixture of Gaussians, where the number
of mixture components is exponential in the number of discrete variables. (This phenomenon is
precisely what underlies the NP-hardness proof for inference in CLG networks.)
In the case of a temporal model, this problem is even more severe, since the number of
mixture components grows exponentially, and unboundedly, over time.
Example 15.6

684
Chapter 15. Inference in Temporal Models
(a)
(b)
20
15
10
5
0
–5
–10
–15
–20
–25
0
5
10
20
15
–5
–10
–15
North (meters)
East (meters)
20
15
10
5
0
–5
–10
–15
–20
–25
0
5
10
20
15
–5
–10
–15
North (meters)
East (meters)
Y1
(1)
L1
L2
Y1
(3)
Y2
(2)
Y2
(T)
Y2
(1)
X(1)
X(2)
X(3)
X(T)
Figure 15.A.4 — Collapsed particle ﬁltering for SLAM (a) DBN illustrating the conditional indepen-
dencies in the SLAM probabilistic model. (b) Sample predictions using a SLAM algorithm (solid path, star
landmarks) in comparison to ground-truth measurements obtained from a GPS sensor, not given as input
to the algorithm (dotted lines, circle landmarks): left — EKF; right — FastSLAM. The results show that both
approaches achieve excellent results, and that there is very little diﬀerence in the quality of the estimates
between these two approaches.

15.4. Hybrid DBNs
685
Consider a one-dimensional switching linear-dynamical system whose state consists of a single
discrete variable D and a single continuous variable X. In the SLDS, both X and D are persistent,
and in addition we have D′ →X′. The transition model is deﬁned by a CLG model for X′, and
a standard discrete CPD for D. In this example, the belief state at time t, marginalized over X(t),
is a mixture distribution with a number of mixture components that is exponential in t.
In order to make the propagation algorithm tractable, we must make sure that the mixture of
Gaussians represented by the belief state does not get too large. The two main techniques to do
so are pruning and collapsing.
Pruning algorithms reduce the size of the mixture distribution in the belief state by discarding
mixture pruning
some of its Gaussians. The standard pruning algorithm simply keeps the N Gaussians with the
highest probabilities, discards the rest, and then renormalizes the probabilities of the remaining
Gaussian to sum to 1.
This is a form of beam search for marginal MAP, as described in
beam search
exercise 15.10.
Collapsing algorithms partition the mixture of Gaussians into subsets, and then they collapse
mixture
collapsing
all the Gaussians in each subset into one Gaussian. Thus, if the belief state were partitioned into
N subsets, the result would be a belief state with exactly N Gaussians. The diﬀerent collapsing
algorithms can diﬀer in the choice of subsets of Gaussians to be collapsed, and on exactly when
the collapsing is performed. The collapsed Gaussian ˆp for a mixture p is generally chosen to
be its M-projection — the one that minimizes ID(p||ˆp). Recall from proposition 14.3 that the
M-projection can be computed simply and eﬃciently by matching moments.
We now describe some commonly used collapsing algorithms in this setting and show how
they can be viewed within the framework of expectation propagation, as described in sec-
tion 14.3.3. More precisely, consider a CLG DBN containing the discrete state variables A and
the continuous state variables X.
Let M = |Val(A)| — the number of discrete states at
any given time slice. Assume, for simplicity, that the state at time 0 is fully observed. We
note that, throughout this section, we focus solely on techniques for CLG networks. It is not
diﬃcult to combine these techniques with linearization methods (as described earlier), allowing
us to accommodate both nonlinear dynamics and discrete children of continuous parents (see
section 14.4.2.5).
As we discussed, after t time slices, the total number of Gaussian mixture components in the
belief state is M t — one for every assignment to A(1), . . . , A(t). One common approximation
is the class of general pseudo-Bayesian algorithms, which lump together components whose
general
pseudo-Bayes
assignment in the recent past is identical. That is, for a positive integer k, σ(t) has M k−1
mixture components — one for every assignment to A((t−(k−2)):t) (which for k = 1 we take
to be a single “vacuous” assignment). If σ(t) has M k−1 mixture components, then one step
of forward propagation results in a distribution with M k components. Each of these Gaussian
components is conditioned on the evidence, and its weight in the mixture is multiplied with the
likelihood it gives the evidence. The resulting mixture is now partitioned into M k−1 subsets,
and each subset is collapsed separately, producing a new belief state.
For k = 1, the algorithm, known as GPB1, maintains exactly one Gaussian in the belief
GPB1
state; it also maintains a distribution over A(t).
Thus, σ(t) is essentially a product of a
discrete distribution σ(t)(A(t)) and a Gaussian σ(t)(X(t)). In the forward-propagation step
(corresponding to equation (15.1)), we obtain a mixture of M Gaussians: one Gaussian p(t+1)
a(t+1)

686
Chapter 15. Inference in Temporal Models
for each assignment a(t+1), whose weight is
X
A(t)
P(a(t+1) | A(t))σ(t)(a(t)).
In the conditioning step, each of these Gaussian components is conditioned on the evidence
o(t+1), and its weight is multiplied by the likelihood of the evidence relative to this Gaussian.
The resulting weighted mixture is then collapsed into a single Gaussian, using the M-projection
operation described in proposition 14.3.
We can view this algorithm as an instance of the expectation propagation (EP) algorithm applied
expectation
propagation
to the clique tree for the DBN described in section 15.2.3. The messages, which correspond to the
belief states, are approximated using a factorization that decouples the discrete variables A(t)
and the continuous variables X(t); the distribution over the continuous variables is maintained
as a Gaussian. This approximation is illustrated in ﬁgure 15.7a. It is not hard to verify that
the GPB1 propagation update is precisely equivalent to the operation of incorporating the time
t message into the (t, t + 1) clique, performing the in-clique computation, and then doing the
M-projection to produce the time t + 1 message.
The GPB2 algorithm maintains M Gaussians {p(t)
a(t)} in the time t belief state, each one
GPB2
corresponding to an assignment a(t).
After propagation, we get M 2 Gaussians, each one
corresponding to an assignment to both a(t) and a(t+1). We now partition this mixture into M
subsets, one for each assignment to a(t+1). Each subset is collapsed, resulting in a new belief
state with M Gaussians. Once again, we can view this algorithm as an instance of EP, using the
message structure A(t) →X(t), where the distribution of X(t) given A(t) in the message has
the form of a conditional Gaussian. This EP formulation is illustrated in ﬁgure 15.7b.
A limitation of the GPB2 algorithm is that, at every propagation step we must generate M 2
Gaussians, a computation that may be too expensive. An alternative approach is called the inter-
acting multiple model (IMM) algorithm. Like GPB2, it maintains a belief state with M Gaussians
p(t)
a(t); but like GPB1, it collapses all of the Gaussians in the mixture into a single Gaussian, prior
to incorporating them into the transition model P(X(t+1) | X(t)). Importantly, however, it
performs the collapsing step after incorporating the discrete transition model P(A(t+1) | A(t)).
This produces a diﬀerent mixture distribution for each assignment a(t+1) — these distribu-
tions are all mixtures of the same set of time t Gaussians, but with diﬀerent mixture weights,
generally producing a better approximation. Each of these components, representing a condi-
tional distribution over X(t) given A(t+1), is then propagated using the continuous dynamics
P(X(t+1) | X(t), A(t+1)), and conditioned on the evidence, producing the time t + 1 be-
lief state. The IMM algorithm can also be formulated in the EP framework, as illustrated in
ﬁgure 15.7c.
Although we collapse our belief state in M diﬀerent ways when using the IMM algorithm,
we only create M Gaussians at time t + 1 (as opposed to M 2 Gaussians in GPB2). The extra
work compared to GPB1 is the computation of the new mixture probabilities and collapsing M
mixtures instead of just one, but usually this extra computational cost is small relative to the
cost of computing the M Gaussians at time t+1. Therefore, the computational cost of the IMM
algorithm is only slightly higher than the GPB1 algorithm, since both algorithms generate only
M Gaussians at every propagation step, and it is signiﬁcantly lower than GPB2. In practice, it
seems that IMM often performs signiﬁcantly better than GPB1 and almost as well as GPB2. Thus,

15.4. Hybrid DBNs
687
(b)
A(t+1)
X(t+1)
A(t)
X (t)
A(t)
X (t)
A(t+1)
X(t+1)
O(t+1)
(a)
A(t+1)
X(t+1)
A(t)
X (t)
A(t)
X (t)
A(t+1)
X(t+1)
O(t+1)
(c)
A(t+1)
X(t)
A(t)
X (t)
A(t)
X (t)
A(t+1)
A(t+1)
X(t+1)
X (t)
A(t+1)
X(t+1)
O(t+1)
Figure 15.7
Three approaches to collapsing the Gaussian mixture obtained when tracking in a
hybrid CLG DBN, viewed as instances of the EP algorithm. The ﬁgure illustrates the case where the
network contains a single discrete variable A and a single continuous variable X: (a) the GPB1 algorithm;
(b) the GPB2 algorithm; (c) the IMM algorithm.

688
Chapter 15. Inference in Temporal Models
the IMM algorithm appears to be a good compromise between complexity and performance.
We note that all of these clustering methods deﬁne the set of mixture components to corre-
spond to assignments of discrete variables in the network. For example, in both GPB1 and IMM,
each component in the mixture for σ(t) corresponds to an assignment to A(t). In general, this
approach may not be optimal, as Gaussians that correspond to diﬀerent discrete assignments of
A(t) may be more similar to each other than Gaussians that correspond to the same assignment.
In this case, the collapsed Gaussian would have a variance that is unnecessarily large, leading to
a poorer approximation to the belief state. The solution to this problem is to select dynamically
a partition of Gaussians in the current belief state, where the Gaussian components in the same
partition are collapsed.
Our discussion has focused on cases where the number of discrete states at each time step is
tractably small. How do we extend these ideas to more complex systems, where the number of
discrete states at each time step is too large to represent explicitly? The EP formulation of these
collapsing strategies provides an easy route to generalization. In section 14.3.3, we discussed the
application of the EP algorithm to CLG networks, using either a clique tree or a cluster-graph
message passing scheme. When faced with a more complex DBN, we can construct a cluster
graph or a clique tree for the 2-TBN, where the clusters contain both discrete and continuous
variables. These clusters pass messages to each other, using M-projection to the appropriate
parametric family chosen for the messages.
When a cluster contains one or more discrete
variables, the M-projection operation may involve one of the collapsing procedures described.
We omit details.
15.5
Summary
In this chapter, we discussed the problem of performing inference in a dynamic Bayesian
network. We showed how the most natural inference tasks in this setting map directly onto
probabilistic queries in the ground DBN. Thus, at a high level, the inference task here is not
diﬀerent from that of any other Bayesian network: we can simply unroll the network, instantiate
our observations, and run inference to compute the answers to our queries. A key challenge
lies in the fact that the networks that are produced in this approach can be very (or even
unboundedly) large, preventing us from applying many of our exact and approximate inference
schemes.
We showed that the tracking problem can naturally be formulated as a single upward pass of
clique tree propagation, sending messages from time 0 toward later time slices. The messages
naturally represent a belief state: our current beliefs about the state of the system. Importantly,
this forward-propagation pass can be done in a way that does not require that the clique tree
for the entire unrolled network be maintained.
Unfortunately, the entanglement property that arises in all but the most degenerate DBNs
typically implies that the belief state has no conditional independence structure, and therefore it
does not admit any factorization. This fact generally prevents the use of exact inference, except
in DBNs over a fairly small state space.
As for exact inference, approximate inference techniques can be mapped directly to the
unrolled DBN. Here also, however, we wish to avoid maintaining the entire DBN in memory
during the course of inference. Some algorithms lend themselves more naturally than others to

15.5. Summary
689
this “online” message passing. Two methods that have been speciﬁcally adapted to this setting
are the factored message passing that lies at the heart of the expectation propagation algorithm,
and the likelihood-weighting (importance-sampling) algorithm.
The application of the factored message passing is straightforward: We represent the message
as a product of factors, possibly with counting number corrections to avoid double-counting;
we employ a nested clique tree or cluster graph within each time slice to perform approximate
message propagation, mapping a time t approximate belief state to a time t + 1 approximate
belief state. In the case of ﬁltering, this application is even simpler than the original EP, since
no backward message passing is used.
The adaptation of the likelihood-weighting algorithm is somewhat more radical. Here, if we
simply propagate particles forward, using the evidence to adjust their weights, the weight of the
particles will generally rapidly go to zero, leaving us with a set of particles that has little or no
relevance to the true state. From a technical perspective, the variance of this estimator rapidly
grows as a function of t. We therefore introduce a resampling step, which allows “good” samples
to duplicate while removing poor samples. We discussed several approaches for selecting new
samples, including the simple (but very common) bootstrap ﬁlter, as well as more complex
schemes that use MCMC or other approaches to generate a better proposal distribution for new
samples.
As in static networks, hybrid schemes that combine sampling with some form of
(exact or approximate) global inference can be very useful in DBNs. Indeed, we saw examples of
practical applications where this type of approach has been used successfully.
Finally, we discussed the task of inference in continuous or hybrid models. The issues here
are generally very similar to the ones we already tackled in the case of static networks. In purely
Gaussian networks, a straightforward application of the basic Gaussian message propagation
algorithm gives rise to the famous Kalman ﬁlter.
For nonlinear systems, we can apply the
techniques of chapter 14 to derive the extended Kalman ﬁlter and the unscented ﬁlter. More
interesting is the case where we have a hybrid system that contains both discrete and continuous
variables. Here, in order to avoid an unbounded blowup in the number of components in the
Gaussian mixture representing the belief state, we must collapse multiple Gaussians into one.
Various standard approaches for performing this collapsing procedure have been developed in
the tracking community; these approaches use a window length in the trajectory to determine
which Gaussians to collapse. Interestingly, these approaches can be naturally viewed as variants
of the EP algorithm, with diﬀerent message approximation schemes.
Our presentation in this chapter has focused on inference methods that exploit in some way
the speciﬁc properties of inference in temporal models. However, all of the inference methods
that we have discussed earlier in this book (as well as others) can be adapted to this setting.
If we are willing to unroll the network fully, we can use any inference algorithm that has been
developed for static networks. But even in the temporal setting, we can adapt other inference
algorithms to the task of inference within a pair of consecutive time slices for the purpose of
propagating a belief state. Thus, for example, we can consider variational approximation over a
pair of time slices, or MCMC sampling, or various combinations of diﬀerent algorithms. Many
such variants have been proposed; see section 15.6 for references to some of them.
All of our analysis in this chapter has assumed that the structure of the diﬀerent time slices
is the same, so that a ﬁxed approximation structure could be reasonably used for all time slices
t. In practice, we might have processes that are not homogeneous, whether because the process
itself is nonstationary or because of sparse events that can radically change the momentary

690
Chapter 15. Inference in Temporal Models
system dynamics. The problem of dynamically adapting the approximation structure to changing
circumstances is an exciting direction of research. Also related is the question of dealing with
systems whose very structure changes over time, for example, a road where the number of cars
can change dynamically.
This last point is related, naturally, to the problem of inference in other types of template-based
models, some of which we described in chapter 6 but whose inference properties we did not
tackle here. Of course, one option is to construct the full ground network and perform standard
inference. However, this approach can be quite costly and even intractable. An important goal is
to develop methods that somehow exploit structure in the template-based model to reduce the
computational burden. Ideally, we would run our entire inference at the template level, avoiding
the step of generating a ground network. This process is called lifted inference, in analogy to
lifted inference
terms used in ﬁrst-order logic theorem proving. As a somewhat less lofty goal, we would like
to develop algorithms that use properties of the ground network, for example, the fact that it
has repeated substructures due to the use of templates. These directions provide an important
trajectory for current and future research.
15.6
Relevant Literature
The earliest instances of inference in temporal models were also some of the ﬁrst applications
of dynamic programming for probabilistic inference in graphical models: the forward-backward
algorithm of Rabiner and Juang (1986) for hidden Markov models, and the Kalman ﬁltering
algorithm.
Kjærulﬀ(1992, 1995a) provided the ﬁrst algorithm for probabilistic inference in DBNs, based on
a clique tree formulation applied to a moving window in the DBNs. Darwiche (2001a) studies the
concept of slice-by-slice triangulation in a DBN, and suggests some new elimination strategies.
Bilmes and Bartels (2003) extend on this work by providing a triangulation algorithm speciﬁcally
designed for DBNs; they also show that it can be beneﬁcial to allow the inference data structure
to span a window larger than two time slices. Binder, Murphy, and Russell (1997) show how
exact clique tree propagation in a DBN can be performed in a space-eﬃcient manner by using
a time-space trade-oﬀ.
Simple variants of particle-based ﬁltering were ﬁrst introduced by Handschin and Mayne
(1969); Akashi and Kumamoto (1977). The popularity of these methods dates to the mid-1990s,
where the resampling step was ﬁrst introduced to avoid the degeneracy problems inherent to
the naive approaches. This idea was introduced independently in several communities under
several names, including: dynamic mixture models (West 1993), bootstrap ﬁlters (Gordon et al.
1993), survival of the ﬁttest (Kanazawa et al. 1995), condensation (Isard and Blake 1998a), Monte
Carlo ﬁlters (Kitagawa 1996), and sequential importance sampling (SIS) with resampling (SIR)
(Doucet 1998). Kanazawa et al. (1995) propose the use of arc reversal for generating a better
sampling distribution. Particle smoothing was ﬁrst proposed by Isard and Blake (1998b) and
Godsill, Doucet, and West (2000).
The success of these methods on a range of practical applications led to the development of
multiple improvements, as well as some signiﬁcant analysis of the theoretical properties of these
methods. Doucet, de Freitas, and Gordon (2001) and Ristic, Arulampalam, and Gordon (2004)
provide an excellent overview of some of the key developments.

15.6. Relevant Literature
691
The Viterbi algorithm for ﬁnding the MAP assignment in an HMM was proposed by Viterbi
(1967); it is the ﬁrst incarnation of the variable elimination algorithm for MAP algorithm.
The application of collapsed particles to switching linear systems were proposed by Akashi
and Kumamoto (1977); Chen and Liu (2000); Doucet et al. (2000). Lerner et al. (2002) propose de-
terministic particle methods as an alternative to the sampling based approach, and demonstrate
signiﬁcant advantages in cases (such as fault diagnosis) where the distribution is highly peaked,
so that sampling would generate the same particle multiple times. Marthi et al. (2002) describe
an alternative sampling algorithm based on an MCMC approach with a decaying time window.
Boyen and Koller (1998b) were the ﬁrst to study the entanglement phenomenon explicitly and
to propose factorization of belief states as an approximation in DBN inference, describing an
algorithm that came to be known as the BK algorithm. They also provided a theoretical analysis
BK algorithm
demonstrating that, under certain conditions, the error accumulated over time remains bounded.
Boyen and Koller (1998a) extend these ideas to the smoothing task. Murphy and Weiss (2001)
suggested an algorithm that uses belief propagation within a time slice rather than a clique tree
algorithm, as well as an iterated variant of the BK algorithm. Boyen and Koller (1999) study
the properties of diﬀerent belief state factorizations and oﬀer some guidance on how to select
a factorization that leads to a good approximation; Paskin (2003b); Frogner and Pfeﬀer (2007)
suggest concrete heuristics for this adaptive process. Several methods that combine factorization
with particle-based methods have also been proposed (Koller and Fratkina 1998; Murphy 1999;
Lerner et al. 2000; Montemerlo et al. 2002; Ng et al. 2002).
The Kalman ﬁltering algorithm was proposed by (Kalman 1960; Kalman and Bucy 1961); a
simpler version was proposed as early as the nineteenth century (Thiele 1880). Normand and
Tritchler (1992) provide a derivation of the Kalman ﬁltering equations from a Bayesian network
perspective.
Many extensions and improvements have been developed over the years.
Bar-
Shalom, Li, and Kirubarajan (2001) provide a good overview of these methods, including the
extended Kalman ﬁlter, and a variety of methods for collapsing the Gaussian mixture in a
switching linear-dynamical system. Kim and Nelson (1998) reviews a range of deterministic and
MCMC-based methods for these systems.
Lerner et al. (2000) and Lerner (2002) describe an alternative collapsing algorithm that pro-
vides more ﬂexibility in deﬁning the Gaussian mixture; they also show how collapsing can be
applied in a factored system, where discrete variables are present in multiple clusters. Heskes
and Zoeter (2002) apply the EP algorithm to switching linear systems. Zoeter and Heskes (2006)
describe the relationship between the GPB algorithms and expectation propagation and provide
an experimental comparison of various collapsing methods. The unscented ﬁlter, which we de-
scribed in chapter 14, was ﬁrst developed in the context of a ﬁltering task by Julier and Uhlmann
(1997). It has also been used as a proposal distribution for particle ﬁltering (van der Merwe et al.
2000b,a), producing a ﬁlter of higher accuracy and asymptotic correctness guarantees.
When a temporal model is viewed in terms of the ground network it generates, it is amenable
to the application of a range of other approximate inference methods. In particular, the global
variational methods of section 11.5 have been applied to various classes of sequence-based
models (Ghahramani and Jordan 1997; Ghahramani and Hinton 1998).
Temporal models have been applied to very many real-world problems, too numerous to
list. Bar-Shalom, Li, and Kirubarajan (2001) describe the key role of these methods to target
tracking. Isard and Blake (1998a) ﬁrst proposed the use of particle ﬁltering for visual tracking
tasks; this approach, often called condensation, has been highly inﬂuential in the computer-

692
Chapter 15. Inference in Temporal Models
vision community and has led to much follow-on work. The use of probabilistic models has also
revolutionized the ﬁeld of mobile robotics, providing greatly improved solutions to the tasks of
navigation and mapping (Fox et al. 1999; Thrun et al. 2000); see Thrun et al. (2005) for a detailed
overview of this area. The use of particle ﬁltering, under the name Monte Carlo localization,
Monte Carlo
localization
has been particularly inﬂuential (Thrun et al. 2000; Montemerlo et al. 2002). However, factored
models, both separately and in combination with particle-based methods, have also played a
role in these applications (Murphy 1999; Paskin 2003b; Thrun et al. 2004,?), in particular as a
representation of complex maps. Dynamic Bayesian models are also playing a role in speech-
recognition systems (Zweig and Russell 1998; Bilmes and Bartels 2005) and in fault monitoring
and diagnosis of complex systems (Lerner et al. 2002).
Finally, we also refer the reader to Murphy (2002), who provides an excellent tutorial on
inference in DBNs.
15.7
Exercises
Exercise 15.1
Consider clique-tree message passing, described in section 15.2.2, where the messages in the clique tree of
ﬁgure 15.1 are passed ﬁrst from the beginning of the clique tree chain toward the end.
a. Show that if sum-product message passing is used, the backward messages (in the “downward” pass)
represent P(o((t+1):T ) | S(t+1)).
b. Show that if belief-update message passing is used, the backward messages (in the “downward” pass)
represent P(S(t+1) | o(1:T )).
Exercise 15.2⋆
Consider the smoothing task for HMMs, implemented using the clique tree algorithm described in sec-
tion 15.2.2. As discussed, the O(NT) space requirement may be computationally prohibitive in certain
settings. Let K be the space required to store the evidence at each time slice. Show how, by caching
a certain subset of messages, we can trade oﬀtime for space, resulting in an algorithm whose time
requirements are O(N 2T log T) and space requirements are O(N log T).
Exercise 15.3
Prove proposition 15.1.
Exercise 15.4⋆
a. Prove the entanglement theorem, theorem 15.1.
b. Is there any 2-TBN (not necessarily fully persistent) where (X ⊥Y | Z) holds persistently but
(X ⊥Y | ∅) does not? If so, give an example. If not, explain formally why not.
Exercise 15.5
Consider a fully persistent DBN over n state variables X. Show that any clique tree over X(t), X(t+1)
that we can construct for performing the belief-state propagation step has induced width at least n + 1.
Exercise 15.6⋆
Recall that a hidden Markov model!factorialfactorial HMMfactorial HMM (see ﬁgure 6.3) is a DBN over
X1, . . . , Xn, O such that the only parent of X′
i in the 2-TBN is Xi, and the parents of O′ are X′
1, . . . , X′
n.
(Typically, some structured model is used to encode this CPD compactly.) Consider the problem of using a
structured variational approximation (as in section 11.5) to perform inference over the unrolled network for
a ﬁxed number T of time slices.

15.7. Exercises
693
a. Consider a space of approximate distributions Q composed of disjoint clusters {X(0)
i
, . . . , X(T )
i
} for
i = 1, . . . , n. Show the variational update equations, describe the use of inference to compute the
messages, and analyze the computational complexity of the resulting algorithm.
b. Consider a space of approximate distributions Q composed of disjoint clusters {X(1)
1 , . . . , X(t)
n } for
t = 1 . . . , T. Show the variational update equations, describe the use of inference to compute the
messages, and analyze the computational complexity of the resulting algorithm.
c. Discuss the circumstances when you would use one approximation over the other.
Exercise 15.7⋆
In this question, you will extend the particle ﬁltering algorithm to address the smoothing task, that is,
particle
smoothing
computing P(X(t) | o(1:T )) where T > t is the “end” of the sequence.
a. Prove using formal probabilistic reasoning that
P(X(t) | o(1:T )) = P(X(t) | o(1:t)) ·
X
X(t+1)
P(X(t+1) | o(1:T ))P(X(t+1) | X(t))
P(X(t+1) | o(1:t))
.
b. Based on this formula, construct an extension to the particle ﬁltering algorithm that:
•
Does a ﬁrst pass that is identical to particle ﬁltering, but where it keeps the samples ¯x(0:t)[m], w(t)[m]
generated for each time slice t.
•
Has a second phase that updates the weights of the samples at the diﬀerent time slices based on
the formula in part 1, with the goal of getting an approximation to P(X(t) | o(1:T )).
Write your algorithm using a few lines of pseudo-code. Provide a brief description of how you would
use your algorithm to estimate the probability of P(X(t) = x | o(1:T )) for some assignment x.
Exercise 15.8⋆
One of the problems with the particle ﬁltering algorithm is that it uses only the observations obtained so
far to select the samples that we continue to propagate. In many cases, the “right” choice of samples at
time t is not clear based on the evidence up to time t, but it manifests a small number of time slices into
the future. By that time, however, the relevant samples may have been eliminated in favor of ones that
appeared (based on the limited evidence available) to be better.
In this problem, your task is to extend the particle ﬁltering algorithm to deal with this problem. More
precisely, assume that the relevant evidence usually manifests within k time slices (for some small k).
Consider performing the particle-ﬁltering algorithm with a lookahead of k time slices rather than a single
time slice.
Present the algorithm clearly and mathematically, specifying exactly how the weights are
computed and how the next-state samples are generated. Brieﬂy explain why your algorithm is sampling
from (roughly) the right distribution (right in the same sense that standard particle ﬁltering is sampling
from the right distribution).
For simplicity of notation, assume that the process is structured as a state-observation model.
(Hint: Use your answer to exercise 15.7.)
Exercise 15.9⋆⋆
In this chapter, we have discussed only the application of particle ﬁltering that samples all of the network
variables.
In this exercise, you will construct a collapsed-particle-ﬁltering method.
Consider a DBN
where we have observed variables O, and the unobserved variables are divided into two disjoint groups
X and Y . Assume that, in the 2-TBN, the parents of X′ are only variables in X, and that we can
eﬃciently perform exact inference on P(Y ′ | X, X′, o′).
Describe an algorithm that uses collapsed
collapsed particle
ﬁltering
particle ﬁltering for this type of DBN, where we represent the approximate belief state ˆσ(t) using a set of
weighted collapsed particles, where the variables X(t) are sampled, and for each sample X(t)[m], we
have an exact distribution over the variables Y (t). Speciﬁcally, describe how each sample x(t+1)[m] is

694
Chapter 15. Inference in Temporal Models
generated from the time t samples, how to compute the associated distribution over Y (t), and how to
compute the appropriate importance weights. Make sure your derivation is consistent with the analysis
used in section 12.4.1.
Exercise 15.10
In this exercise, we apply the beam-search methods described in section 13.7 to the task of ﬁnding high-
beam search
probability assignments in an HMM. Assume that our hidden state in the HMM is denoted X(t) and
the observations are denoted O(t). Our goal is to ﬁnd x∗(1:T ) = arg maxx(1:T ) P(X(1:T ) | O(1:t)).
Importantly, however, we want to perform the beam search in an online fashion, adapting our set of
candidates as new observations arrive.
a. Describe how beam search can be applied in the context of an HMM. Specify the search procedure and
suggest a number of pruning strategies.
b. Now, assume we have an additional set of variables Y (t) that are part of our model and whose value
we do not care about. That is, our goal has not changed. Assume that our 2-TBN model does not
contain an arc Y →X′. Describe how beam search can be applied in this setting.
Exercise 15.11
Construct an algorithm that performs tracking in linear-Gaussian dynamical systems, maintaining the belief
state in terms of the canonical form described in section 14.2.1.2.
Exercise 15.12⋆
Consider a nonlinear 2-TBN where we have a set of observation variables O1, . . . , Ok, where each O′
i is a
leaf in the 2-TBN and has the parents U i ∈X′. Show how we can use the methods of exercise 14.9 to
perform the step of conditioning on the observation O′ = o′, without forming an entire joint distribution
over X′ ∪O.
Your method should perform the numerical integration over a space with as small a
dimension as possible.
Exercise 15.13⋆
a. Write down the probabilistic model for the Gaussian SLAM problem with K landmarks.
b. Derive the equations for a collapsed bootstrap-sampling particle-ﬁltering algorithm in FastSLAM. Show
how the samples are generated, how the importance weights are computed, and how the posterior is
maintained.
c. Derive the equations for the posterior collapsed-particle-ﬁltering algorithm, where x(t+1)[m] is gener-
ated from P(X(t+1) | x(t)[m], y(t+1)). Show how the samples are generated, how the importance
weights are computed, and how the posterior is maintained.
d. Now, consider a diﬀerent approach of applied collapsed-particle ﬁltering to this problem. Here, we
select the landmark positions L = {L1, . . . , Lk} as our set of sampled variables, and for each particle
l[m], we maintain a distribution over the robot’s state at time t. Without describing the details of this
algorithm, explain qualitatively what will happen to the particles and their weights eventually (as T
grows). Are there conditions under which this algorithm will converge to the correct map?

Part III
Learning


16
Learning Graphical Models: Overview
16.1
Motivation
In most of our discussions so far, our starting point has been a given graphical model. For
example, in our discussions of conditional independencies and of inference, we assumed that
the model — structure as well as parameters — was part of the input.
There are two approaches to the task of acquiring a model. The ﬁrst, as we discussed in
box 3.C, is to construct the network by hand, typically with the help of an expert. However,
as we saw, knowledge acquisition from experts is a nontrivial task. The construction of even
a modestly sized network requires a skilled knowledge engineer who spends several hours (at
least) with one or more domain experts. Larger networks can require weeks or even months of
work. This process also generally involves signiﬁcant testing of the model by evaluating results
of some “typical” queries in order to see whether the model returns plausible answers.
Such “manual” network construction is problematic for several reasons. In some domains,

the amount of knowledge required is just too large or the expert’s time is too valuable.
In others, there are simply no experts who have suﬃcient understanding of the domain.
In many domains, the properties of the distribution change from one application site to
another or over time, and we cannot expect an expert to sit and redesign the network
every few weeks. In many settings, however, we may have access to a set of examples generated
from the distribution we wish to model. In fact, in the Information Age, it is often easier to
obtain even large amounts of data in electronic form than it is to obtain human expertise. For
example, in the setting of medical diagnosis (such as box 3.D), we may have access to a large
collection of patient records, each listing various attributes such as the patient’s history (age,
sex, smoking, previous medical complications, and so on), reported symptoms, results of various
tests, the physician’s initial diagnosis and prescribed treatment, and the treatment’s outcome. We
may hope to use these data to learn a model of the distribution of patients in our population. In
the case of pedigree analysis (box 3.B), we may have some set of family trees where a particular
disease (for example, breast cancer) occurs frequently. We can use these family trees to learn the
parameters of the genetics of the disease: the transmission model, which describes how often
a disease genotype is passed from the parents to a child, and the penetrance model, which
deﬁnes the probability of diﬀerent phenotypes given the genotype. In an image segmentation
application (box 4.B), we might have a set of images segmented by a person, and we might wish
to learn the parameters of the MRF that deﬁne the characteristics of diﬀerent regions, or those
that deﬁne how strongly we believe that two neighboring pixels should be in the same segment.

698
Chapter 16. Learning Graphical Models: Overview
It seems clear that such instances can be of use in constructing a good model for the
underlying distribution, either in isolation or in conjunction with some prior knowledge acquired
from a human. This task of constructing a model from a set of instances is generally called
model learning. In this part of the book, we focus on methods for addressing diﬀerent variants
of this task. In the remainder of this chapter, we describe some of these variants and some of
the issues that they raise.
To make this discussion more concrete, let us assume that the domain is governed by some
underlying distribution P ∗, which is induced by some (directed or undirected) network model
M∗= (K∗, θ∗). We are given a data set D = {d[1], . . . , d[M]} of M samples from P ∗.
The standard assumption, whose statistical implications were brieﬂy discussed in appendix A.2,
is that the data instances are sampled independently from P ∗; as we discussed, such data
instances are called independent and identically distributed (IID). We are also given some family
IID samples
of models, and our task is to learn some model
˜
M in this family that deﬁnes a distribution
P ˜
M (or ˜P when
˜
M is clear from the context).
We may want to learn only model parameters
for a ﬁxed structure, or some or all of the structure of the model. In some cases, we might wish
to present a spectrum of diﬀerent hypotheses, and so we might return not a single model but
rather a probability distribution over models, or perhaps some estimate of our conﬁdence in the
model learned.
We ﬁrst describe the set of goals that we might have when learning a model and the diﬀerent
evaluation metrics to which they give rise. We then discuss how learning can be viewed as
an optimization problem and the issues raised by the design of that problem.
Finally, we
provide a detailed taxonomy of the diﬀerent types of learning tasks and discuss some of their
computational ramiﬁcations.
16.2
Goals of Learning
To evaluate the merits of diﬀerent learning methods, it is important to consider our goal in
learning a probabilistic model from data. A priori, it is not clear why the goal of the learning
is important. After all, our ideal solution is to return a model
˜
M that precisely captures the
distribution P ∗from which our data were sampled. Unfortunately, this goal is not generally
achievable, because of computational reasons and (more importantly) because a limited data
set provides only a rough approximation of the true underlying distribution. In practice, the

amount of data we have is rarely suﬃcient to obtain an accurate representation of a
high-dimensional distribution involving many variables. Thus, we have to select
˜
M so
as to construct the “best” approximation to M∗. The notion of “best” depends on our
goals. Diﬀerent models will generally embody diﬀerent trade-oﬀs. One approximate model
may be better according to one performance metric but worse according to another. Therefore,
to guide our development of learning algorithms, we must deﬁne the goals of our learning task
and the corresponding metrics by which diﬀerent results will be evaluated.
16.2.1
Density Estimation
The most common reason for learning a network model is to use it for some inference task
that we wish to perform. Most obviously, as we have discussed throughout most of this book so
far, a graphical model can be used to answer a range of probabilistic inference queries. In this

16.2. Goals of Learning
699
setting, we can formulate our learning goal as one of density estimation: constructing a model
density
estimation
˜
M such that ˜P is “close” to the generating distribution P ∗.
How do we evaluate the quality of an approximation
˜
M? One commonly used option is to
use the relative entropy distance measure deﬁned in deﬁnition A.5:
ID(P ∗|| ˜P) = IEξ∼P ∗

log
P ∗(ξ)
˜P(ξ)

.
Recall that this measure is zero when ˜P = P ∗and positive otherwise. Intuitively, it measures
the extent of the compression loss (in bits) of using ˜P rather than P ∗.
To evaluate this metric, we need to know P ∗. In some cases, we are evaluating a learning
algorithm on synthetically generated data, and so P ∗may be known. In real-world applications,
however, P ∗is not known. (If it were, we would not need to learn a model for it from a data
set.) However, we can simplify this metric to obtain one that is easier to evaluate:
Proposition 16.1
For any distributions P, P ′ over X:
ID(P||P ′) = −IHP (X) −IEξ∼P [log P ′(ξ)].
Proof
ID(P||P ′)
=
IEξ∼P

log
 P(ξ)
P ′(ξ)

=
IEξ∼P [log P(ξ) −log P ′(ξ)]
=
IEξ∼P [log P(ξ)] −IEξ∼P [log P ′(ξ)]
=
−IHP (X) −IEξ∼P [log P ′(ξ)].
Applying this derivation to P ∗, ˜P, we see that the ﬁrst of these two terms is the negative
entropy of P ∗; because it does not depend on ˜P, it does not aﬀect the comparison between
diﬀerent approximate models. We can therefore focus our evaluation metric on the second term
IEξ∼P ∗
h
log ˜P(ξ)
i
and prefer models that make this term as large as possible. This term is called
an expected log-likelihood. It encodes our preference for models that assign high probability to
expected
log-likelihood
instances sampled from P ∗.
Intuitively, the higher the probability that
˜
M gives to points
sampled from the true distribution, the more reﬂective it is of this distribution. We note that,
in moving from the relative entropy to the expected log-likelihood, we have lost our baseline
IEP ∗[log P ∗], an inevitable loss since we do not know P ∗. As a consequence, although we
can use the log-likelihood as a metric for comparing one learned model to another, we cannot
evaluate a particular
˜
M in how close it is to the unknown optimum.
More generally, in our discussion of learning we will be interested in the likelihood of the
likelihood
data, given a model M, which is P(D : M), or for convenience using the log-likelihood
log-likelihood
ℓ(D : M) = log P(D : M).
It is also customary to consider the negated form of the log-likelihood, called the log-loss. The
log-loss
log-loss reﬂects our cost (in bits) per instance of using the model ˜P. The log-loss is our ﬁrst
example of a loss function, a key component in the statistical machine-learning paradigm. A
loss function
loss function loss(ξ : M) measures the loss that a model M makes on a particular instance

700
Chapter 16. Learning Graphical Models: Overview
ξ. When instances are sampled from some distribution P ∗, our goal is to ﬁnd a model that
minimizes the expected loss, or the risk:
risk
IEξ∼P ∗[loss(ξ : M)].
In general, of course, P ∗is unknown. However, we can approximate the expectation using an
empirical average and estimate the risk relative to P ∗with an empirical risk averaged over a set
empirical risk
D of instances sampled from P ∗:
IED[loss(ξ : M)] =
1
|D|
X
ξ∈D
loss(ξ : M).
(16.1)
In the case of the log-loss, this expression has a very intuitive interpretation. Consider a data
set D = {ξ[1], . . . , ξ[M]} composed of IID instances. The probability that M ascribes to D is
P(D : M) =
M
Y
m=1
P(ξ[m] : M).
Taking the logarithm, we obtain
log P(D : M) =
M
X
m=1
log P(ξ[m] : M),
which is precisely the negative of the empirical log-loss that appears inside the summation of
equation (16.1).
The risk can be used both as a metric for evaluating the quality of a particular model and as
a factor for selecting a model among a given class, given a training set D. We return to these
ideas in section 16.3.1 and box 16.A.
16.2.2
Speciﬁc Prediction Tasks
In the preceding discussion, we assumed that our goal was to use the learning model to perform
probabilistic inference. With that assumption, we jumped to the conclusion that we wish to ﬁt
the overall distribution P ∗as well as possible. However, that objective measures only our ability
to evaluate the overall probability of a full instance ξ. In reality, the model can be used for
answering a whole range of queries of the form P(Y | X). In general, we can devise a test
suite of queries for our learned model, which allows us to evaluate its performance on a range of
queries. Most attention, however, has been paid to the special case where we have a particular
set of variables Y that we are interested in predicting, given a certain set of variables X.
Most simply, we may want to solve a simple classiﬁcation task, the goal of a large fraction of
classiﬁcation task
the work in machine learning. For example, consider the task of document classiﬁcation, where
we have a set X of words and other features characterizing the document, and a variable Y
that labels the document topic. In image segmentation, we are interested in predicting the class
labels for all of the pixels in the image (Y ), given the image features (X). There are many other
such examples.

16.2. Goals of Learning
701
A model trained for a prediction task should be able to produce for any instance characterized
by x, the probability distribution ˜P(Y | x). We might also wish to select the MAP assignment
of this conditional distribution to produce a speciﬁc prediction:
h ˜
P (x) = arg max
y
˜P(y | x).
What loss function do we want to use for evaluating a model designed for a prediction task?
We can, for example, use the classiﬁcation error, also called the 0/1 loss:
classiﬁcation
error
0/1 loss
IE(x,y)∼˜
P [11{h ˜
P (x) ̸= y}],
(16.2)
which is simply the probability, over all (x, y) pairs sampled from ˜P, that our classiﬁer selects
the wrong label. While this metric is suitable for labeling a single variable, it is not well suited
to situations, such as image segmentation, where we simultaneously provide labels to a large
number of variables. In this case, we do not want to penalize an entire prediction with an error
of 1 if we make a mistake on only a few of the target variables. Thus, in this case, we might
consider performance metrics such as the Hamming loss, which, instead of using the indicator
Hamming loss
function 11{h ˜
P (x) ̸= y}, counts the number of variables Y in which h ˜
P (x) diﬀers from the
ground truth y.
We might also wish to take into account the conﬁdence of the prediction. One such criterion
is the conditional likelihood or its logarithm (sometimes called the conditional log-likelihood):
conditional
likelihood
IE(x,y)∼P ∗
h
log ˜P(y | x)
i
.
(16.3)
Like the log-likelihood criterion, this metric evaluates the extent to which our learned model is
able to predict data generated from the distribution. However, it requires the model to predict
only Y given X, and not the distribution of the X variables. As before, we can negate this
expression to deﬁne a loss function and compute an empirical estimate by taking the average
relative to a data set D.

If we determine, in advance, that the model will be used only to perform a particular
task, we may want to train the model to make trade-oﬀs that make it more suited to that
task. In particular, if the model is never evaluated on predictions of the variables X, we
may want to design our training regime to optimize the quality of its answers for Y . We
return to this issue in section 16.3.2.
16.2.3
Knowledge Discovery
Finally, a very diﬀerent motivation for learning a model for a distribution P ∗is as a tool for
discovering knowledge about P ∗. We may hope that an examination of the learned model can
knowledge
discovery
reveal some important properties of the domain: what are the direct and indirect dependencies,
what characterizes the nature of the dependencies (for example, positive or negative correlation),
and so forth. For example, in the genetic inheritance domain, it may be of great interest to
discover the parameter governing the inheritance of a certain property, since this parameter
can provide signiﬁcant biological insight regarding the inheritance mechanism for the allele(s)
governing the disease. In a medical diagnosis domain, we may want to learn the structure of
the model to discover which predisposing factors lead to certain diseases and which symptoms

702
Chapter 16. Learning Graphical Models: Overview
are associated with diﬀerent diseases. Of course, simpler statistical methods can be used to
explore the data, for example, by highlighting the most signiﬁcant correlations between pairs of
variables. However, a learned network model can provide parameters that have direct causal
interpretation and can also reveal much ﬁner structure, for example, by distinguishing between
direct and indirect dependencies, both of which lead to correlations in the resulting distribution.
The knowledge discovery task calls for a very diﬀerent evaluation criterion and a diﬀerent set
of compromises from a prediction task. In this setting, we really do care about reconstructing
the correct model M∗, rather than some other model
˜
M that induces a distribution similar to
M∗. Thus, in contrast to density estimation, where our metric was on the distribution deﬁned
by the model (for example, ID(P ∗|| ˜P)), here our measure of success is in terms of the model,
that is, diﬀerences between M∗and
˜
M. Unfortunately, this goal is often not achievable, for
several reasons.
First, even with large amounts of data, the true model may not be identiﬁable. Consider, for
identiﬁability
example, the task of recovering aspects of the correct network structure K∗. As one obvious
diﬃculty, recall that a given Bayesian network structure often has several I-equivalent structures.
If such is the case for our target distribution K∗, the best we can hope for is to recover an
I-equivalent structure. The problems are signiﬁcantly greater when the data are limited. Here,
for example, if X and Y are directly related in K∗but the parameters relating them induce only
a weak relationship, it may be very diﬃcult to detect the correlation in the data and distinguish
it from a random ﬂuctuations. This limitation is less of a problem for a density estimation task,
where ignoring such weak correlations often has very few repercussions on the quality of our
learned density; however, if our task focuses on correct reconstruction of structure, examples
such as this reduce our accuracy. Conversely, when the number of variables is large relative
to the amount of the training data, there may well be pairs of variables that appear strongly
correlated simply by chance. Thus, we are also likely, in such settings, to infer the presence of
edges that do not exist in the underlying model. Similar issues arise when attempting to infer
other aspects of the model.
The relatively high probability of making model identiﬁcation errors can be signiﬁcant if the
goal is to discover the correct structure of the underlying distribution. For example, if our goal
is to infer which genes regulate which other genes (as in box 21.D), and if we plan to use the
results of our analysis for a set of (expensive and time-consuming) wet-lab experiments, we may
want to have some conﬁdence in the inferred relationship. Thus, in a knowledge discovery

application, it is far more critical to assess the conﬁdence in a prediction, taking into
account the extent to which it can be identiﬁed given the available data and the number
of hypotheses that would give rise to similar observed behavior. We return to these issues
more concretely later on in the book (see, in particular, section 18.5).
16.3
Learning as Optimization
The previous section discussed diﬀerent ways in which we can evaluate our learned model. In
many of the cases, we deﬁned a numerical criterion — a loss function — that we would like to
optimize. This perspective suggests that the learning task should be viewed as an optimization
problem: we have a hypothesis space, that is, a set of candidate models, and an objective function,
hypothesis space
objective
function
a criterion for quantifying our preference for diﬀerent models. Thus, our learning task can be

16.3. Learning as Optimization
703
formulated as one of ﬁnding a high-scoring model within our model class. The view of learning
as optimization is currently the predominant approach to learning (not only of probabilistic
models).
In this section, we discuss diﬀerent choices of objective functions and their ramiﬁcation on the
results of our learning procedure. This discussion raises important points that will accompany
us throughout this part of the book. We note that the formal foundations for this discussion will
be established in later chapters, but the discussion is of fundamental importance to our entire
discussion of learning, and therefore we introduce these concepts here.
16.3.1
Empirical Risk and Overﬁtting
Consider the task of constructing a model M that optimizes the expectation of a particular
loss function IEξ∼P ∗[loss(ξ : M)]. Of course, we generally do not know P ∗, but, as we have
discussed, we can use a data set D sampled from P ∗to produce an empirical estimate for this
expectation. More formally, we can use the data D to deﬁne an empirical distribution ˆPD, as
empirical
distribution
follows:
ˆPD(A) = 1
M
X
m
11{ξ[m] ∈A}.
(16.4)
That is, the probability of the event A is simply the fraction of training examples that satisfy
A. It is clear that ˆPD(A) is a probability distribution. Moreover, as the number of training
examples grows, the empirical distribution approaches the true distribution.
Theorem 16.1
Let ξ[1], ξ[2], . . . be a sequence of IID samples from P ∗(X), and let DM = ⟨ξ[1], . . . , ξ[M]⟩, then
lim
M→∞
ˆPDM (A) = P ∗(A)
almost surely.
Thus, for a suﬃciently large training set, ˆPD will be quite close to the original distribution
P ∗with high probability (one that converges to 1 as M →∞). Since we do not have direct
access to P ∗, we can use ˆPD as the best proxy and try to minimize our loss function relative
to ˆPD. Unfortunately, a naive application of this optimization objective can easily lead to very
poor results.
Consider, for example, the use of the empirical log-loss (or log-likelihood) as the objective. It
is not diﬃcult to show (see section 17.1) that the distribution that maximizes the likelihood of a
data set D is the empirical distribution ˆPD itself. Now, assume that we have a distribution over
a probability space deﬁned by 100 binary random variables, for a total of 2100 possible joint
assignments. If our data set D contains 1,000 instances (most likely distinct from each other),
the empirical distribution will give probability 0.001 to each of the assignments that appear in
D, and probability 0 to all 2100 −1, 000 other assignments. While this example is obviously
extreme, the phenomenon is quite general. For example, assume that M∗is a Bayesian network
where some variables, such as Fever, have a large number of parents X1, . . . , Xk. In a table-
CPD, the number of parameters grows exponentially with the number of parents k. For large
k, we are highly unlikely to encounter, in D, instances that are relevant to all possible parent
instantiations, that is, all possible combinations of diseases X1, . . . , Xk. If we do not have

704
Chapter 16. Learning Graphical Models: Overview
enough data, many of the cases arising in our CPD will have very little (or no) relevant training
data, leading to very poor estimates of the conditional probability of Fever in this context. In
general, as we will see in later chapters, the amount of data required to estimate parameters
reliably grows linearly with the number of parameters, so that the amount of data required can
grow exponentially with the network connectivity.
As we see in this example, there is a signiﬁcant problem with using the empirical risk (the
loss on our training data) as a surrogate for our true risk. In particular, this type of objective
tends to overﬁt the learned model to the training data. However, our goal is to answer queries
overﬁtting
about examples that were not in our training set. Thus, for example, in our medical diagnosis
example, the patients to which the learned network will be applied are new patients, not the
ones on whose data the network was trained. In our image-segmentation example, the model
will be applied to new (unsegmented) images, not the (segmented) images on which the model
was trained. Thus, it is critical that the network generalize to perform well on unseen data.
generalization
The need to generalize to unseen instances and the risk of overﬁtting to the training set raise
an important trade-oﬀthat will accompany us throughout our discussion. On one hand, if our
hypothesis space is very limited, it might not be able to represent our true target distribution
P ∗. Thus, even with unlimited data, we may be unable to capture P ∗, and thereby remain with
a suboptimal model. This type of limitation in a hypothesis space introduces inherent error in
the result of the learning procedure, which is called bias, since the learning procedure is limited
in how close it can approximate the target distribution. Conversely, if we select a hypothesis
space that is highly expressive, we are more likely to be able to represent correctly the target
distribution P ∗. However, given a small data set, we may not have the ability to select the “right”
model among the large number of models in the hypothesis space, many of which may provide
equal or perhaps even better loss on our limited (and thereby unrepresentative) training set D.
Intuitively, when we have a rich hypothesis space and limited number of samples, small random
ﬂuctuations in the choice of D can radically change the properties of the selected model, often
resulting in models that have little relationship to P ∗. As a result, the learning procedure will
suﬀer from a high variance — running it on multiple data sets from the same P ∗will lead to
highly variable results. Conversely, if we have a more limited hypothesis space, we are less likely
to ﬁnd, by chance, a model that provides a good ﬁt to D. Thus, a high-scoring model within
our limited hypothesis space is more likely to be a good ﬁt to P ∗, and thereby is more likely to
generalize to unseen data.

This bias-variance trade-oﬀunderlies many of our design choices in learning. When
bias-variance
trade-oﬀ
selecting a hypothesis space of diﬀerent models, we must take care not to allow too
rich a class of possible models.
Indeed, with limited data, the error introduced by
variance may be larger than the potential error introduced by bias, and we may choose
to restrict our learning to models that are too simple to correctly encode P ∗. Although the
learned model is guaranteed to be incorrect, our ability to estimate its parameters more
reliably may well compensate for the error arising from incorrect structural assumptions.
Moreover, when learning structure, although we will not correctly learn all of the edges, this
restriction may allow us to more reliably learn the most important edges.
In other words,
restricting the space of possible models leads us to select models
˜
M whose performance on
the training objective is poorer, but whose distance to P ∗is better.
Restricting our model class is one way to reduce overﬁtting. In eﬀect, it imposes a hard
constraint that prevents us from selecting a model that precisely captures the training data. A

16.3. Learning as Optimization
705
second, more reﬁned approach is to change our training objective so as to incorporate a soft
preference for simpler models. Thus, our learning objective will usually incorporate competing
components: some components will tend to move us toward models that ﬁt well with our
observed data; others will provide regularization that prevents us from taking the speciﬁcs of
regularization
the data to extremes.
In many cases, we adopt a combination of the two approaches,

utilizing both a hard constraint over the model class and an optimization objective that
leads us away from overﬁtting.
The preceding discussion described the phenomenon of overﬁtting and the importance of
ensuring that our learned model generalizes to unseen data.
However, we did not discuss
how to tell whether our model generalizes, and how to design our hypothesis space and/or
objective function so as to reduce this risk. Box 16.A discusses some of the basic experimental
protocols that one uses in the design and evaluation of machine learning procedures. Box 16.B
discusses a basic theoretical framework that one can use to try and answer questions regarding
the appropriate complexity of our model class.
Box 16.A — Skill: Design and Evaluation of Learning Procedures. A basic question in learn-
ing is to evaluate the performance of our learning procedure. We might ask this question in a rela-
tive sense, to compare two or more alternatives (for example, diﬀerent hypothesis spaces, or diﬀerent
training objectives), or in an absolute sense, when we want to test whether the model we have
learned “captures” the distribution. Both questions are nontrivial, and there is a large literature on
how to address them. We brieﬂy summarize some of the main ideas here.
In both cases, we would ideally like to compare the learned model to the real underlying distri-
bution that generated the data. This is indeed the strategy we use for evaluating performance when
learning from synthetic data sets where we know (by design) the generating distribution (see, for
example, box 17.C). Unfortunately, this strategy is infeasible when we learn from real-life data sets
where we do not have access to the true generating distribution. And while synthetic studies can
help us understand the properties of learning procedures, they are limited in that they are often not
representative of the properties of the actual data we are interested in.
Evaluating Generalization Performance
We start with the ﬁrst question of evaluating the per-
formance of a given model, or a set of models, on unseen data. One approach is to use holdout
holdout testing
testing. In this approach, we divide our data set into two disjoint sets: the training set Dtrain
training set
and test set Dtest. To avoid artifacts, we usually use a randomized procedure to decide on this
test set
partition. We then learn a model using Dtrain (with some appropriate objective function), and
we measure our performance (using some appropriate loss function) on Dtest. Because Dtest is
also sampled from P ∗, it provides us with an empirical estimate of the risk. Importantly, however,
because Dtest is disjoint from Dtrain, we are measuring our loss using instances that were unseen
during the training, and not on ones for which we optimized our performance. Thus, this approach
provides us with an unbiased estimate of the performance on new instances.
Holdout testing can be used to compare the performance of diﬀerent learning procedures. It can
also be used to obtain insight into the performance of a single learning procedure. In particular,
we can compare the performance of the procedure (say the empirical log-loss per instance, or the
classiﬁcation error) on the training set and on the held-out test set. Naturally, the training set
performance will be better, but if the diﬀerence is very large, we are probably overﬁtting to the

706
Chapter 16. Learning Graphical Models: Overview
training data and may want to consider a less expressive model class, or some other method for
discouraging overﬁtting.
Holdout testing poses a dilemma. To get better estimates of our performance, we want to increase
the size of the test set. Such an increase, however, decreases the size of the training set, which
results in degradation of quality of the learned model. When we have ample training data, we
can ﬁnd reasonable compromises between these two considerations. When we have few training
samples, there is no good compromise, since decreasing either the training or the test set has large
ramiﬁcations either on the quality of the learned model or the ability to evaluate it.
An alternative solution is to attempt to use available data for both training and testing. Of
course, we cannot test on our training data; the trick is to combine our estimates of performance
from repeated rounds of holdout testing. That is, in each iteration we train on some subset of the
instances and test on the remaining ones. If we perform multiple iterations, we can use a relatively
small test set in each iteration, and pool the performance counts from all of them to estimate
performance. The question is how to allocate the training and test data sets. A commonly used
procedure is k-fold cross-validation, where we use each instance once for testing. This is done by
cross-validation
partitioning the original data into k equally sized sets, and then in each iteration holding as test
data one partition and training from all the remaining instances; see algorithm 16.A.1. An extreme
case of cross-validation is leave one out cross-validation, where we set k = M, that is, in each
iteration we remove one instance and use it as a testing case. Both cross-validation schemes allow
us to estimate not only the average performance of our learning algorithm, but also the extent to
which this performance varies across the diﬀerent folds.
Both holdout testing and cross-validation are primarily used as methods for evaluating a learning
procedure. In particular, a cross-validation procedure constructs k diﬀerent models, one for each
partition of the training set into training/test folds, and therefore does not result in even a single
model that we can subsequently use. If we want to write a paper on a new learning procedure, the
results of cross-validation provide a good regime for evaluating our procedure and comparing it to
others. If we actually want to end up with a real model that we can use in a given domain, we
would probably use cross-validation or holdout testing to select an algorithm and ensure that it is
working satisfactorily, but then train our model on the entire data set D, thereby making use of the
maximum amount of available data to learn a single model.
Selecting a Learning Procedure
One common use for these evaluation procedures is as a mech-
anism for selecting a learning algorithm that is likely to perform well on a particular application.
That is, we often want to choose among a (possibly large) set of diﬀerent options for our learning
procedure: diﬀerent learning algorithms, or diﬀerent algorithmic parameters for the same algo-
rithm (for example, diﬀerent constraints on the complexity of the learned network structures). At
ﬁrst glance, it is straightforward to use holdout testing or cross-validation for this purpose: we take
each option LearnProcj, evaluate its performance, and select the algorithm whose estimated loss is
smallest. While this use is legitimate, it is also tempting to use the performance estimate that we
obtained using this procedure as a measure for how well our algorithm will generalize to unseen
data. This use is likely to lead to misleading and overly optimistic estimates of performance, since
we have selected our particular learning procedure to optimize for this particular performance
metric. Thus, if we use cross-validation or a holdout set to select a learning procedure, and we
want to have an unbiased estimate of how well our selected procedure will perform on unseen data,
we must hold back a completely separate test set that is never used in selecting any aspect of the

16.3. Learning as Optimization
707
Algorithm 16.A.1 — Algorithms for holdout and cross-validation tests.
Procedure Evaluate (
M,
// parameters to evaluate
D
// test data set
)
1
loss ←0
2
for m = 1, . . . , M
3
loss ←loss + loss(ξ[m] : M)
4
return loss
M
Procedure Train-And-Test (
LearnProc,
// Learning procedure
Dtrain,
// Training data
Dtest,
// Test data
)
1
M ←LearnProc(Dtrain)
2
return Evaluate(M, Dtest)
Procedure Holdout-Test (
LearnProc,
// Learning procedure
D,
// Data Set
ptest
// Fraction of data for testing
)
1
Randomly reshuﬄe instances in D
2
Mtrain ←round(M · (1 −ptest))
3
Dtrain ←{ξ[1], . . . , ξ[Mtrain]}
4
Dtest ←{ξ[Mtrain + 1], . . . , ξ[M]}
5
return Train-And-Test(LearnProc, Dtrain, Dtest)
Procedure Cross-Validation (
LearnProc,
// Learning procedure
D,
// Data Set
K,
// number of cross-validation folds
)
1
Randomly reshuﬄe instances in D
2
Partition D into K disjoint data sets D1, . . . , DK
3
loss ←0
4
for k = 1, . . . , K
5
D−k ←D −Dk
6
loss ←loss + Train-And-Test(LearnProc, D−k, Dk)
7
return loss
K

708
Chapter 16. Learning Graphical Models: Overview
model, on which our model’s ﬁnal performance will be evaluated. In this setting, we might have: a
training set, using which we learn the model; a validation set, which we use to evaluate diﬀerent
validation set
variants of our learning procedure and select among them; and a separate test set, on which our
ﬁnal performance is actually evaluated. This approach, of course, only exacerbates the problem of
fragmenting our training data, and so one can develop nested cross-validation schemes that achieve
the same goal.
Goodness of Fit
Cross-validation and holdout tests allow us to evaluate performance of diﬀerent
learning procedures on unseen data. However, without a “gold standard” for comparison, they do not
allow us to evaluate whether our learned model really captures everything there is to capture about
the distribution. This question is inherently harder to answer. In statistics, methods for answering
such questions fall under the category of goodness of ﬁt tests. The general idea is the following.
goodness of ﬁt
After learning the parameters, we have a hypothesis about a distribution that generated the data.
Now we can ask whether the data behave as though they were sampled from this distribution. To
do this, we compare properties of the training data set to properties of simulated data sets of the
same size that we generate according to the learned distribution. If the training data behave in a
manner that deviates signiﬁcantly from what we observed in the majority of the simulations, we
have reason to believe that the data were not generated from the learned distribution.
More precisely, we consider some property f of data sets, and evaluate f(Dtrain) for the training
set. We then generate new data sets D from our learned model M and evaluate f(D) for these
randomly generated data sets.
If f(Dtrain) deviates signiﬁcantly from the distribution of the
values f(D) among our randomly sampled data sets, we would probably reject the hypothesis that
Dtrain was generated from M. Of course, there are many choices regarding which properties
f we should evaluate. One natural choice is to deﬁne f as the empirical log-loss in the data set,
IED[loss(ξ : M)], as per equation (16.1). We can then ask whether the empirical log-loss for Dtrain
diﬀers signiﬁcantly from the expected empirical log-loss for data set D sampled from M. Note that
the expected value of this last expression is simply the entropy of M, and, as we saw in section 8.4,
we can compute the entropy of a Bayesian network fairly eﬃciently. To check for signiﬁcance, we
also need to consider the tail distribution of the log-loss, which is more involved. However, we
can approximate that computation by computing the variance of the log-loss as a function of M.
Alternatively, because generating samples from a Bayesian network is relatively inexpensive (as in
section 12.1.1), we might ﬁnd it easier to generate a large number of data sets D of size M sampled
from the model and use those to estimate the distribution over IED[loss(ξ : M)].
Box 16.B — Concept: PAC-bounds. As we discussed, given a target loss function, we can esti-
mate the empirical risk on our training set Dtrain. However, because of possible overﬁtting to the
training data, the performance of our learned model on the training set might not be representative
of its performance on unseen data. One might hope, however, that these two quantities are related,
so that a model that achieves low training loss also achieves low expected loss (risk).
Before we tackle a proof of this type, however, we must realize that we cannot guarantee with
certainty the quality of our learned model. Recall that the data set D is sampled stochastically from
P ∗, so there is always a chance that we would have “bad luck” and sample a very unrepresentative

16.3. Learning as Optimization
709
data set from P ∗. For example, we might sample a data set where we get the same joint assignment
in all of the instances. It is clear that we cannot expect to learn useful parameters from such a data
set (assuming, of course, that P ∗is not degenerate). The probability of getting such a data set is
very low, but it is not zero. Thus, our analysis must allow for the chance that our data set will be
highly unrepresentative, in which case our learned model (which presumably performed well on the
training set) may not perform well on expectation.
Our goal is then to prove that our learning procedure is probably approximately correct: that is,
probably
approximately
correct
for most training sets D, the learning procedure will return a model whose error is low. Making
this discussion concrete, assume we use relative entropy to the true distribution as our loss function.
Let P ∗
M be the distribution over data sets D of size M sampled IID from P ∗. Now, assume that we
have a learning procedure L that, given a data set D, returns a model ML(D). We want to prove
results of the form:
Let ϵ > 0 be our approximation parameter and δ > 0 our conﬁdence parameter. Then, for
M “large enough,” we have that
P ∗
M({D : ID(P ∗||PML(D)) ≤ϵ}) ≥1 −δ.
That is, for suﬃciently large M, we have that, for most data sets D of size M sampled from P ∗, the
learning procedure, applied to D, will learn a close approximation to P ∗. The number of samples
M required to achieve such a bound is called the sample complexity. This type of result is called
sample
complexity
a PAC-bound.
PAC-bound
This type of bound can only be obtained if the hypothesis space contains a model that can
correctly represent P ∗. In many cases, however, we are learning with a hypothesis space that is
not guaranteed to be able to express P ∗. In this case, we cannot expect to learn a model whose
relative entropy to P ∗is guaranteed to be low. In such a setting, the best we can hope for is to get
a model whose error is at most ϵ worse than the lowest error found within our hypothesis space.
The expected loss beyond the minimal possible error is called the excess risk. See section 17.6.2.2
excess risk
for one example of a generalization bound for this case.
16.3.2
Discriminative versus Generative Training
In the previous discussion, we implicitly assumed that our goal is to get the learned model
˜
M
to be a good approximation to P ∗. However, as we discussed in section 16.2.2, we often know
in advance that we want the model to perform well on a particular task, such as predicting Y
from X. The training regime that we described would aim to get
˜
M close to the overall joint
distribution P ∗(Y , X). This type of objective is known as generative training, because we are
generative
training
training the model to generate all of the variables, both the ones that we care to predict and the
features that we use for the prediction. Alternatively, we can train the model discriminatively,
discriminative
training
where our goal is to get ˜P(Y | X) to be close to P ∗(Y | X). The same model class can be
trained in these two diﬀerent ways, producing diﬀerent results.

710
Chapter 16. Learning Graphical Models: Overview
Example 16.1
As the simplest example, consider a simple “star” Markov network structure with a single target
variable Y connected by edges to each of a set of features X1, . . . , Xn. If we train the model
generatively, we are learning a naive Markov model, which, because the network is singly connected,
naive Markov
is equivalent to a naive Bayes model. On the other hand, we can train the same network structure
discriminatively, to obtain a good ﬁt to P ∗(Y | X1, . . . , Xn). In this case, as we showed in
example 4.20, we are learning a model that is a logistic regression model for Y given its features.
Note that a model that is trained generatively can still be used for speciﬁc prediction tasks. For
example, we often train a naive Bayes model generatively but use it for classiﬁcation. However, a
model that is trained for a particular prediction task P(Y | X) does not encode a distribution
over X, and hence it cannot be used to reach any conclusions about these variables.
Discriminative training can be used for any class of models. However, its application in the
context of Bayesian networks is less appealing, since this form of training changes the inter-
pretation of the parameters in the learned model. For example, if we discriminatively train a
(directed) naive Bayes model, as in example 16.1, the resulting model would essentially represent
the same logistic regression as before, except that the pairwise potentials between Y and each
Xi would be locally normalized to look like a CPD. Moreover, most of the computational prop-
erties that facilitate Bayesian network learning do not carry through to discriminative training.
For this reason, discriminative training is usually performed in the context of undirected models.
In this setting, we are essentially training a conditional random ﬁeld (CRF), as in section 4.6.1: a
conditional
random ﬁeld
model that directly encodes a conditional distribution P(Y | X).
There are various trade-oﬀs between generative and discriminative training, both statistical
and computational, and the question of which to use has been the topic of heated debates. We
now brieﬂy enumerate some of these trade-oﬀs.
Generally speaking, generative models have a higher bias — they make more assumptions
bias
about the form of the distribution. First, they encode independence assumptions about the
feature variables X, whereas discriminative models make independence assumptions only about
Y and about their dependence on X. An alternative intuition arises from the following view.
A generative model deﬁnes ˜P(Y , X), and thereby also induces ˜P(Y | X) and ˜P(X), using
the same overall model for both. To obtain a good ﬁt to P ∗, we must therefore tune our model
to get good ﬁts to both P ∗(Y | X) and P ∗(X). Conversely, a discriminative model aims to
get a good ﬁt only to P ∗(Y | X), without constraining the same model to provide a good ﬁt
to P ∗(X) as well.

The additional bias in the setting oﬀers a standard trade-oﬀ. On one hand, it can
help regularize and constrain the learned model, thereby reducing its ability to overﬁt
the data. Therefore, generative training often works better when we are learning from
limited amounts of data. However, imposing constraints can hurt us when the constraints are
wrong, by preventing us from learning the correct model. In practice, the class of models we use
always imposes some constraints that do not hold in the true generating distribution P ∗. For
limited amounts of data, the constraints might still help reduce overﬁtting, giving rise to better
generalization. However, as the amount of data grows, the bias imposed by the constraints

starts to dominate the error of our learned model. Because discriminative models make
fewer assumptions, they will tend to be less aﬀected by incorrect model assumptions and
will often outperform the generatively trained models for larger data sets.

16.4. Learning Tasks
711
Example 16.2
Consider the problem of optical character recognition — identifying letters from handwritten im-
ages. Here, the target variable Y is the character label (for example, “A”). Most obviously, we can
use the individual pixels as our feature variables X1, . . . , Xn. We can then either generatively
train a naive Markov model or discriminatively train a logistic regression model. The naive Bayes
(or Markov) model separately learns the distribution over the 256 pixel values given each of the 26
labels; each of these is estimated independently, giving rise to a set of fairly low-dimensional estima-
tion problems. Conversely, the discriminative model is jointly optimizing all of the approximately
26 × 256 parameters of the multinomial logit distribution, a much higher-dimensional estimation
problem. Thus, for sparse data, the naive Bayes model may often perform better.
However, even in this simple setting, the independence assumption made by the naive Bayes
model — that pixels are independent given the image label — is clearly false. As a consequence,
the naive Bayes model may be counting, as independent, features that are actually correlated,
leading to errors in the estimation. The discriminative model is not making these assumptions; by
ﬁtting the parameters jointly, it can compensate for redundancy and other correlations between the
features. Thus, as we get enough data to ﬁt the logistic model reasonably well, we would expect it
to perform better.
A related beneﬁt of discriminative models is that they are able to make use a much richer
feature set, where independence assumptions are clearly violated. These richer features can
often greatly improve classiﬁcation accuracy.
Example 16.3
Continuing our example, the raw pixels are fairly poor features to use for the image classiﬁcation
task. Much work has been spent by researchers in computer vision and image processing in devel-
oping richer feature sets, such as the direction of the edge at a given image pixel, the value of a
certain ﬁlter applied to an image patch centered at the pixel, and many other features that are even
more reﬁned. In general, we would expect to be able to classify images much better using these
features than using the raw pixels directly. However, each of these features depends on the values
of multiple pixels, and the same pixels are used in computing the values of many diﬀerent features.
Therefore, these features are certainly not independent, and using them in the context of a naive
Bayes classiﬁer is likely to lead to fairly poor answers. However, there is no reason not to include
such correlated features within a logistic regression or other discriminative classiﬁer.

Conversely, generative models have their own advantages.
They often oﬀer a more
natural interpretation of a domain. And they are better able to deal with missing values
and unlabeled data. Thus, the appropriate choice of model is application dependent, and
often a combination of diﬀerent training regimes may be the best choice.
16.4
Learning Tasks
We now discuss in greater detail the diﬀerent variants of the learning task.
As we brieﬂy
mentioned, the input of a learning procedure is:
•
Some prior knowledge or constraints about
˜
M.
•
A set D of data instances {d[1], . . . , d[M]}, which are independent and identically dis-
tributed (IID) samples from P ∗.

712
Chapter 16. Learning Graphical Models: Overview
The output is a model
˜
M, which may include the structure, the parameters, or both.
There are many variants of this fairly abstract learning problem; roughly speaking, they vary
along three axes, representing the two types of input and the type of output. First, and most
obviously, the problem formulation depends on our output — the type of graphical model we
are trying to learn — a Bayesian network or a Markov network. The other two axes summarize
the input of the learning procedure.
The ﬁrst of these two characterizes the extent of the
constraints that we are given about
˜
M, and the second characterizes the extent to which the
data in our training set are fully observed. We now discuss each of these in turn. We then
present a taxonomy of the diﬀerent tasks that are deﬁned by these axes, and we review some
of their computational implications.
16.4.1
Model Constraints
The ﬁrst question is the extent to which our input constrains the hypothesis space — the class
hypothesis space
of models that we are allowed to consider as possible outputs of our learning algorithm. There
is an almost unbounded set of options here, since we can place various constraints on the
structure or on the parameters of the model. Some of the key points along the spectrum are:
•
At one extreme, we may be given a graph structure, and we have to learn only (some of) the
parameters; note that we generally do not assume that the given structure is necessarily the
correct one K∗.
•
We may not know the structure, and we have to learn both parameters and structure from
the data.
•
Even worse, we may not even know the complete set of variables over which the distribution
P ∗is deﬁned. In other words, we may only observe some subset of the variables in the
domain and possibly be unaware of others.
The less prior knowledge we are given, the larger the hypothesis space, and the more pos-
sibilities we need to consider when selecting a model. As we discussed in section 16.3.1, the
complexity of the hypothesis space deﬁnes several important trade-oﬀs. The ﬁrst is statistical.
If we restrict the hypothesis space too much, it may be unable to represent P ∗adequately.
Conversely, if we leave it too ﬂexible, our chances increase of ﬁnding a model within the hy-
pothesis space that accidentally has high score but is a poor ﬁt to P ∗. The second trade-oﬀis
computational: in many cases (although not always), the richer the hypothesis space, the more
diﬃcult the search to ﬁnd a high-scoring model.
16.4.2
Data Observability
Along the second input axis, the problem depends on the extent of the observability of our
data observability
training set. Here, there are several options:
•
The data are complete, or fully observed, so that each of our training instances d[m] is a full
complete data
instantiation to all of the variables in X ∗.
•
The data are incomplete, or partially observed, so that, in each training instance, some
incomplete data
variables are not observed.

16.4. Learning Tasks
713
•
The data contain hidden variables whose value is never observed in any training instance.
hidden variable
This option is the only one compatible with the case where the set of variables X ∗is
unknown, but it may also arise if we know of the existence of a hidden variable but never
have the opportunity to observe it directly.
As we move along this axis, more and more aspects of our data are unobserved. When data
are unobserved, we must hypothesize possible values for these variables. The greater the extent
to which data are missing, the less we are able to hypothesize reliably values for the missing
entries.
Dealing with partially observed data is critical in many settings.
First, in many settings,
observing the values of all variables can be diﬃcult or even impossible. For example, in the
case of patient records, we may not perform all tests on all patients, and therefore some of the
variables may be unobserved in some records. Other variables, such as the disease the patient
had, may never be observed with certainty. The ability to deal with partially observed data cases
is also crucial to adapting a Bayesian network using data cases obtained after the network is
operational. In such situations, the training instances are the ones provided to the network as
queries, and as such, are never fully observed (at least when presented as a query).
A particularly diﬃcult case of missing data occurs when we have hidden variables.
Why
should we worry about learning such variables? For the task of knowledge discovery, these
variables may play an important role in the model, and therefore they may be critical for our
understanding of the domain. For example, in medical settings, the genetic susceptibility of a
patient to a particular disease might be an important variable. This might be true even if we
do not know what the genetic cause is, and thus cannot observe it. As another example, the
tendency to be an “impulse shopper” can be an important hidden variable in an application to
supermarket data mining. In these cases, our domain expert can ﬁnd it convenient to specify a
model that contains these variables, even if we never expect to observe their values directly.
In other cases, we might care about the hidden variable even when it has no predeﬁned
semantic meaning. Consider, for example, a naive Bayes model, such as the one shown in
ﬁgure 3.2, but where we assume that the Xi’s are observed but the class variable C is hidden.
In this model, we have a mixture distribution: Each value of the hidden variable represents a
mixture
distribution
separate distribution over the Xi’s, where each such mixture component distribution is “simple”
— all of the Xi’s are independent in each of the mixture components. Thus, the population is
composed of some number of separate subpopulations, each of which is generated by a distinct
distribution. If we could learn this model, we could recover the distinct subpopulations, that is,
ﬁgure out what types of individuals we have in our population. This type of analysis is very
useful from the perspective of knowledge discovery.
Finally, we note that the inclusion of a hidden variable in the network can greatly

simplify the structure, reducing the complexity of the network that needs to be learned.
Even a sparse model over some set of variables can induce a large number of dependencies
over a subset of its variables; for example, returning to the earlier naive Bayes example, if the
class variable C is hidden and therefore is not included in the model, the distribution over
the variables X1, . . . , Xn has no independencies and requires a fully connected graph to be
represented correctly. Figure 16.1 shows another example. (This ﬁgure illustrates another visual
convention that will accompany us throughout this part of the book: Variables whose values
are always hidden are shown as white ovals.) Thus, in many cases, ignoring the hidden variable

714
Chapter 16. Learning Graphical Models: Overview
H
17 parameters
X1
X3
X2
Y1
Y3
Y2
X1
X3
X2
Y1
Y3
Y2
59 parameters
Figure 16.1
The eﬀect of ignoring hidden variables.
The model on the right is an I-map for the
distribution represented by the model on the left, where the hidden variable is marginalized out. The
counts indicate the number of independent parameters, under the assumption that the variables are
binary-valued. The variable H is hidden and hence is shown as a white oval.
leads to a signiﬁcant increase in the complexity of the “true” model (the one that best ﬁts P ∗),
making it harder to estimate robustly. Conversely, learning a model that usefully incorporates
a hidden variable is far from trivial. Thus, the decision of whether to incorporate a hidden
variable is far from trivial, and it requires a careful evaluation of the trade-oﬀs.
16.4.3
Taxonomy of Learning Tasks
Based on these three axes, we can provide a taxonomy of diﬀerent learning tasks and discuss
some of the computational issues they raise.
The problem of parameter estimation for a known structure is one of numerical optimization.
Although straightforward in principle, this task is an important one, both because numbers are
diﬃcult to elicit from people and because parameter estimation forms the basis for the more
advanced learning scenarios.
In the case of Bayesian networks, when the data are complete, the parameter estimation
problem is generally easily solved, and it often even admits a closed-form solution. Unfortunately,
this very convenient property does not hold for Markov networks. Here, the global partition
function induces entanglement of the parameters, so that the dependence of the distribution
on any single parameter is not straightforward. Nevertheless, for the case of a ﬁxed structure
and complete data, the optimization problem is convex and can be solved optimally using
simple iterated numerical optimization algorithms. Unfortunately, each step of the optimization
algorithm requires inference over the network, which can be expensive for large models.
When the structure is not given, the learning task now incorporates an additional level of
complexity: the fact that our hypothesis space now contains an enormous (generally superexpo-
nentially large) set of possible structures. In most cases, as we will see, the problem of structure
selection is also formulated as an optimization problem, where diﬀerent network structures are
given a score, and we aim to ﬁnd the network whose score is highest. In the case of Bayesian
networks, the same property that allowed a closed-form solution for the parameters also allows

16.5. Relevant Literature
715
the score for a candidate network to be computed in closed form. In the case of Markov network,
most natural scores for a network structure cannot be computed in closed form because of the
partition function. However, we can deﬁne a convex optimization problem that jointly searches
over parameter and structure, allowing for a single global optimum.
The problem of dealing with incomplete data is much more signiﬁcant. Here, the multiple
hypotheses regarding the values of the unobserved variables give rise to a combinatorial range
of diﬀerent alternative models, and induce a nonconvex, multimodal optimization problem even
in parameter space.
The known algorithms generally work by iteratively using the current
parameters to ﬁll in values for the missing data, and then using the completion to reestimate the
model parameters. This process requires multiple calls to inference as a subroutine, making this
process expensive for large networks. The case where the structure is not known is even harder,
since we need to combine a discrete search over network structure with nonconvex optimization
over parameter space.
16.5
Relevant Literature
Most of the topics reviewed here are discussed in greater technical depth in subsequent chapters,
and so we defer the bibliographic references to the appropriate places. Hastie, Tibshirani, and
Friedman (2001) and Bishop (2006) provide an excellent overview of basic concepts in machine
learning, many of which are relevant to the discussion in this book.


17
Parameter Estimation
In this chapter, we discuss the problem of estimating parameters for a Bayesian network. We
assume that the network structure is ﬁxed and that our data set D consists of fully observed
instances of the network variables: D = {ξ[1], . . . , ξ[M]}. This problem arises fairly often in
practice, since numerical parameters are harder to elicit from human experts than structure
is. It also plays a key role as a building block for both structure learning and learning from
incomplete data. As we will see, despite the apparent simplicity of our task deﬁnition, there is
surprisingly much to say about it.
As we will see, there are two main approaches to dealing with the parameter-estimation task:
one based on maximum likelihood estimation, and the other using Bayesian approaches. For
each of these approaches, we ﬁrst discuss the general principles, demonstrating their application
in the simplest context: a Bayesian network with a single random variable.
We then show
how the structure of the distribution allows the techniques developed in this very simple case
to generalize to arbitrary network structures.
Finally, we show how to deal with parameter
estimation in the context of structured CPDs.
17.1
Maximum Likelihood Estimation
In this section, we describe the basic principles behind maximum likelihood estimation.
17.1.1
The Thumbtack Example
We start with what may be considered the simplest learning problem: parameter learning for a
single variable. This is a classical Statistics 101 problem that illustrates some of the issues that
we will encounter in more complex learning problems. Surprisingly, this simple problem already
contains some interesting issues that we need to tackle.
Imagine that we have a thumbtack, and we conduct an experiment whereby we ﬂip the
thumbtack in the air. It comes to land as either heads or tails, as in ﬁgure 17.1. We toss the
thumbtack several times, obtaining a data set consisting of heads or tails outcomes. Based on
this data set, we want to estimate the probability with which the next ﬂip will land heads or
tails. In this description, we already made the implicit assumption that the thumbtack tosses are
controlled by an (unknown) parameter θ, which describes the frequency of heads in thumbtack
tosses. In addition, we also assume that the data instances are independent and identically
distributed (IID).

718
Chapter 17. Parameter Estimation
heads
tails
Figure 17.1
A simple thumbtack tossing experiment
0
0.2
0.4
0.6
0.8
1
L(q:   )
Figure 17.2
The likelihood function for the sequence of tosses H, T, T, H, H
Assume that we toss the thumbtack 100 times, of which 35 come up heads. What is our
estimate for θ?
Our intuition suggests that the best estimate is 0.35.
Had θ been 0.1, for
example, our chances of seeing 35/100 heads would have been much lower.
In fact, we
examined a similar situation in our discussion of sampling methods in section 12.1, where we
used samples from a distribution to estimate the probability of a query. As we discussed, the
central limit theorem shows that, as the number of coin tosses grows, it is increasingly unlikely
to sample a sequence of IID thumbtack ﬂips where the fraction of tosses that come out heads is
very far from θ. Thus, for suﬃciently large M, the fraction of heads among the tosses is a good
estimate with high probability.
To formalize this intuition, assume that we have a set of thumbtack tosses x[1], . . . , x[M]
that are IID, that is, each is sampled independently from the same distribution in which X[m]
is equal to H (heads) or T (tails) with probability θ and 1 −θ, respectively. Our task is to
ﬁnd a good value for the parameter θ. As in many formulations of learning tasks, we deﬁne a
hypothesis space Θ — a set of possibilities that we are considering — and an objective function
hypothesis space
objective
function
that tells us how good diﬀerent hypotheses in the space are relative to our data set D. In this
case, our hypothesis space Θ is the set of all parameters θ ∈[0, 1].
How do we score diﬀerent possible parameters θ? As we discussed in section 16.3.1, one
way of evaluating θ is by how well it predicts the data. In other words, if the data are likely
given the parameter, the parameter is a good predictor. For example, suppose we observe the
sequence of outcomes H, T, T, H, H. If we know θ, we could assign a probability to observing
this particular sequence. The probability of the ﬁrst toss is P(X[1] = H) = θ. The probability
of the second toss is P(X[2] = T | X[1] = H), but our assumption that the coin tosses are
independent allows us to conclude that this probability is simply P(X[2] = T) = 1 −θ. This

17.1. Maximum Likelihood Estimation
719
is also the probability of the third outcome, and so on. Thus, the probability of the sequence is
P(⟨H, T, T, H, H⟩: θ) = θ(1 −θ)(1 −θ)θθ = θ3(1 −θ)2.
As expected, this probability depends on the particular value θ. As we consider diﬀerent values
of θ, we get diﬀerent probabilities for the sequence. Thus, we can examine how the probability
of the data changes as a function of θ. We thus deﬁne the likelihood function to be
likelihood
function
L(θ : ⟨H, T, T, H, H⟩) = P(⟨H, T, T, H, H⟩: θ) = θ3(1 −θ)2.
Figure 17.2 plots the likelihood function in our example.
Clearly, parameter values with higher likelihood are more likely to generate the observed
sequences. Thus, we can use the likelihood function as our measure of quality for diﬀerent
parameter values and select the parameter value that maximizes the likelihood; this value is
called the maximum likelihood estimator (MLE). By viewing ﬁgure 17.2 we see that ˆθ = 0.6 = 3/5
maximum
likelihood
estimator
maximizes the likelihood for the sequence H, T, T, H, H.
Can we ﬁnd the MLE for the general case? Assume that our data set D of observations
contains M[1] heads and M[0] tails. We want to ﬁnd the value ˆθ that maximizes the likelihood
of θ relative to D. The likelihood function in this case is:
L(θ : D) = θM[1](1 −θ)M[0].
It turns out that it is easier to maximize the logarithm of the likelihood function. In our case,
the log-likelihood function is:
log-likelihood
ℓ(θ : D) = M[1] log θ + M[0] log(1 −θ).
Note that the log-likelihood is monotonically related to the likelihood. Therefore, maximizing
the one is equivalent to maximizing the other. However, the log-likelihood is more convenient
to work with, since products are converted to summations.
Diﬀerentiating the log-likelihood, setting the derivative to 0, and solving for θ, we get that the
maximum likelihood parameter, which we denote ˆθ, is
ˆθ =
M[1]
M[1] + M[0],
(17.1)
as expected (see exercise 17.1).
As we will see, the maximum likelihood approach has many advantages.
However, the
approach also has some limitations. For example, if we get 3 heads out of 10 tosses, the MLE
estimate is 0.3. We get the same estimate if we get 300 heads out of 1,000 tosses. Clearly, the
two experiments are not equivalent. Our intuition is that, in the second experiment, we should
be more conﬁdent of our estimate. Indeed, statistical estimation theory deals with conﬁdence
conﬁdence
interval
intervals.
These are common in news reports, for example, when describing the results of
election polls, where we often hear that “61 ± 2 percent” plan to vote for a certain candidate.
The 2 percent is a conﬁdence interval — the poll is designed to select enough people so that
the MLE estimate will be within 0.02 of the true parameter, with high probability.

720
Chapter 17. Parameter Estimation
17.1.2
The Maximum Likelihood Principle
We now generalize the discussion of maximum likelihood estimation to a broader range of
learning problems. We then consider how to apply it to the task of learning the parameters of
a Bayesian network.
We start by describing the setting of the learning problem. Assume that we observe several
IID samples of a set of random variables X from an unknown distribution P ∗(X). We assume
we know in advance the sample space we are dealing with (that is, which random variables, and
what values they can take). However, we do not make any additional assumptions about P ∗.
We denote the training set of samples as D and assume that it consists of M instances of X:
training set
ξ[1], . . . ξ[M].
Next, we need to consider what exactly we want to learn. We assume that we are given a
parametric model for which we wish to estimate parameters. Formally, a parametric model (also
parametric model
known as a parametric family; see section 8.2) is deﬁned by a function P(ξ : θ), speciﬁed in
terms of a set of parameters. Given a particular set of parameter values θ and an instance ξ of
parameters
X, the model assigns a probability (or density) to ξ. Of course, we require that for each choice
of parameters θ, P(ξ : θ) is a legal distribution; that is, it is nonnegative and
X
ξ
P(ξ : θ) = 1.
In general, for each model, not all parameter values are legal. Thus, we need to deﬁne the
parameter space Θ, which is the set of allowable parameters.
parameter space
To get some intuition, we consider concrete examples. The model we examined in section 17.1.1
has parameter space Θthumbtack = [0, 1] and is deﬁned as
Pthumbtack(x : θ) =
 θ
if x = H
1 −θ
if x = T.
There are many additional examples.
Example 17.1
Suppose that X is a multinomial variable that can take values x1, . . . , xK. The simplest represen-
multinomial
tation of a multinomial distribution is as a vector θ ∈IRK, such that
Pmultinomial(x : θ) = θk if x = xk.
The parameter space of this model is
Θmultinomial =
(
θ ∈[0, 1]K :
X
i
θi = 1
)
.
Example 17.2
Suppose that X is a continuous variable that can take values in the real line. A Gaussian model
Gaussian
for X is
PGaussian(x : µ, σ) =
1
√
2πσ e−(x−µ)2
2σ2 ,
where θ = ⟨µ, σ⟩. The parameter space for this model is ΘGaussian = IR × IR+. That is, we allow
any real value of µ and any positive real value for σ.

17.1. Maximum Likelihood Estimation
721
The next step in maximum likelihood estimation is deﬁning the likelihood function. As we
likelihood
function
saw in our example, the likelihood function for a given choice of parameters θ is the probability
(or density) the model assigns the training data:
L(θ : D) =
Y
m
P(ξ[m] : θ).
In the thumbtack example, we saw that we can write the likelihood function using simpler
terms. That is, using the counts M[1] and M[0], we managed to have a compact description of
the likelihood. More precisely, once we knew the values of M[1] and M[0], we did not need to
consider other aspects of training data (for example, the order of tosses). These are the suﬃcient
statistics for the thumbtack learning problem. In a more general setting, a suﬃcient statistic is a
function of the data that summarizes the relevant information for computing the likelihood.
Deﬁnition 17.1
A function τ(ξ) from instances of X to IRℓ(for some ℓ) is a suﬃcient statistic if, for any two data
suﬃcient
statistics
sets D and D′ and any θ ∈Θ, we have that
X
ξ[m]∈D
τ(ξ[m]) =
X
ξ′[m]∈D′
τ(ξ′[m])
=⇒
L(θ : D) = L(θ : D′).
We often refer to the tuple P
ξ[m]∈D τ(ξ[m]) as the suﬃcient statistics of the data set D.
Example 17.3
Let us reconsider the multinomial model of example 17.1. It is easy to see that a suﬃcient statistic
for the data set is the tuple of counts ⟨M[1], . . . , M[K]⟩, such that M[k] is number of times the
value xk appears in the training data. To obtain these counts by summing instance-level statistics,
we deﬁne τ(x) to be a tuple of dimension K, such that τ(x) has a 0 in every position, except at
the position k for which x = xk, where its value is 1:
τ(xk) = (
k−1
z }| {
0, . . . , 0, 1,
n−k
z }| {
0, . . . , 0).
Given the vector of counts, we can write the likelihood function as
L(θ : D) =
Y
k
θM[k]
k
.
Example 17.4
Let us reconsider the Gaussian model of example 17.2. In this case, it is less obvious how to construct
suﬃcient statistics. However, if we expand the term (x −µ)2 in the exponent, we can rewrite the
model as
PGaussian(x : µ, σ) = e−x2
1
2σ2 +x µ
σ2 −µ2
2σ2 −1
2 log(2π)−log(σ).
We then see that the function
sGaussian(x) = ⟨1, x, x2⟩
is a suﬃcient statistic for this model. Note that the ﬁrst element in the suﬃcient statistics tuple is
“1,” which does not depend on the value of the data item; it serves, as in the multinomial case, to
count the number of data items.

722
Chapter 17. Parameter Estimation
We venture several comments about the likelihood function. First, we stress that the likelihood
function measures the eﬀect of the choice of parameters on the training data. Thus, for example,
if we have two sets of parameters θ and θ′, so that L(θ : D) = L(θ′ : D), then we cannot,
given only the data, distinguish between the two choices of parameters. Moreover, if L(θ :
D) = L(θ′ : D) for all possible choices of D, then the two parameters are indistinguishable for
any outcome. In such a situation, we can say in advance (that is, before seeing the data) that
some distinctions cannot be resolved based on the data alone.
Second, since we are maximizing the likelihood function, we usually want it to be continuous
(and preferably smooth) function of θ.
To ensure these properties, most of the theory of
statistical estimation requires that P(ξ : θ) is a continuous and diﬀerentiable function of θ, and
moreover that Θ is a continuous set of points (which is often assumed to be convex).
Once we have deﬁned the likelihood function, we can use maximum likelihood estimation to
maximum
likelihood
estimation
choose the parameter values. Formally, we state this principle as follows.
Maximum Likelihood Estimation: Given a data set D, choose parameters ˆθ that satisfy
L(ˆθ : D) = max
θ∈Θ L(θ : D).
Example 17.5
Consider estimating the parameters of the multinomial distribution of example 17.3. As one might
guess, the maximum likelihood is attained when
ˆθk = M[k]
M
(see exercise 17.2). That is, the probability of each value of X corresponds to its frequency in the
training data.
Example 17.6
Consider estimating the parameters of a Gaussian distribution of example 17.4. It turns out that
the maximum is attained when µ and σ correspond to the empirical mean and variance of the
empirical mean,
variance
training data:
ˆµ
=
1
M
X
m
x[m]
ˆσ
=
s
1
M
X
m
(x[m] −ˆµ)2
(see exercise 17.3).
17.2
MLE for Bayesian Networks
We now move to the more general problem of estimating parameters for a Bayesian network. It
turns out that the structure of the Bayesian network allows us to reduce the parameter estimation
problem to a set of unrelated problems, each of which can be addressed using the techniques
of the previous section. We begin by considering a simple example to clarify our intuition, and
then generalize to more complicated networks.

17.2. MLE for Bayesian Networks
723
17.2.1
A Simple Example
The simplest example of a nontrivial network structure is a network consisting of two binary
variables, say X and Y , with an arc X →Y . (A network without such an arc trivially reduces
to the cases we already discussed.)
As for a single parameter, our goal in maximum likelihood estimation is to maximize the
likelihood (or log-likelihood) function. In this case, our network is parameterized by a parameter
vector θ, which deﬁnes the set of parameters for all the CPDs in the network. In this example,
our parameterization would consist of the following parameters: θx1, and θx0 specify the
probability of the two values of X; θy1|x1, and θy0|x1 specify the probability of Y given that
X = x1; and θy1|x0, and θy0|x0 describe the probability of Y given that X = x0. For brevity,
we also use the shorthand θY |x0 to refer to the set {θy1|x0, θy0|x0}, and θY |X to refer to
θY |x1 ∪θY |x0.
In this example, each training instance is a tuple ⟨x[m], y[m]⟩that describes a particular
assignment to X and Y . Our likelihood function is:
L(θ : D) =
M
Y
m=1
P(x[m], y[m] : θ).
Our network model speciﬁes that P(X, Y : θ) has a product form. Thus, we can write
L(θ : D) =
Y
m
P(x[m] : θ)P(y[m] | x[m] : θ).
Exchanging the order of multiplication, we can equivalently write this term as
L(θ : D) =
 Y
m
P(x[m] : θ)
!  Y
m
P(y[m] | x[m] : θ)
!
.
That is, the likelihood decomposes into two separate terms, one for each variable. Moreover,
each of these terms is a local likelihood function that measures how well the variable is predicted
given its parents.
Now consider the two individual terms. Clearly, each one depends only on the parameters
for that variable’s CPD. Thus, the ﬁrst is Q
m P(x[m] : θX).
This term is identical to the
multinomial likelihood function we discussed earlier. The second term is more interesting, since
we can decompose it even further:
Y
m
P(y[m] | x[m] : θY |X)
=
Y
m:x[m]=x0
P(y[m] | x[m] : θY |X) ·
Y
m:x[m]=x1
P(y[m] | x[m] : θY |X)
=
Y
m:x[m]=x0
P(y[m] | x[m] : θY |x0) ·
Y
m:x[m]=x1
P(y[m] | x[m] : θY |x1).
Thus, in this example, the likelihood function decomposes into a product of terms, one for each
group of parameters in θ. This property is called the decomposability of the likelihood function.
likelihood
decomposability

724
Chapter 17. Parameter Estimation
We can do one more simpliﬁcation by using the notion of suﬃcient statistics. Let us consider
one term in this expression:
Y
m:x[m]=x0
P(y[m] | x[m] : θY |x0).
(17.2)
Each of the individual terms P(y[m] | x[m] : θY |x0) can take one of two values, depending on
the value of y[m]. If y[m] = y1, it is equal to θy1|x0. If y[m] = y0, it is equal to θy0|x0. How
many cases of each type do we get? First, we restrict attention only to those data cases where
x[m] = x0. These, in turn, partition into the two categories. Thus, we get θy1|x0 in those data
cases where x[m] = x0 and y[m] = y1; we use M[x0, y1] to denote their number. We get
θy0|x0 in those data cases where x[m] = x0 and y[m] = x0, and use M[x0, y0] to denote their
number. Thus, the term in equation (17.2) is equal to:
Y
m:x[m]=x0
P(y[m] | x[m] : θY |x0)
=
θM[x0,y1]
y1|x0
· θM[x0,y0]
y0|x0
.
Based on our discussion of the multinomial likelihood in example 17.5, we know that we
maximize θY |x0 by setting:
θy1|x0 =
M[x0, y1]
M[x0, y1] + M[x0, y0] = M[x0, y1]
M[x0]
,
and similarly for θy0|x0. Thus, we can ﬁnd the maximum likelihood parameters in this CPD by
simply counting how many times each of the possible assignments of X and Y appears in the
training data. It turns out that these counts of the various assignments for some set of variables
are useful in general. We therefore deﬁne:
Deﬁnition 17.2
Let Z be some set of random variables, and z be some instantiation to these random variables. Let
D be a data set. We deﬁne M[z] to be the number of entries in D that have Z[m] = z
M[z] =
X
m
11{Z[m] = z}.
(17.3)
17.2.2
Global Likelihood Decomposition
As we can expect, the arguments we used for deriving the MLE of θY |x0 apply for the parameters
of other CPDs in that example and indeed for other networks as well. We now develop, in several
steps, the formal machinery for proving such properties in Bayesian networks.
We start by examining the likelihood function of a Bayesian network. Suppose we want to
learn the parameters for a Bayesian network with structure G and parameters θ. This means
that we agree in advance on the type of CPDs we want to learn (say table-CPDs, or noisy-ors).
As we discussed, we are also given a data set D consisting of samples ξ[1], . . . , ξ[M]. Writing

17.2. MLE for Bayesian Networks
725
the likelihood, and repeating the steps we performed in our example, we get
L(θ : D)
=
Y
m
PG(ξ[m] : θ)
=
Y
m
Y
i
P(xi[m] | paXi[m] : θ)
=
Y
i
"Y
m
P(xi[m] | paXi[m] : θ)
#
.
Note that each of the terms in the square brackets refers to the conditional likelihood of a
conditional
likelihood
particular variable given its parents in the network. We use θXi|PaXi to denote the subset of
parameters that determines P(Xi | PaXi) in our model. Then, we can write
L(θ : D) =
Y
i
Li(θXi|PaXi : D),
where the local likelihood function for Xi is:
local likelihood
Li(θXi|PaXi : D) =
Y
m
P(xi[m] | paXi[m] : θXi|PaXi).
This form is particularly useful when the parameter sets θXi|PaXi are disjoint. That is, each CPD
is parameterized by a separate set of parameters that do not overlap. This assumption is quite
natural in all our examples so far. (Although, as we will see in section 17.5, parameter sharing
can be handy in many domains.) This analysis shows that the likelihood decomposes as a

product of independent terms, one for each CPD in the network. This important property
is called the global decomposition of the likelihood function.
global
decomposability
We can now immediately derive the following result:
Proposition 17.1
Let D be a complete data set for X1, . . . , Xn, let G be a network structure over these variables, and
suppose that the parameters θXi|PaXi are disjoint from θXj|PaXj for all j ̸= i. Let ˆθXi|PaXi be
the parameters that maximize Li(θXi|PaXi : D). Then, ˆθ = ⟨ˆθX1|Pa1, . . . , ˆθXn|Pan⟩maximizes
L(θ : D).
In other words, we can maximize each local likelihood function independently of rest of

the network, and then combine the solutions to get an MLE solution. This decomposition
of the global problem to independent subproblems allows us to devise eﬃcient solutions to
the MLE problem. Moreover, this decomposition is an immediate consequence of the network
structure and does not depend on any particular choice of parameterization for the CPDs.
17.2.3
Table-CPDs
Based on the preceding discussion, we know that the likelihood of a Bayesian network decom-
poses into local terms that depend on the parameterization of CPDs. The choice of parameters
determines how we can maximize each of the local likelihood functions. We now consider what
is perhaps the simplest parameterization of the CPD: a table-CPD.
table-CPD

726
Chapter 17. Parameter Estimation
Suppose we have a variable X with parents U. If we represent that CPD P(X | U) as a table,
then we will have a parameter θx|u for each combination of x ∈Val(X) and u ∈Val(U). In
this case, we can rewrite the local likelihood function as follows:
LX(θX|U : D)
=
Y
m
θx[m]|u[m]
=
Y
u∈Val(U)


Y
x∈Val(X)
θM[u,x]
x|u

,
(17.4)
where M[u, x] is the number of times ξ[m] = x and u[m] = u in D. That is, we grouped
together all the occurrences of θx|u in the product over all instances. This provides a further
local decomposition of the likelihood function.
local
decomposability
We need to maximize this term under the constraints that, for each choice of value for the
parents U, the conditional probability is legal, that is:
X
θx|u = 1
for all u.
These constraints imply that the choice of value for θx|u can impact the choice of value for
θx′|u. However, the choice of parameters given diﬀerent values u of U are independent of
each other. Thus, we can maximize each of the terms in square brackets in equation (17.4)
independently.
We can thus further decompose the local likelihood function for a tabular CPD into a product
of simple likelihood functions. Each of these likelihood functions is a multinomial likelihood, of
the type that we examined in example 17.3. The counts in the data for the diﬀerent outcomes
x are simply {M[u, x] : x ∈Val(X)}. We can then immediately use the maximum likelihood
estimation for multinomial likelihood of example 17.5 and see that the MLE parameters are
ˆθx|u = M[u, x]
M[u] ,
(17.5)
where we use the fact that M[u] = P
x M[u, x].
This simple formula reveals a key challenge when estimating parameters for a Bayesian net-
works. Note that the number of data points used to estimate the parameter ˆθx|u is M[u]. Data
points that do not agree with the parent assignment u play no role in this computation. As the
number of parents U grows, the number of diﬀerent parent assignments grows exponentially.
Therefore, the number of data instances that we expect to have for a single parent assignment
shrinks exponentially. This phenomenon is called data fragmentation, since the data set is par-
data
fragmentation
titioned into a large number of small subsets. Intuitively, when we have a very small number
of data instances from which we estimate a parameter, the estimates we get can be very noisy
(this intuition is formalized in section 17.6), leading to overﬁtting. We are also more likely to
overﬁtting
get a large number of zeros in the distribution, which can lead to very poor performance. Our

inability to estimate parameters reliably as the dimensionality of the parent set grows is
one of the key limiting factors in learning Bayesian networks from data. This problem is
even more severe when the variables can take on a large number of values, for example, in text
applications.

17.2. MLE for Bayesian Networks
727
Box 17.A — Concept: Naive Bayes Classiﬁer. One of the basic tasks of learning is classiﬁcation.
classiﬁcation
In this task, our goal is build a classiﬁer — a procedure that assigns instances into two or more
categories, for example, deciding whether an email message is junk mail that should be discarded or
a relevant message that should be presented to the user. In the usual setting, we are given a training
example of instances from each category, where instances are represented by various features. In
our email classiﬁcation example, a message might be analyzed by multiple features: its length, the
type of attachments it contains, the domain of the sender, whether that sender appears in the user’s
address book, whether a particular word appears in the subject, and so on.
One general approach to this problem, which is referred to as Bayesian classiﬁer, is to learn a
Bayesian
classiﬁer
probability distribution of the features of instances of each class. In the language of probabilistic
models, we use the random variables X to represent the instance, and the random variable C to
represent the category of the instance. The distribution P(X | C) is the probability of a particular
combination of features given the category. Using Bayes rule, we have that
P(C | X) ∝P(C)P(X | C).
Thus, if we have a good model of how instances of each category behave (that is, of P(X | C)), we
can combine it with our prior estimate for the frequency of each category (that is, P(C)) to estimate
the posterior probability of each of the categories (that is, P(C | X)). We can then decide either
to predict the most likely category or to perform a more complex decision based on the strength
of likelihood of each option. For example, to reduce the number of erroneously removed messages,
a junk-mail ﬁlter might remove email messages only when the probability that it is junk mail is
higher than a strict threshold.
This Bayesian classiﬁcation approach is quite intuitive. Loosely speaking, it states that to classify
objects successfully, we need to recognize the characteristics of objects of each category. Then, we can
classify a new object by considering whether it matches the characteristic of each of the classes. More
formally, we use the language of probability to describe each category, assigning higher probability
to objects that are typical for the category and low probability to ones that are not.
The main hurdle in constructing a Bayesian classiﬁer is the question of representation of the
multivariate distribution p(X | C). The naive Bayes classiﬁer is one where we use the simplest
naive Bayes
representation we can think of. That is, we assume that each feature Xi is independent of all the
other features given the class variable C. That is,
P(X | C) =
Y
i
P(Xi | C).
Learning the distribution P(C)P(X | C) is thus reduced to learning the parameters in the
naive Bayes structure, with the category variable C rendering all other features as conditionally
independent of each other.
As can be expected, learning this classiﬁer is a straightforward application of the parameter
estimation that we consider in this chapter. Moreover, classifying new examples requires simple
computation, evaluating P(c) Q
i P(xi | c) for each category c.
Although this simple classiﬁer is often dismissed as naive, in practice it is often surprisingly
eﬀective. From a training perspective, this classiﬁer is quite robust, since in most applications,
even with relatively few training examples, we can learn the parameters of conditional distribution

728
Chapter 17. Parameter Estimation
P(Xi | C). However, one might argue that robust learning does not compensate for oversimpliﬁed
independence assumption. Indeed, the strong independence assumption usually results in poor
representation of the distribution of instances. However, errors in estimating the probability

of an instance do not necessarily lead to classiﬁcation errors.
For classiﬁcation, we
are interested in the relative size of the conditional distribution of the instances given
diﬀerent categories. The ranking of diﬀerent labels may not be that sensitive to errors in
estimating the actual probability of the instance. Empirically, one often ﬁnds that the naive
Bayes classiﬁer correctly classiﬁes an example to the right category, yet its posterior probability is
very skewed and quite far from the correct distribution.
In practice, the naive Bayes classiﬁer is often a good baseline classiﬁer to try before considering
more complex solutions. It is easy to implement, it is robust, and it can handle diﬀerent choices of
descriptions of instances (for example, box 17.E).
17.2.4
Gaussian Bayesian Networks ⋆
Our discussion until now has focused on learning discrete-state Bayesian networks with multi-
nomial parameters. However, the concepts we have developed in this section carry through
to a wide variety of other types of Bayesian networks. In particular, the global decomposition
properties we proved for a Bayesian network apply, without any change, to any other type of
CPD. That is, if the data are complete, the learning problem reduces to a set of local learning
problems, one for each variable. The main diﬀerence is in applying the maximum likelihood
estimation process to a CPD of a diﬀerent type: how we deﬁne the suﬃcient statistics, and
how we compute the maximum likelihood estimate from them. In this section, we demonstrate
how MLE principles can be applied in the setting of linear Gaussian Bayesian networks. In
section 17.2.5 we provide a general procedure for CPDs in the exponential family.
Consider a variable X with parents U = {U1, . . . , Uk} with a linear Gaussian CPD:
P(X | u) = N
 β0 + β1u1 + . . . , βkuk; σ2
.
Our task is to learn the parameters θX|U = ⟨β0, . . . , βk, σ⟩.
To ﬁnd the MLE values of
these parameters, we need to diﬀerentiate the likelihood and solve the equations that deﬁne a
stationary point. As usual, it will be easier to work with the log-likelihood function. Using the
deﬁnition of the Gaussian distribution, we have that
ℓX(θX|U : D) = log LX(θX|U : D)
=
X
m

−1
2 log(2πσ2) −1
2
1
σ2 (β0 + β1u1[m] + . . . + βkuk[m] −x[m])2

.
We start by considering the gradient of the log-likelihood with respect to β0:
∂
∂β0
ℓX(θX|U : D) =
X
m
−1
σ2 (β0 + β1u1[m] + . . . + βkuk[m] −x[m])
=
−1
σ2
 
Mβ0 + β1
X
m
u1[m] + . . . + βk
X
m
uk[m] −
X
m
x[m]
!
.

17.2. MLE for Bayesian Networks
729
Equating this gradient to 0, and multiplying both sides with σ2
M , we get the equation
1
M
X
m
x[m] = β0 + β1
1
M
X
m
u1[m] + . . . + βk
1
M
X
m
uk[m].
Each of the terms is the average value of one of the variables in the data. We use the notation
IED[X] = 1
M
X
m
x[m]
to denote this expectation. Using this notation, we see that we get the following equation:
IED[X] = β0 + β1IED[U1] + . . . + βkIED[Uk].
(17.6)
Recall that theorem 7.3 speciﬁes the mean of a linear Gaussian variable X in terms of the means
of its parents U1, . . . , Uk, using an expression that has precisely this form. Thus, equation (17.6)
tells us that the MLE parameters should be such that the mean of X in the data is consistent
with the predicted mean of X according to the parameters.
Next, consider the gradient with respect to one of the parameters βi. Using similar arithmetic
manipulations, we see that the equation 0 =
∂
∂βi ℓX(θX|U : D) can be formulated as:
IED[X · Ui] = β0IED[Ui] + β1IED[U1 · Ui] + . . . + βkIED[Uk · Ui].
(17.7)
At this stage, we have k + 1 linear equations with k + 1 unknowns, and we can use standard
linear algebra techniques for solving for the value of β0, β1, . . . , βk. We can get additional
intuition, however, by doing additional manipulation of equation (17.7). Recall that the covariance
CCov[X; Y ] = IE[X · Y ] −IE[X] · IE[Y ]. Thus, if we subtract IED[X] · IED[Ui] from the left-hand
side of equation (17.7), we would get the empirical covariance of X and Ui. Using equation (17.6),
we have that this term can also be written as:
IED[X] · IED[Ui] = β0IED[Ui] + β1IED[U1] · IED[Ui] + . . . + βkIED[Uk] · IED[Ui].
Subtracting this equation from equation (17.7), we get:
IED[X · Ui] −IED[X] · IED[Ui]
=
β1 (IED[U1 · Ui] −IED[U1] · IED[Ui]) + . . . +
βk (IED[Uk · Ui] −IED[Uk] · IED[Ui]) .
Using CCovD[X; Ui] to denote the observed covariance of X and Ui in the data, we get:
CCovD[X; Ui] = β1CCovD[U1; Ui] + . . . + βkCCovD[Uk; Ui].
In other words, the observed covariance of X with Ui should be the one predicted by theorem 7.3
given the parameters and the observed covariances between the parents of X.
Finally, we need to ﬁnd the value of the σ2 parameter. Taking the derivative of the likelihood
and equating to 0, we get an equation that, after suitable reformulation, can be written as
σ2 = CCovD[X; X] −
X
i
X
j
βiβjCCovD[Ui; Uj]
(17.8)

730
Chapter 17. Parameter Estimation
(see exercise 17.4). Again, we see that the MLE estimate has to match the constraints implied by
theorem 7.3.
The global picture that emerges is as follows. To estimate P(X | U), we estimate the means
of X and U and covariance matrix of {X} ∪U from the data. The vector of means and
covariance matrix deﬁnes a joint Gaussian distribution over {X} ∪U.
(In fact, this is the
MLE estimate of the joint Gaussian; see exercise 17.5.) We then solve for the (unique) linear
Gaussian that matches the joint Gaussian with these parameters. For this purpose, we can use
the formulas provided by theorem 7.4. While these equations seem somewhat complex, they are
merely describing the solution to a system of linear equations.
This discussion also identiﬁes the suﬃcient statistics we need to collect to estimate linear
Gaussians.
These are the univariate terms of the form P
m x[m] and P
m ui[m], and the
interaction terms of the form P
m x[m] · ui[m] and P
m ui[m] · uj[m]. From these, we can
estimate the mean and covariance matrix of the joint distribution.
Box 17.B — Concept: Nonparametric Models. The discussion in this chapter has focused on
estimating parameters for speciﬁc parametric models of CPDs: multinomials and linear Gaussians.
However, a theory of maximum likelihood and Bayesian estimation exists for a wide variety of
other parametric models. Moreover, in recent years, there has been a growing interest in the use of
nonparametric Bayesian estimation methods, where a (conditional) distribution is not deﬁned to be
nonparametric
Bayesian
estimation
in some particular parametric class with a ﬁxed number of parameters, but rather the complexity of
the representation is allowed to grow as we get more data instances. In the case of discrete variables,
any CPD can be described as a table, albeit perhaps a very large one; thus a nonparametric method
is less essential (although see section 19.5.2.2 for a very useful example of a nonparametric method
in the discrete case). In the case of continuous variables, we do not have a “universal” parametric
distribution. While Gaussians are often the default, many distributions are not well ﬁt by them,
and it is often diﬃcult to determine which parametric family (if any) will be appropriate for a given
variable. In such cases, nonparametric methods oﬀer a useful substitute. In such methods, we use
the data points themselves as the basis for a probability distribution. Many nonparametric methods
have been developed; we describe one simple variant that serves to illustrate this type of approach.
Suppose we want to learn the distribution P(X | U) from data. A reasonable assumption
is that the CPD is smooth. Thus, if we observe x, u in a training sample, it should increase the
probability of seeing similar values of X for similar values of U. More precisely, we increase the
density of p(X = x + ϵ | U = u + δ) for small values of ϵ and δ.
One simple approach that captures this intuition is the use of kernel density estimation (also
kernel density
estimation
known as Parzen windows). The idea is fairly simple: given the data D, we estimate a “local” joint
density ˜pX(X, U) by spreading out density around each example x[m], u[m]. Formally, we write
˜pX(x, u) = 1
M
X
m
K(x, u; x[m], u[m], α),
where K is a kernel density function and α is a parameter (or vector of parameters) controlling
K. A common choice of kernel is a simple round Gaussian distribution with radius α around
x[m], u[m]:
K(x, u; x[m], u[m], α) = N
 x[m]
u[m]

; α2I

,

17.2. MLE for Bayesian Networks
731
where I is the identity matrix and α is the width of the window. Of course, many other choices for
kernel function are possible; in fact, if K deﬁnes a probability measure (nonnegative and integrates
to 1), then ˜pX(x, u) is also a probability measure. Usually we choose kernel functions that are
local, in that they put most of the mass in the vicinity of their argument. For such kernels, the
resulting density ˜pX(x, u) will have high mass in regions where we have seen many data instances
(x[m], u[m]) and low mass in regions where we have seen none.
We can now reformulate this local joint distribution to produce a conditional distribution:
p(x | u) =
P
m K(x, u; x[m], u[m], α)
P
m K(u; u[m], α)
where K(u; u[m], α) is K(x, u; x[m], u[m], α) marginalized over x.
Note that this learning procedure estimates virtually no parameters: the CPD is derived directly
from the training instances. The only free parameter is α, which is the width of the window.
Importantly, this parameter cannot be estimated using maximum likelihood: The α that maximizes
the likelihood of the training set is α = 0, which gives maximum density to the training instances
themselves. This, of course, will simply memorize the training instances without any generalization.
Thus, this parameter is generally selected using cross-validation.
The learned CPD here is essentially the list of training instances, which has both advantages
and disadvantages.
On the positive side, the estimates are very ﬂexible and tailor themselves
to the observations; indeed, as we get more training data, we can produce arbitrarily expressive
representations of our joint density. On the negative side, there is no “compression” of the original
data, which has both computational and statistical ramiﬁcations. Computationally, when there are
many training samples the learned CPDs can become unwieldy. Statistically, this learning procedure
makes no attempt to generalize beyond the data instances that we have seen. In high-dimensional
spaces with limited data, most points in the space will be “far” from data instances, and therefore
the estimated density will tend to be quite poor in most parts of the space. Thus, this approach
is primarily useful in cases where we have a large number of training instances relative to the
dimension of the space.
Finally, while these approaches help us avoid parametric assumptions on the learning side,
we are left with the question of how to avoid them on the inference side.
As we saw, most
inference procedures are geared to working with parametric representations, mostly Gaussians.
Thus, when performing inference with nonparametric CPDs, we must generally either use parametric
approximations, or resort to sampling.
17.2.5
Maximum Likelihood Estimation as M-Projection ⋆
The MLE principle is a general one, in that it gives a recipe how to construct estimators for
diﬀerent statistical models (for example, multinomials and Gaussians). As we have seen, for
simple examples the resulting estimators are quite intuitive. However, the same principle can
be applied in a much broader range of parametric models. Indeed, as we now show, we have
already discussed the framework that forms the basis for this generalization.
In section 8.5, we deﬁned the notion of projection: ﬁnding the distribution, within a speciﬁed
class, that is closest to a given target distribution. Parameter estimation is similar in the sense

732
Chapter 17. Parameter Estimation
that we select a distribution from a given class — all of those that can be described by the model
— that is “closest” to our data. Indeed, we can show that maximum likelihood estimation aims
to ﬁnd the distribution that is “closest” to the empirical distribution ˆPD (see equation (16.4)).
We start by rewriting the likelihood function in terms of the empirical distribution.
Proposition 17.2
Let D be a data set, then
log L(θ : D) = M · IE ˆ
PD[log P(X : θ)].
Proof We rewrite the likelihood by combining all identical instances in our training set and then
writing the likelihood in terms of the empirical probability of each entry in our joint distribution:
log L(θ : D)
=
X
m
log P(ξ[m] : θ)
=
X
ξ
"X
m
11{ξ[m] = ξ}
#
log P(ξ : θ)
=
X
ξ
M · ˆPD(ξ) log P(ξ : θ)
=
M · IE ˆ
PD[log P(X : θ)].
We can now apply proposition 16.1 to the empirical distribution to conclude that
ℓ(θ : D) = M

IH ˆ
PD(X) −ID( ˆPD(X)||P(X : θ))

.
(17.9)
From this result, we immediately derive the following relationship between MLE and M-projections.
Theorem 17.1
The MLE ˆθ in a parametric family relative to a data set D is the M-projection of ˆPD onto the
parametric family
ˆθ = arg min
θ∈Θ ID( ˆPD||Pθ).
We see that MLE ﬁnds the distribution P(X : θ) that is the M-projection of ˆPD onto the set
of distributions representable in our parametric family.
This result allows us to call upon our detailed analysis of M-projections in order to generalize
MLE to other parametric classes in the exponential family.
In particular, in section 8.5.2,
we discussed the general notion of suﬃcient statistics and showed that the M-projection of
a distribution P into a class of distributions Q was deﬁned by the parameters θ such that
IEQθ[τ(X)] = IEP [τ(X)]. In our setting, we seek the parameters θ whose expected suﬃcient
statistics match those in ˆPD, that is, the suﬃcient statistics in D.
If our CPDs are in an exponential family where the mapping ess from parameters to suﬃcient
statistics is invertible, we can simply take the suﬃcient statistic vector from ˆPD, and invert this
mapping to produce the MLE. Indeed, this process is precisely the one that gave rise to our MLE
for multinomials and for linear Gaussians, as described earlier. However, the same process can
be applied to many other classes of distributions in the exponential family.
This analysis provides us with a notion of suﬃcient statistics τ(X) and a clearly deﬁned
path to deriving MLE parameters for any distribution in the exponential family. Somewhat more
surprisingly, it turns out that a parametric family has a suﬃcient statistic only if it is in the
exponential family.

17.3. Bayesian Parameter Estimation
733
17.3
Bayesian Parameter Estimation
17.3.1
The Thumbtack Example Revisited
Although the MLE approach seems plausible, it can be overly simplistic in many cases. Assume
again that we perform the thumbtack experiment and get 3 heads out of 10. It may be quite
reasonable to conclude that the parameter θ is 0.3. But what if we do the same experiment
with a standard coin, and we also get 3 heads? We would be much less likely to jump to the
conclusion that the parameter of the coin is 0.3. Why? Because we have a lot more experience
with tossing coins, so we have a lot more prior knowledge about their behavior. Note that we
do not want our prior knowledge to be an absolute guide, but rather a reasonable starting
assumption that allows us to counterbalance our current set of 10 tosses, under the assumption
that they may not be typical. However, if we observe 1,000,000 tosses of the coin, of which
300,000 came out heads, then we may be more willing to conclude that this is a trick coin, one
whose parameter is closer to 0.3.
Maximum likelihood allows us to make neither of these distinctions: between a thumbtack
and a coin, and between 10 tosses and 1,000,000 tosses of the coin. There is, however, another
approach, the one recommended by Bayesian statistics.
17.3.1.1
Joint Probabilistic Model
In this approach, we encode our prior knowledge about θ with a probability distribution; this
distribution represents how likely we are a priori to believe the diﬀerent choices of parameters.
Once we quantify our knowledge (or lack thereof) about possible values of θ, we can create
a joint distribution over the parameter θ and the data cases that we are about to observe
X[1], . . . , X[M]. This joint distribution captures our assumptions about the experiment.
Let us reconsider these assumptions. Recall that we assumed that tosses are independent of
each other. Note, however, that this assumption was made when θ was ﬁxed. If we do not
know θ, then the tosses are not marginally independent: Each toss tells us something about the
parameter θ, and thereby about the probability of the next toss. However, once θ is known,
we cannot learn about the outcome of one toss from observing the results of others. Thus, we
assume that the tosses are conditionally independent given θ. We can describe these assumptions
using the probabilistic model of ﬁgure 17.3.
Having determined the model structure, it remains to specify the local probability models in
this network. We begin by considering the probability P(X[m] | θ). Clearly,
P(x[m] | θ) =
 θ
if x[m] = x1
1 −θ
if x[m] = x0.
Note that since we now treat θ as a random variable, we use the conditioning bar, instead of
P(x[m] : θ).
To ﬁnish the description of the joint distribution, we need to describe P(θ). This is our prior
prior parameter
distribution
distribution over the value of θ. In our case, this is a continuous density over the interval [0, 1].
Before we discuss particular choices for this distribution, let us consider how we use it.
The network structure implies that the joint distribution of a particular data set and θ

734
Chapter 17. Parameter Estimation
qX
qX
X
Data m
(a)
(b)
X[2]
X[1]
X[M]
. . .
Figure 17.3
Meta-network for IID samples of a random variable X. (a) Plate model; (b) Ground
Bayesian network.
factorizes as
P(x[1], . . . , x[M], θ)
=
P(x[1], . . . , x[M] | θ)P(θ)
=
P(θ)
M
Y
m=1
P(x[m] | θ)
=
P(θ)θM[1](1 −θ)M[0],
where M[1] is the number of heads in the data, and M[0] is the number of tails. Note that the
expression P(x[1], . . . , x[M] | θ) is simply the likelihood function L(θ : D).
This network speciﬁes a joint probability model over parameters and data. There are several
ways in which we can use this network. Most obviously, we can take an observed data set D of
M outcomes, and use it to instantiate the values of x[1], . . . , x[M]; we can then compute the
posterior distribution over θ:
posterior
parameter
distribution
P(θ | x[1], . . . , x[M]) = P(x[1], . . . , x[M] | θ)P(θ)
P(x[1], . . . , x[M])
.
In this posterior, the ﬁrst term in the numerator is the likelihood, the second is the prior
over parameters, and the denominator is a normalizing factor that we will not expand on right
now. We see that the posterior is (proportional to) a product of the likelihood and the prior.
This product is normalized so that it will be a proper density function. In fact, if the prior
is a uniform distribution (that is, P(θ) = 1 for all θ ∈[0, 1]), then the posterior is just the
normalized likelihood function.
17.3.1.2
Prediction
If we do use a uniform prior, what then is the diﬀerence between the Bayesian approach and
the MLE approach of the previous section? The main philosophical diﬀerence is in the use of
the posterior. Instead of selecting from the posterior a single value for the parameter θ, we use
it, in its entirety, for predicting the probability over the next toss.
To derive this prediction in a principled fashion, we introduce the value of the next coin
toss x[M + 1] to our network. We can then compute the probability over x[M + 1] given the
observations of the ﬁrst M tosses. Note that, in this model, the parameter θ is unknown, and

17.3. Bayesian Parameter Estimation
735
we are considering all of its possible values. By reasoning over the possible values of θ and
using the chain rule, we see that
P(x[M + 1] | x[1], . . . , x[M]) =
=
Z
P(x[M + 1] | θ, x[1], . . . , x[M])P(θ | x[1], . . . , x[M])dθ
=
Z
P(x[M + 1] | θ)P(θ | x[1], . . . , x[M])dθ,
where we use the conditional independencies implied by the meta-network to rewrite P(x[M +
1] | θ, x[1], . . . , x[M]) as P(x[M + 1] | θ). In other words, we are integrating our posterior
over θ to predict the probability of heads for the next toss.
Let us go back to our thumbtack example. Assume that our prior is uniform over θ in the
interval [0, 1]. Then P(θ | x[1], . . . , x[M]) is proportional to the likelihood P(x[1], . . . , x[M] |
θ) = θM[1](1 −θ)M[0]. Plugging this into the integral, we need to compute
P(X[M + 1] = x1 | x[1], . . . , x[M]) =
1
P(x[1], . . . , x[M])
Z
θ · θM[1](1 −θ)M[0]dθ.
Doing all the math (see exercise 17.6), we get (for uniform priors)
P(X[M + 1] = x1 | x[1], . . . , x[M]) =
M[1] + 1
M[1] + M[0] + 2.
(17.10)
This prediction, called the Bayesian estimator, is quite similar to the MLE prediction of equa-
Bayesian
estimator
tion (17.1), except that it adds one “imaginary” sample to each count. Clearly, as the number
of samples grows, the Bayesian estimator and the MLE estimator converge to the same value.
The particular estimator that corresponds to a uniform prior is often referred to as Laplace’s
Laplace’s
correction
correction.
17.3.1.3
Priors
We now want to consider nonuniform priors. The challenge here is to pick a distribution over
this continuous space that we can represent compactly (for example, using an analytic formula),
and update eﬃciently as we get new data. For reasons that we will discuss, an appropriate prior
in this case is the Beta distribution:
Beta distribution
Deﬁnition 17.3
A Beta distribution is parameterized by two hyperparameters α1, α0, which are positive reals. The
Beta
hyperparameters
distribution is deﬁned as follows:
θ ∼Beta(α1, α0) if p(θ) = γθα1−1(1 −θ)α0−1.
The constant γ is a normalizing constant, deﬁned as follows:
γ = Γ(α1 + α0)
Γ(α1)Γ(α0),
where Γ(x) =
R ∞
0
tx−1e−tdt is the Gamma function.
Gamma function

736
Chapter 17. Parameter Estimation
0
0.2
0.4
0.6
0.8
1
q
p(q)
Beta(1,1)
0
0.2
0.4
0.6
0.8
1
q
p(q)
Beta(10,10)
0
0.2
0.4
0.6
0.8
1
q
p(q)
Beta(2,2)
0
0.2
0.4
0.6
0.8
1
q
p(q)
Beta(3,2)
0
0.2
0.4
0.6
0.8
1
q
p(q)
0
0.2
0.4
0.6
0.8
1
q
p(q)
Beta(15,10)
Beta(0.5,0.5)
Figure 17.4
Examples of Beta distributions for diﬀerent choices of hyperparameters
Intuitively, the hyperparameters α1 and α0 correspond to the number of imaginary heads and
tails that we have “seen” before starting the experiment. Figure 17.4 shows Beta distributions for
diﬀerent values of α.
At ﬁrst glance, the normalizing constant for the Beta distribution might seem somewhat
obscure. However, the Gamma function is actually a very natural one: it is simply a continuous
generalization of factorials. More precisely, it satisﬁes the properties Γ(1) = 1 and Γ(x + 1) =
xΓ(x). As a consequence, we easily see that Γ(n + 1) = n! when n is an integer.
Beta distributions have properties that make them particularly useful for parameter estimation.
Assume our distribution P(θ) is Beta(α1, α0), and consider a single coin toss X. Let us compute
the marginal probability over X, based on P(θ). To compute the marginal probability, we need
to integrate out θ; standard integration techniques can be used to show that:
P(X[1] = x1)
=
Z 1
0
P(X[1] = x1 | θ) · P(θ)dθ
=
Z 1
0
θ · P(θ)dθ =
α1
α1 + α0
.
This conclusion supports our intuition that the Beta prior indicates that we have seen α1
(imaginary) heads α0 (imaginary) tails.

17.3. Bayesian Parameter Estimation
737
Now, let us see what happens as we get more observations. Speciﬁcally, we observe M[1]
heads and M[0] tails. It follows easily that:
P(θ | x[1], . . . , x[M])
∝
P(x[1], . . . , x[M] | θ)P(θ)
∝
θM[1](1 −θ)M[0] · θα1−1(1 −θ)α0−1
=
θα1+M[1]−1(1 −θ)α0+M[0]−1,
which is precisely Beta(α1 + M[1], α0 + M[0]). This result illustrates a key property of the
Beta distribution: If the prior is a Beta distribution, then the posterior distribution, that is, the
prior conditioned on the evidence, is also a Beta distribution. In this case, we say that the Beta
distribution is conjugate to the Bernoulli likelihood function (see deﬁnition 17.4).
conjugate prior
An immediate consequence is that we can compute the probabilities over the next toss:
P(X[M + 1] = x1 | x[1], . . . , x[M]) = α1 + M[1]
α + M
,
where α = α1 + α0. In this case, our posterior Beta distribution tells us that we have seen
α1 + M[1] heads (imaginary and real) and α0 + M[0] tails.
It is interesting to examine the eﬀect of the prior on the probability over the next coin toss.
For example, the prior Beta(1, 1) is very diﬀerent than Beta(10, 10): Although both predict that
the probability of heads in the ﬁrst toss is 0.5, the second prior is more entrenched, and it
requires more observations to deviate from the prediction 0.5. To see this, suppose we observe
3 heads in 10 tosses. Using the ﬁrst prior, our estimate is
3+1
10+2 = 1
3 ≈0.33. On the other hand,
using the second prior, our estimate is
3+10
10+20 = 13
30 ≈0.43. However, as we obtain more data,
the eﬀect of the prior diminishes. If we obtain 1, 000 tosses of which 300 are heads, the ﬁrst
prior gives us an estimate of
300+1
1,000+2 and the second an estimate of
300+10
1,000+20, both of which are
very close to 0.3. Thus, the Bayesian framework allows us to capture both of the relevant

distinctions. The distinction between the thumbtack and the coin can be captured by the
strength of the prior: for a coin, we might use α1 = α0 = 100, whereas for a thumbtack,
we might use α1 = α0 = 1. The distinction between a few samples and many samples is
captured by the peakedness of our posterior, which increases with the amount of data.
17.3.2
Priors and Posteriors
We now turn to examine in more detail the Bayesian approach to dealing with unknown pa-
rameters. We start with a discussion of the general principle and deal with the case of Bayesian
networks in the next section.
As before, we assume a general learning problem where we observe a training set D that
contains M IID samples of a set of random variable X from an unknown distribution P ∗(X).
We also assume that we have a parametric model P(ξ | θ) where we can choose parameters
from a parameter space Θ.
Recall that the MLE approach attempts to ﬁnd the parameters ˆθ in Θ that are “best” given the
data. The Bayesian approach, on the other hand, does not attempt to ﬁnd such a point estimate.
point estimate
Instead, the underlying principle is that we should keep track of our beliefs about θ’s values, and
use these beliefs for reaching conclusions. That is, we should quantify the subjective probability
we assign to diﬀerent values of θ after we have seen the evidence. Note that, in representing

738
Chapter 17. Parameter Estimation
such subjective probabilities, we now treat θ as a random variable. Thus, the Bayesian approach
requires that we use probabilities to describe our initial uncertainty about the parameters θ, and
then use probabilistic reasoning (that is, Bayes rule) to take into account our observations.
To perform this task, we need to describe a joint distribution P(D, θ) over the data and the
parameters. We can easily write
P(D, θ) = P(D | θ)P(θ).
The ﬁrst term is just the likelihood function we discussed earlier. The second term is the prior
parameter prior
distribution over the possible values in Θ. This prior captures our initial uncertainty about the
parameters. It can also capture our previous experience before starting the experiment. For
example, if we study coin tossing, we might have prior experience that suggests that most coins
are unbiased (or nearly unbiased).
Once we have speciﬁed the likelihood function and the prior, we can use the data to derive
the posterior distribution over the parameters. Since we have speciﬁed a joint distribution over
parameter
posterior
all the quantities in question, the posterior is immediately derived by Bayes rule:
P(θ | D) = P(D | θ)P(θ)
P(D)
.
The term P(D) is the marginal likelihood of the data
marginal
likelihood
P(D) =
Z
Θ
P(D | θ)P(θ)dθ,
that is, the integration of the likelihood over all possible parameter assignments. This is the a
priori probability of seeing this particular data set given our prior beliefs.
As we saw, for some probabilistic models, the likelihood function can be compactly described
by using suﬃcient statistics. Can we also compactly describe the posterior distribution? In
general, this depends on the form of the prior.
As we saw in the thumbtack example of
section 17.1.1, we can sometimes ﬁnd priors for which we have a description of the posterior.
As another example of the forms of priors and posteriors, let us examine the learning problem
of example 17.3. Here we need to describe our uncertainty about the parameters of a multinomial
distribution. The parameter space Θ is the space of all nonnegative vectors θ = ⟨θ1, . . . , θK⟩
such that P
k θk = 1. As we saw in example 17.3, the likelihood function in this model has the
form:
L(θ : D) =
Y
k
θM[k]
k
.
Since the posterior is a product of the prior and the likelihood, it seems natural to require that
the prior also have a form similar to the likelihood.
One such prior is the Dirichlet distribution, which generalizes the Beta distribution we dis-
Dirichlet
distribution
cussed earlier. A Dirichlet distribution is speciﬁed by a set of hyperparameters α1, . . . , αK, so
Dirichlet
hyperparameters
that
θ ∼Dirichlet(α1, . . . , αK) if P(θ) ∝
Y
k
θαk−1
k
.
We use α to denote P
j αj. If we use a Dirichlet prior, then the posterior is also Dirichlet:
Dirichlet
posterior

17.3. Bayesian Parameter Estimation
739
Proposition 17.3
If P(θ) is Dirichlet(α1, . . . , αK) then P(θ | D) is Dirichlet(α1 + M[1], . . . , αK + M[K]),
where M[k] is the number of occurrences of xk.
Priors such as the Dirichlet are useful, since they ensure that the posterior has a nice com-
pact description. Moreover, this description uses the same representation as the prior. This
phenomenon is a general one, and one that we strive to achieve, since it makes our computa-
tion and representation much easier.
Deﬁnition 17.4
A family of priors P(θ : α) is conjugate to a particular model P(ξ | θ) if for any possible data set
conjugate prior
D of IID samples from P(ξ | θ), and any choice of legal hyperparameters α for the prior over θ,
there are hyperparameters α′ that describe the posterior. That is,
P(θ : α′) ∝P(D | θ)P(θ : α).
For example, Dirichlet priors are conjugate to the multinomial model. We note that this does
not preclude the possibility of other families that are also conjugate to the same model. See
exercise 17.7 for an example of such a prior for the multinomial model. We can ﬁnd conjugate
priors for other models as well.
See exercise 17.8 and exercise 17.11 for the development of
conjugate priors for the Gaussian distribution.
This discussion shows some examples where we can easily update our beliefs about θ after
observing a set of instances D. This update process results in a posterior that combines our
prior knowledge and our observations. What can we do with the posterior? We can use the
posterior to determine properties of the model at hand. For example, to assess our beliefs that a
coin we experimented with is biased toward heads, we might compute the posterior probability
that θ > t for some threshold t, say 0.6.
Another use of the posterior is to predict the probability of future examples. Suppose that we
are about to sample a new instance ξ[M +1]. Since we already have observations over previous
instances, the Bayesian estimator is the posterior distribution over a new example:
Bayesian
estimator
P(ξ[M + 1] | D)
=
Z
P(ξ[M + 1] | D, θ)P(θ | D)dθ
=
Z
P(ξ[M + 1] | θ)P(θ | D)dθ
=
IEP (θ|D)[P(ξ[M + 1] | θ)],
where, in the second step, we use the fact that instances are independent given θ. Thus, our
prediction is the average over all parameters according to the posterior.
Let us examine prediction with the Dirichlet prior. We need to compute
P(x[M + 1] = xk | D) = IEP (θ|D)[θk].
To compute the prediction on a new data case, we need to compute the expectation of particular
parameters with respect for a Dirichlet distribution over θ.
Proposition 17.4
Let P(θ) be a Dirichlet distribution with hyperparameters α1, . . . , αk, and α = P
j αj, then
E [θk] = αk
α .

740
Chapter 17. Parameter Estimation
Recall that our posterior is Dirichlet(α1 + M[1], . . . , αK + M[K]) where M[1], . . . , M[K]
are the suﬃcient statistics from the data. Hence, the prediction with Dirichlet priors is
P(x[M + 1] = xk | D) = M[k] + αk
M + α
.
This prediction is similar to prediction with the MLE parameters. The only diﬀerence is that
we added the hyperparameters to our counts when making the prediction. For this reason the
Dirichlet hyperparameters are often called pseudo-counts. We can think of these as the number
pseudo-counts
of times we have seen the diﬀerent outcomes in our prior experience before conducting our
current experiment.
The total α of the pseudo-counts reﬂects how conﬁdent we are in our prior, and is often
called the equivalent sample size. Using α, we can rewrite the hyperparameters as αk = αθ′
k,
equivalent
sample size
where θ′ = {θ′
k : k = 1, . . . , K} is a distribution describing the mean prediction of our prior.
mean prediction
We can see that the prior prediction (before observing any data) is simply θ′. Moreover, we can
rewrite the prediction given the posterior as:
P(x[M + 1] = xk | D) =
α
M + αθ′
k +
M
M + α · M[k]
M .
(17.11)
That is, the prediction is a weighted average (convex combination) of the prior mean and the
MLE estimate. The combination weights are determined by the relative magnitude of α — the
conﬁdence of the prior (or total weight of the pseudo-counts) — and M — the number of
observed samples. We see that the Bayesian prediction converges to the MLE estimate when
M →∞. Intuitively, when we have a very large training set the contribution of the prior is
negligible, and the prediction will be dominated by the frequency of outcomes in the data. We
also get convergence to the MLE estimate when α →0, so that we have only a very weak prior.
Note that the case where α = 0 is not achievable: the normalization constant for the Dirichlet
prior grows to inﬁnity when the hyperparameters are close to 0. Thus, the prior with α = 0
(that is, αk = 0 for all k) is not well deﬁned. The prior with α = 0 is often called a improper
improper prior
prior. The diﬀerence between the Bayesian estimate and the MLE estimate arises when M is not
too large, and α is not close to 0. In these situations, the Bayesian estimate is “biased” toward
the prior probability θ′.
To gain some intuition for the interaction between these diﬀerent factors, ﬁgure 17.5 shows
the eﬀect of the strength and means of the prior on our estimates. We can see that, as the
amount of real data grows, our estimate converges to the true underlying distribution, regardless
of the starting point. The convergence time grows both with the diﬀerence between the prior
mean and the empirical mean, and with the strength of the prior. We also see that the Bayesian
estimate is more stable than the MLE estimate, because with few instances, even single samples
will change the MLE estimate dramatically.
Example 17.7
Suppose we are trying to estimate the parameter associated with a coin, and we observe one head
and one tail. Our MLE estimate of θ1 is 1/2 = 0.5. Now, if the next observation is a head, we will
change our estimate to be 2/3 ≈0.66. On the other hand, if our next observation is a tail, we will
change our estimate to 1/3 ≈0.33. In contrast, consider the Bayesian estimate with a Dirichlet
prior with α = 1 and θ′
1 = 0.5. With this estimator, our original estimate is 1.5/3 = 0.5. If
we observe another head, we revise to 2.5/4 = 0.625, and if observe another tail, we revise to

17.4. Bayesian Parameter Estimation in Bayesian Networks
741
0.6
0.5
0.4
0.3
0.2
0.1
0
P(X = H)
M = # samples
0
20
40
60
80
100
0.6
0.5
0.4
0.3
0.2
0.1
0
P(X = H)
M = # samples
0
20
40
60
80
100
Figure 17.5
The eﬀect of the strength and means of the Beta prior on our posterior estimates. Our
data set is an idealized version of samples from a biased coin where the frequency of heads is 0.2: for a
given data set size M, we assume that D contains 0.2M heads and 0.8M tails. The x axis represents the
number of samples (M) in our data set D, and the y axis the expected probability of heads according to
the Bayesian estimate. (a) shows the eﬀect of varying the prior means θ′
1, θ′
0, for a ﬁxed prior strength α.
(b) shows the eﬀect of varying the prior strength for a ﬁxed prior mean θ′
1 = θ′
0 = 0.5.
1.5/4 = 0.375. We see that the estimate changes by slightly less after the update. If α is larger,
then the smoothing is more aggressive. For example, when α = 5, our estimate is 4.5/8 = 0.5625
after observing a head, and 3.5/8 = 0.4375 after observing a tail. We can also see this eﬀect
visually in ﬁgure 17.6, which shows our changing estimate for P(θH) as we observe a particular
sequence of tosses.
This smoothing eﬀect results in more robust estimates when we do not have enough data to
reach deﬁnite conclusions. If we have good prior knowledge, we revert to it. Alternatively, if
we do not have prior knowledge, we can use a uniform prior that will keep our estimate from
taking extreme values. In general, it is a bad idea to have extreme estimates (ones where some
of the parameters are close to 0), since these might assign too small probability to new instances
we later observe. In particular, as we already discussed, probability estimates that are actually
0 are dangerous, since no amount of evidence can change them. Thus, if we are unsure about
our estimates, it is better to bias them away from extreme estimates. The MLE estimate, on the
other hand, often assigns probability 0 to values that were not observed in the training data.
17.4
Bayesian Parameter Estimation in Bayesian Networks
We now turn to Bayesian estimation in the context of a Bayesian network.
Recall that the
Bayesian framework requires us to specify a joint distribution over the unknown parameters and
the data instances. As in the single parameter case, we can understand the joint distribution
over parameters and data as a Bayesian network.

742
Chapter 17. Parameter Estimation
0.7
0.6
0.5
0.4
0.3
0.2
0.1
M
50
5
10
15
20
25
30
35
40
45
T
H
P(X = H |   )
Figure 17.6
The eﬀect of diﬀerent priors on smoothing our parameter estimates. The graph shows
the estimate of P(X = H|D) (y-axis) after seeing diﬀerent number of samples (x-axis). The graph below
the x-axis shows the particular sequence of tosses. The solid line corresponds to the MLE estimate, and
the remaining ones to Bayesian estimates with diﬀerent strengths and uniform prior means. The large-dash
line corresponds to Beta(1, 1), the small-dash line to Beta(5, 5), and the dotted line to Beta(10, 10).
17.4.1
Parameter Independence and Global Decomposition
17.4.1.1
A Simple Example
Suppose we want to estimate parameters for a simple network with two variables X and Y
so that X is the parent of Y . Our training data consist of observations X[m], Y [m] for m =
1, . . . , M. In addition, we have unknown parameter vectors θX and θY |X. The dependencies
between these variables are described in the network of ﬁgure 17.7. This is the meta-network
meta-network
that describes our learning setup.
This Bayesian network structure immediately reveals several points. For example, as in our
simple thumbtack example, the instances are independent given the unknown parameters. A
simple examination of active trails shows that X[m] and Y [m] are d-separated from X[m′] and
Y [m′] once we observe the parameter variables.
In addition, the network structure embodies the assumption that the priors for the individual
parameters variables are a priori independent. That is, we believe that knowing the value of one
parameter tells us nothing about another. More precisely, we deﬁne
Deﬁnition 17.5
Let G be a Bayesian network structure with parameters θ = (θX1|PaX1 , . . . , θXn|PaXn). A prior
P(θ) is said to satisfy global parameter independence if it has the form:
global parameter
independence
P(θ) =
Y
i
P(θXi|PaXi).
This assumption may not be suitable for all domains, and it should be considered with care.

17.4. Bayesian Parameter Estimation in Bayesian Networks
743
X
Y
Data m
(a)
(b)
Y[2]
Y[1]
Y[M]
qY |X
. . .
X[2]
X[1]
X[M]
qX
qX
. . .
qY|X
Figure 17.7
Meta-network for IID samples from a network X →Y with global parameter inde-
pendence. (a) Plate model; (b) Ground Bayesian network.
Example 17.8
Consider an extension of our student example, where our student takes multiple classes. For each
class, we want to learn the distribution of Grade given the student’s Intelligence and the course
Diﬃculty. For classes taught by the same instructor, we might believe that the grade distribution is
the same; for example, if two classes are both diﬃcult, and the student is intelligent, his probability
of getting an A is the same in both. However, under the global parameter independence assumption,
these are two diﬀerent random variables, and hence their parameters are independent.
Thus, although we use the global parameter independence in much of our discussion, it is not
always appropriate, and we relax it in some of our later discussion (such as section 17.5 and
section 18.6.2).
If we accept global parameter independence, we can draw an important conclusion. Complete
data d-separates the parameters for diﬀerent CPDs. For example, if x[m] and y[m] are observed
for all m, then θX and θY |X are d-separated. To see this, note that any path between the two
has the form
θX →X[m] →Y [m] ←θY |X,
so that the observation of x[m] blocks the path. Thus, if these two parameter variables are
independent a priori, they are also independent a posteriori. Using the deﬁnition of conditional
independence, we conclude that
P(θX, θY |X | D) = P(θX | D)P(θY |X | D).
This decomposition has immediate practical ramiﬁcations. Given the data set D, we can deter-
mine the posterior over θX independently of the posterior over θY |X. Once we can solve each
problem separately, we can combine the results. This is the analogous result to the likelihood
decomposition for MLE estimation of section 17.2.2. In the Bayesian setting this property has
additional importance. It tells us the posterior can be represented in a compact factorized form.

744
Chapter 17. Parameter Estimation
17.4.1.2
General Networks
We can generalize this conclusion to the general case of Bayesian network learning. Suppose
we are given a network structure G with parameters θ. In the Bayesian framework, we need
to specify a prior P(θ) over all possible parameterizations of the network.
The posterior
distribution over parameters given the data samples D is simply
P(θ | D) = P(D | θ)P(θ)
P(D)
.
The term P(θ) is our prior distribution, P(D | θ) is the probability of the data given a particular
parameter settings, which is simply the likelihood function. Finally, P(D) is the normalizing
constant. As we discussed, this term is called the marginal likelihood; it will play an important
marginal
likelihood
role in the next chapter. For now, however, we can ignore it, since it does not depend on θ and
only serves to normalize the posterior.
As we discussed in section 17.2, we can decompose the likelihood into local likelihoods:
P(D | θ) =
Y
i
Li(θXi|PaXi : D).
Moreover, if we assume that we have global parameter independence, then
P(θ) =
Y
i
P(θXi|PaXi).
Combining these two decompositions, we see that
P(θ | D) =
1
P(D)
Y
i
h
Li(θXi|PaXi : D)P(θXi|PaXi)
i
.
Now each subset θXi|PaXi of θ appears in just one term in the product. Thus, we have that the
posterior can be represented as a product of local terms.
Proposition 17.5
Let D be a complete data set for X, let G be a network structure over these variables. If P(θ)
satisﬁes global parameter independence, then
P(θ | D) =
Y
i
P(θXi|PaXi | D).
The proof of this property follows from the steps we discussed. It can also be derived directly
from the structure of the meta-Bayesian network (as in the network of ﬁgure 17.7).
17.4.1.3
Prediction
This decomposition of the posterior allows us to simplify various tasks. For example, suppose
that, in our simple two-variable network, we want to compute the probability of another instance
x[M + 1], y[M + 1] based on our previous observations x[1], y[1], . . . , x[M], y[M]. According
to the structure of our meta-network, we need to sum out (or more precisely integrate out) the
unknown parameter variables
P(x[M + 1], y[M + 1] | D) =
Z
P(x[M + 1], y[M + 1] | D, θ)P(θ | D)dθ,

17.4. Bayesian Parameter Estimation in Bayesian Networks
745
where the integration is over all legal parameter values. Since θ d-separates instances from each
other, we have that
P(x[M + 1], y[M + 1] | D, θ)
=
P(x[M + 1], y[M + 1] | θ)
=
P(x[M + 1] | θX)P(y[M + 1] | x[M + 1], θY |X).
Moreover, as we just saw, the posterior probability also decomposes into a product. Thus,
P(x[M + 1], y[M + 1] | D)
=
Z Z
P(x[M + 1] | θX)P(y[M + 1] | x[M + 1], θY |X)
P(θX | D)P(θY |X | D)dθXdθY |X
=
Z
P(x[M + 1] | θX)P(θX | D)dθX

Z
P(y[M + 1] | x[M + 1], θY |X)P(θY |X | D)dθY |X

.
In the second step, we use the fact that the double integral of two unrelated functions is the
product of the integrals. That is:
Z Z
f(x)g(y)dxdy =
Z
f(x)dx
 Z
g(y)dy

.
Thus, we can solve the prediction problem for the two variables X and Y separately.
The same line of reasoning easily applies to the general case, and thus we can see that, in
the setting of proposition 17.5, we have
P(X1[M + 1], . . . , Xn[M + 1] | D) =
Y
i
Z
P(Xi[M + 1] | PaXi[M + 1], θXi|PaXi)P(θXi|PaXi | D)dθXi|PaXi.
(17.12)
We see that we can solve the prediction problem for each CPD independently and then combine
the results.
We stress that the discussion so far was based on the assumption that the priors over
parameters for diﬀerent CPDs are independent. We see that, when learning from complete data,
this assumption alone suﬃces to get a decomposition of the learning problem to several “local”
problems, each one involving one CPD.
At this stage it might seem that the Bayesian framework introduces new complications that
did not appear in the MLE setup. Note, however, that in deriving the MLE decomposition, we
used the property that we can choose parameters for one CPD independently of the others.
Thus, we implicitly made a similar assumption to get decomposition. The Bayesian treatment
forces us to make such assumptions explicit, allowing us to more carefully evaluate their validity.
We view this as a beneﬁt of the Bayesian framework.

746
Chapter 17. Parameter Estimation
X
Y
Data m
(a)
(b)
Y[2]
Y[1]
Y[M]
qY|x1
qY|x0
. . .
X[2]
X[1]
X[M]
qX
qX
. . .
qY|x0
qY|x1
Figure 17.8
Meta-network for IID samples from a network X →Y with local parameter indepen-
dence. (a) Plate model. (b) Ground Bayesian network.
17.4.2
Local Decomposition
Based on the preceding discussion, we now need to solve localized Bayesian estimation problems
to get a global Bayesian solution. We now examine this localized estimation task for table-CPDs.
The case for tree-CPDs is treated in section 17.5.2.
Consider, for example, the learning setting described in ﬁgure 17.7, where we take both X and
Y to be binary. As we have seen, we need to represent the posterior θX and θY |X given the
data. We already know how to deal with the posterior over θX. If we use a Dirichlet prior over
θX, then the posterior P(θX | x[1], . . . , x[M]) is also represented as a Dirichlet distribution.
A less obvious question is how to deal with the posterior over θY |X. If we are learning table-
CPDs, this parameter vector contains four parameters θy0|x0, . . . , θy1|x1. In our discussion of
maximum likelihood estimation, we saw how the local likelihood over these parameters can be
further decomposed into two terms, one over the parameters θY |x0 and one over the parameters
θY |x1. Do we have a similar phenomenon in the Bayesian setting?
We start with the prior over θY |X. One obvious choice is a Dirichlet prior over θY |x1 and
another over θY |x0. More precisely, we have
P(θY |X) = P(θY |x1)P(θY |x0),
where each of the terms on the right is a Dirichlet prior. Thus, in this case, we assume that the
two groups of parameters are independent a priori.
This independence assumption, in eﬀect, allows us to replace the node θY |X in ﬁgure 17.7
with two nodes, θY |x1 and θY |x0 that are both roots (see ﬁgure 17.8). What can we say about the
posterior distribution of these parameter groups? At ﬁrst, it seems that the two are dependent
on each other given the data. Given an observation of y[m], the path
θY |x0 →Y [m] ←θY |x1
is active (since we observe the sink of a v-structure), and thus the two parameters are not

17.4. Bayesian Parameter Estimation in Bayesian Networks
747
d-separated.
This, however, is not the end of the story. We get more insight if We examine how y[m]
depends on the two parameters. Clearly,
P(y[m] = y | x[m], θY |x0, θY |x1) =

θy|x0
if x[m] = x0
θy|x1
if x[m] = x1.
We see that y[m] does not depends on the value of θY |x0 when x[m] = x1. This example is an
instance of the same type of context speciﬁc independence that we discussed in example 3.7.
As discussed in section 5.3, we can perform a more reﬁned form of d-separation test in such a
situation by removing arcs that are ruled inactive in particular contexts. For the CPD of y[m],
we see that once we observe the value of x[m], one of the two arcs into y[m] is inactive. If
x[m] = x0, then the arc θY |x1 →y[m] is inactive, and if x[m] = x1, then θY |x0 →y[m] is
inactive. In either case, the v-structure θY |x0 →y[m] ←θY |x1 is removed. Since this removal
occurs for every m = 1, . . . , M, we conclude that no active path exists between θY |x0 and
θY |x1 and thus, the two are independent given the observation of the data. In other words, we
can write
P(θY |X | D) = P(θY |x1 | D)P(θY |x0 | D).
Suppose that P(θY |x0) is a Dirichlet prior with hyperparameters αy0|x0 and αy1|x0.
As
in our discussion of the local decomposition for the likelihood function in section 17.2.3, we
have that the likelihood terms that involve θY |x0 are those that measure the probability of
P(y[m] | x[m], θY |X) when x[m] = x0. Thus, we can decompose the joint distribution over
parameters and data as follows:
P(θ, D)
=
P(θX)LX(θX : D)
P(θY |x1)
Y
m:x[m]=x1
P(y[m] | x[m] : θY |x1)
P(θY |x0)
Y
m:x[m]=x0
P(y[m] | x[m] : θY |x0).
Thus, this joint distribution is a product of three separate joint distributions with a Dirichlet
prior for some multinomial parameter and data drawn from this multinomial. Our analysis for
updating a single Dirichlet now applies, and we can conclude that the posterior P(θY |x0 | D)
is Dirichlet with hyperparameters αy0|x0 + M[x0, y0] and αy1|x0 + M[x0, y1].
We can generalize this discussion to arbitrary networks.
Deﬁnition 17.6
Let X be a variable with parents U. We say that the prior P(θX|U) satisﬁes local parameter
local parameter
independence
independence if
P(θX|U) =
Y
u
P(θX|u).
The same pattern of reasoning also applies to the general case.
Proposition 17.6

748
Chapter 17. Parameter Estimation
Let D be a complete data set for X, let G be a network structure over these variables with table-
CPDs. If the prior P(θ) satisﬁes global and local parameter independence, then
P(θ | D) =
Y
i
Y
paXi
P(θXi|paXi | D).
Moreover, if P(θX|u) is a Dirichlet prior with hyperparameters αx1|u, . . . , αxK|u, then the
posterior P(θX|u | D) is a Dirichlet distribution with hyperparameters αx1|u + M[u, x1], . . . ,
αxK|u + M[u, xK].
As in the case of a single multinomial, this result induces a predictive model in which, for
the next instance, we have that
P(Xi[M + 1] = xi | U[M + 1] = u, D) =
αxi|u + M[xi, u]
P
i αxi|u + M[xi, u].
(17.13)
Plugging this result into equation (17.12), we see that for computing the probability of a new
instance, we can use a single network parameterized as usual, via a set of multinomials, but
ones computed as in equation (17.13).
17.4.3
Priors for Bayesian Network Learning
It remains only to address the question of assessing the set of parameter priors required for
Bayesian network
parameter prior
a Bayesian network. In a general Bayesian network, each node Xi has a set of multinomial
distributions θXi|paXi, one for each instantiation paXi of Xi’s parents PaXi. Each of these
parameters will have a separate Dirichlet prior, governed by hyperparameters
αXi|paXi = (αx1
i |paXi, . . . , αx
Ki
i
|paXi),
where Ki is the number of values of Xi.
We can, of course, ask our expert to assign values to each of these hyperparameters based on
his or her knowledge. This task, however, is rather unwieldy. Another approach, called the K2
prior, is to use a ﬁxed prior, say αxj
i |paXi = 1, for all hyperparameters in the network. As we
discuss in the next chapter, this approach has consequences that are conceptually unsatisfying;
see exercise 18.10.
A common approach to addressing the speciﬁcation task uses the intuitions we described
in our discussion of Dirichlet priors in section 17.3.1.
As we showed, we can think of the
hyperparameter αxk as an imaginary count in our prior experience. This intuition suggests the
following representation for a prior over a Bayesian network. Suppose we have an imaginary
data set D′ of “prior” examples. Then, we can use counts from this imaginary data set as
hyperparameters. More speciﬁcally, we set
αxi|paXi = α[xi, paXi],
where α[xi, paXi] is the number of times Xi = xi and PaXi = paXi in D′. We can easily see
that prediction with this setting of hyperparameters is equivalent to MLE prediction from the
combined data set that contains instances of both D and D′.

17.4. Bayesian Parameter Estimation in Bayesian Networks
749
One problem with this approach is that it requires storing a possibly large data set of pseudo-
instances. Instead, we can store the size of the data set α and a representation P ′(X1, . . . , Xn)
of the frequencies of events in this prior data set. If P ′(X1, . . . , Xn) is the distribution of
events in D′, then we get that
αxi|paXi = α · P ′(xi, paXi).
How do we represent P ′? Clearly, one natural choice is via a Bayesian network. Then, we can
use Bayesian network inference to eﬃciently compute the quantities P ′(xi, paXi). Note that P ′
does not have to be structured in the same way as the network we learn (although it can be). It
is, in fact, quite common to deﬁne P ′ as a set of independent marginals over the Xi’s. A prior
that can be represented in this manner (using α and P ′) is called a BDe prior. Aside from being
BDe prior
philosophically pleasing, it has some additional beneﬁts that we will discuss in the next chapter.
Box 17.C — Case Study: Learning the ICU-Alarm Network. To give an example of the tech-
niques described in this chapter, we evaluate them on a synthetic example. Figure 17.C.1 shows the
graph structure of the real-world ICU-Alarm Bayesian network, hand-constructed by an expert, for
ICU-Alarm
monitoring patients in an Intensive Care Unit (ICU). The network has 37 nodes and a total of 504
parameters. We want to evaluate the ability of our parameter estimation algorithms to reconstruct
the network parameters from data.
We generated a training set from the network, by sampling from the distribution speciﬁed by
the network. We then gave the algorithm only the (correct) network structure, and the generated
data, and measured the ability of our algorithms to reconstruct the parameters. We tested the MLE
approach, and several Bayesian approaches. All of the approaches used a uniform prior mean, but
diﬀerent prior strengths α.
In performing such an experiment, there are many ways of measuring the quality of the learned
network. One possible measure is the diﬀerence between the original values of the model parameters
and the estimated ones. A related approach measures the distance between the original CPDs and
the learned ones (in the case of table-CPDs, these two approaches are the same, but not for general
parameterizations). These approaches place equal weights on diﬀerent parameters, regardless of the
extent to which they inﬂuence the overall distribution.
The approach we often take is the one described in section 16.2.1, where we measure the rel-
ative entropy between the generating distribution P ∗and the learned distribution ˜P (see also
section 8.4.2). This approach provides a global measure of the extent to which our learned distribu-
tion resembles the true distribution. Figure 17.C.2 shows the results for diﬀerent stages of learning.
As we might expect, when more instances are available, the estimation is better. The improvement
is drastic in early stages of learning, where additional instances lead to major improvements. When
the number of instances in our data set is larger, additional instances lead to improvement, but a
smaller one.
More surprisingly, we also see that the MLE achieves the poorest results, a consequence of its
extreme sensitivity to the speciﬁc training data used. The lowest error is achieved with a very weak
prior — α = 5 — which is enough to provide smoothing. As the strength of the prior grows, it
starts to introduce a bias, not giving the data enough importance. Thus, the error of the estimated
probability increases. However, we also note that the eﬀect of the prior, even for α = 50, disappears
reasonably soon, and all of the approaches converge to the same line. Interestingly, the diﬀerent

750
Chapter 17. Parameter Estimation
MINVOLSET
PRESS
DISCONNECT
VENTMACH
KINKEDTUBE
INTUBATION
PULMEMBOLUS
VENITUBE
SHUNT
PAP
VENTLUNG
VENTALV
FIO2
MINOVL
ARTCO2
PVSAT
ANAPHYLAXIS
INSUFF ANESTH
EXPCO2
SAO2
TPR
CATECHOL
LV FAILURE
HYPOVOLEMIA
ERRCAUTER
HR
ERRBLOW OUTPUT
HISTORY
STROE VOLUME
LVED VOLUME
HRBP
BP
HRSAT
HREKG
CO
PCWP
CVP
Figure 17.C.1 — The ICU-Alarm Bayesian network.
Bayesian approaches converge to this line long before the MLE approach. Thus, at least in this
example, an overly strong bias provided by the prior is still a better compromise than the complete
lack of smoothing of the MLE approach.

17.4. Bayesian Parameter Estimation in Bayesian Networks
751
1.4
1.2
1
0.8
0.6
0.4
0.2
0
KL Divergence
0
500
1000 1500 2000 2500 3000 3500 4000 4500 5000
M = # instances
Bayes; α = 5
MLE
Bayes; α = 20
Bayes; α = 50
Figure 17.C.2 — Learning curve for parameter estimation for the ICU-Alarm network Relative en-
tropy to true model as the amount of training data grows, for diﬀerent values of the prior strength
α.
17.4.4
MAP Estimation ⋆
Our discussion in this chapter has focused solely on Bayesian estimation for multinomial CPDs.
Here, we have a closed form solution for the integral required for Bayesian prediction, and thus
we can perform it eﬃciently. In many other representations, the situation is not so simple. In
some cases, such as the noisy-or model or the logistic CPDs of section 5.4.2, we do not have
a conjugate prior or a closed-form solution for the Bayesian integral. In those cases, Bayesian
prediction requires numerical solutions for high-dimensional integrals. In other settings, such
as the linear Gaussian CPD, we do have a conjugate prior (the normal-Gamma distribution), but
normal-Gamma
distribution
we may prefer other priors that oﬀer other desirable properties (such as the sparsity-inducing
Laplacian prior described in section 20.4.1).
When a full Bayesian solution is impractical, we can resort to using maximum a posteriori
MAP estimation
(MAP) estimation. Here, we search for parameters that maximize the posterior probability:
˜θ = arg max
θ
log P(θ | D).
When we have a large amount of data, the posterior is often sharply peaked around its maximum
˜θ. In this case, the integral
P(X[M + 1] | D) =
Z
P(X[M + 1] | θ)P(θ | D)dθ
will be roughly P(X[M + 1] | ˜θ). More generally, we can view the MAP estimate as a way of
using the prior to provide regularization over the likelihood function:
regularization

752
Chapter 17. Parameter Estimation
arg max
θ
log P(θ | D)
=
arg max
θ
log
P(θ)P(D | θ)
P(D)

=
arg max
θ
(log P(θ) + log P(D | θ)) .
(17.14)
That is, ˜θ is the maximum of a function that sums together the log-likelihood function and
log P(θ). This latter term takes into account the prior on diﬀerent parameters and therefore
biases the parameter estimate away from undesirable parameter values (such as those involving
conditional probabilities of 0) when we have few learning instances.
When the number of
samples is large, the eﬀect of the prior becomes negligible, since ℓ(θ : D) grows linearly with
the number of samples whereas the prior does not change.
Because our parameter priors are generally well behaved, MAP estimation is often no harder
than maximum likelihood estimation, and is therefore often applicable in practice, even in cases
where Bayesian estimation is not. Importantly, however, it does not oﬀer all of the same beneﬁts
as a full Bayesian estimation. In particular, it does not attempt to represent the shape of the
posterior and thus does not diﬀerentiate between a ﬂat posterior and a sharply peaked one.
As such, it does not give us a sense of our conﬁdence in diﬀerent aspects of the parameters,
and the predictions do not average over our uncertainty. This approach also suﬀers from issues
regarding representation independence; see box 17.D.
Box 17.D — Concept: Representation Independence. One important property we may want
of an estimator is representation independence. To understand this concept better, suppose that
representation
independence
in our thumbtack example, we choose to use a parameter η, so that P ′(X = H | η) =
1
1+e−η .
We have that η = log
θ
1−θ where θ is the parameter we used earlier. Thus, there is a one-to-one
correspondence between a choice θ and a choice η. Although one choice of parameters might seem
more natural to us than another, there is no formal reason why we should prefer one over the other,
since both can represent exactly the same set of distributions.
More generally, a reparameterization of a given family is a new set of parameter values η in a
space Υ and a mapping from the new parameters to the original one, that is, from η to θ(η) so
that P(· | η) in the new parameterization is equal to P(· | θ(η)) in the original parameterization.
In addition, we require that the reparameterization maintain the same set of distributions, that is,
for each choice of θ there is η such that P(· | η) = P(· | θ).
This concept immediately raises the question as to whether the choice of representation can
impact our estimates. While we might prefer a particular way of parameterization because it is
more intuitive or interpretable, we may not want this choice to bias our estimated parameters.
Fortunately, it is not diﬃcult to see that maximum likelihood estimation is insensitive to repa-
rameterization. If we have two diﬀerent ways to represent the same family of the distribution,
then the distributions in the family that maximize the likelihood using one parameterization also
maximize the likelihood with the other parameterization. More precisely, if ˆη is MLE, then the
matching parameter values θ(ˆη) are also MLE when we consider the likelihood function in the θ
space. This property is a direct consequence of the fact that the likelihood function is a function of
the distribution induced by the parameter values, and not of the actual parameter values.
The situation with Bayesian inference is subtler.
Here, instead of identifying the maximum
parameter value, we now perform integration over all possible parameter values. Naively, it seems
that such an estimation is more sensitive to the parameterization than MLE, which depends only

17.4. Bayesian Parameter Estimation in Bayesian Networks
753
on the maximum of the likelihood surface. However, a careful choice of prior can account for the
representation change and thereby lead to representation independence. Intuitively, if we consider
a reparameterization η with a function θ(η) mapping to the original parameter space, then we
would like the prior on η to maintain the probability of events. That is,
P(A) = P({θ(η) : η ∈A}), ∀A ⊂Υ.
(17.15)
This constraint implies that the prior over diﬀerent regions of parameters is maintained. Under this
assumption, Bayesian prediction will be identical under the two parameterizations.
We illustrate the notion of a reparameterized prior in the context of a Bernoulli distribution:
Example 17.9
Consider a Beta prior over the parameter θ of a Bernoulli distribution:
P(θ : α0, α1) = cθα1−1(1 −θ)α0−1,
(17.16)
where c is the normalizing constant described in deﬁnition 17.3. Recall (example 8.5) that the
natural parameter for a Bernoulli distribution is
η = log
θ
1 −θ
with the transformation
θ =
1
1 + e−η ,
1 −θ =
1
1 + eη .
What is the prior distribution on η? To preserve the probability of events, we want to make sure
that for every interval [a, b]
b
Z
a
cθα1−1(1 −θ)α0−1dθ =
log
b
1−b
Z
log
a
1−a
P(η)dη.
To do so, we need to perform a change of variables. Using the relation between η and θ, we get
dη =
1
θ(1 −θ)dθ.
Plugging this into the equation, we can verify that an appropriate prior is:
P(η) = c

1
1 + e−η
α1 
1
1 + eη
α0
,
where c is the same constant as before. This means that the prior on η, when stated in terms of θ,
is θα1(1 −θ)α0, in contrast to equation (17.16). At ﬁrst this discrepancy seems like a contradiction.
However, we have to remember that the transformation from θ to η takes the region [0, 1] and
stretches it to the whole real line. Thus, the matching prior cannot be uniform.
This example demonstrates that a uniform prior, which we consider to be unbiased or uninfor-
mative, can seem very diﬀerent when we consider a diﬀerent parameterization.

754
Chapter 17. Parameter Estimation
Thus, both MLE and Bayesian estimation (when carefully executed) are representation-independent.
This property, unfortunately, does not carry through to MAP estimation. Here we are not interested
in the integral over all parameters, but rather in the density of the prior at diﬀerent values of the
parameters. This quantity does change when we reparameterize the prior.
Example 17.10
Consider the setting of example 17.9 and develop the MAP parameters for the priors we considered
there. When we use the θ parameterization, we can check that
˜θ = arg max
θ
log P(θ) =
α1 −1
α0 + α1 −2.
On the other hand,
˜η = arg max
η
log P(η) = log α1
α0
.
To compare the two, we can transform ˜η to θ representation and ﬁnd
θ(˜η) =
α1
α0 + α1
.
In other words, the MAP of the η parameterization gives the same predictions as the mean param-
eterization if we do the full Bayesian inference.

Thus, MAP estimation is more sensitive to choices in formalizing the likelihood and the
prior than MLE or full Bayesian inference. This suggests that the MAP parameters involve,
to some extent, an arbitrary choice. Indeed, we can bias the MAP toward diﬀerent solutions if
we construct a speciﬁc reparameterization where the density is particularly large in speciﬁc regions
of the parameter space. The parameterization dependency of MAP is a serious caveat we should be
aware of.
17.5
Learning Models with Shared Parameters
In the preceding discussion, we focused on parameter estimation for Bayesian networks with
table-CPDs. In this discussion, we made the strong assumption that the parameters for each
conditional distribution P(Xi | ui) can be estimated separately from parameters of other
conditional distributions.
In the Bayesian case, we also assumed that the priors on these
distributions are independent.
This assumption is a very strong one, which often does not
hold in practice. In real-life systems, we often have shared parameters: parameters that occur
shared
parameters
in multiple places across the network. In this section, we discuss how to perform parameter
estimation in networks where the same parameters are used multiple times.
Analogously to our discussion of parameter estimation, we can exploit both global and local
structure. Global structure occurs when the same CPD is used across multiple variables in the
network. This type of sharing arises naturally from the template-based models of chapter 6.
Local structure is ﬁner-grained, allowing parameters to be shared even within a single CPD; it
arises naturally in some types of structured CPDs. We discuss each of these scenarios in turn,
focusing on the simple case of MLE.

17.5. Learning Models with Shared Parameters
755
We then discuss the issues arising when we want to use Bayesian estimation. Finally, we dis-
cuss the hierarchical Bayes framework, a “softer” version of parameter sharing, where parameters
are encouraged to be similar but do not have to be identical.
17.5.1
Global Parameter Sharing
Let us begin with a motivating example.
Example 17.11
Let us return to our student, who is now taking two classes c1, c2, each of which is associated with
a Diﬃculty variable, D1, D2. We assume that the grade Gi of our student in class ci depends
on his intelligence and the class diﬃculty. Thus, we model Gi as having I and Di as parents.
Moreover, we might assume that these grades share the same conditional distribution. That is, the
probability that an intelligent student receives an “A” in an easy class is the same regardless of the
identity of the particular class. Stated diﬀerently, we assume that the diﬃculty variable summarizes
all the relevant information about the challenge the class presents to the student.
How do we formalize this assumption? A straightforward solution is to require that for all choices
of grade g, diﬃculty d and intelligence i, we have that
P(G1 = g | D1 = d, I = i) = P(G2 = g | D2 = d, I = i).
Importantly, this assumption does not imply that the grades are necessarily the same, but rather
that the probability of getting a particular grade is the same if the class has the same diﬃculty.
This example is simply an instance of a network induced by the simple plate model described
in example 6.11 (using Di to encode D(ci) and similarly for Gi). Thus, as expected, template
models give rise to shared parameters.
17.5.1.1
Likelihood Function with Global Shared Parameters
As usual, the key to parameter estimation lies in understanding the structure of the likelihood
function. To analyze this structure, we begin with some notation. Consider a network structure
G over a set of variables X = {X1, . . . , Xn}, parameterized by a set of parameters θ. Each
variable Xi is associated with a CPD P(Xi | U i, θ). Now, rather than assume that each such
CPD has its own parameterization θXi|U i, we assume that we have a certain set of shared
parameters that are used by multiple variables in the network. Thus, the sharing of parameters
is global, over the entire network.
global parameter
sharing
More precisely, we assume that θ is partitioned into disjoint subsets θ1, . . . , θK; with each
such subset, we associate a set of variables Vk ⊂X, such that V1, . . . , VK is a disjoint partition
of X. For Xi ∈Vk, we assume that the CPD of Xi depends only on θk; that is,
P(Xi | U i, θ) = P(Xi | U i, θk).
(17.17)
Moreover, we assume that the form of the CPD is the same for all Xi, Xj ∈Vk; that is,
P(Xi | U i, θk) = P(Xj | U j, θk).
(17.18)
We note that this last statement makes sense only if Val(Xi) = Val(Xj) and Val(U i) =
Val(U j). To avoid ambiguous notation, for any variable Xi ∈Vk, we use yl
k to range over
possible values of Xi and wl
k to range over the possible values of its parents.

756
Chapter 17. Parameter Estimation
Consider the decomposition of the probability distribution in this case:
P(X1, . . . , Xn | θ)
=
n
Y
i=1
P(Xi | PaXi, θ)
=
K
Y
k=1
Y
Xi∈Vk
P(Xi | PaXi, θ)
=
K
Y
k=1
Y
Xi∈Vk
P(Xi | PaXi, θk),
where the second equality follows from the fact that V1, . . . , VK deﬁnes a partition of X, and
the third equality follows from equation (17.17).
Now, let D be some assignment of values to the variables X1, . . . , Xn; our analysis can easily
handle multiple IID instances, as in our earlier discussion, but this extension only clutters the
notation. We can now write
L(θ : D) =
K
Y
k=1
Y
Xi∈Vk
P(xi | ui, θk).
This expression is identical to the one we used in section 17.2.2 for the case of IID instances.
There, for each set of parameters, we had multiple instances {(xi[m], ui[m])}M
m=1, all of
which were generated from the same conditional distribution. Here, we have multiple instances
{(xi, ui)}Xi∈Vk, all of which are also generated from the same conditional distribution. Thus,
it appears that we can use the same analysis as we did there.
To provide a formal derivation, consider ﬁrst the case of table-CPDs. Here, our parameteriza-
tion is a set of multinomial parameters θk
yk|wk, where we recall that yk ranges over the possible
values of each of the variables Xi ∈Vk and wk over the possible value assignments to its
parents. Using the same derivation as in section 17.2.2, we can now write:
L(θ : D)
=
K
Y
k=1
Y
yk,wk
Y
Xi∈Vk :
xi=yk,ui=wk
θk
yk|wk
=
K
Y
k=1
Y
yk,wk
(θk
yk|wk)
ˇ
Mk[yk,wk],
where we now have a new deﬁnition of our counts:
ˇ
Mk[yk, wk] =
X
Xi∈Vk
11{xi = yk, ui = wk}.
In other words, we now use aggregate suﬃcient statistics, which combine suﬃcient statistics from
aggregate
suﬃcient
statistics
multiple variables across the same network.

17.5. Learning Models with Shared Parameters
757
Given this formulation of the likelihood, we can now obtain the maximum likelihood solution
for each set of shared parameters to get the estimate
ˆθk
yk|wk =
ˇ
Mk[yk, wk]
ˇ
Mk[wk]
.
Thus, we use the same estimate as in the case of independent parameters, using our aggregate
suﬃcient statistics. Note that, in cases where our variables Xi have no parents, wk is the empty
tuple ε. In this case, ˇ
Mk[ε] is the number of variables Xi in Vk.
This aggregation of suﬃcient statistics applies not only to multinomial distributions. Indeed,
for any distribution in the linear exponential family, we can perform precisely the same aggre-
linear
exponential
family
gation of suﬃcient statistics over the variables in Vk. The result is a likelihood function in the
same form as we had before, but written in terms of the aggregate suﬃcient statistics rather
than the suﬃcient statistics for the individual variables. We can then perform precisely the same
maximum likelihood estimation process and obtain the same form for the MLE, but using the
aggregate suﬃcient statistics. (See exercise 17.14 for another simple example.)
Does this aggregation of suﬃcient statistics make sense? Returning to our example, if we
treat the grade of the student in each class as independent sample from the same parameters,
then each data instance provides us with two independent samples from this distribution. It is
important to clarify that, although the grades of the student are dependent on his intelligence,
the samples are independent samples from the same distribution. More precisely, if D1 = D2,
then both G1 and G2 are governed by the same multinomial distribution, and the student’s
grades are two independent samples from this distribution.
Thus, when we share parameters, multiple observations from within the same network con-
tribute to the same suﬃcient statistic, and thereby help estimate the same parameter. Reducing
the number of parameters allows us to obtain parameter estimates that are less noisy and closer
to the actual generating parameters. This beneﬁt comes at a price, since it requires us to make
an assumption about the domain. If the two distributions with shared parameters are actually
diﬀerent, the estimated parameters will be a (weighted) average of the estimate we would have
had for each of them separately. When we have a small number of instances, that approxima-
tion may still be beneﬁcial, since each of the separate estimates may be far from its generating
parameters, owing to sample noise. When we have more data, however, the shared parame-
ters estimate will be worse than the individual ones. We return to this issue in section 17.5.4,
where we provide a solution that allows us to gradually move away from the shared parameter
assumption as we get more data.
17.5.1.2
Parameter Estimation for Template-Based Models
As we mentioned, the template models of chapter 6 are speciﬁcally designed to encode global
parameter sharing. Recall that these representations involve a set of template-level parameters,
each of which is used multiple times when the ground network is deﬁned. For the purpose of
our discussion, we focus mostly on plate models, since they are the simplest of the (directed)
template-based representations and serve to illustrate the main points.
As we discussed, it is customary in many plate models to explicitly encode the parameter
sharing by including, in the model, random variables that encode the model parameters. This
approach allows us to make clear the exact structure of the parameter sharing within the model.

758
Chapter 17. Parameter Estimation
Courses c
Students s
(a)
(b)
q D
q G,C
q G
q I
Intelligence
Difﬁculty
Grade
Courses c
Students s
q I
Intelligence
Grade
Figure 17.9
Two plate models for the University example, with explicit parameter variables. (a)
Model where all parameters are global. (b) Model where the diﬃculty is a course-speciﬁc parameter rather
than a discrete random variable.
As we mentioned, when parameters are global and shared across over all ground variables
derived from a particular template variables, we may choose (purely as a notational convenience)
not to include the parameters explicitly in the model. We begin with this simple setting, and
then we extend it to the more general case.
Example 17.12
Figure 17.9a is a representation of the plate model of example 6.11, except that we now explicitly
encode the parameters as variables within the model. In this representation, we have made it clear
that there is only a single parameter θG
G|I,D, which is the parent of the variables within the plates.
Thus, as we can see, the same parameters are used in every CPD P(G(s, c) | I(s), D(c)) in the
ground network.
When all of our parameters are global, the sharing structure is very simple. Let A(U1, . . . , Uk)
be any attribute in the set of template attributes ℵ. Recall from deﬁnition 6.10 that this attribute
induces a ground random variable A(γ) for any assignment γ = ⟨U1 7→u1, . . . , Uk 7→uk⟩∈
Γκ[A]. All of these variables share the same CPD, and hence the same parameters. Let θA be
the parameters for the CPD for A in the template-level model. We can now simply deﬁne VA to
be all of the variables of the form A(γ) in the ground network. The analysis of section 17.5.1.1
now applies unchanged.
Example 17.13
Continuing example 17.12, the likelihood function for an assignment ξ to a ground network in the
University domain would have the form
Y
i
(θI
i )
ˇ
MI[i] Y
d
(θD
d )
ˇ
MD[d] Y
g,i,d
(θG
g|i,d)
ˇ
MG[g,i,d].
Importantly, the counts are computed as aggregate suﬃcient statistics, each with its own appropriate
set of variables. In particular,
ˇ
MI[i] =
X
s∈Oκ[Student]
11{I(s) = i},

17.5. Learning Models with Shared Parameters
759
whereas
ˇ
MG[g, i, d] =
X
⟨s,c⟩∈Γκ[Grade]
11{I(s) = i, D(c) = d, G(s, c) = g}.
For example, the counts for g1, i1, d1 would be the number of all of the student,course pairs s, c
such that student s has high intelligence, course c has high diﬃculty, and the grade of s in c is an
A. The MLE for θG
g1|i1,d1 is the fraction of those students among all (student,course) pairs.
To provide a concrete formula in the more general case, we focus on table-CPDs. We can
now deﬁne, for any attribute A(X) with parents B1(U 1), . . . , Bk(U k), the following aggregate
suﬃcient statistics:
ˇ
MA[a, b1, . . . , bk] =
X
γ∈Γκ[A]
11{A(γ) = a, B1(γ[U 1]) = b1, . . . , Bk(γ[U k]) = bk},
(17.19)
where γ[U j] denotes the subtuple of the assignment to U that corresponds to the logical
variables in U j. We can now deﬁne the template-model log-likelihood for a given skeleton:
ℓ(θ : κ) =
X
A∈ℵ
X
a∈Val(A)
X
b∈Val(PaA)
ˇ
MA[a, b] log θA=a,PaA=b.
(17.20)
From this formula, the maximum likelihood estimate for each of the model parameters follows
easily, using precisely the same analysis as before.
In our derivation so far, we focused on the setting where all of the model parameters are
global.
However, as we discussed, the plate representation can also encompass more local
parameterizations.
Fortunately, from a learning perspective, we can reduce this case to the
previous one.
Example 17.14
Figure 17.9b represents the setting where each course has its own model for how the course diﬃculty
aﬀects the students’ grades. That is, we now have a set of parameters θG,c, which are used in the
CPDs of all of the ground variables G(c, s) for diﬀerent values of s, but there is no sharing between
G(c, s) and G(c′, s).
As we discussed, this setting is equivalent to one where we add the course ID c as a parent to the
D variable, forcing us to introduce a separate set of parameters for every assignment to c. In this
case, the dependence on the speciﬁc course ID subsumes the dependence on the diﬃculty parameter
D, which we have (for clarity) dropped from the model. From this perspective, the parameter
estimation task can now be handled in exactly the same way as before for the parameters in the
(much larger) CPD for G. In eﬀect, this transformation converts global sharing to local sharing,
which we will handle.
There is, however, one important subtlety in scenarios such as this one. Recall that, in general,
diﬀerent skeletons will contain diﬀerent objects. Parameters that are speciﬁc to objects in the
model do not transfer from one skeleton to another. Thus, we cannot simply transfer the object-
speciﬁc parameters learned from one skeleton to another. This limitation is important, since a
major beneﬁt of the template-based formalisms is the ability to learn models in one instantiation
of the template and use it in other instantiations. Nevertheless, the learned models are still useful

760
Chapter 17. Parameter Estimation
in many ways. First, the learned model itself often provides signiﬁcant insight about the objects
in the training data. For example, the LDA model of box 17.E tells us, for each of the documents
in our training corpus, what the mix of topics in that particular document is.
Second, the
model parameters that are not object speciﬁc can be transferred to other skeletons. For example,
the word multinomials associated with diﬀerent topics that are learned from one document
collection can also be used in another, leaving only the new document-speciﬁc parameters to
be inferred.
Although we have focused our formal presentation on plate models, the same analysis also
applies to DBNs, PRMs, and a variety of other template-based languages that share parameters.
See exercise 17.16 and exercise 17.17 for two examples.
17.5.2
Local Parameter Sharing
In the ﬁrst part of this section, we focused on cases where all of the parameter sharing occurred
between diﬀerent CPDs. However, we might also have shared parameters that are shared locally,
local parameter
sharing
within a single CPD.
Example 17.15
Consider the CPD of ﬁgure 5.4 where we model the probability of the student getting a job based
on his application, recommendation letter, and SAT scores. As we discussed there, if the student
did not formally apply for a position, then the recruiting company does not have access to the
recommendation letter or SAT scores. Thus, for example, the conditional probability distribution
P(J | a0, s0, l0) is equal to P(J | a0, s1, l1).
In fact, the representations we considered in section 5.3 can be viewed as encoding parameter
sharing for diﬀerent conditional distributions. That is, each of these representations (for example,
tree-CPDs) is a language to specify which of the conditional distributions within a CPD are
equal to each other. As we saw, these equality constraints had implications in terms of the
independence statements encoded by a model and can also in some cases be exploited in
inference. (We note that not all forms of local structure can be reduced to a simple set of
equality constraints on conditional distributions within a CPD. For example, noisy-or CPDs or
generalized linear models combine their parameters in very diﬀerent ways and require very
diﬀerent techniques than the ones we discuss in this section.)
Here, we focus on the setting where the CPD deﬁnes a set of multinomial distributions, but
some of these distributions are shared across multiple contexts. In particular, we assume that
our graph G now deﬁnes a set of multinomial distributions that make up CPDs for G: for each
variable Xi and each ui ∈Val(U i) we have a multinomial distribution. We can use the tuple
⟨Xi, ui⟩to designate this multinomial distribution, and deﬁne
D = ∪n
i=1{⟨Xi, ui⟩: ui ∈Val(U i)}
to be the set containing all the multinomial distributions in G. We can now deﬁne a set of
shared parameters θk, k = 1, . . . , K, where each θk is associated with a set Dk ⊆D of
multinomial distributions. As before, we assume that D1, . . . , DK deﬁnes a disjoint partition
of D. We assume that all conditional distributions within Dk share the same parameters θk.
Thus, we have that if ⟨Xi, ui⟩∈Dk, then
P(xj
i | ui) = θk
j .

17.5. Learning Models with Shared Parameters
761
For this constraint to be coherent, we require that all the multinomial distributions within the
same partition have the same set of values: for any ⟨Xi, ui⟩, ⟨Xj, uj⟩∈Dk, we have that
Val(Xi) = Val(Xj).
Clearly, the case where no parameter is shared can be represented by the trivial partition into
singleton sets. However, we can also deﬁne more interesting partitions.
Example 17.16
To capture the tree-CPD of ﬁgure 5.4, we would deﬁne the following partition:
Da0
=
{⟨J, (a0, s0, l0)⟩, ⟨J, (a0, s0, l1)⟩, ⟨J, (a0, s1, l0)⟩, ⟨J, (a0, s1, l1)⟩}
Da1,s0,l0
=
{⟨J, (a1, s0, l0)⟩}
Da1,s0,l1
=
{⟨J, (a1, s0, l1)⟩}
Da1,s1
=
{⟨J, (a1, s1, l0)⟩, ⟨J, (a1, s1, l1)⟩}.
More generally, this partition-based model can capture local structure in both tree-CPDs
and in rule-based CPDs. In fact, when the network is composed of multinomial CPDs, this
ﬁner-grained sharing also generalizes the sharing structure in the global partition models of
section 17.5.1.
We can now reformulate the likelihood function in terms of the shared parameters θ1, . . . , θK.
Recall that we can write
P(D | θ) =
Y
i
Y
ui
"Y
xi
P(xi | ui, θ)M[xi,ui]
#
.
Each of the terms in square brackets is the local likelihood of a single multinomial distribution.
We now rewrite each of the terms in the innermost product in terms of the shared parameters,
and we aggregate them according to the partition:
P(D | θ)
=
Y
i
Y
ui
"Y
xi
P(xi | ui, θ)M[xi,ui]
#
=
Y
k
Y
⟨Xi,ui⟩∈Dk
Y
j
(θk
j )M[xj
i ,ui]
=
Y
k

Y
j
(θk
j )
P
⟨Xi,ui⟩∈Dk M[xj
i ,ui]

.
(17.21)
This ﬁnal expression is reminiscent of the likelihood in the case of independent parameters,
except that now each of the terms in the square brackets involves the shared parameters. Once
again, we can deﬁne a notion of aggregate suﬃcient statistics:
ˇ
Mk[j] =
X
⟨Xi,ui⟩∈Dk
M[xj
i, ui],
and use those aggregate suﬃcient statistics for parameter estimation, exactly as we used the
unaggregated suﬃcient statistics before.

762
Chapter 17. Parameter Estimation
17.5.3
Bayesian Inference with Shared Parameters
To perform Bayesian estimation, we need to put a prior on the parameters. In the case without
parameter sharing, we had a separate (independent) prior for each parameter. This model is
clearly in violation of the assumptions made by parameter sharing. If two parameters are shared,
we want them to be identical, and thus it is inconsistent to assume they have independent prior.
The right approach is to place a prior on the shared parameters.
Consider, in particular, the local analysis of section 17.5.2, we would place a prior on each of
the multinomial parameters θk. As before, it is very convenient to assume that each of these
set of parameters are independent from each other. This assumption corresponds to the local
parameter independence we made earlier, but applied in the context where we force the given
parameter-sharing strategy. We can use a similar idea in the global analysis of section 17.5.1,
introducing a prior over each set of parameters θk. If we impose an independence assumption
for the priors of the diﬀerent sets, we obtain a shared-parameter version of the global parameter
independence assumption.
One important subtlety relates to the choice of the prior. Given a model with shared param-
eters, the analysis of section 17.4.3 no longer applies directly. See exercise 17.13 for one possible
extension.
As usual, if the prior decomposes as a product and the likelihood decomposes as a product
along the same lines, then our posterior also decomposes. For example, returning to equa-
tion (17.21), we have that:
P(θ | D) ∝
K
Y
k=1
P(θk)
Y
j
(θk
j )
ˇ
Mk[j].
The actual form of the posterior depends on the prior.
Speciﬁcally, if we use multinomial
distributions with Dirichlet priors, then the posterior will also be a Dirichlet distribution with
the appropriate hyperparameters.
This discussion seems to suggest that the line of reasoning we had in the case of independent
parameters is applicable to the case of shared parameters. However, there is one subtle point
that can be diﬀerent. Consider the problem of predicting the probability of the next instance,
which can be written as:
P(ξ[M + 1] | D) =
Z
P(ξ[M + 1] | θ)P(θ | D)dθ.
To compute this formula, we argued that, since P(ξ[M + 1] | θ) is a product of parameters
Q
i θxi[M+1]|ui[M+1], and since the posterior of these parameters are independent, then we can
write (for multinomial parameters)
P(ξ[M + 1] | D) =
Y
i
IE

θxi[M+1]|ui[M+1] | D

,
where each of the expectations is based on the posterior over θxi[M+1]|ui[M+1].
When we have shared parameters, we have to be more careful. If we consider the network
of example 17.11, then when the (M + 1)st instance has two courses of the same diﬃculty, the
likelihood term P(ξ[M +1] | θ) involves a product of two parameters that are not independent.
More explicitly, the likelihood involves P(G1[M +1] | I[M +1], D1[M +1]) and P(G2[M +1] |

17.5. Learning Models with Shared Parameters
763
D1[M]
D2[M]
D1[M+1]
I [M]
q
I [M+1]
D2[M+1]
G1[M]
G2[M]
G1[M+1]
G2[M+1]
Figure 17.10
Example meta-network for a model with shared parameters, corresponding to exam-
ple 17.11.
I[M +1], D2[M +1]); if D1[M +1] = D2[M +1] then the two parameters are from the same
multinomial distribution. Thus, the posterior over these two parameters is not independent, and
we cannot write their expectation as a product of expectations.
Another way of understanding this problem is by examining the meta-network for learning
in such a situation.
The meta-network for Bayesian parameter learning for the network of
example 17.11 is shown in ﬁgure 17.10.
As we can see, in this network G1[M + 1] is not
independent of G2[M + 1] given I[M + 1] because of the trail through the shared parameters.
Stated diﬀerently, observing G1[M + 1] will cause us to update the parameters and therefore
change our estimate of the probability of G2[M + 1].
Note that this problem can happen only in particular forms of shared parameters. If the
shared parameters are within the same CPD, as in example 17.15, then the (M + 1)st instance
can involve at most one parameter from each partitions of shared parameters.
In such a
situation, the problem does not arise and we can use the average of the parameters to compute
the probability of the next instance. However, if we have shared parameters across diﬀerent
CPDs (that is, entries in two or more CPDs share parameters), this problem can occur.
How do we solve this problem?
The correct Bayesian prediction solution is to compute
the average for the product of two (or more) parameters from the same posterior.
This is
essentially identical to the question of computing the probability of two or more test instances.
See exercise 17.18. This solution, however, leads to many complications if we want to use the
Bayesian posterior to answer queries about the distribution of the next instance. In particular,
this probability no longer factorizes in the form of the original Bayesian network, and thus, we
cannot use standard inference procedures to answer queries about future instances. For this
reason, a pragmatic approximation is to use the expected parameters for each CPD and ignore the
dependencies induced by the shared parameters. When the number of training samples is large,
this solution can be quite a good approximation to the true predictive distribution. However,
when the number of training examples is small, this assumption can skew the estimate; see
exercise 17.18.
17.5.4
Hierarchical Priors ⋆
In our discussion of Bayesian methods for table-CPDs, we made the strong independence as-
sumption (global and local parameter independence) to decouple the estimation of parameters.

764
Chapter 17. Parameter Estimation
Our discussion of shared parameters relaxed these assumptions by moving to the other end of
the spectrum and forcing parameters to be identical. There are many situations where neither
solution is appropriate.
Example 17.17
Using our favorite university domain, suppose we learn a model from records of students, classes,
teachers, and so on. Now suppose that we have data from several universities. Because of various
factors, the data from each university have diﬀerent properties. These diﬀerences can be due to
the diﬀerent population of students in each one (for example, one has more engineering oriented
students while the other has more liberal arts students), somewhat diﬀerent grade scale, or other
factors. This leads to a dilemma. We can learn one model over all the data, ignoring the university
speciﬁc bias. This allows us to pool the data to get a larger and more reliable data set. Alternatively,
we can learn a diﬀerent model for each university, or, equivalently, add a University variable that is
the parent of many of the variables in the network. This approach allows us to tailor the parameters
to each university. However, this ﬂexibility comes at a price — learning parameters in one university
does not help us learn better parameters from the data in the other university. This partitions the
data into smaller sets, and in each one we need to learn from scratch that intelligent students tend
to get an A in easy classes.
Example 17.18
A similar problem might arise when we consider learning dependencies in text domains. As we
discussed in box 6.B, a standard model for word sequences is a bigram model, which views the
bigram model
words as forming a Markov chain, where we have a conditional probability over the next word
given the current word. That is, we want to learn P(W (t+1) | W (t)) where W is a random
variable taking values from the dictionary of words. Here again, the context W (t) can deﬁnitely
change our distribution over W (t+1), but we still want to share some information across these
diﬀerent conditional distributions; for example, the probability of common words, such as “the,”
should be high in almost all of the conditional distributions we learn.
In both of these examples, we aim to learn a conditional distribution, say P(Y | X). Moreover,
we want the diﬀerent conditional distributions P(Y | x) to be similar to each other, yet not
identical. Thus, the Bayesian learning problem assuming local independence (ﬁgure 17.11a) is not
appropriate.
One way to bias the diﬀerent distributions to be similar to each other is to have the same
prior over them. If the prior is very strong, it will bias the diﬀerent estimates to the same values.
In particular, in the domain of example 17.18, we want the prior to bias both distributions toward
giving high probability to frequent words. Where do we get such a prior? One simple ad hoc
solution is to use the data to set the prior. For example, we can use the frequency of words in
training set to construct a prior where more frequent words have a larger hyperparameter. Such
an approach ensures that more frequent words have higher posterior in each of the conditional
distributions, even if there are few training examples for that particular conditional distribution.
This approach (which is a slightly more Bayesian version of the shrinkage of exercise 6.2) works
shrinkage
fairly well, and is often used in practice. However, it seems to contradict the whole premise of
the Bayesian framework: a prior is a distribution that we formulate over our parameters prior
to seeing the data. This approach also leaves unaddressed some important questions, such as
determining the relative strength of the prior based on the amount of data used to compute
it. A more coherent and general approach is to stay within the Bayesian framework, and to

17.5. Learning Models with Shared Parameters
765
Data m
X
Y
Z
(c)
qY |x1
qY |x0
qZ |x1
a |x1
a |x0
qZ |x0
X
Y
Data m
(a)
(b)
qY |x0
qY|x1
X
Y
Data m
qY |x0
qY |x1
aY |X
Figure 17.11
Independent and hierarchical priors. (a) A plate model for P(Y | X) under assumption
of parameter independence. (b) A plate model for a simple hierarchical prior for the same CPD. (c) A plate
model for two CPDs P(Y | X) and P(Z | X) that respond similarly to X.
introduce explicitly our uncertainty about the joint prior as part of the model.
Just as we
introduced random variables to denote parameters of CPDS, we now take one step further and
introduce a random variable to denote the hyperparameters. The resulting model is called a
hierarchical Bayesian model. It uses a factored probabilistic model to describe our prior. This
hierarchical Bayes
idea, of speciﬁcally deﬁning a probabilistic model over our priors, can be used to deﬁne rich
priors in a broad range of settings. Exercise 17.7 provides another example.
Figure 17.11b shows a simple example, where we have a variable that is the parent of both
θY |x0 and θY |x1. As a result, these two parameters are no longer independent in the prior, and
consequently in the posterior. Intuitively, the eﬀect of the prior will be to shift both θY |x0 and
θY |x1 to be closer to each other. However, as usual when using priors, the eﬀect of the prior
diminishes as we get more data. In particular, as we have more data for the diﬀerent contexts
x1 and x0, the eﬀect of the prior will gradually decrease. Thus, the hierarchical priors (as for
all priors) are particularly useful in the sparse-data regime.
How do we represent the distribution over the hyperparameter P(⃗α)? One option is to create
a prior where each component αy is governed by some distribution, say a Gamma distribution
(recall that components are strictly positive). That is,
P(⃗α) =
Y
y
P(αy),
where P(αy) ∼Gamma(µy) is a Gamma distribution with (hyper-)hyperparameter µy. The
Gamma
distribution
other option is to write ⃗α as the product of equivalent sample size N0 with a probability distri-
bution p0, the ﬁrst governed by a Gamma distribution and the other by a Dirichlet distribution.
(These two representations are actually closely related; see box 19.E.)
Moreover, the same general idea can be used in broader ways. In this example, we used
a hierarchical prior to relate two (or more) conditional distributions in the same CPD. We
can similarly relax the global parameter independence assumption and introduce dependencies
between the parameters for two (or more) CPDs. For example, if we believe that two variables

766
Chapter 17. Parameter Estimation
Y and Z depend on X in a similar (but not identical) way, then we introduce a common prior
on θY |x0 and θZ|x0, and similarly another common prior for θY |x1 and θZ|x1; see ﬁgure 17.11c.
The idea of a hierarchical structure can also be extended to additional levels of a hierarchy.
For example, in the case of similar CPDs, we might argue that there is a similarity between
the distributions of Y and Z given x0 and x1. If we believe that this similarity is weaker
than the similarity between the distributions of the two variables, we can introduce another
hierarchy layer to relate the hyperparameters α·|x0 and α·|x1. To do so, we might introduce
hyper-hyperparameters µ that specify a joint prior over α·|x0 and α·|x1.
The notion of hierarchical priors can be readily applied to other types of CPDs. For example,
if X is a Gaussian variable without parents, then, as we saw in exercise 17.8, a conjugate prior
for the mean of X is simply a Gaussian prior on the mean parameter. This observation suggests
that we can easily create a hierarchical prior that relates X and another Gaussian variable
Y , by having a common Gaussian over the means of the variables.
Since the distribution
over the hyperparameter is a Gaussian, we can easily extend the hierarchy upward. Indeed,
hierarchical priors over Gaussians are often used to model the dependencies of parameters of
related populations. For example, we might have a Gaussian over the SAT score, where one level
of the hierarchy corresponds to diﬀerent classes in the same school, the next one to diﬀerent
schools in the same district, the following to diﬀerent districts in the same state, and so on.

The framework of hierarchical priors gives us a ﬂexible language to introduce depen-
dencies in the priors over parameters. Such dependencies are particularly useful when
we have small amount of examples relevant to each parameter but many such parameters
that we believe are reasonably similar. In such situations, hierarchical priors “spread”
the eﬀect of the observations between parameters with shared hyperparameters.
One question we did not discuss is how to perform learning with hierarchical priors. As
with previous discussions of Bayesian learning, we want to compute expectations with respect
to the posterior distribution. Since we relaxed the global and local independence assumptions,
the posterior distribution no longer decomposes into a product of independent terms. From
the perspective of the desired behavior, this lack of independence is precisely what we wanted
to achieve. However, it implies that we need to deal with a much harder computational task.
In fact, when introducing the hyperparameters as a variable into the model, we transformed
our learning problem into one that includes a hidden variable.
To address this setting, we
therefore need to apply methods for Bayesian learning with hidden variables; see section 19.3
for a discussion of such methods.
Box 17.E — Concept: Bag-of-Word Models for Text Classiﬁcation. Consider the problem of
text classiﬁcation: classifying a document into one of several categories (or classes). Somewhat
text classiﬁcation
surprisingly, some of the most successful techniques for this problem are based on viewing the
document as an unordered bag of words, and representing the distribution of this bag in diﬀerent
bag of words
categories. Most simply, the distribution is encoded using a naive Bayes model. Even in this simple
approach, however, there turn out to be design choices that can make important diﬀerences to the
performance of the model.
Our ﬁrst task is to represent a document by a set of random variables. This involves various
processing steps. The ﬁrst removes various characters such as punctuation marks as well as words
that are viewed as having no content (such as “the,” “and,” and so on). In addition, most applica-

17.5. Learning Models with Shared Parameters
767
tions use a variety of techniques to map words in the document to canonical words in a predeﬁned
dictionary D (for example, replace “apples,” “used,” and “running” with “apple,” “use,” and “run”
respectively). Once we ﬁnish this processing step, there are two common approaches to deﬁning the
features that describe the document.
In the Bernoulli naive Bayes model, we deﬁne a binary attribute (feature) Xi to denote whether
Bernoulli naive
Bayes
the i’th dictionary word wi appears in the document. This representation assumes that we care only
about the presence of a word, not about how many times it appeared. Moreover, when applying
the naive Bayes classiﬁer with this representation we assume that the appearance of one word in
the document is independent of the appearance of another (given the document’s topic). When
learning a naive Bayes classiﬁer with this representation, we learn frequency (over a document)
of encountering each dictionary word in documents of speciﬁc categories — for example, the
probability that the word “ball” appears in a document of category “sports.” We learn such a
parameter for each pair (dictionary word, category).
In the multinomial naive Bayes model, we deﬁne attribute to describe the speciﬁc sequence of
multinomial
naive Bayes
words in the document. The variable Xi denotes which dictionary word appeared the ith word in
the document. Thus, each Xi can take on many values, one for each possible word. Here, when we
use the naive Bayes classiﬁer, we assume that the choice of word in position i is independent of the
choice of word in position j (again, given the document’s topic). This model leads to a complication,
since the distribution over Xi is over all words in the document, which requires a large number of
parameters. Thus, we further assume that the probability that a particular word is used in position
i does not depend on i; that is, the probability that Xi = w (given the topic) is the same as the
probability that Xj = w. In other words, we use parameter sharing between P(Xi | C) and
P(Xj | C). This implies that the total number of parameters is again one for each (dictionary
word, category).
In both models, we might learn that the word “quarterback” is much more likely in documents
whose topic is “sports” than in documents whose topic is “economics,” the word “bank” is more
associated with the latter subjects, and the word “dollar” might appear in both.
Nevertheless,
the two models give rise to quite diﬀerent distributions. Most notably, if a word w appears in
several diﬀerent positions in the document, in the Bernoulli model the number of occurrences will
be ignored, while in the multinomial model we will multiply the probability P(w | C) several
times. If this probability is very small in one category, the overall probability of the document given
that category will decrease to reﬂect the number of occurrences. Another diﬀerence is in how the
document length plays a role here. In the Bernoulli model, each document is described by exactly
the same number of variables, while the multinomial model documents of diﬀerent lengths are
associated with a diﬀerent number of random variables.
The plate model provides a compact and elegant way of making explicit the subtle distinctions
between these two models. In both models, we have two diﬀerent types of objects — documents,
and individual words in the documents. Document objects d are associated with the attribute T,
representing the document topic. However, the notion of “word objects” is diﬀerent in the diﬀerent
models.
In the Bernoulli naive Bayes model, our words correspond to words in some dictionary (for
example, “cat,” “computer,” and so on). We then have a binary-valued attribute A(d, w) for each
document d and dictionary word w, which takes the value true if the word w appears in the
document d. We can model this case using a pair of intersecting plates, one for documents and the
other for dictionary words, as shown in ﬁgure 17.E.1a.

768
Chapter 17. Parameter Estimation
(a)
(c)
(b)
Documents d
Words w
q
b
Appear
Topic
Documents d
Positions p
q
b
W
Topic
Documents d
Positions p
q
a
b
W
Topic
Figure 17.E.1 — Diﬀerent plate models for text (a) Bernoulli naive Bayes; (b) Multinomial naive Bayes.
(c) Latent Dirichlet Allocation.
In the multinomial naive Bayes model, our word objects correspond not to dictionary words, but
to word positions P within the document. Thus, we have an attribute W of records representing
pairs (D, P), where D is a document and P is a position within it (that is, ﬁrst word, second word,
and so on). This attribute takes on values in the space of dictionary words, so that W(d, p) is the
random variable whose value is the actual dictionary word in position p in document d. However,
all of these random variables are generated from the same multinomial distribution, which depends
on the document topic. The appropriate plate model is shown in ﬁgure 17.E.1b.1
The plate representation of the two models makes explicit the fact that the Bernoulli parameter
βW [w] in the Bernoulli model is diﬀerent for diﬀerent words, whereas in the multinomial model,
the parameter βW is the same for all positions within the document. Empirical evidence suggests
that the multinomial model is, in general, more successful than the Bernoulli.
In both models, the parameters are estimated from data, and the resulting model used for classi-
fying new documents. The parameters for these models measure the probability of a word given a
topic, for example, the probability of “bank” given “economics.” For common words, such probabil-
ities can be assessed reasonably well even from a small number of training documents. However,
as the number of possible words is enormous, Bayesian parameter estimation is used to avoid over-
ﬁtting, especially ascribing probability zero to words that do not appear in the training set. With
Bayesian estimation, we can learn a naive Bayes model for text from a fairly small corpus, whereas
1. Note that, as deﬁned in section 6.4.1.2, a skeleton for a plate model speciﬁes a ﬁxed set of objects for each class. In
the multinomial plate model, this assumption implies that we specify a ﬁxed set of word positions, which applies to all
documents. In practice, however, documents have diﬀerent lengths, and so we would want to allow a diﬀerent set of
word positions for each document. Thus, we want the set Oκ[p] to depend on the speciﬁc document d. When plates
are nested, as they are in this case, we can generalize our notion of skeleton, allowing the set of objects in a nested
plate Q1 to depend on the index of an enclosing plate Q2.

17.6. Generalization Analysis ⋆
769
more realistic models of language are generally much harder to estimate correctly. This ability to
deﬁne reasonable models with a very small number of parameters, which can be acquired from a
small amount of training data, is one of the key advantages of the naive Bayes model.
We can also deﬁne much richer representations that capture more ﬁne-grained structure in
the distribution. These models are often easily viewed in the plate representation. One such model,
shown in ﬁgure 17.E.1c, is the latent Dirichlet allocation (LDA) model, which extends the multinomial
latent Dirichlet
allocation
naive Bayes model. As in the multinomial naive Bayes model, we have a set of topics associated
with a set of multinomial distributions θW over words. However, in the LDA model, we do not
assume that an entire document is about a single topic. Rather, we assume that each document d
is associated with a continuous mixture of topics, deﬁned using parameters θ(d). These parameters
are selected independently from each document d, from a Dirichlet distribution parameterized by
a set of hyperparameters α. The word in position p in the document d is then selected by ﬁrst
selecting a topic Topic(d, p) = t from the mixture θ(d), and then selecting a speciﬁc dictionary
word from the multinomial βt associated with the chosen topic t. The LDA model generally provides
much better results (in measures related to log-likelihood of test data) than other unsupervised
clustering approaches to text. In particular, owing to the ﬂexibility of assigning a mixture of topics
to a document, there is no problem with words that have low probability relative to a particular
topic; thus, this approach largely avoids the overﬁtting problems with the two naive Bayes models
described earlier.
17.6
Generalization Analysis ⋆
One intuition that permeates our discussion is that more training instances give rise to more
accurate parameter estimates.
This intuition is supported by our empirical results.
In this
section, we provide some formal analysis that supports this intuition. This analysis also allows
us to quantify the extent to which the error in our estimates decreases as a function of the
number of training samples, and increases as a function of the number of parameters we want
to learn or the number of variables in our networks.
We begin with studying the asymptotic behavior of our estimator at the large-sample limit.
We then provide a more reﬁned analysis that studies the error as a function of the number of
samples.
17.6.1
Asymptotic Analysis
We start by considering the asymptotic behavior of the maximum likelihood estimator. In this
case, our analysis of section 17.2.5 provides an immediate conclusion: At the large sample limit,
ˆPD approaches P ∗; thus, as the number of samples grows, ˆθ approaches θ∗— the projection
of P ∗onto the parametric family.
A particular case of interest arises when P ∗(X) = P(X : θ∗), that is, P ∗is representable in
the parametric family. Then, we have that ˆθ →θ∗as M →∞. An estimator with this property
is called consistent estimator. In general, maximum likelihood estimators are consistent.
consistent
estimator

770
Chapter 17. Parameter Estimation
We can make this analysis even more precise. Using equation (17.9), we can write
log P(D | ˆθ)
P(D | θ) = M

ID( ˆPD||Pθ) −ID( ˆPD||Pˆθ)

.
This equality implies that the likelihood function is sharply peaked: the decrease in the likelihood
for parameters that are not the MLE is exponential in M. Of course, when we change M the
data set D and hence the distribution ˆPD also change, and thus this result does not guarantee
exponential decay in M. However, for suﬃciently large M, ˆPD →P ∗. Thus, the diﬀerence in
log-likelihood of diﬀerent choices of θ is roughly M times their distance from P ∗:
log P(D | ˆθ)
P(D | θ) ≈M
 ID(P ∗||Pθ) −ID(P ∗||Pˆθ)

.
The terms on the right depend only on M and not on ˆPD. Thus, we conclude that for large
values of M, the likelihood function approaches a delta function, for which all values are
virtually 0 when compared to the maximal value at θ∗, the M-projection of P ∗.
To summarize, this argument basically allows us to prove the following result, asserting that
the Bayesian estimator is consistent:
Theorem 17.2
Let P ∗be the generating distribution, let P(· | θ) be a parametric family of distributions, and let
θ∗= arg minθ ID(P ∗||P(· | θ)) be the M-projection of P ∗on this family. Then
lim
M→∞P(· | ˆθ) = P(· | θ∗)
almost surely.
That is, when M grows larger, the estimated parameters describe a distribution that is close to
the distribution with the “optimal” parameters in our parametric family.
Is our Bayesian estimator consistent?
Recall that equation (17.11) shows that the Bayesian
estimate with a Dirichlet prior is an interpolation between the MLE and the prior prediction.
The interpolation weight depends on the number of samples M: as M grows, the weight of the
prior prediction diminishes and disappears in the limit. Thus, we can conclude that Bayesian
learning with Dirichlet priors is also consistent.
17.6.2
PAC-Bounds
This consistency result guarantees that, at the large sample limit, our estimate converges to the
true distribution. Though a satisfying result, its practical signiﬁcance is limited, since in most
cases we do not have access to an unlimited number of samples. Hence, it is also important to
evaluate the quality of our learned model as a function of the number of samples M. This type
of analysis allows us to know how much to trust our learned model as a function of M; or,
from the other side, how many samples we need to acquire in order to obtain results of a given
quality. Thus, using relative entropy to the true distribution as our notion of solution quality,
we use PAC-bound analysis (as in box 16.B) to bound ID(P ∗|| ˆP) as a function of the number of
PAC-bound
data samples M.

17.6. Generalization Analysis ⋆
771
17.6.2.1
Estimating a Multinomial
We start with the simplest case, which forms the basis for our more extensive analysis. Here,
our task is to estimate the multinomial parameters governing the distribution of a single random
variable. This task is relevant to many disciplines and has been studied extensively. A basic tool
used in this analysis are the convergence bounds described in appendix A.2.
convergence
bound
Consider a data set D deﬁned as a set of M IID Bernoulli random variables D = {X[1],
. . . , X[M]}, where P ∗(X[m] = x1) = p∗for all m. Note that we are now considering D
itself to be a stochastic event (a random variable), sampled from the distribution (P ∗)M. Let
ˆpD =
1
M
P
m X[m]. Then an immediate consequence of the Hoeﬀding bound (theorem A.3) is
Hoeﬀding bound
PM(|ˆpD −p∗| > ϵ) ≤2e−2Mϵ2,
where PM is a shorthand for PD∼(P ∗)M .
Thus, the probability that the MLE ˆpD deviates
from the true parameter by more than ϵ is bounded from above by a function that decays
exponentially in M.
As an immediate corollary, we can prove a PAC-bound on estimating p∗:
Corollary 17.1
Let ϵ, δ > 0, and let
M >
1
2ϵ2 log 2
δ .
Then PM(|ˆp −p∗| ≤ϵ) ≥1 −δ, where PM, as before, is a probability over data sets D of size M
sampled IID from P ∗.
Proof
PM(|ˆp −p∗| ≤ϵ)
=
1 −PM(ˆp −p∗> ϵ) −PM(ˆp −p∗< −ϵ)
≥
1 −2e−2Mϵ2 ≥1 −δ.
The number of data instances M required to obtain a PAC-bound grows quadratically in the error
1/ϵ, and logarithmically in the conﬁdence 1/δ. For example, setting ϵ = 0.05 and δ = 0.01, we
get that we need M ≥1059.66. That is, we need a bit more than 1, 000 samples to conﬁdently
estimate the probability of an event to within 5 percent error.
This result allows us to bound the absolute value of the error between the parameters. We,
however, are interested in the relative entropy between the two distributions. Thus, we want to
relative entropy
bound terms of the form log p∗
ˆp .
Lemma 17.1
For PM deﬁned as before:
PM(log p∗
ˆp > ϵ) ≤e
−2Mp2ϵ2
1
(1+ϵ)2 .
Proof The proof relies on the following fact:
If ϵ ≤x ≤y ≤1 then
(log y −log x) ≤1
ϵ (y −x).
(17.22)

772
Chapter 17. Parameter Estimation
Now, consider some ϵ′. If p∗−ˆp ≤ϵ′ then ˆp > p∗−ϵ′. Applying equation (17.22), we get that
log p∗
ˆp ≤
ϵ′
p∗−ϵ′ . Setting ϵ′ = ϵp∗
1+ϵ, and taking the contrapositive, we conclude that, if log p∗
ˆp > ϵ
then p∗−ˆp > ϵp∗
1+ϵ. Using the Hoeﬀding bound to bound the probability of the latter event, we
derive the desired result.
This analysis applies to a binary-valued random variable. We now extend it to the case of
a multivalued random variable. The result provides a bound on the relative entropy between
P ∗(X) and the maximum likelihood estimate for P(X), which is simply its empirical probability
ˆPD(X).
Proposition 17.7
Let P ∗(X) be a discrete distribution such that P ∗(x) ≥λ for all x ∈Val(X).
Let D =
{X[1], . . . , X[M]} consist of M IID samples of X. Then
PM(ID(P ∗(X)|| ˆPD(X)) > ϵ) ≤|Val(X)|e
−2Mλ2ϵ2
1
(1+ϵ)2 .
Proof We want to bound the error
ID(P ∗(X)|| ˆPD(X)) =
X
x
P ∗(x) log P ∗(x)
ˆPD(x)
.
This expression is a weighted average of log-ratios of the type we bounded in lemma 17.1. If we
bound each of the terms in this average by ϵ, we can obtain a bound for the weighted average
as a whole. That is, we say that a data set is well behaved if, for each x, log P ∗(x)
ˆ
PD(x) ≤ϵ. If the
data set is well behaved, we have a bound of ϵ on the term for each x, and therefore an overall
bound of ϵ for the entire relative entropy.
With what probability is our data set not well behaved? It suﬃces that there is one x for
which log P ∗(x)
ˆ
PD(x) > ϵ. We can provide an upper bound on this probability using the union bound,
which bounds the probability of the union of a set of events as the sum of the probabilities of
the individual events:
P
 
∃x, log P ∗(x)
ˆPD(x)
> ϵ
!
≤
X
x
P
 
log P ∗(x)
ˆPD(x)
> ϵ
!
.
The union bound is an overestimate of the probability, since it essentially represents the case
where the diﬀerent “bad” events are disjoint. However, our focus is on the situation where these
events are unlikely, and the error due to such overcounting is not signiﬁcant.
Formalizing this argument, we obtain:
PM(ID(P ∗(X)|| ˆPD(X)) > ϵ)
≤
PM
 
∃x : log P ∗(x)
ˆPD(x)
> ϵ
!
≤
X
x
PM
 
log P ∗(x)
ˆPD(x)
> ϵ
!
≤
X
x
e
−2MP ∗(x)2ϵ2
1
(1+ϵ)2
≤
|Val(X)|e
−2Mλ2ϵ2
1
(1+ϵ)2 ,

17.6. Generalization Analysis ⋆
773
where the second inequality is derived from the union bound, the third inequality from
lemma 17.1, and the ﬁnal inequality from the assumption that P ∗(x) ≥λ.
This result provides us with an error bound for estimating the distribution of a random
variable. We can now easily translate this result to a PAC-bound:
Corollary 17.2
Assume that P ∗(x) ≥λ for all x ∈Val(X). For ϵ, δ > 0, let
M ≥1
2
1
λ2
(1 + ϵ)2
ϵ2
log |Val(X)|
δ
.
Then PM(ID(P ∗(X)|| ˆPD(X)) ≤ϵ) ≥1 −δ.
As with the binary-valued case, the number of samples grows quadratically with
1
ϵ and
logarithmically with 1
δ. Here, however, we also have a quadratic dependency on 1
λ. The value λ
is a measure of the “skewness” of the distribution P ∗. This dependence is not that surprising;
we expect that, if some values of X have small probability, then we need many more samples
to get a good approximation of their probability. Moreover, underestimates of ˆPD(x) for such
events can lead to a big error in estimating log P ∗(x)
ˆ
PD(x). Intuitively, we might suspect that when
P ∗(x) is small, it is harder to estimate, but at the same time it also less crucial for the total
error. Although it is possible to use this intuition to get a better estimation (see exercise 17.19),
the asymptotic dependence of M on 1
λ remains quadratic.
17.6.2.2
Estimating a Bayesian Network
We now consider the problem of learning a Bayesian network. Suppose that P ∗is consistent
with a Bayesian network G and that we learned parameters θ for G that deﬁne a distribution
Pθ. Using theorem 8.5, we have that
ID(P ∗||Pθ) =
X
i
ID(P ∗(Xi | PaG
i )||Pθ(Xi | PaG
i )),
where (as shown in appendix A.1.3.2), we have that
ID(P ∗(X | Y )||P(X | Y )) =
X
y
P ∗(y)ID(P ∗(X | y)||P(X | y)).
Thus, as we might expect, the error is a sum of errors in estimating the conditional probabilities.
This error term, however, makes the strong assumption that our generating distribution P ∗
is consistent with our target class — those distributions representable using the graph G. This
assumption is usually not true in practice. When this assumption is false, the given network
structure limits the ability of the learning procedure to generalize.
For example, if we give
a learning procedure a graph where X and Y are independent, then no matter how good
our learning procedure, we cannot achieve low generalization error if X and Y are strongly
dependent in P ∗.
More broadly, if the given network structure is inaccurate, then there is
inherent error in the learned distribution that the learning procedure cannot overcome.
One approach to deal with this problem is to assume away the cases where P ∗does not
conform with the given structure G. This solution, however, makes the analysis brittle and of

774
Chapter 17. Parameter Estimation
little relevance to real-life scenarios. An alternative solution is to relax our expectations from
the learning procedure. Instead of aiming for the error to become very small, we might aim to
show that the error is not far away from the inherent error that our procedure must incur due to
the limitations in expressive power of the given network structure. In other words, rather than
bounding the risk, we provide a bound on the excess risk (see box 16.B).
excess risk
More formally, let Θ[G] be the set of all possible parameterizations for G. We now deﬁne
θopt = arg min
θ∈Θ[G] ID(P ∗||Pθ).
That is, θopt is the best result we might expect the learning procedure to return. (Using the
terminology of section 8.5, θopt is the M-projection of P ∗on the family of distributions deﬁned
M-projection
by G, Θ[G].)
The distance ID(P ∗||Pθopt) reﬂects the minimal error we might achieve with networks with
the structure G. Thus, instead of deﬁning “success” for our learning procedure in terms of
obtaining low values for ID(P ∗||Pθ) (a goal which may not be achievable), we aim to obtain low
values for ID(P ∗||Pθ) −ID(P ∗||Pθopt).
What is the form of this error term? By solving for Pθopt and using basic manipulations we
can deﬁne it in precise terms.
Theorem 17.3
Let G be a network structure, let P ∗be a distribution, and let Pθ = (G, θ) be a distribution
consistent with G. Then:
ID(P ∗||Pθ) −ID(P ∗||Pθopt) =
X
i
ID(P ∗(Xi | PaG
i )||Pθ(Xi | PaG
i )).
The proof is left as an exercise (exercise 17.20).
This theorem shows that the error in our learned network decomposes into two compo-

nents. The ﬁrst is the error inherent in G, and the second the error due to inaccuracies
in estimating the conditional probabilities for parameterizing G. This theorem also shows
that, in terms of error analysis, the treatment of the general case leads to exactly the same terms
that we had to bound when we made the assumption that P ∗was consistent with G. Thus, in
this learning task, the analysis of the inconsistent case is not more diﬃcult than the analysis of
the consistent case. As we will see in later chapters, this situation is not usually the case: we
can often provide bounds for the consistent case, but not for the inconsistent case.
To continue the analysis, we need to bound the error in estimating conditional probabilities.
The preceding treatment showed that we can bound the error in estimating marginal probabilities
of a variable or a group of variables. How diﬀerent is the estimate of conditional probabilities
from that of marginal probabilities? It turns out that the two are easily related.
Lemma 17.2
Let P and Q be two distributions on X and Y . Then
ID(P(X | Y )||Q(X | Y )) ≤ID(P(X, Y )||Q(X, Y )).
See exercise 17.21.
As an immediate corollary, we have that
ID(P ∗||P) −ID(P ∗||Pθopt) ≤
X
i
ID(P ∗(Xi, PaG
i )||P(Xi, PaG
i )).

17.6. Generalization Analysis ⋆
775
Thus, we can use proposition 17.7 to bound the error in estimating P(Xi, PaG
i ) for each Xi
(where we treat Xi, PaG
i as a single variable) and derive a bound on the error in estimating the
probability of the whole network.
Theorem 17.4
Let G be a network structure, and P ∗a distribution consistent with some network G∗such that
P ∗(xi | paG∗
i ) ≥λ for all i, xi, and paG∗
i . If P is the distribution learned by maximum likelihood
estimate for G, then
P (ID(P ∗||P) −ID(P ∗||Pθopt) > nϵ) ≤nKd+1e
−2Mλ2(d+1)ϵ2
1
(1+ϵ)2 ,
where K is the maximal variable cardinality and d is the maximum number of parents in G.
Proof The proof uses the union bound
P (ID(P ∗||P) −ID(P ∗||Pθopt) > nϵ) ≤
X
i
P
 ID(P ∗(Xi, PaG
i )||P(Xi, PaG
i )) > ϵ

with application of proposition 17.7 to bound the probability of each of these latter events. The
only technical detail we need to consider is to show that if conditional probabilities in P ∗are
always larger than λ, then P ∗(xi, paG
i ) ≥λ|PaG
i |+1; see exercise 17.22.
This theorem shows that we can indeed learn parameters that converge to the optimal ones
as the number of samples grows. As with previous bounds, the number of samples we need is
Corollary 17.3
Under the conditions of theorem 17.4, if
M ≥1
2
1
λ2(d+1)
(1 + ϵ)2
ϵ2
log nKd+1
δ
,
then
P (ID(P ∗||P) −ID(P ∗||Pθopt) < nϵ) > 1 −δ.
As before, the number of required samples grows quadratically in 1
ϵ . Conversely, we expect the
error to decrease roughly with
1
√
M , which is commensurate with the behavior we observe in
practice (for example, see ﬁgure 17.C.2 in box 17.C). We see also that λ and d play a major role
in determining M. In practice, we often do not know λ in advance, but such analysis allows us
to provides guarantees under the assumption that conditional probabilities are not too small. It
also allows us to predict the improvement in error (or at least in our upper bound on it) that we
would obtain if we add more samples.
Note that in this analysis we “allow” the error to grow with n as we consider ID(P ∗||P) > nϵ.
The argument is that, as we add more variables, we expect to incur some prediction error on
each one.
Example 17.19
Consider the network where we have n independent binary-valued variables X1, . . . , Xn. In this
case, we have n independent Bernoulli estimation problems, and would expect a small number of
samples to suﬃce. Indeed, we can obtain an ϵ-close estimate to each of them (with high-probability)
using the bound of lemma 17.1. However, the overall relative entropy between P ∗and ˆPD over
the joint space X1, . . . , Xn will grow as the sum of the relative entropies between the individual
marginal distributions P ∗(Xi) and ˆPD(Xi). Thus, even if we perform well at predicting each
variable, the total error will scale linearly with n.

776
Chapter 17. Parameter Estimation
Thus, our formulation of the bound in corollary 17.3 is designed to separate out this “inevitable”
linear growth in the error from any additional errors that arise from the increase in dimension-
ality of the distribution to be estimated.
We provided a theoretical analysis for the generalization error of maximum likelihood estimate.
A natural question is to carry similar analysis when we use Bayesian estimates. Intuitively, the
asymptotic behavior (for M →∞) will be similar, since the two estimates are asymptotically
identical. For small values of M we do expect to see diﬀerences, since the Bayesian estimate
is smoother and cannot be arbitrarily small and thus the relative entropy is bounded.
See
exercise 17.23 for an analysis of the Bayesian estimate.
17.7
Summary
In this chapter we examined parameter estimation for Bayesian networks when the data are
complete. This is the simplest learning task for Bayesian networks, and it provides the basis
for the more challenging learning problems we examine in the next chapters. We discussed
two approaches for estimation: MLE and Bayesian prediction. Our primary emphasis here was
on table-CPDs, although the ideas generalize to other representations. We touched on a few of
these.
As we saw, a central concept in both approaches is the likelihood function, which captures
how the probability of the data depends on the choice of parameters. A key property of the
likelihood function for Bayesian networks is that it decomposes as a product of local likelihood
functions for the individual variables. If we use table-CPDs, the likelihood decomposes even
further, as a product of the likelihoods for the individual multinomial distributions P(X | paXi).
This decomposition plays a central role in both maximum likelihood and Bayesian estimation,
since it allows us to decompose the estimation problem and treat each of these CPDs or even
each of the individual multinomials separately.
When the local likelihood has suﬃcient statistics, then learning is viewed as mapping values
of the statistics to parameters. For networks with discrete variables, these statistics are counts
of the form M[xi, paXi].
Thus, learning requires us to collect these for each combination
xi, paXi of a value of Xi and an instantiation of values to its parents. We can collect all of
these counts in one pass through the data using a data structure whose size is proportional to
the representation of the network, since we need one counter for each CPD entry.
Once we collect the suﬃcient statistics, the estimate of both methods is similar. The MLE
estimate for table-CPDs has a simple closed form:
ˆθxi|paXi = M[xi, paXi]
M[paXi] .
The Bayesian estimate is based on the use of a Dirichlet distribution, which is a conjugate prior
to the multinomial distribution. In a conjugate prior, the posterior — which is proportional to
the prior times the likelihood — has the same form as the prior. This property allows us to
maintain posteriors in closed form. In particular, for a discrete Bayesian network with table-CPDs
and a Dirichlet prior, the posterior of the local likelihood has the form
P(xi | paXi, D) =
M[xi, paXi] + αxi|paXi
M[paXi] + αpaXi
.

17.8. Relevant Literature
777
Since all we need in order to learn are the suﬃcient statistics, then we can easily adapt them
to learn in an online setting, where additional training examples arrive. We simply store a vector
of suﬃcient statistics, and update it as new instances are obtained.
In the more advanced sections, we saw that the same type of structure applies to other
parameterizations in the exponential family. Each family deﬁnes the suﬃcient statistics we need
to accumulate and the rules for ﬁnding the MLE parameters. We developed these rules for
Gaussian CPDs, where again learning can be done using a closed-form analytical formula.
We also discussed networks where some of the parameters are shared, whether between CPDs
or within a single CPD. We saw that the same properties described earlier — decomposition
and suﬃcient statistics — allow us to provide an easy analysis for this setting. The likelihood
function is now deﬁned in terms of suﬃcient statistics that aggregate statistics from diﬀerent
parts of the network.
Once the suﬃcient statistics are deﬁned, the estimation procedures,
whether MLE or Bayesian, are exactly the same as in the case without shared parameters.
Finally, we examined the theoretical foundations of learning. We saw that parameter estimates
are asymptotically correct in the following sense. If the data are actually generated from the
given network structure, then, as the number of samples increases, both methods converge to the
correct parameter setting. If not, then they converge to the distribution with the given structure
that is “closest” to the distribution from which the data were generated. We further analyzed
the rate at which the estimates converge. As M grows, we see a concentration phenomenon;
concentration
phenomenon
for most samples, the empirical distribution is in a close neighborhood of the true distribution.
Thus, the chances of sampling a data set in which the MLE estimates are far from the true
parameters decays exponentially with M. This analysis allowed us to provide a PAC-bound on
the number of samples needed to obtain a distribution that is “close” to optimal.
17.8
Relevant Literature
The foundations of maximum likelihood estimation and Bayesian estimation have a long history;
see DeGroot (1989); Schervish (1995); Hastie et al. (2001); Bishop (2006); Bernardo and Smith (1994)
for some background material. The thumbtack example is adapted from Howard (1970).
Heckerman (1998) and Buntine (1994, 1996) provide excellent tutorials and reviews on the
basic principles of learning Bayesian networks from data, as well as a review of some of the key
references.
The most common early application of Bayesian network learning, and perhaps even now, is
learning a naive Bayes model for the purpose of classiﬁcation (see, for example, Duda and Hart
1973). Spiegelhalter and Lauritzen (1990) laid the foundation for the problem of learning general
Bayesian networks from data, including the introduction of the global parameter independence
assumption, which underlies the decomposability of the likelihood function. This development
led to a stream of signiﬁcant extensions, most notably by Buntine (1991); Spiegelhalter et al.
(1993); Cooper and Herskovits (1992). Heckerman et al. (1995) deﬁned the BDe prior and showed
its equivalence to a combination of assumptions about the prior.
Many papers (for example, Spiegelhalter and Lauritzen 1990; Neal 1992; Buntine 1991; Diez
1993) have proposed the use of structured CPDs as an approach for reducing the number of
parameters that one needs to learn from data.
In many cases, the speciﬁc learning algo-
rithms are derived from algorithms for learning conditional probability models for probabilistic

778
Chapter 17. Parameter Estimation
classiﬁcation. The probabilistic derivation of tree-CPDs was performed by Buntine (1993) and
introduced to Bayesian networks by Friedman and Goldszmidt (1998). The analysis of Bayesian
learning of Bayesian networks with linear Gaussian dependencies was performed by Heckerman
and Geiger (1995); Geiger and Heckerman (Geiger and Heckerman). Buntine (1994) emphasizes
the important connection between the exponential family and the task of learning Bayesian
networks. Bernardo and Smith (1994) describe conjugate priors for many distributions in the
exponential family. Some material on nonparametric density estimation can be found in Hastie
et al. (2001); Bishop (2006). Hofmann and Tresp (1995) use Parzen window to capture conditional
distributions in continuous Bayesian networks. Imoto et al. (2003) learn semiparametric spline-
regression models as CPDs in Bayesian networks. Rasmussen and Williams (2006) describe
Gaussian processes, a state-of-the-art method for nonparametric estimation, which has also been
Gaussian
processes
used for estimating CPDs in Bayesian networks (Friedman and Nachman 2000).
Plate models as a representation for parameter sharing in learning were introduced by Gilks
et al. (1994) and Buntine (1994). Hierarchical Bayesian models have a long history in statistics;
see, for example, Gelman et al. (1995).
The generalization bounds for parameter estimation in Bayesian networks were ﬁrst analyzed
by Höﬀgen (1993), and subsequently improved and reﬁned by Friedman and Yakhini (1996) and
Dasgupta (1997).
Beinlich et al. (1989) introduced the ICU-Alarm network, which has formed the benchmark for
numerous studies of Bayesian network learning.
17.9
Exercises
Exercise 17.1
Show that the estimate of equation (17.1) is the maximum likelihood estimate.
Hint: diﬀerentiate the
log-likelihood with respect to θ.
Exercise 17.2⋆
Derive the MLE for the multinomial distribution (example 17.5). Hint, maximize the log-likelihood function
using a Lagrange coeﬃcient to enforce the constraint P
k θk = 1.
Exercise 17.3
Derive the MLE for Gaussian distribution (example 17.6). Solve the equations
∂log L(D : µ, σ)
∂µ
=
0
∂log L(D : µ, σ)
∂σ2
=
0.
Exercise 17.4
Derive equation (17.8) by diﬀerentiating the log-likelihood function and using equation (17.6) and equa-
tion (17.7).
Exercise 17.5⋆
In this exercise, we examine how to estimate a joint multivariate Gaussian.
Consider two continuous
variables X and Y , and assume we have a data set consisting of M samples D = {⟨x[m], y[m]⟩: m =

17.9. Exercises
779
1, . . . , M}. Show that the MLE estimate for a joint Gaussian distribution over X, Y is the Gaussian with
mean vector ⟨IED[X], IED[Y ]⟩, and covariance matrix
ΣX,Y =
 CCovD[X; X]
CCovD[X; Y ]
CCovD[X; Y ]
CCovD[Y ; Y ]

.
Exercise 17.6⋆
Derive equation (17.10) by solving the integral
R 1
0 θk(1 −θ)M−kdθ for diﬀerent values of k. (Hint: use
integration by parts.)
Exercise 17.7⋆
In this problem we consider the class of parameter priors deﬁned as a mixture of Dirichlets. These comprise
mixture of
Dirichlets
a richer class of priors than the single Dirichlet that we discussed in this chapter. A mixture of Dirichlets
represents another level of uncertainty, where we are unsure about which Dirichlet distribution is a more
appropriate prior for our domain. For example, in a simple coin-tossing situation, we might be uncertain
whether the coin whose parameter we are trying to learn is a fair coin, or whether it is a biased one. In
this case, our prior might be a mixture of two Dirichlet distributions, representing those two cases.
In this problem, our goal is to show that the family of mixture of Dirichlet priors is conjugate to the
multinomial distribution; in other words, if our prior is a mixture of Dirichlets, and our likelihood function
is multinomial, then our posterior is also a mixture of Dirichlets.
a. Consider the simple possibly biased coin setting described earlier. Assume that we use a prior that
is a mixture of two Dirichlet (Beta) distributions: P(θ) = 0.95 · Beta(5000, 5000) + 0.05 · Beta(1, 1);
the ﬁrst component represents a fair coin (for which we have seen many imaginary samples), and the
second represents a possibly-biased coin, whose parameter we know nothing about. Show that the
expected probability of heads given this prior (the probability of heads averaged over the prior) is 1/2.
Suppose that we observe the data sequence (H, H, T, H, H, H, H, H, H, H). Calculate the posterior
over θ, P(θ | D). Show that it is also a 2-component mixture of Beta distributions, by writing the
posterior in the form λ1Beta(α1
1, α1
2) + λ2Beta(α2
1, α2
2).
Provide actual numeric values for the
diﬀerent parameters λ1, λ2, α1
1, α1
2, α2
1, α2
2.
b. Now generalize your calculations from part (1) to the case of a mixture of d Dirichlet priors over a
k-valued multinomial parameters. More precisely, assume that the prior has the form
P(θ) =
d
X
i=1
λiDirichlet(αi
1, . . . , αi
k),
and prove that the posterior has the same form.
Exercise 17.8⋆
We now consider a Bayesian approach for learning the mean of a Gaussian distribution. It turns out that
in doing Bayesian inference with Gaussians, it is mathematically easier to use the precision τ =
1
σ2 rather
than the variance. Note that larger the precision, the narrower the distribution around the mean.
Suppose that we have M IID samples x[1], . . . , x[M] from X ∼N
 θ; τ −1
X

. Moreover, assume that
we know the value of τX. Thus, the unknown parameter θ is the mean. Show that if the prior P(θ) is
N
 µ; τ −1
θ

, then the posterior P(θ | D) is N
 µ′; (τ ′
θ)−1
where
τ ′
θ
=
MτX + τθ
µ′
=
MτX
τ ′
θ
IED[X] + τθ
τ ′
θ
µ0.
Hint: Start by proving P
m(x[m]−θ)2 = M(θ −IED[X])+c, where c is a constant that does not depend
on θ.

780
Chapter 17. Parameter Estimation
Exercise 17.9
We now consider making predictions with the posterior of exercise 17.8. Suppose we now want to compute
the probability
P(x[M + 1] | D) =
Z
P(x[M + 1] | θ)P(θ | D)dθ.
Show that this distribution is Gaussian. What is the mean and precision of this distribution?
Exercise 17.10⋆
We now consider the complementary case to exercise 17.8, where we know the mean of X, but do not
know the precision. Suppose that X ∼N
 µ; θ−1
, where θ is the unknown precision.
We start with a deﬁnition. We say that Y ∼Gamma (α; β) (for α, β > 0) if
P(y : α, β) =
βα
Γ(α)yαe−βy.
This distribution is called a Gamma distribution. Here, we have that IE[Y ] = α
β and VVar[Y ] =
α
β2 .
Gamma
distribution
a. Show that Gamma distributions are a conjugate family for this learning task. More precisely, show that
if P(θ) ∼Gamma (α; β), then P(θ | D) ∼Gamma (α′; β′) where
α′
=
α + 1
2M
β′
=
β + 1
2
X
m
(x[m] −µ)2.
Hint: do not worry about the normalization constant, instead focus on the terms in the posterior that
involve θ.
b. Derive the mean and variance of θ in the posterior. What can we say about beliefs given the data?
How do they diﬀer from the MLE estimate?
Exercise 17.11⋆⋆
Now consider the case where we know neither the mean nor the precision of X. We examine a family of
distributions that are conjugate in this case.
A normal-Gamma distribution over µ, τ is of the form:
normal-Gamma
distribution
P(τ)P(µ | τ)
where P(τ) is Gamma (α; β) and P(µ | τ) is N (µ0; λτ). That is, the distribution over precisions
is a Gamma distribution (as in exercise 17.10), and the distribution over the mean is a Gaussian (as in
exercise 17.8), except that the precision of this distribution depends on τ.
Show that if P(µ, τ) is normal-Gamma with parameters α, β, µ0, λ, then the posterior P(µ, τ | D) is
also a Normal-Gamma distribution.
Exercise 17.12
Suppose that a prior on a parameter vector is p(θ) ∼Dirichlet(α1, . . . , αk). What is the MAP value of
the parameters, that is, arg maxθ p(θ)?
Exercise 17.13⋆
In this exercise, we will deﬁne a general-purpose prior for models with shared parameters, along the lines
of the BDe prior of section 17.4.3.

17.9. Exercises
781
a. Following the lines of the derivation of the BDe prior, construct a parameter prior for a network with
shared parameters, using the uniform distribution P ′ as the basis.
b. Now, extend your analysis to construct a BDe-like parameter prior for a plate model, using, as the basis,
a sample size of α(Q) for each Q and a uniform distribution.
Exercise 17.14
Perform the analysis of section 17.5.1.1 in the case where the network is a Gaussian Bayesian network.
Derive the form of the likelihood function in terms of the appropriate aggregate suﬃcient statistics, and
show how the MLE is computed from these statistics.
Exercise 17.15
In section 17.5, we discussed sharing at both the global and the local level. We now consider an example
where we have both. Consider the following elaboration of our University domain: Each course has an
additional attribute Level, whose values are undergrad (l0) or grad (l1). The grading curve (distribution
for Grade) now depends on Level: The curve is the same for all undergraduate courses, and depends on
Diﬃculty and Intelligence, as before. For graduate courses, the distribution is diﬀerent for each course and,
moreover, does not depend on the student’s intelligence.
Specify the set of multinomial parameters in this model, and the partition of the multinomial distributions
that correctly captures the structure of the parameter sharing.
Exercise 17.16
a. Using the techniques and notation of section 17.5.1.2, describe the likelihood function for the DBN
model of ﬁgure 6.2.
Your answer should deﬁne the set of shared parameters, the partition of the
variables in the ground network, the aggregate suﬃcient statistics, and the MLE.
b. Now, assume that we want to use Bayesian inference for the parameter estimation in this case. Assuming
local parameter independence and a Dirichlet prior, write down the form of the prior and the posterior.
How would you use your learned model to compute the probability of a new trajectory?
Exercise 17.17
Consider the application of the global sharing techniques of section 17.5.1.2 to the task of parameter
estimation in a PRM. A key diﬀerence between PRMs and plate models is that the diﬀerent instantiations
of the same attribute in the ground network may not have the same in-degree. For instance, returning to
example 6.16, let Job(S) be an attribute such that S is a logical variable ranging over Student. Assume
that Job(S) depends on the average grade of all courses that the student has taken (where we round the
grade-point-average to produce a discrete set of values). Show how we can parameterize such a model,
and how we can aggregate statistics to learn its parameters.
Exercise 17.18
Suppose we have a single multinomial variable X with K values and we have a prior on the parameters
governing X so that θ ∼Dirichlet(α1, . . . , αK). Assume we have some data set D = {x[1], . . . , x[M]}.
a. Show how to compute
P(X[M + 1] = xi, X[M + 2] = xj | D).
(Hint: use the chain rule for probabilities.)
b. Suppose we decide to use the approximation
P(X[M + 1] = xi, X[M + 2] = xj | D) ≈P(X[M + 1] = xi, | D)P(X[M + 2] = xj | D).
That is, we ignore the dependencies between X[M + 1] and X[M + 2]. Analyze the error in this
approximation (the ratio between the approximation and the correct probability). What is the quality of
this approximation for small M? What is the asymptotic behavior of the approximation when M →∞.
(Hint: deal separately with the case where i = j and the case where i ̸= j.)

782
Chapter 17. Parameter Estimation
Exercise 17.19⋆
We now prove a variant on proposition 17.7. Show that in the setting of that proposition 17.7
P(ID(P|| ˆP) > ϵ) ≤Ke
−2Mϵ2
1
K2
2
(1+
ϵ
Kλ )2
where K = |Val(X)|.
a. Show that
P(ID(P|| ˆP) > ϵ) ≤
X
x
P(log P ∗(x)
ˆP(x)
>
ϵ
KP(x)).
b. Use this result and lemma 17.1 to prove the stated bound.
c. Show that the stated bound is tighter than the original bound of proposition 17.7. (Hint: examine the
case when λ =
1
K .)
Exercise 17.20
Prove theorem 17.3. Speciﬁcally, ﬁrst prove that Pθopt = Q
i P ∗(Xi | PaG
Xi) and then use theorem 8.5.
Exercise 17.21
Prove lemma 17.2. Hint: Show that ID(P(X, Y )||Q(X, Y )) = ID(P(X | Y )||Q(X | Y ))+ID(P(X)||Q(X)),
and then show that the inequality follows.
Exercise 17.22
Suppose P is a Bayesian network with P(xi | pai) ≥λ for all i, xi and pai. Consider a family Xi, Pai,
show that
P(xi, pai) ≥λ|Pai|+1.
Exercise 17.23⋆
We now prove a bound on the error when using Bayesian estimates. Let D = {X[1], . . . , X[M]} consist
of M IID samples of a discrete variable X.
Let α and P0 be the equivalent sample size and prior
distribution for a Dirichlet prior. The Bayesian estimator will return the distribution
˜P(x) =
M
M + α
ˆP(x) +
α
M + αP0(x).
We now want to analyze the error of such an estimate.
a. Prove the analogue of lemma 17.1. Show that
P(log P ∗(x)
˜P(x)
> ϵ) ≤e
−
2M(
M
M+α P ∗(x)+
α
M+α P0(x))2ϵ2
x
(1+ϵx)2
.
b. Use the union bound to show that if P ∗(x) ≥λ and P0(x) ≥λ0 for all x ∈Val(X), then
P(ID(P ∗(X)|| ˜P(X)) > ϵ) ≤|Val(X)|e
−2M(
M
M+α λ+
α
M+α λ0)2ϵ2
1
(1+ϵ)2 .
c. Show that
M
M + αλ +
α
M + αλ0 ≥max(λ,
α
M + αλ0).
d. Suppose that λ0 > λ. That is, our prior is less extreme than the real distribution, which is deﬁnitely
the case if we take P0 to be the uniform distribution. What can you conclude about a PAC result for
the Bayesian estimate?

18
Structure Learning in Bayesian Networks
18.1
Introduction
18.1.1
Problem Deﬁnition
In the previous chapter, we examined how to learn the parameters of Bayesian networks. We
made a strong assumption that we know in advance the network structure, or at least we decide
on one regardless of whether it is correct or not. In this chapter, we consider the task of learning
in situations where do not know the structure of the Bayesian network in advance. Throughout
this chapter, we continue with the (very) strong assumption that our data set is fully observed,
deferring the discussion of learning with partially observed data to the next chapter.
As in our discussion so far, we assume that the data D are generated IID from an underlying
distribution P ∗(X). Here, we also assume that P ∗is induced by some Bayesian network G∗
over X. We begin by considering the extent to which independencies in G∗manifest in D.
Example 18.1
Consider an experiment where we toss two standard coins X and Y independently. We are given
a data set with 100 instances of this experiment. We would like to learn a model for this scenario.
A “typical” data set may have 27 head/head, 22 head/tail, 25 tail/head, and 26 tail/tail entries. In
the empirical distribution, the two coins are not independent. This may seem reasonable, since the
probability of tossing 100 pairs of fair coins and getting exactly 25 outcomes in each category is
quite small (approximately 1/1, 000). Thus, even if the two coins are independent, we do not expect
the observed empirical distribution to satisfy independence.
Now suppose we get the same results in a very diﬀerent situation. Say we scan the sports section
of our local newspaper for 100 days and choose an article at random each day. We mark X = x1
if the word “rain” appears in the article and X = x0 otherwise. Similarly, Y denotes whether the
word “football” appears in the article. Here our intuitions as to whether the two random variables
are independent are unclear. If we get the same empirical counts as in the coins described before,
we might suspect that there is some weak connection. In other words, it is hard to be sure whether
the true underlying model has an edge between X and Y or not.
The importance of correctly reconstructing the network structure depends on our learning
goal. As we discussed in chapter 16, there are diﬀerent reasons for learning the model structure.
One is for knowledge discovery: by examining the dependencies in the learned network, we can
knowledge
discovery
learn the dependency structure relating variables in our domain. Of course, there are other
methods that reveal correlations between variables, for example, simple statistical independence
independence
test

784
Chapter 18. Structure Learning in Bayesian Networks
tests.
A Bayesian network structure, however, reveals much ﬁner structure.
For instance, it
can potentially distinguish between direct and indirect dependencies, both of which lead to
correlations in the resulting distribution.
If our goal is to understand the domain structure, then, clearly, the best answer we can aspire
to is recovering G∗. Even here, must be careful. Recall that there can be many perfect maps for
a distribution P ∗: all of the networks in the same I-equivalence class as G∗. All of these are
I-equivalence
equally good structures for P ∗, and therefore we cannot distinguish between them based only
on the data D. In other words, G∗is not identiﬁable from the data. Thus, the best we can

identiﬁability
hope for is an algorithm that, asymptotically, recovers G∗’s equivalence class.
Unfortunately, as our example indicates, the goal of learning G∗(or an equivalent network) is
hard to achieve. The data sampled from P ∗are noisy and do not reconstruct this distribution
perfectly. We cannot detect with complete reliability which independencies are present in the
underlying distribution. Therefore, we must generally make a decision about our willingness

to include in our learned model edges about which we are less sure. If we include more
of these edges, we will often learn a model that contains spurious edges. If we include
fewer edges, we may miss dependencies. Both compromises lead to inaccurate structures
that do not reveal the correct underlying structure. The decision of whether it is better
to have spurious correlations or spurious independencies depends on the application.
The second and more common reason to learn a network structure is in an attempt to per-
form density estimation — that is, to estimate a statistical model of the underlying distribution.
density
estimation
As we discussed, our goal is to use this model for reasoning about instances that were not in
our training data. In other words, we want our network model to generalize to new instances.
generalization
It seems intuitively reasonable that because G∗captures the true dependencies and indepen-
dencies in the domain, the best generalization will be obtained if we recover the the structure
G∗. Moreover, it seems that if we do make mistakes in the structure, it is better to have too
many rather than too few edges. With an overly complex structure, we can still capture P ∗, and
thereby represent the true distribution.
Unfortunately, the situation is somewhat more complex. Let us go back to our coin example
and assume that we had 20 data cases with the following frequencies: 3 head/head, 6 head/tail,
5 tail/head, and 6 tail/tail. We can introduce a spurious correlation between X and Y , which
would give us, using maximum likelihood estimation, the parameters P(X = H) = 0.45,
P(Y = H | X = H) = 1/3, and P(Y = H | X = T) = 5/11. On the other hand, in
the independent structure (with no edge between X and Y ), the parameter of Y would be
P(Y = H) = 0.4. All of these parameter estimates are imperfect, of course, but the ones in
the more complex model are signiﬁcantly more likely to be skewed, because each is estimated
from a much smaller data set. In particular, P(Y = H | X = H) is estimated from a data
set of 9 instances, as opposed to 20 for the estimation of P(Y = H). Recall that the standard
deviation of the maximum likelihood estimate behaves as 1/
√
M. Thus, if the coins are fair,
the standard deviation of the MLE estimate from 20 samples is approximately 0.11, while the
standard deviation from 9 samples is approximately 0.17. This example is simply an instance
of the data fragmentation issue that we discussed in section 17.2.3 in the previous chapter. As
data
fragmentation
we discussed, when we add more parents to the variable Y , the data used to estimate the CPD
fragment into more bins, leaving fewer instances in each bin to estimate the parameters and
reducing the quality of the estimated parameters. In a table-CPD, the number of bins grows
exponentially with the number of parents, so the (statistical) cost of adding a parent can be very

18.1. Introduction
785
large; moreover, because of the exponential growth, the incremental cost of adding a parent
grows with the number of parents already there.

Thus, when doing density estimation from limited data, it is often better to prefer
a sparser structure.
The surprising fact is that this observation applies not only to
networks that include spurious edges relative to G∗, but also to edges in G∗. That is,
we can sometimes learn a better model in term of generalization by learning a structure
with fewer edges, even if this structure is incapable of representing the true underlying
distribution.
18.1.2
Overview of Methods
Roughly speaking, there are three approaches to learning without a prespeciﬁed structure.
One approach utilizes constraint-based structure learning. These approaches view a Bayesian
constraint-based
structure learning
network as a representation of independencies. They try to test for conditional dependence
and independence in the data and then to ﬁnd a network (or more precisely an equivalence
class of networks) that best explains these dependencies and independencies. Constraint-based
methods are quite intuitive: they decouple the problem of ﬁnding structure from the notion
of independence, and they follow more closely the deﬁnition of Bayesian network: we have
a distribution that satisﬁes a set of independencies, and our goal is to ﬁnd an I-map for this
distribution. Unfortunately, these methods can be sensitive to failures in individual indepen-
dence tests. It suﬃces that one of these tests return a wrong answer to mislead the network
construction procedure.
The second approach is score-based structure learning. Score-based methods view a Bayesian
score-based
structure learning
network as specifying a statistical model and then address learning as a model selection problem.
model selection
These all operate on the same principle: We deﬁne a hypothesis space of potential models —
hypothesis space
the set of possible network structures we are willing to consider — and a scoring function that
measures how well the model ﬁts the observed data. Our computational task is then to ﬁnd
the highest-scoring network structure. The space of Bayesian networks is a combinatorial space,
consisting of a superexponential number of structures — 2O(n2). Therefore, even with a scoring
function, it is not clear how one can ﬁnd the highest-scoring network. As we will see, there are
very special cases where we can ﬁnd the optimal network. In general, however, the problem is
(as usual) NP-hard, and we resort to heuristic search techniques. Score-based methods consider
the whole structure at once; they are therefore less sensitive to individual failures and better at
making compromises between the extent to which variables are dependent in the data and the
“cost” of adding the edge. The disadvantage of the score-based approaches is that they pose a
search problem that may not have an elegant and eﬃcient solution.
Finally, the third approach does not attempt to learn a single structure; instead, it generates an
ensemble of possible structures. These Bayesian model averaging methods extend the Bayesian
Bayesian model
averaging
reasoning we encountered in the previous chapter and try to average the prediction of all
possible structures. Since the number of structures is immense, performing this task seems
impossible. For some classes of models this can be done eﬃciently, and for others we need to
resort to approximations.

786
Chapter 18. Structure Learning in Bayesian Networks
18.2
Constraint-Based Approaches
18.2.1
General Framework
In constraint-based approaches, we attempt to reconstruct a network structure that best captures
the independencies in the domain. In other words, we attempt to ﬁnd the best minimal I-map
for the domain.
Recall that in chapter 3 we discussed algorithms for building I-maps and P-maps that assume
that we can test for independence statements in the distribution. The algorithms for constraint-
based learning are essentially variants of these algorithms. The main technical question is how
to answer independence queries. For now, assume that we have some procedure that can answer
such queries. That is, for a given distribution P, the learning algorithm can pose a question,
such as “Does P satisfy (X1 ⊥X2, X3 | X4)?” and receive a yes/no answer. The task of
the algorithm is to carry out some algorithm that interacts with this procedure and results in a
network structure that is the minimal I-map of P.
We have already seen such an algorithm in chapter 3: Build-Minimal-I-Map constructs a
minimal I-map given a ﬁxed ordering. For each variable Xi, it then searches for the minimal
minimal I-map
subset of X1, . . . , Xi−1 that render Xi independent of the others. This algorithm was useful
in illustrating the deﬁnition of an I-map, but it suﬀers from several drawbacks in the context
of learning. First, the input order over variables can have a serious impact on the complexity
of the network we ﬁnd. Second, in learning the parents of Xi, this algorithm poses indepen-
dence queries of the form (Xi ⊥{X1, . . . , Xi−1} −U | U). These conditional independence
statements involve a large number of variables. Although we do not assume much about the
independence testing procedure, we do realize that independence statements with many vari-
ables are much more problematic to resolve from empirical data. Finally, Build-Minimal-I-Map
performs a large number of queries. For determining the parents of Xi, it must, in principle,
examine all the 2i−1 possible subsets of X1, . . . , Xi−1.
To avoid these problems, we learn an I-equivalence class rather than a single network, and
we use a class PDAG to represent this class. The algorithm that we use is a variant of the
class PDAG
Build-PDAG procedure of algorithm 3.5. As we discuss, this algorithm reconstructs the network
that best matches the domain without a prespeciﬁed order and uses only a polynomial number
of independence tests that involve a bounded number of variables.
independence
test
To achieve these performance guarantees, we must make some assumptions:
•
The network G∗has bounded indegree, that is, for all i, |PaG∗
Xi| ≤d for some constant d.
•
The independence procedure can perfectly answer any independence query that involves up
to 2d + 2 variables.
•
The underlying distribution P ∗is faithful to G∗, as in deﬁnition 3.8.
The ﬁrst assumption states the boundaries of when we expect the algorithm to work. If the
network is simple in this sense, the algorithm will be able to learn it from the data. If the
network is more complex, then we cannot hope to learn it with “small” independence queries
that involve only a few variables.
The second assumption is stronger, since it requires that the oracle can deal with queries
up to a certain size. The learning algorithm does not depend on how the these queries are
answered. They might be answered by performing a statistical test for conditional dependence

18.2. Constraint-Based Approaches
787
on a training data, or by an active mechanism that gathers more samples until it can reach
a signiﬁcant conclusion about this relations. We discuss how to construct such an oracle in
more detail later in this chapter. Note that the oracle can also be a human expert who helps in
constructing a model of the network.
The third assumption is the strongest. It is required to ensure that the algorithm is not misled
by spurious independencies that are not an artifact of the oracle but rather exist in the domain.
By requiring that G∗is a perfect map of P ∗, we rule out quite a few situations, for example, the
(noisy) XOR example of example 3.6, and various cases where additional independencies arise
from structure in the CPDs.
Once we make these assumptions, the setting is precisely the one we tackled in section 3.4.3.
Thus, given an oracle that can answer independence statements perfectly, we can now simply
apply Build-PMap-Skeleton. Of course, determining independencies from the data is not a trivial
problem, and the answers are rarely guaranteed to be perfect in practice. We will return to these
important questions. For the moment, we focus on analyzing the number of independence
queries that we need to answer, and thereby the complexity of the algorithm.
Recall that, in the construction of perfect maps, we perform independence queries only in the
Build-PMAP-Skeleton procedure, when we search for a witness to the separation between every
pair of variables. These witnesses are also used within Mark-Immoralities to determine whether
the two parents in a v-structure are conditionally independent. According to lemma 3.2, if X
and Y are not adjacent in G∗, then either PaG∗
X or PaG∗
Y
is a witness set. If we assume that G∗
has indegree of at most d, we can therefore limit our attention to witness sets of size at most
d. Thus, the number of independence queries in this step is polynomial in n, the number of
variables. Of course, this number is exponential in d, but we assume that d is a ﬁxed constant
throughout the analysis.
Thus, given our assumptions, we can perform a variant of Build-PDAG that performs a polyno-
mial number of independence tests. We can also check all other operations; that is, applying the
edge orientation rules, we can also require a polynomial number of steps. Thus, the procedure
is polynomial in the number of variables.
18.2.2
Independence Tests
The only remaining question is how to answer queries about conditional independencies be-
tween variables in the data. As one might expect, this question has been extensively studied in
the statistics literature. We brieﬂy touch on some of the issues and outline one commonly-used
methodology to answer this question.
The basic query of this type is to determine whether two variables are independent. As in
the example in the introduction to this chapter, we are given joint samples of two variables X
and Y , and we want to determine whether X and Y are independent. This basic question is
often referred to as hypothesis testing.
hypothesis testing
18.2.2.1
Single-Sided Hypothesis Tests
In hypothesis testing, we have a base hypothesis that is usually denoted by H0 and is referred
to as the null hypothesis. In the particular case of the independence test, the null hypothesis
null hypothesis
is “the data were sampled from a distribution P ∗(X, Y ) = P ∗(X)P ∗(Y ).”
Note that this

788
Chapter 18. Structure Learning in Bayesian Networks
assumption states that the data were sampled from a particular distribution in which X and Y
are independent. In real life, we do not have access to P ∗(X) and P ∗(Y ). As a substitute, we
use ˆP(X) and ˆP(Y ) as our best approximation for this distribution. Thus, we usually form H0
as the assumption that P ∗(X, Y ) = ˆP(X) ˆP(Y ).
We want to test whether the data conform to this hypothesis. More precisely, we want to ﬁnd
a procedure that we will call a decision rule that will take as input a data set D, and return
decision rule
a verdict, either Accept or Reject. We will denote the function the procedure computes to be
R(D). If R(D) = Accept, then we consider that the data satisfy the hypothesis. In our case, that
would mean that we believe that the data were sampled from P ∗and that the two variables are
independent. Otherwise, we decide to reject the hypothesis, which in our case would imply that
the variables are dependent.
The question is then, of course, how to choose a “good” decision rule. A liberal decision rule
that accepts many data sets runs the risk of accepting ones that do not satisfy the hypothesis.
A conservative rule that rejects many data sets runs the risk of rejecting many that satisfy the
hypothesis. The common approach to evaluating a decision rule is analyze the probability of
false rejection. Suppose we have access to the distribution P(D : H0, M) of data sets of M
instances given the null hypothesis. That is, we can evaluate the probability of seeing each
particular data set if the hypothesis happens to be correct. In our case, since the hypothesis
speciﬁes the distribution P ∗, this distribution is just the probability of sampling the particular
instances in the data set (we assume that the size of the data set is known in advance).
If we have access to this distribution, we can compute the probability of false rejection:
P({D : R(D) = Reject} | H0, M).
Then we can say that a decision rule R has a probability of false rejection p. We often refer to
1 −p as the conﬁdence in the decision to reject an hypothesis.1
At this point we cannot evaluate the probability of false acceptances.
Since we are not
willing to assume a concrete distribution on data sets that violate H0, we cannot quantify this
probability. For this reason, the decision is not symmetric. That is, rejecting the hypothesis “X
and Y are independent” is not the same as accepting the hypothesis “X and Y are dependent.”
In particular, to deﬁne the latter hypothesis we need to specify a distribution over data sets.
18.2.2.2
Deviance Measures
The preceding discussion suggests how to evaluate decision rules. Yet, it leaves open the question
of how to design such a rule. A standard framework for this question is to deﬁne a measure of
deviance from the null hypothesis. Such a measure d is a function from possible data sets to
deviance
the real line. Intuitively, large value of d(D) implies that D is far away from the null hypothesis.
To consider a concrete example, suppose we have discrete-valued, independent random
variables X and Y .
Typically, we expect that the counts M[x, y] in the data are close to
M · ˆP(x) · ˆP(y) (where M is the number of samples). This is the expected value of the count,
and, as we know, deviances from this value are improbable for large M. Based on this intuition,
we can measure the deviance of the data from H0 in terms of these distances. A common
measure of this type is the χ2 statistic:
χ2 statistic
1. This leads statisticians to state, “We reject the null hypothesis with conﬁdence 95 percent” as a precise statement that
can be intuitively interpreted as, “We are quite conﬁdent that the variables are correlated.”

18.2. Constraint-Based Approaches
789
dχ2(D) =
￿
x,y
(M[x, y] −M · ˆP(x) · ˆP(y))2
M · ˆP(x) · ˆP(y)
.
(18.1)
A data set that perfectly ﬁts the independence assumption has dχ2(D) = 0, and a data set
where the empirical and expected counts diverge signiﬁcantly has a larger value.
Another potential deviance measure for the same hypothesis is the mutual information
mutual
information
II ˆ
PD(X; Y ) in the empirical distribution deﬁned by the data set D. In terms of counts, this can
be written as
dII(D) = II ˆ
PD(X; Y ) =
￿
x,y
M[x, y]
M
log
M[x, y]/M
M[x]/M · M[y]/M .
(18.2)
In fact, these two deviance measures are closely related to each other; see exercise 18.1.
Once we agree on a deviance measure d (say the χ2 statistic or the empirical mutual infor-
mation), we can devise a rule for testing whether we want to accept the hypothesis
Rd,t(D) =
￿Accept
d(D) ≤t
Reject
d(D) > t.
This rule accepts the hypothesis if the deviance is small (less than the predetermined threshold
t) and rejects the hypothesis if the deviance is large.
The choice of threshold t determines the false rejection probability of the decision rule. The
computational problem is to compute the false rejection probability for di￿erent values of t.
This value is called the p-value of t:
p-value
p-value(t) = P({D : d(D) > t} | H0, M).
18.2.2.3
Testing for Independence
Using the tools we developed so far, we can reexamine the independence test. The basic tool
we use is a test to reject the null hypothesis that distribution of X and Y is the one we would
estimate if we assume that they are independent. The typical signiﬁcance level we use is 95
percent. That is, we reject the null hypothesis if the deviance in the observed data has p-value
of 0.05 or less.
If we want to test the independence of discrete categorical variables, we usually use the χ2
statistic or the mutual information. The null hypothesis is that P ∗(X, Y ) = ˆP(X) ˆP(Y ).
We start by considering how to perform an exact test. The deﬁnition of p-value requires
summing over all possible data sets. In fact, since we care only about the su￿cient statistics
of X and Y in the data set, we can sum over the smaller space of di￿erent su￿cient statistics
vectors. Suppose we have M samples; we deﬁne the space CM
X,Y to be the set of all empirical
counts over X and Y , we might observe in a data set with M samples. Then we write
p-value(t) =
￿
C[X,Y ]∈CM
X,Y
11{d(C[X, Y ]) > t}P(C[X, Y ] | H0, M),
where d(C[X, Y ]) is the deviance measure (that is, χ2 or mutual information) computed with
the counts C[X, Y ], and
P(C[X, Y ] | H0, M) = M!
￿
x,y
1
C[x, y]!P(x, y | H0)C[x,y]
(18.3)

790
Chapter 18. Structure Learning in Bayesian Networks
is the probability of seeing a data set with these counts given H0; see exercise 18.2.
This exact approach enumerates through all data sets. This is clearly infeasible except for
small values of M. A more common approach is to examine the asymptotic distribution of
M[x, y] under the null hypothesis. Since this count is a sum of binary indicator variables, its
distribution is approximately normal when M is large enough. Statistical theory develops the
asymptotic distribution of the deviance measure under the null hypothesis. For the χ2 statistic,
this distribution is called the χ2 distribution. We can use the tail probability of this distribution
χ2 distribution
to approximate p-values for independence tests. Numerical procedures for such computations
are part of most standard statistical packages.
A natural extension of this test exists for testing conditional independence.
Suppose we
want to test whether X and Y are independent given Z. Then, H0 is that P ∗(X, Y, Z) =
ˆP(Z) ˆP(X | Z) ˆP(Y | Z), and the χ2 statistic is
dχ2(D) =
X
x,y,z
(M[x, y, z] −M · ˆP(z) ˆP(x | z) ˆP(y | z))2
M · ˆP(z) ˆP(x | z) ˆP(y | z)
.
This formula extends easily to conditioning on a set of variables Z.
18.2.2.4
Building Networks
We now return to the problem of learning network structure.
With the methods we just
discussed, we can evaluate independence queries in the Build-PDAG procedure, so that whenever
the test rejects the null hypothesis we treat the variables as dependent.
One must realize,
however, that these tests are not perfect. Thus, we run the risk of making wrong decisions on
some of the queries. In particular, if we use signiﬁcance level of 95 percent, then we expect that
on average 1 in 20 rejections is wrong. When testing a large number of hypotheses, a scenario
called multiple hypothesis testing, the number of incorrect conclusions can grow large, reducing
multiple
hypothesis testing
our ability to reconstruct the correct network. We can try to reduce this number by taking
stricter signiﬁcance levels (see exercise 18.3). This, however, runs the risk of making more errors
of the opposite type.
In conclusion, we have to be aware that some of the independence tests results can be
wrong. The procedure Build-PDAG can be sensitive to such errors. In particular, one misleading
independence test result can produce multiple errors in the resulting PDAG (see exercise 18.4).
When we have relatively few variables and large sample size (and “strong” dependencies among
variables), the reconstruction algorithm we described here is eﬃcient and often manages to ﬁnd
a structure that is quite close to the correct structure. When the independence test results are
less pronounced, the constraint-based approach can run into trouble.
18.3
Structure Scores
As discussed earlier, score-based methods approach the problem of structure learning as an
optimization problem. We deﬁne a score function that can score each candidate structure with
respect to the training data, and then search for a high-scoring structure. As can be expected,
one of the most important decisions we must make in this framework is the choice of scoring
function. In this section, we discuss two of the most obvious choices.

18.3. Structure Scores
791
18.3.1
Likelihood Scores
18.3.1.1
Maximum Likelihood Parameters
A natural choice for scoring function is the likelihood function, which we used for parameter
estimation. Recall that this function measures the probability of the data given a model. Thus,
it seems intuitive to ﬁnd a model that would make the data as probable as possible.
Assume that we want to maximize the likelihood of the model. In this case, our model is
a pair ⟨G, θG⟩.
Our goal is to ﬁnd both a graph G and parameters θG that maximize the
likelihood.
In the previous chapter, we determined how to maximize the likelihood for a given structure
G. We simply use the maximum likelihood parameters ˆθG for that graph. A simple analysis now
shows that:
max
G,θG L(⟨G, θG⟩: D)
=
max
G [max
θG L(⟨G, θG⟩: D)]
=
max
G [L(⟨G, ˆθG⟩: D)].
In other words, to ﬁnd the maximum likelihood (G, θG) pair, we should ﬁnd the graph structure
G that achieves the highest likelihood when we use the MLE parameters for G. We deﬁne:
scoreL(G : D) = ℓ(ˆθG : D),
where ℓ(ˆθG : D) is the logarithm of the likelihood function and ˆθG are the maximum likelihood
parameters for G. (As usual, it will be easier to deal with the logarithm of the likelihood.)
18.3.1.2
Information-Theoretic Interpretation
To get a better intuition of the likelihood score, let us consider the scenario of example 18.1.
Consider the model G0 where X and Y are independent. In this case, we get
scoreL(G0 : D) =
X
m
log ˆθx[m] + log ˆθy[m].
On the other hand, we can consider the model G1 where there is an arc X →Y .
The
log-likelihood for this model is
scoreL(G1 : D) =
X
m
log ˆθx[m] + log ˆθy[m]|x[m],
where ˆθx is again the maximum likelihood estimate for P(x), and ˆθy|x is the maximum likeli-
hood estimate for P(y | x).
We see that the score of two models share a common component (the terms of the form
log ˆθx). Thus, we can write the diﬀerence between the two scores as
scoreL(G1 : D) −scoreL(G0 : D) =
X
m
log ˆθy[m]|x[m] −log ˆθy[m].
By counting how many times each conditional probability parameter appears in this term, we
can write this sum as:
scoreL(G1 : D) −scoreL(G0 : D) =
X
x,y
M[x, y] log ˆθy|x −
X
y
M[y] log ˆθy.

792
Chapter 18. Structure Learning in Bayesian Networks
Let ˆP be the empirical distribution observed in the data; that is, ˆP(x, y) is simply the empirical
frequency of x, y in D. Then, we can write M[x, y] = M · ˆP(x, y), and M[y] = M ˆP(y).
Moreover, it is easy to check that ˆθy|x = ˆP(y | x), and that ˆθy = ˆP(y). We get:
scoreL(G1 : D) −scoreL(G0 : D) = M
X
x,y
ˆP(x, y) log
ˆP(y | x)
ˆP(y)
= M · II ˆ
P (X; Y ),
where II ˆ
P (X; Y ) is the mutual information between X and Y in the distribution ˆP.
mutual
information
We see that the likelihood of the model G1 depends on the mutual information between X
and Y . Recall that higher mutual information implies stronger dependency. Thus, stronger
dependency implies stronger preference for the model where X and Y depend on each other.
Can we generalize this information-theoretic formulation of the maximum likelihood score to
general network structures? Going through a similar arithmetic transformations, we can prove
the following result.
Proposition 18.1
The likelihood score decomposes as follows:
decomposable
score
scoreL(G : D) = M
n
X
i=1
II ˆ
P (Xi; PaG
Xi) −M
n
X
i=1
IH ˆ
P (Xi).
(18.4)
Proof We have already seen that by combining all the occurrences of each parameter θxi|u, we
can rewrite the log-likelihood function as
ℓ(ˆθG : D) =
n
X
i=1


X
ui∈Val(PaG
Xi)
X
xi
M[xi, ui] log ˆθxi|ui

.
Consider one of the terms in the square brackets, and let U i = PaXi.
1
M
X
ui
X
xi
M[xi, ui] log ˆθxi|ui
=
X
ui
X
xi
ˆP(xi, ui) log ˆP(xi | ui)
=
X
ui
X
xi
ˆP(xi, ui) log
 ˆP(xi, ui)
ˆP(ui)
ˆP(xi)
ˆP(xi)
!
=
X
ui
X
xi
ˆP(xi, ui) log
ˆP(xi, ui)
ˆP(ui) ˆP(xi)
+
X
xi
 X
ui
ˆP(xi, ui)
!
log ˆP(xi)
=
II ˆ
P (Xi; U i) −
X
xi
ˆP(xi) log
1
ˆP(xi)
=
II ˆ
P (Xi; U i) −IH ˆ
P (Xi),
where (as implied by the deﬁnition) the mutual information II ˆ
P (Xi; PaXi) is 0 if PaXi = ∅.

18.3. Structure Scores
793
Note that the second sum in equation (18.4) does not depend on the network structure, and
thus we can ignore it when we compare two structures with respect to the same data set.
Recall that we can interpret IIP (X; Y ) as the strength of the dependence between X and Y in
P. Thus, the likelihood of a network measures the strength of the dependencies between

variables and their parents. In other words, we prefer networks where the parents of
each variable are informative about it.
This result can also be interpreted in a complementary manner.
Corollary 18.1
Let X1, . . . , Xn be an ordering of the variables that is consistent with edges in G. Then,
1
M scoreL(G : D) = IH ˆ
P (X1, . . . , Xn) −
n
X
i=1
II ˆ
P (Xi; {X1, . . . Xi−1} −PaG
Xi | PaG
Xi). (18.5)
For proof, see exercise 18.5.
Again, this second reformulation of the likelihood has a term that does not depend on the
structure, and one that does. This latter term involves conditional mutual-information expres-
sions of the form II ˆ
P (Xi; {X1, . . . Xi−1} −PaG
Xi | PaG
Xi). That is, the information between
Xi and the preceding variables in the order given Xi’s parents. Smaller conditional mutual-
information terms imply higher scores. Recall that conditional independence is equivalent to
having zero conditional mutual information. Thus, we can interpret this formulation as measur-
ing to what extent the Markov properties implied by G are violated in the data. The smaller the
violations of the Markov property, the larger the score.
These two interpretations are complementary, one measuring the strength of dependence
between Xi and its parents PaG
Xi, and the other measuring the extent of the independence of
Xi from its predecessors given PaG
Xi.
The process of choosing a network structure is often subject to constraints. Some constraints
are a consequence of the acyclicity requirement, others may be due to a preference for simpler
structures. Our previous analysis shows that the likelihood score provides valuable guidance in
selecting between diﬀerent candidate networks.
18.3.1.3
Limitations of the Maximum Likelihood Score
Based on the developments in the previous chapter and the preceding analysis, we see that
the likelihood score is a good measure of the ﬁt of the estimated Bayesian network and the
training data. In learning structure, however, we are also concerned about the performance
of the learned network on new instances sampled from the same underlying distribution P ∗.
Unfortunately, in this respect, the likelihood score can run into problems.
To see this, consider example 18.1. Let G∅be the network where X and Y are independent,
and GX→Y the one where X is the parent of Y . As we have seen, scoreL(GX→Y
: D) −
scoreL(G∅: D) = M · II ˆ
P (X; Y ). Recall that the mutual information between two variables is
nonnegative. Thus, scoreL(GX→Y
: D) ≥scoreL(G∅: D) for any data set D. This implies
that the maximum likelihood score never prefers the simpler network over the more complex
one. And it assigns both networks the same score only in these rare situations when X and Y
are truly independent in the training data.
As explained in the introduction to this chapter, there are situations where we should prefer to

794
Chapter 18. Structure Learning in Bayesian Networks
learn the simpler network (for example, when X and Y are nearly independent in the training
data). We see that the maximum likelihood score would never lead us to make that choice.
This observation applies to more complex networks as well. It is easy to show that adding an
edge to a network structure can never decrease the maximum likelihood score. Furthermore, the
more complex network will have a higher score in all but a vanishingly small fraction of cases.
One approach to proving this follows directly from the notion of likelihood; see exercise 18.6.
Another uses the fact that, for any X, Y , Z and any distribution P, we have that:
IIP (X; Y ∪Z) ≥IIP (X; Y ),
with equality holding only if Z is conditionally independent of X given Y , see exercise 2.20.
This inequality is fairly intuitive: if Y gives us a certain amount of information about X, adding
Z can only give us more information. Thus, the mutual information between a variable and its
parents can only go up if we add another parent, and it will go up except in those few cases
where we get a conditional independence assertion holding exactly in the empirical distribution.
It follows that the maximum likelihood network will exhibit a conditional independence

only when that independence happens to hold exactly in the empirical distribution. Due
to statistical noise, exact independence almost never occurs, and therefore, in almost all
cases, the maximum likelihood network will be a fully connected one. In other words,
the likelihood score overﬁts the training data (see section 16.3.1), learning a model that
overﬁtting
precisely ﬁts the speciﬁcs of the empirical distribution in our training set. This model
therefore fails to generalize well to new data cases: these are sampled from the underlying
distribution, which is not identical to the empirical distribution in our training set.
We note that the discussion of the maximum likelihood score was in the context of networks
with table-CPDs. However, the same observations also apply to learning networks with other
forms of CPDs (for example, tree-CPDs, noisy-ors, or Gaussians). In these cases, the information-
theoretic analysis is somewhat more elaborate, but the general conclusions about the trade-oﬀs
between models and about overﬁtting apply.
Since the likelihood score does not provide us with tools to avoid overﬁtting, we have to be
careful when using it. It is reasonable to use the maximum likelihood score when there are
additional mechanisms that disallow overly complicated structures. For example, we will discuss
learning networks with a ﬁxed indegree. Such a limitation can constrain the tendency to overﬁt
when using the maximum likelihood score.
18.3.2
Bayesian Score
We now examine an alternative scoring function that is based on a Bayesian perspective; this
approach extends ideas that we described in the context of parameter estimation in the previous
chapter. We will start by deriving the score from the Bayesian perspective, and then we will try
to understand how it avoids overﬁtting.
Recall that the main principle of the Bayesian approach was that whenever we have uncer-
tainty over anything, we should place a distribution over it. In this case, we have uncertainty
both over structure and over parameters. We therefore deﬁne a structure prior P(G) that puts
a prior probability on diﬀerent graph structures, and a parameter prior P(θG | G), that puts a

18.3. Structure Scores
795
probability on diﬀerent choice of parameters once the graph is given. By Bayes rule, we have
P(G | D) = P(D | G)P(G)
P(D)
,
where, as usual, the denominator is simply a normalizing factor that does not help distinguish
between diﬀerent structures. Thus, we deﬁne the Bayesian score as:
Bayesian score
scoreB(G : D) = log P(D | G) + log P(G).
(18.6)
The ability to ascribe a prior over structures gives us a way of preferring some structures over
others. For example, we can penalize dense structures more than sparse ones. It turns out,
however, that the structure-prior term in the score is almost irrelevant compared to the ﬁrst
term. This term, P(D | G), takes into consideration our uncertainty over the parameters:
P(D | G) =
Z
ΘG
P(D | θG, G)P(θG | G)dθG,
(18.7)
where P(D | θG, G) is the likelihood of the data given the network ⟨G, θG⟩and P(θG | G) is
our prior distribution over diﬀerent parameter values for the network G. Recall from section 17.4
that P(D | G) is called the marginal likelihood of the data given the structure, since we
marginal
likelihood
marginalize out the unknown parameters.
It is important to realize that the marginal likelihood is quite diﬀerent from the maximum
likelihood score.
Both terms examine the likelihood of the data given the structure.
The
maximum likelihood score returns the maximum of this function. In contrast, the marginal
likelihood is the average value of this function, where we average based on the prior measure
P(θG | G). This diﬀerence will become apparent when we analyze the marginal likelihood term.
One explanation of why the Bayesian score avoids overﬁtting examines the sensitivity of the
likelihood to the particular choice of parameters. As we discussed, the maximal likelihood is
overly “optimistic” in its evaluation of the score: It evaluates the likelihood of the training data
using the best parameter values for the given data.
This estimate is realistic only if these
parameters are also reﬂective of the data in general, a situation that never occurs.
The Bayesian approach tells us that, although the choice of parameter ˆθ is the most likely
given the training set D, it is not the only choice. The posterior over parameters provides us
with a range of choices, along with a measure of how likely each of them is. By integrating
P(D | θG, G) over the diﬀerent choices of parameters θG, we are measuring the expected
likelihood, averaged over diﬀerent possible choices of θG. Thus, we are being more conservative
in our estimate of the “goodness” of the model.
Another motivation can be derived from the holdout testing methods discussed in box 16.A.
holdout testing
Here, we consider diﬀerent network structures, parameterized by the training set, and test their
predictiveness (likelihood) on the validation set. When we ﬁnd a network that best generalizes
to the validation set (that is, has the best likelihood on this set), we have some reason to hope
that it will also generalize to other unseen instances. As we discussed, the holdout method is
sensitive to the particular split into training and test sets, both in terms of the relative sizes
of the sets and in terms of which instances fall into which set. Moreover, it does not use all
the available data in learning the structure, a potentially serious problem when we have limited
amounts of data to learn from.

796
Chapter 18. Structure Learning in Bayesian Networks
–15.3
–15.4
–15.5
–15.6
–15.7
–15.8
–17.5
–17.4 
500 instances
10,000 instances
–17.3
–17.2
–17.1
–17.0
log P(      )
M
1
–15
–15.1
–15.2
–15.3
–15.4
–15.5
–15.7
–15.2
–15.3
–15.4
–15.5
–15.6
EP*[log P(   G,D)]
EP*[log P(   G,D)]
log P(      )
M
1
Figure 18.1
Marginal likelihood in training data as predictor of expected likelihood on underlying
distribution. Comparison of the average log-marginal-likelihood per sample in training data (x-axis) to the
expected log-likelihood of new samples from the underlying distribution (y-axis) for two data sets sampled
from the ICU-Alarm network. Each point corresponds to a network structure; the true network structure is
marked by a circle.
It turns out that the Bayesian approach can be viewed as performing a similar evaluation
without explicitly splitting the data into two parts. Using the chain rule for probabilities, we can
rewrite the marginal likelihood as
P(D | G) =
M
Y
m=1
P(ξ[m] | ξ[1], . . . , ξ[m −1], G).
Each of the terms in this product — P(ξ[m] | ξ[1], . . . , ξ[m −1], G) — is the probability of
the m’th instance using the parameters learned from the ﬁrst m −1 instances (using Bayesian
estimation). We see that in this term we are using the m’th instance as a test case, since we are
computing its probability using what we learned from previous instances. Thus, it provides us
with one data point for testing the ability of our model to predict a new data instance, based on
the model learned from the previous ones. This type of analysis is called a prequential analysis.
prequential
analysis
However, unlike the holdout approach, we are not holding out any data. Each instance is
evaluated in incremental order, and contributes both to our evaluation of the model and to our
ﬁnal model score. Moreover, the Bayesian score does not depend on the order of instances.
Using the chain law of probabilities, we can generate a similar expansion for any ordering of
the instances. Each one of these will give the same result (since these are diﬀerent ways of
expanding the term P(D | G)).
This intuition suggests that
1
M log P(D | G) ≈IEP ∗[log P(X | G, D)]
(18.8)
is an estimator for the average log-likelihood of a new sample from the distribution P ∗. In

18.3. Structure Scores
797
P(D |q)P(q)
0
0.2
0.017
0.4
0.6
0.8
1
q
0.04
0.03
0.02
0.01
0
0.035
Figure 18.2
Maximal likelihood score versus marginal likelihood for the data ⟨H, T, T, H, H⟩.
practice, it turns out that for reasonable sample sizes this is indeed a fairly good estimator of the
ability of a model to generalize to unseen data. Figure 18.1 demonstrates this property empirically
for data sets sampled from the ICU-Alarm network.
We generated a collection of network
structures by sampling from the posterior distribution over structures given diﬀerent data sets
(see section 18.5). For each structure we evaluated the two sides of the preceding approximation:
the average log-likelihood per sample, and the expected likelihood of new samples from the
underlying distribution. As we can see, there is a general agreement between the estimate using
the training data and the actual generalization error of each network structure. In particular,
the diﬀerence in scores of two structures correlates with the diﬀerences in generalization error.
This phenomenon is particularly noticeable in the larger training set.
We note that the Bayesian score is not the only way of providing “test set” performance using
each instance. See exercise 18.12 for an alternative score with similar properties.
18.3.3
Marginal Likelihood for a Single Variable
We now examine how to compute the marginal likelihood for simple cases, and then in the next
section treat the case of Bayesian networks.
Consider a single binary random variable X, and assume that we have a prior distribution
Dirichlet(α1, α0) over X. Consider a data set D that has M[1] heads and M[0] tails. Then,
the maximum likelihood value given D is
P(D | ˆθ) =
M[1]
M
M[1]
·
M[0]
M
M[0]
.
Now, consider the marginal likelihood.
Here, we are not conditioning on the parameter.
Instead, we need to compute the probability P(X[1], . . . , X[M]) of the data given our prior.
One approach to computing this term is to evaluate the integral equation (18.7). An alternative
approach uses the chain rule
P(x[1], . . . , x[M]) = P(x[1]) · P(x[2] | x[1]) · . . . · P(x[M] | x[1], . . . , x[M −1]).

798
Chapter 18. Structure Learning in Bayesian Networks
Recall that if we use a Beta prior, then
P(x[m + 1] = H | x[1], . . . , x[m]) = M m[1] + α1
m + α
,
where M m[1] is the number of heads in the ﬁrst m examples. For example, if D = ⟨H, T, T, H, H⟩,
P(x[1], . . . , x[5])
=
α1
α ·
α0
α + 1 · α0 + 1
α + 2 · α1 + 1
α + 3 · α1 + 2
α + 4
=
[α1(α1 + 1)(α1 + 2)][α0(α0 + 1)]
α · · · (α + 4)
.
Picking α1 = α0 = 1, so that α = α1 + α0 = 2, we get
[1 · 2 · 3] · [1 · 2]
2 · 3 · 4 · 5 · 6
= 12
720 = 0.017
(see ﬁgure 18.2), which is signiﬁcantly lower than the likelihood
3
5
3
·
2
5
2
= 108
3125 ≈0.035.
Thus, a model using maximum-likelihood parameters ascribes a much higher probability to this
sequence than does the marginal likelihood. The reason is that the log-likelihood is making an
overly optimistic assessment, based on a parameter that was designed with full retrospective
knowledge to be an optimal ﬁt to the entire sequence.
In general, for a binomial distribution with a Beta prior, we have
P(x[1], . . . , x[M]) = [α1 · · · (α1 + M[1] −1)][α0 · · · (α0 + M[0] −1)]
α · · · (α + M −1)
.
Each of the terms in square brackets is a product of a sequence of numbers such as α · (α +
1) · · · (α + M −1). If α is an integer, we can write this product as (α+M−1)!
(α−1)!
. However, we do
not necessarily know that α is an integer. It turns out that we can use a generalization of the
factorial function for this purpose. Recall that the Gamma function is such that Γ(m) = (m−1)!
Gamma function
and Γ(x + 1) = x · Γ(x). Using the latter property, we can rewrite
α(α + 1) · · · (α + M −1) = Γ(α + M)
Γ(α)
.
Hence,
P(x[1], . . . , x[M]) =
Γ(α)
Γ(α + M) · Γ(α1 + M[1])
Γ(α1)
· Γ(α0 + M[0])
Γ(α0)
.
A similar formula holds for a multinomial distribution over the space x1, . . . , xk, with a
Dirichlet prior with hyperparameters α1, . . . , αk:
P(x[1], . . . , x[M]) =
Γ(α)
Γ(α + M) ·
k
Y
i=1
Γ(αi + M[xi])
Γ(αi)
.
(18.9)

18.3. Structure Scores
799
Note that the ﬁnal expression for the marginal likelihood is invariant to the order we selected
in the expansion via the chain rule. In particular, any other order results in exactly the same
ﬁnal expression. This property is reassuring, because the IID assumption tells us that the speciﬁc
order in which we get data cases is insigniﬁcant. Also note that the marginal likelihood can be
computed directly from the same suﬃcient statistics used in the computation of the likelihood
function — the counts of the diﬀerent values of the variable in the data. This observation will
continue to hold in the general case of Bayesian networks.
18.3.4
Bayesian Score for Bayesian Networks
We now generalize the discussion of the Bayesian score to more general Bayesian networks.
Consider two possible structures over two binary random variables X and Y . G∅is the graph
with no edges. Here, we have:
P(D | G∅) =
Z
ΘX×ΘY
P(θX, θY | G∅)P(D | θX, θY , G∅)d[θX, θY ].
We know that the likelihood term P(D | θX, θY , G∅) can be written as a product of terms,
one involving θX and the observations of X in the data, and the other involving θY and
the observations of Y in the data. If we also assume parameter independence, that is, that
parameter
independence
P(θX, θY | G∅) decomposes as a product P(θX | G∅)P(θY | G∅), then we can simplify the
integral
P(D | G∅)
=
 Z
ΘX
P(θX | G∅)
Y
m
P(x[m] | θX, G∅)dθX
!
 Z
ΘY
P(θY | G∅)
Y
m
P(y[m] | θY , G∅)dθY
!
,
where we used the fact that the integral of a product of independent functions is the product
of integrals. Now notice that each of the two integrals is the marginal likelihood of a single
variable. Thus, if X and Y are multinomials, and each has a Dirichlet prior, then we can write
each integral using the closed form of equation (18.9).
Now consider the network GX→Y = (X →Y ).
Once again, if we assume parameter
independence, we can decompose this integral into a product of three integrals, each over a
single parameter family.
P(D | GX→Y )
=
 Z
ΘX
P(θX | GX→Y )
Y
m
P(x[m] | θX, GX→Y )dθX
!


Z
ΘY |x0
P(θY |x0 | GX→Y )
Y
m:x[m]=x0
P(y[m] | θY |x0, GX→Y )dθY |x0




Z
ΘY |x1
P(θY |x1 | GX→Y )
Y
m:x[m]=x1
P(y[m] | θY |x1, GX→Y )dθY |x1

.
Again, each of these can be written using the closed form solution of equation (18.9).

800
Chapter 18. Structure Learning in Bayesian Networks
Comparing the marginal likelihood of the two structures, we see that the term that corre-
sponds to X is similar in both. In fact, the terms P(x[m] | θX, G∅) and P(x[m] | θX, GX→Y )
are identical (both make the same predictions given the parameter values). Thus, if we choose
the prior P(ΘX | G∅) to be the same as P(ΘX | GX→Y ), we have that the ﬁrst term in the
marginal likelihood of both structures is identical.
Thus, given this assumption about the prior, the diﬀerence between the marginal likelihood of
G∅and GX→Y is due to the diﬀerence between the marginal likelihood of all the observations of
Y and the marginal likelihoods of the observations of Y when we partition our examples based
on the observed value of X. Intuitively, if Y has a diﬀerent distribution in these two cases,
then the latter term will have better marginal likelihood. On the other hand, if Y is distributed
in roughly the same manner in both subsets, then the simpler network will have better marginal
likelihood.
To see this behavior, we consider an idealized experiment where the empirical distribution is
such that P(x1) = 0.5, and P(y1 | x1) = 0.5 + p and P(y1 | x0) = 0.5 −p, where p is a free
parameter. Larger values of p imply stronger dependence between X and Y . Note, however,
that the marginal distributions of X and Y are the same regardless of the value of p. Thus,
the score of the empty structure G∅does not depend on p. On the other hand, the score of
the structure GX→Y depends on p. Figure 18.3 illustrates how these scores change as functions
of the number of training samples. The graph compares the average score per instance (of
equation (18.8)) for both structures for diﬀerent values of p.
We can see that, as we get more data, the Bayesian score prefers the structure GX→Y where
X and Y are dependent. When the dependency between them is strong, this preference arises
very quickly. But as the dependency becomes weaker, more data are required in order to justify
this selection. Thus, if the two variables are independent, small ﬂuctuations in the data, due to
sampling noise, are unlikely to cause a preference for the more complex structure. By contrast,
any ﬂuctuation from pure independence in the empirical distribution will cause the likelihood
score to select the more complex structure.
We now return to consider the general case. As we can expect, the same arguments we
applied to the two-variable networks apply to any network structure.
Proposition 18.2
Let G be a network structure, and let P(θG | G) be a parameter prior satisfying global parameter
independence. Then,
P(D | G) =
Y
i
Z
ΘXi|PaXi
Y
m
P(xi[m] | paXi[m], θXi|PaXi, G)P(θXi|PaXi | G)dθXi|PaXi.
Moreover, if P(θG) also satisﬁes local parameter independence, then
P(D | G) =
Y
i
Y
ui∈Val(PaG
Xi)
Z
ΘXi|ui
Y
m,ui[m]=ui
P(Xi[m] | ui, θXi|ui, G)P(θXi|ui | G)dθXi|ui.
Using this proposition and the results about the marginal likelihood of Dirichlet priors, we
conclude the following result: If we consider a network with Dirichlet priors where P(θXi|paXi |

18.3. Structure Scores
801
P = 0.00
P = 0.05
P = 0.10
P = 0.15
–1.3
–1.4
–1.5
–1.6
–1.7
–1.8
10
100
1,000
log P(      )
M
1
M
Figure 18.3
The eﬀect of correlation on the Bayesian score. The solid line indicates the score of the
independent model G∅. The remaining lines indicate the score of the more complex structure GX→Y , for
diﬀerent sampling distributions parameterized by p.
G) has hyperparameters {αG
xj
i |ui : j = 1, . . . , |Xi|} then
P(D | G) =
Y
i
Y
ui∈Val(PaG
Xi)
Γ(αG
Xi|ui)
Γ(αG
Xi|ui + M[ui])
Y
xj
i ∈Val(Xi)


Γ(αG
xj
i |ui + M[xj
i, ui])
Γ(αG
xj
i |ui)

,
where αG
Xi|ui = P
j αG
xj
i |ui. In practice, we use the logarithm of this formula, which is more
manageable to compute numerically.2
18.3.5
Understanding the Bayesian Score
As we have just seen, the Bayesian score seems to be biased toward simpler structures, but

as it gets more data, it is willing to recognize that a more complex structure is necessary.
In other words, it appears to trade oﬀﬁt to data with model complexity, thereby reducing
the extent of overﬁtting. To understand this behavior, it is useful to consider an approximation
overﬁtting
to the Bayesian score that better exposes its fundamental properties.
Theorem 18.1
If we use a Dirichlet parameter prior for all parameters in our network, then, when M →∞, we
have that:
log P(D | G) = ℓ(ˆθG : D) −log M
2
Dim[G] + O(1),
where Dim[G] is the model dimension, or the number of independent parameters in G.
model dimension
independent
parameters
See exercise 18.7 for the proof.
2. Most scientiﬁc computation libraries have eﬃcient numerical implementation of the function log Γ(x), which enables
us to compute this score eﬃciently.

802
Chapter 18. Structure Learning in Bayesian Networks
–16
–18
–20
–22
–24
0
500
1,500
2,000
2,500
3,000
1,000
log P(      )
M
1
M
Figure 18.4
The Bayesian score of three structures, evaluated on synthetic data generated from the
ICU-Alarm network. The solid line is the original structure, which has 509 parameters. The dashed line is
a simpliﬁcation that has 359 parameters. The dotted line is a tree-structure and has 214 parameters.
Thus, we see that the Bayesian score tends to trade oﬀthe likelihood — ﬁt to data — on one
hand and some notion of model complexity on the other hand. This approximation is called the
BIC score (for Bayesian information criterion):
BIC score
scoreBIC(G : D) = ℓ(ˆθG : D) −log M
2
Dim[G].
We note that the negation of this quantity can be viewed as the number of bits required to
encode both the model (log M/2 bits per model parameter, a derivation whose details we omit)
and the data given the model (as per our discussion in section A.1.3). Thus, this objective is also
known as minimum description length.
minimum
description
length
We can decompose this score even further using our analysis from equation (18.4):
scoreBIC(G : D) = M
n
X
i=1
II ˆ
P (Xi; PaXi) −M
n
X
i=1
IH ˆ
P (Xi) −log M
2
Dim[G].
We can observe several things about the behavior of this score function. First, the entropy
terms do not depend on the graph, so they do not inﬂuence the choice of structure and can be
ignored. The score exhibits a trade-oﬀbetween ﬁt to data and model complexity: the stronger
the dependence of a variable on its parents, the higher the score; the more complex the network,
the lower the score. However, the mutual information term grows linearly in M, whereas the
complexity term grows logarithmically. Therefore, the larger M is, the more emphasis will be
given to the ﬁt to data.
Figure 18.4 illustrates this theorem empirically. It shows the Bayesian score of three structures
on a data set generated by the ICU-Alarm network. One of these structures is the correct one,
and the other two are simpliﬁcations of it. We can see that, for small M, the simpler structures
have the highest scores. This is compatible with our analysis: for small data sets, the penalty
term outweighs the likelihood term. But as M grows, the score begins to exhibit an increasing
preference for the more complex structures. With enough data, the true model is preferred.

18.3. Structure Scores
803
This last statement is a general observation about the BIC and Bayesian scores: Asymptotically,
these scores will prefer a structure that exactly ﬁts the dependencies in the data. To make this
statement precise, we introduce the following deﬁnition:
Deﬁnition 18.1
Assume that our data are generated by some distribution P ∗for which the network G∗is a perfect
map. We say that a scoring function is consistent if the following properties hold as the amount of
consistent score
data M →∞, with probability that approaches 1 (over possible choices of data set D):
• The structure G∗will maximize the score.
• All structures G that are not I-equivalent to G∗will have strictly lower score.
Theorem 18.2
The BIC score is consistent.
Proof Our goal is to prove that for suﬃciently large M, if the graph that maximizes the BIC
score is G, then G is I-equivalent to G∗. We brieﬂy sketch this proof.
Consider some graph G that implies an independence assumption that G∗does not support.
Then G cannot be an I-map of the true underlying distribution P.
Hence, G cannot be a
maximum likelihood model with respect to the true distribution P ∗, so that we must have:
X
i
IIP ∗(Xi; PaG∗
Xi) >
X
i
IIP ∗(Xi; PaG
Xi).
As M →∞, our empirical distribution ˆP will converge to P ∗with probability 1. Therefore, for
large M,
scoreL(G∗: D) −scoreL(G : D) ≈∆· M,
where ∆= P
i IIP ∗(Xi; PaG∗
Xi) −P
i IIP ∗(Xi; PaG
Xi). Therefore, asymptotically we have that
scoreBIC(G∗: D) −scoreBIC(G : D) ≈∆M + 1
2(Dim[G] −Dim[G∗]) log M.
The ﬁrst term grows much faster than the second, so that eventually its eﬀect will dominate,
and the score of G∗will be better.
Now, assume that G implies all the independence assumptions in G∗, but that G∗implies an
independence assumption that G does not. (In other words, G is a superset of G∗.) In this case,
G can represent any distribution that G∗can. In particular, it can represent P ∗. As ˆP converges
to P ∗, we will have that:
scoreL(G∗: D) −scoreL(G : D) →0.
Therefore, asymptotically we have that for
scoreBIC(G∗: D) −scoreBIC(G : D) ≈1
2(Dim[G] −Dim[G∗]) log M.
Now, since G makes fewer independence assumptions than G∗, it must be parameterized by a
larger set of parameters. Thus, Dim[G] > Dim[G∗], so that G∗will be preferred to G.

804
Chapter 18. Structure Learning in Bayesian Networks
As the Bayesian score is asymptotically identical to BIC (the remaining O(1) terms do not
grow with M), we get:
Corollary 18.2
The Bayesian score is consistent.
Note that consistency is an asymptotic property, and thus it does not imply much about the
properties of networks learned with limited amounts of data. Nonetheless, the proof illustrates
the trade-oﬀs that are playing a role in the deﬁnition of score.
18.3.6
Priors
Until now we did not specify the actual choice of priors we use. We now discuss possible
choices of priors and their eﬀect on the score.
18.3.6.1
Structure Priors
We begin with the prior over network structures, P(G). Note that although this term seems
structure prior
to describe our bias for certain structure, in fact, it plays a relatively minor role. As we can
see in theorem 18.1, the logarithm of the marginal likelihood grows linearly with the number
of examples, while the prior over structures remains constant. Thus, the structure prior does
not play an important role in asymptotic analysis as long as it does not rule out (that is, assign
probability 0) any structure.
For this reason, we often use a uniform prior over structures. Nonetheless, the structure prior
can make some diﬀerence when we consider small samples. Thus, we might want to encode
some of our preferences in this prior. For example, we might penalize edges in the graph, and
use a prior P(G) ∝c|G|, where c is some constant smaller than 1, and |G| is the number of
edges in the graph.
Note that in both these choices (the uniform and the penalty per edge) it suﬃces to use a
value that is proportional to the prior, since the normalizing constant is the same for all choice
of G and hence can be ignored. For this reason, we do not need to worry about the exact
number of possible network structures in order to use these priors.
As we will immediately see, it will be mathematically convenient to assume that the structure
prior satisﬁes structure modularity. This condition requires that the prior P(G) be proportional
structure
modularity
to a product of terms, where each term relates to one family. Formally,
P(G) ∝
Y
i
P(PaXi = PaG
Xi),
where P(PaXi = PaG
Xi) denotes the prior probability we assign to choosing the speciﬁc set of
parents for Xi. Structure priors that satisfy this property do not penalize for global properties
of the graph (such as its depth) but only for local properties (such as the indegrees of variables).
This is clearly the case for both priors we discuss here.
In addition, it also seems reasonable to require that I-equivalent network structures are
assigned the same prior. Again, this means that when two networks are equivalent, we do not
distinguish between them by subjective preferences.

18.3. Structure Scores
805
18.3.6.2
Parameter Priors and Score Decomposability
In order to use Bayesian scores, we also need to have parameter priors for the parameterization
parameter prior
corresponding to every possible structure. Before we discuss how to represent such priors, we
consider the desired properties from these priors.
Proposition 18.2 shows that the Bayesian score of a network structure G decomposes into
decomposable
score
a product of terms, one for each family. This is a consequence of the global parameter in-
global parameter
independence
dependence assumption.
In the case of parameter learning, this assumption was crucial for
decomposing the learning problem into independent subproblems. Can we exploit a similar
phenomenon in the case of structure learning?
In the simple example we considered in the previous section, we compared the score of two
networks G∅and GX→Y . We saw that if we choose the priors P(ΘX | G∅) and P(ΘX | GX→Y )
to be identical, the score associated with X is the same in both graphs. Thus, not only does
the score of both structures have a product form, but in the case where the same variable has
the same parents in both structures, the term associated with it also has the same value in both
scores.
Considering more general structures, if PaG
Xi = PaG′
Xi then it would seem natural that the
term that measures the score of Xi given its parents in G would be identical to the one in G′.
This seems reasonable. Recall that the score associated with Xi measures how well it can be
predicted given its parents. Thus, if Xi has the same set of parents in both structures, this term
should have the same value.
Deﬁnition 18.2
A structure score score(G : D) is decomposable if the score of a structure G can be written as
decomposable
score
score(G : D) =
X
i
FamScore(Xi | PaG
Xi : D),
where the family score FamScore(X | U : D) is a score measuring how well a set of variables
family score
U serves as parents of X in the data set D.
As an example, the likelihood score is decomposable. Using proposition 18.1, we see that in
this decomposition
FamScoreL(X | U : D) = M ·

II ˆ
P (X; U) −IH ˆ
P (X)

.

Score decomposability has important ramiﬁcations when we search for structures that
maximize the scores. The high-level intuition is that if we have a decomposable score,
then a local change in the structure (such as adding an edge) does not change the score
of other parts of the structure that remained the same. As we will see, the search algo-
rithms we consider can exploit decomposability to reduce dramatically the computational
overhead of evaluating diﬀerent structures during search.
Under what conditions is the Bayesian score decomposable?
It turns out that a natural
restriction on the prior suﬃces.
Deﬁnition 18.3
Let {P(θG | G) : G ∈GG} be a set of parameter priors that satisfy global parameter independence.
The prior satisﬁes parameter modularity if for each G, G′ such that PaG
Xi = PaG′
Xi = U, then
parameter
modularity
P(θXi|U | G) = P(θXi|U | G′).

806
Chapter 18. Structure Learning in Bayesian Networks
Parameter modularity states that the prior over the CPD of Xi depends only on the local
structure of the network (that is, the set of parents of Xi), and not on other parts of the network.
It is straightforward to see that parameter modularity implies that the score is decomposable.
Proposition 18.3
Let G be a network structure, let P(G) be a structure prior satisfying structure modularity, and
let P(θG | G) be a parameter prior satisfying global parameter independence and parameter
modularity. Then, the Bayesian score over network structures is decomposable.
18.3.6.3
Representing Parameter Priors
How do we represent our parameter priors? The number of possible structures is superexponen-
tial, which makes it diﬃcult to elicit separate parameters for each one. How do we elicit priors
for all these networks? If we require parameter modularity, the number of diﬀerent priors we
need is somewhat smaller, since we need a prior for each choice of parents for each variable.
This number, however, is still exponential.
A simpleminded approach is simply to take some ﬁxed Dirichlet distribution, for example,
Dirichlet(α, . . . , α), for every parameter, where α is a predetermined constant.
A typical
choice is α = 1. This prior is often referred to as the K2 prior, referring to the name of the
K2 prior
software system where it was ﬁrst used.
The K2 prior is simple to represent and eﬃcient to use. However, it is somewhat inconsistent.
Consider a structure where the binary variable Y has no parents. If we take Dirichlet(1, 1) for
θY , we are in eﬀect stating that our imaginary sample size is two. But now, consider a diﬀerent
structure where Y has the parent X, which has 4 values. If we take Dirichlet(1, 1) as our prior
for all parameters θY |xi, we are eﬀectively stating that we have seen two imaginary samples
in each context xi, for a total of eight. It seems that the number of imaginary samples we
have seen for diﬀerent events is a basic concept that should not vary with diﬀerent candidate
structures.
A more elegant approach is one we already saw in the context of parameter estimation: the
BDe prior. We elicit a prior distribution P ′ over the entire probability space and an equivalent
BDe prior
sample size α for the set of imaginary samples. We then set the parameters as follows:
αxi|paXi = α · P ′(xi, paXi).
This choice will avoid the inconsistencies we just discussed. If we consider the prior over
θY |xi in our example, then
αy = α · P ′(y) =
X
xi
α · P ′(y, xi) =
X
xi
αy|xi.
Thus, the number of imaginary samples for the diﬀerent choices of parents for Y will be
identical.
As we discussed, we can represent P ′ as a Bayesian network whose structure can represent
our prior about the domain structure. Most simply, when we have no prior knowledge, we set
P ′ to be the uniform distribution, that is, the empty Bayesian network with a uniform marginal
distribution for each variable. In any case, it is important to note that the network structure is
used only to provide parameter priors. It is not used to guide the structure search directly.

18.4. Structure Search
807
18.3.7
Score Equivalence ⋆
The BDe score turns out to satisfy an important property.
Recall that two networks are I-
equivalent if they encode the same set of independence statements. Hence, based on observed
independencies, we cannot distinguish between I-equivalent networks. This suggests that based
on observing data cases, we do not expect to distinguish between equivalent networks.
Deﬁnition 18.4
Let score(G
:
D) be some scoring rule. We say that it satisﬁes score equivalence if for all
score equivalence
I-equivalent networks G and G′ we have score(G : D) = score(G′ : D) for all data sets D.
In other words, score equivalence implies that all networks in the same equivalence class have
the same score. In general, if we view I-equivalent networks as equally good at describing the
same probability distributions, then we want to have score equivalence. We do not want the
score to introduce artiﬁcial distinctions when we choose networks.
Do the scores discussed so far satisfy this condition?
Theorem 18.3
The likelihood score and the BIC score satisfy score equivalence.
For a proof, see exercise 18.8 and exercise 18.9
What about the Bayesian score? It turns out that the simpleminded K2 prior we discussed is
not score-equivalent; see exercise 18.10. The BDe score, on the other hand, is score-equivalent.
In fact, something stronger can be said.
Theorem 18.4
Let P(G) be a structure prior that assigns I-equivalent networks identical prior. Let P(θG | G)
be a prior over parameters for networks with table-CPDs that satisﬁes global and local parameter
independence and where for each Xi and ui ∈Val(PaG
Xi), we have that P(θXi|ui | G) is a
Dirichlet prior. The Bayesian score with this prior satisﬁes score equivalence if and only if the prior
is a BDe prior for some choice of α and P ′.
We do not prove this theorem here. See exercise 18.11 for a proof that the BDe score in this case
satisﬁes score equivalence.
In other words, if we insist on using Dirichlet priors and also want the decomposition property,
then to satisfy score equivalence, we must use a BDe prior.
18.4
Structure Search
In the previous section, we discussed scores for evaluating the quality of diﬀerent candidate
Bayesian network structures. These included the likelihood score, the Bayesian score, and the
BIC score (which is an asymptotic approximation of the Bayesian score). We now examine how
to ﬁnd a structure with a high score.
We now have a well-deﬁned optimization problem. Our input is:
•
training set D;
•
scoring function (including priors, if needed);
•
a set GG of possible network structures (incorporating any prior knowledge).

808
Chapter 18. Structure Learning in Bayesian Networks
Our desired output is a network structure (from the set of possible structures) that maximizes
the score.
It turns out that, for this discussion, we can ignore the speciﬁc choice of score. Our search
algorithms will apply unchanged to all three of these scores.
As we will discuss, the main property of the scores that aﬀect the search is their decompos-
score
decomposability
ability. That is, we assume we can write the score of a network structure G:
score(G : D) =
X
i
FamScore(Xi | PaG
Xi : D).
Another property that is shared by all these scores is score equivalence: if G is I-equivalent to
score equivalence
G′ then score(G : D) = score(G′ : D). This property is less crucial for search, but, as we
will see, it can simplify several points.
18.4.1
Learning Tree-Structured Networks
We begin with the simplest variant of the structure learning task — the task of learning a
tree-structured network. More precisely:
Deﬁnition 18.5
A network structure G is called tree-structured if each variable X has at most one parent in G,
tree network
that is, | PaG
X |≤1.
Strictly speaking, the notion of tree-structured networks covers a broader class of graphs than
those comprising a single tree; it also covers graphs composed of a set of disconnected trees,
that is, a forest. In particular, the network of independent variables (no edges) also satisﬁes this
deﬁnition. However, as the basic structure of these networks is still a collection of trees, we
continue to use the term tree-structure.
Note that the class of trees is narrower than the class of polytrees that we discussed in
chapter 9. A polytree can have variables with multiple parents, whereas a tree cannot. In other
words, a tree-structured network cannot have v-structures.
In fact, the problem of learning
polytree-structured networks has very diﬀerent computational properties than that of learning
trees (see section 18.8).
Why do we care about learning trees? Most importantly, because unlike richer classes of
structures, they can be learned eﬃciently — in polynomial time. But learning trees can also be
useful in themselves. They are sparse, and therefore they avoid most of the overﬁtting problems
associated with more complex structures. They also capture the most important dependencies
in the distribution, and they can therefore provide some insight into the domain. They can
also provide a better baseline for approximating the distribution than the set of independent
marginals of the diﬀerent variables (another commonly used simple approximation). They are
thus often used as a starting point for learning a more complex structure, or even on their own
in cases where we cannot aﬀord signiﬁcant computational resources.
The key properties we are going to use for learning trees are the decomposability of the score
on one hand and the restriction on the number of parents on the other hand. We start by
examining the score of a network and performing slight manipulations. Instead of maximizing
the score of a tree structure G, we will try to maximize the diﬀerence between its score and the
score of the empty structure G∅. We deﬁne
∆(G) = score(G : D) −score(G∅: D).

18.4. Structure Search
809
We know that score(G∅: D) is simply a sum of terms FamScore(Xi : D) for each Xi. That
is the score of Xi if it does not have any parents. The score score(G : D) consists of terms
FamScore(Xi | PaG
Xi : D). Now, there are two cases. If PaG
Xi = ∅, then the term for Xi in
both scores cancel out. If PaG
Xi = Xj, then we are left with the diﬀerence between the two
terms. Thus, we conclude that
∆(G) =
X
i,PaG
Xi̸=∅
 FamScore(Xi | PaG
Xi : D) −FamScore(Xi : D)

.
If we deﬁne the weight
wj→i = FamScore(Xi | Xj : D) −FamScore(Xi : D),
then we see that ∆(G) is the sum of weights on pairs Xi, Xj such that Xj →Xi in G
∆(G) =
X
Xj→Xi∈G
wj→i.
We have transformed our problem to one of ﬁnding a maximum weight spanning forest in a
maximum weight
spanning forest
directed weighted graph. Deﬁne a fully connected directed graph, where each vertex is labeled
by a random variable in X, and the weight of the edge from vertex Xj to vertex Xi is wj→i,
and then search for a maximum-weight-spanning forest. Clearly, the sum of edge weights in
a forest is exactly ∆(G) of the structure G with the corresponding set of edges. The graph
structure that corresponds to that maximum-weight forest maximizes ∆(G).
How hard is the problem of ﬁnding a maximal-weighted directed spanning tree? It turns out
that this problem has a polynomial-time algorithm. This algorithm is eﬃcient but not simple.
The task becomes simpler if the score satisﬁes score equivalence. In this case, we can show
(see exercise 18.13) that wi→j = wj→i. Thus, we can examine an undirected spanning tree
(forest) problem, where we choose which edges participate in the forest, and only afterward
determine their direction. (This can be done by choosing an arbitrary root and directing all
edges away from it.) Finding a maximum spanning tree in undirected graph is an easy problem.
One algorithm for solving it is shown in algorithm A.2; an eﬃcient implementation of this
algorithm requires time complexity of O(n2 log n), where n is the number of vertices in the
graph.
Using this reduction, we end up with an algorithm whose complexity is O(n2 ·M +n2 log n)
where n is the number of variables in X and M is the number of data cases. This complexity
is a result of two stages.
In the ﬁrst stage we perform a pass over the data to collect the
suﬃcient statistics of each of the O(n2) edges. This step takes O(n2 · M) time. The spanning
tree computation requires O(n2 log n) using standard data structures, but it can be reduced
to O(n2 + n log n) = O(n2) using more sophisticated approaches. We see that the ﬁrst stage
dominates the complexity of the algorithm.
18.4.2
Known Order
We now consider a special case that also turns out to be easier than the general case. Suppose
we restrict attention to structures that are consistent with some predetermined variable ordering
variable ordering
≺over X. In other words, we restrict attention to structures G where, if Xi ∈PaG
Xj, then
Xi ≺Xj.

810
Chapter 18. Structure Learning in Bayesian Networks
This assumption was a standard one in the early work on learning Bayesian networks from
data. In some domains the ordering is indeed known in advance. For example, if there is a
clear temporal order by which the variables are assigned values, then it is natural to try to learn
a network that is consistent with the temporal ﬂow.
Before we proceed, we stress that choosing an ordering in advance may be problematic. As
we have seen in the discussion of minimal I-maps in section 3.4, a wrong choice of order can
result in unnecessarily complicated I-map. Although learning does not recover an exact I-map,
the same reasoning applies. Thus, a bad choice of order can result in poor learning result.
With this caveat in mind, assume that we select an ordering ≺; without loss of generality,
assume that our ordering is X1 ≺X2 ≺. . . ≺Xn. We want to learn a structure that maximizes
the score, but so that PaXi ⊆{X1, . . . , Xi−1}.
The ﬁrst observation we make is the following. We need to ﬁnd the network that maximizes
the score. This score is a sum of local scores, one per variable. Note that the choice of parents
for one variable, say Xi, does not restrict the choice of parents of another variable, say Xj. Since
we obey the ordering, none of our choices can create a cycle. Thus, in this scenario, learning
the parents of each variable is independent of the other variables. Stated more formally:
Proposition 18.4
Let ≺be an ordering over X, and let score(G : D) be a decomposable score. If we choose G to
be the network where
PaG
Xi = arg
max
U i⊆{Xj:Xj≺Xi} FamScore(Xi | U i : D)
for each i, then G maximizes the score among the structures consistent with ≺.
Based on this observation, we can learn the parents for each variable independently from the
parents of other variables. In other words, we now face n small learning problems.
Let us consider these learning problems. Clearly, we are forced to make X1 a root. In the
case of X2 we have a choice. We can either have the edge X1 →X2 or not. In this case,
we can evaluate the diﬀerence in score between these two options, and choose the best one.
Note that this diﬀerence is exactly the weight w1→2 we deﬁned when learned tree networks. If
w1→2 > 0, we add the edge X1 →X2; otherwise we do not.
Now consider X3. Now we have four options, corresponding to whether we add the edge
X1 →X3, and whether we add the edge X2 →X3. A naive approach to making these choices
is to decouple the decision whether to add the edge X1 →X3 from the decision about the
edge X2 →X3. Thus, we might evaluate w1→3 and w2→3 and based on these two numbers
try to decide what is the best choice of parents.
Unfortunately, this approach is ﬂawed. In general, the score FamScore(X3 | X1, X2 : D)
is not a function of FamScore(X3 | X1
: D) and FamScore(X3 | X2
: D). An extreme
example is an XOR-like CPD where X3 is a probabilistic function of the XOR of X1 and
X2.
In this case, FamScore(X3 | X1
:
D) will be small (and potentially smaller than
FamScore(X3 | : D) since the two variables are independent), yet FamScore(X3 | X1, X2 :
D) will be large. By choosing the particular dependence of X3 on the XOR of X1 and X2, we
can change the magnitude of the latter term.
We conclude that we need to consider all four possible parent sets before we choose the
parents of X3. This does not seem that bad. However, when we examine X4 we need to

18.4. Structure Search
811
consider eight parent sets, and so on. For learning the parents of Xn, we need to consider 2n−1
parent sets, which is clearly too expensive for any realistic number of variables.
In practice, we do not want to learn networks with a large number of parents. Such networks
are expensive to represent, most often are ineﬃcient to perform inference with, and, most
important, are prone to overﬁtting. So, we may ﬁnd it reasonable to restrict our attention to
networks the indegree of each variable is at most d.
If we make this restriction, our situation is somewhat more reasonable.
The number of
possible parent sets for Xn is 1 +
 n−1
1

+ . . . +
 n−1
d

= O(d
 n−1
d

) (when d < n/2). Since
the number of choices for all other variables is less than the number of choices for Xn, the
procedure has to evaluate O(dn
 n−1
d

) = O(d
 n
d

) candidate parent sets.
This number is
polynomial in n (for a ﬁxed d).
We conclude that learning given a ﬁxed order and a bound on the indegree is computationally
tractable. However, the computational cost is exponential in d. Hence, the exhaustive algorithm
that checks all parent sets of size ≤d is impractical for values of d larger than 3 or 4. When a
larger d is required, we can use heuristic methods such as those described in the next section.
18.4.3
General Graphs
What happens when we consider the most general problem, where we do not have an ordering
over the variables? Even if we restrict our attention to networks with small indegree, other
problems arise. Suppose that adding the edge X1 →X2 is beneﬁcial, for example, if the score
of X1 as a parent of X2 is higher than all other alternatives. If we decide to add this edge,
we cannot add other edges — for example, X2 →X1 — since this would introduce a cycle.
The restriction on the immediate reverse of the edge we add might not seem so problematic.
However, adding this edge also forbids us from adding together pairs of edges, such as X2 →X3
and X3 →X1. Thus, the decision on whether to add X1 →X2 is not simple, since it has
ramiﬁcations for other choices we make for parents of all the other variables.
This discussion suggests that the problem of ﬁnding the maximum-score network might be
more complex than in the two cases we examined. If fact, we can make this statement more
precise. Let d be an integer, we deﬁne GGd = {G : ∀i, |PaG
Xi| ≤d}.
Theorem 18.5
The following problem is NP-hard for any d ≥2:
Given a data set D and a decomposable score function score, ﬁnd
G∗= arg max
G∈GGd
score(G : D).
The proof of this theorem is quite elaborate, and so we do not provide it here.
Given this result, we realize that it is unlikely that there is an eﬃcient algorithm that constructs
the highest-scoring network structure for all input data sets. Unlike the situation in inference,
for example, the known intermediate situations where the problem is easier are not the ones we
usually encounter in practice; see exercise 18.14.
As with many intractable problems, this is not the end of the story. Instead of aiming for
an algorithm that will always ﬁnd the highest-scoring network, we resort to heuristic algorithms
that attempt to ﬁnd the best network but are not guaranteed to do so. In our case, we are

812
Chapter 18. Structure Learning in Bayesian Networks
faced with a combinatorial optimization problem; we need to search the space of graphs (with
bounded indegree) and return a high-scoring one. We solve this problem using a local search
local search
approach. To do so, we deﬁne three components: a search space, which deﬁnes the set of
candidate network structures; a scoring function that we aim to maximize (for example, the
BDe score given the data and priors); and the search procedure that explores the search space
without necessarily seeing all of it (since it is superexponential in size).
18.4.3.1
The Search Space
We start by considering the search space.
As discussed in appendix A.4.2, we can think of
search space
a search space as a graph over candidate solutions, connected by possible operators that the
search procedure can perform to move between diﬀerent solutions. In the simplest setting, we
consider the search space where each search state denotes a complete network structure G over
X. This is the search space we discuss for most of this chapter. However, we will see other
formulations for search spaces.
A crucial design choice that has large impact on the success of heuristic search is how the
space is interconnected.
If each state has few neighbors, then the search procedure has to
consider only a few options at each point of the search. Thus, it can aﬀord to evaluate each
of these options. However, this comes at a price. Paths from the initial solution to a good one
might be long and complex. On the other hand, if each state has many neighbors, we may be
able to move quickly from the initial state to a good state, but it may be diﬃcult to determine
which step to take at each point in the search.
A good trade-oﬀfor this problem chooses
reasonably few neighbors for each state but ensures that the “diameter” of the search space
remains small. A natural choice for the neighbors of a state representing a network structure is
a set of structures that are identical to it except for small “local” modiﬁcations. Thus, we deﬁne
the connectivity of our search space in terms of operators such as:
search operators
•
edge addition;
edge addition
•
edge deletion;
edge deletion
•
edge reversal.
edge reversal
In other words, the states adjacent to a state G are those where we change one edge, either
by adding one, deleting one, or reversing the orientation of one. Note that we only consider
operations that result in legal networks. That is, acyclic networks that satisfy the constraints we
put in advance (such as indegree constraints).
This deﬁnition of search space is quite natural and has several desirable properties. First,
notice that the diameter of the search space is at most n2. That is, there is a relatively short
path between any two networks we choose. To see this, note that if we consider traversing a
path from G1 to G2, we can start by deleting all edges in G1 that do not appear in G2, and then
we can add the edges that are in G2 and not in G1. Clearly, the number of steps we take is
bounded by the total number of edges we can have, n2.
Second, recall that the score of a network G is a sum of local scores. The operations we
consider result in changing only one local score term (in the case of addition or deletion of an
edge) or two (in the case of edge reversal). Thus, they result in a local change in the score; most
components in the score remain the same. This implies that there is some sense of “continuity”
in the score of neighboring networks.

18.4. Structure Search
813
A
B
D
C
A
B
D
C
A
B
D
(a)
(b)
(c)
C
Figure 18.5
Example of a search problem requiring edge deletion. (a) original network that generated
the data. (b) and (c) intermediate networks encountered during the search.
The choice of the three particular operations we consider also needs some justiﬁcation. For
example, if we always start the search from the empty graph G∅, we may wonder why we include
the option to delete an edge. We can reach every network by adding the appropriate arcs to the
empty network. In general, however, we want the search space to allow us to reverse our choices.
As we will see, this is an important property in escaping local maxima (see appendix A.4.2).
However, the ability to delete edges is important even if we perform only “greedy” operations
that lead to improvement. To see this, consider the following example. Suppose the original
network is the one shown in ﬁgure 18.5a, and that A is highly informative about both C and
B. Starting from an empty network and adding edges greedily, we might add the edges A →B
and A →C. However, in some data sets, we might add the edge A →D. To see why, we
need to realize that A is informative about both B and C, and since these are the two parents
of D, also about D. Now B and C are also informative about D. However, each of them
provides part of the information, and thus neither B nor C by itself is the best parent of D. At
this stage, we thus end up with the network ﬁgure 18.5b. Continuing the search, we consider
diﬀerent operators, adding the edge B →D and C →D. Since B is a parent of D in the
original network, it will improve the prediction of D when combined with A. Thus, the score
of A and B together as parents of D can be larger than the score of A alone. Similarly, if there
are enough data to support adding parameters, we will also add the edge C →D, and reach
the structure shown in ﬁgure 18.5c. This is the correct structure, except for the redundant edge
A →D. Now the ability to delete edges comes in handy. Since in the original distribution
B and C together separate A from D, we expect that choosing B, C as the parents of D will
have higher score than choosing A, B, C. To see this, note that A cannot provide additional
information on top of what B and C convey, and having it as an additional parent results in a
penalty. After we delete the edge A →D we get the original structure.
A similar question can be raised about the edge reversal operator. Clearly, we can achieve
the eﬀect of reversing an edge X →Y in two steps, ﬁrst deleting the edge X →Y and then
adding the edge Y →X. The problem is that when we delete the edge X →Y , we usually
reduce the score (assuming that there is some dependency between X and Y ). Thus, these two
operations require us to go “downhill” in the ﬁrst step in order to get to a better structure in the
next step. The reverse operation allows us to realize the trade-oﬀbetween a worse parent set
for Y and a better one for X.

814
Chapter 18. Structure Learning in Bayesian Networks
A
C
B
(a)
A
C
B
(b)
A
C
B
(d)
A
C
B
(c)
Figure 18.6
Example of a search problem requiring edge reversal. (a) original network that generated
the data. (b) and (c) intermediate networks encountered during the search. (d) an undesirable outcome.
To see the utility of the edge reversal operator, consider the following simple example. Suppose
the real network generating the data has the v-structure shown in ﬁgure 18.6a. Suppose that
the dependency between A and C is stronger than that between B and C. Thus, a ﬁrst step
in a greedy-search procedure would add an edge between A and C. Note, however, that score
equivalence implies that the network with the edge A →C has exactly the same score as the
network with the edge C →A. At this stage, we cannot distinguish between the two choices.
Thus, the decision between them is arbitrary (or, in some implementations, randomized). It
is thus conceivable that at this stage we have the network shown in ﬁgure 18.6b. The greedy
procedure proceeds, and it decides to add the edge B →C, resulting in the network of
ﬁgure 18.6c. Now we are in the position to realize that reversing the edge C →A can improve
the score (since A and B together should make the best predictions of C). However, if we do
not have a reverse operator, a greedy procedure would not delete the edge C →A, since that
would deﬁnitely hurt the score.
In this example, note that when we do not perform the edge reversal, we might end up with
the network shown in ﬁgure 18.6d. To realize why, recall that although A and B are marginally
independent, they are dependent given C. Thus, B and C together make better predictions of
A than C alone.
18.4.3.2
The Search Procedure
Once we deﬁne the search space, we need to design a procedure to explore it and search
for high-scoring states. There is a wide literature on heuristic search. The vast majority of
the search methods used in structure learning are local search procedures such as greedy hill
local search
climbing, as described in appendix A.4.2. In the structure-learning setting, we pick an initial
network structure G as a starting point; this network can be the empty one, a random choice,
the best tree, or a network obtained from some prior knowledge. We compute its score. We
then consider all of the neighbors of G in the space — all of the legal networks obtained by
applying a single operator to G — and compute the score for each of them. We then apply
the change that leads to the best improvement in the score. We continue this process until no
modiﬁcation improves the score.
There are two questions we can ask. First, how expensive is this process, and second, what
can we say about the ﬁnal network it returns?

18.4. Structure Search
815
Computational Cost
We start by brieﬂy considering the time complexity of the procedure. At
each iteration, the procedure applies |O| operators and evaluates the resulting network. Recall
that the space of operators we consider is quadratic in the number of variables. Thus, if we
perform K steps before convergence, then we perform O(K · n2) operator applications. Each
operator application involves two steps. First, we need to check that the network is acyclic.
This check can be done in time linear in the number of edges. If we are considering networks
with indegree bounded by d, then there are at most nd edges. Second, if the network is legal,
we need to evaluate it. For this, we need to collect suﬃcient statistics from the data. These
might be diﬀerent for each network and require O(M) steps, and so our rough time estimate is
O(K ·n2 ·(M +nd)). The number of iterations, K, varies and depends on the starting network
and on how diﬀerent the ﬁnal network is. However, we expect it not to be much larger than n2
(since this is the diameter of the search space). We emphasize that this is a rough estimate, and
not a formal statement. As we will show, we can make this process faster by using properties of
the score that allow for smart caching.
When n is large, considering O(n2) neighbors at each iteration may be too costly. How-
ever, most operators attempt to perform a rather bad change to the network. So can we skip
evaluating them? One way of avoiding this cost is to use search procedures that replace the
exhaustive enumeration in line 5 of Greedy-Local-Search (algorithm A.5) by a randomized choice
of operators. This ﬁrst-ascent hill climbing procedure samples operators from O and evaluates
ﬁrst-ascent hill
climbing
them one by one. Once it ﬁnds one that leads to a better-scoring network, it applies it without
considering other operators. In the initial stages of the search, this procedure requires relatively
few random trials before it ﬁnds such an operator. As we get closer to the local maximum, most
operators hurt the score, and more trials are needed before an upward step is found (if any).
Local Maxima
What can we say about the network returned by a greedy hill-climbing search
procedure? Clearly, the resulting network cannot be improved by applying a single operator
(that is, changing one edge). This implies that we are in one of two situations. We might have
reached a local maximum from which all changes are score-reducing. The other option is that
local maximum
we have reached a plateau: a large set of neighboring networks that have the same score. By
plateau
design, the greedy hill-climbing procedure cannot “navigate” through a plateau, since it relies on
improvement in score to guide it to better structures.
Upon reﬂection, we realize that greedy hill climbing will encounter plateaus quite often. Recall
that we consider scores that satisfy score equivalence. Thus, all networks in an I-equivalence
I-equivalence
class will have the same score. Moreover, as shown in theorem 3.9, the set of I-equivalent
networks forms a contiguous region in the space, which we can traverse using a set of covered
edge-reversal operations. Thus, any I-equivalence class necessarily forms a plateau in the search
space.
Recall that equivalence classes can potentially be exponentially large. Ongoing work stud-
ies the average size of an equivalence class (when considering all networks) and the actual
distributions of sizes encountered in realistic situations, such as during structure search.
It is clear, however, that most networks we encounter have at least a few equivalent networks.
Thus, we conclude that most often, greedy hill climbing will converge to an equivalence class.
There are two possible situations: Either there is another network in this equivalence class from
which we can continue the upward climb, or the whole equivalence class is a local maximum.
Greedy hill climbing cannot deal with either situation, since it cannot explore without upward

816
Chapter 18. Structure Learning in Bayesian Networks
indications.
As we discussed in appendix A.4.2, there are several strategies to improve on the network
G returned by a greedy search algorithm. One approach that deals with plateaus induced by
equivalence classes is to enumerate explicitly all the network structures that are I-equivalent to
G, and for each one to examine whether it has neighbors with higher score. This enumeration,
however, can be expensive when the equivalence class is large. An alternative solution, described
in section 18.4.4, is to search directly over the space of equivalence classes. However, both of
these approaches save us from only some of the plateaus, and not from local maxima.
Appendix A.4.2 describes other methods that help address problem of local maxima. For
example, basin ﬂooding keeps track of all previous networks and considers any operator leading
basin ﬂooding
from one of them to a structure that we have not yet visited. A key problem with this approach
is that storing the list of networks we visited in the recent past can be expensive (recall that
greedy hill climbing stores just one copy of the network). Moreover, we do not necessarily want
to explore the whole region surrounding a local maximum, since it contains many variants of
the same network. To see why, suppose that three diﬀerent edges in a local maximum network
can be removed with very little change in score. This means that all seven networks that contain
at least one deletion will be explored before a more interesting change will be considered.
A method that solves both problems is the tabu search of algorithm A.6. Recall that this
tabu search
procedure keeps a list of recent operators we applied, and in each step we do not consider
operators that reverse the eﬀect of recently applied operators. Thus, once the search decides to
add an edge, say X →Y , it cannot delete this edge in the next L steps (for some prechosen
L). Similarly, once an arc is reversed, it cannot be reversed again. As for the basin-ﬂooding
approach, tabu search cannot use the termination criteria of greedy hill climbing. Since we want
the search to proceed after reaching the local maxima, we do not want to stop when the score
of the current candidate is smaller than the previous one. Instead, we continue the search with
the hope of reaching a better structure. If this does not happen after a prespeciﬁed number of
steps, we decide to abandon the search and select the best network encountered at any time
during the search.
Finally, as we discussed, one can also use randomization to increase our chances of escaping
local maxima. In the case of structure learning, these methods do help. In particular, simulated
annealing was reported to outperform greedy hill-climbing search. However, in typical example
domains (such as the ICU-Alarm domain) it appears that simple methods such as tabu search
with random restarts ﬁnd higher-scoring networks much faster.
Data Perturbation Methods
So far, we have discussed only the application of general-purpose
local search methods to the speciﬁc problem of structure search. We now discuss one class of
methods — data-perturbation methods — that are more speciﬁc to the learning task.
The
data perturbation
idea is similar to random restarts: We want to perturb the search in a way that will allow
it to overcome local obstacles and make progress toward the global maxima. Random restart
methods achieve this perturbation by changing the network. Data perturbation methods, on the
other hand, change the training data.
To understand the idea, consider a perturbation that duplicates some instances (say by
random choice) and removes others (again randomly).
If we do a reasonable number of
these modiﬁcations, the resulting data set D′ has most of the characteristics of the original
data set D.
For example, the value of suﬃcient statistics in the perturbed data are close

18.4. Structure Search
817
Algorithm 18.1 Data perturbation search
Procedure Search-with-Data-Perturbation (
G∅,
// initial network structure
D
// Fully observed data set
score,
// Score
O,
// A set of search operator
Search,
// Search procedure
t0,
// Initial perturbation size
γ,
// Reduction in perturbation size
)
1
G ←Search(G∅, D, score, O)
2
Gbest ←G
3
t ←t0
4
for i = 1, . . . until convergence
5
D′ ←Perturb(D, t)
6
G ←Search(G, D′, score, O)
7
if score(G : D) > score(Gbest : D) then
8
Gbest ←G
9
t ←γ · t
10
11
return Gbest
to the values in the original data.
Thus, we expect that big diﬀerences between networks
are preserved.
That is, if score(GX→Y
:
D) ≫score(G2
:
D), then we expect that
score(GX→Y
: D′) ≫score(G2 : D′). On the other hand, the perturbation does change the
comparison between networks that are similar. The basic intuition is that the score using D′
has the same broad outline as the score using D, yet might have diﬀerent ﬁne-grained topology.
This suggests that a structure G that is a local maximum when using the score on D is no
longer a local maximum when using D′. The magnitude of perturbation determines the level of
details that are preserved after the perturbation.
We note that instead of duplicating and removing instances, we can achieve perturbation
by weighting data instances. Much of the discussion on scoring networks and related topics
weighted data
instances
applies without change if we assign weight to each instance. Formally, the only diﬀerence is
the computation of suﬃcient statistics. If we have weights w[m] for the m’th instance, then the
suﬃcient statistics are redeﬁned as:
M[z] =
X
m
11{Z[m] = z} · w[m].
Note that when w[m] = 1, this reduces to the standard deﬁnition of suﬃcient statistics. Instance
duplication and deletion lead to integer weights. However, we can easily consider perturbation
that results in fractional weights. This leads to a continuous spectrum of data perturbations that

818
Chapter 18. Structure Learning in Bayesian Networks
range from small changes to weights to drastic ones.
The actual search procedure is shown in algorithm 18.1. The heart of the procedure is the
Perturb function. This procedure can implemented in diﬀerent ways. A simple approach is
to sample each w[m] for a distribution whose variance is dictated by t, for example, using a
Gamma distribution, with mean 1 and variance t. (Note that we need to use a distribution
that attains nonnegative values. Thus, the Gamma distribution is more suitable than a Gaussian
distribution.)
18.4.3.3
Score Decomposition and Search
The discussion so far has examined how generic ideas in heuristic search apply to structure
learning. We now examine how the particulars of the problem impact the search.
The dominant factor in the cost of the search algorithm is the evaluation of neighboring
networks at each stage. As discussed earlier, the number of such networks is approximately
n2. To evaluate each of these network structures, we need to score them. This process requires
that we traverse all the diﬀerent data cases, computing suﬃcient statistics relative to our new
structure. This computation can get quite expensive, and it is the dominant cost in any structure
learning algorithm.
This key task is where the score decomposability property turns out to be useful. Recall that
score
decomposability
the scores we examine decompose into a sum of terms, one for each variable Xi. Each of these
family scores is computed relative only to the variables in the family of Xi. A local change
— adding, deleting, or reversing an edge — leaves almost all of the families in the network
unchanged. (Adding and deleting changes one family, and reversing changes two.) For families
whose composition does not change, the associated component of the score also does not
change. To understand the importance of this observation, assume that our current candidate
network is G. For each operator, we compute the improvement in the score that would result in
making that change. We deﬁne the delta score
delta score
δ(G : o) = score(o(G) : D) −score(G : D)
to be the change of score associated with applying o on G. Using score decomposition, we can
compute this quantity relatively eﬃciently.
Proposition 18.5
Let G be a network structure and score be a decomposable score.
• If o is “Add X →Y ,” and X →Y ̸∈G, then
δ(G : o) = FamScore(Y, PaG
Y ∪{X} : D) −FamScore(Y, PaG
Y : D).
• If o is “Delete X →Y ” and X →Y ∈G, then
δ(G : o) = FamScore(Y, PaG
Y −{X} : D) −FamScore(Y, PaG
Y : D).
• If o is “Reverse X →Y ” and X →Y ∈G, then
δ(G : o)
=
FamScore(X, PaG
X ∪{Y } : D) + FamScore(Y, PaG
Y −{X} : D)
−FamScore(X, PaG
X : D) −FamScore(Y, PaG
Y : D).

18.4. Structure Search
819
See exercise 18.18.
Note that these computations involve only the suﬃcient statistics for the particular family
that changed. This requires a pass over only the appropriate columns in the table describing the
training data.
Now, assume that we have an operator o, say “Add X →Y ,” and instead of applying this
edge addition, we have decided to apply another operator o′ that changes the family of some
variable Z (for Z ̸= Y ), producing a new graph G′. The key observation is that δ(G′
:
o)
remains unchanged — we do not need to recompute it. We need only to recompute δ(G′ : o′)
for operators o′ that involve Y .
Proposition 18.6
Let G and G′ be two network structures and score be a decomposable score.
• If o is either “Add X →Y ” or “Delete X →Y ” and PaG
Y = PaG′
Y , then δ(G : o) = δ(G′ :
o).
• If o is “Reverse X →Y ,” PaG
Y = PaG′
Y , and PaG
X = PaG′
X , then δ(G : o) = δ(G′ : o).
See exercise 18.19.
This shows that we can cache the computed δ(G
:
o) for diﬀerent operators and then
reuse most of them in later search steps. The basic idea is to maintain a data structure that
records for each operator o the value of δ(G : o) with respect to the current network G. After
we apply a step in the search, we have a new current network, and we need to update this
data structure. Using proposition 18.6 we see that most of the computed values do not need
to be changed. We need to recompute δ(G′
: o) only for operators that modify one of the
families that we modiﬁed in the recent step. By careful data-structure design, this cache can
save us a lot of computational time; see box 18.A for details. Overall, the decomposability

of the scoring function provides signiﬁcant reduction in the amount of computation that
we need to perform during the search. This observation is critical to making structure
search feasible for high-dimensional spaces.
Box 18.A — Skill: Practical Collection of Suﬃcient Statistics. The passes over the training data
required to compute suﬃcient statistics generally turn out to be the most computationally intensive
part of structure learning. It is therefore crucial to take advantage of properties of the score in order
to ensure eﬃcient computations as well as use straightforward organizational tricks.
One important source of computational savings derives from proposition 18.6. As we discussed,
this proposition allows us to avoid recomputing many of the delta-scores after taking a step in the
search. We can exploit this observation in a variety of ways. For example, if we are performing
greedy hill climbing, we know that the search will necessarily examine all operators. Thus, after
each step we can update the evaluation of all the operators that were “damaged” by the last move.
The number of such operators is O(n), and so this requires O(n · M) time (since we need to
collect suﬃcient statistics from data). Moreover, if we keep the score of diﬀerent operators in a heap,
we spend O(n log n) steps to update the heap but then can retrieve the best operator in constant
time. Thus, although the cost of a single step in the greedy hill-climbing procedure seems to involve
quadratic number of operations of O(n2 · M), we can perform it in time O(n · M + n log n).
We can further reduce the time consumed by the collection of suﬃcient statistics by considering
additional levels of caching. For example, if we use table-CPDs, then the counts needed for evalu-

820
Chapter 18. Structure Learning in Bayesian Networks
2
1.5
1
0.5
0
0
500 1000 1500 2000 2500
5000
3000
KL Divergence
3500 4000 4500
Parameter learning
Structure learning
M
Figure 18.7
Performance of structure and parameter learning for instances generated from the
ICU-Alarm network. The graph shows the KL-divergence of the learned to the true network, and compares
two learning tasks: learning the parameters only, using a correct network structure, and learning both
parameters and structure. The curves shows average performance over 10 data sets of the same size, with
error bars showing +/- one standard deviation. The error bars for parameter learning are much smaller and
are not shown.
ating X as a parent of Y and the counts needed to evaluate Y as a parent of X are the same.
Thus, we can save time by caching previously computed counts, and also by marginalizing counts
such as M[x, y] to compute M[x]. A more elaborate but potentially very eﬀective approach is one
where we plan the collection of the entire set of suﬃcient statistics needed. In this case, we can
use eﬃcient algorithms for the set cover problem to choose a smaller set of suﬃcient statistics that
covers all the needed computations. There are also eﬃcient data structures (such as AD-trees for
discrete spaces and KD-trees or metric trees for continuous data) that are designed explicitly for
maintaining and retrieving suﬃcient statistics; these data structures can signiﬁcantly improve the
performance of the algorithm, particularly when we are willing to approximate suﬃcient statistics
in favor of dramatic speed improvements.
One cannot overemphasize the importance of these seemingly trivial caching tricks. In practice,
learning the structure of a network without making use of such tricks is infeasible even for a modest
number of variables.
18.4.3.4
Empirical Evaluation
In practice, relatively cheap and simple algorithms, such as tabu search, work quite well. Fig-
ure 18.7 shows the results of learning a network from data generated from the ICU-Alarm network.
The graph shows the KL-divergence to the true network and compares two learning tasks: learn-
ing the parameters only, using a correct network structure, and learning both parameters and
structure. Although the graph does show that it is harder to recover both the structure and

18.4. Structure Search
821
the parameters, the diﬀerence in the performance achieved on the two tasks is surprisingly
small. We see that structure learning is not necessarily a harder task than parameter estimation,
although computationally, of course, it is more expensive. We note, however, that even the com-
putational cost is not prohibitive. Using simple optimization techniques (such as tabu search
with random restarts), learning a network with a hundred variables takes a few minutes on a
standard machine.
We stress that the networks learned for diﬀerent sample sizes in ﬁgure 18.7 are not the same
as the original networks. They are usually simpler (with fewer edges). As the graph shows, they
perform quite similarly to the real network. This means that for the given data, these networks
seem to provide a better score, which means a good trade-oﬀbetween complexity and ﬁt to the
data. As the graph suggests, this estimate (based on training data) is quite reasonable.
18.4.4
Learning with Equivalence Classes ⋆
The preceding discussion examined diﬀerent search procedures that attempt to escape local
maxima and plateaus in the search space.
An alternative approach to avoid some of these
pitfalls is to change the search space. In particular, as discussed, many of the plateaus we
encounter during the search are a consequence of score equivalence — equivalent networks
have equivalent scores.
This observation suggests that we can avoid these plateaus if we
consider searching over equivalence classes of networks.
To carry out this idea, we need to examine carefully how to construct the search space.
Recall that an equivalence class of networks can be exponential in size.
Thus, we need a
compact representation of states (equivalence classes) in our search space.
Fortunately, we
already encountered such a representation. Recall that a class PDAG is a partially directed graph
class PDAG
that corresponds to an equivalence class of networks. This representation is relatively compact,
and thus, we can consider the search space over all possible class PDAGs.
Next, we need to answer the question how to score a given class PDAG. The scores we
discussed are deﬁned for over network structures (DAGs) and not over PDAGs. Thus, to score
a class PDAG K, we need to build a network G in the equivalence class represented by K and
then score it. As we saw in section 3.4.3.3, this is a fairly straightforward procedure.
Finally, we need to decide on our search algorithm. Once again, we generally resort to a
hill-climbing search using local graph operations. Here, we need to deﬁne appropriate search
operations on the space of PDAGs. One approach is to use operations at the level of PDAGs.
In this case, we need operations that add, remove, and reverse edges; moreover, since PDAGs
contain both directed edges and undirected ones, we may wish to consider operations such as
adding an undirected edge, orienting an undirected edge, and replacing a directed edge by an
undirected one. An alternative approach is to use operators in DAG space that are guaranteed
to change the equivalence class. In particular, consider an equivalence class E (represented as a
class PDAG). We can deﬁne as our operators any step that takes a DAG G ∈E, adds or deletes
an edge from G to produce a new DAG G′, and then constructs the equivalence class E′ for
G′ (represented again as a class PDAG). Since both edge addition and edge deletion change the
skeleton, we are guaranteed that E and E′ are distinct equivalence classes.
One algorithm based on this last approach is called the GES algorithm, for greedy equivalence
GES algorithm
search. GES starts out with the equivalence class for the empty graph and then takes greedy
edge-addition steps until no additional edge-addition steps improve the score. It then executes

822
Chapter 18. Structure Learning in Bayesian Networks
the reverse procedure, removing edges one at a time until no additional edge-removal steps
improve the score.
When used with a consistent score (as in deﬁnition 18.1), this simple two-pass algorithm has
consistent score
some satisfying guarantees. Assume that our distribution P ∗is faithful for the graph G∗over
X; thus, as in section 18.2, there are no spurious independencies in P ∗. Moreover, assume that
we have (essentially) inﬁnite data. Under these assumptions, our (consistent) scoring function
gives only the correct equivalence class — the equivalence class of G∗— the highest score. For
this setting, one can show that GES is guaranteed to produce the equivalence class of G∗as its
output.
Although the assumptions here are fairly strong, this result is still important and satisfying.
Moreover, empirical results suggest that GES works reasonably well even when some of its
assumptions are violated (to an extent).
Thus, it also provides a reasonable alternative in
practice.
Although simple in principle, there are two signiﬁcant computational issues associated with
GES and other algorithms that work in the space of equivalence classes. The ﬁrst is the cost of
generating the equivalence classes that result from the local search operators discussed before.
The second is the cost of evaluating their scores while reusing (to the extent possible) the
suﬃcient statistics from our current graph. Although nontrivial (and outside the scope of this
book), local operations that address both of these tasks have been constructed, making this
algorithm a computationally feasible alternative to search over DAG space.
Box 18.B — Concept: Dependency Networks. An alternative formalism for parameterizing a
Markov network is by associating with each variable Xi a conditional probability distribution
(CPD) Pi(Xi | X −{Xi}) = Pi(Xi | MBH(Xi)). Networks parameterized in this way are
sometimes called dependency networks and are drawn as a cyclic directed graph, with edges to
dependency
networks
each variable from all of the variables in its Markov blanket.
This representation oﬀers certain trade-oﬀs over other representations. In terms of semantics, a
key limitation of this parameterization is that a set of CPDs {Pi(Xi | MBH(Xi)) : Xi ∈X}
may not be consistent with any probability distribution P; that is, there may not be a distribution
P such that Pi(Xi | MBH(Xi)) = P(Xi | MBH(Xi)) for all i (hence the use of the subscript i
on Pi). Moreover, determining whether such a set of CPDs is consistent with some distribution P
is a computationally diﬃcult problem. Thus, eliciting or learning a consistent dependency network
can be quite diﬃcult, and the semantics of an inconsistent network is unclear.
However, in a noncausal domain, dependency networks arguably provide a more appropriate
representation of the dependencies in the distribution than a Bayesian network. Certainly, for a lay
user, understanding the notion of a Markov blanket in a Bayesian network is not trivial. On the
other hand, in comparison to Markov networks, the CPD parameterization is much more natural
and easy to understand. (As we discussed, there is no natural interpretation for a Markov network
factor in isolation.)
From the perspective of inference, dependency networks provide a very easy mechanism for
answering queries where all the variables except for a single query variable are observed (see
box 18.C). However, answering other queries is not as obvious. The representation lends itself very
nicely to Gibbs sampling, which requires precisely the distribution of individual variables given their
Markov blanket. However, exact inference requires that we transform the network to a standard

18.4. Structure Search
823
Law & Order
Frasier
NBC Monday
Night Movies
Mad About You
Beverly Hills 90210
Seinfeld
Friends
Melrose Place
Models Inc.
Figure 18.C.1 — Learned Bayesian network for collaborative ﬁltering. A fragment of a Bayesian
network for collaborative ﬁltering, learned from Nielsen TV rating data, capturing the viewing record
of sample viewers. The variables denote whether a TV program was watched.
parameterization, a task that requires a numerical optimization process.
The biggest advantage arises in the learning setting. If we are willing to relax the consistency
requirement, the problem of learning such networks from data becomes quite simple: we simply have
to learn a CPD independently for each variable, a task to which we can apply a wide variety of
standard supervised learning algorithms. In this case, however, it is arguable whether the resulting
network can be considered a uniﬁed probabilistic model, rather than a set of stand-alone predictors
for individual variables.
Box 18.C — Case Study: Bayesian Networks for Collaborative Filtering. In many marketing
settings, we want to provide to a user a recommendation of an item that he might like, based on
previous items that he has bought or liked. For example, a bookseller might want to recommend
books that John might like to buy, using John’s previous book purchases. Because we rarely have
enough data for any single user to determine his or her preferences, the standard solution is an
approach called collaborative ﬁltering, which uses the observed preferences of other users to try to
collaborative
ﬁltering
determine the preferences for any other user. There are many possible approaches to this problem,
including ones that explicitly try to infer key aspects of a user’s preference model.
One approach is to learn the dependency structure between diﬀerent purchases, as observed in
the population. We treat each item i as a variable Xi in a joint distribution, and each user as an
instance. Most simply, we view a purchase of an item i (or some other indication of preference) as
one value for the variable Xi, and the lack of a purchase as a diﬀerent value. (In certain settings,

824
Chapter 18. Structure Learning in Bayesian Networks
we may get explicit ratings from the user, which can be used instead.) We can then use structure
learning to obtain a Bayesian network model over this set of random variables.
Dependency
networks (see box 18.B) have also been used for this task; these arguably provide a more intuitive
visualization of the dependency model to a lay user.
Both models can be used to address the collaborative ﬁltering task. Given a set of purchases for
a set of items S, we can compute the probability that the user would like a new item i. In general,
this question is reduced to a probabilistic inference task where all purchases other than S and i
are set to false; thus, all variables other than the query variable Xi are taken to be observed. In a
Bayesian network, this query can be computed easily by simply looking at the Markov blanket of
Xi. In a dependency network, the process is even simpler, since we need only consider the CPD for
Xi. Bayesian networks and Markov networks oﬀer diﬀerent trade-oﬀs. For example, the learning
and prediction process for dependency networks is somewhat easier, and the models are arguably
more understandable. However, Bayesian networks allow answering a broader range of queries —
for example, queries where we distinguish between items that the user has viewed and chosen not
to purchase and items that the user simply has not viewed (whose variables arguably should be
taken to be unobserved).
Heckerman et al. (2000) applied this approach to a range of diﬀerent collaborative ﬁltering data
sets. For example, ﬁgure 18.C.1 shows a fragment of a Bayesian network for TV-watching habits
learned from Nielsen viewing data. They show that both the Bayesian network and the dependency
network methods performed signiﬁcantly better than previous approaches proposed for this task.
The performance of the two methods in terms of predictive accuracy is roughly comparable, and
both were ﬁelded successfully as part of Microsoft’s E-Commerce software system.
18.5
Bayesian Model Averaging ⋆
18.5.1
Basic Theory
We now reexamine the basic principles of the learning problem. Recall that the Bayesian method-
ology suggests that we treat unknown parameters as random variables and should consider all
possible values when making predictions. When we do not know the structure, the Bayesian
methodology suggests that we should consider all possible graph structures. Thus, according to
Bayesian theory, given a data set D = {ξ[1], . . . , ξ[M]} we should make predictions according
to the Bayesian estimation rule:
Bayesian
estimation
P(ξ[M + 1] | D) =
X
G
P(ξ[M + 1] | D, G)P(G | D),
(18.10)
where P(G | D) is posterior probability of diﬀerent networks given the data
P(G | D) = P(G)P(D | G)
P(D)
.
In our discussion so far, we searched for a single structure G that maximized the Bayesian score,
and thus also the posterior probability. When is the focus on a single structure justiﬁed?

18.5. Bayesian Model Averaging ⋆
825
Recall that the logarithm of the marginal likelihood log P(D | G) grows linearly with the
number of samples. Thus, when M is large, there will be large diﬀerences between the top-
scoring structure and all the rest. We used this property in the proof of theorem 18.2 to show
that for asymptotically large M, the best-scoring structure is the true one. Even when M is
not that large, the posterior probability of this particular equivalence class of structures will be
exponentially larger than all other structures and dominate most of the mass of the posterior.
In such a case, the posterior mass is dominated by this single equivalence class, and we can
approximate equation (18.10) with
P(ξ[M + 1] | D) ≈P(ξ[M + 1] | D, ˜G),
where ˜G = arg maxG P(G | D). The intuition is that P( ˜G | D) ≈1, and P(G | D) ≈0 for any
other structure.
What happens when we consider learning with smaller number of samples?
In such a
situation, the posterior mass might be distributed among many structures (which may not
include the true structure). If we are interested only in density estimation, this might not be a
serious problem. If P(ξ[M + 1] | D, G) is similar for the diﬀerent structure with high posterior,
then picking one of them will give us reasonable performance. Thus, if we are doing density
estimation, then we might get away with learning a single structure and using it. However, we
need to remember that the theory suggests that we consider the whole set of structures when
making predictions.

If we are interested in structure discovery, we need to be more careful. The fact that
structure
discovery
several networks have similar scores suggests that one or several of them might be close
to the “true” structure. However, we cannot really distinguish between them given the
data. If this is the situation, then we should not be satisﬁed with picking one of these
structures (say, even the one with the best score) and drawing conclusions about the
domain. Instead, we want to be more cautious and quantify our conﬁdence about the
conclusions we make. Such conﬁdence estimates are crucial in many domains where we
conﬁdence
estimation
use Bayesian network learning to learn about the structure of processes that generated
data. This is particularly true when the available data is limited.
To consider this problem, we need to specify more precisely what would help us understand
the posterior over structures. In most cases, we do not want to quantify the posterior explicitly.
Moreover, the set of networks with “high” posterior probability is usually large. To deal with
this issue, there are various approaches we can take. A fairly general one is to consider network
network features
feature queries. Such a query can ask what is the probability that an edge, say X →Y , appears
in the “true” network. Another possible query might be about separation in the “true” network
— for example, whether d-sep(X; Y | Z) holds in this network. In general, we can formulate
such a query as a function f(G). For a binary feature, f(G) can return 1 when the network
structure contains the feature, and 0 otherwise. For other features, f(G) may return a numerical
quantity that is relevant to the graph structure (such as the length of the shortest trail from X
to Y , or the number of nodes whose outdegree is greater than some threshold k).
Given a numerical feature f(G), we can compute its expectation over all possible network
structures G:
IEP (G|D)[f(G)] =
X
G
f(G)P(G | D).
(18.11)

826
Chapter 18. Structure Learning in Bayesian Networks
In particular, for a binary feature f, this quantity is simply the posterior probability P(f | D).
The problem in computing either equation (18.10) or equation (18.11), of course, is that the
number of possible structures is superexponential; see exercise 18.20.
We can reduce this number by restricting attention to structures G where there is a bound d
on the number of parents per variable. This assumption, which we will make throughout this
section, is a fairly innocuous one. There are few applications in which very large families are
called for, and there are rarely enough data to support robust parameter estimation for such
families. From a more formal perspective, networks with very large families tend to have low
score. Let GGd be the set of all graphs with indegree bounded by some constant d. Note that the
number of structures in GGd is still superexponential; see exercise 18.20.
Thus, an exhaustive enumeration over the set of possible network structures is feasible only
for tiny domains (4–5 variables). In the next sections we consider several approaches to address
this problem. In the following discussion, we assume that we are using a Bayesian score that
decomposes according to the assumptions we discussed in section 18.3. Recall that the Bayesian
score, as deﬁned earlier, is equal to log P(D, G). Thus, we have that
P(D, G) =
Y
i
exp{FamScoreB(Xi | PaG
Xi : D)}.
(18.12)
18.5.2
Model Averaging Given an Order
In this section, we temporarily turn our attention to a somewhat easier problem. Rather than
perform model averaging over the space of all structures, we restrict attention to structures that
are consistent with some predetermined variable ordering ≺. As in section 18.4.2, we restrict
variable ordering
attention to structures G such that there is an edge Xi →Xj, only if Xi ≺Xj.
18.5.2.1
Computing the marginal likelihood
We ﬁrst consider the problem of computing the marginal likelihood of the data given the order:
marginal
likelihood
P(D |≺) =
X
G∈GGd
P(G |≺)P(D | G).
(18.13)
Note that this summation, although restricted to networks with bounded indegree and consistent
with ≺, is still exponentially large; see exercise 18.20.
Before we compute the marginal likelihood, we note that computing the marginal likelihood
is equivalent to making predictions with equation (18.10). To see this, we can use the deﬁnition
of probability to see that
P(ξ[M + 1] | D, ≺) = P(ξ[M + 1], D |≺)
P(D |≺)
.
Now both terms on the right are marginal likelihood terms, one for the original data and the
other for original data extended by the new instance.
We now return to the computation of the marginal likelihood. The key insight is that when
we restrict attention to structures consistent with a given order ≺, the choice of family for one
variable places no additional constraints on the choice of family for another. Note that this

18.5. Bayesian Model Averaging ⋆
827
property does not hold without the restriction on the order; for example, if we pick Xi to be a
parent of Xj, then Xj cannot in turn be a parent of Xi.
Therefore, we can choose a structure G consistent with ≺by choosing, independently, a
family U i for each variable Xi. Global parameter modularity assumption states that the choice
of parameters for the family of Xi is independent of the choice of family for another family in
the network. Hence, summing over possible graphs consistent with ≺is equivalent to summing
over possible choices of family for each variable, each with its parameter prior.
Given our
constraint on the size of the family, the possible parent sets for the variable Xi are
Ui,≺= {U : U ≺Xi, |U| ≤d},
where U ≺Xi is deﬁned to hold when all variables in U precede Xi in ≺. Let GGd,≺be the
set of structures in GGd consistent with ≺. Using equation (18.12), we have that
P(D |≺)
=
X
G∈GGd,≺
Y
i
exp{FamScoreB(Xi | PaG
Xi : D)}
=
Y
i
X
U i∈Ui,≺
exp{FamScoreB(Xi | U i : D)}.
(18.14)
Intuitively, the equality states that we can sum over all network structures consistent with ≺
by summing over the set of possible families for each variable, and then multiplying the results
for the diﬀerent variables. This transformation allows us to compute P(D |≺) eﬃciently. The
expression on the right-hand side consists of a product with a term for each variable Xi, each
of which is a summation over all possible families for Xi. Given a bound d over the number of
parents, the number of possible families for a variable Xi is at most
 n
d

≤nd. Hence, the total
cost of computing equation (18.14) is at most n · nd = nd+1.
18.5.2.2
Probabilities of features
For certain types of features f, we can use the technique of the previous section to compute,
in closed form, the probability P(f |≺, D) that f holds in a structure given the order and the
data.
In general, if f is a feature. We want to compute
P(f |≺, D) = P(f, D |≺)
P(D |≺) .
We have just shown how to compute the denominator.
The numerator is a sum over all
structures that contain the feature and are consistent with the order:
P(f, D |≺) =
X
G∈GGd,≺
f(G)P(G |≺)P(D | G).
(18.15)
The computation of this term depends on the speciﬁc type of feature f.
The simplest situation is when we want to compute the posterior probability of a particular
choice of parents U. This in eﬀect requires us to sum over all graphs where PaG
Xi = U. In
this case, we can apply the same closed-form analysis to (18.15). The only diﬀerence is that we
restrict Ui,≺to be the singleton {U}. Since the terms that sum over the parents of Xj for i ̸= j
are not disturbed by this constraint, they cancel out from the equation.

828
Chapter 18. Structure Learning in Bayesian Networks
Proposition 18.7
P(PaG
Xi = U | D, ≺)
=
exp{FamScoreB(Xi | U : D)}
P
U ′∈Ui,≺exp{FamScoreB(Xi | U ′ : D)}.
A slightly more complex situation is when we want to compute the posterior probability of
the edge feature Xi →Xj. Again, we can apply the same closed-form analysis to (18.15). The
only diﬀerence is that we restrict Uj,≺to consist only of subsets that contain Xi.
Proposition 18.8
P(Xj ∈PaG
Xi |≺, D) =
P
{U∈Ui,≺: Xj∈U} exp{FamScoreB(Xi | U : D)}
P
U∈Ui,≺exp{FamScoreB(Xi | U : D)}
.
Unfortunately, this approach cannot be used to compute the probability of arbitrary structural
features. For example, we cannot compute the probability that there exists some directed path
from Xi to Xj, as we would have to consider all possible ways in which a path from Xi to Xj
could manifest itself through exponentially many structures.
We can overcome this diﬃculty using a simple sampling approach. Proposition 18.7 provides
us with a closed-form expression for the exact posterior probability of the diﬀerent possible
families of the variable Xi. We can therefore easily sample entire networks from the posterior
distribution given the order: we simply sample a family for each variable, according to the
distribution speciﬁed in proposition 18.7. We can then use the sampled networks to evaluate
any feature, such as Xi is ancestor of Xj.
18.5.3
The General Case
In the previous section, we made the simplifying assumption that we were given a predetermined
order. Although this assumption might be reasonable in certain cases, it is clearly too restrictive
in domains where we have very little prior knowledge. We therefore want to consider structures
consistent with all possible orders. Here, unfortunately, we have no elegant tricks that allow an
eﬃcient, closed-form solution. As with search problems we discussed, the choices of parents for
one variable can interfere with the choices of parents for another.
A general approach is try to approximate the exhaustive summation over structures in quan-
tities of interest (that is, equation (18.10) and equation (18.11)) with approximate sums. For this
purpose we utilize ideas that are similar to the ones we discuss in chapter 12 in the context of
approximate inference.
A ﬁrst-cut approach for approximating the sums in our case is to ﬁnd a set GG′ of high scoring
structures, and then estimate the relative mass of the structures in GG′. Thus, for example, we
would approximate equation (18.11) with
P(f | D) ≈
P
G∈GG
′ P(G | D)f(G)
P
G∈GG
′ P(G | D)
.
(18.16)

18.5. Bayesian Model Averaging ⋆
829
This approach leaves open the question of how we construct GG′.
The simplest approach
is to use model selection to pick a single high-scoring structure and then use that as our
approximation. If the amount of data is large relative to the size of the model, then the posterior
will be sharply peaked around a single model, and this approximation is a reasonable one.
However, as we discussed, when the amount of data is small relative to the size of the model,
there is usually a large number of high-scoring models, so that using a single model as our set
GG′ is a very poor approximation.
We can ﬁnd a larger set of structures by recording all the structures examined during the
search and returning the high-scoring ones. However, the set of structures found in this manner
is quite sensitive to the search procedure we use. For example, if we use greedy hill climbing,
then the set of structures we collect will all be quite similar. Such a restricted set of candidates
also shows up when we consider multiple restarts of greedy hill climbing. This is a serious
problem, since we run the risk of getting estimates of conﬁdence that are based on a biased
sample of structures.
18.5.3.1
MCMC Over Structures
An alternative approach is based on the use of sampling. As in chapter 12, we aim to approximate
the expectation over graph structures in equation (18.10) and equation (18.16) by an empirical
average. Thus, if we manage to sample graphs G1, . . . , GK from P(G | D), we can approximate
equation (18.16) as
P(f | D) ≈1
K
X
k
f(Gk).
The question is how to sample from the posterior distribution. One possible answer is to
use the general tool of Markov chain Monte Carlo (MCMC) simulation; see section 12.3.
In
this case, we deﬁne a Markov chain over the space of possible structures whose stationary
distribution is the posterior distribution P(G | D). We then generate a set of possible structures
by doing a random walk in this Markov chain. Assuming that we continue this process until
the chain converges to the stationary distribution, we can hope to get a set of structures that is
representative of the posterior.
How do we construct a Markov chain over the space of structures?
The idea is a fairly
straightforward application of the principles discussed in section 12.3. The states of the Markov
chain are graphs in the set GG of graphs we want to consider. We consider local operations
(for example, add edge, delete edge, reverse edge) that transform one structure to another. We
assume we have a proposal distribution T Q over such operations. We then apply the Metropolis-
Hastings acceptance rule. Suppose that the current state is G, and we sample the transition to
G′ from the proposal distribution, then we accept this transition with probability
min

1, P(G′, D)T Q(G′ →G)
P(G, D)T Q(G →G′)

.
As discussed in section 12.3, this strategy ensures that we satisfy the detailed balance condition.
To ensure that we have a regular Markov chain, we also need to verify that the space GG is
connected, that is, that we can reach each structure in GG from any other structure in GG through
a sequence of operations. This is usually easy to ensure with the set of operators we discussed.

830
Chapter 18. Structure Learning in Bayesian Networks
(a)
–8.40
–8.45
–8.50
–8.55
–8.60
–8.65
–8.70
1
0.8
0.6
0.4
0.2
0
0
1
0.2
0.4
0.6
0.8
(c)
Score (× 1000)
(b)
Iteration
Iteration
Figure 18.8
MCMC structure search using 500 instances from ICU-Alarm network. (a) & (b) Plots of
the progression of the MCMC process for two runs for 500 instances sampled from the ICU-Alarm network.
The x-axis denotes the iteration number, and the y-axis denotes the score of the current structure. (a) A
run initialized from the empty structure, and (b) a run initialized from the structure found by structure
search. (c) Comparison of the estimates of the posterior probability of edges using 100 networks sampled
every 2,000 iterations from the each of the runs (after initial burn-in of 20,000 iterations). Each point
denotes an edge, the x-coordinate is the estimate using networks from run (a) and the y-coordinate is the
estimate using networks from run (b).
It is important to stress that if the operations we apply are local (such as edge addition,
deletion, and reversal), then we can eﬃciently compute the ratio P (G′,D)
P (G,D) . To see this, note that
the logarithm of this ratio is the diﬀerence in score between the two graphs. As we discussed
in section 18.4.3.3, this diﬀerence involves only terms that relate to the families of the variables
that are involved in the local move. Thus, performing MCMC over structures can use the same
caching schemes discussed in section 18.4.3.3, and thus it can be executed eﬃciently.
The ﬁnal detail we need to consider is the choice the proposal distribution T Q.
Many
choices are reasonable. The simplest one to use, and one that is often used in practice, is the
uniform distribution over all possible operations (excluding ones that violate acyclicity or other
constraints we want to impose).
To demonstrate the use of MCMC over structures, we sampled 500 instances from the ICU-
Alarm network.
This is a fairly small training set, and so we do not hope to recover one
network. Instead, we run the MCMC sampling to collect 100 networks and use these to estimate
the posterior of diﬀerent edges. One of the hard problems in using such an approach is checking
whether the MCMC simulation converged to the stationary distribution. One ad-hoc test we can
perform is to compare the results of running several independent simulations. For example, in
ﬁgure 18.8a and 18.8b we plot the progression of two runs, one starting from the empty structure
and the other from the structure found by structure search on the same data set. We see that
initially the two runs are very diﬀerent; they sample networks with rather diﬀerent scores. In
later stages of the simulation, however, the two runs are in roughly the same range of scores.
This suggests that they might be exploring the same region. Another way of testing this is to
compare the estimate we computed based on each of the two runs. As we see in ﬁgure 18.8c,

18.5. Bayesian Model Averaging ⋆
831
–16.3
–16.4
–16.5
–16.6
–16.7
–16.8
–16.9
–17.0
(c)
Score (× 1000)
1
0.8
0.6
0.4
0.2
0
0
1
0.2
0.4
0.6
0.8
(a)
(b)
Iteration
Iteration
Figure 18.9
MCMC structure search using 1,000 instances from ICU-Alarm network. The protocol
is the same as in ﬁgure 18.8. (a) & (b) Plots two MCMC runs. (c) A comparison of the estimates of the
posterior probability of edges.
although the estimates are deﬁnitely not identical, they are mostly in agreement with each other.
Variants of MCMC simulation have been applied successfully to this task for a variety of
small domains, typically with 4–14 variables. However, there are several issues that potentially
limit its eﬀectiveness for large domains involving many variables. As we discussed, the space
of network structures grows superexponentially with the number of variables. Therefore, the
domain of the MCMC traversal is enormous for all but the tiniest domains. More importantly,
the posterior distribution over structures is often quite peaked, with neighboring structures
having very diﬀerent scores. The reason is that even small perturbations to the structure — a
removal of a single edge — can cause a huge reduction in score. Thus, the “posterior landscape”
can be quite jagged, with high “peaks” separated by low “valleys.” In such situations, MCMC is
known to be slow to mix, requiring many samples to reach the posterior distribution.
To see this eﬀect, consider ﬁgure 18.9, where we repeated the same experiment we performed
before, but this time for a data set of 1000 instances. As we can see, although we considered
a large number of iterations, diﬀerent MCMC runs converge to quite diﬀerent ranges of scores.
This suggests that the diﬀerent runs are sampling from diﬀerent regions of the search space.
Indeed, when we compare estimates in ﬁgure 18.9c, we see that some edges have estimated
posterior of almost 1 in one run and of 0 in another. We conclude that each of the runs became
stuck in a local “hill” of the search space and explored networks only in that region.
18.5.3.2
MCMC Over Orders
To avoid some of the problems with MCMC over structures, we consider an approach that
utilizes collapsed particles, as described in section 12.4. Recall that a collapsed particle consists
collapsed MCMC
of two components, one an assignment to a set of random variables that we sampled, and the
other a distribution over the remaining variables.
In our case, we utilize the notion of collapsed particle as follows. Instead of working in the
space of graphs G, we work in the space of pairs ⟨≺, G⟩so that G is consistent with the ordering
≺. As we have seen, we can use closed-form equations to deal the distribution over G given an

832
Chapter 18. Structure Learning in Bayesian Networks
ordering ≺. Thus, each ordering can represent a collapsed particle.
We now construct a Markov chain over the state of all n! orders.
Our construction will
guarantee that this chain has the stationary distribution P(≺| D). We can then simulate this
Markov chain, obtaining a sequence of samples ≺1, . . . , ≺T . We can now approximate the
expected value of any function f as
P(f | D) ≈1
T
T
X
t=1
P(f | D, ≺t),
where we compute P(f |≺t, D) as described in section 18.5.2.2.
It remains only to discuss the construction of the Markov chain. Again, we use a standard
Metropolis-Hastings algorithm. For each order ≺, we deﬁne a proposal probability T Q(≺→≺′),
Metropolis-
Hastings
which deﬁnes the probability that the algorithm will “propose” a move from ≺to ≺′. The
algorithm then accepts this move with probability
min

1, P(≺′, D)T Q(≺′ →≺)
P(≺, D)T Q(≺→≺′) .

As we discussed in section 12.3, this suﬃces to ensure the detailed balance condition, and thus
the resulting chain is reversible and has the desired stationary distribution.
We can consider several speciﬁc constructions for the proposal distribution, based on diﬀerent
neighborhoods in the space of orders.
In one very simple construction, we consider only
operators that ﬂip two variables in the order (leaving all others unchanged):
(Xi1 . . . Xij . . . Xid . . . Xin) 7→(Xi1 . . . Xid . . . Xij . . . Xin).
Clearly, such operators allow to get from any ordering to another in relatively few moves.
We note that, again, we can use decomposition to avoid repeated computation during the
evaluation of candidate operators.
Let ≺be an order and let ≺′ be the order obtained by
ﬂipping Xij and Xik. Now, consider the terms in equation (18.14); those terms corresponding to
variables Xiℓin the order ≺that precede Xij or succeed Xik do not change, since the set of
potential parent sets Uil,≺is the same.
Performing MCMC on the space of orderings is much more expensive than MCMC on the
space of networks. Each proposed move requires performing summation over a fairly large set
of possible parents for each variable. On the other hand, since each ordering corresponds to a
large number of networks, a few moves in the space of orderings correspond to a much larger
number of moves in the space of networks.
Empirical results show that using MCMC over orders alleviate some of the problems we
discussed with MCMC over network structures. For example, ﬁgure 18.10 shows two runs of this
variant of MCMC for the same data set as in ﬁgure 18.9. Although these two runs involve much
fewer iterations, they quickly converge to the same “area” of the search space and agree on the
estimation of the posterior. This is an empirical indication that these are reasonably close to
converging on the posterior.
18.6
Learning Models with Additional Structure
So far, our discussion of structure learning has deﬁned the task as one of ﬁnding the best graph
structure G, where a graph structure is simply a speciﬁcation of the parents to each of the

18.6. Learning Models with Additional Structure
833
–16.235
–16.230
–16.225
Iteration
(c)
Score (× 1000)
–16.240
–16.220
1
0.8
0.6
0.4
0.2
0
0
1
0.2
0.4
0.6
0.8
(a)
(b)
Iteration
Figure 18.10
MCMC order search using 1,000 instances from ICU-Alarm network. The protocol is the
same as in ﬁgure 18.8. (a) & (b) Plots two MCMC runs. (c) A comparison of the estimates of the posterior
probability of edges.
random variables in the network. However, some classes of models have additional forms of
structure that we also need to identify. For example, when learning networks with structured
CPDs, we may need to select the structure for the CPDs. And when learning template-based
models, our model is not even a graph over X, but rather a set of dependencies over a set of
template attributes.
Although quite diﬀerent, the approach in both of these settings is analogous to the one we
used for learning a simple graph structure: we deﬁne a hypothesis space of potential models
and a scoring function that allows us to evaluate diﬀerent models. We then devise a search
procedure that attempts to ﬁnd a high-scoring model. Even the choice of scoring functions
is essentially the same: we generally use either a penalized likelihood (such as the BIC score)
or a Bayesian score based on the marginal likelihood. Both of these scoring functions can be
computed using the likelihood function for models with shared parameters that we developed
in section 17.5. As we now discuss, the key diﬀerence in this new setting is the structure of the
search space, and thereby of the search procedure. In particular, our new settings require that
we make decisions in a space that is not the standard one of deciding on the set of parents for
a variable in the network.
18.6.1
Learning with Local Structure
As we discussed, we can often get better performance in learning if we use more compact models
of CPDs rather than ones that require a full table-based parameterization. More compact models
decrease the number of parameters and thereby reduce overﬁtting. Some of the techniques that
we described earlier in this chapter are applicable to various forms of structured CPDs, including
table-CPDs, noisy-or CPDs, and linear Gaussian CPDs (although the closed-form Bayesian score
is not applicable to some of those representations). In this section, we discuss the problem of
learning CPDs that explicitly encode local parameter sharing within a CPD, focusing on CPD
trees as a prototypical (and much-studied) example. Here, we need to make decisions about the
local structure of the CPDs as well as about the network structure. Thus, our search space is

834
Chapter 18. Structure Learning in Bayesian Networks
now much larger: it consists both of “global” decisions — the assignment of parents for each
variable — and of “local” decisions — the structure of the CPD tree for each variable.
18.6.1.1
Scoring Networks
We ﬁrst consider how the development of the score of a network changes when we consider
tree-CPDs instead of table-CPDs. Assume that we are given a network structure G; moreover,
for each variable Xi, we are given a description of the CPD tree Ti for P(Xi | PaG
Xi). We view
the choice of the trees as part of the speciﬁcation of the model. Thus, we consider the score of
G with T1, . . . , Tn
scoreB(G, T1, . . . , Tn : D) = log P(D | G, T1, . . . , Tn) + log P(G) +
X
i
log P(Ti | G),
where P(D | G, T1, . . . , Tn) is the marginal likelihood when we integrate out the parameters in
all the CPDs, and P(Ti | G) is the prior probability over the tree structure.
We usually assume that the structure prior over trees does not depend on the graph, but only
structure prior
on the choice of parents. Possible priors include the uniform prior
P(Ti | G) ∝1
and a prior that penalizes larger trees
P(Ti | G) ∝c|Ti|
(for some constant c < 1).
We now turn to the marginal likelihood term. As in our previous discussion, we assume global
and local parameter independence. This means that we have an independent prior over the
parameters in each leaf of each tree. For each Xi and each assignment paXi to Xi’s parents,
let λ(paXi) be the leaf in Ti to which paXi is assigned.
Using an argument that parallels our development of proposition 18.2, we have:
Proposition 18.9
Let G be a network structure, T1, . . . , Tn be CPD trees, and P(θG | G, T1, . . . , Tn) be a parameter
prior satisfying global and local parameter independence. Then,
P(D | G, T1, . . . , Tn) =
Y
i
Y
ℓ∈Leaves(Ti)
Z
Y
m : λ(paXi[m])=ℓ
P(Xi[m] | ℓ, θXi|ℓ, G, Ti)P(θXi|ℓ| G, Ti)dθXi|ℓ.
Thus, the score over a tree-CPD decomposes according to the structure of each of the trees.
Each of the terms that corresponds to a leaf is the marginal likelihood of a single variable
distribution and can be solved using standard techniques. Usually, we use Dirichlet priors at the
leaves, and so the term for each leaf will be a term of the form of equation (18.9).
Similar to the developments in table-CPDs, we can extend the notion of score decomposability
to networks with tree-CPDs. Recall that score decomposability implies that identical substruc-
tures receive the same score (regardless of structure of other parts of the network). Suppose we
have two possible tree-CPDs for X (not necessarily with the same set of parents in each), and
suppose that there are two leaves ℓ1 and ℓ2 in the two trees such that cℓ1 = cℓ2; that is, the

18.6. Learning Models with Additional Structure
835
same conditions on the parents of X in each of the two CPDs lead to the respective leaves.
Thus, the two leaves represent a similar situation (even though the tree structures can diﬀer
elsewhere), and intuitively they should receive the same score.
To ensure this property, we need to extend the notion of parameter modularity:
Deﬁnition 18.6
Let {P(θG | G, T1, . . . , Tn)} be a set of parameter priors over networks with tree-CPDs that
satisfy global and local parameter independence. The prior satisﬁes tree parameter modularity
tree parameter
modularity
if for each G, Ti and G′, T ′
i and ℓ∈Leaves(Ti), ℓ′ ∈Leaves(T ′
i ) such that cℓ= cℓ′, then
P(θXi|ℓ| G, Ti) = P(θXi|ℓ′ | G′, T ′
i ).
A natural extension of the BDe prior satisﬁes this condition.
Suppose we choose a prior
BDe prior
distribution P ′ and an equivalent sample size α; then we choose hyperparameters for P(θXi|ℓ|
G, Ti) to be αxi|ℓ= α·P ′(xi, cℓ). Using this assumption, we can now decompose the Bayesian
score for a tree-CPD; see exercise 18.17.
18.6.1.2
Search with Local Structure
Our next task is to describe how to search our space of possible hypotheses for one that has
high score. Recall that we now have a much richer hypothesis space over which we need to
search.
The key question in the case of learning tree-CPDs is that of choosing a tree structure for
P(X | U) for some set of possible parents U. (We will discuss diﬀerent schemes for choosing
U.) There are two natural operators in this space.
•
Split – replace a leaf in the tree by an internal variable that leads to a leaf. This step increases
the tree height by 1 on a selected branch.
•
Prune – replace an internal variable by a leaf.
Starting from the “empty” tree (that consists of single leaf), we can reach any other tree by a
sequence of split operations.
The prune operations allow searches to retract some of the moves. This capability is critical,
since there are many local maxima in growing trees. For example, it is often the case that a
sequence of two splits leads to a high-scoring tree, but the intermediate step (after performing
only the ﬁrst split) has low score. Thus, greedy hill-climbing search can get stuck in a local
maximum early on.
As a consequence, the search algorithm for tree-CPDs is often not a
straightforward hill climbing. In particular, one common strategy is to choose the best-scoring
split at each point, even if that split actually decreases the score.
Once we have ﬁnished
constructing the tree, we go back and evaluate earlier splitting decisions that we made.
The search space over trees can explored using a “divide and conquer” approach. Once we
decide to split our tree on one variable, say Y , the choices we make in one subtree, say the
one corresponding to Y = y0, are independent of the choices we make in other subtrees. Thus,
we can explore for the best structure for each one of the subsets independently of the other.
Thus, we can devise recursive search procedures for trees, which works roughly as follows. The
procedure receives a set of instances to learn from, a target variable X, and a set U of possible
parents. It evaluates each Y ∈U as a potential question by scoring the tree-CPD that has Y as
a root and has no further question. This myopic evaluation can miss pairs or longer sequences

836
Chapter 18. Structure Learning in Bayesian Networks
of questions that lead to high accuracy predictions and hence to a high scoring model. However,
it can be performed very eﬃciently (see exercise 18.17), and so it is often used, under the hope
that other approaches (such as random restarts or tabu search) can help us deal with local
optima.
After choosing the root, the procedure divides the data set into smaller data sets, each
one corresponding to one value of the variable Y . Using the decomposition property we just
discussed, the procedure is recursively invoked to learn a subtree in each Dy. These subtrees
are put together into a tree-CPD that has Y as a root. The ﬁnal test compares the score of the
constructed subtree to that of the empty subtree. This test is necessary, since our construction
algorithm selected the best split at this point, but without checking that this split actually
improves the score. This strategy helps avoid local maxima arising from the myopic nature of
the search, but it does potentially lead to unnecessary splits that actually hurt our score. This
ﬁnal check helps avoid those. In eﬀect, this procedure evaluates, as it climbs back up the tree
in the recursion, all of the possible merge steps relative to our constructed tree.
There are two standard strategies for applying this procedure. One is an encapsulated search.
Here, we perform one of the network-structure search procedures we described in earlier sections
of this chapter. Whenever we perform a search operation (such as adding or removing an edge),
we use a local procedure to ﬁnd a representation for the newly created CPD. The second option
is to use a uniﬁed search, where we do not distinguish between operators that modify the
network and operators that modify the local structure. Instead, we simply apply a local search
that modiﬁes the joint representation of the network and local structure for each CPD. Here,
each state in the search space consists of a collection of n trees ⟨T1, . . . , Tn⟩, which deﬁne both
the structure and the parameters of the network. We can structure the operations in the search
in many ways: we can update an entire CPD tree for one of the variables, or we can evaluate
the delta-score of the various split and merge operations in tree-CPDs all across the network
and choose the best one. In either case, we must remember that not every collection of trees
deﬁnes an acyclic network structure, and so we must construct our search carefully to ensure
acyclic structures, as well as any other constraints that we want to enforce (such as bounded
indegree).
Each of these two options for learning with local structure has its beneﬁts and drawbacks.
Encapsulated search spaces decouple the problem of network structure learning from that of
learning CPD structures. This modularity allows us to “plug in” any CPD structure learning
procedure within generic search procedure for network learning. Moreover, since we consider
the structure of each CPD as a separate problem, we can exploit additional structure in the CPD
representation. A shortcoming of encapsulated search spaces is that they can easily cause us to
repeat a lot of eﬀort. In particular, we redo the local structure search for X’s CPD every time we
consider a new parent for X. This new parent is often irrelevant, and so we end up discarding
the local structure we just learned. Uniﬁed search spaces alleviate this problem. In this search
formulation, adding a new parent Y to X is coupled with a proposal as to the speciﬁc position
in the tree-CPD of X where Y is used. This ﬂexibility comes at a cost: the number of possible
operators at a state in the search is very large — we can add a split on any variable at each leaf
of the n trees. Moreover, key operations such as edge reversal or even edge deletion can require
many steps in the ﬁner-grained search space, and therefore they are susceptible to forming local
optima. Overall, there is no clear winner between these two methods, and the best choice is
likely to be application-speciﬁc.

18.6. Learning Models with Additional Structure
837
18.6.2
Learning Template Models
We now brieﬂy discuss the task of structure learning for template-based models. Here, once
we deﬁne the hypothesis space, the solution becomes straightforward. Importantly, when learn-
ing template-based models, our hypothesis space is the space of structures in the template
representation, not in the ground network from which we are learning.
For DBNs, we are learning a representation as in deﬁnition 6.4: a time 0 network B0, and
a transition network B→.
Importantly, the latter network is speciﬁed in terms of template
attributes; thus, the learned structure will be used for all time slices in the unrolled DBN.
The learned structure must satisfy the constraints on our template-based representation. For
example, B→must be a valid conditional Bayesian network for X ′ given X: it must be acyclic,
and there must be no incoming edges into any variables in X.
For object-relational models, each hypothesis in our space speciﬁes an assignment PaA for
every A ∈ℵ. Here also, the learned structure is at the template level and is applied to all
instantiations of A in the ground network. The learned structure must satisfy the constraints
of the template-based language with which we are dealing.
For example, in a plate model,
the structure must satisfy the constraints of deﬁnition 6.9, for example, the fact that for any
template parent Bi(U i) ∈PaA, we have that the parent’s arguments U i must be a subset of
the attribute’s argument signature α(A).
Importantly, in both plate models and DBNs, the set of possible structures is ﬁnite. This fact is
obvious for DBNs. For plate models, note that the set of template attributes is ﬁnite; the possible
argument signatures of the parents is also ﬁnite, owing to the constraints that U i ⊆α(A).
However, this constraint does not necessarily hold for richer languages. We will return to this
point.
Given a hypothesis space, we need to determine a scoring function and a search algorithm.
Based on our analysis in section 17.5.1.2 and section 17.5.3, we can easily generalize any of
our standard scoring functions (say the BIC score or the marginal likelihood) to the case of
template-based representations. The analysis in section 17.5.1.2 provides us with a formula for
a decomposed likelihood function, with terms corresponding to each of the model parameters
at the template level. From this formula, the derivation of the BIC score is immediate. We
use the decomposed likelihood function and penalize with the number of parameters in the
template model. For a Bayesian score, we can follow the lines of section 17.5.3 and assume global
parameter independence at the template level. The posterior now decomposes in the same way
as the likelihood, giving us, once again, a decomposable score.
With a decomposable score, we can now search over the set of legal structures in our
hypothesis space, as deﬁned earlier.
In this case, the operators that we typically apply are
the natural variants of those used in standard structure search: edge addition, deletion, and
(possibly) reversal. The only slight subtlety is that we must check, at each step, that the model
resulting from the operator satisﬁes the constraints of our template-based representation. In
particular, in many cases for both DBNs and PRMs, edge reversal will lead to an illegal structure.
For example, in DBNs, we cannot reverse an edge A →A′. In plate models, we cannot reverse
an edge B(U) →A(U, U ′).
Indeed, the notion of edge reversal only makes sense when
α(A) = α(B). Thus, a more natural search space for plate models (and other object-relational
template models) is one that simply searches for a parent set for each A ∈ℵ, using operators
such as parent addition and deletion.

838
Chapter 18. Structure Learning in Bayesian Networks
We conclude this discussion by commenting brieﬂy on the problem of structure learning for
more expressive object-relational languages. Here, the complexity of our hypothesis space can be
signiﬁcantly greater. Consider, for example, a PRM. Most obviously, we see that the speciﬁcation
here involves not only possible parents but also possibly a guard and an aggregation function.
Thus, our model speciﬁcation requires many more components. Subtler, but also much more
important, is the fact that the space of possible structures is potentially inﬁnite. The key issue
here is that the parent signature α(PaA) can be a superset of α(A), and therefore the set of
possible parents we can deﬁne is unboundedly large.
Example 18.2
Returning to the Genetics domain, we can, if we choose, allow the genotype of a person depending
on the genotype of his or her mother, or of his or her grandmother, or of his or her paternal uncles,
or on any type of relative arbitrarily far away in the family tree (as long as acyclicity is maintained).
For example, the dependence on paternal uncle (not by marriage) can be written as a dependence
of Genotype(U) on Genotype(U ′) with the guard
Father(V, U) ∧Brother(U ′, V ),
assuming Brother has already been deﬁned.
In general, by increasing the number of logical variables in the attribute’s parent argument
signature, we can introduce dependencies over objects that are arbitrarily far away. Thus, when
learning PRMs, we generally want to introduce a structure prior that penalizes models that are
more complex, for example, as an exponential penalty on the number of logical variables in
α(PaA) −α(A), or on the number of clauses in the guard.
In summary, the key aspect to learning structure of template-based models is that both

the structure and the parameters are speciﬁed at the template level, and therefore that
is the space over which our structure search is conducted. It is this property that allows
us to learn a model from one skeleton (for example, one pedigree, or one university) and
apply that same model to a very diﬀerent skeleton.
18.7
Summary and Discussion
In this chapter, we considered the problem of structure learning from data. As we have seen,
there are two main issues that we need to deal with: the statistical principles that guide the
choice between network structures, and the computational problem of applying these principles.
The statistical problem is easy to state. Not all dependencies we can see in the data are real.
Some of them are artifacts of the ﬁnite sample we have at our disposal. Thus, to learn (that is,
generalize to new examples), we must apply caution in deciding which dependencies to model
in the learned network.
We discussed two approaches to address this problem.
The ﬁrst is the constraint-based
approach. This approach performs statistical tests of independence to collect a set of depen-
dencies that are strongly supported by the data. Then it searches for the network structure
that “explains” these dependencies and no other dependencies. The second is the score-based
approach. This approach scores whole network structures against the data and searches for a
network structure that maximize the score.
What is the diﬀerence between these two approaches? Although at the outset they seem quite
diﬀerent, there are some similarities. In particular, if we consider a choice between the two

18.7. Summary and Discussion
839
possible networks over two variables, then both approaches use a similar decision rule to make
the choice; see exercise 18.27.
When we consider more than two variables, the comparison is less direct. At some level, we

can view the score-based approach as performing a test that is similar to a hypothesis
test. However, instead of testing each pair of variables locally, it evaluates a function
that is somewhat like testing the complete network structure against the null hypothesis
of the empty network. Thus, the score-based approach takes a more global perspective,
which allows it to trade oﬀapproximations in diﬀerent part of the network.
The second issue we considered was the computational issue. Here there are clear diﬀerences.
In the constraint-based approach, once we collect the independence tests, the construction of
the network is an eﬃcient (low-order polynomial) procedure. On the other hand, we saw that
the optimization problem in the score-based approach is NP-hard. Thus, we discussed various
approaches for heuristic search.
When discussing this computation issue, one has to remember how to interpret the theoretical
results.
In particular, the NP-hardness of score-based optimization does not mean that the
problem is hopeless. When we have a lot of data, the problem actually becomes easier, since
one structure stands out from the rest. In fact, recent results indicates that there might be search
procedures that when applied to suﬃciently large data sets are guaranteed to reach the global
optimum. This suggests that the hard cases might be the ones where the diﬀerences between
the maximal scoring network and that other local maxima might not be that dramatic. This is a
rough intuition, and it is an open problem to characterize formally the trade-oﬀbetween quality
of solution and hardness of the score-based learning problem.
Another open direction of research attempts to combine the best of both worlds. Can we use
the eﬃcient procedures developed for constraint-based learning to ﬁnd high-scoring network
structure? The high-level motivation that the Build-PDAG we discussed uses knowledge about
Bayesian networks to direct its actions. On the other hand, the search procedures we discussed
so far are fairly uninformed about the problem. A simpleminded combination of these two
approaches uses a constraint-based method to ﬁnd starting point for the heuristic search. More
elaborate strategies attempt to use the insight from constraint-based learning to reformulate the
search space — for example, to avoid exploring structures that are clearly not going to score
well, or to consider global operators.
Another issue that we touched on is estimating the conﬁdence in the structures we learned.
We discussed MCMC approaches for answering questions about the posterior. This gives us a
measure of our conﬁdence in the structures we learned. In particular, we can see whether a
part of the learned network is “crucial” in the sense that it has high posterior probability, or
closer to arbitrary when it has low posterior probability. Such an evaluation, however, compares
structures only within the class of models we are willing to learn. It is possible that the data do
not match any of these structures. In such situations, the posterior may not be informative about
the problem. The statistical literature addresses such questions under the name of goodness of
goodness of ﬁt
ﬁt tests, which we brieﬂy described in box 16.A. These tests attempt to evaluate whether a given
model would have data such as the one we observed. This topic is still underdeveloped for
models such as Bayesian networks.

840
Chapter 18. Structure Learning in Bayesian Networks
18.8
Relevant Literature
We begin by noting that many of the works on learning Bayesian networks involve both param-
eter estimation and structure learning; hence most of the references discussed in section 17.8
are still relevant to the discussion in this chapter.
The constraint-based approaches to learning Bayesian networks were already discussed in
chapter 3, under the guise of algorithms for constructing a perfect map for a given distribution.
Section 3.6 provides the references relevant to that work; some of the key algorithms of this type
include those of Pearl and Verma (1991); Verma and Pearl (1992); Spirtes, Glymour, and Scheines
(1993); Meek (1995a); Cheng, Greiner, Kelly, Bell, and Liu (2002). The application of these methods
to the task of learning from an empirical distribution requires the use of statistical independence
tests (see, for example, Lehmann and Romano (2008)). However, little work has been devoted to
analyzing the performance of these algorithms in that setting, when some of the tests may fail.
Much work has been done on the development and analysis of diﬀerent scoring functions for
probabilistic models, including the BIC/MDL score (Schwarz 1978; Rissanen 1987; Barron et al.
1998) and the Bayesian score (Dawid 1984; Kass and Raftery 1995), as well as other scores, such
as the AIC (Akaike 1974). These papers also establish the basic properties of these scores, such
as the consistency of the BIC/MDL and Bayesian scores.
The Bayesian score for discrete Bayesian networks, using a Dirichlet prior, was ﬁrst proposed
by Buntine (1991) and Cooper and Herskovits (1992) and subsequently generalized by Spiegelhalter,
Dawid, Lauritzen, and Cowell (1993); Heckerman, Geiger, and Chickering (1995). In particular,
Heckerman et al.
propose the BDe score, and they show that the BDe prior is the only
one satisfying certain natural assumptions, including global and local parameter independence
and score-equivalence. Geiger and Heckerman (1994) perform a similar analysis for Gaussian
networks, resulting in a formal justiﬁcation for a Normal-Wishart prior that they call the BGe
BGe prior
prior. The application of MDL principles to Bayesian network learning was developed in parallel
(Bouckaert 1993; Lam and Bacchus 1993; Suzuki 1993). These papers also deﬁned the relationship
between the maximum likelihood score and information theoretic scores. These connections in
a more general setting were explored in early works in information theory Kullback (1959), as
well as in early work on decomposable models Chow and Liu (1968).
Buntine (1991) ﬁrst explored the use of nonuniform priors over Bayesian network structures,
utilizing a prior over a ﬁxed node ordering, where all edges were included independently.
Heckerman, Mamdani, and Wellman (1995) suggest an alternative approach that uses the extent
of deviation between a candidate structure and a “prior network.”
Perhaps the earliest application of structure search for learning in Bayesian networks was the
work of Chow and Liu (1968) on learning tree-structured networks. The ﬁrst practical algorithm
for learning general Bayesian network structure was proposed by Cooper and Herskovits (1992).
Their algorithm, known as the K2 algorithm, was limited to the case where an ordering on the
variables is given, allowing families for diﬀerent variables to be selected independently. The
approach of using local search over the space of general network structures was proposed and
studied in depth by Chickering, Geiger, and Heckerman (1995) (see also Heckerman et al. 1995),
although initial ideas along those lines were outlined by Buntine (1991). Chickering et al. compare
diﬀerent search algorithms, including K2 (with diﬀerent orderings), local search, and simulated
annealing. Their results suggest that unless a good ordering is known, local search oﬀers the
best time-accuracy trade-oﬀ. Tabu search is discussed by Glover and Laguna (1993). Several

18.8. Relevant Literature
841
works considered the combinatorial problem of searching over all network structures (Singh and
Moore 2005; Silander and Myllymaki 2006) based on ideas of Koivisto and Sood (2004).
Another line of research proposes local search over a diﬀerent space, or using diﬀerent
operators. Best known in this category are algorithms, such as the GES algorithm, that search
over the space of I-equivalence classes. The foundations for this type of search were developed
by Chickering (1996b, 2002a). Chickering shows that GES is guaranteed to learn the optimal
Bayesian network structure at the large sample limit, if the data is sampled from a graphical
model (directed or undirected). Other algorithms that guarantee identiﬁcation of the correct
structure at the large-sample limit include the constraint-based SGS method of Spirtes, Glymour,
and Scheines (1993) and the KES algorithm of Nielsen, Koˇcka, and Peña (2003).
Other search methods that use alternative search operators or search spaces include the
optimal reinsertion algorithm of Moore and Wong (2003), which takes a variable and moves it
to a new position in the network, and the ordering-based search of Teyssier and Koller (2005),
which searches over the space of orderings, selecting, for each ordering the optimal (bounded-
indegree) network consistent with it. Both of these methods take much larger steps in the space
than search over the space of network structures; although each step is also more expensive,
empirical results show that these algorithms are nevertheless faster. More importantly, they are
signiﬁcantly less susceptible to local optima. A very diﬀerent approach to avoiding local optima
is taken by the data perturbation method of Elidan et al. (2002), in which the suﬃcient statistics
are perturbed to move the algorithm out of local optima.
Much work has been done on eﬃcient caching of suﬃcient statistics for machine learning
tasks in general, and for Bayesian networks in particular. Moore and Lee (1997); Komarek and
Moore (2000) present AD-trees and show their eﬃcacy for learning Bayesian networks in high
dimension. Deng and Moore (1989); Moore (2000); Indyk (2004) present some data structures for
continuous spaces.
Several papers also study the theoretical properties of the Bayesian network structure learning
task.
Some of these papers involve the computational feasibility of the task.
In particular,
Chickering (1996a) showed that the problem of ﬁnding the network structure with indegree
≤d that optimizes the Bayesian score for a given data set is NP-hard, for any d ≥2.
NP-hardness is also shown for ﬁnding the maximum likelihood structures within the class
of polytree networks (Dasgupta 1999) and path-structured networks (Meek 2001).
Chickering
et al. (2003) show that the problem of ﬁnding the optimal structure is also NP-hard at the
large-sample limit, even when: the generating distribution is perfect with respect to some DAG
containing hidden variables, we are given an independence oracle, we are given an inference
oracle, and we restrict potential solutions to structures in which each node has at most d parents
(for any d ≥3). Importantly, all of these NP-hardness results hold only in the inconsistent
case, that is, where the generating distribution is not perfect for some DAG. In the case where
the generating distribution is perfect for some DAG over the observed variables, the problem
is signiﬁcantly easier. As we discussed, several algorithms can be guaranteed to identify the
correct structure. In fact, the constraint-based algorithms of Spirtes et al. (1993); Cheng et al.
(2002) can be shown to have polynomial-time performance if we assume a bounded indegree (in
both the generating and the learned network), providing a sharp contrast to the NP-hardness
result in the inconsistent setting.
Little work has been done on analyzing the PAC-learnability of Bayesian network structures,
that is, their learnability as a function of the number of samples. A notable exception is the work

842
Chapter 18. Structure Learning in Bayesian Networks
of Höﬀgen (1993), who analyzes the problem of PAC-learning the structure of Bayesian networks
with bounded indegree. He focuses on the maximum likelihood network subject to the indegree
constraints and shows that this network has, with high probability, low KL-divergence to the
true distribution, if we learn with a number of samples M that grows logarithmically with the
number of variables in the network. Friedman and Yakhini (1996) extend this analysis for search
with penalized likelihood — for example, MDL/BIC scores. We note that, currently, no eﬃcient
algorithm is known for ﬁnding the maximum-likelihood Bayesian network of bounded indegree,
although the work of Abbeel, Koller, and Ng (2006) does provide a polynomial-time algorithm
for learning a low-degree factor graph under these assumptions.
Bayesian model averaging has been used in the context of density estimation, as a way
of gaining more robust predictions over those obtained from a single model. However, most
often it is used in the context of knowledge discovery, to obtain a measure of conﬁdence in
predictions relating to structural properties. In a limited set of cases, the space of legal networks
is small enough to allow a full enumeration of the set of possible structures (Heckerman et al.
1999). Buntine (1991) ﬁrst observed that in the case of a ﬁxed ordering, the exponentially large
summation over model structures can be reformulated compactly. Meila and Jaakkola (2000)
show that one can eﬃciently infer and manipulate the full Bayesian posterior over all the
superexponentially many tree-structured networks. Koivisto and Sood (2004) suggest an exact
method for summing over all network structure. Although this method is exponential in the
number of variables, it can deal with domains of reasonable size.
Other approaches attempt to approximate the superexponentially large summation by con-
sidering only a subset of possible structures. Madigan and Raftery (1994) propose a heuristic
approximation, but most authors use an MCMC approach over the space of Bayesian network
structure (Madigan and York 1995; Madigan et al. 1996; Giudici and Green 1999). Friedman and
Koller (2003) propose the use of MCMC over orderings, and show that it achieves much faster
mixing and therefore more robust estimates than MCMC over network space. Ellis and Wong
(2008) improve on this algorithm, both in removing bias in its prior distribution and in improving
its computational eﬃciency.
The idea of using local learning of tree-structured CPDs for learning global network structure
was proposed by Friedman and Goldszmidt (1996, 1998), who also observed that reducing the
number of parameters in the CPD can help improve the global structure reconstruction. Chick-
ering et al. (1997) extended these ideas to CPDs structured as a decision graph (a more compact
generalization of a decision tree). Structure learning in dynamic Bayesian networks was ﬁrst
proposed by Friedman, Murphy, and Russell (1998). Structure learning in object-relational mod-
els was ﬁrst proposed by Friedman, Getoor, Koller, and Pfeﬀer (1999); Getoor, Friedman, Koller,
and Taskar (2002). Segal et al. (2005) present the module network framework, which combines
clustering with structure learning.
Learned Bayesian networks have also been used for speciﬁc prediction and classiﬁcation tasks.
Friedman et al. (1997) deﬁne a tree-augmented naive Bayes structure, which extends the tradi-
tree-augmented
naive Bayes
tional naive Bayes classiﬁer by allowing a tree-structure over the features. They demonstrate
that this enriched model provides signiﬁcant improvements in classiﬁcation accuracy. Depen-
dency networks were introduced by Heckerman et al. (2000) and applied to a variety of settings,
including collaborative ﬁltering. This latter application extended the earlier work of Breese et al.
(1998), which demonstrated the success of Bayesian network learning (with tree-structured CPDs)
to this task.

18.9. Exercises
843
18.9
Exercises
Exercise 18.1
Show that the χ2(D) statistic of equation (18.1) is approximately twice of dII(D) of equation (18.2). Hint:
examine the ﬁrst-order Taylor expansion of z
t ≈1 + z−t
t
in z around t.
Exercise 18.2
Derive equation (18.3). Show that this value is the sum of the probability of all possible data sets that have
the given empirical counts.
Exercise 18.3⋆
Suppose we are testing multiple hypotheses 1, . . . , N for a large value. Each hypothesis has an observed
multiple
hypothesis testing
deviance measure Di, and we computed the associated p-value pi. Recall that under the null hypothesis,
pi has a uniform distribution between 0 and 1. Thus, P(pi < t | H0) = t for every t ∈[0, 1].
We are worried that one of our tests received a small p-value by chance. Thus, we want to consider the
distribution of the best p-value out of all of our tests under the assumption that the null hypothesis is true
in all of the tests. More formally, we want to examine the behavior of mini pi under H0.
a. Show that
P(min
i
pi < t | H0) ≤t · N.
(Hint: Do not assume that the variables pi are independent of each other.)
b. Suppose we want to ensure that the probability of a random rejection is below 0.05, what p-value
should we use in individual hypothesis tests?
c. Suppose we assume that the tests (that is, the variables pi) are independent of each other. Derive the
bound in this case. Does this bound give better (higher) decision p-value when we use N = 100 and
global rejection rate below 0.05 ? How about N = 1, 000?
The bound you derive in [b] is called Bonferroni bound, or more often Bonferroni correction for a multiple-
Bonferroni
correction
hypothesis testing scenario.
Exercise 18.4⋆
Consider again the Build-PDAG procedure of algorithm 3.5, but now assume that we apply it in a setting
where the independence tests might return incorrect answers owing to limited and noisy data.
a. Provide an example where Build-PDAG can fail to reconstruct the true underlying graph G∗even in the
presence of a single incorrect answer to an independence question.
b. Now, assume that the algorithm constructs the correct skeleton but can encounter a single incorrect
answer when extracting the immoralities.
Exercise 18.5
Prove corollary 18.1. Hint: Start with the result of proposition 18.1, and use the chain rule of entropies and
the chain rule mutual information.
Exercise 18.6
Show that adding edges to a network increases the likelihood.
Exercise 18.7⋆
Prove theorem 18.1. Hint: You can use Stirling’s approximation for the Gamma function
Stirling’s
approximation
Γ(x) ≈
√
2π xx−1
2 e−x

844
Chapter 18. Structure Learning in Bayesian Networks
or
log Γ(x) ≈1
2 log(2π)x log(x) −1
2 log(x) −x
Exercise 18.8
Show that if G is I-equivalent to G′, then if we use table-CPDs, we have that scoreL(G
:
D) =
scoreL(G′ : D) for any choice of D.
Hint: Consider the set of distributions that can be represented by parameterization each network structure.
Exercise 18.9
Show that if G is I-equivalent to G′, then if we use table-CPDs, we have that scoreBIC(G
:
D) =
scoreBIC(G′ : D) for any choice of D. You can use the results of exercise 3.18 and exercise 18.8 in your
proof.
Exercise 18.10
Show that the Bayesian score with a K2 prior in which we have a Dirichlet prior Dirichlet(1, 1, . . . , 1) for
each set of multinomial parameters is not score-equivalent.
Hint: Construct a data set for which the score of the network X →Y diﬀers from the score of the
network X ←Y .
Exercise 18.11⋆
We now examine how to prove score equivalence for the BDe score. Assume that we have a prior speciﬁed
by an equivalent sample size α and prior distribution P ′. Prove the following:
a. Consider networks over the variables X and Y . Show that the BDe score of X →Y is equal to that
of X ←Y .
b. Show that if G and G′ are identical except for a covered edge reversal of X →Y , then the BDe score
of both networks is equal.
c. Show that the proof of score equivalence follows from the last result and theorem 3.9.
Exercise 18.12⋆
In section 18.3.2, we have seen that the Bayesian score can be posed a sequential procedure that estimates
the performance on new unseen examples. In this example, we consider another score that is based on
this motivation.
Recall that leave-one-out cross-validation (LOOCV), described in box 16.A, is a procedure for estimating the
performance of a learning method on new samples. In our context, this deﬁnes the following score:
LOOCV(D | G) =
M
Y
m=1
P(ξ[m] | G, D−m),
where D−m is the data with the m’th instance removed.
a. Consider the setting of section 18.3.3 where we observe a series of values of a binary variable. Develop
a closed-form equation for LOOCV(D | G) as a function of the number of heads and the number of
tails.
b. Now consider the network GX→Y and a data set D that consists of observations of two binary variables
X and Y . Develop a closed-form equation for LOOCV(D | GX→Y ) as a function of the suﬃcient
statistics of X and Y in D.
c. Based on these two examples, what are the properties of the LOOCV score? Is it decomposable?

18.9. Exercises
845
Exercise 18.13
Consider the algorithm for learning tree-structured networks in section 18.4.1.
Show that the weight
wi→j = wj→i if the score satisﬁes score equivalence.
Exercise 18.14⋆⋆
We now consider a situation where we can ﬁnd the high-scoring structure in a polynomial time. Suppose
we are given a directed graph C that imposes constraints on the possible parent-child relationships in the
learned network: an edge X—Y in C implies that X might be considered as a parent of Y . We deﬁne
GGC = {G : G ⊆C} to be the set of graphs that are consistent with the constraints imposed by C.
Describe an algorithm that, given a decomposable score score and a data set D, ﬁnds the maximal scoring
network in GGC. Show that your algorithm is exponential in the tree-width of the graph C.
Exercise 18.15⋆
Consider the problem of learning a Bayesian network structure over two random variables X and Y .
a. Show a data set — an empirical distribution and a number of samples M — where the optimal
network structure according to the BIC scoring function is diﬀerent from the optimal network structure
according to the ML scoring function.
b. Assume that we continue to get more samples that exhibit precisely the same empirical distribution.
(For simplicity, we restrict attention to values of M that allow that empirical distribution to be achieved;
for example, an empirical distribution of 50 percent heads and 50 percent tails can be achieved only
for an even number of samples.) At what value of M will the network that optimizes the BIC score be
the same as the network that optimizes the likelihood score?
Exercise 18.16
This problem considers the performance of various types of structure search algorithms. Suppose we have
a general network structure search algorithm, A, that takes a set of basic operators on network structures
as a parameter. This set of operators deﬁnes the search space for A, since it deﬁnes the candidate network
structures that are the “immediate successors” of any current candidate network structure—that is, the
successor states of any state reached in the search. Thus, for example, if the set of operators is {add an
edge not currently in the network}, then the successor states of any candidate network G is the set of
structures obtained by adding a single edge anywhere in G (so long as acyclicity is maintained).
Given a set of operators, A does a simple greedy search over the set of network structures, starting from
the empty network (no edges), using the BIC scoring function. Now, consider two sets of operators we can
use in A. Let A[add] be A using the set of operations {add an edge not currently in the network}, and let
A[add,delete] be A using the set of operations {add an edge not currently in the network, delete an edge
currently in the network}.
a. Show a distribution where, regardless of the amount of data in our training set (that is, even with
inﬁnitely many samples), the answer produced by A[add] is worse (that is, has a lower BIC score) than
the answer produced by A[add,delete]. (It is easiest to represent your true distribution in the form of a
Bayesian network; that is, a network from which the sample data are generated.)
b. Show a distribution where, regardless of the amount of data in our training set, A[add,delete] will
converge to a local maximum. In other words, the answer returned by the algorithm has a lower score
than the optimal (highest-scoring) network. What can we conclude about the ability of our algorithm
to ﬁnd the optimal structure?
Exercise 18.17⋆
This problem considers the problem of learning a CPD tree structure for a variable in a Bayesian network,
using the Bayesian score. Assume that the network structure G includes a description of the CPD trees in

846
Chapter 18. Structure Learning in Bayesian Networks
it; that is, for each variable Xi, we have a CPD tree Ti for P(Xi | PaG
Xi). We view the choice of the trees
as part of the speciﬁcation of the model. Thus, we consider the score of G with T1, . . . , Tn
scoreB(G, T1, . . . , Tn : D) = log P(D | G, T1, . . . , Tn) + log P(G) +
X
i
log P(Ti | G).
Here, we assume for simplicity that the two structure priors are uniform, so that we focus on the marginal
likelihood term P(D | G, T1, . . . , Tn). Assume we have selected a ﬁxed choice of parents U i for each
variable Xi. We would like to ﬁnd a set of trees T1, . . . , Tn that together maximizes the Bayesian score.
a. Show how you can decompose the Bayesian score in this case as a sum of simpler terms; make sure
you state the assumptions necessary to allow this decomposition.
b. Assume that we consider only a single type of operator in our search, a split(Xi, ℓ, Y ) operator, where
ℓis a leaf in the current CPD tree for Xi, and Y ∈U i is a possible parent of Xi. This operator
replaces the leaf ℓby an internal variable that splits on the values of Y . Derive a simple formula for
the delta-score δ(T
: o) of such an operator o = split(Xi, ℓ, Y ). (Hint: Use the representation of the
decomposed score to simplify the formula.)
c. Suppose our greedy search keeps track of the delta-score for all the operators. After we take a step in
the search space by applying operator o = split(X, ℓ, Y ), how should we update the delta score for
another operator o′ = split(X′, ℓ′, Y ′)? (Hint: Use the representation of the delta-score in terms of
decomposed score in the previous question.)
d. Now, consider applying the same process using the likelihood score rather than the Bayesian score.
What will the resulting CPD trees look like in the general case? You can make any assumption you
want about the behavior of the algorithm in case of ties in the score.
Exercise 18.18
Prove proposition 18.5.
Exercise 18.19
Prove proposition 18.6.
Exercise 18.20⋆
Recall that the Θ(f(n)) denotes both an asymptotic lower bound and an asymptotic upper bound (up to
a constant factor).
a. Show that the number of DAGs with n vertices is 2Θ(n2).
b. Show that the number of DAGs with n vertices and indegree bounded by d is 2Θ(dn log n).
c. Show that the number of DAGs with n vertices and indegree bounded by d that are consistent with a
given order is 2Θ(dn log n).
Exercise 18.21
Consider the problem of learning the structure of a 2-TBN over X = {X1, . . . , Xn}. Assume that we
are learning a model with bounded indegree k. Explain, using the argument of asymptotic complexity,
why the problem of learning the 2-TBN structure is considerably easier if we assume that there are no
intra-time-slice edges in the 2-TBN.
Exercise 18.22⋆
In this problem, we will consider the task of learning a generalized type of Bayesian networks that involves
shared structure and parameters. Let X be a set of variables, which we assume are all binary-valued. A
module network over X partitions the variables X into K disjoint clusters, for K ≪n = |X|. All of the
module network
variables assigned to the same cluster have precisely the same parents and CPD. More precisely, such a
network deﬁnes:

18.9. Exercises
847
C
E
D
F
A
B
Cluster III
Cluster II
Cluster I
Figure 18.11
A simple module network
•
An assignment function A, which deﬁnes for each variable X, a cluster assignment A(X) ∈
{C1, . . . , CK}.
•
For each cluster Ck (k = 1, . . . , K), a graph G that deﬁnes a set of parents PaCk = U k ⊂X and a
CPD Pk(X | U k).
The module network structure deﬁnes a ground Bayesian network where, for each variable X, we have the
parents U k for k = A(X) and the CPD Pk(X | U k). Figure 18.11 shows an example of such a network.
Assume that our goal is to learn a module network that maximizes the Bayesian score given a data set D,
where we need to learn both the assignment of variables to clusters and the graph structure.
a. Deﬁne an appropriate set of parameters and an appropriate notion of suﬃcient statistics for this class
of models, and write down a precise formula for the likelihood function of a pair (A, G) in terms of
the parameters and suﬃcient statistics.
b. Draw the meta-network for the module network shown in ﬁgure 18.11.
Assuming a uniform prior
over each parameter, write down exactly (normalizing constants included) the appropriate parameter
posterior given a data set D.
c. We now turn to the problem of learning the structure of the cluster network. We will use local search,
using the following types of operators:
•
Add operators that add a parent for a cluster;
•
Delete operators that delete a parent for a cluster;
•
Node-Move operators ok→k′(X) that change from A(X) = k to A(X) = k′.
Describe an eﬃcient implementation of the Node-Move operator.
d. For each type of operator, specify (precisely) which other operators need to be reevaluated once the
operator has been taken? Brieﬂy justify your response.
e. Why did we not include edge reversal in our set of operators?
Exercise 18.23⋆
It is often useful when learning the structure of a Bayesian network to consider more global search
operations. In this problem we will consider an operator called reinsertion, which works as follows: For
reinsertion
operator
the current structure G, we choose a variable Xi to be our target variable. The ﬁrst step is to remove

848
Chapter 18. Structure Learning in Bayesian Networks
the variable from the network by severing all connections to its children and parents. We then select the
optimal set of at most Kp parents and at most Kc children for X and reinsert it into the network with
edges from the selected parents and to the selected children. Throughout this problem, assume the use of
the BIC score for structure evaluation.
a. Let Xi be our current target variable, and assume for the moment that we have somehow chosen U i
to be optimal parents of Xi. Consider the case of Kc = 1, where we want to choose the single optimal
child for Xi. Candidate children — those that do not introduce a cycle in the graph — are Y1, . . . , Yℓ.
Write an argmax expression for ﬁnding the optimal child C. Explain your answer.
b. Now consider the case of Kc = 2. How do we ﬁnd the optimal pair of children? Assuming that our
family score for any {Xk, U k} can be computed in a constant time f, what is the best asymptotic
computational complexity of ﬁnding the optimal pair of children? Explain. Extend your analysis to
larger values of Kc. What is the computational complexity of this task?
c. We now consider the choice of parents for Xi. We now assume that we have already somehow chosen
the optimal set of children and will hold them ﬁxed. Can we do the same trick when choosing the
parents? If so, show how. If not, argue why not.
Exercise 18.24
Prove proposition 18.7.
Exercise 18.25
Prove proposition 18.8.
Exercise 18.26⋆
Consider the idea of searching for a high-scoring network by searching over the space of orderings ≺over
the variables. Our task is to search for a high-scoring network that has bounded indegree k. For simplicity,
we focus on the likelihood score. For a given order ≺, let G∗
≺be the highest-likelihood network consistent
with the ordering ≺of bounded indegree k. We deﬁne scoreL(≺: D) = scoreL(G∗
≺: D). We now
search over the space of orderings using operators o that swap two adjacent nodes in the ordering, that is:
X1, . . . , Xi−1, Xi, Xi+1, . . . , Xn 7→X1, . . . , Xi−1, Xi+1, Xi, . . . , Xn.
Show how to use score decomposition properties to search this space eﬃciently.
Exercise 18.27⋆
Consider the choice between G∅and GX→Y given a data set of joint observations of binary variables X
and Y .
a. Show that scoreBIC(GX→Y
: D) > scoreBIC(G∅: D) if and only if II ˆ
PD(X; Y ) > c. What is
this constant c?
b. Suppose that we have BDe prior with uniform P ′ and α = 0. Write the condition on the counts when
scoreBDe(GX→Y
: D) > scoreBDe(G∅: D).
c. Consider empirical distributions of the form discussed in ﬁgure 18.3. That is, ˆP(x, y) = 0.25+0.5·p if
x and y are equal, and ˆP(x, y) = 0.25 −0.5 · p otherwise. For diﬀerent values of p, plot as a function
of M both the χ2 deviance measure and the mutual information. What do you conclude about these
diﬀerent functions?
d. Implement the exact p-value test described in section 18.2.2.3 for the χ2 and the mutual information
deviance measures.
e. Using the same empirical distribution, plot for diﬀerent values of M the decision boundary between
G∅and GX→Y for each of the three methods we considered in this exercise. That is, ﬁnd the value of
p at which the two alternatives have (approximately) equal score, or at which the p-value of rejecting
the null hypothesis is (approximately) 0.05.
What can you conclude about the diﬀerences between these structure selection methods?

19
Partially Observed Data
Until now, our discussion of learning assumed that the training data are fully observed: each
instance assigns values to all the variables in our domain. This assumption was crucial for some
of the technical developments in the previous two chapters. Unfortunately, this assumption is
clearly unrealistic in many settings. In some cases, data are missing by accident; for example,
some ﬁelds in the data may have been omitted in the data collection process. In other cases,
certain observations were simply not made; in a medical-diagnosis setting, for example, one
never performs all possible tests or asks all of the possible questions. Finally, some variables are
hidden, in that their values are never observed. For example, some diseases are not observed
hidden variable
directly, but only via their symptoms.
In fact, in many real-life applications of learning, the available data contain missing values.
Hence, we must address the learning problem in the presence of incomplete data.
As we
incomplete data
will see, incomplete data pose both foundational problems and computational problems. The
foundational problems are in formulating an appropriate learning task and determining when we
can expect to learn from such data. The computational problems arise from the complications
incurred by incomplete data and the construction of algorithms that address these complications.
In the ﬁrst section, we discuss some of the subtleties encountered in learning from incomplete
data and in formulating an appropriate learning problem. In subsequent sections, we examine
techniques for addressing various aspects of this task. We focus initially on the parameter-
learning task, assuming ﬁrst that the network structure is given, and then treat the more
complex structure-learning question at the end of the chapter.
19.1
Foundations
19.1.1
Likelihood of Data and Observation Models
A central concept in our discussion of learning so far was the likelihood function that measures
the probability of the data induced by diﬀerent choices of models and parameters. The likelihood
function plays a central role both in maximum likelihood estimation and in Bayesian learning.
In these developments, the likelihood function was determined by the probabilistic model we
are learning. Given a choice of parameters, the model deﬁned the probability of each instance.
In the case of fully observed data, we assumed that each instance ξ[m] in our training set D is
simply a random sample from the model.
It seems straightforward to extend this idea to incomplete data. Suppose our domain consists

850
Chapter 19. Partially Observed Data
of two random variables X and Y , and in one particular instance we observed only the value
of X to be x[m], but not the value of Y .
Then, it seems natural to assign the instance
the probability P(x[m]). More generally, the likelihood of an incomplete instance is simply
the marginal probability given our model. Indeed, the most common approach to deﬁne the
likelihood of an incomplete data set is to simply marginalize over the unobserved variables.
This approach, however, embodies some rather strong assumptions about the nature of our
data. To learn from incomplete data, we need to understand these assumptions and examine the
situation much more carefully. Recall that when learning parameters for a model, we assume
that the data were generated according to the model, so that each instance is a sample from
the model. When we have missing data, the data-generation process actually involves two

steps. In the ﬁrst step, data are generated by sampling from the model. In this step,
values of all the variables are selected. The next step determines which values we get
to observe and which ones are hidden from us. In some cases, this process is simple; for
example, some particular variable may always be hidden. In other situations, this process might
be much more complex.
To analyze the probabilistic model of the observed training set, we must consider not only
the data-generation mechanism, but also the mechanism by which data are hidden. Consider
the following two examples.
Example 19.1
We ﬂip a thumbtack onto a table, and every now and then it rolls oﬀthe table. Since a fall from
the table to the ﬂoor is quite diﬀerent from our desired experimental setup, we do not use results
from these ﬂips (they are missing). How would that change our estimation? The simple solution is
to ignore the missing values and simply use the counts from the ﬂips that we did get to observe.
That is, we pretend that missing ﬂips never happened. As we will see, this strategy can be shown to
be the correct one to use in this case.
Example 19.2
Now, assume that the experiment is performed by a person who does not like “tails” (because the
point that sticks up might be dangerous). So, in some cases when the thumbtack lands “tails,”
the experimenter throws the thumbtack on the ﬂoor and reports a missing value. However, if the
thumbtack lands “heads,” he will faithfully report it. In this case, the solution is also clear. We can
use our knowledge that every missing value is “tails” and count it as such. Note that this leads to
very diﬀerent likelihood function (and hence estimated parameters) from the strategy that we used
in the previous case.
While this example may seem contrived, many real-life scenarios have very similar properties.
For example, consider a medical trial evaluating the eﬃcacy of a drug, but one where patients can
drop out of the trial, in which case their results are not recorded. If patients drop out at random,
we are in the situation of example 19.1; on the other hand, if patients tend to drop out only when
the drug is not eﬀective for them, the situation is essentially analogous to the one in this example.
Note that in both examples, we observe sequences of the form H, T, H, ?, T, ?, . . ., but never-
theless we treat them diﬀerently. The diﬀerence between these two examples is our knowledge
about the observation mechanism. As we discussed, each observation is derived as a combi-
nation of two mechanisms: the one that determines the outcome of the ﬂip, and the one that
determines whether we observe the ﬂip. Thus, our training set actually consists of two variables
for each ﬂip: the ﬂip outcome X, and the observation variable OX, which tells us whether we
observed the value of X.

19.1. Foundations
851
X
q
OX
y
X
q
OX
y
(a) Random missing values
(b) Deliberate missing values
Figure 19.1
Observation models in two variants of the thumbtack example
Deﬁnition 19.1
Let X = {X1, . . . , Xn} be some set of random variables, and let OX = {OX1, . . . , OXn} be
their observability variable. The observability model is a joint distribution Pmissing(X, OX) =
observability
variable
observability
model
P(X)·Pmissing(OX | X), so that P(X) is parameterized by parameters θ, and Pmissing(OX | X)
is parameterized by parameters ψ.
We deﬁne a new set of random variables Y = {Y1, . . . , Yn}, where Val(Yi) = Val(Xi) ∪{?}.
The actual observation is Y , which is a deterministic function of X and OX,
Yi =

Xi
OXi = o1
?
OXi = o0.
The variables Y1, . . . , Yn represent the values we actually observe, either an actual value or a ? that
represents a missing value.
Thus, we observe the Y variable. This observation always implies that we know the value
of the OX variables, and whenever Yi ̸= ?, we also observe the value of Xi. To illustrate
the deﬁnition of this concept, we consider the probability of the observed value Y in the two
preceding examples.
Example 19.3
In the scenario of example 19.1, we have a parameter θ that describes the probability of X = 1
(Heads), and another parameter ψ that describes the probability of OX = o1. Since we assume that
the hiding mechanism is random, we can describe this scenario by the meta-network of ﬁgure 19.1a.
This network describes how the probability of diﬀerent instances (shown as plates) depend on the
parameters. As we can see, this network consists of two independent subnetworks. The ﬁrst relates
the values of X in the diﬀerent examples to the parameter θ, and the second relates the values of
OX to ψ.
Recall from our earlier discussion that if we can show that θ and ψ are independent given
the evidence, then the likelihood decomposes into a product. We can derive this decomposition as
follows. Consider the three values of Y and how they could be attained. We see that
P(Y = 1)
=
θψ
P(Y = 0)
=
(1 −θ)ψ
P(Y = ?)
=
(1 −ψ).

852
Chapter 19. Partially Observed Data
Thus, if we see a data set D of tosses with M[1], M[0], and M[?] instances that are Heads, Tails,
and ?, respectively, then the likelihood is
L(θ, ψ : D) = θM[1](1 −θ)M[0]ψM[1]+M[0](1 −ψ)M[?].
As we expect, the likelihood function in this example is a product of two functions: a function of θ,
and a function of ψ. We can easily see that the maximum likelihood estimate of θ is
M[1]
M[1]+M[0],
while the maximum likelihood estimate of ψ is
M[1]+M[0]
M[1]+M[0]+M[?].
We can also reach the conclusion regarding independence using a more qualitative analysis. At
ﬁrst glance, it appears that observing Y activates the v-structure between X and OX, rendering
them dependent. However, the CPD of Y has a particular structure, which induces context-speciﬁc
independence. In particular, we see that X and OX are conditionally independent given both
values of Y : when Y = ?, then OX is necessarily o0, in which case the edge X →Y is spurious
(as in deﬁnition 5.7); if Y ̸= ?, then Y deterministically establishes the values of both X and OX,
in which case they are independent.
Example 19.4
Now consider the scenario of example 19.2. Recall that in this example, the missing values are
a consequence of an action of the experimenter after he sees the outcome of the toss. Thus, the
probability of missing values depends on the value of X. To deﬁne the likelihood function, suppose
θ is the probability of X = 1. The observation parameters ψ consist of two values: ψOX|x1 is
probability OX = o1 when X = 1, and ψOX|x0 is the probability of OX = o1 when X = 0.
We can describe this scenario by the meta-network of ﬁgure 19.1b. In this network, OX depends
directly on X. When we get an observation Y = ?, we essentially observe the value of OX but not
of X. In this case, due to the direct edge between X and OX, the context-speciﬁc independence
properties of Y do not help: X and OX are correlated, and therefore so are their associated
parameters. Thus, we cannot conclude that the likelihood decomposes.
Indeed, when we examine the form of the likelihood, this becomes apparent. Consider the three
values of Y and how they could be attained. We see that
P(Y = 1)
=
θψOX|x1
P(Y = 0)
=
(1 −θ)ψOX|x0
P(Y = ?)
=
θ(1 −ψOX|x1) + (1 −θ)(1 −ψOX|x0).
And so, if we see a data set D of tosses with M[1], M[0], and M[?] instances that are Heads, Tails,
and ?, respectively, then the likelihood is
L(θ, ψOX|x1, ψOX|x0 : D)
=
θM[1](1 −θ)M[0]ψM[1]
OX|x1ψM[0]
OX|x0
(θ(1 −ψOX|x1) + (1 −θ)(1 −ψOX|x0))M[?].
As we can see, the likelihood function in this example is more complex than the one in the previous
example. In particular, there is no easy way of decoupling the likelihood of θ from the likelihood
of ψOX|x1 and ψOX|x0. This makes sense, since diﬀerent values of these parameters imply diﬀerent
possible values of X when we see a missing value and so aﬀect our estimate of θ; see exercise 19.1.

19.1. Foundations
853
X1
OX1
y
OX2
q
X2
Figure 19.2
An example satisfying MAR but not MCAR. Here, the observability pattern depends on the
value of underlying variables.
19.1.2
Decoupling of Observation Mechanism
As we saw in the last two examples, modeling the observation variables, that is, the process that
generated the missing values, can result in nontrivial modeling choices, which in some cases
result in complex likelihood functions. Ideally, we would hope to avoid dealing with these issues
and instead focus on the likelihood of the process that we are interested in (the actual random
variables). When can we ignore the observation variables? In the simplest case, the observation
mechanism is completely independent of the domain variables. This case is precisely the one
we encountered in example 19.1.
Deﬁnition 19.2
A missing data model Pmissing is missing completely at random (MCAR) if Pmissing |= (X ⊥OX).
missing
completely at
random
In this case, the likelihood of X and OX decomposes as a product, and we can maximize each
part separately. We have seen this decomposition in the likelihood function of example 19.3. The
implications of the decoupling is that we can maximize the likelihood of the parameters of the
distribution of X without considering the values of the parameters governing the distribution
of OX. Since we are usually only interested in the former parameters, we can simply ignore the
later parameters.
The MCAR assumption is a very strong one, but it holds in certain settings. For example,
momentary sensor failures in medical/scientiﬁc imaging (for example, ﬂecks of dust) are typically
uncorrelated with the relevant variables being measured, and they induce MCAR observation
models. Unfortunately, in many other domains the MCAR simply does not hold. For example, in
medical records, the pattern of missing values owes to the tests the patient underwent. These,
however, are determined by some of the relevant variables, such as the patient’s symptoms, the
initial diagnosis, and so on.
As it turns out, MCAR is suﬃcient but not necessary for the decomposition of the likelihood
function. We can provide a more general condition where, rather than assuming marginal inde-
pendence between OX and the values of X, we assume only that the observation mechanism
is conditionally independent of the underlying variables given other observations.
Example 19.5
Consider a scenario where we ﬂip two coins in sequence. We always observe the ﬁrst toss X1, and
based on its outcome, we decide whether to hide the outcome of the second toss X2. See ﬁgure 19.2

854
Chapter 19. Partially Observed Data
for the corresponding model. In this case, Pmissing |= (OX2 ⊥X2 | X1). In other words, the true
values of both coins are independent of whether they are hidden or not, given our observations.
To understand the issue better, let us write the model and likelihood explicitly. Because we assume
that the two coins are independent, we have two parameters θX1 and θX2 for the probability of the
two coins. In this example, the ﬁrst coin is always observed, and the observation of the second one
depends on the value of the ﬁrst. Thus, we have parameters ψOX2|x1
1 and ψOX2|x0
1 that represent
the probability of observing X2 given that X1 is heads or tails, respectively.
To derive the likelihood function, we need to consider the probability of all possible observations.
There are six possible cases, which fall in two categories.
In the ﬁrst category are the four cases where we observe both coins. By way of example, consider
the observation Y1 = y1
1 and Y2 = y0
2. The probability of this observation is clearly P(X1 =
x1
1, X2 = x0
2, OX1 = o1, OX2 = o1). Using our modeling assumption, we see that this is simply
the product θX1(1 −θX2)ψOX2|x1
1.
In the second category are the two cases where we observe only the ﬁrst coin.
By way of
example, consider the observation Y1 = y1
1, Y2 = ?. The probability of this observation is P(X1 =
x1
1, OX1 = o1, OX2 = o0). Note that the value of X2 does not play a role here. This probability is
simply the product θX1(1 −ψOX2|x1
1).
If we write all six possible cases and then rearrange the products, we see that we can write the
likelihood function as
L(θ : D)
=
θM[y1
1]
X1
(1 −θX1)M[y0
1]
θM[y1
2]
X2
(1 −θX2)M[y0
2]
ψM[y1
1,y1
2]+M[y1
1,y0
2]
OX2|x1
1
(1 −ψOX2|x1
1)M[y1
1,y?
2]
ψM[y0
1,y1
2]+M[y0
1,y0
2]
OX2|x0
1
(1 −ψOX2|x0
1)M[y0
1,y?
2].
This likelihood is a product of four diﬀerent functions, each involving just one parameter. Thus, we
can estimate each parameter independently of the rest.
As we saw in the last example, conditional independence can help us decouple the estimate
of parameters of P(X) from these of P(OX | X). Is this a general phenomenon? To answer
this question, we start with a deﬁnition.
Deﬁnition 19.3
Let y be a tuple of observations. These observations partition the variables X into two sets, the
observed variables Xy
obs = {Xi : yi ̸= ?} and the hidden ones Xy
hidden = {Xi : yi = ?}. The
values of the observed variables are determined by y, while the values of the hidden variables are
not.
We say that a missing data model Pmissing is missing at random (MAR) if for all observations y
missing at
random
with Pmissing(y) > 0, and for all xy
hidden ∈Val(Xy
hidden), we have that
Pmissing |= (oX ⊥xy
hidden | xy
obs)
where oX are the speciﬁc values of the observation variables given Y .
In words, the MAR assumption requires independence between the events oX and xy
hidden given
xy
obs.
Note that this statement is written in terms of event-level conditional independence

19.1. Foundations
855
rather than conditional independence between random variables. This generality is necessary
since every instance might have a diﬀerent pattern of observed variables; however, if the set of
observed variables is known in advance, we can state MAR as conditional independence between
random variables.
This statement implies that the observation pattern gives us no additional information about
the hidden variables given the observed variables:
Pmissing(xy
hidden | xy
obs, oX) = Pmissing(xy
hidden | xy
obs).
Why should we require the MAR assumption? If Pmissing satisﬁes this assumption, then we can
write
Pmissing(y)
=
X
xy
hidden

P(xy
obs, xy
hidden)Pmissing(oX | xy
hidden, xy
obs)

=
X
xy
hidden

P(xy
obs, xy
hidden)Pmissing(oX | xy
obs)

=
Pmissing(oX | xy
obs)
X
xy
hidden
P(xy
obs, xy
hidden)
=
Pmissing(oX | xy
obs)P(xy
obs).
The ﬁrst term depends only on the parameters ψ, and the second term depends only on
the parameters θ. Since we write this product for every observed instance, we can write the
likelihood as a product of two likelihoods, one for the observation process and the other for the
underlying distribution.
Theorem 19.1
If Pmissing satisﬁes MAR, then L(θ, ψ : D) can be written as a product of two likelihood functions
L(θ : D) and L(ψ : D).
This theorem implies that we can optimize the likelihood function in the parameters θ of
the distribution P(X) independently of the exact value the observation model parameters. In
other words, the MAR assumption is a license to ignore the observation model while learning
parameters.
The MAR assumption is applicable in a broad range of settings, but it must be considered
with care. For example, consider a sensor that measures blood pressure B but can fail to record
a measurement when the patient is overweight.
Obesity is a very relevant factor for blood
pressure, so that the sensor failure itself is informative about the variable of interest. However,
if we always have observations W of the patient’s body weight and H of the height, then OB
is conditionally independent of B given W and H. As another example, consider the patient
description in hospital records. If the patient does not have an X-ray result X, he probably does
not suﬀer from broken bones. Thus, OX gives us information about the underlying domain
variables. However, assume that the patient’s chart also contains a “primary complaint” variable,
which was the factor used by the physician in deciding which tests to perform; in this case, the
MAR assumption does hold.
In both of these cases, we see that the MAR assumption does not hold given a limited set of
observed attributes, but if we expand our set of observations, we can get the MAR assumption
to hold. In fact, one can show that we can always extend our model to produce one where the

856
Chapter 19. Partially Observed Data
MAR assumption holds (exercise 19.2). Thus, from this point onward we assume that the data
satisfy the MAR assumption, and so our focus is only on the likelihood of the observed data.
However, before applying the methods described later in this chapter, we always need to

consider the possible correlations between the variables and the observation variables,
and possibly to expand the model so as to guarantee the MAR assumption.
19.1.3
The Likelihood Function
Throughout our discussion of learning, the likelihood function has played a major role, either
on its own, or with the prior in the context of Bayesian estimation. Under the assumption of
MAR, we can continue to use the likelihood function in the same roles. From now on, assume
we have a network G over a set of variables X. In general, each instance has a diﬀerent set of
observed variables. We will denote by O[m] and o[m] the observed variables and their values
in the m’th instance, and by H[m] the missing (or hidden) variables in the m’th instance. We
use L(θ : D) to denote the probability of the observed variables in the data, marginalizing out
the hidden variables, and ignoring the observability model:
L(θ : D) =
M
Y
m=1
P(o[m] | θ).
As usual, we use ℓ(θ : D) to denote the logarithm of this function.
With this deﬁnition, it might appear that the problem of learning with missing data does
not diﬀer substantially from the problem of learning with complete data. We simply use the
likelihood function in exactly the same way. Although this intuition is true to some extent, the
computational issues associated with the likelihood function are substantially more complex in
this case.
To understand the complications, we consider a simple example on the network GX→Y with
the edge X →Y . When we have complete data, the likelihood function for this network has
the following form:
L(θX, θY |x0, θY |x1 : D) =
θM[x1]
x1
θM[x0]
x0
· θM[x0,y1]
y1|x0
θM[x0,y0]
y0|x0
· θM[x1,y1]
y1|x1
θM[x1,y0]
y0|x1
.
In the binary case, we can use the constraints to rewrite θx0 = 1 −θx1, θy0|x0 = 1 −θy1|x0,
and θy0|x1 = 1 −θy1|x1. Thus, this is a function of three parameters. For example, if we have a
data set with the following suﬃcient statistics:
x1, y1: 13
x1, y0: 16
x0, y1: 10
x0, y0: 4,
then our likelihood function has the form:
θ29
x1(1 −θx1)14 · θ10
y1|x0(1 −θy1|x0)4 · θ13
y1|x1(1 −θy1|x1)16.
(19.1)
This function is well-behaved: it is log-concave, and it has a unique global maximum that has a
simple analytic closed form.

19.1. Foundations
857
L(Q |   )
Q
Figure 19.3
A visualization of a multimodal likelihood function with incomplete data. The data
likelihood is the sum of complete data likelihoods (shown in gray lines). Each of these is unimodal, yet
their sum is multimodal.
Assume that the ﬁrst instance in the data set was X[1] = x0, Y [1] = y1. Now, consider
a situation where, rather than observing this instance, we observed only Y [1] = y1. We now
have to reason that this particular data instance could have arisen in two cases: one where
X[1] = x0 and one where X[1] = x1. In the former case, our likelihood function is as before.
In the second case, we have
θ30
x1(1 −θx1)13 · θ9
y1|x0(1 −θy1|x0)4 · θ14
y1|x1(1 −θy1|x1)16.
(19.2)
When we do not observe X[1], the likelihood is the marginal probability of the observations.
That is, we need to sum over possible assignments to the unobserved variables. This implies
that the likelihood function is the sum of the two complete likelihood functions of equation (19.1)
and equation (19.2). Since both likelihood functions are quite similar, we can rewrite this sum as
θ29
x1(1 −θx1)13 · θ9
y1|x0(1 −θy1|x0)4 · θ13
y1|x1(1 −θy1|x1)16 
θx1θy1|x1 + (1 −θx1)θy1|x0
.
This form seems quite nice, except for the last sum, which couples the parameter θx1 with
θy1|x1 and θy1|x0.
If we have more missing values, there are other cases we have to worry about. For example,
if X[2] is also unobserved, we have to consider all possible combinations for X[1] and X[2].
This results in a sum over four terms similar to equation (19.1), each one with diﬀerent counts.
In general, the likelihood function with incomplete data is the sum of likelihood functions, one
for each possible joint assignment of the missing values. Note that the number of possible
assignments is exponential in the total number of missing values.
We can think of the situation using a geometric intuition. Each one of the complete data
likelihood deﬁnes a unimodal function. Their sum, however, can be multimodal. In the worst
case, the likelihood of each of the possible assignments to the missing values contributes to a
diﬀerent peak in the likelihood function. The total likelihood function can therefore be quite
complex. It takes the form of a “mixture of peaks,” as illustrated pictorially in ﬁgure 19.3.
To make matters even more complicated, we lose the property of parameter independence,
parameter
independence
and thereby the decomposability of the likelihood function.
Again, we can understand this
likelihood
decomposability
phenomenon either qualitatively, from the perspective of graphical models, or quantitatively, by

858
Chapter 19. Partially Observed Data
X
qX
qY|X
Y
Figure 19.4
The meta-network for parameter estimation for X →Y . When X[m] is hidden but
Y [m] is observed, the trail θX →X[m] →Y [m] ←θY |X is active. Thus, the parameters are not
independent in the posterior distribution.
qy1| x0
qy1| x1
qy1| x0
qy1| x1
qy1| x0
qy1| x1
Figure 19.5
Contour plots for the likelihood function for the network X →Y , over the parameters
θy1|x0 and θy1|x1. The total number of data points is 8. (a) No X values are missing. (b) Two X values
missing. (c) Three X values missing.
looking at the likelihood function. Qualitatively, recall from section 17.4.2 that, in the complete
data case, θY |x1 and θY |x0 are independent given the data, because they are independent given
Y [m] and X[m]. But if X[m] is unobserved, they are clearly dependent. This fact is clearly
illustrated by the meta-network (as in ﬁgure 17.7) that represents the learning problem. For
example, in a simple network over two variables X →Y , we see that missing data can couple
the two parameters’ variables; see ﬁgure 19.4.
We can also see this phenomenon numerically. Assume for simplicity that θX is known.
Then, our likelihood is a function of two parameters θy1|x1 and θy1|x0.
Intuitively, if our
missing X[1] is H, then it cannot be T. Thus, the likelihood functions of the two parameters
are correlated; the more missing data we have, the stronger the correlation. This phenomenon
is shown in ﬁgure 19.5.

19.1. Foundations
859
This example shows that we have lost the local decomposability property in estimating the
local
decomposability
CPD P(Y | X). What about global decomposability between diﬀerent CPDs? Consider a simple
global
decomposability
model where there is one hidden variable H, and two observed variables X and Y , and edges
H →X and H →Y . Thus, the probability of observing the values x and y is
P(x, y) =
X
h
P(h)P(x | h)P(y | h).
The likelihood function is a product of such terms, one for each observed instance x[m], y[m],
and thus has the form
L(θ : D) =
Y
x,y
 X
h
P(h)P(x | h)P(y | h)
!M[x,y]
.
When we had complete data, we rewrote the likelihood function as a product of local like-
lihood functions, one for each CPD. This decomposition was crucial for estimating each CPD
independently of the others. In this example, we see that the likelihood is a product of sum
of products of terms involving diﬀerent CPDs. The interleaving of products and sums means
that we cannot write the likelihood as a product of local likelihood functions. Again, this result
is intuitive: Because we do not observe the variable H, we cannot decouple the estimation
of P(X | H) from that of P(Y | H). Roughly speaking, both estimates depend on how we
“reconstruct” H in each instance.
We now consider the general case. Assume we have a network G over a set of variables X. In
general, each instance has a diﬀerent set of observed variables. We use D to denote, as before,
the actual observed data values; we use H = ∪mh[m] to denote a possible assignment to all
of the missing values in the data set. Thus, the pair (D, H) deﬁnes an assignment to all of the
variables in all of our instances.
The likelihood function is
L(θ : D) = P(D | θ) =
X
H
P(D, H | θ).
Unfortunately, the number of possible assignments in this sum is exponential in the number
of missing values in the entire data set. Thus, although each of the terms P(D, H | θ) is a
unimodal distribution, the sum can have, in the worst case, an exponential number of modes.
However, unimodality is not the only property we lose. Recall that our likelihood function in
the complete data case was compactly represented as a product of local terms. This property
was important both for the analysis of the likelihood function and for the task of evaluating
the likelihood function. What about the incomplete data likelihood? If we use a straightforward
representation, we get an exponential sum of terms, which is clearly not useful. Can we use
additional properties of the data to help in representing the likelihood? Recall that we assume
that diﬀerent instances are independent of each other. This allows us to write the likelihood
function as a product over the probability of each partial instance.

860
Chapter 19. Partially Observed Data
Proposition 19.1
Assuming IID data, the likelihood can be written as
L(θ : D) =
Y
m
P(o[m] | θ) =
Y
m
X
h[m]
P(o[m], h[m] | θ).
This proposition shows that, to compute the likelihood function, we need to perform inference
for each instance, computing the probability of the observations. As we discussed in section 9.1,
this problem can be intractable, depending on the network structure and the pattern of missing
values. Thus, for some learning problems, even the task of evaluating likelihood function for a
particular choice of parameters is a diﬃcult computational problem. This observation suggests
that optimizing the choice of parameters for such networks can be computationally challenging.
To conclude, in the presence of partially observed data, we have lost all of the important

properties of our likelihood function: its unimodality, its closed-form representation, and
the decomposition as a product of likelihoods for the diﬀerent parameters. Without these
properties, the learning problem becomes substantially more complex.
19.1.4
Identiﬁability
Another issue that arises in the context of missing data is our ability to identify uniquely a
model from the data.
Example 19.6
Consider again our thumbtack tossing experiments. Suppose the experimenter can randomly choose
to toss one of two thumbtacks (say from two diﬀerent brands). Due to a miscommunication between
the statistician and the experimenter, only the toss outcomes were recorded, but not the brand of
thumbtack used.
To model the experiment, we assume that there is a hidden variable H, so that if H = h1,
the experimenter tossed the ﬁrst thumbtack, and if H = h2, the experimenter tossed the second
thumbtack. The parameters of our model are θH, θX|h1, and θX|h2, denoting the probability of
choosing the ﬁrst thumbtack, and the probability of heads in each thumbtack. This setting satisﬁes
MCAR (since H is hidden). It is straightforward to write the likelihood function:
L(θ : D) = P(x1)M[1](1 −P(x1))M[0],
where
P(x1) = θHθX|h1 + (1 −θH)θX|h2.
If we examine this term, we see that P(x1) is the weighted average of θX|h1 and θX|h2. There
are multiple choices of these two parameters and θH that achieve the same value of P(x1). For
example, θH = 0.5,θX|h1 = 0.5,θX|h2 = 0.5 leads to the same behavior as θH = 0.5,θX|h1 =
0.8,θX|h2 = 0.2. Because the likelihood of the data is a function only of P(x1), we conclude that
there is a continuum of parameter choices that achieve the maximum likelihood.
This example illustrates a situation where the learning problem is underconstrained: Given
the observations, we cannot hope to recover a unique set of parameters. Recall that in previous
sections, we showed that our estimates are consistent and thus will approach the true parameters

19.1. Foundations
861
when suﬃcient data are available. In this example, we cannot hope that more data will let us
recover the true parameters.
Before formally treating the issue, let us examine another example that does not involve
hidden variables.
Example 19.7
Suppose we conduct an experiment where we toss two coins X and Y that may be correlated
with each other. After each toss, one of the coins is hidden from us using a mechanism that is
totally unrelated to the outcome of the coins. Clearly, if we have suﬃcient observations (that is,
the mechanism does not hide one of the coins consistently), then we can estimate the marginal
probability of each of the coins. Can we, however, learn anything about how they depend on each
other? Consider some pair of marginal probabilities P(X) and P(Y ); because we never get to
observe both coins together, any joint distribution that has these marginals has the same likelihood.
In particular, a model where the two coins are independent achieves maximum likelihood but is
not the unique point. In fact, in some cases a model where one is a deterministic function of the
other also achieves the same likelihood (for example, if we have the same frequency of observed X
heads as of observed Y heads).
These two examples show that in some learning situations we cannot resolve all aspects of
the model by learning from data. This issue has been examined extensively in statistics, and is
known as identiﬁability, and we brieﬂy review the relevant notions here.
identiﬁability
Deﬁnition 19.4
Suppose we have a parametric model with parameters θ ∈Θ that deﬁnes a distribution P(X | θ)
over a set X of measurable variables. A choice of parameters θ is identiﬁable if there is no θ′ ̸= θ
identiﬁability
such that P(X | θ) = P(X | θ′). A model is identiﬁable if all parameter choices θ ∈Θ are
identiﬁable.
In other words, a model is identiﬁable if each choice of parameters implies a diﬀerent
distribution over the observed variables.
Nonidentiﬁability implies that there are parameter
settings that are indistinguishable given the data, and therefore cannot be identiﬁed from the
data. Usually this is a sign that the parameterization is redundant with respect to the actual
observations. For example, the model we discuss in example 19.6 is unidentiﬁable, since there
are regions in the parameters space that induce the same probability on the observations.
Another source of nonidentiﬁability is hidden variables.
Example 19.8
Consider now a diﬀerent experiment where we toss two thumbtacks from two diﬀerent brands:
Acme (A) and Bond (B). In each round, both thumbtacks are tossed and the entries are recorded.
Unfortunately, due to scheduling constraints, two diﬀerent experimenters participated in the ex-
periment; each used a slightly diﬀerent hand motion, changing the probability of heads and tails.
Unfortunately, the experimenter name was not recorded, and thus we only have measurements of
the outcome in each experiment.
To model this situation, we have three random variables to describe each round. Suppose A
denotes the outcome of the toss of the Acme thumbtack and B the outcome of the toss of the Bond
thumbtack. Because these outcomes depend on the experimenter, we add another (hidden) variable
H that denotes the name of the experimenter. We assume that the model is such that A and B are
independent given H. Thus,
P(A, B) =
X
h
P(h)P(A | h)P(B | h).

862
Chapter 19. Partially Observed Data
Because we never observe H, the parameters of this model can be reshuﬄed by “renaming” the
values of the hidden variable. If we exchange the roles of h0 and h1, and change the corresponding
entries in the CPDs, we get a model with exactly the same likelihood, but with diﬀerent parameters.
In this case, the likelihood surface is duplicated. For each parameterization, there is an equivalent
parameterization by exchanging the names of the hidden variable. We conclude that this model is
not identiﬁable.
This type of unidentiﬁability exists in any model where we have hidden variables we never
observe. When we have several hidden variables, the problem is even worse, and the number of
equivalent “reﬂections” of each solution is exponential in the number of hidden variables.
Although such a model is not identiﬁable due to “renaming” transformations, it is in some
sense better than the model of example 19.6, where we had an entire region of equivalent
parameterizations. To capture this distinction, we can deﬁne a weaker version of identiﬁability.
Deﬁnition 19.5
Suppose we have a parametric model with parameters θ ∈Θ that deﬁnes a distribution P(X | θ)
over a set X of measurable variables. A choice of parameters θ is locally identiﬁable if there is a
locally
identiﬁable
constant ϵ > 0 such that there is no θ′ ̸= θ such that ||θ −θ′||2 < ϵ and P(X | θ) = P(X | θ′).
A model is locally identiﬁable if all parameter choices θ ∈Θ are locally identiﬁable.
In other words, a model is locally identiﬁable if each choice of parameters deﬁnes a distribu-
tion that is diﬀerent than the distribution of neighboring parameterization in a suﬃciently small
neighborhood. This deﬁnition implies that, from a local perspective, the model is identiﬁable.
The model of example 19.8 is locally identiﬁable, while the model of example 19.6 is not.
It is interesting to note that we have encountered similar issues before: As we discussed in
chapter 18, our data do not allow us to distinguish between structures in the same I-equivalence
class.
This limitation did not prevent us from trying to learn a model from data, but we
needed to avoid ascribing meaning to directionality of edges that are not consistent throughout
the I-equivalence class. The same approach holds for unidentiﬁability due to missing data: A

nonidentiﬁable model does not mean that we should not attempt to learn models from
data. But it does mean that we should be careful not to read into the learned model
more than what can be distinguished given the available data.
19.2
Parameter Estimation
As for the fully observable case, we ﬁrst consider the parameter estimation task.
As with
complete data, we consider two approaches to estimation, maximum likelihood estimation (MLE),
and Bayesian estimation. We start with a discussion of methods for MLE estimation, and then
consider the Bayesian estimation problem in the next section.
More precisely, suppose we are given a network structure G and the form of the CPDs. Thus,
we only need to set the parameters θ to deﬁne a distribution P(X | θ). We are also given
a data set D that consists of M partial instances to X. We want to ﬁnd the values ˆθ that
maximize the log-likelihood function: ˆθ = arg maxθ ℓ(θ : D). As we discussed, in the presence
of incomplete data, the likelihood does not decompose. And so the problem requires optimizing
a highly nonlinear and multimodal function over a high-dimensional space (one consisting of
parameter assignments to all CPDs). There are two main classes of methods for performing

19.2. Parameter Estimation
863
this optimization: a generic nonconvex optimization algorithm, such as gradient ascent; and
expectation maximization, a more specialized approach for optimizing likelihood functions.
19.2.1
Gradient Ascent
One approach to handle this optimization task is to apply some variant of gradient ascent, a
gradient ascent
standard function-optimization technique applied to the likelihood function (see appendix A.5.2).
These algorithms are generic and can be applied if we can evaluate the gradient function at
diﬀerent parameter choices.
19.2.1.1
Computing the Gradient
The main technical question we need to tackle is how to compute the gradient. We begin with
considering the derivative relative to a single CPD entry P(x | u). We can then use this result
as the basis for computing derivatives relative to other parameters, which arise when we have
structured CPDs.
Lemma 19.1
Let B be a Bayesian network with structure G over X that induces a probability distribution P,
let o be a tuple of obserations for some of the variables, and let X ∈X be some random variable.
Then
∂
∂P(x | u)P(o) =
1
P(x | u)P(x, u, o)
if P(x | u) > 0, where x ∈Val(X), u ∈Val(PaX).
Proof We start by considering the case where the evidence is a full assignment ξ to all variables.
The probability of such an assignment is a product of the relevant CPD entries.
Thus, the
gradient of this product with respect to the parameter P(x | u) is simply
∂
∂P(x | u)P(ξ) =

1
P (x|u)P(ξ)
if ξ⟨X, PaX⟩= ⟨x, u⟩
0
otherwise.
We now consider the general case where the evidence is a partial assignment. As usual, we
can write P(o) as a sum over all full assignments consistent with P(o)
P(o) =
X
ξ:ξ⟨O⟩=o
P(ξ).
Applying the diﬀerentiation formula to each of these full assignments, we get
∂
∂P(x | u)P(o)
=
X
ξ:ξ⟨O⟩=o
∂
∂P(x | u)P(ξ)
=
X
ξ:ξ⟨O⟩=o,ξ⟨X,PaX⟩=⟨x,u⟩
1
P(x | u)P(ξ)
=
1
P(x | u)P(x, u, o).

864
Chapter 19. Partially Observed Data
C
D
B
A
Figure 19.6
A simple network used to illustrate learning algorithms for missing data
When o is inconsistent with x or u, then the gradient is 0, since the probability P(x, u, o)
is 0 in this case. When o is consistent with x and u, the gradient is the ratio between the
probability P(x, u, o) and the parameter P(x | u). Intuitively, this ratio takes into account the
weight of the cases where P(x | u) is “used” in the computation of P(o). Increasing P(x | u)
by a small amount will increase the probability of these cases by a multiplicative factor.
The lemma does not deal with the case where P(x | u) = 0, since we cannot divide by 0.
Note, however, that the proof shows that this division is mainly a neat manner of writing the
product of all terms except P(x | u). Thus, even in this extreme case we can use a similar proof
to compute the gradient, although writing the term explicitly is less elegant. Since in learning
we usually try to avoid extreme parameter assignments, we will continue our discussion with
the assumption that P(x | u) > 0.
An immediate consequence of lemma 19.1 is the form of the gradient of the log-likelihood
function.
Theorem 19.2
Let G be a Bayesian network structure over X, and let D = {o[1], . . . , o[M]} be a partially
observable data set. Let X be a variable and U its parents in G. Then
∂ℓ(θ : D)
∂P(x | u) =
1
P(x | u)
M
X
m=1
P(x, u | o[m], θ).
The proof is left as an exercise (exercise 19.5).
This theorem provides the form of the gradient for table-CPDs. For other CPDs, such as
noisy-or CPDs, we can use the chain rule of derivatives to compute the gradient. Suppose that
chain rule of
derivatives
the CPD entries of P(X | U) are written as functions of some set of parameters θ. Then, for a
speciﬁc parameter θ ∈θ, we have
∂ℓ(θ : D)
∂θ
=
X
x,u
∂ℓ(θ : D)
∂P(x | u)
∂P(x | u)
∂θ
,
where the ﬁrst term is the derivative of the log-likelihood function when parameterized in terms
of the table-CPDs induced by θ. For structured CPDs, we can use this formula to compute the
gradient with respect to the CPD parameters. For some CPDs, however, this may not be the
most eﬃcient way of computing these gradients; see exercise 19.4.

19.2. Parameter Estimation
865
19.2.1.2
An Example
We consider a simple example to clarify the concept. Consider the network shown in ﬁgure 19.6,
and a partially speciﬁed data case o = ⟨a1, ?, ?, d0⟩.
We want to compute the gradient of one family of parameters P(D | c0) given the observation
o. Using theorem 19.2, we know that
∂log P(o)
∂P(d0 | c0) = P(d0, c0 | o)
P(d0 | c0) ,
and similarly for other values of D and C.
Assume that our current θ is:
θa1
= 0.3
θb1
= 0.9
θc1|a0,b0 = 0.83
θc1|a0,b1 = 0.09
θc1|a1,b0 = 0.6
θc1|a1,b1 = 0.2
θd1|c0
= 0.1
θd1|c1
= 0.8.
In this case, the probabilities of the four data cases that are consistent with o are
P(⟨a1, b1, c1, d0⟩)
=
0.3 · 0.9 · 0.2 · 0.2 = 0.0108
P(⟨a1, b1, c0, d0⟩)
=
0.3 · 0.9 · 0.8 · 0.9 = 0.1944
P(⟨a1, b0, c1, d0⟩)
=
0.3 · 0.1 · 0.6 · 0.2 = 0.0036
P(⟨a1, b0, c0, d0⟩)
=
0.3 · 0.1 · 0.4 · 0.9 = 0.0108.
To compute the posterior probability of these instances given the partial observation o, we
divide the probability of each instance with the total probability, which is 0.2196, that is,
P(⟨a1, b1, c1, d0⟩| o)
=
0.0492
P(⟨a1, b1, c0, d0⟩| o)
=
0.8852
P(⟨a1, b0, c1, d0⟩| o)
=
0.0164
P(⟨a1, b0, c0, d0⟩| o)
=
0.0492.
Using these computations, we see that
∂log P(o)
∂P(d1 | c0)
=
P(d1, c0 | o)
P(d1 | c0)
= 0
0.1 = 0
∂log P(o)
∂P(d0 | c0)
=
P(d0, c0 | o)
P(d0 | c0)
= 0.8852 + 0.0492
0.9
= 1.0382
∂log P(o)
∂P(d1 | c1)
=
P(d1, c1 | o)
P(d1 | c1)
= 0
0.8 = 0
∂log P(o)
∂P(d0 | c1)
=
P(d0, c1 | o)
P(d0 | c1)
= 0.0492 + 0.0164
0.2
= 0.328.

866
Chapter 19. Partially Observed Data
These computations show that we can increase the probability of the observations o by either
increasing P(d0 | c0) or P(d0 | c1). Moreover, increasing the former parameter will lead to a
bigger change in the probability of o than a similar increase in the latter parameter.
Now suppose we have an observation o′ = ⟨a0, ?, ?, d1⟩. We can repeat the same computation
as before and see that
∂log P(o′)
∂P(d1 | c0)
=
P(d1, c0 | o′)
P(d1 | c0)
= 0.2836
0.1
= 2.8358
∂log P(o′)
∂P(d0 | c0)
=
P(d0, c0 | o′)
P(d0 | c0)
= 0
0.9 = 0
∂log P(o′)
∂P(d1 | c1)
=
P(d1, c1 | o′)
P(d1 | c1)
= 0.7164
0.8
= 0.8955
∂log P(o′)
∂P(d0 | c1)
=
P(d0, c1 | o′)
P(d0 | c1)
= 0
0.2 = 0.
Suppose our data set consists only of these two instances. The gradient of the log-likelihood
function is the sum of the gradient with respect to the two instances. We get that
∂ℓ(θ : D)
∂P(d1 | c0)
=
2.8358
∂ℓ(θ : D)
∂P(d0 | c0)
=
1.0382
∂ℓ(θ : D)
∂P(d1 | c1)
=
0.8955
∂ℓ(θ : D)
∂P(d0 | c1)
=
0.328.
Note that all the gradients are nonnegative. Thus, increasing any of the parameters in the CPD
P(D | C) will increase the likelihood of the data. It is clear, however, that we cannot increase
both P(d1 | c0) and P(d0 | c0) at the same time, since this will lead to an illegal conditional
probability. One way of solving this is to use a single parameter θd1|c0 and write
P(d1 | c0) = θd1|c0
P(d0 | c0) = 1 −θd1|c0.
Using the chain rule of conditional probabilities, we have that
∂ℓ(θ : D)
∂θd1|c0
=
∂P(d1 | c0)
∂θd1|c0
∂ℓ(θ : D)
∂P(d1 | c0) + ∂P(d0 | c0)
∂θd1|c0
∂ℓ(θ : D)
∂P(d0 | c0)
=
∂ℓ(θ : D)
∂P(d1 | c0) −∂ℓ(θ : D)
∂P(d0 | c0)
=
2.8358 −1.0382 = 1.7976.
Thus, in this case, we prefer to increase P(d1 | c0) and decrease P(d0 | c0), since the resulting
increase in the probability of o′ will be larger than the decrease in the probability of o.

19.2. Parameter Estimation
867
Algorithm 19.1 Computing the gradient in a network with table-CPDs
Procedure Compute-Gradient (
G,
// Bayesian network structure over X1, . . . , Xn
θ,
// Set of parameters for G
D
// Partially observed data set
)
1
// Initialize data structures
2
for each i = 1, . . . , n
3
for each xi, ui ∈Val(Xi, PaG
Xi)
4
¯
M[xi, ui] ←0
5
// Collect probabilities from all instances
6
for each m = 1 . . . M
7
Run clique tree calibration on ⟨G, θ⟩using evidence o[m]
8
for each i = 1, . . . , n
9
for each xi, ui ∈Val(Xi, PaG
Xi)
10
¯
M[xi, ui] ←
¯
M[xi, ui] + P(xi, ui | o[m])
11
// Compute components of the gradient vector
12
for each i = 1, . . . , n
13
for each xi, ui ∈Val(Xi, PaG
Xi)
14
δxi|ui ←
1
θxi|ui
¯
M[xi, ui]
15
return {δxi,|ui : ∀i = 1, . . . , n, ∀(xi, ui) ∈Val(Xi, PaG
Xi)}
19.2.1.3
Gradient Ascent Algorithm
We now generalize these ideas to case of an arbitrary network. For now we focus on the case
of table-CPDs. In this case, the gradient is given by theorem 19.2. To compute the gradient for
the CPD P(X | U), we need to compute the joint probability of x and u relative to our current
parameter setting θ and each observed instance o[m]. In other words, we need to compute the
joint distribution P(X[m], U[m] | o[m], θ) for each m. We can do this by running an inference
procedure for each data case.
Importantly, we can do all of the required inference for

each data case using one clique tree calibration, since the family preservation property
guarantees that X and its parents U will be together in some clique in the tree. Procedure
Compute-Gradient, shown in algorithm 19.1, performs these computations.
Once we have a procedure for computing the gradient, it seems that we can simply plug it
into a standard package for gradient ascent and optimize the parameters. As we have illustrated,
however, there is one issue that we need to deal with.
It is not hard to conﬁrm that all
components of the gradient vector are nonnegative. This is natural, since increasing each of the
parameters will lead to higher likelihood. Thus, a step in the gradient direction will increase all
the parameters. Remember, however, that we want to ensure that our parameters describe a legal
probability distribution. That is, the parameters for each conditional probability are nonnegative
and sum to one.

868
Chapter 19. Partially Observed Data
In the preceding example, we saw one approach that works well when we have binary vari-
ables. In general networks, there are two common approaches to deal with this issue. The ﬁrst
approach is to modify the gradient ascent procedure we use (for example, conjugate gradient)
to respect these constraints. First, we must project each gradient vector onto the hyperplane
that satisﬁes the linear constraints on the parameters; this step is fairly straightforward (see
exercise 19.6). Second, we must ensure that parameters are nonnegative; this requires restricting
possible steps to avoid stepping out of the allowed bounds.
The second approach is to reparameterize the problem. Suppose we introduce new parameters
reparameteriza-
tion
λx|u, and deﬁne
P(x | u) =
eλx|u
P
x′∈Val(X) eλx′|u ,
(19.3)
for each X and its parents U. Now, any choice of values for the λ parameters will lead to legal
conditional probabilities. We can compute the gradient of the log-likelihood with respect to the
λ parameters using the chain rule of partial derivatives, and then use standard (unmodiﬁed)
conjugate gradient ascent procedure. See exercise 19.7.
Another way of dealing with the constraints implied by conditional probabilities is to use
the method of Lagrange multipliers, reviewed in appendix A.5.3. Applying this method to the
Lagrange
multipliers
optimization of the log-likelihood leads to the method we discuss in the next section, and we
defer this discussion; see also exercise 19.8.
Having dealt with this subtlety, we can now apply any gradient ascent procedure to ﬁnd
a local maximum of the likelihood function. As discussed, in most missing value problems,
the likelihood function has many local maxima. Unfortunately, gradient ascent procedures are
guaranteed to achieve only a local maximum of the function.
Many of the techniques we
discussed earlier in the book can be used to avoid local maxima and increase our chances of
ﬁnding a global maximum, or at least a better local maximum: the general-purpose methods of
appendix A.4.2, such as multiple random starting points, or applying random perturbations to
convergence points; and the more specialized data perturbation methods of algorithm 18.1.
19.2.2
Expectation Maximization (EM)
An alternative algorithm for optimizing a likelihood function is the expectation maximization
expectation
maximization
algorithm. Unlike gradient ascent, EM is not a general-purpose algorithm for nonlinear function
optimization. Rather, it is tailored speciﬁcally to optimizing likelihood functions, attempting to
build on the tools we had for solving the problem with complete data.
19.2.2.1
Intuition
Recall that when learning from complete data, we can collect suﬃcient statistics for each CPD.
We can then estimate parameters that maximize the likelihood with respect to these statistics.
As we saw, in the case of missing data, we do not have access to the full suﬃcient statistics.
Thus, we cannot use the same strategy for our problem. For example, in a simple X →Y
network, if we see the training instance ⟨?, y1⟩, then we do not know whether to count this
instance toward the count M[x1, y1] or toward the count M[x0, y1].

19.2. Parameter Estimation
869
A simple approach is to “ﬁll in” the missing values arbitrarily. For example, there are strategies
that ﬁll in missing values with “default values” (say false) or by randomly choosing a value. Once
we ﬁll in all the missing values, we can use standard, complete data learning procedure. Such
approaches are called called data imputation methods in statistics.
data imputation
The problem with such an approach is that the procedure we use for ﬁlling in the missing
values introduces a bias that will be reﬂected in the parameters we learn. For example, if we
ﬁll all missing values with false, then our estimate will be skewed toward higher (conditional)
probability of false. Similarly, if we use a randomized procedure for ﬁlling in values, then the
probabilities we estimate will be skewed toward the distribution from which we sample missing
values. This might be better than a skew toward one value, but it still presents a problem.
Moreover, when we consider learning with hidden variables, it is clear that an imputation
procedure will not help us.
The values we ﬁll in for the hidden variable are conditionally
independent from the values of the other variables, and thus, using the imputed values, we will
not learn any dependencies between the hidden variable and the other variables in the network.
A diﬀerent approach to ﬁlling in data takes the perspective that, when learning with missing
data, we are actually trying to solve two problems at once: learning the parameters, and
hypothesizing values for the unobserved variables in each of the data cases. Each of these tasks
is fairly easy when we have the solution to the other. Given complete data, we have the statistics,
and we can estimate parameters using the MLE formulas we discussed in chapter 17. Conversely,
given a choice of parameters, we can use probabilistic inference to hypothesize the likely values
(or the distribution over possible values) for unobserved variables. Unfortunately, because we
have neither, the problem is diﬃcult.
The EM algorithm solves this “chicken and egg” problem using a bootstrap approach. We
start out with some arbitrary starting point. This can be either a choice of parameters, or some
initial assignment to the hidden variables; these assignments can be either random, or selected
using some heuristic approach. Assuming, for concreteness, that we begin with a parameter
assignment, the algorithm then repeats two steps. First, we use our current parameters to
complete the data, using probabilistic inference. We then treat the completed data as if it were
data completion
observed and learn a new set of parameters.
More precisely, suppose we have a guess θ0 about the parameters of the network.
The
resulting model deﬁnes a joint distribution over all the variables in the domain. Given a par-
tial instance, we can compute the posterior (using our putative parameters) over all possible
assignments to the missing values in that instance. The EM algorithm uses this probabilis-
tic completion of the diﬀerent data instances to estimate the expected value of the suﬃcient
statistics. It then ﬁnds the parameters θ1 that maximize the likelihood with respect to these
statistics.
Somewhat surprisingly, this sequence of steps provably improves our parameters. In fact, as
we will prove formally, unless our parameters have not changed due to these steps (such that
θ0 = θ1), our new parameters θ1 necessarily have a higher likelihood than θ0. But now we
can iteratively repeat this process, using θ1 as our new starting point. Each of these operations
can be thought of as taking an “uphill” step in our search space. More precisely, we will show
(under very benign assumptions) that: each iteration is guaranteed to improve the log-likelihood
function; that this process is guaranteed to converge; and that the convergence point is a
ﬁxed point of the likelihood function, which is essentially always a local maximum. Thus, the
guarantees of the EM algorithm are similar to those of gradient ascent.

870
Chapter 19. Partially Observed Data
19.2.2.2
An Example
We start with a simple example to clarify the concepts. Consider the simple network shown
in ﬁgure 19.6. In the fully observable case, our maximum likelihood parameter estimate for the
parameter ˆθd1|c0 is:
ˆθd1|c0 = M[d1, c0]
M[c0]
=
PM
m=1 11{ξ[m]⟨D, C⟩= ⟨d1, c0⟩}
PM
m=1 11{ξ[m]⟨C⟩= c0}
,
where ξ[m] is the m’th training example. In the fully observable case, we knew exactly whether
the indicator variables were 0 or 1. Now, however, we do not have complete data cases, so we
no longer know the value of the indicator variables.
Consider a partially speciﬁed data case o = ⟨a1, ?, ?, d0⟩. There are four possible instantiations
to the missing variables B, C which could have given rise to this partial data case: ⟨b1, c1⟩,
⟨b1, c0⟩, ⟨b0, c1⟩, ⟨b0, c0⟩. We do not know which of them is true, or even which of them is
more likely.
However, assume that we have some estimate θ of the values of the parameters in the model.
In this case, we can compute how likely each of these completions is, given our distribution.
That is, we can deﬁne a distribution Q(B, C) = P(B, C | o, θ) that induces a distribution over
the four data cases. For example, if our parameters θ are:
θa1
= 0.3
θb1
= 0.9
θd1|c0
= 0.1
θd1|c1
= 0.8
θc1|a0,b0 = 0.83
θc1|a1,b0 = 0.6
θc1|a0,b1 = 0.09
θc1|a1,b1 = 0.2,
then Q(B, C) = P(B, C | a1, d0, θ) is deﬁned as:
Q(⟨b1, c1⟩) = 0.3 · 0.9 · 0.2 · 0.2/0.2196 = 0.0492
Q(⟨b1, c0⟩) = 0.3 · 0.9 · 0.8 · 0.9/0.2196 = 0.8852
Q(⟨b0, c1⟩) = 0.3 · 0.1 · 0.6 · 0.2/0.2196 = 0.0164
Q(⟨b0, c0⟩) = 0.3 · 0.1 · 0.4 · 0.9/0.2196 = 0.0492,
where 0.2196 is a normalizing factor, equal to P(a1, d0 | θ).
If we have another example o′ = ⟨?, b1, ?, d1⟩. Then Q′(A, C) = P(A, C | b1, d1, θ) is
deﬁned as:
Q′(⟨a1, c1⟩) =
0.3 · 0.9 · 0.2 · 0.8/0.1675 = 0.2579
Q′(⟨a1, c0⟩) =
0.3 · 0.9 · 0.8 · 0.1/0.1675 = 0.1290
Q′(⟨a0, c1⟩) = 0.7 · 0.9 · 0.09 · 0.8/0.1675 = 0.2708
Q′(⟨a0, c0⟩) = 0.7 · 0.9 · 0.91 · 0.1/0.1675 = 0.3423.
Intuitively, now that we have estimates for how likely each of the cases is, we can treat
these estimates as truth.
That is, we view our partially observed data case ⟨a1, ?, ?, d0⟩as
consisting of four complete data cases, each of which has some weight lower than 1.
The
weighted data
instances
weights correspond to our estimate, based on our current parameters, on how likely is this
particular completion of the partial instance. (This approach is somewhat reminiscent of the
weighted particles in the likelihood weighting algorithm.) Importantly, as we will discuss, we do

19.2. Parameter Estimation
871
not usually explicitly generate these completed data cases; however, this perspective is the basis
for the more sophisticated methods.
More generally, let H[m] denote the variables whose values are missing in the data instance
o[m]. We now have a data set D+ consisting of
∪m{⟨o[m], h[m]⟩: h[m] ∈Val(H[m])},
where each data case ⟨o[m], h[m]⟩has weight Q(h[m]) = P(h[m] | o[m], θ).
We can now do standard maximum likelihood estimation using these completed data cases.
We compute the expected suﬃcient statistics:
expected
suﬃcient
statistics
¯
Mθ[y] =
M
X
m=1
X
h[m]∈Val(H[m])
Q(h[m])11{ξ[m]⟨Y ⟩= y}.
We then use these expected suﬃcient statistics as if they were real in the MLE formula. For
example:
˜θd1|c0 =
¯
Mθ[d1, c0]
¯
Mθ[c0]
.
In our example, suppose the data consist of the two instances o = ⟨a1, ?, ?, d0⟩and o′ =
⟨?, b1, ?, d1⟩. Then, using the calculated Q and Q′ from above, we have that
¯
Mθ[d1, c0]
=
Q′(⟨a1, c0⟩) + Q′(⟨a0, c0⟩)
=
0.1290 + 0.3423 = 0.4713
¯
Mθ[c0]
=
Q(⟨b1, c0⟩) + Q(⟨b0, c0⟩) + Q′(⟨a1, c0⟩) + Q′(⟨a0, c0⟩)
=
0.8852 + 0.0492 + 0.1290 + 0.3423 = 1.4057.
Thus, in this example, using these particular parameters to compute expected suﬃcient statistics,
we get
˜θd1|c0 = 0.4713
1.4057 = 0.3353.
Note that this estimate is quite diﬀerent from the parameter θd1|c0 = 0.1 that we used in our
estimate of the expected counts. The initial parameter and the estimate are diﬀerent due to the
incorporation of the observations in the data.
This intuition seems nice. However, it may require an unreasonable amount of computation.
To compute the expected suﬃcient statistics, we must sum over all the completed data cases.
The number of these completed data cases is much larger than the original data set. For each
o[m], the number of completions is exponential in the number of missing values. Thus, if we
have more than few missing values in an instances, an implementation of this approach will not
be able to ﬁnish computing the expected suﬃcient statistics.
Fortunately, it turns out that there is a better approach to computing the expected suﬃcient
statistic than simply summing over all possible completions. Let us reexamine the formula for
an expected suﬃcient statistic, for example, ¯
Mθ[c1]. We have that
¯
Mθ[c1] =
M
X
m=1
X
h[m]∈Val(H[m])
Q(h[m])11{ξ[m]⟨C⟩= c1}.

872
Chapter 19. Partially Observed Data
Let us consider the internal summation, say for a data case o = ⟨a1, ?, ?, d0⟩. We have four
possible completions, as before, but we are only summing over the two that are consistent with
c1, that is, Q(b1, c1) + Q(b0, c1). This expression is equal to Q(c1) = P(c1 | a1, d0, θ) =
P(c1 | o[1], θ). This idea clearly generalizes to our other data cases. Thus, we have that
¯
Mθ[c1] =
M
X
m=1
P(c1 | o[m], θ).
Now, recall our formula for suﬃcient statistics in the fully observable case:
M[c1] =
M
X
m=1
11{ξ[m]⟨C⟩= c1}.
Our new formula is identical, except that we have substituted our indicator variable — either 0
or 1 — with a probability that is somewhere between 0 and 1. Clearly, if in a certain data case
we get to observe C, the indicator variable and the probability are the same. Thus, we can view
the expected suﬃcient statistics as ﬁlling in soft estimates for hard data when the hard data are
not available.
We stress that we use posterior probabilities in computing expected suﬃcient statistics. Thus,
although our choice of θ clearly inﬂuences the result, the data also play a central role. This is
in contrast to the probabilistic completion we discussed earlier that used a prior probability to
ﬁll in values, regardless of the evidence on the other variables in the same instances.
19.2.2.3
The EM Algorithm for Bayesian Networks
We now present the basic EM algorithm and describe the guarantees that it provides.
Networks with Table-CPDs
Consider the application of the EM algorithm to a general Bayesian
network with table-CPDs. Assume that the algorithm begins with some initial parameter assign-
ment θ0, which can be chosen either randomly or using some other approach. (The case where
we begin with some assignment to the missing data is analogous.) The algorithm then repeatedly
executes the following phases, for t = 0, 1, . . .
Expectation (E-step): The algorithm uses the current parameters θt to compute the expected
expected
suﬃcient
statistics
suﬃcient statistics.
•
For each data case o[m] and each family X, U, compute the joint distribution P(X, U |
o[m], θt).
•
Compute the expected suﬃcient statistics for each x, u as:
¯
Mθt[x, u] =
X
m
P(x, u | o[m], θt).
This phase is called the E-step (expectation step) because the counts used in the formula are
E-step
the expected suﬃcient statistics, where the expectation is with respect to the current set of
parameters.

19.2. Parameter Estimation
873
Algorithm 19.2 Expectation-maximization algorithm for BN with table-CPDs
Procedure Compute-ESS (
G,
// Bayesian network structure over X1, . . . , Xn
θ,
// Set of parameters for G
D
// Partially observed data set
)
1
// Initialize data structures
2
for each i = 1, . . . , n
3
for each xi, ui ∈Val(Xi, PaG
Xi)
4
¯
M[xi, ui] ←0
5
// Collect probabilities from all instances
6
for each m = 1 . . . M
7
Run inference on ⟨G, θ⟩using evidence o[m]
8
for each i = 1, . . . , n
9
for each xi, ui ∈Val(Xi, PaG
Xi)
10
¯
M[xi, ui] ←
¯
M[xi, ui] + P(xi, ui | o[m])
11
return { ¯
M[xi, ui] : ∀i = 1, . . . , n, ∀xi, ui ∈Val(Xi, PaG
Xi)}
Procedure Expectation-Maximization (
G,
// Bayesian network structure over X1, . . . , Xn
θ0,
// Initial set of parameters for G
D
// Partially observed data set
)
1
for each t = 0, 1 . . . , until convergence
2
// E-step
3
{ ¯
Mt[xi, ui]} ←Compute-ESS(G, θt, D)
4
// M-step
5
for each i = 1, . . . , n
6
for each xi, ui ∈Val(Xi, PaG
Xi)
7
θt+1
xi|ui ←
¯
Mt[xi,ui]
¯
Mt[ui]
8
return θt
Maximization (M-step): Treat the expected suﬃcient statistics as observed, and perform
maximum likelihood estimation, with respect to them, to derive a new set of parameters. In
other words, set
θt+1
x|u =
¯
Mθt[x, u]
¯
Mθt[u] .
This phase is called the M-step (maximization step), because we are maximizing the likelihood
M-step
relative to the expected suﬃcient statistics.
A formal version of the algorithm is shown fully in algorithm 19.2.

874
Chapter 19. Partially Observed Data
The maximization step is straightforward. The more diﬃcult step is the expectation step. How
do we compute expected suﬃcient statistics? We must resort to Bayesian network inference over
the network ⟨G, θt⟩. Note that, as in the case of gradient ascent, the only expected suﬃcient
statistics that we need involve a variable and its parents. Although one can use a variety of
diﬀerent inference methods to perform the inference task required for the E-step, we can, as
in the case of gradient ascent, use the clique tree or cluster graph algorithm. Recall that the
family-preservation property guarantees that X and its parents U will be together in some
cluster in the tree or graph. Thus, once again, we can do all of the required inference for

each data case using one run of message-passing calibration.
General Exponential Family ⋆
The same idea generalizes to other distributions where the
likelihood has suﬃcient statistics, in particular, all models in the exponential family (see deﬁni-
exponential
family
tion 8.1). Recall that such families have a suﬃcient statistic function τ(ξ) that maps a complete
instance to a vector of suﬃcient statistics. When learning parameters of such a model, we can
summarize the data using the suﬃcient statistic function τ. For a complete data set D+, we
deﬁne
τ(D+) =
X
m
τ(o[m], h[m]).
We can now deﬁne the same E and M-steps described earlier for this more general case.
Expectation (E-step): For each data case o[m], the algorithm uses the current parameters θt
E-step
to deﬁne a model, and a posterior distribution:
Q(H[m]) = P(H[m] | o[m], θt).
It then uses inference in this distribution to compute the expected suﬃcient statistics:
expected
suﬃcient
statistics
IEQ[τ(⟨D, H⟩)] =
X
m
IEQ[τ(o[m], h[m])].
(19.4)
Maximization (M-step): As in the case of table-CPDs, once we have the expected suﬃcient
M-step
statistics, the algorithm treats them as if they were real and uses them as the basis for maximum
likelihood estimation, using the appropriate form of the ML estimator for this family.
Convergence Results
Somewhat surprisingly, this simple algorithm can be shown to have
several important properties. We now state somewhat simpliﬁed versions of the relevant results,
deferring a more precise statement to the next section.
The ﬁrst result states that each iteration is guaranteed to improve the log-likelihood of the
current set of parameters.
Theorem 19.3
During iterations of the EM procedure of algorithm 19.2, we have
ℓ(θt : D) ≤ℓ(θt+1 : D).
Thus, the EM procedure is constantly increasing the log-likelihood objective function. Because
the objective function can be shown to be bounded (under mild assumptions), this procedure is
guaranteed to converge. By itself, this result does not imply that we converge to a maximum of
the objective function. Indeed, this result is only “almost true”:

19.2. Parameter Estimation
875
X1
X2
Xn
. . .
Class
Figure 19.7
The naive Bayes clustering model. In this model each observed variables Xi is independent
of the other observed variables given the value of the (unobserved) cluster variable C.
Theorem 19.4
Suppose that θt is such that θt+1 = θt during EM, and θt is also an interior point of the allowed
parameter space. Then θt is a stationary point of the log-likelihood function.
This result shows that EM converges to a stationary point of the likelihood function. Recall
that a stationary point can be a local maximum, local minimum, or a saddle point. Although it
seems counterintuitive that by taking upward steps we reach a local minimum, it is possible to
construct examples where EM converges to such a point. However, nonmaximal convergence

points can only be reached from very speciﬁc starting points, and are moreover not
stable, since even small perturbations to the parameters are likely to move the algorithm
away from this point. Thus, in practice, EM generally converges to a local maximum of
the likelihood function.
19.2.2.4
Bayesian Clustering Using EM
One important application of learning with incomplete data, and EM in particular, is to the
problem of clustering. Here, we have a set of data points in some feature space X. Let us
clustering
even assume that they are fully observable. We want to classify these data points into coherent
categories, that is, categories of points that seem to share similar statistical properties.
The Bayesian clustering paradigm views this task as a learning problem with a single hidden
Bayesian
clustering
variable C that denotes the category or class from which an instance comes. Each class is
associated with a probability distribution over the features of the instances in the class. In
most cases, we assume that the instances in each class c come from some coherent, fairly
simple, distribution. In other words, we postulate a particular form for the class-conditional
distribution P(x | c). For example, in the case of real-valued data, we typically assume that the
class-conditional distribution is a multivariate Gaussian (see section 7.1). In discrete settings, we
typically assume that the class-conditional distribution is a naive Bayes structure (section 3.1.3),
where each feature is independent of the rest given the class variable. Overall, this approach
views the data as coming from a mixture distribution and attempts to use the hidden variable
mixture
distribution
to separate out the mixture into its components.
Suppose we consider the case of a naive Bayes model (ﬁgure 19.7) where the hidden class
naive Bayes
variable is the single parent of all the observed feature. In this particular learning scenario,
the E-step involves computing the probability of diﬀerent values of the class variables for each
instance. Thus, we can think of EM as performing a soft classiﬁcation of the instances, that is,
each data instance belongs, to some degree, to multiple classes.
In the M-step we compute the parameters for the CPDs in the form P(X | C) and the prior

876
Chapter 19. Partially Observed Data
P(C) over the classes. These estimates depends on our expected suﬃcient statistics. These are:
¯
Mθ[c]
←
X
m
P(c | x1[m], . . . , xn[m], θt)
¯
Mθ[xi, c]
←
X
m
P(c, xi | x1[m], . . . , xn[m], θt).
We see that an instance helps determine the parameters for all of the classes that it participates
in (that is, ones where P(c | x[m]) is bigger than 0). Stated a bit diﬀerently, each instance
“votes” about the parameters of each cluster by contributing to the statistics of the conditional
distribution given that value of the cluster variable. However, the weight of this vote depends
on the probability with which we assign the instance to the particular cluster.
Once we have computed the expected suﬃcient statistics, the M-step is, as usual, simple. The
parameters for the class variable CPD are
θt+1
c
←
¯
Mθ[c]
M
,
and for the conditional CPD are
θt+1
xi|c ←
¯
Mθ[xi, c]
¯
Mθ[c] .
We can develop similar formulas for the case where some of the observed variables are
continuous, and we use a conditional Gaussian distribution (a special case of deﬁnition 5.15) to
model the CPD P(Xi | C). The application of EM to this speciﬁc model results in a simple and
eﬃcient algorithm.
We can think of the clustering problem with continuous observations from a geometrical per-
spective, where each observed variable Xi represents one coordinate, and instances correspond
to points. The parameters in this case represent the distribution of coordinate values in each
of the classes. Thus, each class corresponds to a cloud of points in the input data. In each
iteration, we reestimate the location of these clouds. In general, depending on the particular
starting point, EM will proceed to assign each class to a dense cloud.
The EM algorithm for clustering uses a “soft” cluster assignment, allowing each instance to
contribute part of its weight to multiple clusters, proportionately to its probability of belonging
to each of them. As another alternative, we can consider “hard clustering,” where each instance
contributes all of its weight to the cluster to which it is most likely to belong. This variant, called
hard-assigment EM proceeds by performing the following steps.
hard-assignment
EM
•
Given parameters θt, we assign c[m] = arg maxc P(c | x[m], θt) for each instance m.
If we let Ht comprise all of the assignments c[m], this results in a complete data set
(D+)t = ⟨D, Ht⟩.
•
Set θt+1 = arg maxθ ℓ(θ : (D+)t). This step requires collecting suﬃcient statistics from
(D+)t, and then choosing MLE parameters based on these.
This approach is often used where the class-conditional distributions P(X | c) are all “round”
Gaussian distributions with unit variance. Thus, each class c has its own mean vector µc, but
a unit covariance matrix. In this case, the most likely class for an instance x is simply the

19.2. Parameter Estimation
877
class c such that the Euclidean distance between x and µc is smallest. In other words, each
point gravitates to the class to which it is “closest.” The reestimation step is also simple. It
simply selects the mean of the class to be at the center of the cloud of points that have aligned
themselves with it. This process iterates until convergence. This algorithm is called k-means.
k-means
Although hard-assignment EM is often used for clustering, it can be deﬁned more broadly; we
return to it in greater detail in section 19.2.2.6.
Box 19.A — Case Study: Discovering User Clusters. In box 18.C, we discussed the collabora-
collaborative
ﬁltering
tive ﬁltering problem, and the use of Bayesian network structure learning to address it. A diﬀerent
application of Bayesian network learning to the collaborative ﬁltering data task, proposed by Breese
et al. (1998), utilized a Bayesian clustering approach. Here, one can introduce a cluster variable C
denoting subpopulations of customers. In a simple model, the individual purchases Xi of each user
are taken to be conditionally independent given the user’s cluster assignment C. Thus, we have a
naive Bayes clustering model, to which we can apply the EM algorithm. (As in box 18.C, items i
that the user did not purchase are assigned Xi = x0
i .)
This learned model can be used in several ways. Most obviously, we can use inference to compute
the probability that the user will purchase item i, given a set of purchases S. Empirical studies show
that this approach achieves lower performance than the structure learning approach of box 18.C,
probably because the “user cluster” variable simply cannot capture the complex preference patterns
over a large number of items. However, this model can provide signiﬁcant insight into the types
of users present in a population, allowing, for example, a more informed design of advertising
campaigns.
As one example, Bayesian clustering was applied to a data set of people browsing the MSNBC
website. Each article was associated with a binary random variable Xi, which took the value x1
i
if the user followed the link to the article. Figure 19.A.1 shows the four largest clusters produced by
Bayesian clustering applied to this data set. Cluster 1 appears to represent readers of commerce and
technology news (a large segment of the reader population at that period, when Internet news was
in its early stages). Cluster 2 are people who mostly read the top-promoted stories in the main page.
Cluster 3 are readers of sports news. In all three of these cases, the user population was known in
advance, and the website contained a page targeting these readers, from which the articles shown
in the table were all linked. The fourth cluster was more surprising. It appears to contain readers
interested in “softer” news. The articles read by this population were scattered all over the website,
and users often browsed several pages to ﬁnd them. Thus, the clustering algorithm revealed an
unexpected pattern in the data, one that may be useful for redesigning the website.
19.2.2.5
Theoretical Foundations ⋆
So far, we used an intuitive argument to derive the details of the EM algorithm. We now formally
analyze this algorithm and prove the results regarding its convergence properties.
At each iteration, EM maintains the “current” set of parameters. Thus, we can view it as a
local learning algorithm. Each iteration amounts to taking a step in the parameter space from

878
Chapter 19. Partially Observed Data
Cluster 1 (36 percent)
E-mail delivery isn’t exactly guaranteed
Should you buy a DVD player?
Price low, demand high for Nintendo
Cluster 2 (29 percent)
757 Crashes at sea
Israel, Palestinians agree to direct talks
Fuhrman pleads innocent to perjury
Cluster 3 (19 percent)
Umps refusing to work is the right thing
Cowboys are reborn in win over eagles
Did Orioles spend money wisely?
Cluster 4 (12 percent)
The truth about what things cost
Fuhrman pleads innocent to perjury
Real astrology
Figure 19.A.1 — Application of Bayesian clustering to collaborative ﬁltering. Four largest clusters
found by Bayesian clustering applied to MSNBC news browsing data. For each cluster, the table shows the
three news articles whose probability of being browsed is highest.
θt to θt+1. This is similar to gradient-based algorithms, except that in those algorithms we
have good understanding of the nature of the step, since each step attempts to go uphill in the
steepest direction. Can we ﬁnd a similar justiﬁcation for the EM iterations?
The basic outline of the analysis proceeds as follows. We will show that each iteration can
be viewed as maximizing an auxiliary function, rather than the actual likelihood function. The
choice of auxiliary function depends on the current parameters at the beginning of the iteration.
The auxiliary function is nice in the sense that it is similar to the likelihood function in complete
data problems. The crucial part of the analysis is to show how the auxiliary function relates to
the likelihood function we are trying to maximize. As we will show, the relation is such that
we can show that the parameters that maximize the auxiliary function in an iteration also have
better likelihood than the parameters with which we started the iteration.
The Expected Log-Likelihood Function
Assume we are given a data set D that consists of
partial observations. Recall that H denotes a possible assignment to all the missing values in our
data set. The combination of D, H deﬁnes a complete data set D+ = ⟨D, H⟩= {o[m], h[m]}m,
where in each instance we now have a full assignment to all the variables.
We denote by
ℓ(θ : ⟨D, H⟩) the log-likelihood of the parameters θ with respect to this completed data set.
Suppose we are not sure about the true value of H. Rather, we have a probabilistic estimate
that we denote by a distribution Q that assigns a probability to each possible value of H. Note
that Q is a joint distribution over full assignments to all of the missing values in the entire data
set. Thus, for example, in our earlier network, if D contains two instances o[1] = ⟨a1, ?, ?, d0⟩
and o[2] = ⟨?, b1, ?, d1⟩, then Q is a joint distribution over B[1], C[1], A[2], C[2].
In the fully observed case, our score for a set of parameters was the log-likelihood. In this
case, given Q, we can use it to deﬁne an average score, which takes into account the diﬀerent
possible completions of the data and their probabilities. Speciﬁcally, we deﬁne the expected
expected
log-likelihood
log-likelihood as:
IEQ[ℓ(θ : ⟨D, H⟩)] =
X
H
Q(H)ℓ(θ : ⟨D, H⟩)
This function has appealing characteristics that are important in the derivation of EM.

19.2. Parameter Estimation
879
The ﬁrst key property is a consequence of the linearity of expectation. Recall that when
learning table-CPDs, we showed that
ℓ(θ : ⟨D, H⟩) =
n
X
i=1
X
(xi,ui)∈Val(Xi,PaXi)
M⟨D,H⟩[xi, ui] log θxi|ui.
Because the only terms in this sum that depend on ⟨D, H⟩are the counts M⟨D,H⟩[xi, ui], and
these appear within a linear function, we can use linearity of expectations to show that
IEQ[ℓ(θ : ⟨D, H⟩)] =
n
X
i=1
X
(xi,ui)∈Val(Xi,PaXi)
IEQ

M⟨D,H⟩[xi, ui]

log θxi|ui.
If we now generalize our notation to deﬁne
¯
MQ[xi, ui] = IEH∼Q

M⟨D,H⟩[xi, ui]

(19.5)
we obtain
IEQ[ℓ(θ : ⟨D, H⟩)] =
n
X
i=1
X
(xi,ui)∈Val(Xi,PaXi)
¯
MQ[xi, ui] log θxi|ui.
This expression has precisely the same form as the log-likelihood function in the complete data
case, but using the expected counts rather than the exact full-data counts. The implication is
that instead of summing over all possible completions of the data, we can evaluate the expected
log-likelihood based on the expected counts.
The crucial point here is that the log-likelihood function of complete data is linear in the
counts. This allows us to use linearity of expectations to write the expected likelihood as a
function of the expected counts.
The same idea generalizes to any model in the exponential family, which we deﬁned in
chapter 8. Recall that a model is in the exponential family if we can write:
P(ξ | θ) =
1
Z(θ)A(ξ) exp {⟨t(θ), τ(ξ)⟩} ,
where ⟨·, ·⟩is the inner product, A(ξ), t(θ), and Z(θ) are functions that deﬁne the family, and
τ(ξ) is the suﬃcient statistics function that maps a complete instance to a vector of suﬃcient
statistics.
As discussed in section 17.2.5, when learning parameters of such a model, we can summarize
the data using the suﬃcient statistic function τ. We deﬁne
τ(⟨D, H⟩) =
X
m
τ(o[m], h[m]).
Because the model is in the exponential family, we can write the log-likelihood ℓ(θ : ⟨D, H⟩)
as a linear function of τ(⟨D, H⟩)
ℓ(θ : ⟨D, H⟩) = ⟨t(θ), τ(⟨D, H⟩)⟩+
X
m
log A(o[m], h[m]) −M log Z(θ).

880
Chapter 19. Partially Observed Data
Using the linearity of expectation, we see that
IEQ[ℓ(θ : ⟨D, H⟩)] = ⟨t(θ), IEQ[τ(⟨D, H⟩)]⟩+
X
m
IEQ[log A(o[m], h[m])] −M log Z(θ).
Because A(o[m], h[m]) does not depend on the choice of θ, we can ignore it. We are left with
maximizing the function:
IEQ[ℓ(θ : ⟨D, H⟩)] = ⟨t(θ), IEQ[τ(⟨D, H⟩)]⟩−M log Z(θ) + const.
(19.6)
In summary, the derivation here is directly analogous to the one for table-CPDs. The expected
log-likelihood is a linear function of the expected suﬃcient statistics IEQ[τ(⟨D, H⟩)]. We can
compute these as in equation (19.4), by aggregating their expectation in each instance in the
training data. Now, maximizing the right-hand side of equation (19.6) is equivalent to maximum
likelihood estimation in a complete data set where the sum of the suﬃcient statistics coincides
with the expected suﬃcient statistics IEQ[τ(⟨D, H⟩)].
These two steps are exactly the E-
step and M-step we take in each iteration of the EM procedure shown in algorithm 19.2. In the
procedure, the distribution Q that we are using is P(H | D, θt). Because instances are assumed
to be independent given the parameters, it follows that
P(H | D, θt) =
Y
m
P(h[m] | o[m], θt),
where h[m] are the missing variables in the m’th data instance, and o[m] are the observations
in the m’th instance. Thus, we see that in the t’th iteration of the EM procedure, we choose θt+1
to be the ones that maximize IEQ[ℓ(θ : ⟨D, H⟩)] with Q(H) = P(H | D, θt). This discussion
allows us to understand a single iteration as an (implicit) optimization step of a well-deﬁned
target function.
Choosing Q
The discussion so far has showed that we can use properties of exponential
models to eﬃciently maximize the expected log-likelihood function. Moreover, we have seen
that the t’th EM iteration can be viewed as maximizing IEQ[ℓ(θ : ⟨D, H⟩)] where Q is the
conditional probability P(H | D, θt).
This discussion, however, does not provide us with
guidance as to why we choose this particular auxiliary distribution Q. Note that each iteration
uses a diﬀerent Q distribution, and thus we cannot relate the optimization taken in one iteration
to the ones made in the subsequent one. We now show why the choice Q(H) = P(H | D, θt)
allows us to prove that each EM iteration improves the likelihood function.
To do this, we will deﬁne a new function that will be the target of our optimization. Recall
that our ultimate goal is to maximize the log-likelihood function. The log-likelihood is a function
only of θ; however, in intermediate steps, we also have the current choice of Q. Therefore, we
will deﬁne a new function that accounts for both θ and Q, and view each step in the algorithm
as maximizing this function.
We already encountered a similar problem in our discussion of approximate inference in
chapter 11. Recall that in that setting we had a known distribution P and attempted to ﬁnd an
approximating distribution Q. This problem is similar to the one we face, except that in learning
we also change the parameters of target distribution P to maximize the probability of the data.
Let us brieﬂy summarize the main idea that we used in chapter 11. Suppose that P = ˜P/Z is
some distribution, where ˜P is an unnormalized part of the distribution, speciﬁed by a product

19.2. Parameter Estimation
881
of factors, and Z is the partition function that ensures that P sums up to one. We deﬁned the
energy functional as
energy functional
F[P, Q] = IEQ
h
log ˜P
i
+ IHQ(X).
We then showed that the logarithm of the partition function can be rewritten as:
log Z = F[P, Q] + ID(Q||P).
How does this apply to the case of learning from missing data? We can choose
P(H | D, θ) = P(H, D | θ)/P(D | θ)
as our distribution over H (we hold D and θ ﬁxed for now). With this choice, the partition
function Z(θ) is the data likelihood P(D | θ) and ˜P is the joint probability P(H, D | θ), so
that log ˜P = ℓ(θ : ⟨D, H⟩). Rewriting the energy functional for this new setting, we obtain:
FD[θ, Q] = IEQ[ℓ(θ : ⟨D, H⟩)] + IHQ(H).
Note that the ﬁrst term is precisely the expected log-likelihood relative to Q. Applying our earlier
expected
log-likelihood
analysis, we now can prove
Corollary 19.1
For any Q,
ℓ(θ : D)
=
FD[θ, Q] + ID(Q(H)||P(H | D, θ))
=
IEQ[ℓ(θ : ⟨D, H⟩)] + IHQ(H) + ID(Q(H)||P(H | D, θ)).
Both equalities have important ramiﬁcations. Starting from the second equality, since both the
entropy IHQ(H) and the relative entropy ID(Q(H)||P(H | D, θ)) are nonnegative, we conclude
that the expected log-likelihood IEQ[ℓ(θ : ⟨D, H⟩)] is a lower bound on ℓ(θ : D). This result is
true for any choice of distribution Q. If we select Q(H) to be the data completion distribution
data completion
P(H | D, θ), the relative entropy term becomes zero. In this case, the remaining term IHQ(H)
captures to a certain extent the diﬀerence between the expected log-likelihood and the real
log-likelihood. Intuitively, when Q is close to being deterministic, the expected value is close to
the actual value.
The ﬁrst equality, for the same reasons, shows that, for any distribution Q, the F function is
a lower bound on the log-likelihood. Moreover, this lower bound is tight for every choice of θ:
if we choose Q = P(H | D, θ), the two functions have the same value. Thus, if we maximize
the F function, we are bound to maximize the log-likelihood.
There many possible ways to optimize this target function.
We now show that the EM
procedure we described can be viewed as implicitly optimizing the EM functional F using a
particular optimization strategy.
The strategy we are going to utilize is a coordinate ascent
coordinate ascent
optimization. We start with some choice θ of parameters. We then search for Q that maximizes
FD[θ, Q] while keeping θ ﬁxed.
Next, we ﬁx Q and search for parameters that maximize
FD[θ, Q]. We continue in this manner until convergence.
We now consider each of these steps.
•
Optimizing Q. Suppose that θ are ﬁxed, and we are searching for arg maxQ FD[θ, Q].
Using corollary 19.1, we know that, if Q∗= P(H | D, θ), then
FD[θ, Q∗] = ℓ(θ : D) ≥FD[θ, Q].

882
Chapter 19. Partially Observed Data
q
L(q |   )
Figure 19.8
An illustration of the hill-climbing process performed by the EM algorithm. The black
line represents the log-likelihood function; the point on the left represents θt; the gray line represents the
expected log-likelihood derived from θt; and the point on the right represents the parameters θt+1 that
maximize this expected log-likelihood.
Thus, we maximize the EM functional by choosing the auxiliary distribution Q∗. In other
words, we can view the E-step as implicitly optimizing Q by using P(H | D, θt) in computing
the expected suﬃcient statistics.
•
Optimizing θ. Suppose Q is ﬁxed, and that we wish to ﬁnd arg maxθ FD[θ, Q]. Because
the only term in F that involves θ is IEQ[ℓ(θ : ⟨D, H⟩)], the maximization is equivalent to
maximizing the expected log-likelihood. As we saw, we can ﬁnd the maximum by comput-
ing expected suﬃcient statistics and then solving the MLE given these expected suﬃcient
statistics.
Convergence of EM
The discussion so far shows that the EM procedure can be viewed as
maximizing an objective function; because the objective function can be shown to be bounded,
this procedure is guaranteed to converge. However, it is not clear what can be said about the
convergence points of this procedure. We now analyze the convergence points of this procedure
in terms of our true objective: the log-likelihood function.
Intuitively, as our procedure is
optimizing the energy functional, which is a tight lower bound of the log-likelihood function,
each step of this optimization also improves the log-likelihood.
This intuition is illustrated
in ﬁgure 19.8.
In more detail, the E-step is selecting, at the current set of parameters, the
distribution Qt for which the energy functional is a tight lower bound to ℓ(θ : D). The energy
functional, which is a well-behaved concave function in θ, can be maximized eﬀectively via
the M-step, taking us to the parameters θt+1. Since the energy functional is guaranteed to
remain below the log-likelihood function, this step is guaranteed to improve the log-likelihood.
Moreover, the improvement is guaranteed to be at least as large as the improvement in the energy
functional. More formally, using corollary 19.1, we can now prove the following generalization of
theorem 19.3:
Theorem 19.5
During iterations of the EM procedure of algorithm 19.2, we have that
ℓ(θt+1 : D) −ℓ(θt : D) ≥IEP (H|D,θt)

ℓ(θt+1 : D, H)

−IEP (H|D,θt)

ℓ(θt : D, H)

.

19.2. Parameter Estimation
883
As a consequence, we obtain that:
ℓ(θt : D) ≤ℓ(θt+1 : D).
Proof We begin with the ﬁrst statement. Using corollary 19.1, with the distribution Qt(H) =
P(H | D, θt) we have that
ℓ(θt+1 : D)
=
IEQt
ℓ(θt+1 : ⟨D, H⟩)

+ IHQt(H) + ID(Qt(H)||P(H | D, θt+1))
ℓ(θt : D)
=
IEQt
ℓ(θt : ⟨D, H⟩)

+ IHQt(H) + ID(Qt(H)||P(H | D, θt))
=
IEQt
ℓ(θt : ⟨D, H⟩)

+ IHQt(H).
The last step is justiﬁed by our choice of Qt(H) = P(H | D, θt). Subtracting these two terms,
we have that
ℓ(θt+1 : D) −ℓ(θt : D) =
IEQt
ℓ(θt+1 : D, H)

−IEQt
ℓ(θt : D, H)

+ ID(Qt(H)||P(H | D, θt+1)).
Because the last term is nonnegative, we get the desired inequality.
To prove the second statement of the theorem, we note that θt+1 is the value of θ that
maximizes IEP (H|D,θt)[ℓ(θ : D, H)]. Hence the value obtained for this expression for θt+1 is at
least at large as the value obtained for any other set of parameters, including θt. It follows that
the right-hand side of the inequality is nonnegative, which implies the second statement.

We conclude that EM performs a variant of hill climbing, in the sense that it improves
the log-likelihood at each step. Moreover, the M-step can be understood as maximizing
a lower-bound on the improvement in the likelihood. Thus, in a sense we can view the
algorithm as searching for the largest possible improvement, when using the expected
log-likelihood as a proxy for the actual log-likelihood.
For most learning problems, we know that the log-likelihood is upper bounded. For example,
if we have discrete data, then the maximal likelihood we can assign to the data is 1. Thus,
the log-likelihood is bounded by 0. If we have a continuous model, we can construct examples
where the likelihood can grow unboundedly; however, we can often introduce constraints on the
parameters that guarantee a bound on the likelihood (see exercise 19.10). If the log-likelihood is
bounded, and the EM iterations are nondecreasing in the log-likelihood, then the sequence of
log-likelihoods at successive iterations must converge.
The question is what can be said about this convergence point. Ideally, we would like to guar-
antee convergence to the maximum value of our log-likelihood function. Unfortunately, as we
mentioned earlier, we cannot provide this guarantee; however, we can now prove theorem 19.4,
which shows convergence to a ﬁxed point of the log-likelihood function, that is, one where the
gradient is zero. We restate the theorem for convenience:
Theorem 19.6
Suppose that θt is such that θt+1 = θt during EM, and θt is also an interior point of the allowed
parameter space. Then θt is a stationary point of the log-likelihood function.
Proof We start by rewriting the log-likelihood function using corollary 19.1.
ℓ(θ : D) = IEQ[ℓ(θ : ⟨D, H⟩)] + IHQ(H) + ID(Q(H)||P(H | D, θ)).

884
Chapter 19. Partially Observed Data
We now consider the gradient of ℓ(θ : D) with respect to θ. Since the term IHQ(H) does not
depend on θ, we get that
∇θℓ(θ : D) = ∇θIEQ[ℓ(θ : ⟨D, H⟩)] + ∇θID(Q(H)||P(H | D, θ)).
This observation is true for any choice of Q. Now suppose we are in an EM iteration. In this
case, we set Q = P(H | D, θt) and evaluate the gradient at θt.
A somewhat simpliﬁed proof runs as follows. Because θ = θt is a minimum of the KL-
divergence term, we know that ∇θID(Q(H)||P(H | D, θt)) is 0. This implies that
∇θℓ(θt : D) = ∇θIEQ

ℓ(θt : ⟨D, H⟩)

.
Or, in other words, ∇θℓ(θt : D) = 0 if and only if ∇θIEQ

ℓ(θt : ⟨D, H⟩)

= 0.
Recall that θt+1 = arg maxθ IEQ

ℓ(θt : ⟨D, H⟩)

. Hence the gradient of the expected likeli-
hood at θt+1 is 0. Thus, we conclude that θt+1 = θt only if ∇θIEQ

ℓ(θt : ⟨D, H⟩)

= 0. And
so, at this point, ∇θℓ(θt : D) = 0. This implies that this set of parameters is a stationary point
of the log-likelihood function.
The actual argument has to be somewhat more careful. Recall that the parameters must lie
within some allowable set. For example, the parameters of a discrete random variable must sum
up to one. Thus, we are searching within a constrained space of parameters. When we have
constraints, we often do not have zero gradient. Instead, we get to a stationary point when
the gradient is orthogonal to the constraints (that is, local changes within the allowed space do
not improve the likelihood). The arguments we have stated apply equally well when we replace
statements about equality to 0 with orthogonality to the constraints on the parameter space.
19.2.2.6
Hard-Assignment EM
In section 19.2.2.4, we brieﬂy mentioned the idea of using a hard assignment to the hidden
variables, in the context of applying EM to Bayesian clustering. We now generalize this simple
idea to the case of arbitrary Bayesian networks.
This algorithm, called hard-assignment EM, also iterates over two steps: one in which it com-
hard-assignment
EM
pletes the data given the current parameters θt, and the other in which it uses the completion to
estimate new parameters θt+1. However, rather than using a soft completion of the data, as in
standard EM, it selects for each data instance o[m] the single assignment h[m] that maximizes
P(h | o[m], θt).
Although hard-assignment EM is similar in outline to EM, there are important diﬀerences. In
fact, hard-assignment EM can be described as optimizing a diﬀerent objective function, one that
involves both the learned parameters and the learned assignment to the hidden variables. This
objective is to maximize the likelihood of the complete data ⟨D, H⟩, given the parameters:
max
θ,H ℓ(θ : H, D).
See exercise 19.14. Compare this objective to the EM objective, which attempts to maximize
ℓ(θ : D), averaging over all possible completions of the data.
Does this observation provide us insight on these two learning procedures? The intuition is
that these two objectives are similar if P(H | D, θ) assigns most of the probability mass to

19.2. Parameter Estimation
885
0
5
10
15
20
25
30
35
40
45
50
–50
–45
–40
–35
–30
–25
–20
–15
–10
–5
Iteration
Training LL / Instance
0
5
10
15
20
25
30
35
40
45
50
Iteration
Test LL / Instance
0
5
10
15
20
25
30
35
40
45
50
Iteration
(a)
(c)
(b)
Parameter value
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
–110
–100
–90
–80
–70
–60
–50
–40
–30
–20
–10
Figure 19.B.1 — Convergence of EM run on the ICU Alarm network. (a) Training log-likelihood. (b)
Progress of several sample parameters. (c) Test data log-likelihood.
one completion of the data. In such a case, EM will eﬀectively perform hard assignment during
the E-step. However, if P(H | D, θ) is diﬀuse, the two algorithms will lead to very diﬀerent
solutions. In clustering, the hard-assignment version tends to increase the contrast between
diﬀerent classes, since assignments have to choose between them. In contrast, EM can learn
classes that are overlapping, by having many instances contributing to two or more classes.
Another diﬀerence between the two EM variants is in the way they progress during the
learning. Note that for a given data set, at the end of an iteration, the hard-assignment EM
can be in one of a ﬁnite number of parameter values. Namely, there is only one parameter
assignment for each possible assignment to H. Thus, hard-assignment EM traverses a path
in the combinatorial space of assignments to H. The soft-assignment EM, on the other hand,
traverses the continuous space of parameter assignments. The intuition is that hard-assignment
EM converges faster, since it makes discrete steps. In contrast, soft-assignment EM can converge
very slowly to a local maximum, since close to the maximum, each iteration makes only small
changes to the parameters. The ﬂip side of this argument is that soft-assignment EM can traverse
paths that are infeasible to the hard-assignment EM. For example, if two clusters need to shift
their means in a coordinated fashion, soft-assignment EM can progressively change their means.
On the other hand, hard-assignment EM needs to make a “jump,” since it cannot simultaneously
reassign multiple instances and change the class means.
Box 19.B — Case Study: EM in Practice. The EM algorithm is guaranteed to monotonically im-
prove the training log-likelihood at each iteration. However, there are no guarantees as to the speed
of convergence or the quality of the local maxima attained. To gain a better perspective of how
the algorithm behaves in practice, we consider here the application of the method to the ICU-Alarm
network discussed in earlier learning chapters.
We start by considering the progress of the training data likelihood during the algorithm’s it-
erations. In this example, 1, 000 samples were generated from the ICU-Alarm network. For each
instance, we then independently and randomly hid 50 percent of the variables. As can be seen in
ﬁgure 19.B.1a, much of the improvement over the performance of the random starting point is in the

886
Chapter 19. Partially Observed Data
25% missing
50% Missing
Hidden variable
# of distinct local maxima
Sample size
Train LL / instance
Percentage of runs
(a)
(b)
0
500 1000 1500 2000 2500 3000 3500 4000
0
5
10
15
20
25
10
20
30
40
50
60
70
80
90
100
–14.5
–14.45
–14.4
–14.35
–14.3
Figure 19.B.2 — Local maxima in likelihood surface. (a) Number of unique local maxima (in 25
runs) for diﬀerent sample sizes and missing value conﬁgurations. (b) Distribution of training likelihood of
local maxima attained for 25 random starting points with 1,000 samples and one hidden variable.
ﬁrst few iterations. However, examining the convergence of diﬀerent parameters in (b), we see that
some parameters change signiﬁcantly after the ﬁfth iteration, even though changes to the likelihood
are relatively small. In practice, any nontrivial model will display a wide range of sensitivity to
the network parameters. Given more training data, the sensitivity will, typically, overall decrease.
Owing to these changes in parameters, the training likelihood continues to improve after the initial
iterations, but very slowly. This behavior of fast initial improvement, followed by slow convergence,
is typical of EM.
We next consider the behavior of the learned model on unseen test data. As we can see in (c), early
in the process, test-data improvement correlates with training-data performance. However, after the
10th iterations, training performance continues to improve, but test performance decreases. This
phenomenon is an instance of overﬁtting to the training data. With more data or fewer unobserved
overﬁtting
values, this phenomenon will be less pronounced. With less data or hidden variables, on the other
hand, explicit techniques for coping with the problem may be needed (see box 19.C).
A second key issue any type of optimization of the likelihood in the case of missing data is that
of local maxima. To study this phenomenon, we consider the number of local maxima for 25
random starting points under diﬀerent settings. As the sample size (x-axis) grows, the number of
local maxima diminishes. In addition, the number of local maxima when more values are missing
(dashed line) is consistently greater than the number of local maxima in the setting where more
data is available (solid line). Importantly, in the case where just a single variable is hidden, the
number of local maxima is large, and remains large even when the amount of training data is quite
large. To see that this is not just an artifact of possible permutations of the values of the hidden
variable, and to demonstrate the importance of achieving a superior local maxima, in (b) we show
the training set log-likelihood of the 25 diﬀerent local maxima attained. The diﬀerence between the

19.2. Parameter Estimation
887
best and worst local maxima is over 0.2 bit-per-instance. While this may not seem signiﬁcant, for a
training set of 1, 000 instances, this corresponds to a factor of 20.2∗1,000 ≈1060 in the training set
likelihood. We also note that the spread of the quality in diﬀerent local maxima is quite uniform,
so that it is not easy to attain a good local maximum with a small number of random trials.
19.2.3
Comparison: Gradient Ascent versus EM
So far we have discussed two algorithms for parameter learning with incomplete data: gradient
ascent and EM. As we will discuss (see box 19.C), there are many issues involved in the actual
implementation of these algorithms: the choice of initial parameters, the stopping criteria, and so
forth. However, before discussing these general points, it is worth comparing the two algorithms.
There are several points of similarity in the overall strategy of both algorithms. Both algorithms
are local in nature. At each iteration they maintain a “current” set of parameters, and use these
to ﬁnd the next set. Moreover, both perform some version of greedy optimization based on the
current point. Gradient ascent attempts to progress in the steepest direction from the current
point. EM performs a greedy step in improving its target function given the local parameters.
Finally, both algorithms provide a guarantee to converge to local maxima (or, more precisely, to
stationary points where the gradient is 0). On one hand, this is an important guarantee, in the
sense that both are at least locally maximal. On the other hand, this is a weak guarantee, since
many real-world problems have multimodal likelihood functions, and thus we do not know how
far the learned parameters are from the global maximum (or maxima).
In terms of the actual computational steps, the two algorithms are also quite similar. For
table-CPDs, the main component of either an EM iteration or a gradient step is computing the
expected suﬃcient statistics of the data given the current parameters. This involves performing
inference on each instance. Thus, both algorithms can exploit dynamic programming procedures
(for example, clique tree inference) to compute all the expected suﬃcient statistics in an instance
eﬃciently.
In term of implementation details, the algorithms provide diﬀerent beneﬁts. On one hand,
gradient ascent allows to use “black box” nonlinear optimization techniques, such as conjugate
gradient ascent (see appendix A.5.2). This allows the implementation to build on a rich set of
existing tools. Moreover, gradient ascent can be easily applied to various CPDs by using the
chain rule of derivatives. On the other hand, EM relies on maximization from complete data.
Thus, it allows for a fairly straightforward use of learning procedure for complete data in the case
of incomplete data. The only change is replacing the part that accumulates suﬃcient statistics
by a procedure that computes expected suﬃcient statistics. As such, most people ﬁnd EM easier
to implement.
A ﬁnal aspect for consideration is the convergence rate of the algorithm. Although we cannot
predict in advance how many iterations are needed to learn parameters, analysis can show the
general behavior of the algorithm in terms of how fast it approaches the convergence point.
Suppose we denote by ℓt = ℓ(θt : D) the likelihood of the solution found in the t’th iteration
(of either EM or gradient ascent). The algorithm converges toward ℓ∗= limt→∞ℓt. The error
at the t’th iteration is
ϵt = ℓ∗−ℓt.

888
Chapter 19. Partially Observed Data
Although we do not go through the proof, one can show that EM has linear convergence rate.
convergence rate
This means that for each domain there exists a t0 and α < 1 such that for all t ≥t0
ϵt+1 ≤αϵt.
On the face of it, this is good news, since it shows that the error decreases at each iteration.
Such a convergence rate means that ℓt+1 −ℓt = ϵt −ϵt+1 ≥ϵt(1 −α). In other words, if we
know α, we can bound the error
ϵt ≤ℓt+1 −ℓt
1 −α
.
While this result provides a bound on the error (and also suggests a way of estimating it), it is
not always a useful one. In particular, if α is relatively close to 1, then even when the diﬀerence
is likelihood between successive iterations is small, the error can be much larger. Moreover, the
number of iterations to convergence can be very large. In practice we see this behavior quite
often. The ﬁrst iterations of EM show huge improvement in the likelihood. These are then

followed by many iterations that slowly increase the likelihood; see box 19.B. Conjugate
gradient often has opposite behavior. The initial iterations (which are far away from the
local maxima) often take longer to improve the likelihood. However, once the algorithm is
in the vicinity of maximum, where the log-likelihood function is approximately quadratic,
this method is much more eﬃcient in zooming on the maximum. Finally, it is important
to keep in mind that these arguments are asymptotic in the number of iterations; the actual
number of iterations required for convergence may not be in the asymptotic regime. Thus, the
rate of convergence of diﬀerent algorithms may not be the best indicator as to which of them is
likely to work most eﬃciently in practice.
Box 19.C — Skill: Practical Considerations in Parameter Learning. There are a few practical
considerations in implementing both gradient-based methods and EM for learning parameters with
missing data. We now consider a few of these. We present these points mostly in the context of the
EM algorithm, but most of our points apply equally to both classes of algorithms.
In a practical implementation of EM, there are two key issues that one needs to address. The
ﬁrst is the presence of local maxima. As demonstrated in box 19.B, the likelihood of even relatively
simple networks can have a large number of local maxima that signiﬁcantly diﬀer in terms of
their quality. There are several adaptations of these local search algorithms that aim to consistently
reach beneﬁcial local maxima. These adaptations include a judicious selection of initialization,
and methods for modifying the search so as to achieve a better local maximum. The second key
issue involves the convergence of the algorithm: determining convergence, and improving the rate
of convergence.
Local Maxima
One of the main limitations of both the EM and the gradient ascent procedures is
that they are only guaranteed to reach a stationary point, which is usually a local maximum. How
do we improve the odds of ﬁnding a global — or at least a good local — maximum?
The ﬁrst place where we can try to address the issue of local maxima is in the initialization of
the algorithm. EM and gradient ascent, as well as most other “local” algorithms, require a starting

19.2. Parameter Estimation
889
point — a set of initial parameters that the algorithm proceeds to improve.
Since both algo-
rithms are deterministic, this starting point (implicitly) determines which local maximum is found.
In practice, diﬀerent initializations can result in radically diﬀerent convergence points,

sometimes with very diﬀerent likelihood values. Even when the likelihood values are
similar, diﬀerent convergence points may represent semantically diﬀerent conclusions
about the data. This issue is particularly severe when hidden variables are involved, where we can
easily obtain very diﬀerent clusterings of the data. For example, when clustering text documents
by similarity (for example, using a version of the model in box 17.E where the document cluster
variable is hidden), we can learn one model where the clusters correspond to document topics, or
another where they correspond to the style of the publication in which the document appeared
(for example, newspaper, webpage, or blog). Thus, initialization should generally be considered very
seriously in these situations, especially when the amount of missing data is large or hidden variables
are involved.
In general, we can initialize the algorithm either in the E-step, by picking an initial set of
parameters, or in the M-step, by picking an initial assignment to the unobserved variables. In
the ﬁrst type of approach, the simplest choices for starting points are either a set of parameters
ﬁxed in advance or randomly chosen parameters. If we use predetermined initial parameters, we
should exercise care in choosing them, since a misguided choice can lead to very poor outcomes.
In particular, for some learning problems, the seemingly natural choice of uniform parameters can
lead to disastrous results; see exercise 19.11.
Another easy choice is applicable for parts of the
network where we have only a moderate amount of missing data. Here, we can sometimes estimate
parameters using only the observed data, and then use those to initialize the E-step. Of course,
this approach is not always feasible, and it is inapplicable when we have a hidden variable. A
diﬀerent natural choice is to use the mean of our prior over parameters. On one hand, if we have
good prior information, this might serve as a good starting position. Note that, although this choice
does bias the learning algorithm to prefer the prior’s view of the data, the learned parameters can
be drastically diﬀerent in the end. On the other hand, if the prior is not too informative, this
choice suﬀers from the same drawbacks we mentioned earlier. Finally, a common choice is to use
a randomized starting point, an approach that avoids any intrinsic bias. However, there is also no
reason to expect that a random choice will give rise to a good solution. For this reason, often one
tries multiple random starting points, and the convergence point of highest likelihood is chosen.
The second class of methods initializes the procedure at the M-step by completing the missing
data. Again, there are many choices for completing the data. For example, we can use a uniform or
a random imputation method to assign values to missing observations. This procedure is particularly
useful when we have diﬀerent patterns of missing observations in each sample. Then, the counts
from the imputed data consist of actual counts combined with imputed ones. The real data thus
bias the estimated parameters to be reasonable. Another alternative is to use a simpliﬁed learning
procedure to learn initial assignment to missing values. This procedure can be, for example, hard-
assignment EM. As we discussed, such a procedure usually converges faster and therefore can serve
as a good initialization. However, hard-assignment EM also requires a starting point, or a selection
among multiple random starting points.
When learning with hidden variables, such procedures can be more problematic. For example, if
we consider a naive Bayes clustering model and use random imputation, the result would be that
we randomly assign instances to clusters. With a suﬃciently large data set, these clusters will be very
similar (since they all sample from the same population). In a smaller data set the sampling noise

890
Chapter 19. Partially Observed Data
might distinguish the initial clusters, but nonetheless, this is not a very informed starting point. We
discuss some methods for initializing a hidden variable in section 19.5.3.
Other than initialization, we can also consider modifying our search so as to reduce the risk of
getting stuck at a poor local optimum. The problem of avoiding local maxima is a standard one,
and we describe some of the more common solutions in appendix A.4.2. Many of these solutions are
applicable in this setting as well. As we mentioned, the approach of using multiple random restarts
is commonly used, often with a beam search modiﬁcation to quickly prune poor starting points.
beam search
In particular, in this beam search variant, K EM runs are carried out in parallel and every few
iterations only the most promising ones are retained. A variant of this approach is to generate K
EM threads at each step by slightly perturbing the most beneﬁcial k < K threads from the previous
iteration. While such adaptations have no formal guarantees, they are extremely useful in practice
in terms trading oﬀquality of solution and computational requirements.
Annealing methods (appendix A.4.2) have also been used successfully in the context of the EM
algorithm. In such methods, we gradually transform from an easy objective with a single local
maximum to the desired EM objective, and thereby we potentially avoid many local maxima that
are far away from the central basin of attraction. Such an approach can be carried out by directly
smoothing the log-likelihood function and gradually reducing the level to which it is smoothed, or
implicitly by gradually altering the weights of training instances.
Finally, we note that we can never determine with certainty whether the EM convergence point
is truly the global maximum. In some applications this limitation is acceptable — for example, if
we care only about ﬁtting the probability distribution over the training examples (say for detecting
instances from a particular subpopulation). In this case, if we manage to learn parameters that
assign high probability for samples in the target population, then we might be content even if
these parameters are not the best ones possible. On the other hand, if we want to use the learned
parameters to reveal insight about the domain, then we might care about whether the parameters
are truly the optimal ones or not. In addition, if the learning procedure does not perform well, we
have to decide whether the problem stems from getting trapped in a poor local maximum, or from
the fact that the model is not well suited to the distribution in our particular domain.
Stopping Criteria
Both algorithms we discussed have the property that they will reach a ﬁxed
point once they converged on a stationary point of the likelihood surface. In practice, we never
really reach the stationary point, although we can get quite close to it. This raises the question of
when we stop the procedure.
The basic idea is that when solutions at successive iterations are similar to each other, additional
iterations will not change the solution by much. The question is how to measure similarity of
solutions. There are two main approaches. The ﬁrst is to compare the parameters from successive
iterations.
The second is to compare the likelihood of these choices of parameters.
Somewhat
surprisingly, these two criteria are quite diﬀerent. In some situations small changes in parameters
lead to dramatic changes in likelihood, and in others large changes in parameters lead to small
changes in the likelihood.
To understand how there can be a discrepancy between changes in parameters and changes
in likelihood, consider the properties of the gradient as shown in theorem 19.2. Using a Taylor
expansion of the likelihood, this gradient provides us with an estimate how the likelihood will
change when we change the parameters. We see that if P(x, u | o[m], θ) is small in most data

19.2. Parameter Estimation
891
instances, then the gradient
∂ℓ(θ:D)
∂P (x|u) will be small. This implies that relatively large changes in
P(x | u) will not change the likelihood by much. This can happen for example if the event u
is uncommon in the training data, and the value of P(x | u) is involved in the likelihood only
in a few instances. On the ﬂip side, if the event x, u has a large posterior in all samples, then
the gradient ∂ℓ(θ:D)
∂P (x|u) will be of size proportional to M. In such a situation a small change in the
parameter can result in a large change in the likelihood.
In general, since we are aiming to maximize the likelihood, large changes in the parameters that
have negligible eﬀect on the likelihood are of less interest. Moreover, measuring the magnitude of
changes in parameters is highly dependent on our parameterization. For example, if we use the
reparameterization of equation (19.3), the diﬀerence (say in Euclidean distance) between two sets
of parameters can change dramatically. Using the likelihood for tracking convergence is thus less
sensitive to these choices and more directly related to the goal of the optimization.
Even once we decide what to measure, we still need to determine when we should stop the
process. Some gradient-based methods, such as conjugate gradient ascent, build an estimate of the
second-order derivative of the function. Using these derivatives, they estimate the improvement we
expect to have. We can then decide to stop when the expected improvement is not more than a
ﬁxed amount of log-likelihood units. We can apply similar stoping criteria to EM, where again, if
the change in likelihood in the last iteration is smaller than a predetermined threshold we stop the
iterations.

Importantly, although the training set log-likelihood is guaranteed to increase mono-
tonically until convergence, there is no guarantee that the generalization performance of
the model — the expected log-likelihood relative to the underlying distribution — also
increases monotonically. (See section 16.3.1.) Indeed, it is often the case that, as we ap-
proach convergence, the generalization performance starts to decrease, due to overﬁtting
overﬁtting
of the parameters to the speciﬁcs of the training data.
Thus, an alternative approach is to measure directly when additional improvement to the training
set likelihood does not contribute to generalization. To do so we need to separate the available data
into a training set and a validation set (see box 16.A). We run learning on the training set, but
at the end of each iteration we evaluate the log-likelihood of the validation set (which is not seen
validation set
during learning). We stop the procedure when the likelihood of the validation set does not improve.
(As usual, the actual performance of the model would then need to be evaluated on a separate test
set.) This method allows us to judge when the procedure stops learning the interesting phenomena
and begins to overﬁt the training data. On the ﬂip side, such a procedure is both slower (since we
need to evaluate likelihood on an additional data set at the end of iteration) and forces us to train
on a smaller subset of data, increasing the risk of overﬁtting. Moreover, if the validation set is small,
then the estimate of the generalization ability by the likelihood on this set is noisy. This noise can
inﬂuence the stopping time.
Finally, we note that in EM much of the improvement is typically observed in the ﬁrst few
iterations, but the ﬁnal convergence can be quite slow. Thus, in practice, it is often useful to limit
the number of EM iterations or use a lenient convergence threshold. This is particularly important
when EM is used as part of a higher-level algorithm (for example, structure learning) and where,
in the intermediate stages of the overall learning algorithm, approximate parameter estimates are
often suﬃcient. Moreover, early stopping can help reduce overﬁtting, as we discussed.

892
Chapter 19. Partially Observed Data
Accelerating Convergence
There are also several strategies that can help improve the rate of
convergence of EM to its local optimum. We brieﬂy list a few of them.
The ﬁrst idea is to use hybrid algorithms that mix EM and gradient methods. The basic intuition
is that EM is good at rapidly moving to the general neighborhood of a local maximum in few
iterations but bad at pinpointing the actual maximum. Advanced gradient methods, on the other
hand, quickly converge once we are close to a maximum. This observation suggests that we should
run EM for few iterations and then switch over to using a method such as conjugate gradient.
Such hybrid algorithms are often much more eﬃcient. Another alternative is to use accelerated EM
accelerated EM
methods that take even larger steps in the search than standard EM (see section 19.7).
Another class of variations comprises incremental methods. In these methods we do not perform
incremental EM
a full E-step or a full M-step. Again, the high-level intuition is that, since we view our procedure as
maximizing the energy functional FD[θ, Q], we can consider steps that increase this functional but
do not necessarily ﬁnd the maximum value parameters or Q. For example, recall that θ consists
of several subcomponents, one per CPD. Rather than maximizing all the parameters at once, we
can consider a partial update where we maximize the energy functional with respect to one of the
subcomponents while freezing the others; see exercise 19.16. Another type of partial update is based
on writing Q as a product of independent distributions — each one over the missing values in a
particular instance. Again, we can optimize the energy functional with respect to one of these while
freezing the other; see exercise 19.17. These partial updates can provide two types of beneﬁt: they
can require less computation than a full EM update, and they can propagate changes between the
statistics and the parameters much faster, reducing the total number of iterations.
Box 19.D — Case Study: EM for Robot Mapping. One interesting application of the EM algo-
rithm is to robotic mapping. Many variants of this applications have been proposed; we focus on
one by Thrun et al. (2004) that explicitly tries to use the probabilistic model to capture the structure
in the environment.
The data in this application are a point cloud representation of an indoor environment. The point
cloud can be obtained by collecting a sequence of point clouds, measured along a robot’s motion
trajectory. One can use a robot localization procedure to (approximately) assess the robot’s pose
(position and heading) along each point in the trajectory, which allows the diﬀerent measurements
to be put on a common frame of reference. Although the localization process is not fully accurate,
the estimates are usually reasonable for short trajectories. One can then take the points obtained
over the trajectory, and ﬁt the points using polygons, to derive a 3D map of the surfaces in the
robot’s environment. However, the noise in the laser measurements, combined with the errors in
localization, leads adjacent polygons to have slightly diﬀerent surface normals, giving rise to a very
jagged representation of the environment.
The EM algorithm can be used to ﬁt a more compact representation of the environment to
the data, reducing the noise and providing a smoother, more realistic output. In particular, in this
example, the model consists of a set of 3D planes p1, . . . , pK, each characterized by two parameters
αk, βk, where αk is a unit-length vector in IR3 that encodes the plane’s surface normal vector,
and βk is a scalar that denotes its distance to the origin of the global coordinate system. Thus, the
distance of any point x to the plane is d(x, pk) = |αkx −βk|.

19.2. Parameter Estimation
893
(a)
(d)
(c)
(b)
Figure 19.D.1 — Sample results from EM-based 3D plane mapping (a) Raw data map obtained from
range ﬁnder. (b) Planes extracted from map using EM. (c) Fragment of reconstructed surface using raw
data. (d) The same fragment reconstructed using planes.
The probabilistic model also needs to specify, for each point xm in the point cloud, to which
plane xm belongs. This assignment can be modeled via a set of correspondence variables Cm
correspondence
variable
such that Cm = k if the measurement point xm was generated by the kth plane. Each assignment
to the correspondence variables, which are unobserved, encodes a possible solution to the data
data association
association problem. (See box 12.D for more details.) We deﬁne P(Xm | Cm = k : θk) to be
∝N
 d(x, pk) | 0; σ2
. In addition, we also allow an additional value Cm = 0 that encodes
points that are not generated by any of the planes; the distribution P(Xm | Cm = 0) is taken to
be uniform over the (ﬁnite) space.
Given a probabilistic model, the EM algorithm can be applied to ﬁnd the assignment of points to
planes — the correspondence variables, which are taken to be hidden; and the parameters αk, βk
that characterize the planes. Intuitively, the E-step computes the assignment to the correspondence
variables by assigning the weight of each point proportionately to its distance to each of them. The
M-step then recomputes the parameters of each plane to ﬁt the points assigned to it. See exer-
cise 19.18 and exercise 19.19. The algorithm also contains an additional outer loop that heuristically
suggests new surfaces to be added to the model, and removes surfaces that do not have enough
support in the data (for example, one possible criterion can depend on the total weight that diﬀerent
data points assign to the surface).
The results of this algorithm are shown in ﬁgure 19.D.1. One can see that the resulting map is
considerably smoother and more realistic than the results derived directly from the raw data.
19.2.4
Approximate Inference ⋆
The main computational cost in both gradient ascent and EM is in the computation of expected
suﬃcient statistics. This step requires running probabilistic inference on each instance in the
training data.
These inference steps are needed both for computing the likelihood and for
computing the posterior probability over events of the form x, paX for each variable and its
parents.
For some models, such as the naive Bayes clustering model, this inference step is
almost trivial. For other models, this inference step can be extremely costly. In practice, we

894
Chapter 19. Partially Observed Data
often want to learn parameters for models where exact inference is impractical. Formally, this
happens when the tree-width of the unobserved parts of the model is large. (Note the contrast
to learning from complete data, where the cost of learning the model did not depend on the
complexity of inference.) In such situations the cost of inference becomes the limiting factor in
our ability to learn from data.
Example 19.9
Recall the network discussed in example 6.11 and example 19.9, where we have n students taking
m classes, and the grade for each student in each class depends on both the diﬃculty of the class
and his or her intelligence. In the ground network for this example, we have a set of variables
I = {I(s)} for the n students (denoting the intelligence level of each student s), D = {D(c)} for
the m courses (denoting the diﬃculty level of each course c), and G = {G(s, c)} for the grades,
where each variable G(s, c) has as parents I(s) and D(c). Since this network is derived from a
plate model, the CPDs are all shared, so we only have three CPDs that must be learned: P(I(S)),
P(D(C)), P(G(S, C) | I(S), D(C)).
Suppose we only observe the grades of the students but not their intelligence or the diﬃculty of
courses, and we want to learn this model. First, we note that there is no way to force the model to
respect our desired semantics for the (hidden) variables I and D; for example, a model in which
we ﬂip the two values for I is equally good. Nevertheless, we can hope that some value for I will
correspond to “high intelligence” and the other to “low intelligence,” and similarly for D.
To perform EM in this model, we need to infer the expected counts of assignments to triplets
of variables of the form I(s), D(c), G(s, c). Since we have parameter sharing, we will aggregate
these counts and then estimate the CPD P(G(S, C) | I(S), D(C)) from the aggregate counts. The
problem is that observing a variable G(s, c) couples its two parents. Thus, this network induces a
Markov network that has a pairwise potential between any pair of I(s) and D(c) variables that
share an observed child. If enough grade variables are observed, this network will be close to a full
bipartite graph, and exact inference about the posterior probability becomes intractable. This creates
a serious problem in applying either EM or gradient ascent for learning this seemingly simple model
from data.
An obvious solution to this problem is to use approximate inference procedures. A simple
approach is to view inference as a “black box.” Rather than invoking exact inference in the
learning procedures shown in algorithm 19.1 and algorithm 19.2, we can simply invoke one of the
approximate inference procedures we discussed in earlier chapters. This view is elegant because
it decouples the choices made in the design of the learning procedure from the choices made
in the approximate inference procedures.
However, this decoupling can obscure important eﬀects of the approximation on our learning
procedure. For example, suppose we use approximate inference for computing the gradient in
a gradient ascent approach. In this case, our estimate of the gradient is generally somewhat
wrong, and the errors in successive iterations are generally not consistent with each other. Such
inaccuracies can confuse the gradient ascent procedure, a problem that is particularly signiﬁcant
when the procedure is closer to the convergence point and the gradient is close to 0, so that the
errors can easily dominate. A key question is whether learning with approximate inference

results in an approximate learning procedure; that is, whether we are guaranteed to ﬁnd
a local maximum of an approximation of the likelihood function. In general, there are
very few cases where we can provide any types of guarantees on the interaction between
approximate inference and learning. Nevertheless, in practice, the use of approximate

19.2. Parameter Estimation
895
inference is often unavoidable, and so many applications use some form of approximate
inference despite the lack of theoretical guarantees.
One class of approximation algorithms for which a unifying perspective is useful is in the
combination of EM with the global approximate inference methods of chapter 11. Let us con-
sider ﬁrst the structured variational methods of section 11.5, where the integration is easiest to
structured
variational
understand. In these methods, we are attempting to ﬁnd an approximate distribution Q that is
close to an unnormalized distribution ˜P in which we are interested. We saw that algorithms
in this class can be viewed as ﬁnding a distribution Q in a suitable family of distributions that
maximizes the energy functional:
F[ ˜P, Q] = IEQ
h
log ˜P
i
+ IHQ(X).
Thus, in these approximate inference procedures, we search for a distribution Q that maximizes
max
Q∈Q F[ ˜P, Q].
We saw that we can view EM as an attempt to maximize the same energy functional, with
the diﬀerence that we are also optimizing over the parameterization θ of ˜P. We can combine
both goals into a single objective by requiring that the distribution Q used in the EM functional
come from a particular family Q. Thus, we obtain the following variational EM problem:
variational EM
max
θ
max
Q∈Q FD[θ, Q],
(19.7)
where Q is a family of approximate distributions we are considering for representing the distri-
bution over the unobserved variables.
To apply the variational EM framework, we need to choose the family of distributions Q that
will be used to approximate the distribution P(H | D, θ). Importantly, because this posterior
distribution is a product of the posteriors for the diﬀerent training instance, our approximation
Q can take the same form without incurring any error. Thus, we need only to decide how to
represent the posterior P(h[m] | o[m], θ) for each instance m. We therefore deﬁne a class Q
that we will use to approximate P(h[m] | o[m], θ). Importantly, since the evidence o[m] is
diﬀerent for each data instance m, the posterior distribution for each instance is also diﬀerent,
and hence we need to use a diﬀerent distribution Q[m] ∈Q to approximate the posterior for
each data instance. In principle, using the techniques of section 11.5, we can use any class
Q that allows tractable inference. In practice, a common solution is to use the mean ﬁeld
approximation, where we assume that Q is a product of marginal distributions (one per each
unobserved value).
Example 19.10
Consider using the mean ﬁeld approximation (see section 11.5.1) for the learning problem of exam-
ple 19.9. Recall that in the mean ﬁeld approximation we approximate the target posterior distribu-
tion by a product of marginals. More precisely, we approximate P(I(s1), . . . , I(sn), D(c1), . . . , D(cm) |
o, θ) by a distribution
Q(I(s1), . . . , I(sn), D(c1), . . . , D(cm)) = Q(I(s1)) · · · Q(I(sn))Q(D(c1)) · · · Q(D(cm)).
Importantly, although the prior over the variables I(s1), . . . , I(sn) is identical, their posterior is
generally diﬀerent. Thus, the marginal of each of the variable has diﬀerent parameters in Q (and
similarly for the D(c) variables).

896
Chapter 19. Partially Observed Data
In our approximate E-step, given a set of parameters θ for the model, we need to compute
approximate expected suﬃcient statistics. We do so in two steps. First, we use iterations of the mean
ﬁeld update equation equation (11.54) to ﬁnd the best choice of marginals in Q to approximate
P(I(s1), . . . , I(sn), D(c1), . . . , D(cm) | o, θ).
We then use the distribution Q to compute
approximate expected suﬃcient statistics by ﬁnding:
¯
MQ[g(i,j), I(si), D(cj)]
=
Q(I(si), D(cj))11{G(si, cj) = g(i,j)}
=
Q(I(si))Q(D(cj))11{G(si, cj) = g(i,j)}.
Given our choice of Q, we can optimize the variational EM objective very similarly to the
optimization of the exact EM objective, by iterating over two steps:
•
Variational E-step For each m, ﬁnd
variational E-step
Qt[m] = arg max
Q∈Q Fo[m][θ, Q].
This step is identical to our deﬁnition of variational inference in chapter 11, and it can be
implemented using the algorithms we discussed there, usually involving iterations of local
updates until convergence.
At the end of this step, we have an approximate distribution Qt = Q
m Qt[m] and can collect
the expected suﬃcient statistics. To compute the expected suﬃcient statistics, we combine
the observed values in the data with expected counts from the distribution Q. This process
requires answering queries about events in the distribution Q. For some approximations,
such as the mean ﬁeld approximation, we can answer such queries eﬃciently (that is, by
multiplying the marginal probabilities over each variables); see example 19.10.
If we use
a richer class of approximate distributions, we must perform a more elaborate inference
process. Note that, because the approximation Q is simpler than the original distribution P,
we have no guarantee that a clique tree for Q will respect the family-preservation property
relative to families in P. Thus, in some cases, we may need to perform queries that are
outside the clique tree used to perform the E-step (see section 10.3.3.2).
•
M-step We ﬁnd a new set of parameters
θt+1 = arg max
θ
FD[θ, Qt];
this step is identical to the M-step in standard EM.
The preceding algorithm is essentially performing coordinate-wise ascent alternating between
optimization of Q and θ. It opens up the way to alternative ways of maximizing the same
objective function. For example, we can limit the number of iterations in the variational E-step.
Since each such iteration improves the energy functional, we do not need to reach a maximum
in the Q dimension before making an improvement to the parameters.
Importantly, regardless of the method used to optimize the variational EM functional of
equation (19.7), we can provide some guarantee regarding the properties of our optimum. Recall
that we showed that
ℓ(θ : D) = max
Q FD[θ, Q] ≥max
Q∈Q FD[θ, Q].

19.3. Bayesian Learning with Incomplete Data ⋆
897
Thus, maximizing the objective of equation (19.7) maximizes a lower bound of the likelihood.
lower bound
When we limit the choice of Q to be in a particular family, we cannot necessarily get a tight
bound on the likelihood. However, since we are maximizing a lower bound, we know that we do
not overestimate the likelihood of parameters we are considering. If the lower bound is relatively
good, this property implies that we distinguish high-likelihood regions in the parameter space
from very low ones. Of course, if the lower bound is loose, this guarantee is not very meaningful.
We can try to extend these ideas to other approximation methods. For example, general-
ized belief propagation section 11.3 is an attractive algorithm in this context, since it can be
fairly eﬃcient. Moreover, because the cluster graph satisﬁes the family preservation property,
computation of an expected suﬃcient statistic can be done locally within a single cluster in
the graph. The question is whether such an approximation can be understood as maximizing
a clear objective. Recall that cluster-graph belief propagation can be viewed as attempting to
maximize an approximation of the energy functional where we replace the term IHQ(X) by
approximate entropy terms. Using exactly the same arguments as before, we can then show
that, if we use generalized belief propagation for computing expected suﬃcient statistics in the
E-step, then we are eﬀectively attempting to maximize the approximate version of the energy
functional.
In this case, we cannot prove that this approximation is a lower bound to the
correct likelihood. Moreover, if we use a standard message passing algorithm to compute the
ﬁxed points of the energy functional, we have no guarantees of convergence, and we may get
oscillations both within an E-step and over several steps, which can cause signiﬁcant problems
in practice. Of course, we can use other approximations of the energy functional, including ones
that are guaranteed to be lower bounds of the likelihood, and algorithms that are guaranteed to
be convergent. These approaches, although less commonly used at the moment, share the same
beneﬁts of the structured variational approximation.

More broadly, the ability to characterize the approximate algorithm as attempting to
optimize a clear objective function is important. For example, an immediate consequence
is that, to monitor the progress of the algorithm, we should evaluate the approximate
energy functional, since we know that, at least when all goes well, this quantity should
increase until the convergence point.
19.3
Bayesian Learning with Incomplete Data ⋆
19.3.1
Overview
In our discussion of parameter learning from complete data, we discussed the limitations of
maximum likelihood estimation, many of which can be addressed by the Bayesian approach.
In the Bayesian approach, we view the parameters as unobserved variables that inﬂuence the
probability of all training instances. Learning then amounts to computing the probability of
new examples based on the observation, which can be performed by computing the posterior
probability over the parameters, and using it for prediction.
More precisely, in Bayesian reasoning, we introduce a prior P(θ) over the parameters, and
are interested in computing the posterior P(θ | D) given the data. In the case of complete data,
we saw that if the prior has some properties (for example, the priors over the parameters of
diﬀerent CPDs are independent, and the prior is conjugate), then the posterior has a nice form

898
Chapter 19. Partially Observed Data
and is representable in a compact manner. Because the posterior is a product of the prior and
the likelihood, it follows from our discussion in section 19.1.3 that these useful properties are lost
in the case of incomplete data. In particular, as we can see from ﬁgure 19.4 and ﬁgure 19.5, the
parameter variables are generally correlated in the posterior. Thus, we can no longer represent
the posterior as a product of posteriors over each set of parameters. Moreover, the posterior will
generally be highly complex and even multimodal. Bayesian inference over this posterior would
generally require a complex integration procedure, which generally has no analytic solution.
One approach, once we realize that incomplete data makes the prospects of exact Bayesian
reasoning unlikely, is to focus on the more modest goal of MAP estimation, which we ﬁrst
MAP estimation
discussed in section 17.4.4. In this approach, rather than integrating over the entire posterior
P(D, θ), we search for a maximum of this distribution:
˜θ = arg max
θ
P(θ | D) = arg max
θ
P(θ)P(D | θ)
P(D)
.
Ideally, the neighborhood of the MAP parameters is the center of mass of the posterior, and
therefore, using them might be a reasonable approximation for averaging over parameters in
their neighborhood. Using the same transformations as in equation (17.14), the problem reduces
to one of computing the optimum:
scoreMAP(θ : D) = ℓ(θ : D) + log P(θ).
This function is simply the log-likelihood function with an additional prior term. Because this
prior term is usually well behaved, we can generally easily extend both gradient-based methods
and the EM algorithm to this case; see, for example, exercise 19.20 and exercise 19.21. Thus,
MAP-EM
ﬁnding MAP parameters is essentially as hard or as easy as ﬁnding MLE parameters. As such, it
is often applicable in practice. Of course, the same caveats that we discussed in section 17.4.4
— the sensitivity to parameterization, and the insensitivity to the form of the posterior — also
apply here.
A second approach is to try to address the task of full Bayesian learning using an approximate
method. Recall from section 17.3 that we can cast Bayesian learning as inference in the meta-
network that includes all the variables in all the instances as well as the parameters. Computing
the probability of future events amounts to performing queries about the posterior probability
of the (M + 1)st instance given the observations about the ﬁrst M instances. In the case of
complete data, we could derive closed-form solutions to this inference problem. In the case of
incomplete data, these solutions do not exist, and so we need to resort to approximate inference
procedures.
In theory, we can apply any approximate inference procedure for Bayesian networks to this
problem. Thus, all the procedures we discussed in the inference chapter can potentially be used
for performing Bayesian inference with incomplete data. Of course, some are more suitable than
others.
For example, we can conceivably perform likelihood weighting, as described in section 12.2:
we ﬁrst sample parameters from the prior, and then the unobserved variables.
Each such
sample will be weighted by the probability of the observed data given the sampled parameter
and hidden variables. Such a procedure is relatively easy to implement and does not require
running complex inference procedure. However, since the parameter space is a high-dimensional
continuous region, the chance of sampling high-posterior parameters is exceedingly small. As a

19.3. Bayesian Learning with Incomplete Data ⋆
899
result, virtually all the samples will be assigned negligible weight. Unless the learning problem
is relatively easy and the prior is quite informative, this inference procedure would provide a
poor approximation and would require a huge number of samples.
In the next two sections, we consider two approximate inference procedures that can be
applied to this problem with some degree of success.
19.3.2
MCMC Sampling
A common strategy for dealing with hard Bayesian learning problems is to perform MCMC
simulation (see section 12.3). Recall that, in these methods, we construct a Markov chain whose
state is the assignment to all unobserved variables, such that the stationary distribution of the
chain is posterior probability over these variables. In our case, the state of the chain consists
of θ and H, and we need to ensure that the stationary distribution of the chain is the desired
posterior distribution.
19.3.2.1
Gibbs Sampling
One of the simplest MCMC strategies for complex multivariable chains is Gibbs sampling. In
Gibbs sampling
Gibbs sampling, we choose one of the variables and sample its value given the value of all the
other variables. In our setting, there are two types of variables: those in H and those in θ; we
deal with each separately.
Suppose X[m] is one of the variables in H. The current state of the MCMC sampler has a
value for all other variables in H and for θ. Since the parameters are known, selecting a value
for X[m] requires a sampling step that is essentially the same as the one we did when we
performed Gibbs sampling for inference in the m’th instance. This step can be performed using
the same sampling procedure as in section 12.3.
Now suppose that θX|U are the parameters for a particular CPD. Again, the current state of
the sampler assigns value for all of the variables in H. Since the structure of the meta-network
meta-network
is such that θX|U are independent of the parameters of all other CPDs given D and H, then we
need to sample from P(θX|U | D, H) — the posterior distribution over the parameters given
the complete data D, H. In section 17.4 we showed that, if the prior is of a particular form (for
example, a product of Dirichlet priors), then the posterior based on complete data also has a
compact form. Now, we can use these properties to sample from the posterior. To be concrete,
if we consider table-CPDs with Dirichlet priors, then the posterior is a product of Dirichlet
distributions, one for each assignment of values for U. Thus, if we know how to sample from a
Dirichlet distribution, then we can sample from this posterior. It turns out that sampling from
a Dirichlet distribution can be done using a reasonably eﬃcient procedure; see box 19.E.
Thus, we can apply Gibbs sampling to the meta-network. If we simulate a suﬃciently long
run, the samples we generate will be from the joint posterior probability of the parameters and
the hidden variables. We can then use these samples to make predictions about new samples
and to estimate the marginal posterior of parameters or hidden variables. The bugs system (see
box 12.C) provides a simple, general-purpose tool for Gibbs-sampling-based Bayesian learning.

900
Chapter 19. Partially Observed Data
Box 19.E — Skill: Sampling from a Dirichlet distribution. Suppose we have a parameter vec-
tor random variable θ = ⟨θ1, . . . , θk⟩that is distributed according to a Dirichlet distribution
θ ∼Dirichlet(α1, . . . , αK). How do we sample a parameter vector from this distribution?
An eﬀective procedure for sampling such a vector relies on an alternative deﬁnition of the
Dirichlet distribution. We need to start with some deﬁnitions.
Deﬁnition 19.6
A continuous random variable X has a Gamma distribution Gamma(α, β) if it has the density
Gamma
distribution
p(x) =
βα
Γ(α)xα−1e−βx.
We can see that the xα−1 term is reminiscent of components of the Dirichlet distribution. Thus,
it might not be too surprising that there is a connection between the two distributions.
Theorem 19.7
Let X1, . . . , Xk be independent continuous random variables such that Xi ∼Gamma(αi, 1).
Deﬁne the random vector
θ =

X1
X1 + · · · + Xk
, . . . ,
Xk
X1 + · · · + Xk

.
Then, θ ∼Dirichlet(α1, . . . , αk).
Thus, we can think of a Dirichlet distribution as a two-step process. First, we sample k indepen-
dent values, each from a separate Gamma distribution. Then we normalize these values to get a
distribution. The normalization creates the dependency between the components of the vector θ.
This theorem suggests a natural way to sample from a Dirichlet distribution. If we can sam-
ple from Gamma distributions, we can sample values X1, . . . , Xk from the appropriate Gamma
distribution and then normalize these values.
The only remaining question is how to sample from a Gamma distribution. We start with a
special case. If we consider a variable X ∼Gamma(1, 1), then the density function is
p(X = x) = e−x.
In this case, we can solve the cumulative distribution using simple integration and get that
P(X < x) = 1 −e−x.
From this, it is not hard to show the following result:
Lemma 19.2
If U ∼Unif([0 : 1]), then −ln U ∼Gamma(1, 1).
In particular, if we want to sample parameter vectors from Dirichlet(1, . . . , 1), which is the
uniform distribution over parameter vectors, we need to sample k values from the uniform distri-
bution, take their negative logarithm, and normalize. Since a sample from a uniform distribution
can be readily obtained from a pseudo-random number generator, we get a simple procedure for
sampling from the uniform distribution over multinomial distributions. Note that this procedure is
not the one we intuitively would consider for this problem. When α ̸= 1 the sampling problem is
harder, and requires more sophisticated methods, often based on the rejection sampling approach
described in section 14.5.1; these methods are outside the scope of this book.

19.3. Bayesian Learning with Incomplete Data ⋆
901
19.3.2.2
Collapsed MCMC
Recall that, in many situations, we can make MCMC sampling more eﬃcient by using collapsed
particles that represent a partial state of the system. If we can perform exact inference over the
remaining state, then we can use MCMC sampling over the smaller state space and thereby get
more eﬃcient sampling.
We can apply this idea in the context of Bayesian inference in two diﬀerent ways. In one
approach, we have parameter collapsed particles, where each particle is an assignment to the
parameters θ, associated with a distribution over H; in the other, we have data completion
distribution particles, where each particle is an assignment to the unobserved variables H,
associated with a distribution over θ. We now discuss each of these approaches in turn.
Parameter Collapsed Particles
Suppose we choose the collapsed particles to contain assign-
ments to the parameters θ, accompanied by distributions over the hidden variables. Thus, we
need to be able to deal with queries about P(H | θ, D) and P(D, θ). First note that given θ,
the diﬀerent training instances are conditionally independent. Thus, we can perform inference
in each instance separately. The question now is whether we can perform this instance-level
inference eﬃciently. This depends on the structure of the network we are learning.
Consider, for example, the task of learning the naive Bayes clustering model of section 19.2.2.4.
In this case, each instance has a single hidden variable, denoting the cluster of the instance.
Given the value of the parameters, inference over the hidden variable involves summing over all
the values of the hidden variable, and computing the probability of the observation variables
given each value.
These operations are linear in the size of the network, and thus can be
done eﬃciently. This means that evaluating the likelihood of a proposed particle is quite fast.
On the other hand, if we are learning parameters for the network of example 19.9, then the
network structure is such that we cannot perform eﬃcient exact inference. In this case the
cost of evaluating the likelihood of a proposed particle is nontrivial and requires additional
approximations. Thus, the ease of operations with this type of collapsed particle depends on
the network structure.
In addition to evaluating the previous queries, we need to be able to perform the sampling
steps. In particular, for Gibbs sampling, we need to be able to sample from the distribution:
P(θXi|PaXi | {θXj|PaXj }j̸=i, D).
Unfortunately, sampling from this conditional distribution is unwieldy. Even though we assume
the value of all other parameters, since we do not have complete data, we are not guaranteed
to have a simple form for this conditional distribution (see example 19.11). As an alternative to
Gibbs sampling, we can use Metropolis-Hastings. Here, in each proposal step, we suggest new
parameter values and evaluate the likelihood of these new parameters relative to the old ones.
This step requires that we evaluate P(D, θ), which is as costly as computing the likelihood.
This fact makes it critical that we construct a good proposal distribution, since a poor one can
lead to many (expensive) rejected proposals.

902
Chapter 19. Partially Observed Data
C
X
q
Data m
lk
w
Clusters k
Figure 19.9
Plate model for Bayesian clustering
Example 19.11
Let us return to the setting of Bayesian clustering described in section 19.2.2.4. In this model, which
Bayesian
clustering
is illustrated in ﬁgure 19.9, we have a set of data points D = {x[1], . . . , x[M]}, which are taken
from one of a set of K clusters. We assume that each cluster is characterized by a distribution
Q(X | λ), which has the same form for each cluster, but diﬀerent parameters. As we discussed, the
form of the class-conditional distribution depends on the data; typical models include naive Bayes
for discrete data, or Gaussian distributions for continuous data. This decision is orthogonal to our
discussion here. Thus, we have a set of K parameter vectors λ1, . . . , λK, each sampled from a
distribution P(λk | ω). We use a hidden variable C[m] to represent the cluster from which the
m’th data point was sampled. Thus, the class-conditional distribution P(X | C = ck, λ1,...,K) =
Q(X | λk). We assume that the cluster variable C is sampled from a multinomial with parameters
θ, sampled from a Dirichlet distribution θ ∼Dirichlet(α0/K, . . . , α0/K). The symmetry of the
model relative to clusters reﬂects the fact that cluster identiﬁers are meaningless placeholders; it is
only the partition of instances to clusters that is signiﬁcant.
To consider the use of parameter collapsed particles, let us begin by writing down the data
likelihood given the parameters.
P(D | λ1, . . . , λK, θ) =
M
Y
m=1
 K
X
k=1
P(C[m] = ck | θ)P(x[m] | C[m] = ck, λk)
!
.
(19.8)
In this case, because of the simple structure of the graphical model, this expression is easy to
evaluate for any ﬁxed set of parameters.
Now, let us consider the task of sampling λk given θ and λ−k, where we use a subscript of −k
to denote the set consisting of all of the values k′ ∈{1, . . . , K}−{k}. The distribution with which
we wish to sample λk is
P(λk | λ−k, D, θ, ω) ∝P(D | λ1, . . . , λK, θ)P(λk | ω).
Examining equation (19.8), we see that, given the data and the parameters other than λk, all of the
terms P(x[m] | C[m] = ck′, λk′) for k′ ̸= k can now be treated as a constant, and aggregated
into a single number; similarly, the terms P(C[m] = ck | θ) are also constant. Hence, each of
the terms inside the outermost product can be written as a linear function amP(x[m] | C[m] =

19.3. Bayesian Learning with Incomplete Data ⋆
903
ck) + bm. Unfortunately, the entire expression is a product of these linear functions, making the
sampling distribution for λk proportional to a degree M polynomial in its likelihood function
(multiplied by P(λk | ω)). This distribution is rarely one from which we can easily sample.
The Metropolis-Hastings approach is more feasible in this case. Here, as discussed in section 14.5.3,
we can use a random-walk chain as a proposal distribution and use the data likelihood to compute
random-walk
chain
the acceptance probabilities. In this case, the computation is fairly straightforward, since it involves
the ratio of two expressions of the form of equation (19.8), which are the same except for the values
of λk. Unfortunately, although most of the terms in the numerator and denominator are identical,
they appear within the scope of a summation over k and therefore do not cancel. Thus, to compute
the acceptance probability, we need to compute the full data likelihood for both the current and
proposed parameter choices; in this particular network, this computation can be performed fairly
eﬃciently.
Data Completion Collapsed Particles
An alternative choice is to use collapsed particles that
assign a value to H. In this case, each particle represents a complete data set. Recall that if the
prior distribution satisﬁes certain properties, then we can use closed-form formulas to compute
parameter posteriors from P(λ | D, H) and to evaluate the marginal likelihood P(D, H). This
implies that if we are using a well-behaved prior, we can evaluate the likelihood of particles in
time that does not depend on the network structure.
For concreteness, consider the case where we are learning a Bayesian network with table-
CPDs where we have an independent Dirichlet prior over each distribution P(Xi | paXi). In
this case, if we have a particle that represents a complete data instance, we can summarize
it by the suﬃcient statistics M[xi, paxi], and using these we can compute both the posterior
over parameters and the marginal likelihood using closed-form formulas; see section 17.4 and
section 18.3.4.
Example 19.12
Let us return to the Bayesian clustering task, but now consider the setting where each particle c is
an assignment to the hidden variables C[1], . . . , C[M]. Given an assignment to these variables,
we are now in the regime of complete data, in which the diﬀerent parameters are independent in
the posterior. In particular, let Ik(c) be the set of indexes {m : c[m] = k}. We can now compute
the distribution associated with our particle c as a Dirichlet posterior
P(θ | c) = Dirichlet(α0/K + |I1(c)|, . . . , α0/K + |IK(c)|).
We also have that:
P(λk | c, D, ω) = P(λk | DIk(c), ω) ∝P(λk | ω)
Y
m∈Ik(c)
P(x[m] | λk),
that is, the posterior over λk starting from the prior deﬁned by ω and conditioning on the data
instances in Ik(c). If we now further assume that P(λ | ω) is a conjugate prior to Q(X | λ), this
posterior can be computed in closed form.
To apply Gibbs sampling, we also need to specify a distribution for sampling a new value for
C[m′] given c−m′, where again, we use the notation −m′ to indicate all values {1, . . . , M} −
{m′}. Similarly, let Ik(c−m′) denote the set of indexes {m ̸= m′
:
c[m] = k}. Due to the

904
Chapter 19. Partially Observed Data
independencies represented by the model structure, we have:
P(C[m′] = k | c−m′, D, ω) ∝
(19.9)
P(C[m′] = k | c−m′)P(x[m′] | C[m′] = k, x[Ik(c−m′)], ω).
The second term on the right-hand side is simply a Bayesian prediction over X from the parameter
posterior P(λk | DIk(c−m′), ω), as deﬁned. Because of the symmetry of the parameters for the
diﬀerent clusters, the term does not depend on m′ or on k, but only on the data set on which we
condition. We can rewrite this term as Q(X | DIk(c−m′), ω). The ﬁrst term on the right-hand side
is the prior on cluster assignment for the instance m′, as determined by the Dirichlet prior and the
assignments of the other instances. Some algebra allows us to simplify this expression, resulting in:
P(C[m′] = k | c−m′, D, ω) ∝(|Ik(c−m′)| + α0/K)Q(X | DIk(c−m′), ω).
(19.10)
Assuming we have a conjugate prior, this expression can be easily computed. Overall, for most con-
jugate priors, the cost of computing the sampling distribution for C[m] in this model is O(MK).
It turns out that eﬃcient sampling is also possible in more general models; see exercise 19.22.
Alternatively we can use a Metropolis-Hastings approach where the proposal distribution can
propose to modify several hidden values at once; see exercise 19.23.
Comparison
Both types of collapsed particles can be useful for learning in practice, but they
have quite diﬀerent characteristics.
As we discussed, when we use parameter collapsed particles, the cost of evaluating a particle
(for example, in a Metropolis-Hastings iteration) is determined by cost of inference in the net-
work. In the worst case, this cost can be exponential, but in many examples it can be eﬃcient.
In contrast, the cost of evaluating data collapsed particles depends on properties of the prior. If
the prior is properly chosen, the cost is linear in the size of the network.
Another aspect is the space in which we perform MCMC. In the case of parameter collapsed
particles, the MCMC procedure is performing integration over a high-dimensional continuous
space. The simple Metropolis-Hastings procedures we discussed in this book are usually quite
poor for addressing this type of task. However, there is an extensive literature of more eﬃcient
MCMC procedures for this task (these are beyond the scope of this book). In the case of data
collapsed particles, we perform the integration over parameters in closed form and use MCMC
to explore the discrete (but exponential) space of assignments to the unobserved variables. In
this problem, relatively simple MCMC methods, such as Gibbs sampling, can be fairly eﬃcient.
To summarize, there is no clear choice between these two options. Both types of collapsed
particles can speed up the convergence of the sampling procedure and the accuracy of the
estimates of the parameters.
19.3.3
Variational Bayesian Learning
Another class of approximate inference procedures that we can apply to perform Bayesian infer-
ence in the case of incomplete data are variational approximations. Here, we can use the meth-
ods we developed in chapter 11 to the inference problem posed by Bayesian learning paradigm,
resulting in an approach called variational Bayes. Recall that, in a variational approximation, we
variational Bayes

19.3. Bayesian Learning with Incomplete Data ⋆
905
aim to ﬁnd a distribution Q, from a predetermined family of distributions Q, that is close to
the real posterior distribution. In our case, we attempt to approximate P(H, θ | D); thus, the
unnormalized measure ˜P in equation (11.3) is P(H, θ, D), and our approximating distribution
Q is over the parameters and the hidden variables.
Plugging in the variational principle for our problem, and using the fact that P(H, θ, D) =
P(θ)P(H, D | θ), we have that the energy functional takes the form:
F[P, Q] = IEQ[log P(θ)] + IEQ[log P(H, D | θ)] + IHQ(θ, H).
The development of such an approximation requires that we decide on the class of approxi-
mate distributions we want to consider. While there are many choices here, a natural one is to
decouple the posterior over the parameters from the posterior over the missing data. That is,
assume that
Q(θ, H) = Q(θ)Q(H).
(19.11)
This is clearly a nontrivial assumption, since our previous discussion shows that these two
posteriors are coupled by the data.
Nonetheless, we can hope that an approximation that
decouples the two distributions will be more tractable.
Recall that, in our discussion of structured variational methods, we saw that the interactions
between the structure of the approximation Q and the true distribution P can lead to further
structural simpliﬁcations in Q (see section 11.5.2.4). Using these tools, we can ﬁnd the following
simpliﬁcation.
Theorem 19.8
Let P(θ) be a parameter prior satisfying global parameter independence, P(θ) = Q
i P(θXi|U i).
Let D be a partially observable IID data set.
If we consider a variational approximation with
distributions satisfying Q(θ, H) = Q(θ)Q(H), then Q can be decomposed as
Q(θ, H) =
Y
i
Q(θXi|U i)
Y
m
Q(h[m]).
The proof is by direct application of proposition 11.7 and is left as an exercise (exercise 19.24).
This theorem shows that, once we decouple the posteriors over the parameters and missing
data, we also lose the coupling between components of the two distributions (that is, diﬀerent
parameters or diﬀerent instances). Thus, we can further decompose each of the two posteriors
into a product of independent terms.
This result matches our intuition, since the coupling
between the parameters and missing data was the source of dependence between components
of the two distributions.
That is, the posteriors of two parameters were dependent due to
incomplete data, and the posterior of missing data in two instances were dependent due to
uncertainty about the parameters.
This theorem does not necessarily justify the (strong) assumption of equation (19.11), but
it does suggest that it provides signiﬁcant computational gains.
In this case, we see that
we can assume that the approximate posterior also satisﬁes global parameter independence,
and similarly the approximate distribution over H consists of independent posteriors, one per
instance. This simpliﬁcation already makes the representation of Q much more tractable. Other
simpliﬁcations, following the same logic, are also possible.
The variational Bayes approach often gives rise to very natural update rules.

906
Chapter 19. Partially Observed Data
Example 19.13
Consider again the Bayesian clustering model of section 19.2.2.4.
In this case, we aim to rep-
resent the posterior over the parameters θH, θX1|H, . . . , θXn|H and over the hidden variables
H[1], . . . , H[M]. The decomposition of theorem 19.8 allows us write Q as a product distribution,
with a term for each of these variables. Thus, we have that
Q = Q(θH)
"Y
i
Q(θXi|H)
# "Y
m
Q(H[m])
#
.
This factorization is essentially a mean ﬁeld approximation. Using the results of section 11.5.1, we
mean ﬁeld
see that the ﬁxed-point equations for this approximation are of the form
Q(θH)
∝
exp
(
ln P(θH) +
X
m
IEQ(H[m])[ln P(H[m] | θH)]
)
Q(θXi|H)
∝
exp
(
ln P(θXi|H) +
X
m
IEQ(H[m])

ln P(xi[m] | H[m], θXi|H)

)
Q(H[m])
∝
exp

IEQ(θH)[ln P(H[m] | θH)]
+
X
i
IEQ(θXi|H)

ln P(xi[m] | H[m], θXi|H)

)
.
The application of the mean-ﬁeld theory allows us to identify the structure of the update equation.
To provide a constructive solution, we also need to determine how to evaluate the expectations in
these update equations. We now examine these expectations in the case where all the variables are
binary and the priors over parameters are simple Dirichlet distributions (Beta distributions, in fact).
We start with the ﬁrst ﬁxed-point equation. A value for θH is a pair ⟨θh0, θh1⟩. Using the
deﬁnition of the Dirichlet prior, we have that
ln P(θH = ⟨θh0, θh1⟩) = ln c + (αh0 −1) ln θh0 + (αh1 −1) ln θh1,
where αh0 and αh1 are the hyperparameters of the prior P(θH), and c is the normalizing constant
of the prior (which we can ignore). Similarly, we can see that
IEQ(H[m])[ln P(H[m] | θH = ⟨θh0, θh1⟩)] = Q(H[m] = h0) ln θh0 + Q(H[m] = h1) ln θh1.
Combining these results, we get that
Q(θH = ⟨θh0, θh1⟩)
∝
exp
( 
αh0 +
X
m
Q(H[m] = h0) −1
!
ln θh0+
 
αh1 +
X
m
Q(H[m] = h1) −1
!
ln θh1
)
=
θ
αh0+P
m Q(H[m]=h0)−1
h0
θ
αh1+P
m Q(H[m]=h1)−1
h1
.

19.3. Bayesian Learning with Incomplete Data ⋆
907
In other words, Q(θH) is a Beta distribution with hyperparameters
α′
h0
=
αh0 +
X
m
Q(H[m] = h0)
α′
h1
=
αh1 +
X
m
Q(H[m] = h1).
Note that this is exactly the Bayesian update for θH with the expected suﬃcient statistics given
Q(H).
A similar derivation shows that Q(θXi|H) is also a pair of independent Beta distributions (one
for each value of H) that are updated with the expected suﬃcient statistics given Q(H).
These updates are reminiscent of the EM-update (M-step), since we use expected suﬃcient statistics
M-step
to update the posterior. In the EM M-step, we update the MLE using the expected suﬃcient statistics.
If we carry the analogy further, the last ﬁxed-point equation, which updates Q(H[m]), corresponds
to the E-step, since it updates the expectations over the missing values. Recall that, in the E-step of
E-step
EM, we use the current parameters to compute
Q(H[m]) = P(H[m] | x1[m], . . . xn[m]) ∝P(H[m] | θH)
Y
i
P(xi[m] | H[m], θXi|H).
If we were doing a Bayesian approach, we would not simply take our current values for the param-
eters θH, θXi|H; rather, we would average over their posteriors. Examining this last ﬁxed-point
equation, we see that we indeed average over the (approximate) posteriors Q(θH) and Q(θXi|H).
However, unlike standard Bayesian averaging, where we compute the average value of the parameter
itself, here we average its logarithm; that is, we evaluate terms of the form
IEQ(θXi|H)

ln P(xi | H[m], θXi|H)

=
1
Z
0
Q(θxi|H[m]) ln θxi|H[m]dθxi|H[m].
Using methods that are beyond the scope of this book, one can show that this integral has a
closed-form solution:
IEQ(θXi|H)

ln P(xi | H[m], θXi|H)

= ϕ(α′
xi|h) −ϕ(
X
x′
i
α′
x′
i|h),
where α′ are the hyperparameters of the posterior approximation in Q(θXi|H) and ϕ(z) =
(ln Γ(z))′ = Γ′(z)
Γ(z) is the digamma function, which is equal to ln(z) plus a polynomial function
digamma
function
of 1
z. And so, for z ≫1, ϕ(z) ≈ln(z). Using this approximation, we see that
IEQ(θXi|H)

ln P(xi | H[m], θXi|H)

≈ln
α′
xi|h
P
x′
i α′
x′
i|h
,
that is, the logarithm of the expected conditional probability according to the posterior Q(θXi|H).
This shows that if the posterior hyperparameters are large the variational update is almost identical
to EM’s E-step.
To wrap up, we applied the structured variational approximation to the Bayesian learning prob-
lem. Using the tools we developed in previous chapters, we deﬁned tractable ﬁxed-point equations.

908
Chapter 19. Partially Observed Data
As with the mean ﬁeld approximation we discussed in section 11.5.1, we can ﬁnd a ﬁxed-point
solution for Q by iteratively applying these equations.
The resulting algorithm is very similar to applications of EM. Applications of the update equations
for the parameters are almost identical to standard EM of section 19.2.2.4 in the sense that we
use expected suﬃcient statistics.
However, instead of ﬁnding the MLE parameters given these
expected suﬃcient statistics, we compute the posterior assuming these were observed. The update
for Q(H[m]) is reminiscent to the computation of P(H[m]) when we know the parameters.
However, instead of using parameter values we use expectations of their logarithm and then take
the exponent.
This example shows that we can ﬁnd a variational approximation to the Bayesian posterior
using an EM-like algorithm in which we iterate between updates to the parameter posteriors and
updates to the missing data posterior. These ideas generalize to other network structures in a
fairly straightforward way. The update for the posterior over parameter is similar to Bayesian
update with expected suﬃcient statistics, and the update of the posterior over hidden variable is
similar to a computation with the expected parameters (with the diﬀerences discussed earlier).
In more complex examples we might need to make further assumptions about the distribution
Q in order to get a tractable approximation. For example, if there are multiple missing values
per instance, then we might not be able to aﬀord to represent their distribution by the joint
distribution and would instead need to introduce structure into Q. The basic ideas are similar
to ones we explored before, and so we do not elaborate them. See exercise 15.6 for one example.
Of course, this method has some clear drawbacks. Because we are representing the parameter
posterior by a factored distribution, we cannot expect to represent a multimodal posterior.
Unfortunately, we know that the posterior is often multimodal. For example, in the clustering
problem, we know that change in names of values of H would not change the prediction. Thus,
the posterior in this example should be symmetric under such renaming. This implies that a
unimodal distribution can only be a partial approximation to the true posterior. In multimodal
cases, the eﬀect of the variational approximation cannot be predicted. It may select one of the
peaks and try to approximate it using Q, or it may choose a “broad” distribution that averages
over some or all of the peaks.
19.4
Structure Learning
We now move to discuss the more complex task of learning the network structure as well as the
parameters, again in the presence of incomplete data. Recall that in the case of complete data,
we started by deﬁning a score for evaluating diﬀerent network structures and then examined
search procedures that can maximize this score. As we will see, both components of structure

learning — the scoring function and the search procedure — are considerably more
complicated in the case of incomplete data. Moreover, in the presence of hidden variables,
even our search space becomes signiﬁcantly more complex, since we now have to select
the value space for the hidden variables, and even the number of hidden variables that
the model contains.

19.4. Structure Learning
909
19.4.1
Scoring Structures
In section 18.3, we deﬁned three scores: the likelihood score, the BIC score, and the Bayesian
score. As we discussed, the likelihood score does not penalize more complex models, and it is
therefore not useful when we want to compare between models of diﬀerent complexity. Both
the BIC and Bayesian score have built-in penalization for complex models and thus trade oﬀthe
model complexity with its ﬁt to the data. Therefore, they are far less likely to overﬁt.
We now consider how to extend these scores to the case when some of the data are missing.
On the face of it, the score we want to evaluate is the same Bayesian score we considered in the
case of complete data:
scoreB(G : D) = log P(D | G) + log P(G)
where P(D | G) is the marginal likelihood of the data:
P(D | G) =
Z
ΘG
P(D | θG, G)P(θG | G)dθG.
In the complete data case, the likelihood term inside the integral had a multiplicative factor-
ization, and thus we could simplify it. In the case of incomplete data, the likelihood involves
summing out over the unobserved variables, and thus it does not decompose.
As we discussed, we can view the computation of the marginal likelihood as an inference
problem. For most learning problems with incomplete data, this inference problem is a diﬃcult
one. We now consider diﬀerent strategies for dealing with this issue.
19.4.1.1
Laplace Approximation
One approach for approximating an integral in a high-dimensional space is to provide a simpler
approximation to it, which we can then integrate in closed form. One such method is the
Laplace approximation, described in box 19.F.
Laplace
approximation
Box 19.F — Concept: Laplace Approximation. The Laplace approximation can be applied to
any function of the form f(w) = eg(w) for some vector w. Our task is to compute the integral
F =
Z
f(w)dw.
Using Taylor’s expansion, we can expand an approximation of g around a point w0
g(w) ≈g(w0) +
∂g(w)
∂xi

w=w0
(w −w0) + 1
2(w −w0)T
∂∂g(w)
∂xi∂xj

w=w0
(w −w0),
where
h
∂g(w)
∂xi
i
w=w0 denotes the vector of ﬁrst derivatives and
h
∂∂g(w)
∂xi∂xj
i
w=w0 denotes the
Hessian — the matrix of second derivatives.
Hessian
If w0 is the maximum of g(w), then the second term disappears. We now set
C = −
∂2g(w)
∂xi∂xj

w=w0

910
Chapter 19. Partially Observed Data
to be the negative of the matrix of second derivatives of g(w) at w0. Since w0 is a maximum, this
matrix is positive semi-deﬁnitive. Thus, we get the approximation
g(w) ≈g(w0) −1
2(w −w0)T C(w −w0).
Plugging this approximation into the deﬁnition of f(x), we can write
Z
f(w)dw ≈f(w0)
Z
e−1
2 (w−w0)T C(w−w0)dw.
The integral is identical to the integral of an unnormalized Gaussian distribution with covariance
matrix Σ = C−1. We can therefore solve this integral analytically and obtain:
Z
f(w)dw ≈f(w0)|C|−1
2 (2π)
1
2 dim(C),
where dim(C) is the dimension of the matrix C.
At a high level, the Laplace approximation uses the value at the maximum and the curvature
(the matrix of second derivatives) to approximate the integral of the function. This approximation
works well when the function f is dominated by a single peak that has roughly a Gaussian shape.
How do we use the Laplace approximation in our setting? Taking g to be the log-likelihood
function combined with the prior log P(D | θ, G) + log P(θ|G), we get that log P(D, G) can
be approximated by the Laplace score:
Laplace score
scoreLaplace(G : D) = log P(G) + log P(D | ˜θG, G) + dim(C)
2
log 2π −1
2 log |C|,
where ˜θG are the MAP parameters and C is the negative of the Hessian matrix of the log-
likelihood function. More precisely, the entries of C are of the form
−∂2 log P(D | θ, G)
∂θxi|ui∂θxj|uj

˜θG
= −
X
m
∂2 log P(o[m] | θ, G)
∂θxi|ui∂θxj|uj

˜θG
,
where θxi|ui and θxj|uj are two parameters (not necessarily from the same CPD) in the param-
eterization of the network.
The Laplace score takes into account not only the number of free parameters but also the
curvature of the posterior distribution in each direction. Although the form of this expression
arises directly by approximating the posterior marginal likelihood, it is also consistent with our
intuitions about the desired behavior. Recall that the parameter posterior is a concave function,
and hence has a negative deﬁnitive Hessian. Thus, the negative Hessian C is positive deﬁnite
and therefore has a positive determinant. A large determinant implies that the curvature at
the MAP point is sharp; that is, the peak is relatively narrow and most of its mass is at the
maximum. In this case, the model is probably overﬁtting to the training data, and we incur
a large penalty. Conversely, if the curvature is small, the peak is wider, and the mass of the
posterior is distributed over a larger set of parameters. In this case, overﬁtting is less likely, and,
indeed, the Laplace score imposes a smaller penalty on the model.

19.4. Structure Learning
911
To compute the Laplace score, we ﬁrst need to use one of the methods we discussed earlier
to ﬁnd the MAP parameters of the distribution, and then compute the Hessian matrix. The
computation of the Hessian is somewhat involved.
To compute the entry for the derivative
relative to θxi|ui and θxj|uj, we need to compute the joint distribution over xi, xj, ui, uj given
the observation; see exercise 19.9. Because these variables are not necessarily together in a
clique (or cluster), the cost of doing such computations can be much higher than computing the
likelihood. Thus, this approximation, while tractable, is still expensive in practice.
19.4.1.2
Asymptotic Approximations
One way of avoiding the high cost of the Laplace approximation is to approximate the term
|C|−1
2 . Recall that the likelihood is the sum of the likelihood of each instance. Thus, the
Hessian matrix is the sum of many Hessian matrixes, one per instance.
We can consider
asymptotic approximations that work well when the number of instances grows (M →∞). For
this analysis, we assume that all data instances have the same observation pattern; that is, the
set of variables O[m] = O for all m.
Consider the matrix C. As we just argued, this matrix has the form
C =
M
X
m=1
Cm,
where Cm is the negative of the hessian of log P(o[m] | θ, G). We can view each Cm as a
sample from a distribution that is induced by the (random) choice of assignment o to O; each
assignment o induces a diﬀerent matrix Co. We can now rewrite:
C = M 1
M
M
X
m=1
Cm.
As M grows, the term
1
M
PM
m=1 Cm approaches the expectation IEP ∗[Co].
Taking the determinant of both sides, and recalling that det (αA) = αdim(A)det (A), we get
det (C) = M dim(C)det
 
1
M
M
X
m=1
Cm
!
≈M dim(C)det (IEP ∗[Co]) .
Taking logarithms of both sides, we get that
log det (C) ≈dim(C) log M + log det (IEP ∗[Co]) .
Notice that the last term does not grow with M.
Thus, when we consider the asymptotic
behavior of the score, we can ignore it. This rough argument is the outline of the proof for the
following result.
Theorem 19.9
As M →∞, we have that:
scoreLaplace(G : D) = scoreBIC(G : D) + O(1)
where scoreBIC(G : D) is the BIC score
BIC score
scoreBIC(G : D) = log P(D | ˜θG, G) −log M
2
Dim[G] + log P(G) + log P(˜θG | G).

912
Chapter 19. Partially Observed Data
This result shows that the BIC score is an asymptotic approximation to the Laplace score, a
conclusion that is interesting for several important reasons. First, it shows that the intuition we
had for the case of complete data, where the score trades oﬀthe likelihood of the data with a
structure penalty, still holds. Second, as in the complete data case, the asymptotic behavior of
this penalty is logarithmic in the number of samples; this relationship implies the rate at which
more instances can lead us to introduce new parameters.
An important subtlety in this analysis is hidden in the use of the notation Dim[G]. In the case
of complete data, this notation stood for the number of independent parameters in the network,
independent
parameters
a quantity that we could easily compute. Here, it turns out that for some models, the actual
number of degrees of freedom is smaller than the space of parameters. This implies that the
matrix C is not of full rank, and so its determinant is 0. In such cases, we need to perform a
variant of the Laplace approximation in the appropriate subspace, which leads to a determinant
of a smaller matrix. The question of how to determine the right number of degrees of freedom
(and thus the magnitude of Dim[G]) is still an open problem.
19.4.1.3
Cheeseman-Stutz Approximation
We can use the Laplace/BIC approximations to derive an even tighter approximation to the
Bayesian score. The intuition is that, in the case of complete data, the full Bayesian score was
more precise than the BIC score since it took into account the extent to which each parameter
was used and how its range of values inﬂuenced the likelihood. These considerations are explicit
in the integral form of the likelihood and implicit in the closed-form solution of the integral.
When we use the BIC score on incomplete data, we lose these ﬁne-grained distinctions in
evaluating the score.
Recall that the closed-form solution of the Bayesian score is a function of the suﬃcient
statistics of the data. An ad hoc approach for constructing a similar (approximate) score when
we have incomplete data is to apply the closed-form solution of the Bayesian score on some
approximation of the statistics of the data. A natural choice would be the expected suﬃcient
statistics given the MAP parameters. These expected suﬃcient statistics represent the completion
of the data given our most likely estimate of the parameters.
More formally, for a network G and a set of parameters θ, we deﬁne D∗
G,θ to be a ﬁctitious
“complete” data set whose actual counts are the same as the fractional expected counts relative
to this network; that is, for every event x:
MD∗
G,θ[x] = ¯
MP (H|D,θ,G)[x].
(19.12)
Because the expected counts are based on a coherent distribution, there can be such a data
set (although it might have instances with fractional weights). To evaluate a particular network
G, we deﬁne the data set D∗
G,˜θG induced by our network G and its MAP parameters ˜θG, and
approximate the Bayesian score P(D | G) by P(D∗
G,˜θG | G), using the standard integration over
the parameters.
While straightforward in principle, a closer look suggests that this approximation cannot be a
very good one. The ﬁrst term,
P(D | G) =
Z X
H
p(D, H | θ, G)P(θ | G)dθ =
X
H
Z
p(D, H | θ, G)P(θ | G)dθ,

19.4. Structure Learning
913
involves a summation of exponentially many integrals over the parameter space — one for each
assignment to the hidden variables H. On the other hand, the approximating term
P(D∗
G,˜θG | G) =
Z
p(D∗
G,˜θG | θ, G)P(θ | G)dθ
is only a single such integral. In both terms, the integrals are over a “complete” data set, so that
one of these sums is on a scale that is exponentially larger than the other.
One ad hoc solution is to simply correct for this discrepancy by estimating the diﬀerence:
log P(D | G) −log P(D∗
G,˜θG | G).
We use the asymptotic Laplace approximation to write each of these terms, to get:
log P(D | G) −log P(D∗
G,˜θG | G)
≈

log P(D | ˜θG, G) −1
2Dim[G] log M

−

log P(D∗
G,˜θG | ˜θG, G) −1
2Dim[G] log M

=
log P(D | ˜θG, G) −log P(D∗
G,˜θG | ˜θG, G).
The ﬁrst of these terms is the log-likelihood achieved by the MAP parameters on the observed
data. The second is the log-likelihood on the ﬁctional data set, a term that can be computed
in closed form based on the statistics of the ﬁctional data set. We see that the ﬁrst term is,
again, a summation of an exponential number of terms representing diﬀerent assignments to
H. We note that the Laplace approximation is valid only at the large sample limit, but more
careful arguments can show that this construction is actually fairly accurate for a large class of
situations.
Putting these arguments together, we can write:
log P(D | G)
=
log P(D∗
G,˜θG | G) + log P(D | G) −log P(D∗
G,˜θG | G)
≈
log P(D∗
G,˜θG | G) + log P(D | ˜θG, G) −log P(D∗
G,˜θG | ˜θG, G).
This approximation is the basis for the Cheeseman-Stutz score:
Cheeseman-Stutz
score
scoreCS(G : D) = log P(G) + log P(D∗
G,˜θG | G) + log P(D | ˜θG, G) −log P(D∗
G,˜θG | ˜θG, G)
The appealing property of the Cheeseman-Stutz score is that, unlike the BIC score, it uses the
closed-form solution of the complete data marginal likelihood in the context of incomplete data.
Experiments in practice (see box 19.G) show that this score is much more accurate than the BIC
score and much cheaper to evaluate than the Laplace score.
19.4.1.4
Candidate Method
Another strategy for approximating the score is the candidate method; it uses a particular choice
candidate
method
of parameters (the candidate) to evaluate the marginal likelihood. Consider any set of parameters
θ. Using the chain law of probability, we can write P(D, θ | G) in two diﬀerent ways:
P(D, θ | G)
=
P(D | θ, G)P(θ | G)
P(D, θ | G)
=
P(θ | D, G)P(D | G).

914
Chapter 19. Partially Observed Data
Equating the two right-hand terms, we can write
P(D | G) = P(D | θ, G)P(θ | G)
P(θ | D, G)
.
(19.13)
The ﬁrst term in the numerator is the likelihood of the observed data given θ, which we should
be able to evaluate using inference (exact or approximate). The second term in the numerator is
the prior over the parameters, which is usually given. The denominator is the posterior over the
parameters, the term most diﬃcult to approximate.
The candidate method reduces the problem of computing the marginal likelihood to the
problem of generating a reasonable approximation to the parameter posterior P(θ | D, G). It
lets us estimate the likelihood when using methods such as MCMC sampling to approximate
the posterior distribution. Of course, the quality of our approximation depends heavily on the
design of the MCMC sampler. If we use a simple sampler, then the precision of our estimate of
P(θ | D, G) will be determined by the number of sampled particles (since each particle either
has these parameters or not). If, instead, we use collapsed particles, then each particle will have
a distribution over the parameters, providing a better and smoother estimate for the posterior.
The quality of our estimate also depends on the particular choice of candidate θ. We can
obtain a more robust estimate by averaging the estimates from several choices of candidate
parameters (say several likely parameter assignments based on our simulations). However, each
of these requires inference for computing the numerator in equation (19.13), increasing the cost.
An important property of the candidate method is that equation (19.13), on which the method
is based, is not an approximation. Thus, if we could compute the denominator exactly, we
would have an exact estimate for the marginal likelihood. This gives us the option of using more
computational resources in our MCMC approximation to the denominator, to obtain increasingly
accurate estimates. By contrast, the other methods all rely on an asymptotic approximation to
the score and therefore do not oﬀer a similar trade-oﬀof accuracy to computational cost.
19.4.1.5
Variational Marginal Likelihood
A diﬀerent approach to estimating the marginal likelihood is using the variational approximations
we discussed in section 19.3.3. Recall from corollary 19.1 that, for any distribution Q,
ℓ(θ : D) = FD[θ, Q] + ID(Q(H)||P(H | D, θ)).
It follows that the energy functional is a lower bound of the marginal likelihood, and the
diﬀerence between them is the relative entropy between the approximate posterior distribution
and the true one. Thus, if we ﬁnd a good approximation Q of the posterior P(H | D, θ), then
the relative entropy term is small, so that that energy functional is a good approximation of the
marginal likelihood. As we discussed, the energy functional itself has the form:
FD[θ, Q] = IEH∼Q[ℓ(θ : D, H)] + IHQ(H).
Both of these terms can be computed using inference relative to the distribution Q. Because
this distribution was chosen to allow tractable inference, this provides a feasible approach for
approximating the marginal likelihood.

19.4. Structure Learning
915
Box 19.G — Case Study: Evaluating Structure Scores. To study the diﬀerent approximations
to the Bayesian score in a restricted setting, Chickering and Heckerman (1997) consider learning
a naive Bayes mixture distribution, as in section 19.2.2.4, where the cardinality K of the hidden
mixture
distribution
variable (the number of mixture components) is unknown. Adding more values to the class variables
increases the representational power of the model, but also introduces new parameters and thus
increases the ability of the model to overﬁt the data.
Since the class of distributions that are
representable with a cardinality of K is contained within those that are representable with a
cardinality of K′ > K, the likelihood score increases monotonically with the cardinality of the
class variable. The question is whether the diﬀerent structure scores can pinpoint a good cardinality
for the hidden variable. To do so, they perform MAP parameter learning on structures of diﬀerent
cardinality and then evaluate the diﬀerent scores. Since the structure learning problem is one-
dimensional (in the sense that the only parameter to learn is the cardinality of the class variable),
there is no need to consider a speciﬁc search strategy in the evaluation.
It is instructive to evaluate performance on both real data, and on synthetic data where the true
number of clusters is known. However, even in synthetic data cases, where the true cardinality
of the hidden variable is known, using this true cardinality as the “gold standard” for evaluating
methods may not be appropriate, as with few data instances, the “optimal” model may be one with
fewer parameters. Thus, Chickering and Heckerman instead compare all methods to the candidate
method, using MCMC to evaluate the denominator; with enough computation, one can use this
method to obtain high-quality approximations to the correct marginal likelihood.
The data in the synthetic experiments were generated from a variety of models, which varied along
several axes: the true cardinality of the hidden variable (d); the number of observed variables (n);
and the number of instances (M). The ﬁrst round of experiments revealed few diﬀerences between
the diﬀerent scores. An analysis showed that this was because synthetic data sets with random
parameter choices are too easy. Because of the relatively large number of observed variables, such
random models always had distinguished clusters. That is, using the true parameters, the posterior
probability P(c | x1, . . . , xn) is close to 1 for the true cluster value and 0 for all others. Thus, the
instances belonging to diﬀerent clusters are easily separable, making the learning problem too easy.
To overcome this problem, they considered sampling networks where the values of the parameters
for P(Xi | c) are correlated for diﬀerent values of c. If the correlation is absolute, then the clusters
overlap. For intermediate correlation the clusters were overlapping but not identical. By tuning the
degree of correlation in sampling the distribution, they managed to generate networks with diﬀerent
degree of separation between the clusters. On data sets where the generating distribution did not
have any separation between clusters, all the scores preferred to set the cardinality of the cluster
variable to 1, as expected. When they examined data sets where the generating distribution had
partial overlap between clusters they saw diﬀerentiating behavior between scoring methods. They
also performed this same analysis on several real-world data sets. Figure 19.G.1 demonstrates the
results for two data sets and summarizes the results for many of the synthetic data sets, evaluating
the ability of the diﬀerent methods to come close to the “optimal” cardinality, as determined by the
candidate method.
Overall, the results suggest that BIC tends to underﬁt badly, almost always selecting models with
an overly low cardinality for the hidden variable; moreover, its score estimate for models of higher
(and more appropriate) cardinality tended to decrease very sharply, making it very unlikely that

916
Chapter 19. Partially Observed Data
1
2
3
4
5
6
7
8
–600
–650
–700
–750
–800
–850
–900
Diagonal
Candidate/CS MAP
Others
CS ML
BIC ML
BIC MAP
1
2
3
4
5
6
7
8
–2,400
–2,500
–2,600
–2,700
–2,800
Candidate
Others
CS ML
BIC ML
BIC MAP
Generating Model
Error in cluster cardinality
n
d
M
Laplace
CS MAP
CS ML
BIC MAP
BIC ML
32
4
400
0
0
0
0
0
64
4
400
0
0
0
0
0
128
4
400
0.2
0.2
0.2
1
1
64
4
400
0
0
0
0
0
64
6
400
0.4
0.4
0.4
0.4
0.4
64
8
400
0.2
0.6
1
1
1
64
4
50
0.2
0
0.4
0.6
0.6
64
4
100
0
0
0.2
0.8
0.6
64
4
200
0
0
0
0
0
Figure 19.G.1 — Evaluation of structure scores for a naive Bayes clustering model In the graphs
in the top row, the x axis denotes diﬀerent cluster cardinalities, and the y axis the marginal likeli-
hood estimated by the method. The graph on the left represents synthetic data with d = 4, n = 128, and
M = 400. The graph on the right represents a real-world data set, with d = 4, n = 35 and M = 47. The
table at bottom shows errors in model selection for the number of values of a hidden variable, as made by
diﬀerent approximations to the marginal likelihood. The errors are computed as diﬀerences between the
cardinality selected by the method and the “optimal” cardinality selected by the “gold standard” candidate
method. The errors are averaged over ﬁve data sets. The blocks of lines correspond to experiments where
one of the three parameters deﬁning the synthetic network varied while the others were held constant.
(Adapted from Chickering and Heckerman (1997), with permission.)
they would be chosen. The other asymptotic approximations were all reasonably good, although
all of them tended to underestimate the marginal likelihood as the number of clusters grows. A
likely reason is that many of the clusters tend to become empty in this setting, giving rise to a
“ridge-shaped” likelihood surface, where many parameters have no bearing on the likelihood. In
this case, the “peak”-shaped estimate of the likelihood used by the asymptotic approximations tends
to underestimate the true value of the integral. Among the diﬀerent asymptotic approximations, the
Cheeseman-Stutz approximation using the MAP conﬁguration of the parameters had a slight edge
over the other methods in its accuracy, and was more robust when dealing with parameters that are
close to 0 or 1. It was also among the most eﬃcient of the methods (other than the highly inaccurate
BIC approach).

19.4. Structure Learning
917
19.4.2
Structure Search
19.4.2.1
A Naive Approach
Given a deﬁnition of the score, we can now consider the structure learning task. In the most
general terms, we want to explore the set of graph structures involving the variables of interest,
score each one of these, and select the highest-scoring one. For some learning problems, such
as the one discussed in box 19.G, the number of structures we consider is relatively small, and
thus we can simply systematically score each structure and ﬁnd the best one.
This strategy, however, is infeasible for most learning problems.
Usually the number of
structures we want to consider is very large — exponential or even superexponential in the
number of variables — and we do not want to score all them. In section 18.4, we discussed
various optimization procedures that can be used to identify a high-scoring structures. As we
showed, for certain types of constraints on the network structure — tree-structured networks
or a given node ordering (and bounded indegree) — we can actually ﬁnd the optimal structure
eﬃciently. In the more general case, we apply a hill-climbing procedure, using search operators
that consist of local network modiﬁcations, such as edge addition, deletion, or reversal.
Unfortunately, the extension of these methods to the case of learning with missing data quickly
hits a wall, since all of these methods relied on the decomposition of the score into a sum of
individual family scores. This requirement is obvious in the case of learning tree-structured
networks and in the case of learning with a ﬁxed ordering: in both cases, the algorithm relied
explicitly on the decomposition of the score as a sum of family scores.
The diﬃculty is a little more subtle in the case of the hill-climbing search. There, in each
iteration, we consider applying O(n2) possible search operators (approximately 1–2 operators
for each possible edge in the network).
This number is generally quite large, so that the
evaluation of the diﬀerent possible steps in the search is a signiﬁcant computational bottleneck.
Although the same issue arises in the complete data case, there we could signiﬁcantly reduce
the computational burden due to two ideas. First, since the score is based on suﬃcient statistics,
we could cache suﬃcient statistics and reuse them. Second, since the score is decomposable,
the change in score of many of the operators is oblivious to modiﬁcations in another part of
the network. Thus, as we showed in section 18.4.3.3, once we compute the delta-score of an
operator o relative to a candidate solution G:
δ(G : o) = score(o(G) : D) −score(G : D),
the same quantity is also the delta-score δ(G′
:
o) for any other G′ that is similar to G in
the local topology that is relevant for o. For example, if o adds X →Y , then the delta-score
remains unchanged for any graph G′ for which the family of Y is the same as in G.
The
decomposition property implied that the search procedure in the case of complete data could
maintain a priority queue of the eﬀect of diﬀerent search operators from previous iterations of
the algorithm and avoid repeated computations.
When learning from incomplete data, the situation is quite diﬀerent. As we discussed, local
changes in the structure can result in global changes in the likelihood function. Thus, after a
local structure change, the parameters of all the CPDs might change. As a consequence, the
score is not decomposable; that is, the delta-score of one local modiﬁcation (for example, adding
an arc) can change after we modify a remote part of the network.

918
Chapter 19. Partially Observed Data
X1
X2
X3
X4
Count
0
0
0
1
2
0
0
1
0
1
0
1
0
0
2
0
1
0
1
1
0
1
1
0
8
0
1
1
1
1
1
0
0
0
3
1
0
0
1
42
1
0
1
0
10
1
0
1
1
11
1
1
0
1
15
1
1
1
0
3
1
1
1
1
1
X2
X1
X4
X3
C
10
X2
X1
X4
X3
C
X2
X1
X4
X3
C
00
11
X2
X1
X4
X3
C
01
(+3, –0.4)
(+10.6, +7.2)
(+24.1, +17.4)
(a)
(b)
Figure 19.10
Nondecomposability of structure scores in the case of missing data. (a) A training set
over variables X1, . . . , X4. (b) Four possible networks over X1, . . . , X4 and a hidden variable C. Arrows
from the top network to the other three are labeled with the change in log-likelihood (LL) and Cheeseman-
Stutz (CS) score, respectively. The baseline score (for G00) is: −336.5 for the LL score, and −360 for the
CS score. We can see that the contribution of adding the arc C →X3 is radically diﬀerent when X4 is
added as a child of C. This example shows that both the log-likelihood and the Cheeseman-Stutz score
are not decomposable.
Example 19.14
Consider a task of learning a network structure for clustering, where we are also trying to determine
whether diﬀerent features are relevant. More precisely, assume we have a hidden variable C, and
four possibly related variables X1, . . . , X4. Assume that we have already decided that both X1 and
X2 are children of C, and are trying to decide which (if any) of the edges C →X3 and C →X4
to include in the model, thereby giving rise to the four possible models in ﬁgure 19.10a. Our training
set is as shown in ﬁgure 19.10b, and the resulting delta-scores relative to the baseline network G00
are shown in (c). As we can see, adding the edge C →X3 to the original structure G00 leads
only to a small improvement in the likelihood, and a slight decrease in the Cheeseman-Stutz score.
However, adding the edge C →X3 to the structure G01, where we also have C →X4, leads to a
substantial improvement.
This example demonstrates a situation where the score is not decomposable. The intuition here is
simple. In the structure G00, the hidden variable C is “tuned” to capture the dependency between
X1 and X2. In this network structure, there is a weak dependency between these two variables
and X3. In G10, the hidden variable has more or less the same role, and therefore there is little
explanatory beneﬁt for X3 in adding the edge to the hidden variable. However, when we add X3
and X4 together, the hidden variable shifts to capture the strong dependency between X3 and
X4 while still capturing some of the dependencies between X1 and X2. Thus, the score improves

19.4. Structure Learning
919
dramatically, and in a nonadditive manner.
As a consequence of these problems, a search procedure that uses one of the scores we
discussed has to evaluate the score of each candidate structure it considers, and it cannot rely
on cached computations. In all of the scores we considered, this evaluation involves nontrivial
computations (for example, running EM or an MCMC procedure) that are much more expensive
than the cost of scoring a structure in the case of complete data. The actual cost of computation
in these steps depends on the network structure (that is, the cost of inference in the network)
and the number of iterations to convergence. Even in simple networks (for example, ones with a
single hidden variable) this computation is an order of magnitude longer than evaluation of the
score in complete data.
The main problem is that, in this type of search, most of the computation results are discarded.
To understand why, recall that to select a move o from our current graph G, we ﬁrst evaluate
all candidate successors o(G). To evaluate each candidate structure o(G), we compute the MLE
or MAP parameters for o(G), score it, and then compare it to the score of other candidates we
consider at this point. Since we select to apply only one of the proposed search operators o at
each iteration, the parameters learned for other structures o′(G) are not needed. In practice,
search using the modify-score-discard strategy is rarely feasible; it is useful only for learning
in small domains, or when we have many constraints on the network structure, and so do not
have many choices at each decision point.
19.4.2.2
Heuristic Solutions
There are several heuristics for avoiding this signiﬁcant computational cost. We list a few of
them here. We note that nondecomposability is a major issue in the context of Markov network
learning, and so we return to these ideas in much greater length in section 20.7.
One approach is to construct “quick and dirty” estimates of the change in score. We can
employ such estimates in a variety of ways.
In one approach, we can simply use them as
our estimates of the delta-score in any of the search algorithms used earlier. Alternatively, we
can use them as a pruning mechanism, focusing our attention on the moves whose estimated
change in score is highest and evaluating them more carefully. This approach uses the estimates
to prioritize our computational resources and invest computation on careful evaluation of the
real change in score for fewer modiﬁcations.
There are a variety of diﬀerent approaches we can use to estimate the change in score.
One approach is to use computations of delta-scores acquired in previous steps. More precisely,
suppose we are at a structure G0, and evaluate a search operator whose delta-score is δ(G0 : o).
We can then assume for the next rounds of search that the delta-score for this operator has not
changed, even though the network structure has changed. This approach allows us to cache
the results computation for at least some number of subsequent iterations (as though we are
learning from complete data). In eﬀect, this approach approximates the score as decomposable,
at least for the duration of a few iterations. Clearly, this approach is only an approximation,
but one that may be quite reasonable: even if the delta-scores themselves change, it is not
unreasonable to assume that a step that was good relative to one structure is often probably
good also relative to a closely related structure. Of course, this assumption can also break down:
as the score is not decomposable, applying a set of “beneﬁcial” search operators together can
lead to structures with worse score.

920
Chapter 19. Partially Observed Data
The implementation of such a scheme requires us to make various decisions. How long do
we maintain the estimate δ(G : o)? Which other search operators invalidate this estimate after
they are applied? There is no clear right answer here, and the actual details of implementations
of this heuristic approach diﬀer on these counts.
Another approach is to compute the score of the modiﬁed network, but assume that only the
parameters of the changed CPD can be optimized. That is, we freeze all of the parameters except
those of the single CPD P(X | U) whose family composition has changed, and optimize the
parameters of P(X | U) using gradient ascent or EM. When we optimize only the parameters
of a single CPD, EM or gradient ascent should be faster for two reasons. First, because we have
only a few parameters to learn, the convergence is faster. Second, because we modify only the
parameters of a single CPD, we can cache intermediate computations; see exercise 19.15.
The set of parameterizations where only the CPD P(X | U) is allowed to change is a subset
of the set of all possible parameterizations for our network. Hence, any likelihood that we can
achieve in this case would also be achievable if we ran a full optimization. As a consequence,
the estimate of the likelihood in the modiﬁed network is a lower bound of the actual likelihood
we can achieve if we can optimize all the parameters. If we are using a score such as the BIC
score, this estimate is a proven lower bound on the score. If we are using a score such as the
Cheeseman-Stutz score, this argument is not valid, but appears generally to hold in practice.
That is, the score of the network with frozen parameters will be usually either smaller or very
close to that of the one were we can optimize all parameters.
More generally, if a heuristic estimate is a proven lower bound or upper bound, we can
improve the search procedure in a way that is guaranteed not to lose the optimal candidates. In
the case of a lower bound, an estimated value that is higher than moves that we have already
evaluated allows us to prune those other moves as guaranteed to be suboptimal. Conversely,
if we have a move with a guaranteed upper bound that is lower than previously evaluated
candidates, we can safely eliminate it. In practice, however, such bounds are hard to come by.
19.4.3
Structural EM
We now consider a diﬀerent approach to constructing a heuristic that identiﬁes helpful moves
during the search. This approach shares some of the ideas that we discussed. However, by
putting them together in a particular way, it provides signiﬁcant computational savings, as well
as certain guarantees.
19.4.3.1
Approximating the Delta-score
One eﬃcient approach for approximating the score of network is to construct some complete
data set D∗, and then apply a score based on the complete data set. This was precisely the
intuition that motivated the Cheeseman-Stutz approximation. However, the Cheeseman-Stutz
approximation is computationally expensive. The data set we use — D∗
G,˜θG — is constructed
by ﬁnding the MAP parameters for our current candidate G. We also needed to introduce a
correction term that would improve the approximation to the marginal likelihood; this correction
term required that we run inference over G. Because these steps must be executed for each
candidate network, this approach quickly becomes infeasible for large search spaces.
However, what if we do not want to obtain an accurate approximation to the marginal

19.4. Structure Learning
921
likelihood? Rather, we want only a heuristic that would help identify useful moves in the space.
In this case, one simple heuristic is to construct a single completed data set D∗and use it to
completed data
evaluate multiple diﬀerent search steps. That is, to evaluate a search operator o, we deﬁne
ˆδD∗(G : o) = score(o(G) : D∗) −score(G : D∗),
where we can use any complete-data scoring function for the two terms on the right-hand side.
The key observation is that this expression is simply a delta-score relative to a complete data
set, and it can therefore be evaluated very eﬃciently. We will return to this point.
Clearly, the results of applying this heuristic depend on our choice of completed data set
D∗, an observation that immediately raises some important questions: How do we deﬁne our
completed data set D∗? Can we provide any guarantees on the accuracy of this heuristic? One
compelling answer to these questions is obtained from the following result:
Theorem 19.10
Let G0 be a graph structure and ˜θ0 be the MAP parameters for G0 given a data set D. Then for any
graph structure G:
scoreBIC(G : D∗
G0,˜θ0) −scoreBIC(G0 : D∗
G0,˜θ0) ≤scoreBIC(G : D) −scoreBIC(G0 : D).
This theorem states that the true improvement in the BIC score of network G, relative to the
network G0 that we used to construct our completed data D∗
G0,˜θ0, is at least as large as the
estimated improvement of the score using the completed data D∗
G0,˜θ0.
The proof of this theorem is essentially the same as the proof of theorem 19.5; see ex-
ercise 19.25.
Although the analogous result for the Bayesian score is not true (due to the
nonlinearity of the Γ function used in the score), it is approximately true, especially when we
have a reasonably large sample size; thus, we often apply the same ideas in the context of the
Bayesian score, albeit without the same level of theoretical guarantees.
This result suggests the following scheme. Consider a graph structure G0. We compute its
MAP parameters ˜θ0, and construct a complete (fractional) data set D∗
G0,˜θ0. We can now use the
BIC score relative to this completed data set to evaluate the delta-score for any modiﬁcation o
to G. We can thus deﬁne
ˆδD∗
G0,˜θ0(G : o) = scoreBIC(o(G) : D∗
G0,˜θ0) −scoreBIC(G : D∗
G0,˜θ0).
The theorem guarantees that our heuristic estimate for the delta-score is a lower bound on
the true change in the BIC score. The fact that this estimate is a lower bound is signiﬁcant,
since it guarantees that any change that we make that improves the estimated score will also
improve the true score.
19.4.3.2
The Structural EM Algorithm
Importantly, the preceding guarantee holds not just for the application of a single operator, but
also for any series of changes that modify G0. Thus, we can use our completed data set D∗
G0,˜θ0
to estimate and apply an arbitrarily long sequence of operators to G0; as long as we have that
scoreBIC(G : D∗
G0,˜θ0) > scoreBIC(G0 : D∗
G0,˜θ0)
for our new graph G, we are guaranteed that the true score of G is also better.

922
Chapter 19. Partially Observed Data
Algorithm 19.3 The structural EM algorithm for structure learning
Procedure Structural-EM (
G0,
// Initial bayesian network structure over X1, . . . , Xn
θ0,
// Initial set of parameters for G0
D
// Partially observed data set
)
1
for each t = 0, 1 . . . , until convergence
2
// Optional parameter learning step
3
θt′ ←Expectation-Maximization(Gt, θt, D)
4
// Run EM to generate expected suﬃcient statistics for D∗
Gt,θt′
5
Gt+1 ←Structure-Learn(D∗
Gt,θt′)
6
θt+1 ←Estimate-Parameters(D∗
Gt,θt′, Gt+1)
7
return Gt, θt
However, we must take care in interpreting this guarantee. Assume that we have already
modiﬁed G0 in several ways, to obtain a new graph G. Now, we are considering a new operator
o, and are interested in determining whether that operator is an improvement; that is, we wish to
estimate the delta-score: scoreBIC(o(G) : D) −scoreBIC(G : D). The theorem tells us that
if o(G) satisﬁes scoreBIC(o(G) : D∗
G0,˜θ0) > scoreBIC(G0 : D∗
G0,˜θ0), then it is necessarily
better than our original graph G0. However, it does not follow that if ˆδD∗
G0,˜θ0(G
:
o) > 0,
then o(G) is necessarily better than G. In other words, we can verify that each of the graphs
we construct improves over the graph used to construct the completed data set, but not that
each operator improves over the previous graph in the sequence. Note that we are guaranteed
that our estimate is a true lower bound for any operator applied directly to G0. Intuitively, we
believe that our estimates are likely to be reasonable for graphs that are “similar” to G0. (This
intuition was also the basis for some of the heuristics described in section 19.4.2.2.) However,
as we move farther away, our estimates are likely to degrade. Thus, at some point during our
search, we probably want to select a new graph and construct a more relevant complete data
set.
This observation suggests an EM-like algorithm, called structural EM, shown in algorithm 19.3.
structural EM
In structural EM, we iterate over a pair of steps.
In the E-step, we use our current model
to generate (perhaps implicitly) a completed data set, based on which we compute expected
suﬃcient statistics. In the M-step, we use these expected suﬃcient statistics to improve our
model. The biggest diﬀerence is that now our M-step can improve not only the parameters,
but also the structure. (Note that the structure-learning step also reestimates the parameters.)
The structure learning procedure in the M-step can be any of the procedures we discussed
in section 18.4, whether a general-purpose heuristic search or an exact search procedure for a
specialized subset of networks for which we have an exact solution (for example, a maximum
weighted spanning tree procedure for learning trees). If we use the BIC score, theorem 19.10
guarantees that, if this search procedure ﬁnds a structure that is better than the one we used in
the previous iteration, then the structural EM procedure will monotonically improve the score.

19.4. Structure Learning
923
Since the scores are upper-bounded, the algorithm must converge. Unlike the case of EM, we
cannot, however, prove that the structure it ﬁnds is a local maximum.
19.4.3.3
Structural EM for Selective Clustering
We now illustrate the structural EM algorithm on a particular class of networks. Consider the
task of learning structures that generalize our example of example 19.14; these networks are
similar to the naive Bayes clustering of section 19.2.2.4, except that some observed variables may
be independent of the cluster variable. Thus, in our structure, the class variable C is a root,
and each observed attribute Xi is either a child of C or a root by itself. This limited set of
structures contains 2n choices.
Before discussing how to learn these structures using the ideas we just explored, let us
consider why this problem is an interesting one. One might claim that, instead of structure
learning, we can simply run parameter learning within the full structure (where each Xi is
a child of C); after all, if Xi is independent of C, then we can capture this independence
within the parameters of the CPD P(Xi | C). However, as we discussed, statistical noise in
the sampling process guarantees that we will never have true independence in the empirical
distribution. Learning a more restricted model with fewer edges is likely to result in more robust
clustering. Moreover, this approach allows us to detect irrelevant attributes during the clustering,
providing insight into the domain.
If we have a complete data set, learning in this class of models is trivial. Since this class of
structures is such that we cannot have cycles, if the score is decomposable, the choice of family
for Xi does not impact the choice of parents for Xj. Thus, we can simply select the optimal
family for each Xi separately: either C is its only parent, or it has no parents. We can thus
select the optimal structure using 2n local score evaluations.
The structural EM algorithm applies very well in this setting. We initialize each iteration with
our current structure Gt. We then perform the following steps:
•
Run parameter estimation (such as EM or gradient ascent) to learn parameters ˜θt for Gt.
•
Construct a new structure Gt+1 so that Gt+1 contains the edge C →Xi if
FamScore(Xi | {C} : D∗
Gt,˜θt) > FamScore(Xi | ∅: D∗
Gt,˜θt).
We continue this procedure until convergence, that is, until an iteration that makes no changes
to the structure.
According to theorem 19.10, if we use the BIC score in this procedure, then any improvement
to our expected score based on D∗
Gt,˜θt is guaranteed to give rise to an improvement in the true
BIC score; that is, scoreBIC(Gt+1
:
D) ≥scoreBIC(Gt
:
D). Thus, each iteration (until
convergence) improves the score of the model.
One issue in implementing this procedure is how to evaluate the family scores in each
iteration: FamScore(Xi | ∅
:
D∗
Gt,˜θt) and FamScore(Xi | {C}
:
D∗
Gt,˜θt). The ﬁrst term
depends on suﬃcient statistics for Xi in the data set; as Xi is fully observed, these can be
collected once and reused in each iteration. The second term requires suﬃcient statistics of Xi

924
Chapter 19. Partially Observed Data
and C in D∗
Gt,˜θt; here, we need to compute:
¯
MD∗
Gt,˜θt[xi, c]
=
X
m
P(C[m] = c, Xi[m] = xi | o[m], Gt, ˜θt)
=
X
m,Xi[m]=xi
P(C[m] = c | o[m], Gt, ˜θt).
We can collect all of these statistics with a singe pass over the data, where we compute the
posterior over C in each instance. Note that these are the statistics we need for parameter
learning in the full naive Bayes network, where each Xi is connected to C.
In some of
the iterations of the algorithm, we will compute these statistics even though Xi and C are
independent in Gt.
Somewhat surprisingly, even when the joint counts of Xi and C are
obtained from a model where these two variables are independent, the expected counts can
show a dependency between them; see exercise 19.26.
Note that this algorithm can take very large steps in the space. Speciﬁcally, the choice of edges
in each iteration is made from scratch, independently of the choice in the previous structure;
thus, Gt+1 can be quite diﬀerent from Gt. Of course, this observation is true only up to a point,
since the use of the distribution based on (Gt, ˜θt) does bias the reconstruction to favor some
aspects of the previous iteration. This point goes back to the inherent nondecomposability of
the score in this case, which we saw in example 19.14. To understand the limitation, consider
the convergence point of EM for a particular graph structure where C has a particular set
of children X. At this point, the learned model is optimized so that C captures (as much
as possible) the dependencies between its children in X, to allow the variables in X to be
conditionally independent given C. Thus, diﬀerent choices of X will give rise to very diﬀerent
models. When we change the set of children, we change the information that C represents, and
thus change the score in a global way. As a consequence, the choice of Gt that we used to
construct the completed data does aﬀect our ability to add certain edges into the graph.
This issue brings up the important question of how we can initialize this search procedure.
A simple initialization point is to use the full network, which is essentially the naive Bayes
clustering network, and let the search procedure prune edges. An alternative is to start with
a random subset of edges. Such a randomized starting point can allow us to discover “local
maxima” that are not accessible from the full network. One might also tempted to use the empty
network as a starting point, and then consider adding edges. It is not hard to show, however,
that the empty network is a bad starting point: structural EM will never add a new edge if we
initialize the algorithm with the empty network; see exercise 19.27.
19.4.3.4
An Eﬀective Implementation of Structural EM
Our description of the structural EM procedure is at a somewhat abstract level, and it lends
itself to diﬀerent types of implementations. The big unresolved issue in this description is how
to represent and manage the completed data set created in the E-step. Recall that the number of
completions of each instance is exponential in the number of missing values in that instance. If
we have a single hidden variable, as in the selective naive Bayes classiﬁer of section 19.4.3.3, then
storing all completions (and their relative weights) might be a feasible implementation. However,
if we have several unobserved variables in each instance, then this solution rapidly becomes
impractical.

19.5. Learning Models with Hidden Variables
925
We can, however, exploit the fact that procedures that learn from complete data sets do not
need to access all the instances; they require only suﬃcient statistics computed from the data
set. Thus, we do not need to maintain all the instances of the completed data set; we need only
to compute the relevant suﬃcient statistics in the completed data set. These suﬃcient statistics
are, by deﬁnition, the expected suﬃcient statistics based on the current model (Gt, θt) and the
observed data. This is precisely the same idea that we utilized in the E-step of standard EM for
parameter estimation. However, there is one big diﬀerence. In parameter estimation, we know
in advance the suﬃcient statistics we need. When we perform structure learning, this is no
longer true. When we change the structure, we need a new set of suﬃcient statistics for the
parts of the model we have changed. For example, if in the original network X is a root, then,
for parameter estimation, we need only suﬃcient statistics of X alone. Now, if we consider
adding Y as a parent of X, we need the joint statistics of X and Y together. If we do add
the edge Y →X, and now consider Z as an additional parent of X, we now need the joint
statistics of X, Y , and Z.
This suggests that the number of suﬃcient statistics we may need can be quite large. One
strategy is to compute in advance the set of suﬃcient statistics we might need. For specialized
classes of structures, we may know this set exactly. For example, in the clustering scenario
that we examined in section 19.4.3.3, we know the precise suﬃcient statistics that are needed
for the M-step. Similarly, if we restrict ourselves to trees, we know that we are interested only
in pairwise statistics and can collect all of them in advance. If we are willing to assume that
our network has a bounded indegree of at most k, then we can also decide to precompute all
suﬃcient statistics involving k + 1 or fewer variables; this approach, however, can be expensive
for k greater than two or three. An alternative strategy is to compute suﬃcient statistics “on
demand” as the search progresses through the space of diﬀerent structures.
This approach
allows us to compute only the suﬃcient statistics that the search procedure requires. However, it
requires that we revisit the data and perform new inference queries on the instances; moreover,
this inference generally involves variables that are not together in a family and therefore may
require out-of-clique inference, such as the one described in section 10.3.3.2.
Importantly, however, once we compute suﬃcient statistics, all of the decomposability prop-
erties for complete data that we discussed in section 18.4.3.3 hold for the resulting delta-scores.
Thus, we can apply our caching-based optimizations in this setting, greatly increasing the
computational eﬃciency of the algorithm. This property is key to allowing the structural EM
algorithm to scale up to large domains with many variables.
19.5
Learning Models with Hidden Variables
In the previous section we examined searching for structures when the data are incomplete. In
that discussion, we conﬁned ourselves to structures involving a given set of variables. Although
this set can include hidden variables, we implicitly assumed that we knew of the existence of
these variables, and could simply treat them as an extreme case of missing data. Of course,
it is important to remember that hidden variables introduce important subtleties, such as our
inability to identify the model. Nevertheless, as we discussed, in section 16.4.2, hidden variables
are useful for a variety of reasons.
In some cases, prior knowledge may tell us that a hidden variable belongs in the model, and

926
Chapter 19. Partially Observed Data
perhaps even where we should place it relative to the other variables. In other cases (such as
the naive Bayes clustering), the placement of the hidden variable is dictated by the goals of
our learning (clustering of the instances into coherent groups). In still other cases, however, we
may want to infer automatically that it would be beneﬁcial to introduce a hidden variable into
the model. This opportunity raises a whole range of new questions: When should we consider
introducing a hidden variable? Where in the network should we connect it? How many values
should we allow it to have?
In this section, we ﬁrst present some results that provide intuition regarding the role that a
hidden variable can play in the model. We then describe a few heuristics for dealing with some
of the computational questions described before.
19.5.1
Information Content of Hidden Variables
One can view the role of a hidden variable as a mechanism for capturing information about
the interaction between other variables in the network.
In our example of the network of
ﬁgure 16.1, we saw that the hidden variable “conveyed” information from the parents X1, X2, X3
to the children Y1, Y2, Y3. Similarly, in the naive Bayes clustering network of ﬁgure 3.2, the
hidden variable captures information between its children.
These examples suggest that, in
learning a model for the hidden variable, we want to maximize the information that the hidden
information
content
variable captures about its children. We now show that learning indeed maximizes a notion of
information between the hidden variable and its children. We analyze a speciﬁc example, the
naive Bayes network of ﬁgure 3.2, but the ideas can be generalized to other network structures.
Suppose we observe M samples of X1, . . . , Xn and use maximum likelihood to learn the
parameters of the network.
Any choice of parameter set θ deﬁnes a distribution ˆQθ over
X1, . . . , Xn, H so that
ˆQθ(h, x1, . . . , xn) = ˆPD(x1, . . . , xn)P(h | x1, . . . , xn, θ),
(19.14)
where ˆPD is the empirical distribution of the observed variables in the data. This is essentially
the augmentation of the empirical distribution by our stochastic “reconstruction” of the hidden
variable.
Consider for a moment a complete data set ⟨D, H⟩, where H is also observed. Proposition 18.1
shows that
max
θ
1
M ℓ(θ : ⟨D, H⟩) =
X
i
II ˆ
P⟨D,H⟩(Xi; H) −IH ˆ
P⟨D,H⟩(H) −
X
i
IH ˆ
P⟨D,H⟩(Xi).
(19.15)
We now show that a similar relationship holds in the case of incomplete data; in fact, this
relationship holds not only at the maximum likelihood point but also in other points of the
parameter space:
Proposition 19.2
Let D be a data set where X1, . . . , Xn are observed and θ0 be a choice of parameters for the
network of figure 3.2. Deﬁne θ1 to be the result of an EM-iteration if we start with θ0 (that is,
the result of an M-step if we use suﬃcient statistics from ˆQθ0). Then
1
M ℓ(θ0 : D) ≤
X
i
II ˆ
Qθ0(Xi; H) −
X
i
IH ˆ
PD(Xi) ≤1
M ℓ(θ1 : D).
(19.16)

19.5. Learning Models with Hidden Variables
927
Roughly speaking, this result states that the information-theoretic term is approximately equal
to the likelihood. When θ0 is a local maxima of the likelihood, we have that θ1 = θ0, and so
we have equality in the left-hand and right-hand sides of equation (19.16). For other parameter
choices, the information-theoretic term can be larger than the likelihood, but not by “too much,”
since it is bounded above by the next iteration of EM. Both the likelihood and the information-
theoretic term have the same maxima.
Because the entropy terms IH ˆ
PD(Xi) do not depend on θ, this result implies that maximizing
the likelihood is equivalent to ﬁnding a hidden variable H that maximizes the information
about each of the observed variables. Note that the information here is deﬁned in terms of
the distribution ˆQθ0, as in equation (19.14). This information measures what H conveys about
each of the observed variables in the posterior distribution given the observations. This is quite
intuitive: For example, assume we learn a model, and after observing x1, . . . , xn, our posterior
over H has H = h1 with high probability. In this case, we are fairly sure about the cluster
assignment of the cluster, so if the clustering is informative, we can conclude quite a bit of
information about the value of each of the attributes.
Finally, it is useful to compare this result to the complete data case of equation (19.15); there,
we had an additional −IH(H) term, which accounts for the observations of H. In the case
of incomplete data, we do not observe H and thus do not need to account for it. Intuitively,
since we sum over all the possible values of H, we are not penalized for more complex (higher
entropy) hidden variables. This diﬀerence also shows that adding more values to the hidden
variable will always improve the likelihood. As we add more values, the hidden variable can
only become more informative about the observed variables. Since our likelihood function does
not include a penalty term for the entropy of H, this score does not penalize for the increased
number of values of H.
We now turn to the proof of this proposition.
Proof Deﬁne Q(H) = P(H | D, θ0), then, by corollary 19.1 we have that
ℓ(θ0 : D) = IEQ

ℓ(θ0 : ⟨D, H⟩)

+ IHQ(H).
Moreover, if θ1 = arg maxθ IEQ[ℓ(θ : ⟨D, H⟩)], then
IEQ

ℓ(θ0 : ⟨D, H⟩)

≤IEQ

ℓ(θ1 : ⟨D, H⟩)

.
Finally, we can use corollary 19.1 again and get that
IEQ

ℓ(θ1 : ⟨D, H⟩)

+ IHQ(H) ≤ℓ(θ1 : D).
Combining these three inequalities, we conclude that
ℓ(θ0 : D) ≤IEQ

ℓ(θ1 : ⟨D, H⟩)

+ IHQ(H) ≤ℓ(θ1 : D).
Since θ1 maximize the expected log-likelihood, we can apply equation (19.15) for the completed
data set ⟨D, H⟩, and conclude that
IEQ

ℓ(θ1 : ⟨D, H⟩)

= M
"X
i
IEQ
h
II ˆ
P⟨D,H⟩(Xi; H)
i
−IEQ
h
IH ˆ
P⟨D,H⟩(H)
i
−
X
i
IH ˆ
PD(Xi)
#
.

928
Chapter 19. Partially Observed Data
Using basic rewriting, we have that
MIEQ
h
IH ˆ
P⟨D,H⟩(H)
i
= IHQ(H)
and that
IEQ
h
II ˆ
P⟨D,H⟩(Xi; H)
i
= II ˆ
Qθ0 (Xi; H),
which proves the result.
19.5.2
Determining the Cardinality
One of the key questions that we need to address for a hidden variable is that of its cardinality.
19.5.2.1
Model Selection for Cardinality
The simplest approach is to use model selection, where we consider a number of diﬀerent cardi-
model selection
nalities for H, and then select the best one. For our evaluation criterion, we can use a Bayesian
technique, utilizing one of the approximate scores presented in section 19.4.1; box 19.G provides
a comparative study of the diﬀerent scores in precisely this setting. As another alternative, we
can measure test generalization performance on a holdout set or using cross-validation. Both
of these methods are quite expensive, even for a single hidden variable, since they both require
that we learn a full model for each of the diﬀerent cardinalities that we are considering; for
multiple hidden variables, it is generally intractable.
A cheaper approach is to consider a more focused problem of using H to represent a clustering
problem, where we cluster instances based only on the features X in H’s (putative) Markov
blanket. Here, the assumption is that if we give H enough expressive power to capture the
distinctions between diﬀerent classes of instances, we have captured much of the information in
X. We can now use any clustering algorithm to construct diﬀerent clusterings and to evaluate
their explanatory power. Commonly used variants are EM with a naive Bayes model, the simpler
k-means algorithm, or any other of many existing clustering algorithms. We can now evaluate
diﬀerent cardinalities for H at much lower cost, using a score that measures only the quality
of the local clustering. An even simpler approach is to introduce H with a low cardinality
(say binary-valued), and then use subsequent learning stages to tell us whether there is still
information in the vicinity of H. If there is, we can either increase the cardinality of H, or add
another hidden variable.
19.5.2.2
Dirichlet Processes
A very diﬀerent alternative is a Bayesian model averaging approach, where we do not select a
Bayesian model
averaging
cardinality, but rather average over diﬀerent possible cardinalities. Here, we use a prior over
the set of possible cardinalities of the hidden variable, and use the data to deﬁne a posterior.
The Bayesian model averaging approach allows us to circumvent the diﬃcult question of

selecting the cardinality of the hidden variable. On the other side, because it fails to
make a deﬁnitive decision on the set of clusters and on the assignment of instances to
clusters, the results of the algorithm may be harder to interpret. Moreover, techniques

19.5. Learning Models with Hidden Variables
929
that use Bayesian model averaging are generally computationally even more expensive
than approaches that use model selection.
One particularly elegant solution is provided by the Dirichlet process approach. We provide
Dirichlet process
an intuitive, bottom-up derivation for this approach, which also has extensive mathematical
foundations; see section 19.7 for some references.
To understand the basic idea, consider what happens if we apply the approach of example 19.12
but allow the number of possible clusters K to grow very large, much larger than the number of
data instances. In this case, the bound K does not limit the expressive power of model, since
we can (in principle) put each instance in its own cluster.
Our natural concern about this solution is the possibility of overﬁtting: after all, we certainly
do not want to put each point in its own cluster. However, recall that we are using a Bayesian
approach and not maximum likelihood. To understand the diﬀerence, consider the posterior
distribution over the (M + 1)’st instance given a cluster assignment for the previous instances
1, . . . , M. This formula is also proportional to equation (19.10), with M + 1 playing the role
of m′. (The normalizing constant is diﬀerent, because here we are also interested in modeling
the distribution over X[M + 1], whereas there we took x[m′] as given.) The ﬁrst term in the
equation, |Ik(c)| + α0/K, captures the relative prior probability that the (M + 1)’st instance
selects to join cluster k. Note that the more instances are in the k’th cluster, the higher the
probability that the new instance will select to join. Thus, the Dirichlet prior naturally causes
instances to prefer to cluster together and thereby helps avoid overﬁtting.
A second concern is the computational burden of maintaining a very large number of clusters.
Recall that if we use the collapsed Gibbs sampling approach of example 19.12, the cost per
sampling step grows linearly with K. Moreover, most of this computation seems like a waste:
with such a large K, many of the clusters are likely to remain empty, so why should we waste
our time considering them?
The solution is to abstract the notion of a cluster assignment. Because clusters are completely
symmetrical, we do not care about the speciﬁc assignment to the variables C[m], but only about
the partition of the instances into groups. Moreover, we can collapse all of the empty partitions,
treating them all as equivalent. We therefore deﬁne a particle σ in our collapsed Gibbs process to
encode a partition of the the data instances {1, . . . , M}: an unordered set of non-empty subsets
partition
{I1, . . . , Il}. Each I ∈σ is associated with a distribution over the parameters Θσ = {θI}I∈σ
and over the multinomial θ. As usual, we deﬁne σ−m′ to denote the partition induced when
we remove the instance m′.
To deﬁne the sampling process, let C[m′] be the variable to be sampled.
Let L be the
number of (non-empty) clusters in the partition σ−m′. Introducing C[m′] (while keeping the
other instances ﬁxed) induces L + 1 possible partitions: joining one of the L existing clusters,
or opening a new one. We can compute the conditional probabilities of each of these outcomes.
Let I ∈σ.
P(I ←I ∪{m′} | σ−m′, D, ω)
∝

|I| + α0
K

Q(x[m′] | DI, ω)
(19.17)
P(σ ←σ ∪{{m′}} | σ−m′, D, ω)
∝
(K −L)α0
K Q(x[m′] | ω),
(19.18)
where the ﬁrst line denotes the event where m′ joins an existing cluster I, and the second the
event where it forms a new singleton cluster (containing only m′) that is added to σ. To compute
these transition probabilities, we needed to sum over all possible concrete cluster assignments

930
Chapter 19. Partially Observed Data
that are consistent with σ, but this computation is greatly facilitated by the symmetry of our prior
(see exercise 19.29). Using abstract partitions as our particles provides signiﬁcant computational
savings: we need only to compute L + 1 values for computing the transition distribution, rather
than K, reducing the complexity of each Gibbs iteration to O(NL), independent of the number
of classes K.
As long as K is larger than the amount of data, it appears to play no real role in the model.
Therefore, a more elegant approach is to remove it, allowing the number of clusters to grow to
inﬁnity. At the limit, the sampling equation for σ is now even simpler:
P(I ←I ∪{m′} | σ−m′, D, ω)
∝
|I| · Q(x[m′] | DI, ω)
(19.19)
P(σ ←σ ∪{{m′}} | σ−m′, D, ω)
∝
α0 · Q(x[m′] | ω).
(19.20)
This scheme removes the bound on the number of clusters and induces a prior that allows
any possible partition of the samples. Given the data, we obtain a posterior over the space of
possible partitions. This posterior gives positive probability to partitions with diﬀerent numbers
of clusters, thereby averaging over models with diﬀerent complexity. In general, the number of
clusters tends to grow logarithmically with the size of the data. This type of model is called a
nonparametric Bayesian model; see also box 17.B.
nonparametric
Bayesian
estimation
Of course, with K at inﬁnity, our Dirichlet prior over θ is not a legal prior. Fortunately, it turns
out that one can deﬁne a generalization of the Dirichlet prior that induces these conditional
probabilities.
One simple derivation comes from a sampling process known as the Chinese
Chinese
restaurant
process
restaurant process. This process generates a random partition as follows: The guests (instances)
enter a restaurant one by one, and each guest chooses between joining one of the non-empty
tables (clusters) and opening a new table. The probability that a customer chooses to join a
table l at which nl customers are already sitting is ∝nl; the probability that he opens a new
tables is ∝α0. The instances assigned to the same table all use the parameters of that table.
It is not hard to show (exercise 19.30) that this prior induces precisely the update equations in
equation (19.19) and equation (19.20).
A second derivation is called the stick-breaking prior; it is parameterized by α0, and deﬁnes
stick-breaking
prior
an inﬁnite sequence of random variables βi ∼Beta(1, α0). We can now deﬁne an inﬁnite-
dimensional vector deﬁned as:
λk = βk
k−1
Y
l=1
(1 −βl).
This prior is called a stick-breaking prior because it can be viewed as deﬁning a process of
breaking a stick into pieces: We ﬁrst break a piece of fraction β1, then the second piece is
a fraction β2 of the remainder, and so on.
It is not diﬃcult to see that P
k λk = 1.
It
is also possible to show that, under the appropriate deﬁnitions, the limit of the distributions
Dirichlet(α0/K, . . . , α0/K) as K −→∞induces the stick-breaking prior.
19.5.3
Introducing Hidden Variables
Finally, we consider the question of determining when and where to introduce a hidden variable.
The analysis of section 19.5.1 tells us that a hidden variable in a naive Bayes clustering network
is optimized to capture information about the variables to which it is connected. Intuitively,

19.5. Learning Models with Hidden Variables
931
H3
H1
X5
X4
X6
X3
H2
X2
X1
H4
X8
X7
X9
Figure 19.11
An example of a network with a hierarchy of hidden variables
H2
X5
X4
X6
X3
H1
X2
X1
H3
X8
X7
X9
Figure 19.12
An example of a network with overlapping hidden variables
this requirement imposes a signiﬁcant bias on the parameterization of the hidden variable. This
bias helps constrain our search and allows us to learn a hidden variable that plays a meaningful
role in the model. Conversely, if we place a hidden variable where the search is not similarly
constrained, we run the risk of learning hidden variables that are meaningless, and that only
capture the noise in the training data. As a rough rule of thumb, we want the model with the
hidden variables to have a lot fewer independent parameters than the number of degrees of
freedom of the empirical distribution.
Thus, when selecting network topologies involving hidden variables, we must exercise care.
One useful example of such a class of topologies are organized hierarchically (for example,
hierarchical
organization
ﬁgure 19.11), where the hidden variables form a treelike hierarchy. Since each hidden variable
is a parent of several other variables (either observed or hidden), it serves to mediate the de-
pendencies among its children and between these children and other variables in the network
(through its parent). This constraint implies that the hidden variable can improve the likelihood
by capturing such dependencies when they exist. The general topology leaves much freedom
in determining what is the best hierarchy structure. Intuitively, distance in the hierarchy should
roughly correspond to the degree of dependencies between the variables, so that strongly de-
pendent variables would be closer in the hierarchy. This rule is not exact, of course, since the
nature of the dependencies inﬂuences whether the hidden variable can capture them.
Another useful class of networks are those with overlapping hidden variables; see ﬁgure 19.12.
overlapping
organization
In this network, each hidden variable is the parent of several observed variables. The justiﬁcation
is that each hidden variable captures aspects of the instance that several of the observed variables
depend on. Such a topology encodes multiple “ﬂavors” of dependencies between the diﬀerent
variables, breaking up the dependency between them as some combination of independent
axes.
This approach often provides useful information about the structure of the domain.
However, once we have an observed variable depending on multiple hidden ones, we might

932
Chapter 19. Partially Observed Data
need to introduce many parameters, or restrict attention to some compact parameterization of
the CPDs. Moreover, while the tree structure of hierarchical networks ensures eﬃcient inference,
overlapping hidden variables can result in a highly intractable structure.
In both of these approaches, as in others, we need to determine the placements of the hidden
variables. As we discussed, once we introduce a hidden variable somewhere within our structure
and localize it correctly in the model by connecting it to its correct neighbors, we can estimate
parameters for it using EM. Even if we locate it approximately correctly, we can use the structural
EM algorithm to adapt both the structure and the parameters.

However, we cannot simply place a hidden variable arbitrarily in the model and expect
our learning procedures to learn a reasonable model. Since these methods are based on
iterative improvements, running structural EM with a bad initialization usually leads either to a
trivial structure (where the hidden variable has few neighbors or disconnected from the rest of
the variables) or to a structure that is very similar to the initial network structure. One extreme
example of a bad initialization is to introduce a hidden variable that is disconnected from the
rest of the variables; here, we can show that the variable will never be connected to the rest of
the model (see exercise 19.27).
This discussion raises the important question of how to induce the existence of a hidden
variable, and how to assign it a putative position within the network. One approach is based
on ﬁnding “signatures” that the hidden variable might leave. As we discussed, a hidden variable
captures dependencies between the variables to which it is connected. Indeed, if we assume
that a hidden variable truly exists in the underlying distribution, we expect its neighbors in the
graph to be dependent. For example, in ﬁgure 16.1, marginalizing the hidden variable induces
correlations among its children and between its parents and its children (see also exercise 3.11).
Thus, a useful heuristic is to look for subsets of variables that seem to be highly interdependent.
There are several approaches that one can use to ﬁnd such subsets. Most obviously, we can
learn a structure over the observed variables and then search for subsets that are connected
by many edges. An obvious problem with this approach is that most learning methods are
biased against learning networks with large indegrees, especially given limited data.
Thus,
these methods may return sparser structures even when dependencies may exist, preventing
us from using the learned structure to infer the existence of a hidden variable. Nevertheless,
these methods can be used successfully given a reasonably large number of samples. Another
approach is to avoid the structure learning phase and directly consider the dependencies in
the empirical distribution. For example, a quick-and-dirty method is to compute a measure
of dependency, such as mutual information, between all pairs of variables.
This approach
avoids the need to examine marginals over larger sets of variables, and hence it is applicable
in the case of limited data. However, we note that children of an observed variable will also
be highly correlated, so that this approach does not distinguish between dependencies that
can be explained by the observed variables and ones that require introducing hidden variables.
Nevertheless, we can use this approach as a heuristic for introducing a hidden variable, and
potentially employ a subsequent pruning phase to eliminate the variable if it is not helpful, given
the observed variables.

19.6. Summary
933
19.6
Summary
In this chapter, we considered the problem of learning in the presence of incomplete data. We
saw that learning from such data introduces several signiﬁcant challenges.
One set of challenges involves the statistical interpretation of the learning problem in this
setting. As we saw, we need to be aware of the process that generated the missing data and the
eﬀect of nonrandom observation mechanisms on the interpretation of the data. Moreover, we
also need to be mindful of the possibility of unidentiﬁability in the models we learn, and as a
consequence, to take care when interpreting the results.
A second challenge involves computational considerations.
Most of the key properties

that helped make learning feasible in the fully observable case vanish in the partially
observed setting. In particular, the likelihood function no longer decomposes, and is
even multimodal. As a consequence, the learning task requires global optimization over
a high-dimensional space, with an objective that is highly susceptible to local optima.
We presented two classes of approaches for performing parameter estimation in this setting:
a generic gradient-based process, and the EM algorithm, which is speciﬁcally designed for
maximizing likelihood functions. Both of these methods perform hill climbing over the parameter
space, and are therefore guaranteed only to ﬁnd a local optimum (or rather, a stationary point)
of the likelihood function. Moreover, each iteration in these algorithms requires that we solve
an inference problem for each (partially observed) instance in our data set, a requirement that
introduces a major computational burden.
In some cases, we want not only a single parameter estimate, but also some evaluation of
our conﬁdence in those estimates, as would be obtained from Bayesian learning. Clearly, given
the challenges we mentioned, closed-form solutions to the integration are generally impossi-
ble. However, several useful approximations have been developed and used in practice; most
commonly used are the methods based on MCMC methods, and on variational approximations.
We discussed the problem of structure learning in the partially-observed setting. Most com-

monly used are the score-based approaches, where we deﬁne the problem as one of
ﬁnding a high-scoring structure. We presented several approximations to the Bayesian
score; most of these are based on an asymptotic approximation, and hence should be
treated with care given only a small number of samples. We then discussed the challenges
of searching over the space of networks when the score is not decomposable, a setting that (in
principle) forces us to apply a highly expensive evaluation procedure to every candidate that we
are considering in the search. The structural EM algorithm provides one approach to reduce
this cost. It uses an approximation to the score that is based on some completion of the data,
allowing us to use the same eﬃcient algorithms that we applied in the complete data case.
Finally, we brieﬂy discussed some of the important questions that arise when we consider
hidden variables: Where in the model should we introduce a hidden variable? What should we
select as the cardinality of such a variables? And how do we initialize a variable so as to guide
the learning algorithm toward “good” regions of the space? While we brieﬂy described some
ideas here, the methods are generally heuristic, and there are no guarantees.
Overall, owing to the challenges of this learning setting, the methods we discussed in this
chapter are more heuristic and provide weaker guarantees than methods that we encountered
in previous learning chapters. For this reason, the application of these methods is more of an
art than a science, and there are often variations and alternatives that can be more eﬀective for

934
Chapter 19. Partially Observed Data
particular learning scenarios. This is an active area of study, and even for the simple clustering
problem there is still much active research. Thus, we did not attempt to give a complete coverage
and rather focused on the core methods and ideas.

However, while these complications mean that learning from incomplete data is often
challenging or even impossible, there are still many real-life applications where the
methods we discussed here are highly eﬀective. Indeed, the methods that we described
here are some of the most commonly used of any in the book.
They simply require
that we take care in their application, and generally that we employ a fair amount of
hand-tuned engineering.
19.7
Relevant Literature
The problem of statistical estimation from missing data has received a thorough treatment in the
ﬁeld of statistics. The distinction between the data generating mechanism and the observation
mechanism was introduced by Rubin (1976) and Little (1976). Follow-on work deﬁned the notion
of MAR and MCAR Little and Rubin (1987). Similarly, the question of identiﬁability is also a
central in statistical inference Casella and Berger (1990); Tanner (1993). Treatment of the subject
for Bayesian networks appears in Settimi and Smith (1998a); Garcia (2004).
An early discussion that touches on the gradient of likelihood appears in Buntine (1994).
Binder et al. (1997); Thiesson (1995) applied gradient methods for learning with missing values in
Bayesian networks. They derived the gradient form and suggested how to compute it eﬃciently
using clique tree calibration. Gradient methods are often more ﬂexible in using models that
do not have a closed-form MLE estimate even from complete data (see also chapter 20) or
when using alternative objectives. For example, Greiner and Zhou (2002) suggest training using
gradient ascent for optimizing conditional likelihood.
The framework of expectation maximization was introduced by Dempster et al. (1977), who
generalized ideas that were developed independently in several related ﬁelds (for example, the
Baum-Welch algorithm in hidden Markov models (Rabiner and Juang 1986)). The use of expecta-
tion maximization for maximizing the posterior was introduced by Green (1990). There is a wide
literature of extensions of expectation maximization, analysis of convergence rates, and speedup
methods; see McLachlan and Krishnan (1997) for a survey. Our presentation of the theoretical
foundations of expectation maximization follows the discussion by Neal and Hinton (1998).
The use of expectation maximization in speciﬁc graphical models ﬁrst appeared in various
forms (Cheeseman, Self et al. 1988b; Cheeseman, Kelly et al. 1988; Ghahramani and Jordan
1993). Its adaptation for parameter estimation in general graphical models is due to Lauritzen
(1995). Several approaches for accelerating EM convergence in graphical models were examined
by Bauer et al. (1997) and Ortiz and Kaelbling (1999). The idea of incremental updates within
expectation maximization was formulated by Neal and Hinton (1998). The application of expec-
tation maximization for learning the parameters of noisy-or CPDs (or more generally CPDs with
causal independence) was suggested by Meek and Heckerman (1997). The relationship between
expectation maximization and hard-assignment EM was discussed by Kearns et al. (1997).
There are numerous applications of expectation maximization to a wide variety of problems.
The collaborative ﬁltering application of box 19.A is based on Breese et al. (1998). The application
to robot mapping of box 19.D is due to Thrun et al. (2004).

19.8. Exercises
935
There is a rich literature combining expectation maximization with diﬀerent types of approx-
imate inference procedures. Variational EM was introduced by Ghahramani (1994) and further
elaborated by Ghahramani and Jordan (1997). The combination of expectation maximization with
various types of belief propagation algorithms has been used in many current applications (see,
for example, Frey and Kannan (2000); Heskes et al. (2003); Segal et al. (2001)). Similarly, other
combinations have been examined in the literature, such as Monte Carlo EM (Caﬀo et al. 2005).
Applying Bayesian approaches with incomplete data requires approximate inference. A com-
mon solution is to use MCMC sampling, such as Gibbs sampling, using data-completion particles
(Gilks et al. 1994). Our discussion of sampling the Dirichlet distribution is based on (Ripley 1987).
More advanced sampling is based on the method of Fishman (1976) for sampling from Gamma
distributions.
Bayesian Variational methods were introduced by MacKay (1997); Jaakkola and
Jordan (1997); Bishop et al. (1997) and further elaborated by Attias (1999); Ghahramani and Beal
(2000). Minka and Laﬀerty (2002) suggest a Bayesian method based on expectation propagation.
The development of Laplace-approximation structure scores is based mostly on the presenta-
tion in Chickering and Heckerman (1997); this work is also the basis for the analysis of box 19.G.
The BIC score was originally suggested by Schwarz (1978). Geiger et al. (1996, 2001) developed
the foundations for the BIC score for Bayesian networks with hidden variables. This line of
work was extended by several works (D. Rusakov 2005; Settimi and Smith 2000, 1998b). The
Cheeseman-Stutz approximation was initially introduced for clustering models by Cheeseman
and Stutz (1995) and later adapted for graphical models by Chickering and Heckerman (1997).
Variational scores were suggested by Attias (1999) and further elaborated by Beal and Ghahramani
(2006).
Search based on structural expectation maximization was introduced by Friedman (1997, 1998)
and further discussed in Meila and Jordan (2000); Thiesson et al. (1998). The selective clustering
example of section 19.4.3.3 is based on Barash and Friedman (2002). Myers et al. (1999) suggested
a method based on stochastic search.
An alternative approach uses reversible jump MCMC
reversible jump
MCMC
methods that perform Monte Carlo search through both parameter space and structure space
(Green 1995). More recent proposals use Dirichlet processes to integrate over potential structures
(Rasmussen 1999; Wood et al. 2006).
Introduction of hidden variables is a classic problem. Pearl (1988) suggested a method based
on algebraic constraints in the distribution. The idea of using algebraic signatures of hidden
variables has been proposed in several works (Spirtes et al. 1993; Geiger and Meek 1998; Robins
and Wasserman 1997; Tian and Pearl 2002; Kearns and Mansour 1998). Using the structural
signature was suggested by Martin and VanLehn (1995) and developed more formally by Elidan
et al. (2000). Additional methods include hierarchical methods (Zhang 2004; Elidan and Friedman
2005), the introduction of variables to capture temporal correlations (Boyen et al. 1999), and
introduction of variables in networks of continuous variables (Elidan et al. 2007).
19.8
Exercises
Exercise 19.1
Consider the estimation problem in example 19.4.
a. Provide upper and lower bounds on the maximum likelihood estimate of θ.
b. Prove that your bounds are tight; that is, there are values of ψOX|x1 and ψOX|x0 for which these
estimates are equal to the maximum likelihood.

936
Chapter 19. Partially Observed Data
Exercise 19.2⋆
Suppose we have a given model P(X | θ) on a set of variable X = {X1, . . . , Xn}, and some incomplete
data. Suppose we introduce additional variables Y = {Y1, . . . , Yn} so that Yi has the value 1 if Xi is
observed and 0 otherwise. We can extend the data, in the obvious way, to include complete observations of
the variables Y . Show how to augment the model to build a model P(X, Y | θ, θ′) = P(X | θ)P(Y |
X, θ′) so that it satisﬁes the missing at random assumption.
missing at
random
Exercise 19.3
Consider the problem of applying EM to parameter estimation for a variable X whose local probabilistic
model is a tree-CPD. We assume that the network structure G includes the structure of the tree-CPDs in
it, so that we have a structure T for X. We are given a data set D with some missing values, and we
want to run EM to estimate the parameters of T . Explain how we can adapt the EM algorithm in order
to accomplish this task. Describe what expected suﬃcient statistics are computed in the E-step, and how
parameters are updated in the M-step.
Exercise 19.4
Consider the problem of applying EM to parameter estimation for a variable X whose local probabilistic
model is a noisy-or. Assume that X has parents Y1, . . . , Yk, so that our task for X is to estimate the
noise parameters λ0, . . . , λk. Explain how we can use the EM algorithm to accomplish this task. (Hint:
Utilize the structural decomposition of the noisy-or node.)
Exercise 19.5
Prove theorem 19.2. (Hint: use lemma 19.1.)
Exercise 19.6
Suppose we are using a gradient method to learn parameters for a network with table-CPDs. Let X be one
of the variables in the network with parents U. One of the constraints we need to maintain is that
X
x
θx|u = 1
for every assignment u for U. Given the gradient
∂
∂θx|u ℓ(θ : D), show how to project it to null space
of this constraint. That is, show how to ﬁnd a gradient direction that maximizes the likelihood while
preserving this constraint.
Exercise 19.7
Suppose we consider reparameterizing table-CPDs using the representation of equation (19.3). Use the
chain law of partial derivatives to ﬁnd the form of
∂
∂λx|u ℓ(θ : D).
Exercise 19.8⋆
Suppose we have a Bayesian network with table-CPDs.
Apply the method of Lagrange multipliers to
characterize the maximum likelihood solution under the constraint that each conditional probability sums
to one. How does your characterization relate to EM?
Exercise 19.9⋆
We now examine how to compute the Hessian of the likelihood function. Recall that the Hessian of the
log-likelihood is the matrix of second derivatives. Assume that our model is a Bayesian network with
table-CPDs.
a. Prove that the second derivative of the likelihood of an observation o is of the form:
∂2 log P(o)
∂θxi|ui∂θxj|uj
=
1
θxi|uiθxj|uj
[P(xi, ui, xj, uj | o) −P(xi, ui | o)P(xj, uj | o)] .

19.8. Exercises
937
b. What is the cost of computing the full Hessian matrix of log P(o) if we use clique tree propagation?
c. What is the computational cost if we are only interested in entries of the form
∂2
∂θxi|ui∂θx′
i|u′
i
log P(o);
that is, we are interested in the “diagonal band” that involves only second derivatives of entries from
the same family?
Exercise 19.10⋆
a. Consider the task of estimating the parameters of a univariate Gaussian distribution N
 µ; σ2
from a
data set D. Show that if we maximize likelihood subject to the constraint σ2 ≥ϵ for some ϵ > 0, then
the likelihood L(µ, σ2 : D) is guaranteed to remain bounded.
b. Now, consider estimating the parameters of a multivariate Gaussian N (µ; Σ) from a data set D.
Provide constraints on Σ that achieve the same guarantee.
Exercise 19.11⋆
Consider learning the parameters of the network H →X, H →Y , where H is a hidden variable. Show
that the distribution where P(H), P(X | H), P(Y | H) are uniform is a stationary point of the likelihood
(gradient is 0). What does that imply about gradient ascent and EM starting from this point?
Exercise 19.12
Prove theorem 19.5. Hint, note that ℓ(θt : D) = FD[θt, P(H | D, θt)], and use corollary 19.1.
Exercise 19.13
Consider the task of learning the parameters of a DBN with table-CPDs from a data set with missing data.
In particular, assume that our data set consists of a sequence of observations o(0)
0 , o(1)
1 , . . . , o(T )
t
. (Note
that we do not assume that the same variables are observed in every time-slice.)
a. Describe precisely how you would run EM in this setting to estimate the model parameters; your
algorithm should specify exactly how we run the E-step, which suﬃcient statistics we compute and
how, and how the suﬃcient statistics are used within the M-step.
b. Given a single trajectory, as before, which of the network parameters might you be able to estimate?
Exercise 19.14⋆
Show that, until convergence, each iteration of hard-assignment EM increases ℓ(θ : ⟨D, H⟩).
Exercise 19.15⋆
Suppose that we have an incomplete data set D, and network structure G and matching parameters.
Moreover, suppose that we are interested in learning the parameters of a single CPD P(Xi | U i). That
is, we assume that the parameters we were given for all other families are frozen and do not change
during the learning. This scenario can arise for several reasons: we might have good prior knowledge
about these parameters; or we might be using an incremental approach, as mentioned in box 19.C (see also
exercise 19.16).
We now consider how this scenario can change the computational cost of the EM algorithm.
a. Assume we have a clique tree for the network G and that the CPD P(Xi | U i) was assigned to clique
Cj. Analyze which messages change after we update the parameters for P(Xi | U i). Use this analysis
to show how, after an initial precomputation step, we can perform iterations of this single-family EM
procedure with a computational cost that depends only on the size of Cj and not the size of the rest
of the cluster tree.

938
Chapter 19. Partially Observed Data
b. Would this conclusion change if we update the parameters of several families that are all assigned to
the same cluster in the cluster tree?
Exercise 19.16⋆
We can build on the idea of the single-family EM procedure, as described in exercise 19.15, to deﬁne an
incremental EM procedure for learning all the parameters in the network. In this approach, at each step
incremental EM
we optimize the parameters of a single CPD (or several CPDs) while freezing the others. We then iterate
between these local EM runs until all families have converged.
Is this modiﬁed version of EM still guaranteed to converge? In other words, does
ℓ(θt+1 : D) ≥ℓ(θt : D)
still hold? If so, prove the result. If not, explain why not.
Exercise 19.17⋆
We now consider how to use the interpretation of the EM as maximizing an energy functional to allow
partial or incremental updates over the instances. Consider the EM algorithm of algorithm 19.2. In the
Compute-ESS we collect the statistics from all the instances. This requires running inference on all the
instances.
We now consider a procedure that performs partial updates where it update the expected suﬃcient statistics
for some, but not all, of the instances. In particular, suppose we replace this procedure by one that runs
inference on a single instance and uses the update to replace the old contribution of the instance with a
new one; see algorithm 19.4. This procedure, instead of computing all the expected suﬃcient statistics in
each E-step, caches the contribution of each instance to the suﬃcient statistics, and then updates only a
single one in each iteration.
a. Show that the incremental EM algorithm converges to a ﬁxed point of the log-likelihood function. To
do so, show that each iteration improves the EM energy functional. Hint: you need to deﬁne what is
the eﬀect of the partial E-step on the energy functional.
b. How would that analysis generalize if in each iteration the algorithm performs a partial update for k
instances (instead of 1)?
c. Assume that the computations in the M-step are relatively negligible compared to the inference in the
E-step. Would you expect the incremental EM to be more eﬃcient than standard EM? If so, why?
Exercise 19.18⋆
Consider the model described in box 19.D.
a. Assume we perform the E-step for each step xm by deﬁning
˜P(xm | Cm = k : θk) = N
 d(x, pk) | 0; σ2
and ˜P(xm | Cm = 0 : θk) = C for some constant C. Why is this formula not a correct application
of EM? (Hint: Consider the normalizing constants.)
We note that although this approach is mathematically not quite right, it seems to be a reasonable
approximation that works in practice.
b. Given a solution to the E-step, show how to perform maximum likelihood estimation of the model
parameters αk, βk, subject to the constraint that αk be a unit-vector, that is, that αk · αk = 1. (Hint:
Use Lagrange multipliers.)

19.8. Exercises
939
Algorithm 19.4 The incremental EM algorithm for network with table-CPDs
Procedure Incremental-E-Step (
θ,
// Parameters for update
m,
// instance to update
)
1
Run inference on ⟨G, θ⟩using evidence o[m]
2
for each i = 1, . . . , n
3
for each xi, ui ∈Val(Xi, PaG
Xi)
4
// Remove old contribution
5
¯
M[xi, ui] ←
¯
M[xi, ui] −¯
Mm[xi, ui]
6
// Compute new contribution
7
¯
Mm[xi, ui] ←P(xi, ui | o[m])
8
¯
M[xi, ui] ←
¯
M[xi, ui] + ¯
Mm[xi, ui]
Procedure Incremental-EM (
G,
// Bayesian network structure over X1, . . . , Xn
θ0,
// Initial set of parameters for G
D
// Partially observed data set
)
1
for each i = 1, . . . , n
2
for each xi, ui ∈Val(Xi, PaG
Xi)
3
¯
M[xi, ui] ←0
4
for each m = 1 . . . M
5
¯
Mm[xi, ui] ←0
6
// Initialize the expected suﬃcient statistics
7
for each m = 1 . . . M
8
Incremental-E-Step(G, θ0, D, m)
9
m ←1
10
for each t = 0, 1 . . . , until convergence
11
// E-step
12
Incremental-E-Step(G, θt, D, m)
13
m ←(m mod M) + 1
14
// M-step
15
for each i = 1, . . . , n
16
for each xi, ui ∈Val(Xi, PaG
Xi)
17
θt+1
xi|ui ←
¯
M[xi,ui]
¯
M[ui]
18
return θt

940
Chapter 19. Partially Observed Data
Exercise 19.19⋆
Consider the setting of exercise 12.29, but now assume that we cannot (or do not wish to) maintain a
distribution over the Aj’s. Rather, we want to ﬁnd the assignment a∗
1, . . . , a∗
m for which P(a1, . . . , am)
is maximized.
In this exercise, we address this problem using the EM algorithm, treating the values a1, . . . , am as
parameters. In the E-step, we compute the expected value of the Ci variables; in the M-step, we maximize
the value of the aj’s given the distribution over the Cj’s.
a. Describe how one can implement this EM procedure exactly, that is, with no need for approximate
inference.
b. Why is approximate inference necessary in exercise 12.29 but not here? Give a precise answer in terms
of the properties of the probabilistic model.
Exercise 19.20
Suppose that a prior on a parameter vector is p(θ) ∼Dirichlet(α1, . . . , αk). Derive
∂
∂θi log p(θ).
Exercise 19.21
Consider the generalization of the EM procedure to the task of ﬁnding the MAP parameters. Let
˜FD[θ, Q] = FD[θ, Q] + log P(θ).
a. Prove the following result:
Corollary 19.2
For a distribution Q, scoreMAP(θ : D) ≥˜FD[θ, Q] with equality if only if Q(H) = P(H | D, θ).
b. Show that a coordinate ascent approach on ˜FD[θ, Q] requires only changing the M-step to perform
MAP rather than ML estimation, that is, to maximize:
IEQ[ℓ(θ : ⟨D, H⟩)] + log P(θ).
c. Using exercise 17.12, provide a speciﬁc formula for the M-step in a network with table-CPDs.
Exercise 19.22
In this case, we analyze the use of collapsed Gibbs with data completion particles for the purpose of
sampling from a posterior in the case of incomplete data.
a. Consider ﬁrst the simple case of example 19.12. Assuming that the data instances x are sampled from
a discrete naive Bayes model with a Dirichlet prior, derive a closed form for equation (19.9).
b. Now, consider the general case of sampling from P(H | D). Here, the key step would involve sampling
from the distribution
P(Xi[m] | ⟨D, H⟩−Xi[m]) ∝P(Xi[m], ⟨D, H⟩−Xi[m]),
where ⟨D, H⟩−Xi[m] is a complete data set from which the observation of Xi[m] is removed.
Assuming we have table-CPD and independent Dirichlet priors over the parameters, derive this con-
ditional probability from the form of the marginal likelihood of the data. Show how to use suﬃcient
statistics of the particle to perform this sampling eﬃciently.
Exercise 19.23⋆
We now consider a Metropolis-Hastings sampler for the same setting as exercise 19.22. For simplicity,
we assume that the same variables are hidden in each instance. Consider the proposal distribution for
variable Xi speciﬁed in algorithm 19.5. (We are using a multiple-transition chain, as in section 12.3.2.4,
where each variable has its own kernel.) In this proposal distribution, we resample a value for Xi in all of
the instances, based on the current parameters and the completion for all the other variables.
Derive the form of the acceptance probability for this proposal distribution. Show how to use suﬃcient
statistics of the completed data to evaluate this acceptance probability eﬃciently.

19.8. Exercises
941
Algorithm 19.5 Proposal distribution for collapsed Metropolis-Hastings over data comple-
tions
Procedure Proposal-Distribution (
G,
// Bayesian network structure over X1, . . . , Xn
D
// completed data set
Xi
// A variable to sample
)
1
θ ←Estimate-Parameters(D, G)
2
D′ ←D
3
for each m = 1 . . . M
4
Sample x′
i[m] from P(Xi[m] | x−i[m], θ)
5
return D′
Exercise 19.24
Prove theorem 19.8.
Exercise 19.25⋆
Prove theorem 19.10. Hint: Use the proof of theorem 19.5.
Exercise 19.26
Consider learning structure in the setting discussed in section 19.4.3.3. Describe a data set D and parame-
ters for a network where X1 and C are independent, yet the expected suﬃcient statistics ¯
M[X1, C] show
dependency between X1 and C.
Exercise 19.27
Consider using the structural EM algorithm to learn the structure associated with a hidden variable H; all
other variables are fully observed. Assume that we start our learning process by performing an E-step in a
network where H is not connected to any of X1, . . . , Xn. Show that, for any initial parameter assignment
to P(H), the SEM algorithm will not connect H to the rest of the variables in the network.
Exercise 19.28
Consider the task of learning a model involving a binary-valued hidden variable H using the EM algorithm.
Assume that we initialize the EM algorithm using parameters that are symmetric in the two values of H;
that is, for any variable Xi that has H has a parent, we have P(Xi | U i, h0) = P(Xi | U i, h1). Show
that, with this initialization, the model will remain symmetric in the two values of H, over all EM iterations.
Exercise 19.29
Derive the sampling update equations for the partition-based Gibbs sampling of equation (19.17) and
equation (19.18) from the corresponding update equations over particles deﬁned as ground assignments
(equation (19.10)). Your update rules must sum over all assignments consistent with the partition.
Exercise 19.30
Consider the distribution over partitions induced by the Chinese restaurant process.
a. Find a closed-form formula for the probability induced by this process for any partition σ of the guests.
Show that this probability is invariant to the order the guests enter the restaurant.
b. Show that a Gibbs sampling process over the partitions generated by this algorithm satisﬁes equa-
tion (19.19) and equation (19.20).

942
Chapter 19. Partially Observed Data
Algorithm 19.6 Proposal distribution over partitions in the Dirichlet process priof
Procedure DP-Merge-Split-Proposal (
σ
// A partition
)
1
Uniformly choose two diﬀerent instances m, l
2
if m, l are assigned to two diﬀerent clusters I, I′ then
3
// Propose partition that merges the two clusters
4
σ′ ←σ −{I, I′} ∪{I ∪I′}
5
else
6
Let I be the cluster to which m, l are both assigned
7
// Propose to randomly split I so as to separate them
8
I1 ←{m}
9
I2 ←{l}
10
for n ∈I
11
Add n to I1 with probability 0.5 and to I2 with probability 0.5
12
σ′ ←σ −{I} ∪{I1, I2}
13
return (σ′)
Exercise 19.31⋆
Algorithm 19.6 presents a Metropolis-Hastings proposal distribution over partitions in the Dirichlet process
prior. Compute the acceptance probability of the proposed move.

20
Learning Undirected Models
20.1
Overview
In previous chapters, we developed the theory and algorithms for learning Bayesian networks
from data. In this chapter, we consider the task of learning Markov networks. Although many of
the same concepts and principles arise, the issues and solutions turn out to be quite diﬀerent.

Perhaps the most important reason for the diﬀerences is a key distinction between
Markov networks and Bayesian networks: the use of a global normalization constant
(the partition function) rather than local normalization within each CPD. This global
factor couples all of the parameters across the network, preventing us from decomposing
the problem and estimating local groups of parameters separately. This global parameter
coupling has signiﬁcant computational ramiﬁcations.
As we will explain, in contrast to the
situation for Bayesian networks, even simple (maximum-likelihood) parameter estimation with
complete data cannot be solved in closed form (except for chordal Markov networks, which
are therefore also Bayesian networks). Rather, we generally have to resort to iterative methods,
such as gradient ascent, for optimizing over the parameter space. The good news is that the
likelihood objective is concave, and so these methods are guaranteed to converge to the global
optimum. The bad news is that each of the steps in the iterative algorithm requires that we
run inference on the network, making even simple parameter estimation a fairly expensive, or
even intractable, process.
Bayesian estimation, which requires integration over the space of
parameters, is even harder, since there is no closed-form expression for the parameter posterior.
Thus, the integration associated with Bayesian estimation must be performed using approximate
inference (such as variational methods or MCMC), a burden that is often infeasible in practice.
As a consequence of these computational issues, much of the work in this area has gone into
the formulation of alternative, more tractable, objectives for this estimation problem. Other work
has been focused on the use of approximate inference algorithms for this learning problem and
on the development of new algorithms suited to this task.
The same issues have signiﬁcant impact on structure learning.
In particular, because a
Bayesian parameter posterior is intractable to compute, the use of exact Bayesian scoring for
model selection is generally infeasible. In fact, scoring any model (computing the likelihood)
requires that we run inference to compute the partition function, greatly increasing the cost of
search over model space. Thus, here also, the focus has been on approximations and heuristics
that can reduce the computational cost of this task. Here, however, there is some good news,
arising from another key distinction between Bayesian and Markov networks: the lack of a

944
Chapter 20. Learning Undirected Models
global acyclicity constraint in undirected models. Recall (see theorem 18.5) that the acyclicity
constraint couples decisions regarding the family of diﬀerent variables, thereby making the
structure selection problem much harder. The lack of such a global constraint in the undirected
case eliminates these interactions, allowing us to choose the local structure locally in diﬀerent
parts of the network. In particular, it turns out that a particular variant of the structure learning
task can be formulated as a continuous, convex optimization problem, a class of problems
generally viewed as tractable. Thus, elimination of global acyclicity removes the main reason for
the NP-hardness of structure learning that we saw in Bayesian networks. However, this does
not make structure learning of Markov networks eﬃcient; the convex optimization process (as
for parameter estimation) still requires multiple executions of inference over the network.
A ﬁnal important issue that arises in the context of Markov networks is the overwhelmingly
common use of these networks for settings, such as image segmentation and others, where
we have a particular inference task in mind.
In these settings, we often want to train a
network discriminatively (see section 16.3.2), so as to provide good performance for our particular
prediction task. Indeed, much of Markov network learning is currently performed for CRFs.
The remainder of this chapter is structured as follows. We begin with the analysis of the
properties of the likelihood function, which, as always, forms the basis for all of our discussion
of learning. We then discuss how the likelihood function can be optimized to ﬁnd the maximum
likelihood parameter estimates. The ensuing sections discuss various important extensions to
these basic ideas: conditional training, parameter priors for MAP estimation, structure learning,
learning with missing data, and approximate learning methods that avoid the computational
bottleneck of multiple iterations of network inference. These extensions are usually described as
building on top of standard maximum-likelihood parameter estimation. However, it is important
to keep in mind that they are largely orthogonal to each other and can be combined. Thus, for
example, we can also use the approximate learning methods in the case of structure learning
or of learning with missing data. Similarly, all of the methods we described can be used with
maximum conditional likelihood training. We return to this issue in section 20.8.
We note that, for convenience and consistency with standard usage, we use natural logarithms
throughout this chapter, including in our deﬁnitions of entropy or KL-divergence.
20.2
The Likelihood Function
As we saw in earlier chapters, the key component in most learning tasks is the likelihood
function. In this section, we discuss the form of the likelihood function for Markov networks,
its properties, and their computational implications.
20.2.1
An Example
As we suggested, the existence of a global partition function couples the diﬀerent parameters
in a Markov network, greatly complicating our estimation problem. To understand this issue,
consider the very simple network A—B—C, parameterized by two potentials φ1(A, B) and
φ2(B, C). Recall that the log-likelihood of an instance ⟨a, b, c⟩is
ln P(a, b, c) = ln φ1(a, b) + ln φ2(b, c) −ln Z,

20.2. The Likelihood Function
945
lnf1(a1, b1)
lnf2(b0, c1)
Figure 20.1
Log-likelihood surface for the Markov network A—B—C, as a function of ln φ1(a1, b1)
(x-axis) and ln φ2(b0, c1) (y-axis); all other parameters in both potentials are set to 1. Surface is viewed
from the (+∞, +∞) point toward the (−, −) quadrant. The data set D has M = 100 instances, for
which M[a1, b1] = 40 and M[b0, c1] = 40. (The other suﬃcient statistics are irrelevant, since all of the
other log-parameters are 0.)
where Z is the partition function that ensures that the distribution sums up to one.
Now,
consider the log-likelihood function for a data set D containing M instances:
ℓ(θ : D)
=
X
m
(ln φ1(a[m], b[m]) + ln φ2(b[m], c[m]) −ln Z(θ))
=
X
a,b
M[a, b] ln φ1(a, b) +
X
b,c
M[b, c] ln φ2(b, c) −M ln Z(θ).
Thus, we have suﬃcient statistics that summarize the data: the joint counts of variables that
appear in each potential. This is analogous to the situation in learning Bayesian networks, where
we needed the joint counts of variables that appear within the same family. This likelihood
consists of three terms. The ﬁrst term involves φ1 alone, and the second term involves φ2 alone.
The third term, however, is the log-partition function ln Z, where:
Z(θ) =
X
a,b,c
φ1(a, b)φ2(b, c).
Thus, ln Z(θ) is a function of both φ1 and φ2. As a consequence, it couples the two potentials
in the likelihood function.
Speciﬁcally, consider maximum likelihood estimation, where we aim to ﬁnd parameters that
maximize the log-likelihood function. In the case of Bayesian networks, we could estimate each
conditional distribution independently of the other ones. Here, however, when we change one
of the potentials, say φ1, the partition function changes, possibly changing the value of φ2 that
maximizes −ln Z(θ). Indeed, as illustrated in ﬁgure 20.1, the log-likelihood function in our
simple example shows clear dependencies between the two potentials.
In this particular example, we can avoid this problem by noting that the network A—B—C
is equivalent to a Bayesian network, say A →B →C. Therefore, we can learn the parameters

946
Chapter 20. Learning Undirected Models
of this BN, and then deﬁne φ1(A, B) = P(A)P(B | A) and φ2(B, C) = P(C | B). Because
the two representations have equivalent expressive power, the same maximum likelihood is
achievable in both, and so the resulting parameterization for the Markov network will also be
a maximum-likelihood solution. In general, however, there are Markov networks that do not
have an equivalent BN structure, for example, the diamond-structured network of ﬁgure 4.13
(see section 4.5.2). In such cases, we generally cannot convert a learned BN parameterization
into an equivalent MN; indeed, the optimal likelihood achievable in the two representations is
generally not the same.
20.2.2
Form of the Likelihood Function
To provide a more general description of the likelihood function, it ﬁrst helps to provide a
more convenient notational basis for the parameterization of these models. For this purpose,
we use the framework of log-linear models, as deﬁned in section 4.4.1.2. Given a set of features
log-linear model
F = {fi(Di)}k
i=1, where fi(Di) is a feature function deﬁned over the variables in Di, we
have:
P(X1, . . . , Xn : θ) =
1
Z(θ) exp
( k
X
i=1
θifi(Di)
)
.
(20.1)
As usual, we use fi(ξ) as shorthand for fi(ξ⟨Di⟩). The parameters of this distribution corre-
spond to the weight we put on each feature. When θi = 0, the feature is ignored, and it has no
eﬀect on the distribution.
As discussed in chapter 4, this representation is very generic and can capture Markov networks
with global structure and local structure. A special case of particular interest is when fi(Di) is
a binary indicator function that returns the value 0 or 1. With such features, we can encode a
“standard” Markov network by simply having one feature per potential entry. In more general,
however, we can consider arbitrary valued features.
Example 20.1
As a speciﬁc example, consider the simple diamond network of ﬁgure 3.10a, where we take all four
variables to be binary-valued. The features that correspond to this network are sixteen indicator
functions: four for each assignment of variables to each of our four clusters. For example, one such
feature would be:
fa0,b0(a, b) = 11{a = a0}11{b = b0}.
With this representation, the weight of each indicator feature is simply the natural logarithm of the
corresponding potential entry. For example, θa0,b0 = ln φ1(a0, b0).
Given a model in this form, the log-likelihood function has a simple form.
Proposition 20.1
Let D be a data set of M examples, and let F = {fi : i = 1, . . . , k} be a set of features that
deﬁne a model. Then the log-likelihood is
ℓ(θ : D) =
X
i
θi
 X
m
fi(ξ[m])
!
−M ln Z(θ).
(20.2)

20.2. The Likelihood Function
947
The suﬃcient statistics of this likelihood function are the sums of the feature values in the
suﬃcient
statistics
instances in D. We can derive a more elegant formulation if we divide the log-likelihood by the
number of samples M.
1
M ℓ(θ : D) =
X
i
θiIED[fi(di)] −ln Z(θ),
(20.3)
where IED[fi(di)] is the empirical expectation of fi, that is, its average in the data set.
20.2.3
Properties of the Likelihood Function
The formulation of proposition 20.1 describes the likelihood function as a sum of two functions.
The ﬁrst function is linear in the parameters; increasing the parameters directly increases this
linear term. Clearly, because the log-likelihood function (for a ﬁxed data set) is upper-bounded
(the probability of an event is at most 1), the second term ln Z(θ) balances the ﬁrst term.
Let us examine this second term in more detail. Recall that the partition function is deﬁned
as
ln Z(θ) = ln
X
ξ
exp
(X
i
θifi(ξ)
)
.
One important property of the partition function is that it is convex in the parameters θ. Recall
convex partition
function
that a function f(⃗x) is convex if for every 0 ≤α ≤1,
f(α⃗x + (1 −α)⃗y) ≤αf(⃗x) + (1 −α)f(⃗y).
In other words, the function is bowl-like, and every interpolation between the images of two
points is larger than the image of their interpolation. One way to prove formally that the function
f is convex is to show that the Hessian — the matrix of the function’s second derivatives — is
Hessian
positive semideﬁnite. Therefore, we now compute the derivatives of Z(θ).
Proposition 20.2
Let F be a set of features. Then,
∂
∂θi
ln Z(θ)
=
IEθ[fi]
∂2
∂θi∂θj
ln Z(θ)
=
CCovθ[fi; fj],
where IEθ[fi] is a shorthand for IEP (X:θ)[fi].
Proof The ﬁrst derivatives are computed as:
∂
∂θi
ln Z(θ)
=
1
Z(θ)
X
ξ
∂
∂θi
exp



X
j
θjfj(ξ)



=
1
Z(θ)
X
ξ
fi(ξ) exp



X
j
θjfj(ξ)



=
IEθ[fi].

948
Chapter 20. Learning Undirected Models
We now consider the second derivative:
∂2
∂θj∂θi
ln Z(θ)
=
∂
∂θj


1
Z(θ)
X
ξ
fi(ξ) exp
(X
k
θkfk(ξ)
)

=
−
1
Z(θ)2
 ∂
∂θj
Z(θ)
 X
ξ
fi(ξ) exp
(X
k
θkfk(ξ)
)
+
1
Z(θ)
X
ξ
fi(ξ)fj(ξ) exp
(X
k
θkfk(ξ)
)
=
−
1
Z(θ)2 Z(θ)IEθ[fj]
X
ξ
fi(ξ) ˜P(ξ : θ)
+
1
Z(θ)
X
ξ
fi(ξ)fj(ξ) ˜P(ξ : θ)
=
−IEθ[fj]
X
ξ
fi(ξ)P(ξ : θ)
+
X
ξ
fi(ξ)fj(ξ)P(ξ : θ)
=
IEθ[fifj] −IEθ[fi]IEθ[fj]
=
CCovθ[fi; fj].
Thus, the Hessian of ln Z(θ) is the covariance matrix of the features, viewed as random
variables distributed according to distribution deﬁned by θ. Because a covariance matrix is
always positive semideﬁnite, it follows that the Hessian is positive semideﬁnite, and hence that
ln Z(θ) is a convex function of θ.
Because ln Z(θ) is convex, its complement (−ln Z(θ)) is concave. The sum of a linear
function and a concave function is concave, implying the following important result:
Corollary 20.1
The log-likelihood function is concave.

This result implies that the log-likelihood is unimodal and therefore has no local op-
tima. It does not, however, imply the uniqueness of the global optimum: Recall that a
parameterization of the Markov network can be redundant, giving rise to multiple representa-
redundant
parameterization
tions of the same distribution. The standard parameterization of a set of table factors for a
Markov network — a feature for every entry in the table — is always redundant. In our simple
example, for instance, we have:
fa0,b0 = 1 −fa0,b1 −fa1,b0 −fa1,b1.
We thus have a continuum of parameterizations that all encode the same distribution, and
(necessarily) give rise to the same log-likelihood. Thus, there is a unique globally optimal value
for the log-likelihood function, but not necessarily a unique solution. In general, because the
function is concave, we are guaranteed that there is a convex region of continuous global optima.

20.3. Maximum (Conditional) Likelihood Parameter Estimation
949
It is possible to eliminate the redundancy by removing some of the features. However, as we
discuss in section 20.4, that turns out to be unnecessary, and even harmful, in practice.
We note that we have deﬁned the likelihood function in terms of a standard log-linear param-
eterization, but the exact same derivation also holds for networks that use shared parameters,
as in section 6.5; see exercise 20.1 and exercise 20.2.
20.3
Maximum (Conditional) Likelihood Parameter Estimation
We now move to the question of estimating the parameters of a Markov network with a ﬁxed
structure, given a fully observable data set D. We focus in this section on the simplest variant
of this task — maximum-likelihood parameter estimation, where we select parameters that
maximize the log-likelihood function of equation (20.2). In later sections, we discuss alternative
objectives for the parameter estimation task.
20.3.1
Maximum Likelihood Estimation
As for any function, the gradient of the log-likelihood must be zero at its maximum points. For
a concave function, the maxima are precisely the points at which the gradient is zero. Using
proposition 20.2, we can compute the gradient of the average log-likelihood as follows:
∂
∂θi
1
M ℓ(θ : D) = IED[fi(X)] −IEθ[fi].
(20.4)
This analysis provides us with a precise characterization of the maximum likelihood parameters
ˆθ:
Theorem 20.1
Let F be a set of features. Then, θ is a maximum-likelihood parameter assignment if and only if
IED[fi(X)] = IEˆθ[fi] for all i.
In other words, at the maximal likelihood parameters ˆθ, the expected value of each feature
relative to Pˆθ matches its empirical expectation in D. In other words, we want the expected
expected
suﬃcient
statistics
suﬃcient statistics in the learned distribution to match the empirical expectations. This type of
equality constraint is also called moment matching. This theorem easily implies that maximum
moment
matching
likelihood estimation is consistent in the same sense as deﬁnition 18.1: if the model is suﬃ-
MLE consistency
ciently expressive to capture the data-generating distribution, then, at the large sample limit, the
optimum of the likelihood objective is the true model; see exercise 20.3.
By itself, this criterion does not provide a constructive deﬁnition of the maximum likelihood
parameters. Unfortunately, although the function is concave, there is no analytical form for

its maximum. Thus, we must resort to iterative methods that search for the global opti-
mum. Most commonly used are the gradient ascent methods reviewed in appendix A.5.2,
which iteratively take steps in parameter space to improve the objective. At each iteration,
they compute the gradient, and possibly the Hessian, at the current point θ, and use those
estimates to approximate the function at the current neighborhood. They then take a step in the
right direction (as dictated by the approximation) and repeat the process. Due to the convexity
of the problem, this process is guaranteed to converge to a global optimum, regardless of our
starting point.

950
Chapter 20. Learning Undirected Models
To apply these gradient-based methods, we need to compute the gradient. Fortunately, equa-
tion (20.4) provides us with an exact formula for the gradient: the diﬀerence between the
feature’s empirical count in the data and its expected count relative to our current parame-
terization θ.
For example, consider again the fully parameterized network of example 20.1.
Here, the features are simply indicator functions; the empirical count for a feature such as
fa0,b0(a, b) = 11{a = a0}11{b = b0} is simply the empirical frequency, in the data set D, of the
event a0, b0. At a particular parameterization θ, the expected count is simply Pθ(a0, b0). Very
naturally, the gradient for the parameter associated with this feature is the diﬀerence between
these two numbers.
However, this discussion ignores one important aspect: the computation of the expected
counts. In our example, for instance, we must compute the diﬀerent probabilities of the form
Pθt(a, b). Clearly, this computation requires that we run inference over the network. As for the
case of EM in Bayesian networks, a feature is necessarily part of a factor in the original network,
and hence, due to family preservation, all of the variables involved in a feature must occur
together in a cluster in a clique tree or cluster graph. Thus, a single inference pass that calibrates
an entire cluster graph or tree suﬃces to compute all of the expected counts. Nevertheless,
a full inference step is required at every iteration of the gradient ascent procedure.

Because inference is almost always costly in time and space, the computational cost of
parameter estimation in Markov networks is usually high, sometimes prohibitively so. In
section 20.5 we return to this issue, considering the use of approximate methods that reduce
the computational burden.
Our discussion does not make a speciﬁc choice of algorithm to use for the optimization. In
practice, standard gradient ascent is not a particularly good algorithm, both because of its slow
convergence rate and because of its sensitivity to the step size. Much faster convergence is
obtained with second-order methods, which utilize the Hessian to provide a quadratic approx-
imation to the function. However, from proposition 20.2 we can conclude that the Hessian of
log-likelihood
Hessian
the log-likelihood function has the form:
∂
∂θi∂θj
ℓ(θ : D) = −MCCovθ[fi; fj].
(20.5)
To compute the Hessian, we must compute the joint expectation of two features, a task that
is often computationally infeasible. Currently, one commonly used solution is the L-BFGS al-
L-BFGS algorithm
gorithm, a gradient-based algorithm that uses line search to avoid computing the Hessian (see
appendix A.5.2 for some background).
20.3.2
Conditionally Trained Models
As we discussed in section 16.3.2, we often want to use a Markov network to perform a par-
ticular inference task, where we have a known set of observed variables, or features, X, and
a predetermined set of variables, Y , that we want to query. In this case, we may prefer to
use discriminative training, where we train the network as a conditional random ﬁeld (CRF) that
discriminative
training
conditional
random ﬁeld
encodes a conditional distribution P(Y | X).
More formally, in this setting, our training set consists of pairs D = {(y[m], x[m])}M
m=1,
specifying assignments to Y , X.
An appropriate objective function to use in this situation
is the conditional likelihood or its logarithm, deﬁned in equation (16.3).
In our setting, the
conditional
likelihood

20.3. Maximum (Conditional) Likelihood Parameter Estimation
951
log-conditional-likelihood has the form:
ℓY |X(θ : D) = ln P(y[1, . . . , M] | x[1, . . . , M], θ) =
M
X
m=1
ln P(y[m] | x[m], θ).
(20.6)
In this objective, we are optimizing the likelihood of each observed assignment y[m] given the
corresponding observed assignment x[m]. Each of the terms ln P(y[1, . . . , M] | x[1, . . . , M], θ)
is a log-likelihood of a Markov network model with a diﬀerent set of factors — the factors in
the original network, reduced by the observation x[1, . . . , M] — and its own partition function.
Each term is thereby a concave function, and because the sum of concave functions is concave,
we conclude:
Corollary 20.2
The log conditional likelihood of equation (20.6) is a concave function.
As for corollary 20.1, this result implies that the function has a global optimum and no local op-
tima, but not that the global optimum is unique. Here also, redundancy in the parameterization
may give rise to a convex region of contiguous global optima.
The approaches for optimizing this objective are similar to those used for optimizing the
likelihood objective in the unconditional case. The objective function is a concave function,
and so a gradient ascent process is guaranteed to give rise to the unique global optimum. The
form of the gradient here can be derived directly from equation (20.4). We ﬁrst observe that
the gradient of a sum is the sum of the gradients of the individual terms. Here, each term is,
in fact, a log-likelihood — the log-likelihood of a single data case y[m] in the Markov network
obtained by reducing our original model to the context x[m]. A reduced Markov network is
itself a Markov network, and so we can apply equation (20.4) and conclude that:
∂
∂θi
ℓY |X(θ : D) =
M
X
m=1
(fi(y[m], x[m]) −IEθ[fi | x[m]]) .
(20.7)
This solution looks deceptively similar to equation (20.4). Indeed, if we aggregate the ﬁrst
component in each of the summands, we obtain precisely the empirical count of fi in the data
set D. There is, however, one key diﬀerence. In the unreduced Markov network, the expected
feature counts are computed relative to a single model; in the case of the conditional Markov
network, these expected counts are computed as the summation of counts in an ensemble of
models, deﬁned by the diﬀerent values of the conditioning variables x[m].
This diﬀerence
has signiﬁcant computational consequences. Recall that computing these expectations involves
running inference over the model. Whereas in the unconditional case, each gradient step

required only a single execution of inference, when training a CRF, we must (in general)
execute inference for every single data case, conditioning on x[m]. On the other hand, the
inference is executed on a simpler model, since conditioning on evidence in a Markov
network can only reduce the computational cost. For example, the network of ﬁgure 20.2
is very densely connected, whereas the reduced network over Y alone (conditioned on X) is a
simple chain, allowing linear-time inference.
Discriminative training can be particularly beneﬁcial in cases where the domain of X is very
large or even inﬁnite. For example, in our image classiﬁcation task, the partition function in the

952
Chapter 20. Learning Undirected Models
Y5
Y4
Y3
Y2
Y1
X5
X4
X3
X2
X1
Figure 20.2
A highly connected CRF that allows simple inference when conditioned: The edges
that disappear in the reduced Markov network after conditioning on X are marked in gray; the remaining
edges form a simple linear chain.
generative setting involves summation (or integration) over the space of all possible images; if
we have an N × N image where each pixel can take 256 values, the resulting space has 256N 2
values, giving rise to a highly intractable inference problem (even using approximate inference
methods).
Box 20.A — Concept: Generative and Discriminative Models for Sequence Labeling. One
of the main tasks to which probabilistic graphical models have been applied is that of taking
a set of interrelated instances and jointly labeling them, a process sometimes called collective clas-
collective
classiﬁcation
siﬁcation. We have already seen examples of this task in box 4.B and in box 4.E; many other
examples exist. Here, we discuss some of the trade-oﬀs between diﬀerent models that one can apply
to this task. We focus on the context of labeling instances organized in a sequence, since it is simpler
and allows us to illustrate another important point.
In the sequence labeling task, we get as input a sequence of observations X and need to label
sequence labeling
them with some joint label Y . For example, in text analysis (box 4.E), we might have a sequence of
words each of which we want to label with some label. In a task of activity recognition, we might
activity
recognition
obtain a sequence of images and want to label each frame with the activity taking place in it (for
example, running, jumping, walking). We assume that we want to construct a model for this task
and to train it using fully labeled training data, where both Y and X are observed.
Figure 20.A.1 illustrates three diﬀerent types of models that have been proposed and used for
sequence labeling, all of which we have seen earlier in this book (see ﬁgure 6.2 and ﬁgure 4.14). The
ﬁrst model is a hidden Markov model (or HMM), which is a purely generative model: the model
hidden Markov
model
generates both the labels Y and the observations X. The second is called a maximum entropy
maximum
entropy Markov
model
Markov model (or MEMM). This model is also directed, but it represents a conditional distribution
P(Y | X); hence, there is no attempt to model a distribution over the X’s. The ﬁnal model
is the conditional random ﬁeld (or CRF) of section 4.6.1. This model also encodes a conditional
conditional
random ﬁeld
distribution; hence the arrows from X to Y . However, here the interactions between the Y are
modeled as undirected edges.
These diﬀerent models present interesting trade-oﬀs in terms of their expressive power and learn-
ability. First, from a computational perspective, HMMs and MEMMs are much more easily learned.
As purely directed models, their parameters can be computed in closed form using either maximum-
likelihood or Bayesian estimation (see chapter 17); conversely, the CRF requires that we use an

20.3. Maximum (Conditional) Likelihood Parameter Estimation
953
Y5
X5
Y5
X5
Y4
X4
Y3
X3
Y2
X2
Y1
X1
Y4
X4
Y3
X3
Y2
X2
Y1
X1
Y5
X5
Y4
X4
Y3
X3
Y2
X2
Y1
X1
(a) HMM
(b) MEMM
(c) CRF
Figure 20.A.1 — Diﬀerent models for sequence labeling: HMM, MEMM, and CRF
iterative gradient-based approach, which is considerably more expensive (particularly here, when
inference must be run separately for every training sequence; see section 20.3.2).
A second important issue relates to our ability to use a rich feature set. As we discussed in
example 16.3 and in box 4.E, our success in a classiﬁcation task often depends strongly on the
quality of our features. In an HMM, we must explicitly model the distribution over the features,
including the interactions between them. This type of model is very hard, and often impossible,
to construct correctly. The MEMM and the CRF are both discriminative models, and therefore they
avoid this challenge entirely.
The third and perhaps subtler issue relates to the independence assumptions made by the model.
As we discussed in section 4.6.1.2, the MEMM makes the independence assumption that (Yi ⊥Xj |
X−j) for any j > i. Thus, an observation from later in the sequence has absolutely no eﬀect on
the posterior probability of the current state; or, in other words, the model does not allow for any
smoothing. The implications of this can be severe in many settings. For example, consider the task
of activity recognition from a video sequence; here, we generally assume that activities are highly
persistent: if a person is walking in one frame, she is also extremely likely to be walking in the next
frame. Now, imagine that the person starts running, but our ﬁrst few observations in the sequence
are ambiguous and consistent with both running and walking. The model will pick one — the
one whose probability given that one frame is highest — which may well be walking. Assuming
that activities are persistent, this choice of activity is likely to stay high for a large number of steps;
the posterior of the initial activity will never change. In other words, the best we can expect is
a prediction where the initial activity is walking, and then (perhaps) transitions to running. The
model is incapable of going back and changing its prediction about the ﬁrst few frames. This
problem has been called the label bias problem.
label bias
problem

To summarize, the trade-oﬀs between these diﬀerent models are subtle and non-
deﬁnitive. In cases where we have many correlated features, discriminative models are
probably better; but, if only limited data are available, the stronger bias of the generative
model may dominate and allow learning with fewer samples. Among the discriminative
models, MEMMs should probably be avoided in cases where many transitions are close to
deterministic. In many cases, CRFs are likely to be a safer choice, but the computational
cost may be prohibitive for large data sets.

954
Chapter 20. Learning Undirected Models
20.3.3
Learning with Missing Data
We now turn to the problem of parameter estimation in the context of missing data. As we
saw in section 19.1, the introduction of missing data introduces both conceptual and technical
diﬃculties. In certain settings, we may need to model explicitly the process by which data
are observed. Parameters may not be identiﬁable from the data. And the likelihood function
becomes signiﬁcantly more complex: there is coupling between the likelihood’s dependence on
diﬀerent parameters; worse, the function is no longer concave and generally has multiple local
maxima.
The same issues regarding observation processes (ones that are not missing at random) and
identiﬁability arise equally in the context of Markov network learning. The issue regarding the
complexity of the likelihood function is analogous, although not quite the same. In the case of
Markov networks, of course, we have coupling between the parameters even in the likelihood
function for complete data. However, as we discuss, in the complete data case, the log-likelihood
function is concave and easily optimized using gradient methods. Once we have missing data,
we lose the concavity of the function and can have multiple local maxima. Indeed, the example
we used was in the context of a Bayesian network of the form X →Y , which can also be
represented as a Markov network. Of course, the parameterization of the two models is not the
same, and so the form of the function may diﬀer. However, one can verify that a function that
is multimodal in one parameterization will also be multimodal in the other.
20.3.3.1
Gradient Ascent
As in the case of Bayesian networks, if we assume our data is missing at random, we can perform
maximum-likelihood parameter estimation by using some form of gradient ascent process to
optimize the likelihood function. Let us therefore begin by analyzing the form of the gradient
in the case of missing data. Let D be a data set where some entries are missing; let o[m] be
the observed entries in the mth data instance and H[m] be the random variables that are the
missing entries in that instance, so that for any h[m] ∈Val(H[m]), (o[m], h[m]) is a complete
assignment to X.
As usual, the average log-likelihood function has the form:
1
M ln P(D | θ)
=
1
M
M
X
m=1
ln

X
h[m]
P(o[m], h[m] | θ)


(20.8)
= 1
M
M
X
m=1
ln

X
h[m]
˜P(o[m], h[m] | θ)

−ln Z.
Now, consider a single term within the summation, P
h[m] ˜P(o[m], h[m] | θ). This expres-
sion has the same form as a partition function; indeed, it is precisely the partition function
for the Markov network that we would obtain by reducing our original Markov network with
the observation o[m], to obtain a Markov network representing the conditional distribution
˜P(H[m] | o[m]). Therefore, we can apply proposition 20.2 and conclude that:
∂
∂θi
ln
X
h[m]
˜P(o[m], h[m] | θ)
=
IEh[m]∼P (H[m]|o[m],θ)[fi],

20.3. Maximum (Conditional) Likelihood Parameter Estimation
955
that is, the gradient of this term is simply the conditional expectation of the feature, given the
observations in this instance.
Putting this together with previous computations, we obtain the following:
Proposition 20.3
For a data set D
∂
∂θi
1
M ℓ(θ : D) = 1
M
" M
X
m=1
IEh[m]∼P (H[m]|o[m],θ)[fi]
#
−IEθ[fi].
(20.9)
In other words, the gradient for feature fi in the case of missing data is the diﬀerence between
two expectations — the feature expectation over the data and the hidden variables minus the
feature expectation over all of the variables.
It is instructive to compare the cost of this computation to that of computing the gradient
in equation (20.4). For the latter, to compute the second term in the derivative, we need to
run inference once, to compute the expected feature counts relative to our current distribution
P(X | θ). The ﬁrst term is computed by simply aggregating the feature over the data. By
comparison, to compute the derivative here, we actually need to run inference separately for
every instance m, conditioning on o[m]. Although inference in the reduced network may be
simpler (since reduced factors are simpler), the cost of this computation is still much higher
than learning without missing data. Indeed, not surprisingly, the cost here is comparable to the
cost of a single iteration of gradient descent or EM in Bayesian network learning.
20.3.3.2
Expectation Maximization
As for any other probabilistic model, an alternative method for parameter estimation in context
of missing data is via the expectation maximization algorithm. In the case of Bayesian network
learning, EM seemed to have signiﬁcant advantages. Can we deﬁne a variant of EM for Markov
networks? And does it have the same beneﬁts?
The answer to the ﬁrst question is clearly yes. We can perform an E-step by using our current
parameters θ(t) to compute the expected suﬃcient statistics, in this case, the expected feature
counts. That is, at iteration t of the EM algorithm, we compute, for each feature fi, the expected
suﬃcient statistic:
¯
Mθ(t)[fi] = 1
M
" M
X
m=1
IEh[m]∼P (H[m]|o[m],θ)[fi]
#
.
With these expected feature counts, we can perform an M-step by doing maximum likelihood
parameter estimation.
The proofs of convergence and other properties of the algorithm go
through unchanged.
Here, however, there is one critical diﬀerence. Recall that, in the case of directed models,
given the expected suﬃcient statistics, we can perform the M-step eﬃciently, in closed form. By
contrast, the M-step for Markov networks requires that we run inference multiple times, once for
each iteration of whatever gradient ascent procedure we are using. At step k of this “inner-loop”
optimization, we now have a gradient of the form:
¯
Mθ(t)[fi] −IEθ(t,k)[fi].

956
Chapter 20. Learning Undirected Models
The trade-oﬀs between the two algorithms are now more subtle than in the case of Bayesian
networks.
For the joint gradient ascent procedure of the previous section, we need to run
inference M + 1 times in each gradient step: once without evidence, and once for each data
case. If we use EM, we run inference M times to compute the expected suﬃcient statistics in
the E-step, and then once for each gradient step, to compute the second term in the gradient.
Clearly, there is a computational savings here. However, each of these gradient steps now uses
an “out-of-date” set of expected suﬃcient statistics, making it increasingly less relevant as our
optimization proceeds.
In fact, we can view the EM algorithm, in this case, as a form of caching of the ﬁrst term in
the derivative: Rather than compute the expected counts in each iteration, we compute them
every few iterations, take a number of gradient steps, and then recompute the expected counts.
There is no need to run the “inner-loop” optimization until convergence; indeed, that strategy is
often not optimal in practice.
20.3.4
Maximum Entropy and Maximum Likelihood ⋆
We now return to the case of basic maximum likelihood estimation, in order to derive an
alternative formulation that provides signiﬁcant insight. In particular, we now use theorem 20.1
to relate maximum likelihood estimation in log-linear models to another important class or
problems examined in statistics: the problem of ﬁnding the distribution of maximum entropy
subject to a set of constraints.
To motivate this alternative formulation, consider a situation where we are given some sum-
mary statistics of an empirical distribution, such as those that may be published in a census
report. These statistics may include the marginal distributions of single variables, of certain
pairs, and perhaps of other events that the researcher summarizing the data happened to con-
sider of interest. As another example, we might know the average ﬁnal grade of students in
the class and the correlation of their ﬁnal grade with their homework scores. However, we do
not have access to the full data set. While these two numbers constrain the space of possible
distributions over the domain, they do not specify it uniquely. Nevertheless, we might want to
construct a “typical” distribution that satisﬁes the constraints and use it to answer other queries.
One compelling intuition is that we should select a distribution that satisﬁes the given con-
straints but has no additional “structure” or “information.” There are many ways of making this
intuition precise. One that has received quite a bit of attention is based on the intuition that
entropy is the inverse of information, so that we should search for the distribution of highest
entropy. (There are more formal justiﬁcations for this intuition, but these are beyond the scope
of this book.) More formally, in maximum entropy estimation, we solve the following problem:
maximum
entropy
Maximum-Entropy:
Find
Q(X)
maximizing
IHQ(X)
subject to
IEQ[fi] = IED[fi]
i = 1, . . . , k.
(20.10)
The constraints of equation (20.10) are called expectation constraints, since they constrain us
expectation
constraints
to the set of distributions that have a particular set of expectations. We know that this set is

20.3. Maximum (Conditional) Likelihood Parameter Estimation
957
non-empty, since we have one example of a distribution that satisﬁes these constraints — the
empirical distribution.
Somewhat surprisingly, the solution to this problem is a Gibbs distribution over the features
F that matches the given expectations.
Theorem 20.2
The distribution Q∗is the maximum entropy distribution satisfying equation (20.10) if and only if
Q∗= Pˆθ, where
Pˆθ(X) =
1
Z(ˆθ)
exp
(X
i
ˆθifi(X)
)
and ˆθ is the maximum likelihood parameterization relative to D.
Proof For notational simplicity, let P = Pˆθ.
From theorem 20.1, it follows that IEP [fi] =
IED[fi(X)] for i = 1, . . . , k, and hence that P satisﬁes the constraints of equation (20.10).
Therefore, to prove that P = Q∗, we need only show that IHP (X) ≥IHQ(X) for all other
distributions Q that satisfy these constraints. Consider any such distribution Q.
From proposition 8.1, it follows that:
IHP (X) = −
X
i
ˆθiIEP [fi] + ln Z(θ).
(20.11)
Thus,
IHP (X) −IHQ(X)
=
−
"X
i
ˆθiIEP [fi(X)]
#
+ ln ZP −IEQ[−ln Q(X)]
(i)
=
−
"X
i
ˆθiIEQ[fi(X)]
#
+ ln ZP + IEQ[ln Q(X)]
=
IEQ[−ln P(X)] + IEQ[ln Q(X)]
=
ID(Q||P) ≥0,
where (i) follows from the fact that both Pˆθ and Q satisfy the constraints, so that IEPˆθ[fi] =
IEQ[fi] for all i.
We conclude that IHPˆθ(X) ≥IHQ(X) with equality if and only if Pˆθ = Q.
Thus, the
maximum entropy distribution Q∗is necessarily equal to Pˆθ, proving the result.
One can also provide an alternative proof of this result based on the concept of duality
duality
discussed in appendix A.5.4. Using this alternative derivation, one can show that the two prob-
lems, maximizing the entropy given expectation constraints and maximizing the likelihood given
structural constraints on the distribution, are convex duals of each other. (See exercise 20.5.)
Both derivations show that these objective functions provide bounds on each other, and are
identical at their convergence point. That is, for the maximum likelihood parameters ˆθ,
IHPˆθ(X) = −1
M ℓ(ˆθ : D).

958
Chapter 20. Learning Undirected Models
As a consequence, we see that for any set of parameters θ and for any distribution Q that
satisfy the expectation constraints equation (20.10), we have that
IHQ(X) ≤IHPˆθ(X) = −1
M ℓ(ˆθ : D) ≤−1
M ℓ(θ : D)
with equality if and only if Q = Pθ. We note that, while we provided a proof for this result
from ﬁrst principles, it also follows directly from the theory of convex duality.
Our discussion has shown an entropy dual only for likelihood. A similar connection can be
shown between conditional likelihood and conditional entropy; see exercise 20.6.
20.4
Parameter Priors and Regularization
So far, we have focused on maximum likelihood estimation for selecting parameters in a Markov
network. However, as we discussed in chapter 17, maximum likelihood estimation (MLE) is prone
to overﬁtting to the training data. Although the eﬀects are not as transparent in this case (due
to the lack of direct correspondence between empirical counts and parameters), overﬁtting of
the maximum likelihood estimator is as much of a problem here.
As for Bayesian networks, we can reduce the eﬀect of overﬁtting by introducing a prior
distribution P(θ) over the model parameters. Note that, because we do not have a decomposable
closed form for the likelihood function, we do not obtain a decomposable closed form for the
posterior in this case. Thus, a fully Bayesian approach, where we integrate out the parameters
to compute the next prediction, is not generally feasible in Markov networks. However, we can
aim to perform MAP estimation — to ﬁnd the parameters that maximize P(θ)P(D | θ).
MAP estimation
Given that we have no constraints on the conjugacy of the prior and the likelihood, we can
consider virtually any reasonable distribution as a possible prior. However, only a few priors
have been applied in practice.
20.4.1
Local Priors
Most commonly used is a Gaussian prior on the log-linear parameters θ. The most standard
form of this prior is simply a zero-mean diagonal Gaussian, usually with equal variances for
each of the weights:
P(θ | σ2) =
k
Y
i=1
1
√
2πσ exp

−θ2
i
2σ2

,
for some choice of the variance σ2. This variance is a hyperparameter, as were the αi’s in
hyperparameter
the Dirichlet distribution (section 17.3.2). Converting to log-space (in which the optimization is
typically done), this prior gives rise to a term of the form:
−1
2σ2
k
X
i=1
θ2
i ,
This term places a quadratic penalty on the magnitude of the weights, where the penalty is
measured in Euclidean, or L2-norm, generally called an L2-regularization term. This term is
L2-regularization

20.4. Parameter Priors and Regularization
959
0.5
0.4
0.3
0.2
0.1
0
–10
10
–5
5
0
Figure 20.3
Laplacian distribution (β = 1) and Gaussian distribution (σ2 = 1)
concave, and therefore it gives rise to a concave objective, which can be optimized using the
same set of methods as standard MLE.
A diﬀerent prior that has been used in practice uses the zero-mean Laplacian distribution,
Laplacian
distribution
which, for a single parameter, has the form
PLaplacian(θ | β) = 1
2β exp

−|θ|
β

.
(20.12)
One example of the Laplacian distribution is shown in ﬁgure 20.3; it has a nondiﬀerentiable
point at θ = 0, arising from the use of the absolute value in the exponent. As for the Gaussian
case, one generally assumes that the diﬀerent parameters θi are independent, and often (but
not always) that they are identically distributed with the same hyperparameter β. Taking the
logarithm, we obtain a term
−1
β
k
X
i=1
|θi|
that also penalizes weights of high magnitude, measured using the L1-norm. Thus, this approach
is generally called L1-regularization.
L1-regularization
Both forms of regularization penalize parameters whose magnitude (positive or negative)
is large.
Why is a bias in favor of parameters of low magnitude a reasonable one?
Recall
from our discussion in section 17.3 that a prior often serves to pull the distribution toward an
“uninformed” one, smoothing out ﬂuctuations in the data. Intuitively, a distribution is “smooth”
if the probabilities assigned to diﬀerent assignments are not radically diﬀerent. Consider two
assignments ξ and ξ′; their relative probability is
P(ξ)
P(ξ′) =
˜P(ξ)/Zθ
˜P(ξ′)/Zθ
=
˜P(ξ)
˜P(ξ′)
.

960
Chapter 20. Learning Undirected Models
Moving to log-space and expanding the unnormalized measure ˜P, we obtain:
ln P(ξ)
P(ξ′)
=
k
X
i=1
θifi(ξ) −
k
X
i=1
θifi(ξ′)
=
k
X
i=1
θi(fi(ξ) −fi(ξ′)).
When all of the θi’s have small magnitude, this log-ratio is also bounded, resulting in a smooth
distribution. Conversely, when the parameters can be large, we can obtain a “spiky” distribution
with arbitrarily large diﬀerences between the probabilities of diﬀerent assignments.
In both the L2 and the L1 case, we penalize the magnitude of the parameters.
In the
Gaussian case, the penalty grows quadratically with the parameter magnitude, implying that an
increase in magnitude in a large parameter is penalized more than a similar increase in a small
parameter. For example, an increase in θi from 0 to 0.1 is penalized less than an increase
from 3 to 3.1. In the Laplacian case, the penalty is linear in the parameter magnitude, so that
the penalty growth is invariant over the entire range of parameter values. This property has
important ramiﬁcations. In the quadratic case, as the parameters get close to 0, the eﬀect of the
penalty diminishes. Hence, the models that optimize the penalized likelihood tend to have many
small weights. Although the resulting models are smooth, as desired, they are structurally quite
dense. By comparison, in the L1 case, the penalty is linear all the way until the parameter value
is 0. This penalty provides a continued incentive for parameters to shrink until they actually
hit 0. As a consequence, the models learned with an L1 penalty tend to be much sparser

than those learned with an L2 penalty, with many parameter weights achieving a value
of 0. From a structural perspective, this eﬀect gives rise to models with fewer edges and
sparser potentials, which are potentially much more tractable. We return to this issue in
section 20.7.
Importantly, both the L1 and L2 regularization terms are concave. Because the log-likelihood
is also concave, the resulting posterior is concave, and can therefore be optimized eﬃciently
using the gradient-based methods we described for the likelihood case. Moreover, the introduc-
tion of these penalty terms serves to reduce or even eliminate multiple (equivalent) optima that
arise when the parameterization of the network is redundant. For example, consider the trivial
example where we have no data. In this case, the maximum likelihood solution is (as desired)
the uniform distribution. However, due to redundancy, there is a continuum of parameteriza-
tions that give rise to the uniform distribution. However, when we introduce either of the earlier
prior distributions, the penalty term drives the parameters toward zero, giving rise to the unique
optimum θ = 0. Although one can still construct examples where multiple optima occur, they
are very rare in practice. Conversely, methods that eliminate redundancies by reexpressing some
of the parameters in terms of others can produce undesirable interactions with the regularization
terms, giving rise to priors where some parameters are penalized more than others.
The regularization hyperparameters — σ2 in the L2 case, and β in the L1 case — encode the
strength in our belief that the model weights should be close to 0. The larger these parameters
(both in the denominator), the broader our parameter prior, and the less strong our bias toward
0. In principle, any choice of hyperparameter is legitimate, since a prior is simply a reﬂection
of our beliefs. In practice, however, the choice of prior can have a signiﬁcant eﬀect on

the quality of our learned model. A standard method for selecting this parameter is via a

20.5. Learning with Approximate Inference
961
cross-validation procedure, as described in box 16.A: We repeatedly partition the training set,
learn a model over one part with some choice of hyperparameter, and measure the performance
of the learned model (for example, log-likelihood) on the held-out fragment.
20.4.2
Global Priors
An alternative approach for deﬁning priors is to search for a conjugate prior. Examining the
conjugate prior
likelihood function, we see that the posterior over parameters has the following general form:
P(θ | D)
∝
P(θ)P(D | θ)
=
P(θ) exp
(X
i
MIED[fi]θi −M ln Z(θ)
)
.
This expression suggests that we use a family of prior distributions of the form:
P(θ) ∝exp
(X
i
M0αiθi −M0 ln Z(θ)
)
.
This form deﬁnes a family of priors with hyperparameters {αi}.
It is easy to see that the
posterior is from the same family with α′
i = αi + IED[fi] and M ′
0 = M0 + M, so that this prior
is conjugate to the log-linear model likelihood function.
We can think of the hyperparameters {αi} as specifying the suﬃcient statistics from prior
observations and of M0 as specifying the number of these prior observations. This formulation is
quite similar to the use of pseudocounts in the BDe priors for directed models (see section 17.4.3).
The main diﬀerence from directed models is that this conjugate family (both the prior and the
likelihood) does not decompose into independent priors for the diﬀerent features.
20.5
Learning with Approximate Inference
The methods we have discussed here assume that we are able to compute the partition function
Z(θ) and expectations such as IEPθ[fi].
In many real-life applications the structure of the
network does not allow for exact computation of these terms. For example, in applications
to image segmentation (box 4.B), we generally use a grid-structured network, which requires
exponential size clusters for exact inference.
The simplest approach for learning in intractable networks is to apply the learning procedure
(say, conjugate gradient ascent) using an approximate inference procedure to compute the re-
quired queries about the distribution Pθ. This view decouples the question of inference from
learning and treats the inference procedure as a black box during learning. The success of
such an approach depends on whether the approximation method interferes with the learning.
In particular, nonconvergence of the inference method, or convergence to approximate

answers, can lead to inaccurate and even oscillating estimates of the gradient, potentially
harming convergence of the overall learning algorithm. This type of situation can arise
both in particle-based methods (say MCMC sampling) and in global algorithms such as belief
propagation. In this section, we describe several methods that better integrate the inference into
the learning outer loop in order to reduce problems such as this.

962
Chapter 20. Learning Undirected Models
A second approach for dealing with inference-induced costs is to come up with alternative
(possibly approximate) objective functions whose optimization does not require (as much) in-
ference.
Some of these techniques are reviewed in the next section.
However, one of the
main messages of this section is that the boundary between these two classes of methods is
surprisingly ambiguous.
Approximately optimizing the likelihood objective by using an

approximate inference algorithm to compute the gradient can often be reformulated as
exactly optimizing an approximate objective.
When applicable, this view is often more
insightful and also more usable. First, it provides more insight about the outcome of the opti-
mization. Second, it may allow us to bound the error in the optimum in terms of the distance
between the two functions being optimized.
Finally, by formulating a clear objective to be
optimized, we can apply any applicable optimization algorithm, such as conjugate gradient or
Newton’s method.
Importantly, while we describe the methods in this section relative to the plain likelihood
objective, they apply almost without change to the generalizations and extensions we describe
in this chapter: conditional Markov networks; parameter priors and regularization; structure
learning; and learning with missing data.
20.5.1
Belief Propagation
A fairly popular approach for approximate inference is the belief propagation algorithm and its
belief
propagation
variants. Indeed, in many cases, an algorithm in this family would be used for inference in
the model resulting from the learning procedure. In this case, it can be shown that we should

learn the model using the same inference algorithm that will be used for querying it.
Indeed, it can be shown that using a model trained with the same approximate inference
algorithm is better than using a model trained with exact inference.
At ﬁrst glance, the use of belief propgation for learning appears straightforward. We can
simply run BP within every iteration of gradient ascent to compute the expected feature counts
used in the gradient computation. Due to the family preservation property, each feature fi
must be a subset of a cluster Ci in the cluster graph. Hence, to compute the expected feature
count IEθ[fi], we can compute the BP marginals over Ci, and then compute the expectation.
In practice, however, this approach can be highly problematic. As we have seen, BP often does
not converge. The marginals that we derive from the algorithm therefore oscillate, and the ﬁnal
results depend on the point at which we choose to stop the algorithm. As a result, the gradient
computed from these expected counts is also unstable.
This instability can be a signiﬁcant
unstable gradient
problem in a gradient-based procedure, since it can gravely hurt the convergence properties of
the algorithm. This problem is even more severe in the context of line-search methods, where
the function evaluations can be inconsistent at diﬀerent points in the line search.
There are several solutions to this problem: One can use one of the convergent alternatives
to the BP algorithm that still optimizes the same Bethe energy objective; one can use a convex
energy approximation, such as those of section 11.3.7.2; or, as we now show, one can reformulate
the task of learning with approximate inference as optimizing an alternative objective, allowing
the use of a range of optimization methods with better convergence properties.

20.5. Learning with Approximate Inference
963
20.5.1.1
Pseudo-moment Matching
Let us begin by a simple analysis of the ﬁxed points of the learning algorithm. At convergence,
the approximate expectations must satisfy the condition of theorem 20.1; in particular, the
converged BP beliefs for Ci must satisfy
IEβi(Ci)[fCi] = IED[fi(Ci)].
Now, let us consider the special case where our feature model deﬁnes a set of fully param-
eterized potentials that precisely match the clusters used in the BP cluster graph. That is, for
every cluster Ci in the cluster graph, and every assignment cj
i to Ci, we have a feature which
is an indicator function 11{cj
i}, that is, it is 1 when Ci = cj
i and 0 otherwise. In this case, the
preceding set of equalities imply that, for every assignment cj
i to Ci, we have that
βi(cj
i) = ˆP(cj
i).
(20.13)
That is, at convergence of the gradient ascent algorithm, the convergence point of the underlying
belief propagation must be to a set of beliefs that exactly matches the empirical marginals in
the data. But if we already know the outcome of our convergence, there is no point to running
the algorithm!
This derivation gives us a closed form for the BP potentials at the point when both algorithms
— BP inference and parameter gradient ascent — have converged. As we have already discussed,
the full-table parameterization of Markov network potentials is redundant, and therefore there
are multiple solutions that can give rise to this set of beliefs. One of these solutions can be
obtained by dividing each sepset in the calibrated cluster graph into one of the adjacent clique
potentials. More precisely, for each sepset Si,j between Ci and Cj, we select the endpoint for
which i < j (in some arbitrary ordering), and we then deﬁne:
φi ←βi
µi,j
.
We perform this transformation for each sepset.
We use the ﬁnal set of potentials as the
parameterization for our Markov network. We can show that a single pass of message passing in
a particular order gives rise to a calibrated cluster graph whose potentials are precisely the ones
in equation (20.13). Thus, in this particular special case, we can provide a closed-form solution
to both the inference and learning problem. This approach is called pseudo-moment matching.
pseudo-moment
matching
While it is satisfying that we can ﬁnd a solution so eﬀectively, the form of the solution should
be considered with care. In particular, we note that the clique potentials are simply empirical
cluster marginals divided by empirical sepset marginals. These quantities depend only on the
local structure of the factor and not on any global aspect of the cluster graph, including its
structure. For example, the BC factor is estimated in exactly the same way within the diamond
network of ﬁgure 11.1a and within the chain network A—B—C—D. Of course, potentials are
also estimated locally in a Bayesian network, but there the local calibration ensures that the
distribution can be factorized using purely local computations. As we have already seen, this
is not the case for Markov networks, and so we expect diﬀerent potentials to adjust to ﬁt each
other; however, the estimation using loopy BP does not accommodate that. In a sense, this
observation is not surprising, since the BP approach also ignores the more global information.

964
Chapter 20. Learning Undirected Models
We note, however, that this purely local estimation of the parameters only holds under the
very restrictive conditions described earlier. It does not hold when we have parameter priors
(regularization), general features rather than table factors, any type of shared parameters (as in
section 6.5), or conditional random ﬁelds. We discuss this more general case in the next section.
20.5.1.2
Belief Propagation and Entropy Approximations ⋆
We now provide a more general derivation that allows us to reformulate maximum-likelihood
learning with belief propagation as a uniﬁed optimization problem with an approximate objec-
tive. This perspective opens the door to the use of better approximation algorithms.
Our analysis starts from the maximum-entropy dual of the maximum-likelihood problem.
maximum
entropy
Maximum-Entropy:
Find
Q(X)
maximizing
IHQ(X)
subject to
IEQ[fi] = IED[fi]
i = 1, . . . , k.
We can obtain a tractable approximation to this problem by applying the same sequence
of transformations that we used in section 11.3.6 to derive belief propagation from the energy
optimization problem. More precisely, assume we have a cluster graph U consisting of a set
of clusters {Ci} connected by sepsets Si,j. Now, rather than optimize Maximum-Entropy over
the space of distributions Q, we optimize over the set of possible pseudo-marginals in the local
local consistency
polytope
consistency polytope Local[U], as deﬁned in equation (11.16). Continuing as in the BP derivation,
we also approximate the entropy as in its factored form (deﬁnition 11.1):
factored entropy
IHQ(X) ≈
X
Ci∈U
IHβi(Ci) −
X
(Ci—Cj)∈U
IHµi,j(Si,j).
(20.14)
As before, this reformulation is exact when the cluster graph is a tree but is approximate
otherwise.
Putting these approximations together, we obtain the following approximation to the maximum-
entropy optimization problem:
Approx-Maximum-Entropy:
Find
Q
maximizing
P
Ci∈U IHβi(Ci) −P
(Ci—Cj)∈U IHµi,j(Si,j)
subject to
IEβi[fi]
=
IED[fi]
i = 1, . . . , k
(20.15)
Q
∈
Local[U].
This approach is called CAMEL, for constrained approximate maximum enropy learning.
CAMEL

20.5. Learning with Approximate Inference
965
Example 20.2
To illustrate this reformulation, consider a simple pairwise Markov network over the binary variables
A, B, C, with three clusters: C1 = {A, B}, C2 = {B, C}, C3 = {A, C}. We assume that the
log-linear model is deﬁned by the following two features, both of which are shared over all clusters:
f00(x, y) = 1 if x = 0 and y = 0, and 0 otherwise; and f11(x, y) = 1 if x = 1 and y = 1.
Assume we have 3 data instances [0, 0, 0], [0, 1, 0], [1, 0, 0]. The unnormalized empirical counts of
each feature, pooled over all clusters, is then IE ˆ
P [f00] = (3 + 1 + 1)/3 = 5/3, IE ˆ
P [f11] = 0. In
this case, the optimization of equation (20.15) would take the following form:
Find
Q = {β1, β2, β3, µ1,2, µ2,3, µ1,3}
maximizing
IHβ1(A, B) + IHβ2(B, C) + IHβ3(A, C)
−IHµ1,2(B) −IHµ2,3(C) −IHµ1,3(A)
subject to
X
i
IEβi[f00]
=
5/3
X
i
IEβi[f11]
=
0
X
a
β1(a, b) −
X
c
β2(b, c)
=
0
∀b
X
b
β2(b, c) −
X
a
β3(a, c)
=
0
∀c
X
c
β3(a, c) −
X
b
β1(a, b)
=
0
∀a
X
ci
βi(ci)
=
1
i = 1, 2, 3
βi
≥
0
i = 1, 2, 3.
The CAMEL optimization problem of equation (20.15) is a constrained maximization problem
with linear constraints and a nonconcave objective. The problem actually has two distinct sets
of constraints: the ﬁrst set encodes the moment-matching constraints and comes from the
learning problem; and the second set encodes the constraint that Q be in the marginal polytope
and arises from the cluster-graph approximation. It thus forms a uniﬁed optimization problem
that encompasses both the learning task — moment matching — and the inference task —
obtaining a set of consistent pseudo-marginals over a cluster graph. Analogously, if we introduce
Lagrange multipliers for these constraints (as in appendix A.5.3), they would have very diﬀerent
interpretations. The multipliers for the ﬁrst set of constraints would correspond to weights θ in
the log-linear model, as in the max-likelihood / max-entropy duality ; those in the second set
would correspond to messages δi→j in the cluster graph, as in the BP algorithm.
This observation leads to several solution algorithms for this problem. In one class of methods,
we could introduce Lagrange multipliers for all of the constraints and then optimize the resulting
problem over these new variables. If we perform the optimization by a double-loop algorithm
where the outer loop optimizes over θ (say using gradient ascent) and the inner loops “optimizes”
the δi→j by iterating their ﬁxed point equations, the result would be precisely gradient ascent
over parameters with BP in the inner loop for inference.

966
Chapter 20. Learning Undirected Models
20.5.1.3
Sampling-Based Learning ⋆
The partition function Z(θ) is a summation over an exponentially large space. One approach
to approximating this summation is to reformulate it as an expectation with respect to some
distribution Q(X):
Z(θ)
=
X
ξ
exp
(X
i
θifi(ξ)
)
=
X
ξ
Q(ξ)
Q(ξ) exp
(X
i
θifi(ξ)
)
=
IEQ
"
1
Q(X) exp
(X
i
θifi(X)
)#
.
This is precisely the form of the importance sampling estimator described in section 12.2.2. Thus,
importance
sampling
we can approximate it by generating samples from Q, and correcting appropriately via weights.
We can simplify this expression if we choose Q to be Pθ0 for some set of parameters θ0:
Z(θ)
=
IEPθ0
"
Z(θ0) exp {P
i θifi(X)}
exp {P
i θ0
i fi(X)}
#
=
Z(θ0)IEPθ0
"
exp
(X
i
(θi −θ0
i )fi(X)
)#
.
If we can sample instances ξ1, . . . , ξK from Pθ0, we can approximate the log-partition
function as:
ln Z(θ) ≈ln
 
1
K
K
X
k=1
exp
(X
i
(θi −θ0
i )fi(ξk)
)!
+ ln Z(θ0).
(20.16)
We can plug this approximation of ln Z(θ) into the log-likelihood of equation (20.3) and op-
timize it. Note that ln Z(θ0) is a constant that we can ignore in the optimization, and the
resulting expression is therefore a simple function of θ, which can be optimized using methods
such as gradient ascent or one of its extensions. Interestingly, gradient ascent over θ relative to
equation (20.16) is equivalent to utilizing an importance sampling estimator directly to approxi-
mate the expected counts in the gradient of equation (20.4) (see exercise 20.12). However, as we
discussed, it is generally more instructive and useful to view such methods as exactly optimizing
an approximate objective rather than approximately optimizing the exact likelihood.
Of course, as we discussed in section 12.2.2, the quality of an importance sampling estima-
tor depends on the diﬀerence between θ and θ0: the greater the diﬀerence, the larger the
variance of the importance weights. Thus, this type of approximation is reasonable only in a
neighborhood surrounding θ0.
How do we use this approximation? One possible strategy is to iterate between two steps.
In one we run a sampling procedure, such as MCMC, to generate samples from the current
MCMC
parameter set θt. Then in the second iteration we use some gradient procedure to ﬁnd θt+1

20.5. Learning with Approximate Inference
967
that improve the approximate log-likelihood based on these samples. We can then regenerate
samples and repeat the process. As the samples are regenerated from a new distribution, we
can hope that they are generated from a distribution not too far from the one we are currently
optimizing, maintaining a reasonable approximation.
20.5.2
MAP-Based Learning ⋆
As another approximation to the inference step in the learning algorithm, we can consider
approximating the expected feature counts with their counts in the single MAP assignment
MAP assignment
to the current Markov network. As we discussed in chapter 13, in many classes of models,
computing a single MAP assignment is a much easier computational task, making this a very
appealing approach in many settings.
More precisely, to approximate the gradient at a given parameter assignment θ, we compute
IED[fi(X)] −fi(ξMAP(θ)),
(20.17)
where ξMAP(θ) = arg maxξ P(ξ | θ) is the MAP assignment given the current set of parameters
θ. This approach is also called Viterbi training.
Viterbi training
Once again, we can gain considerable intuition by reformulating this approximate inference
step as an exact optimization of an approximate objective. Some straightforward algebra shows
that this gradient corresponds exactly to the approximate objective
1
M ℓ(θ : D) −ln P(ξMAP(θ) | θ),
(20.18)
or, due to the cancellation of the partition function:
1
M
M
X
m=1
ln ˜P(ξ[m] | θ) −ln ˜P(ξMAP(θ) | θ).
(20.19)
To see this, consider a single data instance ξ[m]:
ln P(ξ[m] | θ)−ln P(ξMAP(θ) | θ)
=
[ln ˜P(ξ[m] | θ) −ln Z(θ)] −[ln ˜P(ξMAP(θ) | θ) −ln Z(θ)]
=
ln ˜P(ξ[m] | θ) −ln ˜P(ξMAP(θ) | θ)
=
X
i
θi[fi(ξ[m]) −fi(ξMAP(θ))].
If we average this expression over all data instances and take the partial derivative relative to θi,
we obtain an expression whose gradient is precisely equation (20.17).
The ﬁrst term in equation (20.19) is an average of expressions of the form ln ˜P(ξ | θ). Each
such expression is a linear function in θ, and hence their average is also linear in θ. The
second term, ˜P(ξMAP(θ) | θ), may appear to be the log-probability of an instance. However, as
indicated by the notation, ξMAP(θ) is itself a function of θ: in diﬀerent regions of the parameter
space, the MAP assignment changes. In fact, this term is equal to:
ln P(ξMAP(θ) | θ) = max
ξ
ln P(ξ | θ).

968
Chapter 20. Learning Undirected Models
This is a maximum of linear functions, which is a convex, piecewise-linear function. Therefore,
its negation is concave, and so the entire objective of equation (20.19) is also concave and hence
has a global optimum.
Although reasonable at ﬁrst glance, a closer examination reveals some important issues with
this objective.
Consider again a single data instance ξ[m].
Because ξMAP(θ) is the MAP
assignment, it follows that ln P(ξ[m] | θ) ≤ln P(ξMAP(θ) | θ), and therefore the objective
is always nonpositive. The maximal value of 0 can be achieved in two ways. The ﬁrst is if we
manage to ﬁnd a setting of θ in which the empirical feature counts match the feature counts in
ξMAP(θ). This optimum may be hard to achieve: Because the counts in ξMAP(θ) are discrete,
they take on only a ﬁnite set of values; for example, if we have a feature that is an indicator
function for the event Xi = xi, its count can take on only the values 0 or 1, depending on
whether the MAP assignment has Xi = xi or not. Thus, we may never be able to match the
feature counts exactly. The second way of achieving the optimal value of 0 is to set all of the
parameters θi to 0. In this case, we obtain the uniform distribution over assignments, and the
objective achieves its maximum value of 0. This possible behavior may not be obvious when we
consider the gradient, but it becomes apparent when we consider the objective we are trying to
optimize.
That said, we note that in the early stages of the optimization, when the expected counts are
far from the MAP counts, the gradient still makes progress in the general direction of increasing
the relative log-probability of the data instances. This approach can therefore work fairly well in
practice, especially if not optimized to convergence.
Box 20.B — Case Study: CRFs for Protein Structure Prediction. One interesting application
of CRFs is to the task of predicting the three-dimensional structure of proteins. Proteins are con-
protein structure
structed as chains of residues, each containing one of twenty possible amino acids. The amino acids
are linked together into a common backbone structure onto which amino-speciﬁc side-chains are
attached. An important computational problem is that of predicting the side-chain conformations
given the backbone. The full conﬁguration for a side-chain consists of up to four angles, each of
which takes on a continuous value. However, in practice, angles tend to cluster into bins of very
similar angles, so that the common practice is to discretize the value space of each angle into a
small number (usually up to three) bins, called rotamers.
With this transformation, side-chain prediction can be formulated as a discrete optimization
problem, where the objective is an energy over this discrete set of possible side-chain conformations.
Several energy functions have been proposed, all of which include various repulsive and attractive
terms between the side-chain angles of nearby residues, as well as terms that represent a prior
and internal constraints within the side chain for an individual residue. Rosetta, a state-of-the-
art system, uses a combination of eight energy terms, and uses simulated annealing to search
for the minimal energy conﬁguration. However, even this highly engineered system still achieves
only moderate accuracies (around 72 percent of the discretized angles predicted correctly).
An
obvious question is whether the errors are due to suboptimal answers returned by the optimization
algorithm, or to the design of the energy function, which may not correctly capture the true energy
“preferences” of protein structures.
Yanover, Schueler-Furman, and Weiss (2007) propose to address this optimization problem using
MAP inference techniques. The energy functions used in this type of model can also be viewed as the

20.6. Alternative Objectives
969
log-potentials of a Markov network, where the variables represent the diﬀerent angles to be inferred,
and their values the discretized rotamers. The problem of ﬁnding the optimal conﬁguration is then
simply the MAP inference problem, and can be tackled using some of the algorithms described in
chapter 13. Yanover et al. show that the TRW algorithm of box 13.A ﬁnds the provably global
optimum of the Rosetta energy function for approximately 85 percent of the proteins in a standard
benchmark set; this computation took only a few minutes per protein on a standard workstation.
They also tackled the problem by directly solving the LP relaxation of the MAP problem using
a commercial LP solver; this approach found the global optimum of the energy function for all
proteins in the test set, but at a higher computational cost. However, ﬁnding the global minimum
gave only negligible improvements on the actual accuracy of the predicted angles, suggesting that
the primary source of inaccuracy in these models is in the energy function, not the optimization.
Thus, this problem seems like a natural candidate for the application of learning methods. The
task was encoded as a CRF, whose input is a list of amino acids that make up the protein as
well as the three-dimensional shape of the backbone. Yanover et al. encoded this distribution as a
log-linear model whose features were the (eight) diﬀerent components of the Rosetta energy function,
and whose parameters were the weights of these features. Because exact inference for this model is
intractable, it was trained by using a TRW variant for sum-product algorithms (see section 11.3.7.2).
This variant uses a set of convex counting numbers to provide a convex approximation, and a
lower bound, to the log-partition function. These properties guarantee that the learning process is
stable and is continually improving a lower bound on the true objective. This new energy function
improves performance from 72 percent to 78 percent, demonstrating that learning can signiﬁcantly
improve models, even those that are carefully engineered and optimized by a human expert. Notably,
for the learned energy function, and for other (yet more sophisticated) energy functions, the use of
globally optimal inference does lead to improvements in accuracy. Overall, a combination of these
techniques gave rise to an accuracy of 82.6 percent, a signiﬁcant improvement.
20.6
Alternative Objectives
Another class of approximations can be obtained directly by replacing the objective that we aim
to optimize with one that is more tractable. To motivate the alternative objectives we present in
this chapter, let us consider again the form of the log-likelihood objective, focusing, for simplicity,
on the case of a single data instance ξ:
ℓ(θ : ξ)
=
ln ˜P(ξ | θ) −ln Z(θ)
=
ln ˜P(ξ | θ) −ln

X
ξ′
˜P(ξ′ | θ)

.
Considering the ﬁrst term, this objective aims to increase the log-measure (logarithm of the
unnormalized probability) of the observed data instance ξ. Of course, because the log-measure
is a linear function of the parameters in our log-linear representation, that goal can be achieved
simply by increasing all of the parameters associated with positive empirical expectations in ξ,
and decreasing all of the parameters associated with negative empirical expectations. Indeed,

970
Chapter 20. Learning Undirected Models
we can increase the ﬁrst term unboundedly using this approach. The second term, however,
balances the ﬁrst, since it is the logarithm of a sum of the unnormalized measures of instances,
in this case, all possible instances in Val(X). In a sense, then, we can view the log-likelihood
objective as aiming to increasing the distance between the log-measure of ξ and the aggregate
of the measures of all instances.
We can thus view it as contrasting two terms.
The key
diﬃculty with this formulation, of course, is that the second term involves a summation over
the exponentially many instances in Val(X), and therefore requires inference in the network.
This formulation does, however, suggest one approach to approximating this objective: perhaps

we can still move our parameters in the right direction if we aim to increase the diﬀerence
between the log-measure of the data instances and a more tractable set of other instances,
one that does not require summation over an exponential space. The contrastive objectives
contrastive
objective
that we describe in this section all take that form.
20.6.1
Pseudolikelihood and Its Generalizations
Perhaps the earliest method for circumventing the intractability of network inference is the
pseudolikelihood objective. As one motivation for this approximation, consider the likelihood of
a single instance ξ. Using the chain rule, we can write
P(ξ) =
n
Y
j=1
P(xj | x1, . . . , xj−1).
We can approximate this formulation by replacing each term P(xi | x1, . . . , xi−1) by the
conditional probability of xi given all other variables:
P(ξ) ≈
Y
j
P(xj | x1, . . . , xj−1, xj+1, . . . , xn).
This approximation leads to the pseudolikelihood objective:
pseudolikelihood
ℓPL(θ : D) = 1
M
X
m
X
j
ln P(xj[m] | x−j[m], θ),
(20.20)
where x−j stands for x1, . . . , xj−1, xj+1, . . . , xn. Intuitively, this objective measures our ability
to predict each variable in the model given a full observation over all other variables.
The
predictive model takes a form that generalizes the multinomial logistic CPD of deﬁnition 5.10
multinomial
logistic CPD
and is identical to it in the case where the network contains only pairwise features — factors
over edges in the network. As usual, we can use the conditional independence properties in the
network to simplify this expression, removing from the right-hand side of P(Xj | X−j) any
variable that is not a neighbor of Xj.
At ﬁrst glance, this objective may appear to be more complex than the likelihood objective.
However, a closer examination shows that we have eliminated the exponential summation over
instances with several summations, each of which is far more tractable. In particular:
P(xj | x−j) = P(xj, x−j)
P(x−j)
=
˜P(xj, x−j)
˜P(x−j)
=
˜P(xj, x−j)
P
x′
j ˜P(x′
j, x−j)
.

20.6. Alternative Objectives
971
The critical feature of this expression is that the global partition function has disappeared, and
instead we have a local partition function that requires summing only over the values of Xj.
The contrastive perspective that we described earlier provides an alternative insight on this
derivation. Consider the pseudolikelihood objective applied to a single data instance ξ:
X
j
ln P(xj | x−j)
=
X
j

ln ˜P(xj, x−j) −ln
X
x′
j
˜P(x′
j, x−j)


=
X
j

ln ˜P(ξ) −ln
X
x′
j
˜P(x′
j, x−j)

.
Each of the terms in this ﬁnal summation is a contrastive term, where we aim to increase the
diﬀerence between the log-measure of our training instance ξ and an aggregate of the log-
measures of instances that diﬀer from ξ in the assignment to precisely one variable. In other
words, we are increasing the contrast between our training instance ξ and the instances in a
local neighborhood around it.
We can further simplify each of the summands in this expression, obtaining:
lnP(xj | x−j) =


X
i : Scope[fi]∋Xj
θifi(xj, x−j)

−ln

X
x′
j
exp



X
i : Scope[fi]∋Xj
θifi(x′
j, x−j)




.
(20.21)
Each of these terms is precisely a log-conditional-likelihood term for a Markov network over
a single variable Xj, conditioned on all the remaining variables. Thus, it follows from corol-
lary 20.2 that the function is concave in the parameters θ. Since a sum of concave functions is
also concave, we have that the pseudolikelihood objective of equation (20.20) is concave. Thus,
we are guaranteed that gradient ascent over this objective will converge to the global maximum.
To compute the gradient, we use equation (20.21), to obtain:
∂
∂θi
ln P(xj | x−j) = fi(xj, x−j) −IEx′
j∼Pθ(Xj|x−j)

fi(x′
j, x−j)

.
(20.22)
If Xj is not in the scope of fi, then fi(xj, x−j) = fi(x′
j, x−j) for any x′
j, and the two terms
are identical, making the derivative 0. Inserting this expression into equation (20.20), we obtain:
Proposition 20.4
∂
∂θi
ℓPL(θ : D) =
X
j:Xj∈Scope[fi]
 
1
M
X
m
fi(ξ[m]) −IEx′
j∼Pθ(Xj|x−j[m])

fi(x′
j, x−j[m])

!
.
(20.23)
While this term looks somewhat more involved than the gradient of the likelihood in equa-
tion (20.4), it is much easier to compute: each of the expectation terms requires a summation

972
Chapter 20. Learning Undirected Models
over only a single random variable Xj, conditioned on all of its neighbors, a computation that
can generally be performed very eﬃciently.
What is the relationship between maximum likelihood estimation and maximum pseudo-
likelihood?
In one speciﬁc situation, the two estimators return the same set of parameters.
Theorem 20.3
Assume that our data are generated by a log-linear model Pθ∗that is of the form of equation (20.1).
Then, as the number of data instances M goes to inﬁnity, with probability that approaches 1, θ∗is
a global optimum of the pseudolikelihood objective of equation (20.20).
Proof To prove the result, we need to show that because the size of the data set tends to inﬁnity,
the gradient of the pseudolikelihood objective at θ∗tends to zero. Owing to the concavity of
the objective, this equality implies that θ∗is necessarily an optimum of the pseudolikelihood
objective.
We provide a somewhat informal sketch of the gradient argument, but one that
contains all the essential ideas.
Because M −→∞, the empirical distribution ˆP gets arbitrarily close to Pθ∗. Thus, the
statistics in the data are precisely representative of their expectations relative to Pθ∗. Now,
consider one of the summands in equation (20.23), associated with a feature fi. Due to the
convergence of the suﬃcient statistics,
1
M
X
m
fi(ξ[m]) −→IEξ∼Pθ∗(X)[fi(ξ)].
Conversely,
1
M
X
m
IEx′
j∼Pθ∗(Xj|x−j[m])

fi(x′
j, x−j[m])

=
X
x−j
PD(x−j)
X
x′
j
Pθ∗(x′
j | x−j)fi(x′
j, x−j)
−→
X
x−j
Pθ∗(x−j)
X
x′
j
Pθ∗(x′
j | x−j)fi(x′
j, x−j)
=
IEξ∼Pθ∗[fi(ξ)].
Thus, at the limit, the empirical and expected counts are equal, so that the gradient is zero.
This theorem states that, like the likelihood objective, the pseudolikelihood objective is also
consistent.
If we assume that the models are nondegenerate so that the two objectives are
consistent
strongly concave, the maxima are unique, and hence the two objectives have the same maxi-
mum.
While this result is an important one, it is important to be cognizant of its limitations. In
particular, we note that the two assumptions are central to this argument. First, in order for
the empirical and expected counts to match, the model being learned needs to be suﬃciently
expressive to represent the generating distribution. Second, the data distribution needs to be
close enough to the generating distribution to be well captured within the model, a situation
that is only guaranteed to happen at the large-sample limit. Without these assumptions, the two
objectives can have quite diﬀerent optima that lead to diﬀerent results.

20.6. Alternative Objectives
973
In practice, these assumptions rarely hold: our model is never a perfect representation of
the true underlying distribution, and we often do not have enough data to be close to the large
sample limit. Therefore, one must consider the question of how good this objective is in practice.
The answer to this question depends partly on the types of queries for which we intend to use
the model. If we plan to run queries where we condition on most of the variables and

query the values of only a few, the pseudolikelihood objective is a very close match to
the type of predictions we would like to make, and therefore pseudolikelihood may well
provide a better training objective than likelihood. For example, if we are trying to learn
a Markov network for collaborative ﬁltering (box 18.C), we generally take the user’s preference
for all items except the query item to be observed. Conversely, if a typical query involves
most or all of the variables in the model, the likelihood objective is more appropriate.
For example, if we are trying to learn a model for image segmentation (box 4.B), the segment
value of all of the pixels is unobserved. (We note that this last application is a CRF, where we
would generally use a conditional likelihood objective, conditioned on the actual pixel values.)
In this case, a (conditional) likelihood is a more appropriate objective than the (conditional)
pseudolikelihood.
However, even in cases where the likelihood is the more appropriate objective, we may have
to resort to pseudolikelihood for computational reasons. In many cases, this objective performs
surprisingly well. However, in others, it can provide a fairly poor approximation.
Example 20.3
Consider a Markov network over three variables X1, X2, Y , where each pair is connected by an
edge. Assume that X1, X2 are very highly correlated (almost identical) and both are somewhat
(but not as strongly) correlated with Y . In this case, the best predictor for X1 is X2, and vice
versa, so the pseudolikelihood objective is likely to overestimate signiﬁcantly the parameters on the
X1—X2, and almost entirely dismiss the X1—Y and X2—Y edges. The resulting model would
be an excellent predictor for X2 when X1 is observed, but virtually useless when only Y and not
X1 is observed.
This example is typical of a general phenomenon: Pseudolikelihood, by assuming that each
variable’s local neighborhood is fully observed, is less able to exploit information obtained from
weaker or longer-range dependencies in the distribution.
This limitation also suggests a spectrum of approaches known as generalized pseudolikelihood,
generalized
pseudolikelihood
which can reduce the extent of this problem. In particular, in the objective of equation (20.20),
rather than using a product of terms over individual variables, we can consider terms where
the left-hand side consists of several variables, conditioned on the rest. More precisely, we can
deﬁne a set of subsets of variables {Xs : s ∈S}, and then deﬁne an objective:
ℓGPL(θ : D) = 1
M
X
m
X
s
ln P(xs[m] | x−s[m], θ),
(20.24)
where X−s = X −Xs.
Clearly, there are many possible choices of subsets {Xs}. For diﬀerent such choices, this
expression generalizes several objectives: the likelihood, the pseudolikelihood, and even the
conditional likelihood. When variables are together in the same subset Xs, the relationship
between them is subject (at least in part) to a likelihood-like objective, which tends to induce
a more correct model of the joint distribution over them.
However, as for the likelihood,

974
Chapter 20. Learning Undirected Models
this objective requires that we compute expected counts over the variables in each Xs given
an assignment to X−s. Thus, the choice of Xs oﬀers a trade-oﬀbetween “accuracy” and
computational cost.
One common choice of subsets is the set of all cliques in the Markov
networks, which guarantees that the factor associated with each clique is optimized in at least
one likelihood-like term in the objective.
20.6.2
Contrastive Optimization Criteria
As we discussed, both likelihood and pseudolikelihood can be viewed as attempting to increase
the “log-probability gap” between the log-probability of the observed instances in D and the
logarithm of the aggregate probability of a set of instances. Building on this perspective, one can
construct a range of methods that aim to increase the log-probability gap between D and some
other instances. The intuition is that, by driving the probability of the observed data higher
relative to other instances, we are tuning our parameters to predict the data better.
More precisely, consider again the case of a single training instance ξ.
We can deﬁne a
“contrastive” objective where we aim to maximize the log-probability gap:

ln ˜P(ξ | θ) −ln ˜P(ξ′ | θ)

,
where ξ′ is some other instance, whose selection we discuss shortly. Importantly, this expression
takes a very simple form:

ln ˜P(ξ | θ) −ln ˜P(ξ′ | θ)

= θT [f(ξ) −f(ξ′)].
(20.25)
Note that, for a ﬁxed instantiation ξ′, this expression is a linear function of θ and hence is
unbounded. Thus, in order for this type of function to provide a coherent optimization objective,
the choice of ξ′ will generally have to change throughout the course of the optimization. Even
then, we must take care to prevent the parameters from growing unboundedly, an easy way of
arbitrarily increasing the objective.
One can construct many variants of this type of method. Here, we brieﬂy survey two that
have been particularly useful in practice.
20.6.2.1
Contrastive Divergence
One approach whose popularity has recently grown is the contrastive divergence method. In this
contrastive
divergence
method, we “contrast” our data instances D with a set of randomly perturbed “neighbors” D−.
In particular, we aim to maximize:
ℓCD(θ : D∥D−) =
h
IEξ∼ˆ
PD
h
ln ˜Pθ(ξ)
i
−IEξ∼ˆ
PD−
h
ln ˜Pθ(ξ)
ii
,
(20.26)
where ˆPD and ˆPD−are the empirical distributions relative to D and D−, respectively.
As we discussed, the set of “contrasted” instances D−will necessarily diﬀer at diﬀerent stages
in the search. Given a current parameterization θ, what is a good choice of instances to which
we want to contrast our data instances D? One intuition is that we want to move our parameters
θ in a direction that increases the probability of instances in D relative to “typical” instances
in our current distribution; that is, we want to increase the probability gap between instances

20.6. Alternative Objectives
975
ξ ∈D and instances ξ sampled randomly from Pθ. Thus, we can generate a contrastive set D−
by sampling from Pθ, and then maximizing the objective in equation (20.26).
How do we sample from Pθ? As in section 12.3, we can run a Markov chain deﬁned by
the Markov network Pθ, using, for example, Gibbs sampling, and initializing from the instances
in D; once the chain mixes, we can collect samples from the distribution Pθ. Unfortunately,
sampling from the chain for long enough to achieve mixing usually takes far too long to be
feasible as the inner loop of a learning algorithm. However, there is an alternative approach,
which is both less expensive and more robust. Rather than run the chain deﬁned by Pθ to
convergence, we initialize from the instances in D, and run the chain only for a few steps; we
then use the instances generated by these short sampling runs to deﬁne D−.
Intuitively, this approach has signiﬁcant appeal: We want our model to give high probability
to the instances in D; our current parameters, initialized at D, are causing us to move away
from the instances in D. Thus, we want to move our parameters in a direction that increases
the probability of the instances in D relative to the “perturbed” instances in D−.
The gradient of this objective is also very intuitive, and easy to compute:
∂
∂θi
ℓCD(θ : D∥D−) = IE ˆ
PD[fi(X)] −IE ˆ
PD−[fi(X)].
(20.27)
Note that, if we run the Markov chain to the limit, the samples in D−are generated from Pθ;
in this case, the second term in this diﬀerence converges to IEPθ[fi], which is precisely the
second term in the gradient of the log-likelihood objective in equation (20.4). Thus, at the limit
of the Markov chain, this learning procedure is equivalent (on expectation) to maximizing the
log-likelihood objective. However, in practice, the approximation that we get by taking only a few
steps in the Markov chain provides a good direction for the search, at far lower computational
cost. In fact, empirically it appears that, because we are taking fewer sampling steps, there is
less variance in our estimation of the gradient, leading to more robust convergence.
20.6.2.2
Margin-Based Training ⋆
A very diﬀerent intuition arises in settings where our goal is to use the learned network for
predicting a MAP assignment. For example in our image segmentation application of box 4.B,
we want to use the learned network to predict a single high-probability assignment to the
pixels that will encode our ﬁnal segmentation output. This type of reasoning only arises in
the context of conditional queries, since otherwise there is only a single MAP assignment (in
the unconditioned network). Thus, we describe the objective in this section in the context of
conditional Markov networks.
Recall that, in this setting, our training set consists of a set of pairs D = {(y[m], x[m])}M
m=1.
Given an observation x[m], we would like our learned model to give the highest probability
to y[m]. In other words, we would like the probability Pθ(y[m] | x[m]) to be higher than
any other probability Pθ(y | x[m]) for y ̸= y[m]. In fact, to increase our conﬁdence in this
prediction, we would like to increase the log-probability gap as much as possible, by increasing:
ln Pθ(y[m] | x[m]) −

max
y̸=y[m] ln Pθ(y | x[m])

.
This diﬀerence between the log-probability of the target assignment y[m] and that of the “next
best” assignment is called the margin. The higher the margin, the more conﬁdent the model is

976
Chapter 20. Learning Undirected Models
in selecting y[m]. Roughly speaking, margin-based estimation methods usually aim to maximize
margin-based
estimation
the margin.
One way of formulating this of max-margin objective as an optimization problem is as follows:
Find
γ, θ
maximizing
γ
subject to
ln Pθ(y[m] | x[m]) −ln Pθ(y | x[m])
≥
γ
∀m, y ̸= y[m].
The objective here is to maximize a single parameter γ, which encodes the worst-case margin
over all data instances, by virtue of the constraints, which impose that the log-probability gap
between y[m] and any other assignment y (given x[m]) is at least γ.
Importantly, due to
equation (20.25), the ﬁrst set of constraints can be rewritten in a simple linear form:
θT (f(y[m], x[m]) −f(y, x[m])) ≥γ.
With this reformulation of the constraints, it becomes clear that, if we ﬁnd any solution that
achieves a positive margin, we can increase the margin unboundedly simply by multiplying
all the parameters through by a positive constant factor. To make the objective coherent, we
can bound the magnitude of the parameters by constraining their L2-norm: ∥θ∥2
2 = θT θ =
P
i θ2
i = 1; or, equivalently, we can decide on a ﬁxed margin and try to reduce the magnitude
of the parameters as much as possible.
With the latter approach, we obtain the following
optimization problem:
Simple-Max-Margin:
Find
θ
minimizing
∥θ∥2
2
subject to
θT (f(y[m], x[m]) −f(y, x[m])) ≥1
∀m, y ̸= y[m]
At some level, this objective is simple: it is a quadratic program (QP) with linear constraints,
quadratic
program
and hence is a convex problem that can be solved using a variety of convex optimization
convex
optimization
methods. However, a more careful examination reveals that the problem contains a constraint
for every m, and (more importantly) for every assignment y ̸= y[m]. Thus, the number of
constraints is exponential in the number of variables Y , generally an intractable number.
However, these are not arbitrary constraints: the structure of the underlying Markov network
is reﬂected in the form of the constraints, opening the way toward eﬃcient solution algorithms.
One simple approach uses constraint generation, a general-purpose method for solving optimiza-
constraint
generation
tion problems with a large number of constraints. Constraint generation is an iterative method,
which repeatedly solves for θ, each time using a larger set of constraints. Assume we have some
algorithm for performing constrained optimization. We initially run this algorithm using none of
the margin constraints, and obtain the optimal solution θ0. In most cases, this solution will not
satisfy many of the margin constraints, and it is thus not a feasible solution to our original QP.
We add one or more constraints that are violated by θ0 into a set of active constraints. We now
repeat the constrained optimization process to obtain a new solution θ1, which is guaranteed

20.6. Alternative Objectives
977
to satisfy the active constraints. We again examine the constraints, ﬁnd ones that are violated,
and add them to our active constraints. This process repeats until no constraints are violated by
our solution. Clearly, since we only add constraints, this procedure is guaranteed to terminate:
eventually there will be no more constraints to add. Moreover, when it terminates, the solution
is guaranteed to be optimal: At any iteration, the optimization procedure is solving a relaxed
problem, whose value is at least as good as that of the fully constrained problem. If the optimal
solution to this relaxed problem happens to satisfy all of the constraints, no better solution can
be found to the fully constrained problem.
This description leaves unanswered two important questions. First, how many constraints
we will have to add before this process terminates? Fortunately, it can be shown that, under
reasonable assumptions, at most a polynomial number of constraints will need to be added prior
to termination. Second, how do we ﬁnd violated constraints without exhaustively enumerating
and checking every one? As we now show, we can perform this computation by running MAP
inference in the Markov network induced by our current parameterization θ. To see how, recall
that we either want to show that
ln ˜P(y[m], x[m]) ≥ln ˜P(y, x[m]) + 1
for every y ∈Val(Y ) except y[m], or we want to ﬁnd an assignment y that violates this
inequality constraint. Let
ymap = arg max
y̸=y[m]
˜P(y, x[m]).
There are now two cases: If ln ˜P(y[m], x[m]) < ln ˜P(ymap, x[m]) + 1, then this is a violated
constraint, which can be added to our constraint set.
Alternatively, if ln ˜P(y[m], x[m]) >
ln ˜P(ymap, x[m]) + 1, then, due to the selection of ymap, we are guaranteed that
ln ˜P(y[m], x[m]) > ln ˜P(ymap, x[m]) + 1 ≥ln ˜P(y, x[m]) + 1,
for every y ̸= y[m]. That is, in this second case, all of the exponentially many constraints for
the m’th data instance are guaranteed to be satisﬁed. As written, the task of ﬁnding ymap is not
a simple MAP computation, due to the constraint that ymap ̸= y[m]. However, this diﬃculty
arises only in the case where the MAP assignment is y[m], in which case we need only ﬁnd the
second-best assignment. Fortunately, it is not diﬃcult to adapt most MAP solution methods to
the task of ﬁnding the second-best assignment (see, for example, exercise 13.5).
The use of MAP rather than sum-product as the inference algorithm used in the inner loop
of the learning algorithm can be of signiﬁcance. As we discussed, MAP inference admits the use
of more eﬃcient optimization algorithms that are not applicable to sum-product. In fact, as we
discussed in section 13.6, there are even cases where sum-product is intractable, whereas MAP
can be solved in polynomial time.
However, the margin constraints we use here fail to address two important issues.
First,
we are not guaranteed that there exists a model that can correctly select y[m] as the MAP
assignment for every data instance m: First, our training data may be noisy, in which case y[m]
may not be the actual desired assignment. More importantly, our model may not be expressive
enough to always pick the desired target assignment (and the “simple” solution of increasing its
expressive power may lead to overﬁtting). Because of the worst-case nature of our optimization
objective, when we cannot achieve a positive margin for every data instance, there is no longer

978
Chapter 20. Learning Undirected Models
any incentive in getting a better margin for those instances where a positive margin can be
achieved. Thus, the solution we obtain becomes meaningless. To address this problem, we must
allow for instances to have a nonpositive margin and simply penalize such exceptions in the
objective; the penalization takes the form of slack variables ηm that measure the extent of the
violation for the m’th data instances. This approach allows the optimization to trade oﬀerrors
in the labels of a few instances for a better solution overall.
A second, related problem arises from our requirement that our model achieve a uniform
margin for all y ̸= y[m]. To see why this requirement can be problematic, consider again
our image segmentation problem. Here, x[m] are features derived from the image, y[m] is
our “ground truth” segmentation, and other assignments y are other candidate segmentations.
Some of these candidate segmentations diﬀer from y[m] only in very limited ways (perhaps a
few pixels are assigned a diﬀerent label). In this case, we expect that a reasonable model Pθ
will ascribe a probability to these “almost-correct” candidates that is very close to the probability
of the ground truth. If so, it will be diﬃcult to ﬁnd a good model that achieves a high margin.
Again, due to the worst-case nature of the objective, this can lead to inferior models. We address
this concern by allowing the required margin ln P(y[m] | x[m]) −ln P(y | x[m]) to vary with
the “distance” between y[m] and y, with assignments y that are more similar to y[m] requiring
a smaller margin. In particular, using the ideas of the Hamming loss, we can deﬁne ∆m(y) to
Hamming loss
be the number of variables Yi ∈Y such that yi ̸= yi[m], and require that the margin increase
linearly in this discrepancy.
Putting these two modiﬁcations together, we obtain our ﬁnal optimization problem:
Max-Margin:
Find
θ
maximizing
∥θ∥2
2 + C P
m ηm
subject to
θT (f(y[m], x[m]) −f(y, x[m])) ≥∆m(y) −ηm
∀m, y ̸= y[m].
Here, C is a constant that determines the balance between the two parts of the objective: how
much we choose to penalize mistakes (negative margins) for some instances, versus achieving a
higher margin overall.
Fortunately, the same constraint generation approach that we discussed can also be applied
in this case (see exercise 20.14).
20.7
Structure Learning
We now move to the problem of model selection: learning a network structure from data. As
model selection
usual, there are two types of solution to this problem: the constraint-based approaches, which
search for a graph structure satisfying the independence assumptions that we observe in the
empirical distribution; and the score-based approaches, which deﬁne an objective function for
diﬀerent models, and then search for a high-scoring model.
From one perspective, the constraint-based approaches appear relatively more advantageous
here than they did in the case of Bayesian network learning. First, the independencies associated
with separation in a Markov network are much simpler than those associated with d-separation

20.7. Structure Learning
979
in a Bayesian network; therefore, the algorithms for inferring the structure are much simpler
here. Second, recall that all of our scoring functions were based on the likelihood function;
here, unlike in the case of Bayesian networks, even evaluating the likelihood function is a
computationally expensive procedure, and often an intractable one.
On the other side, the disadvantage of the constraint-based approaches remains: their lack
of robustness to statistical noise in the empirical distribution, which can give rise to incorrect
independence assumptions. We also note that the constraint based approaches produce only
a structure, and not a fully speciﬁed model of a distribution. To obtain such a distribution,
we need to perform parameter estimation, so that we eventually encounter the computational
costs associated with the likelihood function. Finally, in the context of Markov network learning,
it is not clear that learning the global independence structure is necessarily the appropriate
problem. In the context of learning Bayesian networks we distinguished between learning the
global structure (the directed graph) and local structure (the form of each CPD). In learning
undirected models we can similarly consider both the problem of learning the undirected graph
structure and the particular set of factors or features that represent the parameterization of the
graph. Here, however, it is quite common to ﬁnd distributions that have a compact factorization
yet have a complex graph structure. One extreme example is the fully connected network with
pairwise potentials. Thus, in many domains we want to learn the factorization of the joint
distribution, which often cannot be deduced from the global independence assumptions.
We will review both types of approach, but we will focus most of the discussion on score-
based approaches, since these have received more attention.
20.7.1
Structure Learning Using Independence Tests
We ﬁrst consider the idea of constraint-based structure learning. Recall that the structure of a
constraint-based
structure learning
Markov network speciﬁes a set of independence assertions. We now show how we can use
independence tests to reconstruct the Markov network structure. For this discussion, assume
independence
tests
that the generating distribution P ∗is positive and can be represented as a Markov network H∗
that is a perfect map of P ∗. Thus, we want to perform a set of independence tests on P ∗and
recover H∗. To make the problem tractable, we further assume that the degree of nodes in H∗
is at most d∗.
Recall that in section 4.3.2 we considered three set sets of independencies that characterize
a Markov network: global independencies that include all consequences of separation in the
graph; Markov independencies that describe the independence of each variable X from the
rest of the variables given its Markov blanket; and pairwise independencies that describe the
independence of each nonadjacent pair of variables X, Y given all other variables. We showed
there that these three deﬁnitions are equivalent in positive distributions.
Can we use any of these concepts to recover the structure of H∗? Intuitively, we would prefer
to examine a smaller set of independencies, since they would require fewer independence tests.
Thus, we should focus either on the local Markov independencies or pairwise independencies.
Recall that local Markov independencies are of the form
local Markov
independencies
(X ⊥X −{X} −MBH∗(X) | MBH∗(X))
∀X
and pairwise independencies are of the form
pairwise
independencies
(X ⊥Y | X −{X, Y }) ∀(X—Y ) ̸∈H.

980
Chapter 20. Learning Undirected Models
Unfortunately, as written, neither of these sets of independencies can be checked tractably,
since both involve the entire set of variables X and hence require measuring the probability of
exponentially many events. The computational infeasibility of this requirement is obvious. But
equally problematic are the statistical issues: these independence assertions are evaluated not
on the true distribution, but on the empirical distribution. Independencies that involve many
variables lead to fragmentation of the data, and are much harder to evaluate without error. To
estimate the distribution suﬃciently well as to evaluate these independencies reliably, we would
need exponentially many data points.
Thus, we need to consider alternative sets of independencies that involve only smaller subsets
of variables. Several such approaches have been proposed; we review only one, as an example.
Consider the network H∗. Clearly, if X and Y are not neighbors in H∗, then they are separated
by the Markov blanket MBH∗(X) and also by MBH∗(Y ). Thus, we can ﬁnd a set Z with
Markov blanket
|Z| ≤min(|MBH∗(X)|, |MBH∗(Y )|) so that sepH∗(X; Y | Z) holds. On the other hand, if
X and Y are neighbors in H∗, then we cannot ﬁnd such a set Z. Because H∗is a perfect map
of P ∗, we can show that
X—Y ̸∈H∗if and only if ∃Z, |z| ≤d∗&P ∗|= (X ⊥Y | Z).
Thus, we can determine whether X—Y is in H∗using Pd∗
k=0
 n−2
k

independence tests. Each
of these independence tests involves only d∗+ 2 variables, which, for low values of d∗, can
be tractable. We have already encountered this test in section 3.4.3.1, as part of our Bayesian
network construction procedure. If fact, it is not hard to show that, given our assumptions and
perfect independence tests, the Build-PMap-Skeleton procedure of algorithm 3.3 reconstructs the
correct Markov structure H∗(exercise 20.15).
This procedure uses a polynomial number of tests. Thus, the procedure runs in polynomial
time. Moreover, if the probability of a false answer in any single independence test is at most
ϵ, then the probability that any one of the independence tests fails is at most Pd∗
k=0
 n−2
k

ϵ.
Therefore, for suﬃciently small ϵ, we can use this analysis to prove that we can reconstruct the
correct network structure H∗with high probability.
While this result is satisfying at some level, there are signiﬁcant limitations. First, the number

of samples required to obtain correct answers for all of the independence tests can be
very large in practice.
Second, the correctness of the algorithm is based on several
important assumptions: that there is a Markov network that is a perfect map of P ∗; that
this network has a bounded degree; and that we have enough data to obtain reliable
answers to the independence tests. When these assumptions are violated, this algorithm
can learn incorrect network structures.
Example 20.4
Assume that the underlying distribution P ∗is a Bayesian network with a v-structure X →Z ←Y .
We showed in section 3.4.3 that, assuming perfect independence tests, Build-PMap-Skeleton learns
the skeleton of G∗. However, the Markov network H∗that is an I-map for P ∗is the moralized
network, which contains, in addition to the skeleton edges, edges between parents of a joint child.
These edges will not be learned correctly by this procedure. In particular, we have that (X ⊥Y | ∅)
holds, and so the algorithm will allow us to remove the edge between X and Y , even though it
exists in the true network H∗.
The failure in this example results from the fact that the distribution P ∗does not have a perfect

20.7. Structure Learning
981
map that is a Markov network. Because many real-life distributions do not have a perfect map
that is a compact graph, the applicability of this approach can be limited.
Moreover, as we discussed, this approach focuses solely on reconstructing the network struc-
ture and does not attempt to learn the the structure of the factorization, or to estimate the
parameters. In particular, we may not have enough data to reliably estimate parameters for the
structure learned by this procedure, limiting its usability in practice. Nevertheless, as in the
case of Bayesian network structure learning, constraint-based approaches can be a useful tool
for obtaining qualitative insight into the global structure of the distribution, and as a starting
point for the search in the score-based methods.
20.7.2
Score-Based Learning: Hypothesis Spaces
We now move to the score-based structure learning approach. As we discussed earlier, this
approach formulates structure learning as an optimization problem: We deﬁne a hypothesis
hypothesis space
space consisting of a set of possible networks; we also deﬁne an objective function, which
is used to score diﬀerent candidate networks; and then we construct a search algorithm that
attempts to identify a high-scoring network in the hypothesis space. We begin in this section by
discussing the choice of hypothesis space for learning Markov networks. We discuss objective
functions and the search strategy in subsequent sections.
There are several ways of formulating the search space for Markov networks, which vary in
terms of the granularity at which they consider the network parameterization. At the coarsest-
grained, we can pose the hypothesis space as the space of diﬀerent structures of the Markov
network itself and measure the model complexity in terms of the size of the cliques in the
network. At the next level, we can consider parameterizations at the level of the factor graph,
and measure complexity in terms of the sizes of the factors in this graph. At the ﬁnest level
of granularity, we can consider a search space at the level of individual features in a log-linear
model, and measure sparsity at the level of features included in the model.
The more ﬁne-grained our hypothesis space, the better it allows us to select a parameterization
that matches the properties of our distribution without overﬁtting. For example, the factor-graph
approach allows us to distinguish between a single large factor over k variables and a set of
 k
2

pairwise factors over the same variables, requiring far fewer parameters. The feature-based
approach also allows us to distinguish between a full factor over k variables and a single
log-linear feature over the same set of variables.
Conversely, the ﬁner-grained spaces can obscure the connection to the network structure, in
that sparsity in the space of features selected does not correspond directly to sparsity in the
model structure. For example, introducing even a single feature f(d) into the model has the
structural eﬀect of introducing edges between all of the variables in d. Thus, even models with
a fairly small number of features can give rise to dense connectivity in the induced network.
While this is not a problem from the statistical perspective of reliably estimating the model
parameters from limited data, it can give rise to signiﬁcant problems from the perspective of
performing inference in the model. Moreover, a ﬁner-grained hypothesis space also means that
search algorithms take smaller steps in the space, potentially increasing the cost of our learning
procedure. We will return to some of these issues.
We focus our presentation on the formulation of the search space in terms of log-linear
models. Here, we have a set of features Ω, which are those that can potentially have nonzero

982
Chapter 20. Learning Undirected Models
weight. Our task is to select a log-linear model structure M, which is deﬁned by some subset
Φ[M] ⊆Ω. Let Θ[M] be the set of parameterizations θ that are compatible with the model
structure: that is, those where θi ̸= 0 only if fi ∈Φ[M].
A structure and a compatible
parameterization deﬁne a log-linear distribution via:
P(X | M, θ) = 1
Z exp



X
i∈Φ[M]
θifi(ξ)


= 1
Z exp
n
f T θ
o
,
where, because of the compatibility of θ with M, a feature not in Φ[M] does not inﬂuence in
the ﬁnal vector product, since it is multiplied by a parameter that is 0.
Regardless of the formulation chosen, we may sometimes wish to impose structural con-
straints that restrict the set of graph structures that can be selected, in order to ensure that
we learn a network with certain sparsity properties. In particular, one choice that has received
some attention is to restrict the class of networks learned to those that have a certain bound on
the tree-width. By placing a tight bound on the tree-width, we prevent an overly dense network
bounded
tree-width
from being selected, and thereby reduce the chance of overﬁtting. Moreover, because models of
low tree-width allow exact inference to be performed eﬃciently (to some extent), this restriction
also allows the computational steps required for evaluating the objective during the search to be
performed eﬃciently. However, this approach also has limitations. First, it turns out to be non-
trivial to implement, since computing the tree-width of a graph is itself an intractable problem
(see theorem 9.7); even keeping the graph under the required width is not simple. Moreover,
many of the distributions that arise in real-world applications cannot be well represented by
networks of low tree-width.
20.7.3
Objective Functions
We now move to considering the objective function that we aim to optimize in the score-based
approach. We note that our discussion in this section uses the likelihood function as the basis
for the objectives we consider; however, we can also consider similar objectives based on various
approximations to the likelihood (see section 20.6); most notably, the pseudolikelihood has been
used eﬀectively as a substitute for the likelihood in the context of structure learning, and most
of our discussion carries over without change to that setting.
20.7.3.1
Likelihood Function
The most straightforward objective function is the likelihood of the training data. As before, we
take the score to be the log-likelihood, deﬁning:
scoreL(M : D) =
max
θ∈Θ[M] ln P(D | M, θ) = ℓ(⟨M, ˆθM⟩: D),
where ˆθM are the maximum likelihood parameters compatible with M.
The likelihood score measures the ﬁtness of the model to the data. However, for the same
reason discussed in chapter 18, it prefers more complex models. In particular, if Φ[M1] ⊂
Φ[M2] then scoreL(M1 : D) ≤scoreL(M2 : D). Typically, this inequality is strict, due to
the ability of the richer model to capture noise in the data.

20.7. Structure Learning
983
Therefore, the likelihood score can be used only with very strict constraints on the expressive
model of the model class that we are considering. Examples include bounds on the structure of
the Markov network (for example, networks with low tree-width) or on the number of features
used. A second option, which also provides some regularization of parameter values, is to use
an alternative objective that penalizes the likelihood in order to avoid overﬁtting.
20.7.3.2
Bayesian Scores
Recall that, for Bayesian networks, we used a Bayesian score, whose primary term is a marginal
Bayesian score
likelihood that integrates the likelihood over all possible network parameterizations:
R
P(D |
M, θ)P(θ | M)dθ. This score accounts for our uncertainty over parameters using a Bayesian
prior; it avoided overﬁtting by preventing overly optimistic assessments of the model ﬁt to the
training data.
In the case of Bayesian networks, we could eﬃciently evaluate the marginal
likelihood. In contrast, in the case of undirected models, this quantity is diﬃcult to evaluate,
even using approximate inference methods.
Instead, we can use asymptotic approximations of the marginal likelihood.
The simplest
approximation is the BIC score:
BIC score
scoreBIC(M : D) = ℓ(⟨M, ˆθM⟩: D) −dim(M)
2
ln M,
where dim(M) is the dimension of the model and M the number of instances in D. This
model dimension
quantity measures the degrees of freedom of our parameter space.
When the model has
nonredundant features, dim(M) is exactly the number of features. When there is redundancy,
the dimension is smaller than the number of features. Formally, it is the rank of the matrix
whose rows are complete assignments ξi to X, whose columns are features fj, and whose
entries are fj(ξi). This matrix, however, is exponential in the number of variables, and therefore
its rank cannot be computed eﬃciently. Nonetheless, we can often estimate the number of
nonredundant parameters in the model. As a very coarse upper bound, we note that the number
of nonredundant features is always upper-bounded by the size of the full table representation of
the Markov network, which is the total number of entries in the factors.
The BIC approximation penalizes each degree of freedom (that is, free parameter) by a ﬁxed
amount, which may not be the most appropriate penalty.
Several more reﬁned alternatives
have been proposed. One common choice is the Laplace approximation, which provides a more
Laplace
approximation
explicit approximation to the marginal likelihood:
scoreLaplace(M : D) = ℓ(⟨M, ˜θM⟩: D) + ln P(˜θM | M) + dim(M)
2
ln(2π) −1
2 ln |A|,
where ˜θM are the parameters for M obtained from MAP estimation:
MAP estimation
˜θM = arg max
θ
P(D | θ, M)P(θ | M),
(20.28)
and A is the negative Hessian matrix:
Hessian
Ai,j = −
∂
∂θi∂θj
(ℓ(⟨M, θ⟩: D) + ln P(θ | M)) ,
evaluated at the point ˜θM.

984
Chapter 20. Learning Undirected Models
As we discussed in section 19.4.1.1, the Laplace score also takes into account the local shape
of the posterior distribution around the MAP parameters. It therefore provides a better approxi-
mation than the BIC score. However, as we saw in equation (20.5), to compute the Hessian, we
need to evaluate the pairwise covariance of every feature pair given the model, a computation
that may be intractable in many cases.
20.7.3.3
Parameter Penalty Scores
An alternative to approximations of the marginal likelihood are methods that simply evaluate the
maximum posterior probability
scoreMAP (M : D) =
max
θ∈Θ[M] ℓ(⟨M, ˜θM⟩: D) + ln P(˜θM | M),
(20.29)
where ˜θM are the MAP parameters for M, as deﬁned in equation (20.28). One intuition for
this type of MAP score is that the prior “regularizes” the likelihood, moving it away from the
MAP score
maximum likelihood values. If the likelihood of these parameters is still high, it implies that the
model is not too sensitive to particular choice of maximum likelihood parameters, and thus it is
more likely to generalize.
Although the regularized parameters may achieve generalization, this approach achieves model
selection only for certain types of prior. To understand why, note that the MAP score is based on
a distribution not over structures, but over parameters. We can view any parameterization θM
as a parameterization to the “universal” model deﬁned over our entire set of features Ω: one
where features not in Φ[M] receive weight 0. Assuming that our parameter prior simply ignores
zero weights, we can view our score as simply evaluating diﬀerent choices of parameterizations
θΩto this universal model.
We have already discussed several parameter priors and their eﬀect on the learned parameters.
Most parameter priors are associated with the magnitude of the parameters, rather than the
complexity of the graph as a discrete data structure. In particular, as we discussed, although
L2-regularization will tend to drive the parameters toward zero, few will actually hit zero, and
L2-regularization
so structural sparsity will not be achieved. Thus, like the likelihood score, the L2-regularized
MAP objective will generally give rise to a fully connected structure. Therefore, this approach is
generally not used in the context of model selection (at least not in isolation).
A more appropriate approach for this task is L1-regularization, which does have the eﬀect of
L1-regularization
driving model parameters toward zero, and thus can give rise to a sparse set of features. In other
words, the structure that optimizes the L1-MAP score is not, in general, the universal structure
L1-MAP score
Ω. Indeed, as we will discuss, an L1 prior has other useful properties when used as the basis
for a structure selection objective.
However, as we have discussed, feature-level sparsity does not necessarily induce sparsity in
the network. An alternative that does tend to have this property is the block-L1-regularization.
block-L1-
regularization
Here, we partition all the parameters into groups θi = {θi,1, . . . , θi,ki} (for i = 1, . . . , l). We
now deﬁne a variant of the L1 penalty that tends to make each parameter group either go to
zero together, or not:
−
l
X
i=1

v
u
u
t
ki
X
j=1
θ2
i,j

.
(20.30)

20.7. Structure Learning
985
To understand the behavior of this penalty term, let us consider its derivative for the simple
case where we have two parameters in the same group, so that our expression takes the form
p
θ2
1 + θ2
2. We now have that:
∂
∂θ1

−
q
θ2
1 + θ2
2

= −
θ1
p
θ2
1 + θ2
2
.
We therefore see that, when θ2 is large, the derivative relative to θ1 is fairly small, so that there
is no pressure on θ1 to go to 0. Conversely, when θ2 is small, the derivative relative to θ1 tends
to −1, which essentially gives the same behavior as L1 regularization. Thus, this prior tends to
have the following behavior: if the overall magnitude of the parameters in the group is small,
all of them will be forced toward zero; if the overall magnitude is large, there is little downward
pressure on any of them.
In our setting, we can naturally apply this prior to give rise to sparsity in network structure.
Assume that we are willing to consider, within our network, factors over scopes Y 1, . . . , Y l.
For each Y i, let fi,j, for j = 1, . . . , ki, be all of the features whose scope is Y i. We now deﬁne
a block-L1 prior where we have a block for each set of parameters θi1, . . . , θiki. The result of
this prior would be to select together nonzero parameters for an entire set of features associated
with a particular scope.
Finally, we note that one can also use multiple penalty terms on the likelihood function. For
example, a combination of a parameter penalty and a structure penalty can often provide both
regularization of the parameters and a greater bias toward sparse structures.
20.7.4
Optimization Task
Having selected an objective function for our model structure, it remains to address the opti-
mization problem.
20.7.4.1
Greedy Structure Search
As in the approach used for structure learning of general Bayesian networks (section 18.4.3), the
external search over structures is generally implemented by a form of local search. Indeed, the
local search
general form of the algorithms in appendix A.4.2 applies to our feature-based view of Markov
network learning.
The general template is shown in algorithm 20.1.
Roughly speaking, the
algorithm maintains a current structure, deﬁned in terms of a set of features F in our log-linear
model. At each point in the search, the algorithm optimizes the model parameters relative to
the current feature set and the structure score. Using the current structure and parameters, it
estimates the improvement of diﬀerent structure modiﬁcation steps. It then selects some subset
of modiﬁcations to implement, and returns to the parameter optimization step, initializing from
the current parameter setting. This process is repeated until a termination condition is reached.
This general template can be instantiated in many ways, including the use of diﬀerent hypothesis
spaces (as in section 20.7.2) and diﬀerent scoring functions (as described in section 20.7.3).
20.7.4.2
Successor Evaluation
Although this approach is straightforward at a high level, there are signiﬁcant issues with its
implementation. Importantly, the reasons that made this approach eﬃcient in the context of

986
Chapter 20. Learning Undirected Models
Algorithm 20.1 Greedy score-based structure search algorithm for log-linear models
Procedure Greedy-MN-Structure-Search (
Ω,
// All possible features
F0,
// initial set of features
score(· : D),
// Score
)
1
F′ ←F0
// New feature set
2
θ ←0
3
do
4
F ←F′
5
θ ←Parameter-Optimize(F, θ, score(· : D))
6
// Find parameters that optimize the score objective, relative to
current feature set, initializing from the current parameters
7
for each fk ∈F such that θk = 0
8
F ←F −fk
9
// Remove inactive features
10
for each operator o applicable to F
11
Let ˆ∆o be the approximate improvement for o
12
Choose some subset O of operators based on ˆ∆
13
F′ ←O(F)
// Apply selected operators to F
14
while termination condition not reached
15
return (F, θ)
Bayesian networks do not apply here. In the case of Bayesian networks, evaluating the score
of a candidate structure is a very easy task, which can be executed in closed form, at very
low computation cost. Moreover, the Bayesian network score satisﬁes an important property:
it decomposes according to the structure of the network. As we discussed, this property has
score
decomposability
two major implications. First, a local modiﬁcation to the structure involves changing only a
single term in the score (proposition 18.5); second, the change in score incurred by a particular
change (for example, adding an edge) remains unchanged after modiﬁcations to other parts of
the network (proposition 18.6). These properties allowed us to design eﬃcient search procedure
that does not need to reevaluate all possible candidates after every step, and that can cache
intermediate computations to evaluate candidates in the search space quickly.
Unfortunately, none of these properties hold for Markov networks. For concreteness, consider
the likelihood score, which is comparable across both network classes. First, as we discussed,
even computing the likelihood of a fully speciﬁed model — structure as well as parameters —
requires that we run inference for every instance in our training set. Second, to score a structure,
we need to estimate the parameters for it, a problem for which there is no closed-form solution.
Finally, none of the decomposition properties hold in the case of undirected models. By adding
a new feature (or a set of features, for example, a factor), we change the weight (P
i θifi(ξ))
associated with diﬀerent instances. This change can be decomposed, since it is a linear function
of the diﬀerent features. However, this change also aﬀects the partition function, and, as we saw
in the context of parameter estimation, the partition function couples the eﬀects of changes in

20.7. Structure Learning
987
one parameter on the other. We can clearly see this phenomenon in ﬁgure 20.1, where the eﬀect
on the likelihood of modifying f1(b1, c1) clearly depends on the current value of the parameter
for f2(a1, b1).
As a consequence, a local search procedure is considerably more expensive in the context
of Markov networks. At each stage of the search, we need to evaluate the score for all of the
candidates we wish to examine at that point in the search. This evaluation requires that we
estimate the parameters for the structure, a process that itself requires multiple iterations of
a numerical optimization algorithm, each involving inference over all of the instances in our
training set. We can reduce somewhat the computational cost of the algorithm by using the
observation that a single change to the structure of the network often does not result in drastic
changes to the model.
Thus, if we begin our optimization process from the current set of
parameters, a reasonably small number of iterations often suﬃces to achieve convergence to the
new set of parameters. Importantly, because all of the parameter objectives described are convex
(when we have fully observable data), the initialization has no eﬀect, and convergence to the
global optimum remains guaranteed. Thus, this approach simply provides a way of speeding up
the convergence to the optimal answer. (We note, however, that this statement holds only when
we use exact inference; the choice of initialization can aﬀect the accuracy of some approximate
inference algorithms, and therefore the answers that we get.)
Unfortunately, although this observation does reduce the cost, the number of candidate hy-
potheses at each step is generally quite large.
The cost of running inference on each of

the candidate successors is prohibitive, especially in cases where, to ﬁt our target dis-
tribution well, we need to consider nontrivial structures. Thus, much of the work on
the problem of structure learning for Markov networks has been devoted to reducing the
computational cost of evaluating the score of diﬀerent candidates during the search. In
particular, when evaluating diﬀerent structure-modiﬁcation operators in line 11, most algorithms
use some heuristic to rank diﬀerent candidates, rather than computing the exact delta-score of
each operator. These heuristic estimates can be used either as the basis for the ﬁnal selection,
or as a way of pruning the set of possible successors, where the high-ranking candidates are
then evaluated exactly. This design decision is a trade-oﬀbetween the quality of our operator
selection and the computational cost.
Even with the use of heuristics, the cost of taking a step in the search can be prohibitive,
since it requires a reestimation of the network parameters and a reevaluation of the (approximate)
delta-score for all of the operators. This suggests that it may be beneﬁcial to select, at each
structure modiﬁcation step (line 12), not a single operator but a subset O of operators. This
approach can greatly reduce the computational cost, but at a cost: our (heuristic) estimate of
each operator can deteriorate signiﬁcantly if we fail to take into account interactions between
the eﬀects of diﬀerent operators. Again, this is a trade-oﬀbetween the quality and cost of the
operator selection.
20.7.4.3
Choice of Scoring Function
As we mentioned, the rough template of algorithm 20.1 can be applied to any objective function.
However, the choice of objective function has signiﬁcant implications on our ability to eﬀectively
optimize it. Let us consider several of the choices discussed earlier.
We ﬁrst recall that both the log-likelihood objective and the L2-regularized log-likelihood

988
Chapter 20. Learning Undirected Models
generally give nonzero values to all parameters.
In other words, if we allow the model to
consider a set of features F, an optimal model (maximum-likelihood or maximum L2-regularized
likelihood) over F will give a nonzero value to the parameters for all of these features. In other
words, we cannot rely on these objectives to induce sparsity in the model structure. Thus, if we
simply want to optimize these objectives, we should simply choose the richest model available
in our hypothesis space and then optimize its parameters relative to the chosen objective.
One approach for deriving more compact models is to restrict the class of models to ones
with a certain bound on the complexity (for example, networks of bounded tree-width, or with
a bound on the number of edges or features allowed). However, these constraints generally
introduce nontrivial combinatorial trade-oﬀs between features, giving rise to a search space with
multiple local optima, and making it generally intractable to ﬁnd a globally optimal solution. A
second approach is simply to halt the search when the improvement in score (or an approxi-
mation to it) obtained by a single step does not exceed a certain threshold. This heuristic is
not unreasonable, since good features are generally introduced earlier, and so there is a general
trend of diminishing returns. However, there is no guarantee that the solution we obtain is even
close to the optimum, since there is no bound on how much the score would improve if we
continue to optimize beyond the current step.
Scoring functions that explicitly penalize structure complexity — such as the BIC score or
Laplace approximation — also avoid this degeneracy. Here, as in the case of Bayesian networks,
we can consider a large hypothesis space and attempt to ﬁnd the model in this space that
optimizes the score. However, due to the discrete nature of the structure penalty, the score is
discontinuous and therefore nonconcave. Thus, there is generally no guarantee of convergence
to the global optimum. Of course, this limitation was also the case when learning Bayesian
networks; as there, it can be somewhat alleviated by methods that avoid local maxima (such as
tabu search, random restarts, or data perturbation).
However, in the case of Markov networks, we have another solution available to us, one that
avoids the prospect of combinatorial search spaces and the ensuing problem of local optima.
This solution is the use of L1-regularized likelihood.
As we discussed, the L1-regularized
likelihood is a concave function that has a unique global optimum. Moreover, this objective
function naturally gives rise to sparse models, in that, at the optimum, many parameters have
value 0, corresponding to the elimination of features from the model. We discuss this approach
in more detail in the next section.
20.7.4.4
L1-Regularization for Structure Learning
Recall that the L1-regularized likelihood is simply the instantiation of equation (20.29) to the
case of an L1-prior:
scoreL1(θ : D) = ℓ(⟨M, θ⟩: D) −∥θ∥1.
(20.31)
Somewhat surprisingly, the L1-regularized likelihood can be optimized in a way that guar-
antees convergence to the globally optimal solution. To understand why, recall that the task of
optimizing the L1-regularized log-likelihood is a convex optimization problem that has no local
optima.1 Indeed, in theory, we can entirely avoid the combinatorial search component when
1. There might be multiple global optima due to redundancy in the parameter space, but these global optima all form a
single convex region. Therefore, we use the term “the global optimum” to refer to any point in this optimal region.

20.7. Structure Learning
989
using this objective. We can simply introduce all of the possible features into the model and
optimize the resulting parameter vector θ relative to our objective. The sparsifying eﬀect of the
L1 penalty will drive some of the parameters to zero. The parameters that, at convergence, have
zero values correspond to features that are absent from the log-linear model. In this approach,
we are eﬀectively making a structure selection decision as part of our parameter optimization
procedure. Although appealing, this approach is not generally feasible. In most cases, the num-
ber of potential features we may consider for inclusion in the model is quite large. Including all
of them in the model simultaneously gives rise to an intractable structure for the inference that
we use as part of the computation of the gradient.
Therefore, even in the context of the L1-regularized likelihood, we generally implement the
optimization as a double-loop algorithm where we separately consider the structure and param-
eters. However, there are several beneﬁts to the L1-regularized objective:
•
We do not need to consider feature deletion steps in our combinatorial search.
•
We can consider feature introduction steps in any (reasonable) order, and yet achieve conver-
gence to the global optimum.
•
We have a simple and eﬃcient test for determining convergence.
•
We can prove a PAC-learnability generalization bound for this type of learning.
We now discuss each of these points.
For the purpose of this discussion, assume that we currently have a model over a set of
features F, and assume that θl optimizes our L1-regularized objective, subject to the constraint
that θl
k can be nonzero only if fk ∈F. At this convergence point, any feature deletion step
cannot improve the score: Consider any fk ∈F; the case where fk is deleted is already in
the class of models that was considered when we optimized the choice of θl — it is simply
the model where θl
k = 0. Indeed, the algorithm already discards features whose parameter
was zeroed by the continuous optimization procedure (line 7 of algorithm 20.1). If our current
optimized model θl has θl
k ̸= 0, it follows that setting θk to 0 is suboptimal, and so deleting
fk can only reduce the score. Thus, there is no value to considering discrete feature deletion
steps: features that should be deleted will have their parameters set to 0 by the continuous
optimization procedure. We note that this property also holds, in principle, for other smooth
objectives, such as the likelihood or the L2-regularized likelihood; the diﬀerence is that for
those objectives, parameters will generally not be set to 0, whereas the L1 objective does tend
to induce sparsity.
The second beneﬁt arises directly from the fact that optimizing the L1-regularized objective
is a convex optimization problem. In such problems, any sequence of steps that continues to
improve the objective (when possible) is guaranteed to converge to the global optimum. The
restriction imposed by the set F induces a coordinate ascent approach: at each step, we are
optimizing only the features in F, leaving at 0 those parameters θk for fk ̸∈F. As long as each
step continues to improve the objective, we are making progress toward the global optimum.
At each point in the search, we consider the steps that we can take. If some step leads to an
improvement in the score, we can take that step and continue with our search. If none of the
steps lead to an improvement in the score, we are guaranteed that we have reached convergence
to the global optimum. Thus, the decision on which operators to consider at each point

in the algorithm (line 12 of algorithm 20.1) is not relevant to the convergence of the

990
Chapter 20. Learning Undirected Models
algorithm to the true global optimum: As long as we repeatedly consider each operator
until convergence, we are guaranteed that the global optimum is reached regardless of
the order in which the operators are applied.
While this guarantee is an important one, we should interpret it with care. First, when we add
features to the model, the underlying network becomes more complex, raising the cost of infer-
ence. Because inference is executed many times during the algorithm, adding many irrelevant
features, even if they were eventually eliminated, can greatly degrade the computational perfor-
mance time of the algorithm. Even more problematic is the eﬀect when we utilize approximate
inference, as is often the case. As we discussed, for many approximate inference algorithms,
not only the running time but also the accuracy tend to degrade as the network becomes more
complex. Because inference is used to compute the gradient for the continuous optimization,
the degradation of inference quality can lead to models that are suboptimal. Moreover, because
the resulting model is generally also used to estimate the beneﬁt of adding new features, any
inaccuracy can propagate further, causing yet more suboptimal features to be introduced into
the model. Hence, especially when the quality of approximate inference is a concern, it is

worthwhile to select with care the features to be introduced into the model rather than
blithely relying on the “guaranteed” convergence to a global optimum.
Another important issue to be addressed is the problem of determining convergence in line 14
of Greedy-MN-Structure-Search. In other words, how do we test that none of the search operators
we currently have available can improve the score? A priori, this task appears daunting, since
we certainly do not want to try all possible feature addition/deletion steps, reoptimize the
parameters for each of them, and then check whether the score has improved. Fortunately, there
is a much more tractable solution. Speciﬁcally, we can show the following proposition:
Proposition 20.5
Let ∆grad
L
(θk : θl, D) denote the gradient of the likelihood relative to θk, evaluated at θl. Let
β be the hyperparameter deﬁning the L1 prior. Let θl be a parameter assignment for which the
following conditions hold:
• For any k for which θl
k ̸= 0 we have that
∆grad
L
(θk : θl, D) −1
β sign(θl
k) = 0.
• For any k for which θl
k = 0 we have that
|∆grad
L
(θk : θl, D)| < 1
2β .
Then θl is a global optimum of the L1-regularized log-likelihood function:
1
M ℓ(θ : D) −1
β
k
X
i=1
|θi|.
Proof We provide a rough sketch of the proof. The ﬁrst condition guarantees that the gradient
relative to any parameter for which θl
k ̸= 0 is zero, and hence the objective function cannot be
improved by changing its value. The second condition deals with parameters θl
k = 0, for which

20.7. Structure Learning
991
the gradient is discontinuous at the convergence point. However, consider a point θ′ in the
nearby vicinity of θ, so that θ′
k ̸= 0. At θ′, the gradient of the function relative to θk is very
close to
∆grad
L
(θk : θl, D) −1
β sign(θ′
k).
The value of this expression is positive if θ′
k < 0 and negative if θ′
k > 0. Thus, θl is a local
optimum of the L1-regularized objective function. Because the function has only global optima,
θl must be a global optimum.
Thus, we can test convergence easily as a direct by-product of the continuous parameter
optimization procedure executed at each step. We note that we still have to consider every
feature that is not included in the model and compute the relevant gradient; but we do not have
to go through the (much more expensive) process of trying to introduce the feature, optimizing
the resulting model, and evaluating its score.
So far, we have avoided the discussion of optimizing this objective. As we mentioned in
section 20.3.1, a commonly used method for optimizing the likelihood is the L-BFGS algorithm,
L-BFGS algorithm
which uses gradient descent combined with line search (see appendix A.5.2).
The problem
with applying this method to the L1-regularized likelihood is that the regularization term is not
continuously diﬀerentiable: the gradient relative to any parameter θi changes at θi = 0 from +1
to −1. Perhaps the simplest solution to this problem is to adjust the line-search procedure to
avoid changing the sign of any parameter θi: If, during our line search, θi crosses from positive
to negative (or vice versa), we simply ﬁx it to be 0, and continue with the line search for the
remaining parameters. Note that this decision corresponds to taking fi out of the set of active
features in this iteration. If the optimal parameter assignment has a nonzero value for θi, we are
guaranteed that fi will be introduced again in a later stage in the search, as we have discussed.
Finally, as we mentioned, we can prove a useful theoretical guarantee for the results of
L1-regularized Markov network learning. Speciﬁcally, we can show the following PAC-bound:
PAC-bound
Theorem 20.4
Let X be a set of variables such that |Val(Xi)| ≤d for all i. Let P ∗be a distribution, and
δ, ϵ, B > 0. Let F be a set of all indicator features over all subsets of variables X ⊂X such that
|X| ≤c, and let
Θc,B = {θ ∈Θ[F] : ∥θ∥1 ≤B}
be all parameterizations of F whose L1-norm is at most B. Let β =
p
c ln(2nd/δ)/(2M). Let
θ∗
c,B = arg max
θ∈Θc,B ID(P ∗||Pθ)
be the best parameterization achievable within the class Θc,B. For any data set D, let
ˆθ = arg max
θ∈Θ[F] scoreL1(θ : D).
Then, for
M ≥2cB2
ϵ2
ln
2nd
δ

,

992
Chapter 20. Learning Undirected Models
with probability at least 1 −δ,
ID(P ∗||Pˆθ) ≤ID(P ∗||Pθ∗c,B) + ϵ.
In other words, this theorem states that, with high probability over data sets D, the relative
entropy to P ∗achieved by the best L1-regularized model is at most ϵ worse than the relative
entropy achieved by the best model within the class of limited-degree Markov networks. This
guarantee is achievable with a number of samples that is polynomial in ϵ, c, and B, and
logarithmic in δ and d. The logarithmic dependence on n may feel promising, but we note that
B is a sum of the absolute values of all network parameters; assuming we bound the magnitude
of individual parameters, this terms grows linearly with the total number of network parameters.
Thus, L1-regularized learning provides us with a model that is close to optimal (within the class
Θc,B), using a polynomial number of samples.
20.7.5
Evaluating Changes to the Model
We now consider in more detail the candidate evaluation step that takes place in line 11 of
Greedy-MN-Structure-Search.
As we discussed, the standard way to reduce the cost of the
candidate evaluation step is simply to avoid computing the exact score of each of the candidate
successors, and rather to select among them using simpler heuristics. Many approximations
are possible, ranging from ones that are very simple and heuristic to ones that are much more
elaborate and provide certain guarantees.
Most simply, we can examine statistics of the data to determine features that may be worth
including. For example, if two variables Xi and Xj are strongly correlated in the data, it may
be worthwhile to consider introducing a factor over Xi, Xj (or a pairwise feature over one or
more combinations of their values). The limitation of this approach is that it does not take into
account the features that have already been introduced into the model and the extent to which
they already explain the observed correlation.
A somewhat more reﬁned approach, called grafting, estimates the beneﬁt of introducing a
grafting
feature fk by compute the gradient of the likelihood relative to θk, evaluated at the current
model.
More precisely, assume that our current model is (F, θ0).
The gradient heuristic
gradient heuristic
estimate to the delta-score (for score X) obtained by adding fk ̸∈F is deﬁned as:
∆grad
X
(θk : θ0, D) =
∂
∂θk
scoreX(θ : D),
(20.32)
evaluated at the current parameters θ0.
The gradient heuristic does account for the parameters already selected; thus, for example,
it can avoid introducing features that are not relevant given the parameters already introduced.
Intuitively, features that have a high gradient can induce a signiﬁcant immediate improvement
in the score, and therefore they are good candidates for introduction into the model. Indeed,
we are guaranteed that, if θk has a positive gradient, introducing fk into F will result in some
improvement to the score. The problem with this approach is that it does not attempt to evaluate
how large this improvement can be. Perhaps we can increase θk only by a small amount before
further changes stop improving the score.
An even more precise approximation is to evaluate a change to the model by computing the
score obtained in a model where we keep all other parameters ﬁxed. Consider a step where

20.7. Structure Learning
993
we introduce or delete a single feature fk in our model. We can obtain an approximation to
the score by evaluating the change in score when we change only the parameter θk associated
with fk, keeping all other parameters unchanged. To formalize this idea, let (F, θ0) be our
current model, and consider changes involving fk. We deﬁne the gain heuristic estimate to be
gain heuristic
the change in the score of the model for diﬀerent values for θk, assuming the other parameters
are kept ﬁxed:
∆gain
X
(θk : θ0, D) = scoreX((θk, θ0
−k) : D) −scoreX(θ0 : D),
(20.33)
where θ0
−k is the vector of all parameters other than θ0
k. As we discussed, due to the nonde-
composability of the likelihood, when we change θk, the current assignment θ0
−k to the other
parameters is generally no longer optimal. However, it is still reasonable to use this function
as a heuristic to rank diﬀerent steps: Parameters that give rise to a larger improvement in
the objective by themselves often also induce a larger improvement when other parameters are
optimized. Indeed, changing those other parameters to optimize the score can only improve it
further. Thus, the change in score that we obtain when we “freeze” the other parameters and
change only θk is a lower bound on the change in the score.
The gain function can be used to provide a lower bound on the improvement in score
derived from the deletion of a feature fk currently in the model: we simply evaluate the gain
function setting θk = 0. We can also obtain a lower bound on the value of a step where we
introduce into the model a new feature fk (that is, one for which the current parameter θ0
k = 0).
The improvement we can get if we freeze all parameters but one is clearly a lower bound on
the improvement we can get if we optimize over all of the parameters.
Thus, the value of
∆gain
X
(θk : θ0, D) is a lower bound on the improvement in the objective that can be gained by
setting θk to its chosen value and optimizing all other parameters. To compute the best lower
bound, we must maximize the function relative to diﬀerent possible values of θk, giving us the
score of the best possible model when all parameters other than θk are frozen. In particular, we
can deﬁne:
GainX(θ0 : fk, D) = max
θk ∆gain
X
(θk : θ0, D).
This is a lower bound on the change in the objective obtained from introducing a fk ̸∈F.
In principle, lower bounds are more useful than simple approximations. If our lower bound
of the candidate’s score is higher than that of our current best model, then we deﬁnitely want
to evaluate that candidate; this will result in a better current candidate and allow us to prune
additional candidates and focus on the ones that seem more promising for evaluation. Upper
bounds are useful as well. If we have a candidate model and obtain an upper bound on its
score, then we can remove it from consideration once we evaluate another candidate with higher
score; thus, upper bounds help us prune models for which we would never want to evaluate the
true score. In practice, however, fully evaluating the score for all but a tiny handful of candidate
structures is usually too expensive a proposition. Thus, the gain is generally used simply as an
approximation rather than a lower bound.
How do we evaluate the gain function eﬃciently, or ﬁnd its optimal value? The gain function
is a univariate function of θk, which is a projection of the score function onto this single
dimension. Importantly, all of our scoring functions — including any of the penalized likelihood
functions we described — are concave (for a given set of active features).
The projection

994
Chapter 20. Learning Undirected Models
of a concave function onto a single dimension is also concave, so that this single-parameter
delta-score is also concave and therefore has a global optimum.
Nevertheless, given the complexity of the likelihood function, it is not clear how this global
optimum can be eﬃciently found. We now show how the diﬀerence between two log-likelihood
terms can be considerably simpliﬁed, even allowing a closed-form solution in certain important
cases. Recall from equation (20.3) that
1
M ℓ(θ : D) =
X
k
θkIED[fk] −ln Z(θ).
Because parameters other than θk are the same in the two models, we have that:
1
M [ℓ((θk, θ0
−k) : D) −ℓ(θ0 : D)] = (θk −θ0
k)IED[fk] −

ln Z(θk, θ0
−k) −ln Z(θ0)

.
The ﬁrst term is a linear function in θk, whose coeﬃcient is the empirical expectation of fk in
the data. For the second term, we have:
ln Z(θk, θ0
−k)
Z(θ0)
=
ln


1
Z(θ0)
X
ξ
exp



X
j
θ0
jfj(ξ) + (θk −θ0
k)fk(ξ)





=
ln
X
ξ
˜Pθ0(ξ)
Z(θ0)

exp

(θk −θ0
k)fk(ξ)
	
=
ln IEθ0
exp

(θk −θ0
k)fk(dk)
	
.
Thus, the diﬀerence of these two log-partition functions can be rewritten as a log-expectation
relative to our original distribution. We can convert this expression into a univariate function
of θk by computing (via inference in our current model θ0) the marginal distribution over the
variables dk. Altogether, we obtain that:
1
M [ℓ((θk, θ0
−k) : D) −ℓ(θ0 : D)] =
(20.34)
(θk −θ0
k)IED[fk] −ln
X
dk
Pθ0(dk)

exp

(θk −θ0
k)fk(dk)
	
.
We can incorporate this simpliﬁed form into equation (20.33) for any penalized-likelihood
scoring function. We can now easily provide our lower-bound estimates for feature deletions.
For introducing a feature fk, the optimal lower bound can be computed by optimizing the
univariate function deﬁned by equation (20.33) over the parameter θk. Because this function is
concave, it can be optimized using a variety of univariate numerical optimization algorithms.
For example, to compute the lower bound for an L2-regularized likelihood, we would compute:
max
θk
(
θkIED[fk] −ln
X
dk
Pθ0(dk)

exp

(θk −θ0
k)fk(dk)
	
−θ2
k
2σ2
)
.
However, in certain special cases, we can actually provide a closed-form solution for this
optimization problem. We note that this derivation applies only in restricted cases: only in
the case of generative training (that is, not for CRFs); only for the likelihood or L1-penalized
objective; and only for binary-valued features.

20.7. Structure Learning
995
Proposition 20.6
Let fk be a binary-valued feature and let θ0 be a current setting of parameters for a log-linear
model.
Let ˆpk = IED[fk] be the empirical probability of fk in D, and p0
k = Pθ(fk) be its
probability relative to the current model. Then:
max
θk

scoreL((θk, θ0
−k) : D) −scoreL(θ0 : D)

= ID(ˆpk||p0
k),
where the KL-divergence is the relative entropy between the two Bernoulli distributions parameter-
ized by ˆpk and p0
k respectively.
The proof is left as an exercise (exercise 20.16).
To see why this result is intuitive, recall that when we maximize the likelihood relative to some
log-linear model, we obtain a model where the expected counts match the empirical counts. In
the case of a binary-valued feature, in the optimized model we have that the ﬁnal probability of
fk would be the same as the empirical probability ˆpk. Thus, it is reasonable that the amount
of improvement we obtain from this optimization is a function of the discrepancy between the
empirical probability of the feature and its probability given the current model. The bigger the
discrepancy, the bigger the improvement in the likelihood.
A similar analysis applies when we consider several binary-valued features f1, . . . , fk, as long
as they are mutually exclusive and exhaustive; that is, as long as there are no assignments for
which both fi‘(ξ) = 1 and fj(ξ) = 1. In particular, we can show the following:
Proposition 20.7
Let θ0 be a current setting of parameters for a log-linear model, and consider introducing into the
model a complete factor φ over scope d, parameterized with θk that correspond to the diﬀerent
assignments to d. Then
max
θk

scoreL((θk, θ0
−k) : D) −scoreL(θ0 : D)

= ID( ˆP(d)||Pθ(d)).
The proof is left as an exercise (exercise 20.17).
Although the derivations here were performed for the likelihood function, a similar closed-
form solution, in the same class of cases, can also be performed for the L1-regularized likelihood
(see exercise 20.18), but not for the L2-regularized likelihood. Intuitively, the penalty in the L1-
regularized likelihood is a linear function in each θk, and therefore it does not complicate the
form of equation (20.34), which already contains such a term.
However, the L2 penalty is
quadratic, and introducing a quadratic term into the function prevents an analytic solution.
One issue that we did not address is the task of computing the expressions in equation (20.34),
or even the closed-form expressions in proposition 20.6 and proposition 20.7.
All of these
expressions involve expectations over the scope Dk of fk, where fk is the feature that we want
to eliminate from or introduce into the model. Let us consider ﬁrst the case where fk is already
in the model. In this case, if we use a belief propagation algorithm (whether a clique tree or
a loopy cluster graph), the family preservation property guarantees that the feature’s scope Dk
is necessarily a subset of some cluster in our inference data structure. Thus, we can easily
compute the necessary expectations. However, for a feature not currently in the model, we
would not generally expect its scope to be included in any cluster. If not, we must somehow
compute expectations of sets of variables that are not together in the same cluster. In the case
of clique trees, we can use the out-of-clique inference methods described in section 10.3.3.2. For
the case of loopy cluster graphs, this problem is more challenging (see exercise 11.22).

996
Chapter 20. Learning Undirected Models
20.8
Summary
In this chapter, we discussed the problem of learning undirected graphical models from data.
The key challenge in learning these models is that the global partition function couples the
parameters, with signiﬁcant consequences: There is no closed-form solution for the optimal
parameters; moreover, we can no longer optimize each of the parameters independently of the
others. Thus, even simple maximum-likelihood parameter estimation is no longer trivial. For the
same reason, full Bayesian estimation is computationally intractable, and even approximations
are expensive and not often used in practice.
Following these pieces of bad news, there are some good ones: the likelihood function is
concave, and hence it has no local optima and can be optimized using eﬃcient gradient-based
methods over the space of possible parameterizations.
We can also extend this method to
MAP estimation when we are given a prior over the parameters, which allows us to reduce the
overﬁtting to which maximum likelihood is prone.
The gradient of the likelihood at a point θ has a particularly compelling form: the gradient
relative to the parameter θi corresponding to the feature fi is the diﬀerence between the
empirical expectation of fi in the data and its expectation relative to the distribution Pθ. While
very intuitive and simple in principle, the form of the gradient immediately gives rise to some
bad news: to compute the gradient at the point θ, we need to run inference over the model Pθ,
a costly procedure to execute at every gradient step.
This complexity motivates the use of myriad alternative approaches: ones involving the use
of approximate inference for computing the gradient; and ones that utilize a diﬀerent objective
than the likelihood. Methods in the ﬁrst class included using message passing algorithms such as
belief propagation, and methods based on sampling. We also showed that many of the methods
that use approximate inference for optimizing the likelihood can be reformulated as exactly
optimizing an approximate objective. This perspective can oﬀer signiﬁcant insight. For example,
we showed that learning with belief propagation can be reformulated as optimizing a joint
objective that involves both inference and learning; this alternative formulation is more general
and allows the use of alternative optimization methods that are more stable and convergent than
using BP to estimate the gradient.
Methods that use an approximate objective include pseudolikelihood, contrastive divergence,
and maximum-margin (which is speciﬁcally geared for discriminative training of conditional
models). Importantly, both likelihood and these objectives can be viewed as trying to increase
the distance between the log-probability of assignments in our data and those of some set
other assignments. This “contrastive” view provides a diﬀerent view of these objectives, and it
suggests that they are only representatives of a much more general class of approximations.
The same analysis that we performed for optimizing the likelihood can also be extended to
other cases. In particular, we showed a very similar derivation for conditional training, where
the objective is to maximize the likelihood of a set of target variables Y given some set of
observed feature variables X.
We also showed that similar approaches can be applied to learning with missing data. Here,
the optimization task is no longer convex, but the gradient has a very similar form and can be
optimized using the same gradient-ascent methods. However, as in the case of Bayesian network
learning with missing data, the likelihood function is generally multimodal, and so the gradient
ascent algorithm can get stuck in local optima. Thus, we may need to resort to techniques such

20.8. Summary
997
as data perturbation or random restarts.
We also discussed the problem of structure learning of undirected models.
Here again,
we can use both constraint-based and score-based methods. Owing to the diﬃculties arising
from the form of the likelihood function, full Bayesian scoring, where we score a model by
integrating over all of the parameters, is intractable, and even approximations are generally
impractical. Thus, we generally use a simpler scoring function, which combines a likelihood
term (measuring ﬁt to data) with some penalty term.
We then search over some space of
structures for ones that optimize this objective. For most objectives, the resulting optimization
problem is combinatorial with multiple local optima, so that we must resort to heuristic search.
One notable exception is the use of an L1-regularized likelihood, where the penalty on the
absolute value of the parameters tends to drive many of the parameters to zero, and hence often
results in sparse models. This objective allows the structure learning task to be formulated as
a convex optimization problem over the space of parameters, allowing the optimization to be
performed eﬃciently and with guaranteed convergence to a global optimum. Of course, even
here inference is still an unavoidable component in the inner loop of the learning algorithm,
with all of the ensuing diﬃculties.
As we mentioned, the case of discriminative training is a setting where undirected models are
particularly suited, and are very commonly used. However, it is important to carefully weigh
the trade-oﬀs of generative versus discriminative training. As we discussed, there are signiﬁcant
diﬀerences in the computational cost of the diﬀerent forms of training, and the trade-oﬀcan go
either way. More importantly, as we discussed in section 16.3.2, generative models incorporate

a higher bias by making assumptions — ones that are often only approximately correct —
about the underlying distribution. Discriminative models make fewer assumptions, and
therefore tend to require more data to train; generative models, due to the stronger bias,
often perform better in the sparse-data regime. But incorrect modeling assumptions also
hurt performance; therefore, as the amount of training data grows, the discriminative
model, which makes fewer assumptions, often performs better. This diﬀerence between
the two classes of models is particularly signiﬁcant when we have complex features whose
correlations are hard to model. However, it is important to remember that models trained
discriminatively to predict Y given X will perform well primarily in this setting, and even
slight changes may lead to a degradation in performance. For example, a model for predicting
P(Y | X1, X2) would not be useful for predicting P(Y | X1) in situations where X2 is not
observed. In general, discriminative models are much less ﬂexible in their ability to handle
missing data.
We focused most of our discussion of learning on the problem of learning log-linear models
deﬁned in terms of a set of features.
Log-linear models are a ﬁner-grained representation
than a Markov network structure or a set of factors. Thus, they can make better trade-oﬀs
between model complexity and ﬁt to data. However, sparse log-linear models (with few features)
do not directly correspond to sparse Markov network structures, so that we might easily end
up learning a model that does not lend itself to tractable inference.
It would be useful to
consider the development of Markov network structure learning algorithms that more easily
support eﬃcient inference. Indeed, some work has been done on learning Markov networks
of bounded tree-width, but networks of low tree-width are often poor approximations to the
target distribution. Thus, it would be interesting to explore alternative approaches that aim at
structures that support approximate inference.

998
Chapter 20. Learning Undirected Models
This chapter is structured as a core idea with a set of distinct extensions that build on it:
The core idea is the use of the likelihood function and the analysis of its properties.
The
extensions include conditional likelihood, learning with missing data, the use of parameter
priors, approximate inference and/or approximate objectives, and even structure learning. In
many cases, these extensions are orthogonal, and we can easily combine them in various useful
ways. For example, we can use parameter priors with conditional likelihood or in the case
of missing data; we can also use them with approximate methods such as pseudolikelihood,
contrastive divergence or in the objective of equation (20.15). Perhaps more surprising is that we
can easily perform structure learning with missing data by adding an L1-regularization term to
the likelihood function of equation (20.8) and then using the same ideas as in section 20.7.4.4.
In other cases, the combination of the diﬀerent extensions is more involved. For example, as
we discussed, structure learning requires that we be able to evaluate the expected counts for
variables that are not in the same family; this task is not so easy if we use an approximate
algorithm such as belief propagation. As another example, it is not immediately obvious how
we can extend the pseudolikelihood objective to deal with missing data. These combinations
provide useful directions for future work.
20.9
Relevant Literature
Log-linear models and contingency tables have been used pervasively in a variety of communi-
ties, and so key ideas have often been discovered multiple times, making a complete history too
long to include. Early attempts for learning log-linear models were based on the iterative propor-
iterative
proportional
scaling
tional scaling algorithm and its extension, iterative proportional ﬁtting. These methods were ﬁrst
iterative
proportional
ﬁtting
developed for contingency tables by Deming and Stephan (1940) and applied to log-linear models
by Darroch and Ratcliﬀ(1972). The convex duality between the maximum likelihood and maxi-
mum entropy problems appears to have been proved independently in several papers in diverse
communities, including (at least) Ben-Tal and Charnes (1979); Dykstra and Lemke (1988); Berger,
Della-Pietra, and Della-Pietra (1996). It appears that the ﬁrst application of gradient algorithms
to maximum likelihood estimation in graphical models is due to Ackley, Hinton, and Sejnowski
(1985) in the context of Boltzmann machines. The importance of the method used to optimize
the likelihood was highlighted in the comparative study of Minka (2001a); this study focused on
learning for logistic regression, but many of the conclusions hold more broadly. Since then, sev-
eral better methods have been developed for optimizing likelihood. Successful methods include
conjugate gradient, L-BFGS (Liu and Nocedal 1989), and stochastic meta-descent (Vishwanathan
et al. 2006).
Conditional random ﬁelds were ﬁrst proposed by Laﬀerty et al. (2001). They have since been
applied in a broad range of applications, such as labeling multiple webpage on a website (Taskar
et al. 2002), image segmentation (Shental et al. 2003), or information extraction from text (Sutton
and McCallum 2005).
The application to protein-structure prediction in box 20.B is due to
Yanover et al. (2007).
The use of approximate inference in learning is an inevitable consequence of the intractability
of the inference problem. Several papers have studied the interaction between belief propaga-
tion and Markov network learning. Teh and Welling (2001) and Wainwright et al. (2003b) present
methods for certain special cases; in particular, Wainwright, Jaakkola, and Willsky (2003b) derive

20.9. Relevant Literature
999
the pseudo-moment matching argument. Inspired by the moment-matching behavior of learn-
ing with belief propagation, Sutton and McCallum (2005); Sutton and Minka (2006) deﬁne the
piecewise training objective that directly performs moment matching on all network potentials.
Wainwright (2006) provides a strong argument, both theoretical and empirical, for using the
same approximate inference method in training as will be used in performing the prediction
using the learned model. Indeed, he shows that, if an approximate method is used for inference,
then we get better performance guarantees if we use that same method to train the model than
if we train a model using exact inference. He also shows that it is detrimental to use an un-
stable inference algorithm (such as sum-product BP) in the inner loop of the learning algorithm.
Ganapathi et al. (2008) deﬁne the uniﬁed CAMEL formulation that encompasses learning and
inference in a single joint objective, allowing the nonconvexity of the BP objective to be taken
out of the inner loop of learning.
Although maximum (conditional) likelihood is the most commonly used objective for learning
Markov networks, several other objectives have been proposed. The earliest is pseudolikelihood,
proposed by Besag (1977b), of which several extensions have been proposed (Huang and Ogata
2002; McCallum et al. 2006). The asymptotic consistency of both the likelihood and the pseu-
dolikelihood objectives is shown by Gidas (1988). The statistical eﬃciency (convergence as a
function of the number of samples) of the pseudolikelihood estimator has also been analyzed
(for example, (Besag 1977a; Geyer and Thompson 1992; Guyon and Künsch 1992; Liang and Jordan
2008)).
The use of margin-based estimation methods for probabilistic models was ﬁrst proposed by
Collins (2002) in the context of parsing and sequence modeling, building on the voted-perceptron
algorithm (Freund and Schapire 1998). The methods described in this chapter build on a class of
large-margin methods called support vector machines (Shawe-Taylor and Cristianini 2000; Hastie
support vector
machine
et al. 2001; Bishop 2006), which have the important beneﬁt of allowing a large or even inﬁnite
feature space to be used and trained very eﬃciently. This formulation was ﬁrst proposed by
Altun, Tsochantaridis, and Hofmann (2003); Taskar, Guestrin, and Koller (2003), who proposed
two diﬀerent approaches for addressing the exponential number of constraints. Altun et al. use
a constraint-generation scheme, which was subsequently proven to require at most a polynomial
number of steps (Tsochantaridis et al. 2004). Taskar et al. use a closed-form polynomial-size
reformulation of the optimization problem that uses a clique tree-like data structure. Taskar,
Chatalbashev, and Koller (2004) also show that this formulation also allows tractable training for
networks where conditional probability products are intractable, but the MAP assignment can
be found eﬃciently. The contrastive divergence approach was introduced by Hinton (2002); Teh,
Welling, Osindero, and Hinton (2003), and was shown to work well in practice in various studies
(for example, (Carreira-Perpignan and Hinton 2005)). This work forms part of a larger trend of
training using a range of alternative, often contrastive, objectives. LeCun et al. (2007) provide an
excellent overview of this area.
Much discussion has taken place in the machine learning community on the relative merits
of discriminative versus generative training. Some insightful papers of particular relevance to
graphical models include the work of Minka (2005) and LeCun et al. (2007). Also of interest
are the theoretical analyses of Ng and Jordan (2002) and Liang and Jordan (2008) that discuss
the statistical eﬃciency of discriminative versus generative training, and provide theoretical
support for the empirical observation that generative models, even if not consistent with the
true underlying distribution, often work better in the sparse data case, but discriminative models

1000
Chapter 20. Learning Undirected Models
tend to work better as the amount of data grows.
The work of learning Markov networks with hidden variables goes back to the seminal paper
of Ackley, Hinton, and Sejnowski (1985), who used gradient ascent to train Boltzmann machines
with hidden variables. This line of work, largely dormant for many years, has seen a resurgence
in the work on deep belief networks (Hinton et al. 2006; Hinton and Salakhutdinov 2006), a
deep belief
networks
training regime for a multilayer restricted Boltzmann machine that iteratively tries to learn
deeper and deeper hidden structure in the data.
Parameter priors and regularization methods for log-linear models originate in statistics, where
they have long been applied to a range of statistical models. Many of the techniques described
here were ﬁrst developed for traditional statistical models such as linear or logistic regression,
and then extended to the general case of Markov networks and CRFs. See Hastie et al. (2001) for
some background on this extensive literature.
The problem of learning the structure of Markov networks has not received as much attention
as the task of Bayesian network structure learning. One line of work has focused on the problem
of learning a Markov network of bounded tree-width, so as to allow tractable inference. The
work of Chow and Liu (1968) shows that the maximum-likelihood tree-structured network can
be found in quadratic time.
A tree is a network of tree-width 1. Thus, the obvious generalization of is to learning the class
of Markov networks whose tree-width is at most k. Unfortunately, there is a sharp threshold
phenomenon, since Srebro (2001) proves that for any tree-width k greater than 1, ﬁnding the
maximum likelihood tree-width-k network is NP-hard. Interestingly, Narasimhan and Bilmes
(2004) provide a constraint-based algorithm for PAC-learning Markov networks of tree-width at
most k: Their algorithm is guaranteed to ﬁnd, with probability 1 −δ, a network whose relative
entropy is within ϵ of optimal, in polynomial time, and using a polynomial number of samples.
Importantly, their result does not contradict the hardness result of Srebro, since their analysis
applies only in the consistent case, where the data is derived from a k-width network. This
discrepancy highlights again the signiﬁcant diﬀerence between learnability in the consistent and
the inconsistent case. Several search-based heuristic algorithms for learning models with small
tree-width have been proposed (Bach and Jordan 2001; Deshpande et al. 2001); so far, none
of these algorithms have been widely adopted, perhaps because of the limited usefulness of
bounded tree-width networks.
Abbeel, Koller, and Ng (2006) provide a diﬀerent PAC-learnability result in the consistent case,
for networks of bounded connectivity. Their constraint-based algorithm is guaranteed to learn,
with high probability, a network
˜
M whose (symmetric) relative entropy to the true distribution
(ID( ˜P||P ∗) + ID(P ∗|| ˜P)) is at most ϵ. The complexity, both in time and in number of samples,
grows exponentially in the maximum number of assignments to any local neighborhood (a factor
and its Markov blanket). This result is somewhat surprising, since it shows that the class of low-
connectivity Markov networks (such as grids) is PAC-learnable, even though inference (including
computing the partition function) can be intractable.
Of highest impact has been the work on using local search to optimize a (regularized) like-
lihood. This line of work originated with the seminal paper of Della Pietra, Della Pietra, and
Laﬀerty (1997), who deﬁned the single-feature gain, and proposed the gain as an eﬀective
heuristic for feature selection in learning Markov network structure. McCallum (2003) describes
some heuristic approximations that allow this heuristic to be applied to CRFs.
The use of
L1-regularization for feature selection originates from the Lasso model proposed for linear re-

20.10. Exercises
1001
gression by Tibshirani (1996). It was ﬁrst proposed for logistic regression by Perkins et al. (2003);
Goodman (2004). Perkins et al. also suggested the gradient heuristic for feature selection and
the L1-based stopping rule. L1-regularized priors were ﬁrst proposed for learning log-linear
distributions by Riezler and Vasserman (2004); Dudík, Phillips, and Schapire (2004). The use of
L1-regularized objectives for learning the structure of general Markov networks was proposed
by Lee et al. (2006). Building on the results of Dudík et al., Lee et al. also showed that the
number of samples required to achieve close-to-optimal relative entropy (within the target class)
grows only polynomially in the size of the network. Importantly, unlike the PAC-learnability
results mentioned earlier, this result also holds in the inconsistent case.
Pseudolikelihood has also been used as a criterion for model selection.
Ji and Seymour
(1996) deﬁne a pseudolikelihood-based objective and show that it is asymptotically consistent,
in that the probability of selecting an incorrect model goes to zero as the number of training
examples goes to inﬁnity.
However, they did not provide a tractable algorithm for ﬁnding
the highest scoring model in the superexponentially large set of structures. Wainwright et al.
(2006) suggested the use of an L1-regularized pseudolikelihood for model selection, and also
proved a theorem that provides guarantees on the near-optimality of the learned model, using
a polynomial number of samples. Like the result of Lee et al. (2006), this result applies also in
the inconsistent case.
This chapter has largely omitted discussion of the Bayesian learning approach for Markov
networks, for both parameter estimation and structure learning. Although an exact approach
is computationally intractable, some interesting work has been done on approximate methods.
Some of this work uses MCMC methods to sample from the parameter posterior. Murray and
Ghahramani (2004) propose and study several diverse methods; owing to the intractability of the
posterior, all of these methods are approximate, in that their stationary distribution is only an
approximation to the desired parameter posterior. Of these, the most successful methods appear
to be a method based on Langevin sampling with approximate gradients given by contrastive
divergence, and a method where the acceptance probability is approximated by replacing the log
partition function with the Bethe free energy. Two more restricted methods (Møller et al. 2006;
Murray et al. 2006) use an approach called “perfect sampling” to avoid the need for estimating
the partition function; these methods are elegant but of limited applicability. Other approaches
approximate the parameter posterior by a Gaussian distribution, using either expectation prop-
agation (Qi et al. 2005) or a combination of a Bethe and a Laplace approximation (Welling and
Parise 2006a). The latter approach was also used to approximate the Bayesian score in order to
perform structure learning (Welling and Parise 2006b). Because of the fundamental intractability
of the problem, all of these methods are somewhat complex and computationally expensive, and
they have therefore not yet made their way into practical applications.
20.10
Exercises
Exercise 20.1
Consider the network of ﬁgure 20.2, where we assume that some of the factors share parameters. Let θy
i be
the parameter vector associated with all of the features whose scope is Yi, Yi+1. Let θxy
i,j be the parameter
vector associated with all of the features whose scope is Yi, Xj.
a. Assume that, for all i, i′, θy
i = θy
i′, and that for all i, i′ and j, θxy
i,j = θxy
i′,j. Derive the gradient update
for this model.

1002
Chapter 20. Learning Undirected Models
b. Now (without the previous assumptions) assume for all i and j, j′, θxy
i,j = θxy
i,j′. Derive the gradient
update for this model.
Exercise 20.2
In this exercise, we show how to learn Markov networks with shared parameters, such as a relational
relational Markov
network
Markov network (RMN).
a. Consider the log-linear model of example 6.18, where we assume that the Study-Pair relationship is
determined in the relational skeleton. Thus, we have a single template feature, with a single weight,
which is applied to all study pairs. Derive the likelihood function for this model, and the gradient.
b. Now provide a formula for the likelihood function and the gradient for a general RMN, as in deﬁni-
tion 6.14.
Exercise 20.3
Assume that our data are generated by a log-linear model Pθ∗that is of the form of equation (20.1). Show
that, as the number of data instances M goes to inﬁnity, with probability that approaches 1, θ∗is a global
optimum of the likelihood objective of equation (20.3). (Hint: Use the characterization of theorem 20.1.)
Exercise 20.4
Use the techniques described in this chapter to provide a method for performing maximum likelihood
estimation for a CPD whose parameterization is a generalized linear model, as in deﬁnition 5.10.
Exercise 20.5⋆
Show using Lagrange multipliers and the deﬁnitions of appendix A.5.4 that the problem of maximizing
IHQ(X) subject to equation (20.10) is dual to the problem of maximizing the log likelihood max ℓ(θ : D).
Exercise 20.6⋆
In this problem, we will show an analogue to theorem 20.2 for the problems of maximizing conditional
likelihood and maximizing conditional entropy.
Consider a data set D = {(y[m], x[m])}M
m=1 as in section 20.3.2, and deﬁne the following conditional
entropy maximization problem:
Maximum-Conditional-Entropy:
Find
Q(Y | X)
maximizing
PM
m=1 IHQ(Y | x[m])
subject to
M
X
m=1
IEQ(Y |x[m])[fk] =
M
X
m=1
fk(y[m], x[m])
i = 1, . . . , k.
(20.35)
Show that Q∗(Y | X) optimizes this objective if and only if Q∗= Pˆθ where Pˆθ maximizes ℓY |X(θ : D)
as in equation (20.6).
Exercise 20.7⋆
One of the earliest approaches for ﬁnding maximum likelihood parameters is called iterative proportional
iterative
proportional
scaling
scaling (IPS). The idea is essentially to use coordinate ascent to improve the match between the empirical
feature counts and the expected feature counts. In other words, we change θk so as to make IEPθ[fk]
closer to IED[fk]. Because our model is multiplicative, it seems natural to multiply the weight of instances
where fk(ξ) = 1 by the ratio between the two expectations. This intuition leads to the following update
rule:
θ′
k ←θk + ln IED[fk]
IEPθ[fk].
(20.36)

20.10. Exercises
1003
The IPS algorithm iterates over the diﬀerent parameters and updates each of them in turn, using this
update rule.
Somewhat surprisingly, one can show that each iteration increases the likelihood until it reaches a maxi-
mum point. Because the likelihood function is concave, there is a single maximum, and the algorithm is
guaranteed to ﬁnd it.
Theorem 20.5
Let θ be a parameter vector, and θ′ the vector that results from it after an application of equation (20.36).
Then ℓ(θ′ : D) ≥ℓ(θ : D) with equality if only if
∂
∂θk ℓ(θt : D) = 0.
In this exercise, you will prove this theorem for the special case where fk is binary-valued. More precisely,
let ∆(θk) denote the change in likelihood obtained from modifying a single parameter θk, keeping the
others ﬁxed. This expression was computed in equation (20.34). You will now show that the IPS update
step for θk maximizes a lower bound on this single parameter gain.
Deﬁne
˜∆(θk)
=
(θ′
k −θk)IED[fk] −Z(θ′)
Z(θ) + 1
=
(θ′
k −θk)IED[fk] −IEPθ[1 −fk] −eθ′
k−θkIEPθ′ [fk] + 1.
a. Show that ∆(θ′
k) ≥˜∆(θ′
k). (Hint: use the bound ln(x) ≤x −1.)
b. Show that θk + ln IED[fk]
IEPθ [fk] = arg maxθ′
k ˜∆(θ′
k).
c. Use these two facts to conclude that IPS steps are monotonically nondecreasing in the likelihood, and
that convergence is achieved only when the log-likelihood is maximized.
d. This result shows that we can view IPS as performing coordinatewise ascent on the likelihood surface.
At each iteration we make progress along on dimension (one parameter) while freezing the others. Why
is coordinate ascent a wasteful procedure in the context of optimizing the likelihood?
Exercise 20.8
Consider the following hyperbolic prior for parameters in log-linear models.
hyperbolic prior
P(θ) =
1
(eθ + e−θ) /2.
a. Derive a gradient-based update rule for this parameter prior.
b. Qualitatively describe the expected behavior of this parameter prior, and compare it to those of the L2
or L1 priors discussed in section 20.4. In particular, would you expect this prior to induce sparsity?
Exercise 20.9
We now consider an alternative local training method for Markov networks, known as piecewise training.
piecewise
training
For simplicity, we focus on Markov networks parameterized via full table factors. Thus, we have a set of
factors φc(Xc), where Xc is the scope of factor c, and φc(xj
c) = exp(θcj). For a particular parameter
assignment θ, we deﬁne Zc(θ) to be the local partition function for this factor in isolation:
Zc(θc) =
X
xc
φc(xc),
where θc is the parameter vector associated with the factor φc(Xc). We can approximate the global
partition function in the log-likelihood objective of equation (20.3) as a product of the local partition
functions, replacing Z(θ) with Q
c Zc(θc).

1004
Chapter 20. Learning Undirected Models
a. Write down the form of the resulting objective, simplify it, and derive the assignment of parameters
that optimizes it.
b. Compare the result of this optimization to the result of the pseudo-moment matching approach de-
scribed in section 20.5.1.
Exercise 20.10⋆
In this exercise, we analyze the following simpliﬁcation of the CAMEL optimization problem of equa-
CAMEL
tion (20.15):
Simple-Approx-Maximum-Entropy:
Find
Q
maximizing
P
Ci∈U IHβi(Ci)
subject to
IEβi[fi]
=
IED[fi]
i = 1, . . . , k
X
ci
βi(ci)
=
1
i = 1, . . . , k
Q ≥0
Here, we approximate both the objective and the constraints. The objective is approximated by the removal
of all of the negative entropy terms for the sepsets. The constraints are relaxed by removing the requirement
that the potentials in Q be locally consistent (sum-calibrated) — we now require only that they be legal
probability distributions.
Show that this optimization problem is the Lagrangian dual of the piecewise training objective in exer-
cise 20.9.
Exercise 20.11⋆
Consider a setting, as in section 20.3.2, where we have two sets of variables Y and X. Multiconditional
multiconditional
training
training provides a spectrum between pure generative and pure discriminative training by maximizing the
following objective:
αℓY |X(θ : D) + (1 −α)ℓX|Y (θ : D).
(20.37)
Consider the model structure shown in ﬁgure 20.2, and a partially labeled data set D, where in each
instance m we observe all of the feature variables x[m], but only the target variables in O[m].
Write down the objective of equation (20.37) for this case and compute its derivative.
Exercise 20.12⋆
Consider the problem of maximizing the approximate log-likelihood shown in equation (20.16).
a. Derive the gradient of the approximate likelihood, and show that it is equivalent to utilizing an impor-
tance sampling estimator directly to approximate the expected counts in the gradient of equation (20.4).
b. Characterize properties of the maximum point (when the gradient is 0). Is such a maximum always
attainable? Prove or suggest a counterexample.
Exercise 20.13⋆⋆
One approach to providing a lower bound to the log-likelihood is by upper-bounding the partition function.
Assume that we can decompose our model as a convex combination of (hopefully) simpler models, each
with a weight αk and a set of parameters ψk. We deﬁne these submodels as follows: ψk(θ) = wk • θ,
where we require that, for any feature i,
X
k
αkwk
i = 1.
(20.38)

20.10. Exercises
1005
a. Under this assumption, prove that
ln Z(θ) ≤
X
k
αk ln Z(wk • θ).
(20.39)
This result allows us to deﬁne an approximate log-likelihood function:
1
M ℓ(θ : D) ≥ℓconvex(θ : D) =
X
i
θiIED[fi] −
X
k
αk ln Z(wk • θ).
b. Assuming that the submodels are more tractable, we can eﬃciently evaluate this lower bound, and also
compute its derivatives to be used during optimization. Show that
∂
∂θi ℓconvex(θ : D) = IED[fi] −
X
k
αkwk
i IEPwk •θ[fi].
(20.40)
c. We can provide a bound on the error of this approximation. Speciﬁcally, show that:
1
M ℓ(θ : D) −ℓconvex(θ : D) =
X
k
αkID(Pθ||Pwk•θ),
where the KL-divergence measures are deﬁned in terms of the natural logarithm. Thus, we see that
the error is an average of the divergence between the true distribution and each of the approximating
submodels.
d. The justiﬁcation for this approach is that we can make the submodels simpler than the original model
by having some parameters be equal to 0, thereby eliminating the resulting feature from the model
structure. Other than this constraint, however, we still have considerable freedom in choosing the
submodel weight vectors wk. Assume that each weight vector {wk} maximizes ℓconvex(θ : D) subject
to the constraint of equation (20.38) plus additional constraints requiring that certain entries wk
i be
equal to 0. Show that if i, k and l are such that θi ̸= 0 and neither wk
i nor wl
i is constrained to be
zero, then
IEPwk•θ[fi] = IEPwl•θ[fi].
Conclude from this result that for each such i and k, we have that
IEPwk•θ[fi] = IED[fi].
Exercise 20.14
Consider a particular parameterization (θ, η) to Max-margin. Show how we can use second-best MAP
inference to either ﬁnd a violated constraint or guarantee that all constraints are satisﬁed.
Exercise 20.15
Let H∗be a Markov network where the maximum degree of a node is d∗. Show that if we have an
inﬁnitely large data set D generated from H∗(so that independence tests are evaluated perfectly), then the
Build-PMap-Skeleton procedure of algorithm 3.3 reconstructs the correct Markov structure H∗.
Exercise 20.16⋆
Prove proposition 20.6. (Hint: Take the derivative of equation (20.34) and set it to zero.)
Exercise 20.17
In this exercise, you will prove proposition 20.7, which allows us to ﬁnd a closed-form optimum to multiple
features in a log-linear model.
a. Prove the following proposition.

1006
Chapter 20. Learning Undirected Models
Proposition 20.8
Let θ0 be a current setting of parameters for a log-linear model, and suppose that f1, . . . , fl are mutually
exclusive binary features, that is, there is no ξ and i ̸= j, so that fi(ξ) = 1 and fj(ξ) = 1. Then,
max
θ1,...,θl

scoreL((θ1, . . . , θl, θ0
−{1,...,l}) : D) −scoreL(θ0 : D)

= ID(ˆp||p0),
where ˆp is a distribution over l+1 values with ˆpi = IED[fi], and p0 is a distribution with p0(i) = Pθ(fi).
b. Use this proposition to prove proposition 20.7.
Exercise 20.18
Derive an analog to proposition 20.6 for the case of the L1 regularized log-likelihood objective.

Part IV
Actions and Decisions


21
Causality
21.1
Motivation and Overview
So far, we have been somewhat ambivalent about the relation between Bayesian networks
and causality.
On one hand, from a formal perspective, all of the deﬁnitions refer only to
probabilistic properties such as conditional independence. The BN structure may be directed,
but the directions of the arrows do not have to be meaningful. They can even be antitemporal.
Indeed, we saw in our discussion of I-maps that we can take any ordering on the nodes and
create a BN for any distribution. On the other hand, it is common wisdom that a “good” BN
structure should correspond to causality, in that an edge X →Y often suggests that X “causes”
Y , either directly or indirectly. The motivation for this statement is pragmatic: Bayesian networks
with a causal structure tend to be sparser and more natural. However, as long as the network
structure is capable of representing the underlying joint distribution correctly, the answers that
we obtain to probabilistic queries are the same, regardless of whether the network structure
corresponds to some notion of causal inﬂuence.
Given this observation, is there any deeper value to imposing a causal semantics on a Bayesian
network? In this chapter, we discuss a type of reasoning for which a causal interpretation of
the network is critical — reasoning about situations where we intervene in the world, thereby
interfering in the natural course of events. For example, we may wish to know if an intervention
where we prevent smoking in all public places is likely to decrease the frequency of lung cancer.
To answer such queries, we need to understand the causal relationships between the variables
in our model.
In this chapter, we provide a framework for interpreting a Bayesian network as a causal model
whose edges have causal signiﬁcance. Not surprisingly, this interpretation distinguishes between
models that are equivalent in their ability to represent probabilistic correlations. Thus, although
the two networks X →Y and Y →X are equivalent as probabilistic models, they will turn out
to be very diﬀerent as causal models.
21.1.1
Conditioning and Intervention
As we discussed, for standard probabilistic queries it does not matter whether our model is
causal or not. It matters only that it encode the “right” distribution. The diﬀerence between
causal models and probabilistic models arise when we care about interventions in the model —
situations where we do not simply observe the values that variables take but can take actions

1010
Chapter 21. Causality
that can manipulate these values.
In general, actions can aﬀect the world in a variety of ways, and even a single action can have
multiple eﬀects. Indeed, in chapter 23, we discuss models that directly incorporate agent actions
and allow for a range of eﬀects. In this chapter, however, our goal is to isolate the speciﬁc
issue of understanding causal relationships between variables.
One approach to modeling

causal relationships is using the notion of ideal interventions — interventions of the
ideal intervention
form do(Z := z), which force the variable Z to take the value z, and have no other
immediate eﬀect. An ideal intervention is equivalent to a dedicated action whose only eﬀect is
setting Z to z. However, we can consider such an ideal intervention even when such an action
does not exist in the world. For example, consider the question of whether a particular mutation
in a person’s DNA causes a particular disease. This causal question can be formulated as the
question of whether an ideal intervention, whose only eﬀect is to generate this mutation in a
person’s DNA, would lead to the disease. Note that, even if such a process were ethical, current
technology does not permit an action whose only eﬀect is to mutate the DNA in all cells of a
human organism. However, understanding the causal connection between the mutation and the
disease can be a critical step toward ﬁnding a cure; the ideal intervention provides us with a
way of formalizing this question and trying to provide an answer.
More formally, we consider a new type of “conditioning” on an event of the form do(Z :=
z), often abbreviated do(z); this information corresponds to settings where an agent directly
manipulated the world, to set the variable Z to take the value z with probability 1. We are
now interested in answering queries of the form P(Y | do(z)), or, more generally, P(Y |
do(z), X = x). These queries are called intervention queries. They correspond to settings
intervention
query
where we set the variables in Z to take the value z, observe the values x for the variables in
X, and wish to ﬁnd the distribution over the variables Y . Such queries arise naturally in a
variety of settings:
•
Diagnosis and Treatment: “If we get a patient to take this medication, what are her chances
of getting well?” This query can be formulated as P(H | do(M := m1)), where H is the
patient’s health, and M = m1 corresponds to her taking the medication. Note that this query
is not the same as P(H | m1). For example, if patients who take the medication on their
own are more likely to be health-conscious, and therefore healthier in general, the chances
of P(H | m1) may be higher than is warranted for the patient in question.
•
Marketing: “If we lower the price of hamburgers, will people buy more ketchup?” Once
again, this query is not a standard observational query, but rather one in which we intervene
in the model, and thereby possibly change its behavior.
•
Policy Making: “If we lower the interest rates, will that give rise to inﬂation?”
•
Scientiﬁc Discovery: “Does smoking cause cancer?” When we formalize it, this query is an
intervention query, meaning: “If we were to force someone to smoke, would they be more
likely to get cancer?”
A diﬀerent type of causal query arises in situations where we already have some information
about the true state of the world, and want to inquire about the state the world would be in
had we been able to intervene and set the values of certain variables. For example, we might
want to know “Would the U.S. have joined World War II had it not been for the attack on Pearl
Harbor?” Such queries are called counterfactual queries, because they refer to a world that we
counterfactual
query

21.1. Motivation and Overview
1011
know did not happen. Intuitively, our interpretation for such a query is that it refers to a world
that diﬀers only in this one respect. Thus, in this counterfactual world, Hitler would still have
come into power in Germany, Poland would still have been invaded, and more. On the other
hand, events that are direct causal consequences of the variable we are changing are clearly
going to be diﬀerent. For example, in the counterfactual world, the USS Arizona (which sank in
the attack) would not (with high probability) currently be at the bottom of Pearl Harbor.
At ﬁrst glance, counterfactual analysis might seem somewhat pointless and convoluted (who
cares about what would have happened?).
However, such queries actually arise naturally in
several settings:
•
Legal liability cases: “Did the driver’s intoxicated state cause the accident?” In other words,
would the accident have happened had the driver not been drunk? Here, we may want to
preserve many other aspects of the world, for example, that it was a rainy night (so the road
was slippery).
•
Treatment and Diagnosis: “We are faced with a car that does not start, but where the lights
work; will replacing the battery make the car start?” Note that this is not an intervention
query; it is a counterfactual query: we are actually asking whether the car would be working
now had we replaced the battery. As in the previous example, we want to preserve as much
of our scenario as possible. For example, given our observation that the lights work, the
problem probably is not with the battery; we need to account for this conclusion when
reasoning about the situation where the battery has been replaced.
Even without a formal semantics for a causal model, we can see that the answer to an
intervention query P(Y | do(z), X = x) is generally quite diﬀerent from the answer to its
corresponding probabilistic query P(Y | Z = z, X = x).
Example 21.1
Let us revisit our simple Student example of section 3.1.3.1, and consider a particular student Gump.
As we have already discussed, conditioning on an observation that Gump receives an A in the class
increases the probability that he has high intelligence, his probability of getting a high SAT score,
and his probability of getting a good job.
By contrast, consider a situation where Gump is lazy, and rather than working hard to get an A
in the class, he pays someone to hack into the university registrar’s database and change his grade in
the course to an A. In this case, what is his probability of getting a good job? Intuitively, the company
where Gump is applying only has access to Gump’s transcript; thus, the company’s response to a
manipulated grade would be the same as the response to an authentic grade. Therefore, we would
expect P(J | do(g1)) = P(J | g1). What about the other two probabilities? Intuitively, we feel
that the manipulation to Gump’s grade should not aﬀect our beliefs about his intelligence, nor about
his SAT score. Thus, we would expect P(i1 | do(g1)) = P(i1) and P(s1 | do(g1)) = P(s1).
Why is our response to these queries diﬀerent? In all three cases, there is a strong correlation
between Gump’s grade and the variable of interest. However, we perceive the correlation between
Gump’s grade and his job prospects as being causal. Thus, changes to his grade will directly
aﬀect his chances of being hired. The correlation between intelligence and grade arises because
of an opposite causal connection: intelligence is a causal factor in grade. The correlation between
Gump’s SAT score and grade arises due to a third mechanism — their joint dependence on
Gump’s intelligence. Manipulating Gump’s grade does not change his intelligence or his chances

1012
Chapter 21. Causality
of doing well in the class. In this chapter, we describe a formal framework of causal models
that provides a rigorous basis for answering such queries and allows us to distinguish between
these diﬀerent cases. As we will see, this framework can be used to answer both intervention
and counterfactual queries. However, the latter require much ﬁner-grained information, which
may be diﬃcult to acquire in practice.
21.1.2
Correlation and Causation
As example 21.1 illustrates, a correlation between two variables X and Y can arise in multiple
settings: when X causes Y , when Y causes X, or when X and Y are both eﬀects of a single
cause. This observation immediately gives rise to the question of identiﬁability of causal models:
If we observe two variables X, Y to be probabilistically correlated in some observed distribution,
what can we infer about the causal relationship between them. As we saw, diﬀerent relationships
give rise to very diﬀerent answers to causal queries.
This problem is greatly complicated by the broad range of reasons that may lead to an
observed correlation between two variables X and Y . As we saw in example 21.1, when some
variable W causally aﬀects both X and Y , we generally observe a correlation between them.
If we know about the existence of W and can observe it, we can disentangle the correlation
between X and Y that is induced by W and compute the residual correlation between X
and Y that may be attributed to a direct causal relationship. In practice, however, there is

a huge set of possible latent variables, representing factors that exist in the world but
latent variable
that we cannot observe and often are not even aware of. A latent variable may induce
correlations between the observed variables that do not correspond to causal relations
between them, and hence forms a confounding factor in our goal of determining causal
confounding
factor
interactions.
As we discussed in section 19.5, when our task is pure probabilistic reasoning, latent variables
need not be modeled explicitly, since we can always represent the joint distribution over the
observable variables only using a probabilistic graphical model. Of course, this marginalization
process can lead to more complicated models (see, for example, ﬁgure 16.1), and may therefore
be undesirable. We may therefore choose to model certain latent variables explicitly, in order
to simplify the resulting network structure. Importantly, however, for the purpose of answering
probabilistic queries, we do not need to model all latent variables. As long as our model Bobs over
the observable variables allows us to capture exactly the correct marginal distribution over the
observed variables, we can answer any query as accurately with Bobs as with the true network,
where the latent variables are included explicitly.
However, as we saw, the answer to a causal query over X, Y is quite diﬀerent when a
correlation between them is due to a causal relationship and when it is induced by a latent
variable. Thus, for the purposes of causal inference, it is critical to disentangle the component
in the correlation between X and Y that is due to causal relationships and the component due
to these confounding factors. Unfortunately, this requirement poses a major challenge, since it is
virtually impossible, in complex real-world settings, to identify all of the relevant latent variables
and quantify their eﬀects.
Example 21.2
Consider a situation where we observe a signiﬁcant positive correlation in our patient population
between taking PeptAid, an antacid medication (T), and the event of subsequently developing a

21.1. Motivation and Overview
1013
stomach ulcer (O). Because taking PeptAid precedes the ulcer, we might be tempted to conclude
that PeptAid causes stomach ulcers. However, an alternative explanation is that the correlation can
be attributed to a latent common cause — preulcer discomfort: individuals suﬀering from preulcer
discomfort were more likely to take PeptAid and ultimately more likely to develop ulcers. Even if we
account for this latent variable, there are many others that can have a similar eﬀect. For example,
some patients who live a more stressful lifestyle may be more inclined to eat irregular meals and
therefore more likely to require antacid medication; the same patients may also be more susceptible
to stomach ulcers.
Latent variables are only one type of mechanism that induces a noncausal correlation between
variables. Another important class of confounding factors involves selection bias. Selection bias
selection bias
arises when the population that the distribution represents is a segment of the population that
exhibits atypical behavior.
Example 21.3
Consider a university that sends out a survey to its alumni, asking them about their history at the
institution. Assume that the observed distribution reveals a negative correlation between students
who participated in athletic activities (A) and students whose GPA was high (G). Can we conclude
from this ﬁnding that participating in athletic activities reduces one’s GPA? Or that students with
a high GPA tend not to participate in athletic activities? An alternative explanation is that the
respondents to the survey (S = s1) are not a representative segment population: Students who
did well in courses tended to respond, as did students who participated in athletic activities (and
therefore perhaps enjoyed their time at school more); students who did neither tended not to
respond. In other words, we have a causal link from A to S and from G to S. In this case,
even if A and G are independent in the overall distribution over the student population, we may
have a correlation in the subpopulation of respondents. This is an instance of standard intercausal
reasoning, where P(a1 | s1) > P(a1 | g1, s1). But without accounting for the possible bias in
selecting our population, we may falsely explain the correlation using a causal relationship.
There are many other examples where correlations might arise due to noncausal reasons. One
reason involves a mixture of diﬀerent populations.
Example 21.4
It is commonly accepted that young girls develop verbal ability at an earlier age than boys. Con-
versely, boys tend to be taller and heavier than girls. There is certainly no (known) correlation
between height and verbal ability in either girls or boys separately. However, if we simply measure
height and verbal ability across all children (of the same age), then we may well see a negative
correlation between verbal ability and height.
This type of situation is a special case of a latent variable, denoting the class to which the
instance belongs (gender, in this case). However, it deserves special mention both because it
is quite common, and because these class membership variables are often not perceived as
“causes” and may therefore be ignored when looking for a confounding common cause.
A similar situation arises when the distribution we obtain arises from two time series, each
of which has a particular trend.
Example 21.5
Consider data obtained by measuring, in each year over the past century, the average height of the
adult population in the world in that year (H), and the total size of the polar caps in that year (S).

1014
Chapter 21. Causality
Because average population height has been increasing (due to improved nutrition), and the total
size of the polar caps has been decreasing (due to global warming), we would observe a negative
correlation between H and S in these data. However, we would not want to conclude that the size
of the polar caps causally inﬂuences average population height.
In a sense, this situation is also an instance of a latent variable, which in this case is time.

Thus, we see that the correlation between a pair of variables X and Y may be a
consequence of multiple mechanisms, where some are causal and others are not. To
answer a causal query regarding an intervention at X, we need to disentangle these
diﬀerent mechanisms, and to isolate the component of the correlation that is due to the
causal eﬀect of X on Y . A large part of this chapter is devoted to addressing this challenge.
causal eﬀect
21.2
Causal Models
We begin by providing a formal framework for viewing a Bayesian network as a causal model.
A causal model has the same form as a probabilistic Bayesian network. It consists of a directed
causal model
acyclic graph over the random variables in the domain. The model asserts that each variable X
is governed by a causal mechanism that (stochastically) determines its value based on the values
causal
mechanism
of its parents. That is, the value of X is a (stochastic) function of the values of its parents.
A causal mechanism takes the same form as a standard CPD. For a node X and its parents
U, the causal model has a stochastic function from the values of U to the values of X. In other
words, for each value u of U, it speciﬁes a distribution over the values of X. The diﬀerence
is in the interpretation of the edges. In a causal model, we assume that X’s parents are its
direct causes (relative to the variables represented in the model). In other words, we assume
that causality ﬂows in the direction of the edges, so that X’s value is actually determined via
the stochastic function implied by X’s CPD.
The assumption that CPDs correspond to causal mechanisms forms the basis for the treatment
of intervention queries. When we intervene at a variable X, setting its value to x, we replace
its original causal mechanism with one that dictates that it take the value x. This manipulation
corresponds to replacing X’s CPD with a diﬀerent one, where X = x with probability 1,
regardless of anything else.
Example 21.6
For instance, in example 21.1, if Gump changes his grade to an A by hacking into the registrar’s
database, the result is a model where his grade is no longer determined by his performance in the
class, but rather set to the value A, regardless of any other aspects of the situation. An appropriate
graphical model for the postintervention situation is shown in ﬁgure 21.1a. In this network, the
Grade variable no longer depends on Intelligence or Diﬃculty, nor on anything else. It is simply set
to take the value A with probability 1.
The model in ﬁgure 21.1a is an instance of the mutilated network, a concept introduced in
mutilated
network
deﬁnition 12.1. Recall that, in the mutilated network BZ=z, we eliminate all incoming edges into
each variable Zi ∈Z, and set its value to be zi with probability 1.
Based on this intuition, we can now deﬁne a causal model as a model that can answer
intervention queries using the appropriate mutilated network.
Deﬁnition 21.1
A causal model C over X is a Bayesian network over X, which, in addition to answering proba-
causal model

21.2. Causal Models
1015
Grade
Job
SAT
Intelligence
Difﬁculty
Grade
Job
SAT
Intelligence
Difﬁculty
Grade
Job
SAT
Intelligence
Difﬁculty
(a)
(c)
(b)
Figure 21.1
Mutilated Student networks representing interventions (a) Mutilated Student network with
an intervention at G. (b) An expanded Student network, with an additional arc S →J. (c) A mutilated
network from (b), with an intervention at G.
bilistic queries, can also answer intervention queries P(Y | do(z), x), as follows:
intervention
query
PC(Y | do(z), x) = PCZ=z(Y | x).
Example 21.7
It is easy to see that this approach deals appropriately with example 21.1.
Let Cstudent be the
appropriate causal model. When we intervene in this model by setting Gump’s grade to an A, we
obtain the mutilated network shown in ﬁgure 21.1a. The distribution induced by this network over
Gump’s SAT score is the same as the prior distribution over his SAT score in the original network.
Thus,
P(S | do(G := g1)) = PCstudent
G=g1 (S) = PCstudent(S),
as we would expect. Conversely, the distribution induced by this network on Gump’s job prospects
is PCstudent(J | G = g1).
Note that, in general, the answer to an intervention query does not necessarily reduce to the
answer to some observational query.
Example 21.8
Assume that we start out with a somewhat diﬀerent Student network, as shown in ﬁgure 21.1b, which
contains an edge from the student’s SAT score to his job prospects (for example, because the recruiter
can also base her hiring decision on the student’s SAT scores). Now, the query PCstudent(J | do(g1))
is answered by the mutilated network of ﬁgure 21.1c. In this case, the answer to the query is clearly
not PCstudent(J), due to the direct causal inﬂuence of his grade on his job prospects. On the other
hand, it is also not equal to PCstudent(J | g1), because this last expression also includes the inﬂuence
via the evidential trail G ←I →S →J, which does not apply in the mutilated model.
The ability to provide a formal distinction between observational and causal queries can help
resolve some apparent paradoxes that have been the cause of signiﬁcant debate. One striking
example is Simpson’s paradox, a variant of which is the following:
Simpson’s
paradox
Example 21.9
Consider the problem of trying to determine whether a drug is beneﬁcial in curing a particular
disease within some population of patients. Statistics show that, within the population, 57.5 percent

1016
Chapter 21. Causality
Drug
Cure
Gender
Drug
Cure
Gender
Causal network for
Simpson’s paradox
Mutilated network for
answering P(c1|do(d1))
Figure 21.2
Causal network for Simpson’s paradox
of patients who took the drug (D) are cured (C), whereas only 50 percent of the patients who did
not take the drug are cured. Given these statistics, we might be inclined to believe that the drug is
beneﬁcial. However, more reﬁned statistics show that within the subpopulation of male patients, 70
percent who took the drug are cured, whereas 80 percent of those who did not take the drug are
cured. Moreover, within the subpopulation of female patients, 20 percent of who took the drug are
cured, whereas 40 percent of those who did not take the drug are cured. Thus, despite the apparently
beneﬁcial eﬀect of the drug on the overall population, the drug appears to be detrimental to both
men and women! More precisely, we have that:
P(c1 | d1)
>
P(c1 | d0)
P(c1 | d1, G = male)
<
P(c1 | d0, G = male)
P(c1 | d1, G = female)
<
P(c1 | d0, G = female).
How is this possible?
This case can occur because taking the drug is correlated with gender: men are much more
likely to take the drug than women. In this particular example, 75 percent of men take the drug,
whereas only 25 percent of women do. With these parameters, even if men and women are equally
represented in the population of patients, we obtain the surprising behavior described earlier.
The conceptual diﬃculty behind this paradox is that it is not clear which statistics one should use
when deciding whether to prescribe the drug to a patient: the general ones or the ones conditioned
on gender. In particular, it is not diﬃcult to construct examples where this reversal continues, in
that conditioning on yet another variable leads one to the conclusion that the drug is beneﬁcial
after all, and conditioning on one more reverses the conclusion yet again. So how can we decide
which variables we should condition on?
The causal framework provides an answer to this question. The appropriate query we need to
answer in determining whether to prescribe the drug is not P(c1 | d1), but rather P(c1 | do(d1)).
Figure 21.2 shows the causal network corresponding to this situation, and the mutilated network
required for answering the query. We will show how we can use the structure of the causal network
to answer queries such as P(c1 | do(d1)). As we will see in example 21.14, the answer to this query
shows that the drug is not beneﬁcial, as expected.

21.3. Structural Causal Identiﬁability
1017
21.3
Structural Causal Identiﬁability
The framework of the previous section provides us with a mechanism for answering intervention
queries, given a fully speciﬁed causal model. However, fully specifying a causal model is often
impossible. As we discussed, there are often a multitude of (generally unknown) factors that are
latent, and that can induce correlations between the variables. In most cases, we cannot fully
specify the causal connection between the latent variables and the variables in our model.
In general, the only thing that we can reasonably hope to obtain is the marginal distribution
over the observed variables in the model.
As we discussed, for probabilistic queries, this
marginal distribution suﬃces (assuming it can be acquired with reasonable accuracy). However,
for intervention queries, we must disentangle the causal inﬂuence of X and Y from other
factors leading to correlations between them. It is far from clear how we could ever accomplish
this goal.
Example 21.10
Consider a pair of variables X, Y with an observed correlation between them, and imagine that
our task is to determine P(Y | do(X)). Let us even assume that X temporally precedes Y , and
therefore we know that Y cannot cause X. However, if we consider the possibility that at least
some of the correlation between X and Y is due to a hidden common cause, we have no way of
determining how much eﬀect perturbing X would have on Y . If all of the correlation is due to a
causal link, then P(Y | do(X)) = P(Y | X); conversely, if all of the correlation is due to the
hidden common cause, then P(Y | do(X)) = P(Y ). And, in general, any value between those
two distributions is possible.
Thus, given that latent variables are inevitable in many settings, it appears that the situation
regarding causal queries is hopeless. Fortunately, as we now show, we can sometimes answer

causal questions in models involving latent variables using observed correlations alone.
More precisely, in this section, we attempt to address to question of which intervention queries
are identiﬁable, that is, can be answered using only conditional probabilities involving observable
identiﬁability
variables. Probabilities over observed variables can be estimated from data or elicited from an
expert. Thus, if we can reduce the answer to a query to an expression involving only such
probabilities, we may be able to provide a robust and accurate answer to it.
21.3.1
Query Simpliﬁcation Rules
The key observation in this section is that the structure of a causal model give rise to certain
equivalence rules over interventional queries, which allow one query to be replaced by an equiv-
alent one that may have a simpler form. By applying one or more of these simpliﬁcation steps,
we may be able to convert one causal query to another query that involves no interventions,
and can therefore be answered using observational data alone.
These rules can be deﬁned in terms of an augmented causal model that encodes the possible
augmented
causal model
eﬀect of interventions explicitly within the graph structure. More precisely, we view the process
of an intervention in terms of a new decision variable (see chapter 23) bZ that determines whether
decision variable
we intervene at Z, and if so, what its value is. The variable bZ takes on values in {ϵ} ∪Val(Z).
If bZ = ϵ, then Z behaves as a random variable whose distribution is determined by its usual
CPD P(Z | PaZ); if bZ = z, then it deterministically sets the value of Z to be z with probability
1. Let bZ denote the set { bZ : Z ∈Z}.

1018
Chapter 21. Causality
Note that, in those cases where Z’s value is deterministically set by one parent, all of Z’s
other parents U become irrelevant, and so we can eﬀectively remove all edges from U to Z.
Let G† be the augmented model for G. Let G†
Z be the graph obtained from G† except that every
Z ∈Z has only the single parent bZ. Note that G†
Z is similar to the mutilated network used
in deﬁnition 21.1 to deﬁne the semantics of intervention queries; the only diﬀerence is that we
now make the interventions themselves explicit. As we will now see, this diﬀerence allows us to
study the eﬀect of one intervention within a model that contain others.
Based on this construction, we now deﬁne three query simpliﬁcation rules. The ﬁrst simply
intervention
query
simpliﬁcation
allows us to insert or delete observations into a query.
Proposition 21.1
Let C be a causal model over the graph structure G. Then:
P(Y | do(Z := z), X = x, W = w) = P(Y | do(Z := z), X = x),
if W is d-separated from Y given Z, X in the graph G†
Z.
This rule is a simple consequence of the fact that probabilities of intervention queries are deﬁned
relative to the graph G†
Z, and the correspondence between independence and d-separation in
this graph.
The second rule is subtler, and it allows us to replace an intervention with the corresponding
observation.
Proposition 21.2
Let C be a causal model over the graph structure G. Then:
P(Y | do(Z := z), do(X := x), W = w) = P(Y | do(Z := z), X = x, W = w),
if Y is d-separated from c
X given X, Z, W in the graph G†
Z.
Intuitively, this rule holds because it tells us that we get no more information regarding Y from
the fact that an intervention took place at X than the values x themselves. In other words,
knowing that X = x, we do not care whether these values were obtained as a result of an
intervention or not. This criterion is also equivalent to asking whether X have requisite CPD for
requisite CPD
the query P(Y | do(Z := z), X = x, W = w), as deﬁned in exercise 3.20. The relationship
is not surprising, because an intervention at a variable X ∈X corresponds exactly to changing
its CPD; if our query is oblivious to changes in the CPD (given X), then we should not care
whether there was an intervention at X or not.
As a simple example, consider the case where Z = ∅and W = ∅, and where we have single
variables X, Y . In this case, the rule reduces to the assertion that
P(Y | do(X := x)) = P(Y | X = x),
if Y is d-separated from b
X given X in the graph G†. The separation property holds if the only
trails between X and Y in G emanate causally from X (that is, go through its children). Indeed,
in this case, intervening at X has the same eﬀect on Y as observing X. Conversely, if we have
an active trail from b
X to Y given X, then it must go through a v-structure activated by X. In
this case, Y is not a descendant of X, and an observation of X has a very diﬀerent eﬀect than
an intervention at X. The proof of this theorem is left as an exercise (exercise 21.1).

21.3. Structural Causal Identiﬁability
1019
Example 21.11
Consider again the network of ﬁgure 21.1b, and assume that the student somehow manages to cheat
on the SAT exam and get a higher SAT score. We are interested in the eﬀect on the student’s
grade, that is, in evaluating a query of the form P(G | do(S), J, I). Consider the augmented
model G†, which contains a new decision parent bS for the node S. In this graph, we would have
that G is d-separated from bS given S, J and I. Thus, the theorem would allow us to conclude
that P(G | do(S), J, I) = P(G | S, J, I). To provide another intuition for this equality, note
that, in the original graph, there are two possible trails between G and S: G →J ←S and
G ←I →S. The eﬀects of the ﬁrst trail remain unchanged in the mutilated network: there is no
diﬀerence between an intervention at S and an observation at S, relative to the trail G →J ←S.
There is a diﬀerence between these two cases relative to the trail G ←I →S, but that trail is
blocked by the observation at I, and hence there is no eﬀect to changing the intervention at S to
an observation. Note that this last argument would not apply to the query P(G | do(S), J), and
indeed, the theorem would not allow us to conclude that P(G | do(S), J) = P(G | S, J).
The ﬁnal rule allows us to introduce or delete interventions, in the same way that proposi-
tion 21.1 allows us to introduce or delete observations.
Proposition 21.3
Let C be a causal model over the graph structure G. Then:
P(Y | do(Z := z), do(X := x), W = w) = P(Y | do(Z := z), W = w),
if Y is d-separated from c
X given Z, W in the graph G†
Z.
This analysis can also be interpreted in terms of requisite CPDs. Here, the premise is equivalent
to stating that the CPDs of the variables in X are not requisite for the query even when their
values are not observed. In this case, we can ignore both the knowledge of the intervention and
the knowledge regarding the values imposed by the intervention.
Again, we can obtain intuition by considering the simpler case where Z = ∅, W = ∅, and
X is a single variable X. Here, the rule reduces to:
P(Y | do(X := x)) = P(Y ),
if Y is d-separated from b
X in the graph G†. The intuition behind this rule is fairly straight-
forward. Conditioning on b
X corresponds to changing the causal mechanism for X. If the
d-separation condition holds, this operation provably has no eﬀect on Y . From a diﬀerent per-
spective, changing the causal mechanism for X can only aﬀect Y causally, via X’s children. If
there are no trails from b
X to Y without conditioning on X, then there are no causal paths from
X to Y . Not surprisingly, this condition is equivalent to the graphical criterion for identifying
requisite probability nodes (see exercise 21.2), which also test whether the CPD of a variable X
can aﬀect the outcome of a given query; in this case, the CPDs of the variable X is determined
by whether there is an intervention at X.
Example 21.12
Let us revisit ﬁgure 21.1b, and a query of the form P(S | do(G)) (taking Y = S, X = G,
and Z, W = ∅).
Consider the augmented model G†, which contains a new decision parent
bG for the node S. The node bG is d-separated from S in this graph, so that we can conclude
P(S | do(G)) = P(S), as we would expect. On the other hand, if our query is P(S | do(G), J)

1020
Chapter 21. Causality
(so that now W = J), then G itself is an ancestor of our evidence J. In this case, there is an
active trail from bG to S in the network; hence, the rule does not apply, and we cannot conclude
P(S | do(G), J) = P(S | J). Again, this is as we would expect, because when we observe J, the
fact that we intervened at G is clearly relevant to S, due to standard intercausal reasoning.
21.3.2
Iterated Query Simpliﬁcation
The rules in the previous section allow us to simplify a query in certain cases.
But their
applicability appears limited, since there are many queries where none of the rules apply
directly. A key insight, however, is that we can also perform other transformations on the query,
allowing the rules to be applied.
Example 21.13
To illustrate this approach, consider again the example of example 21.8, which involves the query
P(J | do(G)) in the network of ﬁgure 21.1b. As we discussed, none of our rules apply directly to
this query: Obviously we cannot eliminate the intervention — P(J | do(G)) ̸= P(J). We also
cannot turn the intervention into an observation — P(J | do(G)) ̸= P(J | G); intuitively, the
reason is that intervening at G only aﬀects J via the single edge G →J, whereas conditioning G
also inﬂuences J by the indirect trail G ←I →S →J. This trail is called a back-door trail,
since it leaves G by the “back door.”
However, we can use standard probabilistic reasoning to conclude that:
P(J | do(G)) =
X
S
P(J | do(G), S)P(S | do(G)).
Both of the terms in the summation can be further simpliﬁed. For the ﬁrst term, we have that
the only active trail from G to J is now the direct edge G →J. More formally, J is d-separated
from G given S in the graph where outgoing arcs from G have been deleted. Thus, we can apply
proposition 21.2, and conclude:
P(J | do(G), S) = P(J | G, S).
For the second term, we have already shown in example 21.12 that P(S | do(G)) = P(S). Putting
the two together, we obtain that:
P(J | do(G)) =
X
S
P(J | G, S)P(S).
This example illustrates a process whereby we introduce conditioning on some set of variables:
P(Y | do(X), Z) =
X
W
P(Y | do(X), Z, W )P(W | do(X), Z).
Even when none of the transformation rule apply to the query P(Y | do(X), Z), they may
apply to each of the two terms in the summation of the transformed expression.
The example illustrates one special case of this transformation. A back-door trail from X to Y
back-door trail
is an active trail that leaves X via a parent of X. For a query P(Y | do(X)), a set W satisﬁes
the back-door criterion if no node in W is a descendant of X, and W blocks all back-door
back-door
criterion

21.3. Structural Causal Identiﬁability
1021
paths from X to Y . Using an argument identical to the one in the example, we can show that
if a set W satisﬁes the back-door criterion for a query P(Y | do(X)), then
P(Y | do(X)) =
X
W
P(Y | X, W )P(W ).
(21.1)
The back-door criterion can be used to address Simpson’s paradox, as described in exam-
ple 21.9.
Example 21.14
Consider again the query P(c1 | do(d1)). The variable G (Gender) introduces a back-door trail
between C and D. We can account for its inﬂuence using equation (21.1):
P(c1 | do(d1)) =
X
g
P(c1 | d1, g)P(g).
We therefore obtain that:
P(c1 | do(d1))
=
0.7 · 0.5 + 0.2 · 0.5 = 0.45
P(c1 | do(d0))
=
0.8 · 0.5 + 0.4 · 0.5 = 0.6.
And therefore, we should not prescribe the drug.
More generally, by repeated application of these rules, we can sometimes simplify fairly
complex queries, obtaining answers in cases that are far from obvious at ﬁrst glance.
Box 21.A — Case Study: Identifying the Eﬀect of Smoking on Cancer. In the early 1960s, fol-
lowing a signiﬁcant increase in the number of smokers that occurred around World War II, people
began to notice a substantial increase in the number of cases of lung cancer. After a great many
studies, a correlation was noticed between smoking and lung cancer. This correlation was noticed
in both directions: the frequency of smokers among lung cancer patients was substantially higher
than in the general population, and the frequency of lung cancer patients within the population of
smokers was substantially higher than within the population of nonsmokers.
These results, together with some experiments of injecting tobacco products into rats, led the
Surgeon General, in 1964, to issue a report linking cigarette smoking to cancer and, most particularly,
lung cancer. His claim was that the correlation found is causal, namely: If we ban smoking, the rate
of cancer cases will be roughly the same as the one we ﬁnd among nonsmokers in the population.
These studies came under severe attacks from the tobacco industry, backed by some very promi-
nent statisticians. The claim was that the observed correlations can also be explained by a model
in which there is no causal connection between smoking and lung cancer. Instead, an unobserved
genotype might exist that simultaneously causes cancer and produces an inborn craving for nicotine.
In other words, there were two hypothesized models, shown in ﬁgure 21.A.1a,b.
The two models can express precisely the same set of distributions over the observable variables
S, C. Thus, they can do an equally good job of representing the empirical distribution over these
variables, and there is no way to distinguish between them based on observational data alone.
Moreover, both models will provide the same answer to standard probabilistic queries such as P(c1 |

1022
Chapter 21. Causality
(c)
Smoking
Cancer
Genotype
(a)
Cancer
Smoking
(b)
Smoking
Cancer
Genotype
Figure 21.A.1 — Three candidate models for smoking and cancer. (a) a direct causal inﬂuence; (b)
indirect inﬂuence via a latent common parent Genotype; (c) incorporating both types of inﬂuence.
s1). However, relative to interventional queries, these models have very diﬀerent consequences.
According to the Surgeon General’s model, we would have:
P(c1 | do(S := s1)) = P(c1 | s).
In other words, if we force people to smoke, their probability of getting cancer is the same as the
probability conditioned on smoking, which is much higher than the prior probability. On the other
hand, according to the tobacco industry model, we have that
P(c1 | do(S := s1)) = P(c1).
In other words, making the population smoke or stop smoking would have no eﬀect on the rate of
cancer cases.
Pearl (1995) proposes a formal analysis of this dilemma, which we now present. He proposes
that we combine these two models into a single joint model, which accommodates for both possible
types of interactions between smoking and cancer, as shown in ﬁgure 21.A.1c. We now need to
assess, from the marginal distribution over the observed variables alone, the parameterization of
the various links. Unfortunately, it is impossible to determine the parameters of these links from
observational data alone, since both of the original two models (in ﬁgure 21.A.1a,b) can explain the
data perfectly.
However, if we reﬁne the model somewhat, introducing an additional assumption, we can provide
such estimates. Assume that we determine that the eﬀect of smoking on cancer is not a direct one,
but occurs through the accumulation of tar deposits in the lungs, as shown in ﬁgure 21.A.2a. Note
that this model makes the assumption that the accumulation of tar in the lungs is not directly
aﬀected by the latent Genotype variable. As we now show, if we can measure the amount of tar
deposits in the lungs of various individuals (for example, by X-ray or in autopsies), we can determine
the probability of the intervention query P(c1 | do(s1)) using observed correlations alone.
We are interested in P(c1 | do(s1)), which is an intervention query whose mutilated network is
G†
S in ﬁgure 21.A.2b. Standard probabilistic reasoning shows that:
P(C | do(s1))
=
X
t
P(C | do(s1), t)P(t | do(s1)).
We now consider and simplify each term in the summation separately.

21.3. Structural Causal Identiﬁability
1023
Tar
Smoking
Genotype
Cancer
Tar
Smoking
Genotype
Cancer
Tar
Smoking
Genotype
Cancer
(b)
S
(c)
T
S
T
T
T
S
S
(a)
Figure 21.A.2 — Determining causality between smoking and cancer. Augmented network for a
model in which the eﬀect of smoking on cancer is indirect, mediated via tar accumulation in the lungs,
and mutilated variants for two possible interventions.
The second term, which measures the eﬀect of Smoking on Tar, can be simpliﬁed directly using our
rule for converting interventions to observations, stated in proposition 21.2. Here, bS is d-separated
from T given S in the graph G†, shown in ﬁgure 21.A.2a. It follows that:
P(t | do(s))
=
P(t | s).
Intuitively, the only active trail from bS to T goes via S, and the eﬀect of that trail is identical
regardless of whether we condition on S or intervene at S.
Now, let us examine the ﬁrst term, P(C | do(s1), t), which measures the eﬀect of Tar on Cancer
in the presence of our intervention on S. Unfortunately, we cannot directly convert the intervention
at S to an observation, since C is not d-separated from bS given S, T in G†. However, we can
convert the observation at T to an intervention, because C is d-separated from bT given S, T in the
graph G†
S.
P(C | do(s1), t) = P(C | do(s1), do(t)).
We can now eliminate the intervention at S from this expression using proposition 21.3, which
applies because C is d-separated from bS given T in the graph G†
T (ﬁgure 21.A.2c), obtaining:
P(C | do(s1), do(t)) = P(C | do(t)).
Considering this last expression, we can apply standard probabilistic reasoning and introduce
conditioning on S:
P(C | do(t))
=
X
s′
P(C | do(t), s′)P(s′ | do(t))
=
X
s′
P(C | t, s′)P(s′ | do(t))
=
X
s′
P(C | t, s′)P(s′).

1024
Chapter 21. Causality
The second equality is an application of proposition 21.2, which applies because C is d-separated
from bT given T, S in G†. The ﬁnal equality is a consequence of proposition 21.3, which holds
because S is d-separated from bT in G†.
Putting everything together, we get:
P(c | do(s1))
=
X
t
P(c | do(s1), t)P(t | do(s1))
=
X
t
P(c | do(s1), t)P(t | s1)
=
X
t
P(t | s1)
X
s′
P(c | t, s′)P(s′).
Thus, if we agree that tar in the lungs is the intermediary between smoking and lung cancer, we
can uniquely determine the extent to which smoking causes lung cancer even in the presence of a
confounding latent variable!
Of course, this statement is more useful as a thought experiment than as a practical computa-
tional tool, both because it is unlikely that smoking aﬀects cancer only via tar accumulation in
the lungs and because it would be very diﬃcult in practice to measure this intermediate variable.
Nevertheless, this type of analysis provides insight on the extent to which understanding of the
underlying causal model allows us to identify the value of intervention queries even in the presence
of latent variables.
These rules provide a powerful mechanism for answering intervention queries, even when the
causal model involves latent variables (see, for example, box 21.A). More generally, using these
rules, we can show that the query P(Y | do(x)) is identiﬁable in each of the models shown in
ﬁgure 21.3 (see exercise 21.4). In these ﬁgures, we have used a bidirected dashed arrow, known
as a bow pattern, to denote the existence of a latent common cause between two variables. For
bow pattern
example, the model in (d) has the same structure as in our Smoking model of ﬁgure 21.A.2a.
(box 21.A). This notation simpliﬁes the diagrams considerably, and it is therefore quite commonly
used. Note that none of the diagrams contain a bow pattern between X and one of its children.
In general, a necessary condition for identiﬁability is that the graph contain no bow pattern
between X and a child of X that is an ancestor of Y . The reason is that, if such a bow pattern
exists between X and one of its children W, we have no mechanism for distinguishing X’s
direct inﬂuence on W and the indirect correlation induced by the latent variable, which is their
common parent.
It is interesting to note that, in the models shown in (a), (b), and (e), Y has a parent Z whose
eﬀect on Y is not identiﬁable, yet the eﬀect of X on Y , including its eﬀect via Z, is identiﬁable.
For example, in (a), P(Y | X) = P(Y | do(X)), and so there is no need to disentangle the
inﬂuence that ﬂows through the Z →Y edge, and the inﬂuence that ﬂows through the bow
pattern. These examples demonstrate that, to identify the inﬂuence of one variable on another,
it is not necessary to identify every edge involved in the interactions between them.
Figure 21.4 shows a few examples of models where the inﬂuence of X on Y is not identi-
ﬁable. Interestingly, the model in (g) illustrates the converse to the observation we just stated:
identiﬁcation of every edge involved in the interaction between X and Y does not suﬃce to

21.3. Structural Causal Identiﬁability
1025
X
Y
Z
Z
Y
X
X
Y
Z
Z
Y
X
(a)
(b)
(c)
(e)
(d)
Z1
Z2
Y
X
Figure 21.3
Examples of models where P(Y | do(X)) is identiﬁable. The bidirected dashed arrows
denote cases where a latent variable aﬀects both of the linked variables.
Z1
Z2
Y
X
Z
Y
X
X
Y
Z
X
Z
Y
Z
Y
X
Y
X
Z
Y
X
(a)
(b)
(c)
(d)
(g)
(f)
(e)
Figure 21.4
Examples of models where P(Y | do(X)) is not identiﬁable. The bidirected dashed
arrows denote cases where a latent variable aﬀects both of the linked variables.

1026
Chapter 21. Causality
identify their interaction. In particular, in this model, we can identify all of P(Z1 | do(X)),
P(Z2 | do(X)), P(Y | do(Z1)), and P(Y | do(Z2)), but we cannot identify P(Y | do(X))
(see exercise 21.5).
Note that the removal of any edge (directed or bidirected) from a causal diagram of this type
can only help in the identiﬁability of causal eﬀects, since it can only deactivate active trails.
Conversely, adding edges can only reduce identiﬁability. Hence, any subgraph of the graphs in
ﬁgure 21.3 is also identiﬁable, and any extension of the graphs in ﬁgure 21.4 is nonidentiﬁable.
Moreover, it is possible to show that the graphs in ﬁgure 21.3 are maximal, in the sense that
adding any edge renders P(Y | do(X)) unidentiﬁable; similarly, the graphs in ﬁgure 21.4
are minimal, in that the query is identiﬁable in any of their (strict) subgraphs. Similarly, the
introduction of mediating observed variables onto any edge in a causal graph — transforming
an arc A →B into a path A →Z →B for a new observed variable Z — can only increase
our ability to identify causal eﬀects.
Finally, we note that the techniques in this section provide us with methods for answering
only queries that are identiﬁable. Unfortunately, unidentiﬁable queries are quite common in
practice.
In section 21.5, we describe methods that allow us to provide partial answers for
queries that are not identiﬁable.
21.4
Mechanisms and Response Variables ⋆
The underlying intuition for our deﬁnition of a causal model is that the graph structure deﬁnes
a causal progression, where the value of each variable is selected, in order, based on the values
of its parents, via some causal mechanism. So far, the details of this causal mechanism have
remained implicit.
We simply assumed that it induces, for each variable X, a conditional
probability distribution P(X | PaX).
This approach suﬃces in cases, as in the previous section, where we can compute the value of
a causal query in terms of standard probabilistic queries. In general, however, this identiﬁcation
may not be possible. In such cases, we have to reason explicitly about the mechanism governing
the behavior of the variables in the model and the ways in which the latent variables may
inﬂuence the observed variables. By obtaining a ﬁner-grained analysis of these mechanisms,
we may be able to provide some analysis for intervention queries that are not identiﬁable from
probabilities over the observable variables alone.
A second reason for understanding the mechanism in more detail is to answer additional types
of queries: counterfactual queries, and certain types of diagnostic queries. As we discussed, our
intuition for a counterfactual query is that we want to keep as much as we can between the
real and the counterfactual world. By understanding the exact mechanism that governs each
variable, we can obtain more reliable inferences about what took place in the true world, so as
to preserve as much as possible when reasoning about the counterfactual events.
Example 21.15
Consider a signiﬁcantly simpliﬁed causal model for Gump’s job prospects, where we have only the
edge G →J — Gump’s job prospects depend only on his grade. Let us also assume that grades
are binary-valued — high and low. Our counterfactual query is as follows: “Gump received a low
grade. He applied to Acme Consulting, and he was not hired. Would Acme have hired him had he
managed to hack into the registrar’s computer system and change his grade to a high one?”
Consider two very diﬀerent models of the world.
In the ﬁrst, there are two (equally likely)

21.4. Mechanisms and Response Variables ⋆
1027
populations of companies. Those in the ﬁrst population are not in hiring mode, and they always
reject Gump without looking at his grade. Companies in the second population are desperate for
employees and will take anyone, regardless of their grade. The basic intuition of counterfactual
queries is that, when we move to the counterfactual world, Gump does not get another “random
draw” of company. We want to answer the counterfactual query assuming that Acme’s recruiting
response model is the same in both cases. Under this assumption, we can conclude that Acme was
not in hiring mode, and that Gump’s outcome would have been no diﬀerent had he managed to
change his grade.
In a second model, there are also two (equally likely) populations of companies. In the ﬁrst, the
company is highly selective, and it hires a student if and only if his grades are high. In the second
population, the recruiter likes to hire underdogs because he can pay them less, and he hires Gump
if and only if his grades are low. In this setting, if we again preserve the hiring response of the
company, we would conclude that Acme must use a selective recruiting policy, and so Gump would
have been hired had he managed to change his grade to a high one.
Note that these two models are identical from a probabilistic perspective. In both cases, P(J |
G) = (0.5, 0.5). But they are very diﬀerent with respect to answering counterfactual queries.
The same type of situation applies in a setting where we act in the world, and where we
wish to reason about the probabilities of diﬀerent events before and after our action.
For
example, consider the problem of troubleshooting a broken device with multiple components,
troubleshooting
where a fault in each can lead to the observed failure mode.
Assume that we replace one
of the components, and we wish to reason about the probabilities in the system after the
replacement action. Intuitively, if the problem was not with the component we replaced, then
the replacement action should have no eﬀect: the probability that the device remains broken
should be 1. The mechanism by which diﬀerent combinations of faults can lead to the observed
behavior is quite complex, and we are generally uncertain about which one is currently the case.
However, we wish to have this mechanism persist between the prerepair and postrepair state.
Box 21.C describes the use of this approach in a real-world diagnostic setting.
As these examples illustrate, to answer certain types of queries, we need to obtain a detailed
speciﬁcation of the causal mechanism that determines the values of diﬀerent variables in the
model. In general, we can argue that (quantum eﬀects aside) any randomness in the model
is the cumulative eﬀect of latent variables that are simply unmodeled. For example, consider
even a seemingly random event such as the outcome of a coin toss. We can imagine this event
depending on a large number of exogenous variables, such as the rotation of the coin when it
was tossed, the forces applied to it, any air movement in the room, and more. Given all of these
factors, the outcome of the coin toss is arguably deterministic. As another example, a company’s
decision on whether to hire Gump can depend on the company’s current goals and funding level,
on the existence of other prospective applicants, and even on whether the recruiter likes the
color of Gump’s tie. Based on this intuition, we can divide the variables into two groups. The
endogenous variables are those that we choose to include in the model; the exogenous variables
endogenous
variable
are unmodeled, latent variables. We can now argue, as a hypothetical thought experiment, that
the exogenous variables encompass all of the stochasticity in the world; therefore, given all of
the exogenous variables, each endogenous variable (say the company’s hiring decision) is fully
determined by its endogenous parents (Gump’s grade in our example).

1028
Chapter 21. Causality

Clearly, we cannot possibly encode a complete speciﬁcation of a causal model with
all of the relevant exogenous variables. In most cases, we are not even aware of the
entire set of relevant exogenous variables, far less able to specify their inﬂuence on
the model. However, for our purposes, we can abstract away from speciﬁc exogenous
variables and simply focus on their eﬀect on the endogenous variables. To understand
this transformation, consider the following example.
Example 21.16
Consider again the simple setting of example 21.15. Let U be the entire set of exogenous variables
aﬀecting the outcome of the variable J in the causal model G →J. We can now assume that
the value of J is a deterministic function of G, U — for each assignment u to U and g to G,
the variable J deterministically takes the value fJ(g, u). We are interested in modeling only the
interaction between G and J. Thus, we can partition the set of assignments u into four classes,
based on the mapping µ that they induce between G and J:
• µJ
17→1,07→1 — those where fJ(u, g1) = j1 and fJ(u, g0) = j1; these are the “always hire”
cases, where Gump is hired regardless of his grade.
• µJ
17→1,07→0 — those where fJ(u, g1) = j1 and fJ(u, g0) = j0; these are the “rational
recruiting” cases, where Gump is hired if and only if his grade is high.
• µJ
17→0,07→1 — those where fJ(u, g1) = j0 and fJ(u, g0) = j1; these are the “underdog”
cases, where Gump is hired if and only if his grade is low.
• µJ
17→0,07→0 — those where fJ(u, g1) = j0 and fJ(u, g0) = j0; these are the “never hire”
cases, where Gump is not hired regardless of his grade.
Each assignment u induces precisely one of these four diﬀerent mappings between G and J. For
example, we might have a situation where, if Gump wears a green tie with pink elephants, he is
never hired regardless of his grade, which is the last case in the previous list.
For the purposes of reasoning about the interaction between G and J, we can abstract away from
modeling speciﬁc exogenous variables U, and simply reason about how likely we are to encounter
each of the four categories of functions induced by assignments u.
Generalizing from this example, we deﬁne as follows:
Deﬁnition 21.2
Let X be a variable and Y be a set of parents. A response variable for X given Y is a variable
response variable
U X whose domain is the set of all possible functions µ(Y ) from Val(Y ) to Val(X). That is,
let y1, . . . , ym be an enumeration of Val(Y ) (for m = |Val(Y )|). For a tuple of (generally not
distinct) values x1, . . . , xm ∈Val(X), we use µX
(y1,...,ym)7→(x1,...,xm) to denote the function that
assigns xi to yi, for i = 1, . . . , m. The domain of U X contains one such function for each such
tuple, giving a total of km functions of this form (for |Val(X)| = k).
Example 21.17
In example 21.16, we introduce two response variables, U G and U J. Because G has no endogenous
parents, the variable U G is degenerate: its values are simply g1 and g0. The response variable U J
takes one of the four values µJ
17→1,17→1, µJ
17→1,17→0, µJ
17→0,17→1, µJ
17→0,17→0, each of which deﬁnes a
function from G to J. Thus, given U J and G, the value of J is fully determined. For example, if
U J = µJ
17→1,17→0, and G = g1, then J = j1.

21.4. Mechanisms and Response Variables ⋆
1029
Deﬁnition 21.3
A functional causal model C over a set of endogenous variables X is a causal model deﬁned over
two sets of variables: the variables X, and a set of response variables U = {U X : X ∈X}.
functional causal
model
Each variable X ∈X has a set of parents PaX ⊂X, and a response variable parent U X for X
given PaX. The model for each variable X ∈X is deterministic: When U X = µ and PaX = y,
then X = µ(y) with probability 1.
The model C also deﬁnes a joint probability distribution over the response variables, deﬁned as
a Bayesian network over U. Thus, each response variable U has a set of parents PaU ⊂U, and a
CPD P(U | PaU).
Functional causal models provide a much ﬁner-grained speciﬁcation of the underlying causal
mechanisms than do standard causal models, where we only specify a CPD for each endogenous
variable.
In our G →J example, rather than specifying a CPD for J, we must specify a
distribution over U J. A table representation of the CPD has two independent parameters —
one for each assignment to G. The distribution P(U J) has three independent parameters —
there are four possible values for U J, and their probabilities must sum to 1. While, in this case,
the blowup in the representation may not appear too onerous, the general case is far worse. In
general, consider a variable X with parents Y , and let k = |Val(X)| and m = |Val(Y )|. The
total number of possible mappings from Val(Y ) to Val(X) is km — each function selects one
of X’s k values for each of the m assignments to Y . Thus, the total number of independent
parameters in (an explicit representation of) P(U X) is km −1. By comparison, a table-CPD
P(X | Y ) requires m(k −1) independent parameters only.
Example 21.18
Consider the network in ﬁgure 21.5a, representing a causal model for a randomized clinical trial,
where some patients are randomly assigned a medication and others a placebo. The model contains
three binary-valued endogenous variables: A indicates the treatment assigned to the patient; T
indicates the treatment actually received; and O indicates the observed outcome (positive or nega-
tive). The model contains three response variables: U A, U T , and U O. Intuitively, U A (which has a
uniform distribution) represents the stochastic event determining the assignment to the two groups
(medication and placebo); U T determines the patient’s model for complying with the prescribed
course of treatment; and U O encodes the form of the patient’s response to diﬀerent treatments.
The domain of U T consists of the following four functions:
• µT
17→1,07→1 — always taker;
• µT
17→1,07→0 — complier (takes medicine if and only if prescribed);
• µT
17→0,07→1 — deﬁer (takes medicine if and only if not prescribed);
• µT
17→0,07→0 — never taker.
Similarly, the domain of U O consists of the following four functions:
• µO
17→1,07→1 — always well;
• µO
17→1,07→0 — helped (recovers if and only if takes medicine);
• µO
17→0,07→1 — hurt (recovers if and only if does not take medicine);
• µO
17→0,07→0 — never well.

1030
Chapter 21. Causality
Treatment
Assign
UA
UT
UO
Outcome
always taker
complier
deﬁer
never taker
0.05
0.7
0.05
0.2
P(U T )
U T
always well
helped
hurt
never well
always taker
0.2
0.5
0.1
0.2
complier
0.5
0.4
0.05
0.05
deﬁer
0.3
0.4
0.1
0.2
never taker
0.6
0.3
0.05
0.05
P(U O | U T )
U T
U O
probability
always taker
always well
0.01
always taker
helped
0.025
always taker
hurt
0.005
always taker
never well
0.01
complier
always well
0.35
complier
helped
0.28
complier
hurt
0.035
complier
never well
0.035
deﬁer
always well
0.015
deﬁer
helped
0.02
deﬁer
hurt
0.005
deﬁer
never well
0.02
never taker
always well
0.12
never taker
helped
0.06
never taker
hurt
0.01
never taker
never well
0.01
P(U T , U O)
Figure 21.5
A simple functional causal model for a clinical trial. (a) Network structure. (b) Sample
CPDs and resulting joint distribution for the response variables U T and U O.
The model makes explicit two important assumptions. First, the assigned treatment A does
not inﬂuence the response O directly, but only through the actual treatment T; this conditional
independence is achieved by the use of a placebo in the control group, so that the patients do not
know to which of the two groups they were assigned. The second assumption is that A is marginally
independent from {U T , U O}, which is ensured via the randomization of the assignment to the
two groups. Note, however, that U T and U O are not independent, reﬂecting the fact that factors
determining a patient’s decision to comply with the treatment can also aﬀect his ﬁnal outcome.
(For example, patients who are not very ill may neglect to take their medication, and may be more
likely to get well regardless.) Thus, to specify this model fully, we need to deﬁne a joint probability
distribution over U T , U O — a total of ﬁfteen independent parameters.
We see that a full speciﬁcation of even a simple functional causal model is quite complex.

21.5. Partial Identiﬁability in Functional Causal Models ⋆
1031
21.5
Partial Identiﬁability in Functional Causal Models ⋆
In section 21.3, we considered the task of answering causal queries involving interventions. As
we discussed, unlike probability queries, to answer intervention queries we generally need to
consider the eﬀect of latent variables on the observable variables. A functional causal model C
provides a complete speciﬁcation of a causal model, including the eﬀect of the latent variables,
and can thus be used to answer any intervention query. As in deﬁnition 21.1, we mutilate the
network by eliminating all incoming arcs to intervened variables, and then do inference on the
resulting network.
However, this “solution” does not address the fundamental problem.
As we discussed in
section 21.3, our accumulated statistics are generally only over the observable variables. We will
rarely have any direct observations of the ﬁner-grained distributions over the response variables.
Thus, it is unrealistic to assume that we can construct a fully speciﬁed functional causal model.
So, what is the point in deﬁning these constructs if we can never obtain them? As we show in
this section and the next one, these networks, even if never fully elicited, can help us provide
at least partial answers to both intervention and counterfactual queries.
The basis for this approach rests on the fact that the distributions of response variables are
related to the conditional probabilities for the endogenous variables. Thus, we can use our
information about the latter to constrain the former. To understand this relationship, consider
response variable
constraints
the following example:
Example 21.19
Let us revisit example 21.16, and consider the observed probability P(j1 | g1) — the probability
that Gump gets a job if he gets a high grade. Given G = g1, we get j1 in two cases: in the
“always hire” case — U J = µJ
17→1,07→1, and in the “rational recruiting” case — U J = µJ
17→1,07→0.
These two choices for U J are indistinguishable in the context g1, but would have led to diﬀerent
outcomes had Gump had a low grade. As a consequence, we have that:
P(j1 | g1)
=
P(µJ
17→1,07→1) + P(µJ
17→1,07→0)
P(j1 | g0)
=
P(µJ
17→1,07→1) + P(µJ
17→0,07→1)
P(j0 | g1)
=
P(µJ
17→0,07→1) + P(µJ
17→0,07→0)
P(j0 | g0)
=
P(µJ
17→1,07→0) + P(µJ
17→0,07→0).
We see that each conditional probability is a sum of some subset of the probabilities associated with
the response variable.
In the case where the response variable U X is marginally independent of other response
variables, we can generalize this example to provide a full formulation of the constraints that
relate the observed conditional probabilities P(X | Y ) and the distributions over U X:
P(xi | yi) =
X
¯x−i∈Val(X)m−1
P(µX
y7→xi,¯y−i7→¯x−i),
(21.2)
where ¯y−i is the tuple of all assignments yj except for j = i, and similarly for ¯x−i. The more
general case, where response variables may be correlated, is a little more complex, due to the
richer parameterization of the model.

1032
Chapter 21. Causality
Example 21.20
Consider again example 21.18. In this case, our functional causal model has sixteen unknown re-
sponse parameters, specifying the joint distribution P(U T , U O). By observing the statistics over the
endogenous variables, we can evaluate the distribution P(T, O | A), which can be used to constrain
P(U T , U O). Let ν(1,0)7→(i,j),(1,0)7→(k,l) (for i, j, k, l ∈{0, 1} denote P(µT
17→i,07→j, µO
17→k,07→l).
Using reasoning similar to that of example 21.19, we can verify that these two sets of probabilities
are related by the following linear equalities:
P(ti, oj | ak) =
X
i′,j′∈{0,1}
ν(k,1−k)7→(i,i′),(i,1−i)7→(j,j′)
∀i, j, k.
(21.3)
For example,
P(t1, o0 | a1) =
X
i′,j′∈{0,1}
ν(1,0)7→(1,i′),(1,0)7→(0,j′);
that is, to be consistent with the assignment a1, t1, o0, the U T function should map a1 to t1, but it
can map a0 arbitrarily, and the U O function should map t1 to o0, but it can map t0 arbitrarily.
Thus, we see that the observed probabilities over endogenous variables impose constraints
on the possible distributions of the response variables.
These constraints, in turn, impose
constraints over the possible values of various queries of interest.
By reasoning about the
possible values for the distributions over the response variables, we can often obtain reasonably
precise bounds over the values of queries of interest.
Example 21.21
Continuing our clinical trial example, assume that we are interested in determining the extent to
which taking the medication improves a patient’s chances of a cure. Naively, we might think to
answer this question by evaluating P(o1 | t1) −P(o1 | t0). This approach would be incorrect,
since it does not account for the correlation between compliance and cure. For example, if patients
who choose to comply with the treatment are generally sicker and therefore less likely to get well
regardless, then the cure rate in the population of patients for which T = t1 will be misleadingly
low, giving a pessimistic estimate of the eﬃcacy of treatment.
A correct answer is obtained by the corresponding intervention query,
P(o1 | do(t1)) −P(o1 | do(t0)),
which measures the increase in cure probability between a patient who was forced not to take the
treatment and one who was forced to take it. This query is also known as the average causal eﬀect of
average causal
eﬀect
T on O and is denoted ACE(T →O). Unfortunately, the causal model for this situation contains
a bidirected arc between T and O, due to the correlation between their responses. Therefore, the
inﬂuence of T on O is not identiﬁable in this model, and, indeed, none of the simpliﬁcation rules of
section 21.3 apply in this case. However, it turns out that we can still obtain surprisingly meaningful
bounds over the value of this query.
We begin by noting that
P(o1 | do(t1))
=
P(µO
17→1,07→1) + P(µO
17→1,07→0)
P(o1 | do(t0))
=
P(µO
17→1,07→1) + P(µO
17→0,07→1),

21.5. Partial Identiﬁability in Functional Causal Models ⋆
1033
so that
ACE(T →O) = P(o1 | do(t1)) −P(o1 | do(t0)) = P(µO
17→1,07→0) −P(µO
17→0,07→1). (21.4)
These are just marginal probabilities of the distribution P(U T , U O), and they can therefore be
written as a linear combination of the sixteen ν parameters representing the entries in this joint
distribution.
The set of possible values for ν is determined by the constraints equation (21.3), which deﬁnes
eight linear equations constraining various subsets of these parameters to sum up to probabilities in
the observed distribution. We also need to require that ν be nonnegative.
Altogether, we have a linear formulation of ACE(T →O) in terms of the ν parameters,
and a set of linear constraints on these parameters — the equalities of equation (21.3) and the
nonnegativity inequalities. We can now use linear programming techniques to obtain both the
maximum and the minimum value of the function representing ACE(T →O) subject to the
constraints. This gives us bounds over the possible value that ACE(T →O) can take in any
functional causal model consistent with our observed probabilities.
In this fairly simple problem, we can even provide closed-form expressions for these bounds.
Somewhat simpler bounds that are correct but not tight are:
ACE(T →O)
≥
P(o1 | a1) −P(o1 | a0) −P(o1, t0 | a1) −P(o0, t1 | a0).
(21.5)
ACE(T →O)
≤
P(o1 | a1) −P(o1 | a0) + P(o0, t0 | a1) + P(o1, t1 | a0).
(21.6)
Both bounds have a base quantity of P(o1 | a1) −P(o1 | a0); this quantity, sometimes called the
encouragement, represents the diﬀerence in cure rate between the group that was prescribed the
medication and the group that was not, ignoring the issue of how many in each group actually took
the prescribed treatment. In a regime of full compliance with the treatment, the encouragement
would be equivalent to ACE(T →O). The correction factors in each of these equations provide a
bound on the extent to which noncompliance can aﬀect this estimate. Note that the total diﬀerence
between the upper and lower bounds is
P(o0, t0 | a1) + P(o1, t1 | a0) + P(o1, t0 | a1) + P(o0, t1 | a0) = P(t0 | a1) + P(t1 | a0),
which is precisely the total rate of noncompliance. These bounds are known as the natural bounds.
natural bounds
They are generally not as tight as the bounds we would obtain from the linear program, but they are
tight in cases where no patient is a deﬁer, that is, P(µT
17→0,07→1) = 0. In many cases, they provide
surprisingly informative bounds on the result of the intervention query, as shown in box 21.B.
Box 21.B — Case Study: The Eﬀect of Cholestyramine. A study conducted as part of the Lipid
Research Clinics Coronary Primary Prevention Trial produced data about a drug trial relating to a
drug called cholestyramine. In a portion of this data set (337 subjects), subjects were randomized
into two treatment groups of roughly equal size; in one group, all subjects were prescribed the drug
(a1), while in the other group, all subjects were prescribed a placebo (a0). Patients who were in the

1034
Chapter 21. Causality
control group did not have access to the drug. The cholesterol level of each patient was measured
several times over the years of treatment, and the average was computed. In this case, both the
actual consumption of the drug and the resulting cholesterol level are continuous-valued.
Balke and Pearl (1994a) provide a formal analysis of this data set. In their analysis, a patient was
said to have received treatment (t1) if his consumption was above the midpoint between minimal
and maximal consumption among patients in the study. A patient was said to have responded to
treatment (o1) if his average cholesterol level throughout the treatment was at least 28 points lower
than his measurement prior to treatment.
The resulting data exhibited the following statistics:
P(t1, o1 | a1) = 0.473
P(t1, o1 | a0) = 0
P(t1, o0 | a1) = 0.139
P(t1, o0 | a0) = 0
P(t0, o1 | a1) = 0.073
P(t0, o1 | a0) = 0.081
P(t0, o0 | a1) = 0.315
P(t0, o0 | a0) = 0.919.
The encouragement in this case is P(o1 | a1) −P(o1 | a0) = 0.0473 + 0.073 −0.081 = 0.465.
According to equation (21.5) and 21.6, we obtain:
ACE(T →O)
≥
0.465 −0.073 −0 = 0.392
ACE(T →O)
≤
0.465 + 0.315 + 0 = 0.78.
The diﬀerence between these bounds represents the noncompliance rate of P(t0 | a1) = 0.388.
Thus, despite the fact that 38.8 percent of the subjects deviated from their treatment protocol, we
can still assert (ignoring possible errors resulting from using statistics over a limited population)
that, when applied uniformly to the population, cholestyramine increases by at least 39.2 percent
the probability of reducing a patient’s cholesterol level by 28 points or more.
21.6
Counterfactual Queries ⋆
Part of our motivation for moving to functional causal models derived from the issue of an-
swering counterfactual queries. We now turn our attention to such queries, and we show how
functional causal models can be used to address them. As we discussed, similar issues arise in
the context of diagnostic reasoning, where we wish to reason about the probabilities of various
events after taking a repair action; see box 21.C.
21.6.1
Twinned Networks
Recall that a counterfactual query considers a scenario that actually took place in the real world
and a corresponding scenario in a counterfactual world, where we modify the chain of events
via some causal intervention. Thus, we are actually reasoning in parallel about two diﬀerent
worlds: the real one, and the counterfactual one. A variable X may take on one value in the
real world, and a diﬀerent value in the counterfactual world. To distinguish between these two
values, we use X to denote the random variable X in the true world, and X′ to denote X in
the counterfactual world. Intuitively, both the real and the counterfactual worlds are governed
counterfactual
world

21.6. Counterfactual Queries ⋆
1035
by the same causal model, except that one involves an intervention.
However, a critical property of the desired output for a counterfactual query was that it
involves minimal modiﬁcation to the true world. As a degenerate case, consider the following
counterfactual query relating to example 21.18: “A patient in the study took the prescribed
treatment and did not get well. Would he have gotten well had we forced him to comply with the
treatment?” Formally, we can write this query as P(O′ = o1 | T = t1, O = o0, do(T ′ := t1)).
Our intuition in this case says that we change nothing in the counterfactual world, and so
the outcome should be exactly the same.
But even this obvious assertion has signiﬁcant
consequences. Clearly, we cannot simply generate a new random assignment to the variables in
the counterfactual world. For example, we want the patient’s prescribed course of treatment to
be the same in both worlds. We also want his response to the treatment to be the same.
The notion of response variables allows us to make this intuition very precise. A response
variable UX in a functional causal model summarizes the eﬀect of all of the stochastic choices
that can inﬂuence the choice of value for the endogenous variable X. When moving to the
counterfactual world, all of these stochastic choices should remain the same. Thus, we assume
that the values of the response variables are selected at random in the real world and preserved
unchanged in the counterfactual world. Note that, when the counterfactual query includes a
nondegenerate intervention do(X := x′) — one where x′ is diﬀerent from the value of x in
the true world — the values of endogenous variables can change in the counterfactual world.
However, the mechanism by which they choose these values remains constant.
Example 21.22
Consider the counterfactual query P(O′ = o1 | T = t0, O = o0, do(T ′ := t1)). This query
represents a situation where the patient did not take the prescribed treatment and did not get
well, and we wish to determine the probability that the patient would have gotten well had he
complied with the treatment. Assume that the true world is such that the patient was assigned
to the treatment group, so that U A = a1. Assume also that he is in the “helped” category, so
that the response variable U O = µO
17→1,07→0. As we assumed, both of these are conserved in the
counterfactual world. Thus, given T ′ = t1, and applying the deterministic functions speciﬁed by
the response variables, we obtain that A′ = a1, T ′ = t1, and O′ = o1, so that the patient would
have recovered had he complied.
In general, of course, we do not know the value of the response variables in the real world.
However, a functional causal model speciﬁes a prior distribution over their values. Some values
are not consistent with our observations in the real scenario, so the distribution over the
response variables is conditioned accordingly. The resulting posterior can be used for inference
in the counterfactual world.
Example 21.23
Consider again our clinical trial of example 21.18. Assume that the trial is randomized, so that
P(U A = a1) = P(U A = a0) = 0.5. Also, assume that the medication is unavailable outside the
treatment group, so that P(U T = µT
17→1,07→1) = 0. One possible joint distribution for P(U T , U O)
is shown in ﬁgure 21.5b. Assume that we have a patient who, in the real world, was assigned to the
treatment group, did not comply with the treatment, and did not get well; we are interested in the
probability that he would have gotten well had he complied. Thus, our query is:
P(O′ = o1 | A = a1, T = t0, O = o0, do(T ′ := t1)).
Our observations in the real world are consistent only with the following values for the response

1036
Chapter 21. Causality
Treatment
Assign
UA
UT
UO
Outcome
Treatment´
Assign´
Outcome´
Figure 21.6
Twinned counterfactual network for ﬁgure 21.5, with an intervention at T ′.
variables: U T can be “deﬁer” or “never taker”; U O can be “helped” or “never well.” Conditioning
our joint distribution, we obtain the following joint posterior:
U T
U O
probability
deﬁer
helped
4/13
deﬁer
never well
2/13
never taker
helped
6/13
never taker
never well
1/13
In the counterfactual world, we intervene by forcing the patient to comply with the treatment.
Therefore, his outcome is determined by the value that U O deﬁnes for t1. This value is o1 when the
patient is of category “helped,” and o0 when he is of category “never well.” Using the posterior we
computed, we conclude that the answer to the query is 10/13.
This type of reasoning can be obtained as a direct consequence of inference in a joint causal
network that incorporates both the real and the counterfactual world. Speciﬁcally, consider the
following construction:
Deﬁnition 21.4
Let C be a functional causal model over the endogenous variables X and the corresponding response
variables U = {U X : X ∈X}. We deﬁne the counterfactual twinned network to be a functional
counterfactual
twinned network
causal model over the variables X ∪U ∪{X′ : X ∈X}, such that:
• if PaX = Y , then PaX′ = Y ′,
• X′ also has the response variable U X as a parent,
• the deterministic function governing X′ is identical to that of X.
We can answer counterfactual queries for our original causal model by constructing the
twinned causal model and then answering the counterfactual query as an intervention query in
that network, in exactly the same way as we would for any functional causal network.

21.6. Counterfactual Queries ⋆
1037
Example 21.24
Returning to the query of example 21.23, we ﬁrst construct the twinned network for ﬁgure 21.5, and
then use it to answer the intervention query P(O′ = o1 | A = a1, T = t0, O = o0, do(T ′ := t1))
as a simple intervention query in the resulting network. Speciﬁcally, we mutilate the counterfactual
network so as to eliminate incoming arcs into T ′, condition on our evidence A = a1, T = t0, O =
o0, and simply run probabilistic inference. The network is shown in ﬁgure 21.6. It is straightforward
to verify that the answer is precisely what we obtained in the preceding analysis.
Note that this approach is very general, and it allows us to answer a broad range of queries.
We can account for interventions in the real network as well as the counterfactual one and allow
for evidence in both. For example, we can ask, “Assume that we had a patient in the control
(placebo) group who did not get well; if I had assigned that patient to the treatment group
and observed that he did not get well, what is the probability that he complied with treatment
(in the counterfactual world)?” This query is formally written as P(T ′ = t1 | A = a0, O =
o0, do(A′ := a1), O′ = o0), and it can be answered using inference in the twinned network.
Box 21.C — Case Study: Persistence Networks for Diagnosis. One real-world application of
the twinned-network analysis arises in the setting of troubleshooting (see also box 5.A). In this ap-
troubleshooting
plication, described in Breese and Heckerman (1996), we have a network with multiple (unobserved)
variables X1, . . . , Xk that denote the possible failure of various components in a system. Our task
is to repair the system. In an ideal setting, our repair actions can correspond to ideal interventions:
we take one of the failure variables Xk and set it to a value denoting no failure: do(Xi := x0
i ).
Now, assume that we have some set of observations e about the current state of the system,
and want to evaluate the beneﬁt of some repair action do(Xi := x0
i ). How do we compute the
probability that the repair action ﬁxes the problem, or the distribution over the remaining failure
variables following the repair action? It is not diﬃcult to see that an intervention query is not
appropriate in this setting: The evidence that we had about the failure symptoms prior to the
repair is highly relevant for determining these probabilities, and it must be taken into account when
computing the posterior following the intervention. For example, if the probability of x0
i (i not
broken) was very high given e, chances are the system is still broken following the repair. The right
query, to determine the postintervention distribution for a variable Y , is a counterfactual one:
P(Y ′ | e, do(Xi
′ := x0
i )).
Under certain (fairly strong) assumptions — noisy-or CPDs and a single fault only — one can
avoid constructing the twinned network and signiﬁcantly reduce the cost of this computation. (See
exercise 21.6.) We return to the issue of troubleshooting networks in box 23.C.
21.6.2
Bounds on Counterfactual Queries
Although a functional causal model gives us enormous ability to answer sophisticated counter-
factual queries, such a model is rarely available, as we discussed. However, we can apply the
same idea as in section 21.5 to provide bounds on the probabilities of the response variables,

1038
Chapter 21. Causality
and hence on the answers to counterfactual queries. We demonstrate this approach on a ﬁcti-
tious example, which also serves to illustrate the diﬀerences between diﬀerent types of causal
analysis.

21.6. Counterfactual Queries ⋆
1039
Example 21.25
The marketer of PeptAid, an antacid medication, randomly mailed out product samples to 10
percent of the population. A follow-on market survey determined, for each individual, whether he
or she received the sample (A), whether he or she took it (T), and whether he or she subsequently
developed an ulcer (O). The accumulated data exhibited the following statistics:
P(t1, o1 | a1) = 0.14
P(t1, o1 | a0) = 0.32
P(t1, o0 | a1) = 0.17
P(t1, o0 | a0) = 0.32
P(t0, o1 | a1) = 0.67
P(t0, o1 | a0) = 0.04
P(t0, o0 | a1) = 0.02
P(t0, o0 | a0) = 0.32.
The functional causal model for this situation is identical to that of ﬁgure 21.5a.
Examining these numbers, we see a strong correlation between individuals who consumed Pep-
tAid and those who developed ulcers: P(o1 | t1) = 0.5, whereas P(o1 | t0) = 0.26. Moreover,
the probability of developing ulcers was 45 percent greater in individuals who received the PeptAid
samples than in those who did not: P(o1 | a1) = 0.81, whereas P(o1 | a0) = 0.36. Thus,
using the observational data alone, we might conclude that PeptAid causes ulcers, and that its
manufacturer is legally liable for damages to the aﬀected population.
As we discussed in example 21.2, an immediate counterargument is that the high positive corre-
lation is due to some latent common cause such as preulcer discomfort. Indeed, one can show that,
in this case, PeptAid actually helps reduce the risk of ulcers: a causal analysis along the lines of
example 21.21 shows that
−0.23 ≤ACE(T →O) ≤−0.15.
That is, the average causal eﬀect of PeptAid consumption is to reduce an individual’s chance of
getting an ulcer by at least 15 percent.
However, now consider a particular patient George who received the sample, consumed it, and
subsequently developed an ulcer. We would like to know whether the patient would not have devel-
oped the ulcer had he not received the sample. In this case, the relevant query is a counterfactual,
which has the form:
P(O′ = o0 | A = a1, T = t1, O = o1, do(A′ := a0)).
Given our evidence A = a1, T = t1, O = o1, only the responses “complier” and “always taker” are
possible for U T , and only the responses “hurt” and “never well” for U O (where here “never well”
means “always ulcer”, or µO
17→1,07→1). Of these, only the combination (“complier,”“hurt”) is consistent
with the query assertion O′ = o0: if George is a “never well,” he would have developed ulcers
regardless; similarly, if George is an “always taker,” he would have taken PeptAid regardless, with
the same outcome. We conclude that the probability of interest is equal to:
P(U T = complier, U O = hurt)
P(T = t1, O = o1 | A = a1) .
Because the numerator is ﬁxed, this expression is linear in the ν parameters, and so we can compute
bounds using linear programming. The resulting bounds show that:
P(O′ = o0 | A = a1, T = t1, O = o1, do(A′ := a0)) ≥0.93;
thus, at least 93 percent of patients in George’s category — those who received PeptAid, consumed it,
and developed an ulcer — would not have developed an ulcer had they not received the sample!

1040
Chapter 21. Causality
This example illustrates the subtleties of causal analysis, and the huge diﬀerences between the
answers to apparently similar queries. Thus, care must be taken when applying causal analysis
to understand precisely which query it is that we really wish to answer.
21.7
Learning Causal Models
So far, we have focused on the problem of using a given causal model to answer causal queries
such as intervention or counterfactual queries.
In this section, we discuss the problem of
learning a causal model from data.
As usual, there are several axes along which we can partition the space of learning tasks.
Perhaps the most fundamental axis is the notion of what we mean by a causal model. Most
obvious is a causal network with standard CPDs. Here, we are essentially learning a standard
Bayesian network, except that we are willing to ascribe to it causal semantics and use it to
answer interventional queries.
However, we can also consider other choices.
For example,
at one extreme, we can consider a functional causal model, where our network has response
variables and a fully speciﬁed parameterization for them; clearly, this problem is far more
challenging, and such rich models are much more diﬃcult to identify from available data. At
the other extreme, we can simplify our problem considerably by abstracting away all details of
the parameterization and focus solely on determining the causal structure.
Second, we need to determine whether we are given the structure and so have to deal only
with parameter estimation, or whether we also have to learn the structure.
A third axis is the type of data from which we learn the model. In the case of probabilistic
models, we assumed that our data consist of instances sampled randomly from some generating
distribution P ∗. Such data are called observational. In the context of causal models, we may also
observational
data
have access to interventional data, where (some or all of) our data instances correspond to cases
interventional
data
where we intervene in the model by setting the values of some variables and observe the values
of the others. As we will discuss, interventional data provide signiﬁcant power in disambiguating
diﬀerent causal models that can lead to identical observational patterns. Unfortunately, in many
cases, interventions are diﬃcult to perform, and sometimes are even illegal (or immoral), whereas
observational data are usually much more plentiful.
A ﬁnal axis involves the assumptions that we are willing to make regarding the presence of
factors that can confound causal eﬀects. If we assume that there are no confounding factors
— latent variables or selection bias — the problem of identifying causal structure becomes
signiﬁcantly simpler. In particular, under fairly benign assumptions, we can fully delineate the
cases in which we can infer causal direction from observational data. When we allow these
confounding factors, the problem becomes signiﬁcantly harder, and the set of cases where we
can reach nontrivial conclusions becomes much smaller.
Of course, not all of the entries in this many-dimensional grid are interesting, and not all
have been explored. To structure our discussion, we begin by focusing on the task of learning
a causal model, which allows us to infer the causal direction for the interactions between the
variables and to answer interventional queries. We consider ﬁrst the case where there are no
confounding factors, so that all relevant variables are observed in the data. We discuss both
the case of learning only from observational data, then introduce the use of interventional data.
We then discuss the challenges associated with latent variables and approaches for dealing with

21.7. Learning Causal Models
1041
them, albeit in a limited way. Finally, we move to the task of learning a functional causal model,
where even determining the parameterization for a ﬁxed structure is far from trivial.
21.7.1
Learning Causal Models without Confounding Factors
We now consider the problem of learning a causal model from data. At some level, it appears
that there is very little to say here that we have not already said. After all, a causal model is
essentially a Bayesian network, and we have already devoted three chapters in this book to the
problem of learning the structure and the parameters of such networks from data. Although
many of the techniques we developed in these chapters will be useful to us here, they do not
provide a full solution to the problem of learning causal models. To understand why, recall that
our objective in the task of learning probabilistic models was simply to provide a good ﬁt to
the true underlying distribution. Any model that ﬁt that distribution (reasonably well) was an
adequate solution.
As we discussed in section 21.1.2, there are many diﬀerent probabilistic models that can give
rise to the same marginal distribution over the observed variables. While these models are (in
some sense) equivalent with respect to probabilistic queries, as causal models, they generally
give rise to very diﬀerent conclusions for causal queries. Unfortunately, distinguishing between
these structures is often impossible. Thus, our task in this section is to obtain the strongest
possible conclusion about the causal models that could have given rise to the observed data.
21.7.1.1
Key Assumptions
There are two key assumptions that underlie methods that learn causal models from obser-
vational data.
They relate the independence assumptions that hold in the true underlying
distribution P ∗with the causal relationships between the variables in the domain. Assume that
P ∗is derived from a causal network whose structure is the graph G∗.
The ﬁrst, known as the causal Markov assumption, asserts that, in P ∗, each variable is
causal Markov
assumption
conditionally independent of its non-eﬀects (direct or indirect) given its direct causes. Thus,
each variable is conditionally independent of its nondescendants given its parents in G∗. This
assumption is precisely the same as the local Markov assumptions of deﬁnition 3.1, except that
arcs are given a causal interpretation. We can thus restate this assumption as asserting that the
causal network G∗is an I-map for the distribution P ∗.
While the diﬀerence between the local Markov and causal Markov assumptions might appear
purely syntactic, it is fundamental from a philosophical perspective. The local Markov assump-
tions for Bayesian networks are simply phenomenological: they state properties that a particular
distribution has. The causal Markov assumption makes a statement about the world: If we
relate variables by the “causes” relationship, these independence assumptions will hold in the
empirical distribution we observe in the world.
The justiﬁcation for this assumption is that causality is local in time and space, so that the
direct causes of a variable (stochastically) determine its value. Current quantum theory and
experiments show that this assumption does not hold at the quantum level, where there are
nonlocal variables that appear to have a direct causal eﬀect on others. While these cases do not
imply that the causal Markov assumption does not hold, they do suggest that we may see more
violations of this assumption at the quantum level. However, in practice, the causal Markov

1042
Chapter 21. Causality
assumption appears to be a reasonable working assumption in most macroscopic systems.
The second assumption is the faithfulness assumption, which states that the only conditional
faithfulness
assumption
independencies in P ∗are those that arise from d-separation in the corresponding causal graph
G∗. When combined with the causal Markov assumption, the consequence is that G∗is a perfect
map of P ∗. As we discussed in section 3.4.2, there are many examples where the faithfulness
assumption is violated, so that there are independencies in P ∗not implied by the structure of
G∗; however, these correspond to particular parameter values, which (as stated in theorem 3.5)
are a set of measure zero within the space of all possible parameterizations. As we discussed,
there are still cases where one of these parameterizations arises naturally. However, for the
purposes of learning causality, we generally need to make both of these assumptions.
21.7.1.2
Identifying the Model
With these assumptions in hand, we can now assume that our data set consists of samples
from P ∗, and our task is simply to identify a perfect map for P ∗. Of course, as we observed
in section 3.4.2, the perfect map for a distribution P ∗is not unique. Because we might have

several diﬀerent structures that are equivalent in the independence assumptions they
impose, we cannot, based on observational data alone, distinguish between them. Thus,
we cannot, in general, determine a unique causal structure for our domain, even given
inﬁnite data. For the purpose of answering probabilistic queries, this limitation is irrelevant:
any of these structures will perform equally well. However, for causal queries, determining the
correct direction of the edges in the network is critical. Thus, at best, we can hope to identify
the equivalence class of G∗.
The class of constraint-based structure learning methods is suitable to this task. Here, we take
constraint-based
structure learning
the independencies observed in the empirical distribution and consider them as representative
of P ∗.
Given enough data instances, the empirical distribution ˆP will reﬂect exactly the
independencies that hold in P ∗, so that an independence oracle will provide accurate answers
about independencies in P ∗. The task now reduces to that of identifying an I-equivalence class
that is consistent with these observed independencies. For the case without confounding factors,
we have already discussed such a method: the Build-PDAG procedure described in section 18.2.
Recall that Build-PDAG constructs a class PDAG, which represents an entire I-equivalence class
class PDAG
of network structures. In the class PDAG, an edge is oriented X →Y if and only if it is oriented
in that way in every graph that is a member of the I-equivalence class. Thus, the algorithm does
not make unwarranted conclusions about directionality of edges.
Even with this conservative approach, we can often infer the directionality of certain edges.
Example 21.26
Assume that our underlying distribution P ∗is represented by the causal structure of the Student
network shown in ﬁgure 3.3. Assuming that the empirical distribution ˆP reﬂects the independencies
in P ∗, the Build-PDAG will return the PDAG shown in ﬁgure 21.7a. Intuitively, we can infer the
causal directions for the arcs D →G and I →G because of the v-structure at G; we can infer the
causal direction for G →J because the opposite orientation J →G would create a v-structure
involving J, which induces a diﬀerent set of independencies. However, we are unable to infer
a causal direction for the edge I—S, because both orientations are consistent with the observed
independencies.
In some sense, the constraint-based approaches are ideally suited to inferring causal direction;

21.7. Learning Causal Models
1043
Grade
Job
SAT
Intelligence
Difﬁculty
Grade
Job
SAT
Intelligence
Difﬁculty
H
Grade
Job
SAT
Intelligence
Difﬁculty
(a)
(c)
(b)
Figure 21.7
Models corresponding to the equivalence class of the Student network. (a) A PDAG,
representing its equivalence class when all relevant variables are observed. (b) A PAG, representing its
equivalence class when latent variables are a possibility. (c) An unsuccessful attempt to “undirect” the
G →J arc.
in fact, these algorithms were mostly developed for the purpose of causal discovery. However,
as we discussed in section 18.2, these approaches are fairly sensitive to mistakes in the inde-
pendence tests over the distribution. This property can make them somewhat brittle, especially
when the data set size is small relative to the number of variables.
Score-based methods allow us to factor in our conﬁdence in diﬀerent independencies, ﬁnding
a solution that is more globally consistent. Thus, an alternative approach is to apply model
selection with some appropriate score and then construct the I-equivalence class of the network
G produced by the structure learning algorithm. The I-equivalence class can be represented by
a PDAG, which can be constructed simply by applying the procedure Build-PDAG to G.
A better solution, however, accounts for the fact that the highest-scoring network is not gener-
ally the only viable hypothesis. In many cases, especially when data are scarce, there can be

several non-equivalent network structures that all have a reasonably high posterior prob-
ability. Thus, a better approach for causal discovery is to use Bayesian model averaging,
Bayesian model
averaging
as described in section 18.5. The result is a distribution over diﬀerent network structures that
allows us to encode our uncertainty not only about the orientation of an edge, but also about
the presence or absence of an edge. Furthermore, we obtain numerical conﬁdence measures
in these network features. Thus, whereas the PDAG representation prevents us from orienting
an edge X →Y even if there is only a single member in the equivalence class that has the
opposite orientation, the Bayesian model averaging approach allows us to quantify the overall
probability mass of structures in which an edge exists and is oriented in a particular direction.
Although constraint-based methods and Bayesian methods are often viewed as competing
solutions, one successful approach is to combine them, using a constraint-based method to
initialize a graph structure, and then using Bayesian methods to reﬁne it. This approach exploits
the strengths of both methods. It uses the global nature of the constraint-based methods to
avoid local maxima, and it uses the ability of the Bayesian methods to avoid making irreversible
decisions about independencies that may be incorrect due to noise in the data.

1044
Chapter 21. Causality
21.7.2
Learning from Interventional Data
So far, we have focused on the task of learning causal models from observational data alone. In
the causal setting, a natural question is to consider data that are obtained (at least partly) from
interventional queries. That is, some of our data cases are obtained in a situation where we
intervene in the model, sampled from the mutilated network corresponding to an intervention.
One important situation where such data are often available is scientiﬁc discovery, where the
data we obtain can be either a measurement of an existing system or a system that was subjected
to perturbations.
Although both constraint-based and score-based approaches can be applied in this setting,
constraint-based approaches are not as commonly used.
Although it is straightforward to
deﬁne the independencies associated with the mutilated network, we generally do not have
enough data instances for any given type of intervention to measure reliably whether these
independencies hold. Score-based approaches are much more ﬂexible at combining data from
diverse interventions. We therefore focus our discussion on that setting.
To formalize our analysis, we assume that each data instance in D is speciﬁed by an inter-
vention do(Z[m] := z[m]); for each such data case, we have a fully observed data instance
X[m] = ξ[m].
As usual in a score-based setting, our ﬁrst task is to deﬁne the likelihood
function. Consider ﬁrst the probability of a single instance P(ξ | do(Z := Z), C). This term
is deﬁned in terms of the mutilated network CZ=z. In this network, the distribution of each
variable Z ∈Z is deﬁned by the intervention, so that Z = z with probability 1 and therefore
is also the value we necessarily see in ξ. The variables not in Z are subject to their normal
probabilistic model, as speciﬁed in C. Letting ui be the assignment to PaXi in ξ, we obtain:
P(ξ | do(Z := z), C) =
Y
Xi̸∈Z
P(xi | ui).
In the case of table-CPDs (see exercise 21.8 for another example), it follows that the suﬃcient
suﬃcient
statistics
statistics for this type of likelihood function are:
M[xi; ui] =
X
m : Xi̸∈Z[m]
11{Xi[m] = xi, PaXi[m] = ui},
(21.7)
for an assignment xi to Xi and ui to PaXi.
This suﬃcient statistic counts the number
of occurrences of this event, in data instances where there is no intervention at Xi. Unlike our
original suﬃcient statistic M[xi, ui], this deﬁnition of the suﬃcient statistic treats Xi diﬀerently
from its parents, hence the change in notation.
It now follows that:
L(C : D) =
n
Y
i=1
Y
xi∈Val(Xi),ui∈Val(PaXi)
θM[xi;ui]
xi|ui
.
(21.8)
Because this likelihood function has the same functional form as our original likelihood function,
we can proceed to apply any of the likelihood-based approaches described in earlier chapters.
We can perform maximum likelihood estimation or incorporate a prior to deﬁne a Bayesian
posterior over the parameters. We can also deﬁne a Bayesian (or other likelihood-based) score in

21.7. Learning Causal Models
1045
order to perform model selection or model averaging. The formulas and derivations are exactly
the same, only with the new suﬃcient statistics.
Importantly, in this framework, we can now distinguish between I-equivalent models,

which are indistinguishable given observational data alone. In particular, consider a net-
work over two variables X, Y , and assume that we have interventional data at either X, Y , or
both. As we just mentioned, the suﬃcient statistics are asymmetrical in the parent and child, so
that M[X; Y ], for the network Y →X, is diﬀerent from M[Y ; X], for the network X →Y .
Therefore, although the two networks are I-equivalent, the likelihood function can be diﬀerent
for the two networks, allowing us to select one over the other.
Example 21.27
Consider the task of learning a causal model over the variables X, Y . Assume that we have the
samples with the suﬃcient statistics shown in the following table:
Intervention
x1, y1
x1, y0
x0, y1
x0, y0
None
4
1
1
4
do(X := x1)
2
0
0
0
do(Y := y1)
1
0
1
0
The observational data suggest that each of X and Y are (roughly) uniformly distributed, but that
they are correlated with each other. The interventional data, although limited, suggest that, when we
intervene at X, Y tends to follow, but when we intervene at Y , X is unaﬀected. These intuitions
suggest that the causal model X →Y is more plausible. Indeed, computing the suﬃcient statistics
for the model X →Y and these data instances, we obtain:
M[x1]
=
M[x1y1 | None] + M[x1y0 | None] + M[x1y1 | do(y1)] = 4 + 1 + 1
M[x0]
=
M[x0y1 | None] + M[x0y0 | None] + M[x0y1 | do(y1)] = 1 + 4 + 1
M[y1; x1]
=
M[x1y1 | None] + M[x1y1 | do(x1)] = 4 + 2
M[y0; x1]
=
M[x1y0 | None] = 1
M[y1; x0]
=
M[x0y1 | None] = 1
M[y0; x0]
=
M[x0y0 | None] = 4.
Importantly, we note that, unlike the purely observational case, M[y1; x1] + M[y0; x1] ̸= M[x1];
this is because diﬀerent data instances contribute to the diﬀerent counts, depending on the variable
at which the intervention takes place.
We can now compute the maximum likelihood parameters for this model as θx1 = 0.5, θy1|x1 =
6/7, θy1|x0 = 1/5. The log-likelihood of the data is then:
M[x1] log θx1 + M[x0] log θx0+
M[y1; x1] log θy1|x1 + M[y0; x1] log θy0|x1 + M[y1; x0] log θy1|x0 + M[y0; x0] log θy0|x0,
which equals −19.75.
We can analogously execute the same steps for the causal model Y →X, where our suﬃcient
statistics would have the form M[y] and M[x; y], each utilizing a diﬀerent set of instances. Overall,
we would obtain a log-likelihood of −21.41, which is lower than for the causal model X →Y .
Thus, the log-likelihood of the two causal models is diﬀerent, even though they are I-equivalent as
probabilistic models. Moreover, the causal model that is consistent with our intuitions is the one
that obtains the highest score.

1046
Chapter 21. Causality
We also note that an intervention at X can help disambiguate parts of the network

not directly adjacent to X. For example, assume that the true network is a chain X1 →
X2 →. . . →Xn. There are n I-equivalent directed graphs (where we root the graph at Xi
for i = 1, . . . , n). Interventions at any Xi can reveal that Xi+1, . . . , Xn all respond to an
intervention at Xi, whereas X1, . . . , Xi−1 do not. Although these experiments would not fully
disambiguate the causal structure, they would help direct all of the edges from Xi toward any of
its descendants. Perhaps less intuitive is that they also help direct edges that are not downstream
of our intervention. For example, if Xi−1 does not respond to an intervention at Xi, but we
are convinced that they are directly correlated, we now have more conﬁdence that we can direct
the edge Xi−1 →Xi. Importantly, directing some edges can have repercussions on others, as
we saw in section 3.4.3. Indeed, in practice, a series of interventions at some subset of variables
can signiﬁcantly help disambiguate the directionality of many of the edges; see box 21.D.
Box 21.D — Case Study: Learning Cellular Networks from Intervention Data. As we men-
tioned earlier, one of the important settings where interventional data arise naturally is scientiﬁc
discovery. One application where causal network discovery has been applied is to the task of
cellular network reconstruction. In the central paradigm of molecular biology, a gene in a cell (as
cellular network
reconstruction
encoded in DNA) is expressed to produce mRNA, which in turn is translated to produce protein,
which performs its cellular function. The diﬀerent steps of this process are carefully regulated. There
are proteins whose task it is to regulate the expression of their target genes; others change the
activity level of proteins by a physical change to the protein itself. Unraveling the structure of these
networks is a key problem in cell biology. Causal network learning has been successfully applied
to this task in a variety of ways.
One important type of cellular network is a signaling network, where a signaling protein phys-
ically modiﬁes the structure of a target in a process called phosphorylation, thereby changing its
activity level. Fluorescence microscopy can be used to measure the level of a phosphoprotein
— a particular protein in a particular phosphorylation state. The phosphoprotein is fused to a
ﬂuorescent marker of a particular color. The ﬂuorescence level of a cell, for a given color channel,
indicates the level of the phosphoprotein fused with that color marker. Current technology allows
us to measure simultaneously the levels of a small number of phosphoproteins, at the level of single
cells. These data provide a unique opportunity to measure the activity levels of several proteins
within individual cells, and thereby, we hope, to determine the causal network that underlies their
interactions.
Sachs et al. (2005) measured eleven phosphoproteins in a signaling pathway in human T-cells un-
der nine diﬀerent perturbation conditions. Of these conditions, two were general perturbations, but
the remaining seven activated or inhibited particular phosphoproteins, and hence could be viewed
as ideal interventions. Overall, 5,400 measurements were obtained over these nine conditions. The
continuous measurements for each gene were discretized using a k-means algorithm, and the sys-
tem was modeled as a Bayesian network where the variables are the levels of the phosphoproteins
and the edges are the causal connections between them. The network was learned using standard
score-based search, using a Bayesian score based on the interventional likelihood of equation (21.8).
To obtain a measure of conﬁdence in the edges of the learned structure, conﬁdence estimation was
performed using a bootstrap method, where the same learning procedure was applied to diﬀerent
bootstrap
training sets, each sampled randomly, with replacement, from the original data set. This procedure

21.7. Learning Causal Models
1047
gave rise to an ensemble of networks, each with its own structure. The conﬁdence associated with
an edge was then estimated as the fraction of the learned networks that contained the edge.
The result of this procedure gave rise to a network with seventeen high-conﬁdence causal arcs
between various components. A comparison to the known literature for this pathway revealed that
ﬁfteen of the seventeen edges were well established in the literature; the other two were novel but
had supporting evidence in at least one literature citation. Only three well-established connections
were missed by this analysis. Moreover, in all but one case, the direction of causal inﬂuence was
correctly inferred. One of the two novel predictions was subsequently tested and validated in a
wet-lab experiment. This ﬁnding suggested a new interaction between two pathways.
The use of a single global model for inferring the causal connections played an important role
in the quality of the results. For example, because causal directions of arcs are often compelled by
their interaction with other arcs within the overall structure, the learning algorithm was able to
detect correctly causal inﬂuences from proteins that were not perturbed in the assay. In other cases,
strong correlations did not lead to the inclusion of direct arcs in the model, since the correlations
were well explained by indirect pathways. Importantly, although the data were modeled as fully
observed, many relevant proteins were not actually measured. In such cases, the resulting indirect
paths gave rise to direct edges in the learned network.
Various characteristics of this data set played an important role in the quality of the results. For
example, the application of learning to a curtailed data set consisting solely of 1,200 observational
data points gave rise to only ten arcs, all undirected, of which eight were expected or reported; ten of
the established arcs were missing. Thus, the availability of interventional data played an important
role in the accurate reconstruction of the network, especially the directionality of the edges. Another
experiment showed the value of single-cell measurements, as compared to an equal-size data set
each of whose instances is an average over a population of cells. This result suggests that the cell
population is heterogeneous, so that averaging destroys much of the signal present in the data.
Nevertheless, the same techniques were also applied, with some success, to a data set that lacks
these enabling properties. These data consist of gene expression measurements — measurements of
mRNA levels for diﬀerent genes, each collected from a population of cells. Here, data were collected
from approximately 300 experiments, each acquired from a strain with a diﬀerent gene deleted.
Such perturbations are well modeled as ideal interventions, and hence they allow the use of the
same techniques. This data set poses signiﬁcant challenges: there are only 300 measurements (one
for each perturbation experiment), but close to 6,000 variables (genes); the population averaging of
each sample obscures much of the signal; and mRNA levels are a much weaker surrogate for activity
level than direct protein measurements. Nevertheless, by focusing attention on subgraphs where
many edges had high conﬁdence, it was possible to reconstruct correctly some known pathways.
The main limitation of these techniques is the assumption of acyclicity, which does not hold for
cellular networks. Nevertheless, these results suggest that causal-network learning, combined with
appropriate data, can provide a viable approach for uncovering cellular pathways of diﬀerent types.

1048
Chapter 21. Causality
21.7.3
Dealing with Latent Variables ⋆
So far, we have discussed the learning task in the setting where we have no confounding factors.
The situation is more complicated if we have confounding eﬀects such as latent variables or
selection bias. In this case, the samples in our empirical distribution are generated from a “partial
view” of P ∗, where some variables have been marginalized out, and others perhaps instantiated
to particular values. Because we do not know the set of confounding variables, there is an
inﬁnite set of networks that could have given rise to exactly the same set of dependencies over
the observable variables. For example, X →Y , X →H →Y , X →H →H′ →Y , and so
on are completely indistinguishable in terms of their eﬀect on X, Y (assuming that the hidden
variables H, H′ do not aﬀect other variables). The task of determining a causal model in this
case seems unreasonably daunting.
In the remainder of this section, we describe solutions to the problem of discovering causal
structure in presence of confounding eﬀects that induce noncausal correlations between the ob-
served variables. These confounding eﬀects include latent variables and selection bias. Although
there are methods that cover both of these problems, the treatment of the latter is signiﬁcantly
more complex.
Moreover, although selection bias clearly occurs, latent variables are almost
ubiquitous, and they have therefore been the focus of more work. We therefore restrict our
discussion to the case of learning models with latent variables.
21.7.3.1
Score-Based Approaches
A ﬁrst thought is to try to learn a model with hidden variables using score-based methods
such as those we discussed in chapter 19. The implementation of this idea, however, requires
signiﬁcant care. First, we generally do not know where in the model we need to introduce
hidden variables. In fact, we can have an unbounded number of latent variables in the model.
Moreover, as we saw, causal conclusions can be quite sensitive to local maxima or to design
decisions such as the number of values of the hidden variable. We note, again, that probabilistic
conclusions are also somewhat sensitive to these issues, but signiﬁcantly less so, since models
that achieve the same marginals over the observed variables are equivalent with respect to
probabilistic queries, but not with respect to causal queries. Nevertheless, when we have some
strong prior knowledge about the possible number and placement of hidden variables, and we
apply our analysis with care, we can learn informative causal models.
21.7.3.2
Constraint-Based Approaches
An alternative solution is to use constraint-based approaches that try to use the independence
properties of the distribution to learn the structure of the causal model, including the placement
of the hidden variables. Here, as for the fully observable case, the independencies in a distribu-
tion only determine a structure up to equivalence. However, in the case of latent variables, we
observe independence relationships only between the observable variables. We therefore need
to introduce a notion of the independencies induced over the observable variables. We say that
a directed acyclic graph G is a latent variable network over X if it is a causal network structure
latent variable
network
over X ∪H, where H is some arbitrarily large set of (latent) variables disjoint from X.

21.7. Learning Causal Models
1049
Deﬁnition 21.5
Let G be a latent variable network over X. We deﬁne IX (G) to be the set of independencies
(X ⊥Y | Z) ∈I(G) for X, Y , Z ⊂X.
We can now deﬁne a notion of I-equivalence over the observable variables.
Deﬁnition 21.6
Let G1, G2 be two latent variable networks over X (not necessarily over the same set of latent
variables). We say that G1 and G2 are IX -equivalent if IX (G1) = IX (G2).
IX -equivalence
Clearly, we cannot explicitly enumerate the IX -equivalence class of a graph. Even in the
fully observable case, this equivalence class can be quite large; when we allow latent variables,
the equivalence class is generally inﬁnite, since we can have an unbounded number of latent
variables placed in a variety of conﬁgurations.
The constraint-based methods sidestep this
diﬃculty by searching over a space of graphs over the observable variables alone. Like PDAGs,
an edge in these graphs can represent diﬀerent types of relationships between its endpoints.
Deﬁnition 21.7
Let GG be an IX -equivalence class of latent variable networks over X. A partial ancestral graph (PAG)
partial ancestral
graph
P over X is a graph whose nodes correspond to X, and whose edges represent the dependencies in
GG. The presence of an edge between X and Y in P corresponds to the existence, in each G ∈GG,
of an active trail between X and Y that utilizes only latent variables. Edges have three types of
endpoints: −, >, and ◦; these endpoints on the Y end of an edge between X and Y have the
following meanings:
• An arrowhead > implies that Y is not an ancestor of X in any graph in GG.
• A straight end −implies that Y is an ancestor of X in all graphs in GG.
• A circle ◦implies that neither of the two previous cases holds.
The interpretation of the diﬀerent edge types is as follows: An edge X →Y has (almost) the
standard meaning: X is an ancestor of Y in all graphs in GG, and Y is not an ancestor of X
in any graph. Thus, each graph in GG contains a directed path from X to Y . However, some
graphs may also contain trail where a latent variable is an ancestor of both. Thus, for example,
the edge S →C would represent both of the networks in ﬁgure 21.A.1a,b when both G and T
are latent.
An edge X ↔Y means that X is never an ancestor of Y , and Y is never an ancestor of X;
thus, the edge must be due to the presence of a latent common cause. Note that an undirected
edge X—Y is illegal relative to this deﬁnition, since it is inconsistent with the acyclicity of the
graphs in GG. An edge X◦→Y means that Y is not an ancestor of X in any graph, but X is an
ancestor of Y in some, but not all, graphs.
Figure 21.8 shows an example PAG, along with several members of the (inﬁnite) equivalence
class that it represents. All of the graphs in the equivalence class have one or more active trails
between X and Y , none of which are directed from Y to X.
At ﬁrst glance, it might appear that the presence of latent variables completely eliminates our
ability to infer causal direction. After all, any edge can be ascribed to an indirect correlation via
a latent variable. However, somewhat surprisingly, there are conﬁgurations where we can infer a
causal orientation to an edge.

1050
Chapter 21. Causality
X
Y
X
Y
X
Y
X
Y
X
Y
X
Y
PAG
Sample graphs in
equivalence class
Figure 21.8
Example PAG (left), along with several members of the (inﬁnite) equivalence class that
it represents. All of the graphs in the equivalence class have one or more active trails between X and Y ,
none of which are directed from Y to X.
Example 21.28
Consider again the learning problem in example 21.26, but where we now allow for the presence of
latent variables. Figure 21.7b shows the PAG reﬂecting the equivalence class of our original network
of ﬁgure 3.3. Not surprisingly, we cannot reach any conclusions about the edge between I and
S. The edge between D and G can arise both from a directed path from D to G and from the
presence of a latent variable that is the parent of both. However, a directed path from G to D
would not result in a marginal independence between D and I, and a dependence given G. Thus,
we have an arrowhead > on the G side of this edge. The same analysis holds for the edge between
I and G.
Most interesting, however, is the directed edge G →J, which asserts that, in any graph in GG,
there is a directed path from G to J. To understand why, let us try to explain the correlation
between G and J by introducing a common latent parent (see ﬁgure 21.7c). This model is not IX -
equivalent to our original network, because it implies that J is marginally independent of I and
D. More generally, we can conclude that J must be a descendant of I and D, because observing
J renders them dependent. Because G renders J independent of I, it must block any directed path
from I to J. It follows that there is a directed path from G to J in every member of GG. In fact, in
this case, we can reach the stronger conclusions that all the trails between these two variables are
directed paths from G to J. Thus, in this particular case, the causal inﬂuence of G on J is simply
P(J | G), which we can obtain directly from observational data alone.
This example gives intuition for how we might determine a PAG structure for a given distri-
bution. The algorithm for constructing a PAG for a distribution P proceeds along similar lines
to the algorithm for constructing PDAGs, described in section 3.4.3 and 18.2. The full algorithm
for learning PAGs is quite intricate, and we do not provide a full description of it, but only

21.7. Learning Causal Models
1051
give some high-level intuition. The algorithm has two main phases. The ﬁrst phase constructs
an undirected graph over the observed variables, representing direct probabilistic interactions
between them in P. In general, we want to connect X and Y with a direct edge if and only if
there is no subset Z of X −{X, Y } such that P |= (X ⊥Y | Z). Of course, we cannot actually
enumerate over the exponentially many possible subsets Z ⊂X. As in Build-PMap-Skeleton,
we both bound the size of the possible separating set and prune sets that cannot be separating
sets given our current knowledge about the adjacency structure of the graph. The second phase
of the algorithm orients as many edges as possible, using reasoning similar to the ideas used for
PDAGs, but extended to deal with the confounding eﬀect of latent variables.
The PAG-learning algorithm oﬀers similar (albeit somewhat weaker) guarantees than the PDAG
construction algorithm.
In particular, one cannot show that the edge orientation rules are
complete, that is, produce the strongest possible conclusion about edge orientation that is
consistent with the equivalence class. However, one can show that all of the latent variable
networks over X that are consistent with a PAG produced by this algorithm are IX -equivalent.
Importantly, we note that a PAG is only a partial graph structure, and not a full model; thus, it
cannot be used directly for answering causal queries. One possible solution is to use the score-
based techniques we described to parameterize the causal model. This approach, however, is
fraught with diﬃculties: First, we have the standard diﬃculties of using EM to learn parameters
for hidden variables; an even bigger problem is that the PAG provides no guidance about the
number of latent variables, their domain, or the edges between them.
Another alternative is to use the methods of section 21.3 and 21.5, which use a learned causal
structure with latent variables, in conjunction with statistics over the observable data, to answer
causal queries. We note, however, that these methods require a known connectivity structure
among the hidden variables, whereas the learned PAG does not specify this structure. Never-
theless, if we are willing to introduce some assumptions about this structure, these algorithms
may be usable. We return to this option in section 21.7.4, where we discuss more robust ways of
estimating the answers to such queries.
21.7.4
Learning Functional Causal Models ⋆
Finally, we turn to the question of learning a much richer class of models: functional causal
models, where we have a set of response variables with their associated parameters. As we
discussed, these models have two distinct uses.
The ﬁrst is to answer a broader range of
causal queries, such as counterfactual queries or queries regarding the average causal eﬀect.
The second is to avoid, to some extent, the inﬁnite space of possible conﬁgurations of latent
variables. As we discussed in section 21.4, a fully speciﬁed functional causal model summarizes
the eﬀect of all of the exogenous variables on the variables in our model, and thereby, within a
ﬁnite description, speciﬁes the causal behavior of our endogenous variables. Thus, rather than
select a set of concrete latent variables with a particular domain and parameterization for each
one, we use response variables to summarize all of the possibilities. Our conclusions in this
case are robust, and they apply for any true underlying model of the latent variables.
The diﬃculty, of course, is that a functional causal model is a very complex object. The pa-
rameterization of a response variable is generally exponentially larger than the parameterization
of a CPD for the corresponding endogenous variable. Moreover, the data we are given provide
the outcome in only one of the exponentially many counterfactual cases given by the response

1052
Chapter 21. Causality
variable. In this section, we describe one approach for learning with these issues.
Recall that a functional causal model is parameterized by a joint distribution P(U) over the
response variables. The local models of the endogenous variables are, by deﬁnition, deterministic
functions of the response variables. A response variable U X for a variable X with parents Y
is a discrete random variable, whose domain is the space of all functions µ(Y ) from Val(Y )
to Val(X).
The joint distribution P(U) is encoded by a Bayesian network.
We ﬁrst focus
on the case where the structure of the network is known, and our task is only to learn the
parameterization. We then brieﬂy discuss the issue of structure learning.
Consider ﬁrst the simple case, where U X has no parents. In this case, we can parameterize
U X using a multinomial distribution νX = (νX
1 , . . . , νX
m), where m = |Val(U X)|. In the
Bayesian approach, we would take this parameter to be itself a random variable and introduce
an appropriate prior, such as a Dirichlet distribution, over νX. More generally, we can use the
techniques of section 17.3 to parameterize the entire Bayesian network over U.
Our goal is then to compute the posterior distribution P(U | D); this posterior deﬁnes an
answer to both intervention queries and counterfactual queries. Consider a general causal query
P(φ | ψ), where φ and ψ may contain both real and counterfactual variables, and ψ may also
contain interventions. We have that:
P(φ | ψ, D) =
Z
P(φ | ψ, ν)P(ν | ψ, D)dν.
Assuming that D is reasonably large, we can approximate P(ν | ψ, D) as P(ν | D). Thus,
to answer a causal query, we simply take the expectation of the answer to the query over the
posterior parameter distribution P(ν | D).
The main diﬃculty in this procedure is that the data set D is only partly observable: even
if we fully observe the endogenous variables X, the response variables U are not directly
observed. As we saw, an observed assignment ξ to X limits the set of possible values to U to
the subset of functions consistent with ξ. In particular, if x, y is the assignment to X, Y in
ξ, then U X is restricted to the set of possible functions µ for which µ(y) = x, a set that is
exponentially large. Thus, to apply Bayesian learning, we must use techniques that approximate
the posterior parameter distribution P(ν | D). In section 19.3, we discussed several approaches
to approximating this posterior, including variational Bayesian learning and MCMC methods.
Both can be applied in this setting as well.
Thus, in principle, this approach is a straightforward application of techniques we have already
discussed. However, because of the size of the space, the use of functional causal models in
general and in this case in particular is feasible only for fairly small models.
When we also need to learn the structure of the functional causal model, the situation be-
comes even more complex, since the problem is one of structure learning in the presence of
hidden variables. One approach is to use the constraint-based approach of section 21.7.3.2 to
learn a structure involving latent variables, and then the approach described here for ﬁlling in
the parameters. A second approach is to use one of the methods of section 19.4. However, there
is an important issue that arises in this approach: Recall that a response variable for a variable
X speciﬁes the value of X for each conﬁguration of its endogenous parents U. Thus, as our
structure learning algorithm adapts the structure of the network, the domain of the response
variables changes; for example, if our search adds a parent to X, the domain of U X changes.
Thus, when performing the search, we would need to recompute the posterior parameter dis-

21.8. Summary
1053
tribution and thereby the score after every structure change to the model.
However, under
certain independence assumptions, we can use score decomposability to reduce signiﬁcantly
the amount of recomputation required; see exercise 21.10.
21.8
Summary
In this chapter, we addressed the issue of ascribing a causal interpretation to a Bayesian network.
While a causal interpretation does not provide any additional capabilities in terms of answering
standard probabilistic queries, it provides the basic framework for answering causal queries —
queries involving interventions in the world. We provided semantics for causal models in terms
of the causal mechanism by which a variable’s value is generated. An intervention query can
then be viewed as a substitution of the existing causal mechanism with one that simply forces
the intervened variable to take on a particular value.
We discussed the greater sensitivity of causal queries to the speciﬁcs of the model, including
the speciﬁc orientations of the arcs and the presence of latent variables. Latent variables are
particularly tricky, since they can induce correlations between the variables in the model that
are hard to distinguish from causal relationships.
These issues make the identiﬁcation of a
causal model much more diﬃcult than the selection of an adequate probabilistic model.
We presented a class of situations in which a causal query can be answered exactly, using only
a distribution over the observable variables, even when the model as a whole is not identiﬁable.
In other cases, even if the query is not fully identiﬁable, we can often provide surprisingly strong
bounds over the answer to a causal query.
Besides intervention queries, causal models can also be used to answer counterfactual queries
— queries about a sequence of events that we know to be diﬀerent from the sequence that
actually took place in the world. To answer such queries, we need to make explicit the random
choices made in selecting the values of variables in the model; these random choices need to be
preserved between the real and counterfactual worlds in order to maintain the correct semantics
for the idea of a counterfactual. Functional causal models allow us to represent these random
choices in a ﬁnite way, regardless of the (potentially unbounded) number of latent variables in
the domain. We showed how to use functional causal models to answer counterfactual queries.
While these models are even harder to identify than standard causal models, the techniques for
partially identifying causal queries can also be used in this case.
Finally, we discussed the controversial and challenging problem of learning causal models
from data. Much of the work in this area has been devoted to the problem of inferring causal
models from observational data alone. This problem is very challenging, especially when we
allow for the possible presence of latent variables. We described both constraint-based and
Bayesian methods for learning causal models from data, and we discussed their advantages and
disadvantages.
Causality is a fundamental concept when reasoning about many topics, ranging from speciﬁc
scientiﬁc applications to commonsense reasoning. Causal networks provide a framework for
performing this type of reasoning in a systematic and principled way. On the other side, the
learning algorithms we described, by combining prior knowledge about domain structure with
empirical data, can help us identify a more accurate causal structure, and perhaps obtain a
better understanding of the domain. There are many possible applications of this framework in

1054
Chapter 21. Causality
the realm of scientiﬁc discovery, both in the physical and life sciences and in the social sciences.
21.9
Relevant Literature
The use of functional equations to encode causal processes dates back at least as far as the work
of Wright (1921), who used them to model genetic inheritance. Wright (1934) also used directed
graphs to represent causal structures.
The view of Bayesian networks as encoding causal processes was present throughout much of
their history, and certainly played a signiﬁcant role in early work on constraint-based methods
for learning network structure from data (Verma and Pearl 1990; Spirtes et al. 1991, 1993). The
formal framework for viewing a Bayesian network as a causal graph was developed in the early
and mid 1990s, primarily by two groups: by Spirtes, Glymour, and Scheines, and by Pearl and
his students Balke and Galles. Much of this work is summarized in two seminal books: the
early book of Spirtes, Glymour, and Scheines (1993) and the more recent book by Pearl (2000),
on which much of the content of this chapter is based. The edited collection of Glymour and
Cooper (1999) also reviews other important developments.
The use of a causal model for analyzing the eﬀect of interventions was introduced by Pearl
and Verma (1991) and Spirtes, Glymour, and Scheines (1993). The formalization of the causal
calculus, which allows the simpliﬁcation of intervention queries and their reformulation in
terms of purely observable queries, was ﬁrst presented in detail in Pearl (1995). The example
on smoking and cancer was also presented there.
Based on these ideas, Galles and Pearl
(1995) provide an algorithm for determining the identiﬁability of an intervention query. Dawid
(2002, 2007) provides an alternative formulation of causal intervention that makes explicit use
of decision variables. This perspective, which we used in section 21.3, signiﬁcantly simpliﬁes
certain aspects of causal reasoning.
The idea of making mechanisms explicit via response variables is based on ideas proposed in
Rubin’s theory of counterfactuals (Rubin 1974). It was introduced into the framework of causal
networks by Balke and Pearl (1994b,a), and in parallel by Heckerman and Shachter (1994), who use
a somewhat diﬀerent framework based on inﬂuence diagrams. Balke and Pearl (1994a) describe a
method that uses the distribution over the observed variables to constrain the distribution of the
response variables. The PeptAid example (example 21.25) is due to Balke and Pearl (1994a), who
also performed the analysis of the cholesterolymine example (box 21.B). Chickering and Pearl
(1997) present a Gibbs sampling approach to Bayesian parameter estimation in causal settings.
The work on constraint-based structure learning (described in section 18.2) was ﬁrst presented
as an approach for learning causal networks. It was proposed and developed in the work of
Verma and Pearl (1990) and in the work of Spirtes et al. (1993). Even this very early work was
able to deal with latent variables. Since then, there has been signiﬁcant work on extending and
improving these early algorithms. Spirtes, Meek, and Richardson (1999) present a state-of-the-
art algorithm for identifying a PAG from data and show that it can accommodate both latent
variables and selection bias.
Heckerman, Meek, and Cooper (1999) proposed a Bayesian approach to causal discovery and
the use of a Markov chain Monte Carlo algorithm for sampling structures in order to obtain
probabilities of causal features.
The extension of Bayesian structure learning to a combination of observational and inter-

21.10. Exercises
1055
ventional data was ﬁrst developed by Cooper and Yoo (1999). These ideas were extended and
applied by Pe’er et al. (2001) to the problem of identifying regulatory networks from gene ex-
pression data, and by Sachs et al. (2005) to the problem of identifying signaling networks from
ﬂuorescent microscopy data, as described in box 21.D. Tong and Koller (2001a,b) build on these
ideas in addressing the problem of active learning — choosing a set of interventions so as best
active learning
to learn a causal network model.
21.10
Exercises
Exercise 21.1⋆
a. Prove proposition 21.2, which allows us to convert causal interventions in a query into observations.
b. An alternative condition for this proposition works in terms of the original graph G rather than the
graph G†. Let GX denote the graph G, minus all edges going out of nodes in X. Show that the
d-separation criterion used in the proposition is equivalent to requiring that Y is d-separated from X
given Z, W in the graph GZX.
Exercise 21.2⋆
Prove proposition 21.3, which allows us to drop causal interventions from a query entirely.
Exercise 21.3⋆
For probabilistic queries, we have that
min
x P(y | x) ≤P(y) ≤max
x
P(y | x).
Show that the same property does not hold for intervention queries. Speciﬁcally, provide an example where
it is not the case that:
min
x P(y | do(x)) ≤P(y) ≤max
x
P(y | do(x)).
Exercise 21.4⋆⋆
Show that every one of the diagrams in ﬁgure 21.3 is identiﬁable via the repeated application of proposi-
tion 21.1, 21.2, and 21.3.
Exercise 21.5⋆
a. Show that, in the causal model of ﬁgure 21.4g, each of the queries P(Z1 | do(X)), P(Z2 | do(X)),
P(Y | do(Z1)), and P(Y | do(Z2)) are identiﬁable.
b. Explain why the eﬀect of X on Y cannot be identiﬁable in this model.
c. Show that we can identify both P(Y | do(X), do(Z1)) and P(Y | do(X), do(Z2)). This example
illustrates that the eﬀect of a joint intervention may be more easily identiﬁed than the eﬀect of each
of its components.
Exercise 21.6
As we discussed in box 21.C, under certain assumptions, we can reduce the cost of performing counterfac-
tual inference to that of a standard probabilistic query. In particular, assume that we have a system status
variable X that is a noisy-or of the failure variables X1, . . . , Xk, and that there is no leak probability, so
that X = x0 when all Xi = x0
i (that is, X is normal when all its components are normal). Furthermore,
assume that only a single Xi is in the failure mode (Xi = x1
i ). Show that
P(x′0 | x1, do(x0
i ), e) = P(d1
i | x1, e),
where Zi is the noisy version of Xi, as in deﬁnition 5.11.

1056
Chapter 21. Causality
Exercise 21.7
This exercise demonstrates computation of suﬃcient statistics with interventional data. The following table
shows counts for diﬀerent interventions.
Intervention
x0y0z0
x0y0z1
x0y1z0
x0y1z1
x1y0z0
x1y0z1
x1y1z0
x1y1z1
None
4
2
1
0
3
2
1
4
do(X := x0)
3
1
2
1
0
0
0
0
do(Y := y0)
7
1
0
0
2
1
0
0
do(Z := z0)
1
0
1
0
1
0
1
0
Calculate M[x0; y0z0], M[y0; x0z0], and M[x0].
Exercise 21.8
Consider the problem of learning a Gaussian Bayesian network from interventional data D. As in sec-
tion 21.7.2, assume that each data instance in D is speciﬁed by an intervention do(Z[m] := z[m]); for
each such data case, we have a fully observed data instance X[m] = ξ[m]. Write down the suﬃcient
statistics that would be used to score a network structure G from this data set.
Exercise 21.9⋆
Consider the problem of Bayesian learning for a functional causal model C over a set of endogenous
variables X. Assume we have a data set D where the endogenous variables X are fully observed. Describe
a way for approximating the parameter posterior P(ν | X) using collapsed Gibbs sampling. Speciﬁcally,
your algorithm should sample the response variables U and compute a closed-form distribution over the
parameters ν.
Exercise 21.10⋆⋆
Consider the problem of learning the structure of a functional causal model C over a set of endogenous
variables X.
a. Using your answer from exercise 21.9, construct an algorithm for learning the structure of a causal
model. Describe precisely the key steps used in the algorithm, including the search steps and the use
of the Gibbs sampling algorithm to evaluate the score at each step.
b. Now, assume that we are willing to stipulate that the response variables U X for each variable X are
independent. (This assumption is a very strong one, but it may be a reasonable approximation in
some cases.) How can you signiﬁcantly improve the learning algorithm in this case? Provide a new
pseudo-code description of the algorithm, and quantify the computational gains.
Exercise 21.11⋆
As for probabilistic independence, we can deﬁne a notion of causal independence: (X ⊥C Y | Z) if,
causal
independence
for any values x, x′ ∈Val(X), we have that P(Y | do(Z), do(x)) = P(Y | do(Z), do(x′)). (Note
that, unlike probabilistic independence — (X ⊥Y | Z) — causal independence is not symmetric over
X, Y .)
a. Is causal independence equivalent to the statement: “For any value x ∈Val(X), we have that
P(Y | do(Z), do(x)) = P(Y | do(Z)).” (Hint: Use your result from exercise 21.3.)
b. Prove that (X ⊥C Y | Z, W ) and (W ⊥C Y | X, Z) implies that (X, W ⊥C Y | Z). Intuitively,
this property states that if changing X cannot aﬀect P(Y ) when W is ﬁxed, and changing W cannot
aﬀect P(Y ) when X is ﬁxed, then changing X and Y together cannot aﬀect P(Y ).
Exercise 21.12⋆
We discussed the issue of trying to use data to extract causal knowledge, that is, the directionality of
an inﬂuence. In this problem, we will consider the interaction between this problem and both hidden
variables and selection bias.

21.10. Exercises
1057
D
A
E
B
C
Figure 21.9
Learned causal network for exercise 21.12
Assume that our learning algorithm came up with the network in ﬁgure 21.9, which we are willing to
assume is a perfect map for the distribution over the variables A, B, C, D, E. Under this assumption,
among which pairs of variables between which a causal path exists in this model does there also necessarily
exist a causal path . . .
a. . . . if we assume there are no hidden variables?
b. . . . if we allow the possibility of one or more hidden variables?
c. . . . if we allow for the possibility of selection bias?
For each of these options, specify the pairs for which a causal path exists, and explain why it exists in
every IX -equivalent structure. For the other pairs, provide an example of an IX -equivalent structure for
which no causal path exists.


22
Utilities and Decisions
We now move from the task of simply reasoning under uncertainty — reaching conclusions
about the current situation from partial evidence — to the task of deciding how to act in the
world. In a decision-making setting, an agent has a set of possible actions and has to choose
between them. Each action can lead to one of several outcomes, which the agent can prefer to
diﬀerent degrees.
Most simply, the outcome of each action is known with certainty. In this case, the agent must
simply select the action that leads to the outcome that is most preferred. Even this problem is
far from trivial, since the set of outcomes can be large and complex and the agent must weigh
diﬀerent factors in determining which of the possible outcomes is most preferred. For example,
when deciding which computer to buy, the agent must take into consideration the CPU speed,
the amount of memory, the cost, the screen size, and many other factors. Deciding which of
the possible conﬁgurations he most prefers can be quite diﬃcult.
Even more diﬃcult is the decision-making task in situations where the outcome of an action
is not fully determined. In this case, we must take into account both the probabilities of various
outcomes and the preferences of the agent between these outcomes. Here, it is not enough to
determine a preference ordering between the diﬀerent outcomes. We must be able to ascribe
preferences to complex scenarios involving probability distributions over possible outcomes.
The framework of decision theory provides a formal foundation for this type of reasoning. This
decision theory
framework requires that we assign numerical utilities to the various possible outcome, encoding
the agent’s preferences.
In this chapter, we focus on a discussion of utilities functions and
the principle of maximum expected utility, which is the foundation for decision making under
uncertainty. In the next chapter, we discuss computationally tractable representations of an
agent’s decision problem and the algorithmic task of ﬁnding an optimal strategy.
22.1
Foundations: Maximizing Expected Utility
In this section, we formally describe the basic decision-making task and deﬁne the principle of
maximum expected utility. We also provide a formal justiﬁcation for this principle from basic
axioms of rationality.
22.1.1
Decision Making Under Uncertainty
We begin with a simple motivating example.

1060
Chapter 22. Utilities and Decisions
Example 22.1
Consider a decision maker who encounters the following situation. She can invest in a high-tech
company (A), where she can make a proﬁt of $4 million with 20 percent probability and $0 with 80
percent probability; or she can invest in pork belly futures (B), where she can make $3 million with
25 percent probability and $0 with 75 percent probability. (That is, the pork belly investment is less
proﬁtable but also less risky.) In order to choose between these two investment opportunities, the
investor must compare her preferences between two scenarios, each of which encodes a probability
distribution over outcomes: the ﬁrst scenario, which we denote πA, can be written as [$4million :
0.2; $0 : 0.8]; the second scenario, denoted πB, has the form [$3million : 0.25; $0 : 0.75].
In order to ascertain which of these scenarios we prefer, it is not enough to determine that
we prefer $4 million to $3 million to $0. We need some way to aggregate our preferences for
these outcomes with the probabilities with which we will get each of them. One approach
for doing this aggregation is to assign each outcome a numerical utility, where a higher utility
utility
value associated with an outcome indicates that this outcome is more preferred. Importantly,
however, utility values indicate more than just an ordinal preference ranking between outcomes;
their numerical value is signiﬁcant by itself, so that the relative values of diﬀerent states tells us
the strength of our preferences between them. This property allows us to combine the utility
values of diﬀerent states, allowing us to ascribe an expected utility to situations where we are
expected utility
uncertain about the outcome of an action. Thus, we can compare two possible actions using
their expected utility, an ability critical for decision making under uncertainty.
We now formalize these intuitions.
Deﬁnition 22.1
A lottery π over an outcome space O is a set [π1 : α1; . . . ; πk : αk] such that α1, . . . , αk ∈[0, 1],
lottery
P
i αi = 1, and each πi is an outcome in O. For two lotteries π1, π2, if the agent prefers π1, we
preference over
lotteries
say that π1 ≻π2. If the agent is indiﬀerent between the two lotteries, we say that π1 ∼π2.
A comparison between two diﬀerent scenarios involving uncertainty over the outcomes is
quite diﬃcult for most people.
At ﬁrst glance, one might think that the “right” decision is
the one that optimizes a person’s monetary gain. However, that approach rarely reﬂects the
preferences of the decision maker.
Example 22.2
Consider a slightly diﬀerent decision-making situation. Here, the investor must decide between
company C, where she earns $3 million with certainty, and company D, where she can earn $4
million with probability 0.8 and $0 with probability 0.2. In other words, she is now comparing
two lotteries πC = [$3million : 1] and πD = [$4million : 0.8; $0 : 0.2]. The expected proﬁt of
lottery D is $3.2 million, which is larger than the proﬁt of $3 million from lottery C. However, a
vast majority of people prefer the option of lottery C to that of lottery D.
The problem becomes far more complicated when one accounts for the fact that many
decision-making situations involve aspects other than ﬁnancial gain.

A general framework that allows us to make decisions such as these ascribes a numer-
ical utility to diﬀerent outcomes. An agent’s utilities describe her overall preferences,
which can depend not only on monetary gains and losses, but also on all other relevant
aspects. Each outcome o is associated with a numerical value U(o), which is a numerical
encoding of the agent’s “happiness” for this outcome. Importantly, utilities are not just ordinal

22.1. Foundations: Maximizing Expected Utility
1061
values, denoting the agent’s preferences between the outcomes, but are actual numbers whose
magnitude is meaningful. Thus, we can probabilistically aggregate utilities and compute their
expectations over the diﬀerent possible outcomes.
We now make these intuitions more formal.
Deﬁnition 22.2
A decision-making situation D is deﬁned by the following elements:
decision-making
situation
• a set of outcomes O = {o1, . . . , oN};
outcome
• a set of possible actions that the agent can take, A = {a1, . . . , aK};
action
• a probabilistic outcome model P : A 7→∆O, which deﬁnes a lottery πa, which speciﬁes a
probability distribution over outcomes given that the action a was taken;
• a utility function U : O 7→IR, where U(o) is the agent’s preferences for the outcome o.
utility function
Note that the deﬁnition of an outcome can also include the action taken; outcomes that involve
one action a would then get probability 0 in the lottery induced by another action a′.
Deﬁnition 22.3
The principle of maximum expected utility (MEU principle) asserts that, in a decision-making
MEU principle
situation D, we should choose the action a that maximizes the expected utility:
expected utility
EU[D[a]] =
X
o∈O
πa(o)U(o).
Example 22.3
Consider a decision situation IF where a college graduate is trying to decide whether to start up
a company that builds widgets. The potential entrepreneur does not know how large the market
demand for widgets really is, but he has a distribution: the demand is either m0—nonexistent,
m1—low, or m2—high, with probabilities 0.5, 0.3, and 0.2 respectively. The entrepreneur’s proﬁt,
if he founds the startup, depends on the situation. If the demand is nonexistent, he loses a signiﬁcant
amount of money (outcome o1); if it is low, he sells the company and makes a small proﬁt (outcome
o2); if it is high, he goes public and makes a fortune (outcome o3). If he does not found the startup,
he loses nothing and earns nothing (outcome o0). These outcomes might involve attributes other
than money. For example, if he loses a signiﬁcant amount of money, he also loses his credibility
and his ability to start another company later on. Let us assume that the agent’s utilities for the
four outcomes are: U(o0) = 0; U(o1) = −7; U(o2) = 5; U(o3) = 20. The agent’s expected utility
for the action of founding the company (denoted f 1) is
EU[D[f 1]] = 0.5 · (−7) + 0.3 · 5 + 0.2 · 20 = 2.
His expected utility for the action of not founding the company (denoted f 0) is 0. The action choice
maximizing the expected utility is therefore f 1.
Our deﬁnition of a decision-making situation is very abstract, resulting in the impression
that the setting is one where an agent takes a single simple action, resulting in a single simple
outcome. In fact, both actions and outcomes can be quite complex. Actions can be complete
strategies involving sequences of decisions, and outcomes (as in box 22.A) can also involve
multiple aspects. We will return to these issues later on.

1062
Chapter 22. Utilities and Decisions
22.1.2
Theoretical Justiﬁcation ⋆
What justiﬁes the principle of maximizing expected utility, with its associated assumption re-
garding the existence of a numerical utility function, as a deﬁnition of rational behavior? It
turns out that there are several theoretical analyses that can be used to prove the existence of
such a function. At a high level, these analyses postulate some set of axioms that characterize
the behavior of a rational decision maker. They then show that, for any agent whose decisions
abide by these postulates, there exists some utility function U such that the agent’s decisions
are equivalent to maximizing the expected utility relative to U.
The analysis in this chapter is based on the premise that a decision maker under uncertainty
must be able to decide between diﬀerent lotteries. We then make a set of assumptions about
the nature of the agent’s preferences over lotteries; these assumptions arguably should hold for
the preferences of any rational agent. For an agent whose preferences satisfy these axioms,

we prove that there exists a utility function U such that the agent’s preferences are
equivalent to those obtained by maximizing the expected utility relative to U.
We ﬁrst extend the concept of a lottery.
Deﬁnition 22.4
A compound lottery π over an outcome space O is a set [π1 : α1; . . . ; πk : αk] such that
compound lottery
α1, . . . , αk ∈[0, 1], P
i αi = 1, and each πi is either an outcome in O or another lottery.
Example 22.4
One example of a compound lottery is a game where we ﬁrst toss a coin; if it comes up heads, we
get $3 (o1); if it comes up tails, we participate in another subgame where we draw a random card
from a deck, and if it comes out spades, we get $50 (o2); otherwise we get nothing (o3). This lottery
would be represented as [o1 : 0.5; [o2 : 0.25; o3 : 0.75] : 0.5].
We can now state the postulates of rationality regarding the agent’s preferences over lotteries.
rationality
postulates
At ﬁrst glance, each these postulates seems fairly reasonable, but each of them has been subject
to signiﬁcant criticism and discussion in the literature.
•
(A1) Orderability: For all lotteries π1, π2, either
(π1 ≺π2) or (π1 ≻π2) or (π1 ∼π2)
(22.1)
This postulate asserts that an agent must know what he wants; that is, for any pair of
lotteries, he must prefer one, prefer the other, or consider them to be equivalent. Note that
this assumption is not a trivial one; as we discussed, it is hard for people to come up with
preferences over lotteries.
•
(A2) Transitivity: For all lotteries π1, π2, π3, we have that:
If (π1 ≺π2) and (π2 ≺π3) then (π1 ≺π3).
(22.2)
The transitivity postulate asserts that preferences are transitive, so that if the agent prefers
lottery 1 to lottery 2 and lottery 2 to lottery 3, he also prefers lottery 1 to lottery 3. Although
transitivity seems very compelling on normative grounds, it is the most frequently violated
axiom in practice. One hypothesis is that these “mistakes” arise when a person is forced to
make choices between inherently incomparable alternatives. The idea is that each pairwise

22.1. Foundations: Maximizing Expected Utility
1063
comparison invokes a preference response on a diﬀerent “attribute” (for instance, money,
time, health). Although each scale itself may be transitive, their combination need not be. A
similar situation arises when the overall preference arises as an aggregate of the preferences
of several individuals.
•
(A3) Continuity: For all lotteries π1, π2, π3,
If (π1 ≺π2 ≺π3) then there exists α ∈(0, 1) such that (π2 ∼[π1 : α; π3 : (1−α)]). (22.3)
This postulate asserts that if π2 is somewhere between π1 and π3, then there should be some
lottery between π1 and π3, which is equivalent to π2. For our simple Entrepreneur example,
we might have that o0 ∼[o1 : 0.8; o3 : 0.2]. This axiom excludes the possibility that one
alternative is “inﬁnitely better” than another one, in the sense that any probability mixture
involving the former is preferable to the latter. It therefore captures the relationship between
probabilities and preferences and the form in which they compensate for each other.
•
(A4) Monotonicity: For all lotteries π1, π2, and probabilities α, β,
(π1 ≻π2), (α ≥β) ⇒([π1 : α; π2 : (1 −α)] ≻[π1 : β; π2 : (1 −β)]).
(22.4)
This postulate asserts that an agent prefers that better things happen with higher probability.
Again, although this attribute seems unobjectionable, it has been argued that risky behavior
such as Russian roulette violates this axiom. People who choose to engage in such behavior
seem to prefer a probability mixture of “life” and “death” to “life,” even though they (pre-
sumably) prefer “life” to “death.” This argument can be resolved by revising the outcome
descriptions, incorporating the aspect of the thrill obtained by playing the game.
•
(A5) Substitutability: For all lotteries π1, π2, π3, and probabilities α,
(π1 ∼π2) ⇒([π1 : α; π3 : (1 −α)] ∼[π2 : α; π3 : (1 −α)]).
(22.5)
This axiom states that if π1 and π2 are equally preferred, we can substitute one for the other
without changing our preferences.
•
(A6) Decomposability: For all lotteries π1, π2, and probabilities α, β,
[π1 : α, [π2 : β, π3 : (1−β)] : (1−α)] ∼[π1 : α, π2 : (1−α)β, π3 : (1−α)(1−β)]. (22.6)
This postulate says that compound lotteries are equivalent to ﬂat ones. For example, our
lottery in example 22.4 would be equivalent to the lottery
[o1 : 0.5; o2 : 0.125; o3 : 0.375].
Intuitively, this axiom implies that the preferences depend only on outcomes, not the process
in which they are obtained. It implies that a person does not derive any additional pleasure
(or displeasure) from suspense or participation in the game.
If we are willing to accept these postulates, we can derive the following result:

1064
Chapter 22. Utilities and Decisions
Theorem 22.1
Assume that we have an agent whose preferences over lotteries satisfy the axioms (A1)–(A6). Then
there exists a function U : O 7→IR, such that, for any pair of lotteries π, π′, we have that π ≺π′
if and only if U(π) < U(π′), where we deﬁne (recursively) the expected utility of any lottery as:
U([π1 : α1, . . . , πk : αk]) =
k
X
i=1
αiU(πi).
That is, the utility of a lottery is simply the expectation of the utilities of its components.
Proof Our goal is to take a preference relation ≺that satisﬁes these axioms, and to construct
a utility function U over consequences such that ≺is equivalent to implementing the MEU
principle over the utility function U. We take the least and most preferred outcomes omin
and omax; these outcomes are typically known as anchor outcomes. By orderability (A1) and
anchor outcome
transitivity (A2), such outcomes must exist. We assign U(omin) := 0 and U(omax) := 1. By
orderability, we have that for any other outcome o:
omin ⪯o ⪯omax.
By continuity (A3), there must exist a probability α such that
[o : 1] ∼[omin : (1 −α); omax : α]
(22.7)
We assign U(o) := α. The axioms can then be used to show that the assignment of util-
ities to lotteries resulting from applying the expected utility-principle results in an ordering
that is consistent with our preferences. We leave the completion of this proof as an exercise
(exercise 22.1).
From an operational perspective, this discussion gives us a formal justiﬁcation for the principle
of maximum expected utility. When we have a set of outcomes, we ascribe a numerical utility
to each one. If we have a set of actions that induce diﬀerent lotteries over outcomes, we should
choose the action whose expected utility is largest; as shown by theorem 22.1, this choice is
equivalent to choosing the action that induces the lottery we most prefer.
22.2
Utility Curves
The preceding analysis shows that, under certain assumptions, a utility function must exist.
However, it does not provide us with an understanding of utility functions. In this section, we
take a more detailed look at the form of a utility functions and its connection to the utility
function properties.
A utility function assigns numeric values to various possible outcomes.
These outcomes
can vary along multiple dimensions. Most obvious is monetary gain, but most settings involve
other attributes as well. We begin in this section by considering the utility of simple outcomes,
involving only a single attribute. We discuss the form of a utility function over a single attribute
and the eﬀects of the utility function on the agent’s behavior. We focus on monetary outcomes,
which are the most common and easy to understand. However, many of the issues we discuss
in this section — those relating to risk attitudes and rationality — are general in their scope,
and they apply also to other types of outcomes.

22.2. Utility Curves
1065
22.2.1
Utility of Money
Consider a decision-making situation where the outcomes are simply monetary gains or losses.
In this simple setting, it is tempting to assume that the utility of an outcome is simply the amount
of money gained in that outcome (with losses corresponding to negative utilities). However, as
we discussed in example 22.2, most people do not always choose the outcome that maximizes
their expected monetary gain. Making such a decision is not irrational; it simply implies that,
for most people, their utility for an outcome is not simply the amount of money they have in
that outcome.
Consider a graph whose X-axis is the monetary gain a person obtains in an outcome (with
losses corresponding to negative amounts), and whose Y -axis is the person’s utility for that
outcome. In general, most people’s utility is monotonic in money, so that they prefer outcomes
with more money to outcomes with less. However, if we draw a curve representing a person’s
utility curve
utility as a function of the amount of money he or she gains in an outcome, that curve is rarely
a straight line. This nonlinearity is the “justiﬁcation” for the rationality of the preferences we
observe in practice in example 22.2.
Example 22.5
Let c0 represent the agent’s current ﬁnancial status, and assume for simplicity that he assigns a
utility of 0 to c0. If he assigns a utility of 10 to the consequence c0 + 3million and 12 to the
consequence c0 + 4million, then the expected utility of the gamble in example 22.2 is 0.2 · 0 + 0.8 ·
12 = 9.6 < 10. Therefore, with this utility function, the agent’s decision is completely rational.
A famous example of the nonlinearity of the utility of money is the Saint Petersburg paradox:
Saint Petersburg
paradox
Example 22.6
Suppose you are oﬀered a chance to play a game where a fair coin is tossed repeatedly until it
comes up heads. If the ﬁrst head appears on the nth toss, you get $2n. How much would you be
willing to pay in order to play this game?
The probability of the event Hn—the ﬁrst head showing up on the nth toss—is 1/2n. Therefore,
the expected winnings from playing this game are:
∞
X
n=1
P(Hn)Payoﬀ(Hn) =
∞
X
n=1
1
2n 2n = 1 + 1 + 1 + . . . = ∞.
Therefore, you should be willing to pay any amount to play this game. However, most people are
willing to pay only about $2.
Empirical psychological studies show that people’s utility functions in a certain range often
grow logarithmically in the amount of monetary gain. That is, the utility of the outcome ck,
corresponding to an agent’s current ﬁnancial status plus $k, looks like α + β log(k + γ). In the
Saint Petersburg example, if we take U(ck) = log2 k, we get:
∞
X
n=1
P(Hn)U(Payoﬀ(Hn)) =
∞
X
n=1
1
2n U(c2n) =
∞
X
n=1
n
2n = 2,
which is precisely the amount that most people are willing to pay in order to play this game.
In general, most people’s utility function tends to be concave for positive amount of money,
so that the incremental value of additional money decreases as the amount of wealth grows.

1066
Chapter 22. Utilities and Decisions
U
$
Figure 22.1
Example curve for the utility of money
Conversely, for negative amounts of money (debts), the shape of the curve often has the opposite
shape, as shown in ﬁgure 22.1. Thus, for many people, going into debt of $1 million has signiﬁcant
negative utility, but the additional negative utility incurred by an extra $1 million of debt is a
lot lower.
Formally, |U(−$2, 000, 000) −U(−$1, 000, 000)| is often signiﬁcantly less than
|U(−$1, 000, 000) −U($0)|.
22.2.2
Attitudes Toward Risk

There is a tight connection between the form of a person’s utility curve and his behavior
in diﬀerent decision-making situations. In particular, the shape of this curve determines
the person’s attitude toward risk. A concave function, as in ﬁgure 22.2, indicates that the
risk
agent is risk-averse: he prefers a sure thing to a gamble with the same payoﬀ. Consider in
risk-averse
more detail the risk-averse curve of ﬁgure 22.2. We see that the utility of a lottery such as
π = [$1000 : 0.5, $0 : 0.5] is lower than the utility of getting $500 with certainty. Indeed,
risk-averse preferences are characteristic of most people, especially when large sums of money
are involved. In particular, recall example 22.2, where we compared a lottery where we win $3
million with certainty to one where we win $4 million with probability 0.8. As we discussed,
most people prefer the ﬁrst lottery to the second, despite the fact that the expected monetary
gain in the ﬁrst lottery is lower. This behavior can be explained by a risk-averse utility function
in that region.
Returning to the lottery π, empirical research shows that many people are indiﬀerent between
playing π and the outcome where they get (around) $400 with certainty; that is, the utilities of
the lottery and the outcome are similar. The amount $400 is called the certainty equivalent of
certainty
equivalent
the lottery. It is the amount of “sure thing” money that people are willing to trade for a lottery.
The diﬀerence between the expected monetary reward of $500 and the certainty equivalent of
$500 is called the insurance premium, and for good reason. The premium people pay to the
insurance
premium
insurance company is precisely to guarantee a sure thing (a sure small loss) as opposed to a
lottery where one of the consequences involves a large negative utility (for example, the price of
rebuilding the house if it burns down).
As we discussed, people are typically risk-averse. However, they often seek risk when the
certain loss is small (relative to their ﬁnancial situation). Indeed, lotteries and other forms of
gambling exploit precisely this phenomenon. When the agent prefers the lottery to the certainty

22.2. Utility Curves
1067
Certain equivalent
Insurance/risk premium
U($500)
U
U(p)
p
0
400 500
1000
$ Reward
Figure 22.2
Utility curve and its consequences to an agent’s attitude toward risk
equivalent, he is said to be risk-seeking, a behavior that corresponds to a curve whose shape
risk-seeking
is convex.
Finally, if the agent’s utility curve is linear, he is said to be risk-neutral.
Most
risk-neutral
utility curves are locally linear, which means we can assume risk neutrality for small risks and
rewards. Finally, as we noted, people are rarely consistent about risk throughout the entire
monetary range: They are often risk-averse for positive gains, but can be risk-seeking for large
negative amounts (going into debt). Thus, in our example, someone who is already $10 million
in debt might choose to accept a gamble on a fair coin with $10 million payoﬀon heads and a
$20 million loss on tails.
22.2.3
Rationality
The framework of utility curves provides a rich language for describing complex behaviors,
including risk-averse, risk-seeking, or risk-neutral behaviors. They can even change their risk
preferences over the range. One may thus be tempted to conclude that, for any behavior proﬁle,
there is some utility function for which that behavior is rational. However, that conclusion turns
out to be false; indeed, empirical evidence shows that people’s preferences are rarely rational
under our deﬁnitions.
Example 22.7
Consider again the two simple lotteries in example 22.2 and example 22.1. In the two examples,
we had four lotteries:
πA : [$4million : 0.2; $0 : 0.8]
πB : [$3million : 0.25; $0 : 0.75]
πC : [$3million : 1]
πD : [$4million : 0.8; $0 : 0.2].

1068
Chapter 22. Utilities and Decisions
Most people, by an overwhelming majority, prefer πC to πD.
The opinions on πA versus πB
are more divided, but quite a number of people prefer πA to πB. Each of these two preferences
— πD ≻πC and πA ≻πB — is rational relative to some utility functions. However, their
combination is not — there is no utility function that is consistent with both of these preferences.
To understand why, assume (purely to simplify the presentation) that U($0) = 0. In this case,
preferring πC to πD is equivalent to saying that
U(c3,000,000) > 0.8 · U(c4,000,000).
On the other hand, preferring πA to πB is equivalent to:
0.2 · U(c4,000,000)
>
0.25 · U(c3,000,000)
0.8 · U(c4,000,000)
>
U(c3,000,000).
Multiplying both sides of the ﬁrst inequality by 4, we see that these two statements are directly
contradictory, so that these preferences are inconsistent with decision-theoretic foundations, for any
utility function.
Thus, people are often irrational, in that their choices do not satisfy the principle of maximum
expected utility relative to any utility function. When confronted with their “irrationality,” the
responses of people vary. Some feel that they have learned an important lesson, which often
aﬀects other decisions that they make.
For example, some subjects have been observed to
cancel their automobile collision insurance and take out more life insurance. In other cases,
people stick to their preferences even after seeing the expected utility analysis. These latter
cases indicate that the principle of maximizing expected utility is not, in general, an adequate
descriptive model of human behavior. As a consequence, there have been many proposals for
alternative deﬁnitions of rationality that attempt to provide a better ﬁt to the behavior of human
decision makers. Although of great interest from a psychological perspective, there is no reason
to believe that these frameworks will provide a better basis for building automated decision-
making systems. Alternatively, we can view decision theory as a normative model that provides
the “right” formal basis for rational behavior, regardless of human behavior. One can then argue
that we should design automated decision-making systems based on these foundations; indeed,
so far, most such systems have been based on the precepts of decision theory.
22.3
Utility Elicitation
22.3.1
Utility Elicitation Procedures
How do we acquire an appropriate utility function to use in a given setting? In many ways, this
problem is much harder than acquiring a probabilistic model. In general, we can reasonably
assume that the probabilities of chance events apply to an entire population and acquire a
single probabilistic model for the whole population. For example, when constructing a medical
diagnosis network, the probabilities will usually be learned from data or acquired from a human
expert who understands the statistics of the domain. By contrast, utilities are inherently personal,
and people often have very diﬀerent preference orderings in the same situation. Thus, the utility
function we use needs to be acquired for the individual person or entity for whom the decision

22.3. Utility Elicitation
1069
is being made.
Moreover, as we discussed, probability values can be learned from data by
observing empirical frequencies in the population. The individuality of utility values, and the
fact that they are never observed directly, makes it diﬃcult to apply similar learning methods to
the utility acquisition task.
There have been several methods proposed for eliciting utilities from people.
The most
utility elicitation
classical method is the standard gamble method, which is based directly on the axioms of utility
standard gamble
theory.
In the proof of theorem 22.1, we selected two anchor states — our least preferred
and most preferred states s⊥and s⊤. We then used the continuity axiom (equation (22.3)) to
place each state on a continuous spectrum between these two anchor states, by ﬁnding the
indiﬀerence point α — a probability value α ∈[0, 1] such that s ∼[s⊥: (1 −α); s⊤: α].
indiﬀerence point
We can convert this idea to a utility elicitation procedure as follows. We select a pair of anchor
states. In most cases, these are determined in advance, independently of the user. For example,
in a medical decision-making situation, s⊥is often “death,” whereas s⊤is an immediate and
complete cure. For any outcome s, we can now try to ﬁnd the indiﬀerence point. It is generally
assumed that we cannot ask a user to assess the value of α directly. We therefore use some
procedure that searches over the space of possible α’s. If s ≺[s⊥: (1−α); s⊤: α], we consider
lower values of α, and if s ≻[s⊥: (1 −α); s⊤: α], we consider higher values, until we ﬁnd
the indiﬀerence point. Taking U(s⊥) = 0 and U(s⊤) = 1, we simply take U(s) = α.
The standard gamble procedure is satisfying because of its sound theoretical foundations.
However, it is very diﬃcult for people to apply in practice, especially in situations involving
large numbers of outcomes. Moreover, many independent studies have shown that the ﬁnal
values obtained in the process of standard gamble elicitation are sensitive to the choice of
anchors and to the choice of the search procedure.
Several other methods for utility elicitation have been proposed to address these limitations.
For example, time trade-oﬀtries to compare two outcomes: (1) t years (where t is the patient’s
time trade-oﬀ
life expectancy) in the current state of health (state s), and (2) t′ years (where t′ < t) in perfect
health (the outcome s⊤).
As in standard gamble, t′ is varied until the indiﬀerent point is
reached, and the utility of the state s is taken to be proportional to t′ at that point. Another
method, the visual-analog scale, simply asks users to point out their utilities on some scale.
visual-analog
scale
Overall, each of the methods proposed has signiﬁcant limitations in practice. Moreover, the
results obtained for the same individual using diﬀerent methods are usually quite diﬀerent,
putting into question the results obtained by any method. Indeed, one might wonder whether
there even exists such an object as a person’s “true utility value.” Nevertheless, one can still
argue that decisions made for an individual using his or her own utility function (even with
the imprecisions involved in the process) are generally better for that individual than decisions
made using some “global” utility function determined for the entire population.
22.3.2
Utility of Human Life
Attributes whose utility function is particularly diﬃcult to acquire are those involving human
life. Clearly, such factors play a key role in medical decision-making situations. However, they
also appear in a wide variety of other settings. For example, even a simple decision such as
whether to replace worn-out tires for a car involves the reduced risk of death or serious injury
in a car with new tires.
Because utility theory requires that we reduce all outcomes to a single numerical value, we

1070
Chapter 22. Utilities and Decisions
are forced to place a utility value on human life, placing it on the same scale as other factors,
such as money. Many people ﬁnd this notion morally repugnant, and some simply refuse to
do so. However, the fact of the matter is that, in making decisions, one makes these trade-oﬀs,
whether consciously or unconsciously. For example, airplanes are not overhauled after each trip,
even though that would clearly improve safety. Not all cars are made with airbags, even though
they are known to save lives. Many people accept an extra stopover on a ﬂight in order to save
money, even though most airplane accidents happen on takeoﬀand landing.
Placing a utility on human life raises severe psychological and philosophical diﬃculties. One
such diﬃculty relates to actions involving some probability of death. The naive approach would
be to elicit the utility of the outcome death and then estimate the utility of an outcome involving
some probability p of death as p · U(death). However, this approach implies that people’s utility
is linear in their probability of death, an assumption which is generally false. In other words,
even if a person is willing to accept $50 for an outcome involving a one-in-a-million chance
of death, it does not mean that he would be willing to accept $50 million for the outcome of
death with certainty. Note that this example shows that, at least for this case, people violate the
basic assumption of decision theory: that a person’s preference for an uncertain outcome can
be evaluated using expected utility, which is linear in the probabilities.
A more appropriate approach is to encode explicitly the chance of death. Thus, a key metric
used to measure utilities for outcomes involving risk to human life is the micromort — a one-in-
micromort
a-million chance of death. Several studies across a range of people have shown that a micromort
is worth about $20 in 1980 dollars, or under $50 in today’s dollars. We can consider a utility
curve whose X-axis is micromorts. As for monetary utility, this curve behaves diﬀerently for
positive and negative values. For example, many people are not willing to pay very much to
remove a risk of death, but require signiﬁcant payment in order to assume additional risk.
Micromorts are useful for evaluating situations where the primary consideration is the proba-
bility that death will occur. However, in many situations, particularly in medical decision making,
the issue is not the chance of immediate death, but rather the amount of life that a person has
remaining. In one approach, we can evaluate outcomes using life expectancy, where we would
construct a utility curve whose X axis was the number of expected years of life. However, our
preferences for outcomes are generally much more complex, since they involve not only the
quantity but also the quality of life. In some cases, a person may prefer to live for fewer years,
but in better health, than to live longer in a state where he is in pain or is unable to perform
certain activities. The trade-oﬀs here are quite complex, and highly personal.
One approach to simplifying this complex problem is by measuring outcomes using units
called QALYs — quality-adjusted life years. A year of life in good health with no inﬁrmities is
QALY
worth 1 QALY. A year of life in poor health is discounted, and it is worth some fraction (less
than 1) of a QALY. (In fact, some health states — for example, those involving signiﬁcant pain
and loss of function — may even be worth negative QALYs.) Using QALYs, we can assign a
single numerical score to complex outcomes, where a person’s state of health can evolve over
time. QALYs are much more widely used than micromorts as a measure of utility in medical
and social-policy decision making.

22.4. Utilities of Complex Outcomes
1071
22.4
Utilities of Complex Outcomes
So far, we have largely focused on outcomes involving only a single attribute. In this case,
we can write down our utility function as a simple table in the case of discrete outcomes, or
as a curve in the case of continuous-valued outcomes (such as money). In practice, however,
outcomes often involve multiple facets. In a medical decision-making situation, outcomes might
involve pain and suﬀering, long-term quality of life, risk of death, ﬁnancial burdens, and more.
Even in a much “simpler” setting such as travel planning, outcomes involve money, comfort of
accommodations, availability of desired activities, and more. A utility function must incorporate
the importance of these diﬀerent attributes, and the preferences for various values that they
might take, in order to produce a single numeric value for each outcome.
Our utility function in domains such as this has to construct a single number for each
outcome that depends on the values of all of the relevant variables. More precisely, assume that
an outcome is described by an assignment of values to some set of variables V = {V1, . . . , Vk};
we then have to deﬁne a utility function U
:
Val(V ) 7→IR.
As usual, the size of this
representation is exponential in k.
In the case of probabilities, we addressed the issue of exponential blowup by exploiting
structure in the distribution. We showed a direct connection between independence properties
of the distribution and our ability to represent it compactly as a product of smaller factors. As
we now discuss, very similar ideas apply in the setting of utility functions.

Speciﬁcally, we can show a correspondence between “independence properties” among
utility attributes of an agent and our ability to factor his utility function into a combi-
nation of subutility functions, each deﬁned over a subset of utility attributes. A subutility
subutility
function
function is a function f : Val(Y ) 7→IR, for some Y ⊆V , where Y is the scope of f.
However, the notion of independence in this setting is somewhat subtle. A utility function on
its own does not induce behavior; it is a meaningful entity only in the context of a decision-
making setting. Thus, our independence properties must be deﬁned in that context as well. As
we will see, there is not a single deﬁnition of independence that is obviously the right choice;
several deﬁnitions are plausible, each with its own properties.
22.4.1
Preference and Utility Independence ⋆
To understand the notion of independence in decision making, we begin by considering the
simpler setting, where we are making decisions in the absence of uncertainty. Here, we need
only consider preferences on outcomes. Let X, Y be a disjoint partition of our set of utility
attributes V . We thus have a preference ordering ≺over pairs (x, y).
When can we say that X is “independent” of Y ? Intuitively, if we are given Y = y, we
can now consider our preferences over the possible values x, given that y holds. Thus, we
have an induced preference ordering ≺y over values x ∈Val(X), where we write x1 ≺y x2
if (x1, y) ≺(x2, y). In general, the ordering induced by one value y is diﬀerent from the
ordering induced by another. We say that X is preferentially independent of Y if all values y
induce the same ordering over Val(X). More precisely:
Deﬁnition 22.5
The set of attributes X is preferentially independent of Y = V −X in ≺if, for all y, y′ ∈
preference
independence

1072
Chapter 22. Utilities and Decisions
Val(Y ), and for all x1, x2 ∈Val(X), we have that
x1 ≺y x2
⇔
x1 ≺y′ x2.
Note that preferential independence is not a symmetric relation:
Example 22.8
Consider an entrepreneur whose utility function U(S, F) involves two binary-valued attributes:
the success of his company (S) and the fame he gets (F). One reasonable preference ordering over
outcomes might be:
(s0, f 1) ≺(s0, f 0) ≺(s1, f 0) ≺(s1, f 1).
That is, the most-preferred state is where he is successful and famous; the next-preferred state is where
he is successful but not famous; then the second-next-preferred state is where he is unsuccessful but
unknown; and the least-preferred state is where he is unsuccessful and (in)famous. In this preference
ordering, we have that S is preferentially independent of F, since the entrepreneur prefers to be
successful whether he is famous or not ((s0, f 1) ≺(s1, f 1) and (s0, f 0) ≺(s1, f 0)). On the
other hand, F is not preferentially independent of S, since the entrepreneur prefers to be famous if
successful but unknown if he is unsuccessful.
When we move to the more complex case of reasoning under uncertainty, we compare
decisions that induce lotteries over outcomes.
Thus, our notion of independence must be
deﬁned relative to this more complex setting. From now on, let ≺, U be a pair, where U is
a utility function over Val(V ), and ≺is the associated preference ordering for lotteries over
Val(V ). We deﬁne independence properties for U in terms of ≺.
Our ﬁrst task is to deﬁne the notion of a conditional preference structure, where we “ﬁx” the
value of some subset of variables Y . This structure deﬁnes a preference ordering ≺y for lotteries
over Val(X), given some particular instantiation y to Y . The deﬁnition is a straightforward
generalization of the one we used for preferences over outcomes:
Deﬁnition 22.6
Let πX
1 and πX
2 be two distributions over Val(X). We deﬁne the conditional preference structure
conditional
preference
structure
≺y as follows:
πX
1 ≺y πX
2
if
(πX
1 , 1y) ≺(πX
2 , 1y),
where (πX, 1y) assigns probability πX(x) to any assignment (x, y) and probability 0 to any
assignment (x, y′) for y′ ̸= y.
In other words, the preference ordering ≺y “expands” lotteries over Val(X) by having Y = y
with probability 1, and then using ≺.
With this deﬁnition, we can now generalize preferential independence in the obvious way: X
is utility independent of Y = V −X when conditional preferences for lotteries over X do not
depend on the particular value y given to Y .
Deﬁnition 22.7
We say that X is utility independent of Y = V −X if, for all y, y′ ∈Val(Y ), and for any pair
utility
independence
of lotteries πX
1 , πX
2 over Val(X), we have that:
πX
1 ≺y πX
2
⇔
πX
1 ≺y′ πX
2 .

22.4. Utilities of Complex Outcomes
1073
Because utility independence is a straight generalization of preference independence, it, too, is
not symmetric.
Note that utility independence is only deﬁned for a set of variables and its complement. This
limitation is inevitable in the context of decision making, since we can deﬁne preferences only
over entire outcomes, and therefore every variable must be assigned a value somehow.
Diﬀerent sets of utility independence assumptions give rise to diﬀerent decompositions of the
utility function. Most basically, for a pair (≺, U) as before, we have that:
Proposition 22.1
A set X is utility independent of Y = V −X in ≺if and only if U has the form:
U(V ) = f(Y ) + g(Y )h(X).
Note that each of the functions f, g, h has a smaller scope than our original U, and hence this
representation requires (in general) fewer parameters.
From this basic theorem, we can obtain two conclusions.
Proposition 22.2
Every subset of variables X ⊂V is utility independent of its complement if and only if there exist
k functions Ui(Vi) and a constant c such that
U(V ) =
k
Y
i=1
Ui(Vi) + c,
or k functions Ui(Vi) such that
U(V ) =
k
X
i=1
Ui(Vi).
In other words, when every subset is utility independent of its complement, the utility function
decomposes either as a sum or as a product of subutility functions over individual variables. In
utility
decomposition
this case, we need only elicit a linear number of parameters, exponentially fewer than in the
general case.
If we weaken our assumption, requiring only that each variable in isolation is utility indepen-
dent of its complement, we obtain a much weaker result:
Proposition 22.3
If, for every variable Vi ∈V , Vi is utility independent of V −{Vi}, then there exist k functions
Ui(Vi) (i = 1, . . . , k) such that U is a multilinear function (a sum of products) of the Ui’s.
For example, if V = {V1, V2, V3}, then this theorem would imply only that U(V1, V2, V3) can
be written as
c1U1(V1)U2(V2)U3(V3) + c2U1(V1)U2(V2) + c3U1(V1)U3(V3) + c4U2(V2)U3(V3)+
c5U1(V1) + c6U2(V2) + c7U3(V3).
In this case, the number of subutility functions is linear, but we must elicit (in the worst case)
exponentially many coeﬃcients. Note that, if the domains of the variables are large, this might
still result in an overall savings in the number of parameters.

1074
Chapter 22. Utilities and Decisions
22.4.2
Additive Independence Properties
Utility independence is an elegant assumption, but the resulting decomposition of the utility
function can be diﬃcult to work with. The case of a purely additive or purely multiplicative
decomposition is generally too limited, since it does not allow us to express preferences that
relate to combinations of values for the variables. For example, a person might prefer to take
a vacation at a beach destination, but only if the weather is good; such a preference does not
easily decompose as a sum or a product of subutilities involving only individual variables.
In this section, we explore progressively richer families of utility factorizations, where the
utility is encoded as a sum of subutility functions:
U(V ) =
k
X
i=1
Ui(Zi).
(22.8)
We also study how these decompositions correspond to a form of independence assumption
about the utility function.
22.4.2.1
Additive Independence
In our ﬁrst decomposition, we restrict attention to decomposition as in equation (22.8), where
Z1, . . . , Zk is a disjoint partition of V . This decomposition is more restrictive than the one
allowed by utility independence, since we allow a decomposition only as a sum, and not as a
product. This decomposition turns out to be equivalent to a notion called additive independence,
additive
independence
which has much closer ties to probabilistic independence. Roughly speaking, X and Y are
additively independent if our preference function for lotteries over V depends only on the
marginals over X and Y . More generally, we deﬁne:
Deﬁnition 22.8
Let Z1, . . . , Zk be a disjoint partition of V . We say that Z1, . . . , Zk are additively independent
in ≺if, for any lotteries π1, π2 that have the same marginals on all Zi, we have that π1 and π2
are indiﬀerent under ≺.
Additive independence is strictly stronger than utility independence: For two subsets X∪Y =
V that are additively independent, we have both that X is utility independent of Y and Y
is utility independent of X.
It then follows, for example, that the preference ordering in
example 22.8 does not have a corresponding additively independent utility function. Additive
independence is equivalent to the decomposition of U as a sum of subutilities over the Zi’s:
Theorem 22.2
Let Z1, . . . , Zk be a disjoint partition of V , and let ≺, U be a corresponding pair of a preference
ordering and a utility function. Then Z1, . . . , Zk are additively independent in ≺if and only if
U can be written as: U(V ) = Pk
i=1 Ui(Zi).
Proof The “if” direction is straightforward. For the “only if” direction, consider ﬁrst the case
where X, Y is a disjoint partition of V , and X, Y are additively independent in ≺. Let x, y
be some arbitrary ﬁxed assignment to X, Y . Let x′, y′ be any other assignment to X, Y . Let
π1 be the distribution that assigns probability 0.5 to each of x, y and x′, y′, and π2 be the
distribution that assigns probability 0.5 to each of x, y′ and x′, y. These two distributions have

22.4. Utilities of Complex Outcomes
1075
the same marginals over X and Y . Therefore, by the assumption of additive independence,
π1 ∼π2, so that
0.5U(x, y) + 0.5U(x′, y′)
=
0.5U(x, y′) + 0.5U(x′, y)
U(x′, y′)
=
U(x, y′) −U(x, y) + U(x′, y).
(22.9)
Now, deﬁne U1(X) = U(X, y) and U2(Y ) = U(x, Y ) −U(x, y). It follows directly from
equation (22.9) that for any x′, y′, U(x′, y′) = U1(x′) + U2(y′), as desired. The case of a
decomposition Z1, . . . , Zk follows by a simple induction on k.
Example 22.9
Consider a student who is deciding whether to take a diﬃcult course.
Taking the course will
require a signiﬁcant time investment during the semester, so it has a cost. On the other hand,
taking the course will result in a more impressive résumé, making the student more likely to get
a good job with a high salary after she graduates.
The student’s utility might depend on the
two attributes T (taking the course) and J (the quality of the job obtained). The two attributes
are plausibly additively independent, so that we can express the student’s utility as U1(T) +
U2(J). Note that this independence of the utility function is completely unrelated to any possible
probabilistic (in)dependencies. For example, taking the class is deﬁnitely correlated probabilistically
with the student’s job prospects, so T and J are dependent as probabilistic attributes but additively
independent as utility attributes.
In general, however, additive independence is a strong notion that rarely holds in practice.
Example 22.10
Consider a student planning his course load for the next semester. His utility might depend on two
attributes — how interesting the courses are (I), and how much time he has to devote to class work
versus social activities (T). It is quite plausible that these two attributes are not utility independent,
because the student might be more willing to spend signiﬁcant time on class work if the material
is interesting.
Example 22.11
Consider the task of making travel reservations, and the two attributes H — the quality of one’s
hotel — and W — the weather.
Even these two seemingly unrelated attributes might not be
additively independent, because the pleasantness of one’s hotel room is (perhaps) more important
when one has to spend more time in it on account of bad weather.
22.4.2.2
Conditional Additive Independence
The preceding discussion provides a strong argument for extending additive independence to the
case of nondisjoint subsets. For this extension, we turn to probability distributions for intuition:
In a sense, additive independence is analogous to marginal independence. We therefore wish to
construct a notion analogous to conditional independence:
Deﬁnition 22.9
Let X, Y , Z be a disjoint partition of V . We say that X and Y are conditionally additively
CA-independence
independent (CA-independent) given Z in ≺if, for every assignment z to Z, X and Y are
additively independent in the conditional preference structure ≺z.

1076
Chapter 22. Utilities and Decisions
The CA-independence condition is equivalent to an assumption that the utility decomposes
with overlapping subsets:
Proposition 22.4
Let X, Y , Z be a disjoint partition of V , and let ≺, U be a corresponding pair of a preference
ordering and a utility function. Then X and Y are CA-independent given Z in ≺if and only if
U can be written as:
U(X, Y , Z) = U1(X, Z) + U2(Y , Z).
The proof is straightforward and is left as an exercise (exercise 22.2).
Example 22.12
Consider again example 22.10, but now we add an attribute F representing how much fun the
student has in his free time (for example, does he have a lot of friends and hobbies that he enjoys?).
Given an assignment to T, which determines how much time the student has to devote to work
versus social activities, it is quite reasonable to assume that I and F are additively independent.
Thus, we can write U(I, T, F) as U1(I, T) + U2(T, F).
Based on this result, we can prove an important theorem that allows us to view a utility
function in terms of a graphical model. Speciﬁcally, we associate a utility function with

an undirected graph, like a Markov network. As in probabilistic graphical models, the
separation properties in the graph encode the CA-independencies in the utility function.
Conversely, the utility function decomposes additively along the maximal cliques in the
network.
Formally, we deﬁne the two types of relationships between a pair (≺, U) and an
undirected graph:
Deﬁnition 22.10
We say that H is an CAI-map for ≺if, for any disjoint partition X, Y , Z of V , if X and Y are
CAI-map
separated in H given Z, we have that X and Y are CA-independent in ≺given Z.
Deﬁnition 22.11
We say that a utility function U factorizes according to H if we can write U as a sum
utility
factorization
U(V ) =
k
X
c=1
Uc(Cc),
where C1, . . . , Ck are the maximal cliques in H.
We can now show the same type of equivalence between these two deﬁnitions as we did for
probability distributions. The ﬁrst theorem goes from factorization to independencies, showing
that a factorization of the utility function according to a network H implies that it satisﬁes the
independence properties implied by the network. It is analogous to theorem 3.2 for Bayesian
networks and theorem 4.1 for Markov networks.
Theorem 22.3
Let (≺, U) be a corresponding pair of a preference function and a utility function. If U factorizes
according to H, then H is a CAI-map for ≺.
Proof The proof of this result follows immediately from proposition 22.4.
Assume that U
factorizes according to H, so that U = P
c Uc(Cc). Any Cc cannot involve variables from both
X and Y . Thus, we can divide the cliques into two subsets: C1, which involve only variables in

22.4. Utilities of Complex Outcomes
1077
X, Z, and C2, which involve only variables in Y , Z. Letting Ui = P
c∈Ci Uc(Cc), for i = 1, 2,
we have that U(V ) = U1(X, Z) + U2(Y , Z), precisely the condition in proposition 22.4. The
desired CA-independence follows.
The converse result asserts that any utility function that satisﬁes the CA-independence prop-
erties associated with the network can be factorized over the network’s cliques. It is analogous
to theorem 3.1 for Bayesian networks and theorem 4.2 for Markov networks.
Theorem 22.4
Let (≺, U) be a corresponding pair of a preference function and a utility function. If H is a
CAI-map for ≺, then U factorizes according to H.
Although it is possible to prove this result directly, it also follows from the analogous result for
probability distributions (the Hammersley-Cliﬀord theorem — theorem 4.2). The basic idea is to
Hammersley-
Cliﬀord
theorem
construct a probability distribution by exponentiating U and then show that CA-independence
properties for U imply corresponding probabilistic conditional independence properties for P:
Lemma 22.1
Let U be a utility function, and deﬁne P(V ) ∝exp(U(V )). For a disjoint partition X, Y , Z
of V , we have that X and Y are CA-independent given Z in U if and only if X and Y are
conditionally independent given Z in P.
The proof is left as an exercise (exercise 22.3).
Based on this correspondence, many of the results and algorithms of chapter 4 now apply
without change to utility functions. In particular, the proof of theorem 22.4 follows immediately
(see exercise 22.4).
As for probabilistic models, we can consider the task of constructing a graphical model that
reﬂects the independencies that hold for a utility function.
Speciﬁcally, we deﬁne H to be
a minimal CAI-map if it is a CAI-map from which no further edges can be removed without
minimal CAI-map
rendering it not a CAI-map. Our goal is the construction of an undirected graph which is a
minimal CAI-map for a utility function U.
We addressed exactly the same problem in the
context of probability functions in section 4.3.3 and provided two algorithms. One was based on
checking pairwise independencies of the form (X ⊥Y | X −{X, Y }). The other was based
on checking local (Markov blanket) independencies of the form (X ⊥X −{X} −U | U).
Importantly, both of these types of independencies involve a disjoint and exhaustive partition
of the set of variables into three subsets. Thus, we can apply these procedures without change
using CA-independencies.
Because of the equivalence of lemma 22.1, and because P ∝exp(U) is a positive distribution,
all of the results in section 4.3.3 hold without change. In particular, we can show that either
of the two procedures described produces the unique minimal CAI-map for U. Indeed, we can
prove an even stronger result: The unique minimal CAI-map H for U is a perfect CAI-map for
perfect CAI-map
U, in the sense that any CA-independence that holds for U is implied by separation in H:
Theorem 22.5
Let H be any minimal CAI-map for U, and let X, Y , Z be a disjoint partition of V . Then if X
is CA-independent of Y given Z in U, then X is separated from Y by Z in H.
The proof is left as an exercise (exercise 22.5).
Note that this result is a strong completeness property, allowing us to read any CA-independence
completeness

1078
Chapter 22. Utilities and Decisions
that holds in the utility function from a graph. One might wonder why a similar result was so elu-
sive for probability distributions. The reason is not that utility functions are better expressed as
graphical models than are probability distributions. Rather, the language of CA-independencies
is substantially more limited than that of conditional independencies in the probabilistic case:
For probability distributions, we can evaluate any statement of the form “X is independent
of Y given Z,” whereas for utility functions, the corresponding (CA-independence) statement
is well deﬁned only when X, Y , Z form a disjoint partition of V . In other words, although
any CA-independence statement that holds in the utility function can be read from the graph,
the set of such statements is signiﬁcantly more restricted. In fact, a similar weak completeness
statement can also be shown for probability distributions.
22.4.2.3
Generalized Additive Independence
Because of its limited expressivity, the notion of conditional additive independence allows us
only to make fairly coarse assertions regarding independence — independence of two subsets of
variables given all of the rest. As a consequence, the associated factorization is also quite coarse.
In particular, we can only use CA-independencies to derive a factorization of the utility function
over the maximal cliques in the Markov network. As was the case in probabilistic models (see
section 4.4.1.1), this type of factorization can obscure the ﬁner-grained structure in the function,
and its parameterization may be exponentially larger.
In this section, we present the most general additive decomposition: the decomposition of
equation (22.8), but with arbitrarily overlapping subsets. This type of decomposition is the utility
analogue to a Gibbs distribution (deﬁnition 4.3), with the factors here combining additively rather
than multiplicatively.
Once again, we can provide an independence-based formulation for this decomposition:
Deﬁnition 22.12
Let Z1, . . . , Zk be (not necessarily disjoint) subsets of V . We say that Z1, . . . , Zk are generalized
GA-independence
additively independent (GA-independent) in ≺if, for any lotteries π1, π2 that have the same
marginals on all Zi, we have that π1 and π2 are indiﬀerent under ≺.
This deﬁnition is identical to that of additive independence (deﬁnition 22.8), with the exception
that the subsets Z1, . . . , Zk are not necessarily mutually exclusive nor exhaustive. Thus, this
deﬁnition allows us to consider cases where our preferences between two distributions depend
only on some arbitrary set of marginals. It is also not hard to show that GA-independence
subsumes CA-independence (see exercise 22.6).
Satisfyingly, a factorization theorem analogous to theorem 22.2 holds for GA-independence:
Theorem 22.6
Let Z1, . . . , Zk be (not necessarily disjoint) subsets of V , and let ≺, U be a corresponding pair of
a preference ordering and a utility function. Then Z1, . . . , Zk are GA-independent in ≺if and
only if U can be written as:
U(V ) =
k
X
i=1
Ui(Zi).
(22.10)
Thus, the set of possible factorizations associated with GA-independence strictly subsumes
the set of factorizations associated with CA-independence. For example, using GA-independence,

22.4. Utilities of Complex Outcomes
1079
we can obtain a factorization U(X, Y, Z) = U1(X, Y ) + U2(Y, Z) + U3(X, Z). The Markov
network associated with this factorization is a full clique over X, Y, Z, and therefore no CA-
independencies hold for this utility function. Overall, GA-independence provides a rich and
natural language for encoding complex utility functions (see, for example, box 22.A).
Box 22.A — Case Study: Prenatal Diagnosis. An important problem involving utilities arises in
the domain of prenatal diagnosis, where the goal is to detect chromosomal abnormalities present
prenatal
diagnosis
in a fetus in the early stages of the pregnancy. There are several tests available to diagnose these
diseases. These tests have diﬀerent rates of false negatives and false positives, costs, and health
risks.
The task is to decide which tests to conduct on a particular patient.
This task is quite
diﬃcult. The patient’s risk for having a child with a serious disease depends on the mother’s age,
child’s sex and race, and the family history.
Some tests are not very accurate; others carry a
signiﬁcant risk of inducing miscarriages. Both a miscarriage (spontaneous abortion or SAB) and an
elective termination of the pregnancy (induced abortion or IAB) can aﬀect the woman’s chances of
conceiving again.
Box 23.A describes a decision-making system called PANDA (Norman et al. 1998) for assisting
the parents in deciding on a course of action for prenatal testing. The PANDA system requires
that we have a utility model for the diﬀerent outcomes that can arise as part of this process.
Note that, unlike for probabilistic models, we cannot simply construct a single utility model that
applies to all patients. Diﬀerent patients will typically have very diﬀerent preferences regarding
these outcomes, and certainly regarding lotteries over them. Interestingly, the standard protocol
(and the one followed by many health insurance companies), which recommends prenatal diagnosis
(under normal circumstances) only for women over the age of thirty-ﬁve, was selected so that the
risk (probability) of miscarriage is equal to that of having a Down syndrome baby. Thus, this
recommendation essentially assumes not only that all women have the same utility function, but
also that they have equal utility for these two events.
The outcomes in this domain have many attributes, such as the inconvenience and expense of
fairly invasive testing, the disease status of the fetus, the possibility of test-induced miscarriage,
knowledge of the status of the fetus, and future successful pregnancy. Speciﬁcally, the utility could
be viewed as a function of ﬁve attributes: pregnancy loss L, with domain {no loss, miscarriage,
elective termination}; Down status D of the fetus, with domain: {normal, Down}; mother’s knowledge
K, with domain {none, accurate, inaccurate}; future pregnancy F, with domain {yes, no}; type of
test T with domain {none, CVS, amnio}. An outcome is an assignment of values to all the attributes.
For example, ⟨no loss, normal, none, yes, none⟩is one possible outcomes. It represents the situation
in which the fetus is not aﬀected by Down syndrome, the patient decides not to take any tests (as
a consequence, she is unaware of the Down status of the fetus until the end of the pregnancy), the
pregnancy results in normal birth, and there is a future pregnancy. Another outcome, ⟨miscarriage,
normal, accurate, no, CVS⟩represents a situation where the patient decides to undergo the CVS
test. The test result correctly asserts that the fetus is not aﬀected by Down syndrome. However,
a miscarriage occurs as a side eﬀect of the procedure, and there is no future pregnancy. Our
decision-making situation involves comparing lotteries involving complex (and emotionally diﬃcult)
outcomes such as these.
In this domain, we have three ternary attributes and two binary ones, so the total number of
outcomes is 108. Even if we remove outcomes that have probability zero (or are very unlikely), a

1080
Chapter 22. Utilities and Decisions
Knowledge
Future
pregnancy
Loss of 
fetus
Down
syndrome
Testing
Figure 22.A.1 — Typical utility function decomposition for prenatal diagnosis
large number of outcomes remain. In order to perform utility elicitation, we must assign a numerical
utility to each one. A standard utility elicitation process such as standard gamble involves a fairly
large number of comparisons for each outcome. Such a process is clearly infeasible in this case.
However, in this domain, many of the utility functions elicited from patients decompose additively
in natural ways. For example, as shown by Chajewska and Koller (2000), many patients have
utility functions where invasive testing (T) and knowledge (K) are additively independent of other
attributes; pregnancy loss (L) is correlated both with Down syndrome (D) and with future pregnancy
(F), but there is no direct interaction between Down syndrome and future pregnancy. Thus, for
example, one common decomposition is U1(T) + U2(K) + U3(D, L) + U4(L, F), as encoded in
the Markov network of ﬁgure 22.A.1.
Box 22.B — Case Study: Utility Elicitation in Medical Diagnosis. In box 3.D we described the
Pathﬁnder system, designed to assist a pathologist in diagnosing lymph-node diseases. The Pathﬁnder
system was purely probabilistic — it produced only a probability distribution over possible diag-
noses. However, the performance evaluation of the Pathﬁnder system accounted for the implications
of a correct or incorrect diagnosis on the patient’s utility. For example, if the patient has a viral
infection and is diagnosed as having a bacterial infection, the consequences are not so severe: the
patient may take antibiotics unnecessarily for a few days or weeks. On the other hand, if the patient
has Hodgkin’s disease and is incorrectly diagnosed as having a viral infection, the consequences —
such as delaying chemotherapy — may be lethal. Thus, to evaluate more accurately the implications
of Pathﬁnder’s performance, a utility model was constructed that assigned, for every pair of diseases
d, d′, a utility value ud,d′, which denotes the patient’s utility for having the disease d and being
diagnosed with disease d′.
We might be tempted to evaluate the system’s performance on a particular case by computing
ud∗,d −ud∗,d∗, where d∗is the true diagnosis, and d is the most likely diagnosis produced by the
system. However, this metric ignores the important distinction between the quality of the decision
and the quality of the outcome: A bad decision is one that is not optimal relative to the agent’s
state of knowledge, whereas a bad outcome can arise simply because the agent is unlucky. In this
case, the set of observations may suggest (even to the most knowledgeable expert) that one disease
is the most likely, even when another is actually the case. Thus, a better metric is the inferential
inferential loss
loss — the diﬀerence in the expected utility between the gold standard distribution produced by
an expert and the distribution produced by the system, given exactly the same set of observations.

22.5. Summary
1081
Estimating the utility values ud,d′ is a nontrivial task. One complication arises from the fact
that this situation involves outcomes whose consequences are fairly mild, and others that involve
a signiﬁcant risk of morbidity or mortality. Putting these on a single scale is quite challenging.
The approach taken in Pathﬁnder is to convert all utilities to the micromort scale — a one-in-a-
million chance of death. For severe outcomes (such as Hodgkins disease), one can ask the patient
what probability of immediate, painless death he would be willing to accept in order to avoid both
the disease d and the (possibly incorrect) diagnosis d′. For mild outcomes, where the micromort
equivalent may be too low to evaluate reliably, utilities were elicited in terms of monetary equivalents
— for example, how much the patient would be willing to pay to avoid taking antibiotics for two
weeks. At this end of the spectrum, the “conversion” between micromorts and dollars is fairly linear
(see section 22.3.2), and so the resulting dollar amounts can be converted into micromorts, putting
these utilities on the same scale as that of severe outcomes.
The number of distinct utilities that need to be elicited even in this simple setting is impractically
large: with sixty diseases, the number of utilities is 602 = 3, 600.
Even aggregating diseases
that have similar treatments and prognoses, the number of utilities is 362 = 1, 296. However,
utility independence can be used to decompose the outcomes into independent factors, such as the
disutility of a disease d when correctly treated, the disutility of delaying the appropriate treatment,
and the disutility of undergoing an unnecessary treatment. This decomposition reduced the number
of assessments by 80 percent, allowing the entire process of utility assessment to be performed in
approximately sixty hours.
The Pathﬁnder IV system (based on a full Bayesian network) resulted in a mean inferential loss
of 16 micromorts, as compared to 340 micromorts for the Pathﬁnder III system (based on a naive
Bayes model). At a rate of $20/micromort (the rate elicited in the 1980s), the improvement in the
expected utility of Pathﬁnder IV over Pathﬁnder III is equivalent to around $6,000 per case.
22.5
Summary
In this chapter, we discussed the use of probabilistic models within the context of a decision
task.
The key new element one needs to introduce in this context is some representation
of the preferences of the agent (the decision maker). Under certain assumptions about these
preferences, one can show that the agent, whether implicitly or explicitly, must be following the
principle of maximum expected utility, relative to some utility function. It is important to note
that the assumptions required for this result are controversial, and that they do not necessarily
hold for human decision makers.
Indeed, there are many examples where human decision
makers do not obey the principle of maximum expected utility. Much work has been devoted to
developing other precepts of rational decision making that better match human decision making.
Most of this study has taken place in ﬁelds such as economics, psychology, or philosophy. Much
work remains to be done on evaluating the usefulness of these ideas in the context of automated
decision-making systems and on developing computational methods that allow them to be used
in complex scenarios such as the ones we discuss in this book.
In general, an agent’s utility function can involve multiple attributes of the state of the world.
In principle, the complexity of the utility function representation grows exponentially with the
number of attributes on which the utility function depends. Several representations have been

1082
Chapter 22. Utilities and Decisions
developed that assume some structure in the utility function and exploit it for reducing the
number of parameters required to represent the utility function.
Perhaps the biggest challenge in this setting is that of acquiring an agent’s utility functions.
Unlike the case of probability distributions, where an expert can generally provide a single model
that applies to an entire population, an agent’s utility function is often highly personal and even
idiosyncratic. Moreover, the introspection required for a user to understand her preferences and
quantify them is a time-consuming and even distressing process. Thus, there is signiﬁcant value
in developing methods that speed up the process of utility elicitation.
A key step in that direction is in learning better models of utility functions. Here, we can
attempt to learn a model for the structure of the utility function (for example, its decomposition),
or for the parameters that characterize it. We can also attempt to learn richer models that capture
the dependence of the utility function on the user’s background and perhaps even its evolution
over time. Another useful direction is to try to learn aspects of the agent’s utility function by
observing previous decisions that he made.
These models can be viewed as narrowing down the space of possibilities for the agent’s
utility function, allowing it to be elicited using fewer questions. One can also try to develop
algorithms that intelligently reduce the number of utility elicitation questions that one needs to
ask in order to make good decisions. It may be hoped that a combination of these techniques
will make utility-based decision making a usable component in our toolbox.
22.6
Relevant Literature
The principle of maximum expected utility dates back at least to the seventeenth century, where
it played a role in the famous Pascal’s wager (Arnauld and Nicole 1662), a decision-theoretic
Pascal’s wager
analysis concerning the existence of God. Bernoulli (1738), analyzing the St. Petersburg paradox,
made the distinction between monetary rewards and utilities. Bentham (1789) ﬁrst proposed the
idea that all outcomes should be reduced to numerical utilities.
Ramsey (1931) was the ﬁrst to provide a formal derivation of numerical utilities from prefer-
ences. The axiomatic derivation described in this chapter is due to von Neumann and Morgen-
stern (1944). A Bayesian approach to decision theory was developed by Ramsey (1931); de Finetti
(1937); Good (1950); Savage (1954).
Ramsey and Savage both deﬁned axioms that provide a
simultaneous justiﬁcation for both probabilities and utilities, in contrast to the axioms of von
Neumann and Morgenstern, that take probabilities as given. These axioms motivate the Bayesian
approach to probabilities in a decision-theoretic setting. The book by Kreps (1988) provides a
good review of the topic.
The principle of maximum expected utility has also been the topic of signiﬁcant criticism,
both on normative and descriptive grounds. For example, Kahneman and Tversky, in a long series
of papers, demonstrate that human behavior is often inconsistent with the principles of rational
decision making under uncertainty for any utility function (see, for example, Kahneman, Slovic,
and Tversky 1982). Among other things, Tversky and Kahneman show that people commonly use
heuristics in their probability assessments that simplify the problem but often lead to serious
errors. Speciﬁcally, they often pay disproportionate attention to low-probability events and treat
high-probability events as though they were less likely than they actually are.
Motivated both by normative limitations in the MEU principle and by apparent inconsistencies

22.6. Relevant Literature
1083
between the MEU principle and human behavior, several researchers have proposed alternative
criteria for optimal decision making.
For example, Savage (1951) proposed the minimax risk
minimax risk
criterion, which asserts that we should associate with each outcome not only some utility value
but also a regret value. This approach was later reﬁned by Loomes and Sugden (1982) and Bell
(1982), who show how regret theory can be used to explain such apparently irrational behaviors
as gambling on negative-expected-value lotteries or buying costly insurance.
A discussion of utility functions exhibited by human decision makers can be found in von Win-
terfeldt and Edwards (1986). Howard (1977) provides an extensive discussion of attitudes toward
risk and deﬁnes the notions of risk-averse, risk-seeking, and risk-neutral behaviors. Howard (1989)
proposes the notion of micromorts for eliciting utilities regarding human life. The Pathﬁnder
system is one of the ﬁrst automated medical diagnosis systems to use a carefully constructed
utility function; a description of the system, and of the process used to construct the utility
function, can be found in Heckerman (1990) and Heckerman, Horvitz, and Nathwani (1992).
The basic framework of multiattribute utility theory is presented in detail in the seminal book
of Keeney and Raiﬀa (1976). These ideas were introduced into the AI literature by Wellman
(1985). The notion of generalized additive independence (GAI), under the name interdependent
value additivity, was proposed by Fishburn (1967, 1970), who also provided the conditions under
which a GAI model provides an accurate representation of a utility function. The idea of using
a graphical model to represent utility decomposition properties was introduced by Wellman and
Doyle (1992). The rigorous development was performed by Bacchus and Grove (1995), who also
proposed the idea of GAI-networks. These ideas were subsequently extended in various ways by
La Mura and Shoham (1999) and Boutilier, Bacchus, and Brafman (2001).
Much work has been done on the problem of utility elicitation. The standard gamble method
was ﬁrst proposed by von Neumann and Morgenstern (1947), based directly on their axioms
for utility theory. Time trade-oﬀwas proposed by Torrance, Thomas, and Sackett (1972). The
visual analog scale dates back at least to the 1970s (Patrick et al. 1973); see Drummond et al.
(1997) for a detailed presentation. Chajewska (2002) reviews these diﬀerent methods and some
of the documented diﬃculties with them. She also provides a fairly recent overview of diﬀerent
approaches to utility elicitation.
There has been some work on eliciting the structure of decomposed utility functions from
users (Keeney and Raiﬀa 1976; Anderson 1974, 1976). However, most often a simple (for exam-
ple, fully additive) structure is selected, and the parameters are estimated using least-squares
regression from elicited utilities of full outcomes. Chajewska and Koller (2000) show how the
problem of inferring the decomposition of a utility function can be viewed as a Bayesian model
selection problem and solved using techniques along the lines of chapter 18. Their work is based
on the idea of explicitly representing an explicit probabilistic model over utility parameters, as
proposed by Jimison et al. (1992). The prenatal diagnosis example is taken from Chajewska and
Koller (2000), based on data from Kuppermann et al. (1997).
Several authors (for example, Heckerman and Jimison 1989; Poh and Horvitz 2003; Ha and
Haddawy 1997, 1999; Chajewska et al. 2000; Boutilier 2002; Braziunas and Boutilier 2005) have
proposed that model reﬁnement, including reﬁnement of utility assessments, should be viewed
in terms of optimizing expected value of information (see section 23.7). In general, it can be
shown that a full utility function need not be elicited to make optimal decisions, and that
close-to-optimal decisions can be made after a small number of utility elicitation queries.

1084
Chapter 22. Utilities and Decisions
22.7
Exercises
Exercise 22.1
Complete the proof of theorem 22.1. In particular, let U(s) := p, as deﬁned in equation (22.7), be our utility
assignment for outcomes. Show that, if we use the MEU principle for selecting between two lotteries, the
resulting preference over lotteries is equivalent to ≺. (Hint: Do not forget to address the case of compound
lotteries.)
Exercise 22.2
Prove proposition 22.4.
Exercise 22.3
Prove lemma 22.1.
Exercise 22.4
Complete the proof of theorem 22.4.
Exercise 22.5⋆
Prove theorem 22.5. (Hint: Use exercise 4.11.)
Exercise 22.6⋆
Prove the following result without using the factorization properties of U. Let X, Y , Z be a disjoint
partition of V .
Then X, Z and Y , Z are GA-independent in ≺if and only if X and Y are CA-
independent given Z in ≺.
This result shows that CA-independence and GA-independence are equivalent over the scope of independence
assertions to which CA-independence applies (those involving disjoint partitions of V ).
Exercise 22.7
Consider the problem of computing the optimal action for an agent whose utility function we are uncertain
about.
In particular, assume that, rather than a known utility function over outcomes O, we have a
probability density function P(U), which assigns a density for each possible utility function U : O 7→IR.
a. What is the expected utility for a given action a, taking an expectation both over the outcomes of πa,
and over the possible utility functions that the agent might have?
b. Use your answer to provide an eﬃcient computation of the optimal action for the agent.
Exercise 22.8⋆
As we discussed, diﬀerent people have diﬀerent utility functions. Consider the problem of learning a
probability distribution over the utility functions found in a population. Assume that we have a set of
samples U[1], . . . , U[M] of users from a population, where for each user m we have elicited a utility
function U[m] : Val(V ) 7→IR.
a. Assume that we want to model our utility function as in equation (22.10). We want to use the same
factorization for all users, but where diﬀerent users have diﬀerent subutility functions; that is, Ui[m]
and Ui[m′] are not the same. Moreover, the elicited values U(v)[m] are noisy, so that they may not
decompose exactly as in equation (22.10). We can model the actual elicited values for a given user as
the sum in equation (22.10) plus Gaussian noise.
Formulate the distribution over the utility functions in the population as a linear Gaussian graphical
model. Using the techniques we learned earlier in this book, provide a learning algorithm for the
parameters in this model.
b. Now, assume that we allow diﬀerent users in the population to have one of several diﬀerent factoriza-
tions of their utility functions. Show how you can extend your graphical model and learning algorithm
accordingly. Show how your model allows you to infer which factorization a user is likely to have.

23
Structured Decision Problems
In the previous chapter, we described the basic principle of decision making under uncertainty
— maximizing expected utility. However, our deﬁnition for a decision-making problem was
completely abstract; it deﬁned a decision problem in terms of a set of abstract states and a set
of abstract actions. Yet, our overarching theme in this book has been the observation that the
world is structured, and that we can obtain both representational and computational eﬃciency
by exploiting this structure. In this chapter, we discuss structured representations for decision-
making problems and algorithms that exploit this structure when addressing the computational
task of ﬁnding the decision that maximizes the expected utility.
We begin by describing decision trees — a simple yet intuitive representation that describes
a decision-making situation in terms of the scenarios that the decision maker might encounter.
This representation, unfortunately, scales up only to fairly small decision tasks; still, it provides
a useful basis for much of the later development.
We describe inﬂuence diagrams, which
extend Bayesian networks by introducing decisions and utilities. We then discuss algorithmic
techniques for solving and simplifying inﬂuence diagrams. Finally, we discuss the concept of
value of information, which is very naturally encoded within the inﬂuence diagram framework.
23.1
Decision Trees
23.1.1
Representation
A decision tree is a representation of the diﬀerent scenarios that might be encountered by the
decision maker in the context of a particular decision problem. A decision tree has two types of
internal nodes (denoted t-nodes to distinguish them from nodes in a graphical model) — one
set encoding decision points of the agent, and the other set encoding decisions of nature. The
outgoing edges at an agent’s t-node correspond to diﬀerent decisions that the agent might make.
The outgoing edges at one of nature’s t-nodes correspond to random choices that are made by
nature. The leaves of the tree are associated with outcomes, and they are annotated with the
agent’s utility for that outcome.
Deﬁnition 23.1
A decision tree T is a rooted tree with a set of internal t-nodes V and leaves VL. The set V is
decision tree
t-node
partitioned into two disjoint sets — agent t-nodes VA and nature t-nodes VN. Each t-node has
some set of choices C[v], associated with its outgoing edges. We let succ(v, c) denote the child of
v reached via the edge labeled with c. Each of nature’s t-nodes v is associated with a probability

1086
Chapter 23. Structured Decision Problems
s1
s2
s0
c0
c1
0
20
0.43
0.23
0.36
0.34
5
–7
M
S
C
0
8.78
20
0.73
0.05
0.36
0.22
5
–7
M
F
0
2
2
3.26
3.26
3.29
8.78
3.29
–3.01
0
20
0.5
0.2
0.41
0.24
0.35
5
–7
M
0
20
0.42
0.21
0.36
0.37
5
–7
M
f 0
f 1
(b)
0.3
f 0
f 1
F
0
m0
m2
(a)
20
0.5
0.2
0.36
0.3
5
–7
M
m1
F
F
F
Figure 23.1
Decision trees for the Entrepreneur example. (a) one-stage scenario; (b) two-stage scenario,
with the solution (optimal strategy) denoted using thicker edges.
distribution Pv over C[v]. Each leaf v ∈VL in the tree is annotated with a numerical value U(v)
corresponding to the agent’s utility for reaching that leaf.
Most simply, in our basic decision-making scenario of deﬁnition 22.2, a lottery ℓinduces a
two-layer tree. The root is an agent t-node v, and it has an outgoing edge for each possible
action a ∈A, leading to some child succ(v, a). Each node succ(v, a) is a nature t-node; its
children are leaves in the tree, with one leaf for each outcome in O for which ℓa(o) > 0; the
corresponding edge is labeled with the probability ℓa(o). The leaf associated with some outcome
o is annotated with U(o). Most simply, in our basic Entrepreneur scenario of example 22.3, the
corresponding decision tree would be as shown in ﬁgure 23.1a. Note that if the agent decides
not to found the company, there is no dependence of the outcome on the market demand, and
the agent simply gets a utility of 0.

The decision-tree representation allows us to encode decision scenarios in a way that
reveals much more of their internal structure than the abstract setting of outcomes and
utilities. In particular, it allows us to encode explicitly sequential decision settings, where
the agent makes several decisions; it also allows us to encode information that is available
to the agent at the time a decision must be made.
Example 23.1
Consider an extension of our basic Entrepreneur example where the entrepreneur has the opportu-
nity to conduct a survey on the demand for widgets before deciding whether to found the company.
Thus, the agent now has a sequence of two decisions: the ﬁrst is whether to conduct the survey, and
the second is whether to found the company. If the agent conducts the survey, he obtains informa-

23.1. Decision Trees
1087
tion about its outcome, which can be one of three values: a negative reaction, s0, indicating almost
no hope of widget sales; a neutral reaction, s1, indicating some hope of sales; and an enthusiastic
reaction, s2, indicating a lot of potential demand. The probability distribution over the market
demand is diﬀerent for the diﬀerent outcomes of the survey. If the agent conducts the survey, his
decision on whether to found the company can depend on the outcome.
The decision tree is shown in ﬁgure 23.1b. At the root, the agent decides whether to conduct the
survey (c1) or not (c0). If he does not conduct the survey, the next t-node is another decision by
the agent, where he decides whether to found the company (f 1) or not (f 0). If the agent decides
to found the company, nature decides on the market demand for widgets, which determines the
ﬁnal outcome. The situation if the agent decides to conduct the survey is more complex. Nature
then probabilistically chooses the value of the survey. For each choice, the agent has a t-node where
he gets to decide whether he founds the company or not. If he does, nature gets to decide on the
distribution of market demand for widgets, which is diﬀerent for diﬀerent outcomes of the survey.
We can encode the agent’s overall behavior in a decision problem encoded as a decision tree
as a strategy. There are several possible deﬁnitions of a strategy. One that is simple and suitable
strategy
for our purposes is a mapping from agent t-nodes to possible choices at that t-node.
Deﬁnition 23.2
A decision-tree strategy σ speciﬁes, for each v ∈VA, one of the choices labeling its outgoing
decision-tree
strategy
edges.
For example, in the decision tree of ﬁgure 23.1b, a strategy has to designate an action for
the agent t-node, labeled C, and the four agent t-nodes, labeled F. One possible strategy is
illustrated by the thick lines in the ﬁgures.
Decision trees provide a structured representation for complex decision problems, potentially
involving multiple decisions, taken in sequence, and interleaved with choices of nature. However,
they are still instances of the abstract framework deﬁned in deﬁnition 22.2. Speciﬁcally, the
outcomes are the leaves in the tree, each of which is annotated with a utility; the set of agent
actions is the set of all strategies; and the probabilistic outcome model is the distribution over
leaves induced by nature’s random choices given a strategy (action) for the agent.
23.1.2
Backward Induction Algorithm
As in the abstract decision-making setting, our goal is to select the strategy that maximizes
the agent’s expected utility. This computational task, for the decision-tree representation, can
be solved using a straightforward tree-traversal algorithm. This approach is an instance of an
approach called backward induction in the game-theoretic and economic literature, and the
backward
induction
Expectimax algorithm in the artiﬁcial intelligence literature.
Expectimax
The algorithm proceeds from the leaves of the tree upward, computing the maximum expected
utility MEUv achievable by the agent at each t-node v in the tree — his expected utility if he
plays the optimal strategy from that point on. At a leaf v, MEUv is simply the utility U(v)
associated with that leaf’s outcome. Now, consider an internal t-node v for whose children
we have already computed the MEU. If v belongs to nature, the expected utility accruing to
the agent if v is reached is simply the weighted average of the expected utilities at each of
v’s children, where the weighted average is taken relative to the distribution deﬁned by nature
over v’s children. If v belongs to the agent, the agent has the ability to select the action at v.

1088
Chapter 23. Structured Decision Problems
Algorithm 23.1 Finding the MEU strategy in a decision tree
Procedure MEU-for-Decision-Trees (
T
// Decision tree
)
1
L ←Leaves(T )
2
for each node v ∈L
3
Remove v from L
4
Add v’s parents to L
5
if v is a leaf then
6
MEUv ←U(v)
7
else if v belongs to nature then
8
MEUv ←P
c∈C[v] Pv(c)MEUsucc(v,c)
9
else
// v belongs to the Agent
10
σ(v) ←arg maxc∈C[v] MEUsucc(v,c)
11
MEUv ←MEUsucc(v,succ(v,))
12
return (σ)
The optimal action for the agent is the one leading to the child whose MEU is largest, and the
MEU accruing to the agent is the MEU associated with that child. The algorithm is shown in
algorithm 23.1.
23.2
Inﬂuence Diagrams

The decision-tree representation is a signiﬁcant improvement over representing the prob-
lem as a set of abstract outcomes; however, much of the structure of the problem is still
not made explicit. For example, in our simple Entrepreneur scenario, the agent’s utility if he
founds the company depends only on the market demand M, and not on the results of the
survey S. In the decision tree, however, the utility values appear in four separate subtrees: one
for each value of the S variable, and one for the subtree where the survey is not performed. An
examination of the utility values shows that they are, indeed, identical, but this is not apparent
from the structure of the tree.
The tree also loses a subtler structure, which cannot be easily discerned by an examination
of the parameters. The tree contains four nodes that encode a probability distribution over
the values of the market demand M. These four distributions are diﬀerent. We can presume
that neither the survey nor the agent’s decision has an eﬀect on the market demand itself.
The reason for the change in the distribution presumably arises from the eﬀect of conditioning
the distribution on diﬀerent observations (or no observation) on the survey variable S.
In
other words, these distributions represent P(M | s0), P(M | s1), P(M | s2), and P(M) (in
the branch where the survey was not performed). These interactions between these diﬀerent
parameters are obscured by the decision-tree representation.

23.2. Inﬂuence Diagrams
1089
Market
Found
Value
Figure 23.2
Inﬂuence diagram IF for the basic Entrepreneur example
23.2.1
Basic Representation
An alternative representation is the inﬂuence diagram (sometimes also called a decision network),
inﬂuence
diagram
a natural extension of the Bayesian network framework. It encodes the decision scenario via
a set of variables, each of which takes on values in some space. Some of the variables are
random variables, as we have seen so far, and their values are selected by nature using some
probabilistic model. Others are under the control of the agent, and their value reﬂects a choice
made by him. Finally, we also have numerically valued variables encoding the agent’s utility.
This type of model can be encoded graphically, using a directed acyclic graph containing three
types of nodes — corresponding to chance variables, decision variables, and utility variables.
These diﬀerent node types are represented as ovals, rectangles, and diamonds, respectively. An
inﬂuence diagram I is a directed acyclic graph over these nodes, such that the utility nodes
have no children.
Example 23.2
The inﬂuence diagram IF for our entrepreneur example is shown in ﬁgure 23.2. The utility variable
VE encodes the utility of the entrepreneur’s earnings, which are a deterministic function of the
utility variable’s parents. This function speciﬁes the agent’s real-valued utility for each combination
of the parent nodes; in this case, the utility is a function from Val(M) × Val(F) to IR. We can
represent this function as a table:
m0
m1
m2
f 1
−7
5
20
f 0
0
0
0,
where f 1 represents the decision to found the company and f 0 the decision not to do so. The CPD
for the M node is:
m0
m1
m2
0.5
0.3
0.2.
More formally, in an inﬂuence diagram, the world in which the agent acts is represented by
the set X of chance variables, and by a set D of decision variables. Chance variables are those
chance variable
decision variable
whose values are chosen by nature. The decision variables are variables whose values the agent
gets to choose. Each variable V ∈X ∪D has a ﬁnite domain Val(V ) of possible values. We can
place this representation within the context of the abstract framework of deﬁnition 22.2: The
possible actions A are all of the possible assignments Val(D); the possible outcomes are all of

1090
Chapter 23. Structured Decision Problems
the joint assignments in Val(X ∪D). Thus, this framework provides a factored representation
of both the action and the outcome space.
We can also decompose the agent’s utility function. A standard decomposition (see discussion
in section 22.4) is as a linear sum of terms, each of which represents a certain component of the
agent’s utility. More precisely, we have a set of utility variables U, which take on real numbers
utility variable
as values. The agent’s ﬁnal utility is the sum of the value of V for all V ∈U.
Let Z be the set of all variables in the network — chance, decision, and utility variables. We
expand the notion of outcome to encompass a full assignment to Z, which we denote as ζ.
outcome
The parents of a chance variable X represent, as usual, the direct inﬂuences on the choice
of X’s value. Note that the parents of X can be both other chance variables as well as decision
variables, but they cannot be utility variables, since we assumed that utility nodes have no
children. Each chance node X is associated with a CPD, which represents P(X | PaX).
The parents of a utility variable V represent the set of variables on which the utility V
depends. The value of a utility variable V is a deterministic function of the values of PaV ;
we use V (w) to denote the value that node V takes when PaV = w. Note that, as for any
deterministic function, we can also view V as deﬁning a CPD, where for each parent assignment,
some value gets probability 1. When convenient, we will abuse notation and interpret a utility
node as deﬁning a factor.
Summarizing, we have the following deﬁnition:
Deﬁnition 23.3
An inﬂuence diagram I over Z is a directed acyclic graph whose nodes correspond to Z, and
inﬂuence
diagram
where nodes corresponding to utility variables have no children. Each chance variable X ∈X is
associated with a CPD P(X | PaX). Each utility variable V ∈U is associated with a deterministic
function V (PaV ).
23.2.2
Decision Rules
So far, we have not discussed the semantics of the decision node.
For a decision variable
D ∈D, PaD is the set of variables whose values the agent knows when he chooses a value for
D. The edges incoming into a decision variable are often called information edges.
information edge
Example 23.3
Let us return to the setting of example 23.1. Here, we have the chance variable M that represents
the market demand, and the chance variable S that represents the results of the survey. The variable
S has the values s0, s1, s2, and an additional value s⊥, denoting that the survey was not taken.
This additional value is needed in this case, because we allow the agent’s decision to depend on
the value of S, and therefore we need to allow some value for this variable when the survey is not
taken. The variable S has two parents, C and M. We have that P(s⊥| c0, m) = 1, for any value
of m. In the case c1, the probabilities over values of S are:
s0
s1
s2
m0
0.6
0.3
0.1
m1
0.3
0.4
0.3
m2
0.1
0.4
0.5
The entrepreneur knows the result of the survey before making his decision whether to found the
company. Thus, there is an edge between S and his decision F. We also assume that conducting

23.2. Inﬂuence Diagrams
1091
Market
Survey
Found
Test
Value
Cost
Figure 23.3
Inﬂuence diagram IF,C for Entrepreneur example with market survey
the survey has some cost, so that we have an additional utility node VS, with the parent C; VS
takes on the value −1 if C = c1 and 0 otherwise. The resulting inﬂuence diagram IF,C is shown
in ﬁgure 23.3.
The inﬂuence diagram representation captures the causal structure of the problem and its
parameterization in a much more natural way than the decision tree. It is clear in the inﬂuence
diagram that S depends on M, that M is parameterized via a simple (unconditional) prior
distribution, and so on.
The choice that the agent makes for a decision variable D can be contingent only on the
values of its parents. More precisely, in any trajectory through the decision scenario, the agent
will encounter D in some particular information states, where each information state is an
information state
assignment of values to PaD. An agent’s strategy for D must tell the agent how to act at D, at
each of these information states.
Example 23.4
In example 23.3, for instance, the agent’s strategy must tell him whether to found the company or
not in each possible scenario he may encounter; the agent’s information state at this decision is
deﬁned by the possible values of the decision variable C and the survey variable S. The agent must
therefore decide whether to found the company in four diﬀerent information states: if he chose not
to conduct the survey, and in each of the three diﬀerent possible outcomes of the survey.
A decision rule tells the agent how to act in each possible information state. Thus, the agent
is choosing a local conditional model for the decision variable D. In eﬀect, the agent has the
ability to choose a CPD for D.
Deﬁnition 23.4
A decision rule δD for a decision variable D is a conditional probability P(D | PaD) — a
decision rule
function that maps each instantiation paD of PaD to a probability distribution δD over Val(D).
A decision rule is deterministic if each probability distribution δD(D | paD) assigns nonzero
deterministic
decision rule
probability to exactly one value of D. A complete assignment σ of decision rules to every decision
D ∈D is called a complete strategy; we use σD to denote the decision rule at D.
complete strategy

1092
Chapter 23. Structured Decision Problems
Example 23.5
A decision rule for C is simply a distribution over its two values. A decision rule for F must deﬁne,
for every value of C, and for every value s0, s1, s2, s⊥of S, a probability distribution over values
of F. Note, however, that there is a deterministic relationship between C and S, so that many of
the combinations are inconsistent (for example, c1 and s⊥, or c0 and s1). For example, in the case
c1, s1, one possible decision rule for the agent is f 0 with probability 0.7 and f 1 with probability
0.3.
As we will see, in the case of single-agent decision making, one can always choose an optimal
deterministic strategy for the agent. However, it is useful to view a strategy as an assignment
of CPDs to the decision variables. Indeed, in this case, the parents of a decision node have
the same semantics as the parents of a chance node: the agent’s strategy can depend only on
the values of the parent variables. Moreover, randomized decision rules will turn out to be a
useful concept in some of our constructions that follow. In the common case of deterministic
decision rules, which pick a single action d ∈Val(D) for each assignment w ∈Val(PaD),
we sometimes abuse notation and use δD to refer to the decision-rule function, in which case
δD(w) denotes the single action d that has probability 1 given the parent assignment w.
23.2.3
Time and Recall
Unlike a Bayesian network, an inﬂuence diagram has an implicit causal semantics. One assumes
that the agent can intervene at a decision variable D by selecting its value. This intervention
intervention
will aﬀect the values of variables downstream from D. By choosing a decision rule, the agent
determines how he will intervene in the system in diﬀerent situations.
The acyclicity assumption for inﬂuence diagrams, combined with the use of information edges,
ensures that an agent cannot observe a variable that his action aﬀects. Thus, acyclicity implies
that the network respects some basic causal constraints.
In the case of multiple decisions,
we often want to impose additional constraints on the network structure. In many cases, one
assumes that the decisions in the network are all made by a single agent in some sequence
over time; in this case, we have a total ordering ≺on D. An additional assumption that is
often made in this case is that the agent does not forget his previous decisions or information
it once had. This assumption is typically called the perfect recall assumption (or sometime the
no forgetting assumption), formally deﬁned as follows:
Deﬁnition 23.5
An inﬂuence diagram I is said to have a temporal ordering if there is some total ordering ≺over
temporal ordering
D, which is consistent with partial ordering imposed by the edges in I. The inﬂuence diagram I
satisﬁes the perfect recall assumption relative to ≺if, whenever Di ≺Dj, PaDj ⊃(PaDi ∪{Di}).
perfect recall
The edges from PaDi ∪{Di} to Dj are called recall edges.
recall edge
Intuitively, a recall edge is an edge from a variable X (chance or decision) to a decision
variable D whose presence is implied by the perfect recall assumption. In particular, if D′ is
a decision that precedes D in the temporal ordering, then we have recall edges D′ →D and
X →D for X ∈PaD′. To reduce visual clutter, we often omit recall edges in an inﬂuence
diagram when the temporal ordering is known. For example, in ﬁgure 23.3, we omitted the edge
from C to F.
Although the perfect recall assumption appears quite plausible at ﬁrst glance, there are several
arguments against it. First, it is not a suitable model for situations where the “agent” is actually

23.2. Inﬂuence Diagrams
1093
a compound entity, with individual decisions made by diﬀerent “subagents.” For example, our
agent might be a large organization, with diﬀerent members responsible for various decisions.
It is also not suitable for cases where an agent might not have the resources (or the desire) to
remember an entire history of all previous actions and observations.

The perfect recall assumption also has signiﬁcant representational and computational
ramiﬁcations. The size of the decision rule at a decision node is, in general, exponential
in the number of parents of the decision node. In the case of perfect recall, the number of
parents grows with every decision, resulting in a very high-dimensional space of possible
decision rules for decision variables later in the temporal ordering. This blowup makes
computations involving large inﬂuence diagrams with perfect recall intractable in many
cases. The computational burden of perfect recall leads us to consider also inﬂuence diagrams
in which the perfect recall assumption does not hold, also known as limited memory inﬂuence
limited memory
inﬂuence
diagram
diagrams (or LIMIDs). In these networks, all information edges must be represented explicitly,
since perfect recall is no longer universally true. We return to this topic in section 23.6.
23.2.4
Semantics and Optimality Criterion
A choice of a decision rule δD eﬀectively turns D from a decision variable into a chance
variable. Let σD be any partial strategy that speciﬁes a decision rule for the decision variables
D ∈D. We can replace each decision variable in D with the CPD deﬁned by its decision
rule in σ, resulting in an inﬂuence diagram I[σ] whose chance variables are X ∪D and whose
decision variables are D −D. In particular, when σ is a complete strategy, I[σ] is simply
a Bayesian network, which we denote by BI[σ]. This Bayesian network deﬁnes a probability
distribution over possible outcomes ζ.
The agent’s expected utility in this setting is simply:
expected utility
EU[I[σ]] =
X
ζ
PBI[σ](ζ)U(ζ)
(23.1)
where the utility of an outcome is the sum of the individual utility variables in that outcome:
U(ζ) =
X
V ∈U
ζ⟨V ⟩.
The linearity of expectation allows us to simplify equation (23.1) by considering each utility
variable separately, to obtain:
EU[I[σ]]
=
X
V ∈U
IEBI[σ][V ]
=
X
V ∈U
X
v∈Val(V )
PBI[σ](V = v)v.
We often drop the subscript BI[σ] where it is clear from context.
An alternative useful formulation for this expected utility makes explicit the dependence on
the factors parameterizing the network:
EU[I[σ]] =
X
X∪D
" Y
X∈X
P(X | PaX)
!  Y
D∈D
δD
!  
X
i : Vi∈U
Vi
!#
.
(23.2)

1094
Chapter 23. Structured Decision Problems
The expression inside the summation is constructed as a product of three components. The
ﬁrst is a product of all of the CPD factors in the network; the second is a product of all of the
factors corresponding to the decision rules (also viewed as CPDs); and the third is a factor that
captures the agent’s utility function as a sum of the subutility functions Vi. As a whole, the
expression inside the summation is a single factor whose scope is X ∪D. The value of the
entry in the factor corresponding to an assignment o to X ∪D is a product of the probability
of this outcome (using the decision rules speciﬁed by σ) and the utility of this outcome. The
summation over this factor is simply the overall expected utility EU[I[σ]].
Example 23.6
Returning to example 23.3, our outcomes are complete assignments m, c, s, f, us, uf. The agent’s
utility in such an outcome is us + uf. The agent’s expected utility given a strategy σ is
PB(VS = −1) · −1 + PB(VS = 0) · 0+
PB(VE = −7) · −7 + PB(VE = 5) · 5 + PB(VE = 20) · 20 + PB(VE = 0) · 0),
where B = BIF,C[σ]. It is straightforward to verify that the strategy that optimizes the expected
utility is: δC = c1; δF (c1, s0) = f 0, δF (c1, s1) = f 1, δF (c1, s2) = f 1. Because the event
C = c0 has probability 0 in this strategy, any choice of probability distributions for δF (c0, S) is
optimal. By following the deﬁnition, we can compute the overall expected utility for this strategy,
which is 3.22, so that MEU[IF,C] = 3.22.
According to the basic postulate of statistical decision theory, the agent’s goal is to maximize
his expected utility for a given decision setting. Thus, he should choose the strategy σ that
maximizes EU[I[σ]].
Deﬁnition 23.6
An MEU strategy σ∗for an inﬂuence diagram I is one that maximizes EU[I[σ]]. The MEU value
MEU strategy
MEU value
MEU[I] is EU[I[σ∗]].
In general, there may be more than one MEU strategy for a given inﬂuence diagram, but they
all have the same expected utility.
This deﬁnition lays out the basic computational task associated with inﬂuence diagrams:
Given an inﬂuence diagram I, our goal is to ﬁnd the MEU strategy MEU[I]. Recall that a
strategy is an assignment of decision rules to all the decision variables in the network; thus, our
goal is to ﬁnd:
arg
max
δD1,...,δDk
EU[I[δD1, . . . , δDk]].
(23.3)
Each decision rule is itself a complex function, assigning an action (or even a distribution over
actions) to each information state. This complex optimization task appears quite daunting at
ﬁrst. Here we present two diﬀerent ways of tackling it.
Box 23.A — Case Study: Decision Making for Prenatal Testing. As we discussed in box 22.A,
prenatal diagnosis oﬀers a challenging domain for decision making. It incorporates a sequence
prenatal
diagnosis
of interrelated decisions, each of which has signiﬁcant eﬀects on variables that determine the
patient’s preferences. Norman et al. (1998) construct a system called PANDA (which roughly stands

23.3. Backward Induction in Inﬂuence Diagrams
1095
for “Prenatal Testing Decision Analysis”). PANDA uses an inﬂuence diagram to model the sequential
decision process, the relevant random variables, and the patient’s utility. The inﬂuence diagram
contains a sequence of six decisions: four types of diagnostic test (CVS, triple marker screening,
ultrasound, and amniocentesis), as well as early and late termination of the pregnancy. The model
focuses on ﬁve diseases that are serious, relatively common, diagnosable using prenatal testing, and
not readily correctable: Down syndrome, neural-tube defects, cystic ﬁbrosis, sickle-cell anemia, and
fragile X mental retardation. The probabilistic component of the network (43 variables) includes
predisposing factors that aﬀect the probability of these diseases, and it models the errors in the
diagnostic ability of the tests (both false positive and false negative). Utilities were elicited for every
patient and placed on a scale of 0–100, where a utility of 100 corresponds to the outcome of a
healthy baby with perfect knowledge throughout the course of the pregnancy, and a utility of 0
corresponds to the outcome of both maternal and fetal death.
The strategy space in this model is very complex, since any decision (including a decision to
take a test) can depend on the outcome of one or more earlier tests, As a consequence, there are
about 1.62 × 10272 diﬀerent strategies, of which 3.85 × 1038 are “reasonable” relative to a set of
constraints. This enormous space of options highlights the importance of using automated methods
to guide the decision process.
The system can be applied to diﬀerent patients who vary both on their predisposing factors and
on their utilities. Both the predisposing factors and the utilities give rise to very diﬀerent strategies.
However, a more relevant question is the extent to which the diﬀerent strategy choices make a
diﬀerence to the patient’s ﬁnal utility. To provide a reasonable scale for answering this question, the
algorithm was applied to select for each patient their best and worst strategy. As an example, for one
such patient (a young woman with predisposing factors for sickle-cell anemia), the optimal strategy
achieved an expected utility of 98.77 and the worst strategy an expected utility of 87.85, for a
diﬀerence of 10.92 utility points. Other strategies were then evaluated in terms of the percentage
of these 10.92 points that they provided to the patient. For many patients, most of the reasonable
strategies performed fairly well, achieving over 99 percent of the utility gap for that patient. However,
for some patients, even reasonable strategies gave very poor results. For example, for the patient with
sickle-cell anemia, strategies that were selected as optimal for other patients in the study provided
her only 65–70 percent of the utility gap. Notably, the “recommended” strategy for women under the
age of thirty-ﬁve, which is to perform no tests, performed even worse, achieving only 64.7 percent of
the utility gap.
Overall, this study demonstrates the importance of personalizing medical decision making to the
information and the utility for individual patients.
23.3
Backward Induction in Inﬂuence Diagrams
We now turn to the problem of selecting the optimal strategy in an inﬂuence diagram. Our
ﬁrst approach to addressing this problem is a fairly simple algorithm that mirrors the backward
induction algorithm for decision trees described in section 23.1.2. As we will show, this algorithm
can be implemented eﬀectively using the techniques of variable elimination of chapter 9. This
algorithm applies only to inﬂuence diagrams satisfying the perfect recall assumption, a restriction
that has signiﬁcant computational ramiﬁcations.

1096
Chapter 23. Structured Decision Problems
s1
s0
s2
c0
c1
f 0
f 1
F
0
20
0.46
0.18
0.36
0.36
5
–7
M
S
F
C
0
20
0.42
0.21
0.36
0.37
5
–7
M
F
0
20
0.5
0.2
5
–7
M
F
0
20
0.73
0.05
0.36
0.22
5
–7
M
0.3
Figure 23.4
Decision tree for the inﬂuence diagram IF,C in the Entrepreneur example. For clarity,
probability zero events are omitted, and edges are labeled only at representative nodes.
23.3.1
Decision Trees for Inﬂuence Diagrams
Our starting point for the backward induction algorithm is to view an inﬂuence diagram as
deﬁning a set of possible trajectories, deﬁned from the perspective of the agent. A trajectory
includes both the observations made by the agent and the decisions he makes. The set of
possible trajectories can be organized into a decision tree, with a split for every chance variable
and every decision variable.
We note that this construction gives rise to an exponentially
large decision tree. Importantly, we never have to construct this tree explicitly. As we will
show, we can use this tree as a conceptual construct, which forms the basis for deﬁning a
variable elimination algorithm. The VE algorithm works directly on the inﬂuence diagram, never
constructing the exponentially large tree.
We begin by illustrating the decision tree construction on a simple example.
Example 23.7
Consider the possible trajectories that might be encountered by our entrepreneur of example 23.3.
Initially, he has to decide whether to conduct the survey or not (C). He then gets to observe the
value of the survey (S). He then has to decide whether to found the company or not (F). The
variable M inﬂuences his utility, but he never observes it (at least not in a way that inﬂuences
any of his decisions). Finally, the utility is selected based on the entire trajectory. We can organize
this set of trajectories into a tree, where the ﬁrst split is on the agent’s decision C, the second split
(on every branch) is nature’s decision regarding the value of S, the third split is on the agent’s
decision F, and the ﬁnal split is on M. At each leaf, we place the utility value corresponding to
the scenario. Thus, for example, the agent’s utility at the leaf of the trajectory c1, s1, f 1, m1 is
VS(c1) + VE(f 1, m1) = −1 + 5. The decision tree for this example is the same one shown in
ﬁgure 23.4.
Note that the ordering of the nodes in the tree is deﬁned by the agent’s observations, not by the
topological ordering of the underlying inﬂuence diagram. Thus, in this example, S precedes M,
despite the fact that, viewed from the perspective of generative causal model, M is “determined

23.3. Backward Induction in Inﬂuence Diagrams
1097
by nature” before S.
More generally, we assume (as stated earlier) that the inﬂuence diagram satisﬁes perfect recall
relative to some temporal ordering ≺on decisions. Without loss of generality, assume that
D1 ≺. . . ≺Dk. We extend ≺to a partial ordering over X ∪D which is consistent with the
information edges in the inﬂuence diagrams; that is, whenever W is a parent of D for some
D ∈D and W ∈X ∪D, we have that W ≺D. This ordering is guaranteed to extend the total
ordering ≺over D postulated in deﬁnition 23.5, allowing us to abuse notation and use ≺for
both.
This partial ordering constrains the orderings that we can use to deﬁne the decision tree. Let
X1 be the set of variables X such that X ≺D1; these variables are the ones that the agent
observes for the ﬁrst time at decision D1. More generally, let Xi be those variables X such
that X ≺Di but not X ≺Di−1. These variables are the ones that the agent observes for the
ﬁrst time at decision Di. With the perfect recall assumption, the agent’s decision rule at Di
can depend on all of X1 ∪. . . ∪Xi ∪{D1, . . . , Di−1}. Let Y be the variables that are not
observed prior to any decision. The sets X1, . . . , Xk, Y form a disjoint partition of X. We
can then deﬁne a tree where the ﬁrst split is on the set of possible assignments x1 to X1, the
second is on possible decisions in Val(D1), and so on, and where the ﬁnal split is on possible
assignments y to Y .
The choices at nature’s chance moves are associated with probabilities. These probabilities are
not the same as the generative probabilities (as reﬂected in the CPDs in the inﬂuence diagrams),
but reﬂect the agent’s subjective beliefs in nature’s choices given the evidence observed so far.
Example 23.8
Consider the decision tree of ﬁgure 23.4, and consider nature’s choice for the branch S = s1
at the node corresponding to the trajectory C = c1. The probability that the survey returns s1
is the marginal probability P
M P(M) · P(s1 | M, c1).
Continuing down the same branch,
and assuming F = f 1, the branching probability for M = m1 is the conditional probability of
M = m1 given s1 (and the two decision variables, although these are irrelevant to this probability).
In general, consider a branch down the tree associated with the choices x1, d1, . . . , xi−1, di−1.
At this vertex, we have a decision of nature, splitting on possible instantiations xi to Xi. We
associate with this vertex a distribution P(xi | x1, d1, . . . , xi−1, di−1).
As written, this probability expression is not well deﬁned, since we have not speciﬁed a
distribution relative to which it is computed. Speciﬁcally, because we do not have a decision
rule for the diﬀerent decision variables in the inﬂuence diagram, we do not yet have a fully
speciﬁed Bayesian network. We can ascribe semantics to this term using the following lemma:
Lemma 23.1
Let x1, . . . , xi−1, d1, . . . , di−1 be an assignment to X1, . . . , Xi−1, D1, . . . , Di−1 respectively.
Let σ1, σ2 be any two strategies in which PBI[σi](x1, . . . , xi−1, d1, . . . , di−1) ̸= 0 (i = 1, 2).
Then
PBI[σ1](Xi | x1, . . . , xi−1, d1, . . . , di−1) = PBI[σ2](Xi | x1, . . . , xi−1, d1, . . . , di−1).
The proof is left as an exercise (exercise 23.4). Thus, the probability of Xi given x1, . . . , xi−1
, d1, . . . , di−1 does not depend on the choice of strategy σ, and so we can deﬁne a probability
for this event without deﬁning a particular strategy σ. We use P(Xi | x1, d1, . . . , xi−1, di−1)
as shorthand for this uniquely deﬁned probability distribution.

1098
Chapter 23. Structured Decision Problems
23.3.2
Sum-Max-Sum Rule
Given an inﬂuence diagram I, we can construct the decision tree using the previous procedure
and then simply run MEU-for-Decision-Trees (algorithm 23.1) over the resulting tree. The algo-
rithm computes both the MEU value of the tree and the optimal strategy. We now show that
this MEU value and strategy are also the optimal value and strategy for the inﬂuence diagram.
Our ﬁrst key observation is that, in the decision tree we constructed, we can choose a
diﬀerent action at each t-node in the layer for a decision variable D.
In other words, the
decision tree strategy allows us to take a diﬀerent action at D for each assignment to the
decision and observation variables preceding D in ≺. The perfect-recall assumption asserts that
these variables are precisely the parents of D in the inﬂuence diagram I. Thus, a decision rule
at D is precisely as expressive as the set of individual decisions at the t-nodes corresponding
to D, and the decision tree algorithm is simply selecting a set of decision rules for all of the
decision variables in I — that is, a complete strategy for I.
Example 23.9
In the F layer (the third layer) of the decision tree in ﬁgure 23.4, we maximize over diﬀerent
possible values of the decision variable F. Importantly, this layer is not selecting a single decision,
but a (possibly) diﬀerent action at each node in the layer. Each of these nodes corresponds to an
information state — an assignment to C and S. Altogether, the set of decisions at this layer selects
the entire decision rule δF .
Note that the perfect recall assumption is critical here. The decision tree semantics (as we
deﬁned it) makes the implicit assumption that we can make an independent decision at each
t-node in the tree. Hence, if D′ follows D in the decision tree, then every variable on the path
to D t-nodes also appears on the path to D′ t-nodes. Thus, the decision tree semantics can be
consistent with the inﬂuence diagram semantics only when the inﬂuence diagram satisﬁes the
perfect recall assumption.
We now need to show that the strategy selected by this algorithm is the one that maximizes
the expected utility for I. To do so, let us examine more closely the expression computed by
MEU-for-Decision-Trees when applied to the decision tree constructed before.
Example 23.10
In example 23.3, our computation for the value of the entire tree can be written using the following
expression:
max
C
X
S
P(S | C) max
F
X
M
P(M | S, F, C)[VS(C) + VE(M, F)].
Note that we can simplify some of the conditional probability terms in this expression using the
conditional independence properties of the network (which are also invariant under any choice of
decision rules). For example, M is independent of F, C given S, so that P(M | S, F, C) = P(M |
S).
More generally, consider an inﬂuence diagram where, as before, the sequence of chance and
decision variables is: X1, D1, . . . , Xk, Dk, Y . We can write the value of the decision-making
situation using the following expression, known as the sum-max-sum rule:
sum-max-sum
rule
MEU[I] = P
X1 P(X1) maxD1
P
X2 P(X2 | X1, D1) maxD2 . . .
P
Xk P(Xk | X1, . . . , Xk−1, D1, . . . , Dk−1)
maxDk
P
Y P(Y | X1, . . . , Xk, D1, . . . , Dk)U(Y , X1, . . . , Xk, D1, . . . , Dk).

23.3. Backward Induction in Inﬂuence Diagrams
1099
D1
H1
V1
D2
H2
M
V2
D3
H3
V3
D4
H4
V4
Figure 23.5
Iterated optimization versus variable elimination. An inﬂuence diagram that allows an
eﬃcient solution using iterated optimization, but where variable elimination techniques are considerably
less eﬃcient.
This expression is eﬀectively performing the same type of backward induction that we used in
decision trees.
We can now push in the conditional probabilities into the summations or maximizations. This
operation is the inverse to the one we have used so often earlier in the book, where we move
probability factors out of a summation or maximization; the same equivalence is used to justify
both. Once all the probabilities are pushed in, all of the conditional probability expressions
cancel each other, so that we obtain simply:
MEU[I] =
X
X1
max
D1
X
X2
max
D2 . . .
X
Xk
max
Dk
X
Y
P(X1, . . . , Xk, Y | D1, . . . , Dk)U(X1, . . . , Xk, Y , D1, . . . , Dk).
(23.4)
If we view this expression in terms of factors (as in equation (23.2)), we can decompose the
joint probability P(X1, . . . , Xk, Y | D1, . . . , Dk) = P(X | D) as the product of all of the
factors corresponding to the CPDs of the variables X in the inﬂuence diagram. The joint utility
U(X1, . . . , Xk, Y , D1, . . . , Dk) = U(X, D) is the sum of all of the utility variables in the
network.
Now, consider a strategy σ — an assignment of actions to all of the agent t-nodes in the tree.
Given a ﬁxed strategy, the maximizations become vacuous, and we are simply left with a set
of summations over the diﬀerent chance variables in the network. It follows directly from the
deﬁnitions that the result of this summation is simply the expected utility of σ in the inﬂuence
diagram, as in equation (23.2). The fact that the sum-max-sum computation results in the MEU
strategy now follows directly from the optimality of the strategy produced by the decision tree
algorithm.
The form of equation (23.4) suggests an alternative method for computing the MEU value
and strategy, one that does not require that we explicitly form a decision tree. Rather, we can
apply a variable elimination algorithm that directly computes the the sum-max-sum expression:
variable
elimination
We eliminate both the chance and decision variables, one at a time, using the P or max

1100
Chapter 23. Structured Decision Problems
operations, as appropriate. At ﬁrst glance, this approach appears straightforward, but the details
are somewhat subtle. Unlike most of our applications of the variable elimination algorithm,
which involve only two operations (either sum-product or max-product), this expression involves
four — sum-marginalization, max-marginalization, factor product (for probabilities and utilities),
and factor addition (for utilities). The interactions between these diﬀerent operations require
careful treatment, and the machinery required to handle them correctly has a signiﬁcant eﬀect
on the design and eﬃciency of the variable elimination algorithm. The biggest complication
arises from the fact that sum-marginalization and max-marginalization do not commute, and
therefore elimination operations can be executed only in an order satisfying certain constraints;
as we showed in section 13.2.3 and section 14.3.1, such constraints can cause inference even in
simple networks to become intractable. The same issues arise here:
Example 23.11
Consider a setting where a student must take a series of exams in a course. The hardness Hi of
each exam i is not known in advance, but one can assume that it depends on the hardness of the
previous exam Hi−1 (if the class performs well on one exam, the next one tends to be harder, and
if the class performs poorly, the next one is often easier). It also depends on the overall meanness of
the instructor. The student needs to decide how much to study for each exam (Di); studying more
makes her more likely to succeed in the exam, but it also reduces her quality of life. At the time the
student needs to decide on Di, she knows the diﬃculty of the previous one and whether she studied
for it, but she does not remember farther back than that. The meanness of the instructor is never
observed. The inﬂuence diagram is shown in ﬁgure 23.5.
If we apply a straightforward variable elimination algorithm based on equation (23.4), we would
have to work from the inside out in an order that is consistent with the operations in the equation.
Thus, we would ﬁrst have to eliminate M, which is never observed. This step has the eﬀect of
creating a single factor over all of the Hi variables, whose size is exponential in k.
Fortunately, as we discuss in section 23.5, there are better solution methods for inﬂuence
diagrams, which are not based on variable elimination and hence avoid some of these diﬃculties.
23.4
Computing Expected Utilities
In constructing a more eﬃcient algorithm for ﬁnding the optimal decision in an inﬂuence
diagram, we ﬁrst consider the special case of an inﬂuence diagram with no decision variables.
This problem is of interest in its own right, since it allows us to evaluate the expected utility of
a given strategy. More importantly, it is also a key subroutine in the algorithm for ﬁnding an
optimal strategy.
We begin our discussion with the even more restricted setting, where there is a single utility
variable, and then discuss how it can be extended to the case of several utility variables. As we
will see, although there are straightforward generalizations, an eﬃcient implementation for this
extension can involve some subtlety.
23.4.1
Simple Variable Elimination
Assume we have a single utility factor U. In this case, the expected utility is simply a product
of factors: the CPDs of the chance variables, the decision rules, and the utility function of the

23.4. Computing Expected Utilities
1101
D
V1
V2
E
C
B
A
Figure 23.6
An inﬂuence diagram with multiple utility variables
single utility factor U, summed out over all of the variables in the network. Thus, in the setting

of a single utility variable, we can apply our standard variable elimination algorithm in
a straightforward way, to the set of factors deﬁning the expected utility. Because variable
elimination is well deﬁned for any set of factors (whether derived from probabilities or not),
there is no obstacle to applying it in this setting.
Example 23.12
Consider the inﬂuence diagram in ﬁgure 23.6. The inﬂuence diagram is drawn with two utility vari-
ables, but (proceeding with our assumption of a single utility variable) we analyze the computation
for each of them in isolation, assuming it is the only utility variable in the network.
We begin with the utility variable V1, and use the elimination ordering C, A, E, B, D. Note that
C is barren relative to V1 (that is, it has no eﬀect on the utility V1) and can therefore be ignored.
(Eliminating C would simply produce the all 1’s factor.) Eliminating A, we obtain
µ1
1(B, E) =
X
A
V1(A, E)P(B | A)P(A).
Eliminating E, we obtain
µ1
2(B, D) =
X
E
P(E | D)µ1
1(B, E).
We can now proceed to eliminate D and B, to compute the ﬁnal expected utility value.
Now, consider the same variable elimination algorithm, with the same ordering, applied to the
utility variable V2. In this case, C is not barren, so we compute:
µ2
1(E) =
X
C
V2(C, E)P(C).
The variable A does not appear in the scope of µ2
1, and hence we do not use the utility factor in
this step. Rather, we obtain a standard probability factor:
φ2
1(B) =
X
A
P(A)P(B | A).

1102
Chapter 23. Structured Decision Problems
Eliminating E, we obtain:
µ2
2(D) =
X
E
P(E | D)µ2
1(E).
To eliminate B, we multiply P(D | B) (which is a decision rule, and hence simply a CPD) with
φ2
1(B), and then marginalize out B from the resulting probability factor. Finally, we multiply the
result with µ2
2(D) to obtain the expected utility of the inﬂuence diagram, given the decision rule
for D.
23.4.2
Multiple Utility Variables: Simple Approaches
An eﬃcient extension to multiple utility variables is surprisingly subtle. One obvious solution is
to collapse all of the utility factors into a single large factor U = P
V ∈U V . We are now back
to the same situation as above, and we can run the variable elimination algorithm unchanged.
Unfortunately, this solution can lead to unnecessary computational costs:
Example 23.13
Let us return to the inﬂuence diagram of example 23.12, but where we now assume that we
have both V1 and V2. In this simple solution, we add both together to obtain U(A, E, C). If
we now run our variable elimination process (with the same ordering), it produces the following
factors: µU
1 (A, E) = P
C P(C)U(A, C, E); µU
2 (B, E) = P
A P(A)P(B | A)µU
1 (A, E); and
µU
3 (B, D) = P
E P(E | D)µU
2 (B, E).
Thus, this process produces a factor over the scope
A, C, E, which is not created by either of the two preceding subcomputations; if, for example, both
A and C have a large domain, this factor might result in high computational costs.
Thus, this simple solution requires that we sum up the individual subutility functions to con-
struct a single utility factor whose scope is the union of the scopes of the subutilities. As a
consequence, this transformation loses the structure of the utility function and creates a factor
that may be exponentially larger. In addition to the immediate costs of creating this larger factor,
factors involving more variables can also greatly increase the cost of the variable elimination
algorithm by forcing us to multiply in more factors as variables are eliminated.
A second simple solution is based on the linearity of expectations:
X
X−PaD
Y
X∈X
P(X | PaX)(
X
i : Vi∈U
Vi) =
X
i : Vi∈U
 X
X−PaD
Y
X∈X
P(X | PaX)Vi
!
.
Thus, we can run multiple separate executions of variable elimination, one for each utility factor
Vi, computing for each of them an expected utility factor µi
−D; we then sum up these expected
utility factors and optimize the decision rule relative to the resulting aggregated utility factor.
The limitation of this solution is that, in some cases, it forces us to replicate work that arises for
multiple utility factors.
Example 23.14
Returning to example 23.12, assume that we replace the single variable E between D and the two
utility variables with a long chain D →E1 →. . . →Ek, where Ek is the parent of V1 and V2. If
we do a separate variable elimination computation for each of V1 and V2, we would be executing
twice the steps involved in eliminating E1, . . . , Ek, rather than reusing the computation for both
utility variables.

23.4. Computing Expected Utilities
1103
23.4.3
Generalized Variable Elimination ⋆
A solution that addresses both the limitations described before is to perform variable elimination
with multiple utility factors simultaneously, but allow the algorithm to add utility factors to each
other, as called for by the variable elimination algorithm. In other words, just as we multiply
factors together when we eliminate a variable that they have in common, we would combine
two utility factors together in the same situation.
Example 23.15
Let us return to example 23.12 using the same elimination ordering C, A, E. The ﬁrst steps, of
eliminating C and A, are exactly those we took in that example, as applied to each of the two
utility factors separately. In other words, the elimination of C does not involve V1, and hence
produces precisely the same factor µ1
1(B, E) as before; similarly, the elimination of A does not
involve V2, and produces µ2
1(E), φ2
1(B). However, when we now eliminate E, we must somehow
combine the two utility factors in which E appears. At ﬁrst glance, it appears as if we can simply
add these two factors together. However, a close examination reveals an important subtlety. The
utility factor µ2
1(E) = P
C P(C)V2(C, E) is a function that deﬁnes, for each assignment to E,
an expected utility given E. However, the entries in the other utility factor,
µ1
1(B, E)
=
X
A
P(A)P(B | A)V1(A, E)
=
X
A
P(B)P(A | B)V1(A, E) = P(B)
X
A
P(A | B)V1(A, E),
do not represent an expected utility; rather, they are a product of an expected utility with the
probability P(B). Thus, the two utility factors are on “diﬀerent scales,” so to speak, and cannot
simply be added together. To remedy this problem, we must convert both utility factors into the
“utility scale” before adding them together. To do so, we must keep track of P(B) as we do the
elimination and divide µ1
1(B, E) by P(B) to rescale it appropriately, before adding it to µ2
1.
Thus, in order to perform the variable elimination computation correctly with multiple utility
variables, we must keep track not only of utility factors, but also of the probability factors neces-
sary to normalize them. This intuition suggests an algorithm where our basic data structures —
our factors — are actually pairs of factors γ = (φ, µ), where φ is a probability factor, and µ is
a utility factor. Intuitively, φ is the probability factor that can bring µ into the “expected utility
scale.” More precisely, assume for simplicity that the probability and utility factors in a joint
factor have the same scope; we can make this assumption without loss of generality by simply
increasing the scope of either or both factors, duplicating entries as required. Intuitively, the
probability factor is maintained as an auxiliary factor, used to normalize the utility factor when
necessary, so as to bring it back to the standard “utility scale.” Thus, if we have a joint factor
(φ(Y ), µ(Y )), then µ(Y )/φ(Y ) is a factor whose entries are expected utilities associated with
the diﬀerent assignments y.
Our goal is to deﬁne a variable-elimination-style algorithm using these joint factors. As for any
variable elimination algorithm, we must deﬁne operations that combine factors, and operations
that marginalize — sum out — variables out of a factor. We now deﬁne both of these steps. We
consider, for the moment, factors associated only with probability variables and utility variables;
we will discuss later how to handle decision variables.

1104
Chapter 23. Structured Decision Problems
Initially, each variable W that is associated with a CPD induces a probability factor φW ; such
variables include both chance variables in X and decision variables associated with a decision
rule. (As we discussed, a decision rule for a decision variable D essentially turns D into a
chance variable.) We convert φW into a joint factor γW by attaching to it an all-zeros utility
factor over the same scope, 0Scope[φW ]. Similarly, each utility variable V ∈U is associated with
a utility factor µV , which we convert to a joint factor by attaching to it an all-ones probability
factor: γV = (1PaV , V ) for V ∈U.
Intuitively, we want to multiply probability components (as usual) and add utility components.
Thus, we deﬁne the joint factor combination operation as follows:
•
For two joint factors γ1 = (φ1, µ1), γ2 = (φ2, µ2), we deﬁne the joint factor combination
operation:
γ1
M
γ2 = (φ1 · φ2, µ1 + µ2).
(23.5)
We see that, if all of the joint factors in the inﬂuence diagram are combined, we obtain a single
(exponentially large) probability factor that deﬁnes the joint distribution over outcomes, and a
single (exponentially large) utility factor that deﬁnes the utilities of the outcomes. Of course,
this procedure is not one that we would ever execute; rather, as in variable elimination, we want
to interleave combination steps and marginalization steps in a way that preserves the correct
semantics.
The deﬁnition of the marginalization operation is subtler. Intuitively, we want the probability
of an outcome to be multiplied with its utility. However, as suggested by example 23.15, we must
take care that the utility factors derived as intermediate results all maintain the same scale, so
that they can be correctly added in the factor combination operation. Thus, when marginalizing
a variable W, we divide the utility factor by the associated probability factor, ensuring that it
maintains its expected utility interpretation:
•
For a joint factor γ = (φ, µ) over scope W , we deﬁne the joint factor marginalization
operation for W ′ ⊂W as follows:
margW ′(γ) =
 X
W ′
φ,
P
W ′ φ · µ
P
W ′ φ
!
.
(23.6)
Intuitively, this operation marginalizes out (that is, eliminates) the variables in W ′, handling
both utility and probability factors correctly.
Finally, at the end of the process, we can combine the probability and utility factors to obtain
a single factor that corresponds to the overall expected utility:
•
For a joint factor γ = (φ, µ), we deﬁne the joint factor contraction operation as the factor
product of the two components:
cont(γ) = φ · µ.
(23.7)
To understand these deﬁnitions, consider again the problem of computing the expected utility
for some (complete) strategy σ for the inﬂuence diagram I. Thus, we now have a probability

23.4. Computing Expected Utilities
1105
Algorithm 23.2 Generalized variable elimination for joint factors in inﬂuence diagrams
Procedure Generalized-VE-for-IDs (
Φ,
// Set of joint (probability,utility) factors
W1, . . . , Wk
// List of variables to be eliminated
)
1
for i = 1, . . . , k
2
Φ′ ←{φ ∈Φ : Wi ∈Scope[φ]}
3
ψ ←L
φ∈Φ′ φ
4
τ ←margWi(ψ)
5
Φ ←Φ −Φ′ ∪{τ}
6
φ∗←L
φ∈Φ φ
7
return φ∗
factor for each decision variable. Recall that our expected utility is deﬁned as:
EU[I[σ]] =
X
W ∈X∪D
Y
W ∈X∪D
φW · (
X
V ∈U
µV ).
Let γ∗be the marginalization over all variables of the combination of all of the joint factors:
γ∗= (φ∗, µ∗) = marg∅(
M
(W ∈X∪U)
[γW ]).
(23.8)
Note that the factor has empty scope and is therefore simply a pair of numbers. We can now
show the following simple result:
Proposition 23.1
For γ∗deﬁned in equation (23.8), we have: γ∗= (1, EU[I[σ]]).
The proof follows directly from the deﬁnitions and is left as an exercise (exercise 23.2).
Of course, as we discussed, we want to interleave the marginalization and combination steps.
An algorithm implementing this idea is shown in algorithm 23.2. The algorithm returns a single
joint factor (φ, µ).
Example 23.16
Let us consider the behavior of this algorithm on the inﬂuence diagram of example 23.12, assuming
again that we have a decision rule for D, so that we have only chance variables and utility variables.
Thus, we initially have ﬁve joint factors derived from the probability factors for A, B, C, D, E; for
example, we have γB = (P(B | A), 0A,B). We have two joint factors γ1, γ2 derived from the
utility variables V1, V2; for example, we have γ2 = (1C,E, V2(C, E)).
Now, consider running our generalized variable elimination algorithm, using the elimination
ordering C, A, E, B, D. Eliminating C, we ﬁrst combine γC, γ2 to obtain:
γC
M
γ2 = (P(C), V2(C, E)),
where the scope of both components is taken to be C, E. We then marginalize C to obtain:
γ3(E)
=

1E,
P
C(P(C)V2(C, E))
1E

=
(1E, IEP (C)[V2(C, E)]).

1106
Chapter 23. Structured Decision Problems
Continuing to eliminate A, we combine γA, γB, and γ1 and marginalize A to obtain:
γ4(B, E)
=
 X
A
P(A)P(B | A),
P
A P(A)P(B | A)V1(A, E)
P
A P(A)P(B | A)
!
=
(P(B),
X
A
P(A | B)V1(A, E))
=
(P(B), IEP (A|B)[V1(A, E)]).
Importantly, the utility factor here can be interpreted as the expected utility over V1 given B, where
the expectation is taken over values of A. It therefore keeps this utility factor on the same scale as
the others, avoiding the problem of incomparable utility factors that we had in example 23.15.
We next eliminate E. We ﬁrst combine γE, γ3, and γ4 to obtain:
(P(E | D)P(B), IEP (C)[V2(C, E)] + IEP (A|B)[V1(A, E)]).
Marginalizing E, we obtain:
γ5(B, D)
=
(P(B), IEP (C,E|D)[V2(C, E)] + IEP (A,E|B,D)[V1(A, E)]).
To eliminate B, we ﬁrst combine γ5 and γD, to obtain:
(P(D | B)P(B), IEP (C,E|D)[V2(C, E)] + IEP (A,E|D)[V1(A, E)]).
We then marginalize B, obtaining:
γ6(D)
=

P(D),
P
B(IEP (C,E,D,B)[V2(C, E)] + IEP (A,E,D,B)[V1(A, E)])
P(D)

=
 P(D), IEP (C,E|D)[V2(C, E)] + IEP (A,E|B,D)[V1(A, E)]

.
Finally, we have only to marginalize D, obtaining:
γ7(∅)
=
 1, IEP (C,E)[V2(C, E)] + IEP (A,E)[V1(A, E)]

,
as desired.
How do we show that it is legitimate to reorder these marginalization and combination
operators? In exercise 9.19, we deﬁned the notion of generalized marginalize-combine factor
operators and stated a result showing that, for any pair of operators satisfying certain conditions,
any legal reordering of the operators led to the same result. In particular, this result implied, as
special cases, correctness of sum-product, max-product, and max-sum variable elimination. The
same analysis can be used for the operators deﬁned here, showing the following result:
Theorem 23.1
Let Φ be a set of joint factors over Z. Generalized-VE-for-IDs(Φ, W ) returns the joint factor
margW (
M
(γ∈Φ)
γ).

23.5. Optimization in Inﬂuence Diagrams
1107
The proof is left as an exercise (exercise 23.3).
Note that the complexity of the algorithm is the same (up to a constant factor) as that of a
standard VE algorithm, applied to an analogous set of factors — with the same scope — as our
initial probability and utility factors. In other words, for a given elimination ordering, the cost
of the algorithm grows as the induced tree-width of the graph generated by this initial set of
factors.
So far, we have discussed the problem of computing the expected utility of a complete strategy.
How can we apply these ideas to our original task, of optimizing a single decision rule? The
idea is essentially the same as in section 23.4.1. As there, we apply Generalized-VE-for-IDs to
eliminate all of the variables other than FamilyD = {D}∪PaD. In this process, the probability
factor induced by the decision rule for D is only combined with the other factors at the ﬁnal
step of the algorithm, when the remaining factors are all combined. It thus has no eﬀect on
the factors produced up to that point. We can therefore omit φD from our computation, and
produce a joint factor γ−D = (φ−D, µ−D) over FamilyD based only on the other factors in
the network.
For any decision rule δD, if we run Generalized-VE-for-IDs on the factors in the original
inﬂuence diagram plus a joint factor γD = (δD, 0FamilyD), we would obtain the factor
γδD = γ−D
M
γD.
Rewriting this expression, we see that the overall expected utility for the inﬂuence diagram given
the decision rule δD is then:
X
w∈Val(PaD),d∈Val(D)
cont(γ−D)(w, d)δD(w).
Based on this observation, and on the fact that we can always select an optimal decision rule
that is a deterministic function from Val(PaD) to Val(D), we can easily optimize δD. For each
assignment w to PaD, we select
δD(w) = arg
max
d∈Val(D) cont(γ−D)(w, d).
As before, the problem of optimizing a single decision rule can be solved using a standard
variable elimination algorithm, followed by a simple optimization. In this case, we must use a
generalized variable elimination algorithm, involving both probability and utility factors.
23.5
Optimization in Inﬂuence Diagrams
We now turn to the problem of selecting an optimal strategy in an inﬂuence diagram. We begin
with the simple case, where we have only a single decision variable. We then show how to
extend these ideas to the more general case.
23.5.1
Optimizing a Single Decision Rule
We ﬁrst make the important observation that, for the case of a single decision variable, the task
of ﬁnding an optimal decision rule can be reduced to that of computing a single utility factor.

1108
Chapter 23. Structured Decision Problems
We begin by rewriting the expected utility of the inﬂuence diagram in a diﬀerent order:
EU[I[σ]] =
X
D,PaD
δD
X
X−PaD
Y
X∈X
P(X | PaX)(
X
V ∈U
V ).
(23.9)
Our task is to select δD.
We now deﬁne the expected utility factor to be the value of the internal summation in
expected utility
factor
equation (23.9):
µ−D =
X
X−PaD
Y
X∈X
P(X | PaX)(
X
V ∈U
V ).
(23.10)
This expression is the marginalization of this product onto the variables D ∪PaD; importantly,
it does not depend on our choice of decision rule for D. Given µ−D, we can compute the
expected utility for any decision rule δD as:
X
D,PaD
δDµ−D(D, PaD).
Our goal is to ﬁnd δD that maximizes this expression.
Proposition 23.2
Consider an inﬂuence diagram I with a single decision variable D. Letting µ−D be as in equa-
tion (23.10), the optimal decision rule for D in I is deﬁned as:
δD(w) = arg
max
d∈Val(D) µ−D(d, w)
∀w ∈Val(PaD). (23.11)
The proof is left as an exercise (see exercise 23.1).
Thus, we have shown how the problem of optimizing a single decision rule can be solved
very simply, once we have computed the utility factor µ−D(D, PaD).
Importantly, any of the algorithms described before, whether the simpler ones in section 23.4.1
and 23.4.2, or the more elaborate generalized variable elimination algorithm of section 23.4.3, can
be used to compute this expected utility factor. We simply structure our elimination ordering
to eliminate only the variables other than D, PaD; we then combine all of the factors that are
computed via this process, to produce a single integrated factor µ−D(D, PaD). We can then
use this factor as in proposition 23.2 to ﬁnd the optimal decision rule for D, and thereby solve
the inﬂuence diagram.
How do we generalize this approach to the case of an inﬂuence diagram with multiple
decision rules D1, . . . , Dk? In principle, we could generate an expected utility factor where
we eliminated all variables other than the union Y = ∪i({Di} ∪PaDi) of all of the decision
variables and all of their parents. Intuitively, this factor would specify the expected utility of the
inﬂuence diagram given an assignment to Y . However, in this case, the optimization problem
is much more complex, in that it requires that we consider simultaneously the decisions at all
of the decision variables in the network. Fortunately, as we show in the next section, we can
perform this multivariable optimization using localized optimization steps over single variables.
23.5.2
Iterated Optimization Algorithm
In this section, we describe an iterated approach that breaks up the problem into a series of
simpler ones. Rather than optimize all of the decision rules at the same time, we ﬁx all of

23.5. Optimization in Inﬂuence Diagrams
1109
the decision rules but one, and then optimize the remaining one. The problem of optimizing
a single decision rule is signiﬁcantly simpler, and admits very eﬃcient algorithms, as shown in
section 23.5.1. This algorithm is very similar in its structure to the local optimization approach
for marginal MAP problems, presented in section 13.7. Both algorithms are intended to deal with
the same computational bottleneck: the exponentially large factors generated by a constrained
elimination ordering. They both do so by optimizing one variable at a time, keeping the others
ﬁxed. The diﬀerence is that here we are optimizing an entire decision rule for the decision
variable, whereas there we are simply picking a single value for the MAP variable.
We will show that, under certain assumptions, this iterative approach is guaranteed to converge
to the optimal strategy.
Importantly, this approach also applies to inﬂuence diagrams with
imperfect recall, and can therefore be considerably more eﬃcient.
The basic idea behind this algorithm is as follows. The algorithm proceeds by sequentially
optimizing individual decision rules. We begin with some (almost) arbitrary strategy σ, which
assigns a decision rule to all decision variables in the network.
We then optimize a single
decision rule relative to our current assignment to the others. This decision rule is used to
update σ, and another decision rule is now optimized relative to the new strategy.
More
precisely, let σ−D denote the decision rules in a strategy σ other than the one for D. We say
that a decision rule δD is locally optimal for a strategy σ if, for any other decision rule δ′
D,
locally optimal
decision rule
EU[I[(σ−D, δD)]] ≥EU[I[(σ−D, δ′
D)]].
Our algorithm starts with some strategy σ, and then iterates over diﬀerent decision variables
D. It then selects a locally optimal decision rule δD for σ, and updates σ by replacing σD with
the new δD. Note that the inﬂuence diagram I[σ−D] is an inﬂuence diagram with the single
decision variable D, which can be solved using a variety of methods, as described earlier. The
algorithm terminates when no decision rule can be improved by this process.
Perhaps the most important property of this algorithm is its ability to deal with the main
computational limitation of the simple variable elimination strategy described in section 23.3.2:
the fact that the constrained variable elimination ordering can require creation of large factors
even when the network structure does not force them.
Example 23.17
Consider again example 23.11; here, we would begin with some set of decision rules for all of
D1, . . . , Dk. We would then iteratively compute the expected utility factor µ−Di for one of the
Di variables, using the (current) decision rules for the others. We could then optimize the decision
rule for Di, and continue the process. Importantly, the only constraint on the variable elimination
ordering is that Di and its parents be eliminated last. With these constraints, the largest factor
induced in any of these variable elimination procedures has size 4, avoiding the exponential blowup
in k that we saw in example 23.11.
In a naive implementation, the algorithm runs variable elimination multiple times — once for
each iteration — in order to compute µ−Di. However, using the approach of joint (probability,
utility) factors, as described in section 23.4.3, we can provide a very eﬃcient implementation as
a clique tree. See exercise 23.10.
So far, we have ignored several key questions that aﬀect both the algorithm’s complexity and
its correctness. Most obviously, we can ask whether this iterative algorithm even converges.
When we optimize D, we either improve the agent’s overall expected utility or we leave the

1110
Chapter 23. Structured Decision Problems
decision rule unchanged. Because the expected utility is bounded from above and the total
number of strategies is discrete, the algorithm cannot improve the expected utility indeﬁnitely.
Thus, at some point, no additional improvements are possible, and the algorithm will terminate.
A second question relates to the quality of the solution obtained. Clearly, this solution is locally
optimal, in that no change to a single decision rule can improve the agent’s expected utility.
However, local optimality does not, in general, imply that the strategy is globally optimal.
Example 23.18
Consider an inﬂuence diagram containing only two decision variables D1 and D2, and a utility
variable V (D1, D2) deﬁned as follows:
V (d1, d2) =



2
d1 = d2 = 1
1
d1 = d2 = 0
0
d1 ̸= d2.
The strategy (0, 0) is locally optimal for both decision variables, since the unique optimal decision
for Di when Dj = 0 (j ̸= i) is Di = 0. On the other hand, the globally optimal strategy is (1, 1).

However, under certain conditions, local optimality does imply global optimality, so
that the iterated optimization process is guaranteed to converge to a globally optimal
solution. These conditions are more general than perfect recall, so that this algorithm
works in every case where the algorithm of the previous section applies. In this case,
we can provide an ordering for applying the local optimization steps that guarantees
that this process converges to the globally optimal strategy after modifying each decision
rule exactly once. However, this algorithm also applies to networks that do not satisfy
the perfect recall assumption, and in certain such cases it is even guaranteed to ﬁnd
an optimal solution. By relaxing the perfect recall assumption, we can avoid some of the
exponential blowup of the decision rules in terms of the number of decisions in the network.
23.5.3
Strategic Relevance and Global Optimality ⋆
The algorithm described before iteratively changes the decision rule associated with individual
decision variables.
In general, changing the decision rule for one variable D′ can cause a
decision rule previously optimal for another variable D to become suboptimal. Therefore, the
algorithm must revisit D and possibly select a new decision rule for it. In this section, we
provide conditions under which we can guarantee that changing the decision rule for D′ will
not necessitate a change in the decision rule for D. In other words, we deﬁne conditions under
which the decision rule for D′ may not be relevant for optimizing the decision rule for D. Thus,
if we choose a decision rule for D and later select one for D′, we do not have to revisit the
selection made for D. As we show, under certain conditions, this criterion allows us to optimize
all of the decision rules using a single iteration through them.
23.5.3.1
Strategic Relevance
Intuitively, we would like to deﬁne a decision variable D′ as strategically relevant to D if, to
optimize the decision rule at D, the decision maker needs to consider the decision rule at D′.
That is, we want to say that D′ is is relevant to D if there is a partial strategy proﬁle σ over

23.5. Optimization in Inﬂuence Diagrams
1111
D −{D, D′}, two decision rules δD′ and δ′
D′, and a decision rule δD, such that (σ, δD, δD′) is
optimal, but (σ, δD, δ′
D′) is not.
Example 23.19
Consider a simple inﬂuence diagram where we have two decision variables D1 →D2, and a utility
V (D1, D2) that is the same as the one used in example 23.18. Pick an arbitrary decision rule δD1
(not necessarily deterministic), and consider the problem of optimizing δD2 relative to δD1. The
overall expected utility for the agent is
X
d1
δD1(d1)
X
d2
δD2(d2 | d1)V (d1, d2).
An optimal decision for D2 given the information state d1 is arg maxd2 V (d1, d2), regardless of
the choice of decision rule for D1. Thus, in this setting, we can pick an arbitrary decision rule δD1
and optimize δD2 relative to it; our selected decision rule will then be locally optimal relative to any
decision rule for D1. However, there is a subtlety that makes the previous statement false in certain
settings. Let δ′
D1 = d0
1. Then one optimal decision rule for D2 is δ′
D2(d0
1) = δ′
D2(d1
1) = d0
2.
Clearly, d0
2 is the right choice when D1 = d0
1, but it is suboptimal when D1 = d1
1. However,
because δD1 gives this latter event probability 0, this choice for δD2 is locally optimal relative to
δD1.
As this example shows, a decision rule can make arbitrary choices in information states that
have probability zero without loss in utility. In particular, because δ′
D1 assigns probability zero
to d1
1, the “suboptimal” δ′
D2 is locally optimal relative to δ′
D1; however, δ′
D2 is not locally optimal
relative to other decision rules for D1. Thus, if we use the previous deﬁnition, D1 appears
relevant to D2 despite our intuition to the contrary. We therefore want to avoid probability-zero
events, which allow situations such as this. We say that a decision rule is fully mixed if each
fully mixed
decision rule
probability distribution δD(D | paD) assigns nonzero probability to all values of D. We can
now formally deﬁne strategic relevance.
Deﬁnition 23.7
Let D and D′ be decision nodes in an inﬂuence diagram I. We say that D′ is strategically relevant
strategic
relevance
to D (or that D strategically relies on D′) if there exist:
• a partial strategy proﬁle σ over D −{D, D′};
• two decision rules δD′ and δ′
D′ such that δD′ is fully mixed;
• a decision rule δD that is optimal for (σ, δD′) but not for (σ, δ′
D′).
This deﬁnition does not provide us with an operative procedure for determining relevance.
We can obtain such a procedure by considering an alternative mathematical characterization of
the notion of local optimality.
Proposition 23.3
Let δD be a decision rule for a decision variable D in I, and let σ be a strategy for I. Then δD
is locally optimal for σ if and only if for every instantiation w of PaD where PBI[σ](w) > 0, the
probability distribution δD(D | w) is a solution to
arg max
q(D)
X
d∈Val(D)
q(d)
X
V ∈U≻D
X
v∈Val(V )
PBI[σ](v | d, w) · v,
(23.12)
where U≻D is the set of utility nodes in U that are descendants of D in I.

1112
Chapter 23. Structured Decision Problems
D
D´
D
D´
V
Dˆ
ˆD´
V
Dˆ
ˆD´
(a)
(b)
Figure 23.7
Inﬂuence diagrams, augmented to test for s-reachability
The proof is left as an exercise (exercise 23.6).
The signiﬁcance of this result arises from two key points. First, the only probability expres-
sions appearing in the optimization criterion are of the form PBI[σ](V | FamilyD) for some
utility variable V and decision variable D. Thus, we care about a decision rule δD′ only if the
CPD induced by this decision rule aﬀects the value of one of these probability expressions. Sec-
ond, the only utility variables that participate in these expressions are those that are descendants
of D in the network.
23.5.3.2
S-Reachability
We have reduced our problem to one of determining which decision rule CPDs might aﬀect the
value of some expression PBI[σ](V | FamilyD), for V ∈U≻D. In other words, we need to
determine whether the decision variable is a requisite CPD for this query. We also encountered
requisite CPD
this question in a very similar context in section 21.3.1, when we wanted to determine whether
an intervention (that is, a decision) was relevant to a query. As described in exercise 3.20, we can
determine whether the CPD for a variable Z is requisite for answering a query P(X | Y ) with
a simple graphical criterion: We introduce a new “dummy” parent bZ whose values correspond
to diﬀerent choices for the CPD of Z. Then Z is a requisite probability node for P(X | Y ) if
and only if bZ has an active trail to X given Y .
Based on this concept and equation (23.12), we can deﬁne s-reachability — a graphical
criterion for detecting strategic relevance.
Deﬁnition 23.8
A decision variable D′ is s-reachable from a decision variable D in an ID I if there is some utility
s-reachable
node V ∈U≻D such that if a new parent c
D′ were added to D′, there would be an active path in
I from c
D′ to V given FamilyD, where a path is active in an ID if it is active in the same graph,
viewed as a BN.
Note that unlike d-separation, s-reachability is not necessarily a symmetric relation.
Example 23.20
Consider the simple inﬂuence diagrams in ﬁgure 23.7, representing example 23.19 and example 23.18
respectively. In (a), we have a perfect-recall setting. Because the agent can observe D when deciding

23.5. Optimization in Inﬂuence Diagrams
1113
on the decision rule for D′, he does not need to know the decision rule for D in order to evaluate
his options at D′. Thus, D′ does not strategically rely on D. Indeed, if we add a dummy parent
bD to D, we have that V is d-separated from bD given FamilyD′ = {D, D′}. Thus, D is not
s-reachable from D′. Conversely, the agent’s decision rule at D′ does inﬂuence his payoﬀat D,
and so D′ is relevant to D. Indeed, if we add a dummy parent c
D′ to D′, we have that V is not
d-separated from c
D′ given D, PaD.
By contrast, in (b), the agent forgets his action at D when observing D′; as his utility node is
inﬂuenced by both decisions, we have that each decision is relevant to the other. The s-reachability
analysis using d-separation from the dummy parents supports this intuition.
The notion of s-reachability is sound and complete for strategic relevance (almost) in the same
sense that d-separation is sound and complete for independence in Bayesian networks. As for
d-separation, the soundness result is very strong: without s-reachability, one decision cannot be
relevant to another.
Theorem 23.2
If D and D′ are two decision nodes in an ID I and D′ is not s-reachable from D in I, then D
does not strategically rely on D′.
Proof Let σ be a strategy proﬁle for I, and let δD be a decision rule for D that is optimal for
σ. Let B = BI[σ]. By proposition 23.3, for every w ∈Val(PaD) such that PB(w) > 0, the
distribution δD(D | w) must be a solution of the maximization problem:
arg max
P (D)
X
d∈Val(D)
P(d)
X
V ∈U≻D
X
v∈Val(V )
PB(v | d, w) · v.
(23.13)
Now, let σ′ be any strategy proﬁle for I that diﬀers from σ only at D′, and let B′ = BI[σ′].
We must construct a decision rule δ′
D for D that agrees with δD on all w where PB(w) > 0,
and that is optimal for σ′. By proposition 23.3, it suﬃces to show that for every w where
PB′(w) > 0, δ′
D(D | w) is a solution of:
arg max
P (D)
X
d∈Val(D)
P(d)
X
V ∈U≻D
X
v∈Val(V )
PB′(v | d, w) · v.
(23.14)
If PB(w) = 0, then our choice of δ′
D(D | w) is unconstrained; we can simply select a
distribution that satisﬁes equation (23.14). For other w, we must let δ′
D(D | w) = δD(D | w).
We know that δD(D | w) is a solution of equation (23.13), and the two expressions are diﬀerent
only in that equation (23.13) uses PB(v | d, w) and equation (23.14) uses PB′(v | d, w). The
two networks B and B′ diﬀer only in the CPD for D′. Because D′ is not a requisite probability
node for any V ∈U≻D given D, PaD, we have that PB(v | d, w) = PB′(v | d, w), and that
δ′
D(D | w) = δD(D | w) is a solution of equation (23.14), as required.
Thus, s-reachability provides us with a sound criterion for determining which decision vari-
ables D′ are strategically relevant for D. As for d-separation, the completeness result is not as
strong: s-reachability does not imply relevance in every ID. We can choose the probabilities and
utilities in the ID in such a way that the inﬂuence of one decision rule on another does not
manifest itself. However, s-reachability is the most precise graphical criterion we can use: it will
not identify a strategic relevance unless that relevance actually exists in some ID that has the
given graph structure.

1114
Chapter 23. Structured Decision Problems
D
D´
V
D
D´
V
D
D´
V
D
D´
V
D
D´
D
(a)
(b)
(c)
(d)
D´
D
D´
D
D´
X
X
Figure 23.8
Four simple inﬂuence diagrams (top), and their relevance graphs (bottom).
Theorem 23.3
If a node D′ is s-reachable from a node D in an ID, then there is some ID with the same graph
structure in which D strategically relies on D′.
This result is roughly analogous to theorem 3.4, which states that there exists some parame-
terization that manifests the dependencies not induced by d-separation. A result analogous to
the strong completeness result of theorem 3.5 is not known for this case.
23.5.3.3
The Relevance Graph
We can get a global view of the strategic dependencies between diﬀerent decision variables in
an inﬂuence diagram by putting them within a single graphical data structure.
Deﬁnition 23.9
The relevance graph for an inﬂuence diagram I is a directed (possibly cyclic) graph whose nodes
relevance graph
correspond to the decision variables D in I, and where there is a directed edge D′ →D if D′ is
strategically relevant to D.
To construct the graph for a given ID, we need to determine, for each decision node D, the
set of nodes D′ that are s-reachable from D. Using standard methods from chapter 3, we can
ﬁnd this set for any given D in time linear in the number of chance and decision variables
in the ID. By repeating the algorithm for each D, we can derive the relevance graph in time
O((n + k)k) where n = |X| and k = |D|.
Recall our original statement that a decision node D strategically relies on a decision node
D′ if one needs to know the decision rule for D′ in order to evaluate possible decision rules for

23.5. Optimization in Inﬂuence Diagrams
1115
D. Intuitively, if the relevance graph is acyclic, we have a decision variable that has no parents
in the graph, and hence relies on no other decisions. We can optimize the decision rule at
this variable relative to some arbitrary strategy for the other decision rules. Having optimized
that decision rule, we can ﬁx its strategy and proceed to optimize the next one. Conversely, if
we have a cycle in the relevance graph, then we have some set of decisions all of which rely
on each other, and their decision rules need to be optimized together. In this case, the simple
iterative approach we described no longer applies.
However, before we describe this iterative algorithm formally and prove its correctness, it is
instructive to examine some simple IDs and see when one decision node relies on another.
Example 23.21
Consider the four examples shown in ﬁgure 23.8, all of which relate to a setting where the agent
ﬁrst makes decision D and then D′. Examples (a) and (b) are the ones we previously saw in
example 23.20, showing the resulting relevance graphs. As we saw, in (a), we have that D relies
on D′ but not vice versa, leading to the structure shown on the bottom. In (b) we have that each
decision relies on the other, leading to a cyclic relevance graph. Example (c) represents a situation
where the agent does not remember D when making the decision D′. However, the agent knows
everything he needs to about D: his utility does not depend on D directly, but only on the chance
node, which he can observe. Hence D′ does not rely on D.
One might conclude that a decision node D′ never relies on another D when D is observed by
D′, but the situation is subtler. Consider example (d), which represents a simple card game: the
agent observes a card and decides whether to bet (D); at a later stage, the agent remembers only his
bet but not the card, and decides whether to raise his bet (D′); the utility of both depends on the
total bet and the value of the card. Even though the agent does remember the actual decision at D,
he needs to know the decision rule for D in order to know what the value of D tells him about the
value of the card. Thus, D′ relies on D; indeed, when D is observed, there is an active trail from a
hypothetical parent ˆD that runs through the chance node to the utility node.
However, it is the case that perfect recall — remembering both the previous decisions and
the previous observations, does imply that the underlying relevance graph is acyclic.
Theorem 23.4
Let I be an inﬂuence diagram satisfying the perfect recall assumption. Then the relevance graph
for I is acyclic.
The proof follows directly from properties of d-separation, and it is left as an exercise (exer-
cise 23.7). We note that the ordering of the decisions in the relevance graph will be the opposite
of the ordering in the original ID, as in ﬁgure 23.8a.
23.5.3.4
Global Optimality
Using the notion of a relevance graph, we can now provide an algorithm that, under certain
conditions, is guaranteed to ﬁnd an MEU strategy for the inﬂuence diagram.
In particular,
consider an inﬂuence diagram I whose relevance graph is acyclic, and let D1, . . . , Dk be a
topological ordering of the decision variables according to the relevance graph. We now simply
execute the algorithm of section 23.5.2 in the order D1, . . . , Dk.
Why does this algorithm guarantee global optimality of the inferred strategy? When selecting
the decision rule for Di, we have two cases: for j < i, by induction, the decision rules for Dj

1116
Chapter 23. Structured Decision Problems
Algorithm 23.3 Iterated optimization for inﬂuence diagrams with acyclic relevance graphs
Procedure Iterated-Optimization-for-IDs (
I,
// Inﬂuence diagram
G
// Acyclic relevance graph for I
)
1
Let D1, . . . , Dk be an ordering of D that is a topological ordering for G
2
Let σ0 be some fully mixed strategy for I
3
for i = 1, . . . , k
4
Choose δDi to be locally optimal for σi−1
5
σi ←(σi−1
−Di, δDi)
6
return σk
H3, D3
D4, M
H2, H3
D3, M
H2, D2
D3, M
H1, H2
D2, M
H1, D1
D2, M
H4
D4, M
Figure 23.9
Clique tree for the imperfect-recall inﬂuence diagram of ﬁgure 23.5. Although the
network has many cascaded decisions, our ability to “forget” previous decisions allows us to solve the
problem using a bounded tree-width clique tree.
are already stable, and so will never need to change; for j > i, the decision rules for Dj are
irrelevant, so that changing them will not require revisiting Di.
One subtlety with this argument relates, once again, to the issue of probability-zero events. If
our arbitrary starting strategy σ assigns probability zero to a certain decision d ∈Val(D) (in
some setting), then the local optimization of another decision rule D′ might end up selecting a
suboptimal decision for the zero probability cases. If subsequently, when optimizing the decision
rule for D, we ascribe nonzero probability to D = d, our overall strategy will not be optimal.
To avoid this problem, we can use as our starting point any fully mixed strategy σ. One obvious
choice is simply the strategy that, at each decision D and for each assignment to PaD, selects
uniformly at random between all of the possible values of D.
The overall algorithm is shown in algorithm 23.3.
Theorem 23.5
Applying Iterated-Optimization-for-IDs on an inﬂuence diagram I whose relevance graph is acyclic,
returns a globally optimal strategy for I.
The proof is not diﬃcult and is left as an exercise (exercise 23.8).
Thus, this algorithm, by iteratively optimizing individual decision rules, ﬁnds a globally optimal
solution. The algorithm applies to any inﬂuence diagram whose relevance graph is acyclic, and
hence to any inﬂuence diagrams satisfying the perfect recall assumption. Hence, it is at least as
general as the variable elimination algorithm of section 23.3. However, as we saw, some inﬂuence
diagrams that violate the perfect recall assumption have acyclic relevance graphs nonetheless;
this algorithm also applies to such cases.
Example 23.22
Consider again the inﬂuence diagram of example 23.11. Despite the lack of perfect recall in this

23.5. Optimization in Inﬂuence Diagrams
1117
network, an s-reachability analysis shows that each decision variable Di strategically relies only
on Dj for j > i. For example, if we add a dummy parent c
D1 to D1, we can verify that it is
d-separated from V3 and V4 given PaD3 = {H2, D2}, so that the resulting relevance graph is
acyclic.
The ability to deal with problems where the agent does not have to remember his entire history
can provide very large computational savings in large problems. Speciﬁcally, we can solve this
inﬂuence diagram using the clique tree of ﬁgure 23.9, at a cost that grows linearly rather than
exponentially in the number of decision variables.
This algorithm is guaranteed to ﬁnd a globally optimal solution only in cases where the rele-
vance graph is acyclic. However, we can extend this algorithm to ﬁnd a globally optimal solution
in more general cases, albeit at some computational cost. In this extension, we simultaneously
optimize the rules for subsets of interdependent decision variables. Thus, for example, in ex-
ample 23.18, we would optimize the decision rules for D1 and D2 together, rather than each in
isolation. (See exercise 23.9.) This approach is guaranteed to ﬁnd the globally optimal strategy,
but it can be computationally expensive, depending on the number of interdependent decisions
that must be considered together.
Box 23.B — Case Study: Coordination Graphs for Robot Soccer. One subclass of problem in
decision making is that of making a joint decision for a team of agents with a shared utility
function. Let the world state be deﬁned by a set of variables X = {X1, . . . , Xn}. We now have a
team of m agents, each with a decision variable Ai. The team’s utility function is described by a
function U(X, A) (for A = {A1, . . . , Am}). Given an assignment x to X1, . . . , Xn, our goal is
to ﬁnd the optimal joint action arg maxa U(x, a).
joint action
In a naive encoding, the representation of the utility function grows exponentially both in the
number of state variables and in the number of agents. However, we can come up with more
eﬃcient algorithms by exploiting the same type of factorization that we have utilized so far. In
particular, we assume that we can decompose U as a sum of subutility functions, each of which
depends only on the actions of some subset of the agents. More precisely,
U(X1, . . . , Xn, A1, . . . , Am) =
X
i
Vi(Xi, Ai),
where Vi is some subutility function with scope Xi, Ai.
This optimization problem is simply a max-sum problem over a factored function, a problem
that is precisely equivalent to the MAP problem that we addressed in chapter 13. Thus, we can
apply any of the algorithms we described there. In particular, max-sum variable elimination can
be used to produce optimal joint actions, whereas max-sum belief propagation can be used to
construct approximate max-marginals, which we can decode to produce approximate solutions.
The application of these message passing algorithms in this type of distributed setting is satisfying,
since the decomposition of the utility function translates to a limited set of interactions between
agents who need to coordinate their choice of actions. Thus, this approach has been called a
coordination graph.
coordination
graph
Kok, Spaan, and Vlassis (2003), in their UvA (Universiteit van Amsterdam) Trilearn team, applied
coordination graphs to the RoboSoccer domain, a particularly challenging application of decision
RoboSoccer

1118
Chapter 23. Structured Decision Problems
making under uncertainty.
RoboSoccer is an annual event where teams of real or simulated
robotic agents participate in a soccer competition. This application requires rapid decision making
under uncertainty and partial observability, along with coordination between the diﬀerent team
members. The simulation league allows teams to compete purely on the quality of their software,
eliminating the component of hardware design and maintenance. However, key challenges are
faithfully simulated in this environment. For example, each agent can sense its environment via
only three sensors: a visual sensor, a body sensor, and an aural sensor. The visual sensor measures
relative distance, direction, and velocity of the objects in the player’s current ﬁeld of view. Noise is
added to the true quantities and is larger for objects that are farther away. The agent has only a
partial view of the world and needs to take viewing actions (such as turning its neck) deliberately in
order to view other parts of the ﬁeld. Players in the simulator have diﬀerent abilities; for example,
some can be faster than others, but they will also tire more easily. Overall, this tournament provides
a challenge for real-time, multiagent decision-making architectures.
Kok et al. hand-coded a set of utility rules, each of which represents the incremental gain or loss
to the team from a particular combination of joint actions. At every time point t they instantiate
the variables representing the current state and solve the resulting coordination graph. Note that
there is no attempt to address the problem of sequential decision making, where our choice of action
at time t should consider its eﬀect on actions at subsequent time points. The myopic nature of the
decision making is based on the assumption that the rules summarize the long-term beneﬁt to the
team from a particular joint action.
To apply this framework in this highly dynamic, continuous setting, several adaptations are
required. First, to reduce the set of possible actions that need to be considered, each agent is assigned
a role: interceptor, passer, receiver, or passive. The assignment of roles is computed directly from the
current state information. For example, the fastest player close to the ball will be assigned the passer
role when he is able to kick the ball, and the interceptor role otherwise. The assignment of roles
deﬁnes the structure of the coordination graph: interceptors, passers, and receivers are connected,
whereas passive agents do not need to be considered in the joint-action selection process. The roles
also determine the possible actions for each agent, which are discrete, high-level actions such as
passing a ball to another agent in a given direction. The state variables are also deﬁned as a high-
level abstraction of the continuous game state; for example, there is a variable pass-blocked(i, j, d)
that indicates whether a pass from agent i to agent j in direction d is blocked by an opponent.
With this symbolic representation, one can write value rules that summarize the value gained by a
particular combination of actions. For example, one rule says:
[has-role-receiver(j) ∧¬isPassBlocked(i, j, d) ∧Ai = passTo(j, d) ∧Aj = moveTo(d) : V (j, d)]
where V (j, d) depends on the position where the receiving agent j receive the pass — the closer to
the opponent goal the better.
A representation of a utility function as a set of rules is equivalent to a feature-based represen-
tation of a Markov network. To perform the optimization eﬃciently using this representation, we
can easily adapt the rule-based variable-elimination scheme described in section 9.6.2.1. Note that
the actual rules used in the inference are considerably simpler, since they are conditioned on the
state variables, which include the role assignment of the agents and the other aspects of the state
(such as isPassBlocked). However, this requirement introduces other complications: because of the
limited communication bandwidth, each agent needs to solve the coordination graph on its own.
Moreover, the state of the world is not generally fully observed to the agent; thus, one needs to ensure

23.6. Ignoring Irrelevant Information ⋆
1119
that the agents take the necessary observation actions (such as turning the neck) to obtain enough
information to condition the relevant state variables. Depending on the number of agents and their
action space, one can now solve this problem using either variable elimination or belief propagation.
The coordination graph framework allows the diﬀerent agents in the team to conduct complex
maneuvers, an agent j would move to receive a pass from agent i even before agent i was in
position to kick the ball; by contrast, previous methods required j to observe the trajectory of the
ball before being able to act accordingly. This approach greatly increased the capabilities of the UVA
Trilearn team. Whereas their entry took fourth place in the RoboSoccer 2002 competition, in 2003
it took ﬁrst place among the forty-six qualifying team, with a total goal count of 177–7.
23.6
Ignoring Irrelevant Information ⋆
As we saw, there are several signiﬁcant advantages to reducing the amount of information that
the agent considers at each decision. Eliminating an information edge from a variable W into
a decision variable D reduces the complexity of its decision rule, and hence the cognitive load
on the decision maker. Computationally, it decreases the cost of manipulating its factor and of
computing the decision rule. In this section, we consider a procedure for removing information
edges from an inﬂuence diagram.
Of course, removing information edges reduces the agent’s strategy space, and therefore can
potentially signiﬁcantly decrease his maximum expected utility value. If we want to preserve the
agent’s MEU value, we need to remove information edges with care. We focus here on removing
only information edges that do not reduce the agent’s MEU. We therefore study when a variable
W ∈PaD is irrelevant to making the optimal decision at D. In this section, we provide a
graphical criterion for guaranteeing that W is irrelevant and can be dropped without penalty
from the set PaD.
Intuitively, W is not relevant when it has no eﬀect on utility nodes that participate in
determining the decision at D.
Example 23.23
Consider the inﬂuence diagram IS of ﬁgure 23.10. Intuitively, the edge from Diﬃculty (D) to Apply
(A) is irrelevant. To understand why, consider its eﬀect on the diﬀerent utility variables in the
network. On one hand, it inﬂuences VS; however, given the variable Grade, which is also observed
at A, D is irrelevant to VS. On the other hand, it inﬂuences VQ; however, VQ cannot be inﬂuenced
by the decision at A, and hence is not considered by the decision maker when determining the
strategy at A. Overall, D is irrelevant to A given A’s other parents.
We can make this intuition precise as follows:
Deﬁnition 23.10
An information edge W →D from a (chance or decision) variable W is irrelevant for a decision
irrelevant
information edge
variable D if there is no active trail from W to U≻D given PaD −{W}.
According to this criterion, D is irrelevant for A, supporting our intuitive argument. We note
that certain recall edges can also be irrelevant according to this deﬁnition. For example, assume
that we add an edge from Diﬃculty to the decision variable Take. The Diﬃculty →Take edge

1120
Chapter 23. Structured Decision Problems
Job
Grade
Rumors
Letter
Intelligence
Difﬁculty
Apply
Take
VS
VQ
Figure 23.10
More complex inﬂuence diagram IS for the Student scenario. Recall edges that follow
from the deﬁnition are omitted for clarity.
is not irrelevant, but the Diﬃculty →Apply edge, which would be implied by perfect recall, is
irrelevant.
We can show that irrelevant edges can be removed without penalty from the network.
Proposition 23.4
Let I be an inﬂuence diagram, and W →D an irrelevant edge in I. Let I′ be the inﬂuence
diagram obtained by removing the edge W →D. Then for any strategy σ in I, there exists a
strategy σ′ in I′ such that EU[I′[σ′]] ≥EU[I[σ]].
The proof follows from proposition 23.3 and is left as an exercise (exercise 23.11).
An inﬂuence diagram I′ that is obtained from I via the removal of irrelevant edges is called
a reduction of I. An immediate consequence of proposition 23.4 is the following result:
reduction
Theorem 23.6
If I′ is a reduction of I, then any strategy σ that is optimal for I′ is also optimal for I.
The more edges we remove from I, the simpler our computational problem. We would thus
like to ﬁnd a reduction that has the fewest possible edges. One simple method for obtaining
a minimal reduction — one that does not admit the removal of any additional edges — is to
remove irrelevant edges iteratively from the network one at a time until no further edges can
be removed. An obvious question is whether the order in which edges are removed makes a
diﬀerence to the ﬁnal result. Fortunately, the following result implies otherwise:
Theorem 23.7
Let I be an inﬂuence diagram and I′ be any reduction of it. An arc W →D in I′ is irrelevant
in I′ if and only if it is irrelevant in I.
The proof follows from properties of d-separation, and it is left as an exercise (exercise 23.12).

23.7. Value of Information
1121
This theorem implies that we can examine each edge independently, and test whether it is
irrelevant in I. All such edges can then be removed at once. Thus, we can ﬁnd all irrelevant
edges using a single global computation of d-separation on the original ID.
The removal of irrelevant edges has several important computational beneﬁts. First, it de-
creases the size of the strategy representation in the ID. Second, by removing edges in the
network, it can reduce the complexity of the variable-elimination-based algorithms described in
section 23.5. Finally, as we now show, it also has the eﬀect of removing edges from the relevance
graph associated with the ID. By breaking cycles in the relevance graph, it allows more decision
rules to be optimized in sequence, reducing the need for iterations or for jointly optimizing the
decision rules at multiple variables.
Proposition 23.5
If I′ is a reduction of I, then the relevance graph of I′ is a subset (not necessarily strict) of the
relevance graph of I.
Proof It suﬃces to show the result for the case where I′ is a reduction of I by a single
irrelevant edge. We will show that if D′ is not s-reachable from D in I, then it is also not
s-reachable from D in I′. If D′ is s-reachable from D in I′, then for a dummy parent c
D′, we
have that there is some V ∈U≻D and an active trail in I′ from bD to V given D, PaI′
D . By
assumption, that same trail is not active in I. Since removal of edges cannot make a trail active,
this situation can occur only if PaI′
D = PaI
D −{W}, and W blocks the trail from c
D′ to V in
I. Because observing W blocks the trail, it must be part of the trail, in which case there is a
subtrail from W to V in I. This subtrail is active given (PaI
D −{W}), D. However, observing
D cannot activate a trail where we condition on D’s parents (because then v-structures involving
D are blocked). Thus, this subtrail must form an active trail from W to V given PaI
D −{W},
violating the assumption that W →D is an irrelevant edge.
23.7
Value of Information
So far, we have focused on the problem of decision making. Inﬂuence diagrams provide us
with a representation for structured decision problems, and a basis for eﬃcient decision-making
algorithms.
One particularly useful type of task, which arises in a broad range of applications, is that
of determining which variables we want to observe. Most obviously, in any diagnostic task,
we usually have a choice of diﬀerent tests we can perform.
Because tests usually come at
a cost (whether monetary or otherwise), we want to select the tests that are most useful in
our particular setting. For example, in a medical setting, a diagnostic test such as a biopsy
may involve signiﬁcant pain to the patient and risk of serious injury, as well as high monetary
costs. In other settings, we may be interested in determining if and where it is worthwhile to
place sensors — such as a thermostat or a smoke alarm — so as to provide the most useful
information in case of a ﬁre.

The decision-theoretic framework provides us with a simple and elegant measure for
the value of making a particular observation. Moreover, the inﬂuence diagram represen-
tation allows us to formulate this measure using a simple, graph-based criterion, which
also provides considerable intuition.

1122
Chapter 23. Structured Decision Problems
23.7.1
Single Observations
We begin with the question of evaluating the beneﬁt of a single observation. In the setting of
inﬂuence diagrams, we can model this question as one of computing the value of observing the
value of some variable. Our Survey variable in the Entrepreneur example is precisely such a
situation. Although we could (and did) analyze this type of decision using our general framework,
it is useful to consider such decisions as a separate (and simpler) class. By doing so, we can
gain insight into questions such as these.
The key idea is that the beneﬁt of making an observation is the utility the agent can gain by
observing the associated variable, assuming he acts optimally in both settings.
Example 23.24
Let us revisit the Entrepreneur example, and consider the value to the entrepreneur of conducting
the survey, that is, of observing the value of the Survey variable. In eﬀect, we are comparing two
scenarios and the utility to the entrepreneur in each of them: One where he conducts the survey,
and one where he does not. If the agent does not observe the S variable, that node is barren in
the network, and it can therefore be simply eliminated. This would result precisely in the inﬂuence
diagram of ﬁgure 23.2. In example 22.3 we analyzed the agent’s optimal action in this setting and
showed that his MEU is 2. The second case is one in which the agent conducts the survey. This
situation is equivalent to the inﬂuence diagram of ﬁgure 23.3, where we restrict to strategies where
C = c1. As we have already discussed, C = c1 is the optimal strategy in this setting, so that
the optimal utility obtainable by the agent in this situation is 3.22, as computed in example 23.6.
Hence, the improvement in the entrepreneur’s utility, assuming he acts optimally in both cases, is
1.22.
More generally, we deﬁne:
Deﬁnition 23.11
Let I be an inﬂuence diagram, X a chance variable, and D a decision variable such that there
is no (causal) path from D to X. Let I′ be the same as I, except that we add an information
edge from X to D, and to all decisions that follow D (that is, we have perfect information about
X from D onwards). The value of perfect information for X at D, denoted VPII(D | X), is the
value of perfect
information
diﬀerence between the MEU of I′ and the MEU of I.
Let us analyze the concept of value of perfect information. First, it is not diﬃcult to see that
it cannot be negative; if the information is free, it cannot hurt to have it.
Proposition 23.6
Let I be an inﬂuence diagram, D a decision variable in I, and X a chance variable that is a
nondescendant of D. Let σ∗be the optimal strategy in I. Then VPII(D | X) ≥0, and equality
holds if and only if σ∗is still optimal in the new inﬂuence diagram with X as a parent of D.
The proof is left as an exercise (exercise 23.13).
Does information always help? What if the numbers had been such that the entrepreneur
would have founded the company regardless of the survey? In that case, the expected utility
with the survey and without it would have been identical; that is, the VPI of S would have
been zero. This property is an important one: there is no value to information if it does not

change the selected action(s) in the optimal strategy.
Let us analyze more generally when information helps.
To do that, consider a diﬀerent
decision problem.

23.7. Value of Information
1123
Funding1
Company
Funding2
State1
State2
V
Figure 23.11
Inﬂuence diagram for VPI computation in example 23.25. We can compute the value of
information for each of the two State variables by comparing the value with/without the dashed information
edges.
Example 23.25
Our budding entrepreneur has decided that founding a startup is not for him. He is now choosing
between two job opportunities at existing companies. Both positions are oﬀering a similar starting
salary, so his utility depends on his salary a year down the road, which depends on whether the
company is still doing well at that point. The agent has the option of obtaining some information
about the current state of the companies.
More formally, the entrepreneur has a decision variable C, whose value ci is accepting a job
with company i (i = 1, 2). For each company, we have a variable Si that represents the current
state of the company (quality of the management, the engineering team, and so on); this value takes
three values, with s3
i being a very high-quality company and s1
i a poor company. We also have a
binary-valued variable Fi, which represents the funding status of the company in the future, with
f 1
i representing the state of having funding. We assume that the utility of the agent is 1 if he
takes a job with a company for which Fi = f 1
i and 0 otherwise. We want to evaluate the value of
information of observing S1 (the case of observing S2 is essentially the same). The structure of the
inﬂuence diagram is shown in ﬁgure 23.11; the edges that would be added to compute the value of
information are shown as dashed.
We now consider three diﬀerent scenarios and compute the value of information in each of them.
Scenario 1: Company 1 is well established, whereas Company 2 is a small startup. Thus, P(S1) =
(0.1, 0.2, 0.7) (that is, P(s1
1) = 0.1), and P(S2) = (0.4, 0.5, 0.1).
The economic climate is
poor, so the chances of getting funding are not great. Thus, for both companies, P(f 1
i | Si) =
(0.1, 0.4, 0.9) (that is, P(f 1
i | s1
1) = 0.1). Without additional information, the optimal strategy is
c1, with MEU value 0.72. Intuitively, in this case, the information obtained by observing S1 does
not have high value. Although it is possible that c1 will prove less reliable than c2, this outcome
is very unlikely; with very high probability, c1 will turn out to be the better choice even with the
information. Thus, the probability that the information changes our decision is low, and the value
of the information is also low. More formally, a simple calculation shows that the optimal strategy
changes to c2 only if we observe s1
1, which happens with probability 0.1. The MEU value in this
scenario is 0.743, which is not a signiﬁcant improvement over our original MEU value. If observing

1124
Chapter 23. Structured Decision Problems
S1 costs more than 0.023 utility points, the agent should not make the observation.
Scenario 2: The economic climate is still bad, but now c1 and c2 are both small startups. In this
case, we might have P(S2) as in Scenario 1, and P(S1) = (0.3, 0.4, 0.3); P(Fi | Si) is also as in
Scenario 1. Intuitively, our value of information in this case is quite high. There is a reasonably high
probability that the observation will change our decision, and therefore a high probability that we
would gain a lot of utility by ﬁnding out more information and making a better decision. Indeed,
if we go through the calculation, the MEU strategy in the case without the additional observation is
c1, and the MEU value is 0.546. However, with the observation, we change our decision to c2 both
when S1 = s1
1 and when S1 = s2
1, events that are fairly probable. The MEU value in this case is
0.6882, a signiﬁcant increase over the uninformed MEU.
Scenario 3: In this case, c1 and c2 are still both small startups, but the time is the middle
of the Internet boom, so both companies are likely to be funded by investors desperate to get into
this area. Formally, P(S1) and P(S2) are as above, but P(f 1
i | Si) = (0.6, 0.8, 0.99). In this
case, the probability that the observation changes the agent’s decision is reasonably high, but the
change to the agent’s expected utility when the decision changes is low. Speciﬁcally, the uninformed
optimal strategy is c1, with MEU value 0.816. Observing s1
1 changes the decision to c2; but, while
this observation occurs with probability 0.3, the diﬀerence in the expected utility between the two
decisions in this case is less than 0.2. Overall, the MEU of the informed case is 0.8751, which is
not much greater than the uninformed MEU value.
Overall, we see that our deﬁnition of VPI allows us to make fairly subtle trade-oﬀs.
The value of information is critical in many applications. For example, in medical or fault
diagnosis, it often serves to tell us which diagnostic tests to perform (see box 23.C). Note that
its behavior is exactly appropriate in this case. We do not want to perform a test just because
it will help us narrow down the probability of the problem. We want to perform tests that will
change our diagnosis. For example, if we have an invasive, painful test that will tell us which
type of ﬂu a patient has, but knowing that does not change our treatment plan (lie in bed and
drink a lot of ﬂuids), there is no point in performing the test.
23.7.2
Multiple Observations
We now turn to the more complex setting where we can make multiple simultaneous observa-
tions. In this case, we must decide which subset of the m potentially observable variables we
choose to observe. For each such subset, we can evaluate the MEU value with the observations,
as in the single variable case, and select the subset whose MEU value is highest. However, this
approach is overly simplistic in several ways. First, the number of possible subsets of observa-
tions is exponentially large (2m). A doctor, for example, might have available a large number of
tests that she can perform, so the number of possible subsets of tests that she might select is
huge. Even if we place a bound on the number of observations that can be performed or on the
total cost of these observations, the number of possibilities can be very large.
More importantly, in practice, we often do not select in advance a set of observations to be
performed, and then perform all of them at once. Rather, observations are typically made in
sequence, so that the choice of which variable to observe next can be made with knowledge
about the outcome of the previous observations. In general, the value of an observation can
depend strongly on the outcome of a previous one. For example, in example 23.25, if we observe
that the current state of Company 1 is excellent — S1 = s3
1, observing the state of Company 2

23.7. Value of Information
1125
is signiﬁcantly less useful than in a situation where we observe that S1 = s2
1. Thus, the optimal
choice of variable to observe generally depends on the outcomes of the previous observations.
Therefore, when we have the ability to select a sequence of observations, the optimal selection
has the form of a conditional plan: Start by observing X1; if we observe X1 = x1
1, observe X2;
if we observe X1 = x2
1, observe X3; and so on. Each such plan is exponentially large in the
number k of possible observations that we are allowed to perform. The total number of such
plans is therefore doubly exponential. Selecting an optimal observation plan is computationally
a very diﬃcult task, for which no good algorithms exist in general.
The most common solution to this problem is to approximate the solution using myopic
myopic value of
information
value of information, where we incrementally select at each stage the optimal single observation,
ignoring its eﬀect on the later choices that we will have to make. The optimal single observation
can be selected easily using the methods described in the previous section.
This myopic
approximation can be highly suboptimal. For example, we might have an observation that, by
itself, provides very little information useful for our decision, but does tell us which of two other
observations is the most useful one to make.
In situations where the myopic approximation is complex, we can try to generate a condi-
tional plan, as described before. One approach for solving such a problem is to formulate it
as an inﬂuence diagram, with explicit decisions for which variable to observe. This type of
transformation is essentially the one underlying the very simple case of example 23.3, where
the variable C represents our decision on whether to observe S or not. The optimal strategy
for this extended inﬂuence diagram also speciﬁes the optimal observation plan. However, the
resulting inﬂuence diagram can be quite complex, and ﬁnding an optimal strategy for it can be
very expensive and often infeasible.
Box 23.C — Case Study: Decision Making for Troubleshooting. One of the most commonly
used applications of Bayesian network technology is to the task of fault diagnosis and repair. Here,
we construct a probabilistic model of the device in question, where random variables correspond
to diﬀerent faults and diﬀerent types of observations about the device state. Actions in this type of
domain correspond both to diagnostic tests that can help indicate where the problem lies, and to
actions that repair or replace a broken component. Both types of actions have a cost. One can now
apply decision-theoretic techniques to help select a sequence of observation and repair actions.
One of the earliest and largest ﬁelded applications of this type was the decision-theoretic trou-
bleshooting system incorporated into the Microsoft’s Windows 95TM operating system. The system,
described in Heckerman, Breese, and Rommelse (1995) and Breese and Heckerman (1996), included
hundreds of Bayesian networks, each aimed at troubleshooting a type of fault that commonly arises
in the system (for example, a failure in printing, or an application that does not launch). Each
fault had its own Bayesian network model, ranging in size from a few dozen to a few hundred
variables. To compute the probabilities required for an analysis involving repair actions, which
intervene in the model, one must take into account the fact that the system state from before the
repair also persists afterward (except for the component that was repaired). For this computation, a
counterfactual twinned network model was used, as described in box 21.C.
counterfactual
twinned network
The probabilistic models were augmented with utility models for observing the state of a com-
ponent in the system (that is, whether it is faulty) and for replacing it. Under carefully crafted
assumptions (such as a single fault hypothesis), it was possible to deﬁne an optimal series of re-

1126
Chapter 23. Structured Decision Problems
pair/observation actions, given a current state of information e, and thereby compute an exact
formula for the expected cost of repair ECR(e). (See exercise 23.15.) This formula could then be
used to compute exactly the beneﬁt of any diagnostic test D, using a standard value of information
computation:
X
d∈Val(D)
P(D = d | e)ECR(e, D = d).
One can then add the cost of the observation of D to choose the optimal diagnostic test. Note that
the computation of ECR(e, D = d) estimates the cost of the full trajectory of repair actions following
the observation, a trajectory that is generally diﬀerent for diﬀerent values of the observation d. Thus,
although this analysis is still myopic in considering only a single observation action D at a time, it
is nonmyopic in evaluating the cost of the plan of action following the observation.
Empirical results showed that this technique was very valuable. One test, for example, was applied
to the printer diagnosis network of box 5.A. Here, the cost was measured in terms of minutes to
repair. In synthetic cases with known failures, sampled from the network, the system saved about
20 percent of the time over the best predetermined plan. Interestingly, the system also performed
well, providing equal or even better savings, in cases where there were multiple faults, violating the
assumptions of the model.
At a higher level, decision-theoretic techniques are particularly valuable in this setting for several
reasons. The standard system used up to that point was a standard static ﬂowchart where the
answer to a question would lead to diﬀerent places in the ﬂowchart.
From the user side, the
experience was signiﬁcantly improved in the decision-theoretic system, since there was considerably
greater ﬂexibility: diagnostic tests are simply treated as observed variables in the network, so if a
user chooses not to answer a question at a particular point, the system can still proceed with other
questions or tests. Users also felt that the questions they were asked were intuitive and made sense
in context. Finally, there was also signiﬁcant beneﬁt for the designer of the system, because the
decision-theoretic system allowed modular and easily adaptable design. For example, if the system
design changes slightly, the changes to the corresponding probabilistic models are usually small (a
few CPDs may change, or maybe some variables are added/deleted); but the changes to the “optimal”
ﬂowchart are generally quite drastic. Thus, from a software engineering perspective, this approach
was also very beneﬁcial.
This application is one of the best-known examples of decision-theoretic troubleshooting, but
similar techniques have been successfully used in a large number of applications, including in a
decision-support system for car repair shops, in tools for printer and copier repair, and many others.
23.8
Summary
In this chapter, we placed the task of decision making using decision-theoretic principles within
the graphical modeling framework that underlies this entire book. Whereas a purely proba-
bilistic graphical model provides a factorized description of the probability distribution over
possible states of the world, an inﬂuence diagram provides such a factorized representation
for the agent’s actions and utility function as well. The inﬂuence diagram clearly encodes the

23.9. Relevant Literature
1127
breakdown of these three components of the decision-making situation into variables, as well
as the interactions between these variables. These interactions are both probabilistic, where one
variable aﬀects the distribution of another, and informational, where observing a variable allows
an agent to actively change his action (or his decision rule).
We showed that dynamic programming algorithms, similar to the ones used for pure proba-
bilistic inference, can be used to ﬁnd an optimal strategy for the agent in an inﬂuence diagram.
However, as we saw, inference in an inﬂuence diagram is more complex than in a Bayesian net-
work, both conceptually and computationally. This complexity is due to the interactions between
the diﬀerent operations involved: products for deﬁning the probability distribution; summation
for aggregating utility variables; and maximization for determining the agent’s optimal actions.
The inﬂuence diagram representation provides a compact encoding of rich and complex
decision problems involving multiple interrelated factors. It provides an elegant framework for
considering such important issues as which observations are required to make optimal decisions,
the deﬁnition of recall and the value of the perfect recall assumption, the dependence of a
particular decision on particular observations or components of the agent’s utility function, and
the like. Value of information — a concept that plays a key role in many practical applications
— is particularly easy to capture in the inﬂuence diagram framework.
However, there are several factors that can cause the complexity of the inﬂuence diagram
to grow unreasonably large, and signiﬁcantly reduce its usability in many real-world settings.
One such limitation is the perfect recall assumption, which can lead the decision rules to grow
exponentially large in the number of actions and observations the agent makes. We note that
this limitation is not one of the representation, but rather of the requirements imposed by the
notion of optimality and by the algorithms we use to ﬁnd solutions. A second source of blowup
arises when the scenario that arises following one decision by the agent is very diﬀerent from
the scenario following another. For example, imagine that the agent has to decide whether to go
from San Francisco to Los Angeles by air or by car. The subsequent decisions he has to make
and the variables he may observe in these two cases are likely to be very diﬀerent. This example
is an instance of context-speciﬁcity, as described in section 5.2.2; however, the simple solution of
modifying our CPD structure to account for context-speciﬁcity is usually insuﬃcient to capture
compactly these very broad changes in the model structure. The decision-tree structure is better
able to capture this type of structure, but it too has its limitations; several works have tried to
combine the beneﬁts of both representations (see section 23.9).
Finally, the basic formalism for sequential decision making under uncertainty is only a ﬁrst
step toward a more general formalism for planning and acting under uncertainty in many
settings: single-agent, multiagent distributed decision making, and multiagent strategic (game-
theoretic) interactions. A complete discussion of the ideas and methods in any of these areas
is a book in itself; we encourage the reader who is interested in these topics to pursue some
additional readings, some of which are mentioned in section 23.9.
23.9
Relevant Literature
The inﬂuence diagram representation was introduced by Howard and Matheson (1984a), albeit
more as a guide to formulating a decision problem than as a formal language with well-deﬁned
semantics. See also Oliver and Smith (1990) for an overview.

1128
Chapter 23. Structured Decision Problems
Olmsted (1983) and Shachter (1986, 1988) provided the ﬁrst algorithm for decision making in
inﬂuence diagrams, using local network transformations such as edge reversal. This algorithm
was gradually improved and reﬁned over the years in a series of papers (Tatman and Shachter
1990; Shenoy 1992; Shachter and Ndilikilikesha 1993; Ndilikilikesha 1994). The most recent algo-
rithm of this type is due to Jensen, Jensen, and Dittmer (1994); their algorithm utilizes the clique
tree data structure for addressing this task. All of these solutions use a constrained elimination
ordering, and they are therefore generally feasible only for fairly small inﬂuence diagrams.
A somewhat diﬀerent approach is based on reducing the problem of solving an inﬂuence
diagram to inference in a standard Bayesian network. The ﬁrst algorithm along these lines is
due to Cooper (1988), whose approach applied only to a single decision variable. This idea was
subsequently extended and improved considerably by Shachter and Peot (1992) and Zhang (1998).
Nilsson and Lauritzen (2000) and Lauritzen and Nilsson (2001) provide an algorithm based on
the concept of limited memory inﬂuence diagrams, which relaxes the perfect recall assumption
made in almost all previous work. This relaxation allows them to avoid the constraints on the
elimination ordering, and thereby leads to a much more eﬃcient clique tree algorithm. Similar
ideas were also developed independently by Koller and Milch (2001). The clique-tree approach
was further improved by Madsen and Nilsson (2001).
The simple inﬂuence diagram framework poses many restrictions on the type of decision-
making situation that can be expressed naturally.
Key restrictions include the perfect recall
assumption (also called “no forgetting”), and the assumption of the uniformity of the paths that
traverse the inﬂuence diagram.
Regarding this second point, a key limitation of the basic inﬂuence diagram representation is
that it is designed for encoding situations where all trajectories through the system go through
the same set of decisions in the same ﬁxed order. Several authors (Qi et al. 1994; Covaliu and
Oliver 1995; Smith et al. 1993; Shenoy 2000; Nielsen and Jensen 2000) propose extensions that
deal with asymmetric decision settings, where a choice taken at one decision variable can lead
to diﬀerent decision being encountered later on. Some approaches (Smith et al. 1993; Shenoy
2000) use an approach based on context-speciﬁc independence, along the lines of the tree-CPDs
of section 5.3. These approaches are restricted to cases where the sequence of observations and
decisions is ﬁxed in all trajectories of the system. The approach of Nielsen and Jensen (1999,
2000) circumvents this limitation, allowing for a partial ordering over observations and decisions.
The partial ordering allows them to reduce the set of constraints on the elimination ordering
in a variable elimination algorithm, resulting in computational savings. This approach was later
extended by Jensen and Vomlelová (2003).
In a somewhat related trajectory, Shachter (1998, 1999) notes that some parents of a decision
node may be irrelevant for constructing the optimal decision rule, and provided a graphical
procedure, based on his BayesBall algorithm, for identifying such irrelevant chance nodes. The
LIMID framework of Nilsson and Lauritzen (2000); Lauritzen and Nilsson (2001) makes these
notions more explicit by speciﬁcally encoding in the inﬂuence diagram representation the subset
of potentially observable variables relevant to each decision. This allows a relaxation of the
ordering constraints induced by the perfect recall assumption. They also deﬁne a graphical
procedure for identifying which decision rules depend on which others. This approach forms the
basis for the recursive algorithm presented in this chapter, and for its eﬃcient implementation
using clique trees.
The concept of value of information was ﬁrst deﬁned by Howard (1966). Over the years, various

23.9. Relevant Literature
1129
algorithms (Zhang et al. 1993; Chávez and Henrion 1994; Ezawa 1994) have been proposed for
performing value of information computations eﬃciently in an inﬂuence diagram, culminating
in the work of Dittmer and Jensen (1997) and Shachter (1999). All of these papers focus on the
myopic case and provide an algorithm for computing the value of information only for all single
variables in this network (allowing the decision maker to decide which one is best to observe).
Recent work of Krause and Guestrin (2005a,b) addresses the nonmyopic problem of selecting
an entire sequence of observations to make, within the context of a particular class of utility
functions.
There have been several ﬁelded systems that use the decision-theoretic approach described
in this chapter, although many use a Bayesian network and a simple utility function rather than
a full-ﬂedged inﬂuence diagram. Examples of this latter type include the Pathﬁnder system
of Heckerman (1990); Heckerman et al. (1992), and Microsoft’s system for decision-theoretic
troubleshooting (Heckerman et al. 1995; Breese and Heckerman 1996) that was described in
box 23.C. The Vista system of Horvitz and Barry (1995) used an inﬂuence diagram to make
decisions on display of information at NASA Mission Control Center.
Norman et al. (1998)
present an inﬂuence-diagram system for prenatal testing, as described in box 23.A. Meyer et al.
(2004) present a ﬁelded application of an inﬂuence diagram for selecting radiation therapy plans
for prostate cancer.
A framework closely related to inﬂuence diagrams is that of Markov decision processes (MDPs)
Markov decision
process
and its extension to the partially observable case (partially observable Markov decision processes,
or POMDPs).
The formal foundations for this framework were set forth by Bellman (1957);
Bertsekas and Tsitsiklis (1996) and Puterman (1994) provide an excellent modern introduction to
this topic. Although both an MDP and an inﬂuence diagram encode decision problems, the
focus of inﬂuence diagrams has been on richly spaces that involve rich structure in terms of the
state description (and sometimes the utility function), but only a few decisions; conversely, much
of the focus of MDPs has been on state spaces that are fairly unstructured (encoded simply as a
set of states), but on complex decision settings with long (often inﬁnite) sequences of decisions.
Several groups have worked on the synthesis of these two ﬁelds, tackling the problem of
sequential decision making in large, richly structured state spaces. Boutilier et al. (1989, 2000)
were the ﬁrst to explore this extension; they used a DBN representation of the MDP, and relied
on the use of context-speciﬁc structure both in the system dynamics (tree-CPDs) and in the
form of the value function. Boutilier, Dean, and Hanks (1999) provide a comprehensive survey
of the representational issues and of some of the earlier algorithms in this area. Koller and
Parr (1999); Guestrin et al. (2003) were the ﬁrst to propose the use of factored value functions,
which decompose additively as a sum of subutility functions with small scope. Building on
the rule-based variable elimination approach described in section 9.6.2.1, they also show how to
make use of both context-speciﬁc structure and factorization.
Another interesting extension that we did not discuss is the problem of decision making in
multiagent systems. At a high level, one can consider two diﬀerent types of multiagent systems:
ones where the agents share a utility function, and need to cooperate in a decentralized setting
with limited communication; and ones where diﬀerent agents have diﬀerent utility functions,
and must optimize their own utility while accounting for the other agents’ actions. Guestrin et al.
(2003) present some results for the cooperative case, and introduce the notion of coordination
graph; they focus on issues that arise within the context of MDPs, but some of their ideas can
also be applied to inﬂuence diagrams. The coordination graph structure was the basis for the

1130
Chapter 23. Structured Decision Problems
RoboSoccer application of Kok et al. (2003); Kok and Vlassis (2005), described in box 23.B.
The problem of optimal decision making in the presence of strategic interactions is the focus
of most of the work in the ﬁeld of game theory.
In this setting, the notion of a “rational
game theory
strategy” is somewhat more murky, since what is optimal for one player depends on the actions
taken by others. Fudenberg and Tirole (1991) and Osborne and Rubinstein (1994) provide a good
introduction to the ﬁeld of game theory, and to the standard solution concepts used. Generally,
work in game theory has represented multiagent interactions in a highly unstructured way: either
in the normal form, which lists a large matrix indexed by all possible strategies of all agents, or in
the extensive form — a game tree, a multiplayer version of a decision tree. More recently, there
have been several proposals for game representations that build on ideas in graphical models.
These proposals include graphical games (Kearns et al. 2001), multiagent inﬂuence diagrams
(Koller and Milch 2003), and game networks (La Mura 2000). Subsequent work (Vickrey and
Koller 2002; Blum et al. 2006) has shown that ideas similar to those used for inference in
graphical models and inﬂuence diagrams can be used to provide eﬃcient algorithms for ﬁnding
Nash equilibria (or approximate Nash equilibria) in these structured game representations.
23.10
Exercises
Exercise 23.1
Show that the decision rule δD that maximizes: P
D,PaD δDµ−D(D, PaD) is deﬁned as:
δD(w) = arg
max
d∈Val(D) µ−D(d, w)
for all w ∈Val(PaD).
Exercise 23.2
Prove proposition 23.1. In particular:
a. Show that for γ∗deﬁned as in equation (23.8), we have that φ∗= Q
W ∈X∪D φW , µ∗= Q
V ∈U µV .
b. For W ′ ⊂W , show that
cont(margW ′(γ)) =
X
W −W ′
cont(γ),
that is, that contraction and marginalization interchange appropriately.
c. Use your previous results to prove proposition 23.1.
Exercise 23.3⋆
Prove theorem 23.1 by showing that the combination and marginalization operations deﬁned in equa-
tion (23.5) and equation (23.6) satisfy the axioms of exercise 9.19:
a. Commutativity and associativity of combination:
γ1
M
γ2
=
γ2
M
γ1
γ1
M
(γ2
M
γ3)
=
(γ1
M
γ2)
M
γ3.
b. Consonance of marginalization: Let γ be a factor over scope W and let W 2 ⊆W 1 ⊆W . Then:
margW 2(margW 1(γ)) = margW 2(γ).

23.10. Exercises
1131
c. Interchanging marginalization and combination: Let γ1 and γ2 be potentials over W 1 and W 2
respectively. Then:
margW 1((γ1
M
γ2)) = γ1
M
margW 1(γ2).
Exercise 23.4⋆
Prove lemma 23.1.
Exercise 23.5⋆⋆
Extend the variable elimination algorithm of section 23.3 to the case of multiple utility variables, using the
mechanism of joint factors used in section 23.4.3. (Hint: Deﬁne an operation of max-marginalization, as
required for optimizing a decision variable, for a joint factor.)
Exercise 23.6⋆
Prove proposition 23.3.
(Hint: The proof is based on algebraic manipulation of the expected utility
EU[I[(σ−D, δD)]].)
Exercise 23.7
Prove theorem 23.4, as follows:
a. Show that if Di and Dj are two decisions such that Di, PaDi ⊆PaDj, then Di is not s-reachable
from Dj.
b. Use this result to conclude the theorem.
c. Show that the nodes in the relevance graph in this case will be totally ordered, in the opposite order to
the temporal ordering ≺over the decisions in the inﬂuence diagram.
Exercise 23.8⋆
In this exercise, you will prove theorem 23.5, using two steps.
a. We ﬁrst need to prove a result analogous to theorem 23.2, but showing that a decision rule δD remains
optimal even if the decision rules at several decisions D′ change.
Let σ be a fully mixed strategy, and δD a decision rule for D that is locally optimal for σ. Let σ′ be
another strategy such that, whenever σ′(D′) ̸= σ(D′), then D′ is not s-reachable from D. Prove that
δD is also optimal for σ′.
b. Now, let σk be the strategy returned by Iterated-Optimization-for-IDs, and σ′ be some other strategy
for the agent.
Let D1, . . . , Dk be the ordering on decisions used by the algorithm.
Show that
EU[I[σn]] ≥EU[I[σ′]]. (Hint: Use induction on the number of variables l at which σk and σ′ diﬀer.)
Exercise 23.9⋆⋆
Extend the algorithm of algorithm 23.3 to ﬁnd a globally optimal solution even in inﬂuence diagrams with
cyclic relevance graphs. Your algorithm will have to optimize several decision rules simultaneously, but it
should not always optimize all decision rules simultaneously. Explain precisely how you jointly optimize
multiple decision rules, and how you select the order in which decision rules are optimized.
Exercise 23.10⋆⋆
In this exercise, we will deﬁne an eﬃcient clique tree implementation of the algorithm of algorithm 23.3.
a. Describe a clique tree algorithm for a setting where cliques and sepsets are each parameterized with a
joint (probability, utility) potential, as described in section 23.4.3. Deﬁne: (i) the clique tree initialization
in terms of the network parameterization and a complete strategy σ, and (ii) the message passing
operations.

1132
Chapter 23. Structured Decision Problems
b. Show how we can use the clique-tree data structure to reuse computation between diﬀerent steps of
the iterated optimization algorithm. In particular, show how we can easily retract the current decision
rule δD from the calibrated clique tree, compute a new optimal decision rule for D, and then update
the clique tree accordingly. (Hint: Use the ideas of section 10.3.3.1.)
Exercise 23.11⋆
Prove proposition 23.4.
Exercise 23.12
Prove theorem 23.7: Let I be an inﬂuence diagram and I′ be any reduction of it, and let W →D be
some arc in I′.
a. (easy) Prove that if W →D is irrelevant in I, then it is also irrelevant in I′.
b. (hard) Prove that if W →D is irrelevant in I′, then it is also irrelevant in I.
Exercise 23.13
a. Prove proposition 23.6.
b. Is the value of learning the values of two variables equal to the sum of the values of learning each of
them? That is to say, is
VPI(I, D, {X, Y }) = VPI(I, D, X) + VPI(I, D, Y )?
Exercise 23.14⋆
Consider an inﬂuence diagram I, and assume that we have computed the optimal strategy for I using the
clique tree algorithm of section 23.5.2. Let D be some decision in D, and X some variable not observed
at D in I. Show how we can eﬃciently compute VPII(D | X), using the results of our original clique
tree computation, when:
a. D is the only decision variable in I.
b. The inﬂuence diagram contains additional decision variables, but the relevance graph is acyclic.
Exercise 23.15⋆
Consider a setting where we have a faulty device. Assume that the failure can be caused by a failure in one
of n components, exactly one of which is faulty. The probability that repairing component ci will repair
the device is pi. By the single-fault hypothesis, we have that Pn
i=1 pi = 1. Further assume that each
component ci can be examined with cost Co
i and then repaired (if faulty) with cost Cr
i . Finally, assume
that the costs of observing and repairing any component do not depend on any previous actions taken.
a. Show that if we observe and repair components in the order c1, . . . , cn, then the expected cost until
the device is repaired is:
n
X
i=1
" 
1 −
i−1
X
j=1
pj
!
Co
i + piCr
i
#
.
b. Use that to show that the optimal sequence of actions is the one in which we repair components in
order of their pi/Co
i ratio.
c. Extend your analysis to the case where some components can be replaced, but not observed; that is,
we cannot determine whether they are broken or not.
Exercise 23.16
The value of perfect information measures the change in our MEU if we allow observing a variable that
was not observed before. In the same spirit, deﬁne a notion of a value of control, which is the gain to the
value of control
agent if she is allowed to intervene at a chance variable X and set its value. Make reasonable assumptions
about the space of strategies available to the agent, but state your assumptions explicitly.

24
Epilogue
Why Probabilistic Graphical Models?
In this book, we have presented a framework of structured probabilistic models. This framework
rests on two foundations:
•
the use of a probabilistic model — a joint probability distribution — as a representation of
our domain knowledge;
•
the use of expressive data structures (such as graphs or trees) to encode structural properties
of these distributions.
The ﬁrst of these ideas has several important ramiﬁcations. First, our domain knowledge is

encoded declaratively, using a representation that has its own inherent semantics. Thus,
declarative
representation
the conclusions induced by the model are intrinsic to it, and not dependent on a speciﬁc
implementation or algorithm. This property gives us the ﬂexibility to develop a range
of inference algorithms, which may be appropriate in diﬀerent settings. As long as each
algorithm remains faithful to the underlying model semantics, we know it to be correct.
Moreover, because the basic operations of the calculus of probabilities (conditioning, marginal-
ization) are generally well accepted as being sound reasoning patterns, we obtain an important
guarantee: If we obtain surprising or undesirable conclusions from our probabilistic model, the
problem is with our model, not with our basic formalism. Of course, this conclusion relies on
the assumption that we are using exact probabilistic inference, which implements (albeit eﬃ-
ciently) the operations of this calculus; when we use approximate inference, errors induced by
the algorithm may yield undesirable conclusions. Nevertheless, the existence of a declarative

representation allows us to separate out the two sources of error — modeling error and
algorithmic error — and consider each separately. We can ask separately whether our model
is a correct reﬂection of our domain knowledge, and whether, for the model we have, approxi-
mate inference is introducing overly large errors. Although the answer to each of these questions
may not be trivial to determine, each is more easily considered in isolation. For example, to test
the model, we might try diﬀerent queries or perform sensitivity analysis. To test an approximate
inference algorithm, we might try the algorithm on fragments of the network, try a diﬀerent
(approximate or exact) inference algorithm, or compare the probability of the answer obtained
to that of an answer we may expect.
A third beneﬁt to the use of a declarative probabilistic representation is the fact that the
same representation naturally and seamlessly supports multiple types of reasoning. We can

1134
Chapter 24. Epilogue
compute the posterior probability of any subset of variables given observations about any others,
subsuming reasoning tasks such as prediction, explanation, and more. We can compute the most
likely joint assignment to all of the variables in the domain, providing a solution to a problem
known as abduction. With a few extensions to the basic model, we can also answer causal
abduction
queries and make optimal decisions under uncertainty.
The second of the two ideas is the key to making probabilistic inference practical. The ability

to exploit structure in the distribution is the basis for providing a compact representa-
tion of high-dimensional (or even inﬁnite-dimensional) probability spaces. This compact
representation is highly modular, allowing a ﬂexible representation of domain knowledge
that can easily be adapted, whether by a human expert or by an automated algorithm.
This property is one of the key reasons for the use of probabilistic models. For example, as we
discussed in box 23.C, a diagnostic system designed by a human expert to go through a certain
set of menus asking questions is very brittle: even small changes to the domain knowledge can
lead to a complete reconstruction of the menu system. By contrast, a system that uses inference
relative to an underlying probabilistic model can easily be modiﬁed simply by revising the model
(or small parts of it); these changes automatically give rise to a new interaction with the user.
The compact representation is also the key for the construction of eﬀective reasoning al-
gorithms. All of the inference algorithms we discussed exploit the structure of the graph in
fundamental ways to make the inference feasible.
Finally, the graphical representation also
provides the basis for learning these models from data. First, the smaller parameter space uti-
lized by these models allows parameter estimation even of high-dimensional distributions from
a reasonable amount of data. Second, the space of sparse graph structures deﬁnes an eﬀec-
tive and natural bias for structure learning, owing to the ubiquity of (approximate) conditional
independence properties in distributions arising in the real world.
The Modeling Pipeline
The framework of probabilistic graphical models provides support for natural representation,
eﬀective inference, and feasible model acquisition.
Thus, it naturally leads to an integrated
methodology for tackling a new application domain — a methodology that relies on all three of
these components.
Consider a new task that we wish to address. We ﬁrst deﬁne a class of models that encode
the key properties of the domain that are critical to the task. We then use learning to ﬁll in
the missing details of the model. The learned model can be used as the basis for knowledge
discovery, with the learned structure and parameters providing important insights about prop-
erties of the domain; it can also be used for a variety of reasoning tasks: diagnosis, prediction,
or decision making.
Many important design decisions must be made during this process. One is the form of the
graphical model. We have described multiple representations throughout this book — directed
and undirected, static and temporal, ﬁxed or template-based, with a variety of models for local
interactions, and so forth. These should not be considered as mutually exclusive options, but
rather as useful building blocks. Thus, a model does not have to be either a Bayesian network
or a Markov network — perhaps it should have elements of both. A model may be neither a full
dynamic Bayesian network nor a static one: perhaps some parts of the system can be modeled

1135
as static, and others as dynamic.
In another decision, when designing our class of models, we can provide a fairly speciﬁc
description of the models we wish to consider, or one that is more abstract, specifying only high-
level properties such as the set of observed variables. Our prior knowledge can be incorporated
in a variety of ways: as hard constraints on the learned model, as a prior, or perhaps even only
as an initialization for the learning algorithm. Diﬀerent combinations will be appropriate for
diﬀerent applications.
These decisions, of course, inﬂuence the selection of our learning algorithm. In some cases,
we will need to ﬁll in only (some) parameters; in others, we can learn signiﬁcant aspects of the
model structure. In some cases, all of the variables will be known in advance; in others, we will
need to infer the existence and role of hidden variables.

When designing a class of models, it is critical to keep in mind the basic trade-oﬀ
between faithfulness — accurately modeling the variables and interactions in the domain
— and identiﬁability — the ability to reliably determine the details of the model. Given
the richness of the representations one can encode in the framework of probabilistic models, it is
often very tempting to select a highly expressive representation, which really captures everything
that we think is going on in the domain. Unfortunately, such models are often hard to identify
from training data, owing both to the potential for overﬁtting and to the large number of local
maxima that can make it diﬃcult to ﬁnd the optimal model (even when enough training data
are available). Thus, one should always keep in mind Einstein’s maxim:
Everything should be made as simple as possible, but not simpler.
There are many other design decisions that inﬂuence our learning algorithm. Most obviously,
there are often multiple learning algorithms that are applicable to the same class of models.
Other decisions include what priors to use, when and how to introduce hidden variables, which
features to construct, how to initialize the model, and more.
Finally, if our goal is to use
the model for knowledge discovery, we must consider issues such as methods for evaluating
our conﬁdence in the learned model and its sensitivity to various choices that we made in the
design. Currently, these decisions are primarily made using individual judgment and experience.
Finally, if we use the model for inference, we also have various decisions to make. For any
class of models, there are multiple algorithms — both exact and approximate — that one can
apply. Each of these algorithms works well in certain cases and not others. It is important
to remember that here, too, we are not restricted to using only a pure version of one of the
inference algorithms we described. We have already presented hybrid methods such as collapsed
sampling methods, which combine exact inference and sampling. However, many other hybrids
are possible and useful. For example, we might use collapsed particle methods combined with
belief propagation rather than exact inference, or use a variational approximation to provide a
better proposal distribution for MCMC methods.
Overall, it is important to realize that what we have provided is a set of ideas and tools.
One can be ﬂexible and combine them in diﬀerent ways. Indeed, one can also extend these
ideas, constructing new representations and algorithms that are based on these concepts. This
is precisely the research endeavor in this ﬁeld.

1136
Chapter 24. Epilogue
Some Current and Future Directions
Despite the rapid advances in this ﬁeld, there are many directions in which signiﬁcant open
problems remain.
Clearly, one cannot provide a comprehensive list of all of the interesting
open problems; indeed, identifying an open problem is often the ﬁrst step in a research project.
However, we describe here some broad categories of problems where there is clearly much work
that needs to be done.
On the pragmatic side, probabilistic models have been used as a key component in addressing
some very challenging applications involving automated reasoning and decision making, data
analysis, pattern recognition, and knowledge discovery.
We have mentioned some of these
applications in the case studies provided in this book, but there are many others to which this
technology is being applied, and still many more to which it could be applied. There is much
work to be done in further developing these methods in order to allow their eﬀective application
to an increasing range of real-world problems.
However, our ability to easily apply graphical models to solve a range of problems is limited
by the fact that many aspects of their application are more of an art than a science. As we
discussed, there are many important design decisions in the selection of the representation,
the learning procedure, and the inference algorithm used. Unfortunately, there is no systematic
procedure that one can apply in navigating these design spaces. Indeed, there is not even a
comprehensive set of guidelines that tell us, for a particular application, which combination of
ideas are likely to be useful. At the moment, the design process is more the result of trial-
and-error experimentation, combined with some rough intuitions that practitioners learn by
experience. It would be an important achievement to turn this process from a black art into a
science.
At a higher level, one can ask whether the language of probabilistic graphical models is ade-
quate for the range of problems that we eventually wish to address. Thus, a diﬀerent direction is
to extend the expressive power of probabilistic models to incorporate a richer range of concepts,
such as multiple levels of abstractions, complex events and processes, groups of objects with
a rich set of interactions between them, and more. If we wish to construct a representation
of general world knowledge, and perhaps to solve truly hard problems such as perception, nat-
ural language understanding, or commonsense reasoning, we may need a representation that
accommodates concepts such as these, as well as associated inference and learning algorithms.
Notably, many of these issues were tackled, with varying degrees of success, within the dis-
ciplines of philosophy, psychology, linguistics, and traditional knowledge representation within
artiﬁcial intelligence. Perhaps some of the ideas developed in this long-term eﬀort can be inte-
grated into a probabilistic framework, which also supports reasoning from limited observations
and learning from data, providing an alternative starting point for this very long-term endeavor.
The possibility that these models can be used as the basis for solving problems that lie at
the heart of human intelligence raises an entirely new and diﬀerent question: Can we use
models such as these as a tool for understanding human cognition? In other words, can these
structured models, with their natural information ﬂow over a network of concepts, and their
ability to integrate intelligently multiple pieces of weak evidence, provide a good model for
human cognitive processes? Some preliminary evidence on this question is promising, and it
suggests that this direction is worthy of further study.

A
Background Material
A.1
Information Theory
Information theory deals with questions involving eﬃcient coding and transmission of informa-
tion. To address these issues, one must consider how to encode information so as to maximize
the amount of data that can sent on a given channel, and how to deal with noisy channels.
We brieﬂy touch on some technical deﬁnitions that arise in information theory, and use com-
pression as our main motivation. Cover and Thomas (1991) provides an excellent introduction
to information theory, including historical perspective on the development and applications of
these notions.
A.1.1
Compression and Entropy
Suppose that one plans to transmit a large corpus of say English text over a digital line. One
option is to send the text using standard (for example, ASCII) encoding that uses a ﬁxed number
of bits per character. A somewhat more eﬃcient approach is to use a code that is tailored to the
task of transmitting English text. For example, if we construct a dictionary of all words, we can
use binary encoding to describe each word; using 16 bits per word, we can encode a dictionary
of up to 65,536 words, which covers most English text.
We can gain an additional boost in compression by building a variable-length code, which
compression
encodes diﬀerent words in bit strings of diﬀerent length. The intuition is that words that are
frequent in English should be encoded by shorter code words, and rare words should be encoded
by longer ones. To be unambiguously decodable, a variable-length code must be preﬁx free: no
codeword can be a strict preﬁx of another. Without this property, we would not be able to tell
(at least not using a simple scan of the data) when one code word ends and the next begins.
It turns out that variable-length codes can signiﬁcantly improve our compression rate:
Example A.1
Assume that our dictionary contains four words — w1, w2, w3, w4 — with frequencies P(w1) =
1/2, P(w2) = 1/4, P(w3) = 1/8, and P(w4) = 1/8. One preﬁx-free encoding for this dictionary
is to encode w1 using a single bit codeword, say “0”; we would then encode w2 using the 2-bit
sequence “10”, and w3 and w4 using three bits each “110” and “111”.
Now, consider the expected number of bits that we would need for a message sent with this
frequency distribution. We must encode the word w1 on average half the time, and it costs us 1 bit.
We must encode the word w2 a quarter of the time, and it costs us 2 bits. Overall, we get that the

1138
Appendix A. Background Material
expected number of bits used is:
1
2 · 1 + 1
4 · 2 + 1
8 · 3 + 1
8 · 3 = 1.75.
One might ask whether a diﬀerent encoding would give us better compression performance in
this example. It turns out that this encoding is the best we can do, relative to the word-frequency
distribution. To provide a formal analysis for this statement, suppose we have a random variable
X that denotes the next item we need to encode (for example, a word). In order to analyze the
performance of a compression scheme, we need to know the distribution over diﬀerent values
of X. So we assume that we have a distribution P(X) (for example, frequencies of diﬀerent
words in a large corpus of English documents).
The notion of the entropy of a distribution provides us with a precise lower bound for the
expected number of bits required to encode instances sampled from P(X).
Deﬁnition A.1
Let P(X) be a distribution over a random variable X. The entropy of X is deﬁned as
entropy
IHP (X) = IEP

log
1
P(x)

=
X
x
P(x) log
1
P(x),
where we treat 0 log 1/0 = 0.1
When discussing entropies (and other information-theoretic measures) we use logarithms of base
2. We can then interpret the entropy in terms of bits.
The central result in information theory is a theorem by Shannon showing that the entropy
of X is the lower bound on the average number of bits that are needed to encode values
of X. That is, if we consider a proper codebook for values of X (one that can be decoded
unambiguously), then the expected code length, relative to the distribution P(X), cannot be
less than IHP (X) bits.
Going back to our example, we see that the average number of bits for this code is precisely
the entropy. Thus, the lower bound is tight in this case, in that we can construct a code that
achieves precisely that bound.
As another example, consider a uniform distribution P(X).
In this case, the optimal encoding is to represent each word using the same number of bits,
log |Val(X)|. Indeed, it is easy to verify that IHP (X) = log |Val(X)|, so again the bound is
tight (at least for cases where |Val(X)| is a power of 2.) Somewhat surprisingly, the entropy
bound is tight in general, in that there are codes that come very close to the “optimum” of
assigning the value x a code of length −log P(x).2

Another way of viewing the entropy is as a measure of our uncertainty about the value
of X. Consider a game where we are allowed to ask yes/no questions until we pinpoint the
value X. Then the entropy of X is average number of questions we need to ask to get to the
answer (if we have a good strategy for asking them). If we have little uncertainty about X, then
we get to the value with few questions. An extreme case is when IHP (X) = 0. It is easy to
verify that this can happen only when one value of X has probability 1 and the rest probability
1. To justify this, note that limϵ→0 ϵ log 1
ϵ = 0.
2. This value is not generally an integer, so one cannot directly map x to a code word with −log P(x) bits. However,
by coding longer sequences rather than individual values, we can come arbitrarily close to this bound.

A.1. Information Theory
1139
0. In this case, we do not need to ask any questions to get to the value of X. On the other
hand, if the value of X is very uncertain, then we need to ask many questions.
This discussion in fact identiﬁes the two boundary cases for IHP (X).
Proposition A.1
0 ≤IHP (X) ≤log |Val(X)|
The deﬁnition of entropy naturally extends to multiple variables.
Deﬁnition A.2
Suppose we have a joint distribution over random variables X1, . . . , Xn. Then the joint entropy
joint entropy
of X1, . . . , Xn is
IHP (X1, . . . , Xn) = IEP

log
1
P(X1, . . . , Xn)

.
The joint entropy captures how many bits are needed (on average) to encode joint instances
of the variables.
A.1.2
Conditional Entropy and Information
Suppose we are encoding the values of X and Y . A natural question is what is the cost of
encoding X if we are already encoding Y . Formally, we can examine the diﬀerence between
IHP (X, Y ) — the number of bits needed (on average) to encode of both variables, and IHP (Y )
— the number of bits needed to encode Y alone.
Deﬁnition A.3
The conditional entropy of X given Y is
conditional
entropy
IHP (X | Y ) = IHP (X, Y ) −IHP (Y ) = IEP

log
1
P(X | Y )

.
This quantity captures the additional cost (in terms of bits) of encoding X when we are already
encoding Y . The deﬁnition gives rise to the chain rule of entropy:
entropy chain
rule
Proposition A.2
For any distribution P(X1, . . . , Xn), we have that
IHP (X1, . . . , Xn) = IHP (X1) + IHP (X2 | X1) + . . . + IHP (Xn | X1, . . . , Xn−1).
That is, to encode a joint value of X1, . . . , Xn, we ﬁrst need to encode X1, then encode X2
given that we know the value of X1, then encode X3 given the ﬁrst two, and so on. Note that,
similarly to the chain rule of probabilities, we can expand the chain rule in any order we prefer;
that is, all orders result in precisely the same value.
Intuitively, we would expect IHP (X | Y ), the additional cost of encoding X when we already
encode Y , to be at least as small as the cost of encoding X alone. To motivate that, we see
that the worst case scenario is where we encode X as though we did not know the value of Y .
Indeed, one can formally show
Proposition A.3
IHP (X | Y ) ≤IHP (X).
The diﬀerence between these two quantities is of special interest.

1140
Appendix A. Background Material
Deﬁnition A.4
The mutual information between X and Y is
mutual
information
IIP (X; Y ) = IHP (X) −IHP (X | Y ) = IEP

log P(X | Y )
P(X)

.
The mutual information captures how many bits we save (on average) in the encoding of X if
we know the value of Y . Put in other words, it represents the extent to which the knowledge of
Y reduces our uncertainty about X.
The mutual information satisﬁes several nice properties.
Proposition A.4
• 0 ≤IIP (X; Y ) ≤IHP (X).
• IIP (X; Y ) = IIP (Y ; X).
• IIP (X; Y ) = 0 if and only if X and Y are independent.
Thus, the mutual information is nonnegative, and equal to 0 if and only if the two variables
are independent of each other. This is fairly intuitive, since if X and Y are independent, then
learning the value of Y does not tell us any thing new about the value of X. In fact, we can view
the mutual information as a quantitative measure of the strength of the dependency between
X and Y . The bigger the mutual information, the stronger the dependency. The extreme upper
value of the mutual information is when X is a deterministic function of Y (or vice versa). In
this case, once we know Y we are certain about the value of X, and so IIP (X; Y ) = IHP (X).
That is, Y supplies the maximal amount of information about X.
A.1.3
Relative Entropy and Distances Between Distributions
In many situations when doing probabilistic reasoning, we want to compare two distributions.
For example, we might want to approximate a distribution by one with desired qualities (say,
simpler representation, more eﬃcient to reason with, and so on) and want to evaluate the quality
of a candidate approximation. Another example is in the context of learning a distribution from
data, where we want to compare the learned distribution to the “true” distribution from which
the data was generated.
Thus, we want to construct a distance measure d that evaluates the distance between two
distance measure
distributions. There are some properties that we might wish for in such a distance measure:
Positivity: d(P, Q) is always nonnegative, and is zero if and only if P = Q;
Symmetry: d(P, Q) = d(Q, P).
Triangle inequality: for any three distributions P, Q, R, we have that
d(P, R) ≤d(P, Q) + d(Q, R).
When a distance measure d satisﬁes these criteria, it is called a distance metric.
distance metric
We now review several common approaches used to compare distributions. We begin by
describing one important measure that is motivated by information-theoretic considerations. It
also turns out to arise very naturally in a wide variety of probabilistic settings.

A.1. Information Theory
1141
A.1.3.1
Relative Entropy
Consider the preceding discussion of compression. As we discussed, the entropy measures the
performance of “optimal” code that assigns the value x a code of length −log P(x). However,
in many cases in practice, we do not have access to the true distribution P that generates the
data we plan to compress. Thus, instead of using P we use another distribution Q (say one we
estimated from prior data, or supplied by a domain expert), which is our best guess for P.
Suppose we build a code using Q. Treating Q as a proxy to the real distribution, we use
−log Q(x) bits to encode the value x. Thus, the expected number of bits we use on data
generated from P is
IEP

log
1
Q(x)

.
A natural question is how much we lost, due to the inaccuracy of using Q. Thus, we can examine
the diﬀerence between this encoding and the best achievable one, IHP (X). This diﬀerence is
called the relative entropy.
Deﬁnition A.5
Let P and Q be two distributions over random variables X1, . . . , Xn. The relative entropy of P
relative entropy
and Q is
ID(P(X1, . . . , Xn)||Q(X1, . . . , Xn)) = IEP

log P(X1, . . . , Xn)
Q(X1, . . . , Xn)

.
When the set of variables in question is clear from the context, we use the shorthand notation
ID(P||Q). This measure is also often known as the Kullback-Liebler divergence (or KL-divergence).
This discussion suggests that the relative entropy measures the additional cost imposed by
using a wrong distribution Q instead of P. Thus, Q is close, in the sense of relative entropy,
to P if this cost is small. As we expect, the additional cost of using the wrong distribution
is always positive. Moreover, the relative entropy is 0 if and only if the two distributions are
identical:
Proposition A.5
ID(P||Q) ≥0, and is equal to zero if and only if P = Q.
It is also natural to ask whether the relative entropy is also bounded from above. As we can
quickly convince ourselves, if there is a value x such that P(x) > 0 and Q(x) = 0, then the
relative entropy ID(P||Q) is inﬁnite. More precisely, if we consider a sequence of distributions
Qϵ such that Qϵ(x) = ϵ, then limϵ→0 ID(P||Qϵ) = ∞.
It is natural ask whether the relative entropy deﬁnes a distance measure over distributions.
Proposition A.5 shows that the relative entropy satisﬁes the positivity property speciﬁed above.
Unfortunately, positivity is the only property of distances that relative entropy satisﬁes; it

satisﬁes neither symmetry nor the triangle inequality. Given how natural these properties
are, one might wonder why relative entropy is used at all. Aside from the fact that it arises very
naturally in many settings, it also has a variety of other useful properties, that often make up
for the lack of symmetry and the triangle inequality.

1142
Appendix A. Background Material
A.1.3.2
Conditional Relative Entropy
As with entropies, we can deﬁne a notion of conditional relative entropy.
Deﬁnition A.6
Let P and Q be two distributions over random variables X, Y . The conditional relative entropy
conditional
relative entropy
of P and Q, is
ID(P(X | Y )||Q(X | Y )) = IEP

log P(X | Y )
Q(X | Y )

.
We can think of the conditional relative entropy ID(P(X | Y )||Q(X | Y )) as the weighted
sum of the relative entropies between the conditional distributions given diﬀerent values of y
ID(P(X | Y )||Q(X | Y )) =
X
y
P(y)ID(P(X | y)||Q(X | y)).
Using the conditional relative entropy, we can write the chain rule of relative entropy:
relative entropy
chain rule
Proposition A.6
Let P and Q be distributions over X1, . . . , Xn, then
ID(P||Q)
=
ID(P(X1)||Q(X1)) +
ID(P(X2 | X1)||Q(X2 | X1)) + . . . +
ID(P(Xn | X1, . . . , Xn−1)||Q(Xn | X1, . . . , Xn−1)).
Using the chain rule, we can prove additional properties of the relative entropy. First, using
the chain rule and the fact that ID(P(Y | X)||Q(Y | X)) ≥0, we can get the following property.
Proposition A.7
ID(P(X)||Q(X)) ≤ID(P(X, Y )||Q(X, Y )).
That is, the relative entropy of a marginal distributions is upper-bounded by the relative entropy
of the joint distributions. This observation generalizes to situations where we consider sets of
variables. That is,
ID(P(X1, . . . , Xk)||Q(X1, . . . , Xk)) ≤ID(P(X1, . . . , Xn)||Q(X1, . . . , Xn))
for k ≤n.
Suppose that X and Y are independent in both P and Q. Then, we have that P(Y | X) =
P(Y ), and similarly, Q(Y | X) = Q(Y ). Thus, we conclude that ID(P(Y | X)||Q(Y | X)) =
ID(P(Y )||Q(Y )). Combining this observation with the chain rule, we can prove an additional
property.
Proposition A.8
If both P and Q satisfy (X ⊥Y ), then
ID(P(X, Y )||Q(X, Y )) = ID(P(X)||Q(X)) + ID(P(Y )||Q(Y )).

A.2. Convergence Bounds
1143
A.1.3.3
Other Distance Measures
There are several diﬀerent metric distances between distributions that we may consider. Several
simply treat a probability distribution as a vector in IRN (where N is the dimension of our
probability space), and use standard distance metrics for Euclidean spaces. More precisely, let P
and Q be two distributions over X1, . . . , Xn. The three most commonly used distance metrics
of this type are:
•
The L1 distance: ||P −Q||1 = P
x1,...,xn |P(x1, . . . , xn) −Q(x1, . . . , xn)|.
•
The L2 distance: ||P −Q||2 =
P
x1,...,xn(P(x1, . . . , xn) −Q(x1, . . . , xn))2 1
2 .
•
The L∞distance: ||P −Q||∞= maxx1,...,xn |P(x1, . . . , xn) −Q(x1, . . . , xn)|.
An apparently diﬀerent distance measure is the variational distance, which seems more specif-
variational
distance
ically tailored to probability distributions, rather than to general real-valued vectors. It is deﬁned
as the maximal diﬀerence in the probability that two distributions assign to any event that can
be described by the distribution. For two distributions P, Q over an event space S, we deﬁne:
IDvar(P; Q) = max
α∈S |P(α) −Q(α)|.
(A.1)
Interestingly, this distance turns out to be exactly half the L1 distance:
Proposition A.9
Let P and Q be two distributions over S. Then
IDvar(P; Q) = 1
2||P −Q||1.
These distance metrics are all useful in the analysis of approximations, but, unlike the relative
entropy, they do not decompose by a chain-rule-like construction, often making the analytical
analysis of such distances harder. However, we can often use an analysis in terms of relative
entropy to provide bounds on the L1 distance, and hence also on the variational distance:
Theorem A.1
For any two distribution P and Q, we have that
||P −Q||1 ≤((2 ln 2)ID(P||Q))1/2 .
A.2
Convergence Bounds
In many situations that we cover in this book, we are given a set of samples generated from
a distribution, and we wish to estimate certain properties of the generating distribution from
the samples.
We now review some properties of random variables that are useful for this
task.
The derivation of these convergence bounds is central to many aspects of probability
theory, statistics, and randomized algorithms. Motwani and Raghavan (1995) provide one good
introduction on this topic and its applications to the analysis of randomized algorithms.
Speciﬁcally, suppose we have a biased coin that has an unknown probability p of landing
heads.
We can estimate the value of p by tossing the coin several times and counting the

1144
Appendix A. Background Material
frequency of heads.
More precisely, assume we have a data set D consisting of M coin
tosses, that is, M trials from a Bernoulli distribution. The m’th coin toss is represented by a
binary variable X[m] that has value 1 if the coin lands heads, and 0 otherwise. Since each
toss is separate from the previous one, we are assuming that all these random variables are
independent. Thus, these variables are independence and identically distribution, or IID. It is
IID
easy to compute the expectation and variance of each X[m]:
•
IE[X[m]] = p.
•
VVar[X[m]] = p(1 −p).
A.2.1
Central Limit Theorem
We are interested in the sum of all the variables SD = X[1] + . . . + X[M] and in the fraction
of successful trials TD =
1
M SD. Note that SD and TD are functions of the data set D. As D
is chosen randomly, they can be viewed as random variables over the probability space deﬁned
by diﬀerent possible data sets D. Using properties of expectation and variance, we can analyze
the properties of these random variables.
•
IE[SD] = M · p, by linearity of expectation.
•
VVar[SD] = M · p(1 −p), since all the all the X[i]’s are independent.
•
IE[TD] = p.
•
VVar[TD] =
1
M p(1 −p), since VVar
 1
M SD

=
1
M2 VVar[SD].
The fact that VVar[TD] →0 as M →∞suggests that for suﬃciently large M the distribution
of TD is concentrated around p. In fact, a general result in probability theory allows us to
conclude that this distribution has a particular form:
Theorem A.2
(Central Limit Theorem) Let X[1], X[2], . . . be a series of IID random variables, where each
central limit
theorem
X[m] is sampled from a distribution such that IE[X[m]] = µ, and variance VVar[X[m]] = σ2
(0 < σ < ∞). Then
lim
M→∞P
P
m(X[m] −µ)
√
Mσ
< r

= Φ(r),
where Φ(r) = P(Z < r) for a Gaussian variable Z with distribution N (0; 1).
Thus, if we collect a large number of repeated samples from the same distribution, then the
distribution of the random variable (SD −IE[SD])/
p
VVar[SD] is roughly Gaussian. In other
Gaussian
words, the distribution of SD is, at the limit, close to a Gaussian with the appropriate expectation
and variance: N (IE[SD]; VVar[SD]).
There are variants of the central limit theorem for the case where each X[m] has a diﬀerent
distribution. These require additional technical conditions that we do not go into here. However,
the general conclusion is similar — the sum of many independent random variables has a
distribution that is approximately Gaussian. This is often a justiﬁcation for using a Gaussian
distribution in modeling quantities that are the cumulative eﬀect of many independent (or
almost independent) factors.

A.2. Convergence Bounds
1145
The quantity TD is an estimator for the mean µ: a statistical function that we can use to
estimator
estimate the value of µ. The mean and variance of an estimator are the two key quantities for
evaluating it. The mean of the estimator tells us the value around which its values are going to
be concentrated. When the mean of the estimator is the target value µ, it is called an unbiased
unbiased
estimator
estimator for the quantity µ — an estimator whose mean is precisely the desired value. In
general, lack of bias is a desirable property in an estimator: it tells us that, although they are
noisy, at least the values obtained by the estimator are centered around the right value. The
variance of the estimator tells us the “spread” of values we obtain from it. Estimators with high
variance are not very reliable, as their value is likely to be far away from their mean.
Applying the central limit theorem to our problem, we see that, for suﬃciently large M, the
variable TD has a roughly Gaussian distribution with mean p and variance p(1−p)
M
.
A.2.2
Convergence Bounds
In many situations, we are interested not only in the asymptotic distribution of TD, but also in
the probability that TD is close to p for a concrete choice of M. We can bound this probability
in several ways. One of the simplest is by using Chebyshev’s inequality; see exercise 12.1. This
bound, however, is quite loose, as it assumes quadratic decay in the distance |TD −p|. Other,
more reﬁned bounds, can be used to prove an exponential rate of decay in this distance. There
are many variants of these bounds, of which we describe two.
The ﬁrst, called Hoeﬀding bound, measures error in terms of the absolute distance |TD −p|.
Hoeﬀding bound
Theorem A.3
Let D = {X[1], . . . , X[M]} be a sequence of M independent Bernoulli trials with probability of
success p. Let TD =
1
M
P
m X[m]. Then
PD(TD > p + ϵ)
≤
e−2Mϵ2
PD(TD < p −ϵ)
≤
e−2Mϵ2.
The bound asserts that, with very high probability, TD is within an additive error ϵ of the true
probability p. The probability here is taken relative to possible data sets D. Intuitively, we might
end up with really unlikely choices of D, for example, ones where we get the same value all
the time; these choices will clearly give wrong results, but they are very unlikely to arise as
a result of a random sampling process. Thus, the bound tells us that, for most data sets D

that we generate at random, we obtain a good estimate. Furthermore, the fraction of
“bad” sample sets D, those for which the estimate is more than ϵ from the true value,
diminishes exponentially as the number of samples M grows.
The second bound, called the Chernoﬀbound, measures error in terms of the relative size of
Chernoﬀbound
this distance to the size of p.
Theorem A.4
Let D = {X[1], . . . , X[M]} be a sequence of M independent Bernoulli trials with probability of
success p. Let TD =
1
M
P
m X[m], then
PD(TD > p(1 + ϵ))
≤
e−Mpϵ2/3
PD(TD < p(1 −ϵ))
≤
e−Mpϵ2/2.

1146
Appendix A. Background Material
Let σM =
p
VVar[TD] be the standard deviation of TD for D of size M. Using the multiplica-
tive Chernoﬀbound, we can show that
PD(|TD −p| ≥kσ) ≤2e−k2/6.
(A.2)
This inequality should be contrasted with the Chebyshev inequality. The big diﬀerence owes to
the fact that the Chernoﬀbound exploits the particular properties of the distribution of TD.
A.3
Algorithms and Algorithmic Complexity
In this section, we brieﬂy review relevant algorithms and notions from algorithmic complexity.
Cormen et al. (2001) is a good source for learning about algorithms, data structures, graph
algorithms, and algorithmic complexity; Papadimitriou (1993) and Sipser (2005) provide a good
introduction to the key concepts in computational complexity.
A.3.1
Basic Graph Algorithms
Given a graph structure, there are many useful operations that we might want to perform. For
example, we might want to determine whether there is a certain type of path between two
nodes. In this section, we survey algorithms for performing two key tasks that will be of use in
several places throughout this book. Additional algorithms, for more speciﬁc tasks, are presented
as they become relevant.
Algorithm A.1 Topological sort of a graph
Procedure Topological-Sort (
G = (X, E)
// A directed graph
)
1
Set all nodes to be unmarked
2
for i = 1, . . . , n
3
Select any unmarked node X all of whose parents are marked
4
d(X) ←i
5
Mark X
6
return (⃗d)
One algorithm, shown in algorithm A.1, ﬁnds a topological ordering of the nodes in the graph,
topological
ordering
as deﬁned in deﬁnition 2.19.
Another useful algorithm is one that ﬁnds, in a weighted undirected graph H with nonnegative
edge weights, a maximum weight spanning tree. More precisely, a subgraph is said to be a
maximum weight
spanning tree
spanning tree if it is a tree and it spans all vertices in the graph. Similarly, a spanning forest is
a forest that spans all vertices in the graph. A maximum weight spanning tree (or forest) is the
tree (forest) whose edge-weight sum is largest among all spanning trees (forests).

A.3. Algorithms and Algorithmic Complexity
1147
Algorithm A.2 Maximum weight spanning tree in an undirected graph
Procedure Max-Weight-Spanning-Tree (
H = (N, E)
{wij : (Xi, Xj) ∈E}
)
1
NT ←{X1}
2
ET ←∅
3
while NT ̸= X
4
E′ ←{(i, j) ∈E : Xi ∈NT , Xj ̸∈NT }
5
(Xi, Xj) ←arg max(Xi,Xj)∈E′ wij
6
// (Xi, Xj) is the highest-weight edge between a node in T
and a node out of T
7
NT ←NT ∪{Xj}
8
ET ←ET ∪{(Xi, Xj)}
9
return (ET )
A.3.2
Analysis of Algorithmic Complexity
A key step in evaluating the usefulness of an algorithm is to analyze its computational cost:
the amount of time it takes to complete the computation and the amount of space (memory)
required. To evaluate the algorithm, we are usually not interested in the cost for a particular
input, but rather in the algorithm’s performance over a set of inputs. Of course, we would
expect most algorithms to run longer when applied to larger problems. Thus, the complexity of
an algorithm is usually measured in terms of its performance, as a function of the size of the
input given to it. Of course, to determine the precise cost of the algorithm, we need to know
exactly how it is implemented and even which machine it will be run on. However, we can
often determine the scalability of an algorithm at a more abstract level, without worrying about
the details of its implementation. We now provide a high-level overview of some of the basic
concepts underlying such analysis.
Consider an algorithm that takes a list of n numbers and adds them together to compute their
sum. Assuming the algorithm simply traverses the list and computes the sum as it goes along, it
has to perform some ﬁxed number of basic operations for each element in the list. The precise
operations depend on the implementation: we might follow a pointer in a linked list, or simply
increment a counter in an array. Thus, the precise cost might vary based on the implementation.
But, the total number of operations per list element is some ﬁxed constant factor. Thus, for any
reasonable implementation, the running time of the algorithm will be bounded by C ·n for some
constant C. In this case, we say that the asymptotic complexity of the algorithm is O(n), where
asymptotic
complexity
the O() notation makes implicit the precise nature of the constant factor, which can vary from
one implementation to another. This idea only makes sense if we consider the running time as
a function of n. For any ﬁxed problem size, say up to 100, we can always ﬁnd a constant C (for
instance, a million years) such that the algorithm takes time no more than C. However, even if
we are not interested in problems of unbounded size, evaluating the way in which the running
time varies as a function of the problem size is the ﬁrst step to understanding how well it will

1148
Appendix A. Background Material
scale to large problems.
To take a more relevant example, consider the maximum weight spanning tree procedure of
algorithm A.2. A (very) naive implementation of this algorithm traverses all of the edges in the
graph every time a node is added to the spanning tree; the resulting cost is O(mn) where m
is the number of edges and n the number of nodes. A more careful implementation of the
data structures, however, maintains the edges in a sorted data structure known as a heap, and
the list of edges adjacent to a node in an adjacency list. In this case, the complexity of the
algorithm can be O(m log n) or (with a yet more sophisticated data structure) O(m + n log n).
Surprisingly, even more sophisticated implementations exist whose complexity is very close to
linear time in m.
More generally, we can provide the following deﬁnition:
Deﬁnition A.7
Consider an algorithm A that takes as input problems Π from a particular class, and returns an
output. Assume that the size of each possible input problem Π is measured using some set of
parameters n1, . . . , nk. We say that the running time of A is O(f(n1, . . . , nk)) for some function
f (called “big O of f”), if, for n1, . . . , nk suﬃciently large, there exists a constant C such that, for
any possible input problem Π, the running time of A on Π is at most C · f(n1, . . . , nk).
In our example, each problem Π is a graph, and its size is deﬁned by two parameters: the
number of nodes n and the number of edges m. The function f(n, m) is simply n + m.
When the function f is linear in each of the input size parameters, we say that the running
running time
time of the algorithm is linear, or that the algorithm has linear time. We can similarly deﬁne
notions of polynomial time and exponential time. It may be useful to distinguish diﬀerent rates
polynomial time
exponential time
of growth in the diﬀerent parameters. For example, if we have a function that has the form
f(n, m) = n2 + 2m, we might say that the function is polynomial in n but exponential in m.
Although one can ﬁnd algorithms at various levels of complexity, the key cutoﬀbetween
feasible and infeasible computations is typically set between algorithms whose complexity is
polynomial and those whose complexity is exponential. Intuitively, an algorithm whose com-
plexity is exponential allows virtually no useful scalability to larger problems.
For example,
assume we have an algorithm whose complexity is O(2n), and that we can now solve instances
whose size is N. If we wait a few years and get a computer that is twice as fast as the one we
have now, we will be able to solve only instances whose size is N +1, a negligible improvement.
We can also see this phenomenon by comparing the growth curves for various cost functions,
as in ﬁgure A.1. We see that the constant factors in front of the polynomial functions have
some impact on very small problem sizes, but even for moderate problem sizes, such as 20, the
exponential function quickly dominates and grows to the point of infeasibility. Thus, a major
distinction is made between algorithms that run in polynomial time and those whose running
time is exponential. While the exponential-polynomial distinction is a critical one, there is

also a tendency to view polynomial-time algorithms as tractable. This view, unfortunately,
is overly simpliﬁed: an algorithm whose running time is O(n3) is not generally tractable
for problems where n is in the thousands.
Algorithmic theory oﬀers a suite of tools for constructing eﬃcient algorithms for certain
types of problems. One such tool, which we shall use many times throughout the book, is
dynamic programming, which we describe in more detail in appendix A.3.3.
Unfortunately,
not all problems are not amenable to these techniques, and a broad class of highly important
problems fall into a category for which polynomial-time algorithms are extremely unlikely to

A.3. Algorithms and Algorithmic Complexity
1149
100n2
30n3
2n/5
0
5
10
15
20
0
200,000
400,000
600,000
800,000
1e+006
1.2e+006
1.4e+006
1.6e+006
1.8e+006
Figure A.1
Illustration of asymptotic complexity. The growth curve of three functions: The solid line
is 100n2, the dashed line is 30n3, and the dotted line is 2n/5.
exist; see appendix A.3.4.
A.3.3
Dynamic Programming
As we discussed earlier, several techniques can be used to provide eﬃcient solutions to appar-
ently challenging computational problems. One important tool is dynamic programming, a

dynamic
programming
general method that we can apply when the solution to a problem requires that we solve
many smaller subproblems that recur many times. In this case, we are often better oﬀ
precomputing the solution to the subproblems, storing them, and using them to compute
the values to larger problems.
Perhaps the simplest application of dynamic programming is the problem of computing
Fibonacci numbers, deﬁned via the recursive equations:
F0
=
1
F1
=
1
Fn
=
Fn−1 + Fn−2.
Thus, we have that F2 = 2, F3 = 3, F4 = 5, F5 = 8, and so on.
One simple algorithm to compute Fibonacci(n) is to use the recursive deﬁnition directly, as
shown in algorithm A.3. Unrolling the computation, we see that the ﬁrst of these recursive calls,
Fibonacci(n−1), calls Fibonacci(n−2) and Fibonacci(n−3). Thus, we are already have two calls
to Fibonacci(n −2). Similarly, Fibonacci(n −2) also calls Fibonacci(n −3), another redundant
computation. If we carry through the entire recursive analysis, we can show that the running
time of the algorithm is exponential in n.
On the other hand, we can compute “bottom up”, as in algorithm A.4. Here, we start with F0

1150
Appendix A. Background Material
Algorithm A.3 Recursive algorithm for computing Fibonacci numbers
Procedure Fibonacci (
n
)
1
if (n = 0 or n = 1) then
2
return (1)
3
return (Fibonacci(n −1) + Fibonacci(n −2))
Algorithm A.4 Dynamic programming algorithm for computing Fibonacci numbers
Procedure Fibonacci (
n
)
1
F0 ←1
2
F1 ←1
3
for i = 2, . . . , n
4
Fi ←Fi−1 + Fi−2
5
return (Fn)
and F1, compute F2 from F0 and F1, compute F3 from F1 and F2, and so forth. Clearly, this
process computes Fn in time O(n). We can view this alternative algorithm as precomputing
and then caching (or storing) the results of the intermediate computations performed on the
way to each Fi, so that each only has to be performed once.
More generally, if we can deﬁne the set of intermediate computations required and how they
depend on each other, we can often use this caching idea to avoid redundant computation
and provide signiﬁcant savings. This idea underlies most of the exact inference algorithms for
graphical models.
A.3.4
Complexity Theory
In appendix A.3.3, we saw how the same problem might be solvable by two algorithms that
have radically diﬀerent complexities. Examples like this raise an important issue regarding the
algorithm design process: If we come up with an algorithm for a problem, how do we know
whether its computational complexity is the best we can achieve? In general, unfortunately,
we cannot tell. There are very few classes of problems for which we can give nontrivial lower
bounds on the amount of computation required for solving them.
However, there are certain types of problems for which we can provide, not a guarantee,
but at least a certain expectation regarding the best achievable performance. Complexity theory
complexity
theory
has deﬁned classes of problems that are, in a sense, equivalent to each other in terms of their
computational cost. In other words, we can show that an algorithm for solving one problem
can be converted into an algorithm that solves another problem. Thus, if we have an eﬃcient
algorithm for solving the ﬁrst problem, it can also be used to solve the second eﬃciently.
The most prominent such class of problems is that of NP-complete problems; this class

A.3. Algorithms and Algorithmic Complexity
1151
contains many problems for which researchers have unsuccessfully tried, for decades, to ﬁnd
eﬃcient algorithms.
Thus, by proving that a problem is NP-complete, we are essentially
showing that it is “as easy” as all other NP-complete problems. Finding an eﬃcient (polynomial
time) algorithm for this problem would therefore give rise to eﬃcient algorithms for all NP-
complete problems, an extremely unlikely event. In other words, by showing that a problem
is NP-complete, we are essentially showing that it is extremely unlikely to have an eﬃcient
solution. We now provide some of the formal basis for this type of discussion.
A.3.4.1
Decision Problems
A decision problem Π is a task that has the following form: The program must accept an input
ω and decide whether it satisﬁes a certain condition or not. A prototypical decision problem is
the SAT problem, which is deﬁned as the problem of taking as input a formula in propositional
logic, and returning true if the formula has a satisfying assignment and false if it does not. For
example, an algorithm for the SAT problem should return true for the formula
(q1 ∨¬q2 ∨q3) ∧(¬q1 ∨q2 ∨¬q3),
(A.3)
which has (among others) the satisfying assignment q1 = true; q2 = true; q3 = true. It would
return false for the formula
(¬q1 ∨¬q2) ∧(q2 ∨q3) ∧(¬q1 ∨¬q3),
(A.4)
which has no satisfying assignments.
We often use a somewhat restricted version of the SAT problem, called 3-SAT.
Deﬁnition A.8
A formula φ is said to be a 3-SAT formula over the Boolean (binary-valued) variables q1, . . . , qn if
3-SAT
it has the following form: φ is a conjunction: φ = C1 ∧. . . ∧Cm. Each Ci is a clause of the form
ℓi,1 ∨ℓi,2 ∨ℓi,3. Each ℓi,j (i = 1, . . . , m; j = 1, 2, 3) is a literal, which is either qk or ¬qk for
some k = 1, . . . , n.
A decision problem Π is associated with a language LΠ that deﬁnes the precise set of
instances for which a correct algorithm for Π must return true. In the case of 3-SAT, L3SAT is
the set of all correct encodings of propositional 3-SAT formulas that are satisﬁable.
A.3.4.2
P and N P
A decision problem is said to be in the class P if there exists a deterministic algorithm that
takes an instance ω and determines whether or not ω ∈LΠ, in polynomial time in the size of
the input ω. In SAT, for example, the input is the formula, and its size is simply its length.
We can also deﬁne a signiﬁcantly more powerful type of computation that allows us to
provide a formal foundation for a very rich class of problems. Consider again our SAT algorithm.
The naive algorithm for determining whether a formula is satisﬁable enumerates all of the
assignments, and returns true if one of them satisﬁes the formula. Imagine that we allow the
algorithm a notion of a “lucky guess”: the algorithm is allowed to guess an assignment, and
then verify whether it satisﬁes the formula.
The algorithm can determine if the formula is
satisﬁable simply by having one guess that works out. In other words, we assume that the

1152
Appendix A. Background Material
algorithm asserts that the formula is in L3SAT if there is some guess that works out. This type
of computation is called a nondeterministic computation. A fully formal deﬁnition requires that
we introduce a range of concepts (such as Turing Machines) that are outside the scope of this
book. Roughly speaking, a nondeterministic decision algorithm has the following form. The ﬁrst
stage is a guessing stage, where the algorithm nondeterministically produces some guess γ. The
second stage is a deterministic verifying stage that either accepts its input ω based on γ or not.
The algorithm as a whole is said to accept ω if it accepts γ using any one of its guesses. A
decision problem Π is in the class NP if there exists a nondeterministic algorithm that accepts
an instance ω if and only if ω ∈LΠ, and if the veriﬁcation stage can be executed in polynomial
time in the length of ω. Clearly, SAT is in NP: the guesses γ are possible assignments, and
they are veriﬁed in polynomial time simply by testing whether the assignment γ satisﬁes the
input formula φ.
Because deterministic computations are a special case of nondeterministic ones, we have
that P ⊆NP. The converse of this inclusion is the biggest open problem in computational
complexity. In other words, can every problem that can be solved in polynomial time using a
lucky guess also be solved in polynomial time without guessing?
As stated, it seems impossible to get a handle on this problem: The number of problems
in NP is potentially unlimited, and even if we ﬁnd an eﬃcient algorithm for one problem,
what does that tell us about the class in general? The notion of NP-complete problems gives
us a tool for reducing this unmanageable question into a much more compact one. Roughly
speaking, the class NP has a set of problems that are the “hardest problems in NP”: if we can
solve them in polynomial time, we can provably solve any problem in NP in polynomial time.
These problems are known as NP-complete problems.
More formally, we say that a decision problem Π is NP-hard if for every decision problem Π′
NP-hard
in NP, there is a polynomial-time transformation of inputs such that an input for Π′ belongs
to LΠ′ if and only if the transformed instance belongs to LΠ. This type of transformation is
called a reduction of one problem to another. When we have such a reduction, any algorithm A
reduction
that solves the decision problem Π can be used to solve Π′: We simply convert each instance
of Π′ to the corresponding instance of Π, and apply A. An NP-hard problem can be used in
this way for any problem in NP. Thus, it provides a universal solution for any NP-problem. It
is possible to show that the SAT problem is NP-hard. A problem Π is said to be NP-complete
if it is both NP-hard and in NP. The 3-SAT problem is NP-complete, as are many other
important problems. For example, the Max-Clique Problem of deciding whether an undirected
Max-Clique
Problem
graph has a clique of size at least K (where K is a parameter to the algorithm) is also NP-hard.
At the moment, it is not yet known whether P = NP. Much work has been devoted to
investigating both sides of this conjecture. In particular, decades of research have been spent
on failed attempts to ﬁnd polynomial-time algorithms for many NP-complete problems, such
as SAT or Max-Clique. The lack of success suggests that probably no such algorithm exists for
any NP-hard problem, and that P ̸= NP. Thus, a standard way of showing that a particular
problem Π probably is unlikely to have a polynomial time algorithm is to show that it is NP-
hard. In other words, we try to ﬁnd a reduction from some known NP-hard problem, such
as SAT, to the problem of interest. If we construct such a reduction, then we have shown the
following: If we ﬁnd a polynomial-time algorithm for Π, we have also provided a polynomial-
time algorithm for all NP-complete problems, and shown that NP = P. Although this is not
impossible, it is currently believed to be highly unlikely.

A.3. Algorithms and Algorithmic Complexity
1153

Thus, if we show that a problem is NP-hard, we should probably resign ourselves to
algorithms that are exponential-time in the worst case. However, as we will see, there
are many cases where algorithms can be exponential-time in the worst case, yet achieve
signiﬁcantly better performance in practice. Because many of the problems we encounter
are NP-hard, ﬁnding tractable cases and providing algorithms for them is where most
of the interesting work takes place.
A.3.4.3
Other Complexity Classes
The classes P and NP are the most important and commonly used classes used to describe
the computational complexity of problems, but they are only part of a rich framework used for
classifying problems based on their time or space complexity. In particular, the class NP is
only the ﬁrst level in an inﬁnite hierarchy of increasingly larger classes. Classes higher in the
hierarchy might or might not be harder than the lower classes; this problem also is a major open
problem in complexity theory.
A diﬀerent dimension along which complexity can vary relates to the existential nature of the
deﬁnition of the class NP. A problem is in NP if there is some guess on which a polynomial
time computation succeeds (returns true).
In our SAT example, the guesses were diﬀerent
assignments that could satisfy the formula φ deﬁned in the problem instance. However, we
might want to know what fraction of the computations succeed. In our SAT example, we may
want to compute the exact number (or fraction) of assignments satisfying φ. This problem is no
longer a decision problem, but rather a counting problem that returns a numeric output.
The class #P is deﬁned precisely for problems that return a numerical value.
Such a
#P
problem is in #P if the number can be computed as the number of accepting guesses of a
nondeterministic polynomial time algorithm. The problem of counting the number of satisfying
assignments to a 3-SAT formula is clearly in #P. Like the class of NP-hard problems, there are
problems that are at least as hard as any problem in #P. The problem of counting satisfying
assignments is the canonical #P-hard problem. This problem is clearly NP-hard: if we can
solve it, we can immediately solve the 3-SAT decision problem. For trivial reasons, it is not in
NP, because it is a counting problem, not a decision problem. However, it is generally believed
that the counting version of the 3-SAT problem is inherently more diﬃcult than the original
decision problem, in that we can use them to solve problems that are “harder” than NP.
Finally, another, quite diﬀerent, complexity class is the class of randomized polynomial time
algorithms — those that can be solved using a polynomial time algorithm that makes random
guesses. There are several ways of deﬁning when a randomized algorithm accepts a particular
input; we provide one of them. A decision problem Π is in the class RP if there exists a
RP
randomized algorithm that makes a guess probabilistically, and then processes it in polynomial
time, such that the following holds: The algorithm always returns false for an input not in LΠ;
for an input in LΠ, the algorithm returns true with probability greater than 1/2. Thus, the
algorithm only has to get the “right” answer in half of its guesses; this requirement is much
more stringent than that of nondeterministic polynomial time, where the algorithm only had to
get one guess right. Thus, many problems are known to be in NP but are not known to be
in RP. Whether NP = RP is another important open question, where the common belief is
also that the answer is no.

1154
Appendix A. Background Material
A.4
Combinatorial Optimization and Search
A.4.1
Optimization Problems
Many of the problems we address in this book and in other settings can be formulated as an
optimization problem. Here, we are given a solution space Σ of possible solutions σ, and an
optimization
problem
objective function fobj
:
Σ 7→IR that allows us to evaluate the “quality” of each candidate
objective
function
solution. Our aim is then to ﬁnd the solution that achieves the maximum score:
σ∗= arg max
σ∈Σ ff(σ).
This optimization task is a maximization problem; we can similarly deﬁne a minimization
problem, where our goal is to minimize a loss function. One can easily convert one problem to
another (by negating the objective), and so, without loss of generality, we focus on maximization
problems.
Optimization problems can be discrete, where the solution space Σ consists of a certain
(ﬁnite) number of discrete hypotheses. In most such cases, this space is (at least) exponentially
large in the size of the problem, and hence, for reasonably sized problems, it cannot simply be
enumerated to ﬁnd the optimal solution. In other problems the solution space is continuous, so
that enumeration is not even an option.
The available tools for solving an optimization problem depend both on the form of the
solution space Σ and on the form of the objective. For some classes of problems, we can
identify the optimum in terms of a closed-form expression; for others, there exist algorithms
that can provably ﬁnd the optimum eﬃciently (in polynomial time), even when the solution
space is large (or inﬁnite); others are NP-hard; and yet others do not (yet) have any theoretical
analysis of their complexity. Throughout this book, multiple optimization problems arise, and
we will see examples of all of these cases.
A.4.2
Local Search
Many optimization problems do not appear to admit tractable solution algorithms exist, and
we are forced to fall back on heuristic methods that have no guarantees of actually ﬁnding the
optimal solution. One such class of methods that are in common use is the class of local search
local search
methods. Such search procedures operate over a search space. A search space is a collection
search space
of candidate solutions, often called search states. Each search state is associated with a score
search state
and a set of neighboring states. A search procedure is a procedure that, starting from one state,
explores search space in attempt to ﬁnd a high-scoring state.
Local search algorithms keep track of a “current” state. At each iteration they consider several
states that are “similar” to the current one, and therefore are viewed as adjacent to it in the
search space. These states are often generated by a set of search operators, each of which takes a
search operators
state and makes a small modiﬁcation to it. They select one of these neighboring states and make
it the current candidate. These iterations are repeated until some termination condition. These
local search procedures can be thought of as moving around in the solution space by taking
small steps. Generally, these steps are taken in a direction that tends to improve the objective.
If we assume that “similar” solutions tend to have similar values, this approach is likely to move
toward better regions of the space.

A.4. Combinatorial Optimization and Search
1155
This approach can be applied to a broad range of problems. For example, we can use it to ﬁnd
a MAP assignment relative to a distribution P: the space of solutions is the set of assignments
MAP assignment
ξ to a set of random variables X; the objective function is P(ξ); and the search operators take
one assignment x and change the value of one variable Xi from xi to x′
i. As we discuss in
section 18.4, it can also be used to perform structure search over the space of Bayesian network
structure search
structures to ﬁnd one that optimizes a certain “goodness” function: the search space is the set
of network structures, and the search operators make small changes to the current structure,
such as adding or deleting an edge.
Algorithm A.5 Greedy local search algorithm with search operators
Procedure Greedy-Local-Search (
σ0,
// initial candidate solution
score,
// Score function
O,
// Set of search operators
)
1
σbest ←σ0
2
do
3
σ ←σbest
4
Progress ←false
5
for each operator o ∈O
6
σo ←o(σ)
// Result of applying o on σ
7
if σo is legal solution then
8
if score(σo) > score(σbest) then
9
σbest ←σo
10
Progress ←true
11
while Progress
12
13
return σbest
A.4.2.1
Local Hill Climbing
One of the simplest, and often used, search procedures is the greedy hill-climbing procedure. As
greedy
hill-climbing
the name suggests, at each step we take the step that leads to the largest improvement in the
score. This is the search analogue of a continuous gradient-ascent method; see appendix A.5.2.
The actual details of the procedure are shown in algorithm A.5. We initialize the search with
some solution σ0.
Then we repeatedly execute the following steps: We consider all of the
solutions that are neighbors of the current one, and we compute their score. We then select the
neighbor that leads to the best improvement in the score. We continue this process until no
modiﬁcation improves the score. One issue with this algorithm is that the number of operators
that can be applied may be quite large. A slight variant of this algorithm, called ﬁrst-ascent hill
ﬁrst-ascent hill
climbing
climbing, samples operators from O and evaluates them one at a time. Once it ﬁnds one that
leads to better scoring network, it applies it without considering other operators. In the initial
stages of the search, this procedure requires relatively few random trials before it ﬁnds such an

1156
Appendix A. Background Material
operator. As we get closer to the local maximum, most operators hurt the score, and more trials
are needed before an upward step is found (if any).
What can we say about the solution returned by Greedy-Local-Search? From our stopping
criterion, it follows that the score of this solution is no lower than that of its neighbors. This
implies that we are in one of two situations. We might have reached a local maximum from
local maximum
which all changes are score-reducing. Except in rare cases, there is no guarantee that the

local maximum we ﬁnd via local search is actually the global optimum σ∗. Indeed, it
may be a very poor solution. The other option is that we have reached a plateau: a large set
plateau
of neighboring solutions that have the same score. By design, greedy hill-climbing procedure
cannot “navigate” through a plateau, since it relies on improvement in score to guide it to better
solutions. Once again, we have no guarantee that this plateau achieves the highest possible
score.
There are many modiﬁcations to this basic algorithm, mostly intended to address this problem.
We now discuss some basic ideas that are applicable to all local search algorithms. We defer to
the main text any detailed discussion of algorithms speciﬁc to problems of interest to us.
A.4.2.2
Forcing Exploration in New Directions
One common approach is to try to escape a suboptimal convergence point by systematically
exploring the region around that point with the hope of ﬁnding an “outlet” that leads to a new
direction to climb up. This can be done if we are willing to record all networks we “visited”
during the search. Then, instead of choosing the best operator in Line 7 of Greedy-Local-Search,
we select the best operator that leads to a solution we have not visited. We then allow the search
to continue even when the score does not improve (by changing the termination condition). This
variant can take steps that explore new territories even if they do not improve the score. Since
it is greedy in nature, it will try to choose the best network that was not visited before. To
understand the behavior of this method, visualize it climbing to the hilltop. Once there, the
procedure starts pacing parts of the hill that were not visited before. As a result, it will start
circling the hilltop in circles that grow wider and wider until it ﬁnds a ridge that leads to a new
hill. (This procedure is often called basin ﬂooding in the context of minimization problems.)
basin ﬂooding
In this variant, even when no further progress can be made, the algorithm keeps moving,
trying to ﬁnd new directions. One possible termination condition is to stop when no progress
has been made for some number of steps.
Clearly, the ﬁnal solution produced should not
necessarily be the one at which the algorithm stops, but rather the best solution found anywhere
during the search. Unfortunately, the computational cost of this algorithm can be quite high,
since it needs to keep track of all solutions that have been visited in the past.
Tabu search is a much improved variant of this general idea utilizes the fact that the steps in
tabu search
our search space take the form of local modiﬁcations to the current solution. In tabu search, we
keep a list not of solutions that have been found, but rather of operators that we have recently
applied. In each step, we do not consider operators that reverse the eﬀect of operators applied
within a history window of some predetermined length L. Thus, if we ﬂip a variable Xi from xi
to x′
i, we cannot ﬂip it back in the next L steps. These restrictions force the search procedure
to explore new directions in the search space, instead of tweaking with the same parts of the
solution. The size L determines the amount of memory retained by the search.
The tabu search procedure is shown in algorithm A.6. The “tabu list” is the list of operators

A.4. Combinatorial Optimization and Search
1157
Algorithm A.6 Local search with tabu list
Procedure LegalOp (
o,
// Search operator to check
TABU
// List of recently applied operators
)
1
if exists o′ ∈TABU such that o reverses o′ then return false
2
else return true
3
Procedure Tabu-Structure-Search (
σ0,
// initial candidate solution
score,
// Score
O,
// A set of search operators
L,
// Size of tabu list
N,
// Stopping criterion
)
1
σbest ←σ0
2
σ ←σbest
3
t ←1
4
LastImprovement ←0
5
while LastImprovement < N
6
o(t) ←ϵ
// Set current operator to be uninitialized
7
for each operator o ∈O
// Search for best allowed operator
8
if LegalOp(o, {o(t−L), . . . , o(t−1)}) then
9
σo ←o(σ)
10
if σo is legal solution then
11
if o(t) = ϵ or score(σo) > score(σot) then
12
o(t) ←o
13
σ ←σot
14
if score(σ) > score(σbest) then
15
σbest ←σo
16
LastImprovement ←0
17
else
18
LastImprovement ←LastImprovement + 1
19
t ←t + 1
20
21
return σbest

1158
Appendix A. Background Material
applied in the last L steps. The procedure LegalOp checks if a new operator is legal given
the current tabu list. The implementation of this procedure depends on the exact nature of
operators we use. As in the basin-ﬂooding approach, tabu search does not stop when it reaches
a solution that cannot be improved, but rather continues the search with the hope of reaching
a better structure. If this does not happen after a prespeciﬁed number of steps, we decide to
abandon the search.
Algorithm A.7 Beam search
Procedure Beam-Search (
σ0,
// initial candidate solution
score,
// Score
O,
// A set of search operators
K,
// Beam width
)
1
Beam ←{σ0}
2
while not terminated
3
H ←∅
// Current successors
4
for each σ ∈L and each o ∈O
5
Add o(σ) to H
6
Beam ←K-Best(score, H, K)
7
σbest ←K-Best(score, H, 1)
8
return (σbest)
Another variant that forces a more systematic search of the space is beam search. In beam
beam search
search, we conduct a hill-climbing search, but we keep track of a certain ﬁxed number K of
states. The value K is called the beam width. At each step in the search, we take all of the
current states and generate and evaluate all of their successors. The best K are kept, and the
algorithm repeats. The algorithm is shown in algorithm A.7. Note that with a beam width of 1,
beam search reduces to greedy hill-climbing search, and with an inﬁnite beam width, it reduces
to breadth-ﬁrst search. Note that this version of beam search assumes that the (best) steps taken
during the search always improve the score. If that is not the case, we would also have to
compare the current states in our beam Beam to the new candidates in H in order to determine
the next set of states to put in the beam. The termination condition can be an upper bound on
the number of steps or on the improvement achieved in the last iteration.
A.4.2.3
Randomization in Search
Another approach that can help in reducing the impact of local maxima is randomization. Here,
randomization
multiple approaches exist. We note that most randomization procedures can be applied as a
wrapper to a variety of local search algorithm, including both hill climbing and tabu search.
Most simply, we can initialize the algorithm at diﬀerent random starting points, and then use
a hill-climbing algorithm from each one. Another strategy is to interleave random steps and
hill-climbing steps. Here, many strategies are possible. In one approach, we can “revitalize” the
search by taking the best network found so far and applying several randomly chosen operators

A.4. Combinatorial Optimization and Search
1159
Algorithm A.8 Greedy hill-climbing search with random restarts
Procedure Search-with-Restarts (
σ0,
// initial candidate solution
score,
// Score
O,
// A set of search operators
Search,
// Search procedure
l,
// random restart length
k
// number of random restarts
)
1
σbest ←Search(σ0, score, O)
2
for i = 1, . . . , k
3
σ ←σbest
4
// Perform random walk
5
j ←1
6
while j < l
7
sample o from O
8
if o(σ) is a legal network then
9
σ ←o(σ)
10
j ←j + 1
11
σ ←Search(σ, score, O)
12
if score(σ) > score(σbest) then
13
σbest ←σ
14
15
return σbest
to get a network that is fairly similar, yet perturbed. We then restart our search procedure from
the new network. If we are lucky, this random restart step moves us to a network that belongs
random restart
to a better “basin of attraction,” and thus the search will converge to a better structure. A simple
random restart procedure is shown in algorithm A.8; it can be applied as wrapper to plain hill
climbing, tabu search, or any other search algorithm.
This approach can be eﬀective in escaping from fairly local maxima (which can be thought
of as small bumps on the slope of a larger hill). However, it is unlikely to move from one
wide hill to another. There are diﬀerent choices in applying random restart, the most important
one is how many random “steps” to take. If we take too few, we are unlikely to escape the
local maxima. If we take too many, than we move too far oﬀfrom the region of high scoring
network. One possible strategy is to applying random restarts of growing magnitude. That is,
each successive random restart applies more random operations.
To make this method concrete, we need a way of determining how to apply random restarts,
and how to interleave hill-climbing steps and randomized moves. A general framework for doing
is simulated annealing. The basic idea of simulated annealing is similar to Metropolis-Hastings
simulated
annealing
MCMC methods that we discuss in section 12.3, and so we only brieﬂy touch it.
In broad outline, the simulated annealing procedure attempts to mix hill-climbing steps with

1160
Appendix A. Background Material
moves that can decrease the score.
This mixture is controlled by a so-called temperature
temperature
parameter
parameter. When the temperature is “hot,” the search tries many moves that decrease the score.
As the search is annealed (the temperature is slowly reduced) it starts to focus only on moves
that improve the score. The intuition is that during the “hot” phase the search explores the
space and eventually gets trapped in a region of high scores. As the temperature reduces it is
able to distinguish between ﬁner details of the score “landscape” and eventually converge to a
good maximum.
To carry out this intuition, a simulated annealing procedure uses a proposal distribution over
proposal
distribution
operators to propose candidate operators to apply in the search. At each step, the algorithm
selects an operator o using this distribution, and evaluates δ(o) — the change in score incurred
by applying o at the current state. The search accepts this move with probability min(1, e
δ(o)
τ ),
where τ is the current temperature. Note that, if δ(o) > 0, the move is automatically accepted.
If δ(o) < 0, the move is accepted with probability that depends both on the decrease in score
and on the temperature τ. For large value of τ (hot) all moves are applied with probability
close to 1. For small values of τ (cold), all moves that decrease the score are applied with small
probability. The search procedure anneals τ every ﬁxed number of move attempts. There are
various strategies for annealing; the simplest one is simply to have τ decay exponentially. One
can actually show that, if the temperature is annealed suﬃciently slowly, simulated annealing
converges to the globally optimal solution with high probability.
However, in practice, this
“guaranteed” annealing schedule is both unknown and much too slow to be useful in practice.
In practice, the success of simulated annealing depends heavily on the design of the

proposal distribution and annealing schedule.
A.4.3
Branch and Bound Search
Here we discussed one class of solutions to discrete optimization problems: the class of local
hill-climbing search. Those methods are very broadly useful, since they apply to any discrete
optimization problem for which we can deﬁne a set of search operators. In some cases, however,
we may know some additional structure within the problem, allowing more informed methods
to be applied. One useful type of information is a mechanism that allows us to evaluate a
partial assignment y1...i, and to place a bound bound(y1...i) on the best score of any complete
assignment that extends y1...i. In this case, we can use an algorithm called branch and bound
branch and
bound
search, shown in algorithm A.9 for the case of a maximization problem.
Roughly speaking, branch and bound searches the space of partial assignments, beginning
with the empty assignment, and assigning the variables X1, . . . , Xn, one at a time (in some
order), using depth-ﬁrst search. At each point, when considering the current partial assignment
y1...i, the algorithm evaluates it using bound(y1...i) and compares it to the best full assignment
ξ found so far. If score(ξ) is better than the best score that can possibly be achieved starting
from y1...i, then there is no point continuing to explore any of those assignments, and the
algorithm backtracks to try a diﬀerent partial assignment. Because the bound is correct, it is not
diﬃcult to show that the assignments that were pruned without being searched cannot possibly
be optimal. When the bound is reasonably tight, this algorithm can be very eﬀective, pruning
large parts of the space without searching it.
The algorithm shows the simplest variant of the branch-and-bound procedure, but many
extensions exist. One heuristic is to perform the search so as to try and ﬁnd good assignments

A.5. Continuous Optimization
1161
Algorithm A.9 Branch and bound algorithm
Procedure Branch-and-Bound (
score,
// Score function
bound,
// Upper bound function
σbest,
// Best full assignment so far
scorebest,
// Best score so far
i,
// Variable to be assigned next
y1...i−1,
// Current partial assignment
)
// Recursive algorithm, called initially with the following argu-
ments: some arbitrary full assignment σbest, scorebest =
score(σbest), i = 1, and the empty assignment.
1
for each xi ∈Val(Xi)
2
y1...i ←(y1...i−1, xi)
// Extend the assignment
3
if i = n and score(y1...n) > scorebest then
4
(σbest, scorebest) ←(y1...n, score(y1...n))
5
// Found a better full assignment
6
else if bound(y1...i) > scorebest then
7
(σbest, scorebest) ←Branch-and-Bound(score, bound, σbest, scorebest, i+1, y1...i)
8
// If bound is better than current solution, try current partial
assignment; otherwise, prune and move on
9
return (σbest, scorebest)
early. The better the current assignment σbest, the better we can prune suboptimal trajectories.
Other heuristics intelligently select, at each point in the search, which variable to assign next,
allowing this choice to vary across diﬀerent points in the search. When available, one can also
use a lower bound as well as an upper bound, allowing pruning to take place based on partial
(not just full) trajectories. Many other extensions exist, but are outside the scope of this book.
A.5
Continuous Optimization
In the preceding section, we discussed the problem of optimizing an objective over a discrete
space.
In this section we brieﬂy review methods for solving optimization problems over a
continuous space. See Avriel (2003); Bertsekas (1999) for more thorough discussion of nonlin-
ear optimization, and see Boyd and Vandenberghe (2004) for an excellent overview of convex
optimization methods.
A.5.1
Characterizing Optima of a Continuous Function
At several points in this book we deal with maximization (or minimization) problems. In these
problems, we have a function fobj(θ1, . . . , θn) for several parameters, and we wish to ﬁnd joint
values of the parameters that maximizes the value of fobj.
Formally, we face the following problem:
Find values θ1, . . . , θn such that fobj(θ1, . . . , θn) = maxθ′
1,...,θ′n fobj(θ′
1, . . . , θ′
n).

1162
Appendix A. Background Material
Example A.2
Assume we are given a set of points (x[1], y[1]), . . . , (x[m], y[m]). Our goal is to ﬁnd the “centroid”
of these points, deﬁned as a point (θx, θy) that minimizes the square distance to all of the points.
We can formulate this problem into a maximization problem by considering the negative of the
sum of squared distances:
fobj(θx, θy) = −
X
i
 (x[i] −θx)2 + (y[i] −θy)2
.
One way of ﬁnding the maximum of a function is to use the fact that, at the maximum,
the gradient of the function is 0. Recall that the gradient of a function fobj(θ1, . . . , θn) is the
gradient
vector of partial derivatives
∇f =
 ∂f
∂θ1
, . . . ∂f
∂θn

.
Theorem A.5
If ⟨θ1, . . . , θn⟩is an interior maximum point of fobj, then
∇f(θ1, . . . , θn) = 0.
This property, however, does not characterize maximum points. Formally, a point ⟨θ1, . . . , θn⟩
where ∇fobj(θ1, . . . , θn) = 0 is a stationary point of fobj. Such a point can be either a local
stationary point
maximum, a local minimum, or a saddle point. However, ﬁnding such a point can often be the
ﬁrst step toward ﬁnding a maximum.
To satisfy the requirement that ∇f = 0 we need to solve the set of equations
∂
∂θk
fobj(θ1, . . . , θn) = 0
k = 1, . . . , n
Example A.3
Consider the task of example A.2. We can easily verify that:
∂
∂θx
fobj(θx, θy) = 2
X
i
(x[i] −θx).
Equating this term to 0 and performing simple arithmetic manipulations, we get the equation:
θx = 1
m
X
i
x[i].
The exact same reasoning allows us to solve for θy.
In this example, we conclude that fobj has a unique stationary point.
We next need to
verify that this point is a maximum point (rather than a minimum or a saddle point). In our
example, we can check that, for any sequence that extends from the origin to inﬁnity (that is,
θ2
x + θ2
y →∞), we have fobj →−∞. Thus, the single stationary point is a maximum.
In general, to verify that the stationary point is a maximum, we can check that the second
derivative is negative. To see this, recall the multivariate Taylor expansion of degree two:
fobj(⃗θ) = fobj(⃗θ0) + (⃗θ −⃗θ0)T ∇fobj(⃗θ0) + 1
2
h
⃗θ −⃗θ0
iT
A(⃗θ0)
h
⃗θ −⃗θ0
i
,

A.5. Continuous Optimization
1163
where A(⃗θ0) is the Hessian — the matrix of second derivatives at ⃗θ0. If we use this expansion
Hessian
around a stationary point, then the gradient is 0, and we only need to examine the term
[⃗θ −⃗θ0]T A(⃗θ0)[⃗θ −⃗θ0]. In the univariate case, we can verify that a point is a local maximum by
testing that the second derivative is negative. The analogue to this condition in the multivariate
case is that A(⃗θ0) is negative deﬁnite at the point ⃗θ0, that is, that ⃗θT A(⃗θ0)⃗θ < 0 for all ⃗θ.
negative deﬁnite
Theorem A.6
Suppose ⃗θ = ⟨θ1, . . . , θn⟩is an interior point of fobj with ∇⃗θ = 0. Let A(⃗θ) = {
∂2
∂θi∂θj fobj(⃗θ)}
be the Hessian matrix of second derivatives of fobj at ⃗θ. A(⃗θ) is negative deﬁnite at θ if and only
if ⃗θ is a local maximum of fobj.
Example A.4
Continuing example A.2, we can verify that
∂2
∂2
θx
fobj(θx, θy)
=
−2m
∂2
∂2
θy
fobj(θx, θy)
=
−2m
∂2
∂θy∂θx
fobj(θx, θy)
=
0
∂2
∂θx∂θy
fobj(θx, θy)
=
0.
Thus, the Hessian matrix is simply
A =

−2m
0
0
−2m

.
It is easy to verify that this matrix is negative deﬁnite.
A.5.2
Gradient Ascent Methods
The characterization of appendix A.5.1 allows us to provide closed-form solutions for certain
continuous optimization problems. However, there are many problems for which such solutions
cannot be found. In these problems, the equations posed by ∇fobj(θ) = 0 do not have an
analytical solution. Moreover, in many practical problems, there are multiple local maxima, and
then this set of equations does not even have a unique solution.
One approach for dealing with problems that do not yield analytical solutions is to search for
a (local) maximum. The idea is very analogous to the discrete local search of appendix A.4.2:
We begin with an initial point θ0, which can be an arbitrary choice, a random guess, or an
approximation of the solution based on other considerations. From this starting point, we want
to “climb” to a maximum. A great many techniques roughly follow along these lines. In this
section, we survey some of the most common ones.
A.5.2.1
Gradient Ascent
The simplest approach is gradient ascent, an approach directly analogous to the hill-climbing
gradient ascent

1164
Appendix A. Background Material
Algorithm A.10 Simple gradient ascent algorithm
Procedure Gradient-Ascent (
θ1,
// Initial starting point
fobj,
// Function to be optimized
δ
// Convergence threshold
)
1
t ←1
2
do
3
θt+1 ←θt + η∇fobj(θt)
4
t ←t + 1
5
while ∥θt −θt−1∥> δ
6
return (θt)
search of algorithm A.5 (see appendix A.4.2). Using the Taylor expansion of a function, we know
that, in the neighborhood of θ0, the function can be approximated by the linear equation
fobj(θ) ≈fobj(θ0) + (θ −θ0)T ∇fobj(θ0).
Using basic properties of linear algebra, we can check that the slope of this linear function, that
is, ∇fobj(θ0), points to the direction of the steepest ascent. This observation suggests that, if
we take a step in the direction of the gradient, we increase the value of fobj. This reasoning
leads to the simple gradient ascent algorithm shown in algorithm A.10. Here, η is a constant
that determines the rate of ascent at each iteration. Since the gradient ∇fobj approaches 0 as
we approach a maximum point, the procedure will converge if η is suﬃciently small.
Note that, in order to apply gradient ascent, we need to be able to evaluate the function fobj
at diﬀerent points, and also to evaluate its gradient. In several examples we encounter in this
book, we can perform these calculations, although in some cases these are costly. Thus, a major
objective is to reduce the number of points at which we evaluate fobj or ∇fobj.
The performance of gradient ascent depends on the choice of η. If η is too large, then the
algorithm can “overshoot” the maximum in each iteration. For suﬃciently small value of η, the
gradient ascent algorithm will converge, but if η is too small, we will need many iterations to
converge. Thus, one of the diﬃcult points in applying this algorithm is deciding on the value of
η. Indeed, in practice, one typically needs to begin with a large η, and decrease it over time;
this approach leaves us with the problem of choosing an appropriate schedule for shrinking η.
A.5.2.2
Line Search
An alternative approach is to adaptively choose the step size η at each step. The intuition is
that we choose a direction to climb and continue in that direction until we reach a point where
we start to descend. In this procedure, at each point θt in the search, we deﬁne a “line” in the
direction of the gradient:
g(η) = ⃗θt + η∇fobj(θt).
We now use a line search procedure to ﬁnd the value of η that deﬁnes a (local) maximum of
line search

A.5. Continuous Optimization
1165
h1
h2
h’
h3
Figure A.2
Illustration of line search with Brent’s method. The solid line shows a one-dimensional
function. The three points, η1, η2, and η3, bracket the maximum of this function. The dashed line shows
the quadratic ﬁt to these three points and the choice of η′ proposed by Brent’s method.
fobj along the line; that is, we ﬁnd:
ηt = arg max
η
g(η).
We now take an ηt-sized step in the direction of the gradient; that is, we deﬁne:
θt+1 ←θt + ηt∇fobj(θt).
And the process repeats.
There are several methods for performing the line search. The basic idea is to ﬁnd three
points η1 < η2 < η3 so that fobj(g(η2)) is larger than both fobj(g(η1)) and fobj(g(η3)).
In this case, we know that there is at least one local maximum between η1 and η3, and we
say that η1, η2 and η3 bracket a maximum; see ﬁgure A.2 for an illustration. Once we have
a method for ﬁnding a bracket, we can zoom in on the maximum. If we choose a point η′
so that η1 < η′ < η2 we can ﬁnd a new, tighter, bracket. To see this, we consider the two
possible cases. If fobj(g(η′)) > fobj(g(η2)), then η1, η′, η2 bracket a maximum. Alternatively,
if fobj(g(η′)) ≤fobj(g(η2)), then η′, η2, η3 bracket a maximum. In both cases, the new bracket
is smaller than the original one. Similar reasoning applies if we choose η′ between η2 and η3.
The question is how to choose η′. One approach is to perform a binary search and choose
η′ = (η1 + η3)/2. This ensures that the size of the new bracket is half of the old one. A faster
approach, known as Brent’s method, ﬁts a quadratic function based on the values of fobj at the
three points η1, η2, and η3. We then choose η′ to be the maximum point of this quadratic
approximation. See ﬁgure A.2 for an illustration of this method.
A.5.2.3
Conjugate Gradient Ascent
Line search attempts to maximize the improvement along the direction deﬁned by ∇fobj(θt).
This approach, however, often has undesired consequences on the convergence of the search.
To understand the problem, we start by observing that ∇fobj(θt+1) must be orthogonal to

1166
Appendix A. Background Material
(a)
(b)
Figure A.3
Two examples of the convergence problem with line search. The solid line shows the
progression of gradient ascent with line search. The dashed line shows the progression of the conjugate
gradient method: (a) a quadratic function fobj(x, y) = −(x2 + 10y2); (b) its exponential fobj(x, y) =
exp{−(x2 + 10y2)}. In both cases, the two search procedures start from the same initial point (bottom
left of the ﬁgure), and diverge after the ﬁrst line search.
∇fobj(θt).
To see why, observe that θt+1 was chosen to be a local maximum along the
∇fobj(θt) direction. Thus, the gradient of fobj at θt+1 must be 0 in this direction. This implies
that the two consecutive gradient vectors are orthogonal. As a consequence, the progress of the
gradient ascent will be in a zigzag line. As the procedure approaches a maximum point, the size
of each step becomes smaller, and the progress slows down. See ﬁgure A.3 for an illustration of
this phenomenon.
A possible solution is to “remember” past directions of search and to bias the new direction
to be a combination of the gradient at the current point and the direction implied by previous
steps. This intuitive idea can be developed into a variety of algorithms. It turns out, however,
that one variant of this algorithm can be shown to be optimal for ﬁnding the maximum of
quadratic functions. Since, by the Taylor expansion, all functions are approximately quadratic in
the neighborhood of a maximum, it follows that the ﬁnal steps of the algorithm will converge to
a maximum relatively quickly.
The algorithm, known as conjugate gradient ascent, is shown in algorithm A.11. The vector ht
conjugate
gradient ascent
is the “corrected” direction for search. It combines the gradient gt with the previous direction
of search ht−1. The eﬀect of previous search directions on the new one depends on the relative
sizes of the gradients.
If our function fobj is a quadratic function, the conjugate gradient ascent procedure is
guaranteed to converge in n steps, where n is the dimension of the space. Indeed, in ﬁgure A.3a
we see that the conjugate method converges in two steps. When the function is not quadratic,
conjugate gradient ascent might require more steps, but is still much faster than standard
gradient ascent. For example, in ﬁgure A.3b, it converges in four steps (the last step is too small
to be visible in the ﬁgure).
Finallly, we note that gradient ascent is the continuous analogue of the local hill-climbing

approaches described in section A.4.2. As such, it is susceptible to the same issues of
local maxima and plateaus. The approaches used to address these issues in this setting
are similar to those outlined in the discrete case.

A.5. Continuous Optimization
1167
Algorithm A.11 Conjugate gradient ascent
Procedure Conjugate-Gradient-Ascent (
θ1,
// Initial starting point
fobj,
// Function to be optimized
δ
// Convergence threshold
)
1
t ←1
2
g0 ←1
3
h0 ←0
4
do
5
gt ←∇fobj(θt)
6
γt ←
(gt−gt−1)T gt
(gt−1)T gt−1
7
ht ←gt + γtht−1
8
Choose ηt by line search along the line θt + ηht
9
θt+1 ←θt + ηtht
10
t ←t + 1
11
while ∥θt −θt−1∥> δ
12
return (θt)
A.5.3
Constrained Optimization
In appendix A.5.1, we considered the problem of optimizing a continuous function over its entire
domain (see also appendix A.5.2). In many cases, however, we have certain constraints that the
desired solution must satisfy. Thus, we have to optimize the function within a constrained space.
We now review some basic methods that address this problem of constrained optimization.
constrained
optimization
Example A.5
Suppose we want to ﬁnd the maximum entropy distribution over a variable X, with Val(X) =
{x1, . . . , xK}. Consider the entropy of X:
IH(X) = −
K
X
k=1
P(xk) log P(xk).
We can maximize this function using the gradient method by treating each P(xk) as a separate
parameter θk. We compute the gradient of IHP (X) with respect to each of these parameters:
∂
∂θk
IH(X) = −log(θk) −1.
Setting this partial derivative to 0, we get that log(θk) = −1, and thus θk = 1/2. This solution
seems ﬁne until we realize that the numbers do not sum up to 1, and hence our solution does not
deﬁne a probability distribution!
The ﬂaw in our analysis is that we want to maximize the entropy subject to a constraint on
the parameters, namely, P
k θk = 1. In addition, we also remember that we need to require that
θk ≥0. In this case we see that the gradient drives the solution away from from 0 (−log(θk) →∞
as θk →0), and thus we do not need to enforce this constraint actively.

1168
Appendix A. Background Material
Problems of this type appear in many settings, where we are interested in maximizing a
function ff under a set of equality constraints. This problem is posed as follows:
equality
constraint
Find
θ
maximizing
ff(θ)
subject to
c1(θ)
=
0
. . .
cm(θ)
=
0.
(A.5)
Note that any equality constraint (such as the one in our example above) can be rephrased as
constraining a function c to 0. Formally, we are interested in the behavior of ff in the region of
points that satisﬁes all the constraints
C = {θ : ∀j = 1, . . . , n, cj(θ) = 0}.
To deﬁne our goal, remember that we want to ﬁnd a maxima point within C. Since C is a
constrained “surface” we need to adopt the basic deﬁnition of maxima (and similarly minima,
stationary point, etc.) to this situation. We can deﬁne local maxima in two ways. The ﬁrst
deﬁnition is in term of neighborhood. We deﬁne the ϵ-neighborhood of θ in C to be all the
points θ′ ∈C such that ||θ −θ′||2 < ϵ. We then say that θ is a local maxima in C if there
is an ϵ > 0 such that ff(θ) > ff(θ′) for all θ′ in its ϵ-neighborhood. An alternative deﬁnition
that will be easier for the following is in terms of derivatives. Recall that a stationary point
(local maximum, local minimum, or a saddle point) of a function if the derivative is 0. In the
constraint case we have a similar deﬁnition, but we must ensure that the derivatives are ones
that do not take us outside the constrained surface. Stated diﬀerently, if we consider a derivative
in the direction δ, we want to ensure that the constraints remain 0 if we take a small step in
direction δ. Formally, this means that the derivative has to be tangent to each constraint ci, that
is δT ∇ci(θ) = 0.
A general approach to solving such constrained optimization problems is the method of
Lagrange multipliers. We deﬁne a new function, called the Lagrangian, of θ and of a new vector
Lagrange
multipliers
of parameters λ = ⟨λ1, . . . , λm⟩
J (θ, λ) = ff(θ) −
m
X
j=1
λjcj(θ).
Theorem A.7
If ⟨θ, λ⟩is a stationary point of the Lagrangian J , then θ is a stationary point of ff subject to the
constraints c1(θ) = 0, . . . , cm(θ) = 0.
Proof We brieﬂy outline the proof. A formal proof requires the use of more careful tools from
functional analysis.
We start by showing that θ satisﬁes the constraints. Since ⟨θ, λ⟩is a stationary point of J ,
we have that for each j
∂
∂λj
J (θ, λ) = −cj(θ).

A.5. Continuous Optimization
1169
Thus, at stationary points of J , the constraint cj(θ) = 0 must be satisﬁed.
Now consider ∇ff(θ). For each component θi of θ, we have that
0 = ∂
∂θi
J (θ, λ) = ∂
∂θi
ff(θ) −
X
j
λj
∂
∂θi
cj(θ).
Thus,
∇ff(θ) =
X
j
λj∇cj(θ).
(A.6)
In other words, the gradient of ff is a linear combination of the gradients of cj.
We now use this property to prove that θ is a stationary point of ff when constrained to region
C. Consider a direction δ that is tangent to the region C at θ. As δ is tangent to C, we expect
that moving inﬁnitesimally in this direction will maintain the constraint that cj is 0; that is, cj
should not change its value when we move in this direction. More formally, the derivative of cj
in the direction δ is 0. The derivative of cj in a direction δ is δT ∇cj. Thus, if δ is tangent to
C, we have
δT ∇cj(θ) = 0
for all j. Using equation (A.6), we get
δT ∇ff(θ) =
X
j
λjδT ∇cj(θ) = 0.
Thus, the derivative of ff in a direction that is tangent to C is 0. This implies that when moving
away from θ within the allowed region C the value of ff has 0 derivative. Thus, θ is a stationary
point of ff when restricted to C.
We also have the converse property: If ff satisﬁes some regularity conditions, then for every
stationary point of ff in C there is a choice of λ so that ⟨θ, λ⟩is a stationary point of J .

We see that the Lagrangian construction allows us to solve constrained optimization
problems using tools for unconstrained optimization. We note that a local maximum of ff
always corresponds to a stationary point of J , but this stationary point is not necessarily a local
maximum of J . If, however, we restrict attention to nonnegative constraint functions c, then a
local maximum of ff must correspond to a local maximum of J .
We now consider two examples of using this technique.
Example A.6
Let us return to example A.5. In order to ﬁnd the maximum entropy distribution over X, we need
to solve the Lagrangian
J = −
X
k
θk log θk −λ
 X
k
θk −1
!
.

1170
Appendix A. Background Material
Setting ∇J = 0 implies the following system of equations:
0
=
−log θ1 −1 −λ
. . .
0
=
−log θK −1 −λ
0
=
X
k
θk −1.
Each of the ﬁrst K equations can be rewritten as θk = 2−1−λ. Plugging this term into the last
equation, we get that λ = log(K) −1, and thus P(xk) = 1/K. We conclude that we achieve
maximum entropy with the uniform distribution.
To see an example with more than one constraint, consider the following problem.
Example A.7
Suppose we have a distribution P(X, Y ) over two random variables, and we want to ﬁnd the
closest distribution Q(X, Y ) in which X is independent of Y . As we discussed in section 8.5,
this process is called M-projection (see deﬁnition 8.4). Since X and Y are independent in Q, we
M-projection
must have that Q(X, Y ) = Q(X)Q(Y ). Thus, we are searching for parameters θx = Q(x) and
θy = Q(y) for diﬀerent values x ∈Val(X) and y ∈Val(Y ).
Formally, we want to solve the following problem:
Find {θx : x ∈Val(X)} and {θy : y ∈Val(y)} that minimize
ID(P(X, Y )||Q(X)Q(Y )) =
X
x
X
y
P(x, y) log P(x, y)
θxθy
,
subject to the constraints
0
=
X
x
θx −1
0
=
X
y
θy −1.
We deﬁne the Lagrangian
J =
X
x
X
y
P(x, y) log P(x, y)
θxθy
−λx
 X
x
θx −1
!
−λy
 X
y
θy −1
!
.
To simplify the computation of derivatives, we notice that
log P(x, y)
θxθy
= log P(x, y) −log θx −log θy.
Using this simpliﬁcation, we can compute the derivative with respect to the probability of a par-
ticular value of X, say θxk. We note that this parameter appears only when the value of x in the
summation equals xk. Thus,
∂
∂θxk J = −
X
y
P(xk, y)
θxk
−λx.

A.5. Continuous Optimization
1171
Equating this derivative to 0, we get
θxk = −
P
y P(xk, y)
λx
= −P(xk)
λx
.
To solve for the value of λx, we use the ﬁrst constraint, and get that
1 =
X
x
θx = −
X
x
P(x)
λx
.
Thus, we get that λx = −P
x P(x). Thus, we can conclude that λx = −1, and consequently that
θx = P(x). An analogous reasoning shows that θy = P(y).
This solution is very natural.
The closest distribution to P(X, Y ) in which X and Y are
independent is Q(X, Y ) = P(X)P(Y ). This distribution preserves the marginal distributions of
both X and Y , but loses all information about their joint behavior.
A.5.4
Convex Duality
The concept of convex duality plays a central role in optimization theory. We brieﬂy review the
convex duality
main results here for equality-constrained optimization problems with nonnegativity constraints
(although the theory extends quite naturally to the case of general inequality constraints).
In appendix A.5.3, we considered an optimization problem of maximizing ff(θ) subject to
certain constraints, which we now call the primal problem. We showed how to formulate a
Lagrangian J (θ, λ), and proved that if ⟨θ, λ⟩is a stationary point of J then θ is a stationary
point of the objective function ff that we are trying to maximize.
We can extend this idea further and deﬁne the dual function gg(λ) as
gg(λ) = sup
θ≥0
J (θ, λ).
That is, the dual function gg(λ), is the supremum, or maximum, over the parameters θ for a
given λ. In general, we allow the dual function to take the value ∞when J is unbounded
above (which can occur when the primal constraints are unsatisﬁed), and refer to the points λ
at which this happens as dual infeasible.
Example A.8
Let us return to example A.6, where our task is to ﬁnd the distribution P(X) of maximum entropy.
Now, however, we also want the distribution to satisfy the constraint that IEP [X] = µ. Treating
each P(X = k) as a separate parameter θk, we can write our problem formally as:
Constrained-Entropy:
Find
P
maximizing
IHP (X)
subject to
PK
k=1 kθk = µ
PK
k=1 θk = 1
θk ≥0
∀k = 1, . . . , K
(A.7)

1172
Appendix A. Background Material
Introducing Lagrange multipliers for each of the constraints we can write
Lagrange
multipliers
J (θ, λ, ν) = −
K
X
k=1
θk log θk −λ
 K
X
k=1
kθk −µ
!
−ν
 K
X
k=1
θk −1
!
.
Maximizing over θ for each ⟨λ, ν⟩we get the dual function
gg(λ, ν) = sup
θ≥0
J (θ, λ, ν)
= λµ + ν + e−ν−1 X
k
e−kλ.
Thus, the convex dual (to be minimized) is λµ + ν + e−ν−1 P
k e−kλ. We can minimize over ν
analytically by taking derivatives and setting them equal to zero, giving ν = log gg(P
k e−kλ) −1.
Substituting into gg, we arrive at the dual optimization problem
minimize
λµ + log
PK
k=1 e−kλ
.
This form of optimization problem is known as a geometric program. The convexity of the
objective function can be easily veriﬁed by taking second derivatives. Taking the ﬁrst derivative and
setting it to zero provides some insight into the solution to the problem:
PK
k=1 ke−kλ
PK
k=1 e−kλ = µ,
indicating that the solution has θk ∝αk for some ﬁxed α.
Importantly, as we can see in this example, the dual function is a pointwise maximization
over a family of linear functions (of the dual variables). Thus, the dual function is always convex
even when the primal objective function ff is not.
One of the most important results in optimization theory is that the dual function gives an
upper bound on the optimal value of the optimization problem; that is, for any primal feasible
point θ and any dual feasible point λ, we have gg(λ) ≥fobj(θ). This leads directly to the
property of weak duality, which states that the minimum value of the dual function is at least
as large as the maximum value of the primal problem; that is,
gg(λ⋆) = inf
λ gg(λ) ≥ff(θ⋆).
The diﬀerence ff(θ⋆) −gg(λ⋆) is known as the duality gap. Under certain conditions the duality
gap is zero, that is, ff(θ⋆) = gg(λ⋆), in which case we have strong duality. Thus, duality can be
used to provide a certiﬁcate of optimality. That is, if we can show that gg(λ) = ff(θ) for some
value of ⟨θ, λ⟩, then we know that ff(θ) is optimal.
The concept of a dual function plays an important role in optimization. In a number of
situations, the dual objective function is easier to optimize than the primal. Moreover, there are
methods that solve the primal and dual together, using the fact that each bounds the other to
improve the search for an optimal solution.

Bibliography
Abbeel, P., D. Koller, and A. Ng (2006, August). Learning factor graphs in polynomial time &
sample complexity. Journal of Machine Learning Research 7, 1743–1788.
Ackley, D., G. Hinton, and T. Sejnowski (1985). A learning algorithm for Boltzmann machines.
Cognitive Science 9, 147–169.
Aji, S. M. and R. J. McEliece (2000). The generalized distributive law. IEEE Trans. Information
Theory 46, 325–343.
Akaike, H. (1974).
A new look at the statistical identiﬁcation model.
IEEE Transactions on
Automatic Control 19, 716–723.
Akashi, H. and H. Kumamoto (1977). Random sampling approach to state estimation in switching
environments. Automatica 13, 429–434.
Allen, D. and A. Darwiche (2003a). New advances in inference by recursive conditioning. In
Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 2–10.
Allen, D. and A. Darwiche (2003b). Optimal time–space tradeoﬀin probabilistic inference. In
Proc. 18th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 969–975.
Altun, Y., I. Tsochantaridis, and T. Hofmann (2003). Hidden Markov support vector machines. In
Proc. 20th International Conference on Machine Learning (ICML).
Andersen, S., K. Olesen, F. Jensen, and F. Jensen (1989). HUGIN—a shell for building Bayesian
belief universes for expert systems. In Proc. 11th International Joint Conference on Artiﬁcial
Intelligence (IJCAI), pp. 1080–1085.
Anderson, N. (1974). Information integration theory: A brief survey. In Contemporary develop-
ments in Mathematical Psychology, Volume 2, pp. 236–305. San Francisco, California: W.H.
Freeman and Company.
Anderson, N. (1976). How functional measurement can yield validated interval scales of mental
quantities. Journal of Applied Psychology 61(6), 677–692.
Andreassen, S., F. Jensen, S. Andersen, B. Falck, U. Kjærulﬀ, M. Woldbye, A. R. Sørensen, A. Rosen-
falck, and F. Jensen (1989).
MUNIN — an expert EMG assistant.
In J. E. Desmedt (Ed.),
Computer-Aided Electromyography and Expert Systems, Chapter 21. Amsterdam: Elsevier Sci-
ence Publishers.
Anguelov, D., D. Koller, P. Srinivasan, S. Thrun, H.-C. Pang, and J. Davis (2004). The correlated
correspondence algorithm for unsupervised registration of nonrigid surfaces. In Proc. 18th
Conference on Neural Information Processing Systems (NIPS).
Arnauld, A. and P. Nicole (1662). Port-royal logic.

1174
BIBLIOGRAPHY
Arnborg, S. (1985). Eﬃcient algorithms for combinatorial problems on graphs with bounded,
decomposability—a survey. BIT 25(1), 2–23.
Arnborg, S., D. Corneil, and A. Proskurowski (1987). Complexity of ﬁnding embeddings in a
k-tree. SIAM J. Algebraic Discrete Methods 8(2), 277–284.
Attias, H. (1999). Inferring parameters and structure of latent variable models by variational
Bayes. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 21–30.
Avriel, M. (2003). Nonlinear Programming: Analysis and Methods. Dover Publishing.
Bacchus, F. and A. Grove (1995). Graphical models for preference and utility. In Proc. UAI–95, pp.
3–10.
Bach, F. and M. Jordan (2001). Thin junction trees. In Proc. 15th Conference on Neural Information
Processing Systems (NIPS).
Balke, A. and J. Pearl (1994a). Counterfactual probabilities: Computational methods, bounds and
applications. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 46–54.
Balke, A. and J. Pearl (1994b). Probabilistic evaluation of counterfactual queries. In Proc. 10th
Conference on Artiﬁcial Intelligence (AAAI), pp. 230–237.
Bar-Shalom, Y. (Ed.) (1992). Multitarget multisensor tracking: Advanced applications. Norwood,
Massachusetts: Artech House.
Bar-Shalom, Y. and T. Fortmann (1988). Tracking and Data Association. New York: Academic
Press.
Bar-Shalom, Y., X. Li, and T. Kirubarajan (2001). Estimation with Application to Tracking and
Navigation. John Wiley and Sons.
Barash, Y. and N. Friedman (2002). Context-speciﬁc Bayesian clustering for gene expression data.
Journal of Computational Biology 9, 169–191.
Barber, D. and W. Wiegerinck (1998). Tractable variational structures for approximating graphical
models. In Proc. 12th Conference on Neural Information Processing Systems (NIPS), pp. 183–189.
Barbu, A. and S. Zhu (2005).
Generalizing Swendsen-Wang to sampling arbitrary posterior
probabilities. IEEE Trans. on Pattern Analysis and Machine Intelligence 27(8), 1239–1253.
Barnard, S. (1989).
Stochastic stero matching over scale.
International Journal of Computer
Vision 3, 17–32.
Barndorﬀ-Nielsen, O. (1978). Information and Exponential Families in Statistical Theory. Wiley.
Barron, A., J. Rissanen, and B. Yu (1998). The minimum description length principle in coding
and modeling. IEEE Transactions on Information Theory 44(6), 2743–2760.
Bartlett, M. (1935). Contingency table interactions. Journal of the Royal Statistical Society, Series
B 2, 248–252.
Bauer, E., D. Koller, and Y. Singer (1997). Update rules for parameter estimation in Bayesian
networks. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 3–13.
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society of London 53, 370–418.
Beal, M. and Z. Ghahramani (2006). Variational Bayesian learning of directed graphical models
with hidden variables. Bayesian Analysis 1, 793–832.
Becker, A., R. Bar-Yehuda, and D. Geiger (1999). Random algorithms for the loop cutset problem.
In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 49–56.
Becker, A. and D. Geiger (1994). The loop cutset problem. In Proc. 10th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 60–68.
Becker, A. and D. Geiger (2001). A suﬃciently fast algorithm for ﬁnding close to optimal clique

BIBLIOGRAPHY
1175
trees. Artiﬁcial Intelligence 125(1–2), 3–17.
Becker, A., D. Geiger, and C. Meek (2000). Perfect tree-like Markovian distributions. In Proc. 16th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 19–23.
Becker, A., D. Geiger, and A. Schäﬀer (1998). Automatic selection of loop breakers for genetic
linkage analysis. Human Heredity 48, 49–60.
Beeri, C., R. Fagin, D. Maier, and M. Yannakakis (1983). On the desirability of acyclic database
schemes. Journal of the Association for Computing Machinery 30(3), 479–513.
Beinlich, L., H. Suermondt, R. Chavez, and G. Cooper (1989). The ALARM monitoring system:
A case study with two probabilistic inference techniques for belief networks. In Proceedings
of the Second European Conference on Artiﬁcial Intelligence in Medicine, pp. 247–256. Springer
Verlag.
Bell, D. (1982). egret in decision making under uncertainty. Operations Research 30, 961–981.
Bellman, R. E. (1957). Dynamic Programming. Princeton, New Jersey: Princeton University Press.
Ben-Tal, A. and A. Charnes (1979). A dual optimization framework for some problems of infor-
mation theory and statistics. Problems of Control and Information Theory 8, 387–401.
Bentham, J. (1789). An introduction to the principles of morals and legislation.
Berger, A., S. Della-Pietra, and V. Della-Pietra (1996). A maximum entropy approach to natural
language processing. Computational Linguistics 16(2).
Bernardo, J. and A. Smith (1994). Bayesian Theory. New York: John Wiley and Sons.
Bernoulli, D. (1738). Specimen theoriae novae de mensura sortis (exposition of a new theory on
the measurement of risk). English Translation by L. Sommer, Econometrica, 22:23–36, 1954.
Berrou, C., A. Glavieux, and P. Thitimajshima (1993). Near Shannon limit error-correcting coding:
Turbo codes. In Proc. International Conference on Communications, pp. 1064–1070.
Bertelé, U. and F. Brioschi (1972). Nonserial Dynamic Programming. New York: Academic Press.
Bertsekas, D. (1999). Nonlinear Programming (2nd ed.). Athena Scientiﬁc.
Bertsekas, D. P. and J. N. Tsitsiklis (1996). Neuro-Dynamic Programming. Athena Scientiﬁc.
Besag, J. (1977a).
Eﬃciency of pseudo-likelihood estimation for simple Gaussian ﬁelds.
Biometrika 64(3), 616–618.
Besag, J. (1977b). Spatial interaction and the statistical analysis of lattice systems. Journal of the
Royal Statistical Society, Series B 36, 192–236.
Besag, J. (1986). On the statistical analysis of dirty pictures (with discussion). Journal of the Royal
Statistical Society, Series B 48, 259–302.
Bethe, H. A. (1935). Statistical theory of superlattices. in Proceedings of the Royal Society of London
A, 552.
Bidyuk, B. and R. Dechter (2007). Cutset sampling for bayesian networks. Journal of Artiﬁcial
Intelligence Research 28, 1–48.
Bilmes, J. and C. Bartels (2003).
On triangulating dynamic graphical models.
In Proc. 19th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Bilmes, J. and C. Bartels (2005, September). Graphical model architectures for speech recognition.
IEEE Signal Processing Magazine 22(5), 89–100.
Binder, J., D. Koller, S. Russell, and K. Kanazawa (1997). Adaptive probabilistic networks with
hidden variables. Machine Learning 29, 213–244.
Binder, J., K. Murphy, and S. Russell (1997). Space-eﬃcient inference in dynamic probabilistic
networks. In Proc. 15th International Joint Conference on Artiﬁcial Intelligence (IJCAI).
Bishop, C. (2006). Pattern Recognition and Machine Learning. Information Science and Statistics

1176
BIBLIOGRAPHY
(M. Jordan, J. Kleinberg, and B. Schökopf, editors). New York: Springer-Verlag.
Bishop, C., N. Lawrence, T. Jaakkola, and M. Jordan (1997). Approximating posterior distributions
in belief networks using mixtures. In Proc. 11th Conference on Neural Information Processing
Systems (NIPS).
Blalock, Jr., H. (1971). Causal Models in the Social Sciences. Chicago, Illinois: Aldine-Atheson.
Blum, B., C. Shelton, and D. Koller (2006). A continuation method for nash equilibria in structured
games. Journal of Artiﬁcial Intelligence Resarch 25, 457–502.
Bodlaender, H., A. Koster, F. van den Eijkhof, and L. van der Gaag (2001). Pre-processing for
triangulation of probabilistic networks. In Proc. 17th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 32–39.
Boros, E. and P. Hammer (2002).
Pseudo-Boolean optimization.
Discrete Applied Mathemat-
ics 123(1-3).
Bouckaert, R. (1993). Probabilistic network construction using the minimum description length
principle. In Proc. European Conference on Symbolic and Quantitative Approaches to Reasoning
with Uncertainty, pp. 41–48.
Boutilier, C. (2002).
A POMDP formulation of preference elicitation problems.
In Proc. 18th
Conference on Artiﬁcial Intelligence (AAAI), pp. 239–46.
Boutilier, C., F. Bacchus, and R. Brafman (2001). UCP-Networks: A directed graphical represen-
tation of conditional utilities. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 56–64.
Boutilier, C., T. Dean, and S. Hanks (1999). Decision theoretic planning: Structural assumptions
and computational leverage. Journal of Artiﬁcial Intelligence Research 11, 1 – 94.
Boutilier, C., R. Dearden, and M. Goldszmidt (1989). Exploiting structure in policy construction.
In Proc. 14th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1104–1111.
Boutilier, C., R. Dearden, and M. Goldszmidt (2000).
Stochastic dynamic programming with
factored representations. Artiﬁcial Intelligence 121(1), 49–107.
Boutilier, C., N. Friedman, M. Goldszmidt, and D. Koller (1996). Context-speciﬁc independence in
Bayesian networks. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
115–123.
Boyd, S. and L. Vandenberghe (2004). Convex Optimization. Cambridge University Press.
Boyen, X., N. Friedman, and D. Koller (1999).
Discovering the hidden structure of complex
dynamic systems. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
91–100.
Boyen, X. and D. Koller (1998a). Approximate learning of dynamic models. In Proc. 12th Conference
on Neural Information Processing Systems (NIPS).
Boyen, X. and D. Koller (1998b). Tractable inference for complex stochastic processes. In Proc.
14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 33–42.
Boyen, X. and D. Koller (1999). Exploiting the architecture of dynamic systems. In Proc. 15th
Conference on Artiﬁcial Intelligence (AAAI).
Boykov, Y., O. Veksler, and R. Zabih (2001). Fast approximate energy minimization via graph cuts.
IEEE Transactions on Pattern Analysis and Machine Intelligence 23(11), 1222–1239.
Braziunas, D. and C. Boutilier (2005).
Local utility elicitation in GAI models.
In Proc. 21st
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 42–49.
Breese, J. and D. Heckerman (1996). Decision-theoretic troubleshooting: A framework for repair
and experiment. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.

BIBLIOGRAPHY
1177
124–132.
Breese, J., D. Heckerman, and C. Kadie (1998). Empirical analysis of predictive algorithms for
collaborative ﬁltering. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
43–52.
Breiman, L., J. Friedman, R. Olshen, and C. Stone (1984). Classiﬁcation and Regression Trees.
Monterey,CA: Wadsworth & Brooks.
Buchanan, B. and E. Shortliﬀe (Eds.) (1984). Rule-Based Expert Systems: The MYCIN Experiments
of the Stanford Heuristic Programming Project. Reading, MA: Addison-Wesley.
Bui, H., S. Venkatesh, and G. West (2001). Tracking and surveillance in wide-area spatial environ-
ments using the Abstract Hidden Markov Model. International Journal of Pattern Recognition
and Artiﬁcial Intelligence.
Buntine, W. (1991). Theory reﬁnement on Bayesian networks. In Proc. 7th Conference on Uncer-
tainty in Artiﬁcial Intelligence (UAI), pp. 52–60.
Buntine, W. (1993). Learning classiﬁcation trees. In D. J. Hand (Ed.), Artiﬁcial Intelligence Frontiers
in Statistics, Number III in AI and Statistics. Chapman & Hall.
Buntine, W. (1994). Operations for learning with graphical models. Journal of Artiﬁcial Intelligence
Research 2, 159–225.
Buntine, W. (1996). A guide to the literature on learning probabilistic networks from data. IEEE
Transactions on Knowledge and Data Engineering 8, 195–210.
Caﬀo, B., W. Jank, and G. Jones (2005). Ascent-based Monte Carlo Expectation-Maximization.
Journal of the Royal Statistical Society, Series B.
Cannings, C., E. A. Thompson, and H. H. Skolnick (1976). The recursive derivation of likelihoods
on complex pedigrees. Advances in Applied Probability 8(4), 622–625.
Cannings, C., E. A. Thompson, and M. H. Skolnick (1978). Probability functions on complex
pedigrees. Advances in Applied Probability 10(1), 26–61.
Cano, J., L.D., Hernández, and S. Moral (2006). Importance sampling algorithms for the propaga-
tion of probabilities in belief networks. International Journal of Approximate Reasoning 15(1),
77–92.
Carreira-Perpignan, M. and G. Hinton (2005).
On contrastive divergence learning.
In Proc.
11thWorkshop on Artiﬁcial Intelligence and Statistics.
Casella, G. and R. Berger (1990). Statistical Inference. Wadsworth.
Castillo, E., J. Gutiérrez, and A. Hadi (1997a). Expert Systems and Probabilistic Network Models.
New York: Springer-Verlag.
Castillo, E., J. Gutiérrez, and A. Hadi (1997b). Sensitivity analysis in discrete Bayesian networks.
IEEE Transactions on Systems, Man and Cybernetics 27, 412–23.
Chajewska, U. (2002). Acting Rationally with Incomplete Utility Information. Ph.D. thesis, Stanford
University.
Chajewska, U. and D. Koller (2000).
Utilities as random variables: Density estimation and
structure discovery. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
63–71.
Chajewska, U., D. Koller, and R. Parr (2000). Making rational decisions using adaptive utility
elicitation. In Proc. 16th Conference on Artiﬁcial Intelligence (AAAI), pp. 363–369.
Chan, H. and A. Darwiche (2002).
When do numbers really matter?
Journal of Artiﬁcial
Intelligence Research 17, 265–287.
Chávez, T. and M. Henrion (1994). Eﬃcient estimation of the value of information in Monte Carlo

1178
BIBLIOGRAPHY
models. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 119–127.
Cheeseman, P., J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman (1988). Autoclass: a Bayesian
classiﬁcation system. In Proc. 5th International Conference on Machine Learning (ICML).
Cheeseman, P., M. Self, J. Kelly, and J. Stutz (1988). Bayesian classiﬁcation. In Proc. 4th Conference
on Artiﬁcial Intelligence (AAAI), Volume 2, pp. 607–611.
Cheeseman, P. and J. Stutz (1995). Bayesian classiﬁcation (AutoClass): Theory and results. In
Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD-
95). AAAI Press.
Chen, L., M. Wainwright, M. Cetin, and A. Willsky (2003). Multitarget-multisensor data association
using the tree-reweighted max-product algorithm. In Proceedings SPIE Aerosense Conference,
Orlando, Florida.
Chen, R. and S. Liu (2000). Mixture Kalman ﬁlters. Journal of the Royal Statistical Society, Series B.
Cheng, J. and M. Druzdzel (2000).
AIS-BN: An adaptive importance sampling algorithm for
evidential reasoning in large Bayesian networks. Journal of Artiﬁcial Intelligence Research 13,
155–188.
Cheng, J., R. Greiner, J. Kelly, D. Bell, and W. Liu (2002). Learning bayesian networks from data:
An information-theory based approach. Artiﬁcial Intelligence.
Chesley, G. (1978).
Subjective probability elicitation techniques: A performance comparison.
Journal of Accounting Research 16(2), 225–241.
Chickering, D. (1996a). Learning Bayesian networks is NP-Complete. In D. Fisher and H. Lenz
(Eds.), Learning from Data: Artiﬁcial Intelligence and Statistics V, pp. 121–130. Springer-Verlag.
Chickering, D. (2002a, February). Learning equivalence classes of Bayesian-network structures.
Journal of Machine Learning Research 2, 445–498.
Chickering, D., D. Geiger, and D. Heckerman (1995, January). Learning Bayesian networks: Search
methods and experimental results.
In Proceedings of the Fifth International Workshop on
Artiﬁcial Intelligence and Statistics, pp. 112–128.
Chickering, D., C. Meek, and D. Heckerman (2003). Large-sample learning of Bayesian networks
is hard. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 124–133.
Chickering, D. and J. Pearl (1997). A clinician’s tool for analyzing non-compliance. Computing
Science and Statistics 29, 424–31.
Chickering, D. M. (1995). A transformational characterization of equivalent Bayesian network
structures. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 87–98.
Chickering, D. M. (1996b). Learning equivalence classes of Bayesian network structures. In Proc.
12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 150–157.
Chickering, D. M. (2002b, November). Optimal structure identiﬁcation with greedy search. Journal
of Machine Learning Research 3, 507–554.
Chickering, D. M. and D. Heckerman (1997). Eﬃcient approximations for the marginal likelihood
of Bayesian networks with hidden variables. Machine Learning 29, 181–212.
Chickering, D. M., D. Heckerman, and C. Meek (1997). A Bayesian approach to learning Bayesian
networks with local structure. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 80–89.
Chow, C. K. and C. N. Liu (1968). Approximating discrete probability distributions with depen-
dence trees. IEEE Transactions on Information Theory 14, 462–467.
Collins, M. (2002). Discriminative training methods for hidden Markov models: Theory and
experiments with perceptron algorithms. In Proc. Conference on Empirical Methods in Natural

BIBLIOGRAPHY
1179
Language Processing (EMNLP).
Cooper, G. (1990).
Probabilistic inference using belief networks is NP-hard.
Artiﬁcial Intelli-
gence 42, 393–405.
Cooper, G. and E. Herskovits (1992).
A Bayesian method for the induction of probabilistic
networks from data. Machine Learning 9, 309–347.
Cooper, G. and C. Yoo (1999). Causal discovery from a mixture of experimental and observational
data. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 116–125.
Cooper, G. F. (1988). A method for using belief networks as inﬂuence diagrams. In Proceedings
of the Fourth Workshop on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 55–63.
Cormen, T. H., C. E. Leiserson, R. L. Rivest, and C. Stein (2001). Introduction to Algorithms.
Cambridge, Massachusetts: MIT Press. 2nd Edition.
Covaliu, Z. and R. Oliver (1995). Representation and solution of decision problems using sequen-
tial decision diagrams. Management Science 41(12), 1860–81.
Cover, T. M. and J. A. Thomas (1991). Elements of Information Theory. John Wiley & Sons.
Cowell, R. (2005).
Local propagation in conditional gaussian Bayesian networks.
Journal of
Machine Learning Research 6, 1517–1550.
Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter (1999). Probabilistic Networks
and Expert Systems. New York: Springer-Verlag.
Cox, R. (2001). Algebra of Probable Inference. The Johns Hopkins University Press.
Cozman, F. (2000). Credal networks. Artiﬁcial Intelligence 120, 199–233.
Csiszàr, I. (1975). I-divergence geometry of probability distributions and minimization problems.
The Annals of Probability 3(1), 146–158.
Culotta, A., M. Wick, R. Hall, and A. McCallum (2007).
First-order probabilistic models for
coreference resolution. In Proc. Conference of the North American Association for Computational
Linguistics.
D. Rusakov, D. G. (2005). Asymptotic model selection for naive Bayesian networks. Journal of
Machine Learning Research 6, 1–35.
Dagum, P. and M. Luby (1993). Appoximating probabilistic inference in Bayesian belief networks
in NP-hard. Artiﬁcial Intelligence 60(1), 141–153.
Dagum, P. and M. Luby (1997).
An optimal approximation algorithm for Baysian inference.
Artiﬁcial Intelligence 93(1–2), 1–27.
Daneshkhah, A. (2004). Psychological aspects inﬂuencing elicitation of subjective probability.
Technical report, University of Sheﬃeld.
Darroch, J. and D. Ratcliﬀ(1972). Generalized iterative scaling for log-linear models. Annals of
Mathematical Statistics 43, 1470–1480.
Darwiche, A. (1993). Argument calculus and networks. In Proc. 9th Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), pp. 420–27.
Darwiche, A. (2001a). Constant space reasoning in dynamic Bayesian networks. International
Journal of Approximate Reasoning 26, 161–178.
Darwiche, A. (2001b). Recursive conditioning. Artiﬁcial Intelligence 125(1–2), 5–41.
Darwiche, A. (2003). A diﬀerential approach to inference in Bayesian networks. Journal of the
ACM 50(3), 280–305.
Darwiche, A. and M. Goldszmidt (1994). On the relation between Kappa calculus and probabilistic
reasoning. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Dasgupta, S. (1997). The sample complexity of learning ﬁxed-structure Bayesian networks. Ma-

1180
BIBLIOGRAPHY
chine Learning 29, 165–180.
Dasgupta, S. (1999).
Learning polytrees.
In Proc. 15th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 134–141.
Dawid, A. (1979). Conditional independence in statistical theory (with discussion). Journal of the
Royal Statistical Society, Series B 41, 1–31.
Dawid, A. (1980).
Conditional independence for statistical operations.
Annals of Statistics 8,
598–617.
Dawid, A. (1984). Statistical theory: The prequential approach. Journal of the Royal Statistical
Society, Series A 147(2), 278–292.
Dawid, A. (1992). Applications of a general propagation algorithm for probabilistic expert system.
Statistics and Computing 2, 25–36.
Dawid, A. (2002). Inﬂuence diagrams for causal modelling and inference. International Statistical
Review 70, 161–189. Corrections p437.
Dawid, A. (2007, September).
Fundamentals of statistical causality.
Technical Report 279,
RSS/EPSRC Graduate Training Programme, University of Sheﬃeld.
Dawid, A., U. Kjærulﬀ, and S. Lauritzen (1995). Hybrid propagation in junction trees. In Advances
in Intelligent Computing, Volume 945. Springer-Verlag.
de Bombal, F., D. Leaper, J. Staniland, A. McCann, and J. Harrocks (1972).
Computer-aided
diagnosis of acute abdominal pain. British Medical Journal 2, 9–13.
de Finetti, B. (1937).
Foresight: Its logical laws, its subjective sources.
Annals Institute H.
Poincaré 7, 1–68. Translated by H. Kyburg in Kyburg et al. (1980).
de Freitas, N., P. Højen-Sørensen, M. Jordan, and S. Russell (2001). Variational MCMC. In Proc.
17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 120–127.
Dean, T. and K. Kanazawa (1989).
A model for reasoning about persistence and causation.
Computational Intelligence 5(3), 142–150.
Dechter, R. (1997). Mini-Buckets: A general scheme for generating approximations in automated
reasoning.
In Proc. 15th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp.
1297–1303.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artiﬁcial Intelli-
gence 113(1–2), 41–85.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Dechter, R., K. Kask, and R. Mateescu (2002). Iterative join-graph propagation. In Proc. 18th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 128–136.
Dechter, R. and I. Rish (1997). A scheme for approximating probabilistic inference. In Proc. 13th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
DeGroot, M. H. (1989). Probability and Statistics. Reading, MA: Addison Wesley.
Della Pietra, S., V. Della Pietra, and J. Laﬀerty (1997). Inducing features of random ﬁelds. IEEE
Trans. on Pattern Analysis and Machine Intelligence 19(4), 380–393.
Dellaert, F., S. Seitz, C. Thorpe, and S. Thrun (2003). EM, MCMC, and chain ﬂipping for structure
from motion with unknown correspondence. Machine Learning 50(1–2), 45–71.
Deming, W. and F. Stephan (1940). On a least squares adjustment of a sampled frequency table
when the expected marginal totals are known. Annals of Mathematical Statistics 11, 427–444.
Dempster, A., N. M. Laird, and D. Rubin (1977). Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society, Series B 39(1), 1–22.
Deng, K. and A. Moore (1989). Multiresolution instance-based learning. In Proc. 14th International

BIBLIOGRAPHY
1181
Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1233–1239.
Deshpande, A., M. Garofalakis, and M. Jordan (2001). Eﬃcient stepwise selection in decomposable
models. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 128–135.
Diez, F. (1993). Parameter adjustment in Bayes networks: The generalized noisy OR-gate. In Proc.
9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 99–105.
Dittmer, S. L. and F. V. Jensen (1997). Myopic value of information in inﬂuence diagrams. In
Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 142–149.
Doucet, A. (1998).
On sequential simulation-based methods for Bayesian ﬁltering.
Technical
Report CUED/FINFENG/TR 310, Department of Engineering, Cambridge University.
Doucet, A., N. de Freitas, and N. Gordon (Eds.) (2001). Sequential Monte Carlo Methods in Practice.
New York: Springer-Verlag.
Doucet, A., N. de Freitas, K. Murphy, and S. Russell (2000). Rao-Blackwellised particle ﬁltering for
dynamic Bayesian networks. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI).
Doucet, A., S. Godsill, and C. Andrieu (2000). On sequential Monte Carlo sampling methods for
Bayesian ﬁltering. Statistics and Computing 10(3), 197–208.
Drummond, M., B. O’Brien, G. Stoddart, and G. Torrance (1997).
Methods for the Economic
Evaluation of Health Care Programmes, 2nd Edition. Oxford, UK: Oxford University Press.
Druzdzel, M. (1993). Probabilistic Reasoning in Decision Support Systems: From Computation to
Common Sense. Ph.D. thesis, Carnegie Mellon University.
Dubois, D. and H. Prade (1990). Inference i possibilistic hypergraphs. In Proc. of the 6th Conference
on Information Processing and Management of Uncertainty in Knowledge-Based Systems.
Duchi, J., D. Tarlow, G. Elidan, and D. Koller (2006). Using combinatorial optimization within
max-product belief propagation. In Proc. 20th Conference on Neural Information Processing
Systems (NIPS).
Duda, R., J. Gaschnig, and P. Hart (1979). Model design in the prospector consultant system for
mineral exploration. In D. Michie (Ed.), Expert Systems in the Microelectronic Age, pp. 153–167.
Edinburgh, Scotland: Edinburgh University Press.
Duda, R. and P. Hart (1973). Pattern Classiﬁcation and Scene Analysis. New York: John Wiley &
Sons.
Duda, R., P. Hart, and D. Stork (2000). Pattern Classiﬁcation, Second Edition. Wiley.
Dudík, M., S. Phillips, and R. Schapire (2004). Performance guarantees for regularized maximum
entropy density estimation. In Proc. Conference on Computational Learning Theory (COLT).
Durbin, R., S. Eddy, A. Krogh, and G. Mitchison (1998). Biological Sequence Analysis: Probabilistic
Models of Proteins and Nucleic Acids. Cambridge: Cambridge University Press.
Dykstra, R. and J. Lemke (1988). Duality of I projections and maximum likelihood estimation
for log-linear models under cone constraints.
Journal of the American Statistical Associa-
tion 83(402), 546–554.
El-Hay, T. and N. Friedman (2001).
Incorporating expressive graphical models in variational
approximations: Chain-graphs and hidden variables. In Proc. 17th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 136–143.
Elfadel, I. (1995).
Convex potentials and their conjugates in analog mean-ﬁeld optimization.
Neural Computation 7, 1079–1104.
Elidan, G. and N. Friedman (2005). Learning hidden variable networks: The information bottle-
neck approach. Journal of Machine Learning Research 6, 81–127.

1182
BIBLIOGRAPHY
Elidan, G., I. McGraw, and D. Koller (2006). Residual belief propagation: Informed scheduling for
asynchronous message passing. In Proc. 22nd Conf. on Uncertainty in Artiﬁcial Intelligence.
Elidan, G., N. Lotner, N. Friedman, and D. Koller (2000).
Discovering hidden variables: A
structure-based approach. In Proc. 14th Conf. on Neural Information Processing Systems (NIPS).
Elidan, G., I. Nachman, and N. Friedman (2007). “Ideal Parent” structure learning for continuous
variable networks. Journal of Machine Learning Research 8, 1799–1833.
Elidan, G., M. Ninio, N. Friedman, and D. Schuurmans (2002). Data perturbation for escaping
local maxima in learning. In Proc. 18th National Conference on Artiﬁcial Intelligence (AAAI).
Ellis, B. and W. Wong (2008). Learning causal Bayesian network structures from experimental
data. Journal of the American Statistical Association 103, 778–789.
Elston, R. C. and J. Stewart (1971). A general model for the analysis of pedigree data. Human
Heredity 21, 523–542.
Ezawa, K. (1994). Value of evidence on inﬂuence diagrams. In Proc. 10th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 212–220.
Feller, W. (1970). An Introduction to Probability Theory and Its Applications (third ed.), Volume I.
New York: John Wiley & Sons.
Felzenszwalb, P. and D. Huttenlocher (2006, October). Eﬃcient belief propagation for early vision.
International Journal of Computer Vision 70(1).
Fertig, K. and J. Breese (1989). Interval inﬂuence diagrams. In Proc. 5th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI).
Fine, S., Y. Singer, and N. Tishby (1998). The hierarchical Hidden Markov Model: Analysis and
applications. Machine Learning 32, 41–62.
Fishburn, P. (1967).
Interdependence and additivity in multivariate, unidimensional expected
utility theory. International Economic Review 8, 335–42.
Fishburn, P. (1970). Utility Theory for Decision Making. New York: Wiley.
Fishelson, M. and D. Geiger (2003). Optimizing exact genetic linkage computations. In Proc.
International Conf. on Research in Computational Molecular Biology (RECOMB), pp. 114–121.
Fishman, G. (1976, July). Sampling from the gamma distribution on a computer. Communications
of the ACM 19(7), 407–409.
Fishman, G. (1996). Monte Carlo — Concept, Algorithms, and Applications. Series in Operations
Research. Springer.
Fox, D., W. Burgard, and S. Thrun (1999). Markov localization for mobile robots in dynamic
environments. Journal of Artiﬁcial Intelligence Research 11, 391–427.
Freund, Y. and R. Schapire (1998). Large margin classiﬁcation using the perceptron algorithm. In
Proc. Conference on Computational Learning Theory (COLT).
Frey, B. (2003). Extending factor graphs so as to unify directed and undirected graphical models.
In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 257–264.
Frey, B. and A. Kannan (2000). Accumulator networks: suitors of local probability propagation.
In Proc. 14th Conference on Neural Information Processing Systems (NIPS).
Frey, B. and D. MacKay (1997). A revolution: Belief propagation in graphs with cycles. In Proc.
11th Conference on Neural Information Processing Systems (NIPS).
Frey, B. J. (1998). Graphical Models for Machine Learning and Digital Communication. Cambridge,
Massachusetts: MIT Press.
Friedman, N. (1997). Learning belief networks in the presence of missing values and hidden
variables. In Proc. 14th International Conference on Machine Learning (ICML), pp. 125–133.

BIBLIOGRAPHY
1183
Friedman, N. (1998). The Bayesian structural em algorithm. In Proc. 14th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 129–138.
Friedman, N., D. Geiger, and M. Goldszmidt (1997).
Bayesian network classiﬁers.
Machine
Learning 29, 131–163.
Friedman, N., D. Geiger, and N. Lotner (2000). Likelihood computations using value abstraction.
In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Friedman, N., L. Getoor, D. Koller, and A. Pfeﬀer (1999). Learning probabilistic relational models.
In Proc. 16th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1300–1307.
Friedman, N. and M. Goldszmidt (1996). Learning Bayesian networks with local structure. In
Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 252–262.
Friedman, N. and M. Goldszmidt (1998). Learning Bayesian networks with local structure. See
Jordan (1998), pp. 421–460.
Friedman, N. and D. Koller (2003). Being Bayesian about Bayesian network structure: A Bayesian
approach to structure discovery in Bayesian networks. Machine Learning 50(1–2), 95–126.
Friedman, N., K. Murphy, and S. Russell (1998). Learning the structure of dynamic probabilistic
networks. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Friedman, N. and I. Nachman (2000). Gaussian process networks. In Proc. 16th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 211–219.
Friedman, N. and Z. Yakhini (1996). On the sample complexity of learning Bayesian networks. In
Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Frogner, C. and A. Pfeﬀer (2007). Discovering weakly-interacting factors in a complex stochastic
process. In Proc. 21st Conference on Neural Information Processing Systems (NIPS).
Frydenberg, J. (1990). The chain graph Markov property. Scandinavian Journal of Statistics 17,
790–805.
Fudenberg, D. and J. Tirole (1991). Game Theory. MIT Press.
Fung, R. and K. C. Chang (1989). Weighting and integrating evidence for stochastic simulation in
Bayesian networks. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), San
Mateo, California. Morgan Kaufmann.
Fung, R. and B. del Favero (1994). Backward simulation in Bayesian networks. In Proc. 10th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 227–234.
Galles, D. and J. Pearl (1995). Testing identiﬁability of causal models. In Proc. 11th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 185–95.
Gamerman, D. and H. Lopes (2006). Markov Chain Monte Carlo: Stochastic Simulation for Bayesian
Inference. Chapman & Hall, CRC.
Ganapathi, V., D. Vickrey, J. Duchi, and D. Koller (2008). Constrained approximate maximum
entropy learning. In Proc. 24th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Garcia, L. D. (2004). Algebraic statistics in model selection. In Proc. 20th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 177–18.
Geiger, D. and D. Heckerman. A characterization of the bivariate normal-Wishart distribution.
Probability and Mathematical Statistics 18, 119–131.
Geiger, D. and D. Heckerman (1994). Learning gaussian networks. In Proc. 10th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 235–243.
Geiger, D. and D. Heckerman (1996).
Knowledge representation and inference in similarity
networks and Bayesian multinets. Artiﬁcial Intelligence 82(1-2), 45–74.
Geiger, D., D. Heckerman, H. King, and C. Meek (2001). Stratiﬁed exponential families: Graphical

1184
BIBLIOGRAPHY
models and model selection. Annals of Statistics 29, 505–529.
Geiger, D., D. Heckerman, and C. Meek (1996). Asymptotic model selection for directed networks
with hidden variables. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pp. 283–290.
Geiger, D. and C. Meek (1998). Graphical models and exponential families. In Proc. 14th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 156–165.
Geiger, D., C. Meek, and Y. Wexler (2006). A variational inference procedure allowing internal
structure for overlapping clusters and deterministic constraints. Journal of Artiﬁcial Intelligence
Research 27, 1–23.
Geiger, D. and J. Pearl (1988). On the logic of causal models. In Proc. 4th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 3–14.
Geiger, D. and J. Pearl (1993). Logical and algorithmic properties of conditional independence
and graphical models. Annals of Statistics 21(4), 2001–21.
Geiger, D., T. Verma, and J. Pearl (1989). d-separation: From theorems to algorithms. In Proc. 5th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 139–148.
Geiger, D., T. Verma, and J. Pearl (1990). Identifying independence in Bayesian networks. Net-
works 20, 507–534.
Gelfand, A. and A. Smith (1990). Sampling based approaches to calculating marginal densities.
Journal of the American Statistical Association 85, 398–409.
Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (1995). Bayesian Data Analysis. London:
Chapman & Hall.
Gelman, A. and X.-L. Meng (1998). Simulating normalizing constants: From importance sampling
to bridge sampling to path sampling. Statistical Science 13(2), 163–185.
Gelman, A. and D. Rubin (1992). Inference from iterative simulation using multiple sequences.
Statistical Science 7, 457–511.
Geman, S. and D. Geman (1984, November). Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Trans. on Pattern Analysis and Machine Intelligence 6(6),
721–741.
Getoor, L., N. Friedman, D. Koller, A. Pfeﬀer, and B. Taskar (2007). Probabilistic relational models.
See Getoor and Taskar (2007).
Getoor, L., N. Friedman, D. Koller, and B. Taskar (2002). Learning probabilistic models of link
structure. Journal of Machine Learning Research 3(December), 679–707.
Getoor, L. and B. Taskar (Eds.) (2007). Introduction to Statistical Relational Learning. MIT Press.
Geweke, J. (1989).
Bayesian inference in econometric models using Monte Carlo integration.
Econometrica 57, 1317–1339.
Geyer, C. and E. Thompson (1992). Constrained Monte Carlo maximum likelihood for dependent
data. Journal of the Royal Statistical Society, Series B.
Geyer, C. and E. Thompson (1995). Annealing Markov chain Monte Carlo with applications to
ancestral inference. Journal of the American Statistical Association 90(431), 909–920.
Geyer, C. J. (1991). Markov chain Monte Carlo maximum likelihood. In Computing Science and
Statistics: Proceedings of 23rd Symposium on the Interface Interface Foundation, pp. 156–163.
Fairfax Station.
Ghahramani, Z. (1994). Factorial learning and the em algorithm. In Proc. 8th Conference on Neural
Information Processing Systems (NIPS), pp. 617–624.
Ghahramani, Z. and M. Beal (2000). Propagation algorithms for variational Bayesian learning. In

BIBLIOGRAPHY
1185
Proc. 14th Conference on Neural Information Processing Systems (NIPS).
Ghahramani, Z. and G. Hinton (1998). Variational learning for switching state-space models.
Neural Computation 12(4), 963–996.
Ghahramani, Z. and M. Jordan (1993). Supervised learning from incomplete data via an EM
approach. In Proc. 7th Conference on Neural Information Processing Systems (NIPS).
Ghahramani, Z. and M. Jordan (1997). Factorial hidden Markov models. Machine Learning 29,
245–273.
Gibbs, J. (1902). Elementary Principles of Statistical Mechanics. New Haven, Connecticut: Yale
University Press.
Gidas, B. (1988). Consistency of maximum likelihood and pseudo-likelihood estimators for Gibb-
sian distributions. In W. Fleming and P.-L. Lions (Eds.), Stochastic diﬀerential systems, stochastic
control theory and applications. Springer, New York.
Gilks, W. (1992). Derivative-free adaptive rejection sampling for Gibbs sampling. In J. Bernardo,
J. Berger, A. Dawid, and A. Smith (Eds.), Bayesian Statistics 4, pp. 641–649. Oxford, UK: Claren-
don Press.
Gilks, W., N. Best, and K. Tan (1995).
Adaptive rejection Metropolis sampling within Gibbs
sampling. Annals of Statistics 44, 455–472.
Gilks, W., S. Richardson, and D. Spiegelhalter (Eds.) (1996). Markov Chain Monte Carlo in Practice.
Chapman & Hall, London.
Gilks, W., A. Thomas, and D. Spiegelhalter (1994). A language and program for complex Bayesian
modeling. The Statistician 43, 169–177.
Gilks, W. and P. Wild (1992). Adaptive rejection sampling for Gibbs sampling. Annals of Statistics 41,
337–348.
Giudici, P. and P. Green (1999, December). Decomposable graphical Gaussian model determina-
tion. Biometrika 86(4), 785–801.
Globerson, A. and T. Jaakkola (2007a). Convergent propagation algorithms via oriented trees. In
Proc. 23rd Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Globerson, A. and T. Jaakkola (2007b). Fixing max-product: Convergent message passing al-
gorithms for MAP LP-relaxations. In Proc. 21st Conference on Neural Information Processing
Systems (NIPS).
Glover, F. and M. Laguna (1993). Tabu search. In C. Reeves (Ed.), Modern Heuristic Techniques for
Combinatorial Problems, Oxford, England. Blackwell Scientiﬁc Publishing.
Glymour, C. and G. F. Cooper (Eds.) (1999). Computation, Causation, Discovery. Cambridge: MIT
Press.
Godsill, S., A. Doucet, and M. West (2000).
Methodology for Monte Carlo smoothing with
application to time-varying autoregressions. In Proc. International Symposium on Frontiers of
Time Series Modelling.
Golumbic, M. (1980). Algorithmic Graph Theory and Perfect Graphs. London: Academic Press.
Good, I. (1950). Probability and the Weighing of Evidence. London: Griﬃn.
Goodman, J. (2004). Exponential priors for maximum entropy models. In Proc. Conference of the
North American Association for Computational Linguistics.
Goodman, L. (1970). The multivariate analysis of qualitative data: Interaction among multiple
classiﬁcation. Journal of the American Statistical Association 65, 226–56.
Gordon, N., D. Salmond, and A. Smith (1993). Novel approach to nonlinear/non-Gaussian Bayesian
state estimation. IEE Proceedings-F 140(2), 107–113.

1186
BIBLIOGRAPHY
Gorry, G. and G. Barnett (1968). Experience with a model of sequential diagnosis. Computers
and Biomedical Research 1, 490–507.
Green, P. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrika 82, 711–732.
Green, P. J. (1990). On use of the EM algorithm for penalized likelihood estimation. Journal of
the Royal Statistical Society, Series B 52(3), 443–452.
Greig, D., B. Porteous, and A. Seheult (1989). Exact maximum a posteriori estimation for binary
images. Journal of the Royal Statistical Society, Series B 51(2), 271–279.
Greiner, R. and W. Zhou (2002). Structural extension to logistic regression: Discriminant pa-
rameter learning of belief net classiﬁers.
In Proc. 18th Conference on Artiﬁcial Intelligence
(AAAI).
Guestrin, C. E., D. Koller, R. Parr, and S. Venkataraman (2003). Eﬃcient solution algorithms for
factored MDPs. Journal of Artiﬁcial Intelligence Research 19, 399–468.
Guyon, X. and H. R. Künsch (1992). Asymptotic comparison of estimators in the Ising model.
In Stochastic Models, Statistical Methods, and Algorithms in Image Analysis, Lecture Notes in
Statistics, Volume 74, pp. 177–198. Springer, Berlin.
Ha, V. and P. Haddawy (1997). Problem-focused incremental elicitation of multi-attribute utility
models. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 215–222.
Ha, V. and P. Haddawy (1999). A hybrid approach to reasoning with partially elicited preference
models. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 263–270.
Haberman, S. (1974).
The General Log-Linear Model.
Ph.D. thesis, Department of Statistics,
University of Chicago.
Halpern, J. Y. (2003). Reasoning about Uncertainty. MIT Press.
Hammer, P. (1965). Some network ﬂow problems solved with pseudo-Boolean programming.
Operations Research 13, 388–399.
Hammersley, J. and P. Cliﬀord (1971). Markov ﬁelds on ﬁnite graphs and lattices. Unpublished
manuscript.
Handschin, J. and D. Mayne (1969). Monte Carlo techniques to estimate the conditional expecta-
tion in multi-stage non-linear ﬁltering. International Journal of Control 9(5), 547–559.
Hartemink, A., D. Giﬀord, T. Jaakkola, and R. Young (2002, March/April). Bayesian methods for
elucidating genetic regulatory networks. IEEE Intelligent Systems 17, 37–43. special issue on
Intelligent Systems in Biology.
Hastie, T., R. Tibshirani, and J. Friedman (2001). The Elements of Statistical Learning. Springer
Series in Statistics.
Hazan, T. and A. Shashua (2008). Convergent message-passing algorithms for inference over
general graphs with convex free energies. In Proc. 24th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI).
Heckerman, D. (1990). Probabilistic Similarity Networks. MIT Press.
Heckerman, D. (1993). Causal independence for knowledge acquisition and inference. In Proc.
9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 122–127.
Heckerman, D. (1998). A tutorial on learning with Bayesian networks. See Jordan (1998).
Heckerman, D. and J. Breese (1996). Causal independence for probability assessment and in-
ference using Bayesian networks.
IEEE Transactions on Systems, Man, and Cybernetics 26,
826–831.
Heckerman, D., J. Breese, and K. Rommelse (1995, March). Decision-theoretic troubleshooting.

BIBLIOGRAPHY
1187
Communications of the ACM 38(3), 49–57.
Heckerman, D., D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie (2000). Dependency
networks for inference, collaborative ﬁltering, and data visualization. jmlr 1, 49–75.
Heckerman, D. and D. Geiger (1995). Learning Bayesian networks: a uniﬁcation for discrete and
Gaussian domains. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
274–284.
Heckerman, D., D. Geiger, and D. M. Chickering (1995).
Learning Bayesian networks: The
combination of knowledge and statistical data. Machine Learning 20, 197–243.
Heckerman, D., E. Horvitz, and B. Nathwani (1992). Toward normative expert systems: Part I. The
Pathﬁnder project. Methods of Information in Medicine 31, 90–105.
Heckerman, D. and H. Jimison (1989).
A Bayesian perspective on conﬁdence.
In Proc. 5th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 149–160.
Heckerman, D., A. Mamdani, and M. Wellman (1995). Real-world applications of Bayesian net-
works. Communications of the ACM 38.
Heckerman, D. and C. Meek (1997). Embedded Bayesian network classiﬁers. Technical Report
MSR-TR-97-06, Microsoft Research, Redmond, WA.
Heckerman, D., C. Meek, and G. Cooper (1999). A Bayesian approach to causal discovery. See
Glymour and Cooper (1999), pp. 141–166.
Heckerman, D., C. Meek, and D. Koller (2007). Probabilistic entity-relationship models, PRMs,
and plate models. See Getoor and Taskar (2007).
Heckerman, D. and B. Nathwani (1992a). An evaluation of the diagnostic accuracy of Pathﬁnder.
Computers and Biomedical Research 25(1), 56–74.
Heckerman, D. and B. Nathwani (1992b). Toward normative expert systems. II. Probability-based
representations for eﬃcient knowledge acquisition and inference. Methods of Information in
Medicine 31, 106–16.
Heckerman, D. and R. Shachter (1994). A decision-based view of causality. In Proc. 10th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 302–310. Morgan Kaufmann.
Henrion, M. (1986).
Propagation of uncertainty in Bayesian networks by probabilistic logic
sampling. In Proc. 2nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 149–163.
Henrion, M. (1991). Search-based algorithms to bound diagnostic probabilities in very large belief
networks. In Proc. 7th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 142–150.
Hernández, L. and S. Moral (1997). Mixing exact and importance sampling propagation algorithms
in dependence graphs. International Journal of Intelligent Systems 12, 553–576.
Heskes, T. (2002). Stable ﬁxed points of loopy belief propagation are minima of the Bethe free
energy. In Proc. 16th Conference on Neural Information Processing Systems (NIPS), pp. 359–366.
Heskes, T. (2004). On the uniqueness of loopy belief propagation ﬁxed points. Neural Computa-
tion 16, 2379–2413.
Heskes, T. (2006). Convexity arguments for eﬃcient minimization of the Bethe and Kikuchi free
energies. Journal of Machine Learning Research 26, 153–190.
Heskes, T., K. Albers, and B. Kappen (2003). Approximate inference and constrained optimization.
In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 313–320.
Heskes, T., M. Opper, W. Wiegerinck, O. Winther, and O. Zoeter (2005).
Approximate infer-
ence techniques with expectation constraints. Journal of Statistical Mechanics: Theory and
Experiment.
Heskes, T. and O. Zoeter (2002). Expectation propagation for approximate inference in dynamic

1188
BIBLIOGRAPHY
Bayesian networks. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Heskes, T. and O. Zoeter (2003). Generalized belief propagation for approximate inference in
hybrid Bayesian networks. In Proceedings of the Ninth International Workshop on Artiﬁcial
Intelligence and Statistics.
Heskes, T., O. Zoeter, and W. Wiegerinck (2003). Approximate expectation maximization. In Proc.
17th Conference on Neural Information Processing Systems (NIPS), pp. 353–360.
Higdon, D. M. (1998). Auxiliary variable methods for Markov chain Monte Carlo with applications.
Journal of the American Statistical Association 93, 585–595.
Hinton, G. (2002). Training products of experts by minimizing contrastive divergence. Neural
Computation 14, 1771–1800.
Hinton, G., S. Osindero, and Y. Teh (2006). A fast learning algorithm for deep belief nets. Neural
Computation 18, 1527–1554.
Hinton, G. and R. Salakhutdinov (2006). Reducing the dimensionality of data with neural net-
works. Science 313, 504–507.
Hinton, G. and T. Sejnowski (1983). Optimal perceptual inference. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition, pp. 448–453.
Hinton, G. E., P. Dayan, B. Frey, and R. M. Neal (1995). The wake-sleep algorithm for unsupervised
neural networks. Science 268, 1158–1161.
Höﬀgen, K. (1993). Learning and robust learning of product distributions. In Proc. Conference on
Computational Learning Theory (COLT), pp. 77–83.
Hofmann, R. and V. Tresp (1995). Discovering structure in continuous variables using bayesian
networks. In Proc. 9th Conference on Neural Information Processing Systems (NIPS).
Horn, G. and R. McEliece (1997). Belief propagation in loopy bayesian networks: experimental
results. In Proceedings if IEEE International Symposium on Information Theory, pp. 232.
Horvitz, E. and M. Barry (1995). Display of information for time-critical decision making. In Proc.
11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 296–305.
Horvitz, E., J. Breese, and M. Henrion (1988). Decision theory in expert systems and artiﬁcial
intelligence.
International Journal of Approximate Reasoning 2, 247–302.
Special Issue on
Uncertainty in Artiﬁcial Intelligence.
Horvitz, E., H. Suermondt, and G. Cooper (1989). Bounded conditioning: Flexible inference for
decisions under scarce resources. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 182–193.
Howard, R. (1970). Decision analysis: Perspectives on inference, decision, and experimentation.
Proceedings of the IEEE 58, 632–643.
Howard, R. (1977). Risk preference. In R. Howard and J. Matheson (Eds.), Readings in Decision
Analysis, pp. 429–465. Menlo Park, California: Decision Analysis Group, SRI International.
Howard, R. and J. Matheson (1984a). Inﬂuence diagrams. See Howard and Matheson (1984b), pp.
721–762.
Howard, R. and J. Matheson (Eds.) (1984b). The Principle and Applications of Decision Analysis.
Menlo Park, CA, USA: Strategic Decisions Group.
Howard, R. A. (1966).
Information value theory.
IEEE Transactions on Systems Science and
Cybernetics SSC-2, 22–26.
Howard, R. A. (1989). Microrisks for medical decision analysis. International Journal of Technology
Assessment in Health Care 5, 357–370.
Huang, C. and A. Darwiche (1996). Inference in belief networks: A procedural guide. International

BIBLIOGRAPHY
1189
Journal of Approximate Reasoning 15(3), 225–263.
Huang, F. and Y. Ogata (2002). Generalized pseudo-likelihood estimates for Markov random ﬁelds
on lattice. Annals of the Institute of Statistical Mathematics 54, 1–18.
Ihler, A. (2007). Accuracy bounds for belief propagation. In Proc. 23rd Conference on Uncertainty
in Artiﬁcial Intelligence (UAI).
Ihler, A. T., J. W. Fisher, and A. S. Willsky (2003). Message errors in belief propagation. In Proc.
17th Conference on Neural Information Processing Systems (NIPS).
Ihler, A. T., J. W. Fisher, and A. S. Willsky (2005). Loopy belief propagation: Convergence and
eﬀects of message errors. Journal of Machine Learning Research 6, 905–936.
Imoto, S., S. Kim, T. Goto, S. Aburatani, K. Tashiro, S. Kuhara, and S. Miyano (2003). Bayesian
network and nonparametric heteroscedastic regression for nonlinear modeling of genetic net-
work. Journal of Bioinformatics and Computational Biology 1, 231–252.
Indyk, P. (2004). Nearest neighbors in high-dimensional spaces. In J. Goodman and J. O’Rourke
(Eds.), Handbook of Discrete and Computational Geometry (2nd ed.). CRC Press.
Isard, M. (2003). PAMPAS: Real-valued graphical models for computer vision. In Proc. Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 613–620.
Isard, M. and A. Blake (1998a).
Condensation — conditional density propagation for visual
tracking. International Journal of Computer Vision 29(1), 5–28.
Isard, M. and A. Blake (1998b). A smoothing ﬁlter for condensation. In Proc. European Conference
on Computer Vision (ECCV), Volume 1, pp. 767–781.
Isham, V. (1981). An introduction to spatial point processes and Markov random ﬁelds. Interna-
tional Statistical Review 49, 21–43.
Ishikawa, H. (2003). Exact optimization for Markov random ﬁelds with convex priors. IEEE Trans.
on Pattern Analysis and Machine Intelligence 25(10), 1333–1336.
Ising, E. (1925). Beitrag zur theorie des ferromagnetismus. Z. Phys. 31, 253–258.
Jaakkola, T. (2001). Tutorial on variational approximation methods. In M. Opper and D. Saad
(Eds.), Advanced mean ﬁeld methods, pp. 129–160. Cambridge, Massachusetts: MIT Press.
Jaakkola, T. and M. Jordan (1996a).
Computing upper and lower bounds on likelihoods in
intractable networks. In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
340–348.
Jaakkola, T. and M. Jordan (1996b).
Recursive algorithms for approximating probabilities in
graphical models. In Proc. 10th Conference on Neural Information Processing Systems (NIPS), pp.
487–93.
Jaakkola, T. and M. Jordan (1997). A variational approach to bayesian logistic regression models
and their extensions. In Proc. 6thWorkshop on Artiﬁcial Intelligence and Statistics.
Jaakkola, T. and M. Jordan (1998). Improving the mean ﬁeld approximation via the use of mixture
models. See Jordan (1998).
Jaakkola, T. and M. Jordan (1999). Variational probabilistic inference and the QMR-DT network.
Journal of Artiﬁcial Intelligence Research 10, 291–322.
Jarzynski, C. (1997, Apr). Nonequilibrium equality for free energy diﬀerences. Physical Review
Letters 78(14), 2690–2693.
Jaynes, E. (2003). Probability Theory: The Logic of Science. Cambridge University Press.
Jensen, F., F. V. Jensen, and S. L. Dittmer (1994). From inﬂuence diagrams to junction trees. In
Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 367–73.
Jensen, F. and M. Vomlelová (2003). Unconstrained inﬂuence diagrams. In Proc. 19th Conference

1190
BIBLIOGRAPHY
on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 234–41.
Jensen, F. V. (1995). Cautious propagation in Bayesian networks. In Proc. 11th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 323–328.
Jensen, F. V. (1996). An introduction to Bayesian Networks. London: University College London
Press.
Jensen, F. V., K. G. Olesen, and S. K. Andersen (1990, August). An algebra of Bayesian belief
universes for knowledge-based systems. Networks 20(5), 637–659.
Jerrum, M. and A. Sinclair (1997). The Markov chain Monte Carlo method. In D. Hochbaum (Ed.),
Approximation Algorithms for NP-hard Problems. Boston: PWS Publishing.
Ji, C. and L. Seymour (1996). A consistent model selection procedure for Markov random ﬁelds
based on penalized pseudolikelihood. Annals of Applied Probability.
Jimison, H., L. Fagan, R. Shachter, and E. Shortliﬀe (1992). Patient-speciﬁc explanation in models
of chronic disease. AI in Medicine 4, 191–205.
Jordan, M., Z. Ghahramani, T. Jaakkola, and L. K. Saul (1998). An introduction to variational
approximations methods for graphical models. See Jordan (1998).
Jordan, M. I. (Ed.) (1998). Learning in Graphics Models. Cambridge, MA: The MIT Press.
Julier, S. (2002). The scaled unscented transformation. In Proceedings of the American Control
Conference, Volume 6, pp. 4555–4559.
Julier, S. and J. Uhlmann (1997). A new extension of the Kalman ﬁlter to nonlinear systems. In
Proc. of AeroSense: The 11th International Symposium on Aerospace/Defence Sensing, Simulation
and Controls.
Kahneman, D., P. Slovic, and A. Tversky (Eds.) (1982). Judgment under Uncertainty: Heuristics and
Biases. Cambridge: Cambridge University Press.
Kalman, R. and R. Bucy (1961). New results in linear ﬁltering and prediction theory. Trans. ASME,
Series D, Journal of Basic Engineering.
Kalman, R. E. (1960). A new approach to linear ﬁltering and prediction problems. Transactions of
the ASME–Journal of Basic Engineering 82(Series D), 35–45.
Kanazawa, K., D. Koller, and S. Russell (1995). Stochastic simulation algorithms for dynamic
probabilistic networks. In Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pp. 346–351.
Kass, R. and A. Raftery (1995). Bayes factors. Journal of the American Statistical Association 90(430),
773–795.
Kearns, M., M. L. Littman, and S. Singh (2001). Graphical models for game theory. In Proc. 17th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 253–260.
Kearns, M. and Y. Mansour (1998). Exact inference of hidden structure from sample data in
noisy-or networks. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
304–31.
Kearns, M., Y. Mansour, and A. Ng (1997). An information-theoretic analysis of hard and soft
assignment methods for clustering. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelli-
gence (UAI), pp. 282–293.
Keeney, R. L. and H. Raiﬀa (1976).
Decisions with Multiple Objectives: Preferences and Value
Tradeoﬀs. John Wiley & Sons, Inc.
Kersting, K. and L. De Raedt (2007). Bayesian logic programming: Theory and tool. See Getoor
and Taskar (2007).
Kikuchi, R. (1951). A theory of cooperative phenomena. Physical Review Letters 81, 988–1003.

BIBLIOGRAPHY
1191
Kim, C.-J. and C. Nelson (1998). State-Space Models with Regime-Switching: Classical and Gibbs-
Sampling Approaches with Applications. MIT Press.
Kim, J. and J. Pearl (1983). A computational model for combined causal and diagnostic reasoning
in inference systems. In Proc. 7th International Joint Conference on Artiﬁcial Intelligence (IJCAI),
pp. 190–193.
Kirkpatrick, S., C. Gelatt, and M. Vecchi (1983). Optimization by simulated annealing. Science 220,
671–680.
Kitagawa, G. (1996). Monte Carlo ﬁlter and smoother for non-Gaussian nonlinear state space
models. Journal of Computational and Graphical Statistics 5(1), 1–25.
Kjærulﬀ, U. (1990, March). Triangulation of graph — Algorithms giving small total state space.
Technical Report R90-09, Aalborg University, Denmark.
Kjærulﬀ, U. (1992). A computational scheme for reasoning in dynamic probabilistic networks. In
Proc. 8th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 121–129.
Kjærulﬀ, U. (1995a). dHugin: A computational system for dynamic time-sliced Bayesian networks.
International Journal of Forecasting 11, 89–111.
Kjærulﬀ, U. (1995b). HUGS: Combining exact inference and Gibbs sampling in junction trees. In
Proc. 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 368–375.
Kjaerulﬀ, U. (1997). Nested junction trees. In Proc. 13th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 294–301.
Kjærulﬀ, U. and L. van der Gaag (2000). Making sensitivity analysis computationally eﬃcient. In
Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 317–325.
Koivisto, M. and K. Sood (2004). Exact Bayesian structure discovery in Bayesian networks. Journal
of Machine Learning Research 5, 549–573.
Kok, J., M. Spaan, and N. Vlassis (2003). Multi-robot decision making using coordination graphs.
In Proc. International Conference on Advanced Robotics (ICAR), pp. 1124–1129.
Kok, J. and N. Vlassis (2005). Using the max-plus algorithm for multiagent decision making in
coordination graphs. In RoboCup-2005: Robot Soccer World Cup IX, Osaka, Japan.
Koller, D. and R. Fratkina (1998). Using learning for approximation in stochastic processes. In
Proc. 15th International Conference on Machine Learning (ICML), pp. 287–295.
Koller, D., U. Lerner, and D. Anguelov (1999). A general algorithm for approximate inference
and its application to hybrid Bayes nets. In Proc. 15th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 324–333.
Koller, D. and B. Milch (2001). Multi-agent inﬂuence diagrams for representing and solving games.
In Proc. 17th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1027–1034.
Koller, D. and B. Milch (2003).
Multi-agent inﬂuence diagrams for representing and solving
games. Games and Economic Behavior 45(1), 181–221. Full version of paper in IJCAI ’03.
Koller, D. and R. Parr (1999). Computing factored value functions for policies in structured MDPs.
In Proc. 16th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1332–1339.
Koller, D. and A. Pfeﬀer (1997). Object-oriented Bayesian networks. In Proc. 13th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 302–313.
Kolmogorov, V. (2006). Convergent tree-reweighted message passing for energy minimization.
IEEE Transactions on Pattern Analysis and Machine Intelligence.
Kolmogorov, V. and C. Rother (2006). Comparison of energy minimization algorithms for highly
connected graphs. In Proc. European Conference on Computer Vision (ECCV).
Kolmogorov, V. and M. Wainwright (2005). On the optimality of tree reweighted max-product

1192
BIBLIOGRAPHY
message passing. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Kolmogorov, V. and R. Zabih (2004). What energy functions can be minimized via graph cuts?
IEEE Transactions on Pattern Analysis and Machine Intelligence 26(2).
Komarek, P. and A. Moore (2000).
A dynamic adaptation of AD-trees for eﬃcient machine
learning on large data sets. In Proc. 17th International Conference on Machine Learning (ICML),
pp. 495–502.
Komodakis, N., N. Paragios, and G. Tziritas (2007). MRF optimization via dual decomposition:
Message-passing revisited. In Proc. International Conference on Computer Vision (ICCV).
Komodakis, N. and G. Tziritas (2005). A new framework for approximate labeling via graph-cuts.
In Proc. International Conference on Computer Vision (ICCV).
Komodakis, N., G. Tziritas, and N. Paragios (2007). Fast, approximately optimal solutions for
single and dynamic MRFs. In Proc. Conference on Computer Vision and Pattern Recognition
(CVPR).
Kong, A. (1991). Eﬃcient methods for computing linkage likelihoods of recessive diseases in
inbred pedigrees. Genetic Epidemiology 8, 81–103.
Korb, K. and A. Nicholson (2003). Bayesian Artiﬁcial Intelligence. CRC Press.
Koster, J. (1996). Markov properties of non-recursive causal models. The Annals of Statistics 24(5),
2148–77.
Koˇcka, T., R. Bouckaert, and M. Studený (2001). On characterizing inclusion of Bayesian networks.
In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 261–68.
Kozlov, A. and D. Koller (1997). Nonuniform dynamic discretization in hybrid networks. In Proc.
13th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 314–325.
Krause, A. and C. Guestrin (2005a). Near-optimal nonmyopic value of information in graphical
models. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Krause, A. and C. Guestrin (2005b).
Optimal nonmyopic value of information in graphical
models: Eﬃcient algorithms and theoretical limits. In Proc. 19th International Joint Conference
on Artiﬁcial Intelligence (IJCAI).
Kreps, D. (1988). Notes on the Theory of Choice. Boulder, Colorado: Westview Press.
Kschischang, F. and B. Frey (1998). Iterative decoding of compound codes by probability propa-
gation in graphical models. IEEE Journal on Selected Areas in Communications 16, 219–230.
Kschischang, F., B. Frey, and H.-A. Loeliger (2001a). Factor graphs and the sum-product algorithm.
IEEE Transactions on Information Theory 47, 498–519.
Kschischang, F., B. Frey, and H.-A. Loeliger (2001b). Factor graphs and the sum-product algorithm.
IEEE Trans. Information Theory 47, 498–519.
Kullback, S. (1959). Information Theory and Statistics. New York: John Wiley & Sons.
Kumar, M., V. Kolmogorov, and P. Torr (2007).
An analysis of convex relaxations for MAP
estimation. In Proc. 21st Conference on Neural Information Processing Systems (NIPS).
Kumar, M., P. Torr, and A. Zisserman (2006). Solving Markov random ﬁelds using second order
cone programming relaxations. In Proc. Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1045–1052.
Kuppermann, M., S. Shiboski, D. Feeny, E. Elkin, and A. Washington (1997, Jan–Mar).
Can
preference scores for discrete states be used to derive preference scores for an entire path of
events? An application to prenatal diagnosis. Medical Decision Making 17(1), 42–55.
Kyburg, H., , and H. Smokler (Eds.) (1980). Studies in Subjective Probability. New York: Krieger.
La Mura, P. (2000). Game networks. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence

BIBLIOGRAPHY
1193
(UAI), pp. 335–342.
La Mura, P. and Y. Shoham (1999).
Expected utility networks.
In Proc. 15th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 366–73.
Lacoste-Julien, S., B. Taskar, D. Klein, and M. Jordan (2006, June). Word alignment via quadratic
assignment. In Proceedings of the Human Language Technology Conference of the NAACL, Main
Conference, pp. 112–119.
Laﬀerty, J., A. McCallum, and F. Pereira (2001). Conditional random ﬁelds: Probabilistic models
for segmenting and labeling sequence data. In Proc. 18th International Conference on Machine
Learning (ICML).
Lam, W. and F. Bacchus (1993). Using causal information and local measures to learn Bayesian
networks. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 243–250.
Lange, K. and R. C. Elston (1975). Extensions to pedigree analysis. I. Likelihood calculations for
simple and complex pedigrees. Human Heredity 25, 95–105.
Laskey, K. (1995). Sensitivity analysis for probability assessments in Bayesian networks. IEEE
Transactions on Systems, Man, and Cybernetics 25(6), 901 – 909.
Lauritzen, S. (1982). Lectures on contingency tables (2 ed.). Aalborg: Denmark: University of
Aalborg Press.
Lauritzen, S. (1992).
Propagation of probabilities, means, and variances in mixed graphical
association models. Journal of the American Statistical Association 87(420), 1089–1108.
Lauritzen, S. (1996). Graphical Models. New York: Oxford University Press.
Lauritzen, S. and D. Nilsson (2001). Representing and solving decision problems with limited
information. Management Science 47(9), 1235–51.
Lauritzen, S. L. (1995). The EM algorithm for graphical association models with missing data.
Computational Statistics and Data Analysis 19, 191–201.
Lauritzen, S. L. and F. Jensen (2001). Stable local computation with conditional Gaussian distri-
butions. Statistics and Computing 11, 191–203.
Lauritzen, S. L. and D. J. Spiegelhalter (1988). Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society, Series
B B 50(2), 157–224.
Lauritzen, S. L. and N. Wermuth (1989). Graphical models for associations between variables,
some of which are qualitative and some quantitative. Annals of Statistics 17, 31–57.
LeCun, Y., S. Chopra, R. Hadsell, R. Marc’Aurelio, and F.-J. Huang (2007). A tutorial on energy-
based learning. In G. Bakir, T. Hofmann, B. Schölkopf, A. Smola, B. Taskar, and S. Vishwanathan
(Eds.), Predicting Structured Data. MIT Press.
Lee, S.-I., V. Ganapathi, and D. Koller (2006). Eﬃcient structure learning of Markov networks
using L1-regularization.
In Proc. 20th Conference on Neural Information Processing Systems
(NIPS).
Lehmann, E. and J. Romano (2008). Testing Statistical Hypotheses. Springer Texts in Statistics.
Leisink, M. A. R. and H. J. Kappen (2003). Bound propagation. Journal of Artiﬁcial Intelligence
Research 19, 139–154.
Lerner, U. (2002). Hybrid Bayesian Networks for Reasoning about Complex Systems. Ph.D. thesis,
Stanford University.
Lerner, U., B. Moses, M. Scott, S. McIlraith, and D. Koller (2002). Monitoring a complex physical
system using a hybrid dynamic Bayes net. In Proc. 18th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 301–310.

1194
BIBLIOGRAPHY
Lerner, U. and R. Parr (2001). Inference in hybrid networks: Theoretical limits and practical
algorithms. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 310–318.
Lerner, U., R. Parr, D. Koller, and G. Biswas (2000). Bayesian fault detection and diagnosis in
dynamic systems. In Proc. 16th Conference on Artiﬁcial Intelligence (AAAI), pp. 531–537.
Lerner, U., E. Segal, and D. Koller (2001). Exact inference in networks with discrete children of
continuous parents. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
319–328.
Li, S. (2001). Markov Random Field Modeling in Image Analysis. Springer.
Liang, P. and M. Jordan (2008). An asymptotic analysis of generative, discriminative, and pseu-
dolikelihood estimators. In Proc. 25th International Conference on Machine Learning (ICML).
Little, R. J. A. (1976). Inference about means for incomplete multivariate data. Biometrika 63,
593–604.
Little, R. J. A. and D. B. Rubin (1987). Statistical Analysis with Missing Data. New York: John Wiley
& Sons.
Liu, D. and J. Nocedal (1989).
On the limited memory method for large scale optimization.
Mathematical Programming 45(3), 503–528.
Liu, J., W. Wong, and A. Kong (1994). Covariance structure of the Gibbs sampler with applications
to the comparisons of estimators and sampling schemes. Biometrika 81, 27–40.
Loomes, G. and R. Sugden (1982). Regret theory: An alternative theory of rational choice under
uncertainty. The Economic Journal 92, 805–824.
MacEachern, S. and L. Berliner (1994, August). Subsampling the Gibbs sampler. The American
Statistician 48(3), 188–190.
MacKay, D. J. C. (1997). Ensemble learning for hidden markov models. Unpublished manuscripts,
http://wol.ra.phy.cam.ac.uk/mackay.
MacKay, D. J. C. and R. M. Neal (1996). Near shannon limit performance of low density parity
check codes. Electronics Letters 32, 1645–1646.
Madigan, D., S. Andersson, M. Perlman, and C. Volinsky (1996). Bayesian model averaging and
model selection for Markov equivalence classes of acyclic graphs. Communications in Statistics:
Theory and Methods 25, 2493–2519.
Madigan, D. and E. Raftery (1994). Model selection and accounting for model uncertainty in
graphical models using Occam’s window. Journal of the American Statistical Association 89,
1535–1546.
Madigan, D. and J. York (1995).
Bayesian graphical models for discrete data.
International
statistical Review 63, 215–232.
Madsen, A. and D. Nilsson (2001). Solving inﬂuence diagrams using HUGIN, Shafer-Shenoy and
lazy propagation. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
337–45.
Malioutov, D., J. Johnson, and A. Willsky (2006). Walk-sums and belief propagation in Gaussian
graphical models. Journal of Machine Learning Research 7, 2031–64.
Maneva, E., E. Mossel, and M. Wainwright (2007, July). A new look at survey propagation and its
generalizations. Journal of the ACM 54(4), 2–41.
Manning, C. and H. Schuetze (1999). Foundations of Statistical Natural Language Processing. MIT
Press.
Marinari, E. and G. Parisi (1992). Simulated tempering: A new Monte Carlo scheme. Europhysics
Letters 19, 451.

BIBLIOGRAPHY
1195
Marinescu, R., K. Kask, and R. Dechter (2003).
Systematic vs. non-systematic algorithms for
solving the MPE task. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Marthi, B., H. Pasula, S. Russell, and Y. Peres (2002). Decayed MCMC ﬁltering. In Proc. 18th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Martin, J. and K. VanLehn (1995). Discrete factor analysis: Learning hidden variables in Bayesian
networks. Technical report, Department of Computer Science, University of Pittsburgh.
McCallum, A. (2003). Eﬃciently inducing features of conditional random ﬁelds. In Proc. 19th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 403–10.
McCallum, A., C. Pal, G. Druck, and X. Wang (2006).
Multi-conditional learning: Genera-
tive/discriminative training for clustering and classiﬁcation. In Proc. 22nd Conference on Arti-
ﬁcial Intelligence (AAAI).
McCallum, A. and B. Wellner (2005). Conditional models of identity uncertainty with application
to noun coreference. In Proc. 19th Conference on Neural Information Processing Systems (NIPS),
pp. 905–912.
McCullagh, P. and J. Nelder (1989). Generalized Linear Models. London: Chapman & Hall.
McEliece, R., D. MacKay, and J.-F. Cheng (1998, February). Turbo decoding as an instance of
Pearl’s “belief propagation” algorithm. IEEE Journal on Selected Areas in Communications 16(2).
McEliece, R. J., E. R. Rodemich, and J.-F. Cheng (1995). The turbo decision algorithm. In Proc.
33rd Allerton Conference on Communication Control and Computing, pp. 366–379.
McLachlan, G. J. and T. Krishnan (1997). The EM Algorithm and Extensions. Wiley Interscience.
Meek, C. (1995a). Causal inference and causal explanation with background knowledge. In Proc.
11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 403–418.
Meek, C. (1995b).
Strong completeness and faithfulness in Bayesian networks.
In Proc. 11th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 411–418.
Meek, C. (1997). Graphical Models: Selecting causal and statistical models. Ph.D. thesis, Carnegie
Mellon University.
Meek, C. (2001). Finding a path is harder than ﬁnding a tree. Journal of Artiﬁcial Intelligence
Research 15, 383–389.
Meek, C. and D. Heckerman (1997). Structure and parameter learning for causal independence
and causal interaction models. In Proc. 13th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 366–375.
Meila, M. and T. Jaakkola (2000). Tractable Bayesian learning of tree belief networks. In Proc.
16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Meila, M. and M. Jordan (2000). Learning with mixtures of trees. Journal of Machine Learning
Research 1, 1–48.
Meltzer, T., C. Yanover, and Y. Weiss (2005). Globally optimal solutions for energy minimization
in stereo vision using reweighted belief propagation.
In Proc. International Conference on
Computer Vision (ICCV), pp. 428–435.
Metropolis, N., A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller (1953). Equation of state
calculation by fast computing machines. Journal of Chemical Physics 21, 1087–1092.
Meyer, J., M. Phillips, P. Cho, I. Kalet, and J. Doctor (2004). Application of inﬂuence diagrams
to prostate intensity-modulated radiation therapy plan selection.
Physics in Medicine and
Biology 49, 1637–53.
Middleton, B., M. Shwe, D. Heckerman, M. Henrion, E. Horvitz, H. Lehmann, and G. Cooper
(1991). Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base.

1196
BIBLIOGRAPHY
II. Evaluation of diagnostic performance. Methods of Information in Medicine 30, 256–67.
Milch, B., B. Marthi, and S. Russell (2004). BLOG: Relational modeling with unknown objects. In
ICML 2004 Workshop on Statistical Relational Learning and Its Connections to Other Fields.
Milch, B., B. Marthi, S. Russell, D. Sontag, D. Ong, and A. Kolobov (2005). BLOG: Probabilis-
tic models with unknown objects. In Proc. 19th International Joint Conference on Artiﬁcial
Intelligence (IJCAI), pp. 1352–1359.
Milch, B., B. Marthi, S. Russell, D. Sontag, D. Ong, and A. Kolobov (2007). BLOG: Probabilistic
models with unknown objects. See Getoor and Taskar (2007).
Miller, R., H. Pople, and J. Myers (1982). Internist-1, an experimental computer-based diagnostic
consultant for general internal medicine. New England Journal of Medicine 307, 468–76.
Minka, T. (2005). Discriminative models, not discriminative training. Technical Report MSR-TR-
2005-144, Microsoft Research.
Minka, T. and J. Laﬀerty (2002). Expectation propagation for the generative aspect model. In
Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Minka, T. P. (2001a).
Algorithms for maximum-likelihood logistic regression.
Available from
http://www.stat.cmu.edu/~minka/papers/logreg.html.
Minka, T. P. (2001b). Expectation propagation for approximate Bayesian inference. In Proc. 17th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 362–369.
Møller, J. M., A. Pettitt, K. Berthelsen, and R. Reeves (2006). An eﬃcient Markov chain Monte
Carlo method for distributions with intractable normalisation constants. Biometrika 93(2),
451–458.
Montemerlo, M., S. Thrun, D. Koller, and B. Wegbreit (2002). FastSLAM: A factored solution to
the simultaneous localization and mapping problem. In Proc. 18th Conference on Artiﬁcial
Intelligence (AAAI), pp. 593–598.
Monti, S. and G. F. Cooper (1997).
Learning Bayesian belief networks with neural network
estimators. In Proc. 11th Conference on Neural Information Processing Systems (NIPS), pp. 579–
584.
Mooij, J. M. and H. J. Kappen (2007). Suﬃcient conditions for convergence of the sum-product
algorithm. IEEE Trans. Information Theory 53, 4422–4437.
Moore, A. (2000).
The anchors hierarchy:
Using the triangle inequality to survive high-
dimensional data. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
397–405.
Moore, A. and W.-K. Wong (2003). Optimal reinsertion: A new search operator for accelerated
and more accurate bayesian network structure learning. In Proc. 20th International Conference
on Machine Learning (ICML), pp. 552–559.
Moore, A. W. and M. S. Lee (1997). Cached suﬃcient statistics for eﬃcient machine learning with
large datasets. Journal of Artiﬁcial Intelligence Research 8, 67–91.
Morgan, M. and M. Henrion (Eds.) (1990). Uncertainty: A Guide to Dealing with Uncertainty in
Quantitative Risk and Policy Analysis. Cambridge University Press.
Motwani, R. and P. Raghavan (1995). Randomized Algorithnms. Cambridge University Press.
Muramatsu, M. and T. Suzuki (2003). A new second-order cone programming relaxation for
max-cut problems. Journal of Operations Research of Japan 43, 164–177.
Murphy, K. (1999). Bayesian map learning in dynamic environments. In Proc. 13th Conference on
Neural Information Processing Systems (NIPS).
Murphy, K. (2002).
Dynamic Bayesian Networks:
A tutorial.
Technical report, Mas-

BIBLIOGRAPHY
1197
sachussetts Institute of Technology.
Available from http://www.cs.ubc.ca/~murphyk/
Papers/dbnchapter.pdf.
Murphy, K. and M. Paskin (2001). Linear time inference in hierarchical HMMs. In Proc. 15th
Conference on Neural Information Processing Systems (NIPS).
Murphy, K. and Y. Weiss (2001). The factored frontier algorithm for approximate inference in
DBNs. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Murphy, K. P. (1998). Inference and learning in hybrid Bayesian networks. Technical Report
UCB/CSD-98-990, University of California, Berkeley.
Murphy, K. P., Y. Weiss, and M. Jordan (1999). Loopy belief propagation for approximate inference:
an empirical study. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
467–475.
Murray, I. and Z. Ghahramani (2004). Bayesian learning in undirected graphical models: Ap-
proximate MCMC algorithms. In Proc. 20th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI).
Murray, I., Z. Ghahramani, and D. MacKay (2006). MCMC for doubly-intractable distributions. In
Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Myers, J., K. Laskey, and T. Levitt (1999). Learning Bayesian networks from incomplete data with
stochastic search algorithms. In Proc. 15th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 476–485.
Narasimhan, M. and J. Bilmes (2004). PAC-learning bounded tree-width graphical models. In
Proc. 20th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Ndilikilikesha, P. (1994).
Potential inﬂuence diagrams.
International Journal of Approximate
Reasoning 10, 251–85.
Neal, R. (1996). Sampling from multimodal distributions using tempered transitions. Statistics
and Computing 6, 353–366.
Neal, R. (2001). Annealed importance sampling. Statistics and Computing 11(2), 25–139.
Neal, R. (2003). Slice sampling. Annals of Statistics 31(3), 705–767.
Neal, R. M. (1992). Asymmetric parallel Boltzmann machines are belief networks. Neural Com-
putation 4(6), 832–834.
Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods. Technical
Report CRG-TR-93-1, University of Toronto.
Neal, R. M. and G. E. Hinton (1998). A new view of the EM algorithm that justiﬁes incremental
and other variants. See Jordan (1998).
Neapolitan, R. E. (2003). Learning Bayesian Networks. Prentice Hall.
Ng, A. and M. Jordan (2000). Approximate inference algorithms for two-layer Bayesian networks.
In Proc. 14th Conference on Neural Information Processing Systems (NIPS).
Ng, A. and M. Jordan (2002). On discriminative vs. generative classiﬁers: A comparison of logistic
regression and naive Bayes. In Proc. 16th Conference on Neural Information Processing Systems
(NIPS).
Ng, B., L. Peshkin, and A. Pfeﬀer (2002). Factored particles for scalable monitoring. In Proc. 18th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 370–377.
Ngo, L. and P. Haddawy (1996). Answering queries from context-sensitive probabilistic knowledge
bases. Theoretical Computer Science.
Nielsen, J., T. Koˇcka, and J. M. Peña (2003). On local optima in learning Bayesian networks. In
Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 435–442.

1198
BIBLIOGRAPHY
Nielsen, T. and F. Jensen (1999). Welldeﬁned decision scenarios. In Proc. 15th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 502–11.
Nielsen, T. and F. Jensen (2000). Representing and solving asymmetric Bayesian decision prob-
lems. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 416–25.
Nielsen, T., P.-H. Wuillemin, F. Jensen, and U. Kjærulﬀ(2000). Using robdds for inference in
Bayesian networks with troubleshooting as an example. In Proc. 16th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 426–35.
Nilsson, D. (1998). An eﬃcient algorithm for ﬁnding the M most probable conﬁgurations in
probabilistic expert systems. Statistics and Computing 8(2), 159–173.
Nilsson, D. and S. Lauritzen (2000). Evaluating inﬂuence diagrams with LIMIDs. In Proc. 16th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 436–445.
Nodelman, U., C. R. Shelton, and D. Koller (2002). Continuous time Bayesian networks. In Proc.
18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 378–387.
Nodelman, U., C. R. Shelton, and D. Koller (2003). Learning continuous time Bayesian networks.
In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Norman, J., Y. Shahar, M. Kuppermann, and B. Gold (1998).
Decision-theoretic analysis of
prenatal testing strategies.
Technical Report SMI-98-0711, Stanford University, Section on
Medical Informatics.
Normand, S.-L. and D. Tritchler (1992). Parameter updating in a Bayes network. Journal of the
American Statistical Association 87, 1109–1115.
Nummelin, E. (1984). General Irreducible Markov Chains and Non-Negative Operators. Cambridge
University Press.
Nummelin, E. (2002). Mc’s for mcmc’ists. International Statistical Review 70(2), 215–240.
Olesen, K. G., U. Kjærulﬀ, F. Jensen, B. Falck, S. Andreassen, and S. Andersen (1989). A Munin
network for the median nerve — A case study on loops. Applied Artiﬁcial Intelligence 3,
384–403.
Oliver, R. M. and J. Q. Smith (Eds.) (1990). Inﬂuence Diagrams, Belief Nets and Decision Analysis.
New York: John Wiley & Sons.
Olmsted, S. (1983).
On Representing and Solving Inﬂuence Diagrams.
Ph.D. thesis, Stanford
University.
Opper, M. and O. Winther (2005). Expectation consistent free energies for approximate inference.
In Proc. 19th Conference on Neural Information Processing Systems (NIPS).
Ortiz, L. and L. Kaelbling (1999). Accelerating em: An empirical study. In Proc. 15th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 512–521.
Ortiz, L. E. and L. P. Kaelbling (2000). Adaptive importance sampling for estimation in structured
domains. In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 446–454.
Osborne, M. and A. Rubinstein (1994). A Course in Game Theory. The MIT Press.
Ostendorf, M., V. Digalakis, and O. Kimball (1996). From HMMs to segment models: A uniﬁed
view of stochastic modeling for speech recognition. IEEE Transactions on Speech and Audio
Processing 4(5), 360–378.
Pakzad, P. and V. Anantharam (2002). Minimal graphical representation of Kikuchi regions. In
Proc. 40th Allerton Conference on Communication Control and Computing, pp. 1585–1594.
Papadimitriou, C. (1993). Computational Complexity. Addison Wesley.
Parisi, G. (1988). Statistical Field Theory. Reading, Massachusetts: Addison-Wesley.
Park, J. (2002). MAP complexity results and approximation methods. In Proc. 18th Conference on

BIBLIOGRAPHY
1199
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 388–396.
Park, J. and A. Darwiche (2001). Approximating MAP using local search. In Proc. 17th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 403–â˘A¸S410.
Park, J. and A. Darwiche (2003). Solving MAP exactly using systematic search. In Proc. 19th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Park, J. and A. Darwiche (2004a).
Complexity results and approximation strategies for MAP
explanations. Journal of Artiﬁcial Intelligence Research 21, 101–133.
Park, J. and A. Darwiche (2004b). A diﬀerential semantics for jointree algorithms. Artiﬁcial
Intelligence 156, 197–216.
Parter, S. (1961). The user of linear graphs in Gauss elimination. SIAM Review 3, 119–130.
Paskin, M. (2003a). Sample propagation. In Proc. 17th Conference on Neural Information Processing
Systems (NIPS).
Paskin, M. (2003b). Thin junction tree ﬁlters for simultaneous localization and mapping. In Proc.
18th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1157–1164.
Pasula, H., B. Marthi, B. Milch, S. Russell, and I. Shpitser (2002).
Identity uncertainty and
citation matching. In Proc. 16th Conference on Neural Information Processing Systems (NIPS), pp.
1401–1408.
Pasula, H., S. Russell, M. Ostland, and Y. Ritov (1999). Tracking many objects with many sensors.
In Proc. 16th International Joint Conference on Artiﬁcial Intelligence (IJCAI).
Patrick, D., J. Bush, and M. Chen (1973). Methods for measuring levels of well-being for a health
status index. Health Services Research 8, 228–45.
Pearl, J. (1986a).
A constraint-propagation approach to probabilistic reasoning.
In Proc. 2nd
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 357–370.
Pearl, J. (1986b).
Fusion, propagation and structuring in belief networks.
Artiﬁcial Intelli-
gence 29(3), 241–88.
Pearl, J. (1987).
Evidential reasoning using stochastic simulation of causal models.
Artiﬁcial
Intelligence 32, 245–257.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. San Mateo, California: Morgan
Kaufmann.
Pearl, J. (1995). Causal diagrams for empirical research. Biometrika 82, 669–710.
Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge Univ. Press.
Pearl, J. and R. Dechter (1996). Identifying independencies in causal graphs with feedback. In
Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 420–26.
Pearl, J. and A. Paz (1987).
GRAPHOIDS: A graph-based logic for reasoning about relevance
relations. In B. Du Boulay, D. Hogg, and L. Steels (Eds.), Advances in Artiﬁcial Intelligence,
Volume 2, pp. 357–363. Amsterdam: North Holland.
Pearl, J. and T. S. Verma (1991). A theory of inferred causation. In Proc. Conference on Knowledge
Representation and Reasoning (KR), pp. 441–452.
Pe’er, D., A. Regev, G. Elidan, and N. Friedman (2001). Inferring subnetworks from preturbed
expression proﬁles. Bioinformatics 17, S215–S224.
Peng, Y. and J. Reggia (1986). Plausibility of diagnostic hypotheses. In Proc. 2nd Conference on
Artiﬁcial Intelligence (AAAI), pp. 140–45.
Perkins, S., K. Lacker, and J. Theiler (2003, March). Grafting: Fast, incremental feature selection
by gradient descent in function space. Journal of Machine Learning Research 3, 1333–1356.
Peterson, C. and J. R. Anderson (1987).
A mean ﬁeld theory learning algorithm for neural

1200
BIBLIOGRAPHY
networks. Complex Systems 1, 995–1019.
Pfeﬀer, A., D. Koller, B. Milch, and K. Takusagawa (1999).
spook: A system for probabilistic
object-oriented knowledge representation. In Proc. 15th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 541–550.
Poh, K. and E. Horvitz (2003). Reasoning about the value of decision-model reﬁnement: Methods
and application. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 174–
182.
Poland, W. (1994). Decision Analysis with Continuous and Discrete Variables: A Mixture Distribution
Approach. Ph.D. thesis, Department of Engineering-Economic Systems, Stanford University.
Poole, D. (1989). Average-case analysis of a search algorithm for estimating prior and posterior
probabilities in Bayesian networks with extreme probabilities. In Proc. 13th International Joint
Conference on Artiﬁcial Intelligence (IJCAI), pp. 606–612.
Poole, D. (1993a). Probabilistic Horn abduction and Bayesian networks. Artiﬁcial Intelligence 64(1),
81–129.
Poole, D. (1993b). The use of conﬂicts in searching Bayesian networks. In Proc. 9th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 359–367.
Poole, D. and N. Zhang (2003). Exploiting causal independence in Bayesian network inference.
Journal of Artiﬁcial Intelligence Research 18, 263–313.
Poon, H. and P. Domingos (2007).
Joint inference in information extraction.
In Proc. 23rd
Conference on Artiﬁcial Intelligence (AAAI), pp. 913–918.
Pradhan, M. and P. Dagum (1996). Optimal Monte Carlo estimation of belief network inference.
In Proc. 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 446–453.
Pradhan, M., M. Henrion, G. Provan, B. Del Favero, and K. Huang (1996). The sensitivity of belief
networks to imprecise probabilities: An experimental investigation. Artiﬁcial Intelligence 85,
363–97.
Pradhan, M., G. M. Provan, B. Middleton, and M. Henrion (1994). Knowledge engineering for
large belief networks. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
484–490.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley and Sons, New York.
Qi, R., N. Zhang, and D. Poole (1994). Solving asymmetric decision problems with inﬂuence
diagrams. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 491–497.
Qi, Y., M. Szummer, and T. Minka (2005). Bayesian conditional random ﬁelds. In Proc. 11thWork-
shop on Artiﬁcial Intelligence and Statistics.
Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and selected applications in speech
recognition. Proceedings of the IEEE 77(2), 257–286.
Rabiner, L. R. and B. H. Juang (1986, January). An introduction to hidden Markov models. IEEE
ASSP Magazine, 4–15.
Ramsey, F. (1931). The Foundations of Mathematics and other Logical Essays. London: Kegan, Paul,
Trench, Trubner & Co., New York: Harcourt, Brace and Company. edited by R.B. Braithwaite.
Rasmussen, C. and C. Williams (2006). Gaussian Processes for Machine Learning. MIT Press.
Rasmussen, C. E. (1999). The inﬁnite gaussian mixture model. In Proc. 13th Conference on Neural
Information Processing Systems (NIPS), pp. 554–560.
Ravikumar, P. and J. Laﬀerty (2006). Quadratic programming relaxations for metric labelling
and Markov random ﬁeld MAP estimation. In Proc. 23rd International Conference on Machine

BIBLIOGRAPHY
1201
Learning (ICML).
Renooij, S. and L. van der Gaag (2002). From qualitative to quantitative probabilistic networks.
In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 422–429.
Richardson, M. and P. Domingos (2006). Markov logic networks. Machine Learning 62, 107–136.
Richardson, T. (1994). Properties of cyclic graphical models. Master’s thesis, Carnegie Mellon
University.
Riezler, S. and A. Vasserman (2004). Incremental feature selection and l1 regularization for relaxed
maximum-entropy modeling. In Proc. Conference on Empirical Methods in Natural Language
Processing (EMNLP).
Ripley, B. D. (1987). Stochastic Simulation. New York: John Wiley & Sons.
Rissanen, J. (1987). Stochastic complexity (with discussion). Journal of the Royal Statistical Society,
Series B 49, 223–265.
Ristic, B., S. Arulampalam, and N. Gordon (2004). Beyond the Kalman Filter: Particle Filters for
Tracking Applications. Artech House Publishers.
Robert, C. and G. Casella (1996). Rao-Blackwellisation of sampling schemes. Biometrika 83(1),
81–94.
Robert, C. and G. Casella (2005). Monte Carlo Statistical Methods (2nd ed.). Springer Texts in
Statistics.
Robins, J. M. and L. A. Wasserman (1997). Estimation of eﬀects of sequential treatments by
reparameterizing directed acyclic graphs. In Proc. 13th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 409–420.
Rose, D. (1970). Triangulated graphs and the elimination process. Journal of Mathematical Analysis
and Applications 32, 597–609.
Ross, S. M. (1988). A First Course in Probability (third ed.). London: Macmillan.
Rother, C., S. Kumar, V. Kolmogorov, and A. Blake (2005). Digital tapestry. In Proc. Conference on
Computer Vision and Pattern Recognition (CVPR).
Rubin, D. (1974). Estimating causal eﬀects of treatments in randomized and nonrandomized
studies. Journal of Educational Psychology 66(5), 688–701.
Rubin, D. R. (1976). Inference and missing data. Biometrika 63, 581–592.
Rusmevichientong, P. and B. Van Roy (2001). An analysis of belief propagation on the turbo
decoding graph with Gaussian densities. IEEE Transactions on Information Theory 48(2).
Russell, S. and P. Norvig (2003). Artiﬁcial Intelligence: A Modern Approach (2 ed.). Prentice Hall.
Rustagi, J. (1976). Variational Methods in Statistics. New York: Academic Press.
Sachs, K., O. Perez, D. Pe’er, D. Lauﬀenburger, and G. Nolan (2005, April). Causal protein-signaling
networks derived from multiparameter single-cell data. Science 308(5721), 523–529.
Sakurai, J. J. (1985). Modern Quantum Mechanics. Reading, Massachusetts: Addison-Wesley.
Santos, A. (1994). A linear constraint satisfaction approach to cost-based abduction. Artiﬁcial
Intelligence 65(1), 1–28.
Santos, E. (1991).
On the generation of alternative explanations with implications for belief
revision. In Proc. 7th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 339–347.
Saul, L., T. Jaakkola, and M. Jordan (1996). Mean ﬁeld theory for sigmoid belief networks. Journal
of Artiﬁcial Intelligence Research 4, 61–76.
Saul, L. and M. Jordan (1999). Mixed memory Markov models: Decomposing complex stochastic
processes as mixture of simpler ones. Machine Learning 37(1), 75–87.
Saul, L. K. and M. I. Jordan (1996). Exploiting tractable substructures in intractable networks. In

1202
BIBLIOGRAPHY
Proc. 10th Conference on Neural Information Processing Systems (NIPS).
Savage, L. (1951). The theory of statistical decision. Journal of the American Statistical Associa-
tion 46, 55–67.
Savage, L. J. (1954). Foundations of Statistics. New York: John Wiley & Sons.
Schäﬀer, A. (1996). Faster linkage analysis computations for pedigrees with loops or unused
alleles. Human Heredity, 226–235.
Scharstein, D. and R. Szeliski (2003). High-accuracy stereo depth maps using structured light. In
Proc. Conference on Computer Vision and Pattern Recognition (CVPR), Volume 1, pp. 195–202.
Schervish, M. (1995). Theory of Statistics. Springer-Verlag.
Schlesinger, M. (1976).
Sintaksicheskiy analiz dvumernykh zritelnikh singnalov v usloviyakh
pomekh (syntactic analysis of two-dimensional visual signals in noisy conditions).
Kiber-
netika 4, 113–130.
Schlesinger, M. and V. Giginyak (2007a). Solution to structural recognition (max,+)-problems by
their equivalent transformations (part 1). Control Systems and Computers 1, 3–15.
Schlesinger, M. and V. Giginyak (2007b). Solution to structural recognition (max,+)-problems by
their equivalent transformations (part 2). Control Systems and Computers 2, 3–18.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6(2), 461–464.
Segal, E., D. Pe’er, A. Regev, D. Koller, and N. Friedman (2005, April). Learning module networks.
Journal of Machine Learning Research 6, 557–588.
Segal, E., B. Taskar, A. Gasch, N. Friedman, and D. Koller (2001). Rich probabilistic models for
gene expression. Bioinformatics 17(Suppl 1), S243–52.
Settimi, R. and J. Smith (2000). Geometry, moments and conditional independence trees with
hidden variables. Annals of Statistics.
Settimi, R. and J. Q. Smith (1998a). On the geometry of Bayesian graphical models with hidden
variables. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 472–479.
Settimi, R. and J. Q. Smith (1998b). On the geometry of Bayesian graphical models with hidden
variables. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 472–479.
Shachter, R. (1988, July–August).
Probabilistic inference and inﬂuence diagrams.
Operations
Research 36, 589–605.
Shachter, R. (1999).
Eﬃcient value of information computation.
In Proc. 15th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 594–601.
Shachter, R., S. K. Andersen, and P. Szolovits (1994). Global conditioning for probabilistic inference
in belief networks. In Proc. 10th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
514–522.
Shachter, R. and D. Heckerman (1987). Thinking backwards for knowledge acquisition. Artiﬁcial
Intelligence Magazine 8, 55 – 61.
Shachter, R. and C. Kenley (1989). Gaussian inﬂuence diagrams. Management Science 35, 527–550.
Shachter, R. and P. Ndilikilikesha (1993). Using inﬂuence diagrams for probabilistic inference
and decision making. In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
276–83.
Shachter, R. D. (1986). Evaluating inﬂuence diagrams. Operations Research 34, 871–882.
Shachter, R. D. (1989). Evidence absorption and propagation through evidence reversals. In Proc.
5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 173–190.
Shachter, R. D. (1998). Bayes-ball: The rational pastime. In Proc. 14th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 480–487.

BIBLIOGRAPHY
1203
Shachter, R. D., B. D’Ambrosio, and B. A. Del Favero (1990). Symbolic probabilistic inference in
belief networks. In Proc. 6th Conference on Artiﬁcial Intelligence (AAAI), pp. 126–131.
Shachter, R. D. and M. A. Peot (1989). Simulation approaches to general probabilistic inference
on belief networks. In Proc. 5th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
221–230.
Shachter, R. D. and M. A. Peot (1992). Decision making using probabilistic inference methods. In
Proc. 8th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 276–83.
Shafer, G. and J. Pearl (Eds.) (1990).
Readings in Uncertain Reasoning.
Representation and
Reasoning. San Mateo, California: Morgan Kaufmann.
Shafer, G. and P. Shenoy (1990). Probability propagation. Annals of Mathematics and Artiﬁcial
Intelligence 2, 327–352.
Shannon, C. (1948). A mathematical theory of communication. Bell System Technical Journal 27,
379–423; 623–656.
Shawe-Taylor, J. and N. Cristianini (2000). Support Vector Machines and other kernel-based learning
methods. Cambridge University Press.
Shenoy, P. (1989). A valuation-based language for expert systems. International Journal of Ap-
proximate Reasoning 3, 383–411.
Shenoy, P. (2000). Valuation network representation and solution of asymmetric decision prob-
lems. European Journal of Operational Research 121(3), 579–608.
Shenoy, P. and G. Shafer (1990). Axioms for probability and belief-function propagation. In Proc.
6th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 169–198.
Shenoy, P. P. (1992).
Valuation-based systems for Bayesian decision analysis.
Operations Re-
search 40, 463–484.
Shental, N., A. Zomet, T. Hertz, and Y. Weiss (2003). Learning and inferring image segmentations
using the GBP typical cut algorithm. In Proc. International Conference on Computer Vision.
Shimony, S. (1991). Explanation, irrelevance and statistical independence. In Proc. 7th Conference
on Artiﬁcial Intelligence (AAAI).
Shimony, S. (1994). Finding MAPs for belief networks in NP-hard. Artiﬁcial Intelligence 68(2),
399–410.
Shoikhet, K. and D. Geiger (1997). A practical algorithm for ﬁnding optimal triangulations. In
Proc. 13th Conference on Artiﬁcial Intelligence (AAAI), pp. 185–190.
Shwe, M. and G. Cooper (1991). An empirical analysis of likelihood-weighting simulation on a
large, multiply connected medical belief network. Computers and Biomedical Research 24,
453–475.
Shwe, M., B. Middleton, D. Heckerman, M. Henrion, E. Horvitz, H. Lehmann, and G. Cooper
(1991). Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base.
I. The probabilistic model and inference algorithms. Methods of Information in Medicine 30,
241–55.
Silander, T. and P. Myllymaki (2006). A simple approach for ﬁnding the globally optimal Bayesian
network structure. In Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Singh, A. and A. Moore (2005). Finding optimal bayesian networks by dynamic programming.
Technical report, Carnegie Mellon University.
Sipser, M. (2005). Introduction to the Theory of Computation (Second ed.). Course Technology.
Smith, A. and G. Roberts (1993). Bayesian computation via the Gibbs sampler and related Markov
chain Monte Carlo methods. Journal of the Royal Statistical Society, Series B 55, 3–23.

1204
BIBLIOGRAPHY
Smith, J. (1989). Inﬂuence diagrams for statistical modeling. Annals of Statistics 17(2), 654–72.
Smith, J., S. Holtzman, and J. Matheson (1993). Structuring conditional relationships in inﬂuence
diagrams. Operations Research 41(2), 280–297.
Smyth, P., D. Heckerman, and M. Jordan (1997). Probabilistic independence networks for hidden
Markov probability models. Neural Computation 9(2), 227–269.
Sontag, D. and T. Jaakkola (2007). New outer bounds on the marginal polytope. In Proc. 21st
Conference on Neural Information Processing Systems (NIPS).
Sontag, D., T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss (2008). Tightening LP relaxations
for MAP using message passing. In Proc. 24th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI).
Speed, T. and H. Kiiveri (1986). Gaussian Markov distributions over ﬁnite graphs. The Annals of
Statistics 14(1), 138–150.
Spetzler, C. and C.-A. von Holstein (1975). Probabilistic encoding in decision analysis. Manage-
ment Science, 340–358.
Spiegelhalter, D. and S. Lauritzen (1990). Sequential updating of conditional probabilities on
directed graphical structures. Networks 20, 579–605.
Spiegelhalter, D. J., A. P. Dawid, S. L. Lauritzen, and R. G. Cowell (1993). Bayesian analysis in
expert systems. Statistical Science 8, 219–283.
Spirtes, P. (1995). Directed cyclic graphical representations of feedback models. In Proc. 11th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 491–98.
Spirtes, P., C. Glymour, and R. Scheines (1991). An algorithm for fast recovery of sparse causal
graphs. Social Science Computer Review 9, 62–72.
Spirtes, P., C. Glymour, and R. Scheines (1993). Causation, Prediction and Search. Number 81 in
Lecture Notes in Statistics. New York: Springer-Verlag.
Spirtes, P., C. Meek, and T. Richardson (1999). An algorithm for causal inference in the presence
of latent variables and selection bias. See Glymour and Cooper (1999), pp. 211–52.
Srebro, N. (2001).
Maximum likelihood bounded tree-width Markov networks.
In Proc. 17th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Srinivas, S. (1993). A generalization of the noisy-or model. In Proc. 9th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pp. 208–215.
Srinivas, S. (1994). A probabilistic approach to hierarchical model-based diagnosis. In Proc. 10th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Studený, M. and R. Bouckaert (1998). On chain graph models for description of conditional
independence structures. Annals of Statistics 26.
Sudderth, E., A. Ihler, W. Freeman, and A. Willsky (2003). Nonparametric belief propagation. In
Proc. Conference on Computer Vision and Pattern Recognition (CVPR), pp. 605–612.
Sutton, C. and T. Minka (2006). Local training and belief propagation. Technical Report MSR-TR-
2006-121, Microsoft Research.
Sutton, C. and A. McCallum (2004). Collective segmentation and labeling of distant entities in
information extraction. In ICML Workshop on Statistical Relational Learning and Its Connections
to Other Fields.
Sutton, C. and A. McCallum (2005).
Piecewise training of undirected models.
In Proc. 21st
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Sutton, C. and A. McCallum (2007). An introduction to conditional random ﬁelds for relational
learning. In L. Getoor and B. Taskar (Eds.), Introduction to Statistical Relational Learning. MIT

BIBLIOGRAPHY
1205
Press.
Sutton, C., A. McCallum, and K. Rohanimanesh (2007, March). Dynamic conditional random
ﬁelds: Factorized probabilistic models for labeling and segmenting sequence data. Journal of
Machine Learning Research 8, 693–723.
Suzuki, J. (1993). A construction of Bayesian networks from databases based on an MDL scheme.
In Proc. 9th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 266–273.
Swendsen, R. and J. Wang (1987). Nonuniversal critical dynamics in Monte Carlo simulations.
Physical Review Letters 58(2), 86–88.
Swendsen, R. H. and J.-S. Wang (1986, Nov). Replica Monte Carlo simulation of spin-glasses.
Physical Review Letters 57(21), 2607–2609.
Szeliski, R., R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, and
C. Rother (2008, June). A comparative study of energy minimization methods for Markov
random ﬁelds with smoothness-based priors. IEEE Trans. on Pattern Analysis and Machine
Intelligence 30(6), 1068–1080. See http://vision.middlebury.edu/MRF for more detailed
results.
Szolovits, P. and S. Pauker (1992). Pedigree analysis for genetic counseling. In Proceedings of the
Seventh World Congress on Medical Informatics (MEDINFO ’92), pp. 679–683. North-Holland.
Tanner, M. A. (1993). Tools for Statistical Inference. New York: Springer-Verlag.
Tarjan, R. and M. Yannakakis (1984). Simple linear-time algorithms to test chordality of graphs,
test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM Journal of
Computing 13(3), 566–579.
Taskar, B., P. Abbeel, and D. Koller (2002). Discriminative probabilistic models for relational data.
In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 485–492.
Taskar, B., P. Abbeel, M.-F. Wong, and D. Koller (2007). Relational Markov networks. See Getoor
and Taskar (2007).
Taskar, B., V. Chatalbashev, and D. Koller (2004). Learning associative Markov networks. In Proc.
21st International Conference on Machine Learning (ICML).
Taskar, B., C. Guestrin, and D. Koller (2003).
Max margin Markov networks.
In Proc. 17th
Conference on Neural Information Processing Systems (NIPS).
Tatikonda, S. and M. Jordan (2002). Loopy belief propagation and Gibbs measures. In Proc. 18th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Tatman, J. A. and R. D. Shachter (1990). Dynamic programming and inﬂuence diagrams. IEEE
Transactions on Systems, Man and Cybernetics 20(2), 365–379.
Teh, Y. and M. Welling (2001). The uniﬁed propagation and scaling algorithm. In Proc. 15th
Conference on Neural Information Processing Systems (NIPS).
Teh, Y., M. Welling, S. Osindero, and G. Hinton (2003). Energy-based models for sparse over-
complete representations. Journal of Machine Learning Research 4, 1235–1260. Special Issue on
ICA.
Teyssier, M. and D. Koller (2005). Ordering-based search: A simple and eﬀective algorithm for
learning bayesian networks. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 584–590.
Thiele, T. (1880). Sur la compensation de quelques erreurs quasisystematiques par la methode des
moindres carrees. Copenhagen: Reitzel.
Thiesson, B. (1995). Accelerated quantiﬁcation of Bayesian networks with incomplete data. In
Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD-

1206
BIBLIOGRAPHY
95), pp. 306–311. AAAI Press.
Thiesson, B., C. Meek, D. M. Chickering, and D. Heckerman (1998). Learning mixtures of Bayesian
networks. In Proc. 14th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Thomas, A., D. Spiegelhalter, and W. Gilks (1992). BUGS: A program to perform Bayesian inference
using Gibbs sampling. In J. Bernardo, J. Berger, A. Dawid, and A. Smith (Eds.), Bayesian Statistics
4, pp. 837–842. Oxford, UK: Clarendon Press.
Thrun, S., W. Burgard, and D. Fox (2005). Probabilistic Robotics. Cambridge, MA: MIT Press.
Thrun, S., D. Fox, W. Burgard, and F. Dellaert (2000). Robust Monte Carlo localization for mobile
robots. Artiﬁcial Intelligence 128(1–2), 99–141.
Thrun, S., Y. Liu, D. Koller, A. Ng, Z. Ghahramani, and H. Durrant-Whyte (2004). Simultaneous
localization and mapping with sparse extended information ﬁlters. International Journal of
Robotics Research 23(7/8).
Thrun, S., C. Martin, Y. Liu, D. Hähnel, R. Emery-Montemerlo, D. Chakrabarti, and W. Burgard
(2004). A real-time expectation maximization algorithm for acquiring multi-planar maps of
indoor environments with mobile robots. IEEE Transactions on Robotics 20(3), 433–443.
Thrun, S., M. Montemerlo, D. Koller, B. Wegbreit, J. Nieto, and E. Nebot (2004). FastSLAM: An
eﬃcient solution to the simultaneous localization and mapping problem with unknown data
association. Journal of Machine Learning Research.
Tian, J. and J. Pearl (2002). On the testable implications of causal models with hidden variables.
In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 519–527.
Tibshirani, R. (1996).
Regression shrinkage and selection via the lasso.
Journal of the Royal
Statistical Society, Series B 58(1), 267–288.
Tierney, L. (1994). Markov chains for exploring posterior distributions. Annals of Statistics 22(4),
1701–1728.
Tong, S. and D. Koller (2001a). Active learning for parameter estimation in Bayesian networks. In
Proc. 15th Conference on Neural Information Processing Systems (NIPS), pp. 647–653.
Tong, S. and D. Koller (2001b). Active learning for structure in Bayesian networks. In Proc. 17th
International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 863–869.
Torrance, G., W. Thomas, and D. Sackett (1972). A utility maximization model for evaluation of
health care programs. Health Services Research 7, 118–133.
Tsochantaridis, I., T. Hofmann, T. Joachims, and Y. Altun (2004). Support vector machine learning
for interdependent and structured output spaces. In Proc. 21st International Conference on
Machine Learning (ICML).
Tversky, A. and D. Kahneman (1974). Judgment under uncertainty: Heuristics and biases. Sci-
ence 185, 1124–1131.
van der Merwe, R., A. Doucet, N. de Freitas, and E. Wan (2000a, Aug.). The unscented particle
ﬁlter. Technical Report CUED/F-INFENG/TR 380, Cambridge University Engineering Depart-
ment.
van der Merwe, R., A. Doucet, N. de Freitas, and E. Wan (2000b). The unscented particle ﬁlter.
In Proc. 14th Conference on Neural Information Processing Systems (NIPS).
Varga, R. (2000). Matrix Iterative Analysis. Springer-Verlag.
Verma, T. (1988). Causal networks: Semantics and expressiveness. In Proc. 4th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 352–359.
Verma, T. and J. Pearl (1988).
Causal networks: Semantics and expressiveness.
In Proc. 4th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 69–76.

BIBLIOGRAPHY
1207
Verma, T. and J. Pearl (1990). Equivalence and synthesis of causal models. In Proc. 6th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 255 –269.
Verma, T. and J. Pearl (1992). An algorithm for deciding if a set of observed independencies has
a causal explanation. In Proc. 8th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
323–330.
Vickrey, D. and D. Koller (2002). Multi-agent algorithms for solving graphical games. In Proceed-
ings of the Eighteenth National Conference on Artiﬁcial Intelligence (AAAI-02), pp. 345–351.
Vishwanathan, S., N. Schraudolph, M. Schmidt, and K. Murphy (2006).
Accelerated training
of conditional random ﬁelds with stochastic gradient methods. In Proc. 23rd International
Conference on Machine Learning (ICML), pp. 969–976.
Viterbi, A. (1967, April). Error bounds for convolutional codes and an asymptotically optimum
decoding algorithm. IEEE Transactions on Information Theory 13(2), 260–269.
von Neumann, J. and O. Morgenstern (1944). Theory of games and economic behavior (ﬁrst ed.).
Princeton, NJ: Princeton Univ. Press.
von Neumann, J. and O. Morgenstern (1947). Theory of games and economic behavior (second
ed.). Princeton, NJ: Princeton Univ. Press.
von Winterfeldt, D. and W. Edwards (1986). Decision Analysis and Behavioral Research. Cambridge,
UK: Cambridge University Press.
Vorobev, N. (1962). Consistent families of measures and their extensions. Theory of Probability
and Applications 7, 147–63.
Wainwright, M. (2006). Estimating the “wrong” graphical model: Beneﬁts in the computation-
limited setting. Journal of Machine Learning Research 7, 1829–1859.
Wainwright, M., T. Jaakkola, and A. Willsky (2003a). Tree-based reparameterization framework for
analysis of sum-product and related algorithms. IEEE Transactions on Information Theory 49(5).
Wainwright, M., T. Jaakkola, and A. Willsky (2003b).
Tree-reweighted belief propagation and
approximate ML estimation by pseudo-moment matching. In Proc. 9thWorkshop on Artiﬁcial
Intelligence and Statistics.
Wainwright, M., T. Jaakkola, and A. Willsky (2004, April). Tree consistency and bounds on the
performance of the max-product algorithm and its generalizations. Statistics and Computing 14,
143–166.
Wainwright, M., T. Jaakkola, and A. Willsky (2005). MAP estimation via agreement on trees:
Message-passing and linear programming. IEEE Transactions on Information Theory.
Wainwright, M., T. Jaakkola, and A. S. Willsky (2001). Tree-based reparameterization for approx-
imate estimation on loopy graphs. In Proc. 15th Conference on Neural Information Processing
Systems (NIPS).
Wainwright, M., T. Jaakkola, and A. S. Willsky (2002a).
Exact map estimates by (hyper)tree
agreement. In Proc. 16th Conference on Neural Information Processing Systems (NIPS).
Wainwright, M., T. Jaakkola, and A. S. Willsky (2002b). A new class of upper bounds on the log
partition function. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Wainwright, M. and M. Jordan (2003). Graphical models, exponential families, and variational
inference. Technical Report 649, Department of Statistics, University of California, Berkeley.
Wainwright, M. and M. Jordan (2004). Semideﬁnite relaxations for approximate inference on
graphs with cycles. In Proc. 18th Conference on Neural Information Processing Systems (NIPS).
Wainwright, M., P. Ravikumar, and J. Laﬀerty (2006). High-dimensional graphical model selec-
tion using ℓ1-regularized logistic regression. In Proc. 20th Conference on Neural Information

1208
BIBLIOGRAPHY
Processing Systems (NIPS).
Warner, H., A. Toronto, L. Veasey, and R. Stephenson (1961).
A mathematical approach to
medical diagnosis — application to congenital heart disease. Journal of the American Madical
Association 177, 177–184.
Weiss, Y. (1996). Interpreting images by propagating bayesian beliefs. In Proc. 10th Conference on
Neural Information Processing Systems (NIPS), pp. 908–914.
Weiss, Y. (2000). Correctness of local probability propagation in graphical models with loops.
Neural Computation 12, 1–41.
Weiss, Y. (2001).
Comparing the mean ﬁeld method and belief propagation for approximate
inference in MRFs. In M. Opper and D. Saad (Eds.), Advanced mean ﬁeld methods, pp. 229–
240. Cambridge, Massachusetts: MIT Press.
Weiss, Y. and W. Freeman (2001a).
Correctness of belief propagation in Gaussian graphical
models of arbitrary topology. Neural Computation 13.
Weiss, Y. and W. Freeman (2001b). On the optimality of solutions of the max-product belief
propagation algorithm in arbitrary graphs.
IEEE Transactions on Information Theory 47(2),
723–735.
Weiss, Y., C. Yanover, and T. Meltzer (2007). MAP estimation, linear programming and belief
propagation with convex free energies. In Proc. 23rd Conference on Uncertainty in Artiﬁcial
Intelligence (UAI).
Welling, M. (2004). On the choice of regions for generalized belief propagation. In Proc. 20th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Welling, M., T. Minka, and Y. Teh (2005). Structured region graphs: Morphing EP into GBP. In
Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Welling, M. and S. Parise (2006a). Bayesian random ﬁelds: The Bethe-Laplace approximation. In
Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Welling, M. and S. Parise (2006b). Structure learning in Markov random ﬁelds. In Proc. 20th
Conference on Neural Information Processing Systems (NIPS).
Welling, M. and Y.-W. Teh (2001). Belief optimization for binary networks: a stable alternative to
loopy belief propagation. In Proc. 17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
Wellman, M. (1985).
Reasoning about preference models.
Technical Report MIT/LCS/TR-340,
Laboratory for Computer Science, MIT.
Wellman, M., J. Breese, and R. Goldman (1992).
From knowledge bases to decision models.
Knowledge Engineering Review 7(1), 35–53.
Wellman, M. and J. Doyle (1992). Modular utility representation for decision-theoretic planning.
In Procec. First International Conference on AI Planning Systems, pp. 236–42. Morgan Kaufmann.
Wellman, M. P. (1990). Foundamental concepts of qualitative probabilistic networks. Artiﬁcial
Intelligence 44, 257–303.
Wellner, B., A. McCallum, F. Peng, and M. Hay (2004).
An integrated, conditional model of
information extraction and coreference with application to citation matching. In Proc. 20th
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 593–601.
Wermuth, N. (1980). Linear recursive equations, covariance selection and path analysis. Journal
of the American Statistical Association 75, 963–975.
Werner, T. (2007). A linear programming approach to max-sum problem: A review. IEEE Trans.
on Pattern Analysis and Machine Intelligence 29(7), 1165–1179.
West, M. (1993). Mixture models, Monte Carlo, Bayesian updating and dynamic models. Comput-

BIBLIOGRAPHY
1209
ing Science and Statistics 24, 325–333.
Whittaker, J. (1990). Graphical Models in Applied Multivariate Statistics. Chichester, United King-
dom: John Wiley and Sons.
Wiegerinck, W. (2000). Variational approximations between mean ﬁeld theory and the junction
tree algorithm.
In Proc. 16th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
626–636.
Wold, H. (1954). Causality and econometrics. Econometrica 22, 162–177.
Wood, F., T. Griﬃths, and Z. Ghahramani (2006). A non-parametric bayesian method for inferring
hidden causes. In Proc. 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 536–
543.
Wright, S. (1921). Correlation and causation. Journal of Agricultural Research 20, 557–85.
Wright, S. (1934). The method of path coeﬃcients. Annals of Mathematical Statistics 5, 161–215.
Xing, E., M. Jordan, and S. Russell (2003). A generalized mean ﬁeld algorithm for variational in-
ference in exponential families. In Proc. 19th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 583–591.
Yanover, C., T. Meltzer, and Y. Weiss (2006, September). Linear programming relaxations and
belief propagation — an empirical study. Journal of Machine Learning Research 7, 1887–1907.
Yanover, C., O. Schueler-Furman, and Y. Weiss (2007). Minimizing and learning energy functions
for side-chain prediction.
In Proc. International Conference on Research in Computational
Molecular Biology (RECOMB), pp. 381–395.
Yanover, C. and Y. Weiss (2003). Finding the M most probable conﬁgurations using loopy belief
propagation. In Proc. 17th Conference on Neural Information Processing Systems (NIPS).
Yedidia, J., W. Freeman, and Y. Weiss (2005).
Constructing free-energy approximations and
generalized belief propagation algorithms. IEEE Trans. Information Theory 51, 2282–2312.
Yedidia, J. S., W. T. Freeman, and Y. Weiss (2000). Generalized belief propagation. In Proc. 14th
Conference on Neural Information Processing Systems (NIPS), pp. 689–695.
York, J. (1992). Use of the Gibbs sampler in expert systems. Artiﬁcial Intelligence 56, 115–130.
Yuille, A. L. (2002). CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent
alternatives to belief propagation. Neural Computation 14, 1691–1722.
Zhang, N. (1998).
Probabilistic inference in inﬂuence diagrams.
In Proc. 14th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 514–522.
Zhang, N. and D. Poole (1994).
A simple approach to Bayesian network computations.
In
Proceedings of the 10th Biennial Canadian Artiﬁcial Intelligence Conference, pp. 171–178.
Zhang, N. and D. Poole (1996). Exploiting contextual independence in probabilistic inference.
Journal of Artiﬁcial Intelligence Research 5, 301–328.
Zhang, N., R. Qi, and D. Poole (1993). Incremental computation of the value of perfect information
in stepwise-decomposable inﬂuence diagrams.
In Proc. 9th Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), pp. 400–407.
Zhang, N. L. (2004). Hierarchical latent class models for cluster analysis. Journal of Machine
Learning Research 5, 697–723.
Zoeter, O. and T. Heskes (2006). Deterministic approximate inference techniques for conditionally
Gaussian state space models. Statistical Computing 16, 279–292.
Zweig, G. and S. J. Russell (1998). Speech recognition with dynamic Bayesian networks. In Proc.
14th Conference on Artiﬁcial Intelligence (AAAI), pp. 173–180.


Notation Index
|A| — Cardinality of the set A, 20
φ1 × φ2 — Factor product, 107
γ1
L γ2 — Joint factor combination, 1104
p(Z) L g(Z) — Marginal of g(Z) based on
p(Z), 631
P
Y φ — Factor marginalization, 297
X ⇌Y
— Bi-directional edge, 34
X →Y
— Directed edge, 34
X—Y
— Undirected edge, 34
X ↔Y
— Non-ancestor edge (PAGs), 1049
X◦→Y
— Ancestor edge (PAGs), 1049
⟨x, y⟩— Inner product of vectors x and y, 262
||P −Q||1 — L1 distance, 1143
||P −Q||2 — L2 distance, 1143
||P −Q||∞— L∞distance, 1143
(X ⊥Y ) — Independence of random
variables, 24
(X ⊥Y | Z) — Conditional independence of
random variables, 24
(X ⊥c Y | Z, c) — Context-speciﬁc
independence, 162
11{·} — Indicator function, 32
A(x →x′) — Acceptance probability, 517
ℵ— Template attributes, 214
α(A) — The argument signature of attribute
A, 213
AncestorsX — Ancestors of X (in graph), 36
argmax, 26
A — A template attribute, 213
Beta(α1, α0) — Beta distribution, 735
βi — Belief potential, 352
BI[σ] — Induced Bayesian network, 1093
B — Bayesian network, 62
B0 — Initial Bayesian network (DBN), 204
B→— Transition Bayesian network (DBN), 204
BZ=z — Mutilated Bayesian network, 499
BoundaryX — Boundary around X (in
graph), 34
C (K, h, g) — Canonical form, 609
C (X; K, h, g) — Canonical form, 609
C[v] — Choices, 1085
ChX — Children of X (in graph), 34
Ci — Clique, 346
x ∼c — Compatability of values , 20
cont(γ) — Joint factor contraction, 1104
CCov[X; Y ] — Covariance of X and Y , 248
D — A subclique, 104
∆— Discrete variables (hybrid models), 605
d — Value of a subclique, 104
D+ — Complete data, 871
D — Empirical samples (data), 698
D — Sampled data, 489
D∗— Complete data, 912
D — Decisions, 1089
DescendantsX — Descendants of X (in
graph), 36
˜δi→j — Approximate sum-product message,
435
δi→j — Sum-product message, 352
Dim[G] — Dimension of a graph, 801
Dirichlet(α1, . . . , αK) — Dirichlet
distribution, 738
ID(P||Q) — Relative entropy, 1141
IDvar(P; Q) — Variational distance, 1143
Down∗(r) — Downward closure, 422
Down+(r) — Extended downward closure,
422
Down(r) — Downward regions, 422
do(Z := z), do(z) — Intervention, 1010

1212
NOTATION INDEX
d-sepG(X; Y | Z) — d-separation, 71
E — Edges in MRF, 127
EU[D[a]] — Expected utility, 1061
EU[I[σ]] — Expected utility of σ, 1093
ˆIED(f) — Empirical expectation, 490
IED[f] — Empirical expectation, 700
IEP [X] — Expectation (mean) of X, 31
IEP [X | y] — Conditional expectation, 32
IEX∼P [·] — Expectation when X ∼P, 387
f(D) — A feature, 124
F[ ˜P, Q] — Energy functional, 385, 881
˜F[ ˜PΦ, Q] — Region Free Energy functional,
420
˜F[ ˜PΦ, Q] — Factored energy functional, 386
FamScore(Xi | PaXi : D) — Family score,
805
F — Feature set, 125
F — Factor graph, 123
G — Directed graph, 34
GG — Partial ancestral graph, 1049
Γ — Continuous variables (hybrid models), 605
γ — Template assignment, 215
Gamma(α, β) — Gamma distribution, 900
Γ(x) — Gamma function, 736
H — Missing data, 859
H — Undirected graph, 34
IHP (X) — Entropy, 1138
IHP (X | Y ) — Conditional entropy, 1139
˜IH
κ
Q(X) — Weighted approximate entropy, 415
I — Inﬂuence diagram, 1090
I(G) — Markov independencies of G, 72
Iℓ(G) — Local Markov independencies of G,
57
I(P) — The independencies satisﬁed by P,
60
IIP (X; Y ) — Mutual infromation, 1140
InterfaceH(X; Y ) — Y -interface of X, 464
J
— Lagrangian, 1168
J — Precision matrix, 248
K — Partially directed graph, 34
K+[X] — Upward closed subgraph, 35
κ — Object skeleton (template models), 214
κr — Counting number of region r, 415
Ki — Member of a chain, 37
K[X] — Induced subgraph, 35
ℓPL(θ : D) — Pseudolikelihood, 970
L(θ : D) — Likelihood function, 721
Local[U] — Local polytope, 412
ℓ(ˆθG : D) — Maximum likelihood value, 791
ℓ(θ : D) — Log-likelihood function, 719
ℓY |X(θ : D) — Conditional log-likelihood
function, 951
loss(ξ : M) — Loss function, 699
M∗— Model that generated the data, 698
M-project-distri,j — M-projection, 436
M[x] — Counts of event x in data, 724
Marg[U] — Marginal polytope, 411
margW (γ) — Joint factor marginalization, 1104
MaxMargf(x) — Max marginal of f, 553
M[G] — Moralization of G, 134
M — A model, 699
¯
Mθ[x] — Expected counts, 871
˜
M — Learned/estimated model, 698
N
 µ; σ2
— A Gaussian distribution, 28
N
 X | µ; σ2
— Gaussian distribution over
X, 616
NbX — Neighbors of X (in graph), 34
NonDescendantsX — Non-descendants of X
(in graph), 36
NP, 1151
O — Outcome space, 1060
O(f(·)) — “Big O” of f, 1148
Oκ[Q] — Objects in κ (template models), 214
P, 1151
P(X | Y ) — Conditional distribution, 22
P(x), P(x, y) — Shorthand for P(X = x),
P(X = x, Y = y), 21
P ∗— Distribution that generated the data, 698
P |= . . . — P satisﬁes . . ., 23
PaX — Parents of X (in graph), 34
paX — Value of PaX, 157
PaG
Xi — Parents of Xi in G, 57
ˆPD(A) — Empirical distribution, 703
ˆPD(x) — Empirical distribution, 490
θ — Parameters, 262, 720
ˆθ — MLE parameters, 726
φ — A factor (Markov network), 104

NOTATION INDEX
1213
φ[U = u] — Factor reduction, 110
π — Lottery, 1060
π(X) — Stationary probability, 509
˜PΦ(X) — Unnormalized measure deﬁned by
Φ, 345
ψi(Ci) — Initial potential, 349
˜P — Learned/estimated distribution, 698
Q — Approximating distribution, 383
Q — Template classes, 214
R — Region graph, 419
IR — Real numbers, 27
ρ — A rule, 166
R — Rule set, 168
S — Event space, 15
σ — Std of a Gaussian distribution, 28
σ — Strategy, 1092
σ(t)(·) — Belief state, 652
Scope[φ] — Scope of a factor, 104
scoreB(G : D) — Bayesian score, 795
scoreBIC(G : D) — BIC score, 802
scoreCS(G : D) — Cheeseman-Stutz score,
913
scoreL(G : D) — Likelihood score, 791
scoreL1(θ : D) — L1 score, 988
scoreLaplace(G : D) — Laplace score, 910
scoreMAP(θ : D) — MAP score, 898
sepH(X; Y | Z) — Separation in H, 114
sigmoid(x) — Sigmoid function, 145
Si,j — Sepset, 140, 346
succ(v, c) — Successor (decision trees), 1085
T
— Clique tree, 140, 347
Υ — Template clique tree, 656
T
— Decision tree, 1085
t(θ) — Natural parameters function, 261
τ(ξ) — Suﬃcient statistics function, 261, 721
Θ — Parameter space, 261, 720
T (x →x′) — Transition probability, 507
U — Cluster graph, 346
U — Response variables, 1029
µ — Mean of a Gaussian distribution, 28
U(o) — Utility function, 1060
µi,j — Sepset beliefs, 358
Unif[a, b] — Uniform distribution on [a, b], 28
Up∗(r) — Upward closure, 422
Up(r) — Upward regions, 422
U — Utility variables, 1090
U X — Response variable, 1029
Val(X) — Possible values of X, 20
VVarP [X] — Variance of X, 33
VPII(D | X) — Value of perfect information,
1122
νr, νi, νr,i — Convex counting numbers, 416
W <(i,j), 348
X — The set of all variables in the domain, 21
ξ — An assignment to X, 79
X, Y, Z — Random variables, 20
X, Y , Z — Random variable sets, 20
x, y, z — Values of random variable sets, 20
x0, x1 — False/True values of X, 20
x⟨Y ⟩— Assignment in x to variables in Y ,
21
x[m]x[m] — m’th data instance (i.i.d.
samples), 698
xi — The i’th value of X, 20
Xκ[A] — Ground random variables, 214
ξ[m] — m’th data instance (i.i.d. samples), 488
ξmap — MAP assignment, 552
X(t) — X at time t, 200
X(t1:t2) — X in the interval [t1, t2], 200
X ∼. . .
— X is distributed according to . . .,
28
Z — Partition function, 105


Subject Index
2-TBN, 202
3-SAT, 288, 1151
abduction, 1134
action, 1061
joint, 1117
active learning, 1055
activity recognition, 952
Algorithm
Alpha-Expand, 593, 593
Alpha-Expansion, 593
BU-Message, 367, 367, 368, 440, 441
BU-message, 400
Beam-Search, 1158
Branch-and-Bound, 1161, 1161
Build-Minimal-I-Map, 80, 80, 142, 786
Build-PDAG, 89, 90–92, 786, 787, 790, 839,
843, 1042, 1043
Build-PMAP-Skeleton, 787
Build-PMap-Skeleton, 85, 86, 89, 90, 101, 787,
980, 1005, 1051
Build-Saturated-Region-Graph, 423
CGraph-BU-Calibrate, 398, 400, 413
CGraph-SP-Calibrate, 397, 413, 428
CLG-M-Project-Distr, 622, 628
CSI-sep, 173
CTree-BU-Calibrate, 367, 398, 628
CTree-BU-calibrate, 391
CTree-Filter-DBN, 657
CTree-Query, 371
CTree-SP-Calibrate, 357, 364, 365, 368, 398,
413, 436
CTree-SP-Upward, 353, 378, 612
CTree-SP-calibrate, 388, 413
Compute-ESS, 873, 873, 938
Compute-Gradient, 867, 867
Cond-Prob-VE, 304, 317
Conditioning, 604
Conjugate-Gradient-Ascent, 1167
Convex-BP-Msg, 418
Cross-Validation, 707
DP-Merge-Split-Proposal, 942
Data-Dependent-LW, 502, 502, 504
EP-Message, 440, 441, 443, 628
Estimate-Parameters, 922, 941
Evaluate, 707, 707
Expectation-Maximization, 873, 922
Factor-Product, 359
Factored-Project, 434, 435
Fibonacci, 1150, 1150
Find-Immoralities, 89
Forward-Sample, 489
Generalized-MP-BP, 573
Generalized-VE-for-IDs, 1105, 1106, 1107
Gibbs-Sample, 506
Gradient-Ascent, 1164
Greedy-Local-Search, 815, 1155, 1156
Greedy-MN-Structure-Search, 986, 990, 992
Greedy-Ordering, 314, 340
Holdout-Test, 707
Incremental-E-Step, 939, 939
Incremental-EM, 939
Initialize-CGraph, 397, 397, 573
Initialize-CTree, 367, 367
Initialize-Cliques, 353, 353, 357
Iterated-Optimization-for-IDs, 1116, 1116, 1131
K-Best, 1158
LW-2TBN, 666, 666, 670
LW-DBN, 666
LW-Sample, 493, 493, 502
LearnProc, 706, 707
LegalOp, 1157, 1157, 1158
M-Project-Distr, 443, 621

1216
SUBJECT INDEX
MCMC-Sample, 509
MEU-for-Decision-Trees, 1088, 1098
Mark-Immoralities, 86, 87, 89, 102, 787
Max-Cardinality, 312, 312, 313
Max-Message, 562
Max-Product-Eliminate-Var, 557, 557
Max-Product-VE, 557
Max-Weight-Spanning-Tree, 1147
Mean-Field, 455, 455, 459
MinCut-MAP, 591, 593
MinCut, 591
Msg-Truncated-1-Norm, 603
Parameter-Optimize, 986
Particle-Filter-DBN, 670
Perturb, 817, 818
Proposal-Distribution, 941
Reachable, 75, 76, 102
Rule-Split, 332, 332, 333
Rule-Sum-Product-Eliminate-Var, 333, 601
SP-Message, 353, 353, 357, 368, 378, 397, 397,
407, 437, 567, 612
Search-with-Data-Perturbation, 817
Search-with-Restarts, 1159
Search, 817, 1159
Structural-EM, 922
Structure-Learn, 922
Sum-Product-Conditioning, 317
Sum-Product-Eliminate-Var, 298, 298, 306,
347, 611
Sum-Product-VE, 298, 299, 304, 313, 331, 371,
611
Tabu-Structure-Search, 1157
Topological-Sort, 1146
Traceback-MAP, 557, 557, 558, 561, 601
Train-And-Test, 707, 707
alignment, see correspondence
alpha-beta swap, 592, 602
alpha-expansion, 592
ancestor, 36
argument, 213
factor, 216
feature, 229
signature, 213, 223
parent, 221, 223
assignment
local optimality, 566–567, 569
MAP, 26, 967
strong local maximum, 570–572, 602
attribute, 213
object-valued, 234
average causal eﬀect, 1032
back-door
criterion, 1020–1021
trail, 1020
backward induction, 1098
decision tree, 1087
bag of words, 766
barren node, 98, 136
basin ﬂooding, 816, 1156
Bayes’ rule, 18
BayesBall, 94
Bayesian classiﬁer, 727
Bayesian estimation, 735, 739, 752, 781, 782, 824
Bayesian networks, 741–750
BDE prior, see BDe prior
BGe prior, see BGe prior
Dirichlet prior, 739, 740
Gaussian, 779–780
incomplete data, 898–908, 1052
MCMC, 899–904
variational, see variational Bayes
nonparametric, 730–731, 928–930
shared parameters, 762–763
Bayesian model averaging, 785, 824–832, 928,
1043
computational complexity, 827
MCMC, 829–832
Bayesian network, 5, 62
conditional, see conditional Bayesian network
gradient, 339, 483
structure, 57
Bayesian score, 983
BDe prior, 749, 806, 835, 844, 848
shared parameters, 780
beam search, 890
belief propagation
asynchronous, 408, 417
clique tree, 355–358
cluster graph, 396–399
convergence, 392, 401–403, 407–411, 417–419
convergence point, 412–413, 479
stability, 408, 413
convex, 416–419
damping, 408, 479
EM, 897
frustrated loop, 568
Gaussian, see Gaussian, belief propagation

SUBJECT INDEX
1217
local maxima, 409
loopy, 393, 405, 962
Markov network learning, 963–965
max-product, 562, 593, 602
convergence, 602
message scheduling, 408
nonparametric, 646, 649
operator, 402
region graph, 423–428
residual, 408
sum-product, 356
synchronous, 402, 408
tree reparameterization, 408
tree-CPDs, 478
tree-reweighted, 418, 576, 593, 968
belief state, 652
prior, 653
projection, 663
reduced, 656
beliefs, 358
Beta distribution, 735–737
BGe prior, 840
bias, 710
bias-variance trade-oﬀ, 704
bigram model, 764
bipartite matching, 534
BK algorithm, 690
BN2O network, 177, 197
Boltzmann distribution, 126
Bonferroni correction, 843
bootstrap, 1046
bow pattern, 1024
BUGS system, 525–526, 543
c-separation, 150, 156
CAI-map, 1076
minimal, 1077
perfect, 1077
calibrated, 358
CAMEL, 964, 1004
canonical form, 609, 649
division, 610
marginalization, 610
well-deﬁned, 611
operations, 610–611
product, 610
reduction, 611
vacuous, 610
canonical table, 618
marginalization, 619–621
weak, 620
operations, 618–621
causal
eﬀect, 1014
independence, 1056
mechanism, 175, 1014
model, 1014–1030
augmented, 1017–1020, 1022–1024
functional, 1029–1030
identiﬁability, 1042
causal Markov assumption, 1041
causal model learning, 1040–1053
Bayesian model averaging, 1043
constraint-based, 1042–1043
functional causal model, 1051–1053, 1056
interventional data, 1044–1047
latent variables, 1048–1051
constraint-based, 1048–1051, 1056
score-based, 1048
cellular network reconstruction, 1046–1047
central limit theorem, 1144
Markov chain, 521
certainty equivalent, 1066
chain component, 37, 148
chain graph, 37, 148
c-separation, see c-separation
distribution, 149
model, 148
chain rule
Bayesian networks, 54, 62
conditional probabilities, 18, 47
entropy, 1139
mutual information, see mutual information,
chain rule
relative entropy, 1142
chance variable, 1089
Chebyshev’s inequality, 33
Cheeseman-Stutz score, see marginal likelihood
approximation, Cheeseman-Stutz
Chernoﬀbound, 491, 501, 1145
χ2
distribution, 790
statistic, 788, 843, 848
child, 34
Chinese restaurant process, 930
chordal graph, 311
clarity test, 64
class, 213

1218
SUBJECT INDEX
classiﬁcation, 50, 727
collective, 952
error, 701
task, 700
text, 766
CLG, see Gaussian, conditional linear
CLG network, 190, 645, 684
computational complexity, 615–617
clique, 35
clique potentials, 109
clique tree, 140, 346–348, 481, 549, 673, 937
algorithm
correctness, 353–355
beliefs, 352, 357, 365
calibrated, 355–358, 384
CLG network, 626–630
clique, 348
downstream, 347
initial potential, 351
ready, 350, 356
upstream, 347
computational complexity, 358, 374
construction, 372–376, 379, 380
downward pass, 356, 655
family preservation, see cluster graph, family
preservation
incremental update, 369–370, 379
inference as optimization, 387–390
inﬂuence diagram, 1109, 1117, 1131, 1132
invariant, 361–363, 368
max-product, 564, 568
max-calibrated, 563
max-product, 562–565
traceback, 566
measure, 361–364, 383–384, 564
message, 345
scheduling, 357
message passing, see message passing
multiple queries, 371–372
nested, 377
out-of-clique inference, 370–371, 379
reparameterization, 362
rule-based CPDs, 379
running intersection property, 347–348, 353
sampling, 544
sepset, 140
strong root, 627
structure changes, 378, 379
sum-product, 352
template, 656
upward pass, 356, 378, 654
cluster graph, 346, 396
Bayesian network, 478
beliefs, 396
low-temperature-limit, 583
Bethe, 405, 414, 415, 573
calibrated, 396–398, 412
construction, 404–411
family preservation, 346, 420
induced subgraph, 570
invariant, 399–400
max-calibrated, 583
message passing, see message passing
out-of-cluster inference, 481
residual, 401, 477
running intersection property, 396, 407
sepset, 346
template, 664
tree consistency, 401
clustering, 875
Bayesian, see naive Bayes, clustering, 875,
902–908, 915–916
collaborative ﬁltering, 823, 877
collapsed sampling, see Gibbs, collapsed, see
importance sampling, collapsed, see
MCMC, collapsed, 526–532, 645, 650
compression, 1137
computational complexity, 1147–1149
asymptotic, 1147
running time, 1148
theory, 1150
concentration phenomenon, 777
condensation, see ﬁlter, particle
conditional Bayesian network, 191
conditional covariance, 259
conditional expectation, 32, 451
conditional independence, see independence
conditional preference structure, 1072
conditional probability, 18
conditional probability distribution, see CPD
conditional probability table, see table-CPD
conditional random ﬁeld, 113, 143, 191, 197, 710,
950, 952
linear-chain, 146
skip-chain, 146
conditioning, 315–325
bounded, 540
computational complexity, 320–325

SUBJECT INDEX
1219
cutset, 318
incremental, 540
induced graph, 322
marginal MAP, 604
rule-based CPDs, 334
conﬁdence interval, 719
confounding factor, 1012–1014
constraint, 388
equality, 1168
expectation consistency, 446, 447
local polytope, see local polytope
marginal consistency, 384, 387, 416
region graph, 421
marginal polytope, see marginal polytope
mean ﬁeld, 455
constraint generation, 976, 1005
constraint propagation, 89
constraint satisfaction problem, 569
context-speciﬁc independence, see
independence, context-speciﬁc
contingency table, 152
contingent dependency model, 223
contraction, 402
contrastive
divergence, 974–975
objective, 970
convergence bound, 489, 771, 1145–1146
convergence rate, 888
convex optimization, 976
coordinate ascent, 881
coordination graph, 1117
correspondence, 165, 236, 532–536, 544, 550
correlated, 535
EM, 534
Metropolis Hastings, 534
mutual exclusion, 533–534
variable, 166, 533, 893
counterfactual
query, 1010, 1026–1027, 1034–1040
twinned network, 1035–1037
world, 1034
counterfactual twinned network, 1125
counting numbers, 415, 420, 573
convex, 416, 419, 574
CPD, 47, 53, 62
aggregator, 225, 245
conditional linear Gaussian, 190, 618
decomposition
causal independence, 325–329
context-speciﬁc independence, 341
deterministic, 158
encapsulated, 192
Gaussian, see Gaussian, linear
linear Gaussian, 187
logistic, 145, 179, 197, 225, 483
multinomial, 181, 970
multiplexer, 165
noisy-and, 196
noisy-max, 183, 196
noisy-or, 176, 196, 197, 225, 936, 1037
requisite, 100, 1018, 1112
rule-based, 168, 195, 601
inference, see variable elimination,
rule-based CPDs
table-CPD, 157, 725
tree-CPD, 164, 195, 196
CPT, see CPD, table
CRF, see conditional random ﬁeld
cross-validation, 706, 844, 960
CSI-separation, 173, 196
computational complexity, 196
cycle, 37
cyclic graphical model, 95
d-separation, 71
completeness, 72
soundness, 72
DAG, 37, 57
data
complete, 712
completion, 869, 881, 912, 921
incomplete, 712, 849
interventional, 1040, 1044, 1056
observability, 712
observational, 1040
weighted, 817, 870
data association, see correspondence, 165, 244,
532, 550, 680, 893, 940
data fragmentation, 726, 784
data imputation, 869
data perturbation, 816
data-driven approach, 6
decision diagram, 170
decision rule, 1091
deterministic, 1091
fully mixed, 1111
locally optimal, 1109, 1110
optimization, 1107, 1108, 1130

1220
SUBJECT INDEX
iterated, 1115–1117, 1131
local, 1111
decision theory, 1059, 1068
decision tree, 1085, 1096–1097
strategy, 1087
decision variable, 1017, 1089
decision-making situation, 1061
declarative representation, 1, 1133
deep belief networks, 1000
degree, 34
bounded, 992
density estimation, 699, 784
density function, 27–31
conditional, 31
joint, 29
dependency network, 96, 822, 823
descendant, 36
detailed balance, 515, 546
deterministic separation, 160
digamma function, 907
directed acyclic graph, see DAG
Dirichlet distribution, see BDe prior, 738,
746–750
mixture, 779
posterior, 738
sampling, 900
variational update, 906–907
Dirichlet process, 929–930, 941–942
discretization, 606
discriminative training, 709, 950, 997
distance measure, 1140–1143
distance metric, 1140, 1143
distribution, 16
Bernoulli, 20
conditional, 22
cumulative, 28
empirical, 703
Gamma, 765, 780, 900
Gaussian, see Gaussian, 720
joint, 3, 21
Laplacian, 959
marginal, 21
mixture, see Gaussian, mixture, 484, 713, 875,
915
multinomial, 20, 720
normal-Gamma, 751
Poisson, 283
positive, 25, 116
posterior, 3
prior, 47
support, 494
uniform, 28
duality, 957, 1171–1172
convex, 470
dynamic Bayesian network, 202–205, 837
fully persistent, 658
parameter estimation, 781
structure learning, 846
dynamic programming, 292–296, 337, 356, 371,
482, 596, 1149
dynamical system, see ﬁlter
continuous time Bayesian network, 242
Dynamic Bayesian network, see dynamic
Bayesian network
hidden Markov model, see hidden Markov
model
linear, 211
Markovian, 201
semi-Markov, 202, 243
semi-Markovian, 243
stationary, 202
switching linear, 212, 684
E-step, 872, 874, 907
variational, 896
edge
covered, 78, 100
covering, 78
directed, 34
ﬁll, 307, 340
inter-time-slice, 204
intra-time-slice, 204
reversal, 78, 99, 545, 673
spurious, 173
undirected, 34
EKF, see Kalman ﬁlter, extended
EM, 535, 907
accelerated, 892
approximate inference, 893–897
Bayesian network, 868–897
computational complexity, 891
table-CPD, 872–874
belief propagation, 897
clustering, 875–877
convergence, 887
practice, 885–887, 890–892
theory, 874–875, 877–884
dynamic Bayesian network, 937

SUBJECT INDEX
1221
exponential family, 874
hard assignment, 876, 884–885, 889, 937
incremental, 892, 938
initialization, 889–890
local maxima, 886, 888–890
log-linear model, 955–956
MAP, 898, 940
noisy-or, 936
overﬁtting, 891
single family, 937
tree-CPD, 936
variational, 895–897
empirical distribution
Gaussian, 722
endogenous variable, 1027
energy function, 124
canonical, 129
restricted, 592
submodular, 590, 595
truncation, 602
energy functional, 385, 450, 881–882, 905, 914,
940
convex, 416, 962
energy term, 385
entropy term, 385
factored, 386–387, 411
optimization, 411–414
generalized, 414–428
Gibbs distribution, 458
optimization, 459–468
temperature-weighted, 582–585
energy minimization, 553, 599
entanglement, 656–660
entropy, 477, 1138–1142
Bayesian network, 271
conditional, 1139
convex, 417
exponential family, 270
factored, 386, 964
Gaussian, 270
joint, 1139
Markov network, 270
relative, 1141
conditional, 1142
weighted approximate, 415
EP, see expectation propagation
equivalent sample size, 740
error
absolute, 290, 544
relative, 291, 491, 544
estimator, 1145
Bayesian, see Bayesian estimation
consistent, 769
MAP, see MAP estimation
maximum likelihood, see maximum likelihood
estimation
representation independence, 752–754
unbiased, 1145
variance, 495
event, 15
measurable, 16
evidence, 26
evidence retraction, 339
expectation
linearity of, 32
random variable, 31
expectation maximization, see EM
expectation propagation, 430, 441, 444, 664
and belief propagation, 482
convergence point, 447
Gaussian
message passing, 621
mixture, 621–626, 686–688
nonlinear, 630, 637–642
message passing, see message passing,
expectation propagation
expectation step, see E-step
Expectimax, 1087
expert system, 67
expert systems, 13
explaining away, see reasoning, intercausal, 55,
196
exponential family, 261, 442, 874, 879
Bayesian network, 268–269
Bernoulli, 265
composition, 266
CPD, 267
EM, 874
factor, 266
Gaussian, 263
invertible, 263, 278, 283
linear, 264, 757
linear Gaussian, 267
multinomial, 265
parameter estimation, 732
exponential time, 1148
factor, 5, 104, 296

1222
SUBJECT INDEX
division, 365
expected utility, 1108
generalized, 342, 1130
joint, 1103–1107, 1130, 1131
log-space, 360
marginalization, 297, 360, 378
maximization, 555
nonnegative, 104
operations, 358–361
stride, 358
product, 107, 359
reduction, 111, 303
scope, 104
set, 432
marginalization, 432
product, 432
factor graph, 123, 154, 418
factorization, 50
bayesian network, 62
factor graph, 123
Markov network, 109
faithful, 72, 786
faithfulness assumption, 1042
family score, 805
feature
indicator, 125
linear dependence, 132
log-linear model, 125
features, 50
ﬁltering, 652
assumed density, 664
bootstrap, 668
particle, 667–674, 680
collapsed, 674, 693, 694
posterior, 671
Rao-Blackwellized, 674
recursive, 654
state-observation model, 653–654
ﬁxed point
equations, 482
ﬁxed-point, 402
equations, 390, 412, 424, 447, 451, 458, 479
forest, 38
forward pass, 654
forward sampling, 488–492, 541
convergence bounds, 490–491
estimator, 490
sample size, 490, 544
forward-backward algorithm, 337, 655
free energy, 385
Bethe, 414
frequentist interpretation, 16
function
concave, 41
convex, 41
game theory, 1130
Gamma distribution, see distribution, Gamma
Gamma function, 735, 798
Gaussian, 28, 1144
Bayesian network, 251–254, 1084
belief propagation, 612–614
clique tree, 611–612
covariance matrix, 247
exponential family, 264
independencies, 250–251, 258
information matrix, 248
linearization, 631–637, 650
incremental, 640
mean vector, 247
mixture, 190, 616
collapsing, 620–621, 624–626, 685–688
pruning, 685
MRF, 254–257
diagonally dominant, 255
pairwise normalizable, 256, 614
walk-summable, 648
multivariate, 247–251
normalizable, 622–624, 639
standard, 28, 248
Gaussian processes, 778
general pseudo-Bayes, 685, 687
generalization, 704–708, 784
generalized linear model, 178
generative training, 709
genetic inheritance, 57–60
GES algorithm, 821
Gibbs distribution, 108
parameterization, see Markov network,
parameterization
reduced, 111
Gibbs sampling, 505–507, 512–515, 547
block Gibbs, 513
collapsed, 531, 549, 550, 1056
incomplete data, 901–904, 929, 940
continuous state, 644
incomplete data, 899–904
Markov chain, 512

SUBJECT INDEX
1223
regularity, 514
stationary distribution, 512, 546
goodness of ﬁt, 708, 839
GPB, see general pseudo-Bayes
gradient, 1162
ascent, 863, 1163–1166
Bayesian network, 867–868
conjugate, 1166
convergence, 887
L-BFGS, 950, 991
line search, 1164
Bayesian network, 863–866, 936–937
log-likelihood, 864
chain rule, 864
Gaussian, 937
hidden variable, 937
log-linear model, 948
partition function, 947–948
unstable, 962
grafting, 992
graph
acyclic, 37
chordal, 38, 155, 374
connected, 36
directed, 34
moralized
Bayesian network, 134
chain graph, 148
singly connected, 38
skeleton, 77
triangulated, 38
undirected, 34
undirected version, 34
graph cut, 588
ground
Bayesian network, 217, 221, 224
Gibbs distribution, 229
random variable, 215
guard, 223
Hammersley-Cliﬀord theorem, 116, 1077
Hessian, 1163
Bayesian network
incomplete data, 909
log-likelihood, 950
Markov network, 983
partition function, 947
hidden Markov model, 146, 203, 208, 952
coupled, 148, 204
duration, 244
factorial, 204, 482
hierarchical, 210, 244
mixed memory, 244
phylogenetic, 206, 483
segment, 244
hidden variable, 65, 713, 849, 925–932
cardinality, 928–930
model selection, 928
hierarchical, 931
information, 926–928
overlapping, 931
partition, 929
hierarchical Bayes, 765, 779
HMM, see hidden Markov model
Hoeﬀding bound, 490, 771, 1145
holdout testing, 705–708, 795
Hugin, 377
algorithm, see message passing, belief update
hybrid network, 186
hyperbolic tangent, 403
hyperparameter, 958
Beta, 735
Dirichlet prior, 738
hierarchical distribution, 765
hypothesis space, 702, 712, 718, 785
hypothesis testing, 787–790
decision rule, 788
deviance, 788
multiple hypotheses, 790, 843
null hypothesis, 787
p-value, 789, 843
I-equivalence, 76, 784, 815
class, 76, 815, 821
I-map, 60
Markov network
construction, 120–122
minimal, 79, 786
construction, 79–81
I-projection, 274, 282, 383
Gaussian, 274
ICI, see independence, causal
ICU-Alarm, 749, 796, 802, 820, 830, 885
identiﬁability, 702, 861
Bayesian network structure, 784, 841
hidden variable, 861
incomplete data, 860–862
intervention query, 1055

1224
SUBJECT INDEX
local, 862
identity resolution, see correspondence, 165, 532
IID, 698, 1144
image denoising, 112
image registration, 532
image segmentation, 113, 478
immorality, 78
potential, 86
importance sampling, 494–505, 545, 547, 966,
1004
adaptive, 542
annealed, 543, 548
backward, 505
Bayesian network, 498–505
collapsed, 527–530
normalized, 496–498, 503, 545
bias, 497
estimator, 497
variance, 497
sample size
eﬀective, 498
sequential, 667–672
variance, 671
unnormalized, 494–496, 502
bias, 495
estimator, 495
variance, 495
incremental update, 369–370
indegree, 34, 804
bounded, 85, 786, 787, 811, 814, 826, 841–842
independence, 23–25
causal, 182, 196, 197
symmetric, 183
conditional
continuous, 31
events, 24
random variables, 24
context-speciﬁc, 162, 171–175, 196, 1127
events, 23
marginal, 24
persistent, 657
properties, 24–25, 154
contraction, 25
decomposition, 25
intersection, 25
strong union, 154
symmetry, 24
transitivity, 154
weak union, 25
test, 783, 786–790, 843, 848
independence test
Markov network, 979–981
independencies
Bayesian network, 56–57
global, 72
local, 57
chain graph
global, 151
local, 150
pairwise, 150
distribution, 60
Gaussian, see Gaussian, independencies
inclusion, 94
Markov network, 117–120
global, 115
local, 118, 120–122, 979
pairwise, 118, 120–122, 154, 979
indicator function, 32
induced width, 310
inference, 5
inferential loss, 1080
inﬂuence diagram, 93, 1089–1090
expected utility, 1093–1094
limited memory, 1093
reduction, 1120, 1132
inﬂuence graph, 658
information edge, 1090
irrelevant, 1119–1121
information form, 248
information state, 1091
insurance premium, 1066
interface, 464
interface variable, 202
intervention, 1092, 1112
ideal, 1010
query, 1010, 1015
identiﬁability, 1017–1026, 1031–1034
simpliﬁcation, 1018–1026, 1055
Ising model, 126, 127
iterated conditional modes, 599
iterative proportional ﬁtting, 998
iterative proportional scaling, 998, 1002
IX -equivalence, 1049
Jensen inequality, 41
join tree, see clique tree
junction tree, see clique tree

SUBJECT INDEX
1225
k-means, 877
K2 prior, 806, 844
Kalman ﬁlter, 211, 259, 676–684
extended, 212, 631, 678
information form, 677
observation update, 677
state transition update, 676
unscented, 635, 678
kernel density, 730
KL-divergence, see entropy, relative
knowledge discovery, 701, 783
knowledge-based model construction, 241, 242,
651
label bias problem, 953
Lagrange multipliers, 388, 868, 1168–1172
language model, 209
Laplace’s correction, 735
latent Dirichlet allocation, 769
latent variable, 1012
latent variable network, 1048
Lauritzen’s algorithm, 626
Lauritzen-Spiegelhalter algorithm, see message
passing, belief update
leaf, 38
leak probability, 176
lifted inference, 689
likelihood, 699
Bayesian network, 723–726
conditional, 701, 725, 950
decomposability, 723–726, 857
global, 725, 859
local, 726, 859
shared parameters, 755
function, 719, 721
incomplete data, 856–860
computational complexity, 860
local, 725
log-likelihood, 699
log-linear model, 944–949
incomplete data, 954–955
likelihood score, 805
likelihood weighting, 493–494, 541
data dependent, 502
expected sample size, 502
DBN, 665–667
estimator, 493, 500
normalized, 503, 504
ratio, 502, 504
likelihood, marginal, see marginal likelihood
linear program, 579
integer, 577
optimization variables, 577
relaxation, 576, 579
local consistency polytope, 412, 477, 580, 964
local maximum, 1156
local probability model, 53
log-likelihood, see likelihood, 699, 719
expected, 699, 878–881
log-linear model, 125, 155, 946
shared parameters, 228, 965, 1002
log-odds, 179
logical variable, 213
logit, see sigmoid
loop, 38
loopy belief propagation, see belief propagation,
loopy
loss function, 699
0/1 loss, 701
Hamming loss, 701, 978
log-loss, 699
lottery, 1059, 1060
compound, 1062
preference, 1060
lower bound, 386, 412, 469–473, 897
variational, see variational, lower bound
M-projection, 274, 277–283, 383, 433, 443, 620,
621, 624, 632, 774, 1170–1171
Bayesian network, 284
chain network, 280, 284
exponential family, 278
factor set, 433–436
Gaussian, 274, 279, 283
M-step, 873, 874, 907
MAP, see query, marginal MAP, 26, 574
assignment, 534, 537, 1155
computational complexity, 551–552
integer program, 577–579
k-best, 559, 601–603, 977, 1005
linear program, 579–581
marginal, 27, 554, 559–561, 595
MAP estimation, 751, 753, 898, 983
Beta, 754
log-linear model, 958–961, 984–985
block L1 prior, 984
Gaussian prior, see regularization, L2
hyperbolic prior, 1003

1226
SUBJECT INDEX
L1 prior, 988–992
Laplacian prior, see regularization, L1
margin-based estimation, 976–978
marginal independence, see independence
marginal likelihood, 738, 744, 795–799, 826
approximation, 909–916
BIC, 911–912, 915
candidate, 913–915
Cheeseman-Stutz, 912–913, 915
Laplace, 909–911, 915
variational, 914
marginal MAP, see MAP, marginal, 685
computational complexity, 552, 560–561
marginal polytope, 411, 477, 580
marginalization, see factor, marginalization
strong, 627
weak, 621–630
Markov assumption
dynamical system, 201
Markov blanket, 512
Bayesian network, 135, 155
distribution, 121
undirected graph, 118, 980
Markov chain, 507
conductance, 519
ergodic, 510
homogeneous, 507
kernel, 511
mixing, 515, 519–520, 543, 831, 832
empirical, 522–523
multi-kernel, 511, 546
periodic, 510
reducible, 510, 546
regular, 510, 546
reversible, 515
temperature, 524
transition model, 507
Markov chain Monte carlo, see MCMC
Markov decision process, 1129
Markov inequality, 40
Markov model, see hidden Markov model
Markov network, 5, 103–133
decomposition, 155
pairwise, 110, 404, 478
parameterization, 106–109
canonical, 129–132, 154
redundancy, 132–133, 948
tree, 195
reduced, 111
utility, 1076
Markov random ﬁeld, see Markov network, 105
labeling, 127, 547
metric, 128, 588–595
semimetric, 128, 588–595
max-calibrated, 563, 574
Max-Clique Problem, 1152
max-margin, 1005
max-marginal, 553, 562, 563
decoding, 553, 556–559, 565–567
pseudo, see pseudo-max-marginal
ratio optimality, 566
unambiguous, 553
max-product, 552, 582
max-sum, 553, 577, 1117
max-sum-product, 559
maximization step, see M-step
maximum entropy, 956–958
approximate, 964–965
distribution, 1169
expectation constraints, 956
maximum entropy Markov model, 952
maximum expected utility, see MEU
maximum likelihood estimation, 719, 722
Bayesian network, 723–732
conditional random ﬁeld, 950–953
consistent, 949, 1002
Gaussian, 722, 778
incomplete data
computational complexity, 887
linear Gaussian, 728–730
log-linear model, 949–950
dual, 956–958, 1002
using belief propagation, 963–965
using MCMC, 966–967
multinomial, 722
plate models, 757–760
shared parameters, 756–761
table-CPD, 725
maximum spanning tree, 374
MCMC, see Markov chain, 507, 644, 673, 966,
975, 1159
burn-in time, 519
collapsed, 531–532, 831
estimator, 521
variance, 521–522
Gibbs sampling, see Gibbs sampling
Metropolis-Hastings, see Metropolis-Hastings
network structures, 829–831

SUBJECT INDEX
1227
reversible jump, 935
sampling, 508, 520–523
autocovariance, 521, 522
variable ordering, 831–832
mean ﬁeld, 449–456, 895, 906
algorithm, 454–456
cluster, 467
convergence point, 451–453
energy, 449–450
mean prediction, 740
medical diagnosis, 51, 67–68, 177, 183, 197, 1124
message decoding, 393
turbocode, 395
message passing
belief-update, 364–368
CLG network, 624–626
max-product, 563
clique tree, 351–352
DBN, 654–655
expectation propagation
belief-update, 440–442
exponential family, 442–445
Gaussian, 641
sum-product, 437–439
max-product, 563, 603
counting numbers, 573
order-constrained, 623, 639
region graph, 425–428, 480–481
sum-product, 352, 368, 397, 413
generalized, 418
sum-product-divide, 365–368
meta-network, 742, 858, 899
global decomposition, 743
local decomposition, 746
shared parameters, 763
metric, 127
Metropolis-Hastings, 516–518, 542, 547, 832, 942,
1159
acceptance probability, 517
collapsed
incomplete data, 940
continuous state, 644
random walk, 645, 903
MEU
principle, 1061
strategy
decision rule, 1107, 1108
decision tree, 1098–1100
inﬂuence diagram, 1094, 1115, 1131
value, 1094, 1119
micromort, 1070, 1081
min-ﬁll, 314
weighted, 314
min-neighbors, 314
min-weight, 314
minimax risk, 1083
minimum description length, 802
missing at random, 854, 936
missing completely at random, 853
MLE, see maximum likelihood estimation
model dimension, 801, 983
model selection, 785, 978
module network, 846
moment matching, 278, 949
Monte Carlo localization, see ﬁlter, particle, see
robot, localization, 680, 691
moral graph, 135
MPE, see query, MAP
MRF, see Markov random ﬁeld
multiconditional training, 1004
multinet, 170, 195
mutilated network, 499, 1014
interventional, 1014–1017, 1044
proposal distribution, 499–500, 530
mutual information, 789, 792, 848, 1140
chain rule, 41
conditional, 41
naive Bayes, 49, 727
Bernoulli, 767
clustering, 875, 877, 915
multinomial, 767
tree augmented, 842
naive Markov, 144, 197, 710
natural bounds, 1033
negative deﬁnite, 1163
neighbor, 34
network polynomial, 304, 339, 378
noise parameter, 176
noisy-or model, see CPD, noisy-or
normal-Gamma distribution, 780
NP-hardness, 1150–1153
CSI-separation, 196
elimination ordering, 310
inference
approximate, 291–292
exact, 288–290
MAP, 551

1228
SUBJECT INDEX
polytree CLG, 617
reduction, 1152
structure learning
directed, 811, 841
undirected, 1000
triangulation, 313
numerical integration, 633
exact monomials, 634–637
precision, 635
Gaussian quadrature, 633–634
precision, 633
integration rule, 633, 634
precision, 633
object, 213
object skeleton, 214, 229
object uncertainty, 233
object-oriented Bayesian networks, 192
objective function, 702, 718, 1154
concave
over the constraints, 417
observability
model, 851
variable, 851
observation model, 207
observed variable, 24, 71, 114, 142
optimization
constrained, 381, 1167–1171
optimization problem, 1154
outcome, 1061, 1090
anchor, 1064
atomic, 22
space, 15
canonical, 22
overﬁtting, 704–708, 726, 769, 794, 801, 886
P-map, see perfect map
PAC-bound, 709, 770
Bayesian network, 773–776
log-linear model, 991, 1000–1001
multinomial, 771–773
parameter
sharing, see shared parameters
space, 720
parameter distribution
Bernoulli, see Beta distribution
conjugate prior, 739
Gaussian, see normal Gamma distribution
multinomial, see Dirichlet distribution
prior, 733
parameter independence, 799, 834, 857
global, 742, 805–806, 837
local, 747
parameter modularity, 805
CPD-tree, 835
parameter posterior, 734, 738
parameter prior, see parameter distribution,
prior, 738
Bayesian network, 748, 805–806
conjugate, 737
log-linear model
conjugate, 961
L1, 959
L2, 958
parameters, 46, 720
independent, 46, 259, 801
incomplete data, 912
legal, 262
natural, 263
function, 262
space, 264
space, 262
parametric family, 261, 720
parametric model, 720
parent, 34
partial ancestral graph, 1049–1051
partial correlation coeﬃcient, 259
partially directed acyclic graph, see PDAG, 148
particle, 487
collapsed, 487, 526, 543, 674
data completion, 903–904, 940
parameter, 901–903
deterministic, 536–540, 675
deterministic search, 549
weighted, 493
particle ﬁltering
smoothing, 692
partition function, 105, 108, 262, 543
approximate, 966
convex, 947
lower bound, 386, 470
upper bound, 1004–1005
Pascal’s wager, 1082
path, 36
active, 114
Pathﬁnder, 67
PDAG, 37, 843
boundary, 34, 149

SUBJECT INDEX
1229
class, 87, 786, 821, 1042
PDF, see probability density function
peeling, 337
perfect map, 81, 787
construction, 83–92
persistence
edge, 204, 658
variable, 204
piecewise training, 1003, 1004
plate, 217
intersection, 218
nested, 218
plate model, 216–222, 837
text, 767
plateau, 1156
point estimate, 737
polynomial time, 1148
polytree, 38, 313, 340, 552, 617
positive deﬁnite, 248
positive semi-deﬁnite, 248
posterior, 26
potential
edge, 110
node, 110
Potts model, 127
prediction, 652
preference independence, 1071–1072
prenatal diagnosis, 1079, 1094
prequential analysis, 796
prior, 19
improper, 740
probabilistic context-free grammar, 243
probabilistic ﬁnite-state automaton, 209
probabilistic relational model, 223, 837
parameter estimation, 781
probability distribution, see distribution
probability query, 26, 287
approximate, 290–292
computational complexity, 288–292
lower bound, 537
reasoning, 54–55
causal, 54
evidential, 55
intercausal, 55
probability theory, 2
probably approximately correct, 709
projection, see M-projection; I-projection
proposal distribution, 644, 1160
importance sampling, 494, 498, 528–530, 542
MCMC, see Metropolis Hastings, 516
protein structure prediction, 968–969
pseudo-counts, 740
pseudo-marginal, 412, 580
pseudo-max-marginal, 562, 568
decoding, 568–572
pseudo-moment matching, 963, 1004
pseudolikelihood, 970–974
consistent, 972
generalized, 973
QALY, 1070
quadratic program, 976
qualitative probabilistic networks, 94
query variable, 26
random variable, 3, 20–23
Rao-Blackwellization, see collapsed sampling
rationality
human, 1067–1068
postulates, 1062–1064, 1084
recall
edge, 1092
imperfect, 1093, 1109, 1116, 1119
perfect, 1092, 1098, 1131
record matching, see correspondence
redundant
feature, 133
parameterization, 263
reference class, 17
region graph, 419–428, 572
belief propagation, see belief propagtion,
region graph
calibrated, 421
construction, 421–423
saturated, 422
regularization, 705, 751
block-L1, 984
L1, 959, 984
L2, 958, 984
log-linear model, 958–961
rejection sampling, 491, 643
relation, 213
relational Markov network, 229, 1002
relational skeleton, 224
relational uncertainty, 225, 233
relative entropy, 771
Bayesian network, 273
exponential family, 272

1230
SUBJECT INDEX
relevance graph, 1114–1115, 1131
renormalization, 287, 339
reparameterization, 574, 868
max-product
clique tree, 564–565
cluster graph, 568
counting numbers, 574
sum-product
clique tree, 362
cluster graph, 399
response variable, 1028–1030, 1035
constraints, 1031–1033
risk, 700, 1066
averse, 1066
empirical, 700
excess, 709, 774
neutral, 1067
seeking, 1067
RoboSoccer, 1117
robot
localization, 187, 678–684
mapping, 681, 892–893, 938
SLAM, 681, 694
RP, 1153
rule, 166
product, 330
reduced, 172
scope, 166
split, 331
sum, 330
running intersection property, see clique tree,
running intersection property
s-reachable, 1112–1114, 1131
Saint Petersburg paradox, 1065
sample complexity, 709
sample size, 501
search, 675
assignment, 536–540, 595–597
beam, 595, 675, 685, 693, 1158
branch-and-bound, 595, 603, 604, 1160–1161
hill-climbing
ﬁrst-ascent, 815, 1155
greedy, 1155
local, 595, 812, 814, 985, 1154–1160
operators, 596, 1154
random restart, 1159
randomization, 1158–1160
space, 595, 812, 1154
state, 1154
systematic, 595
tabu, 596, 816, 1156
selection bias, 1013
selector variable, 165
semimetric, 128
sensitivity analysis, 67, 95, 305, 339
separation, 115
completeness, 116–117
CA-independence, 1077
soundness, 115–116
sepset, see clique tree, sepset, see cluster graph,
sepset
sequence labeling, 952
shared parameters, 754, 780–781
global, 755–760
local, 760–761
#P, 1153
shrinkage, 243, 764
sigmoid, 145, 179
similarity network, 95, 171
Simpson’s paradox, 1015–1016, 1021
simulated annealing, 524, 1159
smoothing, 652
computational complexity, 692
particle, 692
spanning forest, see spanning tree
spanning tree
maximum weight, 809, 1146, 1148
speech recognition, 209, 675
standard deviation, 33
standard gamble, 1069
state-observation model, 207
stationary distribution, 509–511
stationary point, 1162
stereo reconstruction, 113, 593
stick-breaking prior, 930
Stirling’s approximation, 843
strategic relevance, 1110–1115
strategy, 1087
complete, 1091
MEU, see MEU strategy
structural uncertainty, 232
structure discovery, 825
conﬁdence estimation, 825
network features, 825, 827–828
structure learning
constraint-based, 785–790, 1042
Markov network, 979–981, 1005

SUBJECT INDEX
1231
score-based, 785, 790–824
undirected model, 981–995
convergence, 990
global maximum, 989
hypothesis space, 981–982
L1 prior, 988–992
structure modularity, 804
structure score
Bayesian, 794–807, 843, 983–984
decomposable, 799–801, 805
BIC, 802, 843, 911, 983
consistent, 803, 822
decomposability, 808, 818–820, 917–919, 986
decomposable, 805
equivalence, 808
Laplace approximation, 983
likelihood, 791–794, 982–983
decomposable, 792
MAP, 984–985
L1, 984, 988–995
score equivalence, 807, 821, 844
structure prior, 804
tree-CPD, 834
template model
decomposable, 837
tree-CPD
decomposable, 834
structure search, 807–824, 1155
computational complexity, 809, 811, 814–815,
818–820
delta score, 818, 917
hidden variable
initialization, 932
I-equivalence classes, 821–824
incomplete data, 917–925
heuristics, 919–920
structural EM, 920–925, 932, 941
local maximum, 815–818
operators, 812–814, 845
edge addition, 812
edge deletion, 812
edge reversal, 812, 813
reinsertion, 847
ordering space, 848
parent constraints, 845
plateau, 815
template model, 837
tree-CPD, 835–836
delta-score, 846
operators, 835
trees, 808–809
undirected model, 985–995
computational complexity, 987
delta-score, 987, 992–995
gain heuristic, 993–995, 1005
gradient heuristic, 992
local maximum, 988
variable ordering, 809–811
structured variational, 448–469, 895
algorithm, 459–468, 482
convergence point, 458, 482
update, 460–468, 482
subgraph
complete, 35
induced, 35
subjective interpretation, 17
subutility function, 1071, 1073, 1117
suﬃcient statistics, 721
aggregate, 756
Bernoulli, 265
collection, 819–820
expected, 278, 871–874, 880
belief propagation, 962
conditional random ﬁeld, 951
log-linear model, 949
MAP assignment, 967–968
MCMC, 966–967, 1004
function, 262
Gaussian, 263, 721
interventional data, 1044–1046, 1056
log-linear model, 947
multinomial, 265, 721
sum-max-sum rule, 1098
sum-product, 299, 582, 611
message passing, see message passing,
sum-product
support vector machine, 999
survey propagation, 601
Swendson-Wang algorithm, 547
system state, 200
t-node, 1085
table-CPD, see CPD, table
target distribution, 494
target tracking, 678–684
target variable, 142
Taylor series, 631
temperature, 582

1232
SUBJECT INDEX
temperature parameter, 126, 524, 1160
template
variable
instantiated, see ground random variable
template model
dependency graph, 227, 245
factor, 203, 216
instantiated, 216
feature, 228
lifted inference, 689
parent, 221, 223
structure learning, 837–838, 846
variable, 200, 213
template variable, see attribute
temporal ordering, 1092, 1097, 1131
test set, 705
time slice, 201
time trade-oﬀ, 1069
topological ordering, 36, 62, 1146
trail, 36
active, 71
minimal, 100
training set, 705, 720
trajectory, 200
transition model
dynamical system, 202
state-observation model, 207
tree, 38, 808
tree reparameterization, see belief propagation,
tree reparameterization
tree-CPD, see CPD, tree-CPD, 834, 936
structure learning, 834–836, 845
tree-width, 310
bounded, 982, 1000
triangle inequality, 1140
triangulation, 139, 313, 374
troubleshooting, 166, 1027, 1037, 1055, 1124, 1132
truncated norm, 128, 603
TRW, see belief propagation, tree-reweighted
uncertainty, 2
unrolled Bayesian network, 204
unscented transformation, 634
upward closure, 35, 136
utility, 1060
additive independence, 1074–1075
CA-independence, 1075–1078, 1084
expected, 1060, 1061, 1064, 1087, 1093
GA-independence, 1078–1079, 1084
independence, 1072–1073, 1081
variable, 1090
utility function, 1061
curve, 1065–1067
decomposition, 1073
additive, 1073–1080, 1117
multilinear, 1073
multiplicative, 1073
distribution, 1084
elicitation, 1069, 1080–1081
factorization, 1076
human life, 1069–1070
indiﬀerence point, 1069
money, 1065–1066
v-structure, 71
validation set, 708, 891
value of control, 1132
value of information, 1121–1125, 1132
myopic, 1125, 1126
perfect, 1122
variable elimination, 299, 372, 1099
and conditioning, 319–322, 340
causal independence, 325–329
chordal graph, 310–313
cliques, 308
computational complexity, 305–310, 336
context-speciﬁc independence, 329–334
expected utility, 1100–1107
factor semantics, 301, 338
generalized, 342–343, 1103–1107, 1130, 1131
induced graph, 306–310
max-product, 556
traceback, 558
max-sum-product, 559–561
traceback, 561, 601
ordering, 299–301, 310
computational complexity, 310
constrained, 561, 596, 629, 1100, 1109
heuristics, 310–315, 340
maximum cardinality, 312, 340
rule-based CPDs, 329–334, 341
sum-product, 299
variational, 470–473, 483
with evidence, 303
variable ordering, 79–81, 809, 826
variance, 33
variational
Bayesian network, 483

SUBJECT INDEX
1233
lower bound, 469–472, 484
method, 386, 469–473
mixture distribution, 484
parameter, 470
sigmoid, 483
variable elimination, 470, 472–473
variational Bayes, 904–908
variational distance, 1143
variational, Markov network, see Gibbs
variational
visual-analog scale, 1069
Viterbi algorithm, 598, 675
Viterbi training, 967
witness, 85

