Bridging Text and Video: A Universal Multimodal Transformer
for Video-Audio Scene-Aware Dialog
Zekang Li14, Zongjia Li23, Jinchao Zhang2, Yang Feng1∗, Cheng Niu2, Jie Zhou2
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
2Pattern Recognition Center, WeChat AI, Tencent Inc, China
3School of EECS, Peking University
4University of Chinese Academy of Sciences
{lizekang19g, fengyang}@ict.ac.cn, zongjiali@pku.edu.cn
{dayerzhang, niucheng, withtomzhou}@tencent.com
Abstract
Audio-Visual Scene-Aware Dialog (AVSD) is a task to gen-
erate responses when chatting about a given video, which
is organized as a track of the 8th Dialog System Technol-
ogy Challenge (DSTC8). To solve the task, we propose a
universal multimodal transformer and introduce the multi-
task learning method to learn joint representations among
different modalities as well as generate informative and ﬂu-
ent responses. Our method extends the natural language gen-
eration pre-trained model to multimodal dialogue generation
task. Our system achieves the best performance in both ob-
jective and subjective evaluations in the challenge.
Introduction
Recently, scene-aware dialogue generation has attracted in-
creasing attention in both industry and academia due to its
broad application. It aims to generate informative and ﬂu-
ent dialogue responses grounding on the given scenes. Zhou,
Prabhumoye, and Black (2018) propose a dataset for text-
based movie grounded conversations. Jack Urbanek (2019)
builds a large-scale text adventure game platform, in which
agents can act and speak grounded on the scenes described
in the text. Inspired by human inherent multimodal under-
standing ability, Alamri et al. (2018) integrates multimodal-
ity to scene-aware dialogue and proposes the Audio-visual
Scene-Aware Dialog task.
The goal of the Audio-Visual Scene-Aware Dialog task
is to generate correct and ﬂuent responses by understanding
all modalities (e.g., text, video and audio), which is a more
challenging task than image-based or text-grounded dia-
log tasks. Figure 1 shows an example dialogue in DSTC8-
AVSD dataset (Alamri et al. 2018). There are three chal-
lenges in this task: acquiring accurate representation of the
video, effective interaction among different modalities and
∗Joint work with Pattern Recognition Center, WeChat AI, Ten-
cent Inc, China. Yang Feng is the corresponding author. This work
was done when Zongjia Li was interning at Pattern Recognition
Center, WeChat AI, Tencent.
Copyright c⃝2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Caption: A woman standing in a hallway takes off her slippers. She 
then climbs on a chair and starts doing something with the ceiling light.
Summary: A woman about 30 years old wearing a jean skirt and top is 
standing on a stool and fixing something in the hallway next to a door. 
The hallway has linoleum floors.
Q1: where is the video happening ? 
A1: it is happening inside in the hallway 
Q2: are there any people in the video ?
A2: yes there is one person in the video.
Q10: what is the person doing ?
A10: she is standing on a stool doing something with the ceiling light.
···
Figure 1: A dialogue sampled from the DSTC8-AVSD
dataset. For each dialogue, there are video, audio, video cap-
tion, dialogue summary and 10 turns of conversations about
the video.
better understanding dialogues and generating responses.
Hori et al. (2019) introduces an LSTM-based encoder and
decoder with the multimodal attention. Dat Tien Nguyen
and Asri (2019) propose a hierarchical recurrent encoder-
decoder framework for encoding and generating responses
based on a FiLM-based audio-visual feature extractor. Pa-
sunuru and Bansal (2019) adopts a dual attention mechanism
to encode and align multiple modalities. The winning team
of the DSTC7-AVSD task (Sanabria, Palaskar, and Metze
2019) focuses on using a hierarchical attention to combine
textual and visual modalities and employ the How2 dataset
for pre-training. Moreover, MTN (Le et al. 2019) proposes
multimodal transformer networks to encode video and in-
corporate information from different modalities.
These existing methods mainly use independent encoders
arXiv:2002.00163v1  [cs.CL]  1 Feb 2020

VA1
VA2
VA3
VA4
VA5
Yes
How 
old
…
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
…
n-4
n-3
n-2
[Cap]
A
woman
…
[eos]
Video Embedder
Text Embedder
Is
there
one
person
Segment
Features
Position
[Cap]
[Cap]
[Cap]
[Cap]
[Cap] [user1] [user1] [user1] [user1] [user1] [user2] [user2] [user1] [user1] [user1]
[user2] [user2] [user2]
[video] [video]
[video]
[video]
[video]
…
30
years
old
[user2]
n-1
VA2
VA3
VA4
VA5
…
A
…
[eos]
…
30
old
[eos]
Transformer Block 1
Transformer Block 2
Transformer Block n
ŏ
Video-Audio Sequence  Modeling
Response  Language Modeling
Caption Language Modeling
[user1]
[user2]
[user1]
[user2]
[eos]
n
[user2]
…
years
woman
Figure 2: Our universal multimodal transformer architecture. We concatenate video-audio, caption, dialogue history, and re-
sponse features to a long sequence. For different types of input, we adopt different segments tokens (“[video]”, “[caption]”,
“[user1]”, “[user2]”). We initialize our model with pre-trained GPT2 and introduce three tasks to ﬁne-tune our model: Response
Language Modeling (RLM), Video-Audio Sequence Modeling (VASM), Caption Language Modeling (CLM).
to separately encode different modalities and then exploit
the attention mechanism to fusion the representations of dif-
ferent modalities, which can not fully beneﬁt from the joint
representation of multi-modalities. To tackle the aforemen-
tioned problems of existing methods, in this paper, we de-
sign a universal multimodal transformer to encode different
modalities and generate responses at the same time. Inspired
by Bert (Devlin et al. 2018), GPT2 (Radford et al. 2019) and
other pre-training works, we use the self-supervised learn-
ing method and adopt the multi-task learning (response lan-
guage modeling, video-audio sequence modeling, and cap-
tion language modeling) approach to learn joint representa-
tions and generate informative and ﬂuent responses. To im-
prove the textual representation and generation, we initialize
our model with the pre-trained GPT2 (Radford et al. 2019)
model and then ﬁne-tune it.
Our contributions are as follows:
• We are the ﬁrst to use pre-trained natural language gener-
ation models in multimodal dialogue generation.
• We integrate multimodal features in one encoder and in-
troduce a multi-task leanring method to learn better joint
representations and generate more informative responses.
• We achieve a state-of-the-art result on Audio-Visual
Scene-Aware Dialog (AVSD) Dataset, outperforming ex-
isting methods and other teams in DSTC8-AVSD chal-
lenge by a large margin.
Related Work
Most works in the dialogue system focus on open-domain
dialogues or task-oriented dialogues. As in human-to-human
conversations, there is always background knowledge. Some
recent effort develops dialogue systems that can generate
responses grounding on the document or structured knowl-
edge graph (Li et al. 2019; Zhou, Prabhumoye, and Black
2018; Reddy, Chen, and Manning 2019; Dinan et al. 2018;
Madotto, Wu, and Fung 2018). These systems can gener-
ate responses that are either more relevant to background
knowledge or make more correct interactions. There are also
some works incorporating multimodal information in ques-
tion answering and dialogues. Visual QA (Goyal et al. 2017;
Agrawal et al. 2017) is to answer the given question about
the content of an image. Visual dialog (Das et al. 2017) is a
task to generate natural responses based on the given image
and the dialogue context. These works consider text or im-
ages as the background knowledge, whereas in Audio-Visual
Scene-Aware Dialog the knowledge is video and audio.
It has been shown that pre-trained language models play
an important role in improving the performance of language
generation tasks, such as the dialogue system and text sum-
marization. Zhang et al. (2019) proposes a natural language
generation model based on BERT to make good use of the
pre-trained language model in the encoding and decoding
process. Wolf et al. (2019b) introduces transfer learning to
generative data-driven dialogue systems using Generative
Pretrained Transformer (Radford et al. 2019). In our work,
we extend this transfer learning method to multimodal lan-
guage generation tasks and propose a self-supervised learn-
ing method for better video representation.
Methodology
In this section, we will describe our approaches to mul-
timodal dialogue systems. We will ﬁrst introduce the Au-
dio Visual Scene-Aware Dialog (AVSD) task. Then we will
present our multimodal dialogue generation model and its
training methods.
Task Formulation
Our goal is to integrate multimodal information, which
consists of video, audio and dialog context, to gener-
ate informative and ﬂuent responses. Formally, let V
and A represent video and audio respectively. Con-
sidering the similarity between the summary and the

I3D
···
···
VGGish
···
I3D-rgb
I3D-ﬂow
VGGish
Figure 3: Video and audio feature extractors. For video, we
adopt pre-trained I3D-rgb and I3D-ﬂow to extract rgb fea-
tures and optical ﬂow features. For audio, we use pre-trained
VGGish model.
video caption, we concatenate summary and caption as
a whole caption C
=
{c1, c2, . . . , cI}. We use U
=
{Q1, R1, Q2, R2, . . . QN, RN, } to denote the N turns of
dialogue, where Qn represent the question n and Rn =
{rn1, rn2, . . . , rnm} represent the response n containing m
words. Therefore, the probability to generate the response
Rn for the given question Qn considering video V, audio
A, dialogue history U<n, and caption C can be computed
as:
P(Rn|V, A, C, U<n, Qn; θ) =
m
Y
j=1
P(rnj|V, A, C, U<n, Qn, rn,<j; θ)
(1)
Model Overview
Our model architecture is illustrated in Figure 2, which is
a multilayer Transformer encoder based on the GPT2 ar-
chitecture (Radford et al. 2019). More speciﬁcally, we em-
ployed a 12-layer decoder-only transformer with the multi-
head self-attention.
Text Input. For text features, we follow GPT2 (Radford et
al. 2019) and tokenize the input sentence into WordPieces
(Wu et al. 2016).
Video and Audio Input. For the given video Vk, we
split the video to Tk segments with a sliding window of
l video frames. As shown in Figure 3, for each segment
St = {f1, f2, . . . , fl}, where fi represents one frame, we
use pre-trained I3D-rgb and I3D-ﬂow model to extract dv-
dimensional video features Vrgb and Vflow. Considering
audio is synchronous with video, we select the audio from
the same segment and use pretrained VGGish model to ex-
tract da-dimensional audio features as Avggish. We then
concatenate video I3D-rgb features, I3D-ﬂow features, and
VGGish features:
VAt = [Vrgb, Vflow, Avggish], VAt ∈R2dv+da
(2)
Then video-audio features VA are fed into a fully-connected
layer (Video Embedder), as shown in Figure 2, and projected
to the same embedding space as text embedding.
As shown in Figure 2, to make our model have the ability
to distinguish among different part of the input (video, cap-
tion, speaker1 and speaker2) and make use of the order of
the sequence, the ﬁnal representation for each word token
is obtained via summing up its word embedding (WE), po-
sitional encoding (PE) and segment embedding (SE). Note
that “[video]”, “[cap]”, “[user1]”, and “[user2]” are used
to represent the segment of video, caption, speaker1, and
speaker2 respectively.
Multi-task Learning
We introduce three tasks to ﬁne-tune our model: Response
Language Modeling conditioned on video, audio, caption
and dialogue history, Video-Audio Sequence Modeling
conditioned on caption and dialogue, and Caption Lan-
guage Modeling conditioned on video and audio.
Response Language Modeling (RLM). The goal of this
task is to generate responses Rn = {rn1, rn2, . . . , rnm}
based on the video-audio features VA, caption C, dialogue
history U<n, and question Qn, by minimizing the negative
log-likelihood loss function:
LRLM(θ) = −E(VA,C,U,Q,R)∼D
log
m
Y
j=0
P(rnj|VA, C, U<n, Qn, rn,<j)
(3)
where θ is the trainable parameters and (VA, C, U, Q) pairs
are sampled from the whole training set D.
Video-Audio Sequence Modeling (VASM). This task is to
predict video-audio features given caption and dialogue his-
tory. Unlike textual tokens which are represented as discrete
labels, video-audio features are high-dimensional and con-
tinuous. Instead of clustering video-audio features to dis-
crete labels as Sun et al. (2019) do, we adopt the video-audio
feature regression method following (Chen et al. 2019). This
task regresses the Transformer output of video-audio feature
ot to the next video-audio feature VAt+1. In particular, we
apply a fully-connected layer to transform the output to a
vector gθ(ot) of the same dimensional as VAt+1. We train
this task by minimizing L2 loss:
LV ASM(θ) =E(VA,C,U)∼D
1
T
T
X
t=1
∥gθ(ot) −VAt+1∥2
2
(4)
where ot
= fθ(VA<t+1, C, U) and fθ represents our
model.
Caption Language Modeling (CLM). Similar to Response
Language Modeling task, we train the model to generate
caption C = {c1, c2, . . . , cI} given the video-audio feature
VA:
LCLM(θ) = −E(VA,C)∼D log
IY
i=0
P(ci|VA, c<i)
(5)

Models
BLEU-1
BLEU-2
BLEU-3
BLEU-4
METEOR
ROUGE-L
CIDEr
Input: text only
Hierarchical Attention
-
-
-
0.376
0.264
0.554
1.076
Our model (RLM)
0.747
0.627
0.527
0.445
0.287
0.594
1.261
Input: text + video
Hierarchical Attention
-
-
-
0.394
0.267
0.563
1.094
MTN
-
-
-
0.392
0.269
0.559
1.066
Our model (RLM)
0.759
0.635
0.533
0.448
0.293
0.602
1.282
+ VASM
0.765
0.643
0.543
0.459
0.294
0.606
1.308
Input: text + video w/o caption / summary
Naive fusion
-
-
-
0.309
0.215
0.487
0.746
DSTC7-AVSD Team 9
-
-
-
0.315
0.239
0.481
0.773
Our model (RLM)
0.694
0.570
0.476
0.402
0.254
0.544
1.052
+ VASM
0.677
0.556
0.462
0.389
0.250
0.533
1.004
+ recaption
0.670
0.537
0.438
0.362
0.254
0.535
1.022
Table 1: Objective evaluation results on the test set provided by the organizers in DSTC7-AVSD challenge (6 groundtruth
available).
Models
BLEU-1
BLEU-2
BLEU-3
BLEU-4
METEOR
ROUGE-L
CIDEr
Human rating
Input: text only
Our model (RLM)
0.744
0.626
0.525
0.442
0.287
0.595
1.231
3.934
Input: text + video
Our model (RLM)
0.739
0.624
0.528
0.447
0.284
0.592
1.226
3.895
+ VASM
0.746
0.626
0.528
0.445
0.286
0.598
1.240
-
Input: text + video w/o caption / summary
Our model (RLM)
0.677
0.556
0.462
0.387
0.249
0.544
1.022
-
+ VASM
0.669
0.550
0.457
0.385
0.246
0.540
0.988
-
+ recaption
0.661
0.533
0.437
0.364
0.242
0.533
0.991
-
Table 2: Objective and subjective evaluation results on the test set provided by the organizers in DSTC8-AVSD challenge (6
groundtruth available).
Different Settings
Text-only. We only use text input and Response Language
Modeling (RLM) task.
Text + Video. We use text input and video-audio input and
we train the model with the aforementioned three tasks.
Text + Video w/o caption. In this setting, there are two
methods: 1) Don’t use caption in both training and testing,
and train the model with Response Language Modeling
(RLM) and Video-Audio Sequence Modeling (VASM).
2) Using captions and training the model with the three
aforementioned tasks. When testing, ﬁrst generate video
caption based on the given video-audio input (recaption)
and then generate responses using video-audio input,
generated caption, and dialogue history.
Experiments
Datasets
We use the Audio-Visual Scene-Aware Dialog (AVSD)
dataset (Alamri et al. 2018). In this dataset, each dialog con-
sists of a sequence of questions and answers about the given
video between two speakers. There is a video description for
each video. We use the state-of-the-art video feature extrac-
tor I3D model pre-trained on YouTube videos and the Ki-
netics dataset (Kay et al. 2017). Speciﬁcally, we use the out-
put from the “Mixed 5c” layer of the I3D network, which is
a 2048-dimensional vector. For audio features, we adopted
the VGGish model (Hershey et al. 2017) which outputs a
128-dimensional embedding. There are 7,659 dialogues for
training, 1787 dialogues for validation and 1710 dialogues
for testing.
Baselines
We compare our model with several related baseline meth-
ods:
Naive Fusion: The multimodal baseline provided by the
organizers, which combines all modalities with a projection
matrix (Hori et al. 2019).
Hierarchical Attention: The hierarchical attention ap-
proach to combine textual and visual modalities, which the
team ranked 1st in the DSTC7-AVSD task adopted.
MTN: The state of the art system before the DSTC8-AVSD
Challenge, which proposes Multimodal Transformer Net-
works (MTN) to encode videos and incorporate information
from different modalities (Le et al. 2019).

History Length
BLEU-1
BLEU-2
BLEU-3
BLEU-4
METEOR
ROUGE-L
CIDEr
0
0.729
0.599
0.496
0.413
0.275
0.573
1.182
1
0.760
0.638
0.536
0.452
0.296
0.605
1.305
2
0.755
0.632
0.532
0.450
0.296
0.601
1.297
3
0.765
0.643
0.543
0.459
0.294
0.606
1.308
5
0.758
0.634
0.533
0.451
0.292
0.601
1.293
9
0.759
0.631
0.526
0.441
0.296
0.603
1.294
Table 3: Objective evaluation results on the test set of DSTC7-AVSD (6 groundtruth available) in which maximum history
dialogue turn length ranges from 0 to 3 or 5 or 9. Best result in each metric is highlighted in bold.
Decoding Methods
BLEU-1
BLEU-2
BLEU-3
BLEU-4
METEOR
ROUGE-L
CIDEr
Greedy Search
0.743
0.610
0.503
0.416
0.284
0.587
1.217
Nucleus Sampling
0.680
0.525
0.410
0.321
0.252
0.527
0.955
Beam Search
0.765
0.643
0.543
0.459
0.294
0.606
1.308
Table 4: Objective evaluation results compared between different decoding methods.
Metrics
Objective evaluation: We report the metrics that are com-
monly used in the natural language generation tasks, such
as BLEU (Papineni et al. 2002), METEOR (Denkowski and
Lavie 2014), ROUGE-L (Lin 2004), and CIDEr (Vedantam,
Lawrence Zitnick, and Parikh 2015). We evaluate our mod-
els using the toolkit provided by the competition organizers.
Subjective Evaluation: Subjective evaluations are essential
for dialogue generation. The organizers also evaluate some
systems based on crowd-sourced human ratings. The annota-
tors are asked to consider the correctness, naturalness, infor-
mativeness, and appropriateness of the generated responses
and give a score at 5 levels.
Experiment Settings
In our experiment, we initialize our model with the pre-
trained weights from the GPT2-base model (Radford et al.
2019; Wolf et al. 2019a). In the training process, we use up
to 3 turns of history. The hidden size of our model is 768 and
the batch size is 32. We use Adam optimizer with a learning
rate of 6.25e-5. In the decoding process, we use beam search
with a beam size of 5, max length of 20 and a length penalty
of 0.3.
Experimental Results
In this section, we report the experimental results under dif-
ferent settings: text only, text + video, text + video without
caption/summary.
Text only. As shown in Table 1, compared to Hierarchical
Attention which is used in the winner system of DSTC7-
AVSD challenge, our model gets better performance on all
metrics. In detail, our model improves BLEU-4 by 0.069
and CIDEr by 0.185. Additionally, Table 2 also shows the
human evaluation rating in the DSTC8-AVSD track. During
human evaluation, the evaluators are asked to rate even the
groundtruth references, which are scored 4.000, our model
for this task scores 3.934, which is the highest Human rat-
ing among all DSTC8 submissions. In the human rating per-
spective, the results of our model are very close to human
dialogue.
Text + video. This task uses full information of the track:
caption, summary, dialogue history, and video. As we can
see in Table 1, compared to MTN which is the former state-
of-the-art model for this task, our model also achieves a huge
improvement. In particular, our model improved BLEU4 by
0.056, and CIDEr by 0.216. Compared to the text-only task,
our models achieve better results, which indicates that our
method for video understanding is effective. We adopt multi-
task learning as we described before. Video-Audio sequence
modeling task improves the score of BELU-4 by 0.011 and
CIDEr by 0.026. In DSTC8 results shown in Table 2, this
method get a little lower BELU4 value, but still improved
CIDEr by 0.014, which shows the method is effective.
Text + video w/o caption/summary. This setting is most
similar to the actual scene-aware dialogue: we only have
video-audio information and dialogue history. Therefore,
this task is more challenging. As shown in Table 1, we can
see the lower performance than the text + video task as ex-
pected, but it is gratifying that our model still got a relatively
high performance. We outperform the DSTC7-AVSD Team
9 who got the highest performance in this task by a large
margin. In this task, we also try to use the multi-task learn-
ing method but gets lower performance on almost all met-
rics. We will discuss this phenomenon in the next section.
Analysis and Discussion
Training Method Analysis. As we introduced in Exper-
iment Results, after we adopt the feature regression, our
model gets better performance in text + video task, but get
lower performance in text + video w/o caption/summary
task. We think the reason is the shortage of information: only
uses the history dialogue, it is difﬁcult to rebuild the masked
video feature, and compared to rebuilding text from adjacent
context, we think our model doesn’t have a strong ability in
extracting information from videos, therefore, the method
doesn’t work in this task.
For the poor performance of CLM, we think the reason

may be similar: the poor ability in extracting video infor-
mation of our model limited the performance for inferring
caption from the video. So we think future work can focus
on video comprehension as well as integrating video and text
information.
History Length. We experiment with our model in text +
video settings with video regression loss to explore the in-
ﬂuence of dialogue history length. As shown in Table 3, our
model gets the best performance when the maximum dia-
logue history length is 3.
Decoding Methods. To ﬁnd an effective decoding method
for multimodal dialogue generation, we try various decod-
ing methods, including greedy search, beam search, and nu-
cleus sampling (Holtzman et al. 2019) which samples text
from the dynamic nucleus of the probability distribution.
As shown in Figure 4, decoding with beam search gets the
best results on all objective results among these three de-
coding methods. We consider that in Audio-Visual Scene-
Aware Dialog, grounding on video and caption, responses
are relatively more deﬁnite than that in open-domain dia-
logues. Therefore, it’s better to use the beam search when
decoding in this task.
Conclusion and Future Work
In this paper, we propose a multimodal dialogue generation
model based on a pre-trained language model and introduce
multi-task learning to learn more accurate joint representa-
tion among multi modalities and generate more informative
responses. In the future, we plan to use more video features
like ResNet features and explore more training tasks to im-
prove the joint understanding of video and text. In addition,
we hope to extend these methods to other tasks, such as
video captioning, image captioning, and visual dialog.
Acknowledgments
We sincerely thank the anonymous reviewers for their thor-
ough reviewing and valuable suggestions. This work is sup-
ported by National Natural Science Foundation of China
(NO.61876174) and National Key R&D Program of China
(NO.2017YFE9132900).
References
[Agrawal et al. 2017] Agrawal,
A.;
Lu,
J.;
Antol,
S.;
Mitchell, M.; Zitnick, C. L.; Parikh, D.; and Batra, D. 2017.
Vqa: Visual question answering. International Journal of
Computer Vision 123(1):4–31.
[Alamri et al. 2018] Alamri, H.; Hori, C.; Marks, T. K.; Ba-
tra, D.; and Parikh, D.
2018.
Audio visual scene-aware
dialog (avsd) track for natural language generation in dstc7.
In DSTC7 at AAAI2019 Workshop, volume 2.
[Chen et al. 2019] Chen, Y.-C.; Li, L.; Yu, L.; Kholy, A. E.;
Ahmed, F.; Gan, Z.; Cheng, Y.; and Liu, J.
2019.
Uniter: Learning universal image-text representations. arXiv
preprint arXiv:1909.11740.
[Das et al. 2017] Das, A.; Kottur, S.; Gupta, K.; Singh, A.;
Yadav, D.; Moura, J. M.; Parikh, D.; and Batra, D. 2017.
Visual dialog. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 326–335.
[Dat Tien Nguyen and Asri 2019] Dat
Tien
Nguyen,
Shikhar Sharma, H. S., and Asri, L. E. 2019. From ﬁlm
to video: Multi-turn question answering with multi-modal
context. In DSTC7 at AAAI2019 Workshop.
[Denkowski and Lavie 2014] Denkowski, M., and Lavie, A.
2014. Meteor universal: Language speciﬁc translation eval-
uation for any target language. In Proceedings of the EACL
2014 Workshop on Statistical Machine Translation.
[Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and
Toutanova, K. 2018. Bert: Pre-training of deep bidirectional
transformers for language understanding.
arXiv preprint
arXiv:1810.04805.
[Dinan et al. 2018] Dinan, E.; Roller, S.; Shuster, K.; Fan,
A.; Auli, M.; and Weston, J. 2018. Wizard of wikipedia:
Knowledge-powered conversational agents. arXiv preprint
arXiv:1811.01241.
[Goyal et al. 2017] Goyal, Y.; Khot, T.; Summers-Stay, D.;
Batra, D.; and Parikh, D. 2017. Making the v in vqa matter:
Elevating the role of image understanding in visual question
answering. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 6904–6913.
[Hershey et al. 2017] Hershey, S.; Chaudhuri, S.; Ellis, D. P.;
Gemmeke, J. F.; Jansen, A.; Moore, R. C.; Plakal, M.; Platt,
D.; Saurous, R. A.; Seybold, B.; et al. 2017. Cnn architec-
tures for large-scale audio classiﬁcation. In 2017 ieee in-
ternational conference on acoustics, speech and signal pro-
cessing (icassp), 131–135. IEEE.
[Holtzman et al. 2019] Holtzman, A.; Buys, J.; Forbes, M.;
and Choi, Y. 2019. The curious case of neural text degener-
ation. arXiv preprint arXiv:1904.09751.
[Hori et al. 2019] Hori, C.; Alamri, H.; Wang, J.; Wichern,
G.; Hori, T.; Cherian, A.; Marks, T. K.; Cartillier, V.; Lopes,
R. G.; Das, A.; et al. 2019. End-to-end audio visual scene-
aware dialog using multimodal attention-based video fea-
tures.
In ICASSP 2019-2019 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP),
2352–2356. IEEE.
[Jack Urbanek 2019] Jack Urbanek, Angela Fan, S. K. S. J.
S. H. E. D. T. R. D. K. A. S. J. W. 2019. Learning to speak
and act in a fantasy text adventure game.
[Kay et al. 2017] Kay, W.; Carreira, J.; Simonyan, K.; Zhang,
B.; Hillier, C.; Vijayanarasimhan, S.; Viola, F.; Green, T.;
Back, T.; Natsev, P.; et al. 2017. The kinetics human action
video dataset. arXiv preprint arXiv:1705.06950.
[Le et al. 2019] Le, H.; Sahoo, D.; Chen, N.; and Hoi, S.
2019.
Multimodal transformer networks for end-to-end
video-grounded dialogue systems.
In Proceedings of the
57th Annual Meeting of the Association for Computational
Linguistics, 5612–5623.
Florence, Italy: Association for
Computational Linguistics.
[Li et al. 2019] Li, Z.; Niu, C.; Meng, F.; Feng, Y.; Li, Q.;
and Zhou, J. 2019. Incremental transformer with deliber-
ation decoder for document grounded conversations. arXiv
preprint arXiv:1907.08854.

[Lin 2004] Lin, C.-Y. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization branches
out, 74–81.
[Madotto, Wu, and Fung 2018] Madotto, A.; Wu, C.-S.; and
Fung, P. 2018. Mem2seq: Effectively incorporating knowl-
edge bases into end-to-end task-oriented dialog systems.
arXiv preprint arXiv:1804.08217.
[Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.;
and Zhu, W.-J. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th an-
nual meeting on association for computational linguistics,
311–318. Association for Computational Linguistics.
[Pasunuru and Bansal 2019] Pasunuru, R., and Bansal, M.
2019.
Dstc7-avsd: Scene-aware video-dialogue systems
with dual attention. In DSTC7 at AAAI2019 Workshop.
[Radford et al. 2019] Radford, A.; Wu, J.; Child, R.; Luan,
D.; Amodei, D.; and Sutskever, I. 2019. Language models
are unsupervised multitask learners.
[Reddy, Chen, and Manning 2019] Reddy, S.; Chen, D.; and
Manning, C. D.
2019.
Coqa: A conversational question
answering challenge.
Transactions of the Association for
Computational Linguistics 7:249–266.
[Sanabria, Palaskar, and Metze 2019] Sanabria, R.; Palaskar,
S.; and Metze, F. 2019. Cmu sinbads submission for the
dstc7 avsd challenge. In DSTC7 at AAAI2019 Workshop.
[Sun et al. 2019] Sun, C.; Myers, A.; Vondrick, C.; Murphy,
K.; and Schmid, C.
2019.
Videobert: A joint model for
video and language representation learning. arXiv preprint
arXiv:1904.01766.
[Vedantam, Lawrence Zitnick, and Parikh 2015] Vedantam,
R.; Lawrence Zitnick, C.; and Parikh, D.
2015.
Cider:
Consensus-based image description evaluation. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, 4566–4575.
[Wolf et al. 2019a] Wolf, T.; Debut, L.; Sanh, V.; Chaumond,
J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.;
Funtowicz, M.; and Brew, J. 2019a. Huggingface’s trans-
formers: State-of-the-art natural language processing. ArXiv
abs/1910.03771.
[Wolf et al. 2019b] Wolf, T.; Sanh, V.; Chaumond, J.; and
Delangue, C. 2019b. Transfertransfo: A transfer learning
approach for neural network based conversational agents.
arXiv preprint arXiv:1901.08149.
[Wu et al. 2016] Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.;
Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.;
Macherey, K.; et al. 2016. Google’s neural machine transla-
tion system: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144.
[Zhang et al. 2019] Zhang, H.; Gong, Y.; Yan, Y.; Duan, N.;
Xu, J.; Wang, J.; Gong, M.; and Zhou, M. 2019. Pretraining-
based natural language generation for text summarization.
arXiv preprint arXiv:1902.09243.
[Zhou, Prabhumoye, and Black 2018] Zhou,
K.;
Prabhu-
moye, S.; and Black, A. W. 2018. A dataset for document
grounded conversations. arXiv preprint arXiv:1809.07358.

