Foundations of Bayesianism 

APPLIED LOGIC SERIES 
VOLUME 24 
Managing Editor 
Dov M. Gabbay, Department of Computer Science, King's College, London, 
u.K. 
Co-Editor 
Jon Barwiset 
Editorial Assistant 
Jane Spurr, Department of Computer Science, King's College, London, u.K. 
SCOPE OF THE SERIES 
Logic is applied in an increasingly wide variety of disciplines, from the traditional subjects 
of philosophy and mathematics to the more recent disciplines of cognitive science, compu-
ter science, artificial intelligence, and linguistics, leading to new vigor in this ancient subject. 
Kluwer, through its Applied Logic Series, seeks to provide a home for outstanding books and 
research monographs in applied logic, and in doing so demonstrates the underlying unity and 
applicability of logic. 
The titles published in this series are listed at the end of this volume. 

Foundations of 
Bayesianism 
Edited by 
DAVID CORFIELD 
Department of Philosophy, 
King's College Londoll, u.K. 
and 
JON WILLIAMSON 
Department of Philosophy, 
King 's Col/ege London, U.K. 
SPRJNGER-SCIENCE+BUSINESS MEDIA, B. V. 

A C.I.P. Catalogue record for this book is available from the Library of Congress. 
ISBN 978-90-481-5920-8 
ISBN 978-94-017-1586-7 (eBook) 
DOI 10.1007/978-94-017-1586-7 
Printed on acidjree paper 
All Rights Reserved 
Â© 2001 Springer Science+Business Media Dordrecht 
Originally published by Kluwer Academic Publishers in 2001 
No part of the material protected by this copyright notice may be reproduced or 
utilized in any form or by any means, electronic or mechanical, 
including photocopying, recording or by any information storage and 
retrieval system, without written permission from the copyright owner 

Editorial Foreword 
Editorial Preface 
CONTENTS 
Introduction: Bayesianism into the 21st Century 
Jon Williamson and David Corfield 
Bayesianism, Causality and Networks 
Vll 
Xlll 
1 
Bayesianism and Causality, or, Why I am only a Half-Bayesian 
19 
Judea Pearl 
Causal Inference without Counterfactuals 
Philip Dawid 
Foundations for Bayesian Networks 
Jon Williamson 
Probabilistic Learning Models 
Peter Williams 
Logic, Mathematics and Bayesianism 
37 
75 
117 
The Logic of Bayesian Probability 
137 
Colin Howson 
Subjectivism, Objectivism and Objectivity in Bruno de 
161 
Finetti's Bayesianism 
Maria Carla GalavoUi 
Bayesianism in Mathematics 
175 
David Corfield 
Common Sense and Stochastic Independence 
203 
Jeff Paris and Alena Vencovska 

vi 
Integrating Probabilistic and Logical Reasoning 
James Cussens 
Bayesianism and Decision Theory 
Ramsey and the Measurement of Belief 
Richard Bradley 
Bayesianism and Independence 
Edward F. McClennen 
The Paradox of the Bayesian Experts 
Philippe Mongin 
Criticisms of Bayesianism 
241 
263 
291 
309 
Bayesian Learning and Expectations Formation: 
341 
Anything Goes 
Max Albert 
Bayesianism and the Fixity of the Theoretical Framework 
363 
Donald Gillies 
Principles of Inference and their Consequences 
381 
Deborah Mayo and Michael Kruse 
Index 
405 

FOREWORD 
COMBINING PROBABILISTIC AND LABELLED REASONING 
I welcome this volume, on Bayesianism, to our Applied Logic Series. This 
is an important thematic volume containing papers in the interface area 
between probabilistic networks and ordinary logic. In fact, several of the 
papers in the volume address directly the problem of combining these two 
types of reasoning. I believe the next evolutionary step in the historical de-
velopment of (formal methods of) practical reasoning must include theories 
of integration of ordinary logic (and its numerous varieties) with probabilis-
tic reasoning and with neural reasoning and models. 
Originally, the plan was that I include a contribution to the volume; a 
paper on probabilistic networks and labelled deductive systems. However, 
time being short and the task complex, I have only the initial ideas at this 
point in time. The paper was also going to serve as a background source on 
combining logics for the reader. This task I propose to do in this foreword. 
Let us start with a case study which has meaning both from the proba-
bilistic and from the pure logic point of view. 
Consider a language with -? only and some atomic statements a, b, c, d .... 
let us read d -t P as an insurance policy. A commitment that if at any point 
in time damage is done then payment will be made. First assume that the 
policy premium is paid by direct debit and so the policy is practically open 
ended. 
Given d, we can do modus ponens and get p. However, the damage must 
be done at a time after the policy was taken, not before. We symbolise this 
restriction by writing the data as a sequence, indicating the temporal order 
of becoming true. Thus we have 
(d -? p,d) f- p 
but 
(d, d -? p) If p. 
When we think about this kind of model we find that many such examples 
arise in practice. Here are some 
1. submit thesis -? (pass viva -? get PhD)' 
2. approve project -? (spend expenses -? get reimbursed) 

viii 
The order of x ~ (y ~ z) is first x and then y gives z. 
Thus 
(x ~ (y ~ z),x,y) f- z 
but 
(x ~ (y ~ z),y,x) If z. 
In example (1) the student will probably not be allowed to take the viva 
before submitting his thesis, but in (2) it is quite possible that spending 
occurs a little before (and probably in anticipation of) the project being 
awarded. 
Let us now describe another logic, a slight variation of the above. Suppose 
x ~ y has a limited validity. It is not open ended. Let us assume that time 
runs in days and for simplicity x ~ y is valid the next day only. Thus 
x ~ (y ~ z) means that if next day we have x and the day after we have 
y then we can get z (on the same day as y). 
Let us write x ~1 y for this kind of implication. Now we can have a logic 
with the implications ~1 and ~. How ate we going to integrate them? We 
need to see what kind of problems to expect. 
Let us start with two implications ~1 and ~2 satisfying modus ponens 
and the deduction theorem. We want to put them together. If we just do 
that, unfortunately they collapse. Here is a proof. 
To show 
we need to show 
(x ~1 y,x) f- y 
which holds by modus ponens. 
Obviously we need to be careful. Let us look at the second line of the 
proof more closely. We have a database ~ = {x ~1 y} and we want to 
prove a wff of language 2 namely x ~2 y. So we are in a language 2 mode. 
We add x to ~ as a language 2 item of data and want to show y, using 
a language 2 proof. Why should x be accessible to modus ponens with 
x ~1 y? 
This is the modus ponens of language 1 proofs. 
If x, (which has been added during a language 2 proof) is accessible, then 
we can get y. 
If we take another look at our case study with ~ and ~1, then 

FOREWORD 
ix 
does not hold because if we put x in the data it should be accessible to 
x -t y, since -t1 insists on x being true on the next day, while -t does not 
care about next or later days. The other way round does not work, i.e. 
X -t1 Y If x -t y. 
If we add x to the data, x may not be accessible to -t1 because -t does not 
insist on x being true the next day. 
N ow let us go back to 
X -t y f- X -t1 y. 
We agreed that x is accessible to x -t y. We thus get y. But we got y in 
the logic of -to We need to show that y is accessible in the logic of -t1' Is 
y now accessible to the logic of -t1 ? 
The answer is yes, because both logics yield y in the same day as X. 
To highlight that there can be a difference, assume that the insurance 
policy x -t2 y pays (gives y) the day after x while x -t1 y delivers y on the 
same day as X. 
SO let's try again 
X -t2 Y f-?x -t1 y. 
Since we assumed -t2 accepts x at any time, x is accessible to -t2 and we 
get y. However, y is available not on the same day as x but a day later. So 
we cannot 'export' y to the -t1 proof procedures because -t2 expects y on 
the same day as X. 
The moral of the above is that when we put two languages 1,2 together 
and try to combine their proof procedures, we need two fibring junctions, 
which we denote by 1F1,2 and 1F2,1. Such functions IF take any database ~ of 
one logic into another database IF( ~) of another logic. So given a database 
~ in the mixed language and assume that we are in proof mode 1, we can 
switch to proof mode 2, provided we consult 1F1,2 to tell us what database 
is available for us to use at the moment of the switch, namely, we can use 
1F1,2(~)' We can now get, using language 2 proof rules a new ~', and then if 
we want to switch back to the proof rules of language 1, we need to consult 
1F2,1(~')' 
Obviously a discipline of labelling of data and proofs needs to be installed 
to allow us to define the function IF in both directions, since what can be 
used will depend on how the data was historically proved, therefore the use 
of labelling. 
Having considered the example above, let us see what we need to figure 
out to put together probabilistic networks and say implication. Networks 
use atoms only and connect them in an acyclic graph. Let us take a look 
at the following figure 1. 

x 
Figure 1. 
We assume that A is some factor which can affect the outcome of the 
Viva. 
Suppose now we turn this network into a combined logic and probability 
network by letting A = (Viva -+ Job).l 
In this case A certainly can influence the outcome of the Viva, especially 
for borderline candidates. 
The question now is that suppose we add to the database the additional 
item Viva = T, how do we construct the new probability network? How 
do we reason with it? What probability do we get for the event Job? 
We have a mixed database~. A logical expression Viva -+ Job embedded 
in a node and a logical data item Viva = T as an additional member of ~. 
Our first restriction is to allow into the network A = X -+ Y only X, Y 
S.t. in the acyclic graph Y is a descendant of X. In other words, we are 
only short-circuiting the existing causal chain. 
Our first step for coherence is to add to the network a direct link from 
X to Y and add a new conditional probability distribution. Our network 
looks as follows (figure 2). 
Having made the new connection, A is now considered atomic. The effect 
of the implication has been fibred into the network by the new link and the 
new conditional probability function for the new link. This function must 
satisfy some coherence conditions. 
(i) If the probability of A is identical to 0 we do not make the link. 
I What we are doing is substituting an expression Viva --t Job in the language of --t 
for an atomic A of the language of the networks. If this --t is the causal language of 
the insurance discussed above, then it is not a probabilistic connection but an absolute 
one. In terms of the insurance policy d --t p, an absolute interpretation means that 
the insurance company pays for sure when damage incurs. In probabilistic terms the 
policy just increases the chances of recovering some compensation for the damage, but it 
is not certain the company will pay, as policies have so many exclusions and insurance 
companies always look for excuses not to pay. 

FOREWORD 
xi 
A = (Viva --> Job) ThrS 
~r 
Ph 
Figure 2. 
(ii) If the probability of A is identical to 1 we must rearrange the condi-
tional probabilities in the new network to yield probability(JobIPhD, 
Viva = T) to be 1. 
(iii) Otherwise some formula can be worked out for the general case. 2 
Now to reason from Viva = T, we calculate as usual in the new network, 
regarding A as atomic. 
Let us now consider putting networks into logic. Let us take the most 
typical logical deduction -
modus ponens. 
A --+ B,A f- B. 
Instead of A --+ B, A we substitute the networks of figure 3. 
This looks like, in network terms, the case where we get A = T in the 
A --+ B network, except that here we set the probability of A as the new 
one of the A network. Some other combination may also be reasonable. 
It may be more complicated to provide a formula for new probabilities 
for the case where A is inside a network, as in figure 4. 
I hope the above discussion gave the reader a taste of the kind of problems 
we encounter in integrating probability and logic. 
I have taken advantage of the relative freedom allowed in an editorial to 
present the case before all the details have been worked out. 
2Jon Williamson and I will have a paper oIhthis topic. 
Note that if we read A = X -+ Y probabilistically, as remarked in the previous footnote, 
then X raises the probability of Y, conditional on Y's other parents, i.e. 
probability (YIX /\ 11") > probability (YI..,X /\ 11") 
for each state 11" of Y's other parents. 
Going back to our specific example of A = Viva -+ Job, condition (ii) would need to 
be weakened. We only need to ensure that Job and Viva are probabilistically dependent 
on PhD. 

xii 
A 
â¢ 
A 
B 
Figure 3. 
z 
.A 
Figure 4. 
BIBLIOGRAPHY 
[Gabbay, 1998] D. M. Gabbay. Fibring Logics. Clarendon Press, Oxford, 1998. 
[Gabbay, 1996] D. M. Gabbay. Labelled Deductive Systems 1. Clarendon Press, Oxford, 
1996. 
Dov M. Gabbay 
London 

PREFACE 
Several chapters in this collection were presented at the conference 'Bayes-
ianism 2000', held at King's College London on the 11th and 12th May 
2000. We would like to thank the Centre for Philosophical Studies at King's 
College London and all the speakers and participants for helping to make 
it a great success. 'Causal inference without counterfactuals' appeared in 
the Journal of American Statistical Association 95 (June 2000), pages 407-
427, and 'The paradox of the Bayesian experts and state-dependent utility 
theory', appeared in the Journal of Mathematical Economics 29 (no. 3, 
1998), pages 331-361 -
thanks to the American Statistical Association 
and to Elsevier Science respectively for allowing the reprinting of these 
papers. Thanks also to Oxford University Press for allowing Colin Howson 
to reproduce passages from his book Hume's Problem: Induction and the 
Justification of Belief. 
We are also very grateful to Donald Gillies and Juliana Cardinale for valu-
able editorial advice, to Jane Spurr and Dov Gabbay for their publication 
assistance, and to the Leverhulme Trust and the UK Arts and Humanities 
Research Board for supporting this project financially. 
Jon Williamson and David Corfield 
London 

PART I 
BAYESIANISM, CAUSALITY AND NETWORKS 

JON WILLIAMSON AND DAVID CORFIELD 
INTRODUCTION: BAYESIANISM INTO THE 21ST 
CENTURY 
1 BAYESIAN BELIEFS 
Bayesian theory now incorporates a vast body of mathematical, statistical and 
computational techniques that are widely applied in a panoply of disciplines, from 
artificial intelligence to zoology. Yet Bayesians rarely agree on the basics, even 
on the question of what Bayesianism actually is. This book is about the basics -
about the opportunities, questions and problems that face Bayesianism today. 
So what is Bayesianism, roughly? Most Bayesians maintain that an individual's 
degrees of belief ought to obey the axioms of the probability calculus. If, for 
example, you believe to degree 0.4 that you will be rained on tomorrow, then you 
should also believe that you will not be rained on tomorrow to degree 0.6. Most 
Bayesians also maintain that an individual's degrees of belief should take prior 
knowledge and beliefs into account. According to the Bayesian conditionalisation 
principle, if you come to learn that you will be in Manchester tomorrow (m) then 
your degree of belief in being rained on tomorrow (r) should be your previous 
conditional belief on T given m: pt+l (r) = pt(rlm). By Bayes' theorem this can 
be rewritten pt(mlr)pt(r)jpt(m).1 
Although Bayesianism was founded in the eighteenth century by Thomas Bayes2 
and developed in the nineteenth century by Laplace,3 it was not until well into the 
twentieth century that Frank Ramsey4 and Bruno de Finetti5 provided credible jus-
tifications for the degree of belief interpretation of probability, in the shape of their 
Dutch book arguments. A Dutch book argument aims to show that if an agent bets 
according to her degrees of belief and these degrees are not probabilities, then the 
agent can be made to lose money whatever the outcome of the events on which she 
is betting. Already by this stage we see disagreement as to the nature of Bayesian-
ism, centring on the issue of objectivity. De Finetti was a strict subjectivist: he 
believed that probabilities only represent degrees of rational belief, and that an 
agent's belief function is rational just when it is a probability function -
no fur-
ther constraints need to be satisfied.6 Ramsey, on the other hand, was a pluralist 
in that he also accepted objective frequencies. Further, he advocated a kind of 
calibration between degrees of belief and frequencies: 
1 [Howson & Urbach, 1989; Earman. 1992J and [Gillies, 2000] are good introductions to Bayesian 
thought. 
2[Bayes. 1764]. 
3 [Laplace, 1814]. 
4 [Ramsey, 1926]. 
5 [de Finetti, 1937]. 
6See Galavotti's paper in this volume. 
D. Corfield and J. Williamson (eds.), Foundations of Bayesianism, 1-16. 
Â© 2001 Kluwer Academic Publishers. 

2 
JON WILLIAMSON AND DAVID CORFIELD 
Thus given a single opinion, we can only praise or blame it on the 
ground of truth or falsity: given a habit of a certain form, we can 
praise or blame it accordingly as the degree of belief it produces is 
near or far from the actual proportion in which the habit leads to truth. 
We can then praise or blame opinions derivatively from our praise or 
blame of the habits that produce them.7 
Such a view may be called empirical Bayesianism: degrees of belief should be 
calibrated with objective frequencies, where they are known.8 Ramsey was cau-
tious of too close a connection because of the reference class problem: Bayesian 
probabilities are single-case, defined over sentences or events, whereas frequen-
cies are general-case, defined over classes of outcomes, and there may be no way 
of ascertaining which frequency is to be calibrated with a given degree of belief. 
The Principal Principle of [Lewis, 1980] aims to circumvent this problem by of-
fering an explicit connection between degrees of belief and objective single-case 
probabilities. De Finetti shows that in certain circumstances, if degrees of belief 
are exchangeable then they will automatically calibrate to frequencies as Bayesian 
conditionalisation takes place.9 
John Maynard Keynes advocated logical Bayesianism: 
a probability 
p(bla) is the degree to which a partially entails b, and also the degree to which 
a rational agent should believe b, if she knows a. 10 Thus for Keynes probability 
is truly objective -
there is no room for two agents with the same knowledge to 
hold different belief functions yet remain perfectly rational. Moreover probability 
is fixed not by empirical frequencies but by logical constraints like the principle of 
indifference, which says that if there is no known reason for asserting one out of 
a number of alternatives, then all the alternatives must be given equal probability. 
There are problems with the principle of indifference which crop up when there 
is more than one way of choosing a suitable set of alternatives, but the maximum 
entropy principle, ardently advocated by Edwin Jaynes,11 has been proposed as a 
generalisation of the principle of indifference which is more coherently applicable. 
Empirical and logical Bayesianism may be grouped together under the banner 
of objective Bayesianism. Objective Bayesians may adopt a mixed approach: for 
example Rudolf 
Carnap had a position 
which 
incorporated 
both 
empirical and logical constraits on rational belief. 12 Objective Bayesians 
disagree with a strict subjectivist like de Finetti, since they claim that it 
is not sufficient that a belief function satisfies the axioms of probability -
it must 
satisfy further constraints before it can be called rational. But objective Bayesian-
ism harbours many views and proponents often disagree as to which extra con-
straints must be applied. Also, unlike Keynes many objective Bayesians accept 
7 [Ramsey, 19261 51. 
8See [Dawid, 19821. 
9 [de Finetti, 1937]. See also [Gaifman & Snir, 1982]. 
10 [Keynes, 1921l. 
11 [Jaynes, 1998]. 
12[Carnap, 19501, [Carnap & Jeffrey, 197I]. 

INTRODUCTION: BAYESIANISM INTO THE 21ST CENTURY 
3 
1600 
1400 
1200 
1000 
800 
600 
400 
- -
â¢ 
-
1111111111 
200 
1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 
Figure 1. Number of Bayesian articles by year. 
that in some situations there may be more than one rational probability function 
-
two rational agents may have the same background knowledge but different 
belief functions. 13 
The question of objectivity remains an important issue for Bayesians today, and 
one that will crop up in several papers in this book. 
2 BAYESIANISM TODAY 
The last decade of the twentieth century has witnessed a dramatic shift in the pro-
file of Bayesianism. Bayesianism has emerged from being thought of as a some-
what radical methodology -
for enthusiasts rather than research scientists -
into 
a widely applied, practical discipline well-integrated into many of the sciences. 
A search of the Web of Science database for articles whose subject contains the 
word or prefix 'Bayes' shows a dramatic upturn in the number of Bayesian papers 
in the 1990s -
see Figure 1. A search for Bayesian books on the British library 
catalogue tells a similar story, as do other searches,14 and the rise in the number 
of Bayesian meetings and the success of new organisations like the International 
Society for Bayesian Analysis15 provide further evidence. 
13See [Williamson, 1999) and the paper of Paris and Vencovska in this volume. 
14[Berger, 2(00) Â§2.1. 
15ISBA was established in 1992. See www.bayesian.org. 

4 
JON WILLIAMSON AND DAVID CORFIELD 
This renaissance has occurred largely thanks to computational and sociologi-
cal considerations. The calculation of the posterior probability of a hypothesis 
given data can require, via Bayes theorem, determining the values of integrals. 
These integrals may often have to be solved using numerical approximation tech-
niques, and it is only recently that computers have become powerful enough, 
and the algorithms efficient enough, to perform the integrations. The sociologi-
cal changes have been on two main fronts. First, scientific researchers, who are 
usually taught to present their work as objectively as possible, were often dis-
couraged from applying Bayesian statistics because of the perceived irreducible 
subjectivity of Bayesianism. This has changed as objective Bayesian techniques 
have become more popular. Second, Bayesian statistics has to a certain extent uni-
fied and absorbed classical techniques. Any religion worth its salt absorbs the gods 
of its competitors, and 'Bayesianity' is no different: 16 the diverse and seemingly 
unrelated techniques of classical statistics have been viewed as special-case ap-
proximations to Bayesian techniques, and Bayesianism has been invoked to shed 
light on the successes as well as the failures of classical statistics.17 Present-day 
statistics is often a half-way house between the classical and Bayesian churches: 
increasingly one finds that Bayesian techniques are used to select an appropriate 
statistical model, while the probabilities within the model are tacitly treated as 
being objective. 
In the field of artificial intelligence (AI) Bayesianism has been hugely influ-
ential in the last decade. Expert systems have moved from a logical rule-based 
methodology to probabilistic techniques, largely involving the use of Bayesian 
networks. IS Statistical learning theory has helped integrate machine learning tech-
niques into a probabilistic framework,19 and Bayesian methods are often now used 
to ascertain the parameters of machine learning models, and to determine the er-
ror between model and data.20 Applications in industry have followed quickly: 
Bayesian networks are behind several recent expert systems including the print 
trouble-shooter of Microsoft's Windows '95 (and, alas, the paperclip of Office 
'97);21 Bayesian reasoning is widely implemented using neural networks, forming 
the core of Autonomy's software for dealing with unstructured information (which 
made Autonomy's director, Mike Lynch, Britain's first dollar-billionaire);22 other 
graphical models also form the basis of applications of Bayesian statistics to med-
16The almost religious fervour with which Bayesians pursue the cause of Reverend Bayes, and with 
which non-Bayesians undergo the conversion to Bayesianism, has occasionally been noted. Jaynes 
appears to have coined the term 'Bayesianity'. 
17 [Jaynes, 1998]. 
18See [Pearl, 1988] and the website of the Association for Uncertainty in AI at www.auai.org. 
19 [Vapnik, 1995]. 
20See for example [Bishop, 1995], [Jordan, 1998] and Williams' paper in this volume. 
21 See research.microsoft.comldtasl and [Horvitz et al., 1998]. 
22See the technology white paper at www.autonomy.com. Peter Williams reported at the conference 
Bayesianism 2000 that neural network based Bayesian reasoning also proved successful (and lucrative!) 
when applied to gold prospecting. 

INTRODUCTION: BAYESIANISM INTO THE 21ST CENTURY 
5 
ical expert systems23 and health technology assessment. 24 
These developments in AI and other sciences have stimulated work on more tra-
ditional philosophical issues. Bayesian networks integrate causality and probabil-
ity in a particular way, and the question naturally arises as to how exactly Bayesian 
probability is related to causality, and whether techniques for learning Bayesian 
networks from data can be applied to the problem of discovering causal structure.25 
Probability logics and their AI implementations have prompted renewed investi-
gations into the relationship between Bayesian probability and logic.26 Objective 
Bayesian methods, often involving the use of the maximum entropy principle, have 
been successfully applied in physics?? and this has led to debate about the validity 
of objective Bayesianism28 and further applications of maximum entropy.29 Proba-
bilistic decision-theoretic techniques have now been widely adopted in economics, 
and this has stimulated research in the foundations of Bayesian decision theory. 30 
On the other hand, the application of Bayesianism to scientific methodology may 
lead to a corresponding application to mathematical methodology.3! 
In the context of this recent Bayesian upswell, it is all the more important to 
avoid complacency: criticisms of Bayesianism must be given due attention,32 and 
the key messages of the early proponents of Bayesianism must be better under-
stood.33 
3 PROSPECTS FOR BAYESIANISM 
Judging by the papers in this book, the future of Bayesianism will depend on 
progress on the following foundational questions. 
â¢ Is Bayesianism to be preferred over classical statistics? 
â¢ If so, what type of Bayesianism should one adopt -
strict subjectivism, 
empirical objectivism or logical objectivism? 
â¢ How does Bayesian reasoning cohere with causal, logical, scientific, math-
ematical and decision-theoretic reasoning? 
23 [Spiegelhalter et at., 19931. 
24[Spiegelhalter et at., 2000]. 
25See [Spirtes et at., 1993], [McKim & Turner, 1997], [Hausman & Woodward, 1999], [Hausman, 
1999], [Glymour & Cooper, 1999\, [Pearl, 2000\ and Pearl's, Dawid's and Williamson's papers in this 
volume. 
26See [Williamson, 2000\ and the papers of Cussens, Gabbay, Howson, and Paris and Vencovska in 
this volume. 
27 [Jaynes, 19981. 
28See Howson's and Paris and Vencovska's papers. 
29See Williamson's paper. 
30 See the papers of Mongin, McClennen, Bradley and Albert in this volume. 
31See Corfield's paper in this volume. 
32See the papers of Mayo and Kruse, Albert and Gillies. 
33 See Galavotti' s paper. 

6 
JON WILLIAMSON AND DAVID CORFIELD 
These questions are, of course, intricately linked. The first two are well-worn but 
extremely important: much progress has been made, but it would be foolhardy to 
expect any conclusive answers in the near future. The last question is particularly 
pressing, given the recent applications of Bayesian methods to AI. AI is now faced 
with a confusing plethora of formalisms for automated reasoning, and unification 
is high on the agenda. If Bayesianism can provide a framework into which AI 
techniques slot then its future is guaranteed. 
4 
THIS VOLUME 
The fifteen chapters of this book have been arranged in four parts. The first of 
these parts is entitled 'Bayesianism, Causality and Networks' and consists of four 
chapters. What unites the authors of the first three contributions is an eagerness 
to clarify the relationship between causal and probabilistic reasoning, two of them 
by way of the use of directed acyclic graphs. The author of the fourth chapter, 
on the other hand, reports on research on a different category of network -
neu-
ral networks. In the opening chapter, Pearl proceeds from the fundamental idea 
of Bayesianism that we should integrate our background knowledge with observa-
tional data when we reason. He then argues that our everyday and scientific knowl-
edge is largely couched in causal, rather than statistical, terms, and that as such it is 
not readily expressible in probabilistic terms. Now, clearly it would preferable to 
be able to feed background knowledge directly into our reasoning calculus, and so, 
if possible, we should devise a new mathematical language in which we can repre-
sent causal information and reason about it. The article advertises Pearl's exciting 
new research programme, detailed in his book 'Causality', whose central aim is 
the mathematisation of causality via directed graphs.34 The key questions to be 
addressed then concern the benefits of adopting such a radically new language and 
the safety of the reasoning it warrants. Pearl himself says that is possible to cast his 
causal models in terms of probabilities using hypothetical variables, but then ar-
gues that the only purpose in doing so is to avoid confrontation with the consensus 
position in the statistics community, which sees no limitations to the expressive-
ness of probability theory. Indeed, for Pearl, there is a definite disadvantage in a 
choice of language which gives counterfactual propositions precedence over more 
readily comprehensible causal ones. 
So Pearl's idea is that the previous failure to construct a mathematical system 
capable of integrating background causal knowledge has led to much of this most 
important way of encoding our beliefs about the world being overlooked. As such 
he has located a novel way in which we may take the Bayesian to be failing to act in 
as rational as possible a manner. On the other hand, a long-standing complaint of 
irrationality made against Bayesianism, one which will recur through the chapters 
of this volume, alleges that a Bayesian's tenets do not force her to test whether her 
degrees of belief are, in some sense or other, optima\. These two themes intertwine 
34[Pearl, 2000]. 

INTRODUCTION: BAYESIANISM INTO THE 21ST CENTURY 
7 
in Dawid's article. Dawid is well known for his 'Popperian' Bayesianism which 
aims to assess an agent's degrees of belief by a process of calibration, where, for 
example, weather reporters are to be congratulated if it rains on roughly 30 percent 
of the occasions they give 0.3 as the probability that it will rain. This concern with 
testability recurs in Dawid's contribution to the volume. While he agrees with 
Pearl that statisticians have largely ignored causality and have been wrong to do 
so, still he finds some elements of Pearl's new thinking problematic. What is at 
stake here is the Popperian belief that anything worthy of scientific consideration 
is directly testable. For Dawid some of the counterfactual reasoning warranted 
by Pearl's calculus (and by statisticians adopting other schemes, such as Rubin's 
potential-outcome approach) just is untestable. An example illustrating this key 
difference between Dawid and Pearl is their respective treatments of counterfactual 
questions such as whether my last headache would have gone had I not taken an 
aspirin, given that I did take one and it did go. How should knowledge of the effects 
of aspirin on other headache incidents of mine bear on this question? Pearl says 
that, without evidence to the contrary, we should presume that such knowledge 
does have a bearing on the counterfactual statement. By contrast, Dawid claims 
that singular counterfactual statements are untestable and therefore should not be 
accepted by the scientifically minded.35 For Dawid what may be justifiably said 
about counterfactuals does not involve their essential use. 
As Dawid is a self-professed Popperian, a comparison that comes to mind is 
to think of Pearl as a Lakatosian. While Popper's philosophy allowed that meta-
physical principles might guide the generation of novel scientific theories, thereby 
restoring some worth to them after the Logical Positivists had dismissed them as 
'meaningless', still they accrued no further value even when those theories passed 
severe tests. Where Lakatos went further than Popper was to allow metaphysics 
to be an integral part of a research programme, which was to be assessed by its 
theoretical and empirical success as a whole. Similarly, we could say that Pearl 
has devised a research programme with a powerful heuristic and a new mathemat-
icallanguage. There is a metaphysical belief on Pearl's part in the regularity of 
a world governed by causal mechanisms which is integrated into this programme, 
hence his turn to structural equation models. Dawid, meanwhile, views the pre-
suppositions behind the use of these models as unwarranted -
the world for him 
is not so easily tamed. 
In the third chapter Williamson questions the validity of the causal Markov 
condition, an assumption which links probability to causality and on which the 
theory of Bayesian networks and Pearl's recent account of causality depends. He 
argues that the causal Markov condition does not hold for an empirical account of 
probability, or for a strict subjectivist Bayesian interpretation, but does hold for an 
objective Bayesian interpretation, i.e., one using maximum entropy methods. If 
it can be established that the causal Markov condition does not hold with respect 
35 See the comments and rejoinder to Dawid's paper in the Journal of American Statistical Associa-
tion 95 (June 2000), pages 424-448. 

8 
JON WILLIAMSON AND DAVID CORFIELD 
to a notion of empirical probability, this means that causal networks must be re-
structured if they are to be calibrated with frequency data. This leads Williamson 
to propose a two-stage methodology for using Bayesian networks: first build the 
causal network out of expert knowledge and then restructure it to fit observational 
data more closely. The validity of stage 1 of this methodology depends on the 
validity of a maximum-entropy based objective Bayesian interpretation of prob-
ability and so would not appeal to subjectivists like de Finetti or Howson (see 
below), while the validity of stage 2 depends on acceptance of the idea that one 
ought to calibrate Bayesian beliefs with empirical data. 
Williams rounds out Part 1 of the book by offering us an overview of research 
carried out by the neural network community to provide a principled way of using 
data to fashion an accurate network. All forms of machine learning must find a 
way to reconcile the demands of accuracy and the risks of overfitting data. This 
relates to a long-standing debate in the philosophy of science about the desirability 
of choosing as simple as possible a model to represent empirical data. Now, some 
Bayesians, including those working in the tradition of Harold Jeffreys, claim to 
have found a principled way to effect this reconciliation by according a higher 
prior probability to a model with fewer free parameters. The potential for increased 
accuracy provided by an extra parameter will then be balanced by a lower prior 
probability for the more complicated model. Neural network researchers are now 
invoking these Bayesian notions to arrive at optimal network configurations and 
settings of connection strengths. 
A frequently encountered point of disagreement between the different approaches 
to artificial intelligence concerns the need to represent data and inference in propo-
sitionally encoded form. Neural networks come in for criticism for acting like 
black boxes. They may work well in many situations, the thought is, but we do not 
really understand why. Thus, unlike in the case of Bayesian networks, they offer 
no insight to the expert hoping to use them to support decision-making. Of course, 
one might respond to this criticism by making the point that accuracy, not trans-
parency, is the most important quality of a decision-making process, especially in 
critical situations such as medical diagnosis. In the context of Williams' chapter 
the lack of transparency relates to the fact that the space of weight configurations of 
a network bears no straightforward relation to an expert's qualitative understand-
ing of a domain. Thus background knowledge cannot be encoded directly into a 
prior distribution over possible networks, but only through the mediation of real 
or simulated data. Perhaps this difficulty is the reason that we find such a great 
range of techniques employed by the neural network community, even though, in 
the case of the ones described by Williams at least, Bayesian principles are guiding 
them. 
We turn next to the second part -
Logic, Mathematics and Bayesianism. Here 
the five authors wish to investigate the relationship between Bayesian probabilistic 
reasoning and deductive logic. In two chapters (Howson, Paris & Vencovska) we 
find probability theory presented as an extension of deductive logic, while in two 
others (Galavotti, Corfield) it appears in the guise of realistic personalist degrees 

INTRODUCTION: BAYESIANISM INTO THE 21ST CENTURY 
9 
of belief. Finally, Cussens discusses his use of stochastic logic programming, an 
artificial intelligence technique, to encode probabilistic reasoning. 
Howson views Bayesianism as an extension of deductive logic in the sense that, 
just as the use of deductive logic provides rules to ensure a consistent set of truth 
values for the statements of language, so the probability theory axioms ensure con-
sistent degrees of belief. In doing so he rules out three widely held, yet disputed, 
aspects of Bayesian reasoning: its inextricable link to utility theory; the principle 
of indifference, along with any other notion of objective priors; and, conditionali-
sation. Justification of this logical core of Bayesianism is provided by the idea of 
probability as expected truth value, using the device of the indicator function of 
a proposition, where a proposition is taken a La Carnap as the set of structures in 
which a sentence is true. Howson's belief that probability theory is a form of logic 
sets him against decision theorists, such as Herman Rubin, who believe that 'you 
cannot separate probability from utility' .36 Thus he aims to provide a justification 
for the probability axioms foregoing the use of Dutch Book arguments, thereby 
avoiding reliance on the notion of the desirability of acquiring money. 
Howson also rejects the strain of Bayesianism which hopes to arrive at some 
values to enter into the probability calculus through the use of the principle of 
indifference or of maximum entropy. More radically still, he continues by arguing 
that conditionalisation has no place in a Bayesian logic, since it is a rule relating 
truth values held at different times. He illustrates this thesis in parallel deductive 
terms: if you held 'A implies B' to be true yesterday, then find out today that A 
is true, you are not now forced to accept B, since you may no longer believe that 
A implies B. Similarly, if yesterday you havep(AIB) = x, and today p'(B) = 1, 
this does not mean you need have p' (A) = x. Here the reader might wonder about 
the status of the commonly held notion that, unless you have good reason for this 
change of heart, you should stick to your original beliefs. Is it just an extra-logical 
rule of thumb that p' (AlB) = p(AIB) unless there is good cause to change one's 
mind? 
Galavotti has provided a largely historical piece on the Bayesianism of Bruno 
de Finetti. De Finetti is famous for his assertion that 'probability does not exist' , 
preferring to see probabilities as subjective degrees of belief, rather than some-
thing inherent in the universe. But while he was keen to stress his disapproval of 
an objectivism which sees probabilities as simply out there in the world, this did 
not entail a disregard for objectivity. Empirical frequency data might be integrated 
into one's degrees of belief by the subjective judgement of the exchangeability of 
the data sequence. Moreover, and this may be a surprise for readers who share 
the commonly held impression that de Finetti was the arch-subjectivist Bayesian, 
he had a considerable interest in scoring rules used to judge the success of one's 
personal probability assignments. Comparisons of the accuracy of one's own pre-
vious probability judgements with those of others were to be integrated into one's 
current personal degrees of belief. 
36 [Rubin, 1987]. 

10 
JON WILLIAMSON AND DAVID CORFIELD 
Corfield bases his paper on the ideas of the Hungarian mathematician George 
P6lya, who in his description of plausible mathematical reasoning, which he inter-
preted by means of probabilistic degrees of belief, discerned what he took to be the 
common patterns of everyday reasoning. Corfield argues that no attempt to con-
strue mathematical reasoning in Bayesian terms can assume logical omniscience 
-
the requirement that rational agents accord the same degree of belief to any two 
logically equivalent statements. In the absence of this principle, logical and math-
ematical learning become thinkable in Bayesian terms. The idea that Bayesians 
should put logical and empirical learning on an equal footing goes back at least as 
far as de Finetti, and would seem to set Corfield against Howson who frames his 
Bayesian logic in such a way that logical omniscience comes already built in. 
One could argue that a Bayesian reconstrual of mathematical reasoning as it 
occurs in practice is likely to be a largely empty exercise. Certainly, Bayesian 
reconstructions of scientific reasoning have come in for this kind of criticism. One 
may be able to explain why observing a white tennis shoe provides no support for 
the law 'all ravens are black', despite being an instance of the logically equivalent 
'all non-black things are not ravens', these critics say, but it offers very little by way 
of insight into the rationality of decision making in science. However, one might 
reply that it has' led Corfield to consider the rationality of certain overlooked styles 
of mathematical reasoning: use of analogy, choice of proof strategy, large scale 
induction. Regarding the latter, for instance, to date very little attention has been 
paid by philosophers of mathematics to the rationality of mathematicians raising 
their degrees of belief in conjectures due to confirmations. For example, should 
the computer calculation which shows that the first 1.5 billion nontrivial zeros of 
the Riemann zeta function have real part equal to ~ be thought to lend support to 
the Riemann hypothesis, which claims that all of the infinitely many zeros lie on 
this line in the complex plane? 
Paris and Vencovska share Howson's vision of probability theory as a logic, but 
unlike him they seek to isolate and justify principles which will allow the agent to 
select her priors rationally. In an earlier paper37 they showed that the probability 
function which maximises entropy is the only choice if certain intuitively plausible 
constraints on objective Bayesian reasoning are to be respected. This was a signif-
icant result, but with one drawback: in their framework background knowledge is 
assumed to be encapsulated in a set of linear constraints. This rules out knowledge 
of, say, independencies amongst variables. In this chapter Paris and Vencovska 
extend their result to deal with non-linear constraints in the agent's background 
knowledge. There is now some room for subjectivity since there may be more 
than one most rational (i.e., maximum entropy) probability function. A point to 
note is that the framework adopted here is in the propositional calculus. This may 
be adequate for many AI applications, but it is not clear how it could be extended 
to the predicate calculus. If different reasoning principles are required for predi-
cate reasoning, how does the resulting formalisation cohere with the propositional 
37 [Paris & Vencovskli, 1990]. 

INTRODUCTION: BAYESIANISM INTO THE 21ST CENTURY 
11 
approach given here? 
As uncertainty is now treated probabilistically by the majority of AI practition-
ers, those adopting a logic based approach who wish to discuss uncertain reasoning 
are faced with the thorny problem of integrating logic and probability. Philoso-
phers have worked hard on this problem for many years with no consensus emerg-
ing. The line of thought that takes Bayesianism to be an extension of deductive 
logic would suggest that this should not be very problematic for a degree of be-
lief interpretation of probability. However, this has not turned out to be the case 
-
a very large number of disparate techniques have proposed by the AI commu-
nity. Cussens bases his attempt to integrate probability theory and logic on what 
are called 'stochastic logic programs'. Stochastic logic programs (SLPs) origi-
nated in the inductive logic programming (ILP) paradigm of machine learning.38 
When presented with data, an ILP program will attempt to generate a logic pro-
gram (essentially, a set of Horn clauses) which includes as successful goals as 
many positive examples as possible, while excluding as many negative examples. 
In cases where only positive examples are available, a common situation in sci-
ence, to prevent overfitting, it was found necessary to generate a distribution over 
all possible ground instances. Muggleton did this by labelling the clauses of a 
proposed logic program with probabilities generated from the data. Elsewhere, 
Cussens has extended this idea to apply it to natural language processing, where a 
successful parsing of a sentence will be accorded a probability depending on the 
ways it may be generated by the grammar encoded by the logic program. In the 
present article, he takes SLPs to be capable of representing a very wide range of 
AI techniques, in particular showing how Bayesian networks may be encoded in 
its terms. He then compares his SLP approach to other techniques. 
The reader might be interested to know of two other approaches to the inte-
gration of logic and Bayesian networks. Williamson develops 'logical Bayesian 
networks' (as opposed to causal Bayesian networks) whose nodes are sentences 
(rather than causes and effects) and whose arrows correspond to the logical im-
plication relation (rather than the causal relation).39 Meanwhile, Dov Gabbay is 
working on a way of representing Bayesian networks in the framework of his la-
belled deductive systems. His results were not ready in time for this volume, but 
they will appear in the near future. 
Turning now to the third part we find the contributions of three Bayesian deci-
sion theorists. Probabilistic decision theory has a long heritage, stretching back to 
Pascal's Wager, but there still rage many disputes over its fundamental principles. 
Here, two of the contributors, Mongin and McClennen, scrutinise the acceptability 
of particular axioms, while in the first chapter of this part Bradley discusses the 
problem of the measurement of belief. 
Bradley's claim is that the resources for resolving the issue of how to assess the 
strengths of beliefs and desires of an agent are to found in the writings of Ramsey 
38[Muggleton & de Raedt, 1994]. 
39[Williamson, 2001l. See also [Williamson, 2000] where these logical networks form the basis of a 
proof theory of a probabilistic logic. 

12 
JON WILLIAMSON AND DAVID CORFIELD 
from the 1920s. While decision theorists have followed the lead of Savage, Ram-
sey has largely been overlooked. However, as Bradley points out, Savage relied on 
the assumption of state-independent utility, where the desirability of an outcome 
is independent of the state of the world in which it occurs. This assumption has 
come in for a great deal of criticism, which has given rise to highly complex theo-
ries of state-dependent utility. Bradley argues that if we revive Ramsey's notion of 
'ethically neutral events', ones to whose outcome the agent is indifferent, we gain 
a means to access the strength of an agent's beliefs and desires without the need to 
invoke these complex theories. 
The Independence Principle, and the closely related Sure-Thing Principle, are 
central to Bayesian decision theory. The independence principle states that if an 
agent shows no preference between two gambles, P and P', then, for any 0 < 
o :S 1 and any gamble Q, she will also show no preference between the composite 
gambles R = oP + (1- o)Q and R' = oP' + (1- o)Q. While it appears to be a 
highly plausible principle, McClennen investigates various arguments put forward 
to support it, both directly and via the Sure-Thing Principle, and finds them all 
wanting. One might have supposed that the independence principle holds, since 
these composite gambles are disjunctive in the sense that in the case of R the final 
outcome will either be the outcome of P or the outcome of Q but not both. Still, 
McClennen argues, there may be an interactive effect making the agent prefer R 
to R'. This may occur, for instance, if Q more closely resembles P than P' and 
the agent has a preference for a less varied composite gamble. 
In the final chapter of Part 3, Mongin takes on the task of examining the difficul-
ties created by the simultaneous assumption of Bayesian and Paretian principles. 
The latter refer to assumptions about preferences of outcomes in the light of group 
consensus about preferences. For example, for a group of experts working with 
different utilities or different probabilities in the framework of state-independent 
utility theory, there will not in general be a way to select a utility function and 
probabilities such that one outcome is preferred over another whenever all the 
experts agree to this ordering. This result lends support to the move towards state-
dependent utility theory mentioned above. However, a pure form of this theory 
entails the undesirable consequence that subjective probabilities are not in gen-
eral uniquely determined. Mongin then proceeds to scrutinise a form of state-
dependence which entails unique probabilities. He shows, however, that the as-
sumptions of this theory still conflict with Paretian principles. 
The fourth and final part consists of three contributors' criticisms of Bayesian-
ism. As the chapters up to this point amply demonstrate, Bayesians disagree 
amongst themselves about all manner of issues: the extent of rationality con-
straints, the link to utility theory, the role of conditionalisation, etc. This being 
so, the critic's task is made harder. Whatever principle she attacks, some Bayesian 
may claim not to hold to it. 
Albert's criticism is aimed at the use of Bayesian principles by decision theo-
rists. Adopting a line reminiscent of Popper's critical attitude towards psychoanal-
ysis, he claims that for the Bayesian 'there is no such thing as irrational behavior' 

INTRODUCTION: BAYESIANISM INTO THE 21ST CENTURY 
13 
-
any set of actions can be construed as satisfying the constraints of Bayesian-
ism. Albert argues for this conclusion by discussing a situation involving a chaotic 
clock which outputs a sequence of Os and Is. The agent must judge the likelihood 
of these digits occurring, based on the sequence to date, to help him win as much 
money as possible. What Albert shows is that, even with the agent's utility func-
tion given, whatever he does one can reconstruct it as rational according to some 
choice of prior distribution over the hypothesis space. Now, in response one may 
argue that the chaotic clock situation does not resemble the everyday conditions 
met with in economic life, but Albert argues that his example is sufficiently generic 
in this sense. Objective Bayesians may also claim that knowledge of the chaotic 
clock set up provides the rational agent with an obvious unique choice of prior. 
On the other hand, subjectivists might question whether the ways of recording the 
agent's behaviour are sufficient to pick up fully his belief structure. For example, 
the chaotic clock situation would not allow you to discover the incoherence of an 
agent who is certain that 0 will appear next, but also certain that 1 will appear next. 
In his chapter, Gillies wants to argue that Bayesianism is appropriate only in a 
restricted range of situations, indeed, 'only if we are in a situation in which there is 
a fixed and known theoretical framework which it is reasonable to suppose will not 
be altered in the course of the investigation'. As soon as the reasoner departs from 
the current theoretic framework, Bayesianism is of no assistance. In saying this, 
Gillies appears to be aligning himself with one side of an argument heard before 
in the philosophy of science about the room for Bayesian principles to operate 
when a new theory is proposed. Earman, for instance, who is very sympathetic to 
Bayesianism, argues that changes of conceptual framework require the resetting of 
priors, which occurs in a non-algorithmic fashion by plausibility arguments.40 He 
also indicates that these exogenous redistributions of prior probabilities will occur 
frequently, perhaps within the course of an everyday conversation. Now, Howson's 
Bayesianism might have no problem with this -
after all it is not a diachronic 
theory -
but Bayesians of a different stripe, wishing to salvage a substantial role 
for conditionalisation, might prefer to concede that the advent of novel ideas will 
have their effects not through conditionalisation, but still look to 'normal' science 
to see Bayes' theorem at work. However, the two examples put forward by Gillies 
could hardly be considered revolutionary changes. Rather they appear to involve 
the kind of reasoning that any statistician will have to perform in the course of their 
work, i.e., the assessment of the validity of the current model. The indication is 
that where the error statistician is always eager to challenge the current model and 
put it to severe tests, the Bayesian has neither the means nor the incentive to look 
beyond the current framework. It is true that Dutch Book arguments by themselves 
do not require an agent to take the slightest trouble to make an observation or 
to challenge a modelling assumption that would be beneficial to a bet they are 
making. However, under reasonable assumptions, one can show that it is always 
worth seeking cost-free information before making a decision. It is therefore not 
4O[Earman, 1992]. 

14 
JON WILLIAMSON AND DAVID CORFIELD 
surprising to find Bayesian statisticians engaging in what Box and Tiao call 'model 
criticism' .41 Readers may care to see how a Bayesian statistician works on a very 
similar problem to Gillies' second example in [Gelman et al., 1995], 170-171. 
In their article, Mayo and Kruse take on Bayesian statisti~s on the issue of 
stopping rules. For the error statistician the conditions stipulated before the start of 
an experiment as to when it will be deemed to have ended will usually be relevant 
to the significance of the test. For instance, when testing for a proportion in some 
population, even if the data turns out the same, it makes a difference whether 
it has been generated by deciding to stop after a fixed number of positive cases 
have been observed or whether it has been generated by deciding to stop after a 
fixed number of trials. For most Bayesians (see [Box & Tiao, 1973], 44-46 for 
an exception), on the other hand, acceptance of the likelihood principle entails 
that such considerations should play no part in the calculation of their posterior 
distributions. This marks a very significant difference between the schools. Pace 
Gillies, as Mayo has said elsewhere, 'one cannot be just a little bit Bayesian'. Most 
Bayesians would agree. 
The Bayesian position has a considerable plausibility to it. Should we condemn 
an experimenter who intends to test 100 cases, finds half way through that the pro-
portion of positive cases is low, decides then to wait for 10 such cases to occur, 
which duly happens after precisely 100 trials, and then writes up the experiment as 
originally planned? Are all experiments called off early because of funding prob-
lems worthless? There appears to be something magical occurring in that the 'real' 
intentions of the experimenter make a difference. On the other hand, as critics of 
the Bayesian indifference to stopping rules, Mayo and Kruse need only point out 
the problematic consequences of operating with a single rule of their choosing. 
What then if an experimenter in a binomial situation with, say, p = 0.2 is to con-
tinue testing until this value of p becomes unlikely to a specified degree, putting 
some upper limit on the number of trials to make it a proper rule? Here at least 
the Bayesian can provide bounds for the likelihood of a test achieving this end. 
But Mayo and Kruse go on to discuss an experimental situation where due to the 
stopping rule the Bayesian statistician will necessarily reason to a foregone con-
clusion on the basis of the likelihood principle. This type of situation arises when 
an improper prior, failing to satisfy countable additivity, is employed. One could, 
of course, maintain that countable additivity be enforced, but improper priors are 
commonplace in the Bayesian literature. Mayo and Kruse present several quota-
tions revealing that some Bayesian statisticians are struggling to come to terms 
with this apparent paradox. Readers who wish to read more on this topic may 
well enjoy the discussion of this phenomenon by the Bayesians Kadane, Schervish 
and Seidenfeld.42 Not wishing to forego the use of improper priors, these authors 
consider that further work is required to ascertain when their use is admissible. 
King's College, London. 
41 [Box & Tiao, 19731. 
42[Kadane et aI., 19991 Â§Â§3.7 & 3.8. 

INTRODUCTION: BAYESIANISM INTO THE 21ST CENTURY 
15 
BIBLIOGRAPHY 
[Bayes, 1764] Thomas Bayes. An essay towards solving a problem in the doctrine of chances. Philo-
sophical Transactions of the Royal Society of London, 53, 370-418,1764. 
[Berger, 2000] James O. Berger. Bayesian analysis: a look at today and thoughts of tomorrow. Journal 
of the American Statistical Association, 95 (December), 2000. 
[Bishop, 1995] Christopher M. Bishop. Neural networks for pattern recognition. Oxford University 
Press, 1995. 
[Box & Tiao, 1973] G. Box & G. Tiao. Bayesian Inference in Statistical Analysis. Addison-Wesley, 
1973. 
[Camap, 1950] Rudolf Camap. Logical foundations of probability. Routledge & Kegan Paul Ltd, 
1950. Second edition 1962. 
[Carnap & Jeffrey, 1971l Rudolf Camap & Richard C. Jeffrey(eds.). Studies in inductive logic and 
probability, Volume I, University of California Press, 1971. 
[Dawid, 1982] A.P. Dawid. The well-calibrated Bayesian. With discussion, Journal of the American 
Statistical Association, 77, 604-613, 1982. 
[Earman, 1992] John Earman. Bayes or bust? M.I.T. Press, 1992. 
[de Finetti, 1937] Bruno de Finetti. Foresight. Its logical laws, its subjective sources. In [Kyburg & 
Smokier, 1964],53-118, 1937. 
[Gaifman & Snir, 1982] H. Gaifman & M. Snir. Probabilities over rich languages. Journal of Symbolic 
Logic, 47, 495-548, 1982. 
[Gelman et al., 1995] Andrew B. Gelman, John S. Carlin, Hal S. Stem & Donald B. Rubin. Bayesian 
data analysis. Chapman & Hall/CRC, 1995. 
[Gillies, 2000] Donald Gillies. Philosophical theries of probability. Routledge, 2000. 
[Glymour & Cooper, 1999] Clark Glymour & Gregory F. Cooper(eds.). Computation, causation, and 
discovery. M.I.T. Press, 1999. 
[Hausman, 1999] Daniel M. Hausman. The mathematical theory of causation. Review of [McKim & 
Turner, 1997], British Journalfor the Philosophy of Science, 50,151-162,1999. 
[Hausman & Woodward, 1999] Daniel M. Hausman & James Woodward. Independence, invariance 
and the causal Markov condition. British Journalfor the Philosophy of Science, 50, 521-583,1999. 
[Horvitz et al., 1998] Eric Horvitz, Jack Breese, David Heckerrnan, David Hovel & Koos Rommelse. 
The Lumiere Project: Bayesian user modeling for inferring the goals and needs of software users. In 
Proceedings of the Fourteenth Conference on Uncertainty in Artijiciallntelligence, Morgan Kauf-
mann, pages 256-265, 1998. 
[Howson & Urbach, 1989] Colin Howson & Peter Urbach. Scientijic reasoning: the Bayesian ap-
proach. Open Court, 1989. Second edition, 1993. 
[Jaynes, 19981 E.T. 
Jaynes. 
Probability 
theory: 
the 
logic 
of 
science. 
http:// 
bayes.wustl.eduletjlprob.html. 
[Jordan, 19981 Michael I. Jordan(ed.). Learning in Graphical Models. MIT Press, Cambridge, MA, 
1998. 
[Kadane et al., 1999] 1. Kadane, M. Schervish & T. Seidenfeld(eds.). Rethinking the Foundations of 
Statistics. Cambridge University Press, 1999. 
[Keynes, 1921l John Maynard Keynes. A treatise on probability. Macmillan, 1948. 
[Kyburg & Smokier, 1964] H.E. Kyburg & H.E. Smokler(eds.). Studies in subjective probability. 
1964. Second edition, Robert E. Krieger Publishing Company, 1980. 
[Laplace, 1814] Pierre Simon - Marquis de Laplace. A philosophical essay on probabilities. Dover, 
1951. 
[Lewis, 1980] David K. Lewis. A subjectivist's guide to objective chance. In [Lewis, 1986],83-132, 
1980. 
[Lewis, 1986] David K. Lewis. Philosophical papers II. Oxford University Press, 1986. 
[McKim & Turner, 1997] Vaughn R. McKim & Stephen Turner. Causality in crisis? Statistical meth-
ods and the search for causal knowledge in the social sciences. University of Notre Dame Press, 
1997. 
[Muggleton & de Raedt, 1994] Stephen Muggleton & Luc de Raedt. Inductive logic programming: 
theory and methods. In Journal of Logic Programming, 19,20, 629"'{)79, 1994. 
[Paris & Vencovska, 1990] 1.8. Paris & A. Vencovska. A note on the inevitability of maximum en-
tropy.lnternational Journal of Approximate Reasoning, 4, 181-223, 1990. 

16 
JON WILLIAMSON AND DAVID CORFIELD 
[Pearl, 1988] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible irifer-
ence. Morgan Kaufmann, 1988. 
[Pearl,2000] Judea Pearl. Causality: models, reasoning, and iriference. Cambridge University Press. 
2000. 
[Ramsey, 1926] Frank Plumpton Ramsey. Truth and probability. In [Kyburg & Smolder, 1964],23-
52,1926. 
[Rubin, 1987] H. Rubin. A Weak System of Axioms for "Rational" Behavior and the Nonseparability 
of Utility from Prior. Statistics and Decisions, 5, 47-58, 1987. 
[Spiegelhalter et al., 1993] David J. Spiegelhalter, A. Philip Dawid, Steffen L. Lauritzen & Robert G. 
Cowell. Bayesian analysis in expert systems. Statistical Science, 8(3), 219-283, with discussion, 
1993. 
[Spiegelhalter et al., 2000] DJ. Spiegelhalter, J.P. Myles, D.R. Jones & K.R. Abrams. Bayesian meth-
ods in health technology assessment: a review. Health Technology Assessment. 4(38). 2000. 
[Spirtes et al., 1993] Peter Spirtes. Clark Glymour & Richard Scheines. Causation, Prediction, and 
Search. Lecture Notes in Statistics, 81, Springer-Verlag, 1993. 
[Vapnik,1995] Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag, 1995. 
Second edition 2000. 
[Williamson, 1999] Jon Williamson. Countable additivity and subjective probability. British Journal 
for the Philosophy of Science, 50(3), 401-416,1999. 
[Williamson, 2000] Jon Williamson. Probability logic. In Dov Gabbay, Ralph Johnson, Hans Juergen 
Ohlbach & John Woods (eds.), Handbook of the Logic of Inference and Argument: The Tum Toward 
the Practical, Elsevier, 393-419, 2000. 
[Williamson,2001] Jon Williamson. Bayesian networks for logical reasoning. Proceedings of the 8th 
Workshop on Automated Reasoning, A. Voronkov (ed.), 55-56, 2001. 

JUDEA PEARL 
BAYESIANISM AND CAUSALITY, OR, WHY I AM 
ONLY A HALF-BAYESIAN 
INTRODUCTION 
I turned Bayesian in 1971, as soon as I began reading Savage's monograph The 
Foundations of Statistical Inference [Savage, 1962]. The arguments were unas-
sailable: (i) It is plain silly to ignore what we know, (ii) It is natural and useful 
to cast what we know in the language of probabilities, and (iii) If our subjective 
probabilities are erroneous, their impact will get washed out in due time, as the 
number of observations increases. 
Thirty years later, I am still a devout Bayesian in the sense of (i), but I now doubt 
the wisdom of (ii) and I know that, in general, (iii) is false. Like most Bayesians, I 
believe that the knowledge we carry in our skulls, be its origin experience, school-
ing or hearsay, is an invaluable resource in all human activity, and that combining 
this knowledge with empirical data is the key to scientific enquiry and intelligent 
behavior. Thus, in this broad sense, I am a still Bayesian. However, in order to 
be combined with data, our knowledge must first be cast in some formal language, 
and what I have come to realize in the past ten years is that the language of proba-
bility is not suitable for the task; the bulk of human knowledge is organized around 
causal, not probabilistic relationships, and the grammar of probability calculus is 
insufficient for capturing those relationships. Specifically, the building blocks of 
our scientific and everyday knowledge are elementary facts such as "mud does 
not cause rain" and "symptoms do not cause disease" and those facts, strangely 
enough, cannot be expressed in the vocabulary of probability calculus. It is for this 
reason that I consider myself only a half-Bayesian. 
In the rest of the paper, I plan to review the dichotomy between causal and sta-
tistical knowledge, to show the limitation of probability calculus in handling the 
former, to explain the impact that this limitation has had on various scientific dis-
ciplines and, finally, I will express my vision for future development in Bayesian 
philosophy: the enrichment of personal probabilities with causal vocabulary and 
causal calculus, so as to bring mathematical analysis closer to where knowledge 
resides. 
2 
STATISTICS AND CAUSALITY: A BRIEF SUMMARY 
The aim of standard statistical analysis, typified by regression and other estimation 
techniques, is to infer parameters of a distribution from samples drawn of that 
population. With the help of such parameters, one can infer associations among 
variables, estimate the likelihood of past and future events, as well as update the 
19 
D. Cor field and 1. Williamson (eds.), Foundations of Bayesianism, 19-36. 
Â© 2001 Kluwer Academic Publishers. 

20 
JUDEA PEARL 
likelihood of events in light of new evidence or new measurements. These tasks 
are managed well by statistical analysis so long as experimental conditions remain 
the same. Causal analysis goes one step further; its aim is to infer aspects of 
the data generation process. With the help of such aspects, one can deduce not 
only the likelihood of events under static conditions, but also the dynamics of 
events under changing conditions. This capability includes predicting the effect of 
actions (e.g., treatments or policy decisions), identifying causes of reported events, 
and assessing responsibility and attribution (e.g., whether event x was necessary 
(or sufficient) for the occurrence of event y). 
Almost by definition, causal and statistical concepts do not mix. Statistics deals 
with behavior under uncertain, yet static conditions, while causal analysis deals 
with changing conditions. There is nothing in the joint distribution of symptoms 
and diseases to tell us that curing the former would not cure the latter. In general, 
there is nothing in a distribution function that would tell us how that distribu-
tion would differ if external conditions were to change-say from observational 
to experimental setup-every conceivable difference in the distribution would be 
perfectly compatible with the laws of probability theory, no matter how slight the 
change in conditions. I 
Drawing analogy to visual perception, the information contained in a probabil-
ity function is analogous to a precise description of a three-dimensional object; it 
is sufficient for predicting how that object will be viewed from any angle outside 
the object, but it is insufficient for predicting how the object will be viewed if ma-
nipulated and squeezed by external forces. The additional properties needed for 
making such predictions (e.g., the object's resilience or elasticity) is analogous to 
the information that causal models provide using the vocabulary of directed graphs 
and/or structural equations. The role of this information is to identify those aspects 
of the world that remain invariant when external conditions change, say due to an 
action. 
These considerations imply that the slogan "correlation does not imply cau-
sation" can be translated into a useful principle: one cannot substantiate causal 
claims from associations alone, even at the population level-behind every causal 
conclusion there must lie some causal assumption that is not testable in observa-
tional studies. Nancy Cartwright [1989] expressed this principle as "no causes 
in, no causes out", meaning we cannot convert statistical knowledge into causal 
knowledge. 
The demarcation line between causal and statistical concepts is thus clear and 
crisp. A statistical concept is any concept that can be defined in terms of a distri-
bution (be it personal or frequency-based) of observed variables, and a causal con-
I Even the theory of stochastic processes, which provides probabilistic characterization of certain 
dynamic phenomena, assumes a fixed density function over time-indexed variables. There is nothing in 
such a function to tell us how it would be altered if external conditions were to change. If a parametric 
family of distributions is used, we can represent some changes by selecting a different set of parameters. 
But we are still unable to represent changes that do not correspond to parameter selection; for example, 
restricting a variable to a certain value, or forcing one variable to equal another. 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
21 
cept is any concept concerning changes in variables that cannot be defined from 
the distribution alone. Examples of statistical concepts are: correlation, regression, 
dependence, conditional independence, association, likelihood, collapsibility, risk 
ratio, odd ratio, and so on.2 Examples of causal concepts are: randomization, in-
fluence, effect, confounding, disturbance, spurious correlation, instrumental vari-
ables, intervention, explanation, attribution, and so on. The purpose of this de-
marcation line is not to exclude causal concepts from the province of statistical 
analysis but, rather, to make it easy for investigators and philosophers to trace the 
assumptions that are needed for substantiating various types of scientific claims. 
Every claim invoking causal concepts must be traced to some premises that invoke 
such concepts; it cannot be derived or inferred from statistical claims alone. 
This principle may sound obvious, almost tautological, yet it has some far 
reaching consequences. It implies, for example, that any systematic approach to 
causal analysis must acquire new mathematical notation for expressing causal as-
sumptions and causal claims. The vocabulary of probability calculus, with its 
powerful operators of conditionalization and marginalization, is simply insuffi-
cient for expressing causal information. To illustrate, the syntax of probability 
calculus does not permit us to express the simple fact that "symptoms do not cause 
diseases", let alone draw mathematical conclusions from such facts. All we can 
say is that two events are dependent-meaning that if we find one, we can expect 
to encounter the other, but we cannot distinguish statistical dependence, quantified 
by the conditional probability P( disease I symptom) from causal dependence, for 
which we have no expression in standard probability calculus.3 Scientists seeking 
to express causal relationships must therefore supplement the language of proba-
bility with a vocabulary for causality, one in which the symbolic representation for 
the relation "symptoms cause disease" is distinct from the symbolic representation 
of "symptoms are associated with disease." Only after achieving such a distinction 
can we label the former sentence "false," and the latter "true." 
The preceding two requirements: 
(1) to commence causal analysis with 
untested,4 judgmentally based assumptions, and (2) to extend the syntax of proba-
bility calculus, constitute, in my experience, the two main obstacles to the accep-
tance of causal analysis among statisticians, philosophers and professionals with 
traditional training in statistics. We shall now explore in more detail the nature of 
these two barriers, and why they have been so tough to cross. 
2The teon 'risk ratio' and 'risk factors' have been used ambivalently in the literature; some authors 
insist on a risk factor having causal influence on the outcome, and some embrace factors that are merely 
associated with the outcome. 
3 Attempts to define causal dependence by conditioning on the entire past (e.g., Suppes, 1970) vi-
olate the statistical requirement of limiting the analysis to "observed variables", and encounter other 
insunnountable difficulties (see Eells [1991], Pearl [2000a], pp. 249-257). 
4By "untested" I mean untested using frequency data in nonexperimental studies. 

22 
JUDEA PEARL 
2.1 
The Barrier of Untested Assumptions 
All statistical studies are based on some untested assumptions. For examples, we 
often assume that variables are multivariate normal, that the density function has 
certain smoothness properties, or that a certain parameter falls in a given range. 
The question thus arises why innocent causal assumptions, say, that symptoms do 
not cause disease or that mud does not cause rain, invite mistrust and resistance 
among statisticians, especially of the Bayesian school. 
There are three fundamental differences between statistical and causal assump-
tions. First, statistical assumptions, even untested, are testable in principle, given 
sufficiently large sample and sufficiently fine measurements. Causal assumptions, 
in contrast, cannot be verified even in principle, unless one resorts to experimental 
control. This difference is especially accentuated in Bayesian analysis. Though the 
priors that Bayesians commonly assign to statistical parameters are untested quan-
tities, the sensitivity to these priors tends to diminish with increasing sample size. 
In contrast, sensitivity to priors of causal parameters, say those measuring the ef-
fect of smoking on lung cancer, remains non-zero regardless of (nonexperimental) 
sample size. 
Second, statistical assumptions can be expressed in the familiar language of 
probability calculus, and thus assume an aura of scholarship and scientific re-
spectability. Causal assumptions, as we have seen before, are deprived of that 
honor, and thus become immediate suspect of informal, anecdotal or metaphysical 
thinking. Again, this difference becomes illuminated among Bayesians, who are 
accustomed to accepting untested, judgmental assumptions, and should therefore 
invite causal assumptions with open arms-they don't. A Bayesian is prepared 
to accept an expert's judgment, however esoteric and untestable, so long as the 
judgment is wrapped in the safety blanket of a probability expression. Bayesians 
turn extremely suspicious when that same judgment is cast in plain English, as in 
"mud does not cause rain." A typical example can be seen in Lindley and Novick's 
[1981] treatment of Simpson's paradox. 
Lindley and Novick showed that decisions on whether to use conditional or 
marginal contingency tables should depend on the story behind the tables, that 
is, on one's assumption about how the tables were generated. For example, to 
decide whether a treatment X = x is beneficial (Y = y) in a population, one 
should compare ~zP(ylx, z) to ~zP(ylx', z) if Z stands for the gender of pa-
tients. In contrast, if Z stands for a factor that is affected by the treatment (say 
blood pressure), one should compare the marginal probabilities, P(ylx) vis-a-vis 
P(ylx'), and refrain from conditioning on Z (see [Pearl, 2000a; pp. 174-182] for 
details). Remarkably, instead of attributing this difference to the causal relation-
ships in the story, Lindley and Novick wrote: "We have not chosen to do this; nor 
to discuss causation, because the concept, although widely used, does not seem to 
be well-defined" (p. 51). Thus, instead of discussing causation, they attribute the 
change in strategy to another untestable relationship in the story-exchangeability 
[DeFinetti, 1974] which is cognitively formidable yet, at least formally, can be 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
23 
cast in a probability expression. In Section 4.2, we will return to discuss this trend 
among Bayesians of equating "definability" with expressibility in probabilistic lan-
guage. 
The third resistance to causal (vis-a-vis statistical) assumptions stems from their 
intimidating clarity. Assumptions about abstract properties of density functions 
or about conditional independencies among variables are, cognitively speaking, 
rather opaque, hence they tend to be forgiven, rather than debated. In contrast, as-
sumptions about how variables cause one another are shockingly transparent, and 
tend therefore to invite counter-arguments and counter-hypotheses. A co-reviewer 
on a paper I have read recently offered the following objection to the causal model 
postulated by the author: 
"A thoughtful and knowledgeable epidemiologist could write down 
two or more equally plausible models that leads to different conclu-
sions regarding confounding." 
Indeed, since the bulk of scientific knowledge is organized in causal schema, sci-
entists are incredibly creative in constructing competing alternatives to any causal 
hypothesis, however plausible. Statistical hypotheses in contrast, having been sev-
eral levels removed from our store of knowledge, are relatively protected from 
such challenges. 
I conclude this subsection with a suggestion that statisticians' suspicion of 
causal assumptions, vis-a-vis probabilistic assumptions, is unjustified. Consider-
ing the organization of scientific knowledge, it makes prefect sense that we permit 
scientists to articulate what they know in plain causal expressions, and not force 
them to compromise reliability by converting to the "higher level" language of 
prior probabilities, conditional independence and other cognitively unfriendly ter-
minology.5 
2.2 
The Barrier of New Notation 
If reluctance to making causal assumptions has been a hindrance to causal anal-
ysis, finding a mathematical way of expressing such assumptions encountered a 
formidable mental block. The need to adopt a new notation, foreign to the province 
of probability theory, has been traumatic to most persons trained in statistics; partly 
because the adaptation of a new language is difficult in general, and partly because 
statisticians have been accustomed to assuming that all phenomena, processes, 
thoughts, and modes of inference can be captured in the powerful language of 
probability theory.6 
5Similar observations were expressed by J. Heckman [2001]. 
6Commenting on my set(x) notation [Pearl, 1995a, b], a leading statistician wrote: "Is this a 
concept in some new theory of probability or expectation? If so, please provide it. Otherwise, 'meta-
physics' may remain the leading explanation." Another statistician, commenting on the do(x) notation 
used in Causality [Pearl, 2000al, insisted: " ... the calculus of probability is the calculus of causality." 

24 
JUDEA PEARL 
Not surprisingly, in the bulk of the statistical literature, causal claims never 
appear in the mathematics. They surface only in the verbal interpretation that in-
vestigators occasionally attach to certain associations, and in the verbal description 
with which investigators justify assumptions. For example, the assumption that a 
covariate is not affected by a treatment, a necessary assumption for the control 
of confounding [Cox, 1958], is expressed in plain English, not in a mathematical 
equation. 
In some applications (e.g., epidemiology), the absence of notational distinction 
between causal and statistical dependencies seemed unnecessary, because investi-
gators were able to keep such distinctions implicitly in their heads, and managed 
to confine the mathematics to conventional probability expressions. In others, as 
in economics and the social sciences, investigators rebelled against this notational 
tyranny by leaving mainstream statistics and constructing their own mathematical 
machinery (called Structural Equations Models). Unfortunately, this machinery 
has remained a mystery to outsiders, and eventually became a mystery to insiders 
as welP 
But such tensions could not remain dormant forever. "Every science is only so 
far exact as it knows how to express one thing by one sign," wrote Augustus de 
Morgan in 1858 -
the harsh consequences of not having the signs for expressing 
causality surfaced in the 1980-90's. Problems such as the control of confound-
ing, the estimation of treatment effects, the distinction between direct and indirect 
effects, the estimation of probability of causation, and the combination of experi-
mental and nonexperimental data became a source of endless disputes among the 
users of statistics, and statisticians could not come to the rescue. [Pearl, 2000a] de-
scribes several such disputes, and why they could not be resolved by conventional 
statistical methodology. 
3 LANGUAGES FOR CAUSAL ANALYSIS 
3.1 
The language of diagrams and structural equations 
How can one express mathematically the common understanding that symptoms 
do not cause diseases? The earliest attempt to formulate such relationship mathe-
matically was made in the 1920's by the geneticist Sewall Wright [1921]. Wright 
used a combination of equations and graphs to communicate causal relationships. 
For example, if X stands for a disease variable and Y stands for a certain symptom 
of the disease, Wright would write a linear equation: 
(1) 
y = ax + u 
supplemented with the diagram X ----+ Y, where x stands for the level (or sever-
ity) of the disease, y stands for the level (or severity) of the symptom, and u stands 
7Most econometric texts in the last decade have refrained from defining what an economic model 
is, and those that attempted a definition, erroneously view structural equations models as compact 
representations of probability density functions (see [Pearl, 2000a, pp. 135-138]). 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
25 
for all factors, other than the disease in question, that could possibly affect Y (U 
is called "exogenous", "background", or "disturbance".) The diagram encodes 
the possible existence of (direct) causal influence of X on Y, and the absence of 
causal influence of Y on X, while the equation encodes the quantitative relation-
ships among the variables involved, to be determined from the data. The parameter 
a in the equation is called a "path coefficient" and it quantifies the (direct) causal 
effect of X on Y; given the numerical value of a, the equation claims that, ceteras 
paribus, a unit increase in X would result in an a-unit increase of Y. If correlation 
between X and U is presumed possible, it is customary to add a double arrow 
between X and Y. 
The asymmetry induced by the diagram renders the equality sign in Eq. (1) dif-
ferent from algebraic equality, resembling instead the assignment symbol ( := ) in 
programming languages. Indeed, the distinctive characteristic of structural equa-
tions, setting them apart from algebraic equations, is that they stand for a value-
assignment process -
an autonomous mechanism by which the value of Y (not 
X) is determined. In this assignment process, Y is committed to track changes in 
X, while X is not subject to such commitment. 8 
Wright's major contribution to causal analysis, aside from introducing the lan-
guage of path diagrams, has been the development of graphical rules for writing 
down (by inspection) the covariance of any pair of observed variables in terms 
of path coefficients and of covariances among disturbances. Under certain causal 
assumptions, (e.g. if Cov(U, X) = 0), the resulting equations may allow one to 
solve for the path coefficients in terms of observed covariance terms only, and this 
amounts to inferring the magnitude of (direct) causal effects from observed, non-
experimental associations, assuming of course that one is prepared to defend the 
causal assumptions encoded in the diagram. 
The causal assumptions embodied in the diagram (e.g, the absence of arrow 
from Y to X, or Cov(U, X) = 0) are not generally testable from nonexperimental 
data. However, the fact that each causal assumption in isolation cannot be tested 
does not mean that the sum total of all causal assumptions in a model does not 
have testable implications. The chain model X 
----+ Y 
----+ 
Z for exam-
ple, encodes seven causal assumptions, each corresponding to a missing arrow or 
a missing double-arrow between a pair of variables. None of those assumptions 
is testable in isolation, yet the totality of all those assumptions implies that Z is 
un associated with X, conditioned on Y. Such testable implications can be read off 
the diagrams (see [Pearl 2000a, pp. 16-19]), and these constitute the only open-
ing through which the assumption embodies in structural equation models can be 
tested in observational studies. Every conceivable statistical test that can be ap-
plied to the model is entailed by those implications. 
8Clearly, if we intervene on X, Y would continue to track changes in X. Not so when we intervene 
on Y, X will reman unchanged. Such intervention (on Y) would alter the assignment mechanism for 
Y and, naturally, would cause the equality in Eq. (I) to be violated. 

26 
JUDEA PEARL 
3.2 
From path-diagrams to do-calculus 
Structural equation modeling (SEM) has been the main vehicle for causal analysis 
in economics, and the behavioral and social sciences [Goldberger 1972; Duncan 
1975]. However, the bulk of SEM methodology was developed for linear anal-
ysis and, until recently, no comparable methodology has been devised to extend 
its capabilities to models involving discrete variables, nonlinear dependencies, or 
situations in which the functional form of the equations is unknown. A central 
requirement for any such extension is to detach the notion of "effect" from its al-
gebraic representation as a coefficient in an equation, and redefine "effect" as a 
general capacity to transmit changes among variables. One such extension, based 
on simulating hypothetical interventions in the model, is presented in Pearl [1995a, 
2000a] 
The central idea is to exploit the invariant characteristics of structural equations 
without committing to a specific functional form. For example, the non-parametric 
interpretation of the chain model Z --+ X --+ Y corresponds to a set of three 
functions, each corresponding to one of the variables: 
z = fz(w) 
(2) 
x = fx(z, v) 
y = jy(x,u) 
together with the assumption that the background variables W,V, U (not shown 
in the chain) are jointly independent but, otherwise, arbitrarily distributed. Each 
of these functions represents a causal process (or mechanism) that determines the 
value of the left variable (output) from those on the right variables (input). The ab-
sence of a variable from the right hand side of an equation encodes the assumption 
that it has no direct effect on the left variable. For example, the absence of variable 
Z from the arguments of fy indicates that variations in Z will leave Y unchanged, 
as long as variables U and X remain constant. A system of such functions are said 
to be structural (or modular) if they are assumed to be autonomous, that is, each 
function is invariant to possible changes in the form of the other functions [Simon 
1953; Koopmans 1953]. 
This feature of invariance permits us to use structural equations as a basis for 
modeling actions and counterfactuals. This is done through a mathematical oper-
ator called do(x) which simulates physical interventions by deleting certain func-
tions from the model, replacing them by constants, while keeping the rest of the 
model unchanged. For example, to represent an intervention that sets the value of 
X to Xo the model for Eq. (2) would become 
z = fz(w) 
(3) 
x = Xo 
y = jy(x,u) 
The distribution of Y and Z calculated from this modified model characterizes 
the effect of the action do(X = xo) and is denoted as P(y, zldo(xo)). It is 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
27 
not hard to show that, as expected, the model of Eq. (2) yields P(yldo(xo)) = 
P(ylxo) and P(zldo(xo)) = P(z) regardless of the functions lx, Jy and Iz. 
The general rule is simply to remove from the factorized distribution P(x, y, z) = 
P(z)P(xlz)P(ylx) the factor that corresponds to the manipulated variable (X in 
our example) and to substitute the new value of that variable (xo in our exam-
ple) into the truncated expression -
the resulting expression then gives the post-
intervention distribution of the remaining variables [Pearl, 2000a; section 3.2]. Ad-
ditional features of this transformation are discussed in the Appendix; see [Pearl, 
2000a; chapter 7] for full details. 
The main task of causal analysis is to infer causal quantities from two sources 
of information: (i) the assumptions embodied in the model, and (ii) the observed 
distribution P(x, y, z), or from samples of that distribution. Such analysis requires 
mathematical means of transforming causal quantities, represented by expressions 
such as P(yldo(x)), into do-free expressions derivable from P(z, x, y), since only 
do-free expressions are estimable from non-experimental data. When such a trans-
formation is feasible, we say that the causal quantity is identifiable. A calculus 
for performing such transformations, called do-calculus, was developed in [Pearl, 
1995a1. Remarkably, the rules governing this calculus depend merely on the topol-
ogy of the diagram; it takes no notice of the functional form of the equations, nor 
of the distribution of the disturbance terms. This calculus permits the investigator 
to inspect the causal diagram and 
1. Decide whether the assumptions embodied in the model are sufficient to 
obtain consistent estimates of the target quantity; 
2. Derive (if the answer to item 1 is affirmative) a closed-form expression for 
the target quantity in terms of distributions of observed quantities; and 
3. Suggest (if the answer to item 1 is negative) a set of observations and ex-
periments that, if performed, would render a consistent estimate feasible. 
4 ON THE DEFINITION OF CAUSALITY 
In this section, I return to discuss concerns expressed by some Bayesians that 
causality is an undefined concept and that, although the do-calculus can be an ef-
fective mathematical tool in certain tasks, it does not bring us closer to the deep 
and ultimate understanding of causality, one that is based solely on classical prob-
ability theory. 
4.1 
Is causality reducible to probabilities? 
Unfortunately, aspirations for reducing causality to probability are both untenable 
and unwarranted. Philosophers have given up such aspirations twenty years ago, 

28 
JUDEA PEARL 
and were forced to admit extra-probabilistic primitives (such as "counterfactuals" 
or "causal relevance") into the analysis of causation (see Eells [1991] and Pearl 
[2000a, Section 7.5]). The basic reason was alluded to in Section 2: probability 
theory deals with beliefs about an uncertain, yet static world, while causality deals 
with changes that occur in the world itself, (or in one's theory of such changes). 
More specifically, causality deals with how probability functions change in re-
sponse to influences (e.g., new conditions or interventions) that originate from 
outside the probability space, while probability theory, even when given a fully 
specified joint density function on all (temporally-indexed) variables in the space, 
cannot tell us how that function would change under such external influences. 
Thus, "doing" is not reducible to "seeing", and there is no point trying to fuse 
the two together. 
Many philosophers have aspired to show that the calculus of probabilities, en-
dowed with a time dynamic, would be sufficient for causation [Suppes, 1970]. A 
well known demonstration of the impossibility of such reduction (following Otte 
[1981]) goes as follows. Consider a switch X that turns on two lights, Y and Z, 
and assume that, due to differences in location, Z turns on a split second before 
Y. Consider now a variant of this example where the switch X activates Z, and 
Z, in turns, activates Y. This case is probabilistically identical to the previous one, 
because all functional and temporal relationships are identical. Yet few people 
would perceive the causal relationships to be the same in the two situations; the 
latter represents cascaded process, X --t Z --t Y, while the former represents 
a branching process, Y +-- X --t Z. The difference shows, of course, when we 
consider interventions; intervening on Z would affect Y in the cascaded case, but 
not in the branching case. 
The preceding example illustrates the essential role of mechanisms in defining 
causation. In the branching case, although all three variables are symmetrically 
constrained by the functional relationships: X = Y, X = Z, Z = Y, these 
relationships in themselves do not reveal the information that the three equalities 
are sustained by only two mechanisms, Y = X and Z = X, and that the first 
equality would still be sustained when the second is violated. A set of mechanisms, 
each represented by an equation, is not equivalent to the set of algebraic equations 
that are implied by those mechanisms. Mathematically, the latter is defined as one 
set of n equations, whereas the former is defined as n separate sets, each containing 
one equation. These are two distinct mathematical objects that admit two distinct 
types of solution-preserving operations. The calculus of causality deals with the 
dynamics of such modular systems of equations, where the addition and deletion 
of equations represent interventions (see Appendix). 
4.2 
Is causality well-defined? 
From a mathematical perspective, it is a mistake to say that causality is unde-
fined. The do-calculus, for example, is based on two well-defined mathemati-
cal objects: a probability function P and a directed acyclic graph (DAG) D; the 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
29 
first is standard in statistical analysis while the second is a newcomer that tells 
us (in a qualitative, yet formal language) which mechanisms would remain invari-
ant to a given intervention. Given these two mathematical objects, the definition 
of "cause" is clear and crisp; variable X is a probabilistic-cause of variable Y if 
P(yldo(x)) f. P(y) for some values x and y. Since each of P(yldo(x)) and P(y) 
is well-defined in terms of the pair (P, D), the relation "probabilistic cause" is, 
likewise, well-defined. Similar definitions can be constructed for other nuances 
of causal discourse, for example, "causal effect", "direct cause", "indirect cause", 
"event-to-event cause", "scenario-specific cause", "necessary cause", "sufficient 
cause", "likely cause" and "actual cause" (see [Pearl, 2000a, pp. 222-3, 286-7, 
319]; some of these definitions invoke functional models). 
Not all statisticians/philosophers are satisfied with these mathematical defini-
tions. Some suspect definitions that are based on unfamiliar non-algebraic objects 
(i.e., the DAG) and some mistrust abstract definitions that are based on unverifiable 
models. Indeed, no mathematical machinery can ever verify whether a given DAG 
really represents the causal mechanisms that generate the data -
such verification 
is left either to human judgment or to experimental studies that invoke interven-
tions. I submit, however, that neither suspicion nor mistrust are justified in the case 
at hand; DAGs are no less formal than mathematical equations, and questions of 
model verification need be kept apart from those of conceptual definition. 
Consider, for example, the concept of a distribution mean. Even non-Bayesians 
perceive this notion to be well-defined, for it can be computed from any given (non-
pathological) distribution function, even before ensuring that we can estimate that 
distribution from the data. We would certainly not declare the mean "ill-defined" 
if, for any reason, we find it hard to estimate the distribution from the available 
data. Quite the contrary; by defining the mean in the abstract, as a functional of 
any hypothetical distribution, we can often prove that the defining distribution need 
not be estimated at all, and that the mean can be estimated (consistently) directly 
from the data. Remarkably, by taking seriously the abstract (and untestable) notion 
of a distribution, we obtain a license to ignore it. An analogous logic applies to 
causation. Causal quantities are first defined in the abstract, using the pair (P, D), 
and this abstract definition then provides a theoretical framework for deciding, 
given the type of data available, which of the assumptions embodied in the DAG 
are ignorable, and which are absolutely necessary for establishing the target causal 
quantity from the data.9 
The separation between concept definition and model verification is even more 
pronounced in the Bayesian framework, wllere purely judgmental concepts, such 
as the prior distribution of the mean, are perfectly acceptable, as long as they can 
be assessed reliably from one's experience or knowledge. Dennis Lindley has re-
marked recently (personal communication) that "causal mechanisms may be easier 
91 have used a similar logic in defense of counterfactuals [Pearl, 2000a] , which Dawid [2000] 
deemed dangerous on account of being untestable. (See, also Dawid [2001], this volume.) Had 
Bernoulli been constrained by Dawid's precautions, the notion of a "distribution" would have had 
to wait for another "dangerous" scientist, of Bernoulli's equal, to be created. 

30 
JUDEA PEARL 
to come by than one might initially think". Indeed, from a Bayesian perspective, 
the newcomer concept of a DAG is not an alien at all -
it is at least as legitimate 
as the probability assessments that a Bayesian decision-maker pronounces in con-
structing a decision tree. In such construction, the probabilities that are assigned 
to branches emanating from a decision variable X correspond to assessments of 
P(yldo(x)) and those assigned to branches emanating from a chance variable X 
correspond to assessments of P(ylx). If a Bayesian decision-maker is free to as-
sess P(ylx) and P(yldo(x)) in any way, as separate evaluations, the Bayesian 
should also be permitted to express hislher conception of the mechanisms that en-
tail those evaluations. It is only by envisioning these mechanisms that a decision 
maker can generate a coherent list of such a vast number of P(yldo(x)) type as-
sessments. lO The structure of the DAG can certainly be recovered from judgments 
of the form P(yldo(x)) and, conversely, the DAG combined with a probability 
function P dictates all judgments of the form P(y I do( x)). Accordingly the struc-
ture of the DAG can be viewed as a qualitative parsimonious scheme of encoding 
and maintaining coherence among those assessments. And there is no need to 
translate the DAG into the language of probabilities to render the analysis legiti-
mate. Adding probabilistic veneer to the mechanisms portrayed in the DAG may 
make the do calculus appear more traditional, but would not change the fact that 
the objects of assessment are still causal mechanisms, and that these objects have 
their own special grammar of generating predictions about the effect of actions. In 
summary, recalling the ultimate Bayesian mission of fusing judgment with data, it 
is not the language in which we cast judgments that legitimizes the analysis, but 
whether those judgments can reliably be assessed from our store of knowledge and 
from the peculiar form in which this knowledge is organized. 
If it were not for this concern to maintain reliability (of judgment), one could 
easily translate the information conveyed in a DAG into purely probabilistic formu-
lae, using hypothetical variables. (Translation rules are provided in [Pearl, 2000a, 
p. 232]). Indeed, this is how the potential-outcome approach of Neyman [1923] 
and Rubin [1974] has achieved statistical legitimacy: judgments about causal re-
lationships among observables are expressed as statements about probability func-
tions that involve mixtures of observable and counterfactual variables. The diffi-
culty with this approach, and the main reason for its slow acceptance in statistics, 
is that judgments about counterfactuals are much harder to assess than judgments 
about causal mechanisms. For instance, to communicate the simple assumption 
that symptoms do not cause diseases, we would have to use a rather roundabout 
expression and say that the probability of the counterfactual event "disease had 
symptoms been absent" is equal to the probability of "disease had symptoms been 
present". Judgments of conditional independencies among such counterfactual 
events are even harder for researchers to comprehend or to evaluate. 
IOCoherence requires, f~r example, that for any x, y, and z, the inequality P(yldo(x), do(z)) ~ 
Ply, xldo(z)) be satisfied. This follows from the property of composition (see Appendix, Eq. (6), or 
[Pearl, 2000a; pp. 229] 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
31 
5 SUMMARY 
This paper calls attention to a basic conflict between mission and practice in 
Bayesian methodology. The mission is to express prior knowledge mathemati-
cally and reliably so as to assist the interpretation of data, hence the acquisition of 
new knowledge. The practice has been to express prior knowledge as prior proba-
bilities -
too crude a vocabulary, given the grand mission. Considerations of re-
liability (of judgment) call for enriching the language of probabilities with causal 
vocabulary and for admitting causal judgments into the Bayesian repertoire. The 
mathematics for interpreting causal judgments has matured, and tools for using 
such judgments in the acquisition of new knowledge have been developed. The 
grounds are now ready for mission-oriented Bayesianism. 
APPENDIX 
CAUSAL MODELS, ACTIONS AND COUNTERFACTUALS 
This appendix presents a brief summary of the structural-equation semantics of 
causation and counterfactuals as defined in Balke and Pearl [1995], Galles and 
Pearl [1997, 1998], and Halpern [1998]. For detailed exposition of the structural 
account and its applications see [Pearl, 2000a). 
Causal models are generalizations of the structural equations used in engineer-
ing, biology, economics and social science. I I World knowledge is represented as a 
modular collection of stable and autonomous relationships called "mechanisms", 
each represented as a function, and changes due to interventions or unmodelled 
eventualities are treated as local modifications of these functions. 
A causal model is a mathematical object that assigns truth values to sentences 
involving causal relationships, actions, and counterfactuals. We will first define 
causal models, then discuss how causal sentences are evaluated in such models. 
We will restrict our discussion to recursive (or feedback-free) models; extensions 
to non-recursive models can be found in Galles and Pearl [1997, 1998] and Halpern 
[1998]. 
DEFINITION 1 (Causal model). 
A causal model is a triple 
M = (U,V,F) 
where 
(i) U is a set of variables, called exogenous. (These variables will represent back-
ground conditions, that is, variables whose values are determined outside 
the model.) 
11 Similar models, called "neuron diagrams" [Lewis, 1986, p. 200; Hall, 1998] are used informally 
by philosophers to illustrate chains of causal processes. 

32 
JUDEA PEARL 
(ii) V is an ordered set {VI, V2 , â¢â¢â¢ , Vn } of variables, called endogenous. (These 
represent variables that are determined in the model, namely, by variables in 
UUV.) 
(iii) F is a set of functions {h, h, ... , In} where each Ii is a mapping from 
U x (VI X ... x Vi-d to Vi, In other words, each Ii tells us the value of 
Vi given the values of U and all predecessors of Vi, Symbolically, the set of 
equations F can be represented by writing12 
where pai is any realization of the unique minimal set of variables P Ai in 
V (connoting parents) sufficient for representing liP Likewise, Ui ~ U 
stands for the unique minimal set of variables in U that is sufficient for 
representing Ii. 
Every causal model M can be associated with a directed graph, G(M), in which 
each node corresponds to a variable in V and the directed edges point from mem-
bers of P Ai toward Vi (by convention, the exogenous variables are usually not 
shown explicitly in the graph). We call such a graph the causal graph associated 
with M. This graph merely identifies the endogenous variables P Ai that have 
direct influence on each Vi but it does not specify the functional form of k 
For any causal model, we can define an action operator, do( x), which, from a 
conceptual viewpoint, simulates the effect of external action that sets the value of 
X to x and, from a formal viewpoint, transforms the model into a submodel, that 
is, a causal model containing fewer functions. 
DEFINITION 2 (Submodel). 
Let M be a causal model, X be a set of variables in V, and x be a particular 
assignment of values to the variables in X. A submodel Mx of M is the causal 
model 
where 
(4) 
Fx = {Ii: Vi ~ X} U {X = x} 
In words, Fx is formed by deleting from F all functions Ii corresponding to mem-
bers of set X and replacing them with the set of constant functions X = x. 
If we interpret each function Ii in F as an independent physical mechanism 
and define the action do(X = x) as the minimal change in M required to make 
12We use capital letters (e.g., X, Y) as names of variables and sets of variables, and lower-case 
letters (e.g., x, y) for specific values (called realizations) of the corresponding variables. 
13 A set of variables X is sufficient for representing a given function y = ! (x, z) if ! is trivial in 
Z-that is, iffor every x, z, z' we have !(x, z) = !(x, z'). 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
33 
x = X hold true under any u, then Mx represents the model that results from 
such a minimal change, since it differs from M by only those mechanisms that 
directly determine the variables in X. The transformation from M to Mx modifies 
the algebraic content of F, which is the reason for the name modifiable structural 
equations used in [Galles and Pearl, 1998].14 
DEFINITION 3 (Effect of action). 
Let M be a causal model, X be a set of variables in V, and x be a particular 
realization of X. The effect of action do( X = x) on M is given by the submodel 
MxÂ· 
DEFINITION 4 (Potential response). 
Let Y be a variable in V, let X be a subset of V, and let u be a particular value 
of U. The potential response of Y to action do(X = x) in situation u, denoted 
Yx (u), is the (unique) solution for Y of the set of equations Fx. 
We will confine our attention to actions in the form of do( X = x). Conditi onal 
actions, of the form "do(X = x) if Z = z" can be formalized using the re(ace-
ment of equations by functions of Z, rather than by constants [Pearl, 1994]. We 
will not consider disjunctive actions, of the form "do(X = x or X = x')", since 
these complicate the probabilistic treatment of counterfactuals. 
DEFINITION 5 (Counterfactual). 
Let Y be a variable in V, and let X be a subset of V. The counterfactual expression 
"The value that Y would have obtained, had X been x" is interpreted as denoting 
the potential response Yx ( u ) . 
Definition 5 thus interprets the counterfactual phrase "had X been x" in terms 
of a hypothetical external action that modifies the actual course of history and im-
poses the condition "X = x" with minimal change of mechanisms. This is a cru-
cial step in the semantics of counterfactuals [Balke and Pearl, 1994], as it permits 
x to differ from the actual value X (u) of X without creating logical contradiction; 
it also suppresses abductive inferences (or backtracking) from the counterfactual 
antecedent X = X. 15 
It can be shown [Galles and Pearl, 1997] that the counterfactual relationship 
just defined, Yx (u), satisfies the following two properties: 
Effectiveness: 
For any two disjoint sets of variables, Y and W, we have 
(5) 
Yyw(u) = y. 
14StruCturaI modifications date back 10 Marschak [1950] and Simon [1953]. An explicit translation 
of interventions into "wiping out" equations from the model was first proposed by Strotz and Wold 
[1960] and later used in Fisher [1970], Sobel [1990], Spirtes et aI. [1993], and Pearl [1995]. A similar 
notion of sub-model is introduced in Fine [1985], though not specifically for representing actions and 
counterfactuals. 
IS Simon and Rescher [1966, p. 339] did not include this step in their account of counterfactuals and 
noted that backward inferences triggered by the antecedents can lead 10 ambiguous interpretations. 

34 
JUDEA PEARL 
In words, setting the variables in W to w has no effect on Y, once we set the value 
ofY to y. 
Composition: 
For any two disjoint sets of variables X and W, and any set of variables Y, 
In words, once we set X to x, setting the variables in W to the same values, w, that 
they would attain (under x) should have no effect on Y. Furthermore, effectiveness 
and composition are complete whenever M is recursive (i.e., G(M) is acyclic) 
[Galles and Pearl, 1998; Halpern, 1998], that is, every property of counterfactu-
als that follows from the structural model semantics can be derived by repeated 
application of effectiveness and composition. 
A corollary of composition is a property called consistency by [Robins, 1987]: 
(7) 
(X(u) = x) ==> (Yx(u) = Y(u)) 
Consistency states that, if in a certain context u we find variable X at value x, and 
we intervene and set X to that same value, x, we should not expect any change in 
the response variable Y. Composition and consistency are used in several deriva-
tions of Section 3. 
The structural formulation generalizes naturally to probabilistic systems, as is 
seen below. 
DEFINITION 6 (Probabilistic causal model). 
A probabilistic causal model is a pair 
(M,P(u)) 
where M is a causal model and P(u) is a probability function defined over the 
domainofU. 
P(u), together with the fact that each endogenous variable is a function of U, 
defines a probability distribution over the endogenous variables. That is, for every 
set of variables Y ~ V, we have 
a 
(8) 
P(y) = P(Y = y) = 
P(u) 
{u I Y(u)=y} 
The probability of counterfactual statements is defined in the same manner, through 
the function Yx(u) induced by the submodel Mx. For example, the causal effect 
of X on Y is defined as: 
(9) 
P(Yx = y) = 
P(u) 
{u I Y.(u)=y} 
Likewise, a probabilistic causal model defines a joint distribution on counter-
factual statements, i.e., P(Yx = y, Zw = z) is defined for any sets of variables 
Y,X,Z, W, not necessarily disjoint. In particular, P(Yx = y,X = x') and 
P(Yx = y, Yx' = y') are well defined for x # x', and are given by 

BAYESIANISM AND CAUSALITY, OR, WHY I AM ONLY A HALF-BAYESIAN 
35 
(10) P(Yx = y, X = X') = 
P(u) 
{uIY.(u)=y & X(u)=x'} 
and 
(11) P(Yx = y, Yx' = y') = 
P(u). 
{u I Y.(u)=y & Y.,(u)=y'} 
When x and x' are incompatible, Yx and Yx' cannot be measured simultane-
ously, and it may seem meaningless to attribute probability to the joint statement 
"Y would be y if X = x and Y would be y' if X = x'." Such concerns have 
been a source of recent objections to treating counterfactuals as jointly distributed 
random variables [Dawid, 2000]. The definition of Yx and Yx' in terms of two 
distinct submodels, driven by a standard probability space over U, demonstrates 
that joint probabilities of counterfactuals have solid mathematical and conceptual 
underpinning and, moreover, these probabilities can be encoded rather parsimo-
niously using P(u) and F. 
Computer Science Department, University of California, USA. 
BIBLIOGRAPHY 
[Balke and Pearl, 1994] A. Balke and J. Pearl. Probabilistic evaluation of counterfactual queries. In 
Proceedings of the Twelfth National Conference on Artificial Intelligence, volume I, pages 230-237. 
MIT Press, Menlo Park, CA, 1994. 
[Balke and Pearl, 1995] A. Balke and J. Pearl. Counterfactuals and policy analysis in structural mod-
els. In P. Besnard and S. Hanks, editors, Uncertainty in Artificial Intelligence 11, pages 11-18. 
Morgan Kaufmann, San Francisco, 1995. 
[Cartwright, 1989] N. Cartwright. Nature's Capacities and Their Measurement. Clarendon Press, 
Oxford,1989. 
[Cox,1958] D.R. Cox. The Planning oJExperiments. John Wiley and Sons, NY, 1958. 
[Dawid,2000] A.P. Dawid. Causal inference without counterfactuals (with comments and rejoinder). 
Journal of the American Statistical Association, 95(450):407-448, June 2000. 
[DeFinetti, 1974] B. DeFinetti. Theory of Probability: A Critical Introductory Treatment, 2 volumes 
(Translated by A. Machi and A. Smith). Wiley, London, 1974. 
[Duncan,1975] O.D. Duncan. Introduction to Structural Equation Models. Academic Press, New 
York,1975. 
[Eells,1991l E. Eells. Probabilistic Causality. Cambridge University Press, Cambridge, MA, 1991. 
[Fine, 1985] K. Fine. Reasoning with Arbitrary Objects. B. Blackwell, New York, 1985. 
[Fisher, 1970] F.M. Fisher. A correspondence principle for simultaneous equations models. Econo-
metrica, 38(1):73-92, January 1970. 
[Galles and Pearl, 1997] D. Galles and J. Pearl. Axioms of causal relevance. Artificial Intelligence, 
97(1-2):9-43,1997. 
[Galles and Pearl, 1998] D. Galles and J. Pearl. An axiomatic characterization of causal counterfac-
tuals. Foundation of Science, 3(1):151-182,1998. 
[Goldberger, 1972] A.S. Goldberger. Structural equation models in the social sciences. Econometrica: 
Journal of the Econometric Society, 40:979-1001, 1972. 
[Hall, 1998] N. Hall. Two concepts of causation, 1998. In press. 
[Halpern, 1998] J.Y. Halpern. Axiomatizing causal reasoning. In G.F. Cooper and S. Moral, editors, 
Uncertainty in Artificial Intelligence, pages 202-210. Morgan Kaufmann, San Francisco, CA, 1998. 
[Heckman,2001l J.J. Heckman. Econometrics and empirical economics. Journal of Econometrics, 
100(1): 1-5,2001. 

36 
JUDEA PEARL 
[Koopmans, 1953] T.e. Koopmans. Identification problems in econometric model construction. In 
w.e. Hood and T.e. Koopmans, editors, Studies in Econometric Method, pages 27-48. Wiley, New 
York, 1953. 
[Lewis, 1986] D. Lewis. Philosophical Papers. Oxford University Press, New York, 1986. 
[Lindley and Novick, 1981] D.Y. Lindley and M.R. Novick. The role of exchangeability in inference. 
The Annals of Statistics, 9(1 ):45-58, 1981. 
[Marschak, 1950] J. Marschak. Statistical inference in economics. In T. Koopmans, editor, Statistical 
Inference in Dynamic Economic Models, pages I-50. Wiley, New York, 1950. Cowles Commission 
for Research in Economics, Monograph 10. 
[Neyman,I923] J. Neyman. On the application of probability theory to agricultural experiments. 
Essay on principles. Section 9. Statistical Science, 5(4):465-480, 1990. [Translation] 
[Otte,1981] R.Otte. A critique of Suppes' theory of probabilistic causality. Synthese, 48:167-189, 
1981. 
[Pearl,1994] J. Pearl. A probabilistic calculus of actions. In R. Lopez de Mantaras and D. Poole, 
editors, Uncertainty in Artificial Intelligence 10, pages 454-462. Morgan Kaufmann, San Mateo, 
CA,1994. 
[Pearl,1995a] 1. Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669-710, Decem-
ber 1995. 
[Pearl, 1995b] 1. Pearl. 
Causal inference from indirect experiments. 
Artificial Intelligence in 
Medicine, 7(6):561-582,1995. 
[Pearl,2oooa] 1. Pearl. Causality: Models, Reasoning, and lriference. Cambridge University Press, 
New York, 2000. 
[Pearl, 2ooob] 1. Pearl. Comment on A.P. Dawid's, Causal inference without counterfactuals. Journal 
of the American Statistical Association, 95(450):428-431, June 2000. 
[Robins, 1987] J.M. Robins. A graphical approach to the identification and estimation of causal 
parameters in mortality studies with sustained exposure periods. Journal of Chronic Diseases, 
40(SuppI2):139S-161S, 1987. 
[Rubin, 1974] D.B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized 
studies. Journal of Educational Psychology, 66:688-701, 1974. 
[Savage, 1962] L. J. Savage. The Foundations of Statistical Inference. Methuen and Co. Ltd., London, 
1962. 
[Simon and Rescher, 1966] H.A. Simon and N. Rescher. Cause and counterfactual. Philosophy and 
Science, 33:323-340, 1966. 
[Simon, 1953] H.A. Simon. Causal ordering and identifiability. In Wm. e. Hood and T.C. Kooprnans, 
editors, Studies in Econometric Method, pages 49-74. Wiley and Sons, Inc., 1953. 
[Sobel, 1990] M.E. Sobel. Effect analysis and causation in linear structural equation models. Psy-
chometrika, 55(3):495-515,1990. 
[Spirtes et aI., 1993] P. Spirtes, e. Glymour, and R. Scheines. Causation, Prediction, and Search. 
Springer-Verlag, New York, 1993. 
[Strotz and Wold, 1960] R.H. Strotz and H.O.A. Wold. Recursive versus nonrecursive systems: An 
attempt at synthesis. Econometrica, 28:417-427,1960. 
[Suppes, 1970] P. Suppes. A Probabilistic Theory of Causality. North-Holland Publishing Co., Ams-
terdam, 1970. 
[Wright, 19211 S. Wright. Correlation and causation. Journal of Agricultural Research, 20:557-585, 
1921. 

A. PHILIP DAWID 
CAUSAL INFERENCE WITHOUT 
COUNTERFACTUALS 
PART I: INTRODUCTION 
CAUSAL MODELLING 
Association is not causation. Many have held that Statistics, while well suited to 
investigate the former, strays into treacherous waters when it makes claims to say 
anything meaningful about the latter. Yet others have proceeded as if inference 
about the causes of observed phenomena were indeed a valid object of statisti-
cal enquiry; and it is certainly a great temptation for statisticians to attempt such 
'causal inference'. Among those who have taken the logic of causal statistical 
inference seriously I mention in particular Rubin (1974, 1978), Holland (1986), 
Robins (1986, 1987), Pearl (l995a) and Shafer (1996). This paper represents my 
own attempt to contribute to the debate as to what are the appropriate statistical 
models and methods to use for causal inference, and what causal conclusions can 
be justified by statistical analysis. 
There are many philosophical and statistical approaches to understanding and 
uncovering causation, and I shall not here attempt to attack the problem on a broad 
front. Attention will be confined to a simple decision-based understanding of cau-
sation, wherein an external agent can make interventions in, and observe various 
properties of, some system. Rubin (1978) and Heckerman and Shachter (1995), 
among others, have emphasized the importance of a clear decision-theoretic de-
scription of a causal problem. Understanding of the 'causal effects' of intervention 
will come through the building, testing and application of causal models, relating 
interventions, responses and other variables. 
In my own view, the enterprise of causal statistical modelling is not essentially 
different from any other kind of statistical modelling, and is most satisfactorily 
understood from a Popperian hypothetico-deductive viewpoint. A model is not 
a straightforward reflection of external reality, and to propose a model is not to 
assert or to believe that Nature behaves in a particular way (Nature is surely utterly 
indifferent to our attempts to ensnare her in our theories). Rather, a model is a 
construct within the mental universe, by means of which we attempt somehow 
to describe certain, more or less restricted, aspects of the empirical universe. In 
order to do this we need to have a clear understanding of the semantics of such 
a description. This involves setting up a clear correspondence between the very 
different features of these two universes. In particular, we require very clear (if 
possibly implicit) understandings of: 
37 
D. Corfield and 1. Williamson (eds.), Foundations of Bayesianism, 37-74. 
Â© 2001 Kluwer Academic Publishers. 

38 
A. PHILIP DAWID 
â¢ What the system modelled is (and so, in particular, how to distinguish a valid 
from an invalid instance of the system). 
â¢ What real world quantities are represented by variables appearing in the 
model. 
â¢ What an intervention involves. 'Setting' a patient's treatment to 'none' by 
(a) withholding it from him, (b) wiring his jaw shut, or (c) killing him are 
all very different interventions, with different effects, and must be modelled 
as such. We must also be clear as to what variables are affected by the 
intervention, directly or indirectly, and how. 
â¢ What is meant by replication (in time, space, ... ). 
In addition, it is vital that we have clearly defined methods for understanding, 
assessing and measuring the empirical success of any such attempt at description 
of the real world by a mathematical modell . 
So long as a model appears to describe the relevant aspects of the world satis-
factorily, we may continue, cautiously, to use it; when it fails to do so, we need to 
search for a better one. In particular, any causal understandings we may feel we 
have attained must always be treated as tentative, and subject to revision should 
further observation of the world require it. 
To be fully general we should consider models for complex problems, such as 
those discussed by Robins (1986), Pearl (1995a), wherein interventions, of vari-
ous kinds, are possible at various points in a system, with effects that can cascade 
through a collection of variables. While such problems can be modelled and anal-
ysed (using structures such as influence diagrams) within the general philosophical 
and methodological framework of this paper, that would involve additional theo-
retical development. To keep things simple, we restrict attention here to systems 
on which it is possible to make a single external intervention, which we refer to 
as treatment, and observe a single eventual response. We also suppose, with no 
further real loss of generality, that there are just two treatments available. Another 
restriction, that could again be relaxed at the cost of further elaboration, is that we 
shall not here address the important and challenging problems arising from non-
ignorable treatment assignment or observational studies (e.g. Rubin, 1974, 1978). 
See, however, Â§8.1 for some related analysis. 
2 COUNTERFACTUALS 
Much recent analysis of causal inference is grounded in the manipulation of coun-
terfactuals. Philosophically, a counterfactual statement is an assertion of the form 
"If X had been the case, then Y would have happened", made when it is known to 
lOne approach to such understanding and assessment in the case of ordinary probability modelling, 
based on the concept of probability calibration, may be found in Dawid (1985). 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
39 
be false that X is the case. In a famous historical counterfactual, Pascal (Pensees, 
1669, Â§162), opines: 
Le nez de Cleopatre: s'il eut ete plus court, toute la face de la terre 
aurait change. 
More recent, an intriguing, seemingly self-referring, assertion is that of Shafer 
(1996, p.108): 
Were counterfactuals to have objective meaning, we might take them 
as basic, and define probability and causality in terms of them. 
It is one of the aims of this paper to persuade the reader of the genuinely counter-
factual nature of this claim. 
An archetype of the use of counterfactuals in a causal statistical context is the 
assertion: "If only I had taken aspirin, my headache would have gone by now". It is 
implicit that I did not take aspirin, and I still have the headache. Such an assertion, 
if true, could be regarded as justifying an inference that not taking aspirin has 
'caused' my headache to persist this long; and that, if I had taken aspirin, that 
would have 'caused' my headache to disappear by now. The assignment of cause 
is thus based on a comparison of the real and the counterfactual outcome. 
If YA denotes the duration of my headache when I take aspirin, and YA: its du-
ration when I don't, the above assertion is of the form "YA: > y, YA < y", and 
relates jointly to the pair of values for (YA, YA:)' An important question, which 
motivates much of the development in this paper, is to what extent such assertions 
can be validated or refuted by empirical observation. My approach is grounded 
in a Popperian philosophy, in which the meaningfulness of a purportedly scien-
tific theory, proposition, quantity or concept is related to the implications it has for 
what is or could be observed, and, in particular, to the extent to which it is possible 
to conceive of data that would be affected by the truth of the proposition, or the 
value of the quantity. When this is the case, assertions are empirically refutable, 
and considered 'scientific'. When not so, they may be branded 'metaphysical'. I 
shall argue that counterfactual theories are essentially metaphysical. This in itself 
might not be automatic grounds for rejection of such a theory, if the causal infer-
ences it led to were unaffected by the metaphysical assumptions embodied in it. 
Unfortunately, this is not so, and the answers which the approach delivers to its 
inferential questions are seen, on closer analysis, to be dependent on the validity 
of assumptions that are entirely untestable, even in principle. This can lead to 
distorted understandings and undesirable practical consequences. 
3 TWO PROBLEMS 
There are several different problems of causal inference, which are often conftated. 
In particular, I consider it important to distinguish between causal queries of the 
following two different types (Holland, 1986): 

40 
A. PHILIP DAWID 
I "I have a headache. Will it help if I take aspirin?" 
II "My headache has gone. Is it because I took aspirin?" 
Query I requires inference about the effects of causes, i.e. comparisons among 
the expected consequences of various possible interventions in a system. Such 
queries have long been the focus of the bulk of the standard statistical theory of 
Experimental Design (which, it is worth remarking, has in general displayed little 
eagerness for counterfactual analyses). Query II, by contrast, relates to causes of 
effects: we seek to understand the causal relationship between an already observed 
outcome and an earlier intervention. Queries of this second kind might arise in 
legal inquiries, for example into whether responsibility for a particular claimant's 
leukremia can be attributed to the fact that her father worked in a nuclear power 
station for 23 years. The distinction between queries I and II is closely related 
to that sometimes made between problems of general and of singular causation 
(Hitchcock, 1997), although, in our formulation, both queries relate to singular 
circumstances. 
I consider both types of query valid and important, but they are different, and, as 
we shall see, require different, though related, treatments. Evidence, e.g. findings 
from epidemiological surveys, that is directly relevant to query I, is often used, 
inappropriately, to address query II, without careful attention to the difference be-
tween the queries. 
4 PREVIEW 
I first consider, in Part II, the problem of 'effects of causes'. Section 5 introduces 
the essential ingredients of the problem, and distinguishes two varieties of model: 
a metaphysical model, which allows direct formulation of counterfactual quantities 
and queries, and a physical model, which does not. By means of a simple running 
example I illustrate how certain inferences based on a metaphysical model are 
not completely determined by the data, however extensive, but remain sensitive to 
untestable additional assumptions. I also delimit the extent of the resulting arbi-
trariness. In Section 6, I describe an entirely different approach, based on physical 
modelling and decision analysis, and show how it delivers an unambiguous con-
clusion, avoiding the above problems. Section 7 questions the role of an implicit 
attitude of 'fatalism' in some counterfactual causal models and methods. Section 8 
extends the discussion to cases in which additional covariate information is avail-
able on individual systems. In Section 9, I investigate whether certain analyses 
stemming from a counterfactual approach might nevertheless be acceptable for 
'physical' purposes; examples are given of both possible answers. Section 10 asks 
whether it might ever be strictly advantageous to base physical analyses on a meta-
physical structure. This appears to be sometimes the case for causal modelling, but 
arguably not so for causal inference. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
41 
In Part III, I turn to address the distinct problem of 'causes of effects'. For this, 
purely physical modelling appears inadequate, and the arbitrariness already iden-
tified in metaphysical modelling becomes a much more serious problem. I show in 
Section 11 how this arbitrariness can be reduced by taking account of concomitant 
variables. Section 12 introduces a convention of conditional independence across 
alternative universes, which helps to clarify the counterfactual inference, and pos-
sibly reduce the intrinsic ambiguity. Section 13 considers the possibility of using 
underlying deterministic relations to clarify causal questions and inferences. I 
argue that, to be useful, these must involve genuine concomitant variables. A con-
trast is drawn with 'pseudo-deterministic models', which are always available in 
the counterfactual framework. These do have a deterministic mathematical struc-
ture, but need not involve true concomitants. Such a purely formal structure, I 
argue, is not enough to support meaningful inferences about the causes of effects. 
In Section 14, I discuss more deeply the meaning of concomitance, and argue that 
this is partly a matter of convention, relative to a specific causal inquiry, rather than 
a property of the physical world. 
The general message of this paper is that inferences based on counterfactual 
assumptions and models are generally unhelpful and frequently plain mislead-
ing. Alternative approaches can avoid these problems, while continuing to address 
meaningful causal questions. For inference about the effects of causes, a straight-
forward 'black-box' decision-analytic approach, based on models and quantities 
which are empirically testable and discoverable, is perfectly adequate. For infer-
ence about the causes of effects, we need to suit our causal models to the questions 
addressed as well as to the empirical world, and to seek understanding of the re-
lationships between observed variables and possibly unobserved, but empirically 
meaningful, concomitant variables. The causal inferences which are justified by 
empirical findings will still, in general, retain a degree of arbitrariness and conven-
tion, which should be fully admitted. 
PART II: EFFECTS OF CAUSES 
5 COMPARISON OF TREATMENTS: COUNTERFACTUAL APPROACH 
As a simple and familiar setting to discuss and contrast different approaches to 
inference about the effects of causes, I investigate the problem of making compar-
isons between two treatments, t and c, for example, aspirin and placebo control, 
on the basis of an experiment. In this section, I shall consider counterfactual ap-
proaches to this problem, and show how they can produce ambiguous answers, 
unless arbitrary and unverifiable assumptions are imposed. 
Consider a large homogeneous population U of clearly distinguishable individ-
uals, or systems, or (as we shall generally call them) units, U, to each of which 
we can choose to apply anyone treatment, i, out of the treatment set T = {t, c}, 

42 
A. PHILIP DAWID 
and observe the resulting response, Y. Once one treatment has been applied, the 
other treatment can no longer be applied. This property can be ensured by appro-
priate definition of experimental unit u (e.g. headache episode rather than patient) 
and treatment (combinations of treatments, if available, being redefined as new 
treatments). 
Experimentation consists in selecting disjoint sets of units Ui ~ U (i = t, c), 
applying treatment i to each unit in Ui , and observing the ensuing responses (e.g., 
time for the headache to disappear). The experimental units might be selected for 
treatment by some form of randomisation, but this is inessential to our argument. 
For further clarification of the argument, I shall assume that the treatment groups 
are sufficiently large that all inferential problems associated with finite sampling 
can be ignored. 
Homogeneity of the population is an intuitive concept, which can be formalised 
in a number of ways. From a classical standpoint, the individuals might be re-
garded as drawn randomly and independently from some large population; a 
Bayesian might regard them as exchangeable. In this context, homogeneity is also 
taken to imply that no specific information is available on the units that might serve 
to distinguish one from another (this constraint will be relaxed in Â§8). In particular, 
the experimenter is unable to take any such information into account, either delib-
erately or inadvertently, in deciding which treatment a particular unit is to receive. 
To render this scenario more realistic and versatile, suppose that we did in fact 
have additional measured covariate information on each unit, determined by (but 
not uniquely identifying) that unit. Then we could confine attention to a subpop-
ulation having certain fixed covariate values, and this subpopulation might then 
be reasonably regarded as homogeneous. That is, our discussion should be under-
stood as applying at the level of the residual variation, after all relevant observed 
covariates have been allowed for. (We can then also allow treatment assignment to 
take these observed covariates into account.) 
Counterfactual framework. 
The counterfactual approach to causal analysis for 
this problem focuses on the collection of potential responses Y := (Yi(u) : i E 
T, u E U), where Yi(u) is intended to denote "the response that would be ob-
served if treatment i were assigned to unit u". One can consider Y as arranged in 
a two-way layout of treatments by units, with }j(u) occupying the cell for row i 
and column u. Note that many of the variables in Yare (to borrow a term from 
Quantum Physics) complementary, in that they are not simultaneously observable. 
Specifically, for any unit u, one can observe Yi(u) for at most one treatment i. The 
assignment of treatments to units will determine just which (if any) of these com-
plementary variables are to be observed, yielding a collection X of responses that 
I call a physical array -
in contrast with the metaphysical array y. Although the 
full collection Y is intrinsically unobservable, counterfactual analyses are based 
on consideration of all the (Yi (u)) simultaneously. Current interest in the counter-
factual approach was instigated by Rubin (1974, 1978), although it can be traced 
back at least to Neyman (1935; see also Neyman, 1923). 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
43 
5.1 
Metaphysical model 
What kind of models is it reasonable to entertain for the metaphysical array Y? 
The assumption of homogeneity essentially requires us to model the various pairs 
(yt(u), Ye(u)), for u E U, as independent and identically distributed, given their 
(typically unknown) bivariate distribution P. I shall denote the implied marginal 
distributions for yt and Ye by Pf, Pc respectively. It is important to note that the 
full bivariate distribution P is not completely specified by these marginals, without 
further specification of the dependence between yt and Ye. 
Although the major points of our discussion will apply to a general model of the 
above form, for definiteness I shall concentrate on the following specific bivariate 
normal model. 
EXAMPLE 1. 
The pairs {(yt (u), Ye (u)) : u E U} are modelled as independent and 
identically distributed, each having the bivariate normal distribution 
with means (Bt, Be), common variance rPy, and correlation p. 
When p ~ 0, which seems a reasonable judgment2 one can also represent this 
structure by means of the mixed model: 
(1) 
Yi(u) = Bi + f3(u) + 'Yi(U), 
where all the (f3(u)) and (')'i(U)) are mutually independent normal random vari-
ables, with mean 0 and respective variances rP{3 := PrPy and rP"( := (1 - p )rPyÂ· 
Inversely, one could start with (1) as our model, in which case we have 
(2) 
rPy 
(3) 
p 
In the usual parlance of the Analysis of Variance, Model (1) expresses Yi (u) as 
composed of: a fixed treatment effect Bi associated with the applied treatment i, 
common to all units; a random unit effect f3(u), unique to unit u, but common to 
both treatments; and a random unit-treatment interaction, 'Yi (u), varying from one 
treatment application to another, even on the same unit. (This last term could also 
be interpreted as incorporating intrinsic random variation, which can not be distin-
guished from interaction since replicate observations on Yi (u) are impossible.) 
5.2 
Causal effect 
The counterfactual approach typically takes as the fundamental object of causal 
inference the individual causal effect: a suitable numerical comparison, for a given 
2See Â§ 12. One can also regard (I) as a (fictitious) representation of the bivariate normal model even 
when p < 0, in which case we must have -Â¢y :S Â¢{3 :S 0 and 0 :S Â¢, :S 2Â¢y. Then the calculations 
below, though based on this fictitious representation, are still valid. 

44 
A. PHILIP DAWID 
unit, between the various potential responses it would exhibit, under the various 
treatments that might be applied. Note that such a quantity is meaningless unless 
one regards the several potential responses, complementary though they are, as 
having simultaneous existence. 
The individual causal effect (ICE) for unit U will here be identified with the 
difference 
(4) 
T(U):= yt(u) - Ye(u). 
Alternative possibilities might be logyt(u) -log Ye(u), or yt(u)jYe(u). There 
seems no obvious theoretical reason, within this framework, to prefer anyone such 
comparison to any other, the choice perhaps being made according to one's under-
standing of the applied context and the type of inferential conclusion desired. But, 
however defined, an ICE involves direct comparison of complementary quantities, 
and is thus intrinsically unobservable. 
In most studies, the specific units used in the experiment are of no special inter-
est in themselves, but merely provide a basis for inference about generic properties 
of units under the influence of the various treatments. For this purpose, it is help-
ful to conceive of an entirely new test unit, Uo, from the same population, that has 
not yet been treated; and to regard the purpose of the experiment as to assist us in 
making the decision as to which treatment to apply to it. If we decide on treatment 
t, we shall obtain response yt(uo); if c, we shall obtain Ye(UO)' We thus need to 
make inference about these two quantities, and compare them somehow. Note that, 
although yt(uo) and Ye(UO) are complementary, neither is (as yet) counterfactual. 
The counterfactual approach might focus on the ICE T( uo) = yt( uo) - Ye(UO), 
or a suitable variation thereon. Under model (1), we have 
(5) 
T(U) =T+A(U), 
with T := et - ee, the average causal effect (ACE), and A(U) := 'Yt(u) - 'Ye(U), 
the residual causal effect, having distribution 
(6) 
A(U) '" N(O, 2Â¢,). 
Thus 
(7) 
T(U) '" N(T, 2Â¢,). 
This model holds, in particular, for the inferential target T(UO)' Since T(UO) is 
probabilistically independent of any data on the units in the experiment, inference 
about T( uo) essentially reduces to inference about the pair (T, Â¢,). 
5.3 Physical model 
Suppose a particular experimental assignment has been specified. Label, arbitrar-
ily, the units receiving treatment i as un , Ui2 ... , Ui n .. Then the observed response 
on unit Uij will be Xij := 1";( Uij). The collection (Xij : i = t, c; j = 1, ... , ni) 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
45 
constitutes the physical array X. The mean response on all units receiving treat-
.. X-
1 ",ni X 
mentzls 
i:= ni L..Jj=l 
ijÂ· 
It follows trivially from the model assumptions of Example 1 that the joint 
distribution over X is described by: 
independently for all (i, j). Equivalently, from (1), 
(9) 
Xij = Bi + Eij, 
with Eij := f3( Uij) + 'Yi( Uij) '" N(O, Â¢y), independently for all (i, j). 
Now to the extent that the model (1) says anything about the empirical world, 
this has to be fully captured in the implied models (8) (one such for each pos-
sible physical array). Clearly, from extensive data having the structure (8), one 
can identify Bt , Be and Â¢y; but the individual components Â¢{3 and Â¢-y in (2) -
or, 
equivalently, the correlation p satisfying (3) -
are not identifiable: we have intrin-
sic aliasing (McCullagh and NeIder, 1989, Â§3.5) of unit effect and unit-treatment 
interaction. So far as the desired inference about T(UO) is concerned, we can iden-
tify its mean, T = ACE, in (7). However its variance, 2Â¢-y, is not identifiable from 
the data, beyond the requirement Â¢-y ~ Â¢y (if we restrict to p 2: 0, or Â¢-y ~ 2Â¢y 
for p unrestricted). 
5.4 A quandary 
Here we have an inferential quandary. Consider two statisticians, both of whom 
believe in model (1). However, statistician SI further assumes that Â¢{3 = 0 (p = 
0), and statistician S2 that Â¢-y = 0 (p = 1). Both SI and S2 accept model (8) 
for the physical array, with no further constraints on its parameters. Extensive 
data, assumed to be fully consistent with model (8) for the physical array, lead to 
essentially exact estimates of(lt, Be and Â¢y. However, S 1 infers Â¢f3 = 0, Â¢-y = Â¢y, 
while S2 has Â¢f3 = Â¢y, Â¢-y = O. When they come to inference about T(UO), from 
(7), they will agree on its mean, T, but differ about its variance, 2Â¢-y. A third 
statistician, making different assumptions (e.g. Â¢{3 = Â¢-Y' equivalent to p = ~) 
will come to yet another distinct conclusion. Is it not worrying that models that are 
intrinsically indistinguishable, on the basis of any data that could ever be observed, 
can lead to such different inferences? How can we possibly choose between these 
inferences? 
The above state of affairs is clearly in violation of what, in another context 
(Dawid, 1984, Â§5.2), I have called Jeffreys's Law: the requirement that mathemat-
ically distinct models that cannot be distinguished on the basis of empirical obser-
vation should lead to indistinguishable inferences. This property can be demon-
strated mathematically in cases where those inferences concern future observables; 
and I consider that it has just as much intuitive force in the present context of causal 
inference. 

46 
A. PHILIP DAWID 
There is one important, but very special, case where the above ambiguity van-
ishes: when Â¢y is essentially zero, and hence so are both Â¢(3 and Â¢'Y' In this case, 
the units are not merely homogeneous, but uniform, in that, for each i, Yj(u) is 
the same for all units u. The property Â¢y == 0 can, of course, be investigated 
empirically, and might be regarded as a distinguishing feature of at least some 
problems in the 'hard' sciences. When it holds, one can, in effect, observe both 
yt (u) and Yc (u) simultaneously, by employing distinct units, thus enabling direct 
measurement of causal effects. I shall further consider this case of uniformity, and 
its extensions, in Â§ 13 below. 
5.5 Additional constraints 
How should we proceed if we do not have uniformity? It is common in studies 
based on counterfactual models to impose additional constraints. In the present 
context, a common additional constraint is that of treatment-unit additivity (TVA), 
which asserts that T(U) in (4) is the same for all u E U. In terms of 0), this 
is equivalent to Â¢'Y = 0 (p = 1), and leads to a simple inference: T(Uo) = T, 
with no further uncertainty (T having been identified, from a large experiment, as 
Xt - Xc). However, as pointed out above, there is simply no way that TVA can be 
tested on the basis of any empirically observable data in the context of model (1); 
and it is intuitively clear that the same holds for any other models that might be 
considered: when, for each pair (yt (u), Yc (u)), it is never possible to observe both 
components, how can one ever assess empirically the assertion that yt (u) - Yc (u) 
(unobservable for each u) is the same for all u?3 
A similar untestable assumption commonly made in the case of binary re-
sponses (Imbens and Angrist, 1994) is monotonicity, which requires that P(Yc = 
1, yt = 0) = 0 (where the response 1 represents a successful, and 0 an unsuccess-
ful, outcome). 
5.6 What can we say? 
If we are to restrict our inferences to those that are justified by the data, without 
the imposition of untestable additional constraints, the most that can be said about 
T(UO) (assuming model (1Â» is: 
(10) T(UO) '" N(T,2Â¢'Y)' 
with T estimated precisely, but Â¢'Y subject only to the inequality 0 ::; Â¢'Y ::; Â¢y,4 
whose right-hand-side only is estimated precisely. Only if we are fortunate enough 
3 If we were to have used a more general model in Example I, whereby we allowed the variance to 
be different for two responses, say Â¢t and Â¢c, then TVA does have the testable implication Â¢t = Â¢c, 
and so could be rejected on the basis of data casting doubt on this property. But such data would still not 
distinguish between TVA and any of the other models considered above, all of which would likewise 
be rejected. We have assumed thoughout that the data are consistent with the physical model (8), so 
that this issue does not arise. 
40r O::::: Â¢,::::: 2Â¢y if we allow p < O. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
47 
to find that <jJy is negligible (the situation of uniformity) can we obtain an unam-
biguous inference for T ( uo). 
A very similar analysis can be conducted for other metaphysical models. Al-
though the physical model only allows us to identify the marginal distributions 
Pt and Pc of the joint distribution P, the distribution of an individual causal ef-
fect (however defined) will depend further on the dependence structure of p.5 
Consequently, even when we have conducted very large experiments, we cannot 
make unambiguous inferences about such causal effects without making further 
untestable assumptions, such as TUA or monotonicity. 
There are two contrasting morals that may be drawn from the above analysis, 
both grounded in the principle that we should be careful not to make 'metaphysical 
inferences', sensitive to assumptions that can not be put to empirical test. Moral 1 
is that inference about individual causal effects should be carefully circumscribed, 
as below (10). Alternatively, we might draw the more revolutionary Moral 2: that 
if we can not get a sensible answer to our question, perhaps the question itself, 
with its focus on inference for T( uo), is not well posed. In the next section I shall 
reformulate the question in an entirely different manner, that does allow a clear 
and unambiguous answer. 
6 DECISION-ANALYTIC APPROACH 
As we have seen in the above example, the principal difficulty in the counterfactual 
approach is that the desired inference depends on the joint probability structure of 
the complementary variables (yt(u), Yc(u)), whereas one is only ever able to ob-
serve (at most) one of these for each u. We can, however, consistently estimate 
both marginal distributions Pt, Pc. Can we put these separate marginal distribu-
tions to good use? 
I take a straightforward Bayesian decision-analytic approach (see e.g. Raiffa, 
1968). We have to decide whether to apply treatment t or treatment c to a new 
unit Uo. The marginal distributions Pt and Pc of yt and of Yc having been identi-
fied, from our extensive experimental data on each separate treatment group, these 
now express the appropriate predictive uncertainty about the response on Uo, con-
ditional on its being given t or c. The consequence (loss) of our decision may be 
measured by some function L (.) of the eventual yield Y. The decision tree for this 
problem is given in Figure 1. 
At node Vf, Y '" Pf, and the (negative) value of being at Vt is measured by the 
expected loss Ept {L(Y)}. Similarly, Vc has value Epc {L(Y)}. The principles of 
Bayesian decision analysis now require that, at the decision node Vo, we choose 
that treatment i leading to the smaller expected loss. 
Note that, whatever loss function is used, this solution only involves the two 
identifiable marginal distributions, Pc and Pt. In particular, our statisticians SI 
5There is a large literature on properties, and inequalities for, joint distributions with known 
marginals: see e.g. Riischendorf et al. (1996). 

48 
A. PHILIP DAWID 
Vt 
y 
â¢ L(y) 
t 
Y-R t 
Vo 
C 
y-p c 
y ----- L(y) 
Vc 
Figure 1. Decision tree 
and S2 of Â§5.4, who agree on model (1) and obtain common estimates of (h, ()c and 
(jJy, while disagreeing about p, will be led to the identical decision. It simply does 
not matter that S2 believes that the time for a headache to disappear if aspirin is 
taken will be exactly 10 minutes less than if it is not taken, while S 1 regards the 
difference of these times as uncertain, although again with expectation 10 minutes; 
there is no way in which such differences in beliefs can affect the decision problem. 
It is only for simplicity of the argument that I have assumed the experiment 
large enough to allow full identification of Pt and Pc. With a more limited ex-
periment, we could either replace these with suitable estimates, or, for a whole-
heartedly Bayesian approach, use the appropriate predictive distributions for the 
response on Uo (under either hypothetical treatment application, separately), given 
the experimental data. 
Our analysis extends readily to the case where we consider a number of future 
units, and want to decide how to apply treatments to them. In a quality control 
setting, our loss might be a combination of the sample mean and variance of all 
the responses, for example. 
One can also consider models for more complex problems, involving non-
homogeneous populations. For example, Dawid (1988) used symmetry arguments 
to justify the construction of certain random-effect type models for complex exper-
imentallayouts, generalising models such as those of (1) for the metaphysical array 
or (9) for the physical array. In the general case, we again need to use the data of 
the experiment to make appropriate predictive inferences for test units, under vary-
ing hypothetical treatment assignments; but these predictive inferences will now 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
49 
be more complex, and will also depend upon the relationship assumed between 
the test units and the experimental units. For example, if our experiment involved 
planting different varieties of cereal on plots (units) nested within blocks nested 
within fields, and recording their yields, one might wish to consider predictions 
for the yield, of each variety, if planted on: a new plot in an old (i.e. experimental) 
block in an old field; a plot in a new block in an old field; or (more usefully) a plot 
in a new field. So long as our models relate the responses of the new and the old 
units (under arbitrary treatment assignments), and so support the required predic-
tive inferences, one can conduct whatever decision-analytic analysis appears most 
relevant to one's purpose, eschewing counterfactuals entirely. 
7 FATALISM 
Many counterfactual analyses are based, explicitly or implicitly, on an attitude that 
I term fatalism. This conceives of the various potential responses Yi (u), when 
treatment i is applied to unit u, as pre-determined attributes of unit u, waiting only 
to be uncovered by suitable experimentation. (It is implicit that that the unit u, 
and its properties and propensities, exist independently of, and are unaffected by, 
any treatment that may be applied.) Note that, since each unit label u is regarded 
as individual and unrepeatable, there is never any possibility of empirically testing 
this assumption of fatalism, which can thus be categorised as metaphysical. 
The fatalistic world-view runs very much counter to the philosophy underlying 
statistical modelling and inference in almost every other setting. For example, it 
leaves no scope for introducing realistic stochastic effects of external influences 
acting between the times of application of treatment and of the response. Any ac-
count of causation that requires us to jettison all our familiar statistical framework 
and machinery should be treated with the utmost suspicion, unless and until it has 
shown itself completely indispensable for its purpose. 
7.1 
Some fatalistic concepts 
I do not wish to give the impression that all counterfactual analyses must be fatalis-
tic; there are notable exceptions, for example Robins and Greenland (1989). How-
ever, it is a very natural bed-fellow of counterfactual inference, much of which 
can not proceed without it. For example, only if one takes a fatalistic attitude 
does it make sense even to talk of such properties as 'treatment-unit additivity' or 
'monotonicity' (Â§8). 
A fundamental use of fatalism underlies certain counterfactual analyses of treat-
ment non-compliance (see e.g. Imbens and Rubin, 1997), where each patient is 
supposed categorisable as: a complier (who would take the treatment if prescribed, 
and not take it if not prescribed); a defier (not take it if prescribed, take it if not 
prescribed); an always taker (take it, whether or not prescribed); or a never taker 
(not take it, whether or not prescribed). Some causal inferences are based on con-

50 
A. PHILIP DAWID 
sideration of the responses to treatment of, say, the group of compliers. However, 
it is only under the unrealistic assumption of fatalism that this group has any mean-
ingful identity, and thus only in this case could such inferences even begin to have 
any useful content. 
SVTVA 
An assumption that has often been considered essential to useful causal inferences 
is the stable unit-treatment value assumption (SUTVA) (Rubin, 1980, 1986). To 
describe this, we have to start from a more general metaphysical model of the ef-
fect of experimentation on responses, wherein the response Ye (u) of unit u could 
in principle depend on the full treatment assignment ~ over all units, not just the 
specific treatment i applied to u. Then SUTVA requires that, in fact, this poten-
tial complicating feature is absent, so that we can replace Yd u) by Yi (u), thus 
returning us to the situation already considered. But again, without the fatalistic 
assumption of pre-existing values of the (Ye (u)), for any assignment ~, it is not 
possible to make sense of SUTVA (but see Â§10.1 for a non-fatalistic reinterpreta-
tion of SUTVA). 
Decision analysis and fatalism 
By contrast, the decision-analytic approach requires no commitment to (or, for 
that matter, against) fatalism. There is no conceptual or mathematical difficulty 
in regarding the probability distributions of the response (i.e. Pt and Pc in Exam-
ple 1) as incorporating further uncontrollable influences, over and above effects 
attributable directly to treatment. So far as SUTVA is concerned, the decision an-
alyst has no need of it. In the context of Example 1, SUTVA can be replaced by 
the much weaker assumption that the application of treatments does not destroy 
the homogeneity of the units, beyond the obvious difference that some will now 
have one treatment, some another. Then we will still have complete homogeneity 
of the responses for all units (experimental or future) receiving the same treatment, 
and can thus use the experimental data to identify the distribution, Pi, of response 
within treatment group i, which also expresses our uncertainty about the response 
Yi (uo) of a new unit uo, if it were given treatment i. Hence we are still in a position 
to set up, and solve, the basic decision problem for uo. 
8 USE OF ADDITIONAL INFORMATION 
Now suppose that it is possible to gather, or at least to conceive of gathering, 
additional information about individual units, which might be used to refine our 
uncertainties about their responses to treatments. Any such information can be 
described in terms of a generic variable K, determined by a measurement protocol 
which, when applied to unit u, leads to a measurement K (u). For present purposes 
we restrict our attention to generic variables that are covariates, i.e. features of 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
51 
units that can be observed prior to experimentation. Nevertheless, before it is 
observed each K (u) must be treated as a random variable. 
There are several cases to consider, according as whether or not the covariates 
are observed on the experimental units, and/or on test units. 
1. Covariates on experimental and test units. Suppose that a covariate K is 
measured on all experimental units, and also that, for a test unit uo, K(uo) 
will be measured before the treatment decision has to be made. 
If K takes values in a finite set, we can simply restrict attention to the sub-
set (assumed large) of the experimental units for which K(u) = K(uo). 
Then we essentially recover the homogeneous population problem that has 
already been analysed. 
Otherwise, or if the above restricted subset is not sufficiently large, we can 
conduct appropriate statistical modelling. In a counterfactual treatment, we 
would need to model a joint conditional distribution of (Ye, yt) given K; 
for the decision-analytic treatment, we only need to use the data to assess 
and compare the associated predictive distributions of Y(uo) given K(uo), 
for each treatment. Again, the decision-analytic approach, in contrast to the 
counterfactual approach, is essentially insensitive to any further assumptions 
about, or modelling of, the joint distribution of potential responses. 
2. Covariates on experimental units only. In this case, it is appropriate to ig-
nore altogether the covariate information on the experimental units -
except 
that, when the experiment is not large, modelling of this more detailed infor-
mation might enhance the accuracy of estimation of the required marginal 
predictive distributions of Y (uo) for each treatment. 
3. Covariate on test unit only. This is more problematic, since even for the 
less demanding decision-analytic approach the experiment gives no direct 
information about the required predictive distributions of response given co-
variate and treatment. Whichever approach one takes, there is no escape 
from the fact that the solution will be highly dependent on untested (though 
in principle testable) assumptions about these distributions. One possibility 
would be to ignore K( uo) altogether, but this is itself tantamount to an em-
pirically untested assumption of independence between K and Y for each 
treatment. In any event, however one proceeds, there is no advantage to be 
gained from the introduction of counterfactuals. Similar comments apply 
when information of differing extents is available on the experimental and 
test units. 
8.1 
Alternatives to additivity 
One argument that can be made for the need for a metaphysical assumption such 
as treatment-unit additivity (Â§5.5) is the following. An experiment (e.g. a clinical 

52 
A. PHILIP DAWID 
trial) will often have very specific inclusion criteria that render the experimental 
units non-representative of the population to which it is intended to generalise the 
findings. Then, although we may still have homogeneity of units within the exper-
iment, it might no longer be reasonable to regard the test unit Uo as exchangeable 
with the experimental units. But if we can assume TVA, so that Yt (u) - Ye (u) == 7 
for all units, experimental and test, then our estimate of the treatment effect 7 from 
the experiment will still be applicable to uo. Thus counterfactual analysis based on 
TVA appears unaffected by this modification to the framework. For the decision-
analytic approach, however, the required separate predictive inferences about the 
response Y(uo), given either treatment, for a test unit uo, would be simultane-
ously more complicated and less reliable when the experimental units cannot be 
regarded as representative of the test units. 
However, there is an alternative way of proceeding that avoids metaphysical 
assumptions. For each unit u let Q (u) be a variable, taking values 0, t, c, generated 
by the experimenter as part of the process of designing his experiment. He intends 
to include u in the experiment, and apply treatment t to it, if Q(u) = t; to include 
u in the experiment, and apply treatment e to it, if Q(u) = e; and to exclude u 
from the experiment if Q(u) = O. These intentions do not, however, preclude 
us from considering other possibilities; we can, for example, meaningfully assess 
probabilistic uncertainty about Y (u), given that the assignment Q (u) = t has been 
made, on the hypothesis that u will receive treatment e. 
We assume that, for some covariate K, the distribution of Q(u) given K(u) is 
the same for all units u. Thus K is the information that the experimenter takes ac-
count of in generating Q, and so embodies the inclusion and treatment criteria. The 
distribution of Q given K is supposed unaffected by further conditioning on the 
applied treatment i and the eventual response Y: using the notation and properties 
of conditional independence (Dawid, 1979), Q ll(i, Y)IK, whence 
(11) Y llQIK, i. 
Consider now the following model assumption: 
(12) E(YIK, i) = Bi + ,(K) 
(i = t,e), 
for some unknown parameters Bt , Be and parametric function ,0. If this holds, 
define 7 = Bt - Be. 
Note that, by (11), the left-hand side of (12) is unaffected by further condition-
ing on Q. In particular, (12) implies: E{YIK, i, Q = i} = Bi + ,(K) (i = t, e), 
so that, for any k, 
(13) E{YIK = k, t, Q = t} - E{YIK = k, e, Q = e} == 7. 
Conversely, (13), with (11), implies (12). But we can estimate E{YIK = k, i, Q = 
i} straightforwardly from the measurements of covariate K and outcome Yon the 
set of experimental units to which treatment i has been applied. Consequently 
property (12) is testable from the experimental data, and, if it can be assumed to 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
53 
hold, the parameter T is estimable (a simple unbiased estimator of T is given by 
the difference of the mean responses for the two treated groups). 
Also we can compare hypothetical treatment applications on a test unit uo, with 
observed K(uo) = k and, by construction, Q(uo) = 0, as follows: 
E{Y(uo)IK(uo) = k, t} - E{Y(uo)IK(uo) = k, c} 
E{YIK = k, t, Q = O} - E{YIK = k, c, Q = O} 
= E{YIK = k, t} - E{YIK = k, c}, 
on again using (12). But this is just T, as identified from the experiment. (If K(uo) 
is not observed, we have to take a further expectation over K, but this clearly has 
no effect.) 
The above approach, based on the testable assumption (12) rather than the meta-
physical assumption of TUA, thus allows us to generalise readily from the exper-
iment to the target population, even in the face of differential selection and treat-
ment criteria. 
It has been assumed in the above that it is appropriate to focus directly on the ex-
pected response. In the general framework of Â§6, with a loss function L, we could 
replace E(Y) by E{ L(Y)} throughout (a counterfactual analysis would similarly 
require that TUA be modified to: L{yt(u)} - L{Ye(u)} == T, all u). 
9 SHEEP AND GOATS 
I have argued that any elements of a theory that have no observable or testable 
consequences (for example, TUA) are to be regarded as metaphysical, and, in 
accordance with Jeffreys's Law, should not be permitted to have any inferential 
consequences, either. Causal analyses can be classified into sheep (those obeying 
this dictum) and goats (the rest). We have seen that the decision-analytic approach 
is a sheep. 
What of the counterfactual approach? It certainly has the potential to generate 
goats. In particular, any inference that is dependent on assumptions requiring the 
acceptance of fatalism (for example TUA, or monotonicity, or assertions about the 
group of compliers in clinical trial) must be a goat. However, specific inferential 
uses of counterfactual models may tum out to be sheep. Here is one such use. 
9.1 
ACE 
Suppose, in the counterfactual approach, one were to define the ICE for unit u 
as f{yt(u)} - J{Ye(u)}, for some function f. For example, one might use the 
linear form yt(u) - Ye(u), or the logarithmic form 10g{yt(u)jYc(u)}. If U is ef-
fectively infinite, the ACE (population average ofICE(u)) is Ep{J(yt) - f(Ye)}. 
But this is just Ep, {J(Y)} - Epc {J(Y)}, and thus depends only on the marginal 
distributions Pc and Pt (and is exactly the criterion determining the solution of 
the decision problem having L == f). Hence this particular use of counterfactual 

54 
A. PHILIP DAWID 
analysis, focusing on an infinite-population ACE, is consistent with the decision-
analytic approach, and involves only terms subject to empirical scrutiny. It is for-
tunate that many of the superficially counterfactual analyses in the literature, from 
Rubin (1978) onwards, have in fact confined attention to ACE, and thus lead to 
acceptable conclusions. 
However, seemingly minor variations of the above form for ICE, such as 
Yt(u)/Yc(u), can not be handled in this way: Ep(Yt/Yc) is not determined by 
the marginals Pt and Pc alone (although these can be used to set bounds: Rachev, 
1985). So any form of inference focusing on such causal effects, at either the 
individual or the population average level, would be a metaphysical goat, depen-
dent on untestable ingredients of the metaphysical model, and hence likely to be 
misleading. 
9.2 Neyman and Fisher 
Here is a variation on ACE, even using the simple definition (4), that is nevertheless 
a goat. It is the basis of the approach introduced by Neyman (1935), and followed 
through by Wilk and Kempthorne (1955,1956,1957). 
Let U* := Ut U Uc be the set of experimental units6, say N in total. Neyman 
expressed the null hypothesis of 'no treatment effect' as asserting that yt = Yc*' 
where Yi* := N-1 LUEU* Yi(u) is the average response that would have been 
observed in the experiment, had all units been given treatment i (thus both ~* 
and Yc* are genuinely counterfactual quantities). Wilk and Kempthorne (1955) 
considered averages over a larger, but still finite, population U from which UÂ· was 
drawn. In these approaches, inference is based on the distribution generated by 
random treatment assignment (and, where appropriate, random sampling of the 
levels used for the experiment), under assumed values for the metaphysical array 
of all potential responses (Yi (u)), these values playing the role of parameters in 
the randomisation model. Such an approach (even when extended by introducing 
random errors of observation) is clearly based on a fatalistic world view. 
Neyman showed that, for the Latin square, the usual t-test was an unbiased 
test of his null hypothesis only if TUA could be assumed; similarly, the analyses 
of Wilk and Kempthorne give different answers, according as whether or not one 
assumes TUA. These workers concluded that one needs to think very carefully, 
in each particular context, about the validity of the TUA assumption, and tailor 
one's inferences accordingly. However, since there are no conceivable data that 
could shed any light on this validity, it is not clear how to act on this advice. 
Two statisticians with observationally equivalent models could arrive at discrepant 
conclusions. This suggests very strongly that Neyman's approach is not a helpful 
one, and that his metaphysical null hypothesis is misguided. 
6In the literature, the units are not completely homogeneous, but are classified in an experimental 
layout, e.g. a row-column structure with treatments imposed to form a Latin Square. However. this 
does not affect the essential logic. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
55 
Fisher, in the rapporteur's account of his comments on Neyman's paper, rejected 
this approach, arguing instead that the appropriate null hypothesis was 
Ho: T = 0, 
for which the standard t-test is valid. 
Fisher's null hypothesis is often taken to have been 
i.e. T = Â° 
and <P"( = 0, implying yt(u) = Yc(u) for all u. This, too, is a meta-
physical hypothesis. However, it is not certain that this was Fisher's intention. In 
any case, so far as the observable structure (8) is concerned, these two hypotheses 
are indistinguishable, as are the resulting tests. This identity extends to more com-
plex layouts: Dawid (1988) shows how the standard tests may be justified purely 
on the basis of a hypothesis of invariance of the joint distribution of responses 
under suitable relabelling of units, which is very much weaker than Ho: see also 
Cox (1958). The broader hypothesis Ho is equivalent to Pt = Pc, which is all that 
is needed for indifference in the decision problem -
and is, of course, a sheep, 
being testable from the data. 
10 
INSTRUMENTAL USE OF COUNTERFACTUALS 
Even if one accepts that the output of a causal analysis should not involve any 
direct assertions about counterfactuals, the example of Section 9.1 demonstrates 
that it is at least possible to use counterfactual models for acceptable purposes. 
However, that example also shows no obvious advantage to doing so, and the use 
of counterfactual models always lays one open to the danger of producing 'goat-
like' inferences, without signalling when that is the case (as for the variant forms 
of ACE considered at the end of Section 9.1). 
It nevertheless remains conceivable that purely mathematical use of the richer 
structure inherent in the modelling of the metaphysical array might actually sim-
plify some derivations and analyses of acceptable 'sheep-like' inferences. An anal-
ogy might be the fruitfulness of coupling arguments in probability theory, or of 
complex analysis in number theory. 
In my view, there may be a limited place for such instrumental use of coun-
terfactuals in the context of causal model-building. However, I remain to be per-
suaded of the usefulness of counterfactuals, even in a purely instrumental role, for 
causal inference. 
10.1 
Counterfactualsfor modelling 
The model (9) for the physical array was derived by marginalising the meta-
physical model of Example 1, so as to focus on the subcollection of variables 

56 
A. PHILIP DAWID 
picked out by the experimental design. This may be regarded as an instrumental 
use of counterfactuals for the purposes of modelling. However, in this simple ex-
ample this looks like overkill: model (9) is itself a very natural structure to impose 
on the physical array directly. 
In more complicated problems, there may be some genuine advantage to mod-
elling at the metaphysical level. Thus, suppose the experimental units are laid out 
in a row-column structure. One way to build appropriate models for outcomes is 
to apply the ideas of symmetry modelling (Dawid, 1988). If we associate with 
each plot the full vector of (complementary) potential responses it would exhibit 
under the various different possible treatment applications, then it might be reason-
able to regard the joint distribution for all these vectors as invariant under separate 
relabellings of rows and columns. If (less compellingly, and purely for simplic-
ity of exposition) we also impose invariance under relabellings of the treatments, 
symmetry arguments imply that we can represent the probability structure of the 
metaphysical array Y = (Yirc) (where i labels treatments, i rows and j columns) 
by the random effects model: 
with all the terms uncorrelated, var( (}:i) = a~, etc. 
If we consider the implications of this model for the marginal joint distribution 
of some physical array X = (Xrc), in which a specified treatment i = i(rc) is 
applied to the unit in row r and column c, we find a similar representation, but 
with the last two terms intrinsically confounded (just as the separate terms f3( u) 
and 'Yi(U) in (1) are confounded in the term Eij of (9)). If we further confine 
attention to Latin Square designs, so that no treatment appears more than once 
in any row or column, there is additional (extrinsic) confounding, resulting in the 
model: 
(15) X rc = J.L + (}:i + f3r + 'Yc + Ere, 
where, with i = i(r, c), 
This is of course the (random-effects version of) the usual model for the observ-
abIes in the Latin Square design 7. 
On the other hand, we could have initially restricted attention to the physical 
array X, and considered the group of symmetries that preserve its structure. Such 
a symmetry is represented by the combination of a row-permutation and a column-
permutation having the additional property that any two units receiving identical 
treatments before permutation also receive identical treatments after permutation. 
7The extrinsic confounding between the (0:,6), (O:,/,) and (,6,/,) + (0:,6,/,) terms in (16) will, how-
ever, make predictive inferences, which depend on these terms individually, especially sensitive to 
assumptions that cannot be tested with such a design. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
57 
This group will depend very specifically on the way in which treatments are as-
signed to units, and can have highly variable structure for different Latin Square 
layouts (Bailey, 1991, Example 4). Because of these additional restrictions on the 
symmetry of the physical array X, the implied symmetry model constructed di-
rectly for X can be considerably more complex than that expressed by (15). In 
such a case, modelling the metaphysical array directly, for the purely instrumen-
tal use of deriving an appropriate model for the physical array, appears the more 
fruitful approach. 
Another example of the usefulness (or at least convenience), for construct-
ing models of the physical domain, of direct modelling of the metaphysical do-
main (using 'pseudo-structural nested distribution models') is given by Robins 
and Wasserman (1997). 
Compatibility 
If we do take the approach of modelling each possible physical array by marginal-
ising from a single joint model for the metaphysical array, the resulting collection 
of physical models will have a property I term compatibility: for two different 
experimental layouts that both result in unit u receiving treatment i, the marginal 
models for the associated response on that unit are identical. This identity ex-
tends to the joint model for the responses of a collection of units that happen to be 
treated in the same way in both experiments. This property can be regarded as a 
non-counterfactual counterpart of the counterfactual Stable Unit-Treatment Value 
Assumption (SUTVA) (see Â§7). 
I further distinguish two forms, strong and weak, of compatibility for a collec-
tion of physical models under varying treatment assignments. Weak compatibility 
(which seems the more natural, and makes no reference whatsoever to counterfac-
tuals) simply requires the above-stated property of identity of common marginal 
models. Strong compatibility requires the existence of a single joint model for the 
metaphysical array that can be used to generate, by appropriate marginalisation, 
the various different physical models. To extend the analogy with Quantum The-
ory, strong compatibility requires the existence of 'hidden variables', underlying 
all observations that might be made. Although strong compatibility always im-
plies weak compatibility, in full generality the converse need not hold. Consider, 
for example, variables (Y1 ) Y2 ) Y3 ), where Yi is either 1 or -1, and where we can 
observe any of the pairs (Y1 ) Y2 ), (Y2 ) Y3 ), (Y3 ) Y1), but cannot observe all three 
variables simultaneously. The corresponding bivariate distributions are specified 
by: Y1 = Y2 , Y2 = Y3 , Y3 = -Y1 , with Yi either lor-I, each with probability 
1/2. Then these distributions are weakly, but not strongly, compatible (I am grate-
ful to Steffen Lauritzen for this example). Although the structure of this example 
is not quite the same as that of our current problem, it is conceivable that in this, 
too, we could have weak compatibility without strong compatibility. This opens up 
the possibility of a still deeper analogy with Quantum Theory, where observable 
behaviour cannot be explained by means of a 'hidden variable' theory. 

58 
A. PHILIP DAWID 
In the decision-analytic approach, the property of compatibility, while it may be 
very useful in streamlining the modelling, has no fundamental role to play. All we 
need do is construct appropriate models relating the outcomes on the experimental 
units, according to the treatment assignments actually made, with those on as yet 
untreated units, under various assumptions about how those new units might be 
treated. Then we can use these to make predictive inferences under the varying 
assumptions, and so to assess the relative value of future interventions. 
10.2 
Counterfactuals for inference? 
There are many problems where workers who have grown familiar and comfort-
able with counterfactual modelling and analysis evidently consider that it forms 
the only satisfactory basis for causal inference. However, I have not as yet come 
any use of counterfactual models for inference about the effects of causes which 
is not either (a) a goat, delivering misleading inferences of no empirical content; 
or (b) interpretable, or readily reinterpretable, in non-counterfactual terms. I have 
already given examples of (a), and also, in Â§9.1, of (b). Here are some more cases 
of (b). 
Robins initially (Robins, 1986) developed causal inferential methods on the 
basis of a counterfactual model. However in recent work (Robins and Wasserman, 
1997) both the underlying model and the associated methods are re-expressed in 
non-counterfactual terms. 
Conversely, Pearl, in introducing a semantics for graphicalmodels of causal 
structures (Pearl, 1993), did so in a way that avoided counterfactuals. Later (Pearl, 
1995a) he translated this into a counterfactuallanguage, based on functional mod-
els, but to no obvious advantage: his specific analyses (for example, in the Ap-
pendix to Pearl, 1995a) make no necessary use of this additional structure. 
An interesting problem that did initially appear to require a counterfactual model 
is the development of inequalities for (sheep-like) causal effects in clinical trials 
with imperfect treatment compliance (Balke and Pearl, 1994b); however, I have 
been able to derive the identical inequalities without the additional baggage of 
functional models or counterfactuals (indeed, an example of just such a derivation 
was given by Pearl, 1995b) 
Another interesting recent example of (b) is contained in Greenland et al. (1999), 
which purports to define confounding in terms of counterfactuals, but which ex-
plicitly introduces an alternative interpretation based on exchangeability. Most of 
its analyses make no essential use of counterfactuals. Two appendices, considering 
carefully the interpretation of counterfactual assertions in a number of cases, repre-
sent to me convincing demonstrations of their meaninglessness and pointlessness 
(although the authors themselves stop short of this conclusion). 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
59 
PART III: CAUSES OF EFFECTS 
11 
INFERENCE ABOUT CAUSES OF EFFECTS 
I now turn to address the problem of inference about the causes of effects. As we 
shall see, this is still more problematic than inference about the effects of causes, 
and it may be impossible to avoid a degree of ambiguity in the resulting inferences. 
The major new ingredient is that, as well as having the experimental data, we 
now have a further unit Uo, of individual interest, to which treatment t has al-
ready been applied, and the response yt(uo) = Yo observed. (We may also have 
further relevant information about Uo or its environment, perhaps even gathered 
between the application of treatment and observation of response. This possibility 
will be considered below -
for the moment we assume that this is not so.) We 
are interested in whether, for the specific unit Uo, the application of t 'caused' the 
observed response. It appears that, to address this question, we have no alternative 
but to compare, somehow, the observed valued Yo with the counterfactual quantity 
Ye (uo), the response that would have resulted from application of c to Uo. Equiva-
lently, we require inference about the individual causal effect T( uo) = Yo - Ye (uo). 
However, the fact that such an inference may be desirable does not, of itself, render 
it possible. Let us see what can be justified scientifically from data. 
EXAMPLE 2. Consider again the bivariate normal counterfactual model of Ex-
ample 1. Suppose that there is no possibility of ever measuring any other relevant 
information on any unit, beyond its response to treatment. 
The conditional distribution of T(UO) == yt(uo) - Ye(UO), given yt(uo) = Yo, 
is normal, with mean and variance 
(17) 
A:= E{T(Uo)lyt(uo) = Yo} 
(18) Â£52:= var{T(uo)lyt(uo) = Yo} 
= Yo-Be-p(Yo-Bt), 
(1 _ p2)Â¢y. 
Now, as already emphasised, from the extensive experimental data (even when 
extended with the additional observation yt(uo) = Yo) we can learn only Bt, Be, 
and Â¢y. We can not identify the correlation p. Hence, even with extensive data, 
we have residual arbitrariness. When p = 0 (Â¢{3 = 0, or independence of yt and 
Ye), we get A = Yo - Be, Â£52 = Â¢y. The value p = 1 (Â¢, = 0, or TUA) yields 
A = Bt - Be, Â£52 = 0 (or, at the other extreme, if we allow p = -I. we have 
A = 2yo - Bt - Be. and Â£52 = 0 again). Assuming p ~ 0, we can only infer the 
inequalities 
A lies between Bf - Be and Yo - Bel 
and 
Â£52:::; Â¢y. 
Thus only when Yo is sufficiently close to Bf will we get an unambiguous conclu-
sion about A, insensitive to empirically untestable assumptions about p; and only 

60 
A. PHILIP DAWID 
when cpy is sufficiently small will we be able to say anything empirically support-
able and unambiguous about 152 . If we take p = 1, equivalent to TUA, we obtain a 
seemingly deterministic inference, T( uo) = Bt - Be; but this is of little real value 
when the data give us no reason to choose any particular value of p over any other.8 
Note that, if we do assume TUA, but not otherwise, the retrospective inference 
about T( uo) is not affected by the additional information yt (uo) = Yo on the new 
unit, and thus is the same as for the case of arguing about effects of causes. Be-
cause the TUA assumption is so prevalent in the literature, the essential distinction 
between inference about the effects of causes and inference about the causes of 
effects has not usually been remarked. 
The above sensitivity to assumptions extends to e.g. Bayesian inference, which 
would require integration of the distribution defined by (17) and (18) over the 
posterior distribution of all the parameters. In this posterior, Bt , Be and cpy will 
be essentially degenerate at their sample estimates, so that we can substitute these 
in (17) and (18), and just integrate over the conditional distribution of the non-
identified parameter p, given (Bt, Be. cpy). However, this will be exactly the same 
in the posterior as in the prior, and thus the inference will remain sensitive to the 
assumed form of the prior. 
No amount of wishful thinking, clever analysis or arbitrary untestable assump-
tions can license unambiguous inference about causes of effects, even when the 
model is simple and the data are extensive (unless we are lucky enough to discover 
uniformity among units). 
11.1 
Concomitants 
It appears from the above that there is an inherent ambiguity in inference about the 
causes of effects. However, some progress towards reducing this may be possible 
if we are able to probe more deeply into the hidden workings of our units, by ob-
serving suitable additional variables. This is the basis and purpose of scientific in-
vestigation. As we have seen in Â§Â§ 6 and 8, such deeper scientific understanding is 
not essential for assessing 'effects of causes', which can proceed by an essentially 
'black box' approach, simply modelling dependence of the response on whatever 
covariate information happens to be observed for the test unit. However, it is vital 
for any study of inference about 'causes of effects', which has to take into account 
what has been learned, from experiments, about the inner workings of the black 
box. 
Thus suppose that it is possible to measure concomitant variables associated 
with a unit. These might be covariates, as already considered. However, we can 
8The inequalities developed above rely on the assumption, itself untestable, of joint normality. Even 
though the data may support marginal normality for each of Yt and Yc , any further aspects of the joint 
distribution must remain unknowable, and, in principle, the distribution of Yc , given the observed value 
Yt = y, could be anything (so long as r/>y > 0). Thus a complete sceptic could hold that inference 
about the causes of effects, on the basis of empirical evidence, is impossible. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
61 
also allow other quantities, so long as they can be assumed to be unaffected9 by the 
treatment applied. An example might be the weather between the times of planting 
and of harvesting a crop. Typically the variation in the response conditional on 
concomitants will be smaller than unconditionally. 
EXAMPLE 3. Suppose that, in the context of Example 1, detailed experiments 
have measured a concomitant K, and have found that, conditional on K (u) = k 
and the application of treatment i, the response Y(u) is normally distributed with 
residual variance 1/JK, say, and mean Oi + k. From these experiments the values of 
1/JK and the O's have been calculated. 
Define Â¢K := var(K), and 1/Jo := Â¢y = Â¢K + 1/JK. Then cov(K, Ye) = 
cov(K, yt) = Â¢K. Combining these with the covariance structure for the comple-
mentary pair (Ye, yt) implied by (1), the full dispersion matrix of (K, 1';:, yt) is 
seen to be 
Thus the conditional correlation between Ye and yt, given K, is 
pÂ¢y - Â¢K 
1/Jo 
(19) Pet.K:= Â¢ 
Â¢ 
= 1 - (1 - p)-;;:-. 
y -
K 
'PK 
In parallel to Example 2, we can not identify the arbitrary parameter Pet.K E 
[-1,1] from these more refined experiments (although it might be reasonable to 
take Pet.K ~ 0). 
We now consider inference about 'causes of effects' on a test unit uo. We again 
distinguish between the cases that concomitant information is, or is not, available 
for uo. 
1. If we were to observe K(uo) = k, say, we could conduct an analysis 
very similar to that of Example 2. In particular, (17) would be replaced 
by E{ r(uo)lyt(uo) = y, K(uo) = k} = (y - Oe - k) - Pct.K(Y - Ot - k), 
which, since the final term in parentheses is now of order .j1/J K, rather than 
.j1/Jo previously, should be less sensitive to the arbitrariness in the correla-
tion, now Pet.K. Similarly, (18) would be replaced by var{r(uo)lyt(uo) = 
y, K(uo) = k} = (1- P~t'K)1/JK' now bounded above by 1/JK < 1/Jo, rather 
than by Â¢y = 1/Jo. Clearly these improvements are the more substantial the 
smaller, relatively, is the residual vari~nce 1/J K of Y given K. 
2. Now suppose that we do not observe K(uo), or any other concomitant vari-
able, on uo. In this case -
in contrast to Case 2 of Â§8 for effects of causes-
our analysis is affected by the more detailed findings in the experiments per-
formed. Define ')'K := Â¢K Ny = 1 - 1/JK No. By (19) we have (assuming 
Pct.K ~ 0) 
9Use of this adjective itself begs many causal and counterfactual questions: see Â§14. 

62 
A. PHILIP DAWID 
(20) "(K :::; P :::; 1.10 
Consequently the experimental identification of K, even though it can not 
be observed on uo, has reduced the 'interval of ambiguity' for p from [0, 1] 
to bK' 1]11, and thus yields tighter limits on A and 82 in (17) and (18). 
From this perspective, the ultimate aim of scientific research may be seen as 
discovery of a concomitant variable, K* say, that yields the smallest achievable 
residual variance 'ljJ* := 'ljJK*, and thus, with "(* := "(K* = 1 -
'ljJ* /'ljJo, the 
shortest possible interval of ambiguity, b*, 1], for pP I term such a variable a 
sufficient concomitant. (The collection of all concomitants is always sufficient in 
this sense, but one would hope to be able to reduce it without explanatory loss.) 
However, unless 'ljJ* = 0, and rarely even then, it will not usually be possible to 
know whether we have attained this goal. 
Nonetheless, using (20) with (17) and (18), we can still make scientifically 
sound (though imprecise) inferences on the basis of whatever current level of un-
derstanding, in terms of discovered explanatory concomitant variables K, we have 
attained. This will take into account that there is a non-statistical component of 
uncertainty or arbitrariness in our inferences, expressed by interval bounds on our 
quantitative causal conclusions. 
We have assumed that the experiments performed have been sufficiently large 
that purely statistical uncertainty can be ignored. In practice this will rarely be the 
case. However, we do not as yet have any appropriate methodology for combining 
such statistical uncertainty with the intrinsic ambiguity that still remains in the 
limit. Techniques for dealing with this problem are urgently needed. 
12 CONDITIONAL INDEPENDENCE 
Suppose K* is a sufficient concomitant. Assuming Pct.K* 2:: 0, we have, from 
(19), the ultimate residual variance 'ljJ* 2:: (1 - p)'ljJo. In particular, p < 1 implies 
'ljJ* > O. If'ljJ* = 0 (and thus p = 1), the value of K* determines both potential 
responses yt and Yc, without error, and so, once we have identified K*, the ambi-
guity in our inferences entirely disappears. We call such a situation deterministic, 
and consider it further in Â§ 13 below. 
However, for reasons to be discussed in Â§ 14, we regard determinism as excep-
tional, rather than routine. In this section we consider further the non-deterministic 
case, having 'ljJ* > 0, and, by (19), p constrained only to the interval of ambiguity 
["(*,1] (as Pct.K* ranges from 0 to 1), with p* = 1 - 'ljJ* /'ljJo. 
100r, for Pct.K umestricted, 2'YK - 1 :s P :s l. 
1l0r, for Pct.K umestricted, from [-1, 1] to [2'YK -1,1]. 
12We are here assuming, for simplicity, that the model of Example 3 applies, for any concomitant K 
that might be considered. Although the mathematics is more complicated if this assumption is dropped, 
the essential logic continues to apply. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
63 
So far as any empirical evidence is concerned, there is no constraint whatsoever 
on Pet.K*. However, it would seem odd to hypothesise, for example, Pet.K* = 1, 
since this would imply P = 1, complete dependence between real and counterfac-
tual responses, at the same time as asserting non-determinism, in the sense that 
there is no concomitant information we could gather that would allow us to predict 
the response perfectly. Likewise, to hypothesise any other value of Pet.K* > 0 
would appear to leave open the possibility of finding a more powerful set of pre-
dictors that would explain away this residual dependence, thus further reducing 
the residual variance. 
In order to limit the arbitrariness in the value of P, one could attempt to give P 
further meaning by requiring that Pet.K* = 0: the totally inexplicable components 
of variation of the response, in the real and in the counterfactual universes, should 
be independent. Extending this, we might require that all variables be treated as 
conditionally independent across complementary universes, given all the concomi-
tants (which are of course constant across universes). Under this assumption, the 
interval of ambiguity for P shrinks to the point "'(* = 1 - 'ljJ* /'ljJo. 
The above conditional independence assumption is best regarded as a conven-
tion, providing an interpretation of just what one intends by a counterfactual query. 
It leads to a factor-analysis type decomposition of the joint probabilistic structure 
of complementary variables, into (a) a part fully explained by the concomitants, 
and common to all the complementary universes, and (b) residual 'purely random' 
errors, modelled as independent (for any given unit) across universes. In this way, 
we can at last give a clear structure and meaning (albeit partly conventional) to a 
metaphysical probability model for the collection of all potential responses. Note 
that, if we accept this conditional independence convention, we obtain, on using 
(19), P = "'(K* ~ 0 -
providing some justification for imposing this condition13. 
If we have identified a sufficient concomitant K* (leaving aside, for the mo-
ment, the question of how one could know this), the conditional independence 
convention renders counterfactual inference, in principle, straightforward and un-
ambiguous. In the context of Example 3, we can take P = "'(* = 'ljJ* /'ljJo, thus 
eliminating the ambiguity. More generally, from detailed experiments on treated 
and untreated units, we can discover the joint distribution of K* and yt, and of K* 
and Ye. For a new unit Uo on which no concomitants are observed, on observing 
yt(uo) = y we can condition (e.g. using Bayes' theorem) in the joint distribu-
tion of (K* , yt) to find the revised distribution of K*; and then combine this with 
the conditional distribution of Ye given K* to obtain the appropriate distribution 
of the counterfactual Ye. This two-stage procedure is valid if and only if one ac-
cepts the conditional independence property. Alternatively (and equivalently), we 
can use this property to combine the two experimentally determined distributions 
into a single joint distribution for (K*, yt, Ye), and marginalise to obtain that of 
(yt, Ye); finally we condition on yt (uo) = y in this bivariate distribution. Minor 
I3Without the convention, and with no constraints on Pct.K*, we can only assert P ~ 2'YK* - 1. 

64 
A. PHILIP DAWID 
variations will handle the case where we have, in addition, observed the value of 
some concomitant variables on uo. 
EXAMPLE 4. (with acknowledgments to V. G. Vovk). A certain company regu-
larly needs to send some of its workers into the jungle. It knows that the probability 
that a typical worker will die (D) if sent to the jungle (J) is prob(DIJ) = ~, com-
pared with prob(DIJ) = t if retained at Head Office. Joe is sent to the jungle, 
and dies. What is the probability that Joe would have died if he had been kept at 
Head Office? 
1. Suppose first all workers are equally robust, and that the risk of dying is 
governed purely by the unspecified dangers of the two locations. One might 
then regard the complementary outcomes as independent, so that the answer 
to the question is t. 
2. Now suppose that, in addition to external dangers, the fate of a worker de-
pends in part on his natural strength. With probability ~ each, a worker is ei-
ther strong (S) or weak (8). A strong worker has probability of dying in the 
jungle prob(DIJ, S) = ~, and at Head Office prob(DIJ, S) = O. A weak 
worker has respective probabilities prob(DIJ, 8) = 1, and prob(DIJ, 8) = 
~. (These values are consistent with the earlier probabilities assigned to 
prob(DIJ) and prob(DIJ).) Given that Joe died in the jungle, the posterior 
probability that he was strong is ~. If one assumes conditional indepen-
dence, given strength, between the complementary outcomes, the updated 
probability that he would have died if kept at Head Office now becomes 
~ x 0 + t x ~ = ~. 
3. In fact Joe was replaced at Head Office by Jim, who took his desk. Jim died 
when his filing cabinet fell on him. This gives additional information about 
the dangers Joe might have faced had he stayed behind. How should we 
take it into account? There is no right answer. If we regard the toppling of 
the filing cabinet, killing whoever is at the desk, as unaffected by who that 
occupant may be, and include it as a concomitant, then the answer becomes 
1. Or, we could elaborate, allowing the probability that the occupant is 
killed by the falling cabinet to depend on whether he is strong or weak. But 
it would be equally reasonable to consider that, had Joe stayed behind, the 
dangers he would have met would have been different from those facing 
Jim. In this case the previous arguments and answers (according as whether 
or not we account for strength) could still be reasonable. 
As should be clear from the above example, even with the conditional indepen-
dence convention the answer to a query about 'causes of effects' must depend, in 
part, on what variables it is considered reasonable to regard as concomitants. We 
consider this issue further in Â§ 14. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
65 
12.1 
Undiscovered sufficient concomitants 
What if, as will usually be the case, we have measured concomitants K in our 
experiments, but have not as yet identified a sufficient concomitant K*? In Ex-
ample 3, we could then only assert 'l/J* ~ 'l/JK, and thus, using the conditional 
independence property p = 'Y*, P ~ 'YKÂ· Hence the convention of conditional in-
dependence at the level of the sufficient concomitant has not, in this case, resulted 
in any reduction in the interval of ambiguity for p. 
We can, nevertheless, think, in the light of current knowledge, and having regard 
to the potentially available concomitants (see Â§14 below), about plausible values 
of the ultimate residual variance 'l/J K*, and use this in setting reasonable limits, or 
distributions, for p = 1 - 'l/JK* /'l/Jo. This still leaves our inference dependent on 
(as yet) experimentally unverified assumptions, but it might at least be possible 
to present reasoned arguments for the assumptions made. This approach based 
on conditional independence also obviates the need for new methods of statistical 
inference, combining ambiguity and uncertainty. 
13 
DETERMINISM 
In certain problems of the 'hard' sciences it can happen that, by taking account 
of enough concomitant variables, the residual variation in the response, for any 
treatment, can be made to disappear completely (at least for all practical purposes), 
thus inducing, at this more refined level, the situation of uniformity considered in 
Â§5.4 above, when all problems of causal inference and prediction disappear. In 
Example 3, this would occur if we found 'l/JK = 0, which would imply p = 1, and 
so eliminate all ambiguity. Such problems may be termed deterministic, since the 
response is then given as a function Y = f ( i, D) of the appropriate determining 
concomitant D (which is then necessarily sufficient) and the treatment i, without 
any further variability. This property is, in principle, testable when D is given (if 
it is rejected, it may be possible to reinstate it, at a deeper level, by refining the 
definition of D). However, even when such underlying determinism does exist, 
discovering that this is the case, and identifying the determining concomitant D 
and the form of f, may be practically difficult or impossible, requiring a large-
scale, detailed and expensive scientific investigation, and sophisticated statistical 
analyses. 
If we had a deterministic model, we could use it to define potential responses: 
}'i ( u) = f ( i, D ( u )),I4 We could determine the value of any potential response 
on unit u by measuring D(u). Thus in this special case we can indeed consider 
the complementary variables (}'i(u)) == (f(i, D(u))), for fixed unit u but varying 
treatment i, as having real, rather than merely metaphysical, simultaneous exis-
tence. 
14We need here the property that D, being a concomitant, is unaffected by treatment. However, since 
D need not be a covariate, this model is not necessarily fatalistic. 

66 
A. PHILIP DA WID 
Note especially that, even in this rare case where we can give empirical meaning 
to counterfactuals, we are not basing our causal modelling on the primitive notion 
of counterfactual; rather, it is the counterfactuals that are grounded in, and take 
their meaning from, the model. (In the same way, I consider that Lewis's (1973) 
interpretation of counterfactuals in terms of 'closest possible worlds' is question-
begging, since closeness can not be sensibly defined except in terms of an assumed 
causal model). 
A deterministic model, when available, can also be used to make sense of non-
manipulative accounts of causation. Given D the potential responses, for various 
real or hypothetical values of the variable 'treatment', are determined, and can be 
compared directly, however the specification of treatment may be effected. 
For inference about the causes of effects, assume that we have observed It (uo) = 
Yo, but not D (uo), and wish to assess our uncertainty about Ye ( uo). In the context 
of Example 3, we have p = 1, eliminating all ambiguity, and (in this rare case) 
justifying TVA and the inference T(UO) = Bt - Be. More generally, suppose that 
detailed experimentation has identified a deterministic model Yi (u) = f (i, D (u)). 
Although we have not observed D(uo), we can assess a distribution for it. This 
should reflect both typical natural variation of D across units (as discovered from 
experiments), and any additional concomitant information we may have on uo. 
From this distribution we can derive the induced joint distribution over the col-
lection (f (i, D ( uo))) of complementary potential responses. Then we can con-
dition the distribution of D ( uo) on the observation f (t, D (uo)) = Yo, and thus 
arrive at appropriate posterior uncertainty about a genuine counterfactual such as 
Ye(UO) == f(c,D(uo)). In this way, a fully deterministic model (if known) al-
lows an unambiguous solution to the problem of assessing the 'causes of effects'. 
The essential step is the generation of the joint distribution over the set of com-
plementary responses (together with any observed concomitants), this being fully 
grounded in an understanding of their dependence on determining concomitants, 
and a realistic probabilistic assessment of the uncertainty about those determining 
concomitants. 
The above procedure is merely a special case of that described in Â§ 12, but not 
now dependent on the convention of conditional independence of residual variation 
across parallel universes -
because in this case there is no residual variation. 
EXAMPLE 5. Suppose that a major scientific investigation has demonstrated the 
validity of the model (1), but now reinterpreted as a deterministic model, with all 
the (3's and "('s identified as concomitant variables that can, with suitable instru-
ments, be measured for any unit, and have been so measured in the experimental 
studies. Further, from these studies, the previously specified independent nor-
mal distributions for these quantities have been verified, and all the parameters 
(Bt , Be, rP{3, rP,) have been identified. 
We now examine a new unit Uo, which has been given treatment t, and observe 
the associated response It (uo) = y. The individual causal effect T( uo) is "(t (uo) -
"( e ( uo), which is now, in principle, measurable. In practice measurement of the 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
67 
{3's and 'Y's for unit Uo may not be possible. Then (in the absence of any further 
relevant information) we might describe our uncertainty about their values using 
their known joint population distribution. The appropriate uncertainty about r( uo) 
is then expressed by the normal distribution with mean>. and variance 62 given by 
(17) and (18); however, since the value of p = Â¢{3 / (Â¢{3 + Â¢'Y) is now available 
from the scientific study, the ambiguity in this inference has been eliminated. 
Note that it is vital for the above analysis that the quantities 'Yt (u) and 'Yc (u) 
be simultaneously measurable, with the specified independent distributions. It is 
not enough only to identify (3(u), and define the 'Y's as error terms: 'Yi(U) = 
Yi(u) - Oi - (3(u); in that case, since we can not simultaneously observe both 
yt(u) and Yc(u), we could not verify the required assumption of independence 
between 'Yt(u) and 'Yc(u). 
13.1 
Undiscovered detenninism 
If we believe that our problem is deterministic, but have not, as yet, completely 
identified the determining concomitant D or the function f, we can propose para-
metric forms for f and the distribution of D, and attempt to estimate these (or 
integrate out over the posterior distribution of their parameters), using the avail-
able data. In principle, sufficiently detailed experimentation would render such 
assumptions empirically testable, and identify the parameters. In practice, how-
ever, this may be far from the case. Thus consider Example 2, in which we have 
not been able to measure any concomitants. We could propose an underlying de-
terministic model of the form: 
(i=t,c) 
with Dt , Dc determining concomitants, supposedly measurable on any unit by fur-
ther, more refined, experiments. In the current state of knowledge, however, we 
can say no more than Di '" N(O, Â¢y). Further, we have no information on the 
correlation p between Dt and Dc. It is clear that, until we are able to conduct 
the more detailed experiments, merely positing such an underlying deterministic 
structure makes no progress towards removing current ambiguities, and our infer-
ences remain highly sensitive to our assumptions. In such a case there seems to 
be no obvious advantage in assuming determinism; we might just as well conduct 
analyses such as that of Example 3, basing them only on experimentally observed 
quantities, and deriving suitably qualified inferences encompassing the remaining 
ambiguity -
which should not be artificially eliminated by imposing unverified 
constraints on the model. (It may nevertheless be, as suggested in Â§ 12.1, that think-
ing about the possibilities for what we might discover in further experiments could 
help us towards reasonable and defensible resolution -
subject to later empirical 
confirmation or refutation -
of some of the ambiguities). 

68 
A. PHILIP DAWID 
13.2 Pseudo-determinism 
It seems to me that behind the popularity of counterfactual models lies an implicit 
view that all problems of causal inference can be cast in the deterministic paradigm 
(which in my view is only rarely appropriate), for a suitable (generally unobserved) 
determining concomitant D. If so, this would serve to justify the assumption of 
simultaneous existence of complementary potential responses. Heckerman and 
Shachter (1995), for example, take a lead in this from Savage (1954), who based 
his axiomatic account of Bayesian decision theory on the supposed existence of a 
'state of Nature', entirely unaffected by any decisions taken, which, together with 
those decisions, determines all variables. Shafer (1986) has pointed up some of 
the weaknesses of this conception. 
The functional graphical model framework of Pearl (1995a) posits that, under-
lying observed distributional stabilities of observed variables there are functional 
relationships, involving the treatments and further latent variables. When such a 
deterministic structure can be taken seriously, with all its variables in principle 
observable, it leads to the possibility, at least, of well-defined counterfactual infer-
ences, as described above. These will again, quite reasonably, be sensitive to the 
exact form of tbe functional relationships involved, over and above any purely dis-
tributional properties of the manifest variables; but these functional relationships 
are, in principle, discoverable. Balke (1995), Balke and Pearl (1994a) investigate 
the dependence of causal inferences on the functional assumptions. 
However, often the 'latent variables' involved in such models are not genuine 
concomitants (measurable variables, unaffected by treatment). Then there is no 
way, even in principle, of verifying the assumptions made -
which will, never-
theless, affect the ensuing inferences, in defiance of Jeffreys's law. I term such 
functional models pseudo-deterministic, and regard it as misleading to base anal-
yses on them. In particular, I regard it as unscientific to impose intrinsically un-
verifiable assumed forms for functional relationships, in a misguided attempt to 
eliminate the essential ambiguity in our inferences. 
Within the counterfactual framework it is always possible to construct, math-
ematically, a pseudo-deterministic model: simply define D(u) to be the comple-
mentary collection of all potential outcomes on unit u. In Example 1 we would 
thus take D = (yt, Yc)' We then have the trivial deterministic functional relation-
ship Y = f(i, D), where f has the canonical/arm: f(i, (yt, Yc)) = Yi (i = t, c). 
If we were now to assign a joint distribution to (yt, Yc), the analysis presented 
above for inferring 'causes of effects' in deterministic models could be formally 
applied. 
This is not a true deterministic model: D is not a true concomitant, since it 
is not, even in principle, observable. Construction of such a pseudo-deterministic 
model makes absolutely no headway towards addressing the non-uniqueness prob-
lems exposed in Â§ Â§5.4 and 11: it remains the case that no amount of scientific in-
vestigation will suffice to justify any assumed dependence structure for (yt, Yc), 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
69 
or eliminate the sensitivity to this of our inferences about causes of effects. This 
can only be done by taking into account genuine concomitants. 
14 CONTEXT 
In basing inference about the causes of effects on concomitant variables (as in 
Â§11.1), it appears that I am departing from my insistence that metaphysical as-
sumptions should not be allowed to affect inferences. For to say that a variable is 
a concomitant involves an assertion that it is unaffected by treatment, and hence 
would take the same value, both in the real universe and in parallel counterfac-
tual universes in which different treatments were applied. Such an assumption is, 
clearly, not empirically testable. Nevertheless, our causal inferences will depend 
on the assumptions we make as to which variables are to be treated as concomi-
tants. This arbitrariness is over and above the essential inferential ambiguity we 
have already identified, which remains even after the specification of concomitants 
has been made. 
My attitude is that there is indeed an arbitrariness in the models we can reason-
ably use to make inferences about causes of effects, and hence in the conclusions 
that are justified. But I would regard this as relating, at least in part, to differences 
in the nature of the questions being addressed. The essence of a specific causal 
inquiry is captured in the largely conventional specification of what we may term 
the context of the inference, namely the collection of variables that it is considered 
appropriate to regard as concomitants: see Example 4. Appropriate specification 
of context, relevant to the specific purpose at hand, is vital to render causal ques-
tions and answers meaningful. It may be regarded as providing necessary clarifica-
tion of the ceteris paribus ('other things being equal') clause that is often invoked 
in attempts to explicate the idea of cause. Differing purposes will demand dif-
fering specifications, requiring differing scientific and statistical approaches, and 
yielding differing answers. In particular, whether it is reasonable to employ a de-
terministic model must depend on the context of the problem at hand, since this 
will determine whether it is appropriate to regard a putative determining variable 
D as a genuine concomitant, unaffected by treatment. For varying contexts we 
might have varying models, some deterministic (involving varying definitions of 
D), some non-deterministic. 
EXAMPLE 6. Consider an experiment in which the treatments are varieties of 
com, and the units are field-plots. Suppose we have planted variety 1 on a par-
ticular field-plot, and measured its yield. We might ask "What would the yield 
have been on this plot if we had planted variety 2?". Before we can address this 
question, we need to make it more precise; and this can be done in various ways, 
depending on our meaning and purpose. 
First, the answer must in part depend on the treatment protocol. For example, 
this might lay down the weight, or alternatively the number, of seeds to be planted. 
In the former case our counterfactual universe would be one in which the weight of 

70 
A. PHILIP DAWID 
variety 2 to be planted would the same as the weight of variety 1 actually planted; 
in the latter case, we would need to change 'weight' to 'number' in the above, so 
specifying different counterfactual conditions, and leading us to expect a different 
answer. (In either case the actual and counterfactual responses will depend in part 
on the particular seeds chosen, introducing an irreducibly random element into 
each universe.) We might choose to link the treatments in the two universes in 
further ways: for example, if we had happened to choose larger than average seeds 
of variety 1, we might want to consider a counterfactual universe in which we also 
chose larger than average seeds of variety 2. This would correspond to a fictitious 
protocol in which the treatment conditions were still more closely defined. 
The same counterfactual question might be asked by a farmer who had planted 
variety 1 in non-experimental conditions. In this case there was no treatment pro-
tocol specified, and there is correspondingly still more freedom to specify the fic-
titious protocol linking the real and the counterfactual universe. But only when 
we have clearly specified our hypothetical protocol can we begin to address the 
counterfactual query. 
This done, we must decide what further variables we are to regard as concomi-
tants, unaffected by treatment. It might well be reasonable to include among these 
certain physical properties of the field-plot at the time of planting, and perhaps also 
the weather in its neighbourhood, subsequent to planting. 
We might also want to take into account the effect of insect infestation on yield. 
It would probably not be reasonable to treat this as a concomitant, since different 
crops are differentially attractive to insects. Instead, we might use some specifica-
tion of the abundance and whereabouts of the insects prior to planting. However, it 
would be simply unreasonable to expect this specification to be in any sense com-
plete. Would we really want to consider the exact initial whereabouts and physical 
and mental states of all insects as identical in both the real and the counterfactual 
universe, and so link (though still far from perfectly) the insect infestations suf-
fered in the two universes? If we did, we would need a practically unattainable 
understanding of insect behaviour before we could formulate and interpret, let 
alone answer, our counterfactual query. Furthermore, to insist (perhaps in an at-
tempt to justify a deterministic model) on fixing the common properties of the two 
universes at an extremely fine level of detail risks embroiling us in unfathomable 
arguments about determinism and free will (would we really have been at liberty 
to apply a different treatment in such a closely determined alternative universe?). 
To go down such a path seems to me to embark upon a quest entirely inappropriate 
to any realistic interpretation of our query. Instead, we could imagine a counterfac-
tual universe agreeing with our own at a much less refined level of detail (in which 
initial insect positions are perhaps left unspecified). This corresponds to a broader 
view of the relevant context, with fewer variables considered constant across uni-
verses. It is up to the person asking the counterfactual query, or attempting causal 
inference, to be clear about the appropriate specification, explicit or implicit, of 
the relevant context. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
71 
The conditional independence convention further allows us to tailor our counter-
factual inferences to the appropriate context, as in Example 4, without embarking 
on fruitless searches for 'ultimate causes'. In Example 6, we may wish to omit 
from our specification of context any information about, or relevant to, the popula-
tion and behaviour of the insects. We could then take the amounts of insect infes-
tation, in the real and the counterfactual universes, as independent, conditionally 
on whatever concomitants are regarded as determining our context. This choice 
may be regarded as making explicit our decision to exclude insect information 
from the context, rather than as saying anything meaningful about the behaviour 
of the world. With this understanding, we see that the very meaning (and, hence, 
the unknown value) of the correlation p between yt and Yc (or of any other mea-
sure of the dependence between such complementary quantities) will involve, in 
part, our own specification of the context we consider appropriate to counterfactual 
questions. 
The relation between the partly conventional specification of context and gen-
eral scientific understanding is a subtle one. Certainly the latter should inform 
the former, even when it does not determine it: general scientific or intuitive un-
derstandings of meteorological processes must underlie any identification of the 
weather as a concomitant, unaffected by treatment. Moreover, it is always pos-
sible that further scientific understanding might lead to a refinement of what is 
regarded as the appropriate context: thus the discovery of genetics has allowed us 
to identify previously unrecognised invariant features of an individual, and thus to 
discard previously adequate, but now superseded, causal theories. Causal infer-
ence is, even more than other forms of inductive inference, only tentative; causal 
models and inferences need to be revised, not only when theories and assumptions 
on which they are based cease to be tenable in the light of empirical data, but also 
when the specification of the relevant context has to be reformulated -
be this due 
to changing scientific understanding, or to changing requirements of the problem 
at hand. 
15 
CONCLUSION 
I have argued that the counterfactual approach to causal inference is essentially 
metaphysical, and full of temptations to make 'inferences' that can not be justified 
on the basis of empirical data, and are thus unscientific. An alternative approach 
based on decision analysis, naturally appealling and fully scientific, has been pre-
sented. This approach is completely satisfactory for addressing the problem of 
inference about the effects of causes, and the familiar 'black box' approach of 
experimental statistics is perfectly adequate for this purpose. 
However, inference about the causes of effects poses greater difficulties. A 
completely unambiguous solution can only be obtained in those rare cases where 
it is possible to reach a sufficient scientific understanding of the system under in-
vestigation as to allow the identification of essentially deterministic causal mech-

72 
A. PHILIP DAWID 
anisms (relating responses to interventions and concomitants, appropriately de-
fined). When this is not achievable (whether the difficulties in doing so be fun-
damental or merely pragmatic), the inferences justified even by extensive data are 
not uniquely determined, and we have to be satisfied with inequalities. However, 
these may be reduced by modelling the relevant context, and conducting experi-
ments in which concomitants are measured. A major and detailed scientific study 
may be required to reduce the residual ambiguity to its minimal level (and, even 
then, there can be no prior guarantee that it will do so). 
Thus, if we want to make meaningful and useful assertions about the causes of 
effects, we have to be very clear about the meaning and context of our queries. 
And then there is no magical statistical route that can by-pass the need to do real 
science, in order to attain the clearest possible understanding of the operation of 
relevant (typically non-deterministic) causal mechanisms. 
ACKNOWLEDGMENTS 
The ideas finally presented in this paper have been festering for many years, in 
the course of which I have had valuable discussions (and often heated arguments) 
with many people. I particularly wish to acknowledge the major contributions of 
Don Rubin, Judea Pearl, Glenn Shafer, Jamie Robins, Ross Shachter and Volodya 
Vovk. 
Reproduced with permission from The Journal of the American Statistical As-
sociation. Copyright 2000 by the American Statistical Association. All rights 
reserved. 
Department of Statistical Science, University College London. 
BIBLIOGRAPHY 
[BaileY,1991l Bailey, R. A. (1991). Strata for randomized experiments (with Discussion). 1. Roy. 
Statist. Soc. B 53. 27-78. 
[Balke. 1995) Balke. A. A. (1995). Probabilistic Counteifactuals: Semantics, Computation, and Ap-
plications. PhD Dissertation, Department of Computer Science. University of California. Los An-
geles. 
[Balke and Pearl. 1994a) Balke. A. A. and Pearl. 1. (l994a). Probabilistic evaluation of counterfactual 
queries. In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). 
Seattle. WA. Vol. 1.230-237. 
[Balke and Pearl. 1994b) Balke. A. A. and Pearl, 1. (1994b). Counterfactual probabilities: Computa-
tional methods, bounds and applications. Proc. 10th Conf UAl, 46-54. 
[Cox. 1958) Cox. D. R. (1958). The interpretation of the effects of non-additivity in the Latin Square. 
Biometrika 45, 69-73. 
[Dawid.1979) Dawid. A. P. (1979). Conditional independence in statistical theory (with Discussion). 
1. Roy. Statist. Soc. B 41,1-31. 
[Dawid.1984) Dawid. A. P. (1984). Present position and potential developments: some personal 
views. Statistical theory. The prequential approach (with Discussion). J. Roy. Statist. Soc. A 147. 
278-292. 

CAUSAL INFERENCE WITHOUT COUNTERFACTUALS 
73 
[Dawid, 1985) Dawid, A. P. (1985). Calibration-based empirical probability (with Discussion). Ann. 
Statist. 13,1251-1285. 
[Dawid, 1988) Dawid, A. P. (1988). Symmetry models and hypotheses for structured data layouts 
(with Discussion). 1. Roy. Statist. Soc. B 50, 1-34. 
[Greenland et al., 1999) Greenland S., Robins 1. M. and Pearl, J. (1999). Confounding and collapsi-
bility in causal inference. Stat. Sci. 14,29-46. 
[Heckerman and Shachter, 1995) Heckerman, D. and Shachter, R. (1995). Decision-theoretic founda-
tions for causal reasoning. 1. Artificial Intell. Res. 3, 405-430. 
[Hitchcock, 1997) Hitchcock, c. (1997). Causation, probabilistic. In Stanford Encyclopedia of Phi-
losophy. Online at: http://plato . stanford. edu/ entries/ causat ion-probabilistic 
[Holland, 1986) Holland, P. W. (1986). Statistics and causal inference (with Discussion). 1. Amer. 
Statist. Ass. 81,945-970. 
[Imbens and ngrist, 1994) Imbens, G. W. and Angrist, J. (1994). Identification and estimation of local 
average treatment effects. Econometrica 62, 467-476. 
[Imbens and Rubin, 1997) Imbens, G. W. and Rubin, D. B. (1997). Bayesian inference for causal 
effects in randomized experiments with noncompliance. Ann. Statist. 25, 305-327. 
[Lewis,I973) Lewis, D. K. (1973). Counterfactuals. Oxford: Blackwell. 
[McCullagh and Neider, 1989) McCullagh, P. and Neider, J. A. (1989). Generalized Linear Models 
(Second Edition). London: Chapman and Hall. 
[Neyman, 1923) Neyman, 1. (1923) On the application of probability theory to agricultural experi-
ments. Essay on principles. Roczniki Nauk Rolniczych X, 1-51 (in Polish). English translation of 
Section 9 (D. M. Dabrowska and T. P. Speed): Statistical Science 9 (1990), 465-480. 
[Neyman,1935) Neyman, J. (1935). Statistical problems in agricultural experimentation (with Dis-
cussion). Supp!. 1. Roy. Statist. Soc. 2, 107-180. 
[Pascal, 1669) Pascal, B. (1669) Pensees sur la Religion, et sur Quelques Autres Sujets. Paris: Guil-
laume Desprez. (Edition Garnier Freres, 1964). 
[Pearl,1993) Pearl, J. (1993) Aspects of graphical models connected with causality. Proc. 49th Ses-
sion lSI, 391-401. 
[Pearl, 1995a) Pearl, J. (1995a). Causal diagrams for empirical research (with Discussion). Biometrika 
82, 669-710. 
[Pearl, 1995b) Pearl, J. (1995b). Causal inference from indirect experiments. Artificial Intelligence in 
Medicine 7, 561-582. 
[Rachev,1985) Rachev, S. T. (1985). The Monge-Kantorovich mass transference problem and its 
stochastic applications. Th. Prob. Appl. 29, 647- 671. 
[Raiffa, 1968) Raiffa, H. (1968). Decision Analysis: Introductory Lectures on Choices under Uncer-
tainty. Reading, Mass.: Addison-Wesley. 
[Robins, 1986) Robins, J. M. (1986). A new approach to causal inference in mortality studies with 
sustained exposure periods -
application to control of the healthy worker survivor effect. Math. 
Modelling 7, 1393-1512. 
[Robins, 1987) Robins, J. M. (1987). Addendum to "A new approach to causal inference in mortality 
studies with sustained exposure periods -
application to control of the healthy worker survivor 
effect." Compo Math. App!. 14,923-945. 
[Robins and Greenland, 1989) Robins, J. M. and Greenland, S. (1989). The probability of causation 
under a stochastic model for individual risk. Biometrics 45, 1125-1138. 
[Robins and Wasserman, 1997) Robins, J. M. and Wasserman, L. A. (1997). Estimation of effects of 
sequential treatments by reparameterizing directed acyclic graphs. Technical Report 654, Depart-
ment of Statistics, Carnegie Mellon University. 
[Rubin, 1974) Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and non-
randomized studies. 1. Educ. Psycho!. 66,688-701. 
[Rubin, 1978) Rubin, D. B. (\ 978). Bayesian inference for causal effects: the role of randomization. 
Ann. Statist. 6, 34-68. 
[Rubin,1980) Rubin, D. B. (1980). Comment on "Randomization analysis of experimental data: the 
Fisher randomization test", by D. Basu. 1. Amer. Statist. Ass. 81,961-962. 
[Rubin,1986) Rubin, D. B. (1986). Which Ifs have causal answers? (Comment on "Statistics and 
causal inference", by P. W. Holland). 1. Amer. Statist. Ass. 81,961-962. 
[Riischendorf et al.,) Riischendorf, L., Schweizer, B. and Taylor, M. D. (Eds). Distributions with 
Fixed Marginals and Related Topics. Institute of Mathematical Statistics Lecture Notes -
Mono-
graph Series, Volume 28. Hayward, California: Institute of Mathematical Statistics. 

74 
A. PHILIP DAWID 
[Savage, 1954] Savage, L. 1. (1954). The Foundations of Statistics. New York: Wiley 
[Shafer, 1986] Shafer, Glenn (1986). Savage revisited (with Discussion). Statistical Science 4, 463-
501. 
[Shafer, 1996] Shafer, Glenn (1996). The Art of Causal Conjecture. Cambridge, Mass.: MIT Press. 
[Wilk and Kempthome, 1955] Wilk, M. B. and Kempthorne, O. (1955). Fixed, mixed and random 
models. 1. Amer. Statist. Ass. 50, 1144-1167. 
[Wilk and Kempthorne, 1956] Wilk, M. B. and Kempthorne, O. (1956). Some aspects of the analysis 
of factorial experiments in a completely randomized design. Ann. Math. Statist. 27,950--985. 
[Wilk and Kempthorne, 1957] Wilk, M. B. and Kempthorne, O. (1957). Non-additivities in a Latin 
square. 1. Amer. Statist. Ass. 52, 218-236. 

JON WILLIAMSON 
FOUNDATIONS FOR BAYESIAN NETWORKS 
Bayesian networks are normally given one of two types of foundations: they are 
either treated purely formally as an abstract way of representing probability func-
tions, or they are interpreted, with some causal interpretation given to the graph in 
a network and some standard interpretation of probability given to the probabili-
ties specified in the network. In this chapter I argue that current foundations are 
problematic, and put forward new foundations which involve aspects of both the 
interpreted and the formal approaches. 
One standard approach is to interpret a Bayesian network objectively: the graph 
in a Bayesian network represents causality in the world and the specified probabil-
ities are objective, empirical probabilities. Such an interpretation founders when 
the Bayesian network independence assumption (often called the causal Markov 
condition) fails to hold. In Â§2 I catalogue the occasions when the independence as-
sumption fails, and show that such failures are pervasive. Next, in Â§3, I show that 
even where the independence assumption does hold objectively, an agent's causal 
knowledge is unlikely to satisfy the assumption with respect to her subjective 
probabilities, and that slight differences between an agent's subjective Bayesian 
network and an objective Bayesian network can lead to large differences between 
probability distributions determined by these networks. 
To overcome these difficulties I put forward logical Bayesian foundations in Â§5. 
I show that if the graph and probability specification in a Bayesian network are 
thought of as an agent's background knowledge, then the agent is most rational 
if she adopts the probability distribution determined by the Bayesian network as 
her belief function. Specifically, I argue that causal knowledge constrains rational 
belief via what I call the causal irrelevance condition, and I show that the distribu-
tion determined by the Bayesian network maximises entropy given the causal and 
probabilistic knowledge in the Bayesian network. 
Now even though the distribution determined by the Bayesian network may be 
most rational from a logical point of view, it may not be close enough to objec-
tive probability for practical purposes. I show in Â§6 that by adding arrows to the 
Bayesian network according to a conditional mutual information arrow weight-
ing, one can decrease the cross entropy distance between the Bayesian network 
distribution and the objective distribution. This can be done within the context 
of constraints on the Bayesian network which limit its size and the time taken to 
calculate probabilities from the network, in order to minimise computational com-
plexity. 
This leads to two-stage foundations for Bayesian networks:Â§4 first adopt the 
probability function determined by a Bayesian network (this, according to the log-
ical Bayesian interpretation, is the best subjective probability function one can 
adopt given the knowledge encapsulated in the network), and secondly refine the 
75 
D. Corfield and J. Williamson (eds.), Foundations of Bayesianism, 75-115. 
Â© 2001 Kluwer Academic Publishers. 

76 
JON WILLIAMSON 
Bayesian network to better fit objective probability (this process of calibration is 
required by empirical Bayesianism).l 
To start with I shall give an introduction to Bayesian networks and their foun-
dations in Â§ 1, before proceeding to criticisms of the standard interpretations of 
Bayesian networks in Â§2 and Â§3. The remainder of the paper will be taken up with 
my suggestions for new foundations. 
BAYESIAN NETWORKS 
Suppose we have a domain of N variables, C 1 , ... , C N, each of which takes 
finitely many values, vI, ... ,Vfi, i = 1, ... , N. A literal is an expression Ci 
of the form Ci = v{ and a state is a conjunction of literals. A Bayesian network 
consists of a directed acyclic graph, or dag, G over the nodes C 1 , ... , C N together 
with a set of specifying probability values 5 = {p(CMi) : Ci is a literal involv-
ing node Ci and di is a state of the parents of Ci in G, i = 1, ... , N}.2 Now, 
under an independence assumption,3 namely that given its parents D i , each node 
Ci is probabilistic ally independent of any set 5 of other nodes not containing the 
descendants of Ci, p(cddi A s) = p(cildi), a Bayesian network suffices to deter-
mine a joint probability distribution p over the nodes C1 , ... ,CN .4 Furthermore, 
any probability distribution on C1 , ... , C N can be represented by some Bayesian 
network. 
Bayesian networks are important in many areas where probabilistic inference 
must be performed efficiently, such as in expert systems for artificial intelligence. 
Diagnosis constitutes a typical problem area for expert systems: here one is pre-
sented with a state of symptoms s and, under the probabilistic approach to diagno-
sis, one must find p( Ci Is) for a range of causal literals Ci. 5 Depending on the struc-
ture of the graph G, both the number of specifiers required to determine a proba-
bility distribution p and the computational time required to calculate p(cils) may 
be substantially lower for a Bayesian network under the independence assumption 
than for a representation of p which makes no assumptions. Thus Bayesian net-
1 See the introduction to this volume for more on the distinction between logical and empirical 
Bayesianism. Such forms of Bayesianism are often referred to as 'objective' Bayesian positions, and 
confusion can arise because physical or empirical probability (frequency, propensity or chance) is of-
ten called 'objective' probability in order to distinguish it from Bayesian 'subjective' probability. In 
this chapter I will draw the latter distinction, using 'objective' to refer to empirical interpretations of 
causality and probability that are to do with objects external to an agent, and using 'subjective' to refer 
to interpretations of causality and probability that depend on the perspective of an agent subject. 
2If Ci has no parents, p( Ci Id;) is just p( Ci). 
3The Bayesian network independence assumption is often called the Markov or causal Markov 
condition. 
4The joint distribution p can be determined by the dirert method: p( Cl /\ ... /\ C N ) 
= 
I1~1 p( Ci I di) where di is the state of the direct causes of Ci which is consistent with q /\ ... /\ C NÂ· 
Alternatively p may be determined by potentially more efficient propagation algorithms. See [Pearl 
1988] or [Neapolitan 1990] here and for more on the formal properties of Bayesian networks. 
5 See [Williamson 2000] for more on the probabilistic approach to diagnosis. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
77 
works can offer key pragmatic advantages over formalisms without an assumption 
like independence. 
There are two main types of philosophical foundations given to Bayesian net-
works. One can treat Bayesian networks as abstract structures, and use machine 
learning techniques to learn from a database of past case data (for instance of the 
symptoms and diagnoses of past patients) a Bayesian network that represents, or 
represents an approximation to, a target probability distribution.6 More commonly, 
Bayesian networks are interpreted. Here the graph is taken to represent a causal 
structure, either objective or subjective. In the former case the graph contains an 
arrow from Ci to Cj if Ci is a direct cause of Cj , but in the subjective case the 
graph represents the causal knowledge of an agent X, with an arrow from Ci to Cj 
if X believes, or knows, that Ci is a direct cause of Cj . The specified probabilities 
are also given an interpretation, either objective in terms of empirical frequencies, 
propensities or chances, or more often subjective in terms of degrees of rational 
belief. Finally the independence assumption is posited as a relation between the 
causal interpretation and the interpretation of probability. 
In my view the most important limitation of the abstract approach is that there 
is often not enough initial data for it to get off the ground. The abstract approach 
requires a database of past case data, but there may simply not be enough such data 
to invoke a machine learning algorithm for generating a Bayesian network. Fur-
thermore, new case data may trickle in slowly and it may take a while before the 
learning algorithm yields dependable results. Even if there is plenty of data, the 
data may not be reliable enough to generate a reliable network -
in my experience 
this is a significant problem, since different people often measure or categorise 
variables in different ways even when collecting data for the same database. There 
is also a difficulty when certain variables are not measured at all: diagnostic data, 
for example, rarely includes the presence or absence of every possible symptom 
of a patient, but just the most significant symptoms, and the symptoms considered 
most significant are subject to biases of individual doctors. In sum, the abstract 
approach is not appropriate for applications which require an expert system oper-
ating right from the outset, but where the data is not available, is of poor quality, 
or is subject to mixtures of unknown biases. However the interpreted approach 
does not face this sort of problem: an expert can often from the outset provide 
qualitative causal knowledge, subjective degrees of belief and even estimates of 
objective probabilities, and this information can be used to construct a Bayesian 
network right away -
no past case data is required. 
On the other hand the interpreted approach also has its problems, largely to 
do with the status of the independence assumption.7 In the next two sections I 
shall outline these problems with the independence assumption and then go on to 
develop a hybrid methodology incorporating aspects of both the interpreted and 
abstract accounts: the basic idea behind the hybrid methodology is to form an 
6See (Jordan 19981 
7 One problem that I will not consider here is the knowledge elicitation problem: the expert may find 
it hard to articulate her knowledge, and the elicitation process can be quite slow. 

78 
JON WILLIAMSON 
initial Bayesian network from expert knowledge, and to further refine this network 
in the light of new case data. First we shall tackle the problems with an objective 
interpretation, and then investigate the subjective approach in Â§3. 
2 
OBJECTIVE NETWORKS 
Under an objective interpretation, the Bayesian network independence assumption 
makes a substantive claim about the relationship between objective causality and 
objective, empirical probability. I will show here that this claim is highly problem-
atic, rendering an objective interpretation inadequate. 
It will be useful to note that the principle of the common cause is a logical con-
sequence of the independence assumption.8 The principle of the common cause 
claims the following. Suppose two variables are probabilistically dependent and 
neither causes the other, then 
â¢ existence: they have one or more causes in common,9 and 
â¢ screening: they are probabilistically independent conditional on those com-
mon causes. 
We can exploit the link between independence and the common cause principle 
because when an objective interpretation is given to both principles one can find 
many counterexamples to the latter principle which thereby contradict the former. 
In effect we can translate doubts about probabilistic analyses of causality in the 
philosophical literature -
such analyses often appeal to the objectively-interpreted 
principle of the common cause -
into doubts about the objective interpretation of 
Bayesian networks. Many of the counterexamples are well-known and, when con-
sidered in isolation, thought to be so unusual as to be unimportant, or thought to be 
susceptible to particular rebuttals. I want to provide a taxonomy of the counterex-
amples in order to show that the problem is more widespread than often considered 
and so general that the rebuttals are either too particular or unappealing when gen-
eralised. 1O 
8This principle is due to Reichenbach (see [Reichenbach 1956], Â§19, pages 157-167). It is also 
often assumed as a basis for statistical experimentation -
[Fisher 1935]. One can see that the principle 
of the common cause is a consequence of the independence assumption by generalising the following 
example in the obvious way. Suppose we have a Bayesian network with graph A -----t B, C -----t D. 
Thus neither B nor D cause the other, nor do they have a common cause. Band D must then be 
unconditionally probabilistically independent since for literals band d on B and D respectively, their 
joint probability p(b 1\ d) ::: La cp(bla)p(a)p(dlc)p(c) ::: [LaP(bla)p(a)][Lcp(dlc)p(c)] ::: 
p( b )p( d), where the first equality follows from the direct decomposition of probability in a Bayesian 
network (see [Neapolitan 1990] theorem 5.1 for example). 
9Existence of a common cause resembles Mill's Fifth Canon of Inductive Reasoning: 'Whatever 
phenomenon varies in any manner whenever another phenomenon varies in some particular manner, is 
either a cause or an effect of that phenomenon, or is connected with it through some fact of causation.' 
[Mill 1843], page 287. 
10 A large literature touches on the independence assumption in one way or another. Thus there are 

FOUNDATIONS FOR BAYESIAN NETWORKS 
79 
I shall argue against the independence assumption by documenting two types of 
counterexample to the principle of the common cause: the causal variables Ci and 
Cj may be accidentally correlated, or there may be some extra-causal constraint 
which ensures that they are probabilistic ally correlated.!! There may either be no 
suitable common cause to account for a correlation, contradicting the existence 
condition above, or if there are common causes, they will not account for all of the 
correlation, contradicting the screening condition. 
2.1 
Accidental Correlations 
Christmas trees tend to be sold when most oranges ripen and are sold. Let C rep-
resent the number of Christmas trees sold on any day and 0 represent the number 
of oranges sold on any day (C and 0 are random variables). Then p( C > xlO > 
y) > p( C > x) for some suitable constants x and y. Now it seems clear that sales 
of Christmas trees do not cause sales of oranges, nor vice versa. Hence, some 
common cause must be found to explain their probabilistic dependence if the in-
dependence assumption is to hold. If there is a common cause it would have to be 
something like the time of year or the season. However, intuitively one does not 
endow the time of the year with causal powers, and there are no obvious mecha-
nisms at play underlying any such causation. Intuitively there is no common causal 
explanation for the correlation -
it is accidental. If such intuitions are right, then 
the independence assumption must fail for this causal scenario. 
In order to save the independence assumption one may well be tempted to main-
tain that the time of year really is the common cause here. I shall call this strategy 
causal extension. The idea is that one tries to extend the intuitive concept of cause 
by counting intuitively non-causal variables, like the time of the year, as causal. 
In the context of Bayesian networks, causal extension often takes the form of an 
assumption that there is a 'hidden', 'latent' or 'unmeasured' common cause when-
ever two variables are found to be correlated, even when there is no intuitively 
plausible common cause.!2 Unfortunately, there are a number of difficulties with 
the strategy of causal extension. Firstly, extending the concept of cause creates 
epistemic problems. Identifying causal variables and the causal relationships be-
tween them is a hard problem. Any extension of the concept of cause is likely to 
make the task harder. In particular, it may be very difficult for an expert to pro-
vide a causal graph under the causal extension approach: one is asking the expert 
to identify variables that render the independence assumption valid, rather than 
to identify the causes and effects that she is used to dealing with. Furthermore, 
criticisms (for example [Humphreys & Freedman 1996], [Humphreys 1997], [Lemmer 1993], [Lemmer 
1996], [Lad 1999]) and defences (for example [Spirtes et al. 1997], [Hausman 1999], [Pearl 2000] 
Â§2.9.1) of the independence assumption which I will not cover here. I will however cover the criticisms 
I believe most telling and the most viable reactions to these criticisms. 
11 'Correlation' is occasionally used to denote some kind of linear dependence, but I shall just use it 
as a synonym for 'probabilistic dependence' here. 
12 See [Binder et al. 1997] and [Pearl 2000] for example. 

80 
JON WILLIAMSON 
if one increases the number of nodes and arrows that must be considered in the 
graph of a Bayesian network then one risks the network becoming too complex for 
practical use. The amount of space required to store a Bayesian network and the 
amount of time required to calculate probabilities from the network both increase 
exponentially with the number of nodes in the worst case. This worst case occurs 
when the graph is dense -
that is, there are many arrows in the graph. Thus causal 
extension is a dangerous tactic from an epistemic and practical point of view. 
The second major problem is that by extending the concept of cause we are 
liable to lose qualities that are important to causality. Genuine causal variables 
tend to have various characteristics in common: for example one can normally 
view them as spacio-temporally localised events, and causes and effects tend to be 
related by physical mechanisms. If we allow variables which do not have these 
qualities then we can no longer be said to be explicating the notion of cause -
the 
extension is ad hoc and the word 'cause' loses meaning, just becoming a synonym 
for 'variable' if the process is pursued indefinitely. This is clearly undesirable if 
we require a genuinely causal interpretation of the graph in the Bayesian network, 
as opposed to more abstract foundations. 
Elliott Sober produced the following counterexample to the principle of the 
common cause: 
Consider the fact that the sea level in Venice and the cost of bread 
in Britain have both been on the rise in the past two centuries. Both, 
let us suppose, have monotonically increased. Imagine that we put 
this data in the form of a chronological list; for each date, we list the 
Venetian sea level and the going price of British bread. Because both 
quantities have increased steadily in time, it is true that higher than 
average sea levels tend to be associated with higher than average bread 
prices. The two quantities are very strongly positively correlated. 
I take it that we do not feel driven to explain this correlation by pos-
tulating a common cause. Rather, we regard Venetian sea levels and 
British bread prices as both increasing for somewhat isolated endoge-
nous reasons. Local conditions in Venice have increased the sea level 
and rather different local conditions in Britain have driven up the cost 
of bread. Here, postulating a common cause is simply not very plau-
sible, given the rest of what we believe. 13 
Here Sober calls the existence of a common cause into question -
there is a 
causal explanation of the correlation, but it is not an explanation involving com-
mon causes, so in a sense the correlation is accidental. Postulating a common cause 
conflicts with intuitions here. In particular there appears to be no common causal 
mechanism. We often appeal to non-probabilistic issues like mechanisms to help 
determine which correlations are causal and which are accidental. As Schlegel 
points out, 'we reject a correlation between sun spots and economic cycles as 
13[Sober 19881215. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
81 
probably spurious, because we know of no relating process, but accept a correla-
tion between sun spots and terrestrial magnetic storms because there is a plausible 
physical relationship.' 14 
Besides causal extension, there is a separate line of response one can make to 
such counterexamples, that of restriction, whereby one restricts the application 
of the independence assumption so that it does not apply to awkward cases like 
Sober's.15 This response can take one of two forms, correlation restriction or 
causal restriction. Regarding the former, some, such as Papineau and Price, claim 
that British bread prices and the Venetian water level do not have the right type 
of correlation for the principle of the common cause to be applied since their cor-
relation can be predicted from the co-variation within each time-seriesl6 or from 
determinism within each physical process. I? They thus attempt to avoid the coun-
terexample to the common cause principle by restricting the principle itself. How-
ever, it should be noted that they pursue this strategy in the context of a defence of 
a probabilistic analysis of causality. Whether or not this move is successful in that 
context, it is no help here when thought of in terms of the Bayesian network frame-
work, for restricting the principle of the common cause restricts the independence 
assumption too, and the reduction of a probability function to a Bayesian network 
is not possible without full-blown independence. Hence correlation restriction is 
not a viable move when considering Bayesian networks. 
The other variety of restriction, causal restriction, is more promising. Here the 
strategy is to argue that the variables themselves are not of the sort to which the 
independence assumption applies. One may claim that the correlated variables are 
not causal variables, although this is rather implausible when it comes to the ex-
amples above. Alternatively one may accept that they are causal, but have not been 
individuated correctly for the independence assumption to apply. For example, the 
variables may need to be indexed by time,18 may need to be complete descriptions 
of their corresponding single-case events, or may need to be properties that can be 
repeatedly instantiated. 
While it is possible that for any particular counterexample to independence 
there is another way of individuating the variables so that the dependency is re-
moved, it is less clear that one rule of individuation will overcome all counterex-
amples. I have used examples which exhibit temporal correlation here because it 
is easy to see how such variables could be correlated, but any two events might ex-
hibit accidental correlation, in which case alternative individuation will not help. 
The independence assumption rules out accidental correlation a priori, and such a 
restriction does not appear a priori to be any more plausible applied to one individ-
uation than another. Thus an appeal to individuation is by no means guaranteed to 
overcome the problem of accidental correlation. 
14[Schlege1 1974llO. 
15Lakatos caned this type of defence 'monster-barring'. 
16[Papineau 1992l243. 
17 [Price 1992l 264. 
1BSee [Spirtes et al. 1993] page 63 for example. 

82 
JON WILLIAMSON 
Causal restriction also induces epistemic problems of its own. If individuation 
matters then one has to do a certain amount of analysis before tackling a problem, 
making the application of Bayesian networks harder. Furthermore, in a particular 
problem one may be interested in variables which must be individuated in a way 
for which independence does not hold, in which case the machinery of Bayesian 
networks cannot be applied at all. 
I have illustrated the problem of accidental correlations and introduced strate-
gies for defending the independence assumption, including causal extension and 
causal restriction. These strategies are somewhat less than effective at dealing with 
the problem, and if they can be made to work will only do so at an epistemic and 
intuitive cost. In Â§2.2 we will see how these strategies can be applied to other com-
mon types of counterexample. Our conclusions will be much the same. Yet these 
costs are not ones we have to reluctantly accept. In the foundations I propose later, 
we will stick with our intuitive notion of cause and the individuation of variables 
will not matter. 
2.2 
Extra-Causal Constraints 
I shall now consider counterexamples to the principle of the common cause where 
probabilistic dependencies have an explanation that relates the dependent variables 
-
thus the dependencies are not accidental -
but where the explanation is not 
causal. There are a number of non-causal correlators: two causal variables can be 
correlated 
â¢ in virtue of their meaning, 
â¢ because they are logically related, 
â¢ because they are mathematically related, 
â¢ because they are related by (non-causal) physical laws, or 
â¢ because they are constrained by local laws or boundary conditions. 
Let us look at each of these situations in turn. 
First, the meanings of expressions can constrain their probabilities. 'Flu and 
orthomyxoviridae infection are probabilistic ally dependent, not because they have 
a common cause, but because 'flu is an example of orthomyxoviridae infection-
the variables have overlapping meaning. 
In response one can advocate a kind of causal restriction. One can argue that 
causes should be individuated so as to avoid overlapping meaning, and that one 
should remove a node from a Bayesian network if there is another with related 
meaning. But this is not always a sensible move for a number of reasons. One can 
lose valuable information from a Bayesian network by deleting a node, since both 
the original nodes may be important to the application of the network. Meaning 

FOUNDATIONS FOR BAYESIAN NETWORKS 
83 
might be related through vagueness rather than classification overlap, for exam-
ple if one symptom is a patient's report of fever and another is a thermometer 
reading, and it may be useful to consider all such related nodes. In some cases 
one may even want to include synonyms in a Bayesian network, for example in 
a network for natural language reasoning. Furthermore, removing a node can in-
validate the independence assumption if the removed node is a common cause of 
other nodes. Or one simply may not know that two nodes have related meaning: 
Yersin's discovery that the black death coincides with Pasteurella pestis was a gen-
uine example of scientific inference, not the sort of thing one can do at one's desk 
while building an expert system. 
Causal extension is no better a ploy here. One could suggest that a common 
cause variable called 'synonymy' or 'meaning overlap' should be introduced. But 
this will not in general screen off such dependencies, and as before we have epis-
temic cost in terms of identifying dependencies in virtue of meaning and the likely 
added complexity of incorporating new variables and arrows, as well as a commit-
ment to a counterintuitive concept of cause. 
Probabilistic correlations can also be explained by logical relations. For in-
stance, logically equivalent sentences are necessarily perfectly correlated,19 and if 
one sentence e logically implies sentence d, the probability of d must be greater 
than or equal to that of e. Thus one should be wary of Bayesian networks which 
involve logically complex variables. Suppose C causes complaints D, E and F, 
and that we have three clinical tests, one of which can determine whether or not a 
patient has both D and E, another tells us whether or not the patient has one of E 
and F, and the third tells us whether the patient has C. Thus there is no direct way 
of determiningp(dle),p(ele) or pUle) for literals e, d, e and I of C, D, E, and F 
respectively, but one can findp(dl\ele) andp(eV lie). One might then be tempted 
(in the spirit of causal extension) to incorporate C --t (D 1\ E), C --t (E V F) 
in one's causal graph, so that the probability specification of the corresponding 
Bayesian network can be determined objectively. In such a situation, however, C 
will not screen node D 1\ E off from node E V F and the independence assumption 
is not satisfied. 
This problem seriously affects situations where causal relata are genuinely log-
ically complex, as happens with context-specific causality. A may cause B only 
if the patient has genetic characteristic C: if the patient has any other genetic 
characteristic then there is no possible causal mechanism from A to B. Then the 
conjunction A 1\ C is the cause of B, not A or C on their own. However, A may 
be able to cause D in everyone, so the causal graph would need to contain a node 
A 1\ C and a second node A. One would not expect these two nodes to be screened 
off by any common causes. 
Next we turn to mathematical relations as a probabilistic correlator. By way of 
example, consider the application of Bayesian network theory to colon endoscopy 
as documented in [Sucar et al. 1993] and [Kwoh & Gillies 1996]. The object is 
19 At least according to standard axiomatisations of probability. 

84 
JON WILLIAMSON 
to guide the endoscope inside the colon towards the lumen, avoiding the divertic-
ulum. A Bayesian network was used to identify the lumen and diverticulum from 
the endoscope image. The presence of the lumen causes a large dark region to 
appear on the endoscope screen while the diverticulum causes a small dark region. 
The size of the region can be directly measured, but its darkness was measured 
by its mean intensity level together with its intensity variance in the region. A 
Bayesian network was constructed incorporating these variables and the indepen-
dence assumption was tested and found to fail: the mean and variance variables 
were found to be correlated when, according to the causal graph under the inde-
pendence assumption, they should not have been. The problem was that there is no 
obvious common cause for this correlation: mean and variance are related math-
ematically, not causally. We have that Var X = EX2 - (EX)2, where Var X 
is the variance of random variable X, and E signifies expectation so that EX is 
the mean of X. To take the simplest example, if X is a Bernoulli random variable 
and EX = x then V ar X = x (1 - x), making the mean and variance perfectly 
correlated. In the endoscopy case, the light intensity will have a more compli-
cated distribution, but the mean value will still constrain the variance, making the 
mean and variance probabilistically dependent. To try to resolve this failure of the 
independence assumption, at first one of the two correlated nodes was removed 
(causal restriction). This gave some improvement in performance but suffered 
from significant loss of information. Next (causal extension) [Kwoh & Gillies 
1996] attempted to introduce an extra common cause to screen off the correlation, 
but while this move improved the success rate of the Bayesian network, it raised 
fundamental problems. Firstly it is not clear what the new node represents (it was 
just called a 'hidden node'), so a causal interpretation may no longer be appro-
priate for the graph. Secondly, the distribution specifying probabilities relating 
the new node to the other nodes had to be ascertained: this could only be done 
mathematically, by finding what the probabilities should be if the introduction of 
the new node allowed the unwanted correlation to be fully screened off, and could 
not be tested empirically or equated with any objective probability distribution. 
Therefore the Bayesian network lost both the objective causal and the objective 
probabilistic components of its interpretation. An objective interpretation is just 
not feasible, given extra-causal dependencies like this. 
That extra-causal constraints include physical laws has been exemplified by 
Arntzenius:2o 
Suppose that a particle decays into 2 parts, that conservation of total 
momentum obtains, and that it is not determined by the prior state of 
the particle what the momentum of each part will be after the decay. 
By conservation, the momentum of one part will be determined by 
the momentum of the other part. By indeterminism, the prior state of 
the particle will not determine what the momenta of each part will be 
after the decay. Thus there is no prior screener off. 
20[Amtzenius 1992) pages 227-228, from [van Fraassen 1980) page 29. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
85 
The principle of the common cause fails here because there is nothing obvi-
ous that we can call a common cause -
the existence component of the principle 
fails. But even if some weird and wonderful common cause could be found in 
such quantum situations, independence would still fail because screening condi-
tion would fail. Suppose we consider the spins Band C of two particles: Band 
C have values up or down. The two particles are fired such that one has spin 
up (represented by literal b) if and only if the other does (c). Suppose also that 
either one being spin up is as likely as not, p(b) = p(c) = 1/2, but that a com-
mon cause A is found which explains the spins, so A ~ B, A ~ 
C, and 
p(bla),p(cla) = x > 1/2. But since p(blc) = 1, screening off is satisfied if and 
only if 1 = p(bla A c) = p(bla), so the cause must be deterministic, a wildly inap-
propriate assumption in the quantum world. Thus we must conclude that there are 
quantum constraints on objective probability which are extra-causal.21 
The philosophical literature also contains several examples of how local non-
causal constraints and initial conditions can account for dependencies amongst 
causal variables. Cartwright, for instance, points out that 
independence is not always an appropriate assumption to make .... A 
typical case occurs when a cause operates subject to constraint, so that 
its operation to produce one effect is not independent of its operation 
to produce another. For example, an individual has $10 to spend on 
groceries, to be divided between meat and vegetables. The amount 
that he spends on meat may be a purely probabilistic consequence of 
his state on entering the supermarket; so too may be the amount spent 
on vegetables. But the two effects are not produced independently. 
The cause operates to produce an expenditure of n dollars on meat 
if and only if it operates to produce an expenditure of 10 - n dol-
lars on vegetables. Other constraints may impose different degrees of 
correlation.22 
Salmon23 gives another counterexample to the screening condition. Pool balls 
are set up such that the black is pocketed (B) if and only if the white is (W), 
and a beginner is about to play who is just as likely as not to pot the black if she 
attempts the shot (8), and is very unlikely to pot the white otherwise. Thus if we 
let b, wand s be literals representing the occurrence of B, Wand 8 respectively, 
p(b B w) = 1 and p(bls) = 1/2, so 1/2 = p(wls) i- p(wls A b) = 1 and the 
cause 8 does not screen off its effects Band W from each other. As Salmon says: 
21Note that [Butterfield 1992] looks at Bell's theorem and concludes (page 41) that, 'the violation 
of the Bell inequality teaches us a lesson, ... namely, some pairs of events are not screened off by their 
common past: [Amtzenius 1992] has other examples and also argues on a different front against the 
principle of the common cause assuming determinism. See also [Healey 1991] and [Savitt 1996] pages 
357-360 for a survey. 
22[Cartwright 1989]1l3-114. 
23[Salmon 1980] pp. 150-151, [Salmon 1984] pp. 168-169. 

86 
JON WILLIAMSON 
It may be objected, of course, that we are not entitled to infer ... that 
there is no event prior to B which does the screening. In fact, there is 
such an event -
namely, the compound event which consists of the 
state of motion of the cue-ball shortly after they collide. The need to 
resort to such artificial compound events does suggest a weakness in 
the theory, however, for the causal relations among S, Band W seem 
to embody the salient features of the situation. An adequate theory of 
probabilistic causality should, it seems to me, be able to handle the 
situation in terms of the relations among these events, without having 
to appeal to such ad hoc constructions.24 
I would echo this sentiment in the current context: in my view an adequate 
objective causal-probabilistic interpretation of Bayesian networks should not have 
to appeal to ad hoc constructions. Spirtes, Glymour and Scheines give a causal-
restriction defence against Salmon's counterexample by arguing that the collision 
should be more specifically individuated (in particular the momentum of the cue 
ball should be described).25 Again this is less than satisfactory in the absence of a 
general theory as to how causes should be individuated. 
A further example: repeatedly pull one of two beads (a blue bead B and red 
bead R, otherwise identical) out of a bag. Thenp(blr) = 0 < 1/2 = p(b). But 
rather than saying that pulling out the red bead is a preventative of pulling out the 
blue bead, the correlation is explained by the set-up of the repeatable experiment: 
only one bead is pulled out of the bag in any trial. Here the set-up constrains the 
probabilities and isn't the sort of thing that counts as a cause. 
In response to the problem of extra-causal constraints, one might admit defeat in 
problems such as the diagnosis of apparatus for the investigation of quantum me-
chanical systems,26 or troubleshooting pool players, but maintain that most appli-
cations of intelligent reasoning may be unaffected. But extra-causal constraints oc-
cur just about anywhere, including central diagnosis problems for example. When 
diagnosing circuit boards, one may be constrained by the fact that two components 
cannot fail simultaneously (Fl 1\ F2), for if one of them fails the circuit breaks and 
the other one cannot fail. Suppose there is a common cause C for the failures as in 
Figure 1. Then C fails to screen Fl off from F2 for p(hlc 1\ fd = 0 "I p(hlc). 
In medicine the opposite is the case: failure of one component in the human body 
increases the chances of failure of another, as resources are already weakened. In 
both these cases the constraints are very general and not the sort of thing one would 
want to call causes. 
But why not pursue causal extension and include these extra-causal constraints 
in a Bayesian network? Besides the problem of a loss of the causal interpreta-
tion, we have further difficulties. Knowledge of extra-causal constraints is often in 
some sense superfluous to an intelligent agent's needs. An agent performing diag-
24[Salmon 19801151 (my notation). 
25 [Spirtes et al. 1993163. 
26 As [Spirtes et al. 1993] do, pages 63-64. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
87 
Figure 1. Failure of circuitry components. 
Figure 2. Christmas tree sales, festivity, spending and orange sales 
nosis, for instance, needs to know about causes and effects because she has to find 
the probabilities of various causes given some symptoms, but she is not directly 
concerned with facts about meaning, experimental set-ups or physical laws. Thus 
if there is a requirement to keep the agent's language and causal graph small, as 
in the Bayesian network formalism where computational complexity is an issue, 
extra-causal constraints are the things to leave out. Second, it may be much harder 
for domain experts to provide the relevant extra-causal information than the causal 
information. In particular, discovering all physical laws which have correlational 
consequences on a domain is no mean feat. Third, even if a general constraint is 
identified, it is often difficult to say exactly how it should be connected to the other 
variables in a causal graph. Should there be an arrow between the set-up of a pool 
table and each possible pot. or just some? Extra-causal constraints are generally 
symmetric while causal relations are not. Fourthly, these constraints often vary 
between cases in the way that causal laws don't. If the set-up of a pool table is 
included in a causal graph and we are interested in predicting the next pot then, 
since the set-up changes as play progresses, the causal graph will also have to vary 
radically from shot to shot. This obviously complicates the task. 
Note finally that accidental and extra-causal correlations can combine to com-
plicate matters. If two variables are accidentally correlated then a common cause 
is very unlikely to completely screen off that correlation. More plausibly, the com-
mon cause would account for part of the correlation, and there would be a surplus 
that we might call accidental. An inefficient English bakery might partly explain 
why the water level rises in Venice (through global warming) and also partly why 
bread prices rise in the UK, but the remaining bulk of the correlation might be com-
pletely accidental. Likewise direct causes of an effect may not fully screen it off 
from their causes. In response to our first example of accidental correlation, one 
might put forward some causal story: high Christmas tree sales (C) causes people 
to be festive (F) which causes people to spend more (8) which causes orange sales 

88 
JON WILLIAMSON 
to rise (0), as in Figure 2. But even if this explains some of the correlation (and 
this is rather dubious), it will not explain it all, for p(ojc) = 1, but people spend 
money on many other occasions in the year and p(ojs) is not much bigger than 
p(o). So p(ojc 1\ s) > p(ojs). 
I hope to have shown that many types of dependency can be invoked to contest 
the validity of the objectively-interpreted independence assumption. Two strate-
gies present themselves if we look for a defence against the counterexamples, 
causal restriction and causal extension. However each strategy is subject to episte-
mological, practical and intuitive difficulties, rendering an objective interpretation 
of Bayesian networks at worst impossible and at best undesirable. 
3 
SUBJECTIVE NETWORKS 
We have seen how problems arise for an objective interpretation of the compo-
nents of a Bayesian network. But there is a further reason why an objective in-
terpretation is unattractive in practice: one may simply not know of all the causal 
variables or causal relations relevant to a domain of interest, and one may not be 
able to accurately estimate the corresponding objective probabilities required in 
the specification of a Bayesian network. In practice our knowledge is limited, and 
information in a Bayesian network will often be incomplete and inaccurate. 
Thus it makes sense to relativise the Bayesian network to an agent's perspec-
tive. In this section we shall suppose that the Bayesian network expresses the 
knowledge of a particular agent, X say -
that the graph G is interpreted as X's 
representation of causality, and that the probability specification S is interpreted 
as containing her degrees of belief in literals conditional on parent states. The 
independence assumption then links the agent's picture of causality to her belief 
function p: if it holds then her belief function is reducible to her Bayesian network. 
Does the independence assumption hold here? There is little reason to suppose 
that it might. X's knowledge of causality may be very limited, and her degrees of 
belief may wildly differ from objective probability: according to strict-subjectivist 
Bayesian theory X may hold whatever beliefs she likes, as long as her belief 
function is formally a probability function. Yet the independence assumption is a 
very strong constraint, for it fixes X 's belief function given her Bayesian network, 
thereby restricting X's subjectivity. If X's causal knowledge or the degrees of be-
lief in her probability specification were to change slightly then her other degrees 
of belief would have to change correspondingly, leaving no room for subjectivity 
with regard to these other beliefs. Therefore a strong constraint like independence 
does not fit well with subjectivism, whose appeal is based on the freedom it allows 
causal knowledge and degrees of belief. 
So how can a subjective interpretation of Bayesian networks be maintained? 
One line of reasoning goes something like this: if independence holds objectively, 
and the subjective network is similar to the objective network, then the subjective 
distribution determined by the subjective network will be close enough to objec-

FOUNDATIONS FOR BAYESIAN NETWORKS 
89 
tive probability to be put to practical use. Suppose we require an expert system for 
diagnosis of liver disease. We may think we have a fair idea of the causal picture 
relating this area, and may be able to obtain estimates of the objective probabilities 
for a probability specification, thereby forming a Bayesian network that is in some 
sense close to an objective version. If the independence assumption were to hold 
in the objective case then one might expect it to hold approximately in the subjec-
tive case. One might further suppose that if independence approximately held in 
the subjective case then the probability distribution determined by the subjective 
network might approximate objective probability, at least closely enough for the 
practical purposes of liver diagnosis. 
It is such a position that I want to argue against in this section. There are two 
flaws in the above reasoning. First, as we saw in the last section, there is often 
reason to doubt the independence assumption as made of objective causality and 
probability. Secondly, even if independence were to hold objectively, small differ-
ences between a subjective network and the objective network can lead to signif-
icant differences in the probability distributions determined by these networks. It 
is this second claim that I want to argue for here. 
For this argument it will be necessary to consider subjective and objective dis-
tributions and networks simultaneously, and so it will be worth spelling out the 
notation and concepts clearly in advance. The objective probability distribution 
is p*. We also have an objective Bayesian network consisting of causal graph 
G* and the associated probability specification S*. Independence is assumed to 
hold of objective causality G* with respect to objective probability p*, and this 
has the repercussion that the objective network (G*, S*) determines p*. Agent 
X has a subjective Bayesian network consisting of causal graph G and associated 
probability specification S. This subjective network (G, S) determines probability 
function p under the independence assumption. The question of whether indepen-
dence holds subjectively and p matches X's full belief function is not of concern 
here. Instead, we are concerned with the above alternative justification of the sub-
jective interpretation which claims that if the subjective network (G, S) closely 
resembles the objective network (G*, SO) then the functionp will be close enough 
to objective probability p* to be of practical use. I argue that differences between 
the objective and subjective networks that are likely to occur in practice will yield 
significant differences between resulting probability distributions. 
It will be useful to distinguish two types of difference between the subjective 
and objective networks: differences between the causal graphs G and G* and dif-
ferences between the probability specifications Sand S* . 
3.1 
Causal Subjectivity 
First I shall argue as follows. Even if we make the assumption that independence 
holds objectively, we assume that X's belief specification S consists of objective 
probabilities, and assume that her causal knowledge is correct (G is a subgraph 

90 
.. .. .. 
u 
U 
::l .. .. 
'" 
!! 
~ .. 
0.. 
JON WILLIAMSON 
10 
Figure 3. Nodes removed. 
of G*), then if, as one would expect, her causal knowledge is incomplete (a strict 
subgraph), p may be not be close enough to p* for practical purposes. 
There are two basic types of incompleteness. X may well not know about all 
the variables (G has fewer nodes than G*) or even if she does, she may not know 
about all the causal relations between the variables (G has fewer arrows than G*). 
To deal with the first case, suppose G is just G* minus one node C and the 
arrows connecting it to the rest of the graph. Even if G* satisfies independence 
with respect to p* then G can only be guaranteed (for all p*) to satisfy indepen-
dence if all the direct causes of C are direct causes of C's direct effects, each pair 
D, E of its direct effects have an arrow between them say from D to E, and the 
direct causes of each such D are direct causes of E.27 Needless to say, such a 
state of affairs is rather unlikely and a failure of independence will have practical 
repercussions. 
I ran a simulation to indicate just how close the subjectively-determined distri-
bution p will be to the objective distribution p* , the results of which form Figure 3. 
The bars in the background of the graph show the performance of Bayesian net-
works formed by removing a single node and its incident arrows from networks 
known to satisfy independence. For N = 2, ... , 10 I randomly generated Bayesian 
networks on N nodes, and for each net removed a random node, chose a random 
27 See [Pearl et aI. 1990] 82. 

FQUNDATIONS FOR BAYESIAN NETWORKS 
91 
state of nodes s and calculated p( cl s) for each literal c not in s. The new networks 
were deemed successful if their values for p(cls) differed from the values deter-
mined by the original network by less than 0.05, that is, Ip(cls) - p*(cls) 1< 0.05. 
For each N the percentage success was calculated over a number of trials28 and 
each bar in the chart represents such a percentage. The bars in the foreground of 
the graph represent the percentage success where half the nodes29 and their inci-
dent arrows were removed. 
Such experiments are computationally time-consuming and only practical for 
small values of N. While one should be wary of reading too much into a small 
data set, the results do suggest a trend of decreasing success rate as the size of the 
networks increase. Thus it appears plausible that if one removes a node and its 
incident arrows from a large Bayesian network that satisfies independence, then 
the resulting network will not be useful, in the sense that the probability values 
it determines will not be sufficiently close to objective probability. Moreover, re-
moving more nodes from a Bayesian net is likely to further reduce its probability 
of success, as the graph shows. 
This trend may be surprising, in that if one removes a node from a large causal 
graph one is changing a smaller portion of it than if one removes a node from a 
small graph, so one might expect that removing a node changes the resulting dis-
tribution less as the original number of nodes N increases. But one must bear in 
mind that the independence assumption is non-local: removing a node can imply 
an independency between two nodes which are very far apart in the graph. Thus 
removing a node from a small graph is likely to change fewer implied independen-
cies than removing a node from a large graph. 
Figure 4. Objective causal graph G* . 
o 
Figure 5. B and its incident arrows removed. 
Figure 6. B removed but its incident arrows redirected. 
28 At least 2000 trials for each N, and more in cases where convergence was slow. 
29In fact the nearest integer less than or equal to half the nodes was chosen. 

.. .. .. 
u 
u " .. .. 
co 
~ .. 
~ 
Q. 
92 
JON WILLIAMSON 
Figure 7. Nodes removed - arrows re-routed. 
Of course one may complain that such a simulation is unrealistic in some way. 
For instance, if one doesn't know about some intermediary cause in an objective 
causal graph, one may yet know about the causal chain on which it exists. Thus 
if Figure 4 represents the objective causal graph and one doesn't know about B, 
one may know that A causes C, as in Figure 6 rather than Figure 5. In this case 
removing B's incident arrows introduces an independence assumption which is not 
implied by the original graph, whereas redirecting them does not. In simulations 
I found that while redirecting rather than removing arrows improved success (see 
Figure 7) the qualitative lesson remained: the general trend was still that success 
decreases as the number of nodes increases. 
There is another way that the simulation may be unrealistic. Some types of 
cause may be more likely to be unknown than others, so perhaps one should not 
remove a node at random in the simulation. However, if we adjust for this factor we 
should not expect our conclusions to be undermined. To the extent that effects are 
more likely to be observable and causes to be unobservable, one will be more likely 
to know about nodes in the latter parts of causal chains than in the earlier parts. 
But while removing a leaf in a graph will not introduce any new independence 
constraints, removing common causes can do so. Thus if X is less likely to know 
about causes than effects, her subjective causal graph is even less likely to satisfy 
independence than one with nodes removed at random. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
93 
There may be other factors which render the simulations inappropriate, based 
on the way the networks are chosen at random. Here I made it as likely as not that 
two nodes have an arrow between them, and as likely as not that an arrow is in one 
direction as in another, while maintaining acyclicity. Thus the graphs are unlikely 
to be highly dense or highly sparse. I chose the specifying probabilities uniformly 
over machine reals in [0, 1]. Roughly half the nodes (N /2 nodes if N was even 
otherwise (N - 1)/2 nodes) were chosen to be symptoms in s and the nodes and 
their values were selected uniformly. In the face of a lack of knowledge about the 
large-scale structure of the objective causal graph I suggest these explications of 
'at random' are appropriate. In any case, the trend indicated by the simulation does 
not seem to be sensitive to changes in the way a network is chosen at random. 
In sum then, for a G* large enough to be an objective causal graph the removal 
of an arbitrary node is likely to change the independencies implied by the graph, 
and to change the resulting distribution determined by the Bayesian network. This 
much is arguably true whether or not the objective situation (G* ,p*) satisfies inde-
pendence itself, for if independence fails, removing arbitrary nodes is hardly likely 
to make it hold. 
Having looked at what happens when agent X is ignorant of causal variables, 
we shall now tum to the case where she is ignorant of causal relations. 
Suppose then that G is formed from G* by deleting an arrow, say from node 
Ci to node Cj â¢ Then G can not be guaranteed to satisfy independence with re-
spect to p*. For suppose Ci , D 1, ... , D k are the direct causes of Cj in G*. Then 
the independence of G with respect to p* requires that Ci be independent of Cj , 
conditional on Dl , ... , D k, which is not implied by the independence of G* with 
respect to p* . 
The situation is worse if the following condition holds, which I shall call the 
dependence principle.3o This corresponds to the intuition that a cause will either 
increase the probability of an effect, or, if it is a preventative, make the effect less 
likely. More precisely, 
â¢ dependence: if Ci, D 1, ... , Dk are the direct causes of Cj then Ci and Cj 
are probabilistically dependent conditional on Dl , ... , D k: there are some 
literals Ci and Cj of Ci and Cj and some state d of D 1 , ... , Dk such that 
p* (Cj ICi 1\ d) f:. p* (Cj Id), as long as these probabilities are non-extreme 
(that is, neither 0 nor 1). 
Now if G* satisfies dependence with respect to p*, the arrow between Ci and 
Cj is removed to give G as before, and the probabilities are non-extreme, the 
independence assumption will definitely fail for G with respect to p*. This is sim-
ply because the independence of G with respect to p* requires that Ci and Cj be 
independent conditional on D 1, ... , D k which contradicts the assumption that de-
pendence holds for G* with respect to p*. Note that this result only depends on 
30See [Williamson 1999) for a defence of this principle. Note that the dependence principle is a 
partial converse to the independence assumption. 

:: Â· 
u 
u il Â· 
DO 
~ 
~ Â· 
~ 
94 
JON WILLIAMSON 
10 
Figure 8. Arrows removed. 
the local situation involving Ci, Cj and the other direct causes D1 , ... ,Dk of Cj ' 
so that further changes elsewhere in the graph cannot rectify the situationY Note 
also that this result does not require that objective causality G* satisfy indepen-
dence with respect to objective probability p*. Thus if the dependence principle 
holds of causality in the world it is extremely unlikely that independence will hold 
of a subjective causal theory. 
Of course, we are arguing against independence by appealing to an alternative 
principle here and the sceptical reader may not be convinced by this last argument. 
But we can perform simulations as before to indicate the general trends. The 
back row of Figure 8 represents the results of the same simulation as before (the 
dependence principle is not assumed to hold), except with a random arrow rather 
than a node removed. In this case there is no clear downward trend, but success 
rate is uniformly low. If more arrows are removed, then for all but small N the 
resulting network is less likely still to satisfy independence, as the front row of 
Figure 8 shows, and again we see a downward trend as the number of nodes in G* 
increases. 
31 If one or more of the other direct causes or their arrows to Cj are also absent in G, then indepen-
dence may be reinstated, although this would be a freak occurrence and the extra change may break a 
further independence relation elsewhere in the graph. 

.. 
a 
~ .. 
!! . 
0.. 
FOUNDATIONS FOR BAYESIAN NETWORKS 
95 
10 
Figure 9. Node probabilities perturbed. 
In sum, causal subjectivity can lead to a significant difference between the sub-
jective and objective probability distributions. 
3.2 
Probabilistic Subjectivity 
Turning now to X 's degrees of belief, it is not hard to see how p can differ from p* . 
We suppose that the objective situation satisfies independence, and that X's causal 
graph G matches the objective causal graph G*. However, if her specification S 
differs from the objective specification then the probability function p determined 
by the subjective network (G, S) would not be expected to agree exactly with p* . 
The back row of Figure 9 shows what happens if one of the nodes has its associated 
probability specifiers perturbed by 0.03, the middle row shows what happens if half 
the nodes' probabilities are perturbed by 0 ... 03, and the front rows gives the case 
where all nodes have their probabilities perturbed. 
In practice probabilistic and graphical subjectivity will occur together, making 
it even less likely that p is close enough to p* for practical purposes. The back row 
of Figure 10 shows what happens if a node is removed (arrows re-routed), then an 
arrow is removed, and then one node's probabilities are perturbed by 0.03. The 
front row shows what happens if half the nodes then half the remaining arrows are 
removed, then half the remaining nodes are perturbed. 

96 
.. .. ., 
u 
u 
il . 
'" 
11 
f 
~ â¢ 
a. 
JON WILLIAMSON 
10 
Figure 10. Nodes and arrows removed, node probabilities perturbed. 
Thus subjectivity in a Bayesian network can lead, significantly often, to practi-
cal problems: the distribution determined by a subjective network may differ too 
much from the objective distribution to be of practical use. 
4 
TWO-STAGE BAYESIAN NETWORKS 
We have seen some of the problems that face interpretations of Bayesian networks. 
The independence assumption can fail for an objective interpretation because cor-
relations may be accidental or have non-causal explanations. Independence can 
hardly be expected to hold for a subjective interpretation -
the agent's Bayesian 
network will generally give rise to a probability function p which differs from her 
true belief function -
but more importantly p is also likely to differ from objective 
probability, which upsets the alternative justification of subjective networks. 
I want to argue for another view of Bayesian networks, which I believe rests 
on firmer foundations. The view I put forward here initially adopts a subjective 
interpretation, where the graph in the Bayesian network is an agent's representa-
tion of causal structure and the probability specifiers are her degrees of rational 
belief. I acknowledge the fact that, according to the above arguments, the distri-
bution specified by an agent's Bayesian network may not be close enough to the 
objective distribution to be of much practical use, but I argue that it is a good start-

FOUNDATIONS FOR BAYESIAN NETWORKS 
97 
ing point, and can be refined to better approximate reality. This gives a two-stage 
methodology where stage one is the representation of X's belief function p by an 
initial Bayesian network and stage two is the further refinement of the network. In 
terms of foundations, stage one yields a subjective interpretation (but a different 
subjective interpretation to those given in Â§3), while stage two borrows techniques 
from the abstract approach in order to deliver a network whose distribution more 
closely approximates the objective distribution (and in the process of refinement 
the causal interpretation may be dropped as we shall see). 
Two key questions require attention before we can be convinced of these two-
stage foundations for Bayesian networks. Firstly, how can stage one be justi-
fied? I have argued against a strict subjective interpretation, and so must somehow 
demonstrate that some other kind of subjective interpretation of the Bayesian net-
work is a good starting point. I shall do this in the rest of this section and the next 
section. Secondly, how can stage two be performed? I shall discuss the refinement 
of Bayesian networks in Â§6. 
I shall interpret X's Bayesian network as her background knowledge: the causal 
graph G contains her knowledge of causal variables and their causal relations, and 
the probability specification S is her knowledge of conditional probabilities of 
causes given parent-states.32 The independence assumption may then be used to 
determine X's degrees of belief from her background knowledge: her full belief 
function will be the probability function determined by the Bayesian network on 
G and S under the independence assumption. 
Thus independence is no longer a substantive assumption linking the agent's 
causal graph with some pre-determined rational belieffunction, it is a logic, used to 
derive undetermined degrees of belief from those that are given in X's probability 
specification. 
The central issue then is how we can justify the use of the independence as-
sumption as a means of determining a rational belief function. 
This issue of finding a single rational belief function given some background 
knowledge has received plenty of attention in the literature. Approaches range 
from Laplace's principle of indifference to Jaynes' maximum entropy principle. 
The former says that if X is indifferent as to which of J alternatives is true then 
she should believe each of them to degree 1/ J. The latter explicates and gener-
alises the former as follows. A probability function over C1 , ... ,CN may be fully 
specified by specifying values for each of the parameters x k l> .... kN = p( C1 = 
V~l 1\ ... I\CN = v~), wherevf' E {vi. ... , vf"N} fori = 1, ... , N. We have the 
constraints that each x k1 ..... kN E [0,1], and by additivity" 
x k1 ..... kN = 1, 
L.Jkl ..... kN 
together with any constraints implied by background knowledge. The maximum 
entropy principle says that in the absence of any further information X should 
select a most rational belief function by choosing the x k1 ..... kN subject to these 
321 shall leave it open as to whether these probabilities are taken to be estimates of objective proba-
bilities or informed degrees of belief. It suffices that they count as knowledge and may be used to guide 
X's other beliefs. 

98 
JON WILLIAMSON 
constraints which maximises the entropy 
H = - 2: 
Xkl , ... ,kN log x k 1> â¢â¢â¢ ,kN â¢ 
kl, ... ,kN 
There are several convincing justifications for the maximum entropy principle. 
The most well-known involves Shannon's information-theoretic interpretation of 
entropy as a measure of uncertainty, in which case we maximise entropy subject to 
some background knowledge if we determine a probability function whose infor-
mativeness is as close as possible to that of just the background knowledge itself. 
A second justification is based on Boltzmann's work with entropy in physics, and 
a third involves Paris and Vencovska's demonstration that the maximum entropy 
solution is the only completion to satisfy various intuitively compelling desider-
ata, such as language invariance.33 Grunwald gives a fourth, game-theoretic jus-
tification: maximum entropy is the (worst-case) optimal distribution for a game 
requiring the prediction of outcomes under a logarithmic loss function. 34 
Where does this leave independence and stage one of our two-stage method-
ology? Stage one is justified because the probability function determined by the 
independence assumption from the Bayesian network coincides with that deter-
mined by the maximum entropy principle, as we shall now see. 
5 
BAYESIAN NETWORKS HAVE MAXIMUM ENTROPY 
The argument for the identity of the Bayesian network and maximum entropy func-
tions requires first making the constraints imposed by the background knowledge 
explicit, and next showing that if we maximise entropy subject to these constraints 
then we get the same solution as that determined by the Bayesian network under 
the independence assumption. 
5.1 
Background Knowledge 
Agent X's background knowledge consists of the components of a causally inter-
preted Bayesian network: a causal graph and the specified probabilities of literals 
conditional on states of their parents. We first need to formulate this knowledge 
in a way that can more formally be applied to the maximum entropy procedure. 
Regarding the probability specification, there is no problem. We can simply max-
imise entropy subject to the constraints that certain probabilities, namely those in 
the Bayesian network specification, are fixed from the outset. However, the causal 
graph does not provide obvious constraints -
it is of qualitative form, free from 
notions like entropy or probability. Therefore we need some procedure for turning 
the causal information into a constraint on probability. 
33See [Paris 1994], [Paris & Vencovska 1997], [Paris 1999] and [Paris & Vencovska 2000 for the 
details of these justifications. 
34[Griinwald 2000]. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
99 
I suggest that the causal interpretation imposes the following constraint. Sup-
pose we are presented with the components of a Bayesian network involving vari-
ables C1 , .â¢. ,CN and then use these to determine a single rational belief func-
tion PI, whether by independence, maximum entropy or some other means. Then 
we find out further causal information, namely that there are some new variables 
D 1, ... , D M to be added to the causal graph, and that these variables are not causes 
of the current C-variables C1 , ... ,CN. Intuitively, this new information should 
not affect our understanding of the original problem on the C -variables. More pre-
cisely, suppose the new information takes the form of an extension of the original 
causal graph where the D-variables do not cause C-variables, and an extension to 
the probability specification incorporating new conditional probabilities of the D-
variables given their parents. If we use this new Bayesian network to determine a 
new rational belief function P2 over the larger domain C 1, ... , C N, D1, ... , D M , 
then the restriction of P2 to the C-variables should agree with PI. the function 
based just on the C-variables. I shall call this the principle of causal irrelevance: 
learning of the new variables should be irrelevant to degrees of belief on the pre-
vious domain. 
This principle is based on an asymmetry of causation whereby information 
about causes can lead to information about their effects, but knowledge of effects 
does not provide useful information about causes. This is not to say that informa-
tion about the value or occurrence of an effect is irrelevant to the question of what 
the value of its cause is (which is clearly wrong), but that information of the form 
that a variable has an effect of unknown value is irrelevant to its own value. The 
same need not be true of causes: if two variables thought to be causally unrelated 
are found to have a common cause, one may be wise to suppose that these variables 
are probabilistic ally dependent to a greater extent than previously thought. 
Take a simple example: suppose L signifies lung cancer and B bronchitis. We 
know of no causal relations linking the two variables, and have the probabilities 
p(l),p(b) for each literall, b involving L, B respectively. We then use this infor-
mation to determine a joint probability distribution PI over Land B. Suppose we 
later learn that S, smoking, is a cause of lung cancer and of bronchitis, and we find 
the probabilities p(lls), p(bls), p(s) for each literall, b, s involving L, B, S respec-
tively. Then, because S is a common cause, we might be inclined to form a new 
belief function P2 over L, Band S which renders Land B more dependent than 
they were under PI: P2 (ll b) > PI (ll b) for some literals land b. The motivation 
is that if we find out b, then we now know this may be because some literal s has 
caused b, in which case s may also have caused I, making it more likely than we 
would previously have thought. 
Suppose next we learn that each of lung cancer and bronchitis cause chest pains 
C, as in Figure II. If we find values for p( cll 1\ b) for each literal c, I and b, and 
form a new belief function P3, the causal irrelevance condition requires that P3 
must not differ from P2, over S, Land B. For example, P3 (II b) = P2 (II b), for each 
I and b. The idea here is that if we learn b, then knowledge of the existence of the 
common effect C does not give us a new way l may occur and so our degree of 

100 
JON WILLIAMSON 
Figure 11. Smoking, lung cancer, bronchitis and chest pains. 
belief in l should not change. C is irrelevant to S, Land B. 
In sum, I shall assume that the process of determining a single rational belief 
function is constrained not only by the probability values in the specification of 
the Bayesian network, but also by the causal graph under the principle of causal 
irrelevance. The principle of causal irrelevance is strong enough to allow causal 
information to constrain rational belief, and thereby playa part in our new justifi-
cation of the independence assumption, yet, unlike the independence assumption, 
weak enough to be uncontroversial in itself. 
5.2 
Maximising Entropy 
The key proposition is this: 
BAYESIAN NETWORKS MAXIMISE ENTROPY 
Given the probability specification and causal graph of a Bayesian network 
and the principle of causal irrelevance, the distribution which maximises en-
tropy is just the distribution determined by the Bayesian network under the 
independence assumption. 
Proof. The strategy of the proof will be to use Lagrange multipliers to derive 
conditions for entropy to be maximised, and then show that the Bayesian net-
work distribution satisfies these conditions. This straightforward method is pos-
sible for the following reason. The constraints -
which consist of the specified 
probabilities, certain probabilities fixed by the causal graph under causal irrele-
vance, and additivity constraints common to all probability distributions -
are 
linear and restrict the domain of the entropy function to a compact convex set in 
[O,I]Kl x ... x [0, I]KN ,35 and on that domain, entropy is a strictly concave func-
tion (as shown below). Thus the problem has a unique local maximum, the global 
maximum, and if the Bayesian network distribution satisfies the conditions for an 
optimal solution then it must be the unique global maximum. 
We can see that entropy is strictly concave as follows. H is strictly concave if 
and only if, for any two distinct vectors a and b of the parameters xkl, ... ,kN and 
35See [Paris 1994], proposition 6.1, page 66. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
101 
A E (0,1), 
H(Aa + (1 - A)b) > AH(a) + (1 - A)H(b) Â¢} 
ALai log ai+(I-A) L bi log bi- L(.~ai+(I-A)bi) 10g(Aai+(I-A)bi) > Â° 
" 
ai 
" 
bi 
Â¢} A L... ai log Aai + (1 _ A)bi + (1 - A) L... bi log '\ai + (1 _ '\)bi > Â° 
Â¢} '\d(a,'\a + (1 - '\)b) + (1 - '\)d(b,'\a + (1 - '\)b) > 0, 
where d signifies cross entropy, a measure of distance of probability distributions, 
and a, band'\a + (1 - '\)b are non-zero since L ai = 1 = L bi,'\ E (0,1). dis 
well known to be non-negative and strictly positive if its arguments are distinct.36 
Thus d(a,'\a + (1 - '\)b) is strictly positive if a i- '\a + (1 - '\)b, which is true 
since a and b are distinct and ,\ E (0,1). Therefore H is strictly concave and the 
Lagrange multiplier approach will yield the global maximum. 
The next thing to do is to reformulate the optimisation problem to make it suit 
the Bayesian network framework. This means finding more appropriate param-
eters than the standard X k1 â¢...â¢ kN mentioned above. Without loss of generality 
we can suppose the nodes C1 , ... ,CN are ordered ancestrally with respect to 
the causal graph G in the Bayesian network: that is, all the parents of Ci in G 
come before Ci in the ordering.37 To make the proof clearer we shall also sup-
pose that all the probabilities in the specification are positive -
we shall see later 
that zeros do not affect the result. Let C~i represent the literal Ci = v7i , for 
ki = 1, ... , K i , i = 1, ... , N. The new parameters are 
for i = 1, ... , N. The main thing to note about this parameterisation is that by the 
chain rule of probability, 
Now we shall translate the entropy formula into this framework (in what fol-
lows we shall minimise negative entropy - H, which is equivalent to maximising 
entropy H):38 
- H = L 
x klo .... kN log x k1 ..... kN 
kl ..... kN 
" 
[lIN k1 ..... k;_1] ~l 
k1 ..... ki_l 
L... 
Yj.k; 
L... ogYi.ki 
kl ..... kN 
j=1 
i=1 
~----------------~-
36See [Paris 1994] proposition 8.5 for example. 
37Recall that such an ordering is always possible because of the dag structure of the causal graph. 
38Note that the existence and uniqueness of a maximum is independent of parameterisation. 

102 
JON WILLIAMSON 
[lI
N 
k1, ... ,kj _ 1]1 
kl, .. Â·,ki-l 
Yj,kj 
ogYi,k. 
j=l 
N 
=2: 2: 
N 
[ 
i 
] 
- ~ ~ II kl, ... ,kj _ 1 1 
k1, ... ,k'- 1 
- L..J L..J 
Yj,kj 
ogYi,k. 
' 
i=l kl, ... ,k. 
j=l 
where we make this last step because for each i we can separate out 
and these terms cancel to 1 by additivity of probability. 
We shall deal with three types of constraints. The specification constraints are 
determined by those values provided in the Bayesian network specification. Causal 
constraints are determined by the causal graph under the causal irrelevance condi-
tion. Finally additivity constraints are imposed by the axioms of probability. While 
one might suspect that all these constraints would lead to a complicated optimisa-
tion problem, we will see that by adopting an inductive approach we will be able 
to form a Lagrangian function which only incorporates relatively few specification 
and additivity constraints. 
Within the new framework we can write the specification constraints as 
where the Cr1 , â¢â¢â¢ , crL involve the parents of Gi , T1, ... ,TL < i (thanks to the an-
cestral order) and i = 1, ... , N.39 We also have constraints imposed by additivity: 
"'" 
kl, .. Â·,k'_l 
1~ 
hk 
k 
. 
1 
N 
LJk.Yi,k. 
= 
loreac 
1,"" 
i-1,Z= , ... , 
. 
Decomposing the entropy as H = L:~1 Hi where 
H- -
~ [IIi k1, ... ,kj _ 1]1 
kl, .. Â·,k'_l 
t -
L..J 
Yj,kj 
ogYi,k. 
' 
kl, ... ,k. 
j=l 
we shall prove the proposition by induction on N. The case N = 1 is trivial since 
the constraints p( C~l) = a1,k l completely determine the probability distribution 
over G1 : there is nothing to do to maximise entropy and so the Bayesian network 
distribution, which satisfies the constraints, maximises entropy. Suppose the in-
duction hypothesis holds for N - 1 and consider the case for N. It is here that we 
apply the principle of causal irrelevance to generate the causal constraints on the 
maximisation process from the causal graph. Since the variables are ordered ances-
trally, the move from N -1 to N essentially involves incorporating a new variable 
39Note that the Tl, ... ,r L depend on i. I am inclined to avoid any further subscripting however. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
103 
C N which is not a cause of any of the previous variables C 1, ... , C N -1. Hence 
if we maximise entropy on this new domain and restrict the resulting probability 
function to C1, ... , C N -1 then by causal irrelevance we must have maximised en-
tropy on this smaller domain. Applying the induction hypothesis on this smaller 
domain {C 1, ... , C N -I}, we see that entropy is maximised if the distribution is 
determined by the Bayesian network on C1, ... , CN-1. Thus fori = 1, ... , N -1, 
h 
kl, ...â¢ ki-1 
b fi ed 
kr1.Â·Â·Â·.krL 
N 
H 
H 
. 
t e parameters Yik. 
must e x 
toaik. 
. 
ow 
1, â¢â¢. , 
N-l lD-
volve only these fix~d parameters, so in order t~ 'maximise H all that remains is to 
maximise HN with respect to Y~.k/N-1, subject to the specification constraints 
fixing the values a~.lk~Â·Â·krL and the additivity constraints "LkN Y~.k/N-1 = 1 
for each kl , ... , k N -1 . 
We shall now adapt the specification constraints. 
Let bkr1.Â·Â·Â·.krL = p(C~;l /\ ... /\ C~~L) and ek1 â¢...â¢ kN-1 = n .<N Y~~:Â·Â·Â·kj-1 be 
J 
J. J 
constants, fixed by having maximised entropy on C1, ... , C N -1. Then 
kr 
kr 
k 
k 
(kN 
kr1 
krL ) 
a 1Â·Â·Â·Â·Â· L b r1Â·Â·Â·Â·Â· rL = p CN /\ CT1 
/\ ... /\ CTL 
N.kN 
L 
p(C~l /\ ... /\ C~) 
ki ,itT1 , ... ,TL,N 
ki ,itT1 , ... ,TL.N 
We are now in a position to specify the Lagrangian function for the minimisa-
tion of -HN: 
+ 
L 
/Lk1, .... kN- 1 [LY~,k/N_1 - 1] 
k1, ... ,kN-1 
kN 

104 
JON WILLIAMSON 
jlk1, ... ,kN- 1 [y~,k/N-l - 1/ KN] ). 
By Lagrange's theorem,4o in order to find conditions for a minimum we must 
first check a constraint qualification. Enumerate the constraints h, ... , h. Form 
a matrix A by letting each row i consist of the partial derivatives 
Finally check that the rank of A is J -
this is easily done and I shall avoid the 
details here. 
Entropy is maximised if the partial derivatives of the Lagrangian are zero, 
Given any such equation we can eliminate the Lagrange multiplier jlk1, ... ,kN_ 1 
by finding another equation involving k'rv :/; kN, 
(there will always be another such equation since eN has at least two values), and 
substituting to give a new equation 
We next eliminate the multiplier expression on the left-hand side by finding an-
other such equation involving k~, ... , k'rv -1 such that k~l = kr1 , ... ,k~L = krL Â· 
There will always be another such equation unless L = N - 1, in which case the 
constraints uniquely determine the Bayesian network distribution, and entropy is 
trivially maximised. This then gives 
Finally, all we need do is note that in the Bayesian network distribution the con-
straints are satisfied and the independence assumption implies that 
k~, ... ,k~_l 
k~l , ... ,k~L 
kr1,Â·Â·Â·,krL _ 
kr1,,,Â·,krL 
YN,kN 
= YN,kN 
= YN,kN 
-
aN,kN 
40See for example [Sundaram 1996] Â§5.2.1 

FOUNDATIONS FOR BAYESIAN NETWORKS 
105 
in which case we substitute into our condition: 
and find that it clearly holds. Thus the Bayesian network distribution is the entropy 
maximiser, as required. 
All that remains is to point out what happens when specifiers may be zero. 
There are two (compatible) scenarios: if some aJ~rr,Â·Â·,krL = 0 for j < N then 
, , 
the corresponding ek1,,,,,kN-l = TIj<N y;,~;",kj_l, which by the induction hy-
pothesis is TIj<N a;:r",kr L ,vanishes. This eliminates entropy terms and con-
straints equally, leaving fewer partial derivative conditions. These conditions are 
satisfied as above. The second scenario is that some a~,l~~"krL = O. In this 
case the Lagrangian and partial derivatives are as before, the constraints are sat-
isfied as before, but when substituting zeros in the partial derivatives we make 
use of the convention, common when dealing with the cross entropy measure, that 
O[log 0 - log OJ = 0 log 0/0 = O. Thus the conditions are satisfied by null speci-
~n. 
â¢ 
Thus we see that the independence assumption can be justified after all. The 
important thing to remember is that under the two-stage foundations, the inde-
pendence assumption is neither a fact of causality nor even an assertion about an 
agent's knowledge. It is a mechanism that can be used to derive new probabil-
ity statements from those in the agent's background knowledge. Independence is 
justified because as a logic it coincides with maximum entropy, which has well 
known justifications. 
6 
STAGE TWO 
Given background knowledge consisting of a causal graph G and associated prob-
ability specification S, we can represent the rational (maximum entropy) belief 
function p by the Bayesian network on G and S. This is stage one of the two-stage 
methodology. However, while p is rational given background knowledge, it may 
not bear a close enough resemblance to objective probability to be put to practical 
use. If that is the case then we need to transform the Bayesian network into one 
which more closely approximates objective probability. This is stage two of the 
two-stage methodology. Bayesian networks may be applied to medical diagnosis 
for example, or fault-finding in aeroplanes. In such high risk scenarios it is not 
sufficient that any decisions are deemed reasonable given a lack of relevant infor-
mation: it would be negligent not to collect enough relevant information to reliably 
model the objective situation. 
Thus the next step is to refine the Bayesian network in the light of new in-
formation, in order to achieve greater reliability. Many of the algorithms from the 

106 
JON WILLIAMSON 
extensive literature on learning Bayesian networks from data41 can be applied here. 
In the rest of this section I will summarise my own ideas in this respect -
these 
are simple techniques which I believe have a clear justification that coheres well 
with the entropy-based approach of the last section.42 First I shall deal with the 
case where new causal information comes to be known. After this I shall address 
the following questions. What sort of information should one collect in order to 
best refine the network? How one can limit the complexity of the network? 
6.1 
Causal Information 
Suppose our agent X finds out that Ci causes Cj . I suggest that she should just 
add an arrow from Ci to Cj to her initial causal graph (if there is no arrow there 
already), and she should ensure her specifying probabilities p( Cj I dj) take this new 
parent into account. There are two possible justifications of this adding-arrows 
strategy. One can apply the arguments of the last section. If X learns of the new 
causal link and the corresponding probabilities then her background knowledge 
now includes an extended causal graph and probability specification, in which 
case she shoul~ maximise entropy by adopting the new Bayesian network formed 
by adding the arrow and the specifiers. 
The second possible justification relies on the dependence principle43 as op-
posed to causal irrelevance, as follows. Suppose we start off with Bayesian net-
work (G, Sa), where G is X's causal graph and Sa is her associated probability 
specification, whose entries we shall assume agree with the objective probabilities 
p*(cildi). Then we add an arrow from Ci to Cj and change the specified proba-
bilities to give a new network (H, SH). We measure the improvement of the new 
network over the old by how much closer its induced probability function PH is to 
the objective probability function p* than Pa, according to the usual measure of 
distance between probability functions, cross entropy. Then we have the following 
facts: 
IMPROVEMENT OF ADDING ARROWS 
(i) the new network is no worse a network than the initial network; 
(ii) the new network is a better network if and only if Cj is probabilistically 
dependent on Ci , conditional on Cj's other parents D. 
In particular, if the dependence principle holds then the fact that Ci is a cause 
of Cj entails that the two nodes are conditionally probabilistic ally dependent and 
thus that the probability distribution represented by the new network is closer to 
41See [Jordan 1998] and [Buntine 1996] for good surveys. 
42 Some related work: the Kulat6 algorithm of [Herskovitz 1991] also has an entropy-based justifi-
cation. However it involves minimising entropy and poses significant compulational problems in the 
worst case. Uitnah 1999] employs mutual information as I do, but as a technique for probabilistic 
inference given a Bayesian network rather than a means for deriving the network itself. 
43Reca1l that the dependence principle says that a direct cause changes the probability of its effect 
conditional on the effect's other causes. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
107 
the target objective distribution than that of the old network: we are justified in 
adding an arrow from Ci to Cj . 
Proof. For simplicity (but without loss of generality as we shall see shortly) we 
shall assume that Pa and PH are strictly positive over the atomic states Cl /\ ..â¢ /\CN. 
For (i) we need to show thatd(p* ,PH )-d(p* ,pa) ~ 0, where d is cross entropy 
distance. So, 
d(p* ,PH) - d(p* ,pa) = I>*(s) In P*((S)) - LP*(s) In P*((s)) 
s 
PH S 
s 
Pa s 
= LP*(s) In pa(s) , 
s 
PH(S) 
where the s are the atomic states, and bearing in mind that PH(S) > 0. Now for 
real x> 0, In (x) ~ x - 1. By assumption Pa (S)/PH (s) > 0, so 
and thus we need to show that 
"'" *( )pa(s) < 1 
L...JP s 
() _ . 
s 
PH S 
Now since we are dealing with Bayesian networks, 
pa(s) 
ITp*(ckldr) 
PH(S) = ITp*(ckldf) ' 
for each literal Ck consistent with s, where dr is the state of the parents of C 
according to G which is consistent with s, and likewise for dr. But H is just G 
but with an arrow from Ci to Cj , so the terms in each product are the same and 
cancel, except when it comes to literals Cj involving node Cj . Thus 
pa(s) 
p*(cjldr) 
p*(cjld) 
PH(S) = p*(cjldf) = P*(CjICi /\ d)' 
where we just let d be dr and Ci the remaining literal in df. Substituting and 
simplifying, 
L 
*( )pa(s) 
L 
*( 
) p*(cjld) 
P s -(-) = 
P Ci /\ Cj /\ d (I 
d) 
s 
PHS 
p* Cj Ci /\ 

108 
JON WILLIAMSON 
Consider the new set of variables {Ci , Cj , D} where Ci and Cj are as before 
and D takes as values the states of the parents of Cj according to C. Form a 
Bayesian network T incorporating the graph Ci -----+ D -----+ Cj (with specifying 
probabilities determined as usual from the probability function p*). Then since 
T is a Bayesian network, Lp*(cjld)p*(dlci)P*(Ci) = LPT(Ci /\ Cj /\ d) = 1 
by the additivity of probability. Thus Ls p* (s)pc(s )/PH(S) = 1 so d(p* ,PH) -
d(p* ,pc) ::; 0, as required. 
Let us now turn to (ii). From the above reasoning we see that 
* 
* 
Pc ( S ) 
Pc ( S ) 
d(p ,PH) - d(p ,Pc) < 0 Â¢:} In -(-) < -(-) - 1 
PH S 
PH S 
for some atomic state s. But In x < x-I Â¢:} xi-I, and 
pC(S) --I-
P*(CjICi) 
--I-
*( I 
d) 
*( Id) --I- 0 
-(-) 7'" 1 Â¢:} (I 
d) 7'" 1 Â¢:} P Cj Ci /\ 
- P Cj 
7"', 
PH S 
p* Cj Ci /\ 
where the Ci, Cj, d are consistent with s. Therefore, d(p* ,PH) - d(p*, pc) < 0 if 
and only if there is some Ci, Cj, d for which the conditional dependence holds. 
The assumption that Pc and PH are positive over atomic states is not essential. 
Suppose PH is zero over some atomic states. Then in the above, 
"'" * ( ) I pc ( s) = 
L..-P 
S n 
() 
s 
PH S 
"'" 
* ( ) I Pc ( s) + "'" 
* ( ) I Pc ( S ) 
L..-
P 
S 
n 
() 
L..-
P 
S 
n 
(). 
PH S 
PH S 
S:PH(SÂ»O 
S:PH(S)=O 
The first sum on the right hand side is::; 0 as above. The second sum is zero 
because each component is, as we shall see now. Suppose pH(S) = O. Then 
rr~=l p* (Ck Idf) = 0 so p* (Ck /\ df) = 0 for at least one such k, in which 
case p( s) = 0 since by the axioms of probability, p( u) = 0 => p( u /\ v) = O. 
Now in the sum readp*(s) Inpc(s)/PH(S) to bep*(s) Inpc(s) - p*(s) InpH(s). 
In dealing with cross entropy by convention 0 In 0 is taken to be O. Therefore 
p* (s) lnpc(s)/PH(S) = 0 Inpc(s) - 0 = O. The same reasoning applies if Pc is 
zero over some atomic states. Likewise, if p* (s) is zero then p* (s) In Pc (s) / P H (s) 
is zero too. 
â¢ 
This justifies the adding-arrows approach if X learns of a new causal link 
amongst the current variables. If she learns of a new variable CN +1 that is causally 
related to one or more of the other variables, and she also learns the probabilities 
P(CN+lldN+1), then we can apply the above argument (or equally the arguments 
of Â§5) to show that X's new network should be constructed from her old network 

FOUNDATIONS FOR BAYESIAN NETWORKS 
109 
by adding the new node and causal arrows to her graph and the new probabilities 
to her specification. 
Finally note that the above argument only requires that the added arrow links 
conditionally probabilistic ally dependent nodes. As we have discussed in Â§2, 
nodes need not be causally related to be probabilistically dependent. Therefore, 
if our agent is presented with information to the effect that two nodes are condi-
tionally dependent, she is justified in adding the corresponding arrow to her net-
work, regardless of whether those nodes are causally related. But as a result of 
this generalisation, the graph in the agent's Bayesian network need no longer be 
causally interpreted: the Bayesian network becomes an abstract tool for represent-
ing a probability function. 
6.2 
Mutual Information 
We now have a strategy for changing the network when causal information or other 
probabilistic dependencies are presented to the agent. But is there a strategy for 
seeking out a good arrow to add? By adding arrows we increase both the size of 
the specification required in the Bayesian network (the space complexity) and the 
time taken to calculate probabilities from the network (the time complexity) -
is 
there a means of limiting these complexities to prevent the network from becoming 
impractical? I shall address both these questions in this section. 
The key to limiting complexity consists in finding constraints C such that 
Bayesian networks satisfying C have acceptable complexity, and then ensuring 
that (i) the current network satisfies C, and (ii) an arrow is only added to the cur-
rent network if the reSUlting network continues to satisfy C. Consider by way of 
example the following constraints. 
â¢ C1 : no node has more than K parents, for some constant K. This bound on 
the number of parents serves to restrict the space complexity of a Bayesian 
network. For instance if K = 0 then the discrete network (no arrows) is 
the only available network, if K = 1 then all networks satisfying C1 have 
graphs that are forests, and if K = N - 1 there is no restriction at all on the 
networks. It is easy to see that if all variables are binary, the complexity of 
a network satisfying C1 is less than or equal to (N - K + 1)2K - 1, a value 
that is linear in N. 
â¢ C2 : the Bayesian network has space complexity of at most K.. Now if K. = N 
the only network to satisfy C2 is the discrete network and if K. = 2N - 1 
any network satisfies the constraint. Depending on the problem in hand and 
available resources we will want to choose an appropriate value for K. or K 
which balances the range of networks available with their complexity. 
â¢ C3 : the graph is singly-connected. Having a singly connected graph ensures 
that the Bayesian network can be used to calculate required probabilities 
efficiently (in time linear in the number of nodes N). Note however that a 

110 
JON WILLIAMSON 
singly-connected network can have space complexity up to 2N - 1 + N - 1 
on binary-valued nodes, so in practice this constraint may best be used with 
another which limits space complexity. 
In sum, if we fix some constraints C the goal then is to find a constrained net-
work (a Bayesian network satisfying C) which gives a good approximation to the 
target objective distribution p* (using cross entropy as a measure of degree of ap-
proximation). 
We shall associate a weight with each arrow in a Bayesian network as follows. 
In order to weigh the arrows going into a node Gi we enumerate the parents of Gi 
as Dl, ... , Dk. Then for j = 1, ... , k we weigh the arrow from Dj to Gi by the 
conditional mutual information, 
where d ranges over the states d1 /\ â¢.â¢ /\ dj - 1. Then: 
MAX-WEIGHT ApPROXIMATION 
The network subject to constraints C which affords the closest approximation 
to p* (according to the cross entropy measure of distance) is the network 
satisfying C whose arrow weights are maximised. 
Proof. The distance between the probability function p determined by X 's Bayesian 
network and the target function p* is 
p*(s) 
d(p*,p) = I>*(s)log-
p(s) 
s 
N 
= LP*(s) logp*(s) - LP*(s) log IIp*(cildi) 
s 
s 
i=l 
where the Ci and di are consistent with s, 
N 
= LP*(s)logp*(s) - LP*(s) Llogp*(cildi) 
s 
s 
i=l 
N 
N 
= -H(p*) - LI(Gi,Di) + LH(p*lcJ 
i=l 
i=l 
where H(P*) is the entropy of function p*, I(Gi, Di) is the mutual information 
between Gi and its parents and H (p* b) is the entropy of p* restricted to node 

FOUNDATIONS FOR BAYESIAN NETWORKS 
111 
Gi . The entropies are independent of the choice of Bayesian network so the dis-
tance between the network and target distributions is minimised just when the total 
mutual information is maximised.44 
Note that 
I(A, B) + I(A, GjB) 
"'" * 
[p* (a /\ b) 
p* (a /\ cj b) ] 
= ~ 
p (a /\ b /\ c) log p*(a)p*(b) + log p*(ajb)p*(cjb) 
a,b,c 
L * 
b 
1 
p*(a /\ b)p*(a /\ b /\ c)p* (b)p* (b) 
= 
a/\ /\c 0 
p ( 
) g p* (a)p* (b)p* (b)p*(a /\ b)p*(c /\ b) 
a,b,c 
L * 
p* (a /\ b /\ c) 
= 
p (a/\b/\c)log 
() ( 
b) =I(A,{B,G}). 
p* a p* c /\ 
a,b,c 
By enumerating the parents Di of Gi as D1, ... , Dk, we can iterate the above 
relation to get 
Therefore, 
I(Gi, Di) = I(Gi, D1) + I(Gi, D2jD1)+ 
I(Gi,D3j{D1,D2}) + ... + I(Gi,Dkj{D1, ... ,Dk- 1}). 
i=1 
i=1 
j 
and the cross entropy distance between the network distribution and the target 
distribution is minimised just when the sum of the arrow weights is maximised. â¢ 
Note that this result is independent of choice of enumeration of the variables, 
as can be seen from the proof. 
There are various ways one might try to find a constrained network with max-
imum or close to maximum weight, but perhaps the simplest is a greedy adding-
arrows strategy: start off with the discrete graph and at each stage find and weigh 
the arrows whose addition would ensure that the dag structure and constraints C 
remain satisfied, and add one with maximum weight. If more than one best arrow 
exists we can spawn several new graphs by adding each best arrow to the previous 
graph, and we can constantly prune the number of graphs by eliminating those 
which no longer have maximum weight. We stop the algorithm when no more 
arrows can be added.45 
Given this algorithm and its justification, we now have answers to our two ques-
tions of this section. We seek out a good arrow to add by finding the arrow with 
44This much is a straightforward generalisation of the proof of [Chow & Liu 1968] that the best 
tree-based approximation is the maximum weight spanning tree. 
4SSee [Williamson 2000b] and [Williamson 2000] for analyses of the performance of this algorithm, 
which turns out to be remarkably effective for a greedy approach. 

112 
JON WILLIAMSON 
maximum conditional mutual information weight. We limit the complexity of the 
network by imposing constraints on the network. 
Thus in stage two of the two-stage methodology we can improve the causal 
network obtained in stage one by adding arrows -
these arrows link causally 
related variables or more generally probabilistically dependent variables, and a 
good strategy is to add the weightiest arrow which does not violate constraints 
on the complexity of the network. The conditional mutual information weighting 
is a measure of conditional dependence and so in effect the strategy is to add an 
arrow between two nodes that are most (conditionally) dependent. The resulting 
graph will not necessarily reflect the true causal relations amongst the variables, 
and so stage two corresponds more closely to the abstract foundations for Bayesian 
networks than any causal interpretation. 
7 
CONCLUSION 
While the independence assumption poses significant problems for a straightfor-
ward objective or subjective interpretation of Bayesian networks, independence 
can be though of as a means of determining a rational belief function from an 
agent's background knowledge. Thus Bayesian networks can be given firm foun-
dations by adopting a two-stage approach, whereby one first adopts a subjective 
causal interpretation which may then be dropped as the network is refined in order 
to better approximate a target objective probability function. These foundations 
appeal to information-theoretic notions and assumptions about causality which are 
somewhat less contentious than the independence assumption. Stage one is jus-
tified by maximum entropy considerations while an adding-arrows strategy for 
stage two can be justified by minimising cross entropy relative to the objective 
distribution. This approach is not subject to many of the problems that beset the 
objective or subjective interpretations considered in Â§2 and Â§3: we do not need to 
worry about individuation of variables, and stage two can be used to compensate 
for the presence of accidental and extra-causal dependencies and any discrepan-
cies between the subjective network and an objective causal network. The advan-
tage over the abstract approach is that we don't require a database of past case 
data to determine a network -
stage one makes use of causal and probabilistic 
background knowledge. The two-stage methodology can be viewed as a way of 
integrating background knowledge (including qualitative causal knowledge) with 
machine learning techniques (of which the adding-arrows strategy is one exam-
ple).46 
Department of Philosophy, King's College, London. 
46Thanks to David Corfield, Donald Gillies and Jeff Paris for helpful comments, and the UK Arts 
and Humanities Research Board for funding this research, 

FOUNDATIONS FOR BAYESIAN NETWORKS 
113 
BIBLIOGRAPHY 
[Andersson et al. 1996] S. Andersson, D. Madigan & M. Perlman: 'An alternative Markov property for 
chain graphs', Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence, Port-
land OR: Morgan Kaufmann, pages 40-48. 
[Amtzenius 1992] Frank Arntzenius: 'The common cause principle', Philosophy of Science Association 
1992 (2), pages 227-237. 
[Binder et al. 1997) John Binder, Daphne Koller, Stuart Russell & Keiji Kanazawa: 'Adaptive proba-
bilistic networks with hidden variables', Machine Learning 29, pages 213-244. 
[Buntine 1996) Wray Buntine: 'A guide to the literature on learning probabilistic networks from data', 
IEEE Transactions on Knowledge and Data Engineering 8(2), pages 195-210. 
[Butterfield 1992) Jeremy Butterfield: 'Bell's theorem: what it takes', British Journal for the Philosophy 
of Science 43, pages 41-83. 
[Cartwright 1989) Nancy Cartwright: 'Nature's capacities and their measurement', Oxford: Clarendon 
Press. 
[Chow & Liu 1968) C.K. Chow & C.N. Liu: 'Approximating discrete probability distributions with de-
pendence trees', IEEE Transactions on Information Theory IT-14, pages 462-467. 
[Fisher 1935) Ronald Fisher: 'The design of experiments', Edinburgh: Oliver & Boyd. 
[van Fraassen 1980) Bas C. van Fraassen: 'The scientific image', Clarendon Press, Oxford. 
[Frydenberg 1990) M. Frydenberg: 'The chain graph Markov property', Scandanavian Journal of Statis-
tics 17, pages 333-353. 
[GIymour 1997) Clark GIymour: 'A review of recent work on the foundations of causal inference', 
[McKim & Turner 1997), pages 201-248. 
[GIymour & Cooper 1999) Clark Glymour & Gregory F. Cooper(eds.): 'Computation, causation, and 
discovery', Cambridge, Massachusetts: The M.I.T. Press. 
[Grunwald 2000) Peter Griinwald: 'Maximum entropy and the glasses you are looking through', Pro-
ceedings of the 16th conference of Uncertainty in Artificial Intelligence, Stanford University, Mor-
gan Kaufmann, pages 238-246. 
[Hausman 1998) Daniel M. Hausman: 'Causal asymmteries', Cambridge: Cambridge University Press. 
[Hausman 1999) Daniel M. Hausman: 'The mathematical theory of causation', review of [McKim & 
Turner 1997), British Journal for the Philosophy of Science 50, pages 151-162. 
[Healey 1991) Richard Healey: 'Review of Paul Horwich's "Asymmetries in time"', The Philosophical 
Review 1 00, pages 125-130. 
[Heckerman et al. 1999) David Heckerman, Christopher Meek & Gregory Cooper: 'A Bayesian ap-
proach to causal discovery', in [GIymour & Cooper 1999), pages 141-165. 
[Herskovitz 1991l Edward Herskovitz: 'Computer-based probabilistic-network construction', PhD The-
sis, Stanford University. 
[Humphreys 1997) Paul Humphreys: 'A critical appraisal of causal discovery algorithms', in [McKim & 
Turner 1997), pages 249-263. 
[Humphreys & Freedman 1996] Paul Humphreys & David Freedman: 'The grand leap', British Journal 
for the Philosophy of Science 47, pages 113-123. 
[Jitnah 1999] Nathalie Jitnah: 'Using mutual information for approximate evaluation of Bayesian net-
works', PhD Thesis, School of Computer Science and Software Engineering, Monash University. 
[Jordan 1998] Michael!. Jordan(ed.): 'Learning in Graphical Models', Cambridge, Massachusetts: The 
M.!.T. Press 1999. 
[Kwoh & Gillies 1996] Chee-Keong Kwoh & Duncan F. Gillies: 'Using hidden nodes in Bayesian net-
works', Artificial Intelligence 88, pages 1-38. 
[Lad 1999) Frank Lad: 'Assessing the foundation for Bayesian networks: a challenge to the principles 
and the practice', Soft Computing 3(3), pages 174-180. 
[Lemmer 1993] John F. Lemmer: 'Causal modeling', in Proceedings of the 9th Conference on Uncer-
tainty in Artificial Intelligence, San Mateo: Morgan Kaufmann, pages 143-151. 
[Lemmer 1996] John F. Lemmer: 'The causal Markov condition, fact or artifact?', SIGART 7(3). 
[McKim & Turner 1997] Vaughn R. McKim & Stephen Turner: 'Causality in crisis? Statistical methods 
and the search for causal knowledge in the social sciences', University of Notre Dame Press. 
[Mill 1843] John Stuart Mill: 'A system of logic, ratiocinative and inductive: being a connected view 
of the principles of evidence and the methods of scientific investigation', New York: Harper & 
Brothers, eighth edition, 1874. 

114 
JON WILLIAMSON 
[Neapolitan 1990) Richard E. Neapolitan: 'Probabilistic reasoning in expert systems: theory and algo-
rithms', New York: Wiley. 
[Oliver & Smith 1990) R.M. Oliver & 1.Q. Smith: 'Influence diagrams, belief nets and decision analy-
sis', Chichester: Wiley. 
[Papineau 1992) David Papineau: 'Can we reduce causal direction to probabilities?', Philosophy of Sci-
ence Association 1992 (2), pages 238-252. 
[Paris 1994) Jeff Paris: 'The uncertain reasoner's companion', Cambridge: Cambridge University Press. 
[Paris 1999) Jeff Paris: 'Common sense and maximum entropy', Synthese 117, pages 73-93. 
[Paris & Vencovska 1997) Jeff Paris & Alena Vencovska: 'In defense of the maximum entropy inference 
process', International Journal of Automated Reasoning 17, pages 77-103. 
[Paris & Vencovska 200n 1.B. Paris & A. Vencovska: 'Common sense and stochastic independence', 
this volume. 
[Pearl 1988) Judea Pearl: 'Probabilistic reasoning in intelligent systems: networks of plausible infer-
ence', San Mateo, California: Morgan Kaufmann. 
[Pearl 2000) Judea Pearl: 'Causality: models, reasoning, and inference', Cambridge University Press. 
[Pearl & Dechter 1996) J. Pearl & R. Dechter: 'Identifying independencies in causal graphs with feed-
back', Proceedings of the 12th Conference of Uncertainty in Artificial Intelligence, Portland OR: 
Morgan Kaufmann. 
[Pearl et al. 1990) Judea Pearl, Dan Geiger & Thomas Verma: 'The logic of influence diagrams', in 
[Oliver & Smith 19901, pages 67-87. 
[Price 1992) Huw Price: 'The direction of causation: Ramsey's ultimate contingency', Philosophy of 
Science Association 1992 (2), pages 253-267. 
[Reichenbach 1956) Hans Reichenbach: 'The direction of time', Berkeley & Los Angeles, University of 
California Press, reprinted 1971. 
[Richardson 1996) T. Richardson: 'A discovery algorithm for directed cyclic graphs', Proceedings of the 
12th Conference of Uncertainty in Artificial Intelligence, Portland OR: Morgan Kaufmann, pages 
454-461. 
[Robins & Wasserman 1999) James M. Robins & Larry Wasserman: 'On the impossibility of inferring 
causation from association without background knowledge', in [Glymour & Cooper 19991, pages 
305-321. 
[Rolnick 1974) William B. Rolnick: 'Causality and physical theories', New York: American Institute of 
Physics. 
[Salmon 1980) Wesley C. Salmon: 'Probabilistic causality', in [Salmon 1998), pages 208-232. 
[Salmon 1984) Wesley C. Salmon: 'Scientific explanation and the causal structure of the world' , Prince-
ton: Princeton University Press. 
[Salmon 1998) Wesley C. Salmon: 'Cauality and explanation', Oxford: Oxford University Press. 
[Savitt 1996] Steven F. Savitt: 'The direction of time', British Journal for the Philosophy of Science 47, 
pages 347-370. 
[Scheines 1997) Richard Scheines: 'An introduction to causal inference', in [McKim & Turner 1997], 
pages 185-199. 
[Schlegel 1974] Richard Schlegel: 'Historic views of causalitY', in [Rolnick 1974), pages 3-21. 
[Smith 1990) J.Q. Smith: 'Statistical principles on graphs', in [Oliver & Smith 1990), pages 89-120. 
[Sober 1988) Elliott Sober: 'The principle of the common cause', in James H. Fetzer (ed.): 'Probability 
and causality: essays in honour of Wesley C. Salmon', pages 211-228. 
[Spirtes 1995) P. Spirtes: 'Directed cyclic graphical representation offeedback models', Proceedings of 
the 11th Conference on Uncertainty in Artificial Intelligence, Montreal QU: Morgan Kaufmann, 
pages 491-498. 
[Spirtes et aI. 1993] Peter Spirtes, Clark G1ymour & and Richard Scheines: 'Causation, Prediction, and 
Search', Lecture Notes in Statistics 81, Springer-Verlag. 
[Spirtes et al. 1997] Peter Spirtes, Clark Glymour & Richard Scheines: 'Reply to Humphreys and Freed-
man's review of 'Causation, prediction, and search", British Journal for the Philosophy of Science 
48, pages 555-568. 
[Sucar et al. 1993] L.E. Sucar, D.F. Gillies & D.A. Gillies: 'Objective probabilities in expert systems', 
Artificial Intelligence 61, pages 187-208. 
[Sundaram 1996] Rangarajan K Sundaram: 'A first course in optimisation theory', Cambridge: Cam-
bridge University Press. 
[Verma & Pearl 1991] T. Verma & 1. Pearl: 'Equivalence and synthesis of causal models', Los Angeles, 
Cognitive Systems Laboratory, University of California. 

FOUNDATIONS FOR BAYESIAN NETWORKS 
115 
[Wermuth et al. 1994] N. Wermuth, D. Cox & 1. Pearl: 'Explanations for multivariate structures de-
rived from univariate recursive regressions', Center of Survey Research and Methodology, ZUMA, 
Mannheim, FRG, revised 1998. 
[Williamson 1999] Jon Williamson: 'Does a cause increase the probability of its effects?', philosophy.ai 
report paLjw_99_d, http://www.kcl.ac.uklphilosophy.ai. 
[Williamson 2000] Jon Williamson: 'A probabilistic approach to diagnosis', Proceedings of the Eleventh 
International Workshop on Principles of Diagnosis (DX-OO), Morelia, Michoacen, Mexico, June 8-
11 2000. 
[Williamson 2000b] Jon Williamson: 'Approximating discrete probability distributions with Bayesian 
networks', in Proceedings of the International Conference on Artificial Intelligence in Science and 
Technology, Hobart Tasmania, 16-20 December 2000, pages 106-114. 

PART II 
LOGIC, MATHEMATICS AND BAYESIANISM 

PETER M. WILLIAMS 
PROBABILISTIC LEARNING MODELS 
1 INTRODUCTION 
The purpose of this review is to provide a brief outline of some uses of Bayesian 
methods in artificial intelligence, specifically in the area of neural computation. 
Prior to the 1980s, much of knowledge representation in Artificial Intelligence 
was concerned with rule-based or expert systems. The aim was to write rules and 
facts, expressing human knowledge in some domain, in a quasi-logical language. 
Although achieving some successes, this approach came to be seen by the 1980s as 
cumbersome in adapting to changing circumstances, partly owing to the explosion 
in the list of rules and exceptions needed to cover novel cases. In the mid 1980s 
a new paradigm emerged, referred to variously as parallel distributed processing, 
connectionism or neural computing, largely through the publication of Rumelhart 
and McClelland [1986]. One of the motivating ideas was that living creatures are 
programmed by experience, rather than by spelling out every step in a process, and 
that representations of human knowledge and learning need not to be restricted to 
areas in which rule-based algorithms can be found. An important tool in imple-
menting this programme was the neural network which, in a very rudimentary 
way, might be seen as sharing some of the characteristics of the brain. 
Although having its origins in the biological and cognitive sciences, signifi-
cant contributions to neural computation were soon made by physicists and statis-
ticians. Furthermore applications were made to practical problems outside the 
field of the biosciences. The list includes prediction (weather and utility demand 
forecasting, medical diagnosis, derivative and term-structure models in finance) 
navigation and control (aircraft landing, plant monitoring, autonomous robots) 
pattern recognition (speech and signal processing, hand-written character recog-
nition, finger-print identification, mineral exploration) etc. Possibly the reasons 
for this widespread interest is that neural networks can be used wherever a linear 
model is used, that they can model non-linear relationships, and that they require 
no detailed model of the underlying process. 
During the ten years or so following the publication of [Rumelhart and Mc-
Clelland, 1986] much attention was given to improving the algorithms used for 
fitting neural network models. At the same time, it was appreciated that there were 
close links with established statistical machine learning and pattern recognition 
methods. As the subject has developed, it has grown closer to information theory, 
statistics, image and signal processing, as regards both its engineering and neuro-
science concerns. Inevitably its maturing expression in statistical form has led to 
an application of the Bayesian approach. 
Neural computing methods have been applied to both supervised and unsuper-
vised learning. The practical applications mentioned above have been largely in 
1J7 
D. Corfield and 1. Williamson (eds.), Foundations of Bayesianism, 117-134. 
Â© 2001 Kluwer Academic Publishers. 

118 
PETER M. WILLIAMS 
Yl 
Y2 
... Yn 
y 
Xn 
Xl 
X2 
... Xm 
Figure 1. The diagram on the left shows a layered feed-forward network with 
inputs Xl, ... , Xm and outputs Yl, ... , Yn separated by a single layer of hidden 
units. In general there can be any number of hidden layers. The diagram on the 
right shows an individual processing unit, with input weights Wl, ... , Wn and bias 
Woo 
the area of supervised learning, which includes classification, regression and in-
terpolation. In the past five years, however, much work at the forefront of neural 
computation has been in the area of unsupervised learning, which traditionally in-
cludes clustering and density estimation. Both of these areas will be touched on 
briefly. 
2 
NEURAL NETWORKS 
The simplest form of neural network provides a way of modelling the relationship 
between an independent variable x and a dependent variable y. For example, x 
could be financial data up to a certain time and y could be a future stock index, 
exchange rate, option price etc. Or x could represent geophysical features of a 
mining prospect and y could represent mineralization at a certain depth. In general 
x and y can be any vectors of continuous or discrete numerical quantities. 
Such a network implements an input-output mapping 
y = f(x,w) 
from x = (Xl, ... , Xm) to Y = (Yl, ... , Yn). The mapping depends on the values 
of the connection strengths or weights w in the network. The connections between 
processing elements can be arbitrary but a layered (non-cyclic) architecture, as 
shown on the left of Figure 1, is commonest. 

PROBABILISTIC LEARNING MODELS 
119 
Figure 2. Diagram showing the hyperbolic tangent squashing function. Other 
functions of similar form may be used. Typically such functions are monotonic, 
approximately linear in the central range and saturate to asymptotic values at the 
extremes. 
Each non-input unit in the network has input weights WI, .â¢. , Wn and a bias Wo 
as shown on the right in Figure 1. For hidden units, namely those that are neither 
input nor output units, 
where the transfer function, in this case the hyperbolic tangent, squashes the output 
into the interval ( -1, 1) as shown in Figure 2. For output units, we assume a direct 
linear relationship: 
Y = Wo + WI Xl + ... + WnXn 
A network of the type shown in Figure 1 can have several internal layers of hid-
den units connecting input units and output units. A feedforward neural network is 
therefore a composition of linear and non-linear transformations like a multi-layer 
sandwich 
linear 
squash 
linear 
squash 
linear 
in this case having two hidden layers. Without non-linear squashing, the sand-
wich would collapse, by composition, to a single linear transformation. With the 
interposed non-linearities a neural network becomes a universal approximator ca-
pable of modelling an arbitrary continuous function [Hornik, 1993; Barron, 1994; 
Ripley, 1996]. 
2.1 
Modelfitting 
Suppose we have a training set of pairs of observations (Xl, YI), ... , (XN, YN) 
where each Xi is a vector of inputs and each Yi is now a single scalar target output 

120 
PETER M. WILLIAMS 
(i = 1, ... , N). Then least squares fitting consists of choosing weights w to 
minimise the data misfit 
(1) 
1 N 
2 
E(w) = 2 L (Yi - f(Xi, w)) 
i=l 
where f (x, w) is the function computed by the network for a given set of weights 
w. Since the gradient .(w) is easily computed using the so-called backpropagation 
algorithm, standard optimisation techniques such as conjugate gradient or quasi-
Newton methods [Williams, 1991; Bishop, 1995al can be applied to minimise (1). 
To make the link with Bayesian methods, recall first that least squares fitting is 
equivalent to maximum likelihood estimation, assuming Gaussian noise. To see 
this, suppose the target variable has a conditional normal distribution 
p(Ylx) = _1_ exp _~ (Y-Jt(X))2 
.../2ia 
2 
a 
where Jt(x) is the conditional mean, for a given input x, and where the variance 
a2 is assumed, for the present, to be constant. Then, assuming independence, the 
negative log likelihood of the data (Xl, Yl), . .. , (XN, YN) is proportional to 
(2) 
1 N 
2 
2 L (Yi - Jt(Xi, w)) 
i=l 
up to an additive constant. Comparison of (1) and (2) shows that least squares fit-
ting is equivalent to maximum likelihood estimation of the weights, if the network 
output f(x, w) is understood to compute the conditional mean Jt(x). 
More generally, there is no need to assume that a is constant for all inputs x. If 
we allow the network to have two outputs, we can allow them to compute the two 
input-dependent parameters Jt(x) and a(x) of the predictive distribution for y. It 
is more convenient, however, in order to have an unconstrained parametrisation, to 
model log a(x) rather than a(x), so that the network can be visualised schemat-
ically as in Figure 3. The negative log likelihood of the data can now be written 
as 
(3) 
L(w) ~ ~ t. {IOgU(Xi, w)' + (Yi ~(~(~~)W)n 
and this can be considered as the generalised error function. Maximum likelihood 
fitting is obtained by minimising L(w) where V'L(w) can also be computed by 
backpropagation.l 
I In the general multivariate case the conditional density p (y Ix) for the target variable y = 
(Yl,. ",Yn) is proportional to 1~I-l/2exp{_! (y _I')T~-l(y -I')} where I' is vector of 
conditional means and ~ is the conditional covariance matrix. We can then modell'(x) and log ~(x) 
as functions of x in ways that depend on the outputs of a neural network when x is given as in-
put [Williams, 19991. This permits modelling of the full conditional correlations in multivariate 
data. Applications to heteroskedastic (time-dependent) volatility in financial time series are given 
in [Williams, 1996]. 

PROBABILISTIC LEARNING MODELS 
J.L(x) 
log a(x) 
network 
w 
121 
Figure 3. 
Schematic representation of a neural network with input x = 
(Xl, ... , xm) and weights w, whose output is interpreted as computing the mean 
and log variance of the target variable. 
2.2 Need/or regularisation 
Neural networks, of the type considered here, differ from linear statistical models 
in being universal approximators, capable of fitting arbitrary continuous non-linear 
functions. This means (i) that there is greater potential for overfitting the data, 
leading to poor generalisation outside the training sample, and (ii) that the error 
surface can be of great complexity. Neural network modelling therefore calls for 
special techniques in particular (i) for some form of stabilisation, or regularisation, 
and (ii) for some form of integration over multiple local minima. The second of 
these is discussed in Section 2.6. 
Possible solutions to the overfitting problem include 
(a) limiting the complexity of the network; 
(b) stopping training early before overfitting begins; 
(c) adding extra terms to the cost function to penalise complex models. 
The first of these (a) amounts to a form of hard structural stabilisation. For exam-
ple, the number of weights might be limited to a certain proportion of the number 
of training items, using various rules of thumb for the exact proportion. A deeper 
treatment of this approach, including analysis of error bounds, forms part of sta-
tisticallearning theory [Vapnik, 1998]. 
The second approach (b) observes that, at early stages of training, the network 
rapidly fits the broad features of the training set. For small initial values of the 
weights, the neural network is in fact close to being a linear model (see the cen-
tral linear segment of Figure 2). As training progresses the network uses more 
resources to fit details of the training set. The aim is to stop training before the 
model begins to fit the noise. There are several ways of achieving this including 
monitoring performance on a test set (but see [Cataitepe et al., 1999]). 

122 
PETER M. WILLIAMS 
The third approach (C) is a form of Tikhonov regularisation [Tikhonov and Ar-
senin, 1977; Bishop, 1995b]. In the case of neural networks, (c) often takes the 
form of weight decay [Hinton, 1986]. This adds an extra term to the cost function 
(4) 
E(w) = L(w) + >.R(w) 
where L(w) expresses the data misfit and R(w) is a regularising term that pe-
nalises large weights. The aim becomes minimisation of the overall objective 
function (4) where>. is a regularising parameter that determines a balance be-
tween the two terms, the first expressing misfit and the second complexity. There 
remains the problem of locating this balance by fixing an appropriate value for >.. 
This is often chosen by some form of cross-validation. But performance on a test 
set can be noisy. Different test sets may lead to different values of >.. We therefore 
examine various Bayesian solutions to this problem. 
2.3 Bayesian approach 
Consider the case of Section 2.1 where we have training data D corresponding 
to observed pairs (Xl, yd, ... , (XN' YN). Suppose the aim is to choose the most 
probable network weights w given the data, in other words to maximise the poste-
rior probability density p(wID). Using Bayes' theorem we have 
(5) 
p(wID) ex p(Dlw) p(w) 
where p(Dlw) is the likelihood of the data and p(w) is the prior over weights. 
Maximising (5) is the same as minimising its negative logarithm 
(6) 
E(w) = L(w) -logp(w) + constant 
where the negative log likelihood L(w) = -logp(Dlw) is given by (3) in the 
case of regression with Gaussian noise. 
Now suppose that w has a Laplace priorp(wl>') ex TIi exp ->'IWil where>. > 0 
is a scale parameter [Williams, 1995]. Then, ignoring terms not depending on w, 
(6) becomes 
(7) 
E>.(w) = L(w) + >'llwlll 
where Ilwllp = (Li IWiIP)1/p is the Lp norm (p ~ 1) of the weight vector w. 
Comparison of (7) with (4) shows that the regularising term >.R(w) corresponds 
to the negative logarithm of the prior. The same is true assuming a Gaussian prior 
for weights p(wl>') ex TIi exp -(>./2)lwiI2 when (7) becomes 
(8) 
E>.(w) = L(w) + (>'/2)llwll~ 
The difficulty remains, however, that .A is still unknown.2 
2 In practice it may be assumed that different classes of weights in the network have different typical, 
if unknown, scales. The classes are chosen to ensure invariance under suitable transformations of input 
and output variables. For simplicity we deal here with a single class. 

PROBABILISTIC LEARNING MODELS 
123 
2.4 Eliminating..\ 
One approach is to eliminate ,\ by means of integration [Buntine and Weigend, 
1991; Williams, 1995]. Consider,\ to be a hyperparameter with prior p(,\) so that 
(9) 
p(w) = ! p(wl'\) p(,\) d'\ 
which no longer depends on'\ and can be substituted directly into (6). Since'\ > 0 
is a scale parameter, it is natural to use a non-informative Jeffreys' prior [1961] for 
which p(,\) is proportional 10 1/,\. It is then straightforward to integrate (9) and 
substitute into (6) to give the objective function 
(10) E(w) = L(w) + W log IIwllp 
where W is the total number of weights and p = 1 for the Laplace prior or p = 2 
for the Gaussian prior. (10) involves no adjustable parameters so that regularisation 
of network training, conceived as an optimisation problem, is automatic.3 
This approach to the elimination of ,\ is independent of the form taken by the 
likelihood term L(w), which will depend on the appropriate statistical model 
for the data. Discrete models, for example, correspond to classification. Non-
Gaussian continuous models are discussed in [B ishop and Legleye, 1995; Williams, 
1998b] for example. 
2.5 The evidence approach to ..\ 
An alternative approach to determining of ,\ is to use its most probable value given 
the data. This is the value of ,\ which maximises p('\ID). Since p('\ID) is propor-
tional to p(DI'\)p(,\), this is the same as choosing ,\ to maximise p(DI'\), assum-
ing that p(,\) is relatively insensitive to'\. In this approach p(DI'\) is called the 
evidence for >.. p(DI'\) can be expressed as an integral over weight space 
(11) p(DI'\) = ! p(Dlw,'\) p(wl'\) dw 
and, under suitable simplifying assumptions, this integration can be performed 
analytically. Specifically, assume that the integrand p(Dlw, ,\), which is propor-
tional to the posterior distribution of weights p( wiD, ,\), can be approximated by a 
Gaussian in the neighbourhood of a maximum w = WMP of the posterior density. 
In the case of a Gaussian weight prior, it can then be shown [MacKay, 1992b] that, 
at a maximum of p( D 1'\), ,\ must satisfy 
(12) ,\ IIWMPII~ = L ~ 
A + Vi 
i 
3 Instead of using the improper Jeffreys prior, we could use a proper conjugate prior. This is the 
gamma distribution, for either the Laplace or Gaussian weight priors, with shape and scale parameters 
a, {3 > 0 say. The regularising term is then (W + a) 10g(llwlh + (3) for the Laplace weight prior 
and (W/2 + a) 10g(llwll~ + 2(3) for the Gaussian prior. Both reduce to W log Ilwllp (p = 1,2) as 
a, {3 approach zero. 

124 
PETER M. WILLIAMS 
where Vi are the eigenvalues of the Hessian of L(w) = -logp(Dlw, A) evaluated 
at w = WMP. (12) can be used as a re-estimation formula for A, using AOld on the 
right and Anew on the left, in iterative optimisation of (8).4 
In the case of a Laplace prior, the evidence approach is essentially equivalent 
to the integration approach of the previous section [Williams, 1995, Appendix]. 
MacKay [1994] argues that, in the case of a Gaussian prior, the evidence approach 
provides better results in practice. MacKay [1994] also discusses the range of 
validity of the various approximations involved in this approach. 
2.6 Integration methods 
The preceding discussion has assumed that the aim is to minimise E(w) in (6). 
This is the same as finding a maximum, at w = WMP say, of the posterior distri-
bution p(wID). Predictions for the target value y, given inputs x, could then be 
made using the distribution 
(13) p(ylx, D) = p(ylx, WMP). 
This can be unsatisfactory, however, since it amounts to assuming that the posterior 
distribution for w can be adequately approximated by a delta function at w = 
WMP. In practice, for a general neural network model, there may be several non-
equivalent modes of the posterior distribution, i.e. local minima of E(w) as was 
noted in Section 2.2, and each may extend over a significant region. 
The correct procedure, from a Bayesian point of view, is to make predictions 
using an integral over weight space 
(14) p(ylx, D) = ! p(ylx, w) p(wID) dw 
where the predictive distribution p(ylx, w), corresponding to a particular value of 
w, is weighted by the posterior probability of w. 5 
In some cases (14) can be integrated analytically if suitable assumptions are 
made. For example, suppose the statistical model p(ylx, w) is Gaussian and that 
the posterior distribution p(wID) can be adequately approximated by a Gaussian 
centered at w = WMP. Then the posterior distribution is again Gaussian with 
a variance in which the intrinsic process noise is augmented by an amount, cor-
responding to model uncertainty, which increases with the dispersion of p(wID) 
aroundwMPÂ· 
4Note that since the Hessian is evaluated at a maximum of p(wID, >'), rather than at a maximum 
of p(Dlw, >'), the eigenvalues may not all be positive. Furthermore, since WMP depends on >., the 
derivation of (12) strictly speaking ignores terms involving dv;j d>' [MacKay, 1992al. 
5To emphasise a danger in (13) note that (10) will have a local minimum at w = 0, even for 
sufficiently small (3 > 0 as defined in footnote 3. This mode ofp(wID), however, will normally only 
have very local extent, so that it will contribute little to the integral in (14), except in cases where there 
is little detectable coupling between any Yi and Xi in D, when the opinion implied by w = 0 that 
p(Ylx, D) == p(yID) would deserve some weight. 

PROBABILISTIC LEARNING MODELS 
125 
An interesting alternative is to approximate p(wID) by a more tractable distri-
bution q(w) obtained by minimising the Kullback-Leibler divergence 
J 
q(w) 
q(w) log p(wID) dw. 
This is known as ensemble learning [Hinton and van Camp, 1993; MacKay, 1995; 
Barber and Bishop, 1998]. Typically q takes the form of a Gaussian whose param-
eters may be fitted using Bayesian methods [MacKay, 1995; Barber and Bishop, 
1998]. The advantage of this approach is that the approximating Gaussian is fitted 
to p(wID) globally rather than locally at w = WMP. More generally, q may be 
assumed to have free form, or to be a product of tractable distributions of fixed 
form. 
It is normally still implied, however, that there is essentially only one significant 
mode for the posterior distribution. In some cases it may be necessary to attempt 
the integral in (14) using numerical methods. In general (14) can be approximated 
by 
(15) p(ylx, D) 
provided {WI, ... , W M} is a sample of weight vectors which is representative 
of the posterior distribution p(wID). The problem is to generate the {Wi} by 
searching those regions of the high-dimensional weight space where p(wID) is 
large and extends over a non-negligible region. This problem has been studied by 
Neal [1992; 1996] who has developed extensions of the Metropolis Monte Carlo 
method specifically adapted to neural networks. This method involves a large num-
ber of successive steps through weight space but, to achieve a given error bound, 
only a much smaller number of visited locations need be retained for predictive 
purposes. A less efficient method, though one that is simple and often effective, 
is to choose the {Wi} as local minima obtained by some standard optimisation 
method from sufficiently many independently chosen starting points. 
Note that (15) expresses the resulting predictive distribution as a finite mixture 
of distributions. The variation between these distributions expresses the extent of 
model uncertainty. For example, the variance of the mixture distribution for y will 
be the mean of the predicted variances plus the variance of the predicted means. 
Specifically if f..ti and 0'; are the mean and variance according to Wi, the predicted 
mean is (f..ti) and the predicted variance is (0';) + {(f..t~) - (f..tY} where (f..ti) is the 
average of f..t1, ... ,f..tM etc. The first term (0';) represents modelled noise and the 
second term (f..t~) - (f..ti)2 represents model uncertainty. 

126 
PETER M. WILLIAMS 
3 KERNEL-BASED METHODS 
3.1 
Gaussian processes 
The previous discussion took as its basis the idea of a prior p( w) over network 
weights w. Now, for a given input vector x, the prior p(w) over weights will in-
duce a prior p(y) over the output Y = f (x, w) computed by the network, assuming 
for simplicity that the network has a single output unit. This is because Y depends 
on network weights w as well as on the input x, so that uncertainty in the weights 
induces uncertainty in the output, for given x. 
More generally, if we consider the outputs YI, ... ,YN computed for different 
inputs Xl, ... ,XN, the prior distribution over weights p(w) determines a joint 
prior P(YI, ... , Y N) over the outputs calculated for these inputs. Given the some-
times opaque role of individual weights in a network, it can be argued that it may 
be more natural to specify such a prior directly. Predictions can then be made 
using conditionalisation. Writing Y N = (YI,"" Y N) for the observed values at 
Xl, ... , XN, the predictive conditional distribution for YN+I at a new point XN+! 
is given by 
where, by assumption, the numerator and denominator on the right are known. 
Neal [1994; 1996] has shown that, for neural networks with independent and 
identically distributed priors over weights, the prior P(Yl, ... ,YN) converges, for 
any N, to a multivariate Gaussian as the number of hidden units tends to infinity. 6 
Such a family of variables y(x) is called a Gaussian process? For a Gaussian 
process, the conditional predictive distribution (16) for Y N + 1 is also Gaussian with 
mean and variance which can be expressed as follows. If Y N has covariance matrix 
'EN and if the covariance matrix for YN+I is written in the form 
then the predictive distribution for YN+! has mean (YN+l) = O'Tr;i'/YN and 
variance a - O'TEN10'. Notice that (YN+!) takes the form of a linear combination 
(YN+!) = O!IYI + ... + O!NYN of observed values. The weighting coefficients O!i 
automatically take account of the correlations between values of y(x) at different 
input locations Xl, ... ,XN+I. As might be expected, the predicted variance a -
0' TEN 10' is always less than the prior variance a. 
6Explicit forms for the resulting covariance functions are derived in [Williams, 1998a1 
7Gaussian processes are already used in Wiener-Kolmogorov time series prediction [Wiener, 19491 
and in Matheron's approach to geostatistics [Matheron, 19651. 

PROBABILISTIC LEARNING MODELS 
127 
To implement this model it is necessary to model the covariance matrix l;N. 
Decisions must also be made about modelling the mean, if trends or drifts are 
permitted. Writing 
l;N = {K(Xi'Xj,..\)}~. 
t,)=l 
where ..\ is the vector of parameters of the model, the process is said to be sta-
tionary if K(X,X/,..\) depends only on the separation x - x' for any X,X/. For 
example, Gibbs and MacKay [1997] consider a stationary process 
K(x, x',.x) = .I, exp { - ~ t, ( x, :.. x, )'} + .I, + .I3'(x, x') 
where..\ = (A1' a1, . .. ,am, A2, A3) is the vector of adjustable parameters and m 
is the dimension of x. Williams and Rasmussen [1996] include a further linear 
regression term A4 x T x'. The Bayesian predictive distribution now becomes 
which must be integrated by Monte Carlo methods or by searching for maxima of 
p(..\IY) ex p(YI..\) p(..\) and using the most probable values of the hyperparame-
ters [Williams and Rasmussen, 1996; Neal, 1998]. 
Considerable attention has been paid recently to reducing the computational 
complexity of Gaussian process modelling [Gibbs and MacKay, 1997; Trecate et 
al., 1999; Williams and Seeger, 20011 and much of this work is also applicable to 
other kernel based methods. 
It should be noted that in geostatistics the "covariogram" for stationary pro-
cesses, for which K(x, x') = k(x - x'), is frequently estimated empirically from 
the data. If a parametric model is used, the form is chosen to reflect prior geologi-
calor geochemical knowledge [Cressie, 1993]. Here too, Bayesian methods have 
been applied to the problem of estimating spatial covariance structures [Kitanidis, 
1986; Le and Zidek, 1992]. 
3.2 
Support Vector Machines 
A recent significant development in classification and pattern recognition has been 
the introduction of the Support Vector Machine (SVM) [Vapnik, 1998; Cristianini 
and Shawe-Taylor, 2000]. SVM methods have been applied to regression [Vapnik 
et al., 1997] and density estimation [Vapnik and Mukherjee, 2000] but we shall 
concentrate on binary classification. 
Suppose a set of training examples (Xl, Y1), ... , (XN' YN) is given where each 
Xi is a vector of real-valued inputs and each Yi E {-I, + I} is the correspond-
ing class label (i = 1, ... , N). Assume initially that the two classes are linearly 
separable, in other words there exists a linear functional! such that !(Xi) < 0 

128 
PETER M. WILLIAMS 
whenever Yi = -1 and f(Xi) > 0 whenever Yi = +1. The class label of a new 
item x can then be predicted by the sign of f(x). 
Where such a separating functional f exists, it will not be unique, if only be-
cause it is undetermined to within a positive scalar multiple. More importantly, 
the separating hyperplane {x : f(x) = O} is generally not unique. The central 
tenet of the SVM approach is that the optimal hyperplane is one that maximises 
the minimum distance between the hyperplane and any example in the training 
data. The optimal hyperplane can then be found by convex optimisation methods. 
For data that is not directly linearly separable, an embedding of the input fea-
tures x I-t cp(x) into some inner-product space 1l may allow the resulting training 
examples ((CP(Xi), Yi)} to be linearly separated in 1l. In that case it turns out that 
detailed information about cP and its range are not needed. This is because the 
optimal hyperplane in 1l, where distance is defined by the inner product, depends 
on cP only through inner products K(x, x') = cp(x) . cp(x'). In fact the classifying 
functional, corresponding to the optimal separating hyperplane, can be expressed 
as 
N 
(17) y(x) = Wo + L wiK(x, Xi) 
i=l 
where Wo, Wl, .â¢. , W N are the model parameters and K is the kernel function. The 
practical problem therefore becomes one of choosing a suitable kernel K rather 
than an embedding CP. Suitable kernels, namely those deriving from some implicit 
embedding CP, include K(x, y) = (xÂ· y + l)P and K(x, y) = exp -Allx _ Yll2. 
To avoid overfitting, or in case the dataset is still not linearly separable, the 
extra constraint that IWil < C for i = 1, ... , N is imposed. C corresponds to a 
penalty for misclassification and is chosen by the user. The SVM typically leads 
to a sparse model in the sense that most of the coefficients Wi vanish. The training 
items Xi for which the corresponding Wi is non-zero are called support vectors 
and, typically, lie close to the decision boundary. 
Support Vector Machines have their origins in the principle of structural risk 
minimisation [Vapnik, 1979; Williamson et al., 1998]. The aim is to place distribution-
independent 
bounds 
on 
expected 
generalisation 
error 
[Bartlett 
and 
Shawe-Taylor, 1999; Vapnik and Chapelle, 2000]. The motivation and results 
are somewhat different from those associated with Bayesian analysis. In the case 
of classification, for example, the optimal hyperplane depends only on the sup-
port vectors, which are extreme values of the dataset, whereas likelihood-based 
methods depend on all the data. A Bayesian approach always aims to conclude 
with a probability distribution over unknown quantities, whereas an SVM typi-
cally offers point estimates for regression or hard binary decisions for classifica-
tion. Bayesian methods have nonetheless been applied to the problem of kernel 
selection in [Seeger, 2000] where support vector classification is interpreted as ef-
ficient approximation to Gaussian process classification. By contrast, the problem 
of model selection based on error bounds derived from cross-validation is dis-
cussed in [Chapelle and Vapnik, 2000; Vapnik and Chapelle, 2000). A Bayesian 

PROBABILISTIC LEARNING MODELS 
129 
treatment of a generalised linear model of identical functional form to the SVM 
is introduced in [Tipping, 2000]. This approach, using ideas of automatic rele-
vance determination [Neal, 1996], provides probabilistic predictions. It also yields 
a sparse representation, but one using relevance vectors, which are prototypical 
examples of classes, rather than support vectors which are close to the decision 
boundary or, in cases of misclassification, on the wrong side of it. 
The SVM performs well in practice and is becoming a popular method. This 
is pardy due to the fact that, once parameters of the model are somehow fixed, 
SVM training finds a global solution, whereas neural network training, when con-
sidered as an optimisation process, may find multiple local minima, as discussed 
previously. 
4 UNSUPERVISED LEARNING 
So far we have dealt with labelled data. We assumed given a collection of observa-
tions { (x 1 , yd, ... , (x N , Y N )} where each Xn represents some known features of 
the nth item, and Yn is a class label, or value, associated with X n. The presence of 
the labels makes this a case of supervised learning. The aim is to use the examples 
to predict the value of y on the basis of x for a general out-of-sample case. More 
generally, the problem is to model the conditional probability distributionp(yJx). 
Now suppose we are only given the features {Xl, ... , XN} without correspond-
ing labels. What can be learned about the xn? This question, which at first sight 
seems perplexing, can be given a sense if it is interpreted as a search for interesting 
features of the data. For example, are there clusters in the data? Are there outliers 
which appear novel or unusual? What is the intrinsic dimensionality of the data? 
Can the data be visualised, or otherwise analysed, in a lower dimensional space? 
Do the data exhibit latent structure, which would help to explain how they were 
generated? 
Some aspects of unsupervised learning relate to qualitative ideas of concept 
formation studied in cognitive science and the philosophy of science. In quanti-
tative terms, however, unsupervised learning often corresponds to some form of 
probability density estimation; and special interest attaches to the structures that 
particular density estimators might exhibit. When considered as a statistical prob-
lem, this makes possible the application of Bayesian methods. We consider two 
examples. 
4.1 
The Generative Topographic Mapping 
The first example concerns a case where density estimation is useful for clustering 
and for providing low-dimensional representations enabling visualisation of high 
dimensional data. 
Suppose the data lives in a possibly high dimensional space D. If the dataset 
is {Xl, .. " XN}, where each Xn is a d-dimensional real vector, then D = Rd. 

130 
PETER M. WILLIAMS 
A common form of density estimation uses a mixture of Gaussians. For a finite 
mixture, the density at xED may be estimated by 
(18) p(x) = LP(xli)P(i) 
iEI 
where I is some finite index set, P(i) is the weight of the ith componentandp(xli) 
is a Gaussian density. In the simplest case, each Gaussian will be spherical with 
the same dispersion, so that each covariance matrix is the same scalar multiple, 
(3-1 say, of the identity. Then 
where mi is the centre of the ith mixture component. 
The idea of the Generative Topographic Mapping [Bishop et al., 1998b; Bishop 
et al., 1998a] is to embed the index set I in a linear topological space L, so that 
each i E I becomes the index of an element Ui E L, and to require that the 
mapping Ui f-t mi is the restriction of a continuous map U f-t mu from L to D. 
L is now referred to as the latent space. For computational reasons, the mixture is 
still considered to be finite so that we can write (18) as 
(19) p(x) = LP(xlui)P(Ui) 
iEI 
where the weights of the mixture are interpreted as a prior distribution over L 
concentrated on a certain finite subset. 
The purpose of the continuity of the mapping U f-t mu is seen when the map-
ping from L to D is, in a sense, inverted using Bayes' theorem 
The significance of (20) is that, for any possible data point xED, there is a 
discrete probability distribution P(uilx) over L concentrated on those elements of 
latent space which correspond to components of the Gaussian mixture. Typically 
P(uilx) will be large when x is close to the centre of the mixture component 
generated by Ui. In that case Ui is said to have large responsibility for x. If we 
write mx for the mean in L of the posterior distribution (20), the mapping x f-t mx 
maps data points in D to elements of the latent space L. Normally the mapping is 
continuous, so that points that are close in D should map to points that are close 
inL. 
An important application of the GTM is to data-visualisation. Suppose that 
L has low dimension, specifically L = R 2 , and that the elements of L having 
positive prior probability lie on a regular two-dimensional grid. Then each element 
Xn of the dataset {Xl, ... , XN} will determine. by the mapping x f-t m x â¢ an 
element Un in the convex hull of the regular grid in L. Each data point is therefore 

PROBABILISTIC LEARNING MODELS 
131 
visualisable by its representative in a rectangular region of the two dimensional 
latent space L. Furthermore, it might be expected that distinct populations in data 
space would be represented by distinct clusters in latent space. 
This account has said nothing of the way in which the mapping 'IjJ : u I-t mu 
is characterised. Typically 'IjJ is taken to be a generalised linear regression model 
fitted, together with the noise parameter (3, using the EM algorithm [Bishop et ai., 
1998b). A more flexible manifold-aligned noise model is described in [Bishop 
et al., 1998a] together with methods of Bayesian inference for hyperparameters. 
[Bishop et ai., 1998b] also provides a discussion of the relationship between the 
GTM and Kohonen's earlier well-known self-organising feature map [Kohonen, 
1995]. 
4.2 Blind source separation 
The generative topographic map can be viewed as an instance of a generative 
model in which observations are modelled as noisy expressions of the state of 
underlying latent variables [Roweis and Ghahramani, 1999]. Several techniques 
for modelling multivariate datasets can be viewed similarly, including principal 
component analysis (PCA) and independent component analysis (lCA). Whereas 
conventional PCA resolves a data item linearly into uncorrelated components, ICA 
attempts to resolve it into fully statistically independent components [Common, 
1994). 
ICA has been applied to the problem of blind source separation. Suppose there 
are M statistically independent sources and N sensors. The sources might be au-
ditory, e.g. speakers in a room, and the sensors microphones. Each sensor receives 
a mixture of the sources. The task is to recover the hidden sources from the ob-
served sensors. This problem arises in many areas, for example medical signal and 
image processing, speech processing, target tracking etc. Separation is said to be 
blind since, in the most general form of the problem, nothing is assumed known 
about the mixing process or the sources, apart from their mutual independence. In 
this sense ICA can be considered to be an example of unsupervised learning. 
In the simple linear case, it is assumed that the sensor outputs x can be ex-
pressed as x = As where s are the unknown source signals and A is an unknown 
mixing matrix. One approach to the problem is to recover the source signals by a 
linear transformation y = Wx where W is the separating matrix [Amari et al., 
1996). The intention is that y should coincide with the original s up to scalar mul-
tiplication and permutation of channels. The idea is to fit W by minimising the 
Kullback-Leibler divergence 
between the joint distribution for y and the product of its marginals over the source 
channels m = 1, ... , M. The minimum value of zero is achieved only if W can 

132 
PETER M. WILLIAMS 
be chosen so that the resulting distribution p(y) factorises over channels, in which 
case the components Yl, ... ,YM of Y = Wx are independent. 
Various algorithms have been proposed for solving this problem at varying de-
grees of generality [Bell, 1995; Amari et al., 1996; Attias and Schreiner, 1998]. 
A recent Bayesian approach uses the ideas of ensemble learning mentioned previ-
ously [Miskin and MacKay, 200n The idea is that the separating matrix W may 
be ill-defined if the data is noisy. The approach instead is to provide a method for 
approximating the posterior distribution over all possible sources. Significantly, 
the method allows Bayesian model selection techniques to determine the number 
of sources M which is generally unknown in advance. 
5 CONCLUSION 
Statistical methods are increasingly guiding principled research in neural infor-
mation processing, in both its engineering and neuroscientific forms. Improved 
algorithms and computational resources are making possible the analysis of high 
dimensional datasets using powerful non-linear models. This is appropriate in 
view of the complexity of the systems with which the field is now dealing. In-
evitably there is a danger of using over-complex models which fail to distinguish 
between signal and noise. Bayesian methods are proving invaluable in providing 
model selection techniques for matching complexity of the model to information 
content of the data in both supervised and unsupervised learning. 
School of Cognitive and Computing Sciences, University of Sussex, UK. 
BIBLIOGRAPHY 
[Amari et al., 1996] S. Amari, A. Cichocki, and H. H. Yang. A new learning algorithm for blind 
signal separation. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, Advances in 
Neural Information Processing Systems 8, pages 757-763. MIT Press, 1996. 
[Attias and Schreiner, 1998] H. Attias and C. E. Schreiner. Blind source separation and deconvolu-
tion: The dynarnic component analysis algorithm. Neural Computation, 10(6):1373-1424, 1998. 
[Barber and Bishop, 1998] David Barber and Christopher M. Bishop. Ensemble learning for multi-
layer networks. In M. I. Jordan, M. 1. Kearns, and S. A. Solla, editors, Advances in Neural Infor-
mation Processing Systems 10, pages 395-401. MIT Press, 1998. 
[Barron,1994] A. R. Barron. Approximation and estimation bounds for artificial neural networks. 
Machine Learning, 14:115-133, 1994. 
[Bartlett and Shawe-Taylor, 1999] P. Bartlett and 1. Shawe-Taylor. Generalization performance of 
support vector machines and other pattern classifiers. In B. SchOlkopf, C. J. C. Burgess, and A. 1. 
Smola, editors, Advances in Kernel Methods-Support Vector Learning, pages 43-54. MIT Press, 
1999. 
[Bell, 1995] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation 
and blind deconvolution. Neural Computation, 7(6):1129-1159, 1995. 
[Bishop, 1995a] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 
1995. 
[Bishop, 1995b] Chris M. Bishop. Training with noise is equivalent to Tikhonov regularization. Neu-
ral Computation, 7(1):108-116, 1995. 

PROBABILISTIC LEARNING MODELS 
133 
[Bishop and Legleye, 1995] C. M. Bishop and C. Legleye. Estimating conditional probability densi-
ties for periodic variables. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural 
Iriformation Processing Systems 7, pages 641-648. MIT Press, 1995. 
[Bishop et ai., 1998a] Christopher M. Bishop, Markus Svensen, and Christopher K. I. Williams. De-
velopments of the generative topographic mapping. Neurocomputing, 21:203-224, 1998. 
[Bishop et al., 1998b] Christopher M. Bishop, Markus Svensen, and Christopher K. I. Williams. 
GTM: The generative topographic mapping. Neural Computation, 10(1):215-234, 1998. 
[Buntine and Weigend, 1991] Wray L. Buntine and Andreas S. Weigend. Bayesian back-propagation. 
Complex Systems, 5:603-643,1991. 
[Cataltepe et al., 1999] Zebra Cataltepe, Yaser S. Abu-Mostafa, and Malik Magdon-Ismail. No free 
lunch for early stopping. Neural Computation, 11(4):995-1009, 1999. 
[Chapelle and Vapnik, 2000] Olivier Chapelle and Vladimir Vapnik. Model selection for support vec-
tor machines. In S. A. Solla, T. K. Leen, and K-B MUller, editors, Advances in Neural Iriformation 
Processing Systems 12, pages 230-236. MIT Press, 2000. 
[Common, 1994] P. Common. Independent component analysis: A new concept. Signal Processing, 
36:287-314,1994. 
[Cressie, 1993] Noel A. C. Cressie. Statistics for Spatial Data. Wiley, revised edition, 1993. 
[Cristianini and Shawe-Taylor, 2000] N. Cristianini and 1. Shawe-Taylor. An Introduction to Support 
Vector Machines. Cambridge University Press, 2000. 
[Gibbs and MacKay, 1997] Mark Gibbs and David 1. C. MacKay. Efficient implementation of Gaus-
sian processes. Technical report, Cavendish Laboratory, Cambridge, 1997. 
[Hinton, 1986] G. E. Hinton. Learning distributed representations of concepts. In Proceedings of the 
Eighth Annual Conference of the Cognitive Sicence Society (Amherst, 1986), pages 1-12. Hillsdale: 
Erlbaum, 1986. 
[Hinton and van Camp, 1993] G. E. Hinton and D. van Camp. Keeping neural networks simple by 
minimizing the description length of the weights. In Proceedings of the Sixth Annual Coriference 
on Computational Learning Theory, pages 5-13,1993. 
[Hornik, 1993] K. Hornik. Some new results on neural network approximation. Neural Computation, 
6(8):1069-1072, 1993. 
[Jeffreys,1961] H. Jeffreys. Theory of Probability. Oxford, third edition, 1961. 
[Kitanidis, 1986] Peter K. Kitanidis. 
Parameter uncertainty in estimation of spatial functions: 
Bayesian analysis. Water Resources Research, 22(4):499-507, 1986. 
[Kohonen, 1995] T. Kohonen. Self-Organizing Maps. Springer, 1995. 
[Le and Zidek, 1992] Nhu D. Le and James V. Zidek. Interpolation with uncertain spatial covariances: 
A Bayesian alternative to Kriging. Journal of Multivariate Analysis, 43:351-374,1992. 
[MacKay, 1992a] David J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415-447, 
1992. 
[MacKay, 1992b] David 1. C. MacKay. A practical Bayesian framework for backpropagation net-
works. Neural Computation, 4(3):448-472, 1992. 
[MacKay, 1994] David 1. C. MacKay. In G. Heidbreder, editor, Maximum Entropy and Bayesian 
Methods, Santa Barbara 1993, Dordrecht, 1994. Kluwer. 
[MacKay, 1995] D.1. C. MacKay. Developments in probabilistic modelling with neural networks-
ensemble learning. In Neural Networks: Artijiciaiintelligence and Industrial Applications, pages 
191-198. Springer, 1995. 
[Matheron, 1965] G. Matheron. La Theorie des Variables Regionalisees et ses Applications. Masson, 
1965. 
[Miskin and MacKay, 2001] 1. W. Miskin and D. J. C. MacKay. Ensemble learning for blind source 
separation. In S. Roberts and R. Everson, editors, Independent Component Analysis: Principles 
and Practice. Cambridge University Press, 2001. 
[Neal, 1992] Radford M. Neal. Bayesian training ofbackpropagation networks by the hybrid Monte 
Carlo method. Technical Report CRG-TR-92-1, Department of Computer Science, University of 
Toronto, April 1992. 
[Neal,1994] Radford M. Neal. Priors for infinite networks. Technical Report CRG-TR-94-1, Depart-
ment of Computer Science, University of Toronto, 1994. 
[Neal, 1996] Radford M. Neal. Bayesian Learning for Neural Networks. Lecture Notes in Statistics 
No. 118. Springer-Verlag, 1996. 
[Neal,1998] R. M. Neal. 
Regression and classification using Gaussian process priors. 
In 
1. M. Bernardo et al, editor, Bayesian Statistics 6, pages 475-501. Oxford University Press, 1998. 

134 
PETER M. WILLIAMS 
[Ripley, 1996] B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, 
1996. 
[Roweis and Ghahramani, 1999] Sam Roweis and Zoubin Ghahramani. A unifying review of linear 
gaussian models. Neural Computation, 11(2):305-345, 1999. 
[Rumelhart and McClelland, 1986] D. E. Rumelhart and 1. L. McClelland. Parallel Distributed Pro-
cessing: Explorations in the Microstructure of Cognition. MIT Press, 1986. 
[Seeger,2ooo] Matthias Seeger. Bayesian model selection for Support Vector machines, Gaussian 
processes and other kernel classifiers. In S. A. Solla, T. K. Leen, and K-B Milller, editors, Advances 
in Neural Information Processing Systems 12, pages 603-609. MIT Press, 2000. 
[Tikhonov and Arsenin, 1977] A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-Posed Problems. 
John Wiley & Sons, 1977. 
[Tipping, 2000] Michael E. Tipping. The relevance vector machine. In S. A. Solla, T. K. Leen, and 
K.-B. Milller, editors, Advances in Neural Information Processing Systems 12, pages 652-658. MIT 
Press, 2000. 
[Trecate et al., 1999] Giancarlo Ferrari Trecate, C. K. I. Williams, and M. Opper. Finite-dimensional 
approximation of Gaussian processes. In M. 1. Kearns, S. A. Solla, and D. A. Cohn, editors, 
Advances in Neural Information Processing Systems 11, pages 218-224. MIT Press, 1999. 
[Vapnik, 1979] v. Vapnik. Estimation of Dependences Based on Empirical Data. Nauka, Moscow, 
1979. English translation: Springer Verlag, New York, 1982. 
[Vapnik, 1998] Vladimir Vapnik. Statistical Learning Theory. John Wiley, 1998. 
[Vapnik and Chapelle, 2000] V. Vapnik and O. Chapelle. Bounds on error expectation for support 
vector machines. Neural Computation, 12(9):2013-2036,2000. 
[Vapnik and Mukherjee, 2000] Vladimir N. Vapnik and Sayan Mukherjee. Support vector method for 
multivariate density estimation. In S. A. Solla, T. K. Leen, and K-B Milller, editors, Advances in 
Neural Information Processing Systems 12, pages 659-664. MIT Press, 2000. 
[Vapnik et al., 1997] Vladimir Vapnik, Steven E. Golowich, and Alex Smola. Support vector method 
for function approximation, regression estimation, and signal processing. In Michael C. Mozer, 
Michael I. Jordan, and Thomas Petsche, editors, Advances in Neural Information Processing Sys-
tems 9, pages 281-287. MIT Press, 1997. 
[Wiener,1949] N. Wiener. Extrapolation, Interpolation, and Smoothing of TImes Series. MIT Press, 
1949. 
[Williams, 1998a] Christopher K. I. Williams. Computation with infinite neural networks. Neural 
Computation, 10(5):1203-1216, 1998. 
[Williams and Rasmussen, 1996] Christopher K. I. Williams and Carl Edward Rasmussen. Gaussian 
processes for regression. In Michael C. Mozer David S. Touretzky and Michael E. Hasselmo, 
editors, Advances in Neural Information Processing Systems 8, pages 514-520. The MIT Press, 
1996. 
[Williams and Seeger, 200ll Christopher K. I. Williams and Matthias Seeger. Using the Nystrom 
method to speed up kernel machines. In T. K. Leen, T. G. Diettrich, and V. Tresp, editors, Advances 
in Neural Information Processing Systems 13. The MIT Press, 2001. 
[Williams, 1991l P. M. Williams. A Marquardt algorithm for choosing the step-size in backpropaga-
tion learning with conjugate gradients. Cognitive Science Research Paper CSRP 229, University of 
Sussex, February 1991. 
[Williams, 1995] P. M. Williams. Bayesian regularization and pruning using a Laplace prior. Neural 
Computation, 7(1):117-143,1995. 
[Williams, 1996] P. M. Williams. Using neural networks to model conditional multivariate densities. 
Neural Computation, 8(4):843-854, 1996. 
[Williams, 1998b] P. M. Williams. Modelling seasonality and trends in daily rainfall data. In M. I. 
Jordan, M. 1. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems 
10, pages 985-991. The MIT Press, 1998. 
[Williams, 1999] P. M. Williams. Matrix logarithm parametrizations for neural network covariance 
models. Neural Networks, 12(2):299-308, 1999. 
[Williamson et al., 1998] R. C. Williamson, J. Shawe-Taylor, P. L. Bartlett and M. Anthony. Structural 
risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 
44(5):1926-1940, 1998. 

COLIN HOWSON 
THE LOGIC OF BAYESIAN PROBABILITY 
For the last eighty or so years it has been generally accepted that the theory of 
Bayesian probability is a theory of partial belief subject to rationality constraints. 
There is also a virtual consensus that both the measure of belief and the constraints 
to which it is subject can only be provided via utility theory. It is easy to see why 
this should be so. The underlying idea, accepted initially by both de Finetti and 
Ramsey in their seminal papers ([1964] and [1931] respectively, though the paper 
1964, first published in 1937, built on earlier work), but going back at least as 
far as Bayes' Memoir [1763], is that an agent's degree of belief in or uncertainty 
about a proposition A can be assessed by their rate of substitution of a quantity 
of value for a conditional benefit [S if A is true, 0 if not]. The natural medium 
of value is, of course, money, but the obvious difficulties with sensitivity to loss 
and the consequent diminishing marginal value of money seem to lead, apparently 
inexorably, to the need to develop this idea within an explicit theory of utility. 
This was first done only in this century, by Ramsey [1931]; today it is customary 
to follow Savage [1954] and show that suitable axioms for preference determine 
a reflexive and transitive ordering 'at least as probable as' and thence, given a 
further assumption about how finely the state space can be partitioned, a unique 
probability function. 
The results of these various endeavours have all the hallmarks of a vigorously 
progressing research-programme. For all that, I do not myself think that it is the 
right way to provide a foundation for epistemic probability. I believe that the cur-
rent state of utility theory itself is far from satisfactory, but underlying that concern 
is the feeling that one should not need a general theory of rational preference in or-
der to talk sensibly about estimates of uncertainty and the laws these should obey. 
These estimates are intellectual judgments, and they are constrained by rules of 
consistency. In support of this view is an elementary mathematical fact seldom 
highlighted but of much significance. The probability of a proposition A is the 
expected value of its indicator function, that is the function defined on the space 
of relevant possibilities which takes the value 1 on those possible states of affairs 
that make A true, and 0 on the others. In other words, probability is expected 
truth-value. Truth and its logical neighbourhood are surely the right focus, not 
rationality (whatever that is). 
These considerations suggest a view of mathematical uncertainty well-
represented if not prominent among the seventeenth and eighteenth century pio-
neers, which is that the laws of epistemic probability are, in Leibniz's words, 'une 
nouvelle espece de logique' 1 â¢ And James Bernoulli, in the Ars Conjectandi, talked 
of measuring the probability of an uncertain proposition A in terms of the number 
of 'proofs' of A relative to the number of 'proofs' of not-A. The sort of logical 
1'1 have more than once said that we should have a new kind of logic which would treat degrees of 
probability' (New Essays, bk. IV, ch. XVI). 
137 
D. Corfield and l Williamson (eds.J, Foundations of Bayesianism, 137-159. 
Â© 200t Kluwer Academic Publishers. 

138 
COLIN HOWSON 
analysis of probability proposed by Leibniz and Bernoulli was never properly de-
veloped, however. In retrospect it is easy to see the factors that hindered it: firstly, 
rapid technical development, and exploration of the problem-solving power of the 
mathematical theory, were primary, relegating 'philosophical' investigation to a 
relatively low-priority task; secondly, a satisfactory theory of deductive logic did 
not arrive until the beginning of the twentieth century; thirdly, probability became 
a puzzlingly equivocal notion, with two seemingly quite different aspects. These 
S-D. Poisson [1823] labelled respectively chance, signifying a property of events 
generated by repeatable random devices, measured by long-run frequencies (this is 
more or less Carnap's probabilitY2), and probabilite, signifying warranted degree 
of certainty relative to an agent's knowledge-state (cf. Carnap's probabilitYl). 
The list of factors is not complete. Possibly the most influential, and one 
that seemed to many to prove decisive, was a principle additional to the standard 
probability axioms which was regarded as indispensable for actually determining 
probability-values. The principle is usually known now by the name Keynes (an 
advocate of it) gave it: the Principle of Indifference. Enough has been written 
about the difficulties and paradoxes attending the use of this principle (see, for ex-
ample, [Howson and Urbach, 1993, Ch. 4] to make it unnecessary to undertake 
another long discussion here. But the connection between the principle and the 
logical programme is intimate, and when stated in the context of modern seman-
tics it is very plausible. Ironically, therefore, it is also in such a setting that the 
basic problem with the principle is most easily identified and its gravity appre-
ciated. Thus, suppose that a sentence B admits n models distinct up to isomor-
phism, in r of which A is also true. Then it seems obvious that there is a logical 
interpretation of a conditional probability P(AIB) evaluated according to the rule: 
P(AIB) = r /n. 2 Such an interpretation is practically explicit in Bolzano [1850, 
Sec 66 et seq.], and fully explicit a century later in Carnap [1950], for whom this 
function (denoted by ct) explicated formally the idea of a partial entailment of 
A by B, measured by the proportion of B's models which are also models of A 
(though Carnap abandoned this measure almost immediately because of its inabil-
ity to deliver a type of induction by enumeration; see [Howson and Urbach, 1993, 
Ch.4]. 
Of course, there is a problem when B does not admit only finitely many mod-
els. In such cases it may still nevertheless be possible to exploit a 'natural' metric 
structure, such as when the possibilities are parametrizable by real numbers in a 
compact interval (such a structure was famously exploited by Bayes when he de-
rived a conditional, 'posterior', probability distribution for a binomial parameter 
in his [1763]). However, it was in precisely this type of case that serious prob-
lems with the Principle of Indifference first became evident, with the discovery of 
what were called the 'paradoxes of geometrical probability', where 'geometrical' 
2 Laplace of course enunciated in effect just this rule when he defined the probability of an event to 
be the number of cases favourable to the event divided by the number of all possible cases, where these 
cases are 'equally possible'. The proviso has been much commented on, but in the semantic context its 
meaning is clear enough: that is certainly how Bolzano and the German school understood it later. 

THE LOGIC OF BAYESIAN PROBABILITY 
139 
was the traditional word referring to the real-number continuum, or some compact 
subset thereof (the best-known of these 'paradoxes' is Bertrand's chord problem; 
see [Kac and Ulam, 1968, pp. 37-39]). This underlying problem is that how the 
elementary possibilities are conceived is not absolute but relative to some concep-
tual frame -
in effect a language -
and depending on how this is chosen the 
probability-values will themselves vary. 
The mathematical subtleties of continuous possibility spaces rather conceal this 
point by suggesting that it is only in such spaces that real problems arise (still 
unfortunately a common point of view), so here is a very elementary example 
which shows that the problem lies right at the heart of the idea of taking Laplacean 
ratios to compute probabilities. Consider two simple first-order languages with 
identity and a one-place predicate symbol Q, and no other relation or function 
symbols. Language 1 has no individual names (constants), and language 2 has two 
individual names a, b. In both languages there are identical sentences 
A: Something has the property Q 
B: There are exactly 2 individuals. 
There are only three models of B in language 1 distinguishable up to isomorphism: 
one containing no, one containing one and one containing two instances of Q. Two 
of these satisfy A. In language 2, on the other hand, the fact that the individuals 
can be distinguished by constants means that there are more than three distinct 
models of B: allowing for the possibility that the constants might name the same 
individual there are eight, six of which satisfy A. Using the Laplacean definition 
of P(AIB) above we therefore get different answers for the value of P(AIB) de-
pending on which language we use. In language 1 the value is 2/3 and in Language 
2 it is 3/4 (cf. Maxwell-Boltzmann vs. Bose-Einstein statistics). Relative to each 
language the models are of course all 'equally possible'. 
To sum up: the 'equal possibilities' of the Principle of Indifference are equal 
relative to some conceptual frame, or language. This can in general be chosen in 
different ways and, depending on the choice, the ratios of numbers of favourable 
to possible cases/or the same event or proposition may vary -
where they can be 
computed at all. Moreover, not only does there seem to be no non-arbitrary way 
of determining the 'correct' language, or even being able to assign any meaning 
to the notion of a correct language, but in continuous spaces entirely equivalent 
frames will exist, related by one-to-one bicontinuous transformations, which will 
generate different probabilities. 
This intractable knot was eventually untied, or rather cut, only in the last cen-
tury, in the move from objectively to subjectively measured uncertainty, for with 
that move was jettisoned the Principle of Indifference, as inappropriate in a theory 
merely of consistent degrees of belief. Unfortunately there was no further system-
atic development of the idea that the probability axioms are no more than con-
sistency constraints, at any rate within an explicitly logical setting: Ramsey went 
on to pioneer the development of subjective probability as a subtheory of utility 

140 
COLIN HOWSON 
theory (we shall see why shortly), and de Finetti employed the idea of penalties 
like Dutch Books to generate the probability laws. De Finetti's Dutch Book argu-
ment and its extension to scoring rules generally also lead in the wrong direction, 
of financial prudence in unrealistic circumstances (you always agree to take either 
side of a bet with any specified stake at your personal betting quotient). So despite 
its promising starting idea, from a strictly logical point of view Ramsey's and de 
Finetti's work represented a dead end. In what follows I propose to go back to the 
beginning, and combine the intuitions of Leibniz and J. Bernoulli with the con-
ceptual apparatus of modern logic. We shall then get a 'rational reconstruction' of 
history closer to their intentions than to the actual course of events. 
DEGREE OF BELIEF 
There is a long tradition of informally expressing one's uncertainty about a propo-
sition A in the odds which one takes to reflect the currently best estimate of the 
chances for and against A's being true. Mathematically, odds are a short step from 
probabilities, or at any rate the probability scale of the unit interval. The step 
is taken by normalising the odds, by the rule p = odds/(l +odds). p is called the 
betting quotient associated with those odds. The p-scale has the advantage as an 
uncertainty measure that it is both bounded and symmetrical about even-money 
odds (unlike the odds scale itself where the even-odds point, unity, is close to one 
end of the scale (0) and infinitely far from the other). Since the seventeeth century 
betting quotients as the measure of uncertainty have been called probabilities, and 
for the time being I shall do so myself (I am quite aware that they have not yet 
been shown to be probabilities in the technical sense: that will come later). Note 
that the inverse transformation gives odds = p/(l - p). 
To determine how such probabilities should be evaluated in specific cases was 
the function of the Principle of Indifference, a function which, as we have seen, 
it was unable to discharge consistently. However, abandoning the Principle, as 
Ramsey saw, seems to leave behind only beliefs about chances. This appears to 
signal a move into mathematical psychology, and particularly into measurement 
theory to develop techniques for measuring partial belief. Such at any rate was 
the programme inaugurated and partly carried out by Ramsey in his pioneering 
study 'Truth and Probability' [l931l. According to Ramsey the empirical data of 
partial belief are behaviourally-expressed preferences for rewards which depend 
on the outcomes of uncertain events: for example, in bets. Though Ramsey's idea 
seems to have a scope far beyond ordinary betting, as he pointed out we can always 
think in a general context of bets not just against human opponents but also against 
Nature. But preferences among bets will normally depend not only on the odds but 
also on the size of the stake: for large stakes there will be a natural disinclination 
to risk a substantial proportion of one's fortune, while for very small ones the odds 
will not matter overmuch. The only answer to this problem, Ramsey believed, was 
to give up the idea that odds, at any rate money odds, could measure uncertainty, 

THE LOGIC OF BAYESIAN PROBABILITY 
141 
and invoke instead a very general theory of rationally contrained preference: in 
other words, axiomatic utility theory. 
But invoking the elaborate apparatus of contemporary utility theory, with its 
own more or less serious problems, seems like taking a hammer -
and a hammer 
of dubious integrity and strength -
to crack a nut. Why not simply express your 
beliefs by reporting the probabilities you feel justified by your current information, 
in the traditional way? The answer usually given is that to do so begs questions that 
only full-blown utility theory can answer. These relate to what these probabilities 
actually mean. They are odds, or normalised odds, so presumably they should 
indicate some property of bets at those odds. For example, I would be taken to 
imply that in a bet on A at any other odds than p / (1- p), where p is my probability 
of A, I think one side of that bet would be positively disadvantaged, given what 
I know about A and the sorts of things that I believe make A more likely to be 
true than false -
or not as the case may be. Thus my 'personal probability' (that 
terminology is due to Savage) determines what I believe to the fair odds on A: i.e. 
those odds which I believe give neither side an advantage calculable on the basis 
of my existing empirical knowledge. This is where the objections start. 
The first is that your assessment of which odds, or betting quotients, do or do 
not confer advantage is a judgment which cannot be divorced from considerations 
of your own -
hypothetical or actual- gain or loss and how you value these; 
for to ask which of two "equal" betters ask has the advantage is to ask 
which of them has the preferable alternative. [Savage, 1954, p. 63] 
Granted that, we seem after all ineluctably faced with the task of developing a the-
ory of uncertainty as part of a more general theory of preference, i.e. utility theory: 
precisely what it was thought could be avoided. But should we grant it? One hes-
itates to dismiss summarily a considered claim of someone with the authority of 
Savage, but nonetheless it simply is not true. Here is a simple counterexample (due 
in essence to [Hellman, 1997, p. 195]): imagine the bettors to be coprophiliacs and 
the stakes measures of manure. One's own preferences are irrelevant to judging 
fairness. They have only seemed relevant because gambles are traditionally paid 
in money and money is a universal medium of value. 
Nevertheless, a Savagean objector might continue, to compute advantage you 
still need to know how the bettors themselves evaluate the payoffs, (a) in isolation 
and (b) in combination with what are perceived by those parties to be the chances 
of the payoffs; and both (a) and (b) may vary depending on the individual. For 
example, one party may be risk-averse and the other risk-prone, and a fair bet 
between such antagonists will be quite different from a fair bet between two risk-
neutral ones. The answer to this is that the concept of advantage here is that of 
bias: on such a criterion a bet is fair simply if the money odds match what are 
perceived to be the fair odds, whatever the beliefs or values of the bettors. This 
is easily seen to imply an expected value criterion. For suppose Rand Q are the 
sums of money staked, and that the odds measure of your uncertainty is p : (1- p). 
The money odds R : Q match your fair odds just in case pQ = (1 - p )R, i.e. 

142 
COLIN HOWSON 
(1) 
pQ - (1 - p)R = 0 
tells us that the bet is fair just when what is formally an expected value is equal 
toO. 
Now things do not look at all promising, however, for (1) seems to lead imme-
diately to the notorious St Petersburg Problem. For those unfamiliar with it, this 
involves a denumerably infinite set of bets where a fair coin is repeatedly tossed 
and the bettor pays 1 ducat to receive 2n ducats if the nth toss is the first to land 
heads, n = 1,2, .... Given no other information, the coin's assumed equal ten-
dency to land heads or tails will presumably determine the fair odds. In that case 
the associated probability of getting the first head at the nth toss is 2-n , and the 
expected value of each bet is clearly O. Yet everyone intuitively feels that no bet-
tor would be wise in accepting even a large finite number of these bets, let alone 
all of them -
which would of course mean staking an infinite sum of money. 
The inequity of such bets, according to practically all commentators from Daniel 
Bernoulli onwards, is due to the diminishing marginal utility of money, and in 
particular the inequality in value between losing and gaining the same sum: the 
loss outweighs the gain, the more noticeably the larger the sum involved. In the 
St Petersburg game you are extremely likely to lose most of your 100 ducats if 
you accept the first 100 of the bets, a considerable sum to lose on a trifle. Your 
opponent could even less afford to payout if the IOOth bet won. Either way you 
would be silly to accept the bets even though they are fair by the criterion of money 
expectation. Nowadays it is taken for granted that the only solution to the prob-
lem is to use a utility function which is not only concave (like Daniel Bernoulli's 
logarithmic function) but also bounded above (unlike the logarithm). 
I do not think we should worry too much about the St Petersburg Problem, for it 
begs a question in its turn, namely that a bet cannot be fair which it would be highly 
imprudent for one side to accept. But this is exactly what is being questioned, and 
is I think just false. A contract between a prince and a pauper is not unfair just 
because one can pay their debt easily and the other cannot. That is to confuse two 
senses of fairness: as lack of bias (the sense intended here), in which payoffs are 
balanced against probabilties according to (1), and as lack of differential impact 
those payoffs will have taking into consideration the wealth of the players. And 
indeed these quite distinct ideas have become confused in the Bayesian literature, 
to the extent that probability has become almost uniformly regarded as necessarily 
a subtheory of a theory of prudent behaviour. 
The idea that the expected money-gain principle was vulnerable to the St Peters-
burg Problem was already challenged over two centuries ago by Condorcet, who 
pointed out that in repeated bets at odds 2n - 1 : 1 on heads landing first on the 
nth toss against, the average gain converges in probability to the expected value, 
i.e. 0, while, to quote Todhunter reporting Condorcet, "if any other ratio of stakes 
be adopted a proportional advantage is given to one of the players" [Todhunter, 
1865, pp. 392-3931 Of course, this argument relies on (a) the full apparatus of 
probability theory (the quick modern proof would use Chebychev's Inequality) and 

THE LOGIC OF BAYESIAN PROBABILITY 
143 
(b) assuming that the trials are uncorrelated with constant probability, neither of 
which assumption is appropriate here. But one doesn't need all that anyway: the 
moments equation (1) itself is a sufficient answer. 
To sum up: the agent's probability is the odds, or the betting quotient, they 
currently believe fair, with the sense of 'fair' that there is no calculable advantage 
to either side of a bet at those odds. Despite opposing the widely accepted view that 
subjective probability can be coherently developed only within a theory of utility, 
this view is, I believe, quite unexceptionable, and certainly not vulnerable to what 
are usually taken to be decisive objections to it. Not only is it unexceptionable: it 
will tum out to deliver the probability axioms in a way that is both elegant and fully 
consonant with the idea that they are nothing less than conditions of consistency, 
and a complete set of such conditions at that. 
2 CONSISTENCY 
Now we can move on to the main theme of this paper. Ramsey claimed that the 
laws of probability are, with the probability function interpreted as degree of be-
lief, laws of consistency and so of the species of logic. Unfortunately, as we saw, 
Ramsey then proceeded to divert the theory into the alien path of utility, where 
'consistent' meant something like 'rational'. But rationality has nothing essen-
tially to do with logic, except at the limits. Can we give the idea of consistent 
assignments of fair betting quotients an authentically logical meaning? The an-
swer is that we can. We proceed in easy stages. A traditional sense of consistency 
for assignments of numbers is equation-consistency, or solvability. A set of equa-
tions is consistent if there is at least one single-valued assignment of values to 
its variables. The variables evaluated in terms of betting quotients are proposi-
tions. Correspondingly, we can say that an assignment of fair betting quotients is 
consistent just in case it can be solved in a analogous sense, the sense of being 
extendable to a single-valued assignment to all the propositions in the language 
determined by them (this is the notion of consistency Paris appeals to in a recent 
work on the mathematical analysis of uncertainty [Paris, 1994, p. 6]). But what, 
it might be asked, has the notion of consistency as solvability to do with logical 
consistency? Everything, it turns out. For deductive consistency itself is noth-
ing but solvability. To see why, it will help to look at deductive consistency in a 
slightly different sort of way, though one still equivalent to the standard account, 
as a property not directly of sets of sentences but o/truth-value assignments. 
According to the standard (classical) Tarskian truth-definition for a first or 
higher-order language conjunctions, disjunctions and negations are homomorphi-
cally mapped onto a Boolean algebra of two truth-values, {T, F}, or {1,O} or 
however these elements are to be signified (here T or 1 signifies 'true' and F or 
o signifies 'false'). Now consider any attribution of truth-values to some set ~ 
of sentences of L, i.e. any function from ~ to truth-values. We can say that this 
assignment is consistent if it is capable of being extended to a function from the 

144 
COLIN HOWSON 
entire set of sentences of L to truth-values which satisfies those homomorphism 
constraints. For propositional languages the language of equation-solvability is 
sometimes explictly used: formulas can be regarded as representing boolean poly-
nomial equations [Halmos, 1963, p. 8] in the algebra oftwo truth-values, and sets 
of them are consistent just in case they have a simultaneous solution. 
The theory of 'signed' semantic tableaux or trees is a syntax perfectly adapted 
to seeing whether such equations are soluble and if so, finding all the solutions to 
them. ('Signing' a tableau just means appending Ts and Fs to the constituent sen-
tences. The classic treatment is Smullyan [1968, pp. 15-30], a simplified account 
is in [Howson, 1997b].) Here is a very simple example: 
AT 
A-+BT 
BF 
The tree rule for [A -+ B T] is the binary branching 
/\ 
FA 
B T 
Appending the branches beneath the initial signed sentences results in a closed 
tree, i.e. one on each of whose branches occurs a sentence to which is attached 
both a T and an F. A soundness and completeness theorem for trees [Howson, 
1997b, pp. 107-111] tells us that any such tree closes if and only if the initial 
assignment of values to the three sentences A, A -+ Band B is inconsistent, i.e. 
unsolvable over L subject to the constraints of the general truth-definition. 
To sum up: in deductive logic (semantic) consistency can be equivalently de-
fined in the equational sense of a truth-value assignment being solvable, i.e. ex-
tendable to a valuation over all sentences of L satisfying the general rules gov-
erning truth-valuations. By a natural extension of the more familiar concept we 
can call such an extension a model of the initial assignment. Note that this sense 
of consistency does not pick out a different concept from the more usual one of a 
property of sets of sentences. Indeed, the two are essentially equivalent, as can be 
seen by noting that an assignment of truth-values to a set E of sentences is consis-
tent in the solvability sense above just in case the set obtained from E by negating 
each sentence in E assigned F is consistent in the standard (semantic) sense. 
We have become accustomed to understand by consistency deductive consis-
tency and thereby something that exists, so to speak, only in a truth-centred en-
vironment. That this is not necessarily the case is now clear, for deductive con-
sistency is seen to be merely an application of a much more general (and older) 
idea of consistency as solvability, having nothing necessarily to do with truth at 
all, but merely with assignments of values, not necessarily and indeed not usually 
truth-values, to variables in such a way that does not result in overdetermination. 

THE LOGIC OF BAYESIAN PROBABILITY 
145 
What deductive and probabilistic consistency do have in common however is 
that the variables in question are propositional, and to proceed further we need 
to specify the language relative to which an assignment of fair betting quotients 
is solvable (if it is), subject to the appropriate constraints. For the sake of defini-
tieness let us start, as in deductive logic, with a language relative to which the class 
of propositions will be determined. In fact, we can employ just the same sort of 
language, a first order language. Let L be one such, without identity. Let n = the 
class of structures 8' interpreting the extralogical vocabulary of L. For any sen-
tence A of L let Mod(A) = {8' : A is true in 8'}. Let F = {Mod(A): A a sentence 
of L}. Following Carnap [1971, pp. 35-37] F is the set of propositions of L. Note 
that F is a Boolean algebra isomorphic to the Lindenbaum sentence algebra of L 
[Paris, 1994, p. 34]. In fact, it will be better to work in a rather more extensive class 
of propositions, because F as it stands represents merely the propositions express-
ible by single sentences of L. But it is well-known that the mathematical theories 
incorporated in any minimally acceptable theory of physics, for example, are not 
expressible by single sentences of a first order language: they are not finitely ax-
iomatisable (even the simplest of all mathematical theories, the theory of identity 
investigated by Euclid over 2000 years ago, is not finitely axiomatisable). The 
customary closing off of F under denumerably infinite unions (disjunctions) and 
intersections (conjunctions), generating the Borel field B(F), (more than) allows 
such theories to be treated on a par with their finitely axiomatisable cousins. 
In nand B(F) we have two of the three ingredients of what mathematicians 
call a probability space. The third is a probability function defined on B(F). 
Finding this will be the next task (in what follows I shall use A, B, C, ... now 
to denote members of B(F)). The first step on the way is to determine the ap-
propriate constraints on solutions of assignments of fair betting quotients. These 
will function like the purely general rules of truth in classical truth-definitions, as 
analytic properties of truth. In the context of fair betting quotients the constraints 
should presumably be analytic of the notion of fairness as applied to bets. At this 
point it is helpful to transform the payoff table 
A 
T Q 
F -R 
into the well-known (betting-quotient, stake) 'coordinates' introduced by de Finetti 
in his seminal paper [1964]. The stake 5' is R + Q and the betting quotientp* is 
of course just Rj S, and the table above becomes 
A 
T 
S(I - p*) 
F 
-p*S 
Where I A is the indicator function of A, the bet can now be represented as a 
random quantity S(IA - p*), and the equation (1) now transforms to 
(I') 
pS(I- p*) - p*S(I- p) = 0 

146 
COLIN HOWSON 
where p is your fair betting quotient. Clearly, the left hand side is equal to 0 just 
when p = p', which is merely a different way of stating that a fair bet is one in 
which your estimate of the fair odds is identical with the money odds. Besides bets 
like the above there are also so-called conditional bets, i.e. bets on a proposition 
A which require the truth of some proposition B for the bet to go ahead: if B is 
false the bet on A is annulled. The bet is called a conditional bet on A given B. A 
betting quotient on A in a conditional bet is called a conditional betting quotient. 
A conditional bet on A given B with stake S and conditional betting quotient p 
clearly has the form IBS(IA - p). If your uncertainty about A is registered by 
your personal fair betting quotient on A then your uncertainty, your conditional 
uncertainty, on A on the supposition that B is true will plausibly be given by your 
conditional fair betting quotient on A given B. 
Let (F) be the set of formal constraints other than 0 ~ p ~ 1 which fair betting 
quotients, including conditional fair betting quotients, should in general satisfy. 
This general content is contained in the claim that a fair bet is unbiased given the 
agent's own beliefs. These of course are unspecified, varying as they do from 
individual to individual. We can quickly infer 
(a) If p is the fair betting quotient on A, and A is a logical truth, then p = 1; 
if A is a logical falsehood p = O. Thus logical truth, logical falsehood 
and entailment relations correspond to the extreme values of fair betting 
quotients. Similarly if B entails A then the conditional betting quotient on 
A given B should be 1, and 0 if B entails the negation of A. 
(b) Fair bets are invariant under change of sign of stake. 
The reason for (a) is not difficult to see. If A is a logical truth (i.e. A = D) and pis 
less than 1 then in the bet S (I A - p) with betting quotient p, I A is identically 1 and 
so the bet reduces to the positive scalar quantity S(l- p) received come what may. 
Hence the bet is not fair since one side has a manifest advantage. Similar reasoning 
shows that if A is a logical falsehood then p must be O. Similar reasoning accounts 
for the conditions relating to entailment. As to (b), (I') shows that the condition 
for a bet to be fair is independent both of the magnitude and sign of S. 
But there is something else to (F) besides (a) and (b), a natural closure con-
dition which can be stated as follows: if a set of fair bets determines a bet on 
a proposition B with betting quotient q then q is the fair betting quotient on B. 
What is the justification for this apart from 'naturalness'? It is well-known by pro-
fessional bookmakers that certain combinations of bets amount to a bet on some 
other event, inducing a corresponding relationship between the betting quotients. 
For example, if A and B are mutually inconsistent then simultaneous bets at the 
same stake are extensionally the same as a bet on A V B with that stake, and 
if p and q are the betting quotients on A and B respectively, we easily see that 
S(IA - p) + S(IB - q) = S(IAVB - r) if and only if r = p + q. Now add to 
this the thesis that if each of a set of bets gives zero advantage then the net ad-
vantage of anybody accepting all of them should also be zero (though this thesis 

THE LOGIC OF BAYESIAN PROBABILITY 
147 
is not provable, it seems so fundamentally constitutive of the ordinary notion of a 
fair game that we are entitled to adopt it as a desideratum to be satisfied by any 
formal explication; and, of course, when it is explicated as zero expected value 
within the fully developed mathematical theory we have the elementary theorem 
that expectation is a linear functional and hence all expectations, zero or not, add 
over sums of random variables). Putting all this together we obtain the closure 
principle above. 
To proceed further, note that bets obey the following arithmetical conditions: 
(i) -S(IA - p) = S(I~A - (1 - p)). 
(ii) If A&B = -L then S(IA - p) + S(IE - q) = S(IAvE - (p + q)). 
(iii) If {Ad is a denumerable family of propositions in B(F) and Ai&Aj = -L 
and Pi are corresponding betting quotients and L: Pi exists then L: S (I Ai -
Pi) = S(IVAi - L:Pi). 
(iv) If P, q > 0 then there are nonzero numbers S, T, W such that S(IA&E -
p) + (-T)(IE - q) = IE W(IA - p/q) (T / S must be equal to p/q). The 
right hand side is clearly a conditional bet on A given B with stake Wand 
betting quotient P / q. 
Closure tells us that if the betting quotients on the left hand side are fair then so are 
those on the right. The way the betting quotients on the left combine to give those 
on the right is, of course, just the way the probability calculus tells us that proba-
bilities combine over compound propositions and for conditional probabilities. 
Now for the central definition. Let Q be an assignment of personal fair betting 
quotients to a subset X of B(F). By analogy with the deductive case, we shall 
say that Q is consistent if it can be extended to a single-valued function on all the 
propositions of L satisfying suitable conditions. 
The final stage in our investigation is to generate interesting properties of con-
sistency. If we suggestively signify a fair betting quotient on A by P(A) closure 
tells us 
(i') P(...,A) = 1 - P(A). 
(ii') If A&B = -L then P(A V B) = P(A) + P(B). 
(iii') If {Ad is a denumerable family of propositions in B(F) and Ai&Aj = -L 
and L: P(Ai) exists then P(V Ai) = L: P(Ai). 
(iv') If P(A&B) and P(B) > 0 then P(AIB) = P(A&B)/ P(B). 
It might seem slightly anomalous that in (iv') both P(B) and P(A&B) should be 
positive, but it will turn out that only the former condition need be retained. At 
any rate, from the equations (i')-(iv') and (F) it is a short and easy step to proving 
the following theorem: 

148 
COLIN HOWSON 
THEOREM 1. An assignment Q of fair betting quotients (including conditional 
fair betting quotients) to a subset X of B(F) is consistent (has a model) if and 
only if Q satisfies the constraints of the countably additive probability calculus; 
i. e. if and only if there is a countably additive probability function on B (F) whose 
restriction to X is Q. 
Proof. The proof of the theorem is straightforward. Necessity is a fairly obvious 
inference from the closure property. It is not difficult to see that the condition 
that P(A&B) be positive in (iv/) can be jettisoned once we can assume invariance 
under sign of stake. Also, given finite additivity, the condition in (ii') that I: P(Ai) 
exists is provable (using the Bolzano-Weierstrass Theorem). For sufficiency, all 
we really have to do is show that closure follows, but this is easy. For suppose P is 
a probability function on B(F) and that Xi are bets on a finite or countable set of 
propositions Ai in which the betting quotients are the corresponding probabilities 
P(Ai). Suppose also that I: Xi = S(IB - q) for some proposition B. The 
expected value relative to P of each Xi is 0, and since expectations are linear 
functionals the expected value of the sum is also O. Hence the expected value of 
S(IB - q) must be 0 also and so q = P(B). So we have closure. 
â¢ 
I pointed out earlier that there is a soundness and completeness theorem for trees 
(signed or unsigned), establishing an extensional equivalence between a semantic 
notion of consistency, as a solvable truth-value assignment, and a syntactic notion, 
as the openness of a tree from the initial assignment. In the theorem above we 
seem therefore to have an analogous soundness and completeness theorem, estab-
lishing an extensional equivalence between a semantic notion of consistency, i.e. 
having a model, and a syntactic one, deductive consistency with the probability ax-
ioms when the probability functor P signifies the fair betting quotients in Q. The 
deductive closure of the rules of the probability calculus is now seen as the com-
plete theory of generally valid probability-assignments, just as the closure of the 
logical axioms in a Hilbert-style system is the complete theory of generally valid 
assignments of truth. My proposal is to take the theorem above as a soundness and 
completeness theorem for a logic of uncertainty, of the sort Leibniz seems to have 
had in mind when he called probability 'a new kind of logic'. To complete the 
discussion we must consider briefly what qualifies a discipline for the title 'logic'. 
3 LOGIC (WHAT IS IT?) 
Wilfrid Hodges's well-known text on elementary deductive logic tells us that 
'Logic is about consistency' [Hodges, 1974]. This raises a question. Should 'con-
sistency' just mean 'deductive consistency', or might there be other species of 
consistency closely kindred the deductive variety entitling their ambient theories 
to the status of logic or logics? It may well be the case that logic is about consis-

THE LOGIC OF BAYESIAN PROBABILITY 
149 
tency without foreclosing the possibility of there being logics other than deductive. 
To answer these questions we first need an answer to the question 'What is logic?' . 
My own belief is that there is no fact of the matter about what entitles a theory 
of reasoning to logical status, and one has to proceed as one does in extending 
common law to new cases, by appeal to precedent and common sense. Here again, 
of course, one must be selective, but with modern deductive logic in mind I propose 
-
hesitantly -
the following criteria for a discipline to be a logic: 
(a) Its field is statements and relations between them. 
(b) It adjudicates a mode of non-domain-specific reasoning in its field. 
(c) Ideally it should incorporate a semantic notion of consistency extensionally 
equivalent to a syntactic one: it should have a soundness and completeness 
theorem. First order logic famously has a soundness and completeness theo-
rem; so of course do many modal systems. Second order logic does not, but 
one could argue that that is the exception proving the rule, for it is largely 
for this reason that second order logic is generally regarded as not being a 
logic. 
(a) and (b) are certainly satisfied by Bayesian, i.e. evidential, probability theory: 
any factual statement whatever can be in the domain of such a probability func-
tion. An interesting fact implicit in the discussion above is that the the statements 
involved here are not tied to any language: a Borel field does not of course have 
to be generated by the sets of model-structures of a first, or indeed any, order lan-
guage. It can be completely language-free, and still nevertheless be regarded as a 
set of propositions, propositions in the most general sense of a class of subsets of 
a possibility-space, closed under the finite and countable Boolean operations. This 
creates a great deal of freedom, e.g. to assign probabilities to the measurable sub-
sets of Euclidean n-space, a freedom not available in any of the classical logical 
languages .. 
As to (c), the theorem above establishes an extensional equivalence between a 
semantic notion, having a model, and a syntactic one (the probability calculus is a 
purely syntactic theory, of a function assigning values to elements of an algebra), 
and as such, I have claimed, is in effect a soundness and completeness theorem for 
Bayesian logic. The question is whether fulfilling (a)-(c) is sufficient to warrant 
the title 'logic'. It is of course quite impossible in principle to prove this, just as 
it is impossible in principle to prove the Church-Turing Thesis that the partial re-
cursive functions exhaust the class of computable (partial) functions. In the latter 
case 'computable function', and in the former 'logic', have no precise mathemat-
ical definition. In addition, new theories of reasoning are increasingly marketed 
under the general title 'logic' as the information technology revolution gets under 
way (whereas even twenty years ago most logic texts were produced in university 
philosophy departments, now probably most are produced in computer science de-
partments). Under any reasonable definition of 'theory of general reasoning' the 

150 
COLIN HOWSON 
rules of evidential probability would qualify as such, and hence as logic in this 
broad, liberal construal. But what has been shown is that a much tighter criterion, 
having an authentic semantics provably equivalent extensionally to an equally au-
thentic syntax, applies to evidential probability in much the same sort of way that it 
applies to first order logic. Of course, it is open to anyone to deny that a complete-
ness result is essential to a genuine logic; this is, of course, just what advocates of 
second order logic say (assuming that only full models are counted). Be that as it 
may, there seems little doubt that the completeness, i.e. axiomatisability, of first 
order logic has been a major factor in its widespread acceptance not only as logic 
but pretty well as the (classical) logic. 
I have made consistency the focus of my discussion. It might well be objected 
that central to logic is the idea of consequence, or deduction. In a recent collection 
[Gabbay, 1994] dedicated to the discussion of what is to count as logic we find that 
this is a view common to almost all the authors. For example, 'logic is the science 
of deduction' [Hacking, 1994, p. 5]; 'a logical system is a pair (h Sr-) where Sr-
is a proof theory for r- [r- is a consequence relation],. [Gabbay, 1994, p. 181]; 
'Logic is concerned with what follows from what i.e. with logical consequence' 
[Aczel, 1994, p. 262]; and so on. I think that the reflections above show that this 
view, though widespread, is nevertheless incorrect. It arose because traditionally 
logic has been about conditions necessary and sufficient for the preservation of just 
one of the values in a two-valued system, the truth-value 'true'. In this sense, and 
indeed quite naturally, logic has traditionally been deterministic. It is true that there 
have been proposals for various sorts of many-valued logics, discrete and contin-
uous, but even there the tendency has been to retain as far as possible something 
like a traditional concept of consequence. Even Adams's explicitly probabilistic 
system does this [Adams, 1998]. I believe that it is misguided because it is in ef-
fect a denial of the freedom such a mutli-valued system affords to get away from 
what is, I believe, nothing more than an artifact of two-valued systems. Of course, 
even in the account proposed here there is a consequence relation, but it is only the 
trivial one of deductive consequence from the probability axioms, telling us that 
if such and such are the probability-values of a specified set of propositions, then 
so and so is the probability of some other proposition. Williamson, discussing 
the account I have given above, points out that a relation of probabilistic conse-
quence emerges naturally by analogy with the usual deductive notion of semantic 
consequence. A sentence A is a semantic consequence of a set E of sentences 
iff every model of E is a model of A. This transforms to: an assignment r(A) 
is a consequence of an assignment q(B1), .â¢. , q(Bn) iff every probability function 
extending q also extends r, i.e. iff every model, in the sense I have given above, 
of q is a model of r [Williamson, 2001]. But this does not generate any notion 
of consequence between A and B 1 , â¢.. ,Bn themselves. As Williamson notes, it 
generates a notion of probabilistic consequence, but only in the deductive sense 
above: r(A) is a consequence of q(Bd, ... ,q(Bn) iff P(A) = q(A) follows 
deductively from the probability axioms together with the 'assumption formulas' 
P(Bd = q(Bd,Â· .. ,P(Bn) = q(Bn). 

THE LOGIC OF BAYESIAN PROBABILITY 
151 
Contemporary discussions of the relation between probability and formal de-
ductive logic take a quite different approach to the one I regard as implicit in the 
theorem above. Some, e.g. [Gaifman, 1964; Scott and Krauss, 1970], take the log-
ical aspect of probability to be exhausted by defining a probability function on the 
sentences of a suitable formal language, either a standard first order language or 
an infinitary one (as with Scott and Krauss), and showing how standard measure-
theoretic arguments have to be correspondingly modified, in particular the exten-
sion theorem that states that a countably additive probability function on a field of 
sets has a unique countably additive extension on the Borel closure [Kolmogorov, 
1956] p. 17). Gaifman provides an analogue of this for finitely additive probability 
functions defined on the class of sentences of a first order language with equality, 
showing that if a condition that has consequently come to be known as the Gaif-
man condition is satisfied then there is a unique extension from the quantifier-free 
sentences to all the sentences of the language (the Gaifman condition states that 
the supremum of the probabilities of all the disjunctions of instances of a formula 
is equal to the probability of its existential quantification; in terms of the Linden-
baum algebra the Gaifman condition is that probabilities commute with suprema). 
Others, like Fagin and Halpern [1988], and in a different way Heifetz and Mon-
gin [forthcoming], incorporate probability into the syntax of a formal language. 
'Pulling down' probability into the object language is of course very much in the 
spirit of modal logic, and indeed Heifetz and Mongin introduce what they call a 
modal operator, 'the probability is at least a that .. .', for all rational a in [0,1], 
which they interpret according to a generalisation of Kripke semantics incorporat-
ing a probability distribution over a class of possible worlds. What distinguishes 
my own account most of all from the modal one(s) and the others that I have 
mentioned is that they take the probability axioms as pretty much given: in Gaif-
man and Scott and Krauss probability is just a weakening of the usual two-valued 
semantic valuation, while in Heifetz and Mongin the axioms are, as in Savage, 
derivative from a set of rationality constraints over preferences. I believe I have 
shown in the foregoing that the probability axioms for epistemic probability are 
naturally, if not inescapably, interpreted as being of the same general species of 
consistency constraint as the rules of deductive logic itself, and to that extent in-
trinsically logical in nature. 
We now move to consider some collateral benefits accruing from construing 
the (epistemic) probability calculus as a species of logical axioms. Where a title, 
like 'logic', is already uncontroversially bestowed elsewhere, discharging some 
genuine explanatory function should be a condition of its extension to a putative 
new case. As we shall see below, there are in addition to (a)-(c) above other 
interesting points of contact, or near-contact, with classical deductive logic, and a 
logical understanding of the rules of probability will, I hope, be seen to bring with 
it a considerable explanatory bonus. 
The following topics in Bayesian probability are all regarded as problematic 
to some extent or other: countable additivity; strict coherence versus coherence; 

152 
COLIN HOWSON 
rationality; completeness; conditionalisation; sharp versus fuzzy or interval-valued 
probability; inductive inference; penalties for infringing the rules. The logical view 
at the least usefully illuminates these and at best solves them. I shall deal with them 
in tum. 
3.1 
Countable Additivity 
There has been a good deal of controversy concerning the status of the principle 
of countable additivity within the theory of epistemic probability. Most workers 
in the field, including, famously, de Finetti himself, reject it, while a much smaller 
number accept it. I do not think that it is necessary to go into the details of the 
protagonists' arguments. The theorem above shows that it must be adopted within 
any adequate view of the rules of probability as consistency constraints. The fact 
is that if we wish to assign probabilities as widely as possible then consistency 
in their assignment over compound propositions can then be guaranteed only by 
adding countable additivity to the axioms. 
3.2 Rationality 
I started out by remarking that the recent history of subjective probability has 
tended to neglect the logical aspect identified by Leibniz, favouring instead a ra-
tionality interpretation of the constraints as prudential criteria of one type or an-
other. The trouble with adopting this line is that it is very difficult to demonstrate 
in any uncontroversial and non-question-begging way that violation of any of the 
constraints is positively irrational. Take the requirement of transitivity for pref-
erences, for example: it is not evident that certain types of intransitive preference 
are necessarily irrational, especially when it is considered that the comparisons are 
always pairwise (for a counterexample see Hughes [1981]). The logical view, on 
the other hand, need not in principle be troubled by links with rationality of only 
doubtful strength, since logic is not about rational belief or action as such. Thus, 
deductive logic is about the conditions which sets of sentences must satisfy to be 
capable of being simultaneously true (deductive consistency), and the conditions 
in which the simultaneous truth of some set of sentences necessitates the truth of 
some given sentence (deductive consequence): in other words, it specifies the con-
ditions regulating what might be called consistent truth-value assignments. This 
objectivism is nicely paralleled in the interpretation of the probability axioms as 
the conditions regulating the assignment of consistent fair betting quotients. 
3.3 Completeness 
Under the aspect of logic the probability axioms are as they stand complete. Hence 
any extension of them -
as in principles for determining 'objective' prior proba-
bilities -
goes beyond pure logic. This should come as something of a relief: the 
principles canvassed at one time or another for determining 'objective' priors have 

THE LOGIC OF BAYESIAN PROBABILITY 
153 
been the Principle of Indifference, symmetry principles including principles of 
invariance under various groups of transformations, simplicity, maximum entropy 
and many others. All these ideas have turned out to be more or less problematic: at 
one extreme inconsistent, at the other, empty. It is nice not to have to recommend 
any. 
3.4 
Coherence versus Strict Coherence 
A hitherto puzzling qustion posed first by Shimony [1955] and then repeated by 
Carnap [1971, pp. 111-114] is easily answered if we accept that the probability ax-
ioms are laws of consistency. Consider a set of bets on n propositions A1 , ... , An 
with corresponding betting quotients Pi. The classic Dutch Book argument shows 
that a necessary and sufficient condition for there being, for every set of stakes 
Si, a distribution of truth-values over the Ai such that for that distribution there 
is a non-negative gain to the bettor (or loss: reverse the signs of the stakes), is 
obedience to the probability axioms. However, if we substitute 'positive' for 'non-
negative' we also get an interesting result: the necessary and sufficient condition 
now becomes that the probability function is in addition strictly positive, i.e. it 
takes the value 0 only on logical falsehoods. Which of these two Dutch Book ar-
guments should we take to be the truly normative one: that we should always have 
the possibility of a positive gain, or that we should always have the possibility of a 
non-negative gain? It might seem that the second is the more worthwhile objective: 
what is the point of going to a lot of trouble computing and checking probability 
values just to break even? On the other hand, strictly positive probability functions 
are very restrictive. There can be no continuous distributions, for example, so a 
whole swathe of standard statistics seems to go out of the window. There does 
not seem to be a determinately correct or incorrect answer to the question of what 
to do, which is why it is a relief to learn that the problem is purely an artifact of 
the classic Dutch Book argument. Give up the idea that the probability laws are 
justified in those terms and the problem vanishes. Indeed, we now have a decisive 
objection to specifying any conditions additional to the probability axioms: the 
laws of probability as they stand are complete. 
3.5 
Unsharp Probabilities 
We seldom if ever have personal probabilities, defined by Bayes' procedure of 
evaluating uncertain options, which can be expressed by an exact real number. 
My value for the probability that it will rain some time today is rather vague, and 
the value 0.7, say, is no more than a very rough approximation. In the standard 
Bayesian model the probability function takes real-number values. But if we are 
trying to use the model to understand agents' actual cognitive decisions it would 
seem useful if not mandatory to assume that they have more or less diffuse proba-
bilities -
because they mostly if not invariably do in the real world. 

154 
COLIN HOWSON 
If I am correct then probabilistic and deductive models of reasoning are in-
timately related, suggesting that considerations which prove illuminating in one 
can be profitably transferred, mutatis mutandis, to the other. So ask: what cor-
responds in deductive models to consistent probability-values? Answer: truth-
values. Well, deductive models, or at any rate the standard ones, equally fail to 
be realistic through incorporating 'sharp' truth-values, or what comes to the same 
thing, predicates having sharp 'yes'/' no' boundaries. Thus, it is assumed in stan-
dard deductive logic that for each predicate Q and individual a in the domain, a 
definitely has Q or it definitely does not. An equivalent way of stating the assump-
tion is in terms of characteristic functions: the characteristic function of Q is a 
function fQ on the domain of individuals such that for each a, fq(a) = 1 (i.e. a 
has Q) or fQa) = 0 (a does not have Q). No intermediate values are permitted. 
And this apparatus is used to model reasoning in natural languages which by na-
ture are highly unsharp, except in the very special circumstances when technical 
vocabularies are employed. There are actually good functional reasons why natural 
predicates are not sharp: their flexibility in extending beyond know cases is an in-
dispensable feature of the adaptive success of natural languages. Not surprisingly 
the modelling of these languages by artificially sharp ones results in 'paradoxes' 
of the Sorites type (whose classic exemplar is the Paradox of the Heap). 
Such unpalatable results have prompted the investigation of more accurate meth-
ods of modelling informal deductive reasoning by means of 'vague' predicates, 
and in particular the use of so-called 'fuzzy' ones, where {O,1 }-valued charac-
teristic functions are replaced by continuous functions, with appropriate rules for 
their use. The analogue for blunting 'sharp' probability values is to replace them 
with unsharp, interval-valued ones, and the theory of these is well-understood (see 
[Walley, 1991]). But 'sharp' probability models find their justification in inves-
tigations of how evidence, in the form of reports of observations, should affect 
estimates of the credibility of hypotheses. It is quite difficult to answer this and 
related questions, e.g. how sensitive y is to x where the data are particularly nu-
merous or varied or both, without using a theory that can say things like' Suppose 
the prior value of the probability is x', and then use the machinery of the point-
valued probability calculus (in particular Bayes' Theorem) to calculate that the 
posterior value is y. So we need a fairly strong theory which will tell us things 
like this; and in the standard mathematical theory of probability we have a very 
rich theory indeed. At the same time, the model is not too distant from reality; it 
is quite possible to regard it as a not-unreasonable approximation in many appli-
cations, for example where the results obtained are robust across a considerable 
range of variation in the probability parameters. Many of the limiting theorems in 
particular have this property. 
Similar sorts of considerations apply to the usual formal models of deductive 
reasoning. There are non-sharp models, but it is partly the sharpness itself of the 
more familiar structures that explains why they still dominate logical investiga-
tions: nearly all the deep results of modern logic, like the Completeness Theorem 
for the various formulations of first order logic, and the limitative theorems of 

THE LOGIC OF BAYESIAN PROBABILITY 
155 
Church, GOdel, Tarski etc., are derived within 'sharp' models. Much more could 
be written on this subject, but space and time are limited and enough has, I hope, 
now been said to convey why sharp models are not the unacceptable departures 
from a messier reality that at first sight they might seem to be. 
3.6 Conditionalisation 
If the rules of the probability calculus are a complete set of consistency constraints, 
what is the status of conditionalisation, which is not one of them, though it is 
standardly regarded as a 'core' Bayesian principle? Recall that conditionalisation 
is the rule that 
P(BIA) = r 
PI(A) = 1 
PI(B) = r 
where 'P' (A) = l' signifies an exogenous 'learning' of A; P is your probability 
function up to that point, and pI after. There is a well-known Dutch Book argu-
ment for this rule due to David Lewis (reported in [Teller, 1973]). I have given 
detailed reasons elsewhere [Howson and Urbach, 1993, Ch. 6] why I believe any 
Dutch Book argument in the 'dynamic' context to be radically unsound and I shall 
not repeat them all here. What I will do is show how consideration of a correspond-
ing, obviously unsound, deductive analogue enables us to translate back and see 
why the probabilistic dynamic rule should in principle be unsound too. Consider 
first a possibly slightly unfamiliar -
though sound -
version of modus ponens, 
where v is a truth-valuation and r = 1 (true) or 0 (false): 
(2) 
v(A -+ B) = r 
v(A) = 1 
v(B) = r 
But now suppose v and w are distinct and consider 
v(A -+ B) = r 
w(A) = 1 
w(B) = r 
This 'dynamic' version of (2), where v and w represent earlier and later valuations, 
is clearly invalid. Indeed, suppose A says that B is false, i.e. A = ...,B, and you 
now (v) accept...,B -+ B (i.e. v(...,B -+ B) = 1): it means accepting B), but later 
(w) accept...,B (i.e. w(...,B) = 1). If you try to 'conditionalise' and infer B (i.e. 
w(B) = 1) you will obviously be inconsistent. 
Here is a probabilistic analogue of that counterexample. Let A say not that B 
is false but that B is going to be less than lOO% probable: A = "P' (B) < 1", 
where pI is your probability function at some specified future time t (e.g. you 
are imagining that you will be visited by Descartes's demon). Further, suppose 
P(B) = 1, and suppose P(A) > O. If you are a consistent probablistic reasoner 
P(BIA) = 1. But suppose at t it is true that pI (B) < 1, and you realise this 
by introspection (i.e. PI(A) = 1). If you try to conditionalise on A and infer 

156 
COLIN HOWSON 
P'(B) = P(BIA) you will be inconsistent. In the deductive 'dynamic' modus 
ponens, only if v(A --+ B) = w(A --+ B) can you validly pass from w(A) = 1 
and v(A --+ B) = r to w(B) = r (r = 0 or 1): i.e. 
w(A --+ B) = v(A --+ B) 
w(A) = 1 
(3) 
w(B) = v(A --+ B) 
which is just a substitution instance of (2). This suggests the analogous rule 
(4) 
P'(BIA) = P(BIA) 
P'(B) = 1 
P'(B) = P(BIA) 
Indeed this rule is valid, but you don't need a new rule to tell you so: it already 
follows from the probability calculus! In other words, conditionalisation is only 
conditionally valid, and the conditions for validity are already supplied by the 
probability calculus. Similar considerations apply to 'probability kinematics'. 
The rather striking similarities between (3) and (4) above suggest a further point 
of contact between Bayesian probability and deductive logic. Do not these rules 
imply that P(BIA) is the probability of a type of sentential conditional? But, as 
is well-known, this question seemed to have been answered firmly in the negative 
by David Lewis with his so-called 'triviality theorem' [Lewis, 1973]. In spite of 
the apparent finality of the answer there have been attempts to bypass it via var-
ious types of non-Boolean propositional logics incorporating a conditional B~A 
satisfying the so-called Adams Principle P(BIA) = P(A ~ B) (from [Adams, 
1975]). Long before Lewis's result de Finetti had suggested a non-Boolean, three-
valued logic of conditionals [de Finetti, 1964, Section 1] satisfying that condition, 
with values 'true', 'false' and 'void', where a conditional is 'void' if the antecedent 
is false (cf. called-off bets). This is not the place for a discussion of these at-
tempts and I refer the reader to Milne [1997] for a relatively sympathetic account 
of this and other theories of non-Boolean conditionals, and to Howson [1997 a] for 
a somewhat less sympathetic discussion. 
3.7 Inductive Inference 
This is a vast topic and I can only sketch here the account I have given at length 
elsewhere [Howson, 2000, Ch. 8]. The logical view of the principles of subjective 
probability exhibits an extension of deductive logic which still manages to remain 
non-ampliative. It therefore also respects Hume's argument that there is no sound 
inductive argument from experiential data that does not incorporate an inductive 
premise, and it also tells us what the inductive premise will look like: it will be a 
probability assignment that is not deducible from the probability axioms. Far from 
vitiating Bayesian methodology I believe this Humean view, which arises naturally 
from placing epistemic probability in an explictly logical context, strengthens it 
against many of the objections commonly brought against it (see Howson loco 
cit.). 

THE LOGIC OF BAYESIAN PROBABILITY 
157 
3.8 Sanctions 
A logical view of the probability axioms is all very well, but what sanction at-
taches to infringing them on this view? Indeed, doesn't the analogy with deductive 
logic break down at precisely this point? There is after all an obvious sanction to 
breaking the rules of deductive consistency: what you say cannot be true if it is 
inconsistent. Perhaps surprisingly, however, there seems to be as much -
or as 
little -
sanction in both cases. In the probabilistic one there certainly are sanc-
tions: not just the ( usually remote) theoretical possibility of being Dutch-Booked 
were you to bet indiscriminately at inconsistent betting quotients (note that there 
is no presumption in the earlier discussion that you will and certainly not that you 
ought to bet at your fair betting quotients, or indeed that you ought to do anything 
at all), but those arising in general from accepting fallacious arguments with prob-
abilities: we have only to look at the notorious Harvard Medical School Test to 
see what these might be (see [Howson, 2000, pp. 52-54]). Morover, probabilistic 
inconsistency is as self-stultifying as deductive: as we saw in the previous chap-
ter, inconsistency means that you differ from yourself in the uncertainty value you 
attach to propositions, just as deductive inconsistency means that you differ from 
yourself in the truth-values you attach to them. 
As to the positive sanctions penalising deductive inconsistency, these on closer 
inspection turn out to be less forceful than they initially seemed. Here is Albert 
rehearsing them: 
Logical [deductive] consistency serves a purpose. Beliefs cannot pos-
sibly be true if they are inconsistent. Thus, if one wants truth, log-
ical consistency is necessary. An analogous argument in favor of 
Bayesianism would have to point out some advantage of coherence 
unavailable to those relying on non-probabilistic beliefs and deduc-
tive logic alone. Such an argument is missing. [Albert, 2001, p. 366] 
The two principal assertions here are both incorrect. Firstly, logical consistency 
is not necessary for truth. False statements are well known to have true con-
sequences, lots of them, and inconsistent statements the most of all since every 
statement follows from a contradiction. Current science may well be be inconsis-
tent (many distinguished scientists think it ~s), but it has nevertheless provided a 
rich bounty of truths. The benefits of maintaining deductive consistency are more 
complex and less direct than is often supposed, but the principal penalty is the 
same in both the deductive and the probabilistic case: inconsistency amounts to 
evaluating the same proposition in incompatible ways. It is self-stultifying. More 
generally, the practical penalties attaching to deductive inconsistency are not ob-
viously greater than those attaching to probabilistic inconsistency. What practical 
consequences flow from either will vary with one's situation. 

158 
COLIN HOWSON 
4 
CONCLUSION 
In the foregoing pages I have tried to carry through Leibniz's programme for un-
derstanding the rules of evidential probability as a species of logic as authentic 
as those of deductive logic. In both cases these rules are conditions of solvability 
of value-assignments, with similar completeness properties. I have also argued 
that this leads to a quite different, and much more coherent and fruitful, view of 
Bayesian probability than the usual one which is as a theory of prudential rational-
ity. One thing that Lakatos's well-known theory of scientific research programme 
emphasises is that a programme may be almost written off yet eventually come 
back to win the field (see Lakatos [1970]). It is early perhaps to predict that this 
will happen with the logical programme, but I hope I have shown that its resources 
are more than adequate to the task. 
ACKNOWLEDGEMENT 
I would like to thank Oxford University Press for allowing me to reproduce pas-
sages from Bume 's Problem: Induction and the Justification of Belief, C. Howson, 
2000. 
Department of Philosophy, Logic and Scientific Method, London School of Eco-
nomics, London, UK. 
BIBLIOGRAPHY 
[Aczel, 1994] P. Aczel. Schematic consequence. In [Gabbay, 1994, pp. 261-273]. 
[Adams, 1975] E. W. Adams. The Logic of Conditionals, Reidel, Dordrecht, 1975. 
[Adams, 1998] E. W. Adams. A Primer of Probability Logic, CSLI, Stanford, 1998. 
[Albert, 2001] M. Albert. Bayesian learning and expectations formation: anything goes. This volume, 
pp. 347-368. 
[Anscombe and Aumann, 1963] F.1. Anscombe and R. J. Aumann. A definition of subjective proba-
bility, Annals of Mathematical Statistics, 34,199-205,1963. 
. 
[Bayes, 1763] T. Bayes. An essay towards solving a problem in the doctrine of chances, Philosophical 
Transactions of the Royal Society of London, 1763. 
[Bernoulli, 1715] 1. Bernoulli. Ars Conjectandi, Basel, 1715. 
[Bolzano,1850] B. Bolzano. Theory of Science, 1850 
[Carnap, 1971] R. Camap. A basic system of inductive logic. In Studies in Inductive Logic and Prob-
ability, Volume I, R. Camap and R. C. Jeffrey, eds. pp. 33-167. University of California Press, 
1971. 
[Camap, 1950] R. Camap. The Logical Foundations of Probability, Chicago: University of Chicago 
Press, 1950. 
[Couturat, 1901] L. Couturat. La Logique de Leibniz, Paris, 1901. 
[de Finetti, 1964] B. de Finetti. Foresight; its Logical Laws, its Subjective Sources', Studies in Sub-
jective Probability, H. Kyburg and H. Smolder, eds. pp. 93-159. Wiley, 1964. (de Finetti's paper 
was published originally in 1937 in French.) 
[Fagin and Halpern, 1988] R. Fagin and 1. Y. Halpern. Reasoning about Knowledge and Probability: 
Preliminary Report. In Proceedings of the Second Conference on Theoretical Aspects of Reasoning 
about Knowledge, M. Y. Vardi, ed. pp. 277-293. Morgan Kaufmann, 1988. 
[Gabbay, 1994] D. M. Gabbay, ed. What is a Logical System?, Oxford: Oxford University Press, 1994 

THE LOGIC OF BAYESIAN PROBABILITY 
159 
[Gabbay, 1994a] D. M. Gabbay. What is a logical system? In [Gabbay, 1994, pp. 179-217]. 
[Gaifman, 1964] H. Gaifman. Concerning measures in first order calculi. Israel Journal of Mathemat-
ics, 2,1-18, 1964. 
[Hacking, 1994] I. Hacking. What is logic? In [Gabbay, 1994, pp. 1-35]. 
[Halmos, 1963] P. Halmos. Lectures on Boolean Algebras, Van Nostrand, Princeton" 1963. 
[Heifetz and Mongin, forthcoming] A. Heifetz and P. Mongin. The Modal Logic of Probability, forth-
coming. 
[Hellman, 1997] G. Hellman. Bayes and beyond. Philosophy of Science, 64,190-205,1997. 
[Hodges, 1974] W. Hodges. Logic, Harmondsworth: Penguin Books, 1974. 
[Howson, 1997a] C. Howson. Logic and probability', British Journal for the Philosophy of Science, 
48,517-531,1997. 
[Howson,1997b] C. Howson. Logic With Trees, London: Routledge, 1997 .. 
[Howson,2000] C. Howson. Hume's Problem: Induction and the Justification of Belief, Oxford: Ox-
ford University Press, 2000. 
[Howson and Urbach, 1993] C. Howson and P. Urbach. Scientific Reasoning: the Bayesian Approach, 
2nd edition, Chicago: Open Court, 1993. 
[Hughes, 1980] R. I. G. Hughes. Rationality and intransitive preferences, Analysis, 40, 132-134, 
1980. 
[Kac and Ulam, 1968] M. Kac and S. Ulam. Mathematics and Logic, New York: Dover, 1968. 
[Kolmogorov, 1956] A. N. Kolmogorov. Foundations of the Theory of Probability, New York: 
Chelsea, 1956. 
[Lakatos, 1970] I. Lakatos. Falsification and the methodology of scientific research programmes. In 
Criticism and the Growth of Knowledge, I. Lakatos and A. Musgrave, eds. pp. 91-197. Cambridge: 
Cambridge University Press, 1970. 
[Lewis, 1973] D. Lewis. Probabilities of conditionals and conditional probabilities. Philosophical Re-
view, vol. LXXXV, 297-315,1973. 
[Milne, 1997] P. M. Milne. Bruno de Finetti and the logic of conditional events. British Journal for 
the Philosophy of Science, 48, 195-233, 1997. 
[Paris,1994] 1. Paris. The Uncertain Reasoner's Companion. A Mathematical Perspective, Cam-
bridge: Cambridge University Press, 1994. 
[Poisson, 1823] S.-D. Poisson. Recherches sur la probabilite des jugements en matiere civile et en 
matiere criminelle, Paris, 1823. 
[Ramsey, 19311 F. P. Ramsey. Truth and probability. In The Foundations of Mathematics, R.B. Braith-
waite, ed. London: Kegan Paul, 1931. 
[Savage,1954] L. J. Savage. The Foundations of Statistics, New York: Wiley, 1954. 
[Scott and Krauss, 1970] D. Scott and P. Krauss. Assigning probabilities to logical formulas. In As-
pects of Inductive Logic, J. Hintikka and P. Suppes, eds. pp. 219-264,1970. 
[Shimony, 1955] A. Shimony. Coherence and the axioms of confirmation. Journal of Symbolic Logic, 
20, 1-28, 1955. 
[Smullyan, 1968] R. Smullyan. First Order Logic, New York: Dover, 1968. 
[Teller, 1973] P. Teller. Conditionalisation and Observation, Synthese, 26, 218-58, 1973. 
[Todhunter, 1865] I. Todhunter. A History of the Mathematical Theory of Probability, Cambridge and 
London, 1865. 
[Walley, 19911 P. Walley. Statistical Reasoning with Imprecise Probabilities, London: Chapman and 
Hall, 1991. 
[Williamson,2oo1] J. Williamson. Probability logic. In 'Handbook of the Logic of Inference and 
Argument: The Turn Toward the Practical', D. Gabbay, R. Johnson, H.J. Ohlbach & 1. Woods, eds. 
Elsevier, 2001. 

MARIA CARLA GALAVOTII 
SUBJECTIVISM, OBJECTIVISM AND 
OBJECTIVITY IN BRUNO DE FINETTI'S 
BAYESIANISM 
The paper will focus on Bruno de Finetti's position, which combines Bayesian-
ism with a strictly subjective interpretation of probability. For de Finetti, probabil-
ity is always subjective and expresses the degree of belief of the evaluating subject. 
His perspective does not accommodate a notion of "objective chance" in the way 
other subjectivists, including Frank Ramsey, do. To de Finetti's eyes, objectivism, 
namely the idea that probability depends entirely on some aspects of reality, is a 
distortion, and the same holds for the idea that there exists an absolute notion of 
objectivity, to be grounded on objective facts. For him there is no problem of ob-
jectivity beyond that of the evaluation of probabilities in a Bayesian framework. 
This is a complex procedure. which includes subjective elements as well as the 
consideration of objective elements like observed frequencies. 
1 DE FINETTI'S SUBJECTIVISM 
Bruno de Finetti used to call his perspective "subjective Bayesianism" [de Finetti, 
1969, p. 3], to stress that in his conception Bayes' scheme is assigned a central 
role, and that it goes hand in hand with a subjective view of probability. Inspired 
by what we would today call a radically "anti-realist" philosophy, de Finetti finds 
in the Bayesian approach a way of combining empiricism and pragmatism. The 
resulting position is not only incompatible with any perspective based on an ob-
jective notion of probability, neither can it be assimilated to other subjective views 
of probability. While being opposed both to frequentism and logicism, taken as 
"objective" views of probability, de Finetti's perspective strays from Ramsey's 
subjectivism in important respects. 
De Finetti entrusted his philosophy of probability, called "probabilism", to the 
paper "Probabilismo", 1 which he regarded as his philosophical manifesto. Its start-
ing point is a refusal of the notion of truth, and the related notions of determinism 
and "immutable and necessary" laws. In their place, de Finetti reaffirms a concep-
tion of science seen as a human activity, a product of thought, having as its main 
tool probability. " ... no science - says de Finetti - will permit us to say: this fact 
will come about, it will be thus and so because it follows from a certain law, and 
that law is an absolute truth. Still less will it lead us to conclude skeptically: the 
absolute truth does not exist, and so this fact might or might not come about, it 
may go like this or in a totally different way, I know nothing about it. What we 
can say is this: I foresee that such a fact will come about, and that it will happen 
ITbc paper was written in 1929 and published in 1931: see [de Finetti, 1931bl. 
161 
D. Corfield and J. Williamson (eds.), Foundations of Bayesianism, 161-174. 
Â© 2001 Kluwer Academic Publishers. 

162 
MARIA CARLA GALAVOTTI 
in such and such a way, because past experience and its scientific elaboration by 
human thought make this forecast seem reasonable to me" [de Finetti, 1931b, p. 
170, English edition]. Probability is precisely what makes a forecast possible. And 
since a forecast is always referred to a subject, being the product of his experience 
and convictions, "the logical instrument that we need is the subjective theory of 
probability". In other words, probabilism represents for de Finetti an escape from 
the antithesis between absolutism and skepticism, and at its core one finds the 
subjective notion of probability. 
Following the subjectivist approach, probability "means degree of belief (as 
actually held by someone, on the ground of his whole knowledge, experience, 
information) regarding the truth of a sentence, or event E (a fully specified 'single' 
event or sentence, whose truth or falsity is, for whatever reason, unknown to the 
person)" [de Finetti, 1968, p. 45]. According to de Finetti, one can show not 
only that this notion of probability is the only non contradictory one, but also that 
it covers all uses of probability in science and everyday life. This program is 
realized in two steps: firstly, an operational definition of probability is worked out, 
secondly, it is argued that the notion of objective probability is reducible to that of 
subjective probability. 
The operational definition moves along well known lines: probability is defined 
in terms of betting quotients, namely the degree of probability assigned by an indi-
vidual to a certain event is identified with the betting quotient at which he would be 
ready to bet a certain sum on its occurrence. The fundamental and unique criterion 
one must obey to avoid sure losses is that of coherence. The individual in question 
should be thought of as one in a condition to bet whatever sum against any gam-
bler whatsoever, free to choose the betting conditions, like someone holding the 
bank at a gambling-casino. Probability can be defined as the fair betting quotient 
he would attach to his bets. Coherence is a sufficient condition for the fairness of 
a betting system, and a behaviour conforming to coherence satisfies the principles 
of probability calculus, which can be derived from the notion of coherence defined 
in the specified way. This result was certainly grasped by Ramsey, but is fully 
worked out only by de Finetti in "SuI significato soggettivo della probabilita" [de 
Finetti, 1931 a]. 
Here de Finetti, in addition to the quantitative introduces a qualitative definition 
of subjective probability, based on the relation of "at least as probable as". He then 
argues that it is not essential to adopt a quantitative notion of probability expressed 
by a real number; the latter is the most common way of talking about probability, 
and also the simplest one, but is in no way the only one. This illustrates the role that 
de Finetti assigns to betting quotients within his theory: they offer an apt device 
for measuring probability and defining it operationally, but they do not represent 
an essential component of the notion of probability, which is a primitive notion, 
expressing "the psychological sensation of an individual" [de Finetti, 1931a, p. 
302]. 
This point has been overlooked by the literature. The idea that probability can 
be defined in various ways is a central feature of de Finetti's perspective, where the 

BRUNO DE FINETTI'S BAYESIANISM 
163 
scheme of bets represents only a convenient device for talking about probability in 
a way that makes it understandable to the "man in the street". Also, in his Theory 
of Probability de Finetti points out that the scheme of bets is just a useful tool, 
leading to "simple and useful insights" [de Finetti, 1970a, p. 180, English edition]. 
In addition to the scheme of bets, he adopts another way of measuring probability 
by means of scoring rules based on penalties, which is shown to be equivalent to 
the first one. Something more will be said on this method in the following pages. 
It is worth noting that the autonomous value assigned by the author to the notion 
of probability marks a difference between his position and that of the other major 
supporters of subjectivism, namely F.P. Ramsey and LJ. Savage.2 Unlike these 
authors, de Finetti does not see probability as strictly connected with utility and 
claims that probability and utility have "different 'cogent values': an indisputable 
value in the case of probability, a rather uncertain value in the case of ... utility" [de 
Finetti, 1955, p. 7]. 
The second part of de Finetti's program amounts to the reduction of objective to 
subjective probability. This is done by means of the so-called "representation theo-
rem", which was obtained by de Finetti already in 1928, though its best known for-
mulation is contained in "La prevision: ses lois logiques, ses sources subjectives" 
[de Finetti, 1937]. This result is crucial, because it gives applicability to subjective 
probability by bridging degrees of belief and observed frequencies. The funda-
mental notion here is that of "exchangeability", which can be defined as follows: 
events belonging to a sequence are exchangeable if the probability of h successes 
in n events is the same, for whatever permutation of the n events, and for every n 
and h ~ n. The representation theorem says that the probability of exchangeable 
events can be represented as follows: imagine the events were probabilistically 
independent, with a common probability of occurrence p. Then the probability of 
a sequence with h occurrences in n would be ph(l - p)n-h. But if the events are 
only exchangeable, the sequence has a probability w~n), representable according 
to de Finetti's representation theorem as a mixture over the ph(l - p)n-h with 
varying values of p: 
win) = J ph(l - pt-h j(p)dp. 
Here j (P) is a uniquely defined density for the variable p, or in other words, it 
gives the weights j(p) for the various values ph(l - p)n-h in the above mixture. 
In order to understand de Finetti's position, it is useful to start by considering 
how an objectivist would proceed when assessing the probability of an unknown 
event. An objectivist would assume an objective success probability p. But its 
value would in general remain unknown. One could give weights to the possi-
ble values of p, and determine the weighted average. The same applies to the 
probability of a sequence with hsuccesses in n independent repetitions. Note that 
because of independence it does not matter where the successes appear. De Finetti 
focuses on the latter, calling exchangeable those sequences where the places of 
2See [Ramsey, 1926] and [Savage, 1954]. 

164 
MARIA CARLA GALAVOTTI 
successes don't make a difference in probability. These need not be independent 
sequences. An objectivist who wanted to explain subjective probability, would say 
that the weighted averages are precisely the subjective probabilities. But de Finetti 
proceeds in the opposite direction, with his representation theorem. It says in his 
interpretation: starting from the subjective judgment of exchangeability, one can 
show that there is only one way of giving weights to the possible values of the 
unknown objective probabilities. According to this interpretation, objective prob-
abilities become useless and subjective probability can do the whole job. 
In the course of a comment on the notion of exchangeability, de Finetti reaf-
firms that the latter represents the correct way of expressing the idea that is usually 
conveyed by the phrase "independent events with constant but unknown probabil-
ity". If we take an urn of unknown composition, says de Finetti, the above phrase 
means that, relative to each of all possible compositions of the urn, the events can 
be seen as independent with constant probability. Then he points out that " ... what 
is unknown is the composition of the urn, not the probability: the latter is always 
known and depends on the subjective opinion about the composition, which opin-
ion is modified as new drawings are made, and observed frequencies are taken into 
account" [de Finetti, 1995, p. 214]. It should not pass unnoticed that for de Finetti 
subjective probability, being the expression of the feelings of the subjects evalu-
ating it, is always definite and known: "Probability as degree of belief is surely 
known by anyone" [de Finetti, 1973, p. 356]. 
An example, taken from the article "Logical Foundations and Measurement of 
Subjective Probability" illustrates in what sense "the concept of unknown proba-
bility ... must be seen as fictitious and misleading" [de Finetti, 1970b, p. 144]. The 
example compares the processes of Bayes-Laplace and P6lya: 
"It is well-known that the processes of Bayes-Laplace and P6lya are 
identical as probabilistic models although very different in the way 
they are produced. Bayes-Laplace model is a Bernoulli process: in 
the drawing of balls (with replacement) from an urn containing white 
and black balls in an unknown proportion, the probability distribution 
of this proportion is uniform over the interval (0, 1). A P6lya pro-
cess (contagious probabilities) consists in drawing balls from an urn 
containing, in the beginning, two balls, one white and one black, and 
where after each draw, not only is the ball drawn replaced, but also 
another one of the same color is added. After N = W + B draw-
ings (W = number of white, B = number of black) there are N + 2 
balls (W + 1) white and (B + 1) black; the probability of the next 
trial is (W + 1)/(N + 2). But, surprisingly enough, this is the same 
that happens in the Bayes-Laplace model: that is the famous Laplace 
succession rule. What is the lesson? In the Bayes-Laplace version 
it is correct to call 'unknown probability' the 'unknown proportion' 
(which has a real existence). The wording would be: 'the probability 
of each trial conditional to the knowledge of the unknown proportion 

BRUNO DE FINETTI'S BAYESIANISM 
and given the fact that my subjective opinion agrees with the standard 
assumption that the drawings are stochastically independent and that 
all the balls have equal probability'. In the P6lya version it is formally 
possible to think of a fictitious urn of Bayes-Laplace type existing in 
some supposed world of Platonic ideas... But that, outside Platon-
ism, is obviously a pointless fiction. In conclusion, the recourse to 
concepts like 'objective unknown probability' in a problem is neither 
justified nor useful for intrinsic reasons. It may correspond to some-
thing realistic under particular factual features, not of a probabilistic 
model, but of a specific device"(ibid.). 
165 
From a philosophical point of view, the reduction of objective to subjective 
probability is to be seen in a pragmatic perspective. It is performed in the same 
pragmatic spirit that inspires the operational definition of subjective probability in 
terms of coherent betting quotients, and complements the latter. If such a reduction 
is based on consideration of the role played by objective probability in statistical 
reasoning, it is again the role played by subjective probability in life and science 
that gives an operational basis for its definition. "Probability - says de Finetti - is 
actually already defined implicitly by the role played, with respect to the decisional 
criterion of an individual, by the fact that he evaluates it in a certain way" [de 
Finetti, 1963, p. 66]. 
The representation theorem does not serve only the purpose of reducing ob-
jective to subjective probability; it also shows how subjective probability can be 
applied to statistical inference. In this connection the representation theorem plays 
a vital role within subjectivism, a role whose importance can hardly be over-
rated. According to de Finetti, statistical inference can be entirely performed 
by exchangeability in combination with Bayes' rule. If the notion of probabil-
ity as degree of belief is grounded in an operational definition, probabilistic in-
ference -
taken in a SUbjective sense -
is grounded in Bayes' theorem. There-
fore, de Finetti's probabilism is intrinsically Bayesian; one could say that for him 
Bayesianism represents the crossroads where pragmatism and empiricism meet 
subjectivism. He thinks that one needs to be Bayesian in order to be a subjec-
tivist, but on the other hand subjectivism is a choice to be made if one embraces a 
pragmatist and empiricist philosophy. 
As reflected by the article "Initial Probabilities: A Prerequisite for any Valid 
Induction" the shift from prior (or initial) to posterior (or final) probabilities, is 
considered by de Finetti the cornerstone of statistical inference.3 In this connection 
he takes a "radical approach" by which "all the assumptions of an inference ought 
to be interpreted as an overall assignment of initial probabilities" [de Finetti, 1969, 
p. 9]. Though this shift is given a subjective interpretation, in the sense that going 
from prior to posterior assessments involves a shift from one subjective probability 
to another, it also involves consideration of objective factors. 
30n the problem of the choice of initial probabilities de Finetti wrote a joint paper with Savage: see 
[de Finetti and Savage. 1962]. 

166 
MARIA CARLA GALAVOTTI 
Before we face this issue, it is worth noting that for de Finetti updating one's 
mind in view of new evidence does not mean changing opinion: "If we reason ac-
cording to Bayes' theorem we do not change opinion. We keep the same opinion 
and we update it to the new situation. If yesterday I said 'Today is Wednesday', 
today I say 'It is Thursday'. Yet I have not changed my mind, for the day following 
Wednesday is indeed Thursday" [de Finetti, 1995, p. 100]. If the idea of correct-
ing previous opinions is completely alien to this perspective, so is the notion of 
self-correcting procedure, which occupies a central place within the perspective 
of other authors, such as Hans Reichenbach.4 De Finetti's attitude is grounded in 
the conviction that there are no "correct" and "rational" probability assignments: 
"The subjective theory ... -
he says -
does not contend that the opinions about 
probability are uniquely determined and justifiable. Probability does not corre-
spond to a self-proclaimed 'rational' belief, but to the effective personal belief of 
anyone" [de Finetti, 1951, p. 218]. Incidentally, we might notice that his attitude 
in this connection marks a sharp difference from the logicism of Rudolf Carnap 
and Harold Jeffreys,5 who believe that there are "correct" probability evaluations. 
In this sense, logicism attributes to probability theory a normative aspect which is 
absent from subjectivism. 
2 
OBJECTIVISM AND OBJECTIVITY 
De Finetti's subjective Bayesianism is intransigent, even dogmatic. Not only is 
subjective Bayesianism the sole acceptable way of addressing probabilistic infer-
ence and the whole of statistical methodology, but it makes any form of "objec-
tivism" look silly. In de Finetti's words: 
"The whole of subjective statistics is based on this simple theorem 
of probability calculus [Bayes' theorem]. Consequently, subjective 
statistics has a very simple and general foundation. Moreover, being 
grounded only on the basic axioms of probability, subjective statistics 
does not depend on those definitions of probability that would narrow 
its range of application (like, for instance, the definitions based on the 
idea of equally probable events). Nor -
once one endorses this view 
-
is there any need to resort to empirical formulae, in order to char-
acterize inductive reasoning. Objectivist statisticians, on the contrary, 
make extensive use of empirical formulae. The need to do so stems 
only from their refusal to admit the use of initial probability P(E). 
They reject the use of initial probability because they reject the idea 
of a probability that depends on the state of information. However, by 
doing so they distort everything: not only do they make probability 
an objective entity ... they even make it a theological entity: they claim 
4See [Reichenbach, 1949J. 
5See [Carnap, 19501 and [Jeffreys, 1931; Jeffreys, 19391. 

BRUNO DE FINETTI'S BAYESIANISM 
that 'true' probability exists, outside us, independently of a person's 
judgment" [de Finetti, 1995, p. 99]. 
167 
This passage highlights a main feature of de Finetti's position, namely his refusal 
of objective probability, which is deemed not only useless, but even meaningless, 
like all metaphysical notions. Throughout his life, de Finetti held that "probabil-
ity does not exist". This claim, which appears in capital letters in the Preface to 
the English edition of his Theory of Probability, is the leit-motiv of his produc-
tion. "Objective probability never exists" he says in "II significato soggettivo della 
probabilita" [de Finetti, 1931a], and almost fifty years later he opens the article 
"Probabilita" in the "Einaudi Encyclopedia" with the words: "is it true that prob-
ability 'exists'? What could it be? I would say no, it does not exist" [de Finetti, 
1980, p. 1146]. Such aversion to "objective" probability is inspired by the desire 
to keep probability free from metaphysical "contaminations".6 
De Finetti's refusal to attach an "objective" meaning to probability ends with a 
denial of the notions of "chance" and "physical probability". No doubt, the lack of 
consideration for the notions of "chance" and "physical probability" represents a 
limitation of de Finetti's perspective.7 Spurred by his anti-realism, de Finetti never 
paid much attention to the use made of probability in science, in the conviction 
that science is just a continuation of everyday life and subjective probability is all 
that is needed. Only the volume Filosofia della probabilita, containing the text 
of a course given by de Finetti in 1979, includes a few remarks to the effect that 
probability distributions belonging to statistical mechanics can be taken as more 
solid grounds for subjective opinions [de Finetti, 1995, p. 1171. These remarks 
suggest that late in his life de Finetti might have entertained the idea that when 
probability assignments are strictly related to scientific theories, they acquire a 
special meaning. 
The road to a more flexible form of subjectivism, which can accommodate these 
concepts, has been paved by the other "father" of modern subjectivism, Frank 
Ramsey. He defines "chance" and "probability in physics" in terms of "systems of 
beliefs" making reference to theories accepted by the scientific community. Ram-
sey thought that the probabilities we encounter in physics are derived from phys-
ical theories. Their objective character descends from the objectivity ascribed to 
theories that are commonly accepted as true. Within Ramsey's perspective, this 
idea is combined with a pragmatic approach to theories and truth that would have 
been quite congenial to de Finetti, had he been acquainted with it.8 In fact, his 
remarks contained in Filosofia della probabilita lean in the same direction. But de 
Finetti did not grasp the insights of Ramsey's philosophy, though he knew about 
his subjective definition of probability, to which the French probabilist Maurice 
6See [Galavotti, 1989l for an exposition of the anti-metaphysical and anti-realist basis of de Finetti's 
sUbjectivism. 
7This is argued in [Galavotti, 1995-96l and [Galavotti, 1997]. 
8 For a comparison between the philosophy of probability of Ramsey and de Finetti see [Galavotti, 
1991l. For Ramsey's notion of chance see [Galavotti, 1995; Galavotti, 1999l. 

168 
MARIA CARLA GALAVOTTI 
Frechet called his attention around 1937. In the Cambridge of the Twenties an-
other Bayesian often praised by de Finetti, Harold Jeffreys, put forward the idea 
that one can make sense of physical probability in an epistemic framework, hold-
ing a position akin to that of Ramsey.9 To be sure, Jeffreys was a logicist more 
than a subjectivist. More recently, however, the idea that subjectivism should be 
flexible enough to accommodate for a notion of physical probability has been her-
alded by statisticians as well as philosophers, as testified, for instance, by the work 
of 1.1. Good and R.C. Jeffrey. \0 
Having refused the notion of "objective" probability and denied that there are 
"correct" probability assignments, the radical subjectivist de Finetti still faces the 
problem of objectivity of probability evaluations. Let us examine his position on 
this issue. His point of departure is the conviction that the process through which 
probability judgments are obtained is more complex than is supposed by the other 
interpretations of probability, which define probability on the basis of a unique cri-
terion. While subjectivists distinguish between the definition and the evaluation of 
probability, and do not mix them up, upholders of the other interpretations confuse 
them: they look for a unique criterion - be it frequency, or symmetry - and use it 
as grounds for both the definition and the evaluation of probability. In so doing, 
they embrace a "rigid" attitude towards probability, an attitude which consists "in 
defining (in whatever way, according to whatever conception) the probability of an 
event, and in univocally determining a function" [de Finetti, 1933, p. 740]. On the 
contrary, subjectivists adopt an "elastic" approach, which "consists in demonstrat-
ing that all functions f have all the necessary and sufficient properties to represent 
probability evaluations (also in this case, defined according to whatever concep-
tion, in whatever way) which are not intrinsically contradictory, leaving to a second 
(extra-mathematical) stage the discussion and analysis of reasons and criteria for 
the choice of a particular among all possible ones" (ibid., p. 741). In other words, 
for the subjectivist all coherent functions are admissible; far from being committed 
to a single rule or method, the choice of one particular function is seen as the result 
of a complex and largely context-dependent procedure, which necessarily involves 
subjective elements. 
The explicit recognition of the role played by subjective elements within the 
complex process of the formation of probability judgments is for de Finetti a pre-
requisite for the appraisal of objective elements: "Subjective elements -
he says 
-
will noways (sic) destroy the objective elements nor put them aside, but bring 
forth the implications that originate only after the conjunction of both objective 
and subjective elements at our disposal" [de Finetti, 1973, p. 366]. To be sure, 
Bayesian subjectivism requires that objective elements also be taken into account, 
but such objective elements are not seen as the only basis for judgment. "Sub-
jectivism -
de Finetti says -
is one's degree of belief in an outcome, based on 
an evaluation making the best use of all the information available to him and his 
9 See [Jeffreys, 19551. 
JOSee [Good, 19651 and [Jeffrey, 19971. 

BRUNO DE FINETTI'S BAYESIANISM 
169 
own skill...Subjectivists ... believe that every evaluation of probability is based on 
available information, including objective data" [de Finetti, 1974b, p. 16]. In con-
clusion, "Every probability evaluation essentially depends on two components: (1) 
the objective component, consisting of the evidence of known data and facts; and 
(2) the subjective component, consisting of the opinion concerning unknown facts 
based on known evidence" [de Finetti, 1974a, p. 7]. 
De Finetti warns that the objective component of probability judgments, namely 
factual evidence, is in many ways context-dependent: evidence must be collected 
carefully and skillfully, its exploitation depends on the judgment on what elements 
are relevant to the problem under consideration, and can be useful to the evaluation 
of related probabilities. In addition, the collection and exploitation of evidence de-
pends on economic considerations varying in practical cases. So, one can say that 
the collection and exploitation of factual evidence involves subjective elements of 
various sorts. Equally subjective is the decision on how to let objective elements 
influence belief. Typically, one relies on information regarding frequencies. For 
de Finetti frequencies, like symmetry considerations, are useful and important in-
gredients of probability evaluations, provided that they are not used uncritically as 
automatic rules and simply equated with probability. Those who do so, namely 
frequentists, are simply committed to "superstition": 
"There is no worse conceptual distortion than that owing to which, 
starting from the premise that any sequence can occur, one defines 
probability in terms of a property (that of exhibiting a certain fre-
quency) pertaining only to a portion of all sequences... when we 
define probability in terms of frequency, we define it thoughtlessly. 
The only objective thing is the set of all possible sequences, but it 
does not say anything concerning their probability. The probability 
of sequences can only be the feeling we had before and which char-
acterized our expectation. Here we have a perversion of language, 
logic and common sense. Such a logical mistake is unacceptable, be-
cause the set of all possible sequences (which is logically determined) 
cannot be confused with probability (which, on the contrary, is sub-
jective)" [de Finetti, 1995, pp. 140-141]. 
Keeping in mind the distinction between the definition and the evaluation of 
probability, one can make good use of frequencies within probability evaluations. 
It is precisely in this connection that exchangeability enters the stage, giving the 
reason "why expected future frequencies should be guessed according to past ob-
served frequencies", and thereby creating a strong connection between "subjec-
tivistic and objectivistic interpretations" [de Finetti, 1970b, p. 143]. As already 
stressed, de Finetti assigns to exchangeability a subjective interpretation, accord-
ing to which exchangeability represents a 
"directly observable property of the probability evaluation. It means 
that for every set of n of the events concerned, the probability that 

170 
MARIA CARLA GALAVOTTI 
all events occur is the same; it depends only on n... Under such a 
clear subjective condition (and a few side restrictions to avoid special 
cases, such as that of repeated trials with a known probability that 
remains unchanged), one is perfectly free to improve the evaluation 
of probabilities for any future events according to the frequency of 
the observed ones. This improvement generally entails modifying the 
initial evaluation ... so as to approach gradually the obtained frequency 
of the events observed up to that time" [de Finetti, 1974a, p. 12]. 
Therefore exchangeability allows probability judgments to be improved in view 
of observed frequencies in an empiricist fashion fully in tune with the subjectivist 
approach. 
Since the present contribution is meant to be historically oriented, it is not out of 
place to make a brief digression on the origin of exchangeability. It is de Finetti's 
merit to have combined the subjective notion of probability in terms of coherent 
beliefs with that of exchangeability. In so doing, he was able to guarantee the 
applicability of subjective probability to practical situations, including those en-
countered within experimental science. Exchangeability is the missing aspect in 
Ramsey's perspective, which could have made him see the link between degrees 
of belief and observed frequencies. It can be conjectured that by the time of his 
death Ramsey came very close to recognizing such a link. Evidence that he was in-
trigued by the relationship between frequency and degree of belief is offered by his 
note "Miscellaneous Notes on Probability", II where he ponders over the idea that 
"degree"( of belief means acting appropriately to a frequency "(", of which he says 
that "it is [the] ... one which makes calculus of frequencies applicable to degrees of 
belief" [Ramsey, 1991, p. 275]. The justification of this claim lies precisely with 
exchangeability, a property that Ramsey knew through his teacher and colleague 
in Cambridge William Ernest Johnson, a logicist who is seen as a forerunner of 
Carnap's inductive logic, which assigns a privileged role to the same probabilis-
tic property he calls "symmetry".12 Ramsey himself made use of this property, 
named by him "equiprobability of all permutations", in a short note called "Rule 
of Succession" in [Ramsey, 1991], which contains a derivation of Laplace's Rule 
of Succession from the property in question. However, Ramsey was unable to 
connect it with degree of belief in the way de Finetti did. 
Going back to the evaluation of probabilities, when a considerable amount of 
information about frequencies is available, it influences probability assignments 
through the assumption of exchangeability. Often, however, this kind of infor-
mation is scant, and in this case the problem of how to obtain good probability 
evaluations is open. The problem is addressed by de Finetti in a number of works, 
especially starting from the Sixties. The approach adopted is based on penalty 
methods, like the so called "Brier's rule", named after the meteorologist Brier who 
applied it to weather forecasts. De Finetti did extensive work on scoring rules, 
liThe note was written in 1928 and appears in [Ramsey, 1991]. 
120n this point see [Zabell, 1988]. 

BRUNO DE FINETTY'S BAYESIANISM 
171 
partly in cooperation with Savage. He even tested the goodness of this method 
through an experiment among his students, who were asked to make forecasts on 
the results of soccer matches in the Italian championship. A simple description of 
such methods, referred to the case of three possible results (as with soccer matches) 
is the following: 
"everybody participating in ... [an experiment on probabilistic forecasts] 
is asked to indicate the probabilities of (for instance) the three possi-
ble results -
victory, or draw, or defeat -
of the home-team in a 
specified football match; say, e.g. 50% - 30% - 20%. A scoring 
rule indicates how much the participant is to be penalised depending 
on the 'distance' between the assessed probabilities and the actual re-
sult. The simplest and most practical scoring rule is Brier's rule; if 
someone indicates as his own opinion P(E) = p, the score (i.e. the 
penalisation) is the square of the distance between forecast and result: 
p2( = (p-O)2)ifthe result is 0 (E does not happen), and (1_p)2 if the 
result is 1 (E does happen). The fact that Brier's rule is a proper one 
is proved since for a person indicating as his probability assessment 
a p different from his own effective opinion p, expected penalisation 
is increased by (p - p)2. Analogously, when the possible results are 
three (as in football) indicating them with the vertices of an equilateral 
triangle, and the forecast with the centre of gravity of weights indicat-
ing the probabilities of each vertex, a proper scoring rule is (P - PO)2 
(square of the distance between probability assessment and effectual 
result" [de Finetti, 1981, p. 55]. 
Scoring rules of this kind are based on the idea that the device in question should 
oblige those who make probability evaluations to be as accurate as they can and, 
if they have to compete with others, to be honest. In fact any deviation from the 
true p, on the part of the person who evaluates probability, enables the opponent 
to engage him in a disadvantageous bet. Such rules playa twofold role within de 
Finetti's approach. In the first place, they offer a suitable tool for an operational 
definition of probability. As recollected, in his late works de Finetti adopted such 
a device to define subjective probability. In addition, these rules offer a method 
for improving probability evaluations made both by a single person and by several 
people, because they can be employed as methods for exercising "self-control", as 
well as a "comparative control" over probability evaluations [de Finetti, 1980, p. 
11511. 
De Finetti assigns these methods, which are quite widespread among Bayesian 
statisticians, a straightforward interpretation in tune with subjectivism: 
"The objectivists, who reject the notion of personal probability be-
cause of the lack of verifiable consequences of any evaluation of it, 
are faced with the question of admitting the value of such a 'measure 
of success' as an element sufficient to soften their fore-judgments. 

172 
MARIA CARLA GALAVOTTI 
The subjectivists, who maintain that a probability evaluation, being a 
measure of someone's beliefs, is not susceptible of being proved or 
disproved by the facts, are faced with the problem of accepting some 
significance of the same 'measure of success' as a measure of the 
'goodness of the evaluation'" [de Finetti, 1962a, p. 360]. 
The following remark further clarifies de Finetti's position: "though maintain-
ing the subjectivist idea that no fact can prove or disprove belief, I find no difficulty 
in admitting that any form of comparison between probability evaluations (of my-
self, of other people) and actual events may be an element influencing my further 
judgment, of the same status as any other kind of information" (ibid.). De Finetti's 
work on scoring rules is in tune with a widespread attitude among Bayesians, an 
attitude that has given rise to a vast literature on "well-calibrated" estimation meth-
ods. 
If these methods provide us with useful devices for improving probability eval-
uations, a whole series of elements seem to be relevant in this connection. De 
Finetti mentions an array of such elements, including: 
1. "degree of competence or care in forecasts concerning different 
subject matters, epochs or regions; 
2. optimistic or pessimistic attitudes ... 
3. degree of influence of the most recent facts ... 
4. degree of deviation from statistical standards, according to spe-
cific knowledge of each item ... 
5. stability or flexibility (evolutionary or oscillating) of opinions 
without a change in the available information, by thinking about 
or by the influence of another's opinions ... 
6. conscious or unconscious adaptation of the opinion to standard 
patterns of statistical theory and practice .... " ([de Finetti, 1970b, 
pp. 141-142]. 
To sum up, the evaluation of probability is seen as a most complex procedure, 
resulting from the concurrence of all sorts of factors. Starting from the recognition 
of the fact that probability is subjective, and that there is no unique, "rational" way 
of assessing probability, one can make room for a whole array of elements that can 
influence probability evaluations, suggesting various ways of ameliorating them. 
De Finetti's remarks in this connection may sound very general, but his warnings 
against pre-confectioned recipes for the evaluation of probabilities should be taken 
seriously. 
In a paper dealing with issues related to economic theory, de Finetti discusses 
the "dangerous mirage" of "identifying objectivity and objectivism" [de Finetti, 
1962b, p. 344] and exhorts "to fight against the ambush of pseudo-objectivity 
which is concealed under the false shield of 'objectivism', boasting of it as if it 

BRUNO DE FINETTI'S BAYESIANISM 
173 
were a chrism of 'scientificity'" (ibid., p. 360). Since objectivism is nothing but 
a conceptual distortion, an absolute idea of objectivity grounded on it can only 
be a chimera. A more viable notion of objectivity lies with a "deep analysis of 
problems", aimed at avoiding hasty judgments, superficial intuitions and careless 
conclusions, to form evaluations which are the best one can attain in the light of 
the available information. Such a deep analysis of problems will include consider-
ation of objective elements, in the awareness that, taken by themselves, these are 
neither necessary not sufficient to guarantee objectivity in an absolute sense. This 
is because absolute objectivity does not exist: "only a honest reflection, careful 
of facts and other people's ideas can lead to the maximum possible objectivity" 
(ibid., p. 367). 
Department of Philosophy, University of Bologna, Italy. 
BIBLIOGRAPHY 
[Carnap, 1950] R. Carnap. Logical Foundations of Probability, Chicago: Chicago University Press, 
1950. Second edition with modifications 1962. 
[de Finetti, 1931a] B. de Finetti. SuI significato soggettivo della probabilitA", Fundamenta mathemat-
icae 17, pp. 298-329,1931. 
[de Finetti, 1931b] B. de Finetti. Probabilismo. Logos, pp.l63-219, 1931, reprinted in B. de Finetti, 
La logica dell'incerto, Milano, 11 Saggiatore, 1989, pp. 3-70. English translation in Erkenntnis, 31, 
pp. 169-223, 1989. 
[de Finetti, 1933] B. de Finetti. SuI concetto di probabilitA. Rivista italiana di statistica, economia e 
finanza 5, pp. 723-747, 1933. 
[de Finetti, 1937] B. de Finetti. La prevision: ses lois logiques, ses sources subjectives. Annales de 
l'lnstitut Henri Poincare 7, pp.l-68, 1937. English translation in H. E. Kyburg and H. E. SmokIer 
(eds.), Studies in Subjective Probability, New York-London, Wiley, pp. 95-158, 1964. 
[de Finetti, 1951] B. de Finetti. Recent Suggestions for the Reconciliation of Theories of Probability. 
In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, 
Berkeley, University of Cali fomi a Press, pp. 217-225,1951. 
[de Finetti, 1955] B. de Finetti. La probabilitA e il comportamento di fronte all'incertezza, Assicu-
razioni 1-2, pp. 3-15, 1955. 
[de Finetti, 1962a] B. de Finetti. Does it Make Sense to Speak of 'Good Probability Appraisers'? In 
1.1. Good et al. (eds.), The Scientist Speculates. An Anthology of Partly-Baked Ideas, New York, 
Basic Books, pp. 357-364, 1962. 
[de Finetti, 1962b] B. de Finetti. ObiettivitA e oggettivitA: critica a un miraggio. La Rivista Trimes-
trale, 1, pp. 343-367,1962. 
[de Finetti, 1963] B. de Finetti. La decisione nell'incertezza. Scientia 98, pp. 61-68,1963. 
[de Finetti, 1968] B. de Finetti. Probability: the Subjectivistic Approach. In R. Klibansky (ed.), La 
philosophie, contemporaine, Firenze, La Nuova Italia, pp. 45-53, 1968. 
[de Finetti, 1969] B. de Finetti. Initial Probabilities: a Prerequisite for any Valid Induction. Synthese 
20,pp.2-16,1969. 
[de Finetti, 1970a] B. de Finetti. Teoria delle probabilitii. Einaudi, Torino, 1970. English edition: The-
ory of Probability, New York, Wiley, 1975. 
[de Finetti, 1970b] B. de Finetti. Logical Foundations and Measurement of Subjective Probability. 
Acta Psychologica, 34, pp. 129-145, 1970. 
[de Finetti, 1973] B. de Finetti. Bayesianism: Its Unifying Role for Both the Foundations and the 
Applications of Statistics. Bulletin of the International Statistical Institute, Proceedings of the 39th 
Session, pp. 349-368, 1973. 

174 
MARIA CARLA GALAVOTTI 
[de Finetti, 1974a] B. de Finetti. The Value of Studying Subjective Evaluations of Probability. In C.-
A.S. Sta!!l von Holstein (ed.), The Concept of Probability in Psychological Experiments, Dordrecht-
Boston, Reidel, pp. 1-14, 1974. 
[de Finetti, 1974b] B. de Finetti. The True Subjective Probability Problem. In e.-A.S. Sta!!1 von Hol-
stein (ed.), The Concept of Probability in Psychological Experiments, Dordrecht-Boston, Reidel, 
pp.l5-23,1974. 
[de Finetti, 1980] B. de Finetti. ProbabilitA. In Enciclopedia Einaudi, Torino, Einaudi, X, pp. 1146-
1187, 1980. 
[de Finetti, 198Il B. de Finetti. The Role of 'Dutch Books' and of 'Proper Scoring Rules'. British 
Journal for the Philosophy of Science 32, pp. 55-56, 1981. 
[de Finetti, 1995] B. de Finetti. Filosofia della probabilita. Milano, II Saggiatore, 1995. 
[de Finetti and Savage, 1962] B. de Finetti and L.J. Savage. Sul modo di scegliere Ie probabilitA in-
iziali. Biblioteca del Metron, serie C: Note e commenti, Roma, Istituto di Statistica dell'UniversitA, 
pp. 82-154, 1962. 
[Galavotti, 1989] M.C. Galavotti. Anti-realism in the Philosophy of Probability: Bruno de Finetti's 
Subjectivism. Erkenntnis, 31, pp. 239-261, 1989. 
[Galavotti, 199Il M.e. Galavotti. The Notion of Subjective Probability in the Work of Ramsey and de 
Finetti. Theoria, 57, pp. 239-259, 1991. 
[Galavotti, 1995] M.e. Galavotti. F.P. Ramsey and the Notion of 'Chance'. In 1. Hintikka and K. Puhl 
(eds.), The British Tradition in 20th Century Philosophy. Proceedings of the 17th International 
Wittgenstein Symposium, Wien, HOlder-Pichler-Tempsky, pp. 330-340, 1995. 
[Galavotti, 1995-96] M.C. Galavotti. Operationism, Probability and Quantum Mechanics. Founda-
tions of Science, 1, pp. 99-118, 1995-96. 
[Galavotti, 1997] M.e. Galavotti. Probabilism and Beyond. Erkenntnis, 45, pp. 253-265, 1997. 
[Galavotti, 1999] M.C. Galavotti. Some Remarks on Objective Chance (F.P. Ramsey, K.R. Popper and 
N.R. Campbell). In M.L. Dalla Chiara et al. (eds.), Language, Quantum, Music, Dordrecht-Boston, 
Kluwer, pp. 73-82, 1999. 
[Good, 1965] U. Good. The Estimation of Probabilities. Cambridge, Mass., The M.I.T. Press, 1965. 
(Jeffrey, 1997] R.C. Jeffrey. Unknown Probabilities. In D. Costantini and M.C. Galavotti (eds.), Prob-
ability, Dynamics and Causality, Dordrecht-Boston, Kluwer, pp. 327-335, 1997. 
(Jeffreys,1931] H. Jeffreys. Scientific Inference. Cambridge, Cambridge University Press, 1931. 
Third edition with modifications 1973. 
(Jeffreys, 1939] H. Jeffreys. Theory of Probability. Oxford, Clarendon Press, 1939. 2nd edition 1948. 
Third edition with modifications 1961. 
(Jeffreys, 1955] H. Jeffreys. The Present Position in Probability Theory. British Journal for the Phi-
losophy of Science, 5, pp. 275-289, 1955. Also in Jeffreys, H. and Swirles, B. (eds.), Collected 
Papers of Sir Harold Jeffreys on Geophysics and Other Sciences, London-Paris-New York, Gordon 
and Breach Science Publishers, VI, pp. 421-435, 1971-1977. 
[Ramsey,1926] F.P. Ramsey. Truth and Probability. In Ramsey 1931, pp. 156-198, 1926. 
[Ramsey, 1931] F.P. Ramsey. The Foundations of Mathematics and Other Logical Essays. R.B. Braith-
waite, ed. London, Routledge and Kegan Paul, 1931. 
[Ramsey, 1991] F.P. Ramsey. Notes on Philosophy, Probability and Mathematics. Ed. by M.C. 
Galavotti, Naples, Bibliopolis, 1991. 
[Reichenbach, 1949] H. Reichenbach. The Theory of Probability. Berkeley-Los Angeles, Univer-
sity of California Press, 1949. Second edition 1971. English translation of with modifications of 
Wahrscheinlichkeitslehre, Leyden, Sijthoff, 1935. 
[Savage,1954] L.J. Savage. The Foundations of Statistics. New York, John Wiley and Sons, 1954. 
[Zabell, 1988] S.L. Zabell. Symmetry and its Discontents. In B. Skyrrns and W.L. harper (eds.), Cau-
sation, Chance, and Credence, Dordrecht-Boston, Kluwer, I, pp. 155-190, 1988. 

DAVID CORFIELD 
BAYESIANISM IN MATHEMATICS 
INTRODUCTION 
I shall begin by giving an overview of the research programme named in the title 
of this paper. The term 'research programme' suggests perhaps a concerted effort 
by a group of researchers, so I should admit straight away that since I have started 
looking investigating the idea that plausible mathematical reasoning is illuminated 
by Bayesian ideas, I have not encountered in the literature anyone else who has 
thought to develop the views of the programme's founder, the Hungarian mathe-
matician, George P61ya. I should further admit that P61ya never termed himself a 
Bayesian as such. Motivation for the programme may, therefore, be felt sorely nec-
essary. Let us begin, then, with three reasons as to why one might want to explore 
the possibility of a Bayesian reconstruction of plausible mathematical reasoning: 
(a) To acquire insight into a discipline one needs to understand how its practition-
ers reason plausibly. Understanding how mathematicians choose which problems 
to work on, how they formulate conjectures and the strategies they adopt to tackle 
them requires considerations of plausibility. Since Bayesianism is widely consid-
ered to offer a model of plausible reasoning, it provides a natural starting point. 
Furthermore, P61ya has already done much of the spadework with his informal, 
qualitative type of Bayesianism. 
(b) The computer has only recently begun to make a serious impact on the way 
some branches of mathematics are conducted. A precise modelling of plausibil-
ity considerations might be expected to help in automated theorem proving and 
automated conjecture formation, by providing heuristics to guide the search and 
so prevent combinatorial explosion. Elsewhere, computers are used to provide 
enormous quantities of data. This raises the question of what sort of confirmation 
is provided by a vast number of verifications of a universal statement in an infi-
nite domain. It also suggests that statistical treatments of data will become more 
important, and since the Bayesian approach to statistics is becoming increasingly 
popular, we might expect a Bayesian treatment of mathematical data, especially 
in view of its construal of probability in terms of states of knowledge, rather than 
random variables. 
(c) The plausibility of scientific theories often depends on the plausibility of math-
ematical results. This has always been the case, but now we live in an era where 
for some physical theories the only testable predictions are mathematical ones. If 
we are to understand how physicists decide on the plausibility of their theories, 
this must involve paying due consideration to the effect of verifying mathematical 
predictions. 
175 
D. Corfield and 1. Williamson (eds.), Foundations of Bayesianism, 175-201. 
Â© 2001 Kluwer Academic Publishers. 

176 
DAVID CORFIELD 
Now, if one decides to treat plausible and inductive reasoning in the sciences 
in Bayesian terms, it seems clear that one would want to do the same for math-
ematics. After all, it would appear a little extravagant to devise a second calcu-
lus. In any case, Bayesianism is usually presented by its proponents as capable of 
treating all forms of uncertain reasoning. This leads us to conclude that Bayesian-
ism in science requires Bayesianism in mathematics. Once this is accepted, one 
must respond in two ways according to the discoveries one makes while examining 
Bayesianism in mathematics: 
I Bayesianism cannot be made to work for mathematics, therefore Bayesian-
ism cannot give a complete picture of scientific inference. 
II Some forms of Bayesianism can be made to work for mathematics, there-
fore one of these must be adopted by Bayesian philosophers to give a more 
complete picture of scientific inference. 
The arguments presented in this paper indicate that the antecedent of I is false 
and the antecedent of II true, opening the prospect of an expanded, but modified, 
Bayesianism. 
In this paper there is only space to treat a part of the motivation given above. 
The first two sections question which varieties of the many forms of Bayesianism 
are able to accommodate mathematical reasoning. Many Bayesians hold it as a 
tenet that logically equivalent sentences should be believed with equal confidence 
and any evidence should have an equal impact on their degrees of belief. However, 
such an assumption plays havoc with any attempt to throw light on mathematical 
reasoning. In section 1 I argue that if a Bayesian modelling of plausible reasoning 
in mathematics is to work, then the assumption of logical omniscience must be 
dropped. 
In P6lya's version, we have only the right to specify the direction of change in 
the credence we give to a statement on acquiring new information, not the mag-
nitude. However, Edwin Jaynes demonstrated that one of the central grounds for 
this decision on the part of P6lya to avoid quantitative considerations was wrong. 
In section 2 I consider whether there is anything amiss with a quantitative form of 
Bayesianism in mathematics. 
One criticism often made of Bayesian philosophy of science is that it does not 
help very much in anything beyond toy problems. While it can resolve simple 
issues, such as accounting for how observing a white tennis shoe provides no con-
firmation for the law 'all ravens are black', it provides no insight into real cases of 
theory appraisal and confirmation. Everything rests on the assignment of priors, 
but how an expert could be considered to go about this is enormously complicated. 
Recognising what is correct in this criticism, I think there is still useful work to be 
done. In section 3 I shall be looking in particular at: reasoning by analogy; choice 
of proof strategy (for automated theorem proving); and, large scale induction (par-
ticularly enumerative induction). 

BAYESIANISM IN MATHEMATICS 
177 
1 PROBABILITY THEORY AS LOGIC 
In his Mathematics and Plausible Reasoning (P6lya [1954a; 1954b]), P6lya con-
siders mathematics to be the perfect domain in which to devise a theory of plau-
sible reasoning. After all, where else could you find such unequivocal instances 
of facts satisfying general laws? As a noted mathematician actively engaged in 
research, he delightfully conveys inferential patterns by means of examples of his 
own use of plausible reasoning to generate likely conjectures and workable strate-
gies for their proof. Now, such plausible reasoning in mathematics is, of course, 
necessary only because mathematics does not emerge as it appears on the pages 
of a journal article or textbook, that is, in its semi-rigorous deductive plumage. 
Indeed, it is due to the failure of what we might call "logical omniscience", the ca-
pacity to know immediately the logical consequences of a set of hypotheses, that 
mathematicians are forced to resort to what might be called a guided process of 
trial and error, not so dissimilar to that employed in the natural sciences. 
In the second of the two volumes mentioned above, P6lya works his account of 
plausible reasoning into a probabilistic mould. While he did not name himself as 
such, we can thus reasonably view P6lya as a member of the Bayesian camp and, 
indeed, as a pioneer who influenced some later prominent Bayesians. Certainly, 
Edwin Jaynes learned from his work, and it is clear that Judea Pearl has read him 
closely. So here we have something of a paradox: plausible mathematical reason-
ing, the subject of P6lya's analysis, was an important source of ideas for some of 
the leading figures of Bayesianism, and yet it is necessitated by the fact that people 
involved in this most rigorous branch of knowledge are not able to uphold one of 
the widely held tenets of Bayesianism, namely, that logically equivalent statements 
should receive identical degrees of belief, or alternatively, that tautologies should 
be believed with degree of belief set at 1. 
Logical omniscience comes as part of a package which views Bayesianism as 
an extension of deductive logic, as for example in Howson (this volume). In an-
other of its manifestations, we hear from Jaynes the motto 'probability theory as 
logic'. For him: "Aristotelian deductive logic is the limiting form of our rules for 
plausible reasoning, as the robot becomes more and more certain of its conclu-
sions" [Jaynes, forthcoming, Ch 2, p. 11].1 Here we are to imagine a robot who 
reasons perfectly in Bayesian terms, handicapped only by the imperfections of its 
data and the incompleteness of the set of hypotheses it is considering. 
We have then a tension when it comes to mathematical reasoning: if Bayesian-
ism is to be seen as an extension of deductive logic, in the sense that the premises 
are now not required to be known with certainty, then one should consider the two 
inferential calculi to be similar in as many respects as possible. Since deductive 
logic is held as a regulating ideal, as, for example, when we say: 
( 1) If A is true and A entails B, then B is true, 
1 References to Jaynes are from his unfinished book -
Probability Theory: The Logic of Science -
available at http://bayes.wustl.edu.This is soon to appear in print. 

178 
DAVID CORFIELD 
should we not have 
(2) If Pr(A) = p and A entails B, then Pr(B) ~ p? 
However, making this assumption raises a few problems. For one thing it implies 
that any consequence of a given axiomatised mathematical theory should be be-
lieved at least as strongly as that theory. Then, assuming Wiles is correct, to mimic 
the ideal rational agent you must set Pr(Fermat's Last Theorem) no lower than 
Pr(ZFC set theory), indeed no lower than your degree of belief in whichever sys-
tem you feel confident can cope with arithmetic. There is of course the question as 
to how one might want to interpret Pr(ZFC set theory), but for statements whose 
logical complexity is the same as that of Fermat's Last Theorem, all one needs is 
the consistency of ZFC for truth to entail proof. And if you were to pitch this at 
0.5, say, then this would provide a minimum for all provable truths of arithmetic, 
along with those of just about any other branch of mathematics, of this logical 
complexity. 
If a mathematician suddenly became endowed with such omniscience, it would 
not be the end of mathematics, there is far more to mathematics than truth and 
provability, but one may safely predict that she would be much in demand. The 
logicistic conceptions of mathematics are accurate enough that the discipline would 
become unrecognisable. Without the sixty years leading up to Wiles' work, we 
would have known that (ZFC is consistent & Fermat's Last Theorem) is logically 
equivalent to (ZFC is consistent). And when finding a proof of a result we knew 
by omniscience to be correct, we could check up on the validity of lemmas rather 
than risk wasting time on false ones. How different a picture we gain from P6lya's 
representation of mathematics as a fallibly practised discipline and as the perfect 
place to investigate inductive and plausible reasoning. 
So logical omniscience is an assumption that we cannot hold on to if we wish 
to investigate plausible reasoning in mathematics, which if P6lya was correct is 
perhaps what the Bayesian should be doing. But what prevents us from drop-
ping this assumption? Two of the most common justifications for Bayesianism are 
Cox's theorem and the Dutch Book argument. Cox's theorem merely assumes that 
logical equivalence implies equality of probabilities. On the other hand, Dutch 
Book style arguments or those based on the preference for some linearly valued 
commodity attempt to justify it by claiming that if an agent offers different bet-
ting quotients on what are in fact logically equivalent sentences, then stakes can 
be set so that they will necessarily lose. But then isn't it surprising that there are 
many instances in the past where mathematicians have bet? Indeed, in view of the 
definitive way mathematical statements, even universal ones, may be settled, they 
would seem to make at least as good propositions to wager on as statements from 
the natural sciences. 
Surely it is reasonable to prefer a bet on the trillionth decimal digit of 7r being 
between 0 and 8, than one at the same odds on its being 9. If, however, 9 is 
the correct digit, then it follows as a "mere" calculation from one of the series 
expansions for 7r. That is, "7r = 4(1 - 1/3 + 1/5 - 1/7 + ... )" and "7r = 4(1 -

BAYESIANISM IN MATHEMATICS 
179 
1/3 + 1/5 -1/7 + ... ) & the trillionth decimal place of 7r is 9" would be logically 
equivalent and so to be believed with the same confidence, and so the second 
bet should be preferred. But, mathematicians spend their working lives making 
decisions on the basis of the level of their confidence in the truth of mathematical 
propositions. We would not want to brand them as irrational for devoting time to 
an attempted proof of their hunch that a certain statement follows from a set of 
assumptions merely because the hunch turns out to be wrong. 
There is a suggestion in the writings of several Bayesians that (2) only holds 
when we come to know about the logical relationship between two propositions. 
Given two propositions A, B it may happen that one is true if and 
only if the other is true; we then say that they have the same truth 
value. This may be only a simple tautology (i.e., A and B are verbal 
statements which obviously say the same thing), or it may be that only 
after immense mathematical labors is it proved that A is the necessary 
and sufficient condition for B. From the standpoint of logic it does not 
matter; once it is established, by any means, that A and B have the 
same truth value, then they are logically equivalent propositions, in the 
sense that any evidence concerning the truth of one pertains equally 
well to the truth of the other, and they have the same implications for 
any further reasoning. 
Evidently, then, it must be the most primitive axiom of plausible rea-
soning that two propositions with the same truth-value are equally 
plausible. [Jaynes, forthcoming, Ch. 1, p. 6] (second emphasis mine) 
In this less rigid framework we might say that if A is known by the agent to entail 
B, then she should ensure that she has Pr(B) 2: Pr(A). In other words, we are 
generalising from an interpretation of deductive logic no stronger than: 
(3) 
'If I judge A to be true and I judge A to entail B, then I should judge B 
to be true.' 
Opposed to the 'probability as logic' position are the subjectivists, whose number 
include followers of de Finetti. Here the accent is on uncertainty: 
The only relevant thing is uncertainty -
the extent of our knowledge 
and ignorance. The actual fact of whether or not the events considered 
are in some sense determined, or known by other people, and so on, 
is of no consequence. 
[de Finetti, 1974, p. xi] 
Since probability is seen as a measure of an individual's uncertainty, it is no won-
der that de Finetti permits non-extreme degrees of belief about mathematical facts, 
even those which are decidable. Indeed, this probabilistic treatment seems to ex-
tend to even very accessible truths: 
Even in the field of tautology (i.e. of what is true or false by mere 
definition, independently of any contingent circumstances) we always 

180 
DAVID CORFIELD 
find ourselves in a state of uncertainty. In fact, even a single verifi-
cation of a tautological truth (for instance, of what is the seventh, or 
billionth, decimal place of 7r, or of what are the necessary or sufficient 
conditions for a given assertion) can turn out to be, at a given moment, 
to a greater or lesser extent accessible or affected with error, or to be 
just a doubtful memory. 
[de Finetti, 1974, p. 24] 
Presumably then for de Finetti one may be rational and yet have a degree of 
belief in '91 is prime' less than 1. Perhaps you are unsure so you set it to 0.6. If 
so, when I ask you for Pr(7 x 13 = 91) you had better give me an answer no 
greater than 0.4. But then can't I force you to realise that you have an inconsistent 
betting quotient by making you see that 7 x 13 really is the same as 91, or is it 
just a case where I should allow you to alter your betting quotient after this lesson? 
More radically still, should one be expected to know that the correctness of this 
product contradicts the claim that 91 is prime? 
In his article, Slightly More Realistic Personal Probability, Ian Hacking [1967] 
sets out a hierarchy of strengths of Bayesian ism. These strengths he correlates with 
ways of saying whether a statement can be possibly true. At the weaker end we find 
a position he terms 'realistic personalism', where non-zero probabilities will be 
attributed by a subject to any statement not known by them to be false, knowledge 
being taken in a very strict sense: "a man can know how to use modus ponens, can 
know the rule is valid, can know p, and can know p :J q, and yet not know q, simply 
because he has not thought of putting them together" [Hacking, 1967, p. 319]. At 
the stronger end we find logical omniscience and divine knowledge. Now clearly 
the coherence provided by realistic personalism is not enough to equip you for a 
life as a gambler. For instance, it is advisable not to advertise the odds at which 
you would accept either side of a wager on a decidable mathematical proposition 
on a mathematics electronic bulletin board. But Dutch Book arguments do not 
work to prove your irrationality on the grounds that someone may know more than 
you. If they do know more than you, you will lose whether the subject of your bet 
is mathematics, physics or the date of the next general election. 
However, there is a point here: surely you can be criticised for betting on a 
proposition whose truth value you know you could discover with a modicum of 
effort, perhaps by the tap of a single button on the computer in front of you. As 
Hacking points out [Hacking, 1967, pp. 323-4], besides coherence one needs a 
principle which calls on you to maximize expected subjective utility. Information 
acquired for free can only help increase this, and so inexpensive reasoning or in-
formation gathering is generally a good thing. But this principle is not needed 
solely by a personalism weaker than that based on logical omniscience. Where 
the presupposition of logical omniscience forces you to reason, and indeed reason 
unreasonably much, it does not require you even to look down to note the colour of 
your socks before you bet on it. Only some principle of expected utility does this. 
But then surely you should allow this principle to be the rationale for your logical 
reasoning as well, rather than relying on the very unreasonable idealisation of log-

BAYESIANISM IN MATHEMATICS 
181 
ical omniscience which offers little more by way of advice than to be as perfect a 
mathematician as you can be. 
Even admitting that we should not assume logical omniscience when we con-
sider mathematics, it might be thought that this assumption is not too unrealistic 
for other walks of life. After all, doesn't the uncertainty which necessitates plau-
sible reasoning in ordinary life and the natural sciences arise for other reasons 
- uncertainty in data, inaccessibility of object of study, incompleteness of back-
ground knowledge? You might think that it would count as the least of your wor-
ries that your logical and mathematical powers are not quite perfect. Hence, an 
assumption in standard Bayesian treatments of scientific inference that logically 
equivalent sentences should be accorded the same degree of belief. However, in 
many situations in science the uncertainty of mathematical knowledge plays an 
important part, as I have explained in a companion paper, not least in the area of 
mathematical predictions, a phenomenon as yet largely ignored by philosophers, 
where physicists gain confidence that they are on the right track when purely math-
ematical conjectures arising from their work turn out to be correct. Plausibility of 
scientific statements depends on uncertain mathematical knowledge. 
To give briefly an indication of this, we hear of the mathematical physicist, 
Edward Witten, that he 
... derived a formula for Donaldson invariants on Kahler manifolds 
using a twisted version of supersymmetric Yang-Mills theory in four 
dimensions. His argument depends on the existence of a mass gap, 
cluster decomposition, spontaneous symmetry breaking, asymptotic 
freedom, and gluiI).o condensation. While none of this is rigorous by 
mathematical standards, the final formula is correct in all cases which 
can be checked against rigorous mathematical computations. [Freed 
and Uhlenbeck, 1995, p. 2] 
Such confirmation increases your confidence in the power of your physical mod-
elling. The more surprising the verified mathematical conjecture the greater the 
boost to your confidence. 
It is interesting to wonder why nobody (at least to my knowledge) has taken 
P6lya up on his Bayesianism in mathematics. What is the underlying intuition be-
hind the avoidance of a Bayesian treatment of plausible and inductive reasoning 
in mathematics? We can begin to understand what is at stake when we read Mary 
Hesse's claim that ..... since mathematical theorems, unlike scientific laws, are 
matters of proof, it is not likely that our degree of belief in Goldbach's conjecture 
is happily explicated by probability functions." [Hesse, 1974, p. 191]. There are 
two responses to this. First, while it is true that the nature of mathematics is char-
acterised like no other discipline by its possession of deductive proof as a means 
of attaining the highest confidence in the trustworthiness of its results, proofs are 
never perfectly secure. Second, and more importantly, what gets overlooked here 
is the prevalence in mathematics of factors other than proof for changing degrees 
of belief. 

182 
DAVID CORFIELD 
The lack of attention plausible mathematical reasoning has received reflects the 
refusal of most anglophone philosophers of mathematics to consider the way math-
ematical research is conducted and assessed. On the basis of this refusal, it is very 
easy then to persist in thinking of mathematics merely as a body of established 
truths. As classical deductive logic may be captured from a probability calculus 
which allows propositions to have probabilities either 0 or 1, the belief that math-
ematics is some kind of elaboration of logic and that the mathematical statements 
to be considered philosophically are those known to be right or wrong go hand in 
hand. We could say in fact that mathematics has suffered philosophically from its 
success at accumulating knowledge since this has deflected philosophers' attention 
from mathematics as it is being developed. But one has only to glance at one of 
the many survey articles in which mathematicians discuss the state of play in their 
field, to realise the vastness of what they know to be unknown but are very eager to 
know, and about which they may be thought to have degrees of belief equal neither 
to 0 nor to 1.2 
We shall see in section 3 how mathematical evidence comes in very different 
shapes and sizes. But even remaining with 'proved' or well-established statements, 
although there would appear to be little scope for plausible reasoning, there are a 
number of ways that less than certain degrees of belief can be attributed to these 
results. David Hume described this lack of certainty well: 
There is no Algebraist nor mathematician so expert in his science, as 
to place entire confidence in his proof immediately on his discovery 
of it, or regard it as any thing, but a mere probability. Every time he 
runs over his proofs, his confidence encreases; but still more by the 
approbation of his friends; and is rais'd to its utmost perfection by the 
universal assent and applauses of the learned world. Now 'tis evident, 
that this gradual encrease of assurance is nothing but the addition of 
new probabilities, and is deriv'd from the constant union of causes 
and effects, according to past experience and observation. 
[Hume, 
1739, pp. 180-I] 
Perfect credibility may be difficult to achieve for proofs taking one of a number 
of non-standard forms, from humanly-generated unsurveyable proofs to computer-
assisted proofs to probabilistic proofs. These latter include tests for the primality 
of a natural number, n. Due to the fact that around half of the numbers less than n 
are easily computed "witnesses" to its being composite, if such is the case, a small 
sample can quickly show beyond any set level of doubt whether n is prime. 
While a certain amount of suspicion surrounds the latter type of 'proof', from 
the Bayesian perspective, one can claim that all evidence shares the property that it 
2 I mean to exclude here the immense tracts of totally uninteresting statements expressible in the 
language of ZFC in which one will never care to have a degree of belief. An idea of the plans in 
place for the expansion of (interesting) mathematics can be gleaned from the following claim: 'It is 
clear ... that the set-based mathematics we know and love is just the tip of an immense iceberg of n-
categorical, and ultimately w-categorical, mathematics. The prospect of exploring this huge body of 
new mathematics is both exhilarating and daunting.' [Baez and Dolan, 1999, p. 32]. 

BAYESIANISM IN MATHEMATICS 
183 
produces changes in some degrees of belief. The absence of any qualitative differ-
ence in the epistemic import of different types of proof has recently been noted by 
Don Fallis [1997], who considers many possible ways of distinguishing epistemi-
cally between deductive proofs and probabilistic proofs and finds none of them ad-
equate. He draws the conclusion, therefore, that there is no such difference. Fallis 
centres his discussion around 'proofs' which involve clever ways of getting strands 
of DNA to react to model searches for paths through graphs, putting beyond rea-
sonable doubt the existence or non-existence of such paths. Despite there being 
here a reliance on biochemical knowledge, Fallis still sees no qualitative difference 
as regards the justificatory power of this type of proof. Confidence in mathemat-
ical statements is being determined by natural scientific theory. This appears less 
surprising when you consider how complicated, yet well-modelled, configurations 
of silicon can be used to generate evidence for mathematical propositions. 
Fallis's point may be expressed in Bayesian terms as follows. 3 The acceptability 
of a mathematical statement is dependent solely on your rational degree of belief 
in that statement conditionalised on all the relevant evidence. Whatever level you 
set yourself (0.99 or 0.99999) the type of evidence which has led you there is irrel-
evant. A ten thousand page proof may provide as much support as a probabilistic 
proof or the non-appearance of a counter-example. To contemplate the reliability 
of a result in a particular field we should think of someone from outside the field 
asking a specialist for their advice. If the trustworthy expert says she is very certain 
that the result may be relied upon, does it matter to the enquirer how the special-
ist's confidence arises? This depiction could be taken as part of a larger Bayesian 
picture. The very strong evidence we glorify with the name 'proof' is just as much 
a piece of evidence as is a verification of a single consequence. Bayesianism treats 
in a uniform manner not just the very strong evidence that Fallis considers, but all 
varieties of partial evidence. Let us now see what we are to make of this partial 
evidence. 
2 
QUANTITATIVE BAYESIANISM 
P6lya understood plausible inference to be quite different from deductive logic. In 
his eyes [P6Iya, 1954b, pp. 112-116], deductive logic is: 
(a) Impersonal- independent of the reasoner; 
(b) Universal- independent of the subject matter; 
(c) Self-sufficient - nothing beyond the premises is needed; 
(d) Definitive - the premises may be discarded at the end of the argument. 
3He points out (private communication), however, that he is not necessarily committed to a Bayesian 
analysis of his position which assumes that one's rational degree of belief is all that really matters in 
mathematical justification. 

184 
DAVID CORFIELD 
On the other hand, plausible inference is characterised by the following properties: 
(a) The direction of change in credibility is impersonal, but the strength may be 
personal; 
(b) It can be applied universally, but domain knowledge becomes important for 
the strength of change, so there are practical limitations; 
(c) New information may have a bearing on a plausible inference, causing one 
to revise it; 
(d) The work of plausible inference is never finished as one cannot predict what 
new relevant information may arise. 
One of the principal differences seems to be that in the deductive case nobody re-
quires of you that you maximise the set of deductive consequences of what you 
hold to be certain. If you are asked whether you know the truth status of a state-
ment, you search about for a proof or disproof of it from what you already know. 
If you find nothing, you just admit your ignorance, and no-one can accuse you 
of anything worse than stupidity if you have overlooked such a proof or disproof. 
We do not go around blaming ourselves for not having known before Wiles that 
Fermat's Last Theorem is provable, even though the resources were in some sense 
available. Deductive logic is there to safeguard you from taking a false step, not 
from omitting to take a correct step. On the other hand, we may use plausible 
inference to argue about the plausibility of any statement based on what we know 
at present.4 The question is how to think about the way we go about arriving at 
degrees of belief on the basis of what we already know. 
It is clear that the strength of a mathematician's belief in the correctness of a 
result has an impact on their practice: Andrew Wiles would hardly have devoted 
seven years to Fermat's Last Theorem had he not had great faith in its veracity. 
No doubt we could give a complicated Bayesian reconstruction of his decision to 
do so in terms of the utility of success, the expected utility of lemmas derived in 
a failed attempt, and so on. For a more simple example, we may give a Bayesian 
reconstruction of the following decision of the French Academy: 
The impossibility of squaring the circle was shown in 1885, but be-
fore that date all geometers considered this impossibility as so "prob-
able" that the Academie des Sciences rejected without examination 
the, alas!, too numerous memoirs on this subject that a few unhappy 
madmen sent in every year. Was the Academie wrong? Evidently not, 
4Jaynes [forthcoming, Ch. 10, p. 211 has a similar view on the difference between deductive logic 
and probability theory as logic: "Nothing in our past experience could have prepared us for this; it is 
a situation without parallel in any other field. In other applications of mathematics, if we fail to use 
all of the relevant data of a problem, the result will be that we are unable to get any answer at all. But 
probability theory cannot have any such built-in safety device, because in principle, the theory must be 
able to operate no matter what our incomplete information might be". 

BAYESIANISM IN MATHEMATICS 
and it knew perfectly well that by acting in this manner it did not run 
the least risk of stifling a discovery of moment. The Academie could 
not have proved that it was right, but it knew well that its instincts did 
not deceive it. If you had asked the Academicians, they would have 
answered: "We have compared the probability that an unknown sci-
entist should have found out what has been vainly sought for so long, 
with the probability that there is one madman the more on earth, and 
the latter has appeared to us the greater. [Poincare, 1905, pp. 191-2] 
185 
These alternatives, being mad and being right, were hardly exhaustive. Leaving 
aside the person's sanity we can contrast the probability that their proof is correct 
with the probability that it is incorrect. 
Pr(proof correct I author unknown, I) = 
Pr(proof correct I author unknown, true, I)Â·Pr(truelauthor unknown, I) + 
Pr(proof correct I author unknown, false, I)Â·Pr(falselauthor unknown, I) 
= Pr(proof correct I author unknown, true, I)Â·Pr(truel I) 
where I is the background knowledge. 
Substituting reasonable estimates of the Academie's degrees of belief will lead 
to a very small value for this last expression because its two factors are small. 
On the other hand, a submitted proof of the possibility of squaring the circle by 
a known mathematician, or a submitted proof of its impossibility by an unknown 
author would presumably have been dealt with more tolerantly. 
Notice that this reconstruction would not seem to require one to go beyond 
vague talk of very high or very low probabilities. By contrast, when it comes to 
offering a betting ratio for the trillionth decimal digit of 7l' being 9, it would seem to 
be eminently reasonable to propose precisely 1110, and yet neither the coherence of 
realistic personalism nor any requirement to maximize expected subjective utility 
imposes this value upon you. What appears to determine this value is some form of 
the principle of indifference based on our background knowledge. With a simple 
grasp of the idea of a decimal expansion we simply have no reason to believe any 
single digit more likely than any other. Those who know a little more may have 
heard that there is no statistical evidence to date for any lack of uniformity in the 
known portion of the expansion, probably rendering them much less likely to be 
swayed in their betting ratio by a spate of 9s occurring shortly before the trillionth 
place. So, unless some dramatic piece of theoretical evidence is found, it seems 
that most mathematicians would stick with the same betting ratio until the point 
when they hear that computers have calculated the trillionth place.5 
The issue to be treated in this section is whether we require a quantitative, or 
even algorithmic, form of Bayesianism to allow us to explicate plausible mathe-
matical reasoning, or whether, like P6lya, we can make do with a qualitative form 
5 As of 1999 they had reached the 206 billionth. 

186 
DAVID CORFIELD 
of it. First, it will be helpful for us to contrast P6lya's position with that of Jaynes. 
For Jaynes, P6lya was an inspiration. Indeed, he 
... was the original source of many of the ideas underlying the present 
work. We show how P6lya's principles may be made quantitative, 
with resulting useful applications. [Jaynes, forthcoming, Ch. 1, p. 3] 
As is well known, Jaynes was at the objectivist end of the Bayesian spectrum. In 
other words, his aim was to establish principles (maximum entropy, transformation 
groups, etc.) applicable in as many situations as possible, in which a reasonable 
being could rationally decide on their prior probabilities. P6lya, on the other hand, 
reckoned that one would have to stay with a qualitative treatment (e.g., if A is anal-
ogous to Band B becomes more likely, then A becomes somewhat more likely), 
in that the direction of changes to confidence might be determined but not their 
strength. But Jaynes claimed that this decision was based on a faulty calculation 
made by P6lya when he was considering the support provided to Newton's theory 
of gravitation by its prediction of the existence and location of a new planet, now 
called Neptune. The incorrect calculation occurred when P6lya was discussing the 
boost to confidence in Newtonian gravitation brought about by the observation of 
a previously unknown planet precisely where calculations predicted it to be, based 
on observed deviations in Uranus's orbit. 
P6lya takes Bayes theorem in the form, 
G 
I 
) 
Pr(Newt. Grav.)Â·Pr(NeptuneINewt. Grav.) 
Pr(Newt. 
rav. Neptune = --------"---'------
Pr(N eptune ), 
where Pr(Neptune) corresponds to a scientist's degree of belief that the proposed 
planet lie in the predicted direction. For the purposes of the calculation, he es-
timates Pr(Neptune) in two ways. First, he calculates the probability of a point 
lying within one degree of solid angle of the predicted direction, and arrives at 
a figure of 0.00007615 ~ 1/13100. Second, on the grounds that the new planet 
might have been expected to lie on the ecliptic, he uses the probability of a point 
on a circle lying within one degree of the specified position, yielding a value for 
Pr(Neptune) of 11180. He then argues that Pr(Newtonian Gravitation) must be less 
than Pr(Neptune), otherwise Bayes' theorem will lead to a posterior probability 
greater then 1, but that it is unreasonable to imagine a scientist's degree of belief 
being less than even the larger figure of 11180, since Newtonian Gravitation was 
already well-confirmed by that point. He concludes, "We may be tempted to re-
gard this as a refutation of the proposed inequality." [P6Iya, 1954b, p. 132] and 
suggests we return to a safer qualitative treatment. 
However, as Jaynes points out, P6lya's calculations were in fact of the prior to 
posterior odds ratio of two theories: on the one hand, Newtonian gravitation, and 
on the other, a theory which predicts merely that there be another planet, firstly 
anywhere and secondly on the ecliptic. Indeed, from the confirmation, Newtonian 
gravitation is receiving a boost of 13100 or 180 relative to the theory that there is 

BAYESIANISM IN MATHEMATICS 
187 
one more planet somewhere. P6lya had forgotten that if Pr(Newtonian Gravitation) 
is already high then so too would Pr(Neptune) be. 
We are told by Jaynes that P6lya realised his mistake and went on to participate 
vigorously in the former's lectures at Stanford University in the 1950s. However, 
P6lya had given several further arguments against quantitative plausible reasoning, 
so even if Jaynes could block this particular argument, one would need to confront 
the others. Reading through them, however, one notes that P6lya is making fairly 
standard points: the incomparability of evidence and conjectures, problems with 
the principle of indifference, etc. 
Could it be that your background predisposes you to adopt a certain type of 
Bayesianism? The physicist relies on symmetry considerations pertaining to the 
mechanisms producing the data, the philosopher of science on vaguer considera-
tions of theory evaluation, while the economist must integrate a mass of data with 
her qualitative, quasi-causal understanding of the economy. Are disputes among 
Bayesians like the blind men feeling different parts of an elephant? 
Bayesianism applied to reasoning in the natural sciences appears to fall into two 
rather distinct categories: 
(i) analysis of data from, say, nuclear magnetic resonance experiments or as-
trophysical observations; 
(ii) plausible reasoning of scientists by philosophers of science (e.g., [Franklin, 
1986]). 
We may wonder how strong the relation is between them. Rosenkrantz [1977] 
attempted a unified treatment, and he indicates by his subtitle 'Towards a Bayesian 
Philosophy of Science' that a treatment of history and philosophy of science issues 
alongside statistical issues should be 'mutually enriching' [Rosenkrantz, 1977, p. 
xU. 
Jaynes himself was less sure about how far one could take the historical recon-
structions of scientific inference down a Bayesian route. After his discussion of 
P6lya's attempt to quantify Neptune discovery he claims: 
But the example also shows clearly that in practice the situation faced 
by the scientist is so complicated that there is little hope of applying 
Bayes' theorem to give quantitative results about the relative status of 
theories. Also there is no need to do this, because the real difficulty of 
the scientist is not in the reasoning process itself; his common sense 
is quite adequate for that. The real difficulty is in learning how to 
formulate new alternatives which fit better the facts. Usually, when 
one succeeds in doing this, the evidence for the new theory soon be-
comes so overwhelming that nobody needs probability theory to tell 
him what conclusions to draw. 
[Jaynes, forthcoming, Ch. 5, p. 17] 
This note occurs in a chapter entitled 'Queer uses of probability', by which he 
intends that at present we have no rational means for ascribing priors. So, despite 

188 
DAVID CORFIELD 
his professed debt to Mathematics and Plausible Reasoning, we find two poles 
of Bayesianism represented by Jaynes and P6lya. For Jaynes, any rational agent 
possessing the same information will assign identical probability functions. For 
P6lya, two experts with the same training may accord different changes to their 
degrees of belief on discovery of the same fact. One imagines a machine making 
plausible inferences, the other emphasises the human aspect. 
Jaynes: 
P6lya: 
... instead of asking, "How can we build a mathematical model of 
human common sense?" let us ask, "How could we build a machine 
which would carry out useful plausible reasoning, following clearly 
defined principles expressing an idealized common sense? [Jaynes, 
forthcoming, Ch. 1, p. 5] 
A person has a background, a machine has not. Indeed, you can build 
a machine to draw demonstrative conclusions for you, but I think you 
can never build a machine that will draw plausible inferences. [P6Iya, 
1954b, p. 116] 
Perhaps it is the lack of exactitude which steers Jaynes away from modelling 
scientific reasoning. After a lifetime investigating how symmetry considerations 
allow the derivation of the principles of statistical mechanics, it must be difficult 
to adapt to thinking about plausibility in complex situations. 
But if a physicist might be excused, what of a philosopher? John Earman, while 
discussing how a physicist's degrees of belief in cosmological propositions were 
affected by the appearance of General Relativity on the scene, tells us 
But the problem we are now facing is quite unlike those allegedly 
solved by classical principles of indifference or modern variants 
thereof, such as E. T. Jaynes's maximum entropy principle, where 
it assumed that we know nothing or very little about the possibilities 
in question. In typical cases the scientific community will possess a 
vast store of relevant experimental and theoretical information. Us-
ing that information to inform the redistribution of probabilities over 
the competing theories on the occasion of the introduction of the new 
theory or theories is a process that is, in the strict sense of the term, 
arational: it cannot be accomplished by some neat formal rules, or, to 
use Kuhn's term, by an algorithm. On the other hand, the process is 
far from irrational, since it is 'informed by reasons. But the reasons, 
as Kuhn has emphasized, come in the form of persuasions rather than 
proof. In Bayesian terms, the reasons are marshalled in the guise of 
plausibility arguments. The deployment of plausibility arguments is 
an art form for which there currently exists no taxonomy. And in view 
of the limitless variety of such arguments, it is unlikely that anything 

BAYESIANISM IN MATHEMATICS 
more than a superficial taxonomy can be developed. [Earman, 1992, 
p. 197] 
189 
This seems a rather pessimistic analysis for a professed Bayesian. Does the 
'limitless variety' of these arguments mean that we should not expect to find pat-
terns among them? Despite the talk of their deployment being an 'art form', Ear-
man does allow himself to talk about the objective quality of these plausibility 
arguments. Indeed, he claims that: 
Part of what it means to be an "expert" in a field is to possess the 
ability to recognize when such persuasions are good and when they 
are not. 
[Earman, 1992, p. 140] 
Interestingly, it is P6lya the "expert" in mathematics who believes that it is possible 
to extract the patterns of good plausibility arguments from his field. 
So, out of the three, Jaynes, P6lya and Earman, representatives of three differ-
ent types of Bayesianism, it is P6lya who believes one can say something quite 
concrete about plausible reasoning. All realise that plausible reasoning is a very 
complex process. Neither Jaynes nor Earman cannot see a way forward with plau-
sible scientific reasoning. This leaves P6lya who gets involved with real cases 
of (his own) mathematical reasoning, which he goes on to relate to juridical rea-
soning and reasoning about one's neighbour's behaviour. Is he right to claim that 
mathematics provides a better launch pad to tackle everyday reasoning than does 
science? 
If we want a fourth Bayesian to complete the square, we might look to the com-
puter scientist Judea Pearl. Like P6lya, Pearl believes we can formulate the prin-
ciples of everyday common sense reasoning, and like Jaynes he thinks Bayesian 
inference can be conducted algorithmically. To be able to do the latter requires 
a way of encoding prior information efficiently to allow Bayesian inference to 
occur. For Pearl [Pearl, 2000], (this volume) humans store their background in-
formation efficiently in the form of causal knowledge. The representation of this 
causal knowledge in a reasonably sparse Bayesian network is the means by which 
a machine can be made to carry out plausible reasoning and so extend our powers 
of uncertain reasoning. 
In his earlier work Pearl [1988] expressed his appreciation of P6lya's ideas, 
and yet found fault with his restriction to the elucidation of patterns of plausi-
ble reasoning rather than a logic. He considers P6lya's loose characterisation of 
these patterns not to have distinguished beUyeen evidence factoring through con-
sequences and evidence factoring through causes. For instance, P6lya asserts that 
when B is known to be a consequence of A, the discovery that B holds makes it 
more likely that A holds. This, however, is a well known fallacy of causal reason-
ing. I see that the sprinkler on my lawn is running and that the grass is wet, but 
this does make it more probable to me that it has rained recently even though wet 
grass is a consequence of it having done so. But one need not remain with causal 
stories to reveal this fallacy. A consequence of a natural number being divisible by 

190 
DAVID CORFIELD 
four is that it is even. I find that a number I seek is either 2 or 6. Although I have 
learnt that it is even, this discovery reduces the probability of its being divisible 
by 4 to zero. Essentially, what P6lya overlooked was the web-like nature of our 
beliefs, only departing from patterns involving two propositions when he consid-
ered the possibility of two facts having a common ground. In Bayesian networks, 
converging arrows are equally important but must be treated differently. 
It remains to be seen whether the techniques of Bayesian networks may il-
luminate scientific inference. Now we shall turn our attention to examine what 
Bayesianism has to say about certain aspects of mathematical reasoning. 
3 WHAT MIGHT BE ACHIEVED BY BAYESIANISM IN MATHEMATICS 
Varieties of mathematical evidence may be very subtle, lending support to Earman 
and Jaynes' scepticism. P6lya [1954b, p. 11ll himself had the intuition that two 
mathematicians with apparently similar expertise in a field might have different 
degrees of belief in the truth of a result and treat evidence for that result differ-
ently. Even though each found a consequence of the result equally plausible, the 
establishment of this consequence could have an unequal effect on their ratings of 
the likelihood of the first result being correct. The complex blending of the various 
kinds of evidence experienced through a mathematician's career would explain the 
differences in these reactions, some of which might be attributable to aspirations 
on the part of each of them either to prove or disprove the theorem. But P6lya 
goes further to suggest that such differences of judgement are based on "still more 
obscure, scarcely formulated, inarticulate grounds" [P6Iya, 1954b, p. 111]. 
Such appeals to the inexpressible, or at least to the imprecisely expressed, are 
not at all uncommon. For example, the mathematician Sir Michael Atiyah asserts 
that 
... it is hard to communicate understanding because that is something 
you get by living with a problem for a long time. You study it, perhaps 
for years. You get the feel of it and it is in your bones. [Atiyah, 1984, 
p.305] 
Such comments may have been devised by mathematicians to give an air of 
mystery to their practice. A sceptic could point out that doctors have done like-
wise in the past by alleging that diagnosis requires some profound intuitive faculty 
of divination, an attractive image shattered by the successful construction of ex-
pert systems which have shown physicians to be replaceable in some situations, 
by machines using propositionally encoded evidence. However, the success of 
artificial intelligence in some areas of medical diagnosis may be contrasted with 
the extreme difficulty in getting computers to do anything that might be termed 
creative in mathematics.6 The essential point does not concern whether or not 
6 A possible exception is the recent successful automated solution of the Robbins problem (see 
http://www.mcs.anl.gov/~mccunel). drawn to my attention by Don Fallis. 

BAYESIANISM IN MATHEMATICS 
191 
mathematicians in fact rely on non-propositional knowledge, so much as whether 
there might be something about this type of knowledge which is indispensable to 
doing mathematics. 
Certainly, evidence for the correctness of a statement may be very subtle. It may 
even arise through an experience of failure. In [Corfield, 1997] I pointed out the 
inaccuracy on Lakatos's part of his notion of lemma-incorporation, the idea that 
faulty proofs are generally repaired by some simple insertion of a lemma. As I 
explained there, while proving the so-called 'duality theorem' Poincare had come 
to realise that an assumption he was making about the way differential manifolds 
intersect was invalid in general. However, he still believed that the general strat-
egy could be made to work of constructing for a given set of manifolds of equal 
dimension a manifold of complementary dimension which intersected each of the 
members of the set exactly once. He just needed to have the intersections occur in 
a more controlled fashion. One can only guess how this experience impacted on 
his degree of belief in the duality theorem. It is quite probable that even though the 
initial proof was found to be wrong, the experience of near success with a variant 
of a strategy gave him hope that another variant would work. It must also happen, 
however, that mathematicians are discouraged by such setbacks. 
Evidence can also involve the non-discovery of something as Sherlock Holmes 
well knew when he built his case on the observation of a dog that did not bark. 
The classic example of the unsurveyable human-generated kind of proof at the 
present time is the proof of the classification of finite simple groups into 5 infinite 
families and 26 sporadic outsiders. How does one's degree of belief in this result 
depend on such potentially flawed lengthy evidence? Fallis [1997] has Gorenstein, 
the driving force behind the collective proof, confessing that confidence is boosted 
less by the proof itself than by the fact that no other such groups have been found. 
Similarly, remarks are often to be heard concerning the consistency of ZFC that 
one would have expected to have encountered a contradiction by now. 
We should also remember that evidence for mathematical propositions comes 
from sources which have only recently become available. The use of computers 
to fill in the gaps of human proofs has become acceptable, but computers are used 
in many other ways in mathematics. For example, they provide evidence for con-
jectures via calculations made on samples, and they produce visual evidence in 
dynamical systems theory, as in the drawing of attractors or power spectra. Re-
liance on computer evidence raises some novel issues. Oscar Lanford is attributed 
with pointing out that 
... in order to justify a computer calcUlation as part of a proof ... , you 
must not only prove that the program is correct (and how often is 
that done?) but you must understand how the computer rounds num-
bers, and how the operating system functions, including how the time-
sharing system works. 
[Hirsch, 1994, p. 
188] 
Moreover, if more than one piece of computer evidence is being considered, 

192 
DAVID CORFIELD 
how do we judge how similar they are for conditionalising purposes? This would 
require one to know the mathematics behind any similarities between the algo-
rithms utilised. 
It is clear then that any account of mathematical inference will require a very 
expressive language to represent all the various forms of evidence which impact on 
belief in mathematical propositions. The Bayesian wishing to treat only proposi-
tions couched in the language of the object level might hope to be able to resort to 
Jeffrey conditionalisation, but this comes at the price of glossing over interesting 
features of learning. Concerning scientific inference, Earman [1992, pp. 196-
8] asserts that many experiences will cause the scientist to undergo non-Bayesian 
shifts in their degrees of belief, i.e., ones unaccountable for by any form of algo-
rithmic conditionalisation. These shifts, the resetting of initial probabilities, are 
very common, he claims, arising from the expansion of the theoretic framework or 
from the experience of events such as "[n]ew observations, even offamiliar scenes; 
conversations with friends; idle speculations; dreams ... " [Earman, 1992, p. 198]. 
One might despair of making any headway, but taking P6lya as a guide we may 
be able to achieve something. While recognising that making sense of plausible 
reasoning in mathematics will not be easy, I believe that three key areas of promise 
for this kind of Bayesianism in mathematics are analogy, strategy and enumerative 
induction. 
3.1 
Analogy 
Before turning to a probabilistic analysis of plausible reasoning in the second vol-
ume of Plausible Reasoning, P6lya had devoted the first volume [1954a], as its 
subtitle suggests, to the themes of analogy and induction. Analogies vary as to 
their precision. When vague they contribute to what he called the general at-
mosphere surrounding a mathematical conjecture, which he contrasts to pertinent 
clear facts. While verifications of particular consequences are straightforwardly 
relevant facts, the pertinence of analogical constructions may be hard to discern 
precisely. Nevertheless, mathematicians, succh as Gian-Carlo Rota, take them to 
be vitally important: 
The enrapturing discoveries of our field systematically conceal, like 
footprints erased in the sand, the analogical train of thought that is the 
authentic life of mathematics. [Kac et al., 1986, p. ix] 
Let us illustrate this with an example. At the present time the vast majority of 
mathematicians have a high degree of belief in the Riemann Hypothesis. Recall 
that the Riemann zeta function is defined as the analytic continuation of ((8) = 
~n-s summed over the natural numbers, and that the hypothesis claims that if 
8 is a zero of ((8), then either 8 = -2, -4, -6, ... , or the real part of 8 equals 
112. Many roots have been calculated (including the first 1.5 billion zeros in the 
upper complex plane along with other blocks of zeros), all confirming the theory, 
but despite this "overwhelming numerical evidence, no mathematical proof is in 

BAYESIANISM IN MATHEMATICS 
193 
sight." [Cartier, 1992, p. 15]. As Bayesians have explained, there are limits to 
the value of showing that your theory passes tests which are conceived to be very 
similar. If, for example, a further 100 million zeros of the zeta function are found 
to have their real part equal to 112, then little change will occur in mathematicians' 
degrees of belief, although a little more credibility would be gained if this were true 
of 100 million zeros around the 1020 th, which is precisely what has happened. 
In this example the clear facts making up the numerical evidence can lend only 
limited credence by themselves. After all, there are 'natural' properties of the natu-
ral numbers which are known to hold for exceedingly long initial sequences. What 
counts in addition beyond evidential facts, however numerous, is the credibility 
of stronger results, general consequences and analogies. Indeed, if an analogy is 
deemed strong enough, results holding for one side of it are thought to provide 
considerable support for their parallels. Concerning the Riemann conjecture (RC), 
we are told that: 
There is impressive numerical evidence in its favour but certainly the 
best reason to believe that it is true comes from the analogy of number 
fields with function fields of curves over finite fields where the ana-
logue ofRC has first been proved by A. Weil. 
[Deninger, 1994, p. 
493] 
This analogy7 was postulated early in this century as a useful way of providing 
a halfway house across an older analogy, developed by Dedekind and Weber, from 
algebraic number fields to function fields over the complex numbers. However, 
the translation techniques between the three domains have still not been perfected. 
The more geometric side of the analogy Deninger mentions was able to absorb 
cohomological techniques, allowing Weil to prove the Riemann hypothesis ana-
logue in 1940. An extraordinary amount of effort has since been expended trying 
to apply cohomology to number theory (Weil, Grothendieck, Deligne, etc.) with 
the establishment of the standard Riemann hypothesis as one of its aims. 
How should we judge how analogous two propositions, A and B, are to each 
other? For P6lya [1954b, p. 27] it correlates to the strength of your "hope" for a 
common ground from which they both would naturally follow. Increase in con-
fidence in A will then feed up to the common ground, H, and back down to B. 8 
If analogy is to be treated anywhere, I believe mathematics will provide a good 
location, since there are plenty of excellent examples to be found there. In P6lya's 
principal example, Euler noticed that the function sin x/x resembles a polynomial 
in several respects: it has no poles; it has the right number of zeros, which do not 
accumulate; it behaves symmetrically at Â±oo. On the other hand, unlike a poly-
nomial, sin x/x remains bounded. Even with this disanalogy, it seemed plausible 
that polynomials and sin x/x would share other properties. One notable feature 
of complex polynomials is that anyone of them may be expressed as a product 
7See also [Katz and Samak, 1999], in particular the table on page 12. 
8Notice here the flavour of a Bayesian network: H pointing to both A and B. 

194 
DAVID CORFIELD 
of factors of the form (1 - x/root), taken over all of its roots. Might this prop-
erty also apply to sin X/X? Well, the roots of this function are Â±11", Â±211", Â±311", ... , 
suggesting that we should have 
Si:X = (1 - ;:) (1 - 4:2) (1 - ::2 ) ... , 
On the other hand, expanding the function as a Taylor series, we have 
sinx/x = 1- x2/6 + x4/120 - ... 
Equating coefficients of x 2 suggests then that 
1 + 1/4 + 1/9 + 1/16 + ... = 11"2/6. 
Even after checking this sum to several decimal places Euler was not absolutely 
confident in the result, but he had in fact solved a famous problem by analogical 
reasoning. 
It might be that what is happening here is something similar to what Pearl 
[2000] has termed the "transfer of robust mechanisms to new situations". We have 
a mechanism that links factorisation of a function to its zeros. We find it applies for 
complex polynomials and wonder whether it may be extended. Features of poly-
nomials that may be required in the new setting are that they have the right number 
of zeros, they remain bounded on compact sets, they behave similarly at Â±oo. 
Might the mechanism be expected to work for a non-polynomial function possess-
ing these features, such as sin x/x? What if you force the variable measuring the 
number of roots to be infinite? We may find it hard to estimate quantitatively the 
similarity between a function like sin x/x and a complex polynomial, but it is clear 
that tanx / x or exp x are less similar, the former having poles, the latter having no 
zeros and asymmetric behaviour at Â±oo, and indeed the mechanism does fail for 
them. 
In this case, once the realisation that an analogy was possible, it didn't cost 
much to work through the particular example. Euler hardly needed to weigh up 
the degree of similarity since calculations of the sum quickly convinced him. How-
ever, to develop a general theory of the expansion of complex functions did require 
greater faith in the analogy. This paid off when further exploration into this mecha-
nism allowed mathematicians to form a very general result concerning entire com-
plex functions, providing the "common ground" for the analogues. 
3.2 Strategy 
Moving on now to strategy, the title of Deninger's paper-Evidence for a Coho-
mological Approach to Analytic Number Theory-is also relevant to us. His aim 
in this paper is to increase our degree of belief that a particular means of thinking 
about a field will lead to new results in that field. This is a question of strategy. 

BAYESIANISM IN MATHEMATICS 
195 
At a finer level one talks of tactics. Researchers from the AI community working 
on automated theorem proving, have borrowed these terms. One tactic devised by 
Larry Wos [Wos and Pieper, 1999] involves thinking in terms of how probable it is 
that the computer can reach the target theorem from a particular formula generated 
from the hypotheses during the running of the programme. This tactic takes the 
form of a weighting in the search algorithm in favour of formulas which have a 
syntactical form matching the target. 
Elsewhere, researchers in Edinburgh are interested in the idea of the choice of 
tactics [Bundy, 1999]. Thereis an idea oflikening mathematics to a game of bridge 
where the mathematician, like the declarer, has some information and a range of 
strategies to achieve their goal (finesse, draw trumps, squeeze). Of course, there is 
a difference. In bridge, you are in the dynamic situation where you cannot tryout 
every strategy, as the cards get played. This forces you to pay very close attention 
to which tactics have the best chance of working. In mathematics, on the other 
hand, with a computer it does not cost you much to try things out, although one 
does risk combinatorial explosion. At present, probabilities are being used for their 
computer bridge player, they are not yet being used for their automated theorem 
prover. While the computer has a small repertoire of syntactical tactics (rippling, 
resonance, heat, etc.) there is less need for an assessment of the chance of each 
working, but presumably the number of proof techniques will grow. 
These automated proof strategies are at present syntactically based. Naturally, 
semantic considerations play the dominant role for human mathematician. P6lya 
was active in this area too. To give a brief flavour of his ideas, if when planning 
to solve a problem, any of the following should increase your confidence in your 
plan [P6lya, 1954b, pp. 152-153]; 
â¢ Your plan takes all relevant information into account. 
â¢ Your plan provides for a connection between what is known and what is 
unknown. 
â¢ Your plan resembles some which have been successful in problems of this 
kind. 
â¢ Your plan is similar to one that succeeded in solving an analogous problem. 
â¢ Your plan succeeded in solving a particular case of the problem. 
â¢ Your plan succeeded in solving a part of the problem. 
3.3 Enumerative induction 
Besides the incorrect Bayesian calculation of the confirmation provided by the 
observation of Neptune, P6lya does resort to a quantitative sketch in another place 
[P6lya, 1954b, pp. 96-71. Here he outlines how one might think through the 
boost to the credibility of Euler's formula for a polyhedron (vertices - edges + 

196 
DAVID CORFIELD 
faces = 2) known to hold for some simple cases, when it is found to be true of 
the icosahedron. (12 - 30 + 20 = 2). His approach is to reduce the problem to 
the chances of finding three numbers in the range 1 to 30 with the property that 
the second is equal to the sum of the other two, i.e., (V -
1) + (F -
1) = E. 
The proportion of these triples is around 1 in 60, providing, he argues, a boost of 
approximately 60 to the prior probability of Euler's conjecture. Here again we see 
the same problem that Jaynes located in the Neptune calculation. The ratio of the 
likelihood of the Euler conjecture compared to that of its negation is 60. 
In any case P6lya's construction can only be viewed as sketchy. It is not hard to 
see that the number of edges will always be at least as great as one and a half times 
the number of faces or the number of vertices. (For the latter, for example, note 
that each edge has two ends, but at least three of these ends coincide at a vertex). 
Thus one should have realised that there are further constraints on the possible 
triples and hence that the likelihood ratio due to the evidence for the Euler formula 
should have been in comparison to better informed rival conjecture, and so not so 
large. But the interesting point is that P6lya goes on to say that: 
If the verifications continue without interruption, there comes a mo-
ment, sooner or later when we feel obliged to reject the explanation 
by chance. 
[P6Iya, 1954b, p. 97] 
The question then arises as to whether one is justified in saying such a thing on 
the basis of a finite number of verifications of a law covering an infinite number of 
cases. This will hinge on the issue of the prior probability of such a law. 
Now, consider Laplace's rule of succession. If you imagine yourself drawing 
with replacement from a bag of some unknown mixture of white and black balls, 
and you have seen m white balls, but no black balls, the standard use of the prin-
ciple of indifference suggests that the probability that the next n will be positive 
is 
(m + 1)/(m + n + 1). 
As n ~ 00, this probability tends to zero. In other words, if verifying a mathemat-
ical conjecture could be modelled in this fashion, no amount of verification could 
help you raise your degree of belief above zero. 
This accords with the way Rosenkrantz [1977] views the situation. He considers 
the particular case of the twin prime conjecture: that there are an infinite number 
of pairs of primes with difference 2. He mentions that beyond the verification of 
many cases, there are arguments in analytic number theory which suggest that you 
can form an estimate for the number of twin primes less than n and show that it 
diverges. He then continues: 
Now if Popper's point is that no examination of 'positive cases' could 
ever raise the probability of such a conjecture to a finite positive value, 
I cannot but agree. Instances alone cannot sway us! But if his claim is 
that evidence of any kind (short of proof) can raise the probability of a 

BAYESIANISM IN MATHEMATICS 
general law to a finite positive value, I emphatically disagree. On the 
cited evidence for the twin prime conjecture, for example, it would 
seem to me quite rational to accept a bet on the truth of the conjecture 
at odds of, say 100: 1, that is to stake say $ 100 against a return of $ 
10000 should the conjecture prove true. [Rosenkrantz, 1977, p. 132] 
197 
So for Rosenkrantz, with no background knowledge, the principle of indiffer-
ence forces a universal to have zero, or perhaps an infinitesimal (something also 
considered by P6Iya), prior probability. However, other considerations may deter-
mine a positive probability. 
Subject-specific arguments usually underlie probability assessments 
in mathematics. 
[Rosenkrantz, 1977, p. 90] 
In support of this view, returning to the Euler conjecture, we should note that there 
was background knowledge. For polygons, it is a trivial fact that there is a linear 
relation between the number of vertices and the number of edges, namely, V = E. 
Hence, a simple linear relation might be expected one dimension higher. 
Is it always this kind of background knowledge which gives the prior probabil-
ity of a conjecture a 'leg-up'? Do we ever have a situation with no background 
knowledge, i.e., where a general atmosphere is lacking? Consider the case of John 
Conway's 'Monstrous Moonshine', the conjectured link between the j-function 
and the monster simple group. The j-function arose in the nineteenth century from 
the study of the parameterisation of elliptic curves. It has a Fourier expansion in 
q = exp(27l'iT): 
jeT) = l/q + 744 + 196884q + 21493760q2 + 864299970q3 + ... 
One day while leafing through a book containing this expansion, a mathemati-
cian named John Mackay observed that there was something familiar about the 
third coefficient of this series. He recalled that 196,883 was the dimension of the 
smallest non-trivial irreducible representation of what was to become known as 
the monster group, later confirmed to be the largest of the 26 sporadic finite simple 
groups. Better still, adding on the 1 dimension of the trivial representation of the 
monster group results in equality. 
In view of the very different origins of these entities, the j-function from nine-
teenth century work on elliptic curves and the monster group from contemporary 
work in finite group theory, if one had asked a mathematician how likely she 
thought it that there be some substantial conceptual connection between them or 
common ground explaining them both, the answer would presumably have been 
"vanishingly small". In Bayesian terms, Pr(connection! numerical observation) is 
considerably greater than Pr(connection), but the latter is so low that even this un-
likely coincidence does not bolster it sufficiently to make it credible. Naturally, 
McKay was told that he was 'talking nonsense'. He then went on, however, to ob-
serve that the second nontrivial representation has dimension 21296876. A quick 

198 
DAVID CORFIELD 
calculation revealed that the fourth coefficient of the j-function could be expressed 
as: 21493760 = 21296876 + 196883 + 1. In fact every further coefficient of the 
j-function turns out to be a simple sum of the dimensions of the monster's rep-
resentations. At this point the question of whether there is some connection has 
been all but answered-it has become a near certainty. Conway challenged the 
mathematics community to resolve this puzzle. 
Fourier expansion in q = exp(27rir): 
j(r) = l/q + 744 + 196884q + 21493760q2 + 864299970q3 + ... 
196884 
196883 + 1 
21493760 
21296876 + 196883 + 1 
864299970 842609326 + 21296876 + 196883 + 196883 + 1 + 1 
The answer eventually arrived through a construction by Richard Borcherds, a 
student of Conway, which earned him a Fields' Medal. Borcherds managed to spin 
a thread from the j-function to the 24-dimensional Leech lattice, and from there to 
a 26-dimensional space-time inhabited by a string theory whose vertex algebra has 
the monster as its symmetry group. 
So why does the monster group-j-function connection become so likely by the 
time you have seen three or four of the sums, even with a minuscule prior, when 
other inductions are less certain after billions of verifications? Would we find 
consensus on how the number of instances affects one's confidence? Surely most 
people would agree that it was a little reckless on Fermat's part to conjecture pub-
licly that 22n + 1 is prime after verifying only 5 cases (and perhaps a check on 
divisibility by low primes for the sixth). 
n 
01234 
5 
22n + 1 3 5 17 257 65537 4294967297 = 641 x 6700417 
Is it possible to use Bayes' theorem, even merely suggestively? 
Let us return to the case of the Riemann hypothesis (RH). If we have a prior 
degree of belief for RH, how can 1.5 billion verifications affect it? Surely they 
must, but then is there some asymptotic limit? One might want to factor the RH as 
follows 
Pr(RH I Data) = Pr(RHlp = 1, Data)Â· Pr(p = IIData), 
where p denotes the limiting proportion, if this exists, of the zeros that lie on the 
line, taking the zeros in the order of increasing size of modulus. 
For the second factor we might then have started with a prior distribution over 
p according to the weighted sum of the exhaustive set of hypotheses about p: non-
convergentp; p in [0, 1);p = 1. Then if one can imagine some element of inde-
pendence between the zeros, e.g., the fact that the nth zero lies on the line provides 
no information on the (n + l)th, then the confirmation provided by the 1.5 billion 

BAYESIANISM IN MATHEMATICS 
199 
zeros should push the posterior of p = 1 to take up nearly all the probability ac-
corded to convergent p. This kind of assumption of independence has been used 
by mathematicians to make conjectures about the distribution of primes, so may 
be appropriate here. We might also consider that 1.5 billion positive instances pro-
vides an indication that p is convergent. Again, however, this consideration would 
depend on experience in similar situations. 
For the first factor, out of all the functions you have met for which their zeros 
have held for a large initial section and the proportion of cases is 1, you are won-
dering what proportion are universally true. It is clear, then, that again much would 
depend on prior experience. For example, something that would be kept in mind 
is that the function 7l'(x) , defined as the number of primes less than x, is known 
to be less than a certain function, denoted Ii (x), up to 1012, and that there is good 
evidence that this is so up to 1030 . But it is known not to hold somewhere before 
10400 . Indeed, there appears to be a change close to 1.4 x 10316 . 
Returning finally to 'Monstrous Moonshine', perhaps we should look harder 
for a reliance on background knowledge. First, it is worth remembering that the 
dimensions of the monster group's representations and the coefficients of the j-
function were not 'made up'. They come from 'natural' mathematical considera-
tions. Imagine in the monstrous moonshine case if the two sides were not 'inter-
esting' entities or that you knew for a fact that these numbers were randomly gen-
erated, wouldn't you take more convincing? Similar considerations are discussed 
by Paris [Paris et al., 2000], who wish to justify some 'natural' prior distribution 
of probability functions over n variables . 
. . . what in practice I might claim to know, or at least feel justified 
in believing, is that the data I shall receive will come from some real 
world 'experiment', some natural probability function; it will not sim-
ply have been made up. And in this case, according to my modeling, 
I do have a prior distribution for such functions. [Paris et al., 2000, p. 
313] 
Evidence for the fact that background knowledge is coming into play in this 
case is provided by the fact that on presentation of the example to an audience of 
non-mathematicians they found the numerical coincidences not at all convincing. 
Despite the fact that a mathematician has no knowledge of a reason for a connec-
tion between these two mathematical entities, some slight considerations must play 
a role. Indeed, what seemed to disappoint the non-mathematicians was the need to 
include mUltiples of the dimensions of the irreducible representations. A mathe-
matician, on the other hand, is well aware that in general a group representation is 
a sum of copies of irreducible ones. For example, the right regular representation, 
where the group acts on a vector space with basis its own elements, is such a sum 
where the number of copies of each irreducible representation is equal to its di-
mension. Behind the addition of dimensions are sums of vector spaces. Second, a 
mathematician would know that the j-function arises as a basic function, invariant 

200 
DAVID CORFIELD 
under the action of the modular group. This offers the possibility that group theory 
might shed some light on the connection. 
4 CONCLUSION 
We have covered a considerable stretch of ground here. Clearly much work re-
mains to be done on P6lya's research programme, but I think we can allow our-
selves a little more optimism than Earman. I have isolated the following areas as 
potentially useful to study in a Bayesian light: (1) Analogy; (2) Strategy choice; 
and, (3) The use of large computations to increase plausibility of conjectures. 
Elsewhere I shall consider two additional areas: (4) Mathematical predictions in 
physics; and, (5) The use of stochastic ideas in mathematics (random graphs, ran-
dom matrices, etc.). It is important to note that we need not necessarily arrive 
at some quantitative, algorithmic Bayesian procedure to have made progress. If 
Bayesianism in mathematics suggests interesting questions in the philosophy of 
mathematics, then I think we can say that it has served its purpose. 
Department of Philosophy, King's College London. 
BIBLIOGRAPHY 
[Atiyah, 1984] M. Atiyah. An Interview with Michael Atiyah. Mathematical Intelligencer, 6(1), 1984. 
Reprinted in Collected Works, vol. 1: Early Papers, General Papers, pp. 297-307, Oxford: Oxford 
University Press, 1988. 
[Baez and Dolan, 1999] J. Baez and J. Dolan. Categorification. In Higher Order Category Theory, E. 
Tetzler and M. Kapranov, eds. pp. 1-36. American Mathematical Society, Providence, RI, 1999. 
[Bundy, 1999] A. Bundy. Proof planning methods as schemas. Journal of Symbolic Computation 11, 
1-25,1999. 
[Cartier, 1992] P. Cartier. An introduction to compact Riemann surfaces. In From Number Theory 
to Physics,M. Waldschmidt, P. Moussa, J.-M. Luck and C. Itzykson, eds. Springer-Verlag, Berlin, 
1992. 
[Corfield, 1997] D. Corfield. Assaying Lakatos's philosophy of mathematics. Studies in History and 
Philosophy of Science, 28, 99-121,1997. 
[Deninger, 1994] C. Deninger. Evidence for a cohomologicaJ approach to analytic number theory. In 
First European Congress of Mathematics, Vol. 1" A. Joseph et al. eds. pp. 491-510. Birkhaiiser, 
Basel, 1994. 
[Earman, 1992] J. Earman. Bayes or Bust?: A Critical Examination of Bayesian Confirmation Theory, 
MIT Press, Cambridge, MA, 1992. 
[Fallis, 1997] D. Fallis. The epistemic status of probabilistic proof. Journal of Philosophy 94, 165-
186,1997. 
[de Finetti, 1974] B. de Finetti. Theory of Probability: A Critical Introductory Treatment. Translated 
by A. Machi and A. Smith. Wiley, London, 1974. 
[Franklin, 1986] A. Franklin. The Neglect of Experiment, Cambridge University Press, Cambridge, 
1986. 
[Freed and Uhlenbeck, 1995] D. Freed and K. Uhlenbeck, eds. Geometry and Quantum Field Theory, 
American Mathematical Society, 1995. 
[Hacking, 1967] I. Hacking. Slightly more reaJistic personal probability, Philosophy of Science 34, 
311-325,1967. 
[Hesse, 1974] M. Hesse. The Structure ofScientijic Inference. MacMillan, London, 1974. 

BAYESIANISM IN MATHEMATICS 
201 
[Hirsch, 1994] M. Hirsch. Responses to "Theoretical Mathematics", by A. Jaffe and F. Quinn', Bul-
letin of the American Mathematical Society 30, 187-191, 1994. 
[Hume,1739] D. Hume. A Treatise of Human Nature. Clarendon Press, Oxford, 1739. 
[Jaynes, forthcoming] E. Jaynes. Probability Theory: The Logic of Science, Cambridge University 
Press, forthcoming. 
[Kac et al., 1986] M. Kac, G.-c. Rota and 1. Schwartz. Discrete Thoughts: Essays in Mathematics, 
Science, and Philosophy. Birkhailser, Boston, 1986. 
[Katz and Sarnak, 1999] N. Katz and P. Sarnak. Zeroes of zeta functions and symmetry. Bulletin of 
the American Mathematical Society, 36(1): 1-26, 1999. 
[Paris et aI., 2000] 1. Paris, P. Watton and G. Wilmers. On the structure of probability functions in 
the natural world. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 
2000. 
[Pearl, 1988] 1. Pearl. Probabilistic Reasoning in Intelligent Systems. San Mateo, CA: Morgan Kauf-
man, 1988. 
[Pearl, 2000] 1. Pearl. Causality: Models, Reasoning and Inference, Cambridge University Press, 
2000. 
[Poincare, 1905] H. Poincare. Science and Hypothesis. Dover Publications, New York, 1905. 
[P6Iya, 1954a] G. P6lya. Mathematics and Plausible Reasoning: Induction and analogy in mathe-
matics, vol. 1, Princeton University Press, Princeton, 1954. 
[P6Iya, 1954b] G. P6lya. Mathematics and Plausible Reasoning: Patterns of plausible inference, vol. 
2, Princeton University Press, Princeton, 1954. 
[Rosenkrantz, 1977] R. Rosenkrantz. Inference, Method and Decision. Towards a Bayesian Philoso-
phy of Science. Reidel, Boston, 1977. 
[Wos and Pieper, 1999] L. Wos and G. W. Pieper. A fascinating country in the world of computing: 
Your guide to automated reasoning. Singapore: World Scientific, 1999. 

J. B. PARIS & A. VENCOVSKA 
COMMON SENSE AND STOCHASTIC 
INDEPENDENCE 
INTRODUCTION 
In this paper we shall extend the results in [Paris and Vencovska, 1990] and [Paris, 
1999] on common sense belief formation from (finite) knowledge bases of linear 
probabilistic constraints to include also the case of polynomial non-linear con-
straints and in particular constraints expressing stochastic independence. Indeed 
our results will be seen to extend to entirely general sets of constraints provided 
their solution sets are closed. 
To start with however we shall recall the context, assumptions, definitions and 
conclusion of these earlier papers. Briefly, we assumed that the degrees of belief 
that an agent assigns to sentences of a particular propositional language satisfy the 
standard Kolmogorov axioms for probability (i.e. 'belief equals probability') and 
considered the situation where such beliefs were to be assigned solely on the ba-
sis of a finite set of linear constraints on these beliefs/probabilities (the so called 
Watts Assumption, that the knowledge base is all the agent's knowledge, see [Paris, 
1994]). The question we considered in [Paris and Vencovska, 1990] and [Paris, 
1999] was how such an agent should assign these beliefs if s/he is to act in accor-
dance with 'common sense'. In other words we were interested in what common 
sense 'processes' (if any) might exist for assigning beliefs on the basis of such 
linear knowledge bases, 
We gave an answer to this question by formulating a number of common sense 
principles of belief formation which, as we argued, any such process (formally 
called an inference process) should satisfy, and then went on to show that there was 
precisely one inference process, the Maximum Entropy Inference Process, which 
satisfied all these principles. Hence, if one accepts, not unreasonably, that within 
this context obedience to these principles is a necessary condition for common 
sense, then the adoption of this inference process is identifiable with acting ac-
cording to common sense and hence, we would argue, with acting 'intelligently'. 
Formally, using the notation of [Paris, 1999J, let L stand for the countably in-
finite set of propositional variables Pl,P2, ... ,Pn, ... , n E N, let 5L denote the 
sentences of L built up using the connectives 1\, V, -, and let 5L(Pill ... ,PiJ de-
note the set of sentences of the finite sublanguage of L with (distinct) propositional 
variables Pi! , ... , Pin' We will use 0, <p, 'ljJ etc. to denote sentences. 
We say that w is a probability function on 5L (and similarly for 5L(Pil' .. " Pin)) 
if w : 5L -+ [0,1] and for all 0, zp E 5L, 
(PI) 
if F 0 then w(O) = 1, 
(P2) 
if 1= -,(01\ zp) then w(O V <p) = w(O) + w(<p). 
203 
D. Corfield and 1. Williamson (eds.), Foundations of Bayesianism, 203-240. 
Â© 2001 Kluwer Academic Publishers. 

204 
J. B. PARIS & A. VENCOVSKA 
As shown, for example, in [Paris, 1994], simple consequences of (Pl-2) are that 
for 0, cp E SL (SL(Pip ... ,pd), 
(i) If 1= 0 then w( 0) = 1 and w( ,0) = 0. 
(ii) If 01= cp then w(O) ~ w(cp), and if 0 == cp then w(O) = w(cp). 
(iii) w(O V cp) = w(O) + w(cp) - w(O 1\ cp). 
In what follows w, wo, etc. will be used for probability functions. Notice that if 
'Ij; E SL(Pil' ,,,,PiJ then, by the Disjunctive Normal Form Theorem, 
r 
'Ij; == V Cikj, 
j=l 
for some distinct atoms Cikj of S L(Pil , ... , Pin)' that is sentences of the form 
Â±pil 1\ Â±pi2 1\ ... 1\ Â±Pin 
where Â±p stands for P or 'p. So by (Pl-2) and (ii), for w a probability function 
on SL(Pil' ""Pin), 
r 
(1) 
w('Ij;) = L W(CikJ. 
j=l 
By (i) then, 
2n 
L W(Cij) = 1, 
j=l 
and we see that w is determined by the 2n-ary vector (W(Ci1)' ... , W(Ci2n)) E IDln 
where 
2n 
IDln = {(X1,,,,,X2n) I X1,,,,,X2n ~ 0, LXj = I}, 
j=l 
and conversely every if in this set determines a unique probability function w (with 
w( Cij) = Xj). In what follows we shall, where convenient, identify probability 
functions and points in IDln, the necessary ordering of the atoms being left implicit. 
For w a probability function on SL (and similarly SL(pip ""PiJ) and cp E 
SL, w(cp) =I- 0, the conditional probability function w( -Icp) : SL -+ [0, 1J is 
defined by 
(01 ) = w(O 1\ cp) 
w 
cp 
w(cp)' 
To avoid any problems in conditionals with possibly zero denominators we shall 
adopt the convention in what follows that expressions of the form 
r L aiw(Oilcp) = b, 
i=l 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
205 
stand for 
r 
LaiW(Bi 1\ ((J) = bW(({J). 
i=l 
For the purposes of this paper it will be convenient to formulate the notion of 
a linear knowledge base, i.e. a knowledge base of linear constraints, as in [Paris 
and Vencovska, 1990] rather than adopt the special case simplifications of [Paris, 
1999]. Thus we define a Linear Knowledge Base on SL(Pil , ""Pi,,) to be a finite 
set of constraints, 
{ t aijw(Bi) = bj I j = 1, ... , m } , 
.=1 
where the Bi E SL(Pill ... ,Pi,.) and the aij, bj are real, which is consistent, i.e. 
satisfied by some probability function Wo on SL(Pil' "',Pi,,) (or, equivalently, any 
larger language). 
Let LCL(Pil' ... ,Pi,.) denote the set of such knowledge bases. Notice that if 
{piw",Pin} ~ {pjwÂ·Â·,pj.} then LCL(pill ... ,pd ~ LCL(pjwÂ·Â·,pj'). Let 
LCL = Uil â¢...â¢ inEN LCL(Pil' ... ,pin). 
Define an inference process to be a function N such that for any finite non-
empty subset {Pil' "',Pi,,} of Land K E LCL(Pil' "',Pi,,), N( {Pil , ... ,Pin}, K) 
is a probability function on S L(pil , ... , Pin) which satisfies K. 
In what follows we shall be particularly interested in the Maximum Entropy In-
ference Process, ME, which is defined as follows: Given a language with proposi-
tional variables {Pill""Pi,,}, K E LCL(Pill ... ,Pi,.), let ai, ... , a2n run through 
the atoms of SL(Piw .. ,Pi,.). Then M E( {Pill ... ,Pin}, K) is that probability 
function won SL(Pil' ... ,Pi,.) satisfying K for which the entropy, 
2n 
E(w(al), w(a2)' ... , w(a2n)) = - L w(ai) logw(ai), 
i=l 
is maximal. 1 
Following [Paris and Vencovska, 1990] the 'common sense' principles of un-
certain reasoning (for linear know ledge bases) referred to above may now be stated 
formally as desiderata on an inference process N defined on LC L. With an eye on 
future extensions the formulation of the principles given here will at times differ 
slightly from that given in [Paris and Vencovska, 1990] and [Paris, 1999]. 
I In what follows we shall often use concavity of E, i.e. the property that for ii, b E lIlln, E( kl ii + 
k2b) ~ klE(ii) + k2E(b) whenever kl.k2 ~ 0 and kl + k2 = 1, the inequality becoming strict 
whenever both kl and k2 are (moreover) non-zero. 

206 
1. B. PARIS & A. VENCOVSKA 
Irrelevant Information Principle 
LetKl E LCL(Pil>Â·Â·Â·,pd, K2 E LCL(Ph, ... ,Pi,J with {il, ... ,in } n {iI,Â·Â·Â·, 
jm} = 0. Then for 8 E SL(Piw .. ,pd, 
The principle of Irrelevant Information as presented here provides use with a 
very valuable simplification. Namely, by taking K2 to be empty we see that for 
N satisfying this principle N ( {Pi! , ... , Pin' Pi! , ... , PiTn }, K 1) (0) does not depend 
on the particular overlying language {Pi!, ... ,Pin ,Ph, ... ,PiTn} chosen (a property 
known as Language lnvariance in earlier papers [Paris, 1994], [Paris and Ven-
covska, 1989], [Paris and Vencovska, 1990], [Paris and Vencovska, 19971). Since 
we are interested in inference processes satisfying this principle, we shall hence-
forth therefore omit explicit mention of the argument {pi! , ... , Pin} of N whenever 
this does not cause confusion. 
Equivalence Principle 
If K l , K2 E LCL(Pi17 ... ,pin) are equivalent in the sense that a probability func-
tion w satisfies Kl just if it satisfies K2, then N(Kt} = N(K2). 
In other words N may be thought of as a choice function on the sets V(K) ~ 
IIJ)n of probability functions satisfying K. 
Renaming Principle 
Let aI, ... , a2n and /31, ... ,/32n be the atoms of SL(Pil>Pi2' ... ,PiJ and SL(Pil> 
Ph, Â·Â·Â·'Pin) respectively and let Kl E LCL(Pil>Pi2' ... ,Pin), K2 E LCL(Pil> 
Ph, ... , Pin) be respectively 
Relativization Principle 
Supposethatcp E SL(piw .. ,PiJ and Kl,K2 E LCL(Piw .. ,pin) arerespec-
tively the sets of constraints2 
{t. a;jw(6, A "') ~ b;li d, """' m} 
U {w(",) ~ cj, 
2 According to what looks clearest we shall sometimes specify sets of constraints, as here, as actual 
sets whilst on other occasions simply listing their elements. 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
207 
{t d;;w(,p; A ~<p) = e;ii = 1, ... , 8} 
U {w(~<p) = 1 - cj. 
Then for () E SL(Pil' .",PiJ, N(Kd((} 1\ cp) = N(KI U K2)((} 1\ cp). 
Obstinacy Principle 
If K 1 , K2 E LCL(pil' ... ,pd and N(K1) satisfies K2, then N(K1) = N(KI U 
K2)' 
Independence Principle 
Let K E LC L be the set of constraints 
{W(PI 1\ P2) = a, W(PI 1\ P3) = b, w(pd = c} 
wherec> O. ThenN(K)(PlI\P2I\P3) = ~b. 
Continuity Principle 
If K, Km E LCL for mEN and limm-too 8(V(Km), V(K)) = 0, where 8 is 
the (Blaschke) distance between non-empty subsets X, Y of IDln defined by 
8(X, Y) = inf{8 > Ol'v'x E X3y E Y Ix - yl ~ 8 and 
'v'y E Y3x E X Ix - yl ~ 8}, 
then limm-too N(Km) = N(K). 
Informal justifications for these principles are given in [Paris and Vencovska, 
1990] and [Paris, 1999]. All these principles hold for ME. Indeed, by combining, 
and sharpening slightly, the results in [Paris, 1999], [Paris and Vencovska, 1990], 
[Paris and Vencovska, 19971. [Paris and Vencovska, 1996] we have the following 
result characterizing ME (for three alternate characterizations with a similar 'ax-
iomatic' flavour see the work of Shore and Johnson, [Shore and Johnson, 1980], 
Csiszar, [Csiszar, 1989], and Kern-Isberner [Kern-Isberner, 19981). 
THEOREM 1. Let N be an inference process which satisfies the principles of 
Irrelevant Information, Equivalence, Renaming, Relativization, Obstinacy, Inde-
pendence and Continuity on LC L. Then N agrees with ME on LC L. 
Put another way this theorem says that in the case where an agent's knowledge 
base is in LC Land () E S L then there is one and only one belief value (probability) 
that the agent can assign to () which is consistent with the agent acting (as an 
inference process) common sensically. 
Interesting as this result might be it is certainly not beyond criticism (see for 
example [Paris and Vencovska, 19971). In particular some might feel that it applies 
to an excessively limited class of knowledge bases. It is true, as detailed further 
in [Paris, 1999] for example, that when asserting 'the working knowledge they 
use' experts usually express it in the form of linear constraints, indeed in the even 

208 
J. B. PARIS & A. VENCOVSKA 
simplerformsw(B) = borw(BIÂ¢) = b. Howeveronemightobjectthat in practice 
an expert is surely aware of other forms of constraints, in particular constraints 
expressing stochastic independences, and that our conclusion would have more 
relevance if it could be generalized to this case. In the sections that follow we 
shall look at what happens if we widen our knowledge bases to include also such 
non-linear constraints. 
2 STOCHASTIC INDEPENDENCE 
Our first observation on this goal of widening our knowledge bases to include 
also constraints expressing independence is that if we do it naively then 'common 
sense' proves false! The following example originates with Paul Courtney, see 
[Courtney, 1992] or [Paris, 1994]. 
A certain jam factory receives jars and lids from two manufactures, 
C and D say, each individually supplying the same number of jars as 
lids. At the factory all the lids and all the jars from the two manu-
facturers go to two independent input points. Unfortunately due to 
some oversight years ago the jars and lids from manufacturer Care 
marginally smaller than those from manufacturer D so that if, at the 
final capping, a lid from one manufacturer is put on a jar from the 
other manufacturer it leaks, visibly. Wise old Professor Marmalade is 
well aware of all this from his years of experience moon-lighting as 
a cleaner on the factory floor, and he knows that the probability that 
anyone pot of jam will leak is 3/8. So what probability should he 
give to the next bottle coming down the production line being of the 
larger variety? Clearly from what he knows there is complete sym-
metry here between 'larger' and 'smaller', so in accord with the spirit 
of the principles (precisely the Renaming Principle) he should give 
the same probability to it being larger as he would give to it being 
smaller. So he has to give the answer 112. Unfortunately this answer 
is not actually consistent with his knowledge base! 
In fact the constraints as given are satisfied by just two probability functions, 
which give the probabilities 114 and 3/4 respectively to 'next bottle coming down 
the line is large' and there is no solution which gives this event probability 112. 
Nor, in view of the large/smaller symmetry involved, is it reasonable to hope that 
appealing to some further common sense principle could break the deadlock. The 
fact is that there are only two solutions (even without invoking common sense) and 
there is no rational way to pick between them. If wise old Professor Marmalade 
had done the calculations and was required to give a figure it is hard to see that he 
could improve upon tossing a fair coin to pick between the two options. In sum-
mary then, on the basis of this one contrived example it seems that if we allow in 
also non-linear constraints then 'common sense' may only be capable of cutting 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
209 
down the possibilities to within a number of different, but equally acceptable or 
reasonable, choices. This being the case it seems we should therefore relax our re-
quirement that inference processes are single valued, allowing them instead to take 
as value a non-empty set of probability functions and consider again appropriate 
common sense principles. 
In the sections that now follow the plan is to tread precisely this path with the 
intention of deriving an analogous result to Theorem 1. 
3 EXTENDED INFERENCE PROCESSES AND COMMON SENSE 
PRINCIPLES 
We define an Extended Knowledge Base on SL(Pill ""PiJ to be a finite set of 
constraints, 
{ li(w(Od, ... , w(Om)) = 0 I j = 1, ... , r }, 
where the Oi E SL(pill ""PiJ and the Ii are polynomials with real coeffi-
cients, which is consistent, i.e. satisfied by some probability function Wo on 
SL(Pill ""PiJ (or, equivalently, any largerlanguage). We shall use ECL(pill ... , 
Pin) to denote the set of such knowledge bases. For K E ECL(Pil' "',Pin), let 
V{P'l ' ... 'P'n}(K) denote the set of probability functions on SL(Pill ... ,PiJ sat-
isfying K. When it is clear from the context we shall omit the specification of 
{Pil, "',Pin} and write simply V(K). 
LetECL = Ulo ... ,inENECL(Piw",PiJ. 
Now define an extended inference process to be a function N such that for any 
finite non-empty subset {Pill"" Pin} of Land K E EC L (Pill"" PiJ, 
N ( {Pil , ... , Pin}' K) is a non-empty set of probability functions on S L (Pil , ... , Pin) 
each of which satisfies K. 
As in the linear case we shall formulate desirable, common sense, properties of 
extended inference processes as (extended) principles. In this it is useful to think 
of N as the property of some agent and N( {Pill ... ,Pin}, K) as an agent's choice 
of most reasonable, or acceptable, or preferred solutions to the constraints K. The 
intention of each of the principles is that they place restrictions on this choice of 
most preferred solutions which the agent should obey if s/he is to be considered to 
be acting 'in accord with common sense'. 
Prior to introducing these principles, the following terminology will be use-
ful. For M a set of probability functions on SL(Pkll ... , PkJ, {Pill ... , Pin} ~ 
{Pkw",Pk r } and r.p E SL(Pkw,Â·,PkJ let M r {Pill ... ,pin
} and M r r.p de-
note the set of probability functions from M restricted to S L (Pill.'.' PiJ and the 
set of probability functions from M restricted to {O 1\ r.p I 0 E SL(Pkll .. "PkJ} 
respectively. 
Irrelevant Information Principle 
Let KI E ECL(PillÂ·Â·Â·,PiJ, K2 E ECL(pjll'Â·Â·,Pjm) with {iI, ... , in}n{JI, ... , 
jm} = 0. Then 

210 
J. B. PARIS & A. VENCOVSKA 
The justification for this principle is (as in the LC L case given in [Paris, 1999]) 
that since the knowledge provided by K2 refers to an entirely disjoint language it 
should be irrelevant as far as the restriction of one's (i.e. a common sense agent's3) 
chosen preferred probability functions on {Pi!, "',Pin} are concerned. 
As in the linear case it follows that an extended inference N process satisfy-
ing this principle is language invariant, i.e. N ( {pi! , ... , Pin' Pj! , ... , Pjm }, K d r 
{pit> "',Pin} does not depend on the particular overlying language {Pit> .. Â·,Pin, 
Ph, ... , Pjm } chosen. Consequently we omit the first argument of N whenever this 
does not cause confusion. 
Equivalence Principle 
If K1, K2 E ECL(Pi!, ""PiJ are equivalent in the sense that a probability func-
tion w satisfies K1 just if it satisfies K2, then N(K1) = N(K2)' 
The justification for this principle is that it is common sense that one's choice 
of preferred solutions should depend only on the choices available and not on how 
they are presented (packaged!). 
Notice that for N satisfying Equivalence and K E EC L(Pi!, Pi2' ... , Pin) N is 
essentially a choice function on the subsets V(K) ofJD)n. Consequently when X ~ 
JD)n and we know there is a K E ECL such that V(K) = X, we may sometimes 
write N(X) in place of N(K). Related to this point we shall sometimes use 
w to denote an element of N(K) (when we want to think of N(K) as a set of 
probability functions) and sometimes as w (when we want to think of N(K) as a 
subset of JD)n). 
Henceforth we shall assume that N satisfies Irrelevant Information and Equiv-
alence without further mention. 
Renaming Principle 
Let 0:1, ... , 0:2 n and (31, ... , (32n be the atoms of S L (Pit> Pi2 , ... , PiJ and S L (Ph, 
Ph,Â· .. ,pjJrespectivelyandletK1 E ECL(pil'Pi2"",PiJ,K2 E ECL(pj!, 
Ph, ... , PjJ be respectively 
{ f;(w((3d, ... , W((32n)) = 0 I j = 1,2, ... , r}. 
Then w E N(Kd {:} wa E N(K2), where a is the bijection from the atoms 
of SL(pjl' .. Â·,PjJ to the atoms of SL(Pil' ""PiJ given by a((3k) = O:k for 
k = 1, ... ,2n. 
3In what follows 'one' should be thought of as just a personalized corrunon sense agent. 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
211 
The justification for this principle is that a is no more than a renaming of 'pos-
sible worlds' and one's choice of preferred solutions to K should not be affected 
by this immaterial change. 
In applying the Renaming Principle (and certain later principles) it is useful to 
notice that by (1) any set of constraints in EC L is equivalent (in the sense of having 
the same solutions) to a set of constraints only involving w(a) for a atoms from 
some finite sub language of L and, by our standing assumption of Equivalence, N 
agrees on these two sets of constraints. 
For applications of Renaming it is also useful to introduce a little notation. For 
K E ECL(PillPi2' ... ,pd, 
K = {fJ(w(ad, ... ,w(a2n)) = 0 Ii = 1,2, ... ,r} 
and a a permutation of {I, ... , 2n} let 
aK = { fJ(w(au(l)), ... , w(au(2n))) = 0 Ii = 1,2, ... , r}. 
Similarly for u = (Ul, ""U2n) E lIJ)n let au = (uu(l), ""Uu(2n)). With this 
notation in place Renaming gives that 
au E N(K) Â¢> U E N(aK). 
It turns out that in the extension from LC L to EC L Renaming has actually lost 
a lot of its former power, a point we return to in the concluding section of this 
paper4. 
Relativization Principle 
Supposethat<p E SL(pil'"",PiJ and Kl,K2 E ECL(Pil'"",PiJ arerespec-
tively the sets of constraints5 
{fJ(w((h /\ <p), ... ,w(Br /\ <p)) = 0 I j = 1, ... ,m} U {w(<p) = e}, 
{gj(W('l/Jl/\ -'<p), ... ,w('l/Jt /\ -'<p)) = 0 I j = 1, ... ,s} U {w(-'<p) = 1- e}. 
Then N(Kd r <p = N(Kl U K2) r <po 
Notice that (for e > 0) we can equivalently express Kl as 
{fJ(ew(B11<p), ... , ew(Brl<P)) = 0 I j = 1, ... , m} U {w(<p) = e}, 
and similarly for K 2 . This leads to the justification for this principle, namely 
that conditioned on <p both Kl and Kl U K2 contain the same knowledge so, 
conditioned on <p, the choices of preferred solutions should also agree. 
4However as Lemma 5 will shortly show enough remains that we can still derive the classical 
'principle of indifference' . 
5 In expressions like these we may, for the sake of elegance, write w ( 'P) = c rather than the formally 
correct w( 'P) - c = 0 etc .. 

212 
1. B. PARIS & A. VENCOVSKA 
Obstinacy Principle 
If Kb K2 E ECL(Pil, ... ,pd and N(Kd n V(K2) f. 0 then N(Kl U K2) = 
N(Kl) n V(K2)' 
To justify this principle suppose on the contrary that it failed. First consider the 
case that there was a preferred solution w of Kl which satisfied K2 but was not a 
preferred solution of Kl U K2. Let w' be a preferred solution of Kl U K2. Then 
one would have the situation where w was at least as preferred as w' as a solution 
of Kl and both satisfied K2, but when K2 was explicitly stated this preference 
was lost! The other case is similar. Suppose there was a preferred solution w' of 
Kl U K2 which was not a preferred solution of Kl. Let w be a preferred solution 
of Kl which satisfied K2. Then similarly as before w would be strictly preferred 
to w' as a solution of Kl and both satisfied K2, but when K2 was explicitly stated 
this strict preference was lost! 
Put another way, learning something one already believed should not cause one 
to change one's beliefs! 
In what follows the following immediate consequences of Obstinacy (and of 
our standing assumption of Equivalence) will be useful. Firstly, if w E N(Kl) 
satisfies K2 then wE N(Kl U K2)' Secondly, ifV(Kl ) 2 V(K2) and N(Kl ) n 
V(K2) f. 0 then N(K2) = N(Kl) n V(K2)' 
Independence Principle 
Let K E EC L be the set of constraints 
{w(P1AP2) = a, w(P1AP3) = b,w(pd = c} 
wherec> O. Then for w E N(K),W(PIAP2 A P3) = ~b. 
Notice that this set of constraints K is equivalent to 
The justification for this principle is that the knowledge provides no interrelation-
ship between the conditional beliefs in P2, P3 given PI (nor any other relationships) 
and in consequence P2 and P3 should be treated as independent (i.e. stochastically 
independent) given Pl. 
In [Paris, 1999] a somewhat weaker, and possibly more easily acceptable, form 
of this principle was given for the linear case. We would surmise that at the cost 
of further complicating the proof a similar weakening would also suffice here. 
Continuity Principle 
If K, Km E ECL, ilm E N(Km) for mEN, limm-too 8(V(Km), V(K)) = 0 
and limm-too ilm = il then il E N (K). 
The justification for this is that the property of being a preferred solution should 
not die in the limit. Notice that this is not the same as saying that if il E V(K) is 
a preferred solution of K and K' is 'close to' K then K' should have a preferred 
solution close to il. Indeed it will follow easily from the main theorem of this paper 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
213 
that we cannot possibly hope to have both in general for non-linear constraints. [It 
does hold in the linear case.] 
We end this section by proving some basic properties which hold for any ex-
tended inference process N satisfying the principles of Irrelevant Information, 
Equivalence, Renaming, Relativization, Obstinacy and Independence. At this 
point the first-time reader may wish to jump to the next section and refer back 
to these lemmas (and the notion of an N-solution given prior to Lemma 4) as and 
when required. 
For the rest of this section assume that N satisfies these principles. 
LEMMA 2. Assume that K E ECL(Piw .. ,PiJ, p = (PI, ... , p2n) E N(K) and 
j, k, l, mE {I, ... , 2n} (j, k, l, m distinct) are such that 
V(KU{w(ai) = Pi 1 i ~ {j, k, l, m}}) = V(KIU{ w(ai) = Pi 1 i ~ {j, k, l, m}}), 
where KI is the set of constraints 
and c = Pj + Pk + PI + Pm, a = Pj + Pk, b = Pj + PIÂ· Then PjPm = PkPIÂ· 
Proof. Without loss of generality we may assume j, k, l, m are 1,2,3,4 respec-
tively and the iI, i2 , â¢â¢â¢ , in are 1,2, ... , n. By a remark following the introduction of 
the Obstinacy Principle for extended inference processes, pEN (K U {w (ai) = 
Pi 14 < i :::; 2n}) so p E N(KI U {w(ai) = Pi 14 < i :::; 2n}). Now consider K2 
consisting of constraints (in the extended language L(PI' P2, ... , p2nÂ», 
w(""pj /\pd = 0, w(""pj /\ ""PI /\P2 /\P3) = Pi> 
4:::; j :::; 2n , 
W(""PI /\ ""Pi /\ ""pj) = 0, 4:S i < j :S 2n. 
By Irrelevant Information there is a probability function in 
whose values on the aj are Pj, 1 :S j :::; 2n. But there is a clear renaming of the 
atoms which sends this set of constraints to K 2 and hence using the Renaming 
Principle we see that 
PI 
w(P1 /\P2 /\P3 /\ 1\ Pj), 
4~j9n 
P2 
W(PI /\ P2 /\ ""P3 /\ 1\ Pj), 
4~j9n 
P3 
W(PI /\ ""P2 /\ P3 /\ 1\ Pj), 
4~j~2n 

214 
1. B. PARIS & A. VENCOVSKA 
P4 
W(P1 A 'P2 A 'Pa A 1\ Pj), 
4::Sj$2n 
1\ 
4::Sj$2n,#r 
for some w E N(K2 ). Now using Relativization (with c.p = P1) and Equivalence, 
we can see that the same is true if we replace K2 by K a, where Ka is obtained 
from K2 by replacing w( 'Pj 1\ 'P1 A P2 A pa) = Pi by w( 'Pj A ,pt} = 0 for 
4 ~ j ~ 2n and dropping w ( 'P1 A 'Pi A 'Pj) = 0 for 4 ~ i < j ~ 2n, i.e. taking 
Ka = {W(P1 Ap2) = a, W(P1 Apa) = b, w(pt} = c, w('Pj) = 0 14 ~ j ~ 2n}. 
Using Irrelevant Information (to ignore the w( 'Pj) = 0) and then Independence 
gives (provided P1 + P2 + Pa + P4 f:. 0) 
(P1 + P2)(P1 + Pa) 
P1 = 
, 
P1 + P2 + Pa + P4 
which simplifies to P1P4 = P2Pa (either way), as required. 
â¢ 
LEMMA 3. Let K1 
E ECL(pil' Â·Â·.,PiJ, K2 
E ECL(pjw",PiTn) with 
{i1, ... ,in} n {j1, ... ,jm} = 0. Let a1,Â·Â·Â·,a2" and {31, ... ,{32Tn be the atoms of 
SL(Pil' ""PiJ and SL(Pjl' "',Pj",) respectively. so ai A {3j (i = 1, ... , 2n, j = 
1, ... , 2m) are the atoms OfSL(Pil' ""Pi",PiI' "',Pj",)' Then i1 = (uu, ... ,U12Tn, 
... , U2"2"') is in N(K1 U K 2) just if there are p = (P1, ... , P2") E N(K1) and 
f = (T1, ... , T2"') E N(K2) such that 
Uij = PiTj, 
i = 1, ... ,2n, j = 1, ... ,2m, 
where Pi, Tj and Uij pertain to ai. {3j and ai A {3j respectively. 
Proof. Assume i1 E N (K1 U K 2)' Then by Irrelevant Information, 
is equal to some p E N(Kd and 
is equal to some f E N(K2)' By a remark following the introduction of the 
Obstinacy Principle for extended inference processes, i1 is in N(K) where K is 
obtained from K1 U K2 by adding the constraints 
2"' 2: w(ai A {3j) = Pi, i = 1, ... , 2n, 
j=1 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
and the constraints 
2n L W(ai 1\ (3j) = Tj, 
j = 1, ... , 2m , 
i=1 
w(ail\{3j) =Uij (i,j) Â¢ {(p,r),(q,r),(p,s),(q,s)}, 
215 
where p, q E {I, ... , 2n }, r, s E {I, ... , 2m }, p t- q, r t- s are fixed, but arbitrary. 
By Lemma 2 it follows that upruqs = uqrups , which together with 2:~:1 Uij = Pi 
and 2:;21 Uij = Tj yields that Uij = PiTj, as required. 
Conversely, let p E N(Kd and f E N(K2). By Obstinacy and Irrelevant 
Information, 
2m 
N(K1 U K2 U {L W(ai 1\ (3j) = Pi Ii = 1, ... , 2n}) 
j=1 
is equal to 
2m 
N(K1 U K 2 ) n {W I L W(ai 1\ (3j) = Pi, i = 1, ... , 2n}. 
j=1 
By Equivalence, the former is 
which by Irrelevant Information must contain a w satisfying 
2n 
(2) L w(ai 1\ (3j) = Tj, 
j = 1, ... , 2m. 
i=1 
Consequently, N(K1 U K 2) contains w satisfying both (2) and 
2m L W(ai 1\ (3j) = Pi, i = 1, ... , 2n , 
j=1 
and just as above this w can be shown to satisfy w( ai 1\ (3j) = PiTj. 
â¢ 
Paralleling the development in [Paris and Vencovska, 1990] we now introduce 
some matrix notation which will considerably simplify later proofs, in particular 
that of the first key lemma, Lemma 7. [In the proof of the main Theorem 18 
there will be three key lemmas, Lemmas 7,15,16, each of which can be seen as 
successively stronger special cases of the theorem.] 
First note that any K E LC L corresponds to some system of linear equations 

216 
J. B. PARIS & A. VENCOVSKA 
where B is a m x 2n-matrix with real coefficients such that BillT = 17 forces 
E;:l Wi = 1 and there is some ill ~ 0 satisfying these equations. 
Conversely suppose that B is an m x r matrix such that the system of equa-
tions BillT = 17 forces E~=l Wi = 1 for all solutions ill with ill ~ 0 and at 
least one such solution exists. Let aj), ... , ajr be some (distinct) atoms of some 
SL(pil' "Â·PiJ. Then the set K of constraints 
is in LCL and the set of solutions (w(aj)), ... , w(ajJ), for w E N(K), of 
BillT = 17 is, by Irrelevant Information, Equivalence and Renaming, independent 
of the particular atoms ajl' ... , ajr and overlying language L(pil' ... Pin ) chosen. 
We call these solutions (unambiguously) the N-solutions of BillT = 17. Notice 
also that if in addition Wk = 0 for all solutions ill of BillT = 17 with ill ~ 0 then 
all N -solutions will have kth coordinate 0 and the N -solutions with this coordinate 
.... T 
tr 
removed will be precisely the N -solutions of the system of equations B' w' = b 
where B' is the m x (r - 1) matrix formed by removing the kth column from B 
and ;;, is ill with the kth coordinate omitted. We shall use these facts repeatedly 
and without further mention in what follows. 
The next lemma shows that N satisfies the extension to this context of Shore 
and Johnson's System Independence property, see [Shore and Johnson, 1980]. 
LEMMA 4. Suppose that ill = (WI, ... , Wk) and Z = (Zl' ... , zr) and that the 
system of linear equations 
( B 0) .... 
T 
........ T 
(3) 
0 
C 
(w,Z) 
= (b,d) 
implies that E~=l Wi = a and E~=l Zi = 1 - a for some 1 > a > O. Suppose 
further that (3) has a solution with ill ~ 0, z ~ O. Then X is an N -solution of 
if and only if there is some g such that aX, g is an N -solution of (3). 
Proof. Letn be such that 2n ~ k+r and letal, ... , a2 n be atoms of SL(Pl, ... ,Pn)' 
Then (after discarding zeros) (4) corresponds to Kl defined by 
W( ai) = 0 for i = k + 1, ... , 2n , 
and (3) corresponds to K defined by 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
217 
W(lli 1\ ""'PnH) = 0 for i = 1, ... , k, 
W (lli 1\ PnH) = 0 for i = k + 1, ... , k + r, 
Note that B(W(lll 1\ Pn+l), ... , W(llk 1\ Pn+d)T = bT implies L~=l W(lli 1\ 
PnH) = W(PnH) = a. Let Â¢ = V~=l lliÂ· We need to prove that 
(where the atoms of SL(Pl, ... ,Pn) and SL(Pl, ... ,Pn,Pn+l) are assumed to be or-
dered as lll, ... , ll2n and III I\Pn+l, ... , ll2n I\Pn+l, III 1\ ""'Pn+l, ... , ll2n 1\ ""'Pn+l 
respectively). In other words, (5) amounts to saying that there is some f = 
(Tl, ... , T2n) E N(K1) such that Ti = Ai for i = 1, ... , k if and only if there is 
some p = (PI, ... , P2n+1) E N(K) such that Pi = aAi for i = 1, ... , k. 
Let K2 = Kl U {W(PnH) = a}. By Lemma 3, 
X E N(Kd iÂ¢ {:} aX E N(K2) iÂ¢ 1\ Pn+lÂ· 
Consequently, any p E N(K2 ) satisfies 
B( PI 
Pk)T = -lbl'T 
,'0', 
a, 
a 
a 
i.e. B(Pl, ... , Pk)T = 17. It follows, by Obstinacy, that N(K2) = N(K3 ), where 
K 3 consists of K 2 and 
Now K3 is equivalent to 
By Equivalence and Relativization, N(K3) iÂ¢ I\PnH = N(K) iÂ¢ I\Pn+l. The 
result follows. 
â¢ 

218 
1. B. PARIS & A. VENCOVSKA 
Our next result shows that N satisfies Laplace's 'principle of indifference'. 
LEMMA 5. Assume that K E ECL(Pil' "',Pin) is of the form 
{ Ii ( L w(a), ... , L w(a)) = 0 1 j = 1, ... , m } , 
nEAl 
nEAr 
where the A1, ... , Ar form a partition of the atoms of SL(pill ... ,PiJ. Thenfor 
any wE N(K), w(a) = w(a') whenever a and a' are in the same Aj . 
Proof. We first prove a special case of this lemma, namely that 
N({P1},0) = {(1/2,1/2)}. 
Suppose (a, 1 - a) E N( {pd, 0). Then by Lemma 3 there is wE N( {P1,P2}, 0) 
such that w(pd = W(P2) = a and W(P1 /\ P2) = a2, W(P1 /\ 'P2) = a(l - a) = 
w( 'P1 /\ P2), w( 'P1 /\ 'P2) = (1 - a)2. By Renaming then there is also W1 E 
N( {P1,P2}, 0) with W1 (P1/\P2) = a2, W1 (P1/\ 'P2) = (1-a)2, W1 ('P1/\P2) = 
W1 ('P1 /\ 'P2) = a(l - a). Hence W1 (P1) = a2 + (1 - a)2, W1 (P2) = a2 + 
a(l- a) = a. Again by Lemma 3 we must have W1 (P1/\p2) = a(a2 + (1- a)2). 
But we already know that W1 (P1/\P2) = a2 so this forces a to be one of 0,1/2, l. 
To see that a Â¥ 1 (and hence by Renaming that a Â¥ 0) suppose on the contrary 
a = 1. Then by Lemma 3 again there is W2 E N( {P1,P2}, {W(p2) = 1/3}) with 
W2(P1/\P2) = 1/3, W2(P1/\ 'P2) = 2/3, W2('P1/\P2) = W2('P1/\ 'P2) = o. 
By Renaming there is a W3 E N( {P1,P2}, {W(P2) = 1/3}) with W3(P1 /\ P2) = 
1/3, W3('P1/\ 'P2) = 2/3, W3(P1/\ 'P2) = W3('P1/\p2) = o. But this means 
that by Irrelevant Information 
(w3(pd,W3('P1)) = (1/3,2/3) E N({pd,0), 
which we have already shown is impossible. Thus N( {P1}, 0) = {(1/2, 1/2)}. 
To now prove the general case as stated in the lemma let f E N(K) and with-
out loss of generality let a1, a2 be distinct elements of A1 where a1, ... , a2n run 
through the atoms of SL(Pil' ... , PiJ. Clearly it is enough to show that 71 = 72. 
Let K 1 E LC L (Pil , ... , Pin) be the set of linear constraints 
{w(a1) + W(a2) = 71 + 72} U {w(aj) = 7j 13 ~ j ~ 2n}. 
By Obstinacy f E N(Kd and by Lemma 4 there is a (A1, A2) E N( {pd, 0) such 
that 
(71 + 72)(A1,'\2) = (71,72), 
But as we have already shown the only possibility here is '\1 = 1/2 = '\2, giving 
71 = 72, as required. 
â¢ 
In the next section we shall show that the principles of Irrelevant Information, 
Equivalence, Renaming, Relativization, Obstinacy, Independence and Continuity 
collected in this section are consistent in the sense that there is at least one extended 
inference process satisfying them. 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
219 
THE EXTENDED MAXIMUM ENTROPY PROCESS (EM E) 
The Extended Maximum Entropy Process (EM E) is defined as follows: if K E 
ECL(PiI, Â·Â·Â·,PiJ and 0:1, ... , 0:2 n run through the atoms of SL(Pil' ... ,PiJ, then 
EM E( {Pill .. Â·,Pin}, K) is the set of probability functions won SL(Pill ... ,PiJ 
satisfying K for which the entropy E(W(0:1)' ... , W(0:2n)) is maximal. Note that 
the V{Pil , ... ,Pin}(K) are non-empty and compact, so the EM E( {Pil , ... ,Pin}, K) 
are well defined and non-empty. However, unless V{Pi! "",Pin} (K) is convex, 
EM E( {Pi!, ... , Pin}' K) may contain more than one point. 
THEOREM 6. EM E satisfies the Irrelevant Information, Equivalence, Renam-
ing, Relativization, Obstinacy, Independence and Continuity Principles. 
Proof. Irrelevant Information: This is proved like the corresponding property 
for ME, see [Paris, 1994]. Using the notation of the statement of the principle 
but with EM E in place of N assume that p = (P1, ... , p2n) E EM E(Kd, f = 
(T1, ... , T2m) E EM E(K2) and 1 = (-Y11, ... , 'Y12m, ... , 'Y2n2m) E EM E(K1 UK2) 
where the atoms are ordered in the obvious way so that the atom for 'Yij corre-
sponds to the conjunction of the atoms for Pi and Tj. Notice that (P1 T1, P1 T2, 
Â·Â·Â·,P1T2m,P2T1,Â·Â·Â·,P2nT2m) 
E V(K1 U K2). (-Y1., ... ,'Y2 n .) E V(K1) and 
("(.1, ... ,'Y.2m) E V(K2). where'Yi. = L,j'Yij and'Y.j = L,i'Yij. It suffices to 
prove that 
E(iJ) = E(-yl., ... ,'Y2n.), 
E(P1 T1, ... , P1 T2m, ... , P2n T2m) = E( 1). 
But this follows from the assumed properties of p, f, 1 since they give 
- L L PiTj 10g(PiTj) 
j 
- L Pi log Pi - LTj 10gTj 
j 
> - L 'Yi. log 'Yi. - L 'Y.j log 'Y.j 
j 
- L L 'Yij log 'Yij 
j 
~ 
[ 'Y' 
( 'Y' )] 
+ ~ 
'Yi. 'Y.j 
_tJ_ log 
_tJ_ 
i,j 
'Yi. 'Y.j 
'Yi. 'Y.j 
> - L L PiTj 10g(PiTj), 
j 
using the fact that by convexity of x log x on [0, 1] 
~ . . [....::t.i.L 10 (....::t.i.L)] > 
i..Ji,j 'Yt. 'Y.J 'i. r.j 
g ,i. r.j 
-
(L,ij 'Yi. 'Y.j ,7~j ) log (L,ij 'Yi. 'Y.j ,7~.j) = 10g(1) = o. 

220 
J. B. PARIS & A. VENCOVSKA 
Equivalence, Renaming and Obstinacy are immediate. 
Relativization: Without loss of generality, assume that cp 
Vf=l ai. For all 
(P1, .. Â·,Pp), (p~,,,,,p~) E V(K1) rcpandall('Tp+1, ... ,'T2 n ), 
and similarly for all (P1, ... ,Pp), 
(p~, ... ,p~) E V(K1 U K 2) r cp and all 
('Tp+1, ... ,'T2n), 
(P1, ... ,Pp,'Tp+1, ... ,'T2n) E V(K1 UK2) Â¢} 
(p~, .. Â·,p~,'TP+1, .. Â·,'T2n) E V(K1 UK2). 
It follows that maximizing entropy in V(K1) or in V(K1 U K2) amounts (in either 
case) to independently maximizing both 2::f=1 w( ai) log w( ai) and 
2n 
2::i=p+1 w(ai) logw(ai) so since V(K1) r cp = V(K1 U K 2) r cp, 
EME(K1) r cp = EME(K1 U K2) r cp, 
as required. 
Independence: Here we have the same K as in the linear case and working out the 
(unique) solution maximizing the entropy gives the answer. 
Continuity: Let Km, K, urn, u be as in the formulation of the Continuity Principle 
and assume that u Â¢ EM E (K). Then there is some v E V (K) such that E (v) > 
E(u) and consequently by the continuity of E, for large m there must be vm E 
V(Km) such that E(vm) > E(urn), contradiction. 
â¢ 
4 
THE EXTENDED INEVITABILITY THEOREM 
Henceforth let N be an extended inference process satisfying the (extended) prin-
ciples of Irrelevant Information, Equivalence, Relativization, Obstinacy, Indepen-
dence and Continuity. Our plan now is to show the main theorem of this paper 
(Theorem 18) that this determines N uniquely as EM E, a result analogous to 
Theorem 1 but now for this extended class of knowledge bases. 
The proof of Theorem 18 will take us some time and proceeds via a string of 
lemmas. Our first lemma is a special case of Theorem 18. 
LEMMA 7. Ifl isarationallinethenN(l) = {ME(l)}. 
By a line here we understand a set of constraints 1 in LC L of the form 
where il, b E ~n, il Â¥ b, and, as usual, a1, a2, ... , a2n run through the atoms. We 
say that this line is rational if (bj - aj)/(bi - ai) are rational whenever bi Â¥ ai. 
Hopefully without generating any further confusion we shall also on occasion use 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
221 
1 to denote the straight line (segment), i.e set of points in]]))n satisfying l. Note that 
this line passes through the points ii, b. 
The proof of Lemma 7 is rather technical and requires a detailed analysis of 
the proof of Theorem 1 given in [Paris and Vencovskli, 1990], The reader may be 
forgiven for omitting it on the first reading and skipping directly to the preamble 
prior to Lemma 10. A second alternative would be to simply accept the following 
Uniqueness Principle, from which Lemma 7 follows by a somewhat shorter proof. 
Uniqueness Principle 
If 1 is a rational line then N(l) is a single point. 
After all Theorem 1 shows that restricted to linear knowledge bases ME is the 
unique common sense inference process so that in this sense the (unique) point of 
maximum entropy on a line (in]]))n) can be justified as the preferred solution to a set 
of linear constraints specifying that line. Our reason for preferring not to introduce 
this principle as such is that, entirely reasonable as it is, it is actually derivable from 
our other principles and even at the cost of some technical difficulties we would 
wish to minimize our assumptions. 
Proof of Lemma 7 We need to introduce some notation. For 8 = (81 , ... , 8m ) with 
8k Â¥ 0 let 
-8k 
81 
0 
D(8,k) = 
-8k 
8k - 1 
8k+1 
-8k 
0 
8m 
-8k 
That is, D(8, k) is a (m - 1) x m-matrix with entries Eij where 
Eii = -8k 
Eii+1 = -8k 
Eik = 8i 
Eik = 8i+1 
Eij = 0 
for i ~ k - 1, 
for i ~ k, 
for i ~ k - 1, 
for i ~ k, 
otherwise. 
The main step in our proof of Lemma 7 is provided by the next lemma. 
LEMMA 8. Let 8 = (81 , ... , 8m ) be an in!eger vector satisfying L}:1 8j = 0 
and 8k Â¥ 0 and let D(8, k) be as above. Let J be a vector such that the system 
D(8, k)UjT = Jr, Uj ~ 0 has a solution and all solutions of D(8, k)UjT = Jr 
satisfy L}:1 Wj = 1. Then D(8, k)UjT = Jr has a unique N-solution and this 
agrees with the ME-solution. 
Proof. To start with, assume that 8i Â¥ 0 for i = 1, ... , m. The proof is by induction 
onp(8) = L~l 18il- m. 

222 
J. B. PARIS & A. VENCOVSKA 
Case p(8) = O. 
To deal with this case, we introduce for each kEN the 
(2k - 1) x 2k matrix Ek which has entries dij where dlj = 1 for 1 ::; j ::; 2k, 
dii = di (i+1) = 1 for 2 ::; i ::; 2k-l and dij = 0 otherwise. We have El = (1,1), 
E2~ 0 
1 1 n, 
1 
1 
0 
1 
1 
1 1 1 1 
1 
0 
1 1 0 0 
0 
E3 = 
0 0 
1 1 0 
0 
0 0 0 
1 1 0 
0 0 0 
0 
1 1 
and so on. These matrices are useful since our system D(6, k)iif1' = Jr is 
equivalent (up to renaming, see below) to E~ 'liiT = tI' for an appropriate e. 
To see that this indeed is the case, note that D(6, k)'liiT = Jr is equivalent to 
-. 
T 
.... T 
.... 
m 
.... 
D(6,1)'Iii = d' 
for an appropriate d'. Also since Li=l 6i = 0 and p(6) = 0, 
m must be even and half of the 6i 's must be 1 and the other half -1. Assume that 
62i = 1 and 62i- l = -1 for i = 1, ... , If (this assumption can be justified using 
Renaming, as in [Paris and Vencovska, 1990], p.210). Adding all the equations in 
-
T 
-T 
m 
D(6,1)'Iii = d' 
together produces Li=l Wi = 1 and adding the i + 1st row to 
the ith row for i = 1, ... , m - 2 yields the remaining m - 2 rows of E~ 'liiT = tI' 
(where e = (1, e2, ... , em-l). 
Let 1', pbe, respectively, an N -solution and the ME-solution of the system K k 
given by Ek'liiT = tI'. If some Pi = 0 then by the 'Open-mindedness' of ME (see 
[Paris, 1994, p. 95]) Wi = 0 for every solution 'Iii of Ek'liiT = tI' satisfying 'Iii ~ 0 
so Ti = O. But then p, l' with their ith coordinates removed are both solutions of 
Ek 'liiT = tI' where Ek is the regular matrix produced by deleting the ith column 
from Ek so perforce also Pj = Tj for 1 ::; j ::; 2k, j :I i. 
So now assume that p > 0, noting that, in consequence, we must have e > O. 
For k = 1 and k = 2 the required result follows directly from the Lemma 5 and 
Independence Principle respectively. For k ~ 3 the proof will be far less straight-
forward and will lean heavily on the following result from [Paris and Vencovska, 
1990]6. 
LEMMA 9. Let B'liiT = bT be a system of linear equations which has a solution 
'Iii ~ 0 and is such that for any solution 'Iii, Lj Wj = 1. Then the solution ii of 
B'liiT = bT which maximizes the entropy (the ME-solution) is the only solution 
which satisfies 
(i) TI')';~o vT = TI')';$O vi')'; 
for all 1 such that B1T = or (equivalently, 
for all 1 from some set which spans the kernel of B), 
6Lemma 10, p.l97 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
223 
(ii) If B'liiT = tr. 'Iii ~ 0 does not imply Wj = 0 then Vj > 0 (i.e .â¢ in the 
terminology from [Paris and Vencovskd. 19901. iJ is a positive solution of B'liiT = 
tr). 
First notice that by part (i) of this lemma, 
(6) 
P1P3Â·Â·Â·P2k-l = P2P4Â·Â·Â·P2k, 
a fact that we shall shortly need to employ. 
Let Ck be the (2k -1) x (4k - 4) matrix with entries (Ek,F) where F has 
entries hi with hj = 1 for 1 :::; j :::; 2k - 4, fi(i-2) = 1 for 3 :::; i :::; 2k - 2 and 
hj = 0 otherwise. For example, for k = 4 we have 
1 1 1 1 1 
1 1 1 1 1 1 1 
0 
1 1 0 0 0 
0 0 
0 0 0 0 
0 
0 1 
1 0 0 
0 0 
1 0 
0 0 
C4 = 
0 0 0 
1 1 0 0 0 0 
1 0 0 
0 
0 0 0 
1 1 0 0 0 0 
1 0 
0 
0 0 0 0 
1 
1 0 
0 0 
0 
1 
0 
0 0 0 0 0 
1 1 0 0 
0 0 
Assume that c = (1, C2, â¢â¢â¢ , C2k-l) is some vector such that Ck'liiT = tI' has 
solutions with 'Iii ~ 07â¢ By Lemma 2 the following identities hold for any N-
solution of Ck'liiT = tI'. 
(7) 
W2 W 2k+l = W1W3, 
(8) 
W2k+i W 2k+i+l = W1Wi+3 for 1 :::; i :::; 2k - 5, 
(9) 
W4k-4W2k 
W1W2k-l, 
and also 
(10) 
W2 W 4 
W3 W 2k+2, 
(11) 
W2k+i W i+4 
Wi+3 W 2k+i+2 for 1 ::; i ::; 2k - 6, 
(12) W4k-5 W 2k-l 
W2k-2 W 2kÂ· 
Note that the statement that some 'Iii satisfies (7)-(9) can be reformulated as 
IT wp = IT wi'Yj for 1 E A, 
'Yj~O 
'Yj::;O 
where A contains the vectors 
7We use 'Ill as a variable for vectors both of length 2k and of length 4k - 4, relying on the context 
to prevent confusion. 

224 
1. B. PARIS & A. VENCOVSKA 
1'1 1'2 1'3 1'4 1'5 ... 1'2k-2 1'2k-l 1'2k 1'2k+! 1'2k+2 1'2k+3 ... 1'4k-5 1'4k-4 
1 -1 1 0 
0 
0 
0 
0 
-1 
0 
0 
0 
0 
1 
0 
0 1 0 
0 
0 
0 
-1 
-1 
0 
0 
0 
1 
0 
0 
0 1 
0 
0 
0 
0 
-1 
-1 
0 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
-1 
-1 
1 
0 
0 
0 
0 
0 
1 
-1 
0 
0 
0 
0 
-1 
There are 2k - 3 vectors in A, they are all linearly independent and they all satisfy 
CdT = QT, so they span the kernel of Ck. By Lemma 9, CkwT = tr and (7)-(9) 
determine a unique positive solution. 
Let ql, ... , q2k-4 be values calculated from (7) and (8) under the assumption that 
w = (PI, ... , P2k, ql , ... , q2k -4) satisfies them. We obtain 
-
P2P4 .. Â·P2i+2 
_ PIP3 .... P2i+! 
c 
1 < . < k 
2 
q2i -
, q2i-l -
lor 
_ z _ 
-
. 
P3P5 .. Â·P2i+! 
P2P4 .. Â·P2i 
Then (9) is also satisfied since p satisfies (6). Let H = 1 + l:;!~4 qi and let 
.... _ / 
e2 
e3 + ql 
e2k-2 + q2k-4 e2k-1) 
c - \ 1, H' 
H 
' ... , 
H 
' H 
. 
Note that all entries in c are strictly positive, we will refer to them as (1, C2, ... , 
) Th 
.... -
1 ( 
) . 
(p .. )8 I' 
f C 
.... T_ 
C2k-lÂ· 
enJ.L- H Pl, .. Â·,P2k,ql, .. Â·,q2k-4 lsa oSltlve soutlono 
kW 
-
tr which satisfies (7)-(9). Below, we show that any N-solution of CkWT = tr 
has all entries strictly positive (and thus all N-solutions are positive in the sense 
of [Paris and Vencovska, 1990]). It follows that CkWT = tr has the unique N-
solution [1. By Obstinacy the system 
C .... T 
.... T 
kW 
= C , 
W2k+i = ; 
for 1 ~ i ~ 2k - 4, 
also has a unique N -solution [1. Hence, by Equivalence, 
W2k+i = ; for 1 ~ i ~ 2k - 4, 
also has a unique N-solution [1. By Lemma 4, Kk has a unique N-solution p, 
which proves what we want to show. 
Showing that all the N-solutions of CkWT = tr are positive requires some 
effort. Any N-solution f of this system satisfies (7)-(12). Assume Ti = 0 for some 
i. We shall derive a contradiction. 
First consider the case in which Tl Â¥- 0, so it must be some other Ti which is 
zero. In (i)-(v) below we will eliminate all possibilities. 
8 All coordinates of {l are strictly positive so the {l certainly is positive in the sense of [Paris and 
Vencovska, 1990], see Lemma 9. 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
225 
(i) If 72 = 0 then 73 = 0 by (7), but that is impossible since 72 + 73 = C2 i- O. 
(ii) If 73 = 0 then either 72 = 0 or 72k+1 = 0 by (7). The former is impossible 
as above and the latter implies 74 = 0 by (8) so this is also impossible since 
73 + 74 + 72k+1 = C3 i- O. 
(iii) If for some 1 ~ i ~ 2k - 5, Ti+3 = 0 then either T2k+i = 0 or T2k+i+1 = 0 
by (8). The former implies that Ti+2 = 0 by (8) again, or (7) when i = 1, in 
which case Ti+2 + Ti+3 + T2k+i would be o. But that must equal Ci+2 i- 0, 
contradiction. On the other hand the latter implies that TiH = 0 by (8), or by 
(9) when i = 2k-5, so this is also impossible since Ti+3 +TiH +T2k+i+1 = 
Ci+3 i- O. 
(iv) If T2k-1 = 0 or T2k = 0 then a contradiction follows just as above for T2 
and T3, using (9) and (8). 
(v) If Tj > 0 for 1 ~ j ~ 2k then the remaining T2k+i for 1 ~ i ~ 2k - 4 must 
also be non-zero by (8). 
Now assume T1 = O. We shall derive a contradiction by taking a sum of certain 
lines of CkfI' = ifF and then argue that f cannot possibly satisfy the resulting 
equation. To start with take the sum of all the i + 2nd lines 1 ~ i ~ 2k - 4 of 
CkfI' = ifF for which T2k+i i- 0 along with line 2 if T2 i- 0 and line 2k - 1 if 
T2k i- O. On the left hand side we obtain a sum which involves all the non-zero Ti'S 
at most once, since by (7)-(9) no two subsequent lines are included. To involve all 
the non-zero Ti'S exactly once, we now add in also various other lines according to 
the following prescription (a)-(d): 
(a) If T2k+i i- 0, T2k+i+1 = T2k+i+2 = ... = T2k+i+m-1 = 0 and T2k+i+m i-
o (i 2: 1, 2k - 4 - i 2: m > 2) add in also lines i + 4, i + 6, ... , i + m if m 
is even and lines i + 4, ... i + m - 1 if m is odd. This will ensure that all the 
non-zero TiS (which had been missed before) from TiH, Ti+5, ... , 7i+m+1 
are added exactly once. (If m is even then this is obvious, and if m is odd 
then it would appear that Ti+m+1 may have been missed, but by (11), since 
T2k+i+m =I 0 and T2k+i+m-2 = 0 we have Ti+m+1 = 0.) 
(b) If T2k+1 = T2k+2 = ... = T2k+m-1 = 0 and T2k+m =I 0, for 2 < m ~ 
2k - 4, add in also the lines 2,4, ... , m if m is even and lines 2,4, ... , m - 1 
if m is odd with justification as above in the case of odd m using the fact 
that T m+1 = o. 
(c) If T4k-3-m =I 0 and T4k-m-2 = T4k-m-1 = ... = T4k-4 = 0 for 2 ~ 
m < 2k - 3 add in lines 2k - m + 1,2k - m + 3, ... , 2k - 1 if m is 
even and lines 2k - m + 2, 2k - m + 4, ... , 2k - 1 if m is odd (using the 
fact that T2k-m+1 = 0, which follows from (11) since T4k-3-m i- 0 and 
T4k-m-1 = 0). 

226 
J. B. PARIS & A. VENCOVSKA 
(d) If all 72kH, ... , 74k-4 are zero, then by (10), 72 = 0 or 74 = O. In the former 
case, add lines 3, 5, ... 2k -1 and in the latter case add lines 2, 5, 7, ... 2k-1. 
We now have an equation which has the sum of all the nonzero 7i'S which must 
be 1 on the left hand side and Li: line i was chosen k(ei + qi-2) on the right hand 
side (where we set qo = q2k-3 = 0). Since for each 2 ~ i ~ 2k - 1, we have Pi + 
PHI = ei, and no two subsequent lines were chosen, Li: line i was chosen ei ~ 1. 
Since all the qi, 1 ~ i ~ 2k - 4, are strictly positive and at least one of the lines 
3, ... , 2k - 2 was left out, Li: line i was chosen qi < L~!~4 qi SO considering the 
definition of H we can see that the righthand side of our equation is strictly less 
than one, which gives the desired contradiction. 
This completes the proof of Lemma 8 in the base case when p( 8) = O. We now 
turn to the case whenp(8) > O. 
Case p( 8) > O. This part of the proof uses the same key ideas as the corresponding 
argument in [Paris and Vencovska, 1990], pp.211-215 but now adapted to apply to 
extended inference processes. 
Assume that Lemma 8 holds for 8 with Oi i- 0 for all i and p( 8) < N for 
some N ~ 1. Let 8 = (01, ... , Om) be such that p(8) = N, Oi i- 0 for all i and 
D(8, k)UjT = Jr satisfies the hypotheses of Lemma 8. Without loss of generality 
we can assume that k = m and Or i- Â±1 for some r i- m. Consider the set of 
constraints 
lcirl 
.... 
....T 
1r 
D(o, m)w = d , LZj = 1, 
j=1 
corresponding to K given by CiT = d""r, where 
C= 
-Om, ... , -Om 
~ 
lcirl times 
o 
-Om, ... , -Om 
~ 
lcirl times 
o 
-Om, ... , -Om 
Om-I, ... , Om-l 
~ 
---.......-.-
lcir I times 
lcir I times 
and i = (Xll' ... ,xllcirl,X21, .... ,x2Icir l, ... ,Xmlcirl)' By Lemmas 3 and 5 the N-
solutions of K are precisely 
/ 71 
71 
7m 
7m) 
\ j;q' ... , j;q' ... , j;q' ... , j;q 
---------- ----------
lcir I times 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
227 
where f are the N-solutions of D(8,m)wT = Jr, By Obstinacy, K U {xrj = 
Xr l 11 ::; j ::; l<Srl} has the same N-solutions so by Obstinacy again GxT = gr 
has the same N -solutions, where G is obtained from C by replacing the rth row 
0, .. ,,0, 
-<Sm, .. " -<Sm, 
0, .. ,,0, 
<Sr, .. " <Sr 
-.......-...- -------
-.......-...- -----
(r-l}lorl times 
10rl times 
(m-r-l}lorl times 10rl times 
by l<Srl new rows 
0 
0 
-<Sm 
0 
0 
0 
0 
<Sr l<Sr 1-1 
0 
0 
0 
-<Sm 
0 
0 
0 
<Sr l<Sr 1-1 
0 
0 
0 
0 
-<Sm 
0 
0 
<Sr l<Sr 1-1 
and 
Ii ~ ( d". .. , d,-I, ?>-I',I-' , .;., d,.1', I-'; d,.+I, ... d", ) . 
10rl times 
<Srl<Srl-l 
<Sr l<Sr 1-1 
Now consider an N-solution f of D(8, m)wT = Jr, Let A be the set 
{(i, 1) 11::; i::; m} U {{r,j) 11::; j::; l<Srl}, 
By the above analysis and using Obstinacy, 
.... 
/ 71 
71 
7m 
7m) 
17 = \ TfrI' .. " TfrI' .. " TfrI' .. " TfrI 
'-----'" '-----'" 
lOr Itimes 
lOr Itimes 
remains an N -solution of 
exT = iF, 
Xj = 17j for j ~ A, 
By Equivalence, ij is also an N -solution of 
s 
Xu 
X(r-l}1 
Xrl 
Xml 
= gr, 
Xj = 17j for j ~ A 

228 
1. B. PARIS & A. VENCOVSKA 
where sis 
(dl I8r l-1, ... , dr _ 118r l-1, t, t, ... , t ,dr+118r l-1, ... , dm I8r r1), 
~ 
I"rl times 
-8m 
81 
0 
-8m 
8r - 1 
-8m 
" 
Ttr 
S= 
-8m 
l.r.... 
I"rl 
-8m 
8r+ 1 
o 
Let H = 2:sEA fls. By Lemma 4, 
1 
H (fill, ... , fI(r-1)1' fir!. ... , flrl"r I, fI(r+1)1' ... , flm1) , 
i.e. 
1 / T1 
Tr-1 
Tr 
Tr 
Tr+1 
Tm ) 
H \ TJ:1' ... , TJJ' TJ:1' ... , TJ:1' TJJ' ... , TJ:1 ' 
is an N-solution of SUiT = kP'. Now S is D(,Â¢, m + 18r l- 1), where 
and p('Â¢) = p(8) - (18r l - 1) < p(8), so by the inductive assumption, 
1 / T1 
Tr-1 
Tr 
Tr 
Tr+1 
Tm ) 
H \ TJ:1' ... , TJJ' TJ:1' ... , TJ:1' TJJ' ... , TJ:1 
is also the (unique) ME-solution of SUiT = kP'. 
By Lemma 9, if 8r > 0 then 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
229 
and analogously if dr < O. Hence if some Ti = 0 then there must also be a Tj = 0 
with di, dj having different signs. But then referring back to the original system 
DCS, m)wT = Jr for which T is an N-solution we see that the equation 
-diWj + djWi = 0 
is derivable from this system. As remarked earlier this forces DCS, m)wT = d"""'r 
to have a unique solution, so necessarily the N -solution is unique and agrees with 
the ME-solution. 
On the other hand if no Ti is zero then since (13) forces 
by Lemma 9 T must again be the ME-solution to D (6, m )wT = d"""'r. 
This completes the induction step. 
It remains to remove the assumption that di i- 0 for i = 1, ... , m. Assume that 
D(6, k)wT = Jr satisfies the assumptions of Lemma 8. Let dil' ... , dir be the 
nonzero coordinates of 6, where i1 < ... < ir and k = i q. Let 1 = (dil' ... , diJ. 
Our system is equivalent to the system of equations 
dir - 1 
fori::; k -1, i Â¢. {i1, ... ,ir }, 
for i ~ k + 1, i Â¢. {i1 , ... , ir }. 
Thus the system implies ~;=l Wi; = a for some a. By Lemma 4, the N-
solutions of 
(15) D(1,q) ( W" ) 
w'r 
1 
diq_ l 
a 
diq+l - 1 
are precisely the vectors (Til/a, ... ,Tirla) for N-solutions (Tl, ... ,Tm ) of 
D(6, k)UjT = d"""'r. Since (15) satisfies the assumptions of Lemma 8 and the co-
ordinates of 1 are nonzero such a vector (Til la, ... , Tirla) must, by the case al-
ready proved, equal the ME-solution of (15). But clearly, by re-running the above 

230 
1. B. PARIS & A. VENCOVSKA 
derivation, this is (Pi1 la, ... , Pirla) where pis the ME-solution of D(l, k)wT = 
Jr so Ti; = Pi; for j = 1, ... , r. But this gives that T = P since the remaining 
coordinates are uniquely determined by (14) and finally completes the proof of 
Lemma 8. 
â¢ 
Proof of Lemma 7 continued Using Lemma 8 it is straightforward to prove 
Lemma 7. First notice that the assumption that l is an integer vector can be re-
placed by the assumption that it is a rational vector since for a rational l there 
is some (noE-zero) integer number a such that al is an int~er vector (by Equiva-
lence, D(a8, k)wT = aJr has the sameN-solutions as D(8, k)wT = Ji). Noting 
that a rational line with ak '" bk is given by the set of constraints 
bÂ·-aÂ· 
bÂ·-aÂ· 
{ -Wj + J 
J Wk = ak J 
J - aj I j '" k} 
bk - ak 
bk - ak 
we can see that it corresponds to D(l, k)wT = Jr where 8k = 1, 8j = ~!=:~ for 
j '" k (with the appropriate d). Since L~:l aj = L~:l bj = 1, the above l satis-
fies L~:l 8j = O. Hence Lemma 8 applies and the ME-solution of D(al, k)wT = 
aJr must be the unique N-solution of D(al, k)wT = ad"T, equivalently, N(l) = 
{M E(l)}, as required. 
â¢ 
The aim now is to build on Lemma 7 to give the second key Lemma 15. 
As far as our proof is concerned an important consequence of allowing polyno-
mial constraints is that we can describe any finite set of points. Precisely, given 
{a(l), a(2), ... , a<m)} c IlJ)n let KO:(l) ,0:(2) , .â¢. ,o:(~) be the set consisting of the single 
constraint 
Then 
V(K 
) -
{;;,(1) ... (2) 
;;,(m)} 
a(l) ,a(2) , ... ,a("') -
a 
,a 
, ... , a 
. 
Apart from singletons it is not possible to specify finite sets using just linear 
constraints. The fact that we now have this ability enables us to express a notion 
of relative preference ( -<) between individual pairs of points. Precisely, define the 
binary relations ::S, -< and !::! on IlJ)n by 
a::sb 
Â¢::::} 
bE N(Ka,;;), 
a-<b 
Â¢::::} 
not(b::S a), 
a~b Â¢::::} a ::S b & b ::S a. 
Thus a -< bjust if a ~ N(Ka';;) and a!::! bjust if a, b E N(Ka';;)' 
LEMMA 10. The relations ::S, -< are transitive. 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
231 
Proof. Suppose that a j band b j c, i.e. b E N(Ka,b) and C E N(Kb,c). 
Suppose C ~ N(Kil,b,c). If b E N(Kil,b,c) then 
bE N(Kil,b,c) n {b, C} i- 0, 
so by Obstinacy 
C E N(Kb,c) = N(Ka,b,c) n {b, C}, 
contradiction. So now b ~ N (K ~ ~b ~) and the only option left is that 
a, ,c 
N(K~b~~) = {a}. 
a, ,c 
But in that case 
a E N(Ka,b,c) n {a, b} i- 0, 
so by Obstinacy 
bE N(Kil,b) = N(Kil,b,c) n {a, b}, 
contradiction again. Hence the original assumption, that C ~ N(K ~ ~b ~), must be 
a, ,c 
false. But then, by Obstinacy, C E N(Ka,b,c) n {a, C} i- 0 so C E N(Ka,c) = 
N(K~ b~~) n {a, C} as required. 
a, ,c 
The second part now follows directly. 
â¢ 
LEMMA 11. Suppose K E ECL, a,b E V(K), a E N(K). Then 
bE N(K) 
Â¢::::} a j b. 
In other words all the vectors in N(K) are congruent (e:!.) to each other and these 
are the maximal elements in V(K) with respect to j. 
Proof. Since ii E {ii, b} n N(K), by Obstinacy and the fact that ii, b E V(K), 
{a, b} n N(K) = N(K U Ka,b) = N(Ka,b)Â· 
Hence b E N(K) just if b E N(Ka,b)' equivalently a ~ b. 
LEMMA 12. Let l E ECL be a line. Then M E(l) E N(l) 
â¢ 
Proof. Let lm be rational lines converging (in the metric 8) to l as m --+ 00. 
By Lemma 7 N(lm) = {ME(lm)} and the result follows by Continuity since 
ME(l) = limm-too ME(lm). 
â¢ 
Notice that this lemma does not discount the possibility that N(l) may also 
contain other points. It is essentially the problem of showing that no such further 
points can exist that occupies much of our effort in what follows. 
LEMMA 13. Suppose a, b E IlJ)n and the function E is increasing on the line 
segment [a, b] from a to b. Then a ~ b. 

232 
1. B. PARIS & A. VENCOVSKA 
Proof. We may assume that if ai = 0 then bi = 0 (since otherwise prove the result 
instead for points;; on [a, b] arbitrarily close to, but not equal to, a, noting that 
such points do satisfy this property, and then appeal to Continuity to conclude the 
result for ii). Let l be the line through the points a and b and let e be the point 
on l at which E is maximal. By the concavity of the function E and the fact that 
E(a) < E(b) the point emust be on the opposite side of b to a. Let sbe in j[))n not 
on l and such that if ai = 0 then Si = O. Let P be the plane through a, b, s. If l 
is normal to \7 E(b) then b = e, in which case we immediately have the result by 
Lemma 11 since bE N(l) by Lemma 12. Otherwise, let it be the line in the plane 
P through b normal to the vector \7 E(b). Notice that since E increases along the 
line segment [a, ~,a lies either on it or on the other side of it to e. In the first case 
the result again follows by Lemma 12. 
Otherwise, for x E P, x f. a let l (x) be the line through a and X. Then 
M E(l(x)) is on the other side of it to a for x = b (in this case M E(l(x)) = â¬) 
and, by the Open-mindedness of ME, on the same side of it as a when x is an 
intersection point of it with the boundary of j[))n. Hence there is a point c on it for 
which c = M E(l(CÂ»). By Lemmas 12 and 11 applied to lines l, l1 respectively we 
obtain that a ~ c and c ~ b. The result follows by the transitivity of ~. 
â¢ 
LEMMA 14. Suppose l E EeL is a line and a E N(l). Then [a,ME(l)] ~ 
N(l). 
Proof. If a = M E(l) then the result is obvious so suppose a f. M E(l). Let b 
be on the line segment [a, M E(l)]. Then E is increasing on [a, ~ so by Lemma 
13, a :5 b. Therefore, since a E N(l), and b E V(l), by Lemma 11, bE N(l), as 
~~. 
. 
A consequence of this is that we can now strengthen Lemma 13 to give the 
second key lemma referred to earlier. 
LEMMA 15. Suppose a, bare distinct points in j[))n and the function E is increas-
ing on the line segment [a, b]. Then a -< b. 
Proof. Let c= (a + b)/2. By continuity and concavity of E there is a neighbour-
hood B = {x I Ix - C1 < E} n j[))n such that for J E B E is increasing on both 
[a, dj and [ti; b]. Hence by Lemma 13, a ~ d~ ~ b. Now suppose on the contrary 
that b ~ a. Then by transitivity a ~ J so, since J was an arbitrary point in B, 
x ~ if for any x, if E B. A continuity argument now shows that there is a rational 
line l with ME(l) E B. Precisely, let ebe a rational point in B and let l' be a 
line through enormal to \7E(C), so ME(l') = e. Let lm, mEN, be rational 
lines through e such that limm- Hxl 8(l', lm) = O. Then by continuity of the infer-
ence process ME, limm-too M E(lm) = M E(l') = e, so for sufficiently large m, 
M E(lm) E B and we can take this line to be l. But now, by Lemmas 7, 11, for 
points x on l and close to M E(l) (and so in B), x -< M E(l), contradiction. 
â¢ 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
233 
We are now set up to prove the third key lemma in this paper. 
LEMMA 16. Suppose a, b E I[])n and E(a) < E(b). Then N(K(1,b) 
{b}, 
equivalently if, -< b. 
Proof. Let ei be the vector in I[])n with i'th coordinate 1 and all other coordinates 
O. Notice that b cannot be such a corner point since the global minima of E occur 
at these points. 
The first thing we need to show is that there is some ei such that on the line 
segment lei, b] from ei to b the function E is (strictly) increasing. For suppose not. 
Then for each i = 1, 2, ... , 2n there would (by the concavity of the function E) be 
a number ti, 0 < ti < 1 such that 
In this case however, 
2n 
2n 
b 
2: biei = 2: 1 ~ t {((I - ti)ei + tJ) - tJ} 
i=l 
i=l 
1 
2n 
2n 
2: 
bÂ· 
~ 
bÂ·tÂ· ~ 
= 
_1 ((1 _ t)e .... + t.b) _ '" _1 J_b 
I-t. 
1 
1 
1 
L...-l-t.' 
i=l 
1 
j=l 
1 
so 
{ 
( 
)
-1 
} 
2n 
~ 
b 
tbÂ· 
~ 
b = '" _1 1 + '" _J_1 
((1 - ti)ei + tib) 
. 
L...-
1 - tÂ· 
L...- 1 - t 
i=l 
1 
j 
1 
But since 
bÂ· 
tÂ·bÂ· 
1 
1 + 
J J 
( 
)
-1 
~ 1- ti 
~ 1- tj 
( '" bi + '" !i!!.L) . (1 + '" !i!!.L) -1 = 1, 
L...-
L...-l-t 
L...-l-tÂ· 
i 
j 
1 
j 
J 
the righthand side of this expression is a (strict) convex combination of vectors 
with entropy egual to the entropy of b so by the concavity of E its entropy must 
exceed that of b, contradiction. 
Having established this let us assume for the moment that b and a are in the 
same corner ek, in other words that this property holds for both a and b. If E is 
increasing on the line segment [a, b] then we know the result by Lemma 15. So 
suppose not. In this case the points ek, a, b must all be distinct. [In what follows 

234 
1. B. PARIS & A. VENCOVSKA 
everything will take place in the plane determined by these three points, indeed in 
the convex set determined by the intersection of this plane with lIJ)n.J Also there 
must be a unique point e on this line segment strictly between a and b such that 
E(i!) = E(b). By concavity the function E must be increasing on [a, CJ. Also by 
concavity for any point iI on [e,~, E(iI) ~ E(b) and for any E(ek) ~ Y ~ E(iI) 
there is a unique point z on the line segment [ek, i1] at which E(Z) = Y and E is 
increasing on [ek, Z1. Pick a point b' on [e, b] close to (but not equal to) band let d 
be the point on [ek, b'] with E(d) = E(b). Notice that E increasing on [ek' d] and 
that dis also close to b (otherwise consider a limit point of such d as b' tends to b). 
Now we need to consider two possibilities. First assume that E is increasing on 
[ek, CJÂ· In that case, since E is increasing on [a, CJ we can pick an interior point 
don [ek,CJ such that E is increasing on [a,d] and such that E(d) < E(i!). For 
t E [0, 1] let g(t) be that (unique) point on the line segment [ek, (1- t)e+ td] such 
that 
(16) E(g(t)) = (1 - t)E(d) + tE(i!). 
So g(O) = d and g(I) = J. Now, E is Coo on any set obtained from the triangle 
with vertices ek, e, d by removing a small neighborhood of ek. To see this, note 
that if Xi = Â° 
for some x = (Xl'"'' X2n) from such a set then Yi = Â° 
for all 
ii = (YI, ... , Y2n) from this set, since any point in the triangle (except ek) has to 
be of the form kl ek + k2a + k3b where the kl' k2, k3 are non-negative, sum to 1 
and k2 and k3 are non-zero. So for Xi = Â° 
we have to have ai = bi = Â° 
and thus 
also (considering that the entropy increases all the way along the line from ek to i1) 
i f:. k. It follows by the Implicit Function Theorem that get) is Coo (see [Goursat, 
19041)9. Also, 
dE(g(t)) = E(i!) - E(J) > 0. 
dt 
By the uniform continuity of g(t) and its derivative on [0,1] there are numbers 
0= rl < r2 < ... < rs = 1 such that for j = 1,2, ... , s - 1 and u E [0,1], when 
t = (1 - uh + urj+! both g(t) and Â¥t(t) are close to (1 - u)g{rj) + ug(rj+d 
and y(r;+r)-y(r;) respectively. Hence since 
Tj+l -rj 
, 
(17) dE~(t)) = - ~)1 + log(gi(t))) ~i (t) = E(i!) - E(d) > 0, 
where the sum is over the i for which Xi is non-zero when x is in the triangle 
ek, e, d (excepting ek), and the derivative with respect to u of E((l - u)g(rj) + 
ug(rj+d) is 
- ~)I + 10g((1 - U)gi{rj) + ugi{rj+d))(gi(rj+d - gi(rj)), 
9Consider G(t, s) = E(sek + (1 - s)Â«1 - t)c + td)) on [0, 1] X [e, 1] where e is positive and 
small. G is Coo. By the above remarks. for each t E [0,1] there is a unique s E [e, 1]. s = h(t). 
such that G(t, s) = (1 - t)E(d) + tE(i!'). By the Implicit Function Theorem. h is Coo. We have 
get) = h(t)ek + (1 - h(t))Â«1 - t)c + td). 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
235 
it follows that E is also increasing on the line segment [g(rj), g{rj+dJ. 
If E is not increasing on [ek , C1 then we need to pick 2 to be on the tangent from ek 
to the contour line {i : E(i) = E(C)} close to their meeting point m, on the same 
side of mas ek. Then, again, E is increasing on [a,2J and E(2) < E(C), and we 
can argue similarly as above, letting g(t) for t E [0, 1J be that (unique) point on 
the line segment [ek' (1 - t)m + tdj such that (16) holds (note that m takes on the 
role of C). Omitting details, we can again obtain 0 = rl < r2 < ... < r s = 1 with 
g(O) = g(rd = 2 and g(l) = g(rs) = l such that E is increasing on the line 
segment [g{rj),g{rj+dJ for each j = 1, ... ,8-1. 
In summary, what we now have is that E is increasing on each of the line seg-
ments 
[a, 2], [2, g(r2)], ... , [g(rs-2), Â§(rs-d], [g(rs-d, dj 
so by Lemmas 15 and 10 a -< 2 -< land l E N(K_ -; d-)' Since l can be 
a,e , 
arbitrarily close to b (keeping 2 fixed), by Continuity b E N(K_ -; b-)' so 2 :5 b. 
a,e, 
Hence, since a -< c', by Lemma 10, a -< b. 
We now have the required result in the case where a, bare in the same 'corner'. 
To prove the result in general suppose that a is in the corner determined by ek and 
b is in the corner determined by ~. Then if T is the permutation which simply 
transposes k and j, for each (J, (Ja and (JTb will be in the same corner. Let K' be 
a set of constraints such that 
V(K') = { (Ja, (Jb I (J a permutation of 1, ... , 2n }. 
If (Ja E N(K') for some permutation (J then by Obstinacy, (Ja E N(Kuil,UT';) 
which contradicts the special case proved above. Hence we must have (Jb E 
N(K') for some (J. But then, by Renaming, b E N((JK') = N(K'). Finally, 
by Obstinacy, 
N(Kil,b) = N(K' U Kil,';) = N(K') n {a, b} = {b}, 
as required. 
â¢ 
COROLLARY 17. For a, b E Jl}ln, 
a:5 b {::=} E(a) ~ E(b). 
Proof. The result from left to right follows by Lemma 16. In the other direction 
it is enough to show that if E(a) = E(b) then a :5 b. So suppose E(a) = E(b) 
and a i b (consequently b is not the overall maximum entropy point). Let bm be 
points such that limm -+oo bm = b and each E(bm ) > E(b). Then by Lemma 16 
bm E N(Kil,bJ so by Continuity bE N(K(i,';), equivalently a :5 b, as required . â¢ 

236 
J. B. PARIS & A. VENCOVSKA 
As an immediate corollary we finally have the main theorem of this paper. 
THEOREM 18. EM E is the unique extended inference process satisfying the 
(extended) principles of Irrelevant Information, Equivalence, Renaming, Rela-
tivization, Obstinacy, Independence and Continuity. 
Proof. Let K E EeL and b E V(K), if E N(K). By Lemma 11 and Corollary 
17, 
bE N(K) {::::::} if ~ b {::::::} E(if) :S E(b). 
From this it follows that N(K) must be the set of points in V(K) of (global) 
maximum entropy. 
â¢ 
5 
CONCLUSION AND DISCUSSION 
To sum up the main result of this paper we started off by assuming that the belief 
values agents assigned to sentences were subjective probabilities (in the sense that 
they satisfy (PI-2Â» and that these agents assigned such values solely on the basis 
of know ledge bases consisting of finite sets of polynomial constraints on the their 
beliefs, followed if necessary by a final random choice between equally preferred 
or acceptable alternatives. Indeed, more globally, we identified an agent with a 
process, called an extended inference process, which given a knowledge base K 
produced, on the basis of K, a set of equally acceptable (to the agent) probability 
functions satisfying K, the ultimate working choice of one of these being left to 
the agentlO. Within this framework we then showed that if an agent (as an ex-
tended inference process) was required to abide by the 'common sense' principles 
of Irrelevant Information, Equivalence, Renaming, Obstinacy, Relativization, In-
dependence and Continuity then on such a knowledge base K the agent's set of 
equally acceptable alternatives must be EM E(K). In consequence if the set of 
possible solutions to K was convex (as is the case for the linear knowledge bases 
considered in [Paris and Vencovska, 1990] and [Paris and Vencovska, 1997]) then 
EM E(K) is a singleton (the unique global maximum entropy solution) and the 
agent has no choice. 
The first point to note about the proof of this result is that we hardly used the 
fact that we were allowed polynomial constraints. All we really needed was that 
we had at least all finite sets of linear constraints (in particular to derive Lemma 7), 
that the constraints yielded closed (in Euclidean Space) sets V(K) and that these 
subsets of J[))n included amongst their number all finite sets of probability functions 
(on the same SL(PiuPh, ""PiJ). In other words we very largely got the result 
for polynomial constraints for free. Indeed, provided these minimal requirements 
iOThis relationship between an extended inference process and the knowledge base is directly analo-
gous to the relationship between an expert system shell and the specific data inputted. And just as with 
any particular 'real' agent, in practice this shell will never meet more than a finite number of actual 
data sets. Nevertheless it is in principle structured as a process for dealing with any of the possible data 
sets. 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
237 
are satisfied we can add in any other sorts of constraints that we wish, and still 
draw the same conclusion. 
The idea of allowing very exotic sets of constraints as knowledge bases here 
may appear from the outside rather unnatural. After all, one might add, how could 
such a knowledge base ever have arisen? However, be that as it may, we would 
remark that the choice of allowed knowledge bases is an external matter. Under 
the standing Watts Assumption (see [Paris, 1994]), that the knowledge base is all 
the agent's knowledge, it is of no concern to the agent why a certain knowledge 
base should be considered nor where it came from. 
This point can be illustrated with an example which is sometimes cited as a 
criticism of the maximum entropy inference process. Suppose we had allowed 
also constraints of the form 
r 
:~:::>jW((}j) ~ b 
j=l 
and consider for L the language with just one propositional variable p the knowl-
edge base 
K = { w(p) ~ 1/2 }. 
ME (and EM E) here give the single solution w(P) = 1/2 which has been criti-
cized as 'unreasonable' on the grounds that the chosen solution is right at one end 
of the range of possibilities. Surely, the argument goes, the solution w(P) = 1/4, 
nestling as it does comfortably in the middle of the range, is more reasonable. 
We would maintain however that this answer only appears reasonable (if indeed 
it ever does) because of some possibly unconscious assumptions about where this 
constraint could possibly have come from in the first place. And perhaps the most 
obvious such assumption is that we have obtained empirical evidence that some 
natural, objective, probability lies in the interval [0,1/2]. In that case the answer 
1/4 might initially look appealing II for this probability, or more precisely as an 
estimate of this probability. (Notice that Renaming cannot be employed here, we 
cannot rename 0 as 1/2 !!) But of course there are other, less immediate per-
haps, explanations that one could think up for the origin of the knowledge base 
w(p) :::; 1/2 which would entirely justify w(P) = 1/2 or w(p) = 0 or indeed 
any other value in between. Fortunately we don't have to struggle with the thorny 
problem of which of these explanations is the most likely, our agent isn't aware of 
any of them! 
At this point the reader might be inclined to wonder whether the results in this 
paper have any relevance to the real world. After all in practice we have so much 
'knowledge' about anything that the Watts Assumption can rarely, if ever, be ap-
plicable12 as far as we are concerned13. This is a point well taken. However two 
II But see [Paris et al., 2000) for an alternate view. 
12Not to mention the inherent computational difficulties even with this assumption, see [Maung and 
Paris, 1990). 
13Given this it is intriguing how we ever come to recognize, and (it seems) largely agree upon, what 
constitutes common sense at all! 

238 
1. B. PARIS & A. VENCOVSKA 
comments are worth making. Firstly, for expert systems, as examples of agents 
who we would wish to act common-sensically, the assumption is much more per-
tinent. Secondly, the criticism that the assumption does not in practice apply to 
higher agents such as ourselves seems based in part on the practical difficulty that 
we cannot ever hope to specify all our knowledge in a particular domain rather 
than that our knowledge is somehow unbounded. If we assume that our knowl-
edge is in the form of finitely many constraints (though possibly far more than we 
could ever elucidate) on our beliefs then the results in this paper would, at least 
theoretically, apply. 
Furthermore even if we do not subscribe to the view that all our 'knowledge' 
is of this type (see [Paris and Vencovska, 1990] and [Paris and Vencovska, 1993] 
for an alternative view) the form of Theorem 18, that rationality or common sense 
requirements may essentially constrain the available patterns of reasoning to a 
single tightly knit family separated only by a final random choice, is interesting in 
that it holds out the prospect of similar theorems even in the case of more general 
and realistic knowledge bases. 
A second issue concerning Theorem 18 is our interpretation of what comprises 
'common sense'. Whilst, in [Paris, 1999] particularly, we have given some expla-
nations as to why we find certain principles common-sensical, as matters currently 
rest it seems ultimately up to the individual reader to draw hislher own conclu-
sions. In these deliberations however we would urge the reader to consider the 
rationality ofjiaunting any of these principles (within this framework). 
Our case in support of these principles has not been greatly helped by some of 
the innovations which have occurred in the space between [Paris and Vencovska, 
1990] and this current paper. Two notions which earlier seemed entirely reasonable 
have become incompatible with the other principles. 
The first is Renaming as in [Paris, 1999], which as we have already seen in 
Courtney's Paradox leads to a contradiction in this extended setting. The rethink 
which that paradox precipitated led to the notion of an extended inference pro-
cess. Nevertheless one might feel that one aspect of renaming should still prevail, 
namely that highly symmetric solutions to K should be preferred to less symmet-
ric solutions. Unfortunately with the obvious interpretation of 'symmetric' this 
proto-principle is inconsistent with the others listed in Theorem 18. To see this let 
iiI, ii2, ii3 E ~ be, respectively, 
( 1 1 1 1) (12 13 21 20) (13 12 20 21) 
"6' "6' "3'"3' 
66' 66' 66' 66' 66' 66' 66' 66 . 
Then E(iid < E(ii2) = E(ii3) so by Lemma 11 and Corollary 17 
iiI ~ EM E(Kiil,ihii3) despite the fact that iiI is clearly the most 'symmetric' 
solution of K iil ,ii2 ,ii3. [Nevertheless not all is lost, as remarked earlier, by Lemma 
5 we do still retain the quintessential symmetry principle, Laplace's 'principle of 
indifference' .] 
The second example of an incompatible, yet at the same time ostensibly common-
sensical, principle is Open-mindedness, the requirement that if it is consistent with 

COMMON SENSE AND STOCHASTIC INDEPENDENCE 
239 
K that the belief in () is not zero (i.e. KU {w(()) > O} is consistent) then w(()) > 0 
for w E N(K). In other words don't entirely dismiss belie/in () unless you have 
to. On the face of it this seems entirely reasonable and it was included as one of the 
principles in [Paris and Vencovska, 1990], though not in [Paris, 1999] since by then 
(for linear constraints) it was known to be derivable from the other principles. Un-
fortunately the Open-mindedness principle is inconsistent with the principles used 
in the main theorem. To see this it is enough to note that we can find il, b E ]]))n 
with al equal to zero, all the bi strictly greater than zero yet E(il) > E(b). Hence, 
by Theorem 18, EM E(Kii,b) = {il}, despite thefactthat al = O. 
At the very least the inconsistency of these last two principles with the others 
listed in Theorem 18 provide a clear warning that the title of 'common sense' 
should not be conferred too lightly 14. Of course none of the principles of Theorem 
18 could ever suffer a similar fate since, being satisfied by EM E, we know that as 
a body they are mutually consistent. Nevertheless this poses the intriguing question 
whether or not there might be other sets of arguably common sense principles 
uniquely characterizing other extended inference processes. 
Apart from the questions already raised in this discussion Theorem 18 opens up 
a number of other avenues for exploration. For once we have a characterization of 
what it means to reason common-sensically, or equivalently, as we would argue, 
intelligently, then we can consider the implications of this for processes such as 
updating and induction (see for example [Paris and Vencovska, 1992], [Paris and 
Vencovskli, 1998], [Paris and Wafy, 2001], [Paris et al., 2001], which followed 
from [Paris and Vencovska, 1990]). It would also be interesting to find more natu-
ral sets of principles and/or constraint sets K to replace those in Theorem 18 anal-
ogous to the simplifications of [Paris and Vencovskli, 1990] presented in [Paris and 
Vencovskli, 1997] and [Paris, 1999]. These however remain questions for future 
research. 
Department of Mathematics, University of Manchester, UK. 
BIBLIOGRAPHY 
[Courtney,1992] P. Courtney. Models ofBelieJ, Doctorial Thesis, Manchester University, 1992. 
[Csiszar, 1989] I. Csiszar. Why least squares and maximum entropy? An axiomatic approach to in-
verse problems, Mathematics Institute of the Hungarian Academy of Sciences, Preprint No.19/1989. 
[Goursat, 1904] E. Goursat. A Course in Mathematical Analysis, Ginn and Company, 1904. 
[Kem-Isbemer, 1998] G. Kem-Isbemer. Characterising the principle of minimum cross-entropy 
within a conditional-logical framework, Artificial Intelligence, 98:169-208, 1998. 
[Maung and Paris, 1990] I. Maung and 1. B. Paris. A note on the infeasibility of some inference pro-
cesses, International Journal of Intelligent Systems, 5:595-604, 1990. 
[Paris, 1994] J. B. Paris. The Uncertain Reasoner's Companion - A Mathematical Perspective, Cam-
bridge University Press, Cambridge, UK, 1994. 
[Paris, 1999] 1. B. Paris. Common sense and maximum entropy, Synthese, 117:119-132, 1999. 
14See also concluding section of [Paris, 1999] where we consider the problem of formalizing Van 
Fraassen's 'Symmetry Slogan' from [van Fraassen, 1989]. 

240 
1. B. PARIS & A. VENCOVSKA 
[Paris and Vencovska, 1989] J. B. Paris and A. Vencovska. Maximum entropy and inductive inference. 
In Maximum Entropy and Bayesian Methods, Ed. 1.Skilling, Kluwer Academic Publishers, pp. 397-
403,1989. 
[Paris and Vencovska, 1990] 1. B. Paris and A. Vencovska. A note on the inevitability of maximum 
entropy, International Journal of Approximate Reasoning, 4: 183-224, 1990. 
[Paris and Vencovska, 1990] J. B. Paris and A. Vencovska. Modelling belief, Proceedings of the 
Evolving Knowledge Conference, British Society for the Philosophy of Science, Reading University 
Pitman Press, pp. 133-154, 1990. 
[Paris and Vencovska, 1992] 1. B. Paris and A. Vencovska. A method of updating justifying minimum 
cross entropy, International Journal of Approximate Reasoning, 7:1-18,1992. 
[Paris and Vencovskli, 1993] 1. B. Paris and A. Vencovska. A model of belief, Artificial Intelligence, 
64:197-241,1993. 
[Paris and Vencovska, 1996] 1. B. Paris and A. Vencovska. Some observations on the maximum en-
tropy inference process, Manchester Centre for Pure Mathematics Technical Report L1-96, Depart-
ment of Mathematics, Manchester University, UK, 1996. 
[Paris and Vencovska, 1997] 1. B. Paris and A. Vencovska. In defence of the maximum entropy infer-
ence process, International Journal of Approximate Reasoning, 17:77-103, 1997. 
[Paris and Vencovska, 1998] 1. B. Paris and A. Vencovska. Proof systems for probabilistic uncertain 
reasoning, Journal of Symbolic Logic, 63:1007-1039,1998. 
[Paris et ai., 2001] 1. B. Paris, A. Vencovska and M. Wafy. Some limit theorems for ME, M D and 
C Moo'. Manchester Centre for Pure Mathematics Technical Report, Department of Mathematics, 
Manchester University, UK, preprint number 200119, ISSN 1472-9210. 
[Paris and Wafy, 2001] J. B. Paris and M. Wafy. On the emergence of reasons in inductive logic. Logic 
Journal of the IGPL, 9, 207-216, 2001. 
[Paris et al., 2000] 1. B. Paris, P. N. Watton and G. M. Wilmers. On the structure of probability func-
tions in the natural world. International Journal of Uncertainty, Fuzziness and Knowledge-Based 
Systems, 8:311-329, 2000. 
[Shore and Johnson, 1980] J. E. Shore and R. W. Johnson. Axiomatic derivation of the principle of 
maximum entropy and the principle of minimum cross-entropy. IEEE Transactions on Information 
Theory, IT-26:26--37, 1980. 
[van Fraassen, 1989] B. Van Fraasssen. Laws and Symmetry, Clarendon Press, Oxford, UK, 1989. 

JAMES CUSSENS 
INTEGRATING PROBABILISTIC AND LOGICAL 
REASONING 
1 INTRODUCTION 
1.1 
Probability and Artificial Intelligence 
The ability to perform reasoning with uncertainty is a prerequisite for intelligent 
behaviour. Consequently there has been considerable Artificial Intelligence (AI) 
research into representing and reasoning with uncertainty. Given that there have 
been several centuries of successful applications of probability to uncertain reason-
ing, it would seem a natural tool for uncertainty in AI. However, in 1969 McCarthy 
and Hayes produced an influential paper [McCarthy and Hayes, 1969], which pro-
claimed that probabilities were "epistemologically inadequate" and much early AI 
work on uncertainty accepted this argument. 
More recently though, probabilistic approaches to AI uncertainty handling have 
been used extensively, indeed they are now dominant, as recent proceedings of 
the Uncertainty in Artificial Intelligence conference demonstrate. The success of 
Bayesian networks as a computationally feasible way of combining structural and 
quantitative aspects of probability distributions has been instrumental in this prob-
abilistic resurgence. 
An important part of the work on probability and AI investigates the logical 
formalisation of probabilistic reasoning. The goal is to use a logical analysis to 
answer (or at least clarify) some of the following questions: 
1. What are the semantics behind probabilistic reasoning? 
2. How should probabilities be updated in the light of new evidence (which 
may itself be uncertain)? 
3. If 90% of Swedes are Protestants, does it follow that the probability that a 
particular Swede of unknown religion, say, Henrik Bostrom, has probability 
0.9 of being Protestant? 
4. Is the probability calculus alone sufficient for uncertain reasoning? If not, 
what connections are there with other uncertainty formalisms, e.g. belief 
functions, fuzzy logic and nonmonotonic logic? 
241 
D. Corfield and J. Williamwn (eds.), Foundations qfBayesianism, 241-260. 
Â© 2001 Kluwer Academic Publishers. 

242 
JAMES CUSSENS 
1.2 
Halpern's logic of probability 
Halpern's An Analysis of First-Order Logics of Probability [Halpern, 1990] is 
an important contribution to the first question, where Halpern presents two ap-
proaches to giving semantics to (formalised) probability statements. The statement 
The probability that a randomly chosen bird flies is greater than 0.9 
is formalised as 
(1) 
wx(Flies(x)IBird(x)) > 0.9 
The semantics of such a formula are defined in terms of a type-l probability struc-
ture which is a standard first-order model with an associated probability function 
I-" on its domain. Formula 1 is satisfied by any type-1 probability structure where 
the probability is greater than 0.9 that a domain element, constrained to be in the 
extension of Bird and sampled according to 1-", is in the extension of Flies. 
In contrast, the statement 
The probability that Tweety (a particular bird) flies is greater than 0.9. 
is formalised as 
(2) 
w(Flies(Tweety)) > 0.9 
For such formulae, semantics are defined in terms of a probability distribution 
over first-order models ("possible worlds") rather than on some particular first-
order model. Formula 2 is satisfied by any such distribution that gives probability 
mass greater than 0.9 to the set of those models in which Flies(Tweety) is true. 
These two semantics loosely correspond to the objective and subjective inter-
pretations of probability found in the philosophical literature. Using this logical 
formalisation, Halpern and colleagues, in a series of papers, have clarified many 
of the issues involved in probabilistic reasoning. Most importantly, by connecting 
the two semantics above we can address the "Protestant Swedes" issue in a rigor-
ous manner [Bacchus et aI., 1996]. The clarity of such work is often in contrast to 
much philosophical work on probability which can be "dry, obscure, contrary to all 
ordinary ideas, and on top of that prolix"\ See, for example, the Popper-Miller ar-
gument concerning the connections between deductive and probabilistic relations 
[Popper and Miller, 1987]. The Popper-Miller position is criticised in [Cussens, 
1996]. 
1.3 Probabilistic inference is deduction 
In addition to such foundational work, considerable attention has been devoted to 
the practical issues of incorporating probabilistic and logical inference into some 
1 This is Kant's description of his own Critique of Pure Reason. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
243 
overarching framework. The alleged inability of logic to represent uncertain in-
formation is often cited as a motivation for the augmentation of a logical calculus 
(e.g. first-order predicate calculus) with additional probabilistic capabilities. 
First-order predicate calculus is concerned with valid logical inference: when is 
it certain that the truth of one statement ensures the truth of another? This focus on 
certainty may seem at odds with probabilistic reasoning. However, standard prob-
abilistic inference, by which is meant the inference of probabilities from probabil-
ities, is also exclusively concerned with deductive, certain reasoning. It is not an 
"inductive logic"-whatever that might be. For example given that P(bla) = 0.3 
and P(a) = 0.8, it follows with completely certainty that P(a 1\ b) = 0.24. 
A logical formalisation of probabilistic inference can therefore be carried out 
in the same way as the formalisation of other branches of mathematics, such as 
arithmetic: relevant axioms are added to those of first-order predicate calculus. 
Unfortunately, as GOdel's celebrated incompleteness result shows, not even arith-
metic has a complete axiomatisation. Naturally any axiomatisation for probability 
(such as supplied by Halpern [1990]) can not hope to be complete. Indeed, Abadi 
and Halpern also show that it "takes very little to make reasoning about probability 
highly undecidable" [Abadi and Halpern, 1994]. Despite these rather discouraging 
results, a number of researchers have described systems which incorporate logic 
and probability in a restricted setting, and it to this work that the remainder of the 
paper is devoted. In Section 2, Stochastic Logic Programs are proposed as one 
method of integrating logical and probabilistic reasoning. A number of alternative 
approaches are then considered in Section 3; and in Section 4, there is a summary 
and possible future work in this area is considered. 
2 LOGLINEAR MODELS FOR FIRST-ORDER REASONING 
Here a conservative extension to the logic programming framework is proposed by 
defining probabilities directly on proofs and hence indirectly'bn goals and atomic 
formulae. (This work has been presented previously in Cussens [1999a; 1999b; 
2000a; 2000b; 200la; 2001bl) This conservatism allows us to tie probabilistic 
and logical concepts very closely. Table 1 lists the linkages which the proposed 
approach establishes. 
2.1 
Probabilistic Constraint Logic Programming 
In [Riezler, 1997], Riezler develops Abney's work [Abney, 1997] defining a log-
linear model on the proofs of formulae. This requires defining features on these 
proofs (denoted Ii) and defining the model parameters (denoted Ai). The probabil-
ity of any proof w is then P(w) = Z-l exp(L:i Adi(W)) where Z is a normalis-
ing constant. This approach applies very naturally to natural language processing 
(NLP). In NLP, a proof that a sentence belongs to a language amounts to a parse 

244 
JAMES CUSSENS 
I Logic 
I Probability 
logical variable 
random variable 
instantiation 
instantiation 
relations 
joint distributions 
queries 
queries 
ground definitions 
probability tables 
disjunctive definitions 
mixture models 
defining relations in terms of other 
defining distributions in terms of 
relations 
other distributions 
Table 1. Linking logic and probability 
of that sentence, and the log linear model can be used to find the most likely parse 
of any particular sentence. 
Riezler extends the improved iterative scaling algorithm of Della Pietra, Della 
Pietra and Lafferty [Della Pietra et at., 1997] to induce features and parameters 
for a log linear model from incomplete data. Incomplete data here consists of just 
atomic formulae, rather than the proofs of those atomic formulae. In an NLP con-
text this means having a corpus of sentences rather than sentences annotated with 
their correct parses. The former is a' considerably cheaper resource than the latter, 
so this sort of incomplete data handling is of considerable practical importance. 
2.2 
Stochastic Logic Programs 
Riezler's framework allows arbitrary features of proofs, and recent experiments 
have used features "indicating the number of argument-nodes or adjunct-nodes in 
the tree, and features indicating complexity, parallelism or branching-behaviour" 
(Stefan Riezler, personal communication). 
Here a special case of Riezler's framework is examined, where the clauses 
(rules) used in a proof are the features used to define the probability of that proof 
Eisele [1994] examined this approach from an NLP perspective. Muggleton [1996] 
introduced stochastic logic programs (SLPs), approaching the issue from a general 
logic programming perspective, with a view to applications in Inductive Logic 
Programming. Definition 1 defines SLPs and Figure 1 shows So a simple example 
SLP. 
DEFINITION 1. A stochastic logic program (SLP) S is a definite logic program 
where some of the clauses are parameterised with non-negative numbers. A pure 
SLP is an SLP where all clauses have parameters, as opposed to an impure SLP 
where not aU clauses have parameters. A normalised SLP is one where parameters 
for clauses whose heads share the same predicate symbol sum to one. If this is not 
the case, then we have an unnormalised SLP. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
0.4:5(X) .-
O.6:s(X) 
p(X), p(X). 
q(X). 
O.3:p(a). 
O.7:p(b). 
O.2:q(a). 
O.8:q(b). 
Figure 1. So: A simple pure, normalised SLP. 
245 
Definition 1 generalises that found in [Muggleton, 1996], where Muggleton re-
quires SLPs to be range-restricted and with labels for the same predicate summing 
exactly to one. Also, Muggleton does not use SLPs to define a loglinear model as 
is done here. 
In [Eisele, 1994] and [Muggleton, 1996], Stochastic Context-Free Grammars 
(SCFGs) were 'lifted' to feature grammars (FGs) and logic programs (LPs) re-
spectively. SCFGs are context-free grammars where each production is labelled, 
such that the labels for a particular non-terminal sum to one. The probability of a 
parse is then simply the product of the labels of all production rules used in that 
parse. Sentence probabilities are given by the sum of all parses of a sentence. The 
distributions so defined are special cases of loglinear models where the grammar 
rules define the features Ii and their labels are the parameters li = exp(Ai). The 
normalising constant Z is guaranteed to be one. This is because the labels for each 
non-terminal sum to one and because the context-freeness ensures that derivations 
never fail-when generating a sentence from an SCFG a production rule can al-
ways be applied to a nonterminal. Because of this a number of techniques (such 
as the inside-outside algorithm for parameter estimation [Lari and Young, 1990]) 
can be applied to SCFGs, but cannot be applied directly to stochastic versions of 
FGs or LPs. (See Abney [1997] for a demonstration of this.) 
In the rest of this section an account of (i) the distributions defined by an SLP 
and (ii) the relations between SLPs and some other well-known probabilistic mod-
els are given. For more details on SLPs including accounts of sampling, inference, 
parameter estimation and semantic analysis see the following papers: [Cussens, 
1999a; Cussens, 1999b; Cussens, 2000a; Cussens, 2000b; Cussens, 2001a; Cussens, 
2001b; Muggleton, 1996; Muggleton, 2000]. The presentation of SLPs assumes 
some familiarity with logic programming concepts. 
Defining distributions over derivations 
An SLP S with parameters A together with a goal G defines up to three different 
related distributions: 'l/J(>.,S,G), 1(>.,s,G) and P(>.,S,G), defined over derivations, 
refutations and atomic formulae, respectively. Definition 2 defines the distribution 
over derivations and is an adaptation of a definition found in [Riezler, 1998]. 
DEFINITION 2. Let G be a goal, and S be a pure normalised SLP. S defines a 
probability distribution 'l/J(>.,S,G) (x) on the set D( G) (the set of derivations starting 
with G using S) S.t. for all x E D(G): 
'l/J(>.,S,G)(x) = e>"v(x) = Ir=ll~i(x) 

246 
JAMES CUSSENS 
A = (AI"'" An) E ~n is a vector of log-parameters where Ai is the log of li, 
the parameter attached to the ith parameterised clause, 
v = (VI, ... , vn ) is a vector of clause counts s.t. for each Vi : D( G) --t NU { 00 }, 
Vi(X) is the number of times the ith parameterised clause is used as an input 
clause in derivation x, 
A . v(x) is a weighted count S.t. A' v(x) = L~=1 AiVi(X), 
Usually it will be clear which SLP and goal is being used to define the distribution, 
so we will often abbreviate 1P(A,S,G) (x) to 1PA (x). 
Defining distributions over refutations 
In application of SLPs, the main focus of interest is not 1P(A,S,G) (x) but the condi-
tional distribution 1P(A,s,G)(xlx E R(G)), the distribution over derivations given 
that each is a refutation. We will denote this distribution by I(A,S,G): 
def 
I(A,S,G)(x) = 1P(A,s,G)(xlx E R(G)) 
Let 
def " 
Z(A,S,G) = 
~ 1P(A,S,G)(x) = 1P(>"S,G) (R(G)) 
xER(G) 
then 
I 
(x) -
{Z(>.~S'G)1P(A'S'G)(X) if x E R(G) 
(A,S,G) 
-
0 
if x E D(G) \ R(G) 
I(A,S,G)(X) is a log-linear model over refutations, defined for goals where 
Z(A,S,G) > O. To see this, consider Definition 3, where I(A,S,G) is defined without 
reference to 1P(>"s ,G). The only slight alteration to a standard log-linear model is 
that it is extended so that derivations which are not refutations have probability 
zero, rather than being undefined. 
DEFINITION 3. Let G be a goal such that Z(A,S,G) > 0, and S be a pure nor-
malised SLP. S defines a log-linear probability distribution I(>"s ,G) (r) on the set 
R( G) (the set of refutations of G using S) S.t. for all r E R( G): 
I 
() 
Z-l 
>.Â·v(r) 
Z-1 
TIn 
ZVi(r) 
(>.,S,G) r = 
(>.,S,G)e 
= 
(>.,S,G) 
i=1 i 
Z(>.,S,G) = :ErER(G) e>.Â·v(r) is a normalizing constant, 
A, V and A . v(r) are defined as in Definition 2. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
247 
We extend the normal log-linear definition so that f(>l,s,G)(x) = 0, if x E 
D(G) \ R(G). fp."s,G) (x) is thus a distribution over the whole of D(G). 
The distributions 1/J >. and 1>.. are more easily understood by referring to the SLD-
tree which underlies them. By way of example, Figure 2 shows an annotated 
SLD-tree for refutations of the goal f- 8(X) using So. There are 6 derivations, of 
which 4 are successful and 2 are failures. The branches of the tree are labelled with 
(i) the unification effected by choosing clauses and (ii) the parameters attached to 
these clauses. Since So is pure and normalised 1/J>. is a probability distribution over 
derivations, and the tree shows how the probability mass of one is divided up as we 
move down the tree. To find 1/J>.(x) for any derivation x we multiply the parameters 
on the branches corresponding to that derivation. Both failure derivations have 
probability 0.084, so Z(>.,so,+-s(XÂ» = 1 - 2 x 0.084 = 0.832. So, for example, if 
the leftmost refutation is rl, then f(>.,so,+-s(XÂ» (rl) = (0.4 x 0.3 x 0.3) /0.832 ~ 
0.043 (The tree assumes that the variable in the two 8/2 clauses is renamed to X' 
and X".) 
:- SeX). 
O.4'~6'{X 
â¢â¢ lXl 
7
:-p(x)\,~~~~} A:-
q(X). 
0.3:{X/ 
0.8:{XIb} 
0.2' X/a} 
:-p(a). 
:- pCb). 
0.3:{ v!l 0.7:fail 
0.3:Â£ 
I fll 
fail 
0.7:{} 
Figure 2. Annotated SLD-tree for So. 
Defining distributions over atoms 
f>. defines a distribution over atoms via marginalisation. First define the yield of a 
refutation and the proofs of an atom. 
DEFINITION 4. The yield Y (r) of a refutation r of unit goal G = f- A is A(I 
where (I is the computed answer for G using r. The set of proofs for an atom Yk is 
the set X(Yk) = {rIY(r) = yd. Note that X(Y(r)) is the set of all refutations 
that yield the same atom as r. 

248 
JAMES CUSSENS 
We only define yields with respect to unit goals. This is just a convenience, 
since given a non-unit goal t- G l , ... , G M, we can always add the clause A' t-
Gl , ... , G M, where A' contains all the variables of Gl , ... , G M, and then con-
sider yields of t- A'. Note that from a logical perspective a refutation of t- A 
with computed answer fJ amounts to a proof of AfJ, so this choice of terminology 
is natural. 
We now define a distribution P()..,S,G) over atoms in terms of their proofs. 
(3) 
p()..,S,G) (Yk) d~f 2: 
f()..,s,G)(r) = Z(>.~S,G) 2: 
e)..Â·v(r) 
rEX(Yk) 
rEX(Yk) 
If G has t variables, then P()..,S,G) (Yk) defines a t-dimensional distribution over 
variable bindings for these t variables. Note that we allow non-ground bindings 
unlike in [Muggleton, 1996; Cussens, 1999b; Muggleton, 2000]. We will see in 
Section 2.3 how we can use these t-dimensional distributions to encode probabilis-
tic models using other formalisms into SLPs. Returning to the example SLP So 
we find that it defines a distribution over the sample space {s(a), s(b)}, where 
P()..,So,t-s(X)) (s(a)) = (0.4 x 0.3 x 0.3 + 0.6 x 0.2)/0.832 = 0.1875 
and 
P()..,So,t-s(X)) (s(b)) = (0.4 x 0.7 x 0.7 + 0.6 x 0.8)/0.832 = 0.8125 
2.3 Relations to some existing probabilistic models 
In this section we encode three familiar probabilistic models into SLPs. Consid-
erably more complex SLPs encoding, for example, distributions over a hypothesis 
space of logic programs are used in [Cussens, 2000b]. 
Figure 3 shows the Asia Bayesian network and an SLP representation of it, 
where p( S ,)..,+-asia( A,T,E ,S,L,B,X ,D)) gives the joint distribution represented by the 
Bayesian net. The equation 
P(A,T,E,S,L,B,X,D) 
= P(A)P(S)P(TJA)P(LJS)P(BJS)P(EJT,L)P(DJE,B)P(XJE) 
is directly encoded using an impure, unnormalised SLP, with each of the 8 condi-
tional probability tables defined by a single predicate. Since E is a function of T 
and L, we only need 4 unparameterised clauses to encode P(EJT, L) as opposed 
to the 8 that would be required if P(EJT, L) were encoded as the other condi-
tional probability distributions are. It is clear that any Bayesian net with discrete 
variables can be represented by an SLP in this manner. 
The translation from Bayesian net to SLP is problematic in that the direction-
ality of Bayesian nets is obscured. In contrast, the mapping between Markov nets 
and SLPs is transparent. Figure 4 shows a Markov net derived from the Asia 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
249 
asia(A,T,E,S,L,B,X,D) :-
a(A), s(S), t_a(T,A), 
l_s(L,S), b_s(B,S), 
e_t1(E,T,L), d_eb(D,E,B), 
x_e(X,E). 
O.Ol:a(1) . 
0.50:5(1). 
0.99:a(0). 
0.50:5(0). 
0.05:t_a(1,1). 0.95:t_a(0,1). O.l:t_a(l,O). 0.9:t_a(0,0). 
e_t1(0,0,0). 
e_t1(1,0,1). 
e_t1(1,1,0). 
e_t1(1,1,1). 
t. 1_5/2, b_s/2, d_eb/2 and x_e/2 definitions omitted 
Figure 3. Asia Bayesian net and its encoding as an SLP. 
Bayesian net and its translation to an impure unnormalised SLP. The structure of 
the Markov net can be completely described with a single clause, and the 6 clique 
potentials each get their own predicate symbol. 
asia(A,B,D,E,L,S,T,X) :-
c6(E,X), c5(E,B,D), c4(L,B,S), 
c3(L,E,B), c2(E,L,T), c1(A,T). 
0.0005:c1(1,1).0.0095:c1(1,0). 
0.0099:c1(0,1) 0.9801:c1(0,0). 
%C1auses for c2, c3, 
/,c4, c5, c6 omitted 
Figure 4. Asia Markov net and its encoding as an SLP. 
Since SLPs generalise stochastic context-free grammars (SCFGs) it 
is easy to encode SCFGs as SLPs. 
Consider the context-free grammar 
S -t aSa I bSb I aa I bb which generates palindromes. By placing a probability 
distribution over the four productions we have an SCFG which defines a distribu-
tion over palindromic strings of as and bs. SPALINDROME in Figure 5 encodes 
such an SCFG as an SLP where P('x,SPALINDROME,<-S(X,[])) is the distribution over 
strings. Hidden Markov models, which are essentially stochastic regular gram-
mars, can be dealt with similarly. Figure 6 shows an SLP defining a distribution 
over the the language { an bn en} which is not context-free. 
Consider now the SLP in Figure 7 which represents a simple linear Markov net. 
A is independentofC givenB (A 1.. CIB). For instance P(A = foolB = foo) = 
P(A = foolB = faa, C = faa). This conditional independence phenomenon 

250 
0.5:s(A,D) 
0.3:s(A,D) 
JAMES CUSSENS 
A=[a!B], s(B,C), C=[a!D]. 
A=[b!B], s(B,C), C=[b!D]. 
O.l:s([a,a!T],T). 
O.l:s([b,b!T],T). 
Figure 5. SPALINDROME, an SLP representation of an SCFG. 
anbncn(A) :- build(A-B,B-C,C-[]). 
0.3: build(A-A,B-B,C-C). 
0.7: build([a!A]-Ap, [b!B]-Bp, [c!C]-Cp) 
build(A-Ap,B-Bp,C-Cp). 
Figure 6. Stochastic non context-free grammar defined with an SLP 
is central to probabilistic graphical models such as Markov nets. But note that A 
is independent of C given any value of B. Sometimes such a strong assumption 
will not be justified. It may be that A is only independent of C given particular 
values of B. This conditional conditional independence2 or context-specific inde-
pendence between A and C crop up often in applications and has been investigated 
by Boutilier et al [Boutilier et al., 1996]. 
linear(A,B,C) :-
c1(A,B), 
c2(B,C). 
0.2 
0.1 
0.3 
0.4 
c1(foo,foo) . 
c1 (foo, bar) . 
c1(bar,foo) . 
c1 (bar, bar) . 
0.3 
0.2 
0.3 
0.2 
c2(foo,foo). 
c2(foo,bar). 
c2(bar,foo). 
c2(bar,bar). 
Figure 7. Linear Markov net SLP 
To represent context-sensitive independence, it is necessary to differentiate be-
tween these two sorts of values of B. Assume that there are two predicates, 
strong/land weak/l defined to be mutually exclusive which achieve this. The 
SLP in Figure 8 then defines an appropriate mixture model. A neater alternative 
might be to use negation to differentiate, using strong(B) and \+strong(B)3, 
but the use of negation in SLPs has yet to be properly investigated, hence the cur-
rent restriction to definite clauses. 
2conditional on a variable and conditional on values of that variable 
3\ + is ISO Prolog notation for not. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
mixlin(A,B,C) :-
strong(B) , 
c1(A,B) , 
c2(B,C). 
mixlin(A,B,C) :-
weak(B) , 
c3(A,B,C). 
%labelled defs of c1, c2 and c3 omitted 
Figure 8. Mixture model SLP defining context-specific independence 
2.4 Representing degrees of belief 
251 
In an SLP, a labelled rule l : p(X, Y) +- q(X, Y), r(Y) does not define the prob-
ability that some ground atomic formula p(a, b) is true (as in KBMC, see Sec-
tion 3.2), nor does it provide bounds on the probability that p(a, b) is true as in 
[Shapiro, 1983; Ng and Subrahmanian, 1992]. Instead, there is a binary distribu-
tion associated with p(X, Y) which defines the probability of instantiations such 
as {X/a, Y/b}. In order to reason about the probability of the truth of atomic 
formulae, atomic formulae are simply augmented by introducing an extra logical-
random variable to represent the truth value of unaugmented atomic formulae, and 
then this logical-random variable is treated exactly as any other. This is in keeping 
with a conservative approach-if the truth value of an atomic formula as it varies 
across different "possible worlds" is of interest-then this variation is modelled in 
the standard way: with a random variable. 
Consider a probabilistic rule from [Koller and Pfeffer, 1997], where the "rule 
says that when a person's parent has a gene, the person will inherit it with proba-
bility OS'. 
genotype(P,G) :- (0.5) parent (P,Q), genotype(Q,G). 
It is possible to encode such "degree of belief" probability in an SLP with a 
Boolean truth-value variable as in Figure 9. However, this parameterised clause 
is only guaranteed to define the desired probability if it is the only clause for 
genotype/3. 
To find the probability that genotype(bob, big..ears) is true, the SLP is used 
to compute the probabilities of genotype(bob, big..ears, 1) and genotype(bob, 
big..ears,O). This amounts to demanding arguments (=proofs) for the truth of 
genotype (bob, big..ears) and for its falsity. The strength of these proofs are then 
balanced when deciding on the probability of truth. 

252 
JAMES CUSSENS 
genotype(P,G,T) 
parent(P,Q), 
genotype(Q,G,l), half(T). 
o . 5 
half (1) . 
0.5 
half(O). 
Figure 9. Representing degree of belief with an extra variable 
3 
RESEARCH ON LOGIC AND PROBABILITY: A SAMPLE 
Here a number of alternative approaches to integrating logic and probability are 
considered. This is not a comprehensive survey; but, in the spirit of statistical infer-
ence, it hopefully constitutes a sufficiently representative sample to allow suitably 
qualified inferences. 
3.1 
Probabilities in logic programming 
Clark and McCabe 
Adding an extra argument to atomic formulae to represent the degree of belief as-
sociated with such atomic formulae, as in Section 2.4, has a long history in logic 
programming, although the mechanisms for propagating probabilities through Pro-
log rules have often been ad hoc. 
In [Clark and McCabe, 1982], the extra argument is a probability so that if the 
query f- r(a, b, P) renders an answer substitution P = 0.2, this means that r(a, b) 
holds with probability 0.2. Extra literals are added to the clause which compute the 
probability in the head atomic formula as a function of probabilities of the body 
atomic formulae: they embody a combination rule. There the combination rule is 
simple multiplication (which represents the independence assumptions in SLPs). 
Clark and McCabe also propose more complex combination rules where a query 
matches more than one clause head. Again it is up to the user to devise these rules. 
With SLPs only combination by mixing is allowed. 
Obtaining a degree-of-belief probability (in, say, r( a, b) being true) is both more 
direct and more flexible in [Clark and McCabe, 1982] than with SLPs. It is more 
direct, because there is a single extra argument representing degree of belief. With 
SLPs all refutations of f- r(a, b, T) are found and the ratio of T = 0 to T = 1 
answer substitutions is computed. (Assume that the SLP has been so written so that 
other substitutions are not possible.) When implementing an SLP in Prolog, there 
are two extra arguments, one for the truth-value and one for the clause parameter. 
In [Clark and McCabe, 1982], it is up to the "expert" to define how probabilities 
propagate up to the query, whereas SLPs are representations of particular sorts of 
distributions. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
253 
Shapiro's logic programs with uncertainties (LPUs) 
Shapiro's approach is close to that of SLPs: the calculation of probabilities is 
carried out by a meta-interpreter and if all of Shapiro's certainty factors are 1, 
then operational and declarative semantics degenerate to those of standard logic 
programs. 
LPUs involve certainty factors and certainty functions mapping multisets of 
certainty factors (e.g. the multiset of certainty factors from the body of a clause) 
to a single certainty factor. This is close to the Prolog implementation of SLPs, 
except that the SLP 'certainty function' (simple multiplication) is not necessarily 
monotonic increasing as Shapiro's have to be. 
With an LPU, deducing p(a) with certainty 0: is tantamount to proving that 
Pr(p(a)) ~ 0:. Shapiro defines a minimal model for a LPU where each atomic 
formula is associated with the minimal probability with which that atomic formula 
is true. Consequently, the first clause below is redundant. With SLPs, there is 
no direct representation of degrees of belief and exact probabilities are computed, 
rather than bounds on probabilities. Also the probabilities associated with different 
refutations of a goal add up, so that neither clause would be redundant. 
< p(a) :- true, 0.2> 
< p(a) :- true, 0.3> 
Shapiro introduces an interpreter which essentially searches for a proof that a 
query is true with probability above a certain threshold. It is possible to prune this 
search in many cases-this would be harder with SLPs since it is necessary to add 
up a large (possibly infinite) number of refutations of a goal to get its probability. 
Probabilistic logic programming 
Probabilistic Logic Programming (hereafter PLP) is an extension of Logic Pro-
gramming developed by Ng and Subrahmanian in a series of papers [Ng and Sub-
rahmanian, 1992; Ng and Subrahmanian, 1993; Ng and Subrahmanian, 1994]. 
In [Ng and Subrahmanian, 1992], the basic ideas of PLP are presented, in [Ng 
and Subrahmanian, 1993] variables ranging over probabilities are introduced and 
in [Ng and Subrahmanian, 1994] non-monotonic negation is added. As with 
Shapiro's LPUs, PLPs are designed for the propagation of constraints on prob-
ability values in the form of intervals, rather than the computation of exact proba-
bilities as in SLPs. 
In PLP, conjunctions and disjunctions o(iiterals are annotated with probability 
intervals, allowing clauses such as (4-7). 
(4) 
eastbound(trainl) : [0.7,0.9] 
+-
(5) 
bark(X) : [0.95,1] 
+-
dog(X): [1,1] 
(6) 
(bark(X) 1\ dog(X)) : [0.95 * V, 0.95 * V] 
+-
dog(X): [V, V] 
(7) 
noLdog(X) : [1- V2, 1- Vd 
+-
dog(X): [Vi, V2] 

254 
JAMES CUSSENS 
The probabilities here are to be interpreted as possible-world probabilities, and can 
be translated into Halpern's logic as follows: 
(8) 
w(eastbound(train1)) E [0.7,0.9] +-
(9) 
'<Ix: w(bark(x)) E [0.95,1] +-
w(dog(x)) = 1 
(10) '<Ix, v: w(bark(x) 1\ dog(x)) = 0.95 * v +-
w(dog(x)) = v 
(11) 
'<Ix, v: nOLdog(x) E [1 -
V2, 1 - VI] 
+-
w( dog(X)) E [VI, V2] 
Ng and Subrahmanian introduce probability intervals, since point probability val-
ues for conjuncts and disjuncts are not derivable from their constituents. In general 
only intervals are derivable, as shown in (12) and (13). 
P(a) E [Pa, <Sa], P(,8) E [P/3, <S/3l 
(12) 
F= 
P(a 1\,8) E [max{O, Pa + P/3 - I}, min{ <Sa, <Si3l] 
P(a) E [Pa, <Sa], P(,8) E [P/3, <S/3] 
(13) 
F= 
P(aV,8)E[max{Pa,pi3l,min{l,<Sa+<S/3}] 
It is worth stressing that PLP can not express domain probabilities directly. A 
PLP statement such as 
(14) bronchitic(X): [0.4,0.4] +- smoker(X) : [1.0,1.0] 
which translates to 
(15) '<Ix: w(bronchitic(x)) = 0.4 +- w(smoker(x)) = 1 
does not state that 40% of smokers are bronchitic, but that for each particular 
smoker (i.e. someone who is a smoker in all worlds) there is exactly a 40% chance 
that that individual is bronchitic, a much stronger statement. Hence (15) would 
be contradicted by a single smoker a who is known definitely to be or not to be 
bronchitic, since, respectively, this would translate as w(bronchitic( a)) = 1 or 
w(bronchitic(a)) = O. 
3.2 
Logic and Bayes nets 
Knowledge-based model construction 
Knowledge-based model construction (KBMC) [Ngo et al., 1995] is described by 
Koller and Pfeffer [1997]: 
Knowledge-based model construction (KBMC) goes a considerable 
way towards bridging this gap [between logic and probability] by al-
lowing a set offirst-order probabilistic logic (FOPL) rules (first-order 
rules with associated probabilistic uncertainty parameters) to be used 
as a basis for generating Bayesian networks tailored to particular prob-
lem instances. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
255 
Normally, one constructs a fixed Bayesian network such as the "Asia" net, so 
that it is 'ready' to process any potential query. In KBMC, a Bayesian network 
is constructedfor a particular query, and then that network is used to answer the 
query. 
This query-dependence is built into logic programming with SLD-resolution-
for each query there is a particular SLD-tree. Also, Prolog searches for a refutation 
in the tree, rather than constructing the complete tree and then looking for refuta-
tions. The latter is not even an option when the tree is infinite. 
In Knowledge-based model construction (KBMC) [Ngo and Haddaway, 1997; 
Koller and Pfeffer, 1997; Haddaway, 1999] first-order rules with associated proba-
bilities are used to generate Bayesian networks for particular queries. As in SLD-
resolution queries are matched to the heads of rules, but in KBMC this results in 
nodes representing ground facts being added to a growing (directed) Bayesian net-
work. A context is defined using normal first-order rules, perhaps explicitly as a 
logic program [Ngo and Haddaway, 1997], which specifies logical conditions for 
labelled rules to be used. The ground facts are seen as Boolean variables (either 
true or false). Once the Bayesian network is built it is then used to compute the 
probability that the query is true. 
In KBMC, as in much of the work connecting logic and probability, parame-
terised first-order rules a : c(X) +- a(X) are connected to conditional probability 
statements such as p(c(b)la(b)) = a. Also the objective is to compute the proba-
bility that an atomic formula is true. With SLPs, the focus is more on undirected 
representations, so that I : p(X, Y) +- q(X, Y), r(Y) forms part of the definition 
of a binary distribution associated with pl2 defined in terms of distributions asso-
ciated with ql2 and r 11. No attempt to model causality is made. Despite these 
differences in approach there are clear similarities between KBMC query-specific 
Bayes net construction and the query-specific exploration of an SLD-tree by Pro-
log which deserve further investigation. 
Ngo and Haddaway 
In [Haddaway, 1999], Haddaway states that " ... a Bayesian net is essentially a 
propositional representation of the domain: each node represents a multi-valued 
propositional variable. Thus it is not possible to express general relationships 
among concepts without enumerating all the potential instances in advance". The 
existence of nodes representing random variables with countably and uncountably 
infinite domains would seem to contradicc~his view. Such nodes are common-
place in many applications (see, for example, the documentation accompanying 
the BUGS system [Spiegelhalter et al., 1996]). In any case, SLPs take a different 
view of the connection between logical and random variables: the nodes (random 
variables) in a Bayes net are identified with logical variables, ranging over a pos-
sibly infinite domain. 
In [Ngo et al., 1995] Haddaway uses a normal logic program as a context base. 
Since random variables are functions (measurable ones from some probability 

256 
JAMES CUSSENS 
space to some other space), Haddaway takes the natural step of representing them 
as such. Haddaway has (n + 1)-ary probabilistic p-predicates which map n-tuples 
to values. The value of the random variable is constrained to be the last argument 
of the relevant atomic formula. With each p-predicate p there must be a statement 
of the form V AL(P) = {VI, V2, ... vn} where the Vi are constants enumerating 
the set of possible values of the random variable represented by the p-predicate. 
If A = p( tl , ... , tn, tn+ 1) is a p-atomic formula then obj (A) is used to denote 
the tuple (tl,"" tn) and V AL(A) denotes the value tn. There is thus a demar-
cation between individuals (such as obj(A) and attributes of those individuals: 
VAL(A). 
The probabilistic base of a knowledge base is a set of probabilistic sentences 
of the form (P(AoIA1 , ... , An) = 0:) +- L1 , â¢.. Ln where the Ai are p-atomic 
formulae and the Li are context literals. For example: 
P(burglary(X,yes)lnbrhd(X, good)) =.3 +- in_CALI(X) 
To this are added various type declarations, and rules for combining rules. Had-
daway distinguishes evidence (ground p-atomic formulae) and contextual informa-
tion, which is a set of context atomic formulae. This is in contrast to the SLP ap-
proach where evidence is supplied by instantiating variables in the top-level query 
to the SLP, or by specialising the logic program. Contextual information is repre-
sented in the same way in both approaches. Queries take the form of P(Q) =? 
where the last argument of Q is a variable in Q - the query is asking for the poste-
rior distribution of obj( Q) 
Query answering uses an algorithm (Q-procedure), which builds a "necessary 
portion of the Bayesian network" where each node of the net corresponds to an 
obj(A), where A is a relevant ground p-atomic formula. The Bayesian net is 
updated using the evidence and the probabilities associated with the query are 
computed in the normal fashion. 
Space prevents giving the BUILD-NET procedure for constructing the Bayesian 
net from a query, instead just note that there are similarities with query-answering 
in SLPs. In BUILD-NET all sentences that have the input atomic formula as con-
sequent are collected and the associated probabilities combined according to a 
combination rule. This is analogous to the SLP approach of finding all refutations 
of a goal-except we, unlike Haddaway, only allow one form of combination. 
Haddaway's dynamic query-driven construction of a Bayes net is analogous to the 
query-driven construction of an SLD-tree. 
Jaeger: first-order Bayesian nets 
Jaeger [1997] notes that standard Bayesian nets model a multivariate distribution 
over a fixed number of random variables. These apparently have to be "attributes" 
of a single "random event"-the Bayes net can not adequately model relations 
between different random events. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
257 
Jaeger's basic idea is to have Bayesian networks where the nodes are relation 
symbols, r whose values are possible interpretations of r. So the random variables 
are set-valued. In such a set-up, normal first-order formulae set up hard-constraints 
on possible values for the relation symbols in the language. 
In contrast, the SLP approach does not make any fundamental distinction be-
tween attributes and the individuals (Jaeger's "random events") that possess them. 
This follows standard first-order logic semantics and also standard approaches in 
statistics for modelling related multivariate distributions. For example, the domain 
of a model for the first-order theory 
dist(london,york,333). 
has(james,job,family). 
social_unit(family). 
father(james,rob). 
contains elements corresponding to the constants york, london, 333, james, 
job, family and rob and there is no distinction between individuals and at-
tributes. Indeed something which (intuitively) appears as an attribute in one for-
mula can represent an individual in another, e.g. family. 
4 
CONCLUSIONS AND POSSIBLE FUTURE DIRECTIONS 
Sections 2 and 3 contain a number of approaches to integrating logical and prob-
abilistic reasoning. Generally, they are restricted to computing the probabilities 
of atomic formulae. With the exception of SLPs, the atomic formula-probabilities 
are probabilities that the atomic formula is true. Although this is a natural ex-
tension of logical reasoning, it does not allow the identification of logical and 
random variables which the SLP does. Such an identification is likely to be of 
practical importance. Probabilistic inference relying on SLD-resolution is highly 
inefficient. Inference methods which involve storing and transmitting probabilis-
tic information such as Bayes net algorithms based on join trees are considerably 
more efficient. Writing a meta-logical interpreter to perform such inference should 
be able to exploit the identification of logical and random variables using the sort 
of "level-crossing" tricks contained in Earley deduction interpreters [Pereira and 
Shieber, 1987]. 
In a number of cases bounds on probabilities rather than exact probabilities 
are computed. In Probabilistic Logic Programming (PLP), considerable work has 
been done on propagating such constraints, and checking for consistency. The SLP 
approach is more traditional in that there is a fully defined probabilistic (loglinear) 
model from which it is possible to compute all relevant probabilities. It would 
be interesting to see to what extent these two approaches could be unified. One 
possibility would be to extend SLPs to define constraints on probabilities rather 
than exact values: constraint logic programming over reals (CLP(R)) is an obvious 
choice here. 

258 
JAMES CUSSENS 
On a more fundamental level, there are connections between the 'tree-shaped' 
distributions which an SLP defines via SLD-resolution and the probability trees 
that Shafer uses as "a language for talking about the structure of contingency" 
[Shafer, 19961. Shafer's aim is to provide a more dynamic basis for probability 
than the usual static set-theoretic Kolmogorovian axioms of probability theory. 
Rather than concern ourselves merely with the probability that an event happens, 
Shafer seeks to connect that probability with a probabilistic history of those events 
that led up to it. Shafer refers to the final event, as a Moivrean event-this is 
the event of standard probability theory. The events leading up to a Moivrean 
event- the changes "in some object or circumstance at a particular time" are 
called Humean events by Shafer. SLPs also have this division: there is the Humean 
event of choosing a particular clause head, and there is the Moivrean event of get-
ting a particular instantiation for the variables in a top-level goal. 
ACKNOWLEDGEMENTS 
Many thanks to Sara-Jayne Farmer for weeding out various errors and omissions. 
Thanks to Stephen Muggleton for useful discussions on the role of normalisation 
in SLPs and to Stefan Riezler for clarifying his method. Thanks also Gillian, Jane 
and Robert Higgins for putting up with me. Finally thanks to Luc de Raedt for 
encouraging me to investigate first-order Bayesian nets. 
This chapter is an updated version of [Cussens, 1999a1. 
Department of Computer Science, University of York, UK. 
BIBLIOGRAPHY 
[Abadi and Halpern, 1994] Martin Abadi and Joseph Y. Halpern. Decidability and expressiveness for 
first-order logics of probability. Information and Computation, 112(1):1-36, 1994. 
[Abney,1997] Steven Abney. 
Stochastic attribute-value grammars. 
Computational Linguistics, 
23(4):597-618, 1997. 
[Bacchus et al., 1996] Fahiem Bacchus, Adam Grove, Joseph Y. Halpern, and Daphne Koller. From 
statistics to belief. Artificial Intelligence, 87:75-143, 1996. 
[Boutilier et al., 1996] Craig Boutilier, Nir Friedman, Moises Goldszmidt, and Daphne Koller. 
Context-specific independence in Bayesian networks. In Proceedings of the Twelfth Annual Con-
ference on Uncertainty in Artificial Intelligence (UAI-96), pages 115-123, Portland, Oregon, 1996. 
[Clark and McCabe, 1982] K.L. Clark and F.G. McCabe. PROLOG: a language for implementing ex-
pert systems. In I.E. Hayes, Donald Michie, and Y-H Pao, editors, Machine Intelligence, volume 10, 
chapter 23, pages 455-470. Ellis Horwood, Chichester, 1982. 
[Cussens,1996] James Cussens. Deduction, induction and probabilistic support. Synthese, 108(1):1-
10, July 1996. 
[Cussens, 1999a] James Cussens. Integrating probabilistic and logical reasoning. Electronic Transac-
tions on Artificial Intelligence, 3(B):79-103, 1999. Selected Articles from the Machine Intelligence 
16 Workshop. 
[Cussens, 1999b] James Cussens. 
Loglinear models for first-order probabilistic reasoning. 
In 
Kathryn B. Laskey and Henri Prade, editors, Proceedings of the Fifteenth Annual Conference on 
Uncertainty in Artificial Intelligence (UAI-99), pages 126-133, Stockholm, 1999. Morgan Kauf-
mann. 

INTEGRATING PROBABILISTIC AND LOGICAL REASONING 
259 
[Cussens,2000a] James Cussens. Attribute-value and relationalleaming: A statistical viewpoint. In 
Luc De Raedt and Stefan Kramer, editors, Proceedings of the ICML-2000 Workshop on Attribute-
Value and Relational Learning: Crossing the Boundaries, pages 35-39, 2000. 
[Cussens, 2000b] James Cussens. Stochastic logic programs: Sampling, inference and applications. 
In Craig Boutilier and Moises Goldszmidt, editors, Proceedings of the Sixteenth Annual Conference 
on Uncertainty in Artificial Intelligence (UAI-2000), pages 115-122, Stanford, CA, 2000. Morgan 
Kaufmann. 
[Cussens, 2001a] James Cussens. Parameter estimation in stochastic logic programs. Machine Learn-
ing, 44(3): 245-271,2001. 
[Cussens, 2001 b] James Cussens. Statistical aspects of stochastic logic programs. In Tommi Jaakkola 
and Thomas Richardson, editors, Artificial Intelligence and Statistics 2001: Proceedings of the 
Eighth International Workshop, pages 181-186, Key West, Rorida, January 2001. Morgan Kauf-
mann. 
[Della Pietra et al., 1997] S. Della Pietra, V. Della Pietra, and 1. Lafferty. Inducing features of random 
fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380-393, April 
1997. 
[Eisele, 1994] Andreas Eisele. Towards probabilistic extensions of constraint-based grammars. Con-
tribution to DYANA-2 Deliverable R1.2B, DYANA-2 project, 1994. 
[Haddaway, 1999] Peter Haddaway. An overview of some recent developments in Bayesian problem 
solving techniques. AI Magazine, Spring 1999. 
[Halpern, 1990] Joseph Y. Halpern. An analysis of first-order logics of probability. Artificiallntelli-
gence, 46:311-350,1990. 
[Jaeger, 1997] Manfred Jaeger. Relational Bayesian networks. In Proceedings of the Thirteenth An-
nual Coriference on Uncertainty in Artificial Intelligence (UAI-97), pages 266-273, San Francisco, 
CA, 1997. Morgan Kaufmann Publishers. 
[Koller and Pfeffer, 1997] Daphne Koller and Avi Pfeffer. Learning probabilities for noisy first-order 
rules. In Proceedings of the Fifteenth International Joint Coriference on Artificial Intelligence 
(IJCAI-97), Nagoya, Japan, August 1997. 
[Lari and Young, 1990] K. Lari and SJ. Young. The estimation of stochastic context-free grammars 
using the Inside-Outside algorithm. Computer Speech and Language, 4:35-56, 1990. 
[McCarthy and Hayes, 1969] 1. McCarthy and P. Hayes. Some philosophical problems from the 
standpoint of artificial intelligence. In B. Meltzer and D. Michie, editors, Machine Intelligence 
4, pages 463-502. Edinburgh University Press, Edinburgh, 1969. 
[Muggleton, 1996] S.H. Muggleton. Stochastic logic programs. In L. de Raedt, editor, Advances in 
Inductive Logic Programming, pages 254-264. lOS Press, 1996. 
[Muggleton, 2000] S.H. Muggleton. Semantics and derivation for stochastic logic programs. In 
Richard Dybowski, editor, Proceedings of the UAI-2000 Workshop on Fusion of Domain Knowl-
edge with Datafor Decision Support, 2000. 
[Ng and Subrahmanian, 1992] Raymond Ng and V.S. Subrahmanian. Probabilistic logic program-
ming. Information and Computation, 101(2):150-201, 1992. 
[Ng and Subrahmanian, 1993] Raymond Ng and V.S. Subrahmanian. A semantical framework for 
supporting subjective and conditional probabilities in deductive databases. Journal of Automated 
Reasoning, 10(2):191-235, 1993. 
[Ng and Subrahmanian, 1994] Raymond Ng and V.S. Subrahmanian. Stable semantics for probabilis-
tic databases. Iriformation and Computation, 110(1):42-83, 1994. 
[Ngo and Haddaway, 1997] L. Ngo and Peter Haddaway. Answering queries from context-sensitive 
probabilistic knowledge bases. Theoretical Computer Science, 171:147-171, 1997. 
[Ngo et al., 1995] Liem Ngo, Peter Haddawy, and James Helwig. 
A theorectical framework for 
context-sensitive temporal probability model construction with application to plan projection. In 
Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UA/-95), 
pages 419-426, Montreal, Quebec, Canada, 1995. 
[Pereira and Shieber, 1987] Fernando C. N. Pereira and Stuart M. Shieber. 
Prolog and Natural-
Language Analysis. CLSI, Stanford, 1987. 
[Popper and Miller, 1987] Karl R. Popper and David Miller. Why probabilistic support is not induc-
tive. Philosophical Transactions of the Royal Society of London, 321 :569-591, 1987. 
[Riezler, 1997] Stefan Riezler. Probabilistic constraint logic programming. Arbeitsberichte des SFB 
340 Bericht Nr. 117, Universitat Tiibingen, 1997. 

260 
JAMES CUSSENS 
[Riezler,1998] Stefan Riezler. Probabilistic Constraint Logic Programming. PhD thesis, Universitat 
Tiibingen, 1998. AIMS Report 5(1),1999, IMS, Universitat Stuttgart. 
[Shafer, 1996] Glenn Shafer. The Art of Causal Conjecture. MIT Press, Cambridge, Mass., 1996. 
[Shapiro, 1983] Ehud Shapiro. Logic programs with uncertainties: A tool for implementing rule-
based systems. In Proc. lJCAI-83, pages 529-532, 1983. 
[Spiegelhalter et al., 1996] D. Spiegelhalter, A. Thomas, N. Best, and W. Gilks. BUGS 0.5 Bayesian 
inference using Gibbs Sampling Manual. MRC Biostatistics Unit, Cambridge, 1996. 

PART III 
BAYESIANISM AND DECISION THEORY 

RICHARD BRADLEY 
RAMSEY AND THE MEASUREMENT OF BELIEF 
1 THE PROBLEM OF MEASUREMENT 
Bayesian decision theories are formal theories of rational agency that tell us both 
what the properties of a rational state of mind are and what action it is rational for 
an agent to perform, given her state of mind. What makes a theory of this kind 
Bayesian is commitment to the following claims: 
1. The (only) factors relevant to rational decision making are the options avail-
able, the relative subjective desirability of the possible outcomes of choosing 
one or another option and the subjective likelihood that each outcome will 
be achieved, given the exercise of a particular option. 
2. To act rationally is to choose the option (or one of the options) which has 
the best expected consequences, given one's partial beliefs and desires. 
3. Rational degrees of belief are probabilities. 
4. Rational degrees of desire are utilities. 
Different versions of Bayesian decision theory, such as those of Savage and 
Jeffrey, differ with respect to the kinds of options they postulate, with respect 
to the precise interpretation of the second claim (the expected utility hypothesis) 
and with respect to further constraints they place on rational desire. But such 
differences will not affect our discussion here. (Nor will the question of whether 
Bayesians are committed to conditionalisation as means of revising partial beliefs). 
This paper will discuss a particular problem for Bayesian decision theories that 
derives from the fact that its main variables -
an agent's degrees of partial belief 
and desire -
are not (directly) observable, but have to be inferred from what 
the agent says and does. It is my contention that there is much to be learnt in this 
regard from a proposal made by Frank Ramsey in his paper 'Truth and Probability' 
[Ramsey, 1926]. His proposal will be accordingly be examined in considerable 
detail and then, having established it viability, defended against some influential 
objections. 
1.1 
Two Kinds of Problems 
Let us start by distinguishing two kinds of problems concerning the status of scien-
tific theories which can be called the problems of justification and of measurement. 
The first is the problem of validating the claims made by a theory about the ob-
jects and relations of which it speaks. The second is the problem of determining. 
263 
D. Corfield and 1. Williamson (eds.), Foundations of Bayesianism, 263-290. 
Â© 2001 Kluwer Academic Publishers. 

264 
RICHARD BRADLEY 
in a particular context, the values of the theory's variables. Of course, it is often 
the case that the same or similar observations will be used by scientists both to 
confirm a theory's claims and to determine the values of its variables. But the two 
problems are clearly distinct: the task of justifying Newton's first law of mechan-
ics is different to that of measuring the forces, masses and acceleration of physical 
objects. Indeed the truth of theories may be presupposed in the interpretation of 
measurement observations, such as when Newton's law is used to calculate forces 
from masses and accelerations. 
For a decision theory the problem of justification concerns both the claims it 
makes about the properties of consistent partial belief, desire and preference and 
the claims it makes about the rational way to act in the light of them. The mea-
surement problem on the other hand boils down to the problem of determining an 
agent's partial beliefs and desires on the assumption that they have the properties 
postulated by the theory. Banal though the distinction between these problems 
may be, often enough it is neglected in debates within Bayesian decision theory. 
The reason is, I suspect, that Bayesian decision theories can be interpreted both 
as normative and as descriptive theories of agency and that its representation theo-
rems can be construed as providing a solution to either the problem of justification 
or that of measurement, depending on the interpretation of the theory. 
Let me explain in a bit more detail. Decision-theoretic representation theorems 
show that some set of conditions on an agent's preferences determines the exis-
tence of a quantitative representation of her partial beliefs and desires, consistent 
with her preferences. If the conditions on preference are naturally construed as 
conditions of rationality, then these theorems can serve to address the problem of 
justification in the sense that they show why the theory is normatively compelling. 
For they imply that by accepting that rational preference should satisfy the con-
ditions in question, one is committed to accepting the theory's claims about the 
properties of rational partial belief and desire. The problem of justification then 
reduces to a defence of these qualitative rationality conditions on preference -
supposedly an easier task. 
I simplify considerably, of course. Some of the conditions that such theorems 
postulate are not rationality conditions but idealisations, designed to make the issue 
mathematically tractable. The ubiquitous completeness and continuity conditions 
are cases in point. But the thought is that these idealisations do not distort the 
main results, so that the relation between, for instance, incomplete preferences 
and incomplete probabilities and utilities should approximate the relation between 
complete ones. It would be good to see this rigorously demonstrated, but the claim 
has considerable plausibility. For why should making up my mind about matters 
concerning which I previously had no attitude affect, say, whether my beliefs about 
those that I do are probabilities or not? 
Representation theorems also function as demonstrations of the measurability 
of the main decision theoretic variables -
degrees of belief and desire. Or rather 
they show that if the idealised conditions postulated by the axioms of preference 
are realised then knowledge of agents' preferences suffice for knowledge of their 

RAMSEY AND THE MEASUREMENT OF BELIEF 
265 
degrees of belief and desire. In this context the way in which we evaluate the 
axioms is different from before. When justification was at issue we asked of the 
axioms of preference whether they were really rationality conditions. When the 
task is measurement, and our interest descriptive rather than normative, we ask 
whether the conditions are actually satisfied by agent's preferences or whether they 
can be made to satisfy them. These questions may be connected in practice. There 
may be evolutionary grounds, for instance, for supposing that the preferences of 
actual agents will by and large satisfy rationality conditions. But in principle, one 
is not required to solve the problem of justification in order to solve the measure-
ment problem. In particular it is not necessary to establish the normative validity 
of Bayesian decision theory to demonstrate the measurability of the variables it 
postulates. 
1.2 
The Evidence Base 
We now focus on the concern of this paper: the problem of measurement. To solve 
it we need to do two things. First we need to identify what sort of evidence can 
be used to determine agents' states of partial belief and desire. And second, we 
need show how and to what extent the evidence in question determines a measure 
of these states. We shall attend to the first question in this section and then devote 
most of the rest of the paper to the second, taking for granted a particular answer 
to the first. 
Behaviourism 
Historically discussion of this issue has been dominated by what I will call Epis-
temological Behaviourism: a rather puritanical doctrine of empiricist descent ac-
cording to which the only acceptable evidence for an agent's mental states is in-
tersubjectively observable behaviour. Evidence obtained by introspection, in par-
ticular, should not be countenanced as introspected states are not intersubjectively 
observable (even if 'observable' by the person whose states they are). As a con-
sequence reports by someone on their mental states are to count as evidence only 
insofar as any perceptible effect of a mental cause can count as evidence. It does 
not follow from someone reporting that they believe x, that they do so, except if it 
has already been established through behavioural evidence that such belief reports 
are reliable. 
Epistemological Behaviourism is to be distinguished from the associated meta-
physical and semantic doctrines according to which mental states are nothing 
more than dispositions to behaviour or, more radically, than sets of observable 
behaviours (e.g. to desire that x is to act in ways which tend to bring it about 
that x, to prefer x to y is to choose x over y whenever both are available). Both 
forms of Behaviourism have been influential in the development of decision the-
ory. On the metaphysical side, for instance, De Finetti and to some extent Savage 
saw themselves as giving behavioural meaning to the concept of probability, while 

266 
RICHARD BRADLEY 
Samuelson saw his axioms of weak preference as giving behavioural meaning to 
the concept of preference. Ramsey too gestures in this direction. 
It is with the epistemological form of Behaviourism that I am concerned, how-
ever, because of its influence on Bayesian decision theorists' understanding of the 
problem of the measurement. In behaviourist spirit, Bayesian decision theorists 
typically take a satisfactory solution of this problem to be a demonstration that de-
cision theoretic representations of the states of minds of rational agents are char-
acterisable in terms of sets of intersubjectively observable behaviours, that there 
exist (so to speak) defining sets of observable behaviours for each possible state of 
mind. This attitude is exemplified by Savage's claim regarding strengths of belief: 
"If the state of mind in question is not capable of manifesting itself 
in some sort of extraverbal behaviour, it is extraneous to our main 
interest. If, on the other hand, it does manifest itself through more 
material behaviour, that should, at least in principle, imply the possi-
bility of testing whether a person holds one event to be more probable 
than another, by some behaviour expressing, or giving meaning to, his 
judgement." 1 
If such a reduction of belief and desire to preference were possible it would, 
of course, represent a solution of a kind, for it would imply that a set of obser-
vations of someone's behaviour with the right kinds of properties would suffice 
to determine the values of the main decision theoretic variables. But contrary to 
what seems to be prevailing belief, such a reduction has never been successfully 
achieved and I doubt very much will ever be so. The argument for this is quite 
simple. An agent's state of indifference between two options is something that 
decision theories can (and should) represent. But no behaviour can ever attest in 
a conclusive manner to someone's indifference between two or more possibilities. 
So decision-theoretic models are necessarily underdetermined by behavioural evi-
dence (even all possible behavioural evidence, elicited under ideal conditions). 
Let me elaborate a little by consideration of the example of Savage's representa-
tion theorem, since this is probably the best known.2 Formally what Savage shows 
is that a binary relation, ~, on a set of actions (actions on his account being func-
tions from events to outcomes) that satisfies Savage's axioms will determine the 
existence of a utility function, u, on outcomes (unique up to choice of scale) and a 
unique probability function, Pr, on events such that for all actions, al and a2, al ~ 
a2 <===> EU(al) ~ EU(a2)' where EU(al) is the expectation of utility given al, 
relative to u and Pro If ~ is interpreted as the 'at least as preferred as' relation then 
it is plausible to construe u and Pr as respectively measures of the agent's degrees 
of desire and belief. We are now very near to what we want. Having reduced 
1 Savage [J 954, p. 27-281. This passage is a little misleading: Savage's views seem on closer 
examination to be more like those that I will attribute to Ramsey. But it accurately represents the 
behaviourist tradition in decision theory. 
2See Savage [1954]. 

RAMSEY AND THE MEASUREMENT OF BELIEF 
267 
quantitative mental states -
degrees of belief and desire -
to a qualitative one 
-
preference -
all we require now is a reduction of the 'at least as preferred as' 
mental relation to some behavioural correlate. And this seems like it should not 
be too difficult as preferences between actions will be directly manifested in the 
agent's choice of action under suitable circumstances. 
So near and yet so far. Even if we set aside obvious practical difficulties and 
grant that failure to perform an action indicates the presence of a preferred action 
rather than agent's ignorance of its availability, Savage's theory cannot be con-
strued in a behaviouristically acceptable manner. For an agent's choices reveal, if 
anything, her strict preferences between options and not the 'at least as preferred 
as' relations that are the subject of Savage's rationality conditions. Moreover, strict 
preferences need not satisfy these conditions. In particular it would be wrong to 
require that strict preferences be complete, not just because this requires agents 
to have too much knowledge, but because agents can legitimately be indifferent 
between options. 
Could indifference not be revealed by a failure to choose when confronted by 
a non-empty set of alternatives? But such behaviour has a number of possible in-
terpretations. It could show that the agent prefers doing nothing to all of these op-
tions. Or that two or more of the options are equally preferred. Or even that some 
of the options are not comparable. Each possibility implies something different for 
the measurement process: the first that the 'do nothing' option has greater subjec-
tive expected utility than the others, the second that option set needs to be refined 
so as to determine which options it is that have equal subjective expected utility; 
and the third that the measurement cannot be completed since one of its precondi-
tions (the comparability of the options) is not satisfied. Behavioural evidence may 
allow elimination of the first possibility, but in principle it could not discriminate 
between the second and third. One might of course eliminate the problem by forc-
ing a choice in every circumstance, but the evidence so obtained could not then be 
used for the intended purpose without producing measurement artifacts. 
The Use o/Verbal Evidence 
To my mind the search for a reduction of belief and desire to choice is misguided. 
We should accept that if Savage's theorem (or one like it) does what it says then 
it successfully reduces quantitative belief and desire to preference. We can also 
accept that observed choice constrains (but does not determine) attributions of 
preference to agents. But to attribute determinate preferences to an agent we need 
to accept evidence of a wider source and in particular the verbal reports of the 
agent concerning their preferences and (crucially) instances of their indifference 
between possibilities. 

268 
RICHARD BRADLEY 
This is not to grant agents any kind of epistemic privilege with regard to their 
mental states. People certainly can be wrong about such things. But Behaviourism 
draws too sharp a contrast between the status of behavioural and verbal evidence, 
falsely identifying the former with what is observable and indubitable and the lat-
ter with what is not. In fact both in theory and in practice the distinction is not 
nearly so neat. What people report clearly can be evidence for what they really 
think and feel, certainly not indubitable and perhaps different in kind from the 
evidence gleaned by observing their behaviour, but not therefore better or worse. 
Indeed it is hardly conceivable in practice that we could do without verbal reports. 
For language has the great advantage of allowing precise formulation of the al-
ternatives with respect to which we wish to determine agents' attitudes. Granted, 
this can raise the question as to whether the observer and the subject have a shared 
understanding of what is conveyed by particular linguistic expressions. But these 
difficulties are hardly more severe than those affecting the interpretation of their 
behaviour -
it is just as easy to misinterpret the meaning of a wave of a hand as 
the meaning of an utterance. 
The objection to introspection based evidence is not much more sustainable in 
theory than in practice. The supposed indubitability of behavioural evidence rests 
on the idea that because, perceptual illusions aside, we can see what someone is 
doing, there can be no doubting evidence statements of the 'The agent did such 
and such' kind. But in reality observations of behaviour (or at very least their 
linguistic representation) are always 'polluted' by interpretations: we don't see 
someone making circular motions with a cloth on the table, we see them wiping 
it clean; we don't see limbs describing particular trajectories, we see someone 
going for a walk; and so on. These interpretations are contestable and observers 
with different background knowledge will often interpret what someone is doing 
in different ways. Intersubjective observability does not mean either certainty or 
consensus. 
Once one gives up on the goal of deriving all knowledge claims from indu-
bitable foundations, we need not take an all or nothing attitude to the results of 
introspection. Under certain conditions introspection may reliably produce evi-
dence of a particular kind, under others it may be less satisfactory. A decision 
about what sort of evidence to admit in the determination of agents' mental states 
must be made and motivated. Inevitably there will be trade-offs between the re-
liability of the evidence admitted and its richness. And depending on the attitude 
taken to the admissibility of different types of evidence, there will be different 
problems of measurement and different methods for solving it that depart from 
different trade-offs. For this reason, in addition to the usual ones, the solution 
defended here cannot be regarded as the only one deserving consideration. 
Ramsey's Problem 
Having granted that there is more than one way in which the problem of mea-
surement can be formulated, let us now concentrate on the version of the problem 

RAMSEY AND THE MEASUREMENT OF BELIEF 
269 
found in Ramsey's essay "Truth and Probability" [Ramsey, 1926]. Ramsey's main 
concern in this paper is to explicate the concept of probability, but as he held that 
notions like this had no precise meaning unless a measurement procedure is spec-
ified, much of the paper is devoted to addressing our problem. We need not of 
course endorse Ramsey's operationalism in order to learn from his measurement 
methods. 
Ramsey's thinking on the question of the measurement of belief seems at first to 
be very much in the behaviourist mould. He argues, for instance, that the idea that 
believing something more or less strongly was connected to a perceptible feeling 
of belief of a certain intensity is: 
" . .. observably false, for the beliefs which we hold most strongly 
are often accompanied by practically no feeling at all; no one feels 
strongly about things he takes for granted.,,3 
And that: 
"when we seek to know what is the difference between believing more 
firmly and believing less firmly, we can no longer regard it as consist-
ing in having more or less observable feelings; at least I personally 
cannot recognise any such feelings. The difference seems to me to lie 
in how far we should act on these beliefs . .. ,,4 
But Ramsey is not in fact rejecting introspection wholesale, only the particular 
use of introspection associated with the idea of measuring strength of belief in 
terms of the sensations or feelings that accompany it. Indeed in the argument 
just quoted he makes use of introspective evidence: his own failure to perceive a 
feeling corresponding to his belief. And further on, when he argues that although 
we may feel that: 
"we know how strongly we believe things and that we can only know 
this if we can measure our belief by introspection . .. our judgement 
about the strength of our belief is really about how we should act in 
hypothetical circumstances." 5 
the judgement that he refers to -
as to how we would act under hypothetical 
circumstances -
is presumably an introspective one. In fact, as I shall argue in 
greater detail later on, not much sense can be made of Ramsey's measurement 
procedure unless introspective evidence of this kind is admitted. 
Ramsey takes his arguments to show that although we might be able to in-
trospect whether we do or do not believe something, there is no reliable way of 
introspecting the degree to which we do. It would appear that this suspicion of 
3Ramsey [1926, p. 65] 
4Ibid, p.66 
5Ibid, p.67 

270 
RICHARD BRADLEY 
introspection, if not his arguments against it, extends to the possibility of qualita-
tive judgements as to whether one believes one thing more strongly than another, 
despite the fact that they seem not dissimilar in nature to judgements as to how we 
would choose or act in particular circumstances. In any case, the upshot is that he 
admits only evidence as to the choices that an agent does or would make between 
specified alternatives and not their direct reports on their partial attitudes. In this 
respect Ramsey's approach is very much in line with the norm in decision theory. 
While this is not the only reasonable formulation of the problem, it has the ad-
vantage of expressing it as it is most commonly understood by Bayesian decision 
theorists. 
1.3 Ramsey's Theory a/Measurement 
Conditional Prospects 
The problem, as I have presented it, is to explain how the evidence relating to 
an agent's choices or actions determines a decision-theoretic representation of her 
partial belief and desires. Any approach that admits only evidence of this kind 
faces what is frequently termed the problem of the simultaneous determination of 
belief and desire: essentially that of untangling the respective contributions made 
by an agent's beliefs and desires to her choices. To solve it Ramsey makes use of 
two important ideas: that of a conditional prospect and that of an ethically neutral 
proposition. 
Conditional prospects are possibilities such as that if its hot today, then it will 
rain tomorrow and, if not, it will snow. Such prospects have come to be termed 
'gambles' or 'actions' in the literature on Ramsey, although he does not use either 
term. Both are misleading in some important ways, and I will largely avoid them. 
Ramsey takes it for granted that the desirability of choosing a particular condi-
tional prospect is related in a precise way to the desirability of the possible states 
of the world consistent with it. Namely, that the former is a weighted average of 
the latter, with the weights coming from the agent's degrees of belief. Formally, 
let Pr( P) be a measure of the degree to which the agent believes that P and U (a) 
and U (13) be measures of the degrees to which she desires that respectively a and 
13 be the actual state of the world. Then the desirability, U(r), of the conditional 
prospect r, that a be the case if P is and that 13 be the case if not, is its expectation 
of utility: 
PROPOSITION 1. U(r) = U(a). Pr(P) + U(f3).(l - Pr(P)). 
Ramsey doesn't attempt to justify this assumption, arguing that although the 
theory upon which it rests: 
"cannot be made adequate to all the facts, ... [it is] a useful approx-
imation to the truth, particularly in our self-conscious or professional 
life, and it is presupposed in a great deal of our thought.,,6 
6Ibid, p.69 

RAMSEY AND THE MEASUREMENT OF BELIEF 
271 
It might well be objected that Ramsey's assumption about the way in which 
expectations determine desires is a good deal more specific than the sort of 'folk-
psychology' that arguably much of our thought presupposes. This objection would 
not, I think, worry Ramsey much as he held that concepts like partial belief or 
utility are at least partially defined by the procedures for measuring them.7 And 
Ramsey quite explicitly ties his method to the measurement of partial attitudes qua 
bases or causal components of actions, the concept of partial attitude that has come 
to predominate in the social sciences. Ramsey seems on safe ground here in that 
the quantities implicitly defined by his measurement scheme turn out to have many 
of the properties commonly attributed to them by social scientists. This fact alone 
may suffice to motivate the assumptions that Ramsey makes about the nature of 
partial belief and desire, but it should be remembered that the motivation is only 
as strong as the consensus amongst social scientists from which it stems. 
But we are straying into the issue of justification, which we have already un-
dertaken to set aside. The important fact is that Proposition 1 expresses (albeit 
formally) no more than what I previously stipulated as one of the defining con-
tentions of Bayesian decision theory: namely that they desirability of an option is 
given by the expected benefit of choosing it, given one's beliefs. In attempting to 
solve a measurement problem it is perfectly appropriate to assume the truth of the 
theory whose variables require measurement. So there is no requirement here to 
justify Proposition 1 any further. 
Ethically neutral propositions 
The second critical innovation of Ramsey's is the postulation of what he calls eth-
ically neutral propositions. An ethically neutral proposition is simply one whose 
truth or falsity is a matter of indifference to the agent and does not affect their 
attitude to any other prospects e.g. the prospect of a dust storm on Mars does not 
influence any of my earthly concerns. 
Crucially the probabilities of some ethically neutral propositions can be inferred 
from an agent's preferences. Suppose, for instance, that an agent is not indifferent 
between the prospect of sun and that of snow, but indifferent between the prospect 
that if P is true, then it will be sunny, but if P is not, then it will snow, and the 
prospect that if P is true, then it will snow, but if P is not, then it will be sunny. 
Then we can infer that they regard P as likely to be true as not. For were it not, 
they should prefer one of the conditional prospects over the other (this follows 
directly from Proposition 1). The proposition that a toss of the coin in my hands 
will land heads up may be an example of such a proposition. 
Ramsey's Method (Informally) 
Suppose we want to determine Mary's attitudes to the various kinds of weather 
that the next day might bring: sun, snow, rain, and so on. Then if 
7 "the degree of belief is like a time interval; it has no precise meaning unless we specify how it is 
to be measured" ibid, p.63 

272 
RICHARD BRADLEY 
"we had the power of the Almighty, and could persuade our subject of 
our power" 8 
we could offer each kind of weather as an option to be exercised if she so 
chooses. By getting her to choose between any pair of them we obtain a rank-
ing of all of her prospects and can assign some arbitrary number -
say 1 and 0 for 
simplicity -
to the top and bottom ranked ones. 
Let us suppose that in Mary's case the results of coin tosses are indeed ethically 
neutral and of probability one-half and that the top ranked prospect is a sunny day 
tomorrow and the bottom ranked one is snow tomorrow. Now, Mary's attitude to 
the prospect that there will be sun tomorrow if the coin lands heads and snow if 
it lands tails will depend on the degree to which she respectively desires sunny 
and snowy days and the degree to which she believes that the coin will land heads 
or tails. But she considers the latter to be equi-probable, so she will regard the 
conditional prospect of sunny and snowy days in the event of heads or tails, as 
being midway in desirability between the prospects of a sunny day and that of a 
snowy one. (Again, this follows directly from Proposition 1). Relative then to our 
arbitrary choice of values for the top and bottom of the ranking, this means it has 
a utility of 0.5. And so too will any prospect ranked with this' gamble'. 
Just as we identified the value 0.5 with a particular conditional prospect we can, 
by compounding conditional prospects, identify any point in the interval from 0 to 
1 with some 'gamble' on the top and bottom ranked prospects. For instance, 0.25 
might be identified with the prospect that, in the event of the first toss of the coin 
coming up heads, there will be sun tomorrow if a second toss of the coin lands 
heads as well and snow if it lands tails, and that, in the event of it coming up tails 
on the first toss, there will be snow tomorrow. This gives us a scale with which to 
measure the utility of all prospects, whatever their form. 
We can also use Mary's choices amongst her options to determine her degrees 
of belief. Suppose that she is indifferent between the prospect of it being cloudy 
tomorrow and that of it being sunny tomorrow if it rains tonight and of it being 
snowy if it doesn't. Then the utility of the prospect of cloudy weather tomorrow, 
relative to that of sunny and snowy weather, indicates the degree to which Mary 
believes that it will rain tonight: the closer the utility of cloudy weather is to that 
of sunny weather, the stronger must be her belief that it will rain, the closer it is to 
the utility of snowy weather the weaker must be her belief. In general, if Mary is 
indifferent between the prospect that 0: and the prospect that (3 if P is the case and 
that 'Y if P is not, then by rearrangement of Proposition 1: 
Pr(P) = U(o:) - U(,,(). 
U((3) - U("() 
So, on the assumption that the right conditional prospect can always be found, 
we have a means to determine Mary's degrees of belief in every proposition, 
whether ethically neutral or not. 
8Ibid, p. 72. It is not in fact really necessary to have any powers other than those of persuasion. 

RAMSEY AND THE MEASUREMENT OF BELIEF 
273 
2 RECONSTRUCTING RAMSEY'S ACCOUNT 
2.1 
The Basic Framework 
Our informal presentation suggests that Ramsey has an simple, elegant and effec-
tive method for measuring belief and desire to offer us. Unfortunately, Ramsey 
does not work all the details of his theory, claiming at one point: 
"this would, I think, be rather like working out to seven places of 
decimals a result only valid to two .. 9. 
But it is clearly important to determine whether his demonstration can work 
in principle. The literature on Ramsey is sadly lacking in this respect. Many 
decision theorists have drawn inspiration from Ramsey, without engaging with the 
details of his work or by reconstructing it in terms of ideas drawn from Savage 
and others. lO What little expository literature there is tends to remain too distant 
from the details or too uncritical of Ramsey's claims,u This has meant that crucial 
assumptions have gone unnoticed. The only exception that I am aware of is Sobel 
[1988], which carefully examines Ramsey's argument, filling in some of the details 
and making suggestions for amendments. But Sobel doesn't finish the job and, in 
particular, neither provides proofs that are missing (e.g. of the existence of a utility 
representation of preference) nor carefully examines all the ones that Ramsey does 
give (e.g. of the additivity of degrees of belief). 
Worlds and propositions 
Ramsey makes a distinction between the objects of belief -
propositions -
and 
the objects of desire -
prospects: possible courses of the world (worlds for short) 
and conditional prospects or 'gambles'. We denote propositions by italic upper 
case letters, worlds by lower case Greek letters and arbitrary prospects by upper 
case Greek letters. In Ramsey's framework conditional prospects are essentially 
functions from partitions of propositions to worlds, with a function from the par-
tition {Xl, X 2, '" Xn} to worlds ab a2, ... ,an being written as (al if X I)(a2 
if X 2) ... (an if Xn). We take for granted that the ordering does not matter: (al 
if X I )(a2 if X 2) is, for instance, the same conditional prospect as (a2 if X 2)(al if 
Xl). 
As Ramsey speaks of worlds as being compatible with propositions or of in-
cluding their truth, worlds must be the sorts of entities that can stand in logical 
relationships to propositions. He defines them as: 
"the different possible totalities of events between which our subject 
chooses -
the ultimate organic unities .. 12 
9Ibid, p. 76 
IOFor instance, Davidson and Suppes [1956], Jeffrey [\983], Balker [1967] 
llFor instance, N.-E. Sahlin, [\990] 
12Rarnsey [1926, p. 72-72] 

274 
RICHARD BRADLEY 
From this, and on the basis of what he does with worlds, it might seem that 
we should think of them as something like maximally specific propositions, so 
that for all worlds 0: and propositions X, 0: implies that X or 0: implies that not 
X. (I shall represent the fact of 0: implying that X in the manner of possible 
worlds semantics, by writing 0: E X). In one respect, however, this cannot be 
exactly right. In his definition of an ethically neutral proposition of probability 
one half, and elsewhere, Ramsey implicitly assumes the existence of worlds that 
are compatible with both the truth and falsity of ethically neutral propositions. So 
either we have to say that worlds are only maximally specific about things that 
matter to the agent (and, hence, qualify the claims of the previous paragraph) or 
reformulate the relevant definitions. One way of doing so is to introduce world-
like entities with just the right lack of specificity concerning the truth values of 
ethically neutral propositions. The matter has been thoroughly explored in Sobel 
[1988] and so I permit myself to ignore here the complications that it gives rise to. 
Ethical Neutrality 
The concept of an ethically neutral proposition is intuitively clear: It IS Just a 
proposition whose truth or falsity does not affect an agent's attitude to any of her 
prospects. But to express this more formally Ramsey needs to be able to say what 
it is for two prospects to differ from another only with respect to the truth of the 
proposition in question. Worlds, however, cannot differ with respect to the truth 
of a single proposition only. To get around this problem Ramsey assumes the 
existence of a class of atomic propositions; propositions which are true or false 
mdependently of one another and such that no two worlds are exactly alike with 
regard to the truth of all of them. An atomic proposition P is then defined to 
be ethically neutral for some agent iff she is indifferent between any two worlds 
which differ only in respect to the truth of P. Finally, non-atomic propositions are 
said to be ethically neutral iff all their atomic truth-arguments are. This cost of 
this formulation is that it ties his account to Wittgenstein's logical atomism to a far 
greater extent than I imagine he would care.!3 
Are there any ethically neutral propositions? The standard candidates are propo-
sitions such as that the next card drawn will be an Ace or the coin will land heads. 
But the truth of such propositions do affect agents' attitudes to some prospects. 
Take any proposition X and suppose that the agent is not indifferent to prospect 
that A. Then the truth of X will be a matter of consequence to her attitude to the 
prospect that A if X, because in the event that X is true, the prospect that A if X 
amounts to that of A. So X is not ethically neutral. It follows that there are no 
propositions that are neutral with respect to all prospects. 
On the other hand, Ramsey only assumes that ethically neutral propositions 
leave the agents' attitudes to worlds unaffected. So, as long as conditional prospects 
I3 Sobel [1988, p. 237] says that Ramsey is not committed to the claim that every proposition is a 
truth functional compound of atomic ones. But without this claim his definition of ethical neutrality is 
not completely general. See Sobel for a discussion of what he calls Ramsey's thin logical atomism. 

RAMSEY AND THE MEASUREMENT OF BELIEF 
275 
do not serve to discriminate worlds, the supposition of ethically neutral proposi-
tions will not seem unreasonable.14 And Ramsey's definition of an ethically neu-
tral proposition of probability one-half, one of the linchpins of his method, will be 
well-founded. 
DEFINITION 2. Let P be any ethically neutral proposition and a and (3 be any 
worlds consistent with both the truth and the falsity of P and such that the agent 
prefers a to (3. Then P is of probability one-half iff the agent is indifferent 
between the prospect of (a if P)Â«(3 if -,P) and that of Â«(3 if P)(a if -,P). 
2.2 
The Existence of a Utility Measure 
Axiomatising Preference 
Under what conditions will the kind of measurement process that we previously 
described yield a measure of the strength of an agent's desires and to what ex-
tent will such a measure be unique? Ramsey answers this question by stating a 
representation theorem for preference orderings of prospects that establishes the 
existence of quantitative (utility) representations of the agent's degree of prefer-
ence (or desire). He does not, however, give a full proof of this theorem. As far as 
I know nobody else has supplied one either. Indeed, despite the fact that Ramsey 
gives strong indications of how the proof should go, there seems to be very little 
recognition of the fact that the basic strategy of the proof differs to a considerable 
extent from others to be found in the decision theoretic literature. Perhaps the 
problem is a natural tendency to read his work through the lenses of either von 
Neumann and Morgenstern or Savage. 
I will begin with a statement of Ramsey's representation theorem, before re-
turning to the question of his strategy for proving it. Though I will remain fairly 
close to the letter of Ramsey's account, I will deviate from it at points in the in-
terests of clarity and ease of understanding. Let a preference relation, >, and an 
indifference relation, ~, be defined over the set of all prospects, f: a set of worlds 
plus all conditional prospects defined with respect to them (relative to a given set 
of propositions). By <I> ~ llJ is meant that <I> > llJ or <I> ~ llJ. Although Ramsey 
does not explicitly postulate it, he takes it for granted in subsequent discussion that 
agents' preferences are complete i.e. that <I> > llJ or llJ > <I> or <I> ~ llJ. 
Let us call the set of prospects equally preferred to <I>, its value and denote it by 
~, avoiding Ramsey's economical, but often confusing, practice of denoting values 
as well as worlds by Greek letters. We write ~ ~ ~ iff <I> ~ llJ. While Ramsey 
directly axiomatises the relation on values induced by the preference relation on 
prospects, I will state the axioms in terms of the latter. His way of doing it obscures 
some issues of importance to our discussion and is easily recovered from ours. Let 
14Tbe question left open is: if worlds are not distinguished by conditional prospects, what is the 
relation between the two? This question, indeed the more general one of the logical status of conditional 
prospects is never addressed by Ramsey. We return to the problem right at the end of the paper. 

276 
RICHARD BRADLEY 
~ be a non-empty set of ethically neutral propositions of probability one-half and 
suppose that P belongs to ~. Then Ramsey postulates: 
Rl If Q E ~ and (a if P)((3 if -,P) ~ (-y if P)(6 if -,P), then: 
(a if Q)((3 if -,Q) ~ (-y if Q) (6 if -,Q). 
R2 If (a if P) (6 if -,P) ~ ((3 if P) (-y if -,P) then: 
(i) a > (3 {::::::} 'Y > 6 
(ii) a ~ (3 {::::::} 'Y ~ 6. 
R3 If <I> ~ '11 and '11 ~ e, then <I> ~ e. 
R4 If (a if P) (6 if -,P) ~ ((3 if P) (-y if -,P) and (-y if P) (( if -,P) ~ (6 if 
P) (r] if -,P), then (a if P)( ( if -,P) ~ ((3 if P) (r] if -,P) 
R5 V(a, (3, 'Y)[3(6): (a if P)(-y if -,P) ~ (6 if P)((3 if -'P)] 
R6 V(a, (3)[3(6): (a if P)((3 if -,P) ~ (6 if P)(6 if -'P)] 
R7 Axiom of Continuity 
R8 Archimedean Axiom. 
I have slightly strengthened Ramsey's first, third and fourth axiom by stating 
them in terms ofthe weak order '>' rather than the indifference relation '~'. In the 
presence of R2, my R5 and R6 are jointly equivalent to his fifth and sixth axioms. 
Ramsey doesn't say what he intends by R7 and R8, though R7 is presumably about 
the continuity of preferences and R8 about the values of worlds. In particular, what 
is required of R8, whatever its precise formulation, is that it allows the derivation 
of the Archimedean condition referred to in Definition 7 below. 
Presumably R7 is meant to ensure that for every conditional prospect (a if X) ((3 
if -,X) there exists a world 'Y such that (a if X) ((3 if -,X) ~ 'Y, so that every value 
contains a world. Apart from simplifying movement between values and worlds, 
this assumption plays a crucial role in his subsequent derivation of degrees of be-
liefs. Furthermore, as Ramsey doesn't assume (as we did in our informal presenta-
tion of his method) the existence of compound conditional prospects -
prospects 
of the form (<I> if X) ('11 if -,X), where <I> and'll are conditional prospects -
he 
does need something to ensure, for any two prospects, the existence of a condi-
tional prospect whose value lies midway between theirs. This in turn is required 
for Ramsey to conclude that "These axioms enable the values to be correlated 
one-one with real numbers ... ".15 
15Ramsey [1926, p. 75] 

RAMSEY AND THE MEASUREMENT OF BELIEF 
277 
Intervals of Values 
Ramsey's next move is to "explain what is meant by the difference in value of a 
and /3 being equal to that between 'Y and 8" by defining it to mean that if p. is an 
ethically neutral proposition of probability one-half then the agent is indifferent 
between (a if P) (8 if -,P) and (/3 if P) ("( if -,P). Although Ramsey seems to 
be speaking here of a relation (of difference in value) between worlds, what he 
really needs for his representation theorem is a definition of a difference relation 
between values of worlds.16 So let us denote the difference between the values a 
and ~ by a - ~ and define an ordering, t, of differences in values as follows (the 
coherence of the definition is guaranteed by Rl): 
DEFINITION 3. If P E ~ then a - ~ t "1- 8 iff'v'(a E a,/3 E ~,'Y E "1,8 E 
8), (a if P)(8 if -,P) ~ (/3 if P)("( if -,P). 
With the concept of difference of value in hand, it becomes much easier to 
understand the role played by Ramsey's preference axioms. Essentially they are 
there to ensure that it be possible to give a numerical representation of not only 
such facts as the agent preferring one thing to another, but also of the extent to 
which their preference or desire for one thing exceeds their desire for another. 
To this end axiom Rl ensures the coherence of the definition of differences in 
values, axioms R5, R6, R7 and R8 a correspondence between values and real 
numbers, and R2, R3, and R4 that the difference operation on values functions 
like the subtraction operation on real numbers. 
With regard to the latter, note that Axiom R4, opaque in its original formulation, 
translates as an axiom of transitivity for differences in values: 
R4* If a - ~ t "I - 8 and "I - 8 t r; - C, then a - ~ t r; - C 
while, as we now prove, it follows from the definitions of ethical neutrality and 
thedifferenceoperationthatifa-~ ~ "I-Jthen~-a ~ J-"I and a-"I t ~-8. 
LEMMA 4. If a -
~ ~ "1- 8 then: 
(i) 8 -"I ~ ~ - a 
Oi) a - "I ~ ~ - J 
Proof. Suppose that P E ~. Omitting explicit quantification over worlds where 
the meaning is obvious, we note that a -
~ t "I - 8 Â¢:} (a if P) (8 if -,P) ~ (/3 
if P)('Y if -,P). But by Definition 2, (i) (a if P)(8 if -,P) ~ (/3 if P)("( if 
-,P) Â¢:} (8 if P)(a if -,P) ~ ("( if P)(/3 if -,P) Â¢:} 8 - "I t ~ - a, and (ii) 
(a if P) (8 if -,P) ~ (/3 if P) ("( if -,P) Â¢:} (a if P)( 8 if -,P) ~ ("( if P) (/3 if 
-,P) Â¢:} a - "I t ~ - 8. 
â¢ 
Proving the Representation Theorem 
We are now in a position to state Ramsey's theorem establishing the existence of 
utility measures of agents' desires. Ramsey does not give a uniqueness theorem 
16This ambiguity is reproduced without comment in most expositions of Ramsey. 

278 
RICHARD BRADLEY 
for such utility measures, but his subsequent discussion of the measurement of 
probabilities assumes that they are unique up to affine linear transformation (or 
choice of scale) i.e. that preferences are interval-scale measurable. We state below 
the theorem he requires. 
THEOREM 5 (Existence). There exists a utility function, U, on the set of all val-
ues, f', suchthatV(a,~,t,6 E 1'), a-~ t t-6 {:> U(a)-U(~) ~ U(t)-U(6) 
THEOREM 6 (Uniqueness). If U' is another such a utility function, then there 
exists real numbers a and b, such that a > Â° 
and U' = a.U + b. 
The key to understanding Ramsey's representation theorem is to recognise that 
it implicitly draws on the theory of measurement deriving from the work of the 
German mathematician Holder (with which Ramsey would have been familiar). 
We begin with a statement of the relevant results in this area, drawing from their 
presentation in Krantz et at. [1971, chapter 4].17 
DEFINITION 7. Let A be a non-empty set and t a binary relation on A x A. Then 
< A x A, t> is an algebraic difference structure iffV(a, b, c, d, a', b', c' E A) : 
1. t is a complete and transitive 
2. If ab t cd, then dc ~ ba 
3. If ab t a'b' and bc t b'c' then ac t a'c' 
4. If ab ~ cd ~ ba then there exists x, x' E A, such that ax ~ cd ~ x' b 
5. Archimedean condition 
THEOREM 8. If < A x A, ~> is an algebraic difference structure, then there 
exists a real-valued function, Â¢, on A, such that V(a, b, c, dE A): 
ab t cd {:> Â¢(a) - Â¢(b) ~ Â¢(c) - Â¢(d) 
Furthermore, Â¢ is unique up to positive linear transformation i.e. if Â¢' is another 
such afunction then ~(x, y E ~ : x > 0, Â¢ = x.Â¢ + y). 
Ramsey's basic strategy for proving his representation theorem is to use prefer-
ence orderings of prospects to define an algebraic difference structure and then to 
invoke Theorem 8. We will now reconstruct his proof on that basis. 
THEOREM 9. Let t be the relation on l' x l' induced by definition 3. Then 
< l' x 1', t> is a difference algebra. 
17The authors point out that HOlder's results can be applied to the problem of measurement of degrees 
of preference, but (oddly) make no attempt do so directly. Nor is there explicit recognition of the use 
that Ramsey makes of them. 

RAMSEY AND THE MEASUREMENT OF BELIEF 
279 
Proof. We prove that < r x r, t> satisfies the five conditions given in Defini-
tion 7. 
1. Follows directly from the completeness and transitivity of ~. 
2. By Lemma 4(i). 
3. By R4*, if a - a' ~ {J - {J' and {J - {J' ~ t - t' then a - a' ~ t - t'Â· 
But by Corollary 4 (ii) a - a.' ~ {J - {J' <=> a. - {J ~ a.' - {J', {J - {J' ~ 
t - l' <=> {J - t ~ {J' - t' and a. - a.' ~ t - t' <=> a. - t ~ a.' - t'Â· Hence 
if a - {J ~ a.' - {J' and {J - t ~ (J' - t' then a. - t ~ a.' - t'Â· 
4. By R5, \7'(a,,8, ,)" (5)[3(â¬, â¬'): (a if P)(')' if ...,P) ~ (â¬if P)(,8 if ...,P) and 
(& if P)(,8 if ...,P) ~ (â¬' if P)(')' if ""P)]. But by Lemma 4(i) (& if P)(,8 
if ...,P) ~ (â¬' if P) (')' if ...,P) <=> (,8 if P){ & if ...,P) ~ (')' if P) (â¬' if ...,P). 
Hence 3(l, E') : a. - l ~ {J - t and {J - t ~ f.' - J. 
5. Follows from R8. 
â¢ 
Theorems 5 and 6 clearly follow directly from Theorems 8 and 9. Ramsey does 
not seek to explicitly establish that the utility function, U, referred to in these theo-
rems also represents the agent's preference ranking of possibilities in the sense that 
the utilities of prospects go by their position in the preference order. To establish 
this we would have to make a further, but unobjectionable, assumption. As it turns 
out, the assumption is presupposed in Ramsey's subsequent derivation of degrees 
of belief and so there good reason to make it explicit here. 
R9 Let P be any proposition and a and ,8 any worlds. Then: 
a ? {J ~ 
a ? (a if P)({J if ...,P) ? {J. 
COROLLARY 10. a ~ (a if P) (a if ...,P) 
THEOREM 11. The utility junction, U, on t, is such that \7'( a., (J E t), a. t (J <=> 
U(a.) ~ U({J) 
Proof. By Corollary 10 and Theorem 5 it follows that a ~ ,8 <=> (a if P) (a if 
...,P) ~ (,8 if P){,8 if ...,P) <=> a. - {J t (J - a. <=> U(a.) - U({J) ~ U({J) - U(a.) <=> 
U(a.) ~ U({J). 
â¢ 
2.3 Measuring Partial Belief 
Defining Degrees of Belief 
Recall from our informal presentation of his method that Ramsey's next move is to 
use the utility measure on worlds to determine the agent's degrees of belief in all 

280 
RICHARD BRADLEY 
propositions, including those that are not ethically neutral. The vehicle for doing 
so is the following definition. 
DEFINITION 12 (Degrees of Belief). Suppose that worlds 0: E P, (3 E -,p and ~ 
are such that 0: Â¢ (3 and ~ ~ (0: if P)((3 if -,P). Then: 
U(~) - U((3) 
Pr(P) =defn U(o:) - U((3) 
The existence of the world c in question is presumably secured by R7. As the 
Uniqueness Theorem establishes that ratios of utility difference are independent of 
choice of scale for the utility function, Definition 12 determines a unique measure 
of the agent's degrees of belief. Ramsey notes that in this definition the proposition 
P is not assumed to be ethically neutral, but that it is necessary to assume both 
that this definition is independent of the choice of worlds meeting the antecedent 
conditions and that 
"there is a world with any assigned value in which P is true, and one 
in which P is false,,18 
Why the latter assumption is necessary will only become clear once we look at 
Ramsey's proof that degrees of belief are probabilities. But if it is to be tenable 
it is patently necessary that P be neither logically true nor logically false. But 
this means that some separate treatment of such propositions is required e.g. by 
stipulating that Pr(P) = 1, whenever P is logically true. 
As regards the former assumption (of independence), Ramsey does not say how 
it might be formally expressed as a condition on preference or choice. But it must 
be possible to do so, as we know from the Uniqueness Theorem that the equal-
ity (or otherwise) of ratios of utility differences is determined by the preference 
ranking. One way to proceed would be to define the conditions under which the 
difference in values of 0: and (3 equals a particular fraction of the difference be-
tween the values of'"Y and 8. The definitions are cumbersome, so I will confine 
myself to illustrating the case of one-half. Suppose that Q is an ethically neutral 
proposition of probability one-half, that ( ~ (c if Q)(8 if -,Q) and that 'f/ ~ (c 
if Q)b if -,Q). Then we can say that difference in value of 0: and (3 equals half 
the difference between the values of'"Y and 8 iff (0: if Q)(( if -,Q) ~ ((3 if Q)('f/ if 
-,Q). And so on. 
THEOREM 13. If 0: E P and (3 E -,P, then: 
U((o: if P)((3 if-,P)) = U(o:). Pr(P) + U((3).(1 - Pr(P)). 
Proof. If 0: ~ (3, then it follows from axiom R9 that 0: ~ (0: if P)((3 if -'P) ~ (3. 
So the theorem follows immediately. If 0: Â¢ (3, then suppose that ~ is such that 
~ ~ (0: if P)((3 if -,P). Then by the definition of Pr(P), U(~) = U(o:). Pr(P) -
U((3).(1 - Pr(P)) = U((o: if P)((3 if -,P)). 
â¢ 
18Ramsey. ibid. p. 75 

RAMSEY AND THE MEASUREMENT OF BELIEF 
281 
DEFINITION 14 (Conditional Degrees of Belief). Suppose that (0: if Q)({3 if 
...,Q) ~ ('y if PQ)(8 if ...,PQ)({3 if ...,Q). Then the degree of belief in P given 
Q; 
U(o:) - U(8) 
Pr(P I Q) =defn U(r) _ U(8) 
As with the definition of degrees of belief it must be supposed (though Ramsey 
does not explicitly say so) that "( Â¢ 8,0: E Q, {3 E ...,Q, "( E PQ and 8 E ...,PQ, 
that the definition is independent of particular choices of worlds satisfying the an-
tecedent conditions and that there is a world with any assigned value in which 
PQ, P...,Q and ...,Q are true. It would also appear that the existence of equally 
ranked conditional prospects of kind referred to in the definition is not guaranteed 
by Ramsey's assumptions. There are a number of ways of filling in this gap. The 
most conceptually satisfactory would involve the postulation of compounded con-
ditional prospects and a generalisation of R5. But somewhat more economically, 
we could simply add the following axiom to Ramsey's. 
RIO Let {Pi,P2 , â¢â¢â¢ ,Pn} be a partition of propositions. Then V(r,8, ... ,{3), 
3(0: : ("( if Pd(8 if P2) â¢â¢ â¢ ({3 if Pn) ~ (0: if Pi U P2) â¢â¢â¢ ({3 if PnÂ». 
Proving Coherence 
Ramsey must still demonstrate that the degree of belief function PrO is truly a 
probability function. This is done in Theorems 15 and 17. Ramsey's proof of 
Theorem 17 requires a further assumption, not made explicit by him, but which is 
quite reasonable if one accepts his framework. 
Rll Suppose that P and Q are inconsistent, 0: E P, {3 E Q, and "( E P U Q. If 
0: ~ {3 ~ ,,(, then (0: if P)({3 if Q)(6 if ...,P...,Q) ~ ('y if P U Q)(6 if ...,P...,Q) 
Ramsey's own proof makes no use of RIO, for the obvious reason that he does 
not explicitly postulate it. But we have seen that it is required elsewhere and by 
making use of it here a much simpler alternative proof of Theorem 17 can be given 
which does not require Rll. Both proofs follow below. 
THEOREM 15. Let P be any proposition. Then: 
(i) Pr(P) 2: 0 
(ii) Pr(P) + Pr( ...,P) = 1 
(iii) Pr(P I Q) + Pr(...,P I Q) = 1 
Proof. Suppose that ~ is such that ~ ~ (0: if P) ({3 if ...,P). Then: 

282 
RICHARD BRADLEY 
(i) By R9, 0: ~ 13 <==> 0: ~ ~ ~ 13. So U(~) - U(f3) ~ U(o:) - U(f3), and it 
then follows from the definition of Pr that it never takes negative values. 
(oo) B d fi . . 
P (P) 
P (P) 
u{Q-U(,B) 
U({)-U(al 
1 
11 
Y e mtlOn, r 
+ r --, 
= U(a)-U(f3) + U(f3)-U{a) = . 
(iii) Suppose that 'Y E PQ and 8 E --,PQ are such that (0: if Q)(f3 if --,Q) ~ 
('Y if PQ)(8 if --,PQ)(f3 if --,Q). Then by definition of conditional degrees 
of belief, Pr(P I Q) = ~~~~=~f~~ and Pr( --,p I Q) = ~~~?=~f~? = 
~~~~=~t~?Â· So Pr(P I Q) + Pr(--,P I Q) = u(al-~f~)=~t~?+U(-y) = 1. â¢ 
LEMMA 16. Suppose that 13 E --'Q, 'Y E PQ and 8 E --,PQ. Then UÂ«("( if PQ)(8 
if --'PQ) (13 if --,Q)) = (U("(). Pr(P I Q)+ U(8)(1 - Pr(P I Q))). Pr(Q) + 
U(f3). Pr(--,Q) 
Proof. Let 0: E Q be such that (0: if Q)(f3 if --,Q) ~ ('Y if PQ)(8 if --,PQ)(f3 if 
--,Q). Then by Theorem 13, U('Y if PQ)(8 if --,PQ) (13 if --,Q) = U(o:). Pr(Q) -
U(f3).(l - Pr(Q)). But by Definition 14, U(o:) = U('Y). Pr(P I Q)+ U(8)(1 -
Pr(P I Q)). So UÂ«("( if PQ)(8 if --,PQ) (13 if --,Q)) = (U("(). Pr(P I Q)+ 
U(8)(1 - Pr(P I Q))). Pr(Q) + U(f3). Pr(--,Q). 
â¢ 
THEOREM 17. Pr(P I Q) = P;WQ~) 
Proof.[Ramsey's Proof] Let Pr(Q) = x and Pr(P I Q) = y. Then we need to 
show that Pr(PQ) = xy. Let 0: and 13 be any worlds in Q and --,Q respectively, 
such that, for some real numbert, U(o:) = U(~)+(l-x)t and U(f3) = U(o:)-t = 
U(~) - xt. By assumption such worlds 0: and 13 exist. Now U(~) = U(~).x + 
U(~).(l - x) = U(o:).x + U(f3).(1 - x) = UÂ«o: if Q)(f3 if --,Q)). Then by 
d fi . . 
U({ -u f3) 
e mtlon, x = uta -u f3)' 
Now let worlds 'Y E PQ, 8 E --,PQ be such that U("() = U(o:) + ~ - t and 
U(8) = U(f3) = U(o:) - t. Again by assumption such worlds 'Y and 8 exist. Then 
by Lemma 16, UÂ«("( if PQ)(8 if P--,Q) (13 if --,Q)) = (U("().y+ U(8)(1- y)).x + 
U(f3)(l-x) = U(o:).x+U(f3).(I-x) = UÂ«o: if Q)( 13 if --,Q)). SO by Definition 
14, y = ~f~?=~f~? = ~~~?=~fg?Â· So xy = ~f!)=~t~)Â· ~~~?=~fg? = ~f~~=~~~~Â· 
It also follows from Axiom Rll, that UÂ«'Y if PQ)(8 if P--,Q) (13 if --,Q)) = UÂ«'Y 
if PQ)(f3 if (--,P U --,Q))). Hence ~ ~ ("( if PQ)(f3 if (--,P U --,Q)) But then by 
definition Pr(PQ) = ~f~~=~~~~ = xy. 
â¢ 
Proof. [Alternative Proof] Let worlds 8 E --,PQ and 13 E --,Q be such that 8 ~ 13. 
Now by RIO, there exists worlds 0: and f such that ~ ~ (0: if Q)(f3 if --,Q) ~ ("( 
if PQ)(8 if --,PQ)(f3 if --,Q) ~ ("( if PQ)(f if (--,P U --,Q)). Then by definition, 
Pr(Q) = ~f!~=~~~~, Pr(PQ) = ~f~~=~~~~ and Pr(P I Q) = ~~~?=~W = 
~~~~=~~~? So Pr(PQ) = Pr(Q). Pr(P I Q). 
â¢ 
COROLLARY 18. Pr(PQ) + Pr(--,PQ) = Pr(Q). 

RAMSEY AND THE MEASUREMENT OF BELIEF 
283 
Proof. By Theorem 17, Pr(PQ) = Pr(P I Q).Pr(Q) and Pr(,PQ) = 
Pr(,P I Q). Pr(Q). ThereforePr(PQ)+Pr(,PQ) = (Pr(P I Q)+Pr('PIQ))Â· 
Pr(Q) = Pr(Q) by 15(iii). 
â¢ 
Conditionalism 
The importance of Ramsey's assumption that propositions contain worlds of ev-
ery utility value should now be clear -
it is what allows the assumption of the 
existence of the worlds Q, {3, 'Y and 8 referred to in his proof of Theorem 17 and 
of worlds {3 and 8 referred to in the alternative proof. Ramsey seems to think of 
this as a purely technical condition. But one might derive it from conditionalist 
premises, the relevant one in this context being as follows. 
PROPOSITION 19. (Ethical Conditionalism) For any propositions P and Q. there 
are worlds Q E P and {3 E Q such that Q ~ {3. 
Given the 1-1 correspondence between worlds and real numbers, Ethical Con-
ditionalism implies the assumption of whose necessity Ramsey speaks. Namely, 
that whatever the range of utility values taken by prospects, for every value in 
that range and every proposition, there is a world which implies the truth of that 
proposition and which has the utility value in question. 
Conditionalism is to my mind an imminently defensible doctrine. Essentially 
the conditionalist's claim is that however good (or bad) some possibility might be 
on average, there are imaginable circumstances in which it is not so. No prospect 
is good or bad in itself, but is only so relative to the conditions under which it 
is expected to be realised. Suppose, for instance, that P identifies some good 
prospect like winning the National Lottery and Q some dreadful one, like the death 
of a relative. The conditionalist claim is that even winning the lottery can be a 
bad thing, such as when it alienates one from one's friends or causes one to stop 
activities that gives one's life a sense of purpose. Likewise even the death of a 
relative can be a good thing, such as when it pre-empts a period of great suffering 
for them. 
Defensible though it may be, Ethical Conditionalism is not consistent with 
Ramsey's atomistic framework. For consider worlds Q and {3 such that Q rf:, {3 
and the proposition -
call it A -
that Q is the actual world. Then since worlds 
are (nearly) maximally specific it follows that any world in which A is true is 
ranked with Q. But then there is no world in which A is true which is equally 
preferred to {3. 
The only way I can see of blocking this argument, is to deny that worlds imply 
the existence of propositions stating that they are the actual world. But this would 
be to argue, in effect, that worlds could not be represented propositionally and that 
contradicts the requirement that the agent be able to choose amongst them (which, 
I take it, presupposes that she can distinguish them propositionally). So this is not 
a response open to Ramsey. And though the issue may in some sense be 'merely 
technical', some modification to his framework will be required to deal with the 
problem. 

284 
RICHARD BRADLEY 
3 THE EVALUATION 
3.1 
Ethical Neutrality versus State-Independence 
"Ramsey's essays, though now much appreciated, seem to have had 
relatively little influence.,,19 
Savage's remark applies equally well today and mainstream decision theory 
descends from Savage and not Ramsey. There are, I think, two reasons for his 
lack of influence. One is that Ramsey's style is so elliptical, and his writings so 
lacking in detail, that decision theorists have been unsure as to what exactly he has 
or has not achieved.2o The second is that the distinction between the problem of 
justifying the claims of decision theory regarding the properties of rational belief 
and desire and the problem of the measurement of the decision theoretic variables 
-
degrees of belief and desire -
has not been properly recognised. Due, as I 
suggested before, to the different possible roles played by representation theorems 
with respect to the two problems. 
This is important, because from the point of view of the problem of justifying 
the decision theory he employs, Ramsey's representation theorem is not particu-
larly helpful. For one is very unlikely to accept his axioms as definitive of rational 
preference for conditional prospects unless one accepts the theory of expected util-
ity that motivates them. This is particularly true of axiom R4, which seems to have 
no justification other than that it secures the meaningfulness of utility differences. 
Taken as axioms of measurement, however, they do much better for they spec-
ify in a precise way the conditions under which a measure of the agent's degrees 
of desire, unique up to a choice of scale, is determined by her choices amongst 
prospects. 
With respect to problem of justification, on the other hand, a theory like Sav-
age's is a good deal more impressive. Savage chooses his axioms of preference 
with an eye to their independent plausibility as rationality conditions. Indepen-
dent, that is, of the quantitative theory of belief and desire that he will derive from 
them. Such a claim can justifiably be made for the Sure-Thing principle, for in-
stance. One need not grant much plausibility to expected utility theory to grant 
that of two actions that yield the same outcomes when C is the case, one should 
choose the one with the preferred outcome when C is not. But Savage builds a 
very strong and implausible assumption into the very framework of his decision 
theory. He assumes that the desirability of any possible outcome of an action is 
independent of the state of the world in which it is realised. 
Let us start by getting a general idea of the problem. It is a banal fact about 
our attitudes to many things that they depend on all sorts of contextual factors. 
19Savage [I954, p. 96] 
2oFor instance, Fishburn [I981l rejects Ramsey-type theories in favour of Savage-type ones on the 
grounds of his 'restricted act space'. In fact, however, his set of conditional prospects is roughly 
equivalent to Savage's set of acts. 

RAMSEY AND THE MEASUREMENT OF BELIEF 
285 
Hot chocolate can be delightful on a cold evening, but sickly in the heat of a 
summer's day. Swimming on the other hand is best reserved for those hot days. I 
shall say, somewhat barbarically, that the swimming or drinking hot chocolate is 
desirabilistically dependent on the weather. Many things, on the other hand, are to 
all practical purposes desirabilistically independent, certainly swimming and the 
temperature on the moon are for me. Any reasonable theory of rational agency 
ought to recognise these banal facts. 
How does Savage's theory violate them? Savage uses observations of choices 
amongst actions to determine agents' attitudes. Actions, on his account, are func-
tions from states of the world to possible outcomes: when you choose an action 
you choose to make it true that if the world is in state 81 then outcome 01 will be 
realised, if it is in state 82 then outcome 02 will be realised, and so on. Now if we 
are to recognise that the desirability of the outcomes of actions depend on the state 
of affairs in which they are realised, then either the utilities we derive for them 
must be state-dependent i.e. of the form U (01181), or the outcome 01 must include 
the fact that 81 prevails (as outcomes of Ramsey's conditional prospects do). But 
Savage both assumes that any combination of state and outcome is possible and 
assigns state-independent utilities to outcomes. 
On Ramsey's account, outcomes (worlds) are maximally specific with regard to 
things that matter to the agent, but not all outcomes are achievable in any given 
state. So his theory requires no violation of the banal facts concerning the interde-
pendence of our attitudes. Instead of building desirabilistic independence into his 
framework, he postulates the existence of only a very limited class of possibilities 
-
those represented by ethically neutral propositions -
which are desirabilisti-
cally independent of all others. One may question whether there are any propo-
sitions that are truly ethically neutral, but there are clearly some that are good 
approximations. The postulation of their existence is not a heavy burden for such 
an idealised account to bare. 
This is not, of course, the end of the matter. There have been numerous at-
tempts to solve the problem of state-dependent utilities (as it has become known) 
within Savage's framework.21 Many of the proposed solutions are ingenious, but 
they always come at the cost of greater complexity and more burdensome assump-
tions. This is not the appropriate place to review the literature, but anyone who 
has ploughed through it will have little difficulty in recognising the merits of the 
elegantly simple method that Ramsey invented. Indeed, despite the problems in 
the details that we discovered, there is nothing that matches it as an answer to the 
problem of measurement. 
3.2 Jeffrey's Objection 
In motivating his own method of measuring belief, Ramsey argues that the estab-
lished method of offering bets with monetary rewards to someone to elicit their 
21 For a summary see Schervish et al. [I 9901 

286 
RICHARD BRADLEY 
degrees of belief is 'fundamentally sound' but suffers from being both insuffi-
ciently general and necessarily inexact. Inexact partly because the marginal utility 
of money need not be constant, partly because people may be especially averse (or 
otherwise) to betting because of the excitement involved and partly because "the 
proposal of a bet may. .. alter his state of opinion"22 
Ramsey seems to think that his own theory is not vulnerable to these problems, 
even though his method is similar in many ways to the one he is criticising. Not 
everyone would agree. Richard Jeffrey, for instance, has argued that just such a 
problem plagues Ramsey's own account.23 In order to measure agents' partial be-
liefs, Ramsey requires that they treat possibilities like it being 0: if P and f3 if not 
as real prospects i.e. things that can be brought about by choice. But to persuade 
someone of our power to bring it about at will that it will be sunny tomorrow if the 
coin lands heads and snowy if it lands tails is to cause them to entertain possibil-
ities which at present they do not. That is, one must modify their beliefs in order 
that one may better measure them! There is of course no guarantee then that the 
measurements so effected are not, at least partially, artifacts of the measurement 
process itself. 
It is worth noting that such an objection, if sustainable, can be directed with 
equal force at Savage. For when Savage invites agents to make choices amongst 
actions, he supposes that they know exactly what consequences the action has 
in every possible state of the world and hence what they are committing them-
selves to. This makes the choice of a Savage-type action rather more like a choice 
amongst Ramsey-type conditional prospects than amongst the sorts of things we 
normally think of as actions. Indeed, formally, they are just the same thing: func-
tions from events to outcomes. What is of the essence, in any case, is that agents 
are invited to choose amongst causal mechanisms of some kind whose effects in 
each possible circumstance are advertised in advance. And the essence of Jeffrey's 
objection is that agents may legitimately doubt the efficacy of such mechanisms, 
and make their choice in the light of these doubts. If they do, their choices will 
reflect not their evaluation of the advertised outcomes of the chosen prospect, but 
their evaluations of the outcomes that they actually expect. Even in pure gam~ 
bling situations agents will factor in such possibilities as the casino closing before 
paying up. 
How might Ramsey respond to this problem? Sobel argues that Ramsey must 
require that the probability of a proposition P be measured only by means of con-
ditional prospects which are such that P's probability is evidentially and causally 
independent of the conditional prospect being offered (by, for instance, addition of 
a further restriction in Definition 12).24 But there is no obvious way of express-
ing this independence condition in terms of agents' preferences and so no way of 
applying it until the probability of P has already been measured. 
22Ramsey [1926, p. 68) 
23Jeffrey [1983, chapter 10) 
24SobeJ [1988, p.256) 

RAMSEY AND THE MEASUREMENT OF BELIEF 
287 
A natural response to Jeffrey's objection would be to say that Ramsey does not, 
in fact, require that agents really believe in such fanciful causal possibilities. All 
that he requires is that they choose amongst gambles as if they believed that they 
would truly yield the advertised consequences under the relevant conditions. To be 
sure, such a response will not satisfy the behaviourist, for introspection on the part 
of agents must then playa crucial role in the production of their choices. For when 
we ask Mary to choose between an prospect which yields sunny weather if Labour 
wins the next election and rainy weather if they do not, and one which yields rainy 
weather if Labour wins the next election and sunny weather if they do not, we are 
in effect asking her to determine for herself what she would prefer in the event 
that such gambles were reliable. But then we may as well just ask Mary what she 
would prefer and forget about the observation of choices altogether. 
And indeed why not? Let us see what such a reconstrual of Ramsey's method 
would amount to in the context of the experimental determination of a subject's 
degrees of belief and desire, by comparing the following measurement schemes: 
1. Scheme A: The subject introspects her degrees of belief and desire and then 
reports them to the observer. 
2. Scheme B: The observer presents the subject with a number of options and 
her choice is recorded. The set of options offered is varied until a ranking 
over all of them has been constructed from the observations of her choices. 
This ranking is then used to construct a quantitative representation of her 
degrees of belief and desire. 
3. Scheme C: The observer questions the subject as to which of various pos-
sibilities she would prefer were the true one. Her answers are then used to 
construct a ranking of all possibilities and this in turn determines a quanti-
tative representation of her mental attitudes. 
Scheme A is the method criticised by behaviourists and Ramsey alike for its 
naive dependence on introspection. Scheme B summarises the behaviourist's 
method, Scheme C the alternative interpretation of Ramsey's method. Both are 
underwritten by the representation theorems of Decision Theory. In Scheme C 
introspection plays an essential role: to provide answers to the experimenter's 
questions the subject must reflect upon and judge her own preferences. In Scheme 
B, on the other hand, though it is conceivable that the subject arrives at a choice 
via introspection of her preferences, she ne~d not do so. She may simply choose 
without reflection, indeed without even having the concept of preference. Scheme 
C is a method intimately tied to the possibility of linguistic communication and 
the kind of self-consciousness that typically accompanies it; Scheme B is just as 
applicable to earthworms as to philosophers. 
I see no reason why Ramsey should be resistant to this interpretation of his 
method as a version of Scheme C. Although it requires him to disavow the be-
haviourist pretension that introspection can be completely eliminated in favour of 

288 
RICHARD BRADLEY 
rich observations of behaviour, it does not commit him to the view instantiated in 
Scheme A (and which he clearly rejects) that partial beliefs and desires can be di-
rectly introspected. In this sense this interpretation does not conflict with anything 
that he says. And it has the crucial advantage of extricating him from Jeffrey's 
objection. 
3.3 Ramsey a la Jeffrey? 
In filling in the details of Ramsey's theory of measurement we have had reason 
to raise a number of questions and to make a number of supplementary assump-
tions. But only the incompatibility of Conditionalism with his framework seems 
to raise a serious problem for Ramsey. In fact, however, this problem is largely a 
technical one and can be solved by modifications to Ramsey's framework that are 
not contrary to the 'spirit' of his account. I will content myself with sketching the 
essentials. 
The basic move is to take (non-contradictory) propositions rather than worlds 
to be the elementary objects of preference. One immediate positive spin-off is that 
the notion of ethical neutrality can then be formulated in a manner less dependent 
on the peculiarities of Wittgenstein's theory of atomic propositions.25 
DEFINITION 20. Suppose P and Q are mutually consistent propositions. Then 
P is neutral with respect to Q iff PQ ~ Q ~ p...,Q. 
DEFINITION 21. P is ethically neutral iff P is neutral with respect to all propo-
sitions Q consistent with P. 
Conditional prospects must now be defined as functions from partitions of propo-
sitions to (non-contradictory) propositions, with the constraint that for any condi-
tional prospect ~ and proposition X, ~(X) implies X. But little else need change, 
since most of Ramsey's formal argument is carried out at the level of values. Of 
course, utilities as well as probabilities will now be defined on propositions. Fi-
nally the relevant definition of Conditionalism is as follows. 
PROPOSITION 22. For any propositions P and Q there exists propositions pi 
and Q' such that P' implies P, Q' implies Q and P' ~ Q'. 
Proposition 22 can be satisfied only if there are no propositions X such that, for 
all propositions Y, X implies Y or X implies ...,Y. In other words, Conditionalism 
requires that the domain of the preference relation be atomless. 
All of this takes Ramsey's framework quite a bit closer to that underlying 
Richard Jeffrey's decision theory and Ethan Bolker's representation theorem for 
it.26 So too did the contention that Ramsey's work should be interpreted in such a 
way as to rid it of any dependence on dubious causal devices such as gambles. But 
I do not to propose to go much further in their direction, because from the perspec-
tive of the problem of the measurement of belief, the Jeffrey-Bolker theory suffers 
2SSee Bradley [1997] for a more detailed development of these ideas. 
26See Jeffrey [1983] and Balker [J 967]. 

RAMSEY AND THE MEASUREMENT OF BELIEF 
289 
from a crucial weakness by comparison to Ramsey's,21 For Bolker's representa-
tion theorem does not establish the existence of a unique measure of an agent's 
beliefs or a measure of her degrees of desire unique up to a choice of scale. In 
particular it allows for the possibility that two probability measures of an agent's 
degrees of belief, PI and P2, both be consistent with her expressed preferences yet 
differ to the extent that there are propositions A and B such that PI (A) > PI (B) 
but P2(B) > P2(A).28 
The essential difference, in this regard, between Ramsey's theory and that of 
Jeffrey and Bolker is that the latter make do without any conditional prospects of 
the kind postulated by Ramsey, working only with agents' attitudes to proposi-
tions. The price of this ontological economy would seem to be the underdetermi-
nation of agents' degrees of belief and desire by the evidence of their expressed 
preferences. If the price is too high, we have reason to favour a Ramsey-type 
theory when addressing the problem of measurement. 
But this should not obscure the fact that this discussion raises a difficult ques-
tion concerning the status of Ramsey's conditional prospects. For if conditional 
prospects could be given propositional expression then it should be possible to 
strengthen the Jeffrey's theory by simply adding to it suitably translated versions 
of Ramsey's postulates concerning preferences for conditional prospects. But the 
evidence is that this cannot be done without leading to some unpalatable conse-
quences. But if Ramsey's conditional prospects have no adequate propositional 
correlates, as has already been suggested by his definition of ethical neutral propo-
sitions, what exactly is their nature?29 
Department of Philosophy, Logic and Scientific Method, 
London School of Economics, UK 
BIBLIOGRAPHY 
[Bradley, 1998] R. W. Bradley. A Representation Theorem for a Decision Theory with Conditionals, 
Synthese, 116, 187-229, 1998 
[Bradley, 1997] R. W. Bradley. The Representation of Beliefs and Desires within Decision Theory, 
PhD dissertation, University of Chicago, 1997. 
[Bolker, 1967] E. D. Bolker. A Simultaneous Axiornatisation of Utility and Subjective Probability, 
Philosophy of Science, 34, 292-312,1967. 
[Davidson and Suppes, 1956] D. Davidson and P. Suppes. Finitistic Axiornatisation of Subjective 
Probability and Utility, Econometrica, 24, 264-275, 1956. 
[Fishburn,198I] P. C. Fishburn. Subjective Expected Utility: A Review of Normative Theories, The-
ory and Decision, 13, 139-199, 1981. 
[Jeffrey, 1983] R. C. Jeffrey. The Logic of Decision, 2nd edn, Chicago: University of Chicago Press, 
1983. 
[Joyce, 1999] J. Joyce. The Foundations of Causal Decision Theory, Cambridge University Press, 
1999. 
27 On the other hand, with respect to the problem of normative justification the Jeffrey-Bolker theory 
is much better than Ramsey's. 
28Forfurther discussion of this problem, see Bradley [1997] and Joyce [1999]. 
29 An attempt to give their logical properties is to be found in Bradley [1998] and [1997]. 

290 
RICHARD BRADLEY 
[Krantz et al., 1971] D. H. Krantz, R. Duncan Luce, P. Suppes and A. Tversky. Foundations of Mea-
surement, Volume I, Academic Press, 1971. 
[Pfanzagl,1968] 1. Pfanzagl. Theory of Measurement, New York: Wiley, 1968. 
[Ramsey, 1926] F. P. Ramsey. Truth and Probability. In Philosophical Papers, ed. D. H. Mellor, Cam-
bridge: Cambridge University Press, 1990 .. 
[Sahlin, 1990] N.-E. Sahlin. The Philosophy of F. P. Ramsey, Cambridge: Cambridge University 
Press, 1990. 
[Savage, 1954] L.1. Savage. The Foundations of Statistics, 1954. 2nd edition, New York: Dover, 1972 
[Schervish et al., 1990] M. 1. Schervish, T. Seidenfeld and 1. B. Kadane. State-dependent Utilities, 
Journal of the American Mathematical Association, 85, 840-847, 1990. 
[Sobel, 1988] 1. H. Sobel. Ramsey's Foundations Extended to Desirabilities, Theory and Decision, 
44,231-278,1988. 

EDWARD F. MCCLENNEN 
BAYESIANISM AND INDEPENDENCE 
1 INTRODUCTION 
The cornerstone of the Bayesian theory of utility and subjective probability, the 
independence principle, places a significant restriction on the ordering of options 
that involve risk or uncertainty (in suitably defined senses of each of these terms). 
For the case of options involving risk, the principle is typically formulated in the 
following manner: 
The Independence Principle (IND): Let P, pI and Q be any three 
risky prospects or gambles, and 0 < A ::::; 1 then 
p", pI => AP + (1 - A)Q '" AP' + (1 - A)Q, 
where ""," denotes indifference.l That is, substituting indifferent components for 
one another preserves indifference. 
IND invites particularization and reformulation in a variety of different ways. 
For the matters to be explored below, perhaps the most important particularization 
is where the components are not themselves lotteries, but "sure" outcomes (e.g., 
amounts of money). Since an outcome involving no risk can be viewed as a "gam-
ble" in which one gets that outcome with probability I, IND yields directly the 
following: 
Independence for Sure Outcomes (ISO): Let 0 1, O2 , 0 3 , be any 
three sure outcomes (monetary prizes, etc.), and 0 < A ~ 1: then 
0 1 '" O2 => AOI + (1 - A)03 '" A02 + (1 - >')03 â¢ 
Independence, as formulated above, presupposes that.>. is a well defined proba-
bility value satisfying, in general, the condition 0 ~ >. ~ O. But the principle 
has a natural extension to cases in which the agent faces prospects the likelihoods 
of whose outcomes may not be well defined. In Savage [1972], independence is 
formulated without reference to probabilities at all, but only to the notion of mu-
tually exclusive and exhaustive events that condition the consequences of various 
acts. Formulated somewhat less rigorously than it is in Savage, but in a manner 
that clarifies its connection with the version defined above, the requirement is: 
1 The notations I shall utilize here are taken from Fishburn and Wakker [1995]. That article contains 
an extremely helpful and for the most part insightful guide to the history of the utilization of various 
versions of the independence axiom, as well as a very comprehensive bibliography. For the issues 
to be discussed here, one cannot do better than start with their account. My only complaints are (I) 
that they do not give sufficient attention to the dominance interpretation that clearly motivated Savage 
to embrace a version of independence, and (2) they pass over the manifold criticisms that have been 
mounted to the acceptance of various versions of this axiom. 
291 
D. Corfield and l Williamson (eds.), Foundations of Bayesianism, 291-307. 
Â© 2001 Kluwer Academic Publishers. 

292 
EDWARD F. MCCLENNEN 
Savage Independence (SI): Let E and - E be mutually exclusive 
and exhaustive events conditioning the various risky components of 
four gambles, R, R', 8, 8 ' and let the schedule of consequences be as 
follows: 
E 
-E 
R 
P 
Q 
R' pI 
Q 
8 
P 
Q' 
8' pI 
Q' 
Then: R ~ R' {:} 8 ~ 8 ', where "~" denotes the weak preference 
ordering relation. 
In very general terms, the particular formulation to which appeal is made in 
a given axiomatic construction typically depends in part on the strength of the 
other axioms employed and in part on considerations of simplicity and/or formal 
elegance. But, ~s we shall see, some version or other of independence or something 
that implies independence is invariably to be found in the construction. 
2 
AN ALTERNATIVE FORMULATION: THE SURE-THING PRINCIPLE 
The independence axiom is only one of two ways in which the key axiom of ex-
pected utility and subjective probability has been formulated. The other formu-
lation, following Savage [1972] came to be known as the "sure-thing" principle 
(STP).2 It is introduced in Friedman and Savage [1952] in the following manner 
(once again adjusting its formulation to our present notation): 
[S]uppose a physician now knows that his patient has one of several 
diseases for each of which the physician would prescribe immediate 
2Tbe earliest reference to what came to be known as the "sure-thing" principle, as far as I have 
been able to determine, occurs in a discussion by Savage of a decision situation in which risk is not 
well-defined-what has come to be know as a case of decision making under conditions of uncertainty. 
Savage imagines an agent who is interested simply in maximizing expected income, but who is faced 
with a situation in which he caunot appeal to well-defined probabilities. Under such circumstances, 
Savage argues, 
... there is one unquestionably appropriate criterion for preferring some act to some 
others: If for every possible state, the expected income of one act is never less and is 
in some cases greater than the corresponding income of another, then the former act 
is preferable to the latter. This obvious principle is widely used in everyday life and 
in statistics, but only occasionally does it lead to a complete solution of a decision 
problem. 
[Savage, 1951] 
In neither this article, nor in the one he wrote with Friedman a year later, does Savage characterize 
the principle in question as the "sure-thing" principle: that term appears to occur for the first time in 
Savage [I 9721. 

BAYESIANISM AND INDEPENDENCE 
bed rest. We assert that under this circumstance the physician should, 
and unless confused, will prescribe immediate bed rest whether he is 
now, or later, or never, able to make an exact diagnosis. 
Much more abstractly, consider a person constrained to choose be-
tween a pair of alternatives, Rand R' without knowing whether a 
particular event E does (or will) in fact obtain. Suppose that, depend-
ing on his choice and whether E does obtain, he is to receive one of 
four gambles, according to the following schedule: 
Event 
E 
-E 
Choice 
R 
P 
Q 
R' 
P' 
Q' 
The principle in sufficient generality for the present purpose asserts: 
if the person does not prefer P to P' and does not prefer Q to Q' then 
he will not prefer the choice R to R'. Further, if the person does not 
prefer R to R' , he will either not prefer P to P' or not prefer Q to Q' 
(possibly both). 
We anticipate that if the reader considers this principle, in the light of 
the illustration that precedes and such others as he himself may invent, 
he will concede that the principle is not one he would deliberately 
violate 
[Friedman and Savage, 1952, pp. 468-9] 
293 
As Savage was to make clear two years later, what is being invoked here is 
essentially the dominance principle that had been employed by many statisticians 
as an admissibility criterion.3 As formulated above, it should be noted that it 
applies to cases in which the component entities, P, Q, etc., are themselves risky 
prospects or gambles. It can also, just like IND, be particularized to the case where 
outcomes are not gambles but sure amounts of money or other goods. 
3 THE CONNECTION BETWEEN INDEPENDENCE AND DOMINANCE 
Some sense of the logical connection between IND and STP can be gained by 
considering how one might get from a simple version of dominance to a version 
of independence. Consider the following version of dominance formulated with 
respect to two components, and well-defined probabilities: 
3See Savage [Savage, 1972, p. 1141. The principle can be recast in a form that is applicable to op-
tions defined in terms of some partition of n mutually exclusive and exhaustive events, and also applied 
to cases in which well-defined probabilities can be associated with each event in the partition. In any of 
its formulations, of course, one must presuppose that the choice of an option does not differentially af-
fect the probabilities of the conditioning events. That is, the conditional probability of Ei given choice 
of P must be equal to the conditional probability of Ei, given choice of Q. 

294 
EDWARD F. MCCLENNEN 
(STP*) ForO < A:::; 1,andallP,Q,P',Q', 
P > P' and Q ~ Q' =} [AP + (1 - A)Q] > [AP' + (1 - A)Q']. 
But since indifference implies weak preference, this yields immediately: 
For 0 < A :::; 1, and all P, Q, P', Q', 
P > P' and Q '" Q' =} AP + (1 - A)Q > AP' + (1 - A)Q'. 
And, on the plausible assumption that any component is indifferent to itself, this 
in turn implies: 
(IND*) ForO < A:::; 1,andallP,Q,P', 
P > P' =} AP + (1 - A)Q > AP' + (1 - A)Q. 
Now IND* makes the ordering of the two gambles-with fixed probabilities, and 
one component in common-a simple function of the relative ordering of the other 
component: this speaks to the notion of independence, and in fact IND* is a ver-
sion of the independence axiom. Principle STP*, on the other hand, is a version of 
STP-suggesting as it does that, if the components of one gamble are uniformly 
at least as good as the ones with which they are paired in the other gamble, and in 
one case better, then this determines the ordering of the two gambles.4 One can 
also start with some version of IND and derive a version of STP.5 
Many-starting with Friedman and Savage [I952]-have viewed the domi-
nance principle as intuitively the more compelling, and have thus viewed this as 
an effective way to motivate the independence condition. But the logical connec-
tion between the two principles can also be used to undermine the intuitive status 
of the sure-thing principle. If one had evidence that within some domain the in-
dependence condition is suspect, one could conclude that since independence is 
a logical consequence of the sure-thing principle (together with certain other pu-
tatively unproblematic axioms), the sure-thing principle in that context must also 
be regarded as suspect. In effect, any such argument in favor of independence, 
via modus ponens, could be used to undercut the sure-thing principle, via modus 
tollens. 
Since it was Savage himself who originally framed the sure-thing principle, 
and who, together with Friedman, used it to secure a version of independence, 
one might have expected that he would introduce it as a postulate in the book 
he subsequently published, The Foundations of Statistics. Interestingly enough, 
however, there he adopts just the reverse approach. The appeal to "sure-thing" 
considerations serves tosimply informally motivate the independence postulate: 
A businessman contemplates buying a certain piece of property. He 
considers the outcome of the next presidential election relevant to the 
4Friedman and Savage [1952, p. 469] proceed in this manner, by starting with a version of the sure-
thing principle framed in terms of a partition of events rather than probabilities, togetker with certain 
other assumptions, and deriving a version of the independence axiom. 
5To be sure, as Fishburn and Wakker [J 995, p. I I37] argue, for a full-blown axiomatization one 
can only substitute one of these principles for the other in the presence of certain other axioms. 

BA YESIANISM AND INDEPENDENCE 
attractiveness of the purchase. So, to clarify the matter for himself, he 
asks whether he would buy if he knew that the Republican candidate 
was going to win, and decides he would do so. Similarly, he considers 
whether he would buy if he knew the Democratic candidate were go-
ing to win, and again finds that he would do so. Seeing that he would 
buy in either event, he decides he should buy, even though he does 
not know which event obtains or will obtain, as we would ordinarily 
say. It is all too seldom that a decision can be arrived at on the basis 
of the principle used by this businessman, but, except possibly for the 
assumption of simple ordering, I know of no other extralogical prin-
ciple governing decisions that finds such ready acceptance. [Savage, 
1972, pp. 21-22] 
295 
But right on the heels of this informal discussion, it is a version of the indepen-
dence principle that is introduced as a postulate (and a version of the sure-thing 
principle is then shown to be derivable as a theorem). 
Whether one takes the sure-thing or the independence principle as basic, it 
is important to note that what is requisite for the expected utility and subjective 
probability constructions is some version or other that is explicitly taken to hold 
for gambles (or uncertain prospects) defined over gambles. Specifically, Inde-
pendence for Sure Outcomes (ISO) will not suffice.6 This point has not always 
been stated with the clarity that it deserves. In many expositions, independence or 
dominance is illustrated with reference to a case involving "sure" outcomes (for 
example, monetary payoffs), but what is employed in the construction is the much 
stronger generalized version of the principle. Where the issue is the plausibility of 
the principle in its requisite strength, it does no good to cite weaker versions of the 
same. 
4 
INDEPENDENCE AS NON-COMPLEMENTARITY 
The introduction of the term "independence" appears to have been motivated by 
a perceived analogy to the economic concept of independent goods, in which the 
value of a bundle of various quantities of different goods is an additive function of 
the value of the quantities of the various separate goods that make up that bundle. 
It is, of course, a well-known fact that independence with respect to the value of a 
bundle of commodities can fail. The value of the combination of x amount of one 
good and y amount of another good may not be equivalent to the sum of the value 
of x amount of the one good and the value of y amount of the other good. Failure 
of independence in such cases is said to be due to complementarity or interaction. 
That is, the value of one good may be enhanced or reduced in virtue of its being 
combined with some other good, as, for example, in the proverbial case in which 
white wine is said to complement fish, and red wine to complement beef. 
6Nor would a version of STP framed with regard to sure outcomes. 

296 
EDWARD F. MCCLENNEN 
Starting with von Neumann and Morgenstern [von Neumann and Morgenstern, 
1953], however, one finds repeated appeal to the argument that such a problem 
of complementarity cannot arise in the case of what are disjunctive (as distinct 
from conjunctive) bundles of goods, i.e., lotteries over goods, and hence that the 
assumption of independence in this context is warranted. Here is how the argument 
emerges in their work (adjusting the quote to the notation introduced above): 
By a combination of two events we mean this: Let the two events be 
denoted by P and Q and use, for the sake of simplicity, the probability 
50%-50% . Then the "combination" is the prospect of seeing P occur 
with probability 50% and (if P does not occur) Q with the (remaining) 
probability of 50% . We stress that the two alternatives are mutually 
exclusive, so that no possibility of complementarity and the like exists. 
Samuelson [1952] explicitly marks the analogy and, while acknowledging that 
complementarities can arise in the case of (conjunctive) bundles of goods, insists 
that the nature of a disjunctive (or stochastic) bundle, in which one is to get just 
one of the disjuncts, makes it plausible to impose independence as a condition on 
preferences for gambles. 
The argument for non-complementarity in the case of disjunctive bundles is, 
however, simply not compelling at all. Disjunctive bundles may not be subject to 
the problem of commodity complementarity, but this does not rule out the pos-
sibility of forms of "complementarity" that are special to disjunctive prospects.7 
By way of illustration, consider the following type of decision situation, first 
isolated by Allais [1953], in which the conditioning events are the numbered tick-
ets to be drawn in a lottery, with various associated prizes (in units of $ 100,000) 
attaching to each: 
Lottery ticket number 
1 
2-11 
12-100 
P 
5 
5 
5 
Q 
0 
25 
5 
P' 
5 
Q' 
0 
5 
25 
o 
o 
7The possibility of complementarity is discussed in [Manne, 1952; Allais and Hagen, 1979; 
McClennen, 1983; Loomes, 1984] and [Sen, 1985]. Broome [1991, pp. 37-38, fn. 13] has complained 
that I do not understand that "complementarity" is a term (as used by economists) that applies just in 
the context of conjunctive bundles of commodities. I am prepared to yield to Broome with regard to a 
point of terminology; but terminology, it seems to me, is not the issue. As the examples I am about to 
present show, I believe, the considerations that undercut the application of the independence condition 
to certain cases of choice under conditions of risk andlor uncertainty closely parallel the considerations 
that undercut the application of an analogous independence condition to certain cases of choice among 
commodity bundles. 

BAYESIANISM AND INDEPENDENCE 
297 
Significant proportions of subjects register a preference for P over Q but a pref-
erence for Q' over P'. 8 For one who accepts both that the set of options should 
be weakly ordered, and also accepts a standard reduction principle, this preference 
pattern leads 1:0 a violation of IND. 
Here, the set of alternatives consisting of {P, Q} differs from the set of alter-
natives consisting of {PI, Q'} only in respect to the level of the constant prize 
associated with ticket numbers 12-100. With an appropriate repartitioning of con-
ditioning states-by letting E be the state in which the ticket drawn has a number 
between 1 and 11, and not-E be the state in which the ticket drawn has a num-
ber between 12 and 100--0ne can then directly appeal to SI-Savage's version of 
the independence principle-to show that if P is preferred to Q then pI must be 
preferred to Q'. 
Consider, however, an agent who is concerned in risky choices with the disper-
sion in the monetary value of the various possible prizes, and who, other things 
being equal, prefers less to more dispersion.9 The gamble P has zero dispersion, 
and this may, despite its lower expected monetary return, make it attractive relative 
to Q. When P and Q are transformed, respectively, into pI and Q', by reducing 
the payoff for lottery tickets 12-100 from 5 units to 0, the expected monetary value 
of both pI and Q' is reduced by the same amount. But this is not the case with 
regard to the increases in dispersion. The increase in dispersion from P to pI is 
greater than the increase from Q to Q', regardless of how dispersion is measured. 
If dispersion considerations are relatively important, then, the fact that in the case 
of Q' the alteration in payoffs results in less increase in dispersion might, then, tip 
the balance in favor of Q'. 
The way in which dispersion considerations can result in preference patterns 
that fail to conform to IND can be illustrated in an even more striking manner 
by reference to an example offered by Kahneman and Tversky [1979]. Suppose 
the agent is to make a selection from each of the following two sets of paired 
alternatives: 
P 
Q 
R (66) 
Y (33) 
Black (01) 
$ 2400 
$ 2400 
$0 
$0 
$ 2400 
$2500 
$2400 
$2500 
$ 2400 
$0 
$2400 
$0 
In this instance, the options are defined in terms of drawing a colored ball from 
an urn-Red, Yellow, or Black-where the urn contains only balls of those three 
8Empirical findings are surveyed in, e.g., [Kahneman and Tversky, 1979; MacCrimmon and lars-
son, 1979] and [Shoemaker, 1980], Savage himself, who was firmly committed to IND, admitted that 
on first consideration these were his preferences. See Savage [1972, p. 103], 
9For simplicity, I assume here that the agent is concerned simply with monetary payoffs and prob-
abilities (including, of course, in this case the dispersion features of the probability distributions) and 
that he treats the value of sure amounts of money as a linear function of monetary amount. 

298 
EDWARD F. MCCLENNEN 
colors, and in proportion 66/33/l. 
Again, a significant number of persons prefer P to Q but also prefer Q' to P', 
in violation of IND. The preferences in question can arise, however, if the agent 
ranks the options by appeal to the following dispersion-sensitive rule. Suppose 
the agent prefers, other things being equal, a higher expected monetary return to a 
lower one but also prefers, other things being equal, a smaller expected shortfall, 
where expected shortfall is defined as the expected deviation below the mean (that 
is, 112 the expected deviation from the mean). Assume, for the sake of simplicity, 
that these are the only relevant factors and that the agent's implicit tradeoff rate 
between expected return and expected shortfall, and hence his rule for evaluating 
such gambles, is given by the following linear function: 
V(P) = E(P) - kS(P), 
where V(P) is the value, E(P) is the expected return (mean value), S(P) is ex-
pected shortfall, of the gamble P, and k is some constant, defined on the open 
interval [0, 1].10 For illustration, let k = 1/2. Then one gets the following values: 
g 
S 
E- .5S 
P 
2400 
0 
2400 
Q 
2409 
30.03 
2393.985 
P' 
816 
538.56 
546.72 
Q' 
825 
552.75 
548.625 
If this method of evaluation-based on the specified linear function of expecta-
tion and shortfall (dispersion)-is used, P will be preferred to Q but Q' will be 
preferred to P', in violation of IND.ll 
Now, if a person utilized such a evaluation procedure--one that incorporated 
considerations of expected monetary value and dispersion-he or she could argue 
that precisely why a violation of IND occurs here is because a special kind of 
complementarity arises in the concatenation of risky prospects. To see this it will 
prove useful to change the example slightly. In the example just discussed, P is 
presumed to be preferred to Q. But suppose that P is replaced with P* which 
yields the agent a payoff $ 2393.985 instead of $ 2400, and the agent employs the 
rule stated above. Then one obtains the following values: 
10 Again, for the sake of the example, I assume that the agent is concerned simply with monetary 
payoffs and probabilities (including the dispersion features of the probability distributions) and that he 
treats the value of sure amounts of money as a linear function of monetary amount. Similar results can 
be obtained if shortfall, and, more generally, dispersion, is measured in terms of the Gini-coefficient. 
11 For a very interesting, and much more formal, exploration of measures of risk that are sensitive 
to both mean values and dispersion (specifically variance) see Pollatsek and Tversky [1970]. As they 
go on to note, this approach to the measurement of risk is incompatible with standard expected-utility 
theory. 

BAYESIANISM AND INDEPENDENCE 
299 
E 
8 
E- 58 
P* 
2393.985 
0 
2393.985 
Q 
2409 
30.03 
2393.985 
pI 
803.9549 
530.610234 
538.649783 
Q' 
825 
552.75 
548.625 
The gambles P* and Q are now indifferent to each other, but Q' is still preferred 
to P'. In this instance, then, when Q is substituted for P* in the more complex 
gamble, even though P* and Q are indifferent to one another, the latter enhances 
the value of the resultant complex gamble more than does the former: that is, Q' 
is preferred to P'. Moreover, there is no mystery as to this differential impact 
on the value of the resultant complex gambles. The rule employed makes the 
value of the complex gamble-the whole-a function of mean value and shortfall. 
The expectation factor's impact is "well-behaved". The embedding of P* in the 
more complex gamble results in a proportional decrease in expected value that is 
strictly equal to the proportional decrease that results from embedding Q: that is, 
E(P*)/E(PI ) = E(Q)/E(Q'). The differential impact is due to the shortfall 
factor. The proportional increases in dispersion are not equivalent. Thus, for one 
who is concerned about dispersion, even though P* is indifferent to Q, when the 
two are considered in isolation, there is a better "fit" between Q and [ . , 34/100; $ 
0, 66/100] than there is between P* and [ â¢ , 34/100; $ 0, 66/100]. Combining Q 
with the balance results in a smaller proportional increase in dispersion than that 
which results from combining P* with the balance. 
There are two other much discussed counterexamples to the independence prin-
ciple, which serves to suggest a distinct type of complementarity that can arise in 
the concatenation of risky prospects. The examples are both due to Ellsberg [1961] 
and are directed not at IND but at Savage's version, SI. In the first example, Ells-
berg considers a situation in which the agent is to choose between various gambles 
based upon drawing a ball at random from an urn that contains Red, Black, and 
Yellow balls, where one knows that there are 30 Red balls in the urn, and that 
60 are either Black or Yellow, but the relative proportion of Black and Yellow is 
unknown: 
(30) 
(60) 
Red 
Black 
Yellow 
RangeofEMR 
P 
$ 100 
0 
0 
33113 
Q 
0 
100 
0 o to 66 2/3 
pI 
$ 100 
0 
100 
33113 to 1 
Q' 
0 
100 
100 
662/3 
Since the probabilities of the conditioning events are only partially defined, one 
cannot associate with each such option an unambiguous expected monetary return. 

300 
EDWARD F. MCCLENNEN 
But, as the column to the far right serves to indicate, one can still specify the 
possible range of such values. 
Ellsberg notes that many people prefer P to Q, while preferring Q' to P'. He 
also notes that the following rule of evaluation generates this preference ordering: 
rank options in terms of increasing minimum expected monetary return. Now 
note that the pair of options {P, Q} differ from the pair of options {Pi, Q'} only 
in respect to the payoffs in the event that a Yellow ball is drawn. But in each 
case the amount to be received if a Yellow ball is drawn is constant. Thus, once 
again with an appropriate repartitioning of the states, SI applies, and requires that 
p be preferred to Q just in case pi is preferred to Q', contrary to the described 
preferences. Thus, preferences based on the rule in question violate SI 
Once again, one can interpret what is happening here in terms of the notion 
of complementarities with respect to the value of disjunctions of outcomes. The 
shift from a situation in which, under the condition of drawing a yellow ball one 
receives $ 0, regardless of which act is chosen, to a situation in which, under the 
same chance conditions, one receives $ 100, regardless of which act is chosen, 
results in "contamination" (to use Samuelson's term for complementarity). And, 
once again, there is no mystery here as to how this happens. The person who adopts 
Ellsberg's rule can be characterized as uncertainty (or, as Ellsberg himself terms it, 
"ambiguity") averse: uncertain prospects (as distinct from those whose associated 
expected return is well defined) are discounted to their minimum expected return. 
Although one can think of pi and Q' as resulting from a modification of P and Q, 
respectively-in each case the addition of $ 100 to the payoff when a yellow ball 
is drawn, this proportional increase in payoffs has differential impact with regard 
to uncertainty or ambiguity. In the case of choice between P and Q it is Q that 
presents an uncertainty; but given the substitution, it is now the counterpart to P, 
namely, pi, (and not the counterpart to Q, namely Q') that presents the uncertainty. 
In Ellsberg's other example one is to suppose that the agent is asked to choose 
between the following three gambles: 
P 
[$ 100, E; $ 0, -E] 
Q 
[$ 0, E; $ 100, -E] 
R 
[$ 100, H; $ 0, T], 
E is some event whose likelihood is completely unknown and (H, T) are the two 
possible outcomes of a toss of what is presumed to be a fair coin, so that a given 
subject can be presumed to assign a subjective probability of 1/2 to each of Hand 
T. Many persons report that they prefer R to both P and Q but that (predictably) 
they are indifferent between P and Q. Such a ranking typically expresses itself 
as a willingness to pay more for R than for either P or Q, but to be willing to 
pay exactly the same amount for either P or Q. That is, for the same prizes or 
outcomes, persons typically prefer even-chance odds to ambiguous (i.e., uncertain) 
odds. Consider now, the following compound gambles, based on the above options 
(where, once again, 'H' and 'T' refer to the outcomes of the flip of the same fair 
coin): 

BAYESIAN ISM AND INDEPENDENCE 
301 
RI 
[P, H; Q, T] 
pI 
[P, H; P, T] 
By appeal to the standard reduction rule, the agent must rank RI as indifferent 
to R.l2 Moreover, by analogous reasoning, pI must be ranked indifferent to P. 
Hence, given the preferences projected above for Rand P, and acceptance of the 
standard weak ordering axiom, the agent must prefer RI to pl. But this violates 
IND. pI and RI differ only in that where RI has an occurrence of Q, pI has an 
occurrence of P; but, by hypothesis, the agent is indifferent between P and Q, 
hence by IND, pI should be indifferent to RI. 
The agent who prefers R to P, however, has a natural rejoinder and one that ap-
peals once again to a special kind of complementarity for disjunctive prospects-a 
complementarity that occurs when the agent's method of evaluation is sensitive 
to ambiguity or uncertainty. While the agent is indifferent between Pand Q, the 
combination of Q with P in the even-chance lottery RI results in the ambiguity 
or uncertainty of the odds associated with each gamble taken separately canceling 
each other out, while the combination of P with P in pI results in no correspond-
ing reduction of ambiguity. This is not due to anything pertaining to P or Q taken 
separately. The resolution of the ambiguity is a function of the particular way in 
which the component gambles are combined in pI and in RI. 
What has been isolated here is something that is distinct from the complemen-
tarity that arises in connection with conjunctive bundles. Here there is no question 
of some sort of interaction between prizes, both of which are to be received. It 
arises within the context of a disjunctive concatenation of prizes or goods, and 
turns on the implication of combining both well-defined and indeterminate odds. 
But it bears all the marks of being a type of complementarity. The gambles P and 
Q are clearly indifferent to one another when considered in isolation. When each 
is disjunctively combined with P to form pI and RI, however, there is a "fit" that 
obtains in case of RI that does not obtain in the case of pl. The fit, moreover, is 
perfectly explicable: the rules for combining probabilities imply that RI involves 
no ambiguity with respect to the odds of receiving $ 100; while those same rules 
imply that pI is maximally ambiguous. For one who is uncertainty (or ambiguity) 
averse, then, it does make a difference as to which of the (indifferent) components 
are combined with which. 
The implication of both the Ellsberg and the Allais examples is quite clear. One 
cannot infer that IND is a plausible condition to impose on the ordering of dis-
junctive bundles simply by appeal to the consideration that complementarities of 
the type that arise in connection with conjunctive bundles cannot arise in connec-
tion with disjunctive bundles. That argument is a non-starter, for it ignores a kind 
of "complementarity" that is unique to disjunctive bundles, and that forms an in-
telligible basis for discriminating between prospects, if one is (as in the case of 
I2If one treats the probability of E as an unknown, p, combining P and Q together by tossing a coin 
to decide which one to choose results in p being cancelled out, so its value is irrelevant. That is, the 
coin toss results in one's having an ex ante 50--50 chance of getting $ 100. 

302 
EDWARD F. MCCLENNEN 
the Allais example) concerned with dispersion or (as in the case of the Ellsberg 
examples) concerned with uncertainty (ambiguity). 
5 SURE-THING REASONING AS A BASIS FOR THE INDEPENDENCE 
PRINCIPLE 
How do things fare if we approach the axiomatization by appeal to STP rather 
than IND? There is no question that the dominance idea to which STP appeals is 
intuitively very plausible. Recall, once again, the argument presented by Friedman 
and Savage [I952] in support ofIND. A version ofIND follows directly from STP, 
in the presence a standard reduction principle. Now, STP mandates preference for 
P over Q if, no matter what the turn of events, the outcome of choosing P is 
at least as good as the outcome of choosing Q and, for some turn of events, the 
outcome of choosing P is better than the outcome of choosing Q. And that seems 
plausible enough. If you strictly prefer the consequences of P to those of Q, given, 
say, that the event E occurs, and if you would regard the consequences of P as at 
least as good as those of Q, given that the event not-E occurs, then choice of P 
over Q promises a "sure-thing" with respect to consequences: by choosing P you 
cannot do worse, and may end up doing better, than if you choose Q. Moreover, 
if you strictly prefer the consequences of P to those of Q, given E, and if you 
strictly prefer the consequences of P to those of Q, given not-E, then choice of P 
over Q promises a "sure-thing" in an even stronger sense: it is guaranteed that one 
will do better having chosen P rather than Q, regardless of the turn of events. 
Despite the fact that many have taken this to be a decisive consideration in favor 
of STP, this line of reasoning is also flawed. STP is very strong. It is framed with 
respect to the outcomes that can be associated with arbitrarily selected partitions of 
conditioning states. The principle requires that if there exists any event-partition 
for which the outcomes of P dominate the outcomes of Q, then P must be pre-
ferred to Q. In particular, then, the principle is not limited in its application to 
outcomes that can be characterized in terms of sure or riskless outcomes. 
This raises a substantial issue. Consider a variant of the Kahneman and Tversky 
problem we discussed above, where p(E) = 33/34, and p(F) = 34/100: 
P 
[$ 2400, E or -E] 
Q 
[$ 2500, E; $ 0, -EJ 
P' 
[P, F; $ 0, -F] = [[$ 2400, E or -E], F; $ 0, -F] 
Q' 
[Q,F; $ 0, -F] = [[2500, E; $ 0, -E], F; $ 0, -F] 
Once again, many report they prefer P to Q, but Q' to P'. In the case of P' 
and Q', then, we have a partition for which the associated outcomes satisfy the 
conditions for dominance: P preferred to Q, and $ 0 at least as good as $ O. Thus, 
by STP the agent should rank P' over Q'. But what qualifies these outcomes as 
relevant for the purposes of assessing the choice between P' and Q' from a "sure-
thing" perspective? Within the framework of a finer partitioning of events-and 

BAYESIANISM AND INDEPENDENCE 
303 
one that is an explicit feature of the problem-it is simply not true that one does 
at least as well by choosing P' as by choosing Q', regardless of the turn of (all 
relevant) events. By inspection, the outcome of Q' in the event that both E and F 
occur is $ 2500, which is, by hypothesis, strictly preferred to any of the possible 
outcomes of P'. I do not mean to suggest, of course, that application of STP can 
be undercut in such cases simply by displaying some other partition of events such 
that preferences for the outcomes under that partition fail to satisfy the antecedent 
condition of STP. The issue here concerns the propriety of appealing to a partition 
under which the antecedent conditions are satisfied even though there exists an 
explicit refinement of that very same partition for which the antecedent conditions 
are not satisfied. If there is such an explicit refinement, then by reference to the 
consequences under that (refined) description, it is no longer clear what is the force 
of an appeal to dominance considerations. 
Savage himself was well aware of the full scope of STP, and explicitly raised 
the question of whether it might be appropriate to restrict it to cases where the 
outcomes themselves are not defined in probabilistic terms. Focusing on the case 
of event-defined gambles over "sure" amounts of money, he rejects this suggestion 
on the following grounds: 
A cash prize is to a large extent a lottery ticket in that the uncertainty 
as to what will become of a person if he has a gift of a thousand dollars 
[for example] is not in principle different from the uncertainty about 
what will become of him if he holds a lottery ticket. 
This amounts to denying that there is anything like a bedrock level of certainty. 
On this account, it is risk all the way down. Suppose, however, we grant this, and 
hence understand that one cannot distinguish a more restrictive version of STP. 
What makes this an argument for accepting STP rather than rejecting it? 
The agent can acknowledge, of course, that if he chooses Q' over P', then he 
moves through a state in which a dominance relation obtains. More specifically, 
if we think of the F -events as occurring first, followed by the E-events, then no 
matter what the outcome of the F -events, and before the E-events are run, the 
prospect the agent then faces, if he has chosen Q', is dispreferred to, or no better 
than, the prospect he would then be facing if he had chosen P'. He could argue, 
however, that this is something that he suffers only en passant, and since he is 
concerned only with final outcomes and their probabilities, it is of no consequence 
to him. 
Now, Savage's reply, as reported above, is that, in effect, it is always a matter 
of what we face en passant, since it is risk all the way down. This means, how-
ever, that any problem involves choice between gambles, and thus that the agent 
can never be sure he will always do better choosing one way rather than another. 
But, then, granting Savage's point, why not turn it upside down and regard it as 
undercutting the whole idea of an appeal to dominance with respect to outcomes? 
But perhaps we need not take such a drastic position. Any principle such as 
STP must be interpreted as constraining preferences among alternatives, under a 

304 
EDWARD F. MCCLENNEN 
given description of those alternatives. If the agent has not refined his description 
of certain component gambles, and treats them as simply possible outcomes over 
which he has a preference ranking, then it can be argued that it is appropriate to 
appeal to dominance considerations. Suppose, however, that he has refined his 
description of those outcomes-recognizing explicitly the nature of the further 
risks to be encountered. In such a case, since at that level of discrimination the 
principle is revealed not to apply, it is unclear what force there is to an argument 
that invokes dominance at a coarser level of description. 
I conclude, then, that while sure-thing considerations provide a highly plausible 
basis for a version of STP that is framed with respect to riskless outcomes (relative 
to some base-description), there is little to support the extension of this line of 
reasoning to the full blown principle STP. STP, no less than the IND, is subject to 
serious question. 
6 RAIFFA'S ARGUMENT 
Raiffa [1961; 1968] offers a quite different defense of IND, and one that has been 
extensively cited by others. Consider once again the Kahneman and Tversky ex-
ample: 
P 
Q 
P' 
Q' 
R (66) 
Y (33) 
Black (01) 
$ 2400 
$2400 
$0 
$0 
$2400 
$ 2500 
$ 2400 
$ 2500 
$2400 
$0 
$ 2400 
$0 
Raiffa suggests that the reported preference pattern would presumably hold if both 
preferentially inferior options, Q and P' were augmented by some very small 
amount under each of the conditioning events, say, in the following fashion: 
P 
Q* 
P* 
Q' 
R (66) 
Y (33) 
Black (01) 
$ 2400 
$ 2410 
$10 
$0 
$2400 
$ 2510 
$ 2410 
$ 2500 
$ 2400 
$10 
$ 2410 
$0 
Raiffa argues that if Pis preferred to Q*, and Q' is preferred to P*, by some 
individual, then it is reasonable to suppose that if the individual is offered the 
opportunity to choose between P and Q* if a fair coin comes up Heads, and to 
choose between P* and Q' if the coin lands Tails, the option [Choose P, if Heads; 
and Q', if Tails] will be preferred to option [Choose Q* , if Heads; and P* , if Tails]. 
The point is that the first of these more complex options promises him that he will 

BA YESIANISM AND INDEPENDENCE 
305 
get either one or the other of what he regards as the two superior options; and the 
second promises him that he will get either one or the other of the two inferior 
options. Call the first of these options PI Q' and the second Q* I P*. If we write 
out the schedule of payoffs, we get the following: 
PIQ' 
Q* IP* 
R 
[$ 2400, H; $ 0, T] 
[$ 2410, H; $ 10, T] 
Y 
[$ 2400, H; $ 2500, T] 
[$ 2410, H; $ 2510, T] 
B 
[$ 2400, H; $ 0, T] 
[$ 2410, H; $ 10, T] 
But, by inspection, we can see that the second-the inferior plan dominates the 
first-the superior plan, and does so with respect to sure outcomes! That is, we 
have here a violation of DSO. 
But once again the argument is hardly convincing. What Raiffa has assumed is 
that the agent will rank the option PI Q' over Q* I P*. But why? What mandates 
that ordering of the contingency plans is simply another version of IND, according 
to which if P is preferred to Q*, and Q' is preferred to P*, then [P, 1/2; Q', 1/2] 
must be preferred to [Q*, 1/2; P*, 1/2]. It is no surprise, then, that the agent who 
rejects IND at the first level, and then invokes IND at the second level (the level of 
more complex options) will get into serious difficulties. But there is no reason to 
suppose that having rejected IND at one level, the agent will accept it at the next 
level. For the agent under discussion, the very observations that Raiffa makes will 
suffice for his or her rejection of the "reasonable" assumption that PI Q' will be 
ranked over Q* I P*. So once again, the case for IND seems to elude us. 
7 
DYNAMIC VERSIONS OF RAIFFA'S ARGUMENT 
In recent years quite a number variations on Raiffa's argument have been tried out, 
most of them within the framework of a theory of dynamic choice. The objective 
has been to show that an agent who rejects IND or STP can be subjected to a 
sequence of choices such that, in the end, the agent will end doing less well, as 
measured, for example, by sure amounts of money, than if they had not rejected 
one or the other of the principles in question. The original version of this argument 
is to be found in Raiffa [1968]. The issues here are admittedly rather complicated. 
The reader will find my own detailed review of this line of reasoning in McClennen 
[1990].13 My position, as discussed in McClennen [1990, pp. l73-82]' is two-
fold. First, arguments of this sort only go through on the assumption that (as in 
Raiffa's simple argument discussed above) one supposes the agent to reject some 
version of IND, and then accept another version of the same axiom at another stage 
in the dynamic choice process. Second, within a dynamic framework any problem 
of violating the simple principle DSO can be completely avoided by settling upon 
130ne especially powerful argument of this sort was presented by Seidenfeld [1988a). I offered a 
rejoinder in the same issue of the journal in question, McClennen [1988], and Seidenfeld responded, 
again in the same issue, in Seidenfeld [1988b). 

306 
EDWARD F. MCCLENNEN 
a plan and resolutely carrying it out. For these reasons I fail to see how any of 
these dynamic variations on Raiffa's original argument provide any real support 
for either IND or STP. 
8 SUMMARY 
My concern here has been to examine a variety of standard arguments that have 
been offered in support of a key axiom for the Bayesian theory of rational choice. 
Each of the three arguments examined must be judged as a non-starter. While 
one can agree with Samuelson that complementarities of the classical sort, namely 
those that arise in connection with (conjunctive) bundles of commodities, cannot 
arise in connection with disjunctively defined prospects, still this does not pre-
clude the possibility of complementarity-like effects that are special to disjunctive 
combinations of goods. At the level of strength required for the constructions, 
STP strikes me as not at all plausible. And, finally, one should not be surprised if 
someone who both rejects IND and then accepts it at another level involving the 
same gambles as components can find themselves in the embarrassing position of 
having preferences that violate what is in fact a plausible, weak version of STP. I 
conclude that the Bayesian theory is in great need of something it has yet to find: 
a convincing defense of the Independence Axiom. 
Department of Philosophy, Logic, and Scientific Method, 
London School of Economics and Political Science. 
BIBLIOGRAPHY 
[Allais, 1953] M. Allais. Le comportement de l'homme rationnel devant Ie risque: Critique des pos-
tulats et axiomes de l'ecole americaine. Econometrica 21: 503-546, 1953. 
[Allais and Hagen, 1979] M. Allais and O. Hagen, eds. Expected Utility Hypothesis and the Allais 
Paradox. D. Reidel Publishing Company, 1979. 
[Broome, 199I] 1. Broome. Rationality and the sure-thing principle. In Thoughtful Economic Man. g. 
Meeks, ed. Cambridge University Press, 1991. 
[Ellsberg, 1961] D. Ellsberg. Risk, ambiguity, and the Savage axioms. Quarterly Journal of Eco-
nomics 75: 643-669, 1961. 
[Fishburn and Wakker, 1995] P. Fishburn and P. Wakker. The invention of the independence condition 
for preferences. Management Science 41: 1130-1144, 1995. 
[Friedman and Savage, 1952] M. Friedman and L. J. Savage. The expected-utility hypothesis and the 
measurability of utility. Journal of Political Economy 60: 463-474, 1952. 
[Kahneman and Tversky, 1979] D. Kahneman and A. Tversky. Prospect theory: an analysis of deci-
sion under risk. Econometrica 47: 263-291, 1979. 
[Loomes, 1984] G. Loomes. The importance of what might have been. Progress in Utility and Risk 
Theory. O. Hagen, ed. pp. 219-235. D. Reidel Publishing Company, 1984. 
[MacCrimmon and Larsson, 1979] K. R. MacCrimmon and S. Larsson. Utility theory: axioms versus 
"Paradoxes". In Expected Utility and the Allais Paradox. M. Allais and O. Hagen, eds. pp. 333-409. 
D. Reidel Publishing Company, 1979. 
[Manne, 1952] A. S. Manne. The strong independence assumption -
gasoline blends and probability 
mixtures (with discussion). Econometrica 20: 665-669, 1952. 

BAYESIANISM AND INDEPENDENCE 
307 
[McClennen, 1983] E. F. McClennen. Sure-thing doubts. In Foundations of Utility and Risk Theory 
with Applications. B. P. Stingurn and F. Wenstop, eds. 1983. 
[McClennen, 1988] E. F. McClennen. Ordering and independence: a corurnent on Professor Seiden-
feld. Economics and Philosophy 4: 298-308, 1988. 
[McClennen, 1990] E. F. McClennen. Rationality and Dynamic Choice: Foundational Explorations. 
Cambridge University Press, 1990. 
[Pollatsek and Tversky, 1970] A. Pollatsek and A. Tversky. A theory of risk. Journal of Mathematical 
Psychology 7: 540-553, 1970. 
[Raiffa, 196Il H. Raiffa. Risk, ambiguity, and the Savage axioms: corurnent. Quarterly Journal of 
Economics 75: 690-694, 1961. 
[Raiffa, 1968] H. Raiffa. Decision Analysis. Reading, MA, Addison-Wesley, 1968. 
[Samuelson, 1952] P. A. Samuelson. Probability, utility, and the independence axiom. Econometrica 
20: 670-678, 1952. 
[Savage, 1951] L. 1. Savage. The theory of statistical decision. Journal of the American Statistical 
Association, 1951. 
[Savage, 1972] L. 1. Savage. The Foundations of Statistics. New York, Dover, 1972. First published, 
1954. 
[Seidenfeld, 1988a] T. Seidenfeld. Decision theory without 'independence' or without 'ordering', 
what is the difference? Economics and Philosophy 4: 267-290, 1988 .. 
[Seidenfeld, 1988b] T. Seidenfeld. Rejoinder. Economics and Philosophy 4: 309-315, 1988. 
[Sen, 1985] A. Sen. Rationality and uncertainty. Theory and Decision, 18: 109-27, 1985. 
[Shoemaker, 1980] P.1. H. Shoemaker. Experiments on Decision Under Risk: The Expected-Utility 
Hypothesis. Boston, Martinus Nijhoff Publishing, 1980. 
[von Neumann and Morgenstern, 1953] 1. von Neumann and O. Morgenstern. Theory of Games and 
Economic Behavior, Third Edition. Princeton University Press, Princeton, 1953. 

PART V 
CRITICISMS OF BAYESIANISM 

PlllLIPPE MONGIN 
THE PARADOX OF THE BAYESIAN EXPERTS 
INTRODUCTION 
Suppose that a group of experts are asked to express their preference rankings on 
a set of uncertain prospects and that all of them satisfy the economist's standard 
requisite of Bayesian rationality. Suppose also that there is another individual who 
attempts to summarize the experts' preference judgments into a single ranking. 
What conditions should the observer's ranking normatively be expected to satisfy? 
A natural requirement to impose is that it be Paretian, i.e., it should respect unan-
imously expressed preferences over pairs of prospects. Another condition which 
appears to be desirable is that the observer's and the experts' rankings should con-
form to one and the same decision theory, i.e., the observer himself should be 
Bayesian. The next question is then, are these seemingly compelling normative 
assumptions compatible with each other? 
As a specific application, think of an insurer who considers selling a new insur-
ance policy and consults a panel of experts before deciding which specification of 
the insurance policy, if any, should be marketed. Suppose that the insurer knows 
little or nothing about how to elicit the experts' subjective probabilities. The only 
way in which he could take advantage of the panel's expertise seems to be this: 
he will require the experts to state their preferences between the logically possi-
ble specifications, and then aggregate these data to define his own ranking. If one 
further assumes that the experts are Bayesian, the question naturally arises of how 
the insurer could conform to the double consistency requirement just explained. 
Notice that this question makes perfectly good sense even if one has assumed that 
the insurer is not familiar with Bayesian elicitation methods. Writers in the tra-
dition of de Finetti (1974-75) have emphasized that to conform to the Bayesian 
axioms is tantamount to being "coherent" in one's betting behaviour. regardless of 
whether or not one knows the probability calculus. 
Several writers in the field of collective choice or decision-making have in-
vestigated aggregative problems that are formally similar to the Bayesian experts 
problem. Their nearly unexceptional conclusion is that logical difficulties will re-
sult from the double imposition of Bayesianism and Paretianism on the observer's 
preference. In an earlier paper [Mongin, 1995], we provided an up-to-date analysis 
of these difficulties, using the axiom system which enjoys the highest theoretical 
status among Bayesians, i.e., Savage's [1972]. Essentially, the imposition of rela-
tively weak Paretian conditions, such as Pareto Indifference or Weak Pareto, leads 
to impossibility results in a quasi-Arrovian style, i.e., to dictatorial conclusions, 
whereas the imposition of the Strong Pareto condition involves a sheer logical im-
possibility, unless the individuals have identical probabilities or utilities. In each 
case, the inference depends on "technical" assumptions the role and relevance of 
309 
D. Corfield and 1. Williamson (eds.), Foundations of Bayesianism. 309-338. 
Â© 2001 Kluwer Academic Publishers. 

310 
PHILIPPE MONGIN 
which should carefully be ascertained; unexpected possibility results emerge when 
they are relaxed. Our Savagean conclusions encompass most of the more partial 
or elementary variants of the Bayesian experts paradox that have been discussed 
thusfar. We refer the reader to this earlier paper for references and comparisons. 
The logical difficulties of "consistent Bayesian aggregation" led some writ-
ers to relinquish the first consistency condition -
Paretianism -
while others 
abandonned the second -
Bayesianism. Either way out of the paradox involves 
a diminutio capitis. We have just suggested that the two requirements seemed 
equally natural. There are indeed serious arguments in favour of each, which 
makes the choice of a weaker version of consistency very awkward. In the present 
paper, we shall explore an altogether different potential solution to the paradox 
of the Bayesian experts. It consists in retaining the double consistency condition 
while varying the chosen notion of "Bayesianism". The impossibility results de-
rived from applying Savage's axioms suggest that one should take a fresh look at 
them. A natural candidate to play the culprit's part is the sure-thing principle; but 
it is not our intention here to weaken it. If only for the purpose of theoretical ex-
perimentation, we want to remain within the confines of Bayesianism. There are 
also significant and well-recognized problems connected with the use of Savage's 
divisibility axiom. The present paper will take notice of them, but its primary tar-
get is to investigate the role of those axioms which ensure that the utility value of 
consequences is independent of what state of the world occurs. 
Several writers in the Bayesian tradition, such as Karni [1985] and Dreze [1987], 
have repeatedly emphasized that state-independence is an inappropriate assump-
tion to make in general. The standard example to support this claim involves the 
partition of states into the events "the agent lives" and "the agent dies". Insurance 
economics is replete with examples of a less dramatic sort in which the assumption 
of state-independence appears to be indefensible, both normatively and factually. 
On the constructive side, Dreze, Karni and others have devised axiom systems 
which deliver state-dependent subjective expected utility representations. They 
provide the generalization of Bayesianism that we want to put to the test. The 
general question of this paper is then, does state-dependent utility theory offer a 
solution to the paradox of the Bayesian experts? 
For reasons of tractability rather than of substance, most of the work on state-
dependent utility does not employ Savage's framework but the alternative, highly 
accessible framework introduced by Anscombe and Aumann [1963]. We shall fol-
low the existing literature and rephrase both the paradox and its tentative solutions 
accordingly. As is well-known, Anscombe and Aumann's (AA) approach to un-
certainty involves a loss of generality with respect to Savage's, in that it assumes a 
lottery structure on the consequence set. On the other hand, it makes it possible to 
dispense with Savage's divisibility axiom, and thus to deal with finite state sets - a 
welcome extension of subjective expected utility (SEU) theory. As far as technical 
derivations are concerned, the analysis of "consistent Bayesian aggregation" a La 
Savage depended on the measure-theoretic properties of his construction, in par-
ticular the nonatomicity of his derived subjective probability. The reader should 

THE PARADOX OF THE BAYESIAN EXPERTS 
311 
expect the present analysis to revolve around the convexity properties of the con-
sequence set, as conveniently assumed by AA. 
The paper is organized as follows. Section 2 presents the definitions and axioms 
from SEU theory that will be used throughout. We shall briefly contrast AA's ini-
tial system - which is state-independent - with two later state-dependent variants. 
It is easy enough to axiomatize a completely state-dependent system of SEU. The 
well-known difficulty with this construction is that it leaves the individual's sub-
jective probability indeterminate. Most of the work by Dreze, Karni and others has 
actually consisted in defining systems of intermediary strength, which allow for a 
state-dependent utility valuation of consequences but preserve the determination 
(if perhaps not the uniqueness) of the individual's subjective probability. Among 
the variants of AA's construction, only these intermediary systems can be claimed 
by Bayesianism. To accept complete state-dependence is really to take off the edge 
of the doctrine; this appears to be a well-recognized point. We have selected here 
an influential intermediary system first introduced by Karni, Schmeidler and Vind 
[1983]. 
Section 3 restates the initial paradox by applying AA's own state-independent 
system. As suggested, the results exactly parallel those reached in the Savage case 
but are easier to derive. Section 4 makes a start with an easy possibility result: 
the paradox disappears in the pure state-dependent generalization of AA's system. 
Then, we proceed to reexamine the paradox in the light of the relevant intermediary 
system of Karni, Schmeidler and Vind (KSV). The general conclusion of section 
4 is that impossibility results can be derived in the KSV framework too, but the 
required technical assumptions are even stronger than those put to use in the state-
independent case. This is why we choose to impose these assumptions only on 
a subset of the state space, and accordingly obtain only local analogues of our 
earlier dictatorial or logical impossibility theorems. Section 5 discusses a two-
individual illustration of our impossibility results and compares them with those 
of Schervish, Seidenfeld and Kadane (1991), who have also investigated a state-
dependent version of the Bayesian experts problem. Section 6 elaborates on the 
implications of the present results for the theory of collective decision-making. 
The proofs of all formal statements are in Appendix A. 
2 DEFINITIONS AND AXIOMS FROM SUBJECTIVE EXPECTED 
UTILITY THEORY 
As in Anscombe and Aumann [1963], we assume that there is a finite set S of 
states of the world, to be denoted by s = 1"" ,T, and there is a set X of final 
outcomes, to be denoted by A, B, C, ... Throughout, we require that there are at 
least two distinct states and two distinct final outcomes. (A stronger cardinality 
restriction will be introduced in section 3.) The consequence set is R = ~F(X), 
i.e., the set of all simple probabilities on X. The set of uncertain prospects, or (to 
use Savage's word) acts, is the set H of all functions S -t R, to be denoted by 

312 
PHILIPPE MONGIN 
I, g, h, . " Then, 1 (8) is a simple probability on X; denote by 1 (8, A) the value it 
gives to A EX. Since the state set is finite, it is often convenient to denote acts as 
vectors: 
1 = [Rl ,'" ,Rs,'" ,RT]' 
where Rs stands for 1(8). (Then, we rewrite 1(8, A) as Rs(A).) Finally, consider 
the set R* = tl.F(H), i.e., the set of all simple probabilities on H. A typical 
element of R * may be written as: 
where A = (AI,'" , Ak) is a probability vector and indexes 1,Â· .. , k refer to 
particular acts in H.I Now, following many writers in AA theory, we can identify 
this element of R * with the following, altogether different mathematical object: 
As is well-known, to identify these two mathematical entities with each other is 
equivalent to assuming AA's "reversal of order" axiom. The resulting simplifica-
tion has a price, because it then becomes impossible to discuss the extension of 
AA's approach to "moral hazard", as promoted by Dreze [1987]. 
Granting the identification R* ::: H, H becomes the decision-maker's choice 
set. The preference relation ~ on H can then be subjected to all or part of the 
following axioms. 
AXIOM 1 (VNM axiom). ~ satisfies the von Neumann-Morgenstern axioms. 
Any axiomatic version of VNM theory will do (see [Fishburn, 1982], for details). 
AXIOM 2 (Nontriviality). There are two outcomes A*, A* such that 
[A*,Â·Â·Â· , A*] >- [A*,'" , A*]. 
The axiom below relies on the derived concept of a conditional preference. For 
any 8 E S, define ~s ("the preference conditional on 8") by2: 
1 ~s g iff [V!" g' E H : I~s = g~s' 1(8) = !'(8), g(8) = g'(8)] !' ~ g'. 
Define a state 8 to be null if its conditional preference ~s is trivial, i.e., ~s= 
HxH. 
1 We shall adopt the convention of using square brackets for uncertain prospects and curved ones for 
risky prospects (i.e., prospects with preassigned probabilities). 
2We use the game-theoretic notation f'-., 9'-. to refer to the subvectors obtained from f', 9' by 
deleting their s-component. 

THE PARADOX OF THE BAYESIAN EXPERTS 
313 
AXIOM 3 (State-Independence). For all non-null states s, t, and all constant 
f,g E H, f ts g iff f b g. 
Under state-independence it becomes meaningful to identify the constant act [R, 
... ,R] with its value R E R, so that t induces a preference relation on R. In this 
context Axiom 2 states that there are A * ,A* E X such that A * >- A*. Another 
version of Axiom 2 will also be used at a later stage in this paper: 
AXIOM 2/. There is a non-null state s. 
PROPOSITION 1 ([Anscombe and Aumann, 1963]). If t satisfies Axioms I, 2 
and 3, there exist a nonconstant VNM function u on R and a probability p = 
(PI,Â·Â·Â· ,PT) on S such thatfor all f, g E H: 
sES 
sES 
State s is null iff Ps = O. Any other pair (u' , p') that satisfies the same properties 
as (u, p) is such that p' = P and u' is a positive affine transform of u. 
PROPOSITION 2 ([Fishburn, 1970]). Ii t satisfies Axiom I, there exist VNM 
functions UI, ... ,UT on R such that for all f, g E H: 
f t g iff L us(f(s)) ~ L us(g(s)). 
sES 
sES 
The function Us represents the conditional preference ~s, and is constant iff s is 
null. Any other T -tuple (u~, ... ,u~) satisfying the same properties as (UI' ... , 
UT) is a positive affine transform of the latter vector. 
By a VNM function u on R we mean a function which has the following, 
mixture-preserving property: for any oX E [0,1] and any R, R', 
u((oXR, (1 - oX)R')) = oXu(R) + (1 - oX)U(R'), 
or equivalently (since we are considering here only simple probabilities R, R'), a 
function which has the expected-utility form: 
u(R) = L R(A)u(A). 
AEX 
The fact that the utility representations defined on the consequence set, i.e., u in 
Proposition 1 and the Ui in Proposition 2, are VNM is a characteristic feature of the 
Anscombe-Aumann approach as a whole. Conceptually, this feature is irrelevant 

314 
PHILIPPE MONGIN 
to the aim of the construction; technically, it is an ingenious device - actually, 
comparable with Savage's (P6) -
to facilitate the derivation of the SEU formula. 3 
OBSERVATION. If t satisfies not only Axiom 1, but also Axiom 2, there are 
VNM functions VI, ... ,VT on R and a probability p = (PI,' .. ,PT) on S such 
that for all j, g, E H: 
sES 
sES 
Any (T + I)-tuple (q, v~,,Â·Â· ,v~) such that q is a probability on Sand: 
Ps = 0 Â¢:> qs = 0, 
Vi = &.v if q ...J. 0 Vi arbitrary otherwise, 
s 
q. s 
s r , s 
can be substituted for (p, VI, â¢.. ,VT) in (***). 
NOTATION. We define U(J) = LSESPSu(J(s)) and V(J) = LSESPSvS(J(s)) 
when the suitable assumptions hold. As a rule, U, V, W will refer to representa-
tions of preferences over acts, and u, v, w to representations of preferences over 
consequences. 
The comparison between Proposition 1, Proposition 2, and the ensuing well-
known observation, brings out the classic difficulty of state-dependent utility the-
ory. The system consisting of only Axioms 1 and 2 is not rich enough to determine 
the decision-maker's subjective probability. To add Axiom 3 makes it possible to 
uniquely determine P if one selects a state-independent representation u on R, as 
do AA in their seminal article. However, Axiom 3 is too restrictive; it amounts 
to excluding the relevant complication of state-dependent preferences. Hence a 
dilemma of determination and relevance; see, among others, [Fishburn, 1970; 
Dreze, 1987; Karni, 1985; Karni, 1993; Schervish et al., 1990].4 
Various methods have been put forward to escape from the dilemma just sug-
gested. Most (but not all) of them consist in assuming Axioms 1 and 2, and then 
introducing further axioms to determine the state-dependent functions VI, ... , VT 
that underlie the uninformative representations UI, â¢.. , UT of Proposition 2. Once 
3We made another assumption on the consequence set n which -
by contrast to the VNM as-
sumption -
is dispensable within the AA approach. To save notation, we assumed a state-independent 
consequence set n. Some expositions of state-dependent utility theory, such as Fishburn's [1970], and 
actually the original paper by Karni, Schmeidler and Vind [1983], adopt a more general framework in 
which not only the evaluations but also the availability of consequences vary from one state to another. 
As far as we can judge, the results of the present paper can be extended unproblematically to this more 
general framework. On the issue of state-dependent consequences in expected utility theory, see also 
[Hammond, 1998]. 
4The last paper usefully emphasizes that Anscombe and Aumann's choice of a state-independent u 
on n is to some extent question-begging. Even when Axiom 3 holds, it is trivially possible to replace 
(*) with infinitely many equivalent state-dependent representations, each of which corresponds to one 
particular subjective probability. 

THE PARADOX OF THE BAYESIAN EXPERTS 
315 
the vector (Vl, ... , VT) is known (up to a positive affine transformation, PAT),5 
it becomes possible to write Us = PsVs, where the Ps are well-determined (and 
ideally unique) probability values. We shall not attempt at covering all the variants 
of this axiomatization strategy. A representative system will be enough for the 
purpose of this paper. 
As in Karni, Schmeidler and Vind [1983], we introduce an auxiliary binary re-
lation E. It is meant to describe the preference that the decision-maker would ex-
press between acts ifhis subjective probability were some given q = (ql,' .. , qT). 
KSV's strategy is to infer the agent's actual state-dependent utilities from the (sup-
posedly meaningful and even observable) hypothetical preference E, and then de-
termine his actual, unknown subjective probability p by using this utility-relative 
information. 
Formally, fix a probability q = (ql,'" , qT) on S with qs > 0 for all 8 and 
associate with each I E H an hypothetical act l' defined as follows: l' is on 
S, and for each 8, 1'(8) = 1(8)qs, that is to say, 1'(8) is that function on X 
which satisfies 1'(8, A) = 1(8,A)qs for all A E X. Note that 1'(8) is not a 
probability on X, unlike 1(8), but I' can be viewed as a probability on S x X 
unlike I. (The computation is obvious.) Given the positivity assumption made on 
q, the set H' of all hypothetical acts is clearly in a one-to-one relationship to H; 
this makes the notation I, l' unambiguous. The element l' describes the effect 
of compounding the given probability q on S with each of the lotteries that I 
assigns to 8 = 1,Â· .. , T. Define the hypothetical preference (E) to be a preference 
relation on the set of hypothetical acts. This formal construct is meant to capture 
the modification in the individual's preferences "if his subjective probability were 
q".6 
It is consistent to impose the same decision-theoretic constraints on both E and 
t, i.e., to subject hypothetical preference to the VNM axioms. Beyond this, some 
coordinating condition should relate t to E. In effect, KSV impose the ("consis-
tency") axiom that conditional preferences Es and ts are the same whenever sis 
non-null for t. The point of this axiom is to ensure that hypothetical preference 
data deliver usable information on the individuals' state-dependent utilities. 7 
AXIOM 4 (Hypothetical Preference). For all s E S that are non-null with respect 
to t, and for all I, 9 E H, 
I ts 9 Â¢:} I'Eag', 
where l' and g' are the elements in H' associated with I and 9 respectively. 
5 Formally, two vectors (v~" .. , vT) and (VI, ... ,VT) are identical up to a PAT if there are a 
number JL > 0 and a vector (111,'" ,liT) such that (V~, ... ,VT) = JL(V1," . ,VT )+(111,'" , liT). 
6Notice carefully that although hypothetical acts carry preassigned probabilities with them, they do 
not reduce to VNM lotteries. States of the world matter in the construction of hypothetical acts. 
7KSV's exposition is considerably more complex, due to their detailed analysis of null states. 
Waller (1987), and Schervish, Seidenfeld and Kadane [1990], provide alternative restatements; we 
do not follow them here. 

316 
PHILIPPE MONGIN 
PROPOSITION 3 ([Karni et al., 1983]). Assume that t satisfies Axioms 1 and 2'. 
Take any probability q on S with qs > 0 for all s E S. Assume that the induced 
hypothetical preference ~ also satisfies Axioms 1 and 2', and that t and ~ jointly 
satisfy Axiom 4. Then, there are VNM functions VI, . .. ,VT on n and a probability 
P = (PI,'" ,PT) on S such that for all f, g E H: 
(i) f t g iffLsESPsvs(f(s)) ~ LSESPSvS(g(s)) 
(ii) f'~g' iffLsES qsvs(f(s)) ~ LSES qsvs(g(s)). 
If s is non-null for t, then Ps > O. Any other (T + 1 )-tuple (v~, ... ,vr, p') that 
satisfies conditions (i) and (ii) is such that the vector (v~, ... ,vr) is a PAT of 
(VI,'" ,VT) and p~/p~ = ps/ptfor all s, t non-nullfor t. 
Notice that the theorem does not entirely determine the agent's probability on 
null states: if s is null for t and not for ~, then comparison of (i) and (ii) leads to 
the conclusion that Ps = 0; if s is null for both t and ~, nothing can be said about 
Ps (but necessarily, Vs = constant). 
Although the conclusions of the theorem are stated atemporally, they might be 
interpreted in terms of the two step-experiment mentioned at the outset. Irrespec-
tive of whether Axiom 4 provides a satisfactory formal rendering, the experiment 
itself raises a conceptual problem: the agent might well attach no sense to the 
expression of his preferences conditionally on the use of a subjective probability 
which is not his own.8 
At least, the KSV procedure has a significant negative argument to recommend 
itself: to the best of our knowledge, existing alternatives either entail only a partial 
solution to the indeterminacy-of-probability issue, or involve the same operational 
difficulties as the KSV procedure, or imply an even more radical departure from 
standard Bayesian assumptions. Karni and Schmeidler's [1993] state-dependent 
variant of Savage's axiomatization exemplifies the first problem. In the AA frame-
work, Karni's [1993] assumption of given transformations between the vs (-) il-
lustrates the second problem, while Dreze's [1987] use of a "moral hazard" as-
sumption illustrates the third. For all its shortcomings, KSV's article is a serious 
representative of the work done in the field of state-dependent utility theory. This 
is sufficient to make it relevant to a paper which is primarily concerned with theo-
retical experimentation. 
3 
IMPOSSIBILITY RESULTS IN THE STATE-INDEPENDENT CASE 
The present section will first introduce a multi-individual extension of the AA 
approach broadly speaking, and then restrict attention to the state-independent case 
8 [Dreze, 1987] expresses his critique of the KSV approach differently. He claims that it relies on 
information obtained from verbal behaviour, which he says is unreliable in principle and should be 
ignored. In essence, Dreze disqualifies KSV's contribution on the grounds that they do not follow the 
methodology of revealed preference theory. The critical point in the text does not depend on one's 
adhering to a revealed preference methodology. 

THE PARADOX OF THE BAYESIAN EXPERTS 
317 
with a view of deriving the AA variant of the Bayesian experts paradox. 
Let us assume that there are individuals, to be represented by indices i 
1,Â· .. ,n, and an observer, to be represented by index i = 0, who express their 
subjective probabilities indirectly, i.e., by stating their preferences ~i over uncer-
tain prospects. Throughout, we shall require ~i to satisfy some subset of the ax-
ioms of section 2, for all i = 0,1, ... , n. This requirement reflects the assumption 
that both the individual experts and the observer are Bayesian; it encompasses one 
of the two consistency conditions discussed in the introduction. The remaining, 
Paretian condition can be made precise in terms of one of the following standard 
requirements: for all I, g E H, 
1 ",i g, i = 1,Â·Â·Â· ,n ::} 1 ",0 g. 
(C) 
1 ~i g, i = 1,Â· .. ,n ::} 1 ~o g. 
(Cd 
1 :--i g, i = 1,Â· .. ,n ::} 1:--Â° g. 
(C2 ) 
1 ~i g, i = 1,Â·Â·Â· ,n and 3j E {I,Â·Â·Â· ,n}: I:--i g ::} 1:--Â° g. 
(C3) 
In social choice theory, these are the conditions of Pareto-Indifference, Pareto-
Weak Preference, Weak Pareto, Strict Pareto, respectively. We also introduce the 
Strong Pareto condition: 
(C+) = (C) & (C3). 
Obviously, (Cd ::} (C) and (C3) ::} (C2). Given the rich structure of the 
consequence set in the AA approach, more can be said on the logical relations 
between the Pareto conditions. It will shortly be seen that under a minor restric-
tion on preferences, (C1), hence (C) are implied by any other condition. Let us 
introduce the following restriction of Minimum Agreement on Acts: 
(MAA) 
3/*,1** E H, Vi = 1,Â·Â·Â· ,n, 1* :--i 1**. 
Notice the difference with the requirement of Minimum Agreement on Conse-
quences used in Mongin [1995, Section 3]. In the present context the latter would 
state that: 
(MAC) 
3R*, R** E R, Vi = 1,Â·Â·Â· ,n, R* :-- R**. 
In a pure state-independent context such as that of the earlier article, (MAC) 
provided an appropriate notion of minimum agreement among the individuals. 
We want a weaker condition here since it should also be applicable to the state-
dependent context of the following sections. 
For i = 0,1,Â·Â·Â· ,n denote by Ui the SEU representation of ~i when this 
relation satisfies all of the assumptions of Proposition 1, and Vi the more gen-
eral additive representation of ~ i that satisfies the unique assumption of Propo-
sition 2. Then, Ui(f) = LSESP!Ui(f(s)) and Vi(f) = LSESU!(f(S)). For 

318 
PHILIPPE MONGIN 
any vector-valued function ('PI,'" ,'Pk), denote its range (i.e., set of values) by 
Rge('Pl,'" ,'Pk). A basic consequence of imposing any of the AA systems of 
section 2 on the observer's and individual preferences ~o, ~l, ... ,~n is that the 
vector of corresponding utility representations has a convex range. Lemmas 4, 5 
and 6 spell out this fact and its important consequences in terms of the Vi repre-
sentations. The same results obviously apply to the Ui since they are restricted 
forms of the Vi. 
LEMMA 4. If ~o, ~l, ... ,~n satisfy the assumption of Proposition 2 (= Axiom 
1), then Rge(VO, VI, ... , vn) is convex. 
De Meyer and Mongin [1995] have investigated the aggregative properties of a 
real function 'Po which is related to given real functions 'PI, ... ,'Pn by unanimity 
conditions analogous to (C), (Cd, ... and by the assumption that ('Po, 'PI, ... ,'Pn) 
has convex range. These aggregative results are applicable here because of Lemma 
4 and will be used throughout the paper. Here is the first application:9 
LEMMA 5. If~o, ~1, ... ,~n satisfy Axiom 1, then (C) holds ifand only if there 
are real numbers aI, ... ,an, b such that V O = 2:~=1 ai Vi + b. (Cd [resp. (C+) 1 
holds if and only if this equation is satisfied for some choice of non-negative [resp. 
positive 1 numbers aI, ... ,an, 
Another consequence of Lemma 4 is the following tightening of the logical 
implications between unanimity conditions: 
LEMMA 6. If ~O, ~l, ... ,~n satisfy the assumptions of Proposition 1, and if 
(MAA) holds, then 
Thus, the list of conditions becomes simplified under (MAA). Returning now to 
the conclusion of Lemma 5, we know that it can be applied to the state-independent 
representations Ui . Hence, it seems as if this lemma delivered an aggregative 
rule of the familiar sort -
what social choice theorists call generalized utilitar-
ianism (e.g., [d' Aspremont, 1985]). A simple algebraic argument adapted from 
Mongin [1995, Section 4]) will demonstrate that this is not the case in general. 
Impossibility results lurk behind the apparently well-behaved affine decomposi-
tion UO = 2::'1 ai Ui + b. Dictatorial rules will emerge from the analysis of the 
weaker unanimity conditions (C), (C 1), (C2), while sheer logical impossibility 
will result from imposing the stronger conditions (C3 ) or (C+). 
Given a preference profile ~o, b, ... ,~n satisfying the assumptions of Pro po-
sition 1, hence representable by 
8 
8 
8 
9Lernma 5 is an encompassing version of a famous social aggregation theorem first stated by 
Harsanyi [1955]. 

THE PARADOX OF THE BAYESIAN EXPERTS 
319 
we say that i is a probability dictator if pO = pi, that i is a utility dictator if 
UO = ui (up to a PAT), and that i is an overall dictator if he is both a probability 
and a utility dictator. We define i to be an inverse utility dictator or an inverse 
overall dictator by changing the clause that UO = u i into uO = -ui . We shall 
also say that probability agreement prevails if pi = ... = pn and that pairwise 
utility dependence (p.u.d.) prevails if for all i, j ~ 1, ui = ui (up to an affine 
transformation of any sign). Probability agreement and p.u.d. are two degenerate 
cases of individual profiles; in general, both probabilities and utilities should be 
expected to vary from one individual to another. 
How to capture individual diversity in the language of formal choice theories 
is a difficult problem. As in Coulhon and Mongin [1989], or Mongin [1995], 
we shall use the convenient shortcut of defining diversity in terms of algebraic 
independence. Recall that a set of elements {'Pi,'" ,'Pk} of a vector space is 
affinely independent if for any set of real numbers ai, ... ,ak, b, 
ai 'Pi + ... + ak'Pk + b = 0 ::::} ai = ... = ak = b = O. 
This concept, rather than the weaker one of linear independence, provides the rel-
evant notion of algebraic independence in the case of utility functions. Plainly, 
affine and linear independence become equivalent in the case of probabilities. A 
relevant fact to report here is that a set of VNM functions u 1 , . .. ,un is affinel y in-
dependent if and only if these functions are "separated" from each other by suitable 
lotteries. This equivalence can be immediately extended to AA representations: 
LEMMA 7. Suppose that ui ,'" ,un are VNM utility functions on R. They are 
affinely independent if and only if for every i = 1, ... ,n, there are R!, R!. E R 
such that: 
Similarly, the Vi, ... , vn derived in Proposition 2 are affinely independent if 
and only if for every i = 1, ... ,n, there are f!, f!. such that: 
If affine independence assumptions formalize individual diversity in an obvious 
sense, it is also the case that in a VNM context, they imply some form of min-
imum agreement between individuals. This rather curious consequence deserves 
emphasis here since it means that in some algebraic contexts (MAA) and (MAC) 
are given for free: 10 
LEMMA 8. Suppose that u i , ... ,un are affinely independent VNM functions. 
Then, (MAC) holds. Similarly, if the Vi" .. ,vn of Proposition 2 are affinely 
independent, (MAA) holds. 
IOCompare with the related statements in [Weymark, 1993, Proposition 31 and [Mongin, 1995, Corol-
lary 4.31. 

320 
PHILIPPE MONGIN 
We are now in a position to state the two impossibility theorems which formal-
ize the paradox of the Bayesian experts. In part (*) of both Propositions 9 and 
10 we introduce a linear independence restriction on individual probabilities. To 
ensure that this restriction applies, we shall assume in part (*) that the state space 
S has cardinality at least n. 
PROPOSITION 9. Assume that ~o, ~I, ... ,~n satisfy Anscombe and Aumann's 
axioms of state-independent utility, i.e., the assumptions of Proposition 1. De-
noting by pI"" ,pn the probabilities and by uI ,'" ,un the utility functions on 
consequences provided by Proposition 1, assume that either: 
(*) pI , . .. ,pn are linearly independent, 
or 
( * *) U I , . .. ,un are affinely independent. 
Then, if ( C) holds, there is either a utility or an inverse utility dictator in case (*), 
and there is a probability dictator in case (**). There is an overall or an inverse 
overall dictator when both (*) and (**) apply. If either (Cd or (C2) holds, the 
same results follow, except that there is always a utility dictator in case (*). 
When there is an overall dictator, all of the unanimity conditions are obviously 
satisfied, so that we could have stated part of Proposition 9 in terms of "if and 
only if' conditions. 11 This observation also implies that the problem of Consistent 
Bayesian Aggregation does not involve any logical impossibility in the case of 
conditions (C), (Cd and (C2). The stronger conditions (C3) and (C+) lead to 
altogether different conclusions. 
PROPOSITION 10. The assumptions are as in Proposition 9. Then, if (C3) or 
(C+) holds, case (*) implies that pairwise utility dependence prevails and that 
there is a utility dictator; case (**) implies that probability agreement prevails 
and that there is a probability dictator. 
Notice that in both Proposition 9 and 10, (MAA) is an inference, not an as-
sumption. A modest strengthening of the first part of Proposition 10 would follow 
from assuming (MAC). Then, positive p.u.d. prevails (i.e., all individual utilities 
are identical up to a positive scale factor). 
Proposition 10 can be restated as follows: under the assumptions of Proposition 
9, (MAA) and (C3), if either the n probabilities are linearly independent and (at 
least) two utility functions are affinely independent, or the n utility functions are 
affinely independent and (at least) two probabilities are distinct, then there is no 
solution to the Bayesian experts problem. This wording makes it clear that under 
appropriate distinctiveness restrictions, (C3) is a logical impossibility; given these 
restrictions, even dictatorship fails to deliver a solution. 
11 Note also that inverse utility dictatorship is impossible when (C2) and (MAA) hold. Utility dicta-
torship and inverse utility dictatorship can coexist with each other under the weaker assumption (Cl), 
as the following shows: take n = 2, Uo = Ul and U2 = -Ul. 

THE PARADOX OF THE BAYESIAN EXPERTS 
321 
A word of comparison with the Savagean formulation of the paradox is in order. 
The main technical step in Mongin [1995] was to derive a version of Lemma 5. 
Since Savage does not assume anything on the consequence set, this had to be done 
by a special construction based on his divisibility-of-events axiom (P6). Once the 
affine decomposition of Lemma 5 is obtained, the algebra of impossibility results 
follows similar paths in the Savage and the Anscombe-Aumann variants.12 
4 THE STATE-DEPENDENT CASE 
Suppose that we just impose Axiom 1 on the preference relations t o, t 1 , . .. ,t n . 
This is the pure state-dependent case, as characterized by Proposition 2; each t i is 
represented by Vi(J) = ESES u!(J(s)). It is easy to check that nontrivial solu-
tions to the aggregation problem now exist, whatever individual preferences might 
be. To see that, take any profile t 1 , . .. ,t n that satisfies Axiom 1 and consider 
the added preference relation to defined by means of the following representation: 
n 
(+) 
VÂ°(J) = L ai(L u!(J(s)), 
i=l 
sES 
where ai > 0 for all i. Obviously, to satisfies the whole list of Pareto condi-
tions (C), .. " (C+). It is also clear that to satisfies Axiom 1 (since a sum of 
VNM functions is also VNM, and Axiom 1 does not require anything beyond that 
property). A little more explicitly, (+) can be rearranged as: 
(++) 
VÂ°(J) = 2: u~(J(s)), 
sES 
by defining u~ = E aiu! for all s E S. This rewriting makes it plain that to and 
the ti obey the same (weak) decision theory. 
Hence, in the pure state-dependent case, the paradox of the Bayesian experts 
vanishes. This mathematically trivial resolution can strike one as conceptually 
relevant only if one regards Axiom 1 as a sufficient foundation for Bayesianism. 
We have already suggested that this is not a sensible position to take. Without 
some restriction on the many subjective probabilities that are compatible with 
state-dependent utilities, Bayesianism vanishes at the same time as the paradox. 
Before we proceed to axiomatic systems of intermediary strength, we should 
complete the analysis of the first paragraph. What is not so trivial as the "reso-
lution" just sketched is the fact that equation (+) delivers a necessary solution to 
12A two-person version of Propositions 9 and 10 was obtained by [Seidenfeld et ai., 1989], using an 
expected utility framework in the style of Anscombe and Aumann. Schervish, Seidenfeld and Kadane 
[1991, Theorem 2] state this result more formally. We defer comparison to section 5. 

322 
PHILIPPE MONGIN 
the aggregation problem. This fact follows from Lemma 5 above, when condi-
tions (C), (Cd or (C+) hold, and from Lemmas 5 and 6 when (C2) and (C3) hold 
(assuming (MAAÂ». Let us take stock of the characterization just obtained: 
PROPOSITION 11. Assume that ~o, ~ 1 , ... , ~ n satisfy the unique assumption 
of Proposition 2, i.e., axiom 1, and that Vi(f) = l:,ES u!(f(s)), i = 0,1,Â·Â·Â· , n 
are the state-dependent representations derived in Proposition 2. Then, (C) holds 
if and only if there are real numbers al , ... , an, b such that: 
Similarly, (Cd [or (C2), if one assumes (MAA)) holds if and only if there are 
ai ~ 0, i = 1,Â·Â·Â· , n, and b such that this equation holds; and assuming (MAA), 
(C3) holds if and only if there are ai > 0, i = 1,Â·Â·Â· , n, and b such that the 
equation holds. 
The remainder of this section investigates a multi-agent application of the KSV 
approach. We shall assume that hypothetical probabilities q are used to determine 
the observer's and individuals' state-dependent utilities, following the procedure 
implicitly described in Proposition 3. More precisely, each of i = 0,1,Â·Â·Â· , n 
is endowed with a preference relation ~i, as well as an hypothetical preference 
relation E i, to be thought of here as i's preference over acts conditionally on some 
given, strictly positive qi. We know from section 2 that if ~ i and E 
i conform to 
axioms 1, 2' and 4, for i = 0, 1, ... , n, there are VNM functions vi , ... , v~ on n 
and subjective probabilities pi on 8 such that: 
(i) 
f ~i g iff l:sESP!v!(f(s)) ~ l:sESP!V!(g(s)) 
(ii) 
f'Eig' iff l:sESq!v!(f'(s)) ~ l:,ESq!V!(g'(s)). 
These equivalences and the accompanying uniqueness properties will lead to the 
negative results below. We shall make full use of the flexibility implied by the 
KSV approach, and take the auxiliary probabilities qi to be sometimes identical, 
sometimes different from one individual to another. The upshot of this analysis 
is that if there is sufficient diversity among the individuals' state-dependent utility 
functions, a variant of the earlier probability dictatorship and probability agree-
ment theorems holds. Correspondingly, a variant of the earlier utility dictatorship 
and dependence theorem holds, but as will be explained, the symmetry between 
probability and utility breaks down in the state-dependent case. 
To state these negative results, some further terminology is required. For any 
8' C 8,8' # Â¢, we shall say that i is a probability dictator for S' if either 
pO(S') = pi(S') = 0, or pO(S') ::j:. 0 ::j:. pi(S') and for all s E S', 

THE PARADOX OF THE BAYESIAN EXPERTS 
323 
and that probability agreement prevails on S' if for all i,j 
1"" ,n, either 
pi(S') = pi(S') = 0, or pi(S') t- 0 t- pi(S') and for all s E S', pi(sIS') = 
pi(sIS'). Similarly, we shall say that i is a utility dictator on S' if for all s E 
S', v~ = v!Â· (up to a PAT (which might depend on the particular s); and that 
pairwise utility dependence (p.u.d.) prevails on S' if for all s E S' and for all 
i,j = 1"" ,n,v! = Â±v~, up to PATs (which might depend on s). 
The exposition of impossibility results in this section does not follow the order 
of last section. We first analyze the probabilistic variant of paradox, and then move 
to its variant in terms of utility functions. 
o 
-0 
PROPOSITION 12. Assume that t ,'" ,tn and (for some common q) t ,'" , 
En satisfy Axioms 1, 2' and 4. Denote by pI, ... ,pn the individuals' subjective 
probabilities, and by vi, ... ,v}"" ,vf,'" ,v'T the individuals' state-dependent 
utilities, which are provided by Proposition 3. Assume that (C) applies to both 
sets of preferences. Then, if S' is some nonempty subset of S such that for all 
s E S', v;, ... ,v~ are affinely independent, there is a probability dictator on S'. 
If (C) is replaced by either (CI ), or (C2) together with (MAA), the same results 
hold; if (C) is replaced with (C3) and (MAA), probability agreement prevails, and 
there is a probability dictator, on S'. 
As a particular application of Proposition 12, take S' to be the whole subset of 
those states which are non-null for at least one i = 1"" ,n. Then, depending 
on the Pareto conditions, either the dictator imposes his absolute probability, or 
absolute probability agreement prevails, exactly as in the state-independent case. 
In order to obtain this conclusion, one should resort to the strong assumption that 
for every relevant state s, the v; , ... ,v~ are affinely independent. As explained in 
section 3, the significance of this assumption can be appreciated using its equiv-
alent reformulation: for every relevant state, and every individual i, there are lot-
teries R!, R!. that "separate" v! from the others' utilities vf One would hesitate 
to impose such a strong assumption uniformly across states. To take an example 
in the style of Savage's, suppose that s' is good weather and s" bad weather, and 
that individuals i and j have the following preferences: when s' prevails, i -
the adventurous vacationer -
prefers rockclimbing to canooing and is indifferent 
between going to a picnic or taking a swim, while j - the quiet vacationer - is in-
different between the first two lotteries but strictly prefers one of the last two to the 
other; when s" prevails, both i and j are indifferent between the four lotteries. Or, 
to take an economic example, suppose that final outcomes are money amounts and 
that in some states, widely different amounts are available, whereas in others, only 
trivial increments around a given money amount are.!3 The "separation" property 
might well be satisfied in the former case but fail in the latter (since this case might 
be formalized in terms of linear, hence identical utility functions for money). This 
discussion suggests that the case in which S' is maximal might be irrelevant. It 
13 Admittedly, this example does not quite fit in the formalism of this paper since it involves not only 
state-dependent utilities but also state-dependent consequences. 

324 
PHILIPPE MONGIN 
explains why we chose to emphasize local (i.e., event-relative) properties as in 
Proposition 12. 
The next proposition deals with a utility variant of the paradox. It is concerned 
with the special case of an admixture of state-dependence and state-independence. 
To deal with this case appropriately, we determine the KSV procedure beyond 
what was done by these authors. Suppose that there is a subset S' of states -
all of which we take to be non-null- having the following property: conditional 
preferences on constant acts do not vary across states in S, whereas they vary 
across any two s E S, t ~ S'. Thus, as far as S' is concerned, event-, rather 
than state-dependence, prevails. Restricting attention to acts taking some fixed 
value on each t ~ S', it can be seen that the standard Anscombe-Aumann theorem 
(Proposition 1) applies. Thus, using the AA representation, we have a probability 
7f on S'. The assumptions underlying the KSV procedure in Proposition 3 do not 
ensure that the conditional of the derived probability p on S' will coincide with 7f. 
Since 7f can be revealed by standard betting techniques, it seems natural to require 
that the two probabilities be equal. The way of obtaining this result while applying 
the KSV procedure is to impose that the conditional of the hypothetical probability 
q on S' be equal to the (independently revealed) 7f. 
Formally (in the notation of section 2): 
ASSUMPTION 13. Suppose that there is S' C S, IS'I 2: 2 such that every s E S' 
is non-null, and for every pair of constant acts j, 9 E H: 
'<Is, t E S', j ts 9 iff j tt g. 
Then, we require q in the KSV system to satisfy: 
, q(s) 
'<Is E S 'q(S') = 7f(s), 
where 7f is the probability on S' derived by applying the assumptions of Proposi-
tion 1 to the restriction of t to those acts in H which take some fixed set of values 
onS\S'. 
Now, we are ready for the last variant of the paradox. 
PROPOSITION 14. Assume that to, ... ,tn and f 0 , ... ,f n obey the KSV sys-
tem, i.e., they satisfy Axioms 1, 2' and 4. Suppose that there is S' C S,IS'I 2: 2 
such that: 
for all i = 0, 1, ... ,n, every s E S' is non-null; 
(*) 
for all i = 0,1, ... ,n, all s, t E S', 
(**) 
and all pairs of constant acts j, 9 E H, j t! iff j d g. 
Denote by pO, ... ,pn the subjective probabilities given by Proposition 3 for some 

THE PARADOX OF THE BAYESIAN EXPERTS 
325 
set of hypothetical probabilities satisfying Assumption 13, and suppose that: 
(P!)SES1 , â¢â¢â¢ ,(P~)SESI are linearly independent vectors. 
Then, if ( e) applies to both sets of preferences, there is a utility dictator on S'. 
If (C) is replaced with either (CI ) or (C2), the same result holds. If (C3) is used 
instead, positive pairwise utility dependence prevails, and there is a utility dictator, 
on S'.14 
Supposing that for all i and s, s is non-null, Proposition 14 can be applied to 
S' = S as a particular case, but this would be an uninteresting application. The 
conclusions would just repeat the utility-relative impossibility results already ob-
tained in Propositions 9 and 10. The point of Proposition 14 is to extend these re-
sults slightly by emphasizing event-relative properties. Condition (**) is a limited 
state-independence assumption. It is compatible with a generally state-dependent 
framework. Notice also that it does not involve any uniformity from one i to an-
other beyond the mere fact that the preferences of each just depend on S' (rather 
than on the particular state in S'). Going back to one of the previous examples, 
take S' = {excellent weather, fair weather}. It can no doubt happen that the two 
vacationers' and the observer's preferences are non-trivial and uniform across S', 
as required by (*) and (**), respectively. As far as condition (***) is concerned, 
it is perhaps no more problematic here than it was in the state-independent frame-
work. Take again the vacationers' example: for (***) to be met, it is enough that 
they entertain different probabilities of the weather turning fair or excellent. No-
tice however that there is a rough trade-off in plausibility between (**) and (***): 
the smaller S' is, the more plausible is (**) but the less plausible (***). 
5 
THE TWO-INDIVIDUAL CASE 
In the two-individual case, the impossibility conclusions of sections 3 and 4 can 
be sharpened, as the following corollaries show. 
COROLLARY 15. Assume that to, tl, t 2 satisfy Anscombe and Aumann's ax-
ioms of state-independent utility. Assume that pI f p2 and that u l , u 2 are not 
identical up to an affine transformation. Then, (C) holds if and only if there is an 
overall or an inverse overall dictator; (el ) or (e2) holds if and only if there is an 
overall dictator; and it is impossible for either (e3 ) or (c+ ) to hold. 
Â° 1 
2 
-0 -1 -2 
COROLLARY 16. Assume that t ,t ,t and t ,t ,t satisfy Kami, Schmei-
dler and Vind's system of state-dependent utility as restated in section 2. If (e), 
(el), or (MAA) and (e2) hold, then for each state s E S, either pO(s) = pl(S) 
or pO(s) = p2(s) or VO(s,Â·) = v1(s,Â·) = v2(s,Â·) up to affine transformations. 
1f(MAA) and (e3) hold,for each state s E S, either pO(s) = pl(S) = p2(S) or 
VO (s, .) = VI (s, .) = v2 (s, .) up to affine transformations. 
14We included Axiom 2' among the assumptions just for clarity, since condition (*) makes it redun-
dant. 

326 
PHILIPPE MONGIN 
These two corollaries are closely related to the results of Schervish, Seidenfeld 
and Kadane [1991, Theorems 2 and 4], who formalize a version of the Bayesian 
experts paradox in the two-individual case by assuming an AA framework of first 
state-independent, and then state-dependent theory. In the state-dependent case, 
they use a special variant of the KSV procedure. 15 Like the main theorem of their 
paper, our Corollary 16 comes close to predicting that under relevant assumptions, 
for each state s, either probability agreement or utility-dependence prevails on {s }. 
The difference between this wording and their formal statement appears to come 
mostly from the complication of the null states in the variant they adopt. 
It should be clear that the analysis of "consistent Bayesian aggregation" cannot 
be pursued just in the two-individual case. The conclusion corresponding to Corol-
lary 15 loses its elegant simplicity when n 2: 3. The mild requirements that pI, p2 
should be distinct and that u l , u2 should not be essentially identical or opposite 
functions become the more technical, less interpretable restrictions that pI , ... ,pn 
are linearly independent, and that u l , ... ,un are affinely independent. Earlier ex-
amples demonstrate that in the state-independent framework, nontrivial solutions 
to the aggregation problem emerge in the absence of suitable independence as-
sumptions. 16 As far as the state-dependent framework is concerned, Corollary 16 
appears to derive a quasi-impossibility theorem without making technical restric-
tions. Again, the simplicity of this conclusion disappears when n 2: 3. We shall 
give a three-individual example to illustrate how easily nontrivial solutions to the 
aggregation problem might emerge from the state-dependent case, when algebraic 
independence restrictions are omitted. 
Take X = {Xl, X2, X3}, so that Ll (X) is Sj, i.e., the unit simplex of R3. Denote 
the elements ofR = ~(X) as R = (Rl' R2 , R3)' In the notation used throughout, 
(C) implies that for all R E R: 
( i) 
s 
s 
and (considering now hypothetical instead of actual preferences): 
( ii) 
s 
s 
It is easy to find specific values such that the KSV assumptions and (C) hold, but 
for some S' c S, neither probability dictatorship nor any form of utility dictator-
ship holds. Take: 
15 See their other paper [Schervish et at., 1990] for a statement of this variant. 
16See [Goodman, 1988] and [Mongin, 1995, Example 41. We have belatedly heard of Goodman's 
contribution to the n-person analysis. Thanks are due to Teddy Seidenfeld for bringing this and other 
references to our attention. 

THE PARADOX OF THE BAYESIAN EXPERTS 
327 
q 
u!! 
R1,u!2 = R3,u!3 = 2R2 +3R3, 
u;! 
Rl + 3R2,u;2 = R1,u;3 = R2, 
U;! 
R2,u;2 = R3 + 2R1,u;3 = R3, 
u~! 
2Rl + 4R2, U~2 = 2R3 + 3R1, U~3 = 3R2 + 4R3Â· 
The vectors pO, pl , p2 , p3 can be thought of as KSV probabilities. In the par-
ticular instance, the common hypothetical probability q is taken to be equal to the 
observer's. To see that the above data agree with (C), notice that equations (i) and 
(ii) hold with: 
4 
8 
al = 3' a2 = 2, a3 = 3' b1 = b2 = b3 = 1. 
In contradistinction to the impossibility result stated in Proposition 12, probability 
dictatorship does not hold on S' = S. This fact can be traced to the failure of 
only one assumption in Proposition 12 -
i.e., affine independence. Indeed, the 
individuals' state-dependent utilities are linearly dependent in each state. 
Notice that no form of utility dictatorship prevails either. This frustrates the 
hope of extending the impossibility conclusion of Corollary 16 without adding 
suitable technical assumptions. 
We close the discussion of Corollary 16 by noting that it is a consequence of 
Proposition 12 alone. That is to say, the probability-relative impossibility result 
implies a restriction on the observer's utility whenever there are only two individ-
uals. This convenient property is lost in the general case. 
6 FINAL COMMENTS: THE EX POST SOLUTION TO THE PARADOX 
The present paper has offered a comprehensive treatment of the paradox of the 
Bayesian experts within the framework of the Anscombe-Aumann approach, first 
by assuming complete state-independence of utility, second by considering the op-
posite case of complete state-dependence, and third by applying the "intermediary" 
system of Karni, Schmeidler and Vind [1983] in which utility is state-dependent 
but the subjective probability is shown to be unique. Propositions 9 and 10 state 
the paradox in its pure form. They are the AA counterparts of the impossibility 
theorems recently proved within a state-independent, Savagean framework [Mon-
gin, 1995, Propositions 5 and 7]. Proposition 11 states an easy, but unimpressive 
possibility result for the pure state-dependent case. By assuming the more infor-
mative KSV framework, Propositions 12 and 14 reinstate the paradox, although in 
a significantly different variant from the initial one. 

328 
PHILIPPE MONGIN 
One might perhaps have conjectured that the paradox would reappear in essen-
tially its original form, once the state-dependence assumption is compounded with 
a procedure to determine subjective probabilities uniquely. This conjecture fails. 
The state-by-state analysis uncovers novel and curious situations: combining the 
assumptions of Propositions 12 and 14 on a sufficiently large state set, one might 
end up with juxtaposing probability dictatorship or agreement on some events with 
utility dictatorship or dependence on other events, and a nondescript state of af-
fairs elsewhere. More generally, if "consistent Bayesian aggregation" leads to any 
paradoxical consequences in the state-dependent framework, these are bound to 
be state- or at least event-relative. This is the most obvious difference between 
the negative conclusions delivered by Propositions 9 and 10, on the one hand, and 
Propositions 12 and 14, on the other. 
For more than two experts, technical conditions must be employed to derive 
dictatorship or uniformity conclusions. We argued that these conditions would be 
too stringent if they were to apply to each and every subset of the state set. This 
is why we selected local formulations of both our technical conditions and im-
possibility results. Accordingly, it might be argued that the latter are not really 
impossibility results, i.e., that the paradox of the Bayesian experts has not been 
reproduced in the state-dependent framework of this paper. This would be an ex-
aggerate conclusion. It would be tantamount to abstracting from the important 
differences between a completely unconstrained aggregative rule (Proposition 11) 
and a relatively constrained one (Propositions 12 and 14). The correct interpreta-
tion probably lies half-way between the initial expectation that any sophisticated 
theory of state-dependent utility would reinstate the original paradox, and the ex-
treme view now under discussion. 
Given that we cannot conclude that state-dependent utility theory is the way of 
escape from the logical difficulties of collective Bayesianism, a more radical alter-
native must be sought. Within the province of decision theory at large, it remains 
to investigate suitable relaxations of the sure-thing principle. Within the confines 
of the present paper, which is restricted to Bayesianism, the remaining logical 
possibilities are to relax either Paretian consistency or Bayesian consistency. The 
former solution is illustrated in the field of welfare economics by those writers who 
reject the ex ante formulation of the Pareto principle (i.e., the version which was 
investigated in this paper), while retaining an ex post (i.e., consequence-relative) 
versionY By contrast, the latter solution consists in denying that the aggregate 
should inherit the individuals' method of decision. 
We should like to indicate a (highly qualified) preference for the former over 
the latter direction of analysis. Even more clearly than some earlier and formally 
similar cases in welfare economics, the Bayesian experts problem implies that 
Bayesian consistency should be taken seriously. In the present setting the aggre-
gate does not refer to a collective entity, but to some person acting as an observer. 
17 A leading exponent is Hammond [1982; 1983l. Among the recent applications of the ex post point 
of view, see in particular Zhou's [J 996l axiomatization of Bayesian utilitarianism. 

THE PARADOX OF THE BAYESIAN EXPERTS 
329 
To go back to the example of section 1, the aggregate represents the insurer who 
attempts to summarize the experts' opinions. Given the nature of the observer 
in this problem situation, it seems natural to subject his preferences to the same 
choice-theoretic constraints as those prevailing on the (other) individuals' prefer-
ences. IS 
Here is a scenario which is compatible with the relaxation of Paretian consis-
tency, whereas Bayesianism is preserved throughout. If, contrary to our initial 
assumption, the insurer understands Bayesian elicitation methods, he will be able 
to estimate the experts' underlying probability and utility functions. In the state-
dependent case, we should then assume that he himself, rather than the experi-
menter, applies the KSV procedure. Once in possession of individual probability 
and utility data, he will process them separately to construct a summary probabil-
ity and a summary vector of state-dependent utilities. These two items can then 
be combined unproblematically in the way prescribed by SEU theory. The Pareto 
principle will be used in the construction of the summary utility vector, but not 
necessarily in the construction of the summary probability. When applied to the 
individuals' utilities in each state, it functions as an ex post unanimity principle. 
The previous paragraph shows that there is one (actually, well-known) way of 
keeping Bayesian consistency intact while preserving some form of Paretianism. 
We have just rephrased in terms of our decision-theoretic example the aggregation 
procedure which has long been recommended by the ex post school of welfare 
economics. In doing so, we have emphasized that there are definitive cognitive 
assumptions underlying the ex post approach -
a point which is rarely mentioned 
in welfare theory. Before arguing that the ex post method is a feasible solution to 
the paradox of the Bayesian experts, one should check whether these assumptions 
apply. A definitive advantage of the ex ante method examined throughout this 
paper is that it does not require much knowledge on the observer's part. 
To claim that the ex post approach provides not only a feasible, but a good 
resolution, a closer examination of the Pareto principle is needed. Implicitly, 
the defence of the principle trades on a distinction between factual and norma-
tive considerations. The essence of Paretianism is to proclaim that individuals are 
sovereign in normative matters; this means that their judgments in these matters 
should never be scrutinized or criticized, but taken for granted. "Normative" here 
can be diversely understood by reference to values, objectives, or even tastes, as in 
the "consumer sovereignty" doctrine. These interpretations would correspond to 
particular statements and defences of the Pareto principle. It is not our task here 
to list and compare them. The crucial point is that the individuals' sovereignty can 
be, and has been, argued for in the context of various notions of "normative" judg-
ments, while there is no concept of factual judgment for which this principle makes 
sense. Factual judgements should be scrutinized and criticized. Theoretically at 
18There is a modelling alternative which would make it even clearer that the observer here is just 
another individual. One could possibly endow him with two binary relations. one of which would 
represent his preferences qua ordinary person. the other his preferences qua observer. The insurer 
would then include his own private opinions among those which he tries to amalgamate. 

330 
PHILIPPE MONGIN 
least, they are susceptible of ascertainable truth values; they can be justified or dis-
missed by logic and evidence. So the Pareto conditions can only hold of a special 
class of unanimous judgments. Once all this is made clear, it seems as if the ex 
post variant is automatically warranted, and the ex ante variant automatically re-
jected. As it were, the former reaps all the benefits of the normative versus factual 
distinction. 
Let us first clarify the negative part of the argument. Assuming a standard 
Bayesian framework, comparisons of prospects normally depend on both how the 
agent assesses the values of consequences and how he estimates the likelihood of 
events. Stochastic dominance, in which only the values of consequences matter, 
is an exceptional case. Thus, the scope of the ex ante Pareto principle exceeds the 
province of normative judgments. This, in itself, would not make it invalid, just 
dubious. What makes it invalid is that the excess content of the ex ante principle-
its encroachment upon the province of factual judgments -
leads to spurious rec-
ommendations. Here is one: under any state-independent variant of SEU theory, 
whenever all individuals agree on the strict ranking of two particular consequences, 
the principle implies that unanimous probability judgments should be respected, 
regardless of the evidence available to each individual. This is a spurious recom-
mendation: evidence should matter to the observer. It can be shown that when 
conditioning partitions differ from one individual to another, a Bayesian observer 
who knows what these partitions are will sometimes violate the probabilistic form 
of the Pareto principle. 19 
Now, consider the positive argument in favour of the ex post Pareto principle. It 
says that the latter is justified because it involves only normative judgments. But in 
real life, judgments about consequences are infected with factual considerations. 
A hole in the ozone layer strikes one as an undesirable consequence because of 
certain scientific facts and laws. To own a large fortune becomes less desirable, 
or might even become absolutely undesirable, to somebody who knows that he 
will die tomorrow; and so on. All this suggests that the ex post principle could 
in turn fall a prey to the argument against the ex ante principle. By itself, the 
normative versus factual distinction does not provide the former with a sufficient 
foundation.20 
From the above discussion we might conclude that the factual versus normative 
distinction cuts both ways, and that the foundations of ex post reasoning are shaky. 
But they are at least solider than the foundations of ex ante reasoning, which -
this paper has attempted to demonstrate -
appears to be simply flawed. And in 
the absence of a third alternative,21 the ex post solution has at least the advantage 
of providing a feasible way out of the conundrum of collective Bayesianism. 
19We are indebted here to Ed Green and David Schrneidler. Probabilistic unanimity is discussed at 
grater length in Mongin [J 997]. 
20por a similar argument, see [Broome, 1990]. 
21 A paper by Levi [J 9901 appears to sketch a third alternative by defining restrictions on the ex ante 
principle. This is an interesting avenue to explore. 

THE PARADOX OF THE BAYESIAN EXPERTS 
331 
APPENDIX 
A PROOFS 
Proof [of Lemma 4] Proposition 2 implies that for i = 0,1" .. ,n, Vi preserves 
convex combinations of acts. Hence the vector (VO, VI, ... , vn) has convex 
range. 
â¢ 
Proof [of Lemma 5] See [De Meyer and Mongin, 1995, Proposition n 
â¢ 
Proof [of Lemma 6] See [De Meyer and Mongin, 1995, Propositions 1 and 2]. â¢ 
Proof [of Lemma 7] The former conclusion is proved in Coulhon and Mongin 
[Coulhon and Mongin, 1989]. In view of Axiom I, the latter is an immediate 
application of the former. 
â¢ 
Proof [of Lemma 8] Take R;, R;*, ... ,R~, R~* as in the statement of Lemma 7 
and construct the following elements of R: 
p* = (~R;, ... )R~) and p** = (~R;*, ... ,~R~.). 
n 
n 
n 
n 
Then, from the mixture-preserving property of u1 , ... ,un: 
ui{P.) > ui{pu), i = 1",' ,n, 
so that (MAC) holds. The case of affinely independent VI, . .. ,Vn can be dealt 
with similarly. 
â¢ 
Proof [of Proposition 9] Suppose that (C) and (*) hold. Lemma 5 implies that 
there are aI, ... ,an, b such that 
(AI) 
n 
UO = LaiUi + b. 
i=1 
One of the ai must be nonzero because of AA's nontriviality assumption (Axiom 
2). We may select any R E R and put uO(R) = u1(R) = ... = un(R) = 0; 
there is no assumption of substance in this normalization. Let us now consider the 
following class of f E H: there are s E Sand R E R, such that 
R 
{ f{S) 
f{t) 
R for all t -I- s. 
Applying AA's representation theorem in equation (1), we get: 
n 
Vf E H, 
Lp~uo(J{s)) 
L ai LP~ui(J{S)). 
sES 
i=1 
sES 

332 
PHILIPPE MONGIN 
When we restrict attention to the class of acts just defined, this becomes: 
n 
Vs E S, VR E n, 
p~uO(R) = ~ 
aip!ui(R). 
i=1 
From now on in the proof, we shall use functional notation. The last equation 
becomes: 
(A2) 
n 
pOUO = ~ 
aipiui, 
i=1 
where the functions on the right- and the left-hand sides are defined on S x n. 
Given that in the state-independent case, constant acts may be identified with 
consequences, equation (AI) also implies that: 
(A3) 
Replacing (A3) into (A2) we get: 
n 
(A4) 
~ 
aiui[p0 - pi] 
o. 
i=l 
If the po - pI, ... ,pO - pn were linearly independent, one would have that: 
aiui = 0 for all i = 1,,, . ,n, 
which is impossible since Axiom 2 implies that the ui are nonconstant and one 
ai must be nonzero. Hence, there is j E {I"" ,n} such that for some bl , ... , 
bj- I , bj+I' ... ,bn: 
(A5) 
po _ p3 = ~ 
bi(p0 _ pi). 
if.j 
Now, Li#j bi f:. 1 in view of (*). (Assume that Li#j bi = 1; then (AS) leads to 
the absurd equation pi = Lif.j bipi.) We can rewrite (AS) as: 
which provides a linear decomposition of po in terms of pI, . .. ,pn. Changing the 
notation, we have just derived: 

THE PARADOX OF THE BAYESIAN EXPERTS 
n 
(A6) 
pO = L Cipi, for some CI,Â·Â·Â· ,Cn such that L 
Ci = 1, 
i=l 
so that at least one Ci is positive. 
Now, replacing (A6) into (A2) leads to: 
n 
L(CiUO - aiui)pi = O. 
i=l 
Using (*) again, we conclude that for all i = 1, ... ,n, 
(A7) 
333 
One of the ai must be non-zero, and for any i = 1,Â·Â·Â· ,n, ai -::P 0 if and only 
if Ci -::P 0 (because uO, u l , ... ,un are nonconstant). Hence, there is a utility or 
inverse utility dictator, as was required to show. 
Consider the effect of assuming (Cd instead of (C), while still assuming that 
(*) holds. The argument just made remains available, since (CI ) trivially implies 
(C). But there is now a sign restriction on the ai (Lemma 5). This restriction, 
together with the fact that one Ci must be positive, implies that there is a utility 
dictator. 
To deal with (C2) in case (*) we first note that the latter property implies that: 
ul , ... ,un are affinely independent. 
(Suppose not, and consider the special class of acts at the beginning of the proof; 
then, for some j E {I,Â·Â·Â· ,n}, there are coefficients di , i -::P j, such that pi ui = 
Li#i dipiui, a contradiction.) Then, Lemma 8 says that (MAA) holds, and from 
Lemma 6 the results reached for (CI ) apply to (C2). 
When (C) and (**) hold, equations (AI) to (A4) remain unchanged. Then, the 
affine independence property of the ui implies that for all i, 
(A8) 
whence we conclude that there is a probability dictator. This conclusion still holds 
under either (Cd or (C2). The latter case is dealt with by noting that (**) also 
implies (***). 
â¢ 
Proof [of Proposition 10] Using Lemmas 6 and 8 as in the previous proof, we 
see that under either (*) or (**), (C3) becomes equivalent to (C+). Hence, from 
Lemma 5 there is an affine decomposition: 

334 
PHILIPPE MONGIN 
n 
UO 
La,Ui +b 
i=l 
with positive ai for i = 1"" ,n. Now, assuming that (*) is the case, we can 
reproduce the reasoning of the previous proof and conclude, as in (A 7) above, that 
for all i = 1, ... ,n, 
Since all of the ai are positive, and ai i- Â° 
if and only if Ci i- 0, we conclude 
that for i = 1" .. ,n, there are Cti i-Â° 
such that UO = Ctiui, and pairwise utility 
dependence prevails among UI, â¢.â¢ ,Un. Remember that one of the Ci must be 
positive; this implies that there is a utility dictator. To analyze case (**), we revert 
to equation (AS) in the proof above: 
ai(pO - pi) = 0, 
and now conclude that probability agreement prevails and that there is a probability 
dictator. 
.. 
Proof [of Proposition 12] Throughout the proof, we write vi(s, R) instead of 
v!(R) andpi(s) instead of p!, and assume that for some R* E R, 
vi(s, R*) = Â° 
for all s E S and all i = 0,1" .. ,n. 
(This normalization is permitted by the uniqueness part of Proposition 3.) 
From Proposition 3 we know that if (C) holds, ~i is represented by: 
Vi(f) = Lpi(s)vi(s,/(s)), i = 0,1"" ,n. 
sES 
Lemma 2 and the chosen normalization imply that: 
n 
VÂ°(f) = L ai Vi(f) 
i=l 
for some aI, ... ,an' By identifying the two expressions for VÂ°(f), and restricting 
them to those acts which have values R on sand R* elsewhere, we conclude that: 
n 
(A9) 
pO(s)vo(s, R) = L aipi(s)vi(s, R) Vs E S, VR E R 
i=l 
Repeating the argument for the auxiliary preferences EO,... , En and their 
functional representations leads to: 

THE PARADOX OF THE BAYESIAN EXPERTS 
335 
n 
q(s)VO(s, R) = L biq(s)Vi(S, R) 
"Is E 8, VR E R, 
i=l 
forsomeb1,'" ,bn . Sinceq(s) > Oforalls, 
n 
Â° 
"'. 
(AlO) 
v (s, R) = L...J biV~(S, R) 
"Is E 8, VR E R. 
i=l 
Replacing (AlO) into (A9) we have that: 
n 
(All) 
L[bipO(s) - aipi(s)]vi(s, R) = 0 "Is E 8, VR E R. 
i=l 
Now, consider 8' as in the first part of the Proposition. For any fixed s E 8', since 
the vi(s,Â·) are linearly independent, the equation in R: 
n 
(A12) 
L[biPO(s) - aipi(s)]vi(s,Â·) = 0 
i=l 
implies that: 
(A13) 
Consider the sets of indexes: 
I = {i=l,Â·Â·Â·,nlai7~O}andJ= {i=l,Â·Â·Â·,nlbi7~O}. 
From Axiom 2', as applied to ~o and ~o respectively, we know that I :f Â¢ :f J. 
Suppose that I n J = Â¢. Then, (A13) implies that: 
pO(s) = 0, and for at least onei E I,pi(S) = O. 
If we repeat the reasoning for s' E 8, s :f s', we find that pi (s') = 0 for the same 
i. Hence, in the case in which In J = Â¢, there is i such that po (8') = pi (8') = 0, 
a case of probability dictatorship. 
Now, consider the case in which I n J :f Â¢. There is i such that ai :f 0 :f bi, 
and: 
pO(s) = aibilpi(s), 
"Is E 8'. 
Either pi(8') = 0 = pO(8'), or pi(8') :f 0 and 

336 
PHILIPPE MONGIN 
which again shows that probability dictatorship prevails. The analysis of the other 
conditions than (C) makes use of Lemmas 6 and 7, as in the corresponding parts 
of the proofs of Propositions 10 and 11. Details are left for the reader. 
â¢ 
Proof [of Proposition 14] We first spell out the implications of Assumption 13 
for each KSV representation taken individually. Axiom 1 can be applied to the 
restriction of ~i to the set Hs' of acts having some fixed set of values outside 
S', and because of (*) and (**), Axiom 2 and a version of Axiom 3 hold for this 
preference relation (which we also denote by ~i). From Proposition 1 there is a 
state-independent function wi on n and a probability 7ri on S' such that: 
sES' 
sES' 
Now, the conclusions of Proposition 3 also apply to the restricted preference. Us-
ing Assumption 13, the (IS'I + I)-tuple (wi, ... ,wi, 7ri ) is seen to satisfy condi-
tions (i) and (ii) in Proposition 3, as applied to acts in Hs', so that by the unique-
ness part of this proposition: 
and: 
Hence, for i = 0,1, ... ,n, we may replace the initial vector of KSV represen-
tations relative to Hs' by (Wi, ... ,wi), and use (C) and condition (***) to prove 
impossibility results as if state-independence prevailed. The reader is referred to 
the relevant parts of the proofs of Propositions 9 and 10. 
â¢ 
Proof [of Corollary 15] Immediate from Propositions 9 and 10. 
â¢ 
Proof [of Corollary 16] If VI (8, .), v2 (8, .) are affinely independent, Proposition 12 
implies that either pO (8) = pI (8) or pO (8) = p2 (8) whenever (C), (CI), or (MAA) 
and (C2) hold, a conclusion which is strengthened into pO(8) = pl(8) = p2(8) 
whenever (MAA) and (C3) holds. If VI(8, .), V2(8,Â·) are affinely dependent, the 
conclusion that VO(8) = V1(8) = V2(8), up to relevant affine transformations, 
follows from inspecting equation (A2) in the proof of Proposition 12. 
â¢ 

THE PARADOX OF THE BAYESIAN EXPERTS 
337 
ACKNOWLEDGEMENTS 
Earlier versions of this paper were presented in 1995 at the Tokyo Center of Eco-
nomic Research Conference and the Economics Department, Copenhagen Univer-
sity; and in 1996, at the Economics Departments, Duke University and Princeton 
University. The author is grateful to the Centre for the Philosophy of the Natu-
ral and Social Sciences, The London School of Economics, and the Economics 
Department, Duke University, for hospitality when he was working on this paper. 
Special thanks are due to E. Green, P. Hammond, M. Kaneko, E. Karni, H. Moulin, 
D. Schmeidler, P. Wakker, 1. Weymark. 
Reprinted from Journal of Mathematical Economics, 29, Philippe Mongin, "The 
paradox of the Bayesian experts and state-dependent utility theory", pp. 331-361, 
copyright 1998, with permission from Elsevier Science. 
Laboratoire d' econometrie, Centre National de la Recherche Scientifique & Ecole 
Poly technique, Paris, France. 
BIBLIOGRAPHY 
[Anscombe and Aumann, 1963] Anscombe, EG. and R.I. Aumann, 1963, A definition of subjective 
probability, Annals of Mathematical Statistics 34, 199-205. 
[d' Aspremont, 1985] d' Aspremont, c., 1985, Axioms for social welfare orderings, in: L. Hurwicz, 
D. Schmeidler, H. Sonnenschein, eds., Social goals and social organization (Cambridge, C.U.P.) 
19-76. 
[Broome, 1990] Broome, 1.,1990, Bolker-Jeffrey expected utility theory and axiomatic utilitarianism, 
Review of Economic Studies 57, 477-502. 
[Coulhon and Mongin, 1989] Coulhon, T. and P. Mongin, 1989, Social choice theory in the case of 
von Neumann-Morgenstern utilities, Social Choice and Welfare 6,175-187. 
[de Finetti, 1974-75] de Finetti, B., 1974-75, Theory of probability (New York, Wiley, 2 volumes). 
[De Meyer and Mongin, 1995] De Meyer, B. and P. Mongin, 1995, A note on affine aggregation, 
Economics Letters 47,177-183. 
[Dreze, 1987] Dreze, 1.,1987, Essays on economic decisions under uncertainty (Cambridge, C.U.P.). 
[Fishburn, 1970] Fishburn, P.C., 1970, Utility theory for decision making (New York, Wiley). 
[Fishburn, 1982] Fishburn, P.c., 1982, The foundations of expected utility (Dordrecht, Reidel). 
[Goodman, 1988] Goodman, 1., 1988, Existence of compromises in simple group decisions, unpub-
lished Ph.D. Thesis (Carnegie-Mellon University). 
[Hammond,19821 Hammond, P.I., 1982, Ex-ante and ex-post welfare optimality under uncertainty, 
Economica 48, 235-250. 
[Hammond, 1983] Hammond, P.I., 1983, Ex-post optimality as a dynamically consistent objective for 
collective choice under uncertainty, in: P.K. Pattanaik and M. Salles, eds., Social choice and welfare 
(Amsterdam, North Holland). 
[Hammond,1998] Hammond, P.I., 1998, Subjective expected utility theory, in: S. Barbert!, P. Harn-
mond, and C. Seidl, Handbook of utility theory (Dordrecht, Kluwer). 
[Harsanyi, 1955] Harsanyi, 1.C., 1955, Cardinal welfare, individualistic ethics, and interpersonal com-
parisons of utility, Journal of Political Economy 63, 309-321. 
[Karni, 1985] Karni, E., 1985, Decision making under uncertainty: the case of state-dependent pref-
erences (Cambridge, Mass., Harvard University Press). 
[Karni,1993] Karni, E., 1993, A definition of subjective probabilities with state-dependent prefer-
ences, Econometrica 61, 187-198. 
[Karni and Schmeidler, 1993] Karni, E. and D. Schmeidler, 1993, On the uniqueness of subjective 
probabilities, Economic Theory 3, 267-277. 

338 
PHILIPPE MONGIN 
[Karni et al., 1983] Karni, E., D. Schmeidler and K. Vind, 1983, On state-dependent preferences and 
subjective probabilities, Econometrica 51, 1021-1031. 
[Levi,1990] Levi,l., 1990, Pareto unanimity and consensus, Journal of Philosophy 87. 
[Mongin, 1995] Mongin, P., 1995, Consistent bayesian aggregation, Journal of Economic Theory 66, 
313-351. 
[Mongin, 1997] Mongin, P., 1997, Spurious unanimity and the Pareto principle, paper presented at 
the conference "Utilitarianism Reconsidered" (New Orleans, March 1997). 
[Savage, 1972] Savage, L.J., 1972, The foundations of statistics, (New York, Dover, 1st edition, 1954). 
[Schervish et al., 1990] Schervish, M.J., T. Seidenfeld and lB. Kadane, 1990, State-dependent utili-
ties, Journal of the American Statistical Association 85, 840-847. 
[Schervish et al., 19911 Schervish, M.J., T. Seidenfeld and lB. Kadane, 1991, Shared preferences and 
state-dependent utilities, Management Science 37,1575-1589. 
[Seidenfeld etal., 1989] Seidenfeld, T., lB. Kadane and M.J. Schervish, 1989, On the shared prefer-
ences of two Bayesian decision makers, Journal of Philosophy 86,225-244. 
[Wakker, 1987] Wakker, P., 1987, Subjective probabilities for state-dependent continuous utility, 
Mathematical Social Sciences 14,289-298. 
[Weymark, 1993] Weymark, J.A., 1993, Harsanyi's social aggregation theorem and the weak Pareto 
principle, Social Choice and Welfare 10, 209-222. 
[Zhou,1996] Zhou, L., 1996, Bayesian utilitarianism, CORE Discussion Paper 9611 (Universit 
Catholique de Louvain). 

MAX ALBERT 
BAYESIAN LEARNING AND EXPECTATIONS 
FORMATION: ANYTHING GOES 
GREAT EXPECTATIONS 
When Muth [1961] introduced the rational-expectations hypothesis (REH), his ba-
sic idea was that agents form expectations by rationally acquiring and processing 
information (weak REH). From this, he immediately jumped to a stronger hypoth-
esis (strong REH), which in the following years radically transformed macroeco-
nomic theory and policy. The strong REH is implied by the assumption that agents 
know the true (statistical) model of their environment. Except for trivial cases, this 
model comprises a causal model relating endogenous to exogenous variables, and 
objective probability distributions of the exogenous variables. Agents' expecta-
tions are the objective probability distributions of future developments conditional 
on their current information about past realizations of exogenous and endogenous 
variables. In terms of the famous Knightian distinction, the strong REH implies 
risk and not uncertainty. Rationality then requires that agents choose a strategy 
(i.e., a plan specifying actions for all contingencies) that maximizes the expected 
utility on the basis of a v. Neumann-Morgenstern (NM) utility function. l 
However, the optimistic spirit of the "rational expectations revolution" [Begg, 
1982] has long since evaporated. Theoretical and empirical weaknesses of the 
strong REH have become apparent, and there seem to be good arguments for going 
back to the weak REH. Rationally acquiring and processing information without 
knowing the true statistical model of the environment (Le., under conditions of 
uncertainty rather than risk) means rational learning, which is the domain of the 
subjectively expected utility (SEU) theory a.k.a. Bayesianism. 2 
The simplest version of Bayesian learning requires the agent to proceed from 
a subjective joint probability distribution for all conceivable future observations. 
This distribution reflects personal degrees of belief. The optimal strategy max-
imizes subjectively expected utility. The initial subjective distribution, the so-
called prior distribution, is again revised by conditioning on observed events, 
which yields the so-called posterior distributions. The whole revision process, 
which is equivalent to the use of Bayes' theorem, is also called "updating the 
prior" because the posterior at one stage of the process serves as the prior on the 
next. 
1 Cf. Pearl [2000] for a discussion of causality in relation to statistics. Cf. Hacking [1990] on 
objective vs SUbjective probabilities. For strict subjectivists, the strong REH makes sense only for a 
group of agents, where it translates into the Common Prior Assumption (cf. Aumann [1987, 12ffJ. See 
also 2.1 below. 
2For critical discussions of the strong REH cf. Frydman and Phelps [1983] and Pesaran [1989]; for 
an overview of the learning literature cf. Kinnan and Salmon [1995]. 
341 
D. Corfield and J. Williamson (eds.), Foundations of Bayesianism, 341-362. 
Â© 2001 Kluwer Academic Publishers. 

342 
MAX ALBERT 
Bayesianism's claim to importance rests on the possible use of a two-stage 
procedure for deriving the prior. The agent first considers several models (or hy-
potheses or theories; we use these terms interchangeably) since the true model is 
unknown. Each model leads to different expectations. Then a prior over the set 
of models is chosen, which leads to a weighted average of the model-specific ex-
pectations. Updating the prior implies a shift in the weights of the models. This 
two-stage procedure connects Bayesianism with scientific procedures, leading to 
a unified theory of rationality in economics, statistics and practical decision mak-
ing.3 
In a Bayesian context, the strong REH is implied by the assumption that the 
agent's prior is degenerate and assigns probability 1 to the true model (true beliefs). 
Even without true beliefs, however, rational expectations are possible. Suppose 
that Adam the Agent tries to predict the outcome of once tossing a fair coin, and 
that he considers two hypotheses, namely, a probability of head equal to 0.25 or 
0.75, respectively. Adam has rational expectations if his prior assigns a probability 
of 0.5 to each of the two hypotheses, implying that he assigns zero probability 
to the truth. By definition, rational expectations only require that the subjective 
probability distribution of the observable variables implied by the prior coincides 
with the corresponding objective probability distribution [Pesaran, 1989; 1] 
Does Bayesian learning converge to rational expectations? Again, early opti-
mism turned into disappointment. Even the beliefs of an ideal Bayesian learner 
who does not dismiss the true model from the outset and who faces no costs of 
gathering further information are not inevitably bound to converge to rational ex-
pectations [Blume and Easley 1995: 16-20]. 
It is not clear, however, what lesson, if any, should be drawn from the possible 
failure of convergence. A non-convergent learning process is not necessarily an 
indication of misguided decision making [Kiefer and Nyarko, 1995]. The justifi-
cation of the Bayesian approach lies not in the convergence properties of Bayesian 
learning but in the appeal of certain axioms for preferences on the set of strategies. 
These axioms ensure the existence of an NM utility function and a prior such that 
the SEU of the strategies reflects the preference ordering.4 
Therefore, much of the discussion has centered on the Bayesian system of ax-
3The classical exposition of Bayesianism is Savage [1954]; see also Kiefer and Nyarko [1995] for 
a summary and defense emphasizing leaming and expectations formation. The approach of Anscombe 
and Aumann [1963] is appropriate if beliefs refer to objective probability distributions. For a single 
agent or under the Common Prior Assumption (see 2.1 below), weak and strong REH are formally 
identical, because the models can be treated like unobservable exogenous variables with a given known 
distribution (namely, the prior). 
4Preferences over strategies entail preferences over objective-probability distributions of outcomes 
(expressed by the NM utility function) and beliefs (expressed by the prior). It is fundamental to 
Bayesianism that preferences in the narrower sense and beliefs are separable [Binmore, 1993: 207; 
Aumann 1987: 13 n. 13]. This implies that an agent can adopt an NM utility function independently 
from her beliefs, or beliefs independently of her NM utility function. On the former case, see also 
Binmore [1993: 207] on Savage and "massaging the priors". The latter case is illustrated by Savage's 
own use of the sure-thing principle as a device for (implicitly) adjusting his evalutation of NM utilities 
in the Allais Paradox (where probabilities are given); cf. Pope [1991]. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
343 
ioms. Nevertheless, we postpone any comments on the axioms until the very end 
of the paper. Instead, we focus on the fact that Bayesianism is empty. As a positive 
theory, it implies no "operationally meaningful theorems" (OMTs), i.e., conse-
quences that could potentially be refuted by observations.5 Any behavior can be 
rationalized on the basis of some prior, even if the NM utility function is given. 
For this reason, Bayesianism is also empty as a normative theory. Assume 
that Mike the Manager asks Betty the Bayesian for advice. Betty cannot take 
Mike's current beliefs for granted because it is an open question which beliefs are 
rational given Mike's previous experiences. So it is natural for Mike to ask whether 
there are any OMTs: Given what I know about the past, and given my NM utility 
function, is there anything a rational person would not do? Is there a sequence 
of future choices, including reactions to new information, that can be classified as 
irrational? Since the answer is no, Bayesianism is empty as a normative theory. 
To a Bayesian, there is no such thing as irrational behavior. 
Although several such rationalizability results have appeared in recent years, 
they seem to be not widely known, and their implications are not yet fully appreci-
ated.6 The result discussed in the present paper assumes that an agent considers a 
simple chaotic process as explanation for observed phenomena. Such explanations 
are actually considered in economics and elsewhere; they cannot be dismissed as 
unreasonable. Chaotic systems have the typical features of very rich sets of hy-
potheses. Since perfectly rational agents by definition consider such very rich sets, 
they are necessarily, Le., independentlyfrom the actual complexity of their environ-
ment, in the situation of a person trying to predict a chaotic system. This implies 
that their expectations are completely arbitrary. Muth's [1961] conjecture that the 
weak REH provides a solution to the problem of expectations formation is thus 
refuted, at least if the weak REH is identified with Bayesianism, as it is usually 
done in economics. 
Section 2 reviews the literature and discusses some prima facie arguments 
against the view that Bayesinism is empty. Section 3 introduces an abstract de-
cision problem, section 4 a set of hypotheses based on a simple chaotic system. 
Section 5 shows that this set can be used to rationalize any strategy. Section 6 con-
cludes with a consideration of arguments against the position, taken in the present 
paper, that the emptiness of Bayesianism is a serious flaw. 
2 A FOLK THEOREM 
Presumably, practitioners tend to believe that there are objectively wrong decisions 
or mistakes and that decision theory provides the means to avoid them.7 Theoreti-
5Samuelson's [1947: 3] phrase, rather than "empirical content", is used to remind readers of the 
present paper's close relation to Samuelson's work on revealed preference (see 2.3 below). 
6The present paper is based on Albert [1996; 1999]. A slightly different result is contained in 
Nyarko [1997], who refers to an unpublished 1992 paper of J. S. Jordan for yet another version. 
7See, e.g., Bernstein [1996: 336]. Goldman [1999: 76] also seems to believe that the so-called 
Dutch Book argument demonstrates that Bayesianism protects against unnecessary losses. However, 

344 
MAX ALBERT 
cians think differently. It is the folk theorem of decision theory that the notion of 
rationality employed in economics is "weak".8 As its counterpart in game theory, 
the theorem is of unknown origin and implies that (almost) anything goes. Un-
til quite recently, there has been no general proof, but proofs for finite cases are 
trivial. 
There is a difference, however, between the claim that Bayesian rationality is 
"weak" and that it is completely empty. We therefore discuss three prima facie 
objections to the latter view. 1. It is sometimes suggested that prior beliefs of 
rational agents are not completely arbitrary. 2. Bayesians have always argued that 
their definition of rationality implies the rejection of certain other decision rules 
like the maximin rule; conflict, however, presupposes content. 3. Bayesianism 
encompasses the theory of demand, which is known to imply OMTs, the sCH:alled 
axioms of revealed preference. 
2.1 
Rational Priors 
Some Bayesians defend the view that, even though there are no restrictions on 
priors, all rational agents should hold the same subjective probabilities if they have 
been exposed to the same experience (e.g., [Aumann 1987: 7, 13f]). This view is 
known as the Common Prior Assumption (CPA). Aumann [1987] refers to Savage 
in this context (without giving a reference) and conjectures that Savage would have 
accepted the CPA. I disagree (cf. [Savage 1962: 11,13,14]). However, Savage was 
convinced that in practice experience often leads to convergence of opinion. But 
this is not a starting point for Bayesianism; it is a fact in need of explanation. For 
convergence, one needs priors that are not too different. The CPA just begs the 
question in assuming identical priors. 
The CPA makes sense only if there exist canonical or rational priors before any 
experience. This leads to the classical problem of whether there is an acceptable 
"principle of insufficient reason" determining probabilities before experience. This 
idea, going back to Laplace, has been criticized by many authors (cf. [Leamer 
1978: 11,22-39,61-63, 111, 114; Howson and Urbach 1989: 45-48,285,289; 
Earman 1992: 14-17, 138-141]). It had been revived by Keynes and others in 
the form of a theory of "logical" probabilities, i.e., uniquely determined a priori 
the argument assumes a situation without any uncertainty concerning gains or losses and, therefore, 
completely misses the point when used as a defense of Bayesianism. 
8Hahn [1996: 186] writes that rationality "buys only a small bit of an answer" in an intertemporal 
context since it has to be supplemented by a theory on agents' beliefs. Blume and Easley [1995: 
26] conclude that the content of Bayesian rationality mostly derives from restrictions on the set of 
beliefs. Bicchieri [1993: 14, esp. n. 9] restricts the predictive usefulness of Bayesian rationality to 
stable environments and choice situations familiar to the agent, and mentions convergence problems 
in the case of complicated priors. Arrow [1990: 29] writes that the rationality hypothesis by itself is 
"weak" and that its force derives from supplementary hypotheses. By varying utility functions for given 
beliefs, Ledyard [1986] demonstrates that Bayesianism is empty for a quite general game-theoretic 
setting. However, he is still convinced of its value as a normative theory [Ledyard 1986: 60, 80t]. Bray 
[1983: 123t] quotes Lucas to the effect that Bayesianism "in many applications" has "little empirical 
content" but defends it on account of its convergence properties. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
345 
probabilities of the logically possible hypotheses. One of the arguments in favor of 
Bayesianism has been the discovery that such probabilities do not exist. 9 It seems 
not to be a promising way of further development to revive this idea again. As the 
history of the subject presents itself, the burden of proof that there is an acceptable 
"principle of insufficient reason" rests with those in favor of the CPA. 
2.2 
The Dominance Principle 
Clashes between Bayesianism on the one hand and decision rules for behavior 
under uncertainty like the maximin rule on the other hand are due to the fact that 
the latter violate what we will call the dominance principle.1o We can use the 
NM utility function to define a set of strictly dominated strategies. A strategy A 
is strictly dominated if and only if for every choice of prior probabilities, there 
always exists at least one strategy with a higher SEU than A. Bayesianism implies 
one restriction on behavior for a given NM utility function, namely, that no strictly 
dominated strategy is chosen. Let us call this restriction the dominance principle. 
We illustrate this principle for a case of three strategies leading to different con-
sequences in two mutually exclusive and jointly exhaustive states (figure 1). Given 
an NM utility function, the three strategies can be represented by their utilities in a 
two-dimensional coordinate system. Bayesian analysis implies the linearity of the 
indifference loci in this diagram; the maximin criterion would lead to L-shaped 
indifference loci. Therefore, the latter criterion allows for choices that are ruled 
out by Bayesian analysis. 
The set of strictly dominated strategies is by definition independent of beliefs. 
Nevertheless, the dominance principle yields no OMTs. Identifying strictly domi-
nated strategies already requires at least some knowledge about one's environment. 
If the NM utility function is given, it is always possible to imagine, for each strat-
egy A, a state s such that A yields a higher utility than any other strategy if s holds. 
The assumption that a strategy is strictly dominated means that logical possibili-
ties are excluded. If Bayesianism is empty as a theory of learning, the dominance 
principle yields neither predictions nor advice, because before anything has been 
learned, logical possibilities cannot be excluded. 
2.3 The Revealed-Preference Approach 
In economics, it is usually taken for granted that economic theory only provides 
assumptions about the general structure of agents' preferences, and that the details 
9This appears already to have been a conjecture of Ramsey, the earliest of the modern Bayesians, 
who made this argument against Keynes; cf. Hacking [1990: 165,170]. The Keynesian program was 
taken up later by Camap; it was intended to provide one of the cornerstones of logical positivism. There 
is a widespread agreement today that this program foundered in just the way Ramsey conjectured: there 
are no logical probabilities; cf. Howson and Urbach [1989: 48-56]. 
iOThis clash has most often been stressed in connection with statistical decision theory, see, e.g., 
Lindley [1972: 13-15]. 

346 
MAX ALBERT 
~-------------------------.U2 
Figure 1. A strictly dominated strategy. The axes measure the NM utilities of three 
strategies in the case of two mutually exclusive and jointly exhaustive states. The 
dominated strategy A3 is selected by the maximin criterion. 
necessary for making predictions have to be gathered by observation. According 
to this view, economic theory provides a framework that allows us to use empir-
ical observations to make predictions, much as Newton's theory allows us to use 
empirical observations to determine the masses of the planets, which in tum can 
be used to predict the planets' movements. 
An important question in this context is which kind of observations can be used. 
The economic tradition allows only the use of observations about actual choices. 
This is consistent with the idea that-presumably as a result of selective pressure-
agents act as if they were perfectly rational, or at least as if deviations from per-
fectly rational behavior were unsystematic. The as-if approach views rationality 
as a feature of behavior, not as a feature of the process of deliberation. 11 
This attitude is the basis of the revealed-preference (RP) approach to the the-
ory of demand (cf., e.g., [Varian 1992: 133]). The approach focuses on OMTs-
usually called axioms ofRP since they exhaust the theory-implying that informa-
tion about past choices can be used to rule out certain future choices if preferences 
have a certain structure. 
The theory of demand implicitly assumes that observer and agent share true 
beliefs in a single deterministic model. Bayesianism can only profit from the re-
sults of this theory if the observer can check whether the agent holds appropri-
ate beliefs. According to the economic tradition, such a check must be based on 
observations of actual choices and nothing else. Hence, an extension of the RP 
llCf. Simon's [1976] characterization of economic rationality as "substantive" rather than "proceÂ· 
dural", which seems to be meant as a clarification of Friedman's [1953] as-if approach. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
347 
approach to the case of uncertainty is required. Such an extension would provide 
OMTs demonstrating how, assuming the axioms of Savage [1954] or Anscombe 
and Aumann [1963], information about past choices can be used to rule out certain 
future choices. 
There are some extensions of the RP approach, but none that cover Bayesian 
learning.12 The present paper addresses this issue in a very general way. As a 
concession to Bayesianism, knowledge of the NM utility function summarizing an 
agent's behavior under certainty and risk is granted. This leaves only beliefs as 
missing determinants of behavior. An RP approach to Bayesian learning requires 
OMTs implying that, given the NM utility function, information about past choices 
can be used to rule out certain future choices. 
3 QUESTIONS 
The technical aspects of our problem, and the way to a solution, can be explained 
with the help of a simple decision problem; generalizations are trivial (see 5.2 
below). 
Adam and Eve and the Money Spinner. Adam the Agent owns a mysterious black 
box connected with a screen and a keyboard. The screen displays either 0 or 1; in 
fixed intervals the screen goes black and then again shows one of the two digits. 
The tth observation is denoted by Xt. At every point in time t = 0,1, ... ,00, 
Adam places his bets on the next digit by typing in a number Yt E {O, 1, ... , N}. 
Doing nothing implies Yt = O. So the sequence of events is as follows. At t = 0, 
Adam chooses Yo. Att = I,XI appears and Adam chooses YI. And so on. At time 
t, Adam chooses Yt. If Xt+l = 0, the black box produces Yt perfect one-dollar 
notes. If Xt+1 = I, it produces N - Yt equally perfect one-dollar notes. Adam 
cares only for the money he receives; his NM utility function v: [0, N] t-+ lR at each 
point in time is increasing, strictly concave, and finite. Adam's information at time 
t encompasses all the facts just explained and the history of digits and choices up 
to time t. 
Eve the Economist observes Adam. Her information at time t coincides with 
Adam's; specifically, she knows his NM utility function. We simplify the problem 
by assuming that both Adam and Eve know (Le., rightly believe) that Adam's 
choices have no influence on the sequence of digits appearing on the screen. 
â¢ 
Let X be the set of all finite sequences or histories of observations (Os and Is). 
These sequences are of varying length; l(x) is the length of x E X. Similarly, let 
12Border [1992] develops an RP approach to choice among lotteries with monetary rewards. The 
observer knows only that more money is preferred to less. If observer and observed agree on all 
(objective) probabilities, any choice behavior that is not stochastically dominated can be rationalized 
by postulating a suitable utility function. The RP approach of Green and Osband [1991] is based on 
assumptions that deviate from Savage's [1954] framework in several ways. A direct comparison of 
results is therefore difficult. Kim [1992] considers choice under uncertainty but excludes learning, i.e., 
conditionalization on past observations. 

348 
MAX ALBERT 
Y be the set of all finite sequences of choices (natural numbers from 0 to N), with 
Â£ (y) = t as the length of y E Y. In both cases, we include the sequences of zero 
length ("vacuous" histories). The set XY denotes all pairs (x, y) from X x Y 
with Â£(x) = Â£(y). 
The following three questions define the problem we are interested in by re-
course to the situation of Adam and Eve. 
Question 1. Can Eve exclude some histories (x, y) E XY as inconsistent with 
the assumption that Adam is a perfectly rational Bayesian agent? 
Question 2. Can Eve, on the basis of finitely many observations x EX, give 
good advice to Adam from a Bayesian point of view? 
Question 3. Assume that Eve is a Bayesian-minded economist who has observed 
a sequence of digits and choices (x, y) E XY. Is there any restriction on Eve's be-
liefs concerning Adam's future behavior resulting from the hypothesis that Adam 
is a perfectly rational Bayesian agent? 
Question 1 concerns Bayesianism as a positive theory that should yield predic-
tions of Adam's behavior. Question 2 concerns Bayesianism as a normative theory 
that could be used by Eve to advise Adam. Question 3 concerns Bayesianism as a 
methodology used by Eve to analyze Adam's behavior. 
Of course, all three questions are strongly interrelated. If Adam can rationalize 
any choice of strategy, question I must be answered in the negative. The same 
goes for question 2. If any strategy can be rationalized, there is nothing a Bayesian 
advisor can say except "Do what you want". 
The answer to question 3 is slightly more involved. There is a difference be-
tween questions 1 and 3. A negative answer to question 1 implies that no OMTs 
exist. But a Bayesian could still claim that Bayesianism as a methodology allows 
one to conclude that certain sequences of digits and choices become very improba-
ble if Adam is rational. If that were possible, it would provide a Bayesian argument 
against the requirement that positive theories should provide OMTs. 
Let us shortly summarize the Bayesian analysis of Adam's problem. First of all, 
Adam should choose a set 1i of mutually exclusive hypotheses, each of which im-
plies objective probabilities for all potential future observations. Then, he should 
choose a subjective probability measure or prior J.L on (a IT-algebra of subsets of) 
1i. The prior J.L is chosen such that it generates a preference ordering on the set 
of all strategies. Hence, we know that, for every history x EX, the pair (1i, J.L) 
implies conditional probabilities PI' (Xl(x)+1 = i 11i 1\ x), i = 0,1. 13 
Adam knows that there is no influence from his choice at one point in time 
to consequences at other points in time. The only connection between choices is 
l3The symbol 1\ in 1Â£ 1\ x denotes the conjunction. Read as statements, 1Â£ is a (possibly uncount-
able) disjunction of hypotheses, and x is equivalent to the conjunction of statements "At time s, x. is 
observed", s = 1, ... , Â£(x). If Â£(x) = 0, 1Â£ 1\ x is of course equivalent to 1Â£. If Adam observes a 
sequence x with subjective probability 0, the conditional probabilities Pp. (Xl(x)+l = i 11Â£ 1\ x) are 
not defined. He is then free to choose a new prior distribution, which does not improve the chances of 
predicting his actions. However, we can exclude this case (see below). 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
349 
learning. Given a "forecast function" [Nyarko 1997: 181] p: X H [0,1] assigning 
the probability p = p(x) to the event Xl(x)+l = Â° 
(the next digit is 0), the action 
at t = f(x) can be considered separately from other actions. Adam maximizes his 
SEU, solving the problem 
(1) max{pv(Y)+(I-p)v(N-y):p=p(x), yE{O, ... ,N}}. 
y 
His optimal strategy, then, is described by the function y: X H {O, ... ,N} assign-
ing the utility-maximizing choice to each conceivable history: 
(2) 
y(x) ~f argmax {pv(y) + (1 - p)v(N - y):p = p(x), y E {O, ... , N}} 
y 
His actual choice Yl(x) of course depends on the actual history x. 
Bayesian rationality requires that Adam's forecast function reflects some prior: 
(3) 
3(Ji, J-L)Vx E X [p(x) = PI' (Xl(x)+l = Â° 
IJi 1\ x)] 
Ifthe strategy y(x) is optimal given the set of hypotheses Ji and the attached prior 
J-L, the pair (Ji, J-L) is a rationalization of y(x). 
The analysis is simplified by the fact that a strategy can be rationalized, if at 
all, without assuming any of the conditional probabilities to be 0, because Adam 
chooses between discrete values. Given discreteness, any decision that is optimal if 
some event has zero probability will also be optimal if the probability of the respec-
tive event is small enough. Of course, all policy variables in real-world decision 
problems are discrete since the precision of measurement is always finite. More-
over, each possible choice y in (1) is optimal for some values of p. If, therefore, 
we can find a rationalization for arbitrary forecast functions p: X H (0,1), where 
(0,1) is the open unit interval, we can rationalize any strategy y: X H {O, ... ,N}. 
4 
THE CHAOTIC CLOCK 
The problem in finding rationalizations is that a set of hypotheses might not be rich 
enough to provide a rationalization for a given strategy. However, there are very 
trivial sets of hypotheses that are always rich enough. 
4.1 
The Basic Mechanism 
Assume that the evolution of the inner states of Adam's black box follows a de-
terministic process depending on a starting point. The law of the deterministic 
process is the baker-map dynamics, which can be graphically illustrated as the 
output of a chaotic clock (figure 2). 
There is one pointer that can point to all real numbers in the intervall 1= [0,1), 
where the vertically upward position is zero and the vertically downward position 
is !. Initially, the pointer deviates by an angle w = 2(hr from the vertically upward 
position, thus pointing at the real number e. At t = 1,2, ... ,00, the pointer moves 

350 
MAX ALBERT 
o 
1/2 
Figure 2. A chaotic clock. At each point in time, the angle w is doubled. When the 
pointer is in the first (second) half of the dial, the digit 0 (1) appears on a screen. 
The resulting sequence of digits is described by the baker-map dynamics. 
by doubling the angle w. If the pointer comes to rest in the first half of the dial and 
points at a number in [0, ~), the screen of Adam's black box shows 0; otherwise 
it shows 1. 
According to the chaotic-clock hypothesis, the inner states of the black box at 
t = 0,1, ... ,00 are described by a real variable, the pointer position Zt E I. 
The inner states evolve deterministically, but it can only be observed whether the 
pointer position is Zt E [0, ~) or Zt E [~, 1). These two states result in Xt = 0 and 
Xt = 1, respectively. The deterministic law by itself does not allow for a prediction 
of future observations; an assumption concerning the starting point Zl = () is also 
necessary. Thus, there is a set of hypotheses, one for each starting point () E I. 
The corresponding dynamical system is the baker-map dynamics: 14 
(4) 
(a) 
(b) 
(c) 
Xt 
def 2zt div 1 
def 2zt mod 1 
Note that the chaotic clock cannot produce an unbroken infinite sequence of Is. 
If the pointer is in the second half of the dial (thus generating a 1 on the screen), 
doubling the angle w moves the pointer beyond 0, thus leading to a smaller value 
14"div" denotes integer division; "mod" denotes the indivisible rest of the integer division, i.e., 
x mod n ~f x - (x div n). On the baker-map dynamics, see Ford [1983], Devaney [1989: 18 
example 3.4,39,52] and Schuster [1988: 107f]. The graphical illustration is due to Davies [1987: ch. 
4]. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
351 
of w. As long as the pointer comes to rest in the second half of the dial, w falls at 
every tick of the clock with increasing rates, until, after a finite number of ticks, 
the pointer is in the first half of the dial, which implies that the screen shows O. 
4.2 
Falsification Dynamics 
Assume that Adam believes that his black box contains a chaotic clock. In order 
to analyze the consequences of uncertainty concerning (), Adam has to know how 
Xt develops for a given (). This is very simple in principle. Every () E I can be 
expressed as 
(5) 
where ()n is 0 or 1 (dyadic development). In order to enforce uniqueness of the 
representation (5), we require infinitely many Os on the right-hand side. Thus, ~ 
should be represented as 0.10 = 0.1 and not as 0.01 (where the bar denotes infinite 
repetition).15 
The sequence generated by the chaotic clock is just the dyadic development of 
the starting point, i.e., Xt is equal to ()t in (5). This follows from two mathematical 
facts. (i) We have () ~ ~ if and only if ()l = 1. (ii) Doubling () shifts the point 
in the dyadic development one step to the right; the modulo function sets the digit 
before the point to 0, thus ensuring that the new value is again smaller than 1. 
The first digit ()l in (5) determines whether () is smaller than ~ or not. Hence, 
when Adam observes the first digit Xl = ()l, he finds out whether the starting 
point () is in the first half (Xl = 0) or in the second half (Xl = 1) of the dial. The 
second digit places the starting point into one of the four quarters. For example, if 
Xl = 1 and X2 = 0, the starting point must be in the third quarter [~, ~). And so 
on. The sequence of digits corresponds to a sequence of bisections of the set I of 
potential starting points; with each further digit, the location of the starting point 
is narrowed down to the upper or lower half of the remaining interval on the dial. 
At time t, Adam has made t observations revealing one of the 2t basic intervals 
def [m m 
+ 1) 
{ 
t} 
(6) 
It{m) = 2t ' -2t-
, mE 0, ... ,2 - 1 
as the location of the starting point (). In infinite time, these bisection steps con-
verge to (). 
In other words, each observation falsifies half of the remaining hypotheses con-
cerning the starting point (). Hence, Adam's beliefs converge to the truth if he is 
right in assuming that the black box contains a chaotic clock. This does not mean, 
however, that his chances of predicting the future improve over time. Obviously, 
knowing the first t digits of ()'s dyadic development is no help in predicting the 
next digits. 
lSOn coin tossing and dyadic development see also Bremaud [1988: 28-31]. 

352 
MAX ALBERT 
4.3 Souping up the Clock 
The chaotic clock places one restriction on Adam's system of beliefs: it is impossi-
ble that the number of Os is finite. In order to get rid of this restriction, we assume 
that the angle between the pointer and the zero position is not doubled as before 
but quadrupled: 
(a) 
Xt = g(Zt) 
(7) 
(b) Zt+l 
h(2zt ) 
(c) 
Zl = () 
The functions g, h are defined as in (4). For any starting point () with dyadic 
development (5), the original system (4) yields the observations Xt = (}t. The 
modified system (7) yields Xt = (}2t-l, i.e., every second digit of the the starting 
point's dyadic development is irrelevant. Therefore, starting points like () = 0.10 
or () = 0.1011 generate an unbroken infinite number of Is although they feature 
infinitely many Os as required. 
Almost the same analysis as before applies to the modified chaotic clock. Since 
every second digit of the starting point's dyadic development never appears on the 
screen, an infinite number of starting points lead to the same sequence of obser-
vations. The first t observations determine 2t- 1 basic intervals of length 24~-2' 
one of which contains the starting point. Each basic interval is as rich as the unit 
interval. It still contains points that can produce any kind of extension to the ob-
served sequence. In other words, the set of hypotheses represented by the set of 
starting points is so rich that, for any possible future, there are (actually: infinitely 
many) hypotheses consistent with the past and predicting this future. No future 
developments can be excluded on the basis of past observations. 
Adam's beliefs still converge to the truth but not to the full truth: the observa-
tions reveal a subset of possible starting points containing the true one. This set 
corresponds to a subset of observationally equivalent hypotheses, one of which is 
true. As before, convergence to the truth presupposes that the black box actually 
contains a modified chaotic clock. If this is not the case, however, the analysis of 
the learning process is unaffected since no sequence of observations, not even an 
infinite one, is inconsistent with the assumed mechanism. 
However, Adam does not know whether the black box actually contains a chaotic 
clock. One may ask, then, why he should ever consider such a crazy mechanism. 
There are several reasons. 
First of all, Adam is a perfectly rational being and, therefore, logically om-
niscient (cf. [Earman 1992: 121f1). He is aware not only of the chaotic-clock 
hypotheses but of many more when he considers the question of choosing a prior. 
Therefore, the question should rather be why not more hypotheses are considered. 
Secondly, if the truth is deterministic, it is observationally equivalent to (7) 
(with a specific starting point). Thus, (7) can be viewed as a "reduced form", as 
econometricians would say, of any deterministic theory (plus initial conditions) 
that is capable of explaining an infinite sequence of the events in question. In this 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
353 
sense, the chaotic clock represents all deterministic theories, and Adam's problem 
does not become significantly greater when we add further hypotheses to the set 
of chaotic-clock hypotheses. 16 Hence, considering the chaotic clock is actually a 
simplification (although an insignificant one) of Adam's problem. 
Last but not least, the chaotic-clock model fulfills all the formal requirements 
of a scientific theory. It assumes a simple mechanism governed by a law of motion 
that produces different results according to the initial position of the mechanism. 
While not even the Swiss could actually produce the chaotic clock, processes that 
lead to chaotic dynamics are not rare, and imperfect observability can produce the 
kind of irregular behavior characteristic of the chaotic clock. Moreover, although 
the chaotic-clock model assumes a continuum of starting points (corresponding 
to more hypotheses than we could ever consider explicitly), it is less complicated 
than models encountered in physics or economics. It would be difficult to find 
any acceptable formal requirement that excludes the chaotic clock from consider-
ation. 17 
Let us call "empirically adequate" those hypotheses that are observationally 
equivalent to the truth. Subsequently, convergence to the truth is taken to mean 
that, in the limit, the subjective probability of the set of empirically adequate hy-
potheses is 1. If the truth is deterministic, then, convergence to the truth is ensured. 
5 ANSWERS 
5.1 Adam's Problem 
We turn to the dynamics of probabilistic beliefs generated by the modified chaotic-
clock hypotheses described by (7). The hypotheses form a set 1Â£*, where He E 
1Â£*, () E I denotes the single hypothesis that the starting point of the modified 
chaotic clock is (). Since these hypotheses are deterministic, the probabilities 
P{Xt = 01 He) are either 0 or 1, i.e., He yields certain or point predictions. Uncer-
tainty enters via the uncertainty concerning () E I. As a Bayesian, Adam chooses 
a subjective probability measure on (a a-algebra of subsets of) 1Â£* , and since each 
hypothesis in 1Â£* is represented by a () E I, we can consider instead a subjective 
probability measure on (a a-algebra of subsets of) I. 
Adam needs well-defined conditional probabilities P/L (Xl(x)+l = i 11Â£* 1\ x) 
for any potential sequence of observation x. Hence he must include all the basic 
intervals It{m) in his a-algebra of subsets of I. We can therefore consider the 0'-
algebra generated by the basic intervals (which is just the a-algebra of the Borel 
sets). Any probability measure I-" on this a-algebra then determines the conditional 
16Even adding probabilistic hypotheses would make no difference, as will become obvious below. 
17The chaotic clock poses a generalized version of Goodman's [1955] "new riddle of induction". The 
set of hypotheses considered by Goodman is countable and, therefore, too small to lead to the problems 
discussed in the present paper. Using the chaotic clock for presenting the problem of induction has the 
advantage that no "gruesome" predicates appear. 

354 
MAX ALBERT 
probabilities Adam needs to solve his decision problem. Henceforth, we simply 
speak of a probability measure JI. on lor, equivalently, on 11.* . 
We have already seen in section 3 that the central question is if we can find a ra-
tionalization (11.* , JI.) for arbitrary forecast functions p: X t-t (0, 1). This question 
is answered by the following theorem [Albert 1999: theorem 1]. 
Theorem (Anything Goes) Let 11.* be the set of modified chaotic-clock hypothe-
ses. Consider an arbitrary forecast function p: X t-t (0, 1). Then, there exist in-
finitely many probability measures JI. on 11.* such that the rationality condition 
p(x) = PI'(Xl(x)H = 0111.* AX) holds for all x E X. 
In interpreting the theorem, we have to remember that Adam as a perfectly 
rational person is always aware of the implications of all the assumptions he is 
considering. When choosing the prior JI. on 11.* , he is aware of the implicit assign-
ments of numerical values to the conditional probabilities. The theorem says that, 
instead of choosing a prior on 11.*, Adam may as well choose arbitrary conditional 
probabilities. IS And since these probabilities can generate any contingent choices 
whatsoever, it is immaterial for Adam whether he asks himself "What should I 
doT' or "What prior should I choose?". The Bayesian apparatus provides no re-
strictions and therefore no help in making this choice. 
Let us consider two simple cases. Adam might, for instance, choose a constant 
forecast function p(x) = ! for all X E X. This is rationalized by the uniform 
prior with density /(0) = 1 on I. This choice implies that, independently from 
past observations x, the probability of 0 and 1 is always !. Any other constant 
forecast function leads to a misleadingly complicated prior distribution that has no 
density. 19 
Generalizing slightly, Adam might set p(x) = p[l(x)] with an arbitrary func-
tion p: Nt-t (0, I), thus fixing his decision at time t independently from and prior 
to any observations. Adam poses as a Bayesian learner but is actually completely 
"dogmatic" (and completely unpredictable) in the sense of ignoring any experi-
ence. 
A Bayesian dogmatic is already unpredictable. A run-of-the-mill Bayesian, 
who allows experience to influence his behavior whenever it suits him, is all 
the more unpredictable because he has more options. This answers question 1. 
Bayesian rationality is empty as a positive theory. Eve cannot exclude any behav-
ior of Adam on the basis of the hypothesis that Adam is rational in the Bayesian 
sense. Nor can Eve give any advice to Adam, even if she knows his NM utility 
function, since no strategy is irrational, whatever observations have been made. 
This answers question 2: Bayesianism is empty as a normative theory. 
In relation to the tent-map dynamics (which is mathematically equivalent to 
(4Â», Blume and Easley [1995: 19f, 36f] show that convergence to the truth need 
not mean that predictions improve: if the prior is continuous at the starting point, 
18Thus, including probabilistic hypotheses in addition to 1Â£" would not change the results. 
19Cf. [Brernaud, 1988: 29] who, however, discusses only the uniform distribution. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
355 
the posterior distribution converges to the uniform distribution, implying a prob-
ability of 0.5 for observing O. Hence, convergence to the truth does not imply 
convergence to rational expectations.20 
This is an interesting point but not the one we are making. Blume and Easley's 
point provides no argument against Bayesianism (and is not meant to do so), be-
cause no procedure can improve on Bayesian learning in these cases. We can get 
more mileage out of the machinery of chaotic dynamics. 
In Blume and Easley's analysis, the true law of the process generating the Os 
and Is is chaotic, and this law is known to the agent. In our analysis, the true law 
governing the sequence of Os and Is is unimportant. The anything-goes theorem 
neither refers to some true law nor places restrictions on the sequence of events 
observed by the decision maker. The problem is not the complexity of the environ-
ment but the complexity of a large set of hypotheses. The chaos is in the decision 
maker's head. 
The assumptions of the present analysis are actually quite favorable to Bayesian-
ism since convergence to the truth (in the empirical-adequacy sense) is ensured. 
Allowing for probabilistic hypotheses would open up the possibility of non-
convergence to the truth.21 As Gillies [200 I] shows, a Bayesian learner might then 
never discover that he had dismissed the truth from the outset, which provides a 
strong argument in favor of seriously considering a large set of hypotheses.22 
5.2 Eve's Problem 
We tum to Eve's prediction problem, i.e., to the problem of an economist trying to 
predict the behavior of rational agents and using Bayesianism as a methodology. If 
Eve is a Bayesian herself, she is not overly impressed by the fact that Bayesianism 
yields no OMTs concerning Adam's behavior. The Bayesian methodology allows 
a continuum of beliefs between the two categories "ruled out" and "possible" that 
are considered when we speak of OMTs. However, this is not going to help Eve. 
Her hypothesis that Adam is rational provides no restrictions concerning Adam's 
behavior. Therefore, she is in no better position to predict Adam than to predict 
the digits generated by the black box. 
For a formal proof, we have to generalize the previous results to larger spaces 
of observables. This presents no difficulties. 
20 Hence, "merger of opinions" for different persons (cf. [Earman 1992: ch. 6]) has nothing to do 
with convergence of expectations (opinions concerning the future). 
21 Unfortunately, non-convergence cannot be quite as dramatic as suggested by theorem 2 in Albert 
[1999], which is incorrect. Under the conditions of theorem 2, convergence to rational expectations is 
ensured although the probability of convergence to the truth may be 0 (as is typically the case if, e.g., 
both hypotheses assign a probability of 0.5 to observing 0 in the limit). 
22 A hypothesis is dismissed from the outset iff it is not in the support of the prior. The support is 
a set with zero-probability complement; sometimes, it is also required that the support's intersection 
with any open set, if not empty, has positive probability. In our analysis, the support is I since all open 
sets are measurable and contain basic intervals, which never have probability O. 

356 
MAX ALBERT 
Eve observes the digits on the screen and Adam's choices. Moreover, she might 
observe other things, like Adam's facial expression or his pattern of consumption, 
that are or are not related to Adam's behavior. Sticking to our premise that, re-
alistically, all observable variables can only range over a finite set of values, we 
assume that Eve's observable universe can be "digitalized": each state can be de-
scribed by a binary string of Os and Is with maximum length n ~ 2. If the number 
of different states is between 2n - 1 and 2n for some n, several strings describe the 
same state. 
Again, we look for a set of hypotheses sufficiently rich to allow Eve to ratio-
nalize any forecast. Such a set is again provided by the (modified) chaotic clock. 
Assume that Eve considers Adam and the money-spinner as a big black box that 
displays one of 2n combination of observables at each point in time. The combi-
nations are determined by a chaotic clock that makes n angle-<\uadrupling ticks 
at each point in time; the resulting string St of n digits is then revealed as a solid 
block instead of a succession of digits. Formally, we can leave the chaotic clock 
as it is; we just have to assume that, at each t = 1, ... ,00, St is observed instead 
of just one digit Xt: 
(8) 
(a) 
XT 
g(ZT) 
(c) 
Zl = 0 
(b) 
(d) 
h(2zT ) 
= {X(t-l)n+l, ... ,Xtn}, t = T div n 
This dynamical system is identical to (7) except for the fact that at each point in 
time t, the string St produced by the last n ticks of the clock is observed. The 
sequence generated by the system is the dyadic development of the starting point 
with every second digit removed. 
The chaotic clock is a "theory of everything" for universes the evolution of 
which can be described by an infinite sequence of binary string of length n. Since 
the possibilities of assigning probabilities to sequences are not affected by the fact 
that these sequences are now revealed in a blockwise fashion, the previous results 
still apply: according to the anything-goes theorem, arbitrary assignments to these 
probabilities are possible. This answers question 3. Bayesianism as a method-
ology is completely useless in predicting rational behavior because there are no 
OMTs covering this behavior. Eve's expectations concerning Adam's behavior are 
completely arbitrary. 
For example, Eve may decide to view Adam as attaching equal probabilities to 
two arbitrary hypotheses like "The probability of Xt = 0 is ~" and "The probability 
of Xt = 0 is ~" while she herself attaches a probability of ~ to Xt = O. She then 
can find a prior such that exactly the conditional probabilities implied by this view 
will hold, no matter what happens. 
Moreover, the analysis of Eve's problem shows that the restriction of Adam's 
problem to the prediction of single digits is immaterial. Everything works as before 
as long as there is a maximum amount of information he can access at each point 
in time. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
357 
6 CONCLUSIONS 
Bayesian rationality becomes empty if the decision maker considers a set of hy-
potheses that is as large as the set described with the help of the chaotic clock. 
Whatever the actual process generating a sequence of observations, considering a 
chaotic--dock explanation already implies that any experience can be accommo-
dated without implications for expectations concerning the future. The inclusion 
of further hypotheses does not add to the complexity of the learning problem. 
This is just another version of the problem of induction. Logically, one can 
never infer the laws governing the world from a finite number of past observation. 
While many theories may be eliminated over time, it is quite trivial that there al-
ways remain enough theories consistent with any kind of future. The so-called 
pragmatic problem of induction says that learning, if guided by experience and 
deductive logic alone, yields no restrictions for decision making.23 The any thing-
goes theorem shows that Bayesianism, although employing more than just deduc-
tive logic, cannot solve the pragmatic problem of induction either. 
This result is no surprise once it is clear how many degrees of freedom Bayesian-
ism leaves to the decision maker in setting up the initial beliefs. But it is surprising 
that a sufficiently rich set of hypotheses can be introduced in such a simple and 
compact way. If not much sophistication is needed to experience the problem 
generated by too many logical possibilities, maximum sophistication or perfect 
rationality will necessarily lead to this problem. 
It is a mathematical fact that any strategy can be rationalized even for a given 
NM utility function. Several arguments might be raised against the position, taken 
in the present paper, that this fact speaks strongly against adopting Bayesianism as 
a positive or normative theory. Specifically, four arguments seem to be important. 
Their discussion will conclude this paper. 
6.1 
Supplementing Bayesianism 
It is not necessarily alarming if a theory is devoid of empirical content. As long 
as the theory is not analytic, it might still be possible to supplement it by further 
hypotheses, creating a larger theory the empirical content of which comes neither 
from the original theory alone nor from the supplement alone. The same goes, 
mutatis mutandis, for a normative theory. Even if Bayesianism allows one to ratio-
nalize any strategy, there might be supplementary rules that distinguish between 
good and bad rationalizations.24 
However, such supplementary rules are equivalent to a principle of insufficient 
reason. This can easily be shown. Prima facie, there seem to be two different 
options. Supplementary rules or hypotheses providing content can either a) restrict 
23Cf. Musgrave [1989: section 4] and Miller [1994: 20-23, 38-45], whose solution rests on the 
assumption that it is possible to reduce the number of acceptable theories drastically. 
24Nyarko [1997: 176] makes this point but just provides results concerning the implications of dif-
ferent restrictions placed on priors. 

358 
MAX ALBERT 
prior beliefs on the basis of experience or b) restrict prior beliefs without any basis 
in experience. 
However, option a) is really not different from option b). Assume that there 
is a rule R that restricts the prior on the basis of experience. Thus, for every set 
of data E, the rule R selects a prior. This is exactly what Bayesian updating does 
on the basis of a still earlier prior chosen before E becomes known. We have 
seen that a decision maker can choose the prior before experience such that, for 
every set of data E, an arbitrary predetermined posterior results. Thus, whatever 
the rule R, a prior can be chosen before any experience that is in conformity with 
the recommendations of R. It follows that R can be replaced by restrictions on the 
admissible set of priors before any experience; we are in effect left with option b). 
A rule determining a prior, or at least restricting the choice of priors, without 
any basis in experience is a "principle of insufficient reason". As has been ar-
gued before, Bayesianism is the product of a history of failures to provide such 
a principle. Thus, embracing option b) looks not very promising for normative 
Bayesianism. 
For Bayesianism as a positive theory, of course, it would be irrelevant whether a 
"principle of insufficient reason" looks reasonable or not as long as the package is 
successful empirically. However, this kind of success is missing so far, or at least 
this seems to be the public opinion in economics. 
Given the record of Bayesianism as a positive theory of behavior, the advocates 
of Bayesianism in economics have stressed Bayesianism's methodological virtues 
as a systematic theory of behavior as compared with the "adhockery" of bounded-
rationality approaches. 25 
As we have seen, Bayesianism does not provide a theory of behavior on its own. 
Without a theory of priors, the actual hypothesis in each application must needs 
be chosen in an ad hoc fashion by selecting a prior or an admissible set of priors. 
Thus, Bayesianism as it stands is not methodologically superior to the bounded-
rationality approach. It might be true that we can guess in each application what 
the beliefs of the agents will be. On the other hand, we might equally well guess 
which of a set of rules of thumb they are likely to use. The degree of adhockery on 
both sides seems comparable. 
6.2 
Bayesianism as Adaptation 
Another defense of Bayesianism is the idea that Bayesian perfect rationality might 
be an idealization anticipating-prematurely, so to speak-the long-run effects of 
adaptation and training. This means that selective pressures favor Bayesian perfect 
rationality in the long-run. 
2SFor this argument and the next, cf. Selten's account of a fictitious discussion between exponents 
of the different approaches to the explanation of behavior, where the Bayesian defends his position by 
these arguments [Selten 1989: 5, 11,21]. Selten introduces several counterarguments, which, however, 
seem to be based on the assumption that Bayesian rationality has content. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
359 
Let us compare the hypothetical fate of a perfectly rational with that of a bound-
edly rational agent. Let us call these hypothetical individuals Priscilla and Brian, 
respectively. Brian is not logically omniscient; he does not consider a set of hy-
potheses sufficiently rich to rationalize any behavior. Brian starts with a restricted 
set of hypotheses, and, for whatever reasons, there are only some priors that ap-
peal to him. Moreover, he is bound to make logical mistakes; thus, even if he tried 
to maximize his subjectively expected utility, he would often fail to recognize the 
subjectively optimal actions. On the other hand, Brian might just adopt some rule 
of thumb for decision making and ignore his own beliefs. Would Brian have any 
disadvantages as compared with Priscilla? Will the Brians of this world either 
learn to mimick Priscilla's cleverness or vanish in the long run? 
It seems not, at least from a Bayesian point of view. The difference between 
Brian and Priscilla has nothing to do with the strategies they pick and, conse-
quently, nothing with their success. The difference is just the extent to which their 
choices can be rationalized in terms of their beliefs. In fact, both might choose the 
same actions under identical circumstances. It is not as if the rules of Bayesian-
ism offered protection against mistakes that could be identified as such by clever 
Priscilla. To Priscilla, there are actions that are mistaken in the light of one's be-
liefs but no actions that are mistaken just in light of the known facts. Priscilla is 
able to rationalize any behavior; even if Brian were unable to do the same, she 
could do it for him. 
Thus, while there might be selective pressure to avoid certain kinds of behavior, 
there cannot be any selective pressure in favor of perfecting the rationalization of 
behavior. Nobody stands a better chance in any competition just on account of 
being a Bayesian. Of course, among Bayesians, there might be selective pressure 
against certain priors. But this is a completely different point. 
The idea that there is selective pressure in favor of perfect rationality is histori-
cally connected with the as-if defense of the rationality postulate. The as-if defense 
has been the war cry of empiricist positive economics: "Never mind how people 
actually think; when it comes to action, they behave as if they were rational." The 
anything-goes theorem robs the as-if defense of any empiricist appeal, at least if 
rationality is taken to be Bayesian rationality. The statement "people behave as if 
they were Bayesians" turns out to be analytic; it boils down to "people do whatever 
they do." The adaptationist argument has been used to defend the as-if argument: 
Why should people behave as if they were rational? The adaptationist answer: 
Because those who do are more successful in the long run than those who do not. 
Obviously, this argument, if applied to Bayesian rationality, wrongly presupposes 
that Bayesian rationality helps to avoid mistakes. 
6.3 Logic, Coherence, and the Axioms 
Almost everybody agrees that deductive logic and logical consistency are valuable. 
However, deductive logic restricts only the structure of beliefs and not the choice of 
strategies. One might argue, then, that it cannot be held against Bayesianism if the 

360 
MAX ALBERT 
Bayesian logic of probabilistic beliefs and the corresponding notion of consistency 
(often called "coherence") display the same weakness.26 
This argument, however, is insufficient to defend Bayesianism. Logical consis-
tency serves a purpose. Beliefs cannot possibly be true if they are inconsistent. 
Thus, if one wants truth, logical consistency is necessary. An analogous argument 
in favor of Bayesianism would have to point out some advantage of coherence un-
available to those relying on non-probabilistic beliefs and deductive logic alone. 
Such an argument is missing. 
It is true that Bayesianism provides a logic of beliefs that rational persons must 
respect-if and only if their beliefs take the form of subjective probabilities. Those 
who reject this view-Popperians, classical statisticians, and others-can always 
point out to a Bayesian that their procedures could be rationalized on Bayesian 
grounds. The argument that the rationalizing prior might be "bad"--e.g., assign a 
positive probability to hypotheses they do not consider in earnest-will not worry 
them since for them there are no good priors anyway. In their view, beliefs just do 
not take the form of a probability distribution. 
Of course, it is a theorem that the beliefs of rational agents should take the form 
of probabilities, not an assumption. The theorem follows from the axioms for 
preferences on the set of all strategies. If one argues that beliefs need not take the 
form of probabilities, the real question from a Bayesian point of view is, What's 
wrong with the axioms? 
In my opinion, the axioms are quite reasonable if one is looking for a complete 
preference order on the set of all strategies. In the case of decision making under 
certainty or risk, a complete preference order might indeed be helpful.27 But this 
is different in the case of choice under uncertainty. 
Consider the following thought experiment. Adam has to choose from a menu 
in a restaurant. He orders chicken because he prefers it most. However, chicken is 
out. If pork was the second-best choice before, it is the best choice under the new 
conditions. A complete preference order means that he has decided in advance 
what to order if items are deleted from the menu. 
There is no corresponding thought experiment for choice under uncertainty. 
Bayesian coherence amounts to a preference order among all conceivable strate-
gies, where each strategy specifies the reactions to any new information. Hence, 
the reaction to the information that something is not available after all is already 
part of the chosen strategy. The preference order among the discarded strategies 
is therefore irrelevant by definition of the term strategy. Bayesianism not only is 
no help in choosing a strategy; it additionally requires that one chooses an order 
among the remaining irrelevant strategies. From this point of view, Bayesianism 
is worse than useless. 
26 Analogies between Bayesianism and deductive logic are stressed by Howson [1997]. 
27Even then, rational choice does not require a complete preference order. Look at a simple example. 
Which of the following options would you prefer: losing your left hand, your right foot, or $ 107 I find 
the choice easy although completely ordering the alternatives is beyond me. 

BAYESIAN LEARNING AND EXPECTATIONS FORMATION: ANYTHING GOES 
361 
ACKNOWLEDGEMENTS 
For useful hints, comments, and critical remarks on previous versions, I am grate-
ful to Friedrich Breyer, Donald Gillies, Franco Gori, Daniel Hausman, Frank 
Heinemann, Ronald A. Heiner, Colin Howson, Karl-Josef Koch, Alberto Loayza-
Grbic, JOrgen Meckl, Alan Musgrave, Dieter Schmidtchen, Reinhard Selten, Thus-
nelda Tivig, Heinrich Ursprung, Hermann Vetter, and Elie Zahar. 
Institute of Economics, University of Koblenz-Landau, Germany. 
BIBLIOGRAPHY 
[Albert, 1996] M. Albert. Bayesian learning and expectations formation: Anything goes, Discussion 
Papers of the Faculty of Economics and Statistics, No 284, University of Konstanz, 1996. 
[Albert, 1999] M. Albert. Bayesian learning when chaos looms large, Economics Letters 65, 1-7, 
1999. 
[Arrow, 1990] K. Arrow. Economic theory and the hypothesis of rationality. In Eatwell et al., pp. 
25-37, 1990. 
[Anscombe and Aumann, 1963] F. Anscombe and R. 1. Aumann. A definition of subjective probabil-
ity. Annals of Mathematical Statistics, 34,199-205,1963. 
[Aumann, 1987] R. J. Aumann. Correlated equilibrium as an expression of Bayesian rationality, 
Econometrica 55, 1-18, 1987. 
[Begg, 1982] D. K. H. Begg. The Rational Expectations Revolution in Macroeconomics. Theories and 
Evidence, Oxford: Phillip Allan, 1982. 
[Bernstein, 1996] P. L. Bernstein. Against the Gods. The Remarkable Story of Risk. Wiley, New York, 
1996. 
[Bicchieri, 1993] C. Bicchieri. Rationality and Coordination, Cambridge: Cambridge University 
Press, 1993. 
[Bicchieri et aI., 1997] C. Bicchieri, R. Jeffrey and B. Skyrms, eds. The Dynamics of Norms, Cam-
bridge: Cambridge University Press, 1997. 
[Binmore, 1993] K. Binmore. De-Bayesing game theory. In K. Binmore, A. Kirman and P. Tani, eds., 
Frontiers of Game Theory, pp. 321-339. CambridgelMass.: MIT Press 1993. 
[Binmore, 1994] K. Binmore. Game Theory and the Social Contract, Vol. I: Playing Fair. Cam-
bridgelMass.: MIT Press, 1994. 
[Blume and Easley, 1995] L. E. Blume and D. Easley. What has the rational learning literature taught 
us? In Kinnan and Salmon, pp. 12-39, 1995. 
[Border, 1992] K. C. Border. Revealed preference, stochastic dominance and choice of lotteries, Jour-
nal of Economic Theory 56, 20-42, 1992. 
[Bray,1983] M. Bray. Convergence to rational expectations equilibria. In Frydman and Phelps, pp. 
123-132, 1983a. 
[Bremaud, 1988] P. Bremaud. An Introduction to Probabilistic Modelling, New York: Springer, 1988. 
[Davies, 1987] P. Davies. Cosmic Blueprint, London: Heinemann, 1987. 
[Devaney, 1989] R. L. Devaney. An Introduction to Chaotic Dynamical Systems, 2nd ed., Redwood 
City/Cal.: Addison-Wesley, 1989. 
[Earman, 1992] 1. Earman. Bayes or Bust?, CambridgelMass.: MIT Press, 1992. 
[Eatwell et al., 1990] 1. Eatwell, M. Milgate and P. Newman, eds. The New Palgrave: Utility and 
Probability, New York: Norton, 1990. 
[Ford,1983] 1. Ford. How random is a coin toss? Physics Today, April 1983. 
[Friedman, 1953] M. Friedman. The methodology of positive economics, in: Milton Friedman, Essays 
in Positive Economics, Chicago and London: Chicago University Press 1953, pp. 3-43. 
[Frydman and Phelps, 1983] R. Frydman and E. S. Phelps. Introduction. In Frydman and Phelps, pp. 
1-30, 1983a. 
[Frydman and Phelps, 1983a] R. Frydman and E. S. Phelps. Individual Forecasting and Aggregate 
Outcomes, Cambridge: Cambridge University Press 1983. 

362 
MAX ALBERT 
[Gillies, 2001] D. A. Gillies. Bayesianism and the Fixity of the Theoretical Framework. In present 
volume, pp. 373-390, 2001. 
[Goldman, 1999] A. 1. Goldman. Knowledge in a Social World, Oxford: Oxford University Press, 
1999. 
[Goodman, 1955] N. Goodman. Fact, Fiction, and Forecast, Indianapolis: Bobbs-Merrill, 1955. 
[Green and Osband, 1990 E. 1. Green and K. Osband. A revealed preference theory for expected 
utility, Review of Economic Studies 58, 677-fJ96, 1991. 
[Hacking, 1990] 1. Hacking. Probability. In Eatwell, et al., pp. 163-177, 1990. 
[Hahn, 1996] F. Hahn. Rerum cognoscere causas, Economics and Philosophy 12, 183-195, 1996. 
[Howson, 1997] C. Howson. Logic and probability, British Journal for the Philosophy of Science 48, 
517-531, 1997. 
[Howson and Urbach, 1989] C. Howson and P. Urbach. Scientific Reasoning: The Bayesian Ap-
proach, La Salle/Ill.: Open Court. 1989. 
[Kiefer and Nyarko, 1995] N. M. Kiefer and Y. Nyarko. Savage-Bayesian models of economics, in: 
Kirrnan and Salmon, pp. 40-62, 1995. 
[Kim, 1992] T. Kim. The subjective expected utility hypothesis and revealed preference, Economic 
Theory 1,251-263, 1992. 
[Kirrnan and Salmon, 1995] A. Kirrnan and M. Salmon, eds. Learning and Rationality in Economics, 
Oxford: Blackwell, 1995. 
[Leamer, 1978] E. E. Leamer. Specification searches, New York: Wiley, 1978. 
[Ledyard, 1986] 1. o. Ledyard. The scope of the hypothesis of Bayesian equilibrium, Journal of Eco-
nomic Theory 39, 59-82, 1986. 
[Lindley,1972] D. V. Lindley. Bayesian Statistics. A Review, Philadelphia: SIAM,1972. 
[Miller, 1994] D. Miller. Critical Rationalism. A Restatement and Defense, Chicago and La Salle/Ill.: 
Open Court, 1994. 
[Musgrave,1989] A. Musgrave. Saving science from scepticism. In F. D'Agostino and 1. C. Jarvie, 
eds. Freedom and Rationality, Dordrecht: Kluwer 1989, pp. 297-323. 
[Muth, 196Il 1. F. Muth. Rational expectations and the theory of price movements, Econometrica 29, 
315-335, 1961. 
[Nyarko, 1997] Y. Nyarko. Savage-Bayesian agents playa repeated game. In Bicchieri et aI., pp. 
175-197,1997. 
[Pearl, 2000] 1. Pearl. Causality. Models, Reasoning, and Inference, Cambridge: Cambridge Univer-
sity Press, 2000. 
[Pesaran,1987] M. H. Pesaran. The Limits to Rational Expectations, Oxford: Blackwell, 1987. 
[Pope, 199Il R. E. Pope. The delusion of certainty in Savage's sure-thing principle, Journal of Eco-
nomic Psychology 12,209-241, 1991. 
[Samuelson, 1947] P. A. Samuelson. Foundations of Economic Analysis, enlarged edition, Cambridge/ 
Mass. and London: Harvard University Press 1983 (first published 1947). 
[Savage,1954] L. 1. Savage. The Foundations of Statistics, New York: Wiley, 1954. 
[Savage et al., 1962] L. 1. Savage et al. The Foundations of Statistical Inference, London: 
MethuenlNew York: Wiley, 1962. 
[Schuster, 1988] H. G. Schuster. Deterministic Chaos, 2nd rev. ed., Weinheim: VCH, 1988. 
[Selten, 1989] R. Selten. Evolution, Learning, and Economic Behavior, 1989 Nancy L. Schwartz 
Memorial Lecture, J.L. Kellogg Graduate School of Management, Northwestern University. 
[Simon, 1989] H. A. Simon. From substantive to procedural rationality. In Spiro 1. Latsis, ed., Method 
and Appraisal in Economics, pp. 129-148. Cambridge University Press, 1976, 
[Varian, 1992] H. R. Varian. Microeconomic Analysis, 3rd rev. ed., Norton, New York, 1992. 

DONALD GILLIES 
BAYESIANISM AND THE FIXITY OF THE 
THEORETICAL FRAMEWORK 
1 INTRODUCTION. BAYESIANISM VERSUS CLASSICAL STATISTICS 
Bayesianism is a powerful current of thought in quite a number of different ar-
eas, which include: artificial intelligence, decision theory, economics, philosophy 
of science, and statistics. In the present paper, I will deal only with Bayesianism 
in statistics. In fact since the beginning of this century, the principal controversy 
within statistics has been between Bayesianism and the so-called classical statis-
tics. I will begin therefore by attempting to characterise, in outline at least, these 
two approaches to statistics. 
Let us start with Bayesianism. When this is applied in a particular problem 
situation, it is usually assumed that there is a given a set of possible statistical hy-
potheses He where eel, for some set I, normally an interval of the real line. Some 
data or evidence e say is collected and the problem is to judge the hypotheses in 
the light of this evidence. To do this, the parameter e is given a prior probability 
distribution p( e) say. This represents the degree of belief of the statistician that 
e has various values before the evidence e is considered. Given p( 8), the poste-
rior probability distribution given e, i.e. p(Ole) is then calculated using Bayes' 
Theorem. Our Bayesian statistician now adjusts his or her beliefs from p( 0) to 
p( BI e), a process known as Bayesian conditionalisation. The merits of the various 
hypotheses HIJ are now judged using p(Ble). Statistical inference on this account 
consists essentially of a change from a set of beliefs represented by pCB) to another 
set represented by p( Ole), or, to put it another way by a change of beliefs brought 
about by Bayesian conditionalisation. 
While the concept of change of belief lies at the heart of Bayesianism, the corre-
sponding concept for classical statistics is, in my view. that of hypothesis testing. 
I regard statistical tests as the core of classical statistics. This means that clas-
sical statistics, despite being allegedly 'classical', is in reality much more recent 
than Bayesianism. Bayesianism began with the publication of the famous paper 
of Bayes and Price in [1763]. It received a powerful mathematical development 
from Laplace in his [1812]. By contrast classical statistics can be dated from 1900 
because, in a paper published that year, Karl Pearson introduced the X2 test -
the first really important and widely used statistical test. Further statistical tests 
and a theory of statistical testing were subsequently developed by the founders 
of classical statistics 'Student' Cw. S. Gosset), R. A. Fisher, E. S. Pearson, and 
J. Neyman. The methodology of classical statistics is essentially that of testing. 
Statistical hypothesis are put forward tentatively to explain observed data, and are 
then subjected to statistical tests. If they pass these tests, they continue to be held. 
363 
D. Corfield and 1. Williamson (eds.). Foundations of Bayesian ism. 363-379. 
Â© 2001 Kluwer Academic Publishers. 

364 
DONALD GILLIES 
If they fail the tests, they have to be abandoned or modified. The method here is 
that of conjectures and refutations as advocated by Popper in his [1963]. 
Having stated what I see as the difference between Bayesianism and classical 
statistics, I will now outline the criticism of Bayesianism which I wish to develop 
in this paper. It is not a criticism which attempts to show that Bayesianism is 
wrong in all circumstances. Indeed there are some situations where a Bayesian 
analysis seems to me quite correct -
for just one example see my joint paper 
with Phil Dawid, [1989]. What the argument seeks to do is to place a limit on 
the situations in which Bayesianism should be applied. Roughly the thesis is that 
Bayesianism can be validly applied only if we are in a situation in which there is a 
fixed and known theoretical framework which it is reasonable to suppose will not 
be altered in the course of the investigation. I I call this the condition of the fixity 
of the theoretical framework. For Bayesianism to be appropriate, the framework 
of general laws and theories assumed must not be altered during the procedure of 
belief change in the light of evidence. If this framework were altered at any stage, 
this could lead to changes in the probabilities which were made not in accordance 
with Bayes theorem and Bayesian conditionalisation. It follows that, if we are 
studying a process whose nature is not well known, statistical testing using the 
methodology of classical statistics is essential. 
I will try both to elaborate this criticism and to render it plausible by considering 
two examples, one in each of the next two sections. The first of these examples is 
of an investigation carried out by an eminent classical statistician, and the second 
by an eminent Bayesian. Our eminent classical statistician is Jerzy Neyman, and 
his investigation was into the distribution of larvae in the plots of an experimental 
field. I will try to show in section 2 that this investigation was an admirable one, 
and that its success depended crucially on the use of the methodology of testing 
employed in classical statistics. Our eminent Bayesian is Bruno de Finetti, and 
in section 3 I will consider his use of exchangeability in his [1937]. I will argue 
that this gives reasonable answers if the process under study consists objectively 
of independent events, but can go drastically wrong if the process is e.g. a Markov 
chain. This shows that we have to be very sure of the correctness of our theoretical 
framework (in this case that the process consists of independent events) before 
applying Bayesianism. After going through these two examples, I will conclude 
(The phrase 'a fixed theoretical framework' comes from Lakatos [\968, p. l6I], although he uses 
it in a somewhat different sense. Lakatos is criticising Carnap's inductive logic, and points out that 
Camap's confirmation function (c-function) depends on the language employed so that it cannot cope 
with changes in the language. Lakatos puts the argument like this: 
'Although growth of the evidence within a fixed theoretical framework (the language 
L) leaves the chosen c-function unaltered, growth of the theoretical framework (intro-
duction ofa new language L *) may change it radically.' 
[\968, p. l61l 
Lakatos here identifies a theoretical framework with a language. By contrast I am using 'theoretical 
framework' to refer to the set of theories under consideration. Thus a theoretical framework in my 
sense changes when a new theory is introduced even though the language does not change. Despite this 
difference, the general structure of Lakatos' argument is quite similar to that of the argument developed 
in this paper. 

BAYESIANISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
365 
the paper in section 4, by considering some ways in which Bayesianism might be 
defended against the criticisms presented. 
2 
AN INVESTIGATION OF NEYMAN'S 
Neyman describes his investigation in [Neyman, 1952, pp. 33-7]. His account 
begins as follows: 
'Problems of pest controlled to studies of the distribution of larvae in 
small plots. An experimental field planted with some crop is divided 
into a number of small plots, .... Then all the larvae found in each 
plot are counted. Naturally the number of larvae varies considerably 
from one plot to another.' 
[1952, p. 33] 
Neyman wanted to find a mathematical model which would account for this 
variation. The first such model which suggested itself to him was the Poisson dis-
tribution, according to which the probability of there being a number n of larvae 
in a small plot (Pn) is given by Pn = exp( _>.)>.n In! for some value of the pa-
rameter >.. In a loose sense this corresponds to the assumption that the larvae are 
distributed randomly throughout the field. It was thus a very plausible hypothesis, 
and indeed Neyman says explicitly that it was [1952, p. 33] ' ... one strongly sug-
gested by intuition.' Neyman had moreover used the same hypothesis of a Poisson 
distribution for a very similar problem concerned with the distribution of bacteria 
on a Petri-plate, and there it had proved very successful. Despite these favourable 
a priori indications, Neyman followed the methodology of classical statistics by 
subjecting the hypothesis of a Poisson distribution to a series of tests, and, rather 
surprisingly, these showed that the hypothesis was false. 
In his 1952, Neyman gives the results of 5 trials of the Poisson distribution 
hypothesis. In each case this hypothesis was subjected to a X2 test. In one case the 
test resulted in a confirmation with a value of X2 of 4.0 with 2 degrees of freedom, 
corresponding to 13.5%. The remaining four tests, however, were clear refutations 
with X2 values corresponding to around 0.1 % or less, resulting in falsifications 
even at a 1 % level of significance. There could be no doubt in the light of these 
results that the hypothesis of a Poisson distribution was incorrect. As Neyman 
says: 
'In all cases, the first theoretical distribution tried was that of Poisson. 
It will be seen that the general character of the observed distribution is 
entirely different from that of Poisson. There seems to be no doubt but 
that a very serious divergence exists between the actual phenomenon 
of distribution of larvae and the machinery assumed in the mathemati-
cal model. When this circumstance was brought to my attention by Dr. 
Beall, we set out to discover the reasons for the divergence.' 
[1952, 
p.34] 

366 
DONALD GILLIES 
As the last sentence of the quotation shows, Neyman did not consider any hy-
potheses other than that of the Poisson distribution until after the Poisson distri-
bution hypothesis had been refuted by statistical tests. As so often in science, it 
was the falsification of a hypothesis which stimulated theoretical reasoning. This 
point will be important when we consider how this case might be analysed from 
the Bayesian point of view. Let us now see how Neyman continued with the inves-
tigation. He describes his next steps as follows: 
, ... if we attempt to treat the distribution of larvae from the point of 
view of Poisson, we would have to assume that each larva is placed on 
the field independently of the others. This basic assumption was flatly 
contradicted by the life of larvae as described by Dr. Beall. Larvae 
develop from eggs laid by moths. It is plausible to assume that, when 
a moth feels like laying eggs, it does not make any special choice be-
tween sections of a field planted with the same crop and reasonably 
uniform in other respects. Therefore, as far as the spots where a num-
ber of moths lay their eggs is concerned, it is plausible that the dis-
tribution of spots follows a Poisson Law of frequency, depending on 
just one parameter, say m, representing the average number of spots 
per unit area. 
However, it appears that the moths do not lay eggs one at a time. 
In fact, at each "sitting" a moth lays a whole batch of eggs and the 
number of eggs varies from one cluster to another. Moreover, by the 
time the counts are made the number of larvae is subject to another 
source of variation, due to mortality. 
After hatching in a particular spot, the larvae begin to look for food 
and crawl around. Since the speed of their movements is only moder-
ate, it is obvious that for a larva to be found within a plot, the birth-
place of this larva must be fairly close to this plot. If one larva is 
found, then it is likely that the plot will contain more than one from 
the same cluster.' 
[1952, pp. 34-5] 
It is worth noting here that in his attempt to find a new better hypothesis to de-
scribe the distribution of the larvae, Neyman made use of background knowledge 
about the larvae obtained from the domain expert, Dr. Beall. This led him to sup-
pose that the larvae would be distributed in clusters round points where batches 
of eggs had been laid. The points where the eggs were laid would follow a Pois-
son distribution, but not the larvae themselves. Neyman produced a mathematical 
model of this situation which led to the conclusion that the larvae would be dis-
tributed in what he called a 'Type A distribution' depending on two parameters. 
Using the same data as before, Neyman again applied the X2 test in the 5 cases, 
and this time all the tests confirmed the hypothesis. Neyman had clearly suc-
ceeded in explaining a surprising experimental finding, and his successful investi-
gation shows the merits of classical statistics, or, what is the same thing, Popper's 

BAYESIAN ISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
367 
methodology of conjectures and refutations applied using statistical tests to obtain 
the refutations. 
Neyman himself observes [1952, p. 37]: 'In this example, in order to have 
agreement between the observed and predicted frequencies, it was imperative to 
adjust the mathematical model.' Moreover far from being dogmatic about his new 
Type A distribution, he is anxious to point out that it, like the Poisson distribution, 
has its limitations. Indeed he says: 
' ... there are organisms (e.g., scales) whose distribution on units of 
area of their habitat does not conform with type A. An investigation 
revealed that the processes governing the distribution of these organ-
isms were much more complex than that described and therefore, if a 
statistical treatment is desired, a fresh effort to construct an appropri-
ate mathematical model is necessary.' 
[1952, p. 37] 
That concludes my account of Neyman's investigation of the distribution of lar-
vae in a field, and I now turn to the question of whether a Bayesian statistician 
could have carried out this investigation as successfully as the classical statistician 
Neyman. I do not see how this could have been possible. A Bayesian would start in 
the same way by formulating a set of possible hypotheses H>. where 0 < >. < 00. 
Here H>. is just the Poisson distribution with parameter >.. The next step would 
have been to set up a prior probability distribution p(>.) representing the Bayesian 
statistician's prior degree of belief in the various hypotheses. This would have 
been changed in the light ofthe evidence e to a posterior distributionp(>'le). Yet it 
is difficult to see how all these changes in degrees of belief by Bayesian condition-
alisation could have produced the solution to the problem, namely a Type A dis-
tribution. The Bayesian mechanism seems capable of doing no more than change 
the statistician's degree of belief in particular values of >.. This illustrates very 
nicely my thesis that Bayesianism requires the fixity of the theoretical framework. 
The theoretical framework at the beginning of the investigation was the assump-
tion of a Poisson distribution. If this framework had been adequate, as it was in 
the example of bacteria on a Petri-plate, then Bayesianism would have dealt with 
the problem satisfactorily. However the theoretical framework was not adequate 
for the example of larvae in a field. It had to be changed from the assumption of a 
Poisson distribution to that of a Type A distribution, and the procedure of Bayesian 
conditionalisation cannot cope with such a change in belief. 
To this it might be objected by a Bayesian that the initial set of possible hypothe-
ses should have included both Poisson distributions and Type A distributions. If 
this had been done, then Bayesian conditionalisation would have dealt with the 
problem in a perfectly satisfactory manner. However, the difficulty with this pro-
posal is that, as already pointed out, Neyman only thought of his Type A distri-
bution after the assumption of a Poisson distribution had been refuted by a series 
of X2 tests. Neyman certainly did not consider Type A distributions as an a priori 
possibility at the beginning of the investigation. Indeed Type A distributions did 

368 
DONALD GILLIES 
not exist in the literature of probability and statistics at the beginning of Neyman's 
investigation. It was his analysis of the p~icular problem with the help of the 
domain expert Dr. Beall, which caused Neyman to introduce Type A distributions 
for the first time. Moreover it was only the stimulus provided by the falsification 
of his initial hypothesis which led Neyman to carry out the subtle analysis which 
led him to formulate the Type A distribution. 
A persistent defender of Bayesianism might still argue that a proper analysis of 
the problem at the beginning of the investigation could have led to the introduction 
of the Type A distribution at that stage. I rather doubt whether this is a serious 
possibility, but let us suppose for the moment that it is. The methodology corre-
sponding to this approach would be for the Bayesian statistician to begin with a 
lengthy analysis of the problem, consulting domain experts, and introducing all 
the various distributions which might be relevant. While the views of Dr. Beall 
suggested the Type A distributions, the views of other domain experts, since do-
main experts often disagree, might have suggested further possible distributions, 
say distributions of types B, C, and D. Moreover distributions other than Type A 
are sometimes necessary for problems of this kind, as Neyman's discussion of the 
distribution of scales, quoted earlier, shows. The Bayesian could then formulate 
his prior belief distribution over all these hypotheses, and proceed from there. Un-
fortunately such an approach could very often prove a complete waste of time. 
Suppose a Bayesian statistician had tried such an approach on the example of bac-
teria on a Petri-plate. By the time he or she had formulated the first one or two 
of the hypothetical new distributions which might be possible, Neyman would al-
ready have confirmed by a series of X2 tests that the simple Poisson distribution 
was quite adequate in this case. This shows how easy and straightforward is the 
methodology of classical statistics. It allows us to start with a simple conjecture 
such as the Poisson distribution, provided only we obey the golden rule of testing 
our conjecture severely. If the conjecture passes our tests, then it can be accepted 
provisionally until some further investigations suggest the need for a modification. 
In the interim we have found a workable hypothesis without the need for elabo-
rating a whole series of possible alternatives. Since Bayesianism depends on the 
fixity of the theoretical framework, Bayesian statisticians are faced with an awk-
ward choice. Either they must, at the very beginning of the investigation, consider 
a whole series of arcane possible hypotheses, or they must risk never subsequently 
arriving at the hypothesis which constitutes the solution of the problem. Their dif-
ficulty here arises from the very essence of Baysianism, namely its limitation of 
changes of belief to those produced by Bayesian conditionalisation. 
There are some further ways in which Bayesianism might be defended in the 
context of this particular example, but it will be convenient to postpone their con-
sideration until section 4, and proceed in the next section to give my second ex-
ample. In the first example, I have tried to show the merits of the methodology of 
classical statistics when applied by a leading classical statistician. In the second 
example I will move in the opposite direction by giving an analysis by a leading 
Bayesian, namely de Finetti's use of exchangeability, and then trying to show that 

BAYESIANISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
369 
this analysis only give satisfactory results if no changes are needed in the theoret-
ical framework which is implicitly assumed. 
3 DE FINETTI ON EXCHANGEABILITY 
In Chapter III of his [1937], de Finetti poses the question (p. 118): 'Why are 
we obliged in the majority of problems to evaluate a probability according to the 
observation of a frequency?', commenting that this question (p. 119): 'includes in 
reality the problem of reasoning by induction.' He continues: 
'In order to fix our ideas better, let us imagine a concrete example, or 
rather a concrete interpretation of the problem, which does not restrict 
its generality at all. Let us suppose that the game of heads or tails is 
played with a coin of irregular appearance.' 
[1937, p. 119] 
We will now explain how de Finetti analyses this example of the biassed coin from 
his subjective Bayesian point of view. It will emerge that this concrete example 
does, in a significant respect, fail to represent the full generality of the problem of 
reasoning by induction. 
De Finetti's first step is to consider a sequence of tosses of the coin which we 
suppose gives results: E1 , ... , En, ... , where each Ei is either heads (Hi) or tails 
(Ti). So, in particular, Hn+l = Heads occurs on the n + lth toss. Further let e 
be a complete specification of the results of the first n tosses, that is a sequence n 
places long, at the ith place of which we have either Hi or Ti. Suppose that heads 
occurs r times on the first n tosses. The subjective Bayesian's method is to cal-
culate P(Hn+lle), and to show that under some general conditions which will be 
specified later P(Hn+1Ie) tends to r In for large n. This shows that whatever value 
is assigned to the prior probability P(Hn+l), the posterior probability P(Hn+lle) 
will tend to the observed frequency for large n. Thus different individuals who 
may hold widely differing opinions initially will, if they change their probabili-
ties by Bayesian conditionalisation, come to agree on their posterior probabilities. 
Such is the argument. Let us now give, in our simple case, the mathematical proof 
which underpins it. 
Suppose that P(Ei) "I 0 for all i, so that also P(e) "I O. We then have by the 
definition of conditional probability 
(1) 
P(Hn+1&e) 
P(Hn+lle) =----
P(e) 
To proceed further we introduce the condition of exchangeability. Suppose Mr 
B is making an a priori bet that a particular n-tuple of results (Eil Ei2 ... Ein say) 
occurs. Suppose further that heads occurs r times in this n-tuple. Mr B's betting 
quotients are said to be exchangeable if he assigns the same betting quotient to 

370 
DONALD GILLIES 
any other particular n-tuple of results in which heads occurs r times, where both 
nand r can be chosen to have any finite integral non-negative values with r ~ n. 
Let us write his prior probability (or betting quotient) that there will be r heads 
in n tosses as w~n). There are nCr different ways in which r heads can occur 
. 
h 
I nc -
n! 
-
n{n-I) ... {n-r+l) E h f h 
III n tosses, were, as usua , 
r 
-
(n-r)!r! -
r{r-I) ... I 
. 
ac 
0 t e 
corresponding n-tuples must, by exchangeability, be assigned the same probability, 
which is therefore w~n) ;nCr. Thus 
wen) 
(2) 
P(Ei1 Ei2 ... Ein ) = _r_ 
nCr 
Now e, by definition, is just a particular n-tuple of results in which heads occurs 
r times. Thus, by exchangeability, 
Now H n+1&e is an (n + I)-tuple of results in which heads occurs r + 1 times. 
Thus, by the same argument, 
(4) 
(5) 
(n+1) 
P(Hn+1&e) = _W_r_+_I_ 
n+lc r+1 
And so, substituting in (1), we get 
(n - r)!r! 
(n+1) 
wr+1 
wen) 
r 
(r + l)!(n - r)! 
w~~i1) 
(n + I)! 
r + 1 
(n+l) 
wr+1 
n+1 
Formula (5) (which is de Finetti's formula (6), [1937, p. 122], with a slightly 
(n+l) 
ditferent notation) gives us the result we want. Provided only Wrt;) 
-+ 1 as 
Wr 
n -+ 00 (a very plausible requirement), we may choose our prior probabilities 
w~n) in any way we please, and still get that, as n -+ 00, P(Hn+1le) -+ rln (the 
observed frequency), as required. 
We can, however, obtain an even simpler result if we choose the prior probabli-
ties in a particular way. In n tosses, we can have either 0, 1,2, ... , or n heads. So, 
by coherence, 

BAYESIANISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
371 
In the subjective theory, we can choose the w~n) (the prior probabilities) in 
any way we choose subject only to (6). However we can also, though this is not 
compulsory, make the 'principle of indifference' choice of making them all equal 
so that 
(7) 
w~n) = w~n) = w~n) = ... = w~n) = ... = w~n) = 1/(n + 1) 
Substituting this in (5), we get 
r+1 
(8) 
P(HnHle) =--
n+2 
This is a classical result -
Laplace's Rule of Succession, which de Finetti 
derives in the above way [de Finetti, 1937, p. 144]. 
In the above calculations, de Finetti appears to show that subjective Bayesians 
will be led by the process of Bayesian conditionalisation to choose posterior prob-
abilities which approximate to the observed frequency. He thus appears to have 
provided a foundation for reasoning by induction. I next want to argue that these 
calculations, despite their seeming generality, are only appropriate within a spe-
cific theoretical framework, and can lead us astray if used when that framework 
does not hold in reality. In order to identify this framework, I will now give some 
further results from de Finetti's [1937]. These relate the concept of exchangeabil-
ity, which de Finetti himself had introduced, to the older concept of independence. 
De Finetti's ideas on the relationship between exchangeability and independence 
are discussed in [Galavotti, 2001]. 
De Finetti proved a general theorem showing exchangeability and independence 
are linked, I will now state his result. Let us first define exchangeability for a se-
quence of random variables (or random quantities as de Finetti prefers to call them) 
Xl, . .. ,Xn , .... These are exchangeable if, for any fixed n, X i1 , X i2 , ... ,Xin 
have the same joint distribution no matter how iI, ... ,in are chosen. Now let Yn 
be the average of any n of the random quantities Xi i.e. Yn = (1/n)(Xi1 +Xi2 + 
... + XiJ, since we are dealing with exchangeable random quantities it does not 
matter which iI, ... , in are chosen. de Finetti first shows [1937, p. 126] that the 
distribution q;n(~) = P(Yn ~ ~) tends to a limit q;(~) as n --t 00, except perhaps 
for points of discontinuity. He goes on to say: 
'Indeed, let p{ (E) be the probability attributed to the generic event E 
when the events El , E2 , â¢â¢â¢ , En, ... are considered independent and 
equally probable with probability~; the probability P(E) of the same 
generic event, the Ei being exchangeable events with the limiting dis-
tribution q;(~), is 

372 
DONALD GILLIES 
This fact can be expressed by saying that the probability distributions 
P corresponding to the case of exchangeable events are linear combi-
nations of the distributions p( corresponding to the case of indepen-
dent equiprobable events, the weights in the linear combination being 
expressed by <I>(~).' 
[1937, pp. 128-9] 
This general result can be illustrated by taking a couple of special cases. Sup-
pose that we are dealing with a coin tossing example and the generic event E is 
that heads occurs r times in n tosses. Then 
So 
P(E) = w~n) = nCr 11 ~r (1 - ~)n-r d<I>(~) 
If, in particular, <I>(~) is the uniform distribution, we have 
w~n) 
nCr I; ~r(1- ~)n-rd~ 
nCrB(r + 1, n - r + 1), where B is the Beta function 
1/(n + 1) (cf. formula 7 above). 
Comparing these results with our earlier calculations involving exchangeability, 
we can see how exchangeability and independence are related. 
Roughly speaking we can say that the situation which an objectivist would de-
scribe as one of independent events in which particular outcomes have fixed but 
unknown probabilities corresponds to what de Finetti would describe as one of 
exchangeable events. Of course de Finetti would not have liked this formulation, 
since he regarded the 'unknown probabilities' postulated by objectivists and clas-
sical statisticians as metaphysical and meaningless. Thus he says: 
'If ... one plays heads or tails with a coin of irregular appearance, ... , 
one does not have the right to consider as distinct hypotheses the sup-
positions that this imperfection has a more or less noticeable influence 
on the "unknown probability", for this"unknown probability" cannot 
be defined, and the hypotheses that one would like to introduce in this 
way have no objectivemeaning.' 
[1937, pp. 141-1] 
de Finetti therefore concludes: 
, ... the nebulous and unsatisfactory definition of "independent events 
with fixed but unknown probability" should be replaced by that of 
"exchangeable events",' 
[1937, p. 142] 
Naturally I cannot agree with de Finetti's attempt to eliminate the concept of 
unknown probability. To postulate such probabilities, as is done in classical statis-
tics, is neither meaningless nor metaphysical. Conjectures about such unknown 

BAYESIANISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
373 
probabilities can be tested using statistical tests, and either confirmed or refuted, 
and this shows that such conjectures are scientific rather than metaphysical. It is 
thus both meaningful and scientific to postulate that a particular process consists of 
independent events with fixed but unknown probability. My thesis is that this pos-
tulate gives the theoretical framework within which de Finetti's calculations using 
exchangeability lead to sensible results. If we try to use these calculations in situ-
ations where this theoretical framework does not hold objectively, they are liable 
to give absurd and quite inappropriate conclusions. This can be easily shown by 
seeing what happens when we apply the exchangeability calculations to a situation 
which is not one of independent events but of dependent events. 
To illustrate my argument, it would be possible to use anyone of a wide va-
riety of sequences of events which are dependent rather than independent. To be 
concrete, I have first selected the simplest type of dependent sequence, namely a 
Markov chain, and then chosen one very simple and at the same time striking ex-
ample of a Markov chain. This is the game of 'Red or Blue' .2 At each go of the 
game there is a number s which is determined by the previous results. A fair coin 
is tossed. If the result is heads, we change 8 to 8' = 8 + 1, and if the result is tails, 
we change 8 to 8' = 8 -
1. If 8' ~ 0, the result of the go is said to be blue, while 
if 8' < 0, the result of the go is said to be red. So, although the game is based on 
coin tossing, the results are a sequence of red and blue instead of a sequence of 
heads and tails. Moreover, while the sequence of heads and tails is independent, 
the sequence of red and blue is highly dependent. We would expect much longer 
runs which are all blue, than runs in coin tossing which are all heads. If we start 
the game with 8 = 0, then there is a slight bias in favour of blue which is the initial 
position. However, it is easy to eliminate this by deciding the initial value of 8 by 
a coin toss. If the toss gives heads we set the initial value of 8 at 0, and if the toss 
gives tails we set it at -1. This makes red and blue exactly symmetrical, so that 
the limiting frequency of blue must equal that of red and be 112. It is therefore 
surprising that over even an enormously large number of repetitions of the game, 
there is high probability of one of the colours appearing much more often than 
the other. Feller [1950, pp. 82-3] gives a number of examples of these curious 
features of the game. Suppose for example that the game is played once a second 
for a year, i.e. repeated 31,536,000 times. There is a probability of 70% that the 
more frequent colour will appear for a total of 265.35 days, or about 73% of the 
time, while the less frequent colour will appear for only 99.65 days, or about 27% 
of the time. 
Let us next suppose that a subjective Bayesian (Mr B) is asked to analyse a se-
quence of events, each member of which can have one of two values. Unknown 
to them this sequence is in fact generated by the game of red or blue. Possibly the 
2The game of 'Red or Blue' is described in Feller, [1950, pp. 67-95]' which contains an interesting 
mathematical analysis of its curious properties. Popper read of the game in Feller, and had the idea of 
using it to argue against various theories of induction. Popper uses the game to criticise what he calls 
'the simple inductive rule' in his [1957, pp. 358-601 (reprinted in his [1983, pp. 301-51. I have adapted 
this argument of Popper's to produce the critique of de Finetti's use of exchangeability given here. 

374 
DONALD GILLIES 
sequence might be produced by a man-made device which flashes either 0 (cor-
responding to red) or 1 (corresponding to blue) onto a screen at regular intervals. 
However, it is not impossible that the sequence might be one occurring in the world 
of nature. Consider for example a sequence of days, each of which is classified as 
'rainy' if some rain falls, or dry otherwise. In a study of rainfall at Tel Aviv dur-
ing the rainy season of December, January, and February, it was found that the 
sequence of days could be modelled successfully as a Markov chain. In fact the 
probabilities found empirically were: probability of a dry day given that the pre-
vious day was dry = 0.75, and probability of a rainy day given that the previous 
day was rainy = 0.66. (For further details see [Cox and Miller, 1965, pp. 78-9].) 
It is clear that this kind of dependence will give longer runs of either rainy or dry 
days than would be expected on the assumption of independence. It is thus not 
impossible that the sequence of rainy and dry days at some place and season might 
be represented quite well by the game of red or blue. 
Let us now return to our subjective Bayesian Mr B, who has been asked to deal 
with a process which is really governed, unknown to Mr B, by the game of 'Red or 
Blue'. Being an admirer of de Finetti's, Mr B will naturally make an assumption of 
exchangeability. Let us also assume that he gives a uniform distribution a priori to 
the w~n) (see formula 7 above) so that Laplace's rule of succession holds (formula 
8). This is just for convenience of calculation. The counter-intuitive results would 
appear for any other coherent choice of the w~n). Suppose that we have a run of 
700 blues, followed by 2 reds. Mr B would calculate the probability of getting 
blue on the next go using formula 8 with n = 702, and r = 700. This gives 
the probability of blue as 7011704 = 0.996 to 3 significant figures. Knowing the 
mechanism of the game, we can calculate the true probability of blue on the next 
go, which is very different. Go 700 gave blue, and go 701 gave red. This is only 
possible if s on go 700 was 0, the result of the toss was tails, and s became -1 
on go 701. The next toss must also have yielded tails or there would have been 
blue again on go 702. Thus s at the start of go 703 must be -2, and this implies 
that the probability of blue on that go is zero. Then again let us consider one of 
Feller's massive sessions of 31,536,000 goes. Suppose the result is that the most 
frequently occurring colour appears 73% of the time (as pointed out above there 
is a probability of 70% of this result which is thus not an unlikely outcome). Mr 
B will naturally be estimating the probability of this colour at about 0.73 and so 
much higher than that of the other colour. Yet in the real underlying game, the two 
colours are exactly symmetrical. 
We see that Mr B's calculations using exchangeability will give results at com-
plete variance with the true situation. The reason for this is clear. By making the 
assumption of exchangeability, Mr B is implicitly assuming that the process he 
is considering consists of independent events with a fixed but unknown probabil-
ity. As long as this theoretical framework holds, his Bayesian calculations will 
give him reasonable results, but if the theoretical framework does not hold in a 
particular case, then the same Bayesian calculations will give him completely in-

BAYESIANISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
375 
appropriate results. My conclusion is, once again, that Bayesianism only works if 
the condition of the fixity of the theoretical framework is satisfied. 
Our situation involving the game of 'Red or Blue' does not pose the same prob-
lems for a classical statistician. Suppose such a statistician (Ms C say) is con-
fronted with a sequence of events which, unknown to her, is really governed by 
the game of 'Red or Blue'. It would be perfectly reasonable for Ms C to begin 
by making the simplest and most familiar conjecture, namely that the events are 
independent. Thus Ms C starts tackling the problem in much the same was as Mr 
B. However, being, unlike Mr B, a good Popperian, Ms C will test her conjecture 
rigorously with a series of statistical tests for independence. It will not be long be-
fore she has rejected her initial conjecture, and she will then start exploring other 
hypotheses involving various kinds of dependence among the events. If she is a 
talented scientist, she may soon hit on the red or blue mechanism, and be able to 
confirm that it is correct by another series of statistical tests. In this case the classi-
cal statistician seems better equipped to deal with the problem than the Bayesian. 
However there are some replies to this argument which could be made from the 
Bayesian point of view, and I will consider them in the final section of the paper 
(section 4). 
4 POSSIBLE DEFENCES OF BAYESIANISM 
de Finetti himself does say one or two things which are relevant to the problem. 
Having shown that exchangeable events are the subjective equivalent of the objec-
tivist's independent and equiprobable events, he observes that one could introduce 
subjective equivalents of various forms of dependent events, and, in particular, of 
Markov chains. As he says: 
'One could in the first place consider the case of classes of events 
which can be grouped into Markov "chains" of order 1, 2, ... , m, ... , 
in the same way in which classes of exchangeable events can be re-
lated to classes of equiprobable and independent events.' 
[1937, 
Footnote 4, p. 146] 
We could call such classes of events Markov-exchangeable. De Finetti argues 
that they would constitute a complication and extension of his theory without caus-
ing any fundamental problem: 
'One cannot exclude completely a priori the influence of theorder of 
events ... . There would then be a number of degrees of freedom and 
much more complication, but nothing would be changed in the setting 
up and the conception of the problem ... ,before we restricted our 
demonstration to the case of exchangeable events .. .' 
[1937, p. 145] 
Perhaps de Finetti has in mind something like the following. Instead of just as-
suming exchangeability, we consider not just exchangeability but various forms of 
Markov-exchangeability. To each of these possibilities we give a prior probability. 

376 
DONALD GILLIES 
No doubt exchangeability will have the highest prior probability. If the case is a 
standard one, like the biased coin, this high prior probability will be reinforced, and 
the result will come out more or less like that obtained by just assuming exchange-
ability. If, however, the case is an unusual one, then the posterior probability of 
exchangeability will gradually decline, and that of one of the other possibilities 
will increase until it becomes much more probable than exchangeability. 
This approach to the problem is basically the same as that we attributed to the 
Bayesian in our discussion of Neyman's investigation in section 2, and it is liable to 
the same difficulties which we noted there. If a Bayesian is to adopt this approach 
seriously, he or she must begin every investigation by considering all possible hy-
potheses which might be encountered in the course of the investigation. This is 
scarcely possible, and, even if it were possible, it would often be a waste of time. 
There are many situations in which the most obvious and straightforward hypoth-
esis actually works so that a consideration of a large number of arcane alternatives 
would be useless toil. The classical statisticians do not need to indulge in such 
toil. They can begin with any assumption (or conjecture) they like, provided only 
they obey the golden rule of testing it severely. If the assumption passes such tests, 
it can be provisionally adopted. If it fails, some other better assumption must be 
sought. Thus the classical statistician proceeds, so to speak, one step at a time, 
and there is never any need to engage in the hopeless and time-wasting task of 
surveying all possible hypotheses which might apply to the problem in hand. 
There are moreover, as Albert has shown in his contribution to the present vol-
ume, further difficulties in this defence of Bayesianism. To see what these are, let 
us go back to the formulation of Bayesianism given at the beginning of the paper. 
As I said there, it is usually assumed in a Bayesian statistical analysis that there 
is a given a set of possible statistical hypotheses H8 where () E I, for some set I, 
normally an interval of the real line. The problem we are now considering is how 
the set H8 where () E I should be chosen. If we select a rather narrow set H8 we 
may leave out the hypothesis which would provide the required solution. If we 
try to make H8 broad and inclusive, we set ourselves a very difficult task which 
may well prove a waste of time in a case in which the most simple and obvious 
solution actually works in practice. What Albert shows in his [2001] is that the 
second strategy of searching for a broad and inclusive set H8 is liable to a further 
difficulty. 
Albert considers the possibility of extending the set H8 by including hypotheses 
involving chaos theory. Specifically he defines in section 4 of his paper what he 
calls a 'Chaotic Clock'. In a simple case in which we are considering a sequence 
of O's and 1 's generated by some unknown process, Albert formulates a set H8 
of hypotheses based on a mechanism involving a chaotic clock. He then gives in 
section 5.1 of his paper a remarkable result called the Anything Goes Theorem. 
Suppose Mr B adopts any learning strategy whatever, i.e. he chooses his condi-
tional probabilities given evidence in any arbitrary way. There then exists a prior 
probability distribution p over the set H 8 of hypotheses based on the chaotic clock 
such that Mr B's probabilities are produced by Bayesian conditioning of p. 

BAYESIANISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
377 
Albert's result is very striking indeed. His chaotic clock hypotheses are by no 
means absurd. After all chaos theory is used in both physics and in economics. 
Indeed hypotheses involving chaos are quite plausible as a means of explaining, 
for example, stock market fluctuations. If Mr B were really faced with a bizarre 
sequence of O's and 1 's, why should he not consider a hypothesis based on chaos 
theory? Yet if Mr B is allowed to consider the chaotic clock set of hypotheses, then 
any learning strategy he adopts becomes a Bayesian strategy for a suitable choice 
of priors. In effect Bayesianism has become empty. 
It follows that a Bayesian (Mr B say) is caught on the horns of a dilemma. Mr 
B may adopt a rather limited set of hypotheses to perform his Bayesian condition-
alisation, but then, as the example of the game of Red or Blue shows, if his set 
excludes the true hypothesis, his Bayesian learning strategy may never bring him 
close to grasping what the real situation is. This is the first, or 'Red or Blue', horn 
of the dilemma. If Mr B responds by saying he is prepared to consider a wide 
and comprehensive set of hypotheses, these will surely include hypotheses from 
chaos theory and thus anything he does will become Bayesian, making the whole 
approach empty. This is the second, or 'Chaotic Clock', horn of the dilemma. 
The Bayesian is faced with quite severe difficulties here, but there is one further 
way out which is sometimes suggested, and I will conclude the paper by giving it 
a brief consideration. The suggestion is that in we should start with a reasonably 
specific set of initial hypotheses Ho but add to this set a 'catch all' hypothesis K, 
which simply says that some hypothesis other than the Ho is correct. We then give 
our prior distribution over the Ho and K. If it is a standard case, then one of the 
Ho will emerge as the most probable hypothesis given the evidence. If, however, 
we are dealing with a non-standard case, then K will gain in probability while the 
probability of each of the Ho becomes very small. In such a situation, we will 
divide up K into some specific set 10 say, and a new catch all K', and repeat 
the process. In this way we should, even in a problematic situation, be led to the 
correct hypothesis. 
While such a procedure sounds very reasonable when stated in outline, any 
attempt actually to implement it in detail brings to light a whole host of difficulties 
and complexities, and it is not surprising that there is no instance to my knowledge 
of such a plan being actually carried out in detail by a Bayesian. Let us begin 
by considering how the prior probabilities should be divided between the set HIJ 
and the catch all K. Surely K should have a very large prior probability since 
our background knowledge concerning the development of science would suggest 
that most hypotheses considered at a particular time are eventually shown to be 
inadequate to some degree or in some respects. Yet if K is given a large prior 
probability, this may prevent any of the Ho ever acquiring a large probability, even 
in a straightforward case. 
Suppose this initial difficulty is overcome, we are the faced with another. Let 
us take one of the problematic cases in which we assume to begin with one set of 
hypotheses Ho say, and another set Jo are in fact correct. Ho could be Poisson 
distributions and Jo could be Type A distributions, or Ho could be the hypothesis 

378 
DONALD GILLIES 
of independent events with fixed probability () and J(J could be hypotheses of a 
Markov chain of some type. In this case we have got to show how the probability 
of the catch all K changes from its prior value p(K) say to a posterior value 
p(Kle) in the light of evidence. How is such a calculation to be carried out? It is 
no easy matter, and it must be done in such a way that p(Kle) increases to such a 
value that we decide to abandon the H(J and subdivide K into J(J and the new catch 
all K'. I really think such a calculation is scarcely possible. Of course a Bayesian 
could show that I am wrong by carrying out such a calculation in one of the cases 
dealt with in this paper, but the result would undoubtedly be very complicated. 
At this point one can reasonably ask why the Bayesian wants to get involved in 
such complexities rather than to adopt the methods of classical statistics which, as 
I have shown, deal with the problem in an extremely simple and straightforward 
way, using the method of conjectures and refutations. 
My conclusion is that Bayesianism should only be applied if we are in a situa-
tion in which there is a fixed and known theoretical framework which it is reason-
able to suppose will not be altered in the course of the investigation, that is to say 
if the condition of the fixity of the theoretical framework is satisfied. As regards 
many processes whose nature is not exactly known, statistical testing using the 
methodology of classical statistics is essential. 
Department of Philosophy, King's College London, UK. 
BIBLIOGRAPHY 
[Albert, 2001] M. Albert. Bayesian learning and expectations formation: anything goes. In this vol-
ume, pp. 347-368. 
[Bayes and Price, 1763] T. Bayes and R. Price. An Essay towards Solving a Problem in the Doctrine 
of Chances, reprinted in E.S.Pearson and M.G.Kendall (eds.) Studies in the History of Statistics and 
Probability, Griffin, 1970, pp. 134-53. Originally published 1763. 
[Cox and Miller, 1965] D. R. Cox and H. D. Miller. The Theory of Stochastic Processes. Methuen, 
1965. 
[Dawid and Gillies, 1989] P. Dawid and D. A. Gillies. A Bayesian Analysis of Hume's Argument 
concerning Miracles, The Philosophical Quarterly, 39, 57-{i5, 1989. 
[de Finetti, 1937] B. de Finetti. Foresight: Its Logical Laws, Its Subjective Sources. English transla-
tion in H.E. Kyburg and H.E. Smokier (eds.), Studies in Subjective Probability, pp. 93-158. Wiley, 
1964. 
[Feller, 1950] W. Feller. Introduction to Probability Theory and Its Applications. Third edition, 1971, 
Wiley. 
[Galavotti, 200il M. C. Galavotti. Subjectivism and Objectivity in Bruno de Finetti's Bayesianism, in 
present volume, pp. 171-184. 
[Lakatos, 1968] I. Lakatos. Changes in the Problem of Inductive Logic. Reprinted in John Worrall 
and Gregory Currie (eds.), Imre Lakatos, Philosophical Papers, Volume 2, pp. 128-200, Cambridge 
University Press, 1968. 
[Laplace, 1812] P. S. Laplace. Theorie analytique des probabilites. Reprinted as vol. 5 of Oeuvres 
completes de Laplace, 14 vols, Gauthier-Villars, 1878-1912. 
[Neyman, 1952] 1. Neyman. Lectures and Conferences on Mathematical Statistics and Probability. 
2nd edition, revised and enlarged, Washington: Graduate School of U.S. Department of Agricul-
ture, 1952. 

BAYESIANISM AND THE FIXITY OF THE THEORETICAL FRAMEWORK 
379 
[Pearson, 1900] K. Pearson. On the Criterion that a given System of Deviations from the probable in 
the case of a Correlated System of Variables is such that it can be reasonably be supposed to have 
arisen from Random Sampling, reprinted in Karl Pearson's Early Statistical Papers, Cambridge 
University Press, pp. 339-57, 1956. 
[Popper, 1957] K. R. Popper. Probability Magic or Knowledge out of Ignorance, Dialectica, 11, 354-
74, 1957. 
[Popper, 1963] K. R. Popper. Conjectures and Refutations. Routledge and Kegan Paul, 1963. 
[Popper,1983] K. R. Popper. Realism and the Aim o/Science. Hutchinson, 1983. 

D. G. MAYO AND M. KRUSE 
PRINCIPLES OF INFERENCE AND THEIR 
CONSEQUENCES 
The likelihood principle emphasized in Bayesian statistics implies, 
among other things, that the rules governing when data collection 
stops are irrelevant to data interpretation. It is entirely appropriate 
to collect data until a point has been proved or disproven ... [Edwards 
etaZ., 1963,p. 193]. 
1 INTRODUCTION 
What do data tell us about hypotheses or claims? When do data provide good 
evidence for or a good test of a hypothesis? These are key questions for a philo-
sophical account of evidence and inference, and in answering them, philosophers 
of science have often appealed to formal accounts of probabilistic and statistical 
inference. In so doing, it is obvious that the answer will depend on the princi-
ples of inference embodied in one or another statistical account. If inference is 
by way of Bayes' theorem, then two data sets license different inferences only by 
registering differently in the Bayesian algorithm. If inference is by way of error 
statistical methods (e.g., Neyman and Pearson methods), as are commonly used in 
applications of statistics in science, then two data sets license different inferences 
or hypotheses if they register differences in the error probabilistic properties of the 
methods. 
The principles embodied in Bayesian as opposed to error statistical methods 
lead to conflicting appraisals of the evidential import of data, and it is this conflict 
that is the pivot point around which the main disputes in the philosophy of statistics 
revolve. The differences between the consequences of these conflicting principles, 
we propose, are sufficiently serious as to justify supposing that one "cannot be 
just a little bit Bayesian" [Mayo, 1996], at least when it comes to a philosophical 
account of inference, but rather must choose between fundamentally incompatible 
packages of evidence, inference, and testing. In the remainder of this section we 
will sketch the set of issues that seems to us to serve most powerfully to bring out 
this incompatibility. 
EXAMPLE 1 (ESP Cards). The conflict shows up most clearly with respect to the 
features of the data generation process that are regarded as relevant for assessing 
evidence. To jump right into the crux of the matter, we can consider a familiar 
type of example: To test a subject's ability, say, to predict draws from a deck of 
five ESP cards, he must demonstrate a success rate that would be very improbable 
if he were merely guessing. Supposing that after a long series of trials, our subject 
381 
D. Corfield and 1. Williamson (eds.). Foundations of Bayesianism. 381-403. 
Â© 2001 Kluwer Academic Publishers. 

382 
D. G. MAYO AND M. KRUSE 
attains a "statistically significant" result, the question arises: Would it be relevant 
to your evaluation of the evidence if you learned that he had planned all along to 
keep running trials until reaching such an improbable result? Would you find it 
relevant to learn that, having failed to score a sufficiently high success rate after 10 
trials, he went on to 20 trials, and on and on until finally, say on trial number 1,030, 
he attained a result that would apparently occur only 5% of the time by chance? 
A plan for when to stop an experiment is called a stopping rule. So our ques-
tion is whether you would find knowledge of the subject's stopping rule relevant 
in assessing the evidence for his ESP ability. If your answer is yes, then you are 
in sync with principles from standard error statistics (e.g., significance testing and 
confidence interval estimation). Interestingly enough, however, this intuition con-
flicts with the principles of inference espoused by other popular philosophies of 
inference, i.e., the Bayesian and Likelihoodist accounts. In particular, it conflicts 
with the likelihood principle (LP). According to the LP, the fact that our subject 
planned to persist until he got the desired success rate, the fact that he tried and 
tried again, can make no difference to the evidential import of the data: the data 
should be interpreted in just the same way as if he had decided from the start that 
the experiment would consist of exactly 1,030 trials. 
This challenge to the widely held supposition that stopping rules alter the import 
of data was L. J. Savage's central message to a forum of statisticians in 1959: 
The persistent experimenter can arrive at data that nominally reject 
any null hypothesis at any significance level, when the null hypothesis 
is in fact true .... 
These truths are usually misinterpreted to suggest that the data of such 
a persistent experimenter are worthless or at least need special inter-
pretation ... The likelihood principle, however, affirms that the ex-
perimenter's intention to persist does not change the import of his 
experience [Savage, 1962, p. 18]. 
Savage rightly took this conflict as having very serious consequences for the foun-
dations of statistics: 
In view of the likelihood principle, all of [the] classical statistical ideas 
come under new scrutiny, and must, I believe, be abandoned or seri-
ously modified [Savage, 1962, pp. 17-18]. 
This conflict corresponds to two contrasting principles on what is required by an 
account of inference, "evidential-relationship" (E-R) principles and "testing". 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
383 
2 
EVIDENTIAL-RELATIONSHIP VS. (ERROR STATISTICAL) TESTING 
ACCOUNTS 
In what we are calling E-R accounts, the evidential bearing of data on hypotheses 
is determined by a measure of support, probability, confirmation, credibility or the 
like to hypotheses given data. Testing approaches, in contrast, do not seek to assign 
measures of support or probability to hypotheses, but rather to specify methods by 
which data can be used to test hypotheses. Probabilistic considerations arise to 
characterize the probativeness, reliability, or severity of given tests, and specific 
inferences they license. 
The difference between E-R and testing approaches is most dramatically re-
vealed by the fact that two data sets x and y may have exactly the same evidential 
relationship to hypothesis H, on a given E-R measure, yet warrant very differ-
ent inferences on testing accounts because x and y arose from tests with different 
characteristics. In particular, the two tests may differ in the frequency with which 
they would lead to erroneous inferences (e.g., passing a false or failing a true hy-
pothesis). That is, the tests may have different error probabilities. We will refer to 
the testing philosophy as the error-statistical approach. 
In statistical approaches to evidence, the main E-R measure is given by the 
probability conferred on x under the assumption that H is correct, P(x; H), i.e., 
the likelihood of H with respect to X.I The LP, informally speaking, asserts that 
the evidential import of x on any two hypotheses, H and H', is given by the ratio 
of the likelihoods of H and H' with respect to x. 
To get a quick handle on the connection between the LP and stopping rules, 
suppose x arose from a procedure where it was decided in advance to take just 
n observations (i.e., n was predesignated), and y arose from our ESP subject's 
'try and try again' procedure, which just happened to stop at trial n (sequential 
sampling). If, for every hypothesis H, P(x; H) = P(y; H), then according to the 
LP it can make no difference to the inference which procedure was used. So, the 
fact that the subject stopped along the way to see if his success rate was sufficiently 
far from what is expected under chance makes no difference to "what the data are 
saying" about the hypotheses. This sentiment is quite clear in a seminal paper by 
Edwards, Lindman and Savage: 
In general, suppose that you collect data of any kind whatsoever -
not necessarily Bernoullian, nor identically distributed, nor indepen-
dent of each other ... -
stopping only when the data thus far collected 
satisfy some criterion of a sort that is sure to be satisfied sooner or 
later, then the import of the sequence of n data actually observed will 
be exactly the same as it would be had you planned to take exactly n 
observations in the first place [Edwards et aI., 1963, pp. 238-2391. 
1 Note that P(x; H) is not a conditional probability usually written as P(xIH) because that would 
involve assigning prior probabilities to H -
something outside the standard error statistical approach. 
The way to read P(x; H) is "The probability that X takes value x according to statistical hypothesis 
H." Any statistical hypothesis H must assign probabilities to the different experimental outcomes. 

384 
D. G. MAYO AND M. KRUSE 
This is called the irrelevance of the stopping rule or the Stopping Rule Principle 
(SRP), and is an implication of the LP.2 
To the holder of the LP, the intuition is that the stopping rule is irrelevant, and it 
is a virtue of the LP that it accords with this intuition. To the error statistician the 
situation is exactly the reverse. For her, the stopping rule is relevant because the 
persistent experimenter is more likely to find data in favor of H, even if H is false, 
than one who fixed the sample size in advance. Peter Armitage, in his comments 
to Savage at the 1959 forum, put it thus: 
I think it is quite clear that likelihood ratios, and therefore posterior 
probabilities, do not depend on a stopping rule. Professor Savage, Dr 
Cox and Mr Lindley take this necessarily as a point in favour of the 
use of Bayesian methods. My own feeling goes the other way. I feel 
that if a man deliberately stopped an investigation when he had de-
parted sufficiently far from his particular hypothesis, then 'Thou shalt 
be misled if thou dost not know that'. If so, prior probability meth-
ods seem to appear in a less attractive light than frequency methods, 
where one can take into account the method of sampling [Armitage, 
1962, p. 72], (emphasis added). 
It is easy enough to dismiss long-run frequencies as irrelevant to interpreting given 
evidence, and thereby deny Armitage's concern, but we think that would miss 
the real epistemological rationale underlying Armitage's argument.3 Granting that 
textbooks on "frequency methods" do not adequately supply the rationale, we pro-
pose to remedy this situation. Holders of the error statistical philosophy, as we see 
it, insist that data only provide genuine or reliable evidence for H if H survives a 
severe test. The severity of the test, as probe of H -
e.g., the hypothesis that our 
ESP subject does better than chance -
depends upon the test's ability to find that 
H is false when it is (i.e., when null hypothesis Ho is true). H is not being put to 
a stringent test when a researcher allows trying and trying again until the data are 
far enough from Ho to reject it in favor of H. This conception of tests provides 
the link between a test's error probabilities and what is required for a warranted in-
ference based on the test. It lets us understand Armitage as saying that one would 
be misled if one could not take into account that two plans for generating data 
correspond to tests with different abilities to uncover errors of concern. 
In the 40 years since this forum, the conflict between Bayesian and "classical" 
or error statistics has remained, and the problems it poses for evidence and infer-
ence are unresolved. Indeed, in the past decade, as Bayesian statistics has grown in 
acceptance among philosophers, the crux of this debate seems to have been largely 
forgotten. We think it needs to be revived. 
2There are certain exceptions (the stopping rule may be "informative"), but Bayesians do not regard 
the examples we consider as falling under this qualification. See section 6.1. 
3This dismissal is the basis of Howson and Urbach's response to Gillies' [1990] criticism of them. 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
385 
3 THE LIKELIHOOD PRINCIPLE (LP) 
The LP is typically stated with reference to two experiments considering the same 
set of statistical hypotheses Hi about a particular parameter, J..L, such as the proba-
bility of success (on a Bernoulli trial) or the mean value of some characteristic. 
According to Bayes' theorem, P(x; J..L) â¢â¢â¢ constitutes the entire evi-
dence of the experiment, that is, it tells all that the experiment has to 
tell. More fully and more precisely, if y is the datum of some other ex-
periment, and if it happens that P(x; J..L) and P(y; J..L) are proportional 
functions of J..L (that is, constant multiples of each other), then each 
of the two data x and y have exactly the same thing to say about the 
values of J..L â¢â¢â¢ I, and others, call this important principle the likelihood 
principle. The function P(x; J..L) -
rather this function together with 
all others that result from it by multiplication by a positive constant 
-
is called the likelihood [Savage, 1962, p. 17]. (We substitute his 
Pr(xIA) with P(x; J..L).) 
The likelihoodfunction gives the probability (or density) of a given observed value 
of the sample under the different values of the unknown parameter(s) such as J..L. 
More explicitly, writing the n-fold sample (Xl, X 2 , â¢.. , Xn) as X, the likelihood 
function is defined as the probability (or density) of {x = (Xl, X2, â¢â¢â¢ ,Xn )} -
arising from the joint distribution of the random variables making up the sample 
X -
under the different values of the parameter(s) J..L. 
Even granting that two experiments may have different error probabilities over 
a series of applications, for a holder of the LP, once the data are in hand, only the 
actual likelihoods matter: 
The Likelihood Principle. In making inferences or decisions about 
p, after x is observed, all relevant experimental information is con-
tained in the likelihood function for the observed x. Furthermore, two 
likelihood functions contain the same information about p, if they are 
proportional to each other (as functions of p,) [Berger, 1985, p. 28]. 
That is, the LP asserts that: 
If two data sets x and y have likelihood functions which are (a) func-
tions of the same parameter(s) J..L and (b) proportional to each other, 
then x and y contain the same experimental information about J..L.4 
4We think this captures the generally agreed upon meaning of the LP although statements may be 
found that seem stronger. For example, Pratt, Raiffa, and Schlaifer characterize the LP in the following 
way: 
If, in a given situation, two random variables are observable, and if the value x of the first 
and the value y of the second give rise to the same likelihood function, then observing the 
value x of the first and observing the value y of the second are equivalent in the sense that 
they should give the same inference, analysis, conclusion, decision, action, or anything 
else ([Pratt et al., 1995, p. 542]; emphasis added). 

386 
D. G. MAYO AND M. KRUSE 
4 
STATISTICAL SIGNIFICANCE LEVELS: TESTING PARAMETERS OF 
BERNOULLI TRIALS 
The error statistical approach is not consistent with the LP because the error sta-
tistical calculations upon which its inferences are based depend on more than the 
likelihood function. This can be seen by considering Neyman-Pearson statistical 
significance testing. 
Significance testing requires identifying a statistical hypothesis Ho that will 
constitute the test or null hypothesis and an alternative set of hypotheses reflect-
ing the discrepancy from Ho being probed. A canonical example is where X := 
(Xl, X 2 , ... ,Xn ) is a random sample from the Bernoulli distribution with param-
eter fl, the probability of success at each trial. In a familiar "coin tossing" situation, 
we test Ho : fl = 0.5, (the coin is "fair") against the claim that J : fl > 0.5. 
Once a null hypothesis is selected, we define a test statistic, i.e., a characteristic 
of the sample X = (Xl, X 2 , ... , Xn) that we are interested in such as X, the 
proportion of successes in n Bernoulli trials. Then, we define a measure of fit or 
distance between the test statistic, and the value of the test statistic expected under 
Ho (in the direction of some alternative hypothesis J). For example, in testing 
hypothesis Ho : fl = 0.5, a sensible distance measure d(X; Ho) is the (positive) 
difference between X and the expected proportion of successes under Ho, 0.5, in 
standard deviation units: 
( 
) (X - 0.5) 
d X;Ho = 
. 
Clg 
Our distance measure may also be set out in terms of the likelihoods of Ho as 
against different alternatives in J. A result x is further from Ho to the extent that 
Ho is less likely than members of J, given x. This distance measure, which we 
may write as d' (X, H) gives us a likelihood ratio (LR). That is: 
d' (X H ) = LR = P(x; Ho) 
, 
0 
P(x; J) 
In the case of composite hypotheses we take the maximum value of the likelihood.5 
No matter which distance measure is used, the key feature of the test is based 
on considering not just the one value of d that happened to occur, but all of the 
possible values. That is, d is itself a statistic that can take on different values in 
repeated trials of the experimental procedure generating the data. This probability 
distribution is called the sampling distribution of the distance statistic, and is what 
allows calculating error probabilities, one of which is the statistical significance 
level (SL): 
50therwise, one would need to have prior probability assignments for each hypothesis within the 
composite alternative. Some strict likelihoodists, who do not use prior probabilities, regard this likeli-
hood as undefined (e.g., [Edwards, 1992; Royall, 1997]). 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
Statistical Significance Level of the observed difference d(x) (in test-
ing Ho) = the probability of a difference as large as or larger than 
d(x), under Ho. 
387 
In calculating the statistical significance level, one sums up the probabilities of 
outcomes as far as or further from Ho as x is. The smaller the significance level, 
the further x is from what is expected under Ho: if it is very small, say 0.05 or 
0.01, then the outcome is said to be statistically significant at these levels. 
To highlight how an analysis by way of significance levels violates the LP, the 
most common example alludes to two different ways one could generate a series 
of n independent (Bernoulli) coin-tossing trials, with J.L the probability of "heads" 
on each trial. 
EXAMPLE 2 (Case 1: The Binomial Distribution). In the first case, it is decided 
in advance to carry out n flips, stop, and record the number of successes, which 
we can represent as random variable Z. Here, Z is a Binomial variable with pa-
rameters J.L and n and the probability distribution of Z takes the form: 
Suppose it is decided to observe n = 12 trials and the observed result is Z = 9 
heads. The probability of this result, under the assumption that J.L = J.Lo is: 
EXAMPLE 2 (Case 2: The Negative Binomial Distribution -
A Case of Sequen-
tial Sampling). In case 2, by contrast, we are to consider that the experimenter was 
interested in the number of heads observed, Z, before obtaining r tails, for some 
fixed value r. In this sampling scheme the random variable Z follows the Negative 
Binomial distribution: 
(2) 
P2(Z = Z;J.L) = (Z+:-l )J.Lz (1-J.Lr. 
This experiment can be viewed as conducting Bernoulli trials with the following 
stopping rule: Stop as soon as you get a total of r tails. We are next to imagine that 
r had been set in advance to 3, and it happens that 9 heads were observed before 
the third tail, thereby allowing the trials to terminate. We then have: 
In each of the two cases above, the data set consists of 9 heads and 3 tails. We 
see immediately that (1) and (2) differ only by a constant. So, a set of z heads and 
r tails in n=z+r Bernoulli trials defines the same likelihood whether by Binomial 

388 
D. G. MAYO AND M. KRUSE 
sampling (n fixed) or Negative Binomial sampling (r fixed). In both cases, the 
likelihood of J.L given z = J.Lz (1- J.L )n-z. According to the LP, then, this difference 
between the two cases makes no difference to what the outcomes tell us about the 
various values of J.L: 
If a Bernoulli process results in z successes in n trials, it has the like-
lihood function J.Lz{1 - J.L)n-z and as far as inferences about J.L are 
concerned, it is irrelevant whether either n or r was predetermined 
([Pratt et al., 1995, p. 542]. We replace their p with J.L for consistency 
of notation). 
Nevertheless, as the holder of the LP goes on to show, the significance level at-
tained in case 1 differs from that of case 2, thereby showing that significance levels 
violate the LP. In particular, we have 
(i) The statistical significance level for the Binomial (n fixed at 12)= 
PI (Z ~ 9j Ho : J.L = 0.5) = PI (z = 9 or 10 or 11 or 12j J.L = 0.5) ~ 0.075 
whereas 
(ii) The significance level for the Negative Binomial (r fixed at 3)= 
P2(z = 9 or 10 or .... j J.L = 0.5) ~ 0.0325. 
Thus, if the level of significance before rejecting Ho were fixed at 0.05, we 
would reject Ho if the observations were the result of Binomial trials, but we would 
not reject it if those same observations were the result of Negative Binomial trials. 
5 THE OPTIONAL STOPPING EFFECT 
Although the contrasting analysis demanded by the error statistician in considering 
the Binomial vs. the Negative Binomial (Example 2) was not very pronounced, 
the example we used at the opening of our paper points to much more extreme 
contrasts. 
An example that has received considerable attention is of the type raised by Ar-
mitage at the "1959 Savage Forum." The null hypothesis Ho is an assertion about 
a population parameter J.L, the mean value of some quantity, say, a measure of the 
effectiveness of some medical treatment. The experiment involves taking a ran-
dom sample of size n, Xl, ... Xn and calculating its mean. Let Xn be the sample 
mean of the n observations of the XiS, where we know that each Xi is distributed 
Normally with unknown mean J.L and known variance 1, i.e., Xi '" Normal(J.L,1). 
The null hypothesis asserts the treatment has no effect: 
Ho :J.L=O. 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
389 
The alternative hypothesis HI is the complex hypothesis consisting of all values 
of J..L other than 0: 
As before, we are to consider two different stopping rules: 
EXAMPLE 3 (Case 1: Test T-l (fixed sample size)). In this case we take n sam-
ples, evaluate the distance between the observed mean, xn, and the mean hypoth-
esized in Ho, namely 0, and then calculate the SL of this difference. For example, 
if xn is 2 standard deviation units from 0, then the SL is approximately 0.05, re-
gardless of the true value of the mean.6 This is the nominal (or computed) SL. 
A familiar test rule is to reject Ho whenever the SL reaches some level, say 
0.05. This test rule can be described as follows: 
Test T-l: Reject Ho at SL = 0.05 iff IXnl 2: 2/VTi. 
The standard deviation of Xn in this example is 1/..;n. We have 
P(Test T-l rejects Ho; Ho) = 0.05. 
Since rejecting Ho when Ho is true is called the type I error of the test, we can 
also say 
P(Test T-l commits a type I error) = 0.05. 
EXAMPLE 3 (Case 2: Test T-2 (Sequential testing)). In the second case sample 
size n is not fixed in advance. The stopping rule is: 
(T-2) Keep sampling until Xn is 2 standard deviations away from 0 (the hypothe-
sized value of J..L in Ho) in either direction. 
So we have 
(T-2) Keep sampling untillXnl 2: 2/..;n. 
The difference between the two cases is that in T-2 the tests are applied sequen-
tially. If we have not reached a 2 standard deviation difference after, say, the first 
10 trials, we are to go on to take another 10 trials, and so on, as in the "try and try 
again" procedure of Example 1. The more generalized stopping rule T-2 for the 
Armitage example is: 
Keep sampling untillXnl 2: ka/..;n 
where ka is the number of standard deviations away from 0 that corresponds to 
a (nominal) SL of Ct. The probability that this rule will stop in a finite number 
of trials is one, no matter what the true value of J..L is; it is what is called a proper 
stopping rule. 
6That is because we here have a two-sided test. 

390 
D. G. MAYO AND M. KRUSE 
Table 1. The Effect of Repeated Significance Tests (the "Try and Try Again" 
Method) 
Probability of rejecting Ho with a result nominally 
Number of trials n 
significant at the 0.05 level at or before n trials, 
given H 0 is true 
1 
0.05 
2 
0.083 
10 
0.193 
20 
0.238 
30 
0.280 
40 
0.303 
50 
0.320 
60 
0.334 
80 
0.357 
100 
0.375 
200 
0.425 
500 
0.487 
750 
0.512 
1000 
0.531 
Infinity 
1.000 
Nominal SL VS. Actual SL 
The probability that Test T-2 rejects Ho even though Ho is true -
the probability 
it commits a type I error -
changes according to how many sequential tests are 
run before we are allowed to stop. Because of this, there is a change in the actual 
significance level. 
Suppose it takes 1000 trials to reach the 2-standard deviation difference. The 
SL for a 2-standard deviation difference, in Case 1, where n was fixed, would be 
0.05, the computed or nominal significance level. But the actual probability of 
rejecting Ho when it is true increases as n does, and so to calculate the actual SL, 
we need to calculate: 
P(Test T-2 stops and rejects Ho at or before n=1000; Ho is true). 
That is, the actual or overall significance level is the probability of finding a 
0.05 nominally statistically significant difference from a fixed null hypothesis at 
some stopping point or other up to the point at which one is actually found. In 
other words, in sequential testing, the actual significance level accumulates, a fact 
reflected in Table 1. 
While the nominal SL is 0.05, the actual SL for Case 2 is about 0.53: 53% of 
the time Ho would be rejected even though it is true. More generally, applying 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
391 
stopping rule T-2 would lead to an actual significance level that would differ from, 
and be greater than, a (unless it stopped at the first trial). If allowed to go on long 
enough, the probability of such an erroneous rejection is one!7 
By contrast, as Berger and Wolpert note: 
The SRP would imply, [in the Armitage example], that if the observa-
tion in Case 2 happened to have n = k, then the evidentiary content 
of the data would be the same as if the data had arisen from the fixed 
[kJ sample size experiment in Case 1 [Berger and Wolpert, 1988, p. 
76]. 
So, in particular, if n = 1000, there would be no difference in "the evidentiary 
content of the data" from the two experiments. 
Now holders of the LP do not deny that the actual significance levels differ 
dramatically, nor do error statisticians deny that alternative hypothesis J.t = x is 
more likely than the null hypothesis J.t = O. Where the disputants disagree is with 
respect to what these facts mean for the evidential import of the data. Specifically, 
the error statistician's concern for the actual and not the nominal significance level 
in such cases leads her to infer that the stopping rule matters. 
In contrast, the fact that the likelihood ratio is unaffected leads the proponent of 
the LP to infer that there is no difference in the evidential import, notwithstanding 
the difference in significance levels. Thus, according to the intuitions behind the 
LP, it is a virtue of a statistical account that it reflects this. 
This irrelevance of stopping rules to statistical inference restores a 
simplicity and freedom to experimental design that had been lost by 
classical emphasis on significance levels (in the sense of Neyman and 
Pearson) ... [Edwards et al., 1963, p. 239]. 
We can grant some simplicity is lost, but that is because the error probability assur-
ances are lost if one is allowed to change the experiment as one goes along, without 
reporting the altered significance level. Repeated tests of significance (or sequen-
tial trials) are permitted -
are even desirable -
in many situations. However, 
the error statistician requires that the interpretation of the resulting data reflect the 
fact that the error characteristics of a sequential test are different from those of 
a fixed-sample test. In effect, a penalty must be paid for perseverance. Before-
trial planning stipulates how to select a small enough nominal significance level to 
compute at each trial so that the actual significance level is stil1low.8 By contrast, 
since data x enter the Bayesian computation by means of the likelihood function, 
identical likelihood functions yield identical assignments of posterior probability 
or density -
so no alteration is required with the two stopping rules, according to 
theLP. 
7Feller [1940] is the first to show this explicitly. 
8Medica1 trials, especially, are often deliberately designed as sequential. See [Armitage, 1975]. 

392 
D. G. MAYO AND M. KRUSE 
This leads to the question whether Bayesians are not thereby led into a situation 
analogous to the one that error statisticians would face were they to ignore the 
stopping rule. 
EXAMPLE 3 (continued). Armitage continued his earlier remarks to Savage at 
the 1959 forum as follows: 
[Savage] remarked that, using conventional significance tests, if you 
go on long enough you can be sure of achieving any level of signif-
icance; does not the same sort of result happen with Bayesian meth-
ods? The departure of the mean by two standard errors corresponds 
to the ordinary five per cent level. It also corresponds to the null hy-
pothesis being at the five per cent point of the posterior distribution. 
Does it not follow that by going on sufficiently long one can be sure 
of getting the null value arbitrarily far into the tail of the posterior 
distribution? ([Armitage, 1962, p. 72]; (emphasis added). 
That is, if we consider in Armitage's example the "uninformative" prior distri-
bution of fL, uniform over (-00, +00) and given that (12 = 1, then the posterior 
distribution for fL will be: 
Normal (xn, l/n). 
The methods that Bayesians use to draw inferences about fL all depend on this 
posterior distribution in one way or another. 9 One common method of Bayesian 
inference involves using x to form an interval of fL values with highest posterior 
density, the "highest posterior density" (HPD) interval. In this case, the (approxi-
mate) 0.95 HPD interval will be Cn(x) = (x - 2/..;n, x + 2/..fii). The Armitage 
stopping rule allows us to stop only when IXnl > 2/..;n, and so that stopping rule 
insures that fL = 0 is excluded from the HPD, even if fL = 0 is true. 
As even some advocates of the LP note, this looks very troubling for the Bayesian: 
The paradoxical feature of this example is that " . the experimenter 
can ensure that Cn(x) does not contain zero; thus, as a classical con-
fidence procedure, {Cn(x)} will have zero coverage probability at 
[fL = 0] .... It thus seems that the experimenter can, through sneaky 
choice of the stopping rule, "fool" the Bayesian into believing that [fL] 
is not zero [Berger, 1985, p. 507]. 
That is, (using the non-informative prior density) the use of the stopping rule in 
T-2 ensures the Bayesian will accord a high posterior probability to an interval 
that excludes the true value of fL. Rather than use the HPD intervals, the analogous 
9That this uninformative prior results in posteriors that match the values calculated as error prob-
abilities is often touted by Bayesians as a point in their favor. For example, where the most an error 
statistician can say is that a confidence interval estimator contains the true value of J.I. 95% of the 
time, the Bayesian, with his uniform prior, can assign .95 posterior probability to the specific interval 
obtained. 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
393 
point can be made in reference to Bayesian hypothesis testing. to Nor can one just 
dismiss the issue by noting the obvious fact that the probability for any value of the 
continuous parameter is zero. Bayesians supply many procedures for inferences 
about continuous parameters, and the issue at hand arises for each of them. One 
procedure Bayesians supply is to calculate the posterior probability of a small 
interval around the null, (O-t:, O+t:). With t: small enough, the likelihood is constant 
in a neighborhood of 0, so the posterior probability obtained from the Armitage 
stopping rule (T-2) will be very low for (O-t:, O+t:), even if J.L = O. And since T-2 is 
a proper stopping rule, such a low posterior for a true interval around 0 is assured. 
In discussions of Armitage's example, most of the focus has been on ways to 
avoid this very extreme consequence-the guarantee (with probability 1) of arriving 
at an HPD interval that excludes the true value, J.L = 0, or a low posterior density 
to a true null hypothesis. For example, because the extreme consequence turns on 
using the (improper) uniform prior, some Bayesians have taken pains to show that 
this may be avoided with countably additive priors (e.g., [Kadane et ai., 1999]).11 
Nevertheless, the most important consequence of the Armitage example is not 
so much the extreme cases (where one is guaranteed of strong evidence against 
the true null) but rather the fact that ignoring stopping rules can lead to a high 
probability of error, and that this high error probability is not reflected in the inter-
pretation of data according to the LP. Even allowing that the Bayesians have ways 
to avoid the extreme cases, therefore, these gambits fail to show how to adhere to 
the LP and avoid a high probabilities of strong evidence against a true null. 
To underscore this point, consider a modified version of T-2: the experimenter 
will make at most 1000 trials, but will stop before then if Xn falls more than 2 
standard deviations from zero. This modified rule (while also proper) does not 
assure that when one stops one has IXn I 2: 2/.;n. Nevertheless, were our exper-
imenter to stop at the lOOOth trial, the error probability is high enough (over 0.5) 
to be disturbing for an error statistician. (See Table 1.) So the error statistician 
would be troubled by any interpretation of the data that was not altered by dint of 
this high error probability (due to the stopping rule). Followers of the LP do not 
regard this stopping rule as altering the interpretation of the data -
whatever final 
form of evidential appraisal or inference they favor. None of the discussions of the 
Armitage example address this consequence of the less extreme cases. 
lOHPDs are not invariant under one-one transformations of the parameter space [Berger, 1985, p. 
1441. Some Bayesians find this a compelling reason to avoid HPDs altogether, but this method never-
theless is commonly used. 
11 One might propose that after the first observation, one could use the result to arrive at a new 
countably additive prior. But this altering of the prior so that the so-called "foregone conclusion" is 
avoided is not the Armitage example anymore, and so does not cut against that example which concerns 
an after-trial analysis of the data once one stops. 

394 
D. G. MAYO AND M. KRUSE 
6 REACTIONS TO THE CONSEQUENCES OF THE LP 
For the most part, holders of the LP have not shirked from but have applauded 
the fact that the inferential consequences of the LP conflict with those of error 
statistical principles. Indeed, those who promote Bayesianism over error statistical 
approaches often tout the fact that stopping rules (and other aspects of the data 
generation procedure) do not alter the Bayesian's inference. 
At the same time, however, many Bayesians and other holders of the LP are 
plainly uncomfortable with the fact that the LP can lead to high error probabili-
ties and attempt to deny or mitigate this consequence. We do not think that any 
existing attempts succeed. Before explaining why, we should emphasize that the 
consequences of the Armitage-style stopping rule example are not the only ways 
that adherence to the LP conflicts with the control of error probabilities. Because 
of this conflict, many have rejected the LP -
including some who at first were 
most sympathetic, most notably Allan Birnbaum, who concluded that 
It seems that the likelihood concept cannot be construed so as to al-
low useful appraisal, and thereby possible control, of probabilities of 
erroneous interpretations [Birnbaum, 1969, p. 128].12 
Therefore, in our view, a strategy to block high probabilities of erroneous inter-
pretations as a result of stopping rules will not do unless it can be demonstrated 
that: 
1. It is part of a complete account that blocks high probabilities of erroneous in-
ferences (whatever the form of inference or evidential appraisal the account 
licenses.) 
2. It is not merely ad hoc. There must be a general rationale for the strategy 
that is also consistent with the LP. 
6.1 
Can the Stopping Rule Alter the Likelihood Function? 
Upon first hearing of the Armitage example, one might assume that the stopping 
rule T-2 must make some kind of difference to the likelihood function. This is 
especially so for those inclined to dabble informally with likelihoods or Bayes' 
Theorem apart from any explicit mathematical definition of the likelihood func-
tion. We know of no formal statistical treatment of the Armitage example that 
has seriously claimed that the two stopping rules imply different likelihood func-
tions. (Other types of strategies are proposed, which we will consider.) But these 
informal intuitions are important, especially for philosophers seeking an adequate 
account of statistical inference. 
12See Birnbaum [J961; 1962; 1972), Giere [J977J, as well as citations in [Barnard and Godambe, 
1982) and [Bjornstad, 1992). 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
395 
To begin with, it is worth noting that there are other kinds of situations in which 
stopping rules will imply different likelihood functions. These are known as infor-
mative stopping rules, an example of which is given by Edwards, Lindman, and 
Savage: 
A man who wanted to know how frequently lions watered at a certain 
pool was chased away by lions before he actually saw any of them 
watering there; in trying to conclude how many lions do water there 
he should remember why his observation was interrupted when it was 
[Edwards et al., 1963, p. 239], 
Although a more realistic example might seem more satisfactory, in fact, it is 
apparently very difficult to find a realistic stopping rule that is genuinely informa-
tive. (For a discussion, see [Berger and Wolpert, 1988, pp. 88-90],) As Edwards, 
et al., then add: "We would not give a facetious example had we been able to think 
of a serious one." In any event, this issue is irrelevant for the Armitage-type exam-
ple because T-2 is not an informative stopping rule. Although the probability of 
deciding to take more observations at each stage depends on x, it does not depend 
on the parameter J.L under test. 13 
Nevertheless, we are willing to address those who assume that an informal or 
subjectivist construal of probabilities gives them a legitimate way to alter the like-
lihood based on the stopping rule T-2. But to address them we need more than 
their intuitive hunch, they need to tell us in general how we are to calculate the 
likelihoods that will be needed, whether the account is purely likelihoodist (e.g., 
Royall [1992; 19971) or Bayesian. Are we to substitute error probabilities in for 
likelihoods? Which ones? And how will this escape the Bayesian incoherence to 
which error probabilities such as significance levels are shown to lead? 
To see that any such suggested alteration of likelihoods runs afoul of the LP, it 
must be remembered that the likelihood is a function of the observed x: 
The philosophical incompatibility of the LP and the frequentist view-
point is clear, since the LP deals only with the observed x, while 
frequentist analyses involve averages over possible observations. ... 
enough direct conflicts have been... seen to justify viewing the LP 
as revolutionary from a frequentist perspective [Berger and Wolpert, 
1988, pp. 65-66]. 
Once the data x are in hand, the holder of the LP insists on the "irrelevance of the 
sample space" -
the irrelevance of the other outcomes that could have occurred 
but did not when drawing inferences from x (e.g., [Royall, 19971). This is often 
13 As Berger and Wolpert [I988, p. 90] observe, the mere fact that the likelihood function depends 
on N, the number of observations until stopping, does not imply that the stopping rule is informative: 
"Very often N will carry information about [the parameter], but to be informative a stopping rule must 
carry information about [the parameter] additional to that available in [the sample Xl, and this last 
will be rare in practice" (ibid., 90). For further discussion of informative stopping rules, see [Roberts, 
19671. 

396 
D. G. MAYO AND M. KRUSE 
expressed by saying the holder of the LP is a conditionalist: for them inferences 
are always conditional on the actual value x. 
With respect to stopping rules, the conditionalist asks: Why should our inter-
pretation of the data in front of us, x, depend upon what would have happened if 
the trials were stopped earlier than they actually were stopped? 
Those who do not accept the likelihood principle believe that the prob-
abilities of sequences that might have occurred, but did not, somehow 
affect the import of the sequence that did occur [Edwards et al., 1963, 
p.238]. 
But altering the likelihood because of the stopping rule is to take into account 
the stopping plan, e.g., that if he hadn't gotten a significant result at 10 trials, 
he would have continued, and so on, thereby violating the LP. So anyone who 
thinks a subjectivist or informal construal of likelihoods gives them a legitimate 
way out, must be aware of this conflict with the conditionalist principle. Certainly 
this would put them at odds with leading subjective Bayesians who condemn error 
statisticians for just such a conflict: 
A significance test inference, therefore, depends not only on the out-
come that a trial produced, but also on the outcomes that it could have 
produced but did not. And the latter are determined by certain pri-
vate intentions of the experimenter, embodying his stopping rule. It 
seems to us that this fact precludes a significance test delivering any 
kind of judgment about empirical support ... For scientists would not 
normally regard such personal intentions as proper influences on the 
support which data give to a hypothesis [Howson and Urbach, 1993, 
p.212]. 
Thus, the intuition that the stopping rule should somehow alter the likelihood is 
at odds with the most well-entrenched subjective Bayesian position and constitutes 
a shift toward the error statistical (or "frequentist") camp and away from the central 
philosophy of evidence behind the LP. According to the LP philosophy: 
[I]t seems very strange that a frequentist could not analyze a given set 
of data, such as (Xl, ... , xn) [in Armitage's example] if the stopping 
rule is not given .... data should be able to speak for itself [Berger and 
Wolpert, 1988, p. 78]. 
We say the shift is to the error statistical camp because it reflects agreement with 
the error statistician's position that one cannot properly 'hear' what the data are 
saying without knowing how they were generated -
whenever that information 
alters the capabilities of the test to probe errors of interest, as in the case of stop-
ping rules. It is precisely in order to have a place to record such information that 
Neyman and Pearson were led to go beyond the likelihood ratio (LR): 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
If we accept the criterion suggested by the method of likelihood it is 
still necessary to determine its sampling distribution in order to con-
trol the error involved in rejecting a true hypothesis, because a knowl-
edge of [the LR] alone is not adequate to insure control of this error 
[Pearson and Neyman, 1930, p. 106]. 
397 
When test T-2 stops, it is true that the LR (in favor of Ho) is small. However, to 
the error statistician, we cannot thereby infer we should be Justified in rejecting the 
hypothesis Ho, because: 
In order to fix a limit between 'small' and 'large' values of[LR] we 
must know how often such values appear when we deal with a true 
hypothesis. That is to say we must have knowledge of ... the chance 
of obtaining [LR as small or smaller than the one observed] in the case 
where [Ho] is true (ibid, p. 106). 
Accordingly, without the error probability assessment, Pearson and Neyman are 
saying we cannot determine if there really is any warranted evidence against HO.14 
Stopping rules give crucial information for such an error statistical calculation. 
It is no surprise, then, that the error statistician regards examples like Armitage's 
as grounds for rejecting the LP. To those who share the error statistical intuitions, 
our question is: on what grounds can they then defend the LP? 
6.2 
Can Stopping Rules Alter the Prior? 
In order to avoid assigning the high posterior to a false non-null hypothesis, as 
Berger and Wolpert (1988) point out, "the Bayesian might ... assign some positive 
prior probability, A, to J.t being equal to zero" (p. 81) perhaps to reflect a suspicion 
that the agent is using stopping rule T-2 because he thinks the null hypothesis is 
true. 15 Assume, for example, that one assigns a prior probability mass of 0.50 to 
the null hypothesis and distributes the rest Normally over the remaining values of 
141t should be emphasized that it is not that the N-P inference consists merely of a report of the 
significance level (or other error probabilities), at least not if the tests are being used for inference 
or evidence. It is rather that determining the warranted inference depends on the actual significance 
level and other error probabilities of tests. Granted, the onus is on the error-statistician to defend a 
philosophy of inference that uses and depends on controlling error probabilities (though this is not our 
concern here). See Note 22. 
IS A positive prior probability, A, can be assigned to /-I = 0 and the rest, 1 - A, distributed over the 
remaining values of /-I. (This amounts to giving /-I = 0 a non-zero mass, and every other hypothesis 
zero mass.) When 1 - A is distributed Normally over the remaining hypotheses with mean 0 and 
variance p2, the posterior probability distribution will be: 
( 
K ) 
[ 
(1 ) 1 
~]-1 
P 
/-I = O/Xn = -
= 1 + - - 1 
e 2(1+np ) 
..;n 
A 
J(l + np2) 
where K is the number of standard deviations stipulated in the stopping rule and n is the number of 
observations needed to stop [Berger and Wolpert, 1988, p. 81]. 
See also [Berger and Berry, 1987l, [Smith, 1961, p. 36-37l. 

398 
D. G. MAYO AND M. KRUSE 
fL. If it takes n = 1000 trials to stop, the posterior probability assignment to fL = 0 
is no longer low, but rather, around 0.37. A virtue of such a prior, as Berger and 
Wolpert note, is that it results in an increasing posterior probability assignment to 
fL = 0 as the number of trials before stopping increases. For example, with this 
prior and n = 10,000, the posterior for the null is about 0.65. 
Granted, in a case where one had this prior, the low posterior assignment to 
the null hypothesis is avoided, but this does nothing to mitigate the problem as it 
arises with the uniform prior -
a prior the Bayesian often advocates. Perhaps the 
Bayesian would wish to suggest that whenever one is confronted with an experi-
ment with stopping rule T-2, one should reject the uniform prior in favor of one 
that appears to avoid the problem. But why should a Bayesian alter the prior upon 
learning of the stopping rule? 
There is the motivation suggested by Berger and Wolpert [1988], that if you 
suspected that the person generating the observations was using stopping-rule T-2 
for the purpose of misleading you, you would raise your prior probability assign-
ment to fL = O. Does this not violate the LP? Perhaps one could retain the LP on 
the grounds that one is only allowing the stopping-rule to affect the prior rather 
than the likelihoods (and hence not "what the data say"). 16 
Nevertheless, a Bayesian should have serious objections to this response to the 
stopping rule problem. Why, after all, should we think that the experimenter is us-
ing T-2 to deceive you? Why not regard his determination to demonstrate evidence 
against the null hypothesis as a sign that the null isfalse? Perhaps he is using T-2 
only because he knows that fL f= 0 and he is trying to convince you of the truth! 
Surely it would be unfair to suppose that those who, like Savage, touted the 
irrelevance of the stopping rule were sanctioning deception when they asserted: 
"Many experimenters would like to feel free to collect data until they have either 
conclusively proved their point, [or] conclusively disproved it" [Edwards et al., 
1963, p. 239]. Plainly, what they meant to be saying is that there is no reason to 
interpret the data differently because they arose from optional stopping. Equating 
optional stopping (with rule T-2) with deception runs counter to Savage's insis-
tence that, because "optional stopping is no sin," any measure that is altered by the 
stopping rule, such as the significance level, is thereby inappropriate for assessing 
evidence [Savage, 1964, p. 185].17 
Those who advocate the above move, then, should ask why the sensitivity of 
significance levels to stopping rules violates the LP -
and thus is a bad thing -
but the same kind of sensitivity of priors is acceptable. The LP, after all, asserts that 
all the information contained in the data that is relevant to comparisons between 
different parameter values is given in the likelihood function. But what else could 
16This 'solution' demands that the agent know not only the stopping-rule used, but why the experi-
menter chose that particular stopping-rule, since knowing he wanted to deceive rather than to help you 
could make all the difference in the prior you use. Yet Bayesians have delighted in the fact that the LP 
renders irrelevant the intentions of experimenters to the import of the experiment. 
17There is nothing in the LP to prevent Bayesians from deciding in advance to prohibit certain kinds 
of stopping rules, but again, one would like to know why. 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
399 
it mean to say that one's choice of priors depends on the stopping-rule other than 
that the stopping-rule contains information relevant to comparisons between values 
of J.t? It is little wonder that many Bayesians have balked at allowing the stopping 
rule to alter one's prior probability: "Why should one's knowledge, or ignorance, 
of a quantity depend on the experiment being used to determine it" [Lindley, 1972, 
p. 71]. Why indeed?18 
Finally, even if we put aside the question of stopping rules leading to problem-
atic final posterior probabilities, as long as the Bayesian conceives of likelihood as 
determining "what the data have to say", it is still the case that the data from T-2 
are regarded as much stronger support for the non-null than the null, according to 
the Bayesian criterion of support. 19 
6.3 Does the LP Provide Bounds on Being Misled? 
A third kind of response grants that the stopping rule makes no difference at all 
to either the likelihood function or the priors, and instead attempts to argue that, 
nonetheless, one who holds the LP can avoid having a high probability of being 
misled by the data. This argument is sound only for tests that differ in essential 
ways from the type leading to the Armitage result. Nevertheless, this response is 
important, if only because it is the one first put forward by Savage in responding 
to Armitage (Savage 1962).20 
"Let us examine first a simple case" Savage proposes, where we are testing 
a simple or point null hypothesis Ho against a point alternative HI: that is Ho 
asserts J.t = J.to, and the alternative HI asserts J.t = J.tI. Call this a point against 
point test. In that case, if one is intent on sampling until the likelihood ratio (LR) 
in favor of HI exceeds T (for any value of T > 1), it can be shown that if Ho is 
true, the probability is only liT that one will succeed in stopping the trials. This 
response turns on the fact that when we have a (true) simple null Ho against a 
simple alternative HI, then there is an upper bound to the probability of obtaining 
a result that makes HIT times more likely than Ho, namely, liT, i.e. P(LR > 
T;Ho) ~ liT. 
18Lindley is referring to Bayesians like Jeffreys [19611 and Rosenkrantz [19771 who determine 
'objective' or 'non-subjective' priors by appealing to formal information-theoretic criteria. They 
would, for example, recommend different priors in the Binomial vs. the Negative Binomial case 
[Box and Tiao, 1973]. Doing so apparently violates the LP, and has led many Bayesians to be 
suspicious of such priors [Hill, 1987; Seidenfeld, 19791, or even to declare that "no theory which 
incorporates non-subjective priors can truly be called Bayesian, and no amount of wishful think-
ing can alter this reality" (Dawid, in [Bernardo, 1997, p. 1791). 
For related discussions con-
trasting subjective and objective priors, see also [Akaike, 1982; Barnett, 1982; Bernardo, 1979; 
Bernardo, 19971. 
19This point does not rely on the technical Bayesian definition of "support" as an increase in the 
posterior, but holds for any conception based on the likelihood, e.g., weight of evidence [Good, 19831. 
Bayesians who reject all such notions of Bayesian support need to tell us what notion of support or 
evidence they condone. 
20It is also the first one mentioned by many defenders of the LP, e.g., [Berger and Wolpert, 1988; 
Oakes, 1986; Royall, 19971. 

400 
D. G. MAYO AND M. KRUSE 
This impressively small upper bound, however, does nothing to ameliorate the 
consequences of the Armitage optional stopping example because that example is 
not a case of a point against point test. 21 
6.4 
Extrapolating From Our Intuitions in Simple Cases 
The simple case of testing "point against point" hypotheses has encouraged some 
to suppose that the LP offers such protection in all cases -
yet it does not. Perhaps 
the tendency to turn to the point against point test when confronted with stopping 
rule problems explains why the Armitage-type consequence has not received more 
attention by Bayesians. But there seems to be a different kind of strategy often at 
work in alluding to the point against point test in defending the LP, and we may 
regard this as a distinct response to the stopping rule problem. 
In appraising the LP, say some, we should trust our intuitions about its plau-
sibility when we focus on certain simple kinds of situations, such as testing point 
against point hypotheses, "rather than in extremely complex situations such as [Ar-
mitage's example]" [Berger and Wolpert,.1988, p. 83]. Since looking at just the 
likelihood ratio (and ignoring the stopping rule) seems intuitively plausible in point 
against point testing, they urge, it stands to reason that the LP must be adhered to 
in the more 'complex situation' -
even if its consequences in the latter case seem 
unpalatable. Regarded as an argument for deflecting the Armitage example it is 
clearly unsound. Bracketing a whole class of counterexamples simply on the basis 
that they are "extremely complicated" is ad hoc -
preventing the LP from being 
subject to the relevant kind of test here. Moreover, such sequential tests are hardly 
exotic, being standard in medicine and elsewhere. 
But perhaps it is only intended as a kind of pragmatic appeal to what is imag-
ined to be the lesser of two evils: their reasoning seems to be that even if the LP 
leads to unintuitive consequences in the complex (optional stopping) case, its re-
jection would be so unappealing in the simple cases that it is better to uphold the 
LP and instead discount our intuitions in the complex cases. By contrast, some 
have gone the route of George Barnard -
the statistician credited with first artic-
ulating the LP [Barnard, 1949] -
who confessed at the 1959 Savage Forum that 
the Armitage-type example led him to conclude that whereas the LP is fine for 
the simple cases it must be abandoned in the more complex ones (see [Barnard, 
1962]). The LP adherent owes us an argument as to why Barnard's move should 
not be preferred. 
21 The existence of an upper bound less than I can also be shown in more general cases such as when 
deling with k simple hypotheses, though as k increases, the upper bound is no longer impressively 
small. The general result, stated in [Kerridge, 19631 is that with k + 1 simple hypothses where Ho is 
true and HI"", Hk are false and Pr(H;) = (k + 1)-1 for i = 0,1, ... , k: 
kp 
P(P(Ho/Xn ) ::; p)::; (1- p)' 
Moreover, such bounds depend on having countably addititve probability, while the uniform prior in 
Armitage's example imposes finite additivity. 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
401 
7 CONCLUDING REMARKS 
Philosophers who appeal to principles and methods from statistical theory in tack-
ling problems in philosophy of science need to recognize the consequences of the 
statistical theory they endorse. Nowhere is this more crucial than in the on-going 
debate between Bayesian and non-Bayesian approaches to scientific reasoning. 
Since Bayesianism -
which is committed to the LP -
has emerged as the dom-
inant view of scientific inference among philosophers of science, it becomes all 
the more important to be aware of the LP's many implications regarding evidence, 
inference and methodology. 
Some of the most important of these implications concern the LP's effect on our 
ability to control error and thereby the reliability and severity of our inferences and 
tests -
generally regarded as important goals of science. A consequence of our 
discussion is that there is no obvious way in which approaches consistent with the 
LP can deliver these goods. In giving the spotlight to the kind of unreliability that 
can result from ignoring stopping rules, our goal is really to highlight some of the 
consequences for reliability of accepting the LP, not to argue that examples such 
as Armitage's are common. At the same time, however, it should be realized that 
examining the effect of stopping rules is just one of the ways that facts about how 
the data are generated can affect error probabilities. Embracing the LP is at odds 
with the goal of distinguishing the import of data on grounds of the error statistical 
characteristics of the procedure that generated them. 
Now Bayesians and likelihoodists may deny that this appeal to error probabili-
ties is what matters in assessing data for inference. They often deny, for example, 
that the error statistician's concern with the behavior of a test in a series of rep-
etitions is relevant for inference.22 Strict adherence to this position would lead 
one to expect that they would be unfazed by the Armitage result. In reality, how-
ever, existing Bayesian and Likelihoodist reactions to Armitage-type examples are 
strikingly and surprisingly equivocal, and the Bayesian attempts to deflect the Ar-
mitage result have been unclear e.g. see [Johnstone et at., 1986]. Sometimes they 
say "It's not a problem, we do not care about error rates", while at other times 
the claim is "Even though we don't care about error rates, we can still satisfy one 
who does." The former response is consistent for a holdr of the LP, but it demands 
renouncing error probabilities, as we understand that notion. The latter attitude 
demands an argument showing how to resolve the apparent tension with the LP. 
We have tried to locate the most coherent and consistent arguments, and found that 
22The long-standing challenge of how to interpret error statistical tests "evidentially" cannot be 
delved into here, but we can see the directions in which such an interpretation (or reinterpretation) 
might take us, by extending what we said about why the error statistician regards the stopping rule as 
relevant. The error statistician regards data as evidence for a hypothesis H to the extent that H has 
passed a reliable or severe test of H, and this requires not just that H fit x but also that test T would 
very probably not have resulted in so good a fit with H were H false or specifiably in error. See [Mayo, 
2000], [Mayo and Spanos, 2000]. By contrast, the Armitage stopping rule makes it maximally probable 
that x fits a false H, so H passes a test with minimal severity. 

402 
D. G. MAYO AND M. KRUSE 
they failed to live up to this demand. We invite anyone who can further clarify the 
Bayesian and Likelihoodist position on the Armitage example to do so. 
ACKNOWLEDGEMENTS 
We are indebted to Aris Spanos for numerous, highly important statistical insights 
regarding the Armitage case. We thank Teddy Seidenfeld, and the participants of 
D. Mayo's 1999 National Endowment for the Humanities Summer Seminar, for 
a number of challenging questions, criticisms, and suggestions regarding earlier 
drafts. D. Mayo gratefully acknowledges support for this research from the Na-
tional Science Foundation, grant no. SBR-9731505. 
Virginia Tech, USA. 
BIBLIOGRAPHY 
[Akaike, 1982] H. Akaike. On the fallacy of the likelihood principle. Statistics and Probability Letters 
1,75-78, 1982. 
[Armitage, 1962] P. Armitage. Contribution to discussion in L. Savage, ed. 1962. 
[Armitage, 1975] P. Armitage. Sequential Medical Trials. Oxford: Blackwell, 1975. 
[Barnard, 1949] G. A. Barnard. Statistical inference. Journal of the Royal Statistical Society, Series B 
(Methodologial), 11, 115-149, 1949. 
[Barnard, 1962] G. A. Barnard. Contribution to discussion in L. Savage, ed. 1962. 
[Barnard and Godambe, 1982] G. A. Barnard and V. P. Godambe. Memorial article: Allan Birnbaum 
1923-1976. The Annals of Statistics 10, 1033-1039, 1982. 
[Barnett, 1982] V. Barnett. Comparative Statistical Inference, 2nd edition. John Wiley, New York 
1982. 
[Berger, 1985] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. 2nd edition. Springer-
Verlag, New York, 1985. 
[Berger and Berry, 1987] J. O. Berger and D. A. Berry. The relevance of stopping rules in statistical 
inference. In Statistical Decision Theory and Related Topics IV, vol. 1, S. S. Gupta and 1. Berger, 
eds. Springer-Verlag, 1987. 
[Berger and Wolpert, 1988] J. O. Berger and R. L. Wolpert. The Likelihood Principle, 2nd edition. 
Institute of Mathematical Statistics, Hayward, CA, 1988. 
[Bernardo, 1979] 1. M. Bernardo. Reference posterior distributions for Bayesian inference (with dis-
cussion). Journal of the Royal Statistical Society, series B:41, 113-147, 1979. 
[Bernardo, 1997] J. M. Bernardo. Noninformative priors do not exist: A discussion with Jost M. 
Bernardo (with discussion). Journal of Statistical Planning and Inference 65,159-189,1997. 
[Birnbaum, 1961] A. Birnbaum. On the foundations of statistical inference: binary experiments. An-
nals of Mathematical Statistics 32, 414-435,1961. 
[Birnbaum, 1962] A. Birnbaum. On the foundations of statistical inference. Journal of the American 
Statistical Association, 57, 269-306, 1962. 
[Birnbaum, 1969] A. Birnbaum. Concepts of statistical evidence. In Essays in Honor of Ernest Nagel, 
Sidney Morgenbesser, Patrick Suppes and Morton White, eds. St. Martin's Press, 1969. 
[Birnbaum, 1972] A. Birnbaum. More on concepts of statistical evidence. Journal of the American 
Statistical Association. 67, 858-861, 1972. 
[Bjornstad, 1992] J. F. Bjornstad. Birnbaum (1962) on the foundations of statistical inference. In 
Breakthroughs in Statistics, vol. 1,461-477. Samuel Kotz and Norman L. Johnson, eds. Springer-
Verlag, New York, 1992. 
[Box and Tiao, 1973] G. Box and G. Tiao. Bayesian Inference in Statistical Analysis. Addison-
Wesley, Reading, MA, 1973. 
[Edwards, 1992] A. W. F. Edwards. Likelihood (2nd edition). Cambridge University Press, 1992. 

PRINCIPLES OF INFERENCE AND THEIR CONSEQUENCES 
403 
[Edwards et ai., 1963] W. Edwards, H. Lindman and L. 1. Savage. Bayesian statistical inference for 
psychological research. Psychological Review 70, 45~99, 1963. 
[Feller, 1940] W. K. Feller. Statistical aspects of ESP. Journal of Parapsychology 4, 271-298, 1940. 
[Giere, 1977] R. N. Giere. Alan Birnbaum's conception of statistical evidence. Synthese, 36, 5-13, 
1977. 
[Gillies, 1990] D. A. Gillies. Bayesianism versus falsificationism. Ratio, 3, 82-98, 1990. 
[Good,1983] I. 1. Good. Good Thinking. University of Minnesota Press, Minneapolis, MN, 1983. 
[Hill, 1987] B. M. Hill. The validity of the likelihood principle. The American Statistician, 47, 95-
100,1987. 
[Howson and Urbach, 1993] C. Howson and P. Urbach. Scientific Reasoning: The Bayesian Ap-
proach, second edition. Open Court, Chicago, 1993. 
[Jeffreys, 1961] H. Jeffreys. Theory of Probability, 3rd edition. Clarendon Press, Oxford, 1961. 
(Johnstone et al., 1986] D. J. Johnstone, G. A. Barnard and D. V. Lindley. Tests of significance in 
theory and practice. The American Statistician, 35,491-504, 1986. 
[Kadane etal., 1999] 1. B. Kadane, MJ. Schervish and T. Seidenfeld. Rethinking the Foundations of 
Statistics. Cambridge University Press, Cambridge, 1999. 
[Kerridge,1963] D. Kerridge. Bounds for the frequency of misleading Bayes' inferences. Annals of 
Mathematical Statistics 34, 1109-1110, 1963. 
[Lindley,1972] D. V. Lindley. Bayesian Statistics -A Review. 1. W. Arrowsmith, Bristol, 1972. 
[Mayo, 1996] D. Mayo. Error and the Growth of Experimental Knowledge. University of Chicago 
Press, Chicago, 1996. 
[Mayo, 2000] D. Mayo. experimental practice and an error statistical account of evidence. Philosophy 
of Science, 67, (Proceedings), SI93-S207, 2000. 
[Mayo and Spanos, 2000] D. Mayo and A. Spanos. A Post-data Interpretation of Neyman-Pearson 
Methods Based on a Conception of Severe Testing. Measurements in Physics and Economics Dis-
cussion Paper Series, DP MEAS 8/00. Centre for Philosophy of Natural & Social Science, London 
School of Economics, 2000. 
[Oakes, 1986] M. Oakes. Statistical Inference, Wiley, 1986. 
[Pearson and Neyman, 1930] E. S. Pearson and J. Neyman. On the problem of two samples. Bull. 
Acad. Pol. Sci., 73-96, 1930. Reprinted in J. Neyman and E. S. Pearson, Joint Statistical Papers. 
pp. 81-106 University of California Press, Berkeley, 1967. 
[Pratt et al., 1995] 1. W. Pratt, H. Raffia and R. Schlaifer. Introduction to Statistical Decision Theory. 
The MIT Press, Cambridge, MA, 1995. 
[Roberts, 1967] H. V. Roberts. Informative stopping rules and inferences about population size. Jour-
nal of the American Statistical Association. 62, 763-775, 1967. 
[Rosenkrantz, 1977] R. D. Rosenkrantz. Inference, Method, and Decision: Towards a Bayesian Phi-
losophy of Science. Boston: Reidel, 1977. 
[Royall, 1992] R. Royall. The elusive concept of statistical evidence (with discussion). In Bayesian 
Statistics 4, J.M. Bernardo, J.O. Berger, A.P. Dawid and A.P.M. Smith, eds. pp. 405-418. Oxford 
University Press, Oxford, 1992. 
[Royall, 1997] R. Royall. Statistical Evidence: A Likelihood Paradigm. Chapman & Hall, London, 
1997 
[Savage, 1962] L.1. Savage. The Foundations of Statistical Inference: A Discussion. Methuen, Lon-
don, 1962. 
[Savage, 1964] L. J. Savage. The foundations of statistics reconsidered. In Studies in Subjective Prob-
ability, H. Kyberg and H. Smokier, eds. John Wiley, New York, 1964. 
[Seidenfeld, 1979] T. Seidenfeld. Why I am not an objective Bayesian. Theory and Decision 11,413-
440,1979. 
[Smith, 1961l C. A. B. Smith. Consistency in statistical inference and decision (with discussion). 
Journal of the Royal Statistical Society, (B), Vol. 23, No.1, 1-37, 1961. 

INDEX 
x2 test, 363, 365, 367 
abstract structures, 77 
accidentally correlated, 79 
ACE,53-55 
action, 31, 32 
actual signicance, 391 
actual SL, 390 
Aczel, P., 150 
Adams principle, 156 
Adams, E. w., 150, 156 
adding-arrows, 106 
adhockery, 358 
affinely independent, 319 
Akaike, R., 399 
Albert, M., 12, 157, 343, 354, 355, 
376,378 
Allais Paradox, 342 
Allais, M., 296, 301, 306 
ambiguity, 62, 65-68, 72 
analogy, 192 
analysis of variance, 43 
ancestrally, 101 
Anscombe,F. G., 310, 311, 324, 342, 
347 
anything goes theorem, 354-357, 359, 
376 
Armitage,P., 384, 388, 391-393, 399 
Arrow, K., 344 
Ars Conjectandi, 137 
artificial intelligence, 117 
as-if,359 
as-if approach, 346 
atomic states, 107 
Aumann, R. 1., 310, 311, 324, 341, 
342,344,347 
average causal effect (ACE), 44 
avoid mistakes, 359 
axioms, 342, 347, 359, 360 
of revealed preference, 344 
axioms of RP, 346 
axioms, Bayesian, 342 
background knowledge, 97 
baker-map dynamics, 349, 350 
Barnard, G., 400 
Barnett, V., 399 
Bayes' Memoir, 137 
Bayes' theorem, 1,63,381,385 
Bayes, T., 1,363,378 
Bayesian, 47, 60, 381, 384 
Bayesian account, 382 
Bayesian conditionalisation, 1, 363, 
364,367-369,371,377 
Bayesian decision analysis, 47 
Bayesian decision theory, 68 
Bayesian learning, 347, 354, 355 
Bayesian network, 7,11,76 
Bayesian Networks Maximise Entropy, 
100 
Bayesian rationality, 359 
Bayesian theory of utility and sub-
jective probability, 291, 306 
Beall, G., 365, 366, 368 
Begg, D. K. R., 341 
behaviourism, 265 
belief formation, 203 
Berger, 1. 0., 385, 391, 392, 395-
400 
Bernardo, J. M., 399 
Bernoulli, 29 
Bernoulli trials, 386 
Bernoulli's logarithmic function, 142 
Bernoulli, D., 142 
Bernoulli, 1., 137 
Bernstein, P. L., 343 

406 
betting quotient, 140,369 
Bicchieri, C., 344 
Binmore, K., 342 
binomial distribution, 387 
Birnbaum, A., 394 
blind source separation, 131 
Blume, L. E., 342, 344, 354, 355 
Bolzano, B., l38 
Border, K. C., 347 
bounded-rationality, 358, 359 
Box, G., 399 
Bradley, R., 11 
Bray, M., 344 
Broome, 1., 296, 306 
Bremaud, P., 351, 354 
calibration, 1, 7 
Carnap's probability!, 138 
Carnap's probabilitY2' l38 
Carnap, R., l38, 145, 153, 345 
catch all, 377 
causal 
analysis, 24 
assumptions, 22, 23, 25 
effect, 34, 43 
extension, 79 
graph,32 
inference, 37, 55, 58, 68-70 
irrelevance, 99 
judgment, 31 
Markov condition, 7, 75, 76 
mechanisms, 29 
model, 6, 31, 66 
notation, 23 
restriction, 81 
sentences, 31 
vocabulary, 31 
causal and statistical concepts, 20 
causality reducible to probabilites, 27 
causality-definition, 27, 28 
causes of effects, 40, 59-61, 64, 66, 
68,71 
chaos, 355 
chaotic, 343, 355 
INDEX 
chaotic clock, 349-353, 356, 357,376 
Church-Turing Thesis, 149 
classical statistics, 363-366,368,372, 
378 
coherence, 157,359,360 
coherence versus strict coherence, 153 
Colhon, T., 319 
collapsibility, 21 
commodity complementarity, 296 
common prior assumption (CPA), 341, 
342,344 
common sense, 203, 205 
common sense principles, 203 
compatibility, 57 
complementary, 42, 47, 56, 63, 66 
complementary outcomes, 64 
complementary potential responses, 
68 
complementary quantities, 71 
complete, 34 
completeness, 152 
composition, 34 
computed significance level, 390 
concavity, 205 
concept definition, 29 
concomitant, 60-64, 66, 67, 69, 70, 
72 
conditional independence, 62-66, 71 
conditional mutual information, llO 
conditional prospects, 270 
conditionalism, 283 
conditionalist, 396 
Condorcet, 142 
confidence interval estimation, 382 
confounding, 24 
conjecture and refutation, 364, 366, 
378 
consequence set, 311 
consistency, 34, 143,360 
constrained network, 110 
context, 69-72 
contingency tables, 22 
continuity principle, 207, 212 
convergence,342,344, 351-355 

INDEX 
Corfield, D., 10 
correlation restriction, 81 
countable additivity, 152 
counterfactual, 6, 7, 28, 29, 31, 33, 
37,38,55,58,59,63,66, 
70,71 
approach, 51, 53 
inferences, 68 
model,58 
models, 68 
query, 70 
questions, 71 
universe, 69-71 
variables, 30 
Courtney, P., 208 
covariant, 51, 52 
covariate, 42, 65 
Cox's theorem, 178 
Cox, D. R, 374, 378 
CPA, 344, 345 
cross entropy, 101, 106, 107 
Cussens, 1., 11 
DAG, 28, 30, 76 
Davies, P., 350 
Dawid, P., 7, 364, 378, 399 
decision analysis, 50 
decision making, 342, 357, 360 
decision theory, 11,263,343,344 
decision tree, 30, 47 
decision-analytic approach, 47, 51, 
53 
deductive logic, 157, 357, 359, 360 
degrees of belief, 203 
dependence, 93 
determining concomitant, 65-68 
determinism, 65, 67, 70 
deterministic model, 66, 67, 69 
deterministic structure, 68 
Devaney, R L., 350 
diagrams, 24 
direct and indirect effects, 24 
direct method, 76 
directed acyclic graph (DAG), 28 
407 
disjunctive complementarity, 301, 302, 
306 
disturbance, 25 
divisibility axiom, 310 
do-calculus, 26-28 
domain expert, 366, 368 
dominance principle, 345 
dominated strategy, 346 
Dreze, 1., 310-312 
Dreze, K., 316 
Dutch Book argument, 1,9,13,140, 
178,343 
Earman, 1., 188,344,352,355 
Easley, D., 342, 344, 354, 355 
economic rationality, 346 
economics, 342-345, 353, 358 
Edwards, A. W. F., 386 
Edwards, w., 381, 383, 391, 395, 396, 
398 
effect of action, 33 
effectiveness, 33 
effects of causes, 40, 41,60,71 
Ellsberg's counterexamples to indpen-
dence,299-302 
Ellsberg, D., 299-302, 306 
empirical Bayesianism, 2 
empirical content, 343, 344, 357 
empirically adequate, 353, 355 
emptiness, 343 
empty, 343-345, 354, 357 
endogenous, 32 
ensemble learning, 125 
equivalence principle, 206, 210 
error probabilities, 383 
error statistical methods, 381, 383, 
384 
ethically neutral proposition, 270 
evidential-relationship (E-R) princi-
ples, 382, 383 
exchangeability, 2, 22, 42, 52, 58, 
364,369,371-374 
existence, 78 
exogenous, 25,31 

408 
expectation, 342, 343, 356, 357 
expectations formation, 342 
extended inference processes, 209 
extended knowledge base, 209 
extended maximum entropy process, 
219 
extra-causal constraint, 79 
(extrinsic) confounding, 56 
factor-analysis, 63 
Fagin, R., 151 
Fallis, D., 183 
falsification, 366, 368 
falsification dynamics, 351 
falsifies, 351 
fatalism, 40, 49, 50 
fatalistic, 54, 65 
Feller, W. K., 373, 378, 391 
Finetti, B. de, I, 9, 137, 179, 309, 
364,368,370-375,378 
Fishburn, P., 291, 306 
Fisher, ,54 
fixity of the theoretical framework, 
364,367,368,375,378 
folk theorem, 343, 344 
Ford, J., 350 
forecast function, 349, 354 
free will, 70 
Friedman, M., 292, 294, 306, 346 
Frydman, R., 341 
functional model, 58 
functional relationships, 68 
Gabbay, D. M., 11, 150 
Gaifman, H., 151 
Galavotti, M. C., 9, 371, 378 
game theory, 344 
Gaussian process, 126 
generative model, 131 
generative topographic mapping, 130 
geostatistics, 126 
Gillies, D. A., 13, 14,355,363,378 
goat, 53-55, 58 
Goldman, A. I., 343 
INDEX 
Good, I. 1., 399 
Goodman,N., 353 
Gosset, W. S., 363 
graphical model, 58, 68 
Green, E. 1., 347 
Hacking, I., 150, 180,341,345 
Hagen, 0., 296, 306 
Hahn,F.,344 
Halmos, P., 144 
Halpern,1. Y., 151 
Harvard Medical School Test, 157 
Heifetz, A., 151 
Hellman, G., 141 
hidden variables, 57 
Hill, B. M., 399 
Hodges, w., 148 
homogeneity, 42, 50 
Howson, C., 9, 10, 13, 138, 144, 156, 
344,345,360,396 
HPD,393 
Hume, D., 156, 182 
hypothesis testing, 363 
hypothetical act, 315 
hypothetical preference, 315 
ICE,53 
imperfect treatment compliance, 58 
(improper) uniform prior, 393 
improvement, 106 
Improvement of Adding Arrows, 106 
independence and sure-thing princi-
ples, logical connections be-
tween, 293-295 
independence for sure outcomes (ISO), 
291 
independence principle (IND), 12,207, 
212,291,297-299,306 
independent component analysis, 131 
individual causal effect (ICE), 43, 44, 
59,66 
inductive inference, 71, 156 
inductive logic programming, 11 
inference process, 203, 205 

informative stopping rules, 395 
instrumental,55-57 
instrumental variables, 21 
interpreted, 77 
interval of ambiguity, 62, 63 
interventions, 26, 31 
intrinsic aliasing, 45 
intrinsically confounded, 56 
introspection, 268 
invariance, 26 
irrational, 343, 354 
irrational behaviour, 343 
INDEX 
irrelevant information principle, 206, 
209 
Jaynes, E., 176, 177, 186 
Jeffrey's Law, 45, 53, 68 
Jeffrey, R, 286 
joint probabilites of counterfactuals, 
35 
Jordan, J. S., 343 
Kac,M.,139 
Kadane, J. B., 311, 326, 393 
Kahneman, D., 297, 302, 304, 306 
Karni, E., 310, 311, 316, 327 
Keynes, J. M., 138,344,345 
Kiefer, N. M., 342 
Kim, T., 347 
Kirman, A, 341 
knowledge elicitation problem, 77 
Kolmogorov, AN., 151 
Krauss, P., 151 
Kruse, M., 14 
labelled deductive systems, 11 
Lakatos, I., 158, 378 
language invariance, 206, 210 
Laplace's Rule of Succession, 196, 
371,374 
Laplace, P. S., 1,344,363,378 
Larsson, S., 297 
Latin square, 54, 56, 57 
Leamer, E. E., 344 
409 
learning, 341, 342, 345, 347, 349, 
352,357 
Ledyard, J. 0., 344 
Leibniz, 137 
Lewis, D., 156 
likelihood,383 
likelihood function, 385 
likelihood principle, 14,381-387,393, 
398 
likelihood ratio (LR), 396 
likelihoodist account, 382 
Lindley, D. V., 345, 399 
Lindman, H., 381, 383, 391, 395, 396, 
398 
line, 220 
linear knowledge bases, 203 
linear probabilistic constraints, 203 
literal,76 
logical Bayesianism, 2 
logical consistency, 359, 360 
logical positivism, 345 
logically omniscient, 10, 177, 352, 
359 
Loomes, G., 296, 306 
Lucas, R E., 344 
MacCrimmon, K. R, 297, 306 
machine learning, 8 
macroeconomic,341 
Manne, H., 296, 306 
Markov chain, 364, 373-375, 378 
Markov-exchangeable, 375 
mathematics, 175-178, 180-182, 184, 
188-193,195,198,200 
Max-Weight Approximation, 110 
maximin criterion, 345, 346 
maximin rule, 344, 345 
maximum entropy, 9, 10,203,205 
maximum entropy principle, 2, 97 
Maxwell-Boltzmann vs. Bose-Einstein 
statistics, 139 
Mayo, D., 14,381 
McClennen,E. E, 291,296,305,307 
measurement, 263 

410 
mechanisms, 28, 31 
merger of opinions, 355 
metaphysical, 49,56,65,69,71 
array, 42, 48, 55, 57 
hypothesis, 55 
model, 40, 43 
null hypothesis, 54 
probability model, 63 
methodological, 358 
methodology, 348, 355, 356 
Meyer, B. de, 318 
Miller, D., 357 
Miller, H. D., 374, 378 
Milne, P. M., 156 
minimum agreement on consequences, 
317 
minumum agreement on acts, 317 
mission-oriented Bayesianism, 31 
model verification, 29 
model-building, 55 
modifiable structural equations, 33 
modified chaotic clock, 352-354,356 
modularity, 26 
Mongin, P., 12,151,318,319 
monotonicity, 46, 47 
Monstrous Moonshine, 197 
Monte Carlo method, 125 
Morgenstern, 0.,296,307 
Musgrave, A., 357 
Muth,1. F., 341, 343 
N-solutions,216 
negative binomial, 387, 388 
Neptune, prediction of, 186 
neural computation, 117 
neural network, 8, 118 
Neyman, J., 54, 363-365, 367, 376, 
378,381,391,396,397 
Neyman-Pearson, 386 
nominal (or computed) SL, 389 
nominal significance, 390, 391 
nominal SL, 390 
non-compliance, 49 
non-deterministic, 63, 72 
INDEX 
non-linear constraints, 203 
non-recursive models, 31 
null hypothesis, 386 
Nyarko, Y., 342, 343, 357 
Oakes, M., 399 
objective Bayesianism, 2 
objective priors, 9 
objectivity, 1 
obstinacy principle, 207, 212 
odds, 140 
OMTs,343-348,355 
open-mindedness, 238 
operationally meaningful theorems (MTs), 
343 
optional stopping effect, 388 
Osband,K.,347 
overfitting,121 
P6lya, G., 10, 175-178, 181, 183, 
186-190, 192, 193, 195-
197,200 
parents, 32 
Paretian, 309 
Paretianism, 309, 310, 329 
Pareto principle, 328-330 
Pareto-Indifference, 309, 317 
Pareto-Weak Preference, 317 
Paris, 1., 10, 143 
partial belief, 279 
path diagrams, 25, 26 
pattern recognition, 117 
Pearl, 1., 6, 7,177,189,341 
Pearson, E. S., 363, 381, 391, 396, 
397 
Pearson, K., 363, 379 
perfect rationality, 343, 352, 354, 357-
359 
Pesaran, M. H., 341, 342 
Phelps, E. S., 341 
physical array, 42, 48, 57 
physical model, 40, 44 
pluralist, 1 
Poincare, H., 185 

Poisson distribution, 365-368, 377 
Poisson, S.-D., 138 
Pollatsek, A., 298, 307 
Pope, R E., 342 
Popper, K., 364, 366, 373, 379 
positive solution, 223 
posterior distribution, 67 
INDEX 
potential response, 33,42, 65 
potential-outcome approach, 30 
pragmatic problem of induction, 357 
Pratt,1. w., 385 
predesignated,383 
predictive distributions, 48, 51 
predictive inferences, 48 
preference, 342,345,346,360 
Price, R, 363, 378 
principle of indifference, 2, 9, 97,138, 
218,371 
principle of insufficient reason, 344, 
345,357,358 
principle of the common cause, 78 
probabilistic causal model, 34 
probabilistic-cause, 29 
probability 
dictator, 322 
function, 203 
kinematics, 156 
of causation, 24 
of counterfactual, 34 
problem of induction, 353, 357 
proof strategy, 194 
propagation algorithms, 76 
proper stopping rule, 389 
protection against mistakes, 359 
pseudo-determinism, 68 
pseudo-structural nested distribution 
models, 57 
quantum physics, 42 
quantum theory, 57 
Raiffa, H., 304, 305, 307, 385 
Ramsey, F., 1 
Ramsey, F. P., 11, 137,263,345 
random variables, 371 
randomization, 21 
411 
rational expectation, 341, 342, 355 
rational line, 220 
rational-expectations hypothesis (REH), 
341 
rationalised on, 343 
rationality, 152,341,342, 344, 346, 
349,354,357,358 
rationality hypothesis, 344 
rationalizability, 343 
rationalization, 349, 354 
reasoning by induction, 369, 371 
red or blue, 373-375,377 
reference class problem, 2 
regularisation, 121 
REH,341-343 
relativisation principle, 206, 211 
renaming principle, 206, 210 
repeated significance tests, 390 
representation theorems, 264 
restriction, 81 
revealed preference (RP), 343, 345, 
346 
Riemann Hypothesis, 192 
risk factors, 21 
risk ratio, 21 
Roberts, H. v., 395 
Rosenkrantz, R., 187 
Royall,R,386,395,399 
RP, 346, 347 
Salmon, M., 341 
sampling distribution, 386 
Samuelson, P. A., 296, 307, 343 
Savage Forum, 1959,388,392,400 
SavageIndependence,292,298 
Savage, L. 1.,19,137,141,291-295, 
297-299,302,303,306,310, 
316, 342, 344, 347, 381-
385, 391, 392, 395, 396, 
398,399 
Schervish, M. J., 311, 326, 393 
Schlaifer, R, 385 

412 
Schmeidler, Do, 311, 316, 327 
Schuster, Ho Go, 350 
Scott, Do, 151 
screening, 78 
Seidenfeld, T., 305, 307, 311, 326, 
393,399 
selective pressure, 346, 358, 359 
Selten, R., 358 
SEM,26 
Sen,A.,296,307 
seqential sampling, 383 
sequential testing, 389 
SEV, 342, 345, 349 
severe test, 384 
severity, 383 
sheep, 53, 55 
Shimony, A., 153 
Shoemaker, Po 1. Ho, 297, 307 
significance level, 388 
significance testing, 382 
Simon, Ho A., 346 
Simpson's paradox, 22 
singly-connected, 109 
Smullyan, R., 144 
social choice theory, 317 
space complexity, 109 
SRP,391 
St Petersburg Problem, 142 
stable unit-treatment value assump-
tion (SVTVA), 50, 57 
stage one, 97 
stage two, 97 
stastistics, 342 
state, 76 
state independence, 313 
state-dependent utility, 12 
state-dependent utility theory, 310,328 
state-independence, 310 
state-independent utility, 12 
statistical analysis, 19 
statistical decision theory, 345 
statistical significance, 382, 386, 388 
statistical tests, 363, 366, 373, 375 
statistical uncertainty, 62 
INDEX 
statistics, 341 
stochastic logic programs, 11 
stochastic processes, 20 
stopping rule, 14,382 
stopping rule principle, 384 
stopping rules, 383 
strategy, 341, 345 
Strict Pareto, 317 
strict subjectivist, 1 
strictly dominated, 345, 346 
strictly dominated strategies, 345 
strong compatibility, 57 
Strong Pareto, 309, 317 
structural equation modelling (SEM), 
26 
structural equations, 24 
Structural Equations Model, 24 
Student, 363 
subjective expected utility, 310 
Submodel, 32 
sufficient concomitant, 62, 63, 65 
Support Vector Machine (SVM), 127 
Sure-Thing Principle, 12, 292-295, 
302,310,342 
SVTVA,50 
symmetry, 48 
symmetry modelling, 56 
t-test, 54, 55 
Tarskian truth-definition, 143 
Teller, Po, 155 
tent-map dynamics, 354 
testing, 382 
Tiao, Go, 399 
time complexity, 109 
Todhunter, I., 142 
treatment-unit additivity (TVA), 46, 
49,51 
triviality theorem, 156 
truncated expression, 27 
TVA, 47, 53, 54, 59, 66 
Tversky, A., 297, 298, 302, 304, 306 
two-stage methodology, 97 
Type A distribution, 367, 377 

type I error, 389 
Ulam, S., 139 
ultimate causes, 71 
uncertainty, 65 
uniform prior, 398 
uniformity, 46, 47, 65 
uniqueness principle, 221 
universal approximator, 119 
unsharp probabilities, 153 
unsupervised learning, 129 
Urbach, P., 138, 344, 345, 396 
utilities, 346 
utility function, 341-345, 347 
utility-maximizing, 349 
value-assignment process, 25 
Varian, H. R, 346 
Vencovska, A., 10 
Vind, K., 311, 327 
von Neumann, 1., 296, 307 
INDEX 
von Neumann-Morgenstern axioms, 
312 
Wakker, P., 291, 306 
Watts Assumption, 203 
weak compatibility, 57 
Weak Pareto, 309, 317 
weight, 110 
weight of evidence, 399 
Williams, P., 8 
Williamson, J., 7,11,150 
Wolpert, R L., 391, 395-400 
413 

APPLIED LOGIC SERIES 
1. 
D. Walton: Fallacies Arising from Ambiguity. 1996 
ISBN 0-7923-4100-7 
2. 
H. Wansing (ed.): Proof Theory of Modal Logic. 1996 ISBN 0-7923-4120-1 
3. 
F. Baader and K.U. Schulz (eds.): Frontiers of Combining Systems. First 
International Workshop, Munich, March 1996.1996 
ISBN 0-7923-4271-2 
4. 
M. Marx and Y. Venema: Multi-Dimensional Modal Logic. 1996 
ISBN 0-7923-4345-X 
5. 
S. Akama (ed.): Logic, Language and Computation. 1997 
ISBN 0-7923-4376-X 
6. 
J. Goubault-Larrecq and I. Mackie: Proof Theory and Automated Deduction. 
1997 
ISBN 0-7923-4593-2 
7. 
M. de Rijke (ed.): Advances in Intensional Logic. 1997 ISBN 0-7923-4711-0 
8. 
W. Bibel and P.H. Schmitt (eds.): Automated Deduction - A Basis for Applic-
ations. Volume I. Foundations - Calculi and Methods. 1998 
ISBN 0-7923-5129-0 
9. 
W. Bibel and P.H. Schmitt (eds.): Automated Deduction - A Basis for Applic-
ations. Volume II. Systems and Implementation Techniques. 1998 
ISBN 0-7923-5130-4 
10. w. Bibel and P.H. Schmitt (eds.): Automated Deduction - A Basis for Applic-
ations. Volume ill. Applications. 1998 
ISBN 0-7923-5131-2 
(Set vols. I-ill: ISBN 0-7923-5132-0) 
11. S.O. Hansson: A Textbook of Belief Dynamics. Theory Change and Database 
Updating. 1999 
Hb: ISBN 0-7923-5324-2; Pb: ISBN 0-7923-5327-7 
Solutions to exercises. 1999. 
Pb: ISBN 0-7923-5328-5 
Set: (Hb): ISBN 0-7923-5326-9 
Set: (Pb): ISBN 0-7923-5329-3 
12. R. Pareschi and B. Fronhofer (eds.): Dynamic Worlds from the Frame Problem 
to Knowledge Management. 1999 
ISBN 0-7923-5535-0 
13. D.M. Gabbay and H. Wansing (eds.): What is Negation? 1999 
ISBN 0-7923-5569-5 
14. M. Wooldridge and A. Rao (eds.): Foundations of Rational Agency. 1999 
ISBN 0-7923-5601-2 
15. D. Dubois, H. Prade and E.P. Klement (eds.): Fuzzy Sets, Logics and Reas-
oning about Knowledge. 1999 
ISBN 0-7923-5911-1 
16. H. Barringer, M. Fisher, D. Gabbay and G. Gough (eds.): Advances in Tem-
poral Logic. 2000 
ISBN 0-7923-6149-0 
17. D. Basin, M.D. Agostino, D.M. Gabbay, S. Matthews and L. Vigano (eds.): 
Labelled Deduction. 2000 
ISBN 0-7923-6237-3 
18. P.A. Flach and A.C. Kakas (eds.): Abduction and Induction. Essays on their 
Relation and Integration. 2000 
ISBN 0-7923-6250-0 

19. S. Holldobler (ed.): lntellectics and Computational Logic. Papers in Honor 
of Wolfgang Bibel. 2000 
ISBN 0-7923-6261-6 
20. 
P. Bonzon, M. Cavalcanti and Rolf N os sum (eds.): Formal Aspects of Context. 
2000 
ISBN 0-7923-6350-7 
21. D.M. Gabbay and N. Olivetti: Goal-Directed Proof Theory. 2000 
ISBN 0-7923-6473-2 
22. M.-A. Williams and H. Rott (eds.): Frontiers in Belief Revision. 2001 
ISBN 0-7923-7021-X 
23. E. Morscher and A. Hieke (eds.): New Essays in Free Logic. In Honour of 
Karel Lambert. 2001 
ISBN 1-4020-0216-5 
24. 
D. Corfield and J. Williamson (eds.): Foundations of Bayesianism. 2001 
ISBN 1-4020-0223-8 
KLUWER ACADEMIC PUBLISHERS - DORDRECHT / BOSTON / LONDON 

