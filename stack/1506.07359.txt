arXiv:1506.07359v1  [cs.AI]  24 Jun 2015
Sequential Extensions of Causal and Evidential
Decision Theory∗
Tom Everitt
Jan Leike
Marcus Hutter
October 11, 2018
Abstract
Moving beyond the dualistic view in AI where agent and environment
are separated incurs new challenges for decision making, as calculation of
expected utility is no longer straightforward. The non-dualistic decision
theory literature is split between causal decision theory and evidential
decision theory.
We extend these decision algorithms to the sequential
setting where the agent alternates between taking actions and observ-
ing their consequences. We ﬁnd that evidential decision theory has two
natural extensions while causal decision theory only has one.
Keywords.
Evidential decision theory, causal decision theory, causal graphi-
cal models, planning, dualism, physicalism.
1
Introduction
In artiﬁcial-intelligence problems an agent interacts sequentially with an envi-
ronment by taking actions and receiving percepts [RN10]. This model is dual-
istic: the agent is distinct from the environment. It inﬂuences the environment
only through its actions, and the environment has no other information about
the agent. The dualism assumption is accurate for an algorithm that is play-
ing chess, go, or other (video) games, which explains why it is ubiquitous in
AI research. But often it is not true: real-world agents are embedded in (and
computed by) the environment [OR12], and then a physicalistic model 1 is more
appropriate.
This distinction becomes relevant in multi-agent settings with similar agents,
where each agent encounters ‘echoes’ of its own decision making. If the other
agents are running the same source code, then the agents’ decisions are logically
connected. This link can be used for uncoordinated cooperation
[LFY+14].
Moreover, a physicalistic model is indispensable for self-reﬂection. If the agent is
required to autonomously verify its integrity, and perform maintenance, repair,
or upgrades, then the agent needs to be aware of its own functioning. For this, a
reliable and accurate self-modeling is essential. Today, applications of this level
of autonomy are mostly restricted to space probes distant from earth or robots
navigating lethal situations, but in the future this might also become crucial for
∗The ﬁnal publication is available at http://link.springer.com/.
1Some authors also call this type of model materialistic or naturalistic.
1

environment
hidden state s
at
et
agent π
environment model µ
self-model
Figure 1: The physicalistic model. The hidden state s contains information
about the agent that is unknown to it. The distribution µ is the agent’s (sub-
jective) environment model, and π its (deterministic) policy. The agent models
itself through the beliefs about (future) actions given by its environment model
µ. Interaction with the environment at time step t occurs through an action at
chosen by the agent and a percept et returned by the environment.
sustained self-improvement in generally intelligent agents [Yud08, Bos14, SF14a,
RDT+15].
In the physicalistic model the agent is embedded inside the environment,
as depicted in Figure 1.
The environment has a hidden state that contains
information about the agent that is inaccessible to the agent itself. The agent
has an environment model that describes the behavior of the environment given
the hidden state and includes beliefs about the agent’s own future actions (thus
modeling itself).
Physicalistic agents may view their actions in two ways: as their selected
output, and as consequences of properties of the environment. This leads to
signiﬁcantly more complex problems of inference and decision making, with
actions simultaneously being both means to inﬂuence the environment and evi-
dence about it. For example, looking at cat pictures online may simultaneously
be a means of procrastination, and evidence of bad air quality in the room.
Dualistic decision making in a known environment is straightforward calcu-
lation of expected utilities. This is known as Savage decision theory [Sav72]. For
non-dualistic decision making two main approaches are oﬀered by the decision
theory literature: causal decision theory (CDT) [GH78, Lew81, Sky82, Joy99,
Wei12] and evidential decision theory (EDT) [Jef83, Bri14, Ahm14]. EDT and
CDT both take actions that maximize expected utility, but diﬀer in the way this
expectation is computed: EDT uses the action under consideration as evidence
about the environment while CDT does not. Section 2 formally introduces these
decision algorithms.
Our contribution is to formalize and explore a decision-theoretic setting with
a physicalistic reinforcement learning agent interacting sequentially with an en-
vironment that it is embedded in (Section 3). Previous work on non-dualistic
decision theories has focused on one-shot situations. We ﬁnd that there are
two natural extensions of EDT to the sequential case, depending on whether
the agent updates beliefs based on its next action or its entire policy. CDT
has only one natural extension. We extend two famous Newcomblike problems
to the sequential setting to illustrate the diﬀerences between our (generalized)
decision theories.
Section 4 summarizes our results and outlines future directions. A list of
2

notation can be found on page 16 and Appendix A contains formal details to
our examples.
2
One-Shot Decision Making
In a one-shot decision problem, we take one action a ∈A, receive a percept e ∈E
(typically called outcome in the decision theory literature) and get a payoﬀu(e)
according to the utility function u : E →[0, 1]. We assume that the set of actions
A and the set of percepts E are ﬁnite. Additionally, the environment contains
a hidden state s ∈S. The hidden state holds information that is inaccessible
to the agent at the time of the decision, but may inﬂuence the decision and the
percept. Formally, the environment is given by a probability distribution P over
the hidden state, the action, and the percept that factors according to a causal
graph [Pea09].
A causal graph over the random variables x1, . . . , xn is a directed acyclic
graph with nodes x1, . . . , xn. To each node xi belongs a probability distribution
P(xi | pai), where pai is the set of parents of xi in the graph.
It is natu-
ral to identify the causal graph with the factored distribution P(x1, . . . , xn) =
Qn
i=1 P(xi | pai). Given such a causal graph/factored distribution, we deﬁne
the do-operator as
P(x1, . . . , xj−1, xj+1, . . . , xn | do(xj := b)) =
n
Y
i=1
i̸=j
P(xi | pai)
(1)
where xj is set to b wherever it occurs in pai, 1 ≤i ≤n.
The result is a
new probability distribution that can be marginalized and conditioned in the
standard way. Intuitively, intervening on node xj means ignoring all incoming
arrows to xj, as the eﬀects they represent are no longer relevant when we inter-
vene; the factor P(xj | paj) representing the ingoing inﬂuences to xj is therefore
removed in the right-hand side of (1). Note that the do-operator is only deﬁned
for distributions for which a causal graph has been speciﬁed. See [Pea09, Ch.
3.4] for details.
2.1
Savage Decision Theory
In the dualistic formulation of decision theory, we have a function P that takes
an action a and returns a probability distribution Pa over percepts. Savage
decision theory (SDT) [Sav72, Bri14] takes actions according to
arg max
a∈A
X
e∈E
Pa(e)u(e).
(SDT)
In the dualistic model it is usually conceptually clear what Pa should be.
In the physicalistic model the environment model takes the form of a causal
graph over a hidden state s, action a, and percept e, as illustrated in Figure 2.
According to this causal graph, the probability distribution P factors causally
into P(s, a, e) = P(s)P(a | s)P(e | s, a). The hidden state is not independent of
the decision maker’s action and Savage’s model is not directly applicable since
we do not have a speciﬁcation of Pa. How should decisions be made in this
context?
The literature focuses on two answers to this question: CDT and
EDT.
3

a
e
s
Figure 2: The causal graph P(s, a, e) = P(s)P(a | s)P(e | s, a) for one-step
decision making. The hidden state s inﬂuences both the decision maker’s action
a and the received percept e.
2.2
Causal and Evidential Decision Theory
The literature on causal and evidential decision theory is vast, and we give only a
very superﬁcial overview that is intended to bring the reader up to speed on the
basics. See [Bri14, Wei12] and references therein for more detailed introductions.
Evidential decision theory (endorsed in [Jef83, Ahm14]) considers the prob-
ability of the percept e conditional on taking the action a:
arg max
a∈A
X
e∈E
P(e | a) u(e)
with
P(e | a) =
X
s∈S
P(e | s, a)P(s | a)
(EDT)
Causal decision theory has several formulations [GH78, Lew81, Sky82, Joy99];
we use the one given in [Sky82], with Pearl’s calculus of causality [Pea09]. Ac-
cording to CDT, the probability of a percept e is given by the causal intervention
of performing action a on the causal graph from Figure 2:
arg max
a∈A
X
e∈E
P(e | do(a)) u(e)
with
P(e | do(a)) =
X
s∈S
P(e | s, a)P(s)
(CDT)
where P(e | do(a)) follows from (1) and marginalization over s.
The diﬀerence between CDT and EDT is how the action aﬀects the belief
about the hidden state. EDT assigns credence P(s | a) to the hidden state s if
action a is taken, while CDT assigns credence P(s). A common argument for
CDT is that an action under my direct control should not inﬂuence my belief
about things that are not causally aﬀected by the action. Hence P(s) should be
my belief in s, and not P(s | a). (By assumption, the action does not causally
aﬀect the hidden state.) EDT might reply that if action a does not have the
same likelihood under all hidden states s, then action a should indeed inform
me about the hidden state, regardless of causal connection. The following two
classical examples from the decision theory literature describe situations where
CDT and EDT disagree. A formal deﬁnition of these examples can be found in
Appendix A.
Example 1 (Newcomb’s Problem [Noz69]). In Newcomb’s Problem there are
two boxes: an opaque box that is either empty or contains one million dollars
and a transparent box that contains one thousand dollars. The agent can choose
between taking only the opaque box (‘one-boxing’) and taking both boxes (‘two-
boxing’). The content of the opaque box is determined by a prediction about
4

the agent’s action by a very reliable predictor: if the agent is predicted to one-
box, the box contains the million, and if the agent is predicted to two-box, the
box is empty. In Newcomb’s problem EDT prescribes one-boxing because one-
boxing is evidence that the box contains a million dollars. In contrast, CDT
prescribes two-boxing because two-boxing dominates one-boxing: in either case
we are a thousand dollars richer, and our decision cannot causally aﬀect the
prediction.
Newcomb’s problem has been raised as a critique to CDT, but
many philosophers insist that two-boxing is in fact the rational choice,2 even if
it means you end up poor.
Note how the decision depends on whether the action inﬂuences the belief
about the hidden state (the contents of the opaque box) or not.
Newcomb’s problem may appear as an unrealistic thought experiment. How-
ever, we argue that problems with similar structure are fairly common. The
main structural requirement is that P(s | a) ̸= P(s) for some state or event
s that is not causally aﬀected by a.
In Newcomb’s problem the predictor’s
ability to guess the action induces an ‘information link’ between actions and
hidden states. If the stakes are high enough, the predictor does not have to
be much better than random in order to generate a Newcomblike decision prob-
lem. Consider for example spouses predicting the faithfulness of their partners,
employers predicting the trustworthiness of their employees, or parents predict-
ing their children’s intentions. For AIs, the potential for accurate predictions
is even greater, as the predictor may have access to the AI’s source code. Al-
though rarely perfect, all of these predictions are often substantially better than
random.
To counteract the impression that EDT is generally superior to CDT, we
also discuss the toxoplasmosis problem.
Example 2 (Toxoplasmosis Problem [Alt13]). 3 This problem takes place in a
world in which there is a certain parasite that causes its hosts to be attracted to
cats, in addition to uncomfortable side eﬀects. The agent is handed an adorable
little kitten and is faced with the decision of whether or not to pet it. Petting the
kitten feels nice and therefore yields more utility than not petting it. However,
people suﬀering from the parasite are more likely to pet the kitten. Petting the
kitten is evidence of having the parasite, so EDT recommends against it. CDT
correctly observes that petting the kitten does not cause the parasite, and is
therefore in favor of petting.
Newcomb’s problem and the toxoplasmosis problem cannot be properly for-
malized in SDT, because SDT requires the percept-probabilities Pa to be speci-
ﬁed, but it is not clear what the right choice of Pa would be. However, both CDT
and EDT can be recast in the context of SDT by setting Pa to be P( · | do(a))
and P( · | a) respectively. Thus we could say that the formulation given by
Savage needs a speciﬁcation of the environment that tells us whether to act
evidentially, causally, or otherwise.
2In a 2009 survey, 31.4% of philosophers favored two-boxing, and 21.3% favored one-boxing
(931 responses); see http://philpapers.org/surveys/results.pl. Is that the reason there
are so few wealthy philosophers?
3Historically, this problem has been known as the smoking lesion problem [Ega07]. We
consider the smoking lesion formulation confusing, because today it is universally known that
smoking does cause lung cancer.
5

a1
e1
a2
e2
. . .
s
Figure 3: The (inﬁnite) causal graph for a sequential environment. Each action
at and each percept et is represented by a node in the causal graph. Actions
and percepts aﬀect all subsequent actions and percepts: causality follows time.
The hidden state s is only ever indirectly (partially) observed.
3
Sequential Decision Making
In this section we extend CDT and EDT to the sequential case. We start by
formally specifying the physicalistic model depicted in Figure 1 in the ﬁrst sub-
section, and discuss problems with time consistency in Section 3.2, before deﬁn-
ing the extensions proper in Section 3.3 and 3.4. The ﬁnal subsection dissects
the role of the hidden state.
3.1
The Physicalistic Model
For the remainder of this paper, we assume that the agent interacts sequentially
with an environment. At time step t the agent chooses an action at ∈A and
receives a percept et ∈E which yields a utility of u(et) ∈R; the cycle then
repeats for t + 1. A history is an element of (A × E)∗. We use æ ∈A × E to
denote one interaction cycle, and æ<t to denote a history of length t −1. The
percepts between time t and time m are denoted et:m. A policy is a function
that maps a history æ<t to the next action at. We only consider deterministic
policies.
We assume that the agent is given an environment model µ, but knows
neither the hidden state s nor its own future actions. The unknown hidden
state may inﬂuence both percepts and actions. Actions and percepts in turn
inﬂuence the entire future. The environment model µ is given by a probability
distribution over hidden states and histories that factors as
µ(s, æ<t) = µ(s)
t−1
Y
i=1
µ(ai | s, æ<i)µ(ei | s, æ<iai)
(2)
for any t ∈N. While such a factorization is possible for any distribution, we ad-
ditionally demand that this factorization is causal according to the causal graph
in Figure 3. The distribution µ(at | s, æ<t) gives the likelihood of the agent’s
own actions provided a hidden state s ∈S (for example, the prior probability of
an infected agent petting the kitten in the toxoplasmosis problem above). For
technical reasons, this distribution must always leave some uncertainty about
the actions: if the environment model assigned probability zero for an action a′,
the agent could not deliberate taking action a′ since a′ could not be conditioned
6

on. Formally, we require µ( · | s) to be action-positive for all s ∈S:
∀æ<tat ∈(A × E)∗× A.
 µ(æ<t | s) > 0 =⇒µ(at | s, æ<t) > 0

(3)
The distribution µ is a model of the environment, a belief held by the agent,
but not the distribution from which the actual history is drawn. The actual
history is distributed according to the true environment distribution. Because
the environment contains the agent, the agent’s algorithm might get modiﬁed
by it and the actions that the agent actually ends up taking might not be the
actions that were planned.
In the end, model and reality will disagree: for
example, we simultaneously assume the agent’s policy π to be deterministic
and the environment model to be action positive. Nevertheless, we assume the
given environment model is accurate in the sense that it faithfully represents
the environment in the ways relevant to the agent.
In other words, we are
interested in problems that arise during planning, not problems that arise due
to poor modeling.
3.2
Time Consistency
When planning for the inﬁnite future we need to make sure that utilities do not
sum to inﬁnity; typically this is achieved with discounting. Here, we simplify
by ﬁxing a ﬁnite m ∈N to be the agent’s lifetime: the agent cares about the
sum of the utilities of all percepts e1 . . . em until and including time step m, but
does not care what happens after that (presumably the agent is then retired).
In sequential decision theory we need to plan the next m −t actions in
time step t. We plan what we would do for all possible future percepts et:m by
choosing a policy π : (A×E)∗→A that speciﬁes which action we take depending
on how the history plays out. For example, we take action at, and when we
subsequently receive the percept et, we plan to take action at+1. Problems arise
once we get to the next step and even tough we did take action at and the
percept did turn out to be et, we change our mind and take a diﬀerent action
ˆat+1. This is called time inconsistency. Time inconsistency is an artifact of bad
planning since the agent incorrectly anticipates her own actions. The choice of
discounting can lead to time inconsistency: a sliding ﬁxed-size horizon is time
inconsistent, but a ﬁxed ﬁnite lifetime is time consistent [LH14].
We achieve time consistency by using a ﬁxed ﬁnite lifetime, and by calculat-
ing decisions recursively using value functions. A value function V π
µ,m is a func-
tion of type ((A×E)∗∪((A×E)∗×A)) →R. It gives an estimate of future reward:
V π
µ,m(æ<t) and V π
µ,m(æ<tat) are estimates of how much reward the policy π will
obtain in environment µ within lifetime m subsequent to history æ<t and æ<tat
respectively. For any history æ<t, we deﬁne V π
µ,m(æ<t) := V π
µ,m(æ<tπ(æ<t)).
We say that a policy π is optimal and time consistent for the value function
Vµ,m iﬀπ(æ<t) = arg maxa V π
µ,m(æ<ta) for all histories æ<t ∈(A × E)t−1 and
all t ≤m.
3.3
Sequential Evidential Decision Theory
Evidential decision theory assigns probability P(e | a) to action a resulting in
percept e (Section 2.2). There are two ways to generalize this to the sequential
setting, depending on whether we use only the next action or the whole future
policy as evidence for the next percept.
7

Deﬁnition 3 (Action-Evidential Decision Theory). The action-evidential value
of a policy π with lifetime m in environment µ given history æ<tat is
V aev,π
µ,m (æ<tat) :=
X
et
µ(et | æ<tat)

u(et) + V aev,π
µ,m (æ<tatet)

(SAEDT)
and V aev,π
µ,m (æ<tat) := 0 for t > m. Sequential Action-Evidential Decision The-
ory (SAEDT) prescribes adopting an optimal and time consistent policy π for
V aev
µ,m.
It may be argued that SAEDT does not take all available (deliberative)
information into account.
When considering the consequences of an action,
future developments of the environment-policy interactions could also be used
as evidence. That is, we could condition not only on the next action, but on
the future policy as a whole (within the lifetime). In order to deﬁne conditional
probabilities with respect to (deterministic) policies, we deﬁne the following
events. For a given policy π, let Πt:m be the set of all strings consistent with π
between time step t and m:
Πt:m := {æ1:∞| ∀t ≤i ≤m. π(æ<i) = ai}
The likelihood of a next percept et provided a history æ<t and a (future) policy
π followed from time step t until lifetime m (denoted πt:m) is then deﬁned as
µ(et | æ<t, πt:m) := µ(et | æ<t ∩Πt:m).
(4)
This is an atemporal conditional because we are conditioning on future actions
up until the end of the agent’s lifetime.
The conditional (4) is well-deﬁned
because we only take the actions from time step t to m into account; conditioning
on policies with inﬁnite lifetime leads to technical problems because such policies
typically have µ-measure zero.
Deﬁnition 4 (Policy-Evidential Decision Theory). The policy-evidential value
of a policy π with lifetime m in environment µ given history æ<tat is
V pev,π
µ,m
(æ<tat) :=
X
et
µ(et | æ<tat, πt+1:m) ·

u(et) + V pev,π
µ,m
(æ<tatet)

(SPEDT)
and V pev,π
µ,m
(æ<t) := 0 for t > m. Sequential Policy-Evidential Decision Theory
(SPEDT) prescribes adopting an optimal and time consistent policy π for V pev
µ,m.
For one-step decisions (m = t + 1), SAEDT and SPEDT coincide.
To all our embedded agents, past actions constitute evidence about the hid-
den state. For evidential agents, this principle is extended to future actions.
SAEDT and SPEDT diﬀer in how far they extend it. The action-evidential
agent only updates his belief on the action about to take place. In that sense,
he only updates his belief about the next percept on events taking place before
this percept. The policy-evidential agent takes the principle much further, using
“thought-experiments” of what action he would take in hypothetical situations,
most of which will never be realized. This is illustrated in the next example.
Example 5 (Sequential Toxoplasmosis). In our sequential variation of the
toxoplasmosis problem the agent has some probability of encountering a kit-
ten. Additionally, the agent has the option of seeing a doctor (for a fee) and
8

Healthy
No doc
Kitten (0)
Not pet
Healthy, not pet (0)
Pet
Healthy, pet (1)
Doc
Healthy (−4)
Toxo
No doc
Kitten (0)
Not pet
Sick, not pet (−10)
Pet
Sick, pet (−9)
Sick (−10)
Doc
Cured (−4)
0.5
0.5
0.5
0.5
0.5
0.5
0.2
0.8
0.8
0.2
0.2
0.8
Figure 4: One formalization of the sequential toxoplasmosis problem. Dashed
lines connect states indistinguishable to the agent. The numbers on the edges
indicate probabilities of the environment model µ, and the numbers in parenthe-
sis indicate utilities of the associated percepts. In the ﬁrst step, the environment
selects the hidden state that is unknown to the agent. The agent then decides
whether to go to the doctor. If he does not go, he may encounter a kitten which
he can choose to pet or not. SAEDT and SPEDT will disagree whether going
to the doctor is the best option in this scenario. Appendix A contains the full
calculations.
getting tested for the parasite, which can then be safely removed. In the very
beginning, an SPEDT agent updates his belief on the fact that if he encoun-
tered a kitten, he would not pet it, which lowers the probability that he has
the parasite and makes seeing the doctor unattractive. An SAEDT agent only
updates his belief about the parasite when he actually encounters a kitten, and
thus prefers seeing the doctor. See Figure 4 for more details and a graphical
illustration.
The observant reader may ask whether SPEDT could be enticed to make
some percepts unlikely by choosing improbable actions subsequent to them.
For example, could an SPEDT agent decide on a policy of selecting highly
improbable actions in case it rained to make histories with rain less likely? The
answer is no, as most such policies would not be time consistent. If it does rain,
the highly improbable action would usually not the best one, and so the policy
would not be prescribed by Deﬁnition 4.
3.4
Sequential Causal Decision Theory
In sequential causal decision theory we ask what would happen if we causally
intervened on the node at of the next action and ﬁx it to π(æ<t) according to
the policy π. This is expressed by the notation do(at := π(æ<t)), or do(π(æ<t))
for short.
Deﬁnition 6 (Sequential Causal Decision Theory). The causal value of a policy
9

π with lifetime m in environment µ given history æ<tat is
V cau,π
µ,m (æ<tat) :=
X
et∈E
µ(et | æ<t, do(at))

u(et) + V cau,π
µ,m (æ<tatet)

(SCDT)
and V cau,π
µ,m (æ<tat) := 0 for t > m. Sequential Causal Decision Theory (SCDT)
prescribes adopting an optimal and time consistent policy π for V cau
µ,m.
For sequential evidential decision theory we discussed two versions (SAEDT)
and (SPEDT), based on next action and future policy respectively. In SCDT
we perform the causal intervention do(at := π(æ<t)). We could also consider
a policy-causal decision theory by replacing µ(et | æ<t, do(at)) with µ(et |
æ<t, do(πt:m)) in Deﬁnition 6. The causal intervention do(πt:m)) of a policy
π between time step t and time step m is deﬁned as as
µ(et | æ<t, do(πt:m)) :=
X
et+1:m
µ(et:m | æ<t, do(at := π(æ<t), . . . , am := π(æ<m))).
(5)
However, since the interventions are causal, we do not get any extra evidence
from the future interventions. Therefore policy-causal decision theory is the
same as action-causal decision theory:
Proposition 7 (Policy-Causal = Action-Causal). For all histories æ<t ∈(A ×
E)∗and all et ∈E, we have µ(et | æ<t, do(πt:m)) = µ(et | æ<t, do(π(æ<t))).
We defer the proof to the end of this section.
The following two exam-
ples illustrate the diﬀerence between SCDT and SAEDT/SPEDT in sequential
settings.
Example 8 (Newcomb with Precommitment). In this variation to Newcomb’s problem
the agent ﬁrst has the option to pay $300,000 to sign a contract that binds the
agent to pay $2000 in case of two-boxing. An SAEDT or SPEDT agent knows
that he will one-box anyways and hence has no need for the contract. An SCDT
agent knows that she favors two-boxing, but signs the contract only if this oc-
curs before the prediction is made (so it has a chance of causally aﬀecting the
prediction). With the contract in place, one-boxing is the dominant action, and
thus the SCDT agent is predicted to one-box.
Example 9 (Newcomb with Looking). In this variation to Newcomb’s problem
the agent may look into the opaque box before making the decision which box
to take. An SCDT agent is indiﬀerent towards looking because she will take
both boxes anyways. However, an SAEDT or SPEDT agent will avoid looking
into the box, because once the content is revealed he two-boxes.
3.5
Expansion over the Hidden State
The diﬀerence between sequential versions of EDT and CDT is how they update
their prediction of a next percept et (Deﬁnitions 3, 4 and 6). The following
proposition expands the diﬀerent beliefs in terms of the hidden state.
10

Proposition 10. For all histories æ<tatet ∈(A × E)∗the following holds for
the next-percept beliefs of SAEDT, SPEDT and SCDT respectively:
µ(et | æ<tat) =
X
s∈S
µ(s | æ<tat)µ(et | s, æ<tat)
(6)
µ(et | æ<t, πt:m) =
X
s∈S
µ(s | æ<t, πt:m)µ(et | s, æ<t, πt:m)
(7)
µ(et | æ<t, do(at)) =
X
s∈S
µ(s | æ<t)µ(et | s, æ<tat)
(8)
Proof. For the action-evidential conditional we take the joint distribution with
s, and then split oﬀet:
µ(et | æ<tat) =
P
s∈S µ(s, æ<tatet)
µ(æ<tat)
=
P
s∈S µ(s, æ<tat)µ(et | s, æ<tat)
µ(æ<tat)
=
X
s∈S
µ(s | æ<tat)µ(et | s, æ<tat)
Similarly for the policy-evidential conditional:
µ(et | æ<t, πt:m) =
P
s∈S µ(s, æ<tπ(æ<t)et, πt+1:m)
µ(æ<t, πt:m)
=
P
s∈S µ(s, æ<tπ(æ<t), πt+1:m)µ(et | s, æ<tπ(æ<t), πt+1:m)
µ(æ<t, πt:m)
=
P
s∈S µ(s, æ<t, πt:m)µ(et | s, æ<tπ(æ<t), πt+1:m)
µ(æ<t, πt:m)
=
X
s∈S
µ(s | æ<t, πt:m)µ(et | s, æ<tπ(æ<t), πt+1:m)
=
X
s∈S
µ(s | æ<t, πt:m)µ(et | s, æ<t, πt:m)
For the causal conditional we turn to the rules of the do-operator [Pea09,
Thm. 3.4.1]. The ﬁrst equality below holds by deﬁnition. In the denominator
of the second equality we can use Rule 3 (deletion of actions) to remove do(at)
because the do-operator removes all incoming edges to at and makes at inde-
pendent of the history æ<t. In the numerator of the second equality we use the
deﬁnition of do (1):
µ(et | æ<t, do(at)) = µ(æ<t, et | do(at))
µ(æ<t | do(at))
=
P
s∈S µ(s, æ<t)µ(et | s, æ<tat)
µ(æ<t)
=
X
s∈S
µ(s | æ<t)µ(et | s, æ<tat)
Proposition 10 shows that between SCDT and SAEDT, the diﬀerence in
opinion about et only depends on diﬀerences in their (acausal) posterior belief
µ(s | . . .) about the hidden state. SCDT and SAEDT thus become equivalent in
scenarios where there is only one hidden state s∗with µ(s∗) = 1, as this renders
11

µ(s∗| æ<t) = µ(s∗| æ<tat) = µ(s∗) = 1. SPEDT, on the other hand, may
disagree with the other two also after a hidden state has been ﬁxed.
From a problem modeler’s perspective, it is also instructive to consider
the eﬀect of moving uncertainty between the hidden state and environmen-
tal stochasticity. For two diﬀerent environment models µ and µ′, the action
and percept probabilities may be identical (i.e., µ(at | æ<t) = µ′(at | æ<t) and
µ(et | æ<tat) = µ′(et | æ<tat)) even though µ and µ′ have non-isomorphic sets
of hidden states S and S′. For example, given any µ, an environment model
µ′ with a single hidden state s0, µ′(s0) = 1, may be constructed from µ by
µ′(s0, æ<t) := P
s∈S µ(s, æ<t). The transformation will not aﬀect SAEDT and
SPEDT, as the deﬁnitions of their value functions only depends on the ‘ob-
servable’ action- and percept-probabilities µ(at | æ<t) and µ(et | æ<tat) which
are preserved between µ and µ′. But the transformation will change SCDT’s
behavior in any µ where SCDT disagrees with SAEDT, as SCDT and SAEDT
are equivalent in µ′ that only has a single hidden state. That SCDT depends
on what uncertainty is captured by the hidden state is unsurprising given that
the hidden state has a special place in the causal structure of the problem. Ul-
timately, the modeler must decide what uncertainty to put in the hidden state,
and what to attribute to environmental stochasticity. A general principle for
how to do this is still an open question [SF14b].
The value functions of SAEDT, SPEDT and SCDT can be rewritten in the
following iterative forms, where the latter form uses Proposition 10. Numbers
above equality signs reference a justifying equation. Let ai := π(æ<i) for i ≥t:
V aev,π
µ,m (æ<t) =
m
X
k=t
X
et:k
u(ek)
k
Y
i=t
µ(ei | æ<iai)
(9)
(6)
=
m
X
k=t
X
et:k
u(ek)
k
Y
i=t
X
s∈S
µ(s | æ<iai)µ(ei | s, æ<iai)
(10)
V pev,π
µ,m
(æ<t) =
m
X
k=t
X
et:k
u(ek)
k
Y
i=t
µ(ei | æ<i, πi:m)
(11)
(7)
=
m
X
k=t
X
et:k
u(ek)
k
Y
i=t
X
s∈S
µ(s | æ<iπi:m)µ(ei | s, æ<i, πi:m)
(12)
V cau,π
µ,m (æ<t) =
m
X
k=t
X
et:k
u(ek)
k
Y
i=t
µ(ei | æ<i, do(ai))
(13)
(8)
=
m
X
k=t
X
et:k
u(ei)
k
Y
i=t
X
s∈S
µ(s | æ<i)µ(ei | s, æ<iai)
(14)
12

SAEDT
SPEDT
SCDT
Nwcb
1-box
1-box
2-box
Nwcb w/ precommit not commit, 1-box not commit, 1-box commit, 1-box
Nwcb w/ looking
not look, 1-box
not look, 1-box
indiﬀerent, 2-box
Toxoplasmosis
not pet
not pet
pet
Seq. Toxoplasmosis
doc, not pet
no doc, not pet
doc, pet
Table 1:
Decisions made by SAEDT, SPEDT and SCDT in Example 1,
Example 2, Example 5, Example 8, and Example 9.
The latter three exam-
ples are sequential. Winning moves are in italics; in Newcomb with looking the
winning move is to be indiﬀerent and one-box. Because Savage decision theory
is dualistic, these problems cannot be properly formalized in it.
Proof of Proposition 7. By the deﬁnition (5) of do(πt:m),
µ(et | æ<t, do(πt:m)) =
X
et+1:m
µ(et:m | æ<t, do(at := π(æ<t), . . . , am := π(æ<m)))
=
X
s,et+1:m
µ(s | æ<t)µ(et:m | s, æ<t, do(π(æ<t), . . . , π(æ<m)))
(1)
=
X
s,et+1:m
µ(s | æ<t)
m
Y
i=t
µ(ei | s, æ<iπ(æ<i))
=
X
s
µ(s | æ<t)µ(et | s, æ<tπ(æ<t))
(8)
= µ(et | æ<t, do(π(æ<t)))
The second equality follows from the equivalence P( · ) = P
s P(s)P( · | s)
applied to the distribution µ( · | æ<t, do(at := π(æ<t), . . . , am := π(æ<m))),
and the third equality by (repeated) application of (1) to µ(æt:m | s, æ<t) =
Qm
i=t µ(ai | s, æ<i)µ(ei | s, æ<iai).
4
Discussion
Our paper is a ﬁrst stab at the problem of how physicalistic agents should make
sequential decisions. CDT and EDT provide an existing basis for non-dualistic
decision making, which we extended to the sequential setting. There are two
natural ways for making sequential evidential decisions: do I update my beliefs
about the hidden state based on my next action (‘what I do next’, SAEDT)
or my whole policy (‘the kind of agent I am’, SPEDT)? By Proposition 7, this
distinction does not exist for causal decision theory, because with that theory
the agent does not consider its own actions evidence at all. Therefore we have
only one version of sequential causal decision theory, SCDT.
To illustrate the diﬀerences between the decision theories, we discussed three
variants of Newcomb’s problem (Example 1, Example 8, and Example 9) and
two variants of the toxoplasmosis problem (Example 2 and Example 5). The
formal speciﬁcation of these examples can be found in Appendix A. We imple-
mented SCDT, SAEDT, and SPEDT; Table 1 shows their behavior on those
13

examples.4
So which decision theory is better? The answer to this question depends
on which decision you consider to be correct (or even rational) in each of the
problems. We posit that ultimately, what counts is not whether your decision
algorithm is theoretically pleasing, but whether you win. Winning means getting
the most utility. If maximizing utility involves making crazy decisions, then this
is what you should do!
In Newcomb’s problem, winning means one-boxing, because you end up
richer. In the toxoplasmosis problem, winning means petting the kitten, be-
cause that yields more utility. (S)CDT performs suboptimally in the Newcomb
variations, while the evidential decision theories perform suboptimally in the
toxoplasmosis variations. This entails that neither CDT nor EDT are the ﬁnal
answer to the problem of non-dualistic decision making.
Furthermore, neither CDT nor EDT agents are fully physicalistic: they do
not model the environment to contain themselves [SF14b]. For example, when
playing a prisoner’s dilemma against your own source code [SF15], your oppo-
nent defects if and only if you defect. This logical connection between your
action and your opponent’s is disregarded in the formalization based on causal
graphical models that we discuss here because it is not causal.
Timeless decision theory [Yud10] and updateless decision theory [SF14b] are
recent attempts of more physicalistic decision theories. However, so far both
have eluded explicit formalization [SF15]. We conclude that ﬁnding a physical-
istic decision theory remains an important open problem in artiﬁcial intelligence
research.
Acknowledgements.
This work was in part supported by ARC grant
DP120100950. It started at a MIRIxCanberra workshop sponsored by the Ma-
chine Intelligence Research Institute. Mayank Daswani and Daniel Filan con-
tributed in the early stages of this paper and we thank them for interesting
discussions and helpful suggestions. We also thank Nate Soares for useful feed-
back.
References
[Ahm14]
Arif Ahmed. Evidence, Decision and Causality. Cambridge Univer-
sity Press, 2014.
[Alt13]
Alex Altair. A comparison of decision algorithms on Newcomblike
problems. Technical report, Machine Intelligence Research Institute,
2013. http://intelligence.org/files/Comparison.pdf.
[Bos14]
Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford
University Press, 2014.
[Bri14]
Rachael Briggs.
Normative theories of rational choice: Expected
utility. In Edward N. Zalta, editor, The Stanford Encyclopedia of
Philosophy. Fall 2014 edition, 2014.
4Source code available at http://jan.leike.name/.
14

[Ega07]
Andy Egan. Some counterexamples to causal decision theory. The
Philosophical Review, pages 93–114, 2007.
[GH78]
Allan Gibbard and William L Harper. Counterfactuals and two kinds
of expected utility.
In Foundations and Applications of Decision
Theory, pages 125–162. Springer, 1978.
[Jef83]
Richard C Jeﬀrey. The Logic of Decision. University of Chicago
Press, 2nd edition, 1983.
[Joy99]
James M Joyce. The Foundations of Causal Decision Theory. Cam-
bridge University Press, 1999.
[Lew81]
David Lewis. Causal decision theory. Australasian Journal of Phi-
losophy, 59(1):5–30, 1981.
[LFY+14] Patrick LaVictoire, Benja Fallenstein, Eliezer Yudkowsky, Mihaly
Barasz, Paul Christiano, and Marcello Herreshoﬀ. Program equilib-
rium in the prisoner’s dilemma via L¨ob’s theorem. In AAAI Work-
shop on Multiagent Interaction without Prior Coordination, 2014.
[LH14]
Tor Lattimore and Marcus Hutter. General time consistent discount-
ing. Theoretical Computer Science, 519:140–154, 2014.
[Noz69]
Robert Nozick. Newcomb’s problem and two principles of choice. In
Essays in honor of Carl G. Hempel, pages 114–146. Springer, 1969.
[OR12]
Laurent Orseau and Mark Ring. Space-time embedded intelligence.
In Artiﬁcial General Intelligence, pages 209–218. Springer, 2012.
[Pea09]
Judea Pearl. Causality. Cambridge University Press, 2nd edition,
2009.
[RDT+15] Stuart Russell,
Daniel Dewey,
Max Tegmark, Janos Kramar,
and Richard Mallah. Research priorities for robust and beneﬁcial
artiﬁcial intelligence. Technical report, Future of Life Institute, 2015.
http://futureoflife.org/static/data/documents/research_priorities.pdf.
[RN10]
Stuart J Russell and Peter Norvig. Artiﬁcial Intelligence. A Modern
Approach. Prentice Hall, 3rd edition, 2010.
[Sav72]
Leonard J Savage. The Foundations of Statistics. Dover Publica-
tions, 1972.
[SF14a]
Nate Soares and Benja Fallenstein.
Aligning superintelligence
with human interests:
A technical research agenda.
Tech-
nical
report,
Machine
Intelligence
Research
Institute,
2014.
http://intelligence.org/files/TechnicalAgenda.pdf.
[SF14b]
Nate Soares and Benja Fallenstein. Toward idealized decision theory.
Technical report, Machine Intelligence Research Institute, 2014.
http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf.
[SF15]
Nate Soares and Benja Fallenstein. Counterpossibles as necessary for
decision theory. In Artiﬁcial General Intelligence. Springer, 2015.
15

[Sky82]
Brian Skyrms. Causal decision theory. The Journal of Philosophy,
pages 695–711, 1982.
[Wei12]
Paul Weirich. Causal decision theory. In Edward N. Zalta, editor,
The Stanford Encyclopedia of Philosophy. Winter 2012 edition, 2012.
[Yud08]
Eliezer Yudkowsky. Artiﬁcial intelligence as a positive and negative
factor in global risk. In Nick Bostrom and Milan M ´Cirkovi´c, editors,
Global Catastrophic Risks, pages 308–345. Oxford University Press,
2008.
[Yud10]
Eliezer
Yudkowsky.
Timeless
decision
theory.
Techni-
cal
report,
Machine
Intelligence
Research
Institute,
2010.
http://intelligence.org/files/TDT.pdf.
List of Notation
:=
deﬁned to be equal
N
the natural numbers, starting with 0
R
the real numbers
ε
a small positive real number
A
the (ﬁnite) set of possible actions
E
the (ﬁnite) set of possible percepts
S
the set of hidden states
u
the utility function u : E →[0, 1]
at
the action in time step t
et
the percept in time step t
æ<t
the ﬁrst t −1 interactions, a1e1a2e2 . . . at−1et−1
æi:k
the interactions between and including time step i and time step k,
aieiai+1ei+1 . . . akek
æ1:∞
a history of inﬁnite length
s
a hidden state
π
a deterministic policy, i.e., a function π : (A × E)∗→A
πt:k
policy π restricted to the time steps between and including t and k
V aev,π
µ,m
action-evidential value of policy π in environment µ up to time step
m, deﬁned in (SAEDT)
V pev,π
µ,m
policy-evidential value of policy π in environment µ up to time step
m, deﬁned in (SPEDT)
V cau,π
µ,m
causal value of policy π in environment µ up to time step m, deﬁned
in (SCDT)
k, i
time steps, natural numbers
t
(current) time step
m
lifetime of the agent
Pa
distribution over percepts induced by action a in SDT
P
distribution over percepts and actions in one-shot decision making
µ
an accurate environment model
16

A
Examples
This section contains the formal calculations for Example 1, Example 2, Example 5,
Example 8, and Example 9. These calculations are also available as Python code
at http://jan.leike.name/.
Example 11 (Newcomb’s Problem). This is a formalization of Example 1.
• S := {E, F} where E means the opaque box is empty and F means the
opaque box is full
• A := {B1, B2} where B1 means one-boxing and B2 means two-boxing
• E := {O0, OT , OM, OMT }
• u(O0) := 0, u(OT ) := 1,000, u(OM) := 1,000,000, u(OMT ) := 1,001,000
Let ε > 0 be a small constant denoting the accuracy of the predictor. Because
the environment has to assign non-zero probability to all actions, ε must be
strictly positive. The environment’s distribution µ is deﬁned as follows.
µ(E) = µ(F) = 0.5
µ(OT | E, B2) = 1
µ(B1 | F) = µ(B2 | E) = 1 −ε
µ(O0 | E, B1) = 1
µ(B1 | E) = µ(B2 | F) = ε
µ(OMT | F, B2) = 1
µ(OM | F, B1) = 1
By Bayes’ rule,
µ(F | B1) =
µ(B1 | F)µ(F)
P
s∈S µ(B1 | s)µ(s) =
1
2(1 −ε)
1
2(1 −ε) + 1
2ε = (1 −ε)
which also gives µ(E | B1) = ε. Similarly, µ(F | B2) = ε and µ(E | B2) = 1 −ε.
For EDT we use equation (EDT) to compute the value of an action. Since
the percept e1 is generated deterministically, µ(e | s, a) only attains values 0 or
1. We therefore omit it in the calculation below. For action B1 we get
V evi,B1
µ,1
:=
X
e∈E
µ(e | B1)u(e) =
X
e∈E
X
s∈S
µ(e | s, B1)µ(s | B1)u(e)
= µ(E | B1)u(O0) + µ(F | B1)u(OM)
= ε · 0 + (1 −ε) · 1, 000, 000
For action B2 we get
V evi,B2
µ,1
:=
X
e∈E
µ(e | B2)u(e) =
X
e∈E
X
s∈S
µ(e | s, B2)µ(s | B2)u(e)
= µ(E | B2)u(OT ) + µ(F | B2)u(OMT )
= (1 −ε) · 1, 000 + ε · 1, 001, 000
= 1, 000 + ε · 1, 000, 000
For ε < 49.95 (just slightly better than random guessing), we get that EDT
favors B1 over B2:
V evi,B1
µ,1
= (1 −ε) · 1, 000, 000 > 500, 500 > 1, 000 + ε · 1, 000, 000 = V evi,B2
µ,1
17

For CDT we use equation (CDT) to compute the value of an action. For
action B1 we get
V cau,B1
µ,1
:=
X
e∈E
µ(e | do(B1))u(e) =
X
e∈E
X
s∈S
µ(e | s, B1)µ(s)u(e)
= µ(E)u(O0) + µ(F)u(OM)
= 0.5 · 0 + 0.5 · 1, 000, 000 = 500, 000
For action B2 we get
V cau,B2
µ,1
:=
X
e∈E
µ(e | do(B2))u(e) =
X
e∈E
X
s∈S
µ(e | s, B2)µ(s)u(e)
= µ(E)u(OT ) + µ(F)u(OMT )
= 0.5 · 1, 000 + 0.5 · 1, 001, 000 = 500, 500
We get that CDT favors B2 over B1 regardless of the prediction accuracy ε:
V evi,B1
µ,1
= 500, 000 < 500, 500 = V evi,B2
µ,1
Moreover, CDT prefers B2 regardless of the prior over µ(E). Two-boxing is the
dominant action because it yields $1,000 more regardless of the hidden state.
Example 12 (Newcomb with Looking). This is a formalization of Example 9;
it extends Example 11.
In the ﬁrst time step, the agent gets to choose between looking into the box
(L) and not looking (N). If the agent looks, the subsequent percept will be E
or F, depending on whether the box is empty (E) or full (F). If the agent does
not look, the subsequent percept will be 0. All three of these percepts E, F,
and 0 have zero utility.
In the second time step the agent chooses to one-box (B1) or to two-box
(B2). The payoﬀs are then based on the boxes’ contents as in Example 11.
• S := {E, F} where E means the opaque box is empty and F means the
opaque box is full
• A := {B1, B2} where B1 means one-boxing and B2 means two-boxing,
L := B1 means looking into the box and N := B2 means not looking (the
set of actions has to be the same for all time steps)
• E := {E, F, 0, O0, OT , OM, OMT }
• u(O0) := 0, u(OT ) := 1,000, u(OM) := 1,000,000, u(OMT ) := 1,001,000,
u(E) := u(F) := u(0) := 0
Let ε > 0 be a small constant denoting the prediction accuracy. Because the
environment has to assign non-zero probability to all actions, ε must be strictly
positive. The environment’s distribution µ is deﬁned as follows. Question marks
18

stand for single actions or percepts whose value is irrelevant.
µ(E) = µ(F) = 0.5
µ(E | E, L) = 1
µ(L | F) = µ(L | E) = 0.5
µ(0 | E, N) = 1
µ(N | F) = µ(N | E) = 0.5
µ(F | F, L) = 1
µ(B1 | E, ??) = ε
µ(0 | F, N) = 1
µ(B1 | F, ??) = 1 −ε
µ(O0 | E, ??B1) = 1
µ(B2 | E, ??) = 1 −ε
µ(OT | E, ??B2) = 1
µ(B2 | F, ??) = ε
µ(OM | F, ??B1) = 1
µ(OMT | F, ??B2) = 1
The environment’s game tree is given as follows, where dashed lines connect
states indistinguishable by the agent (also known as information sets):
E
L
E
B1
0
B2
1,000
N
0
B1
0
B2
1,000
F
L
F
B1
1,000,000
B2
1,001,000
N
0
B1
1,000,000
B2
1,001,000
0.5
0.5
0.5
0.5
0.5
0.5
1
1
1
1
ε
1 −ε
ε
1 −ε
1 −ε
ε
1 −ε
ε
Using Bayes’ rule, we calculate the following conditional probabilities of the
hidden state given a history a1 or a1e1a2:
0.5 = µ(E | L) = µ(F | L) = µ(E | N) = µ(F | N)
1 = µ(E | LEB1) = µ(E | LEB2) = µ(F | LFB1) = µ(F | LFB1)
ε = µ(E | N0B1) = µ(F | N0B2)
1 −ε = µ(E | N0B2) = µ(F | N0B1)
Next, we write out the formula for SAEDT for a horizon of 2 based on (10).
The ﬁrst percept has no utility, which simpliﬁes the equation.
V aev,π
µ,2
=
X
e1:2
u(e2)
 X
s∈S
µ(s | a1)µ(e1 | s, a1)
!  X
s∈S
µ(s | æ1a2)µ(e2 | s, æ1a2)
!
where a1 = π(ǫ) and a2 = π(æ1). The formula for SPEDT for a horizon of 2
based on (12) is as follows.
V pev,π
µ,2
=
X
e1:2
u(e2)
P
s∈S µ(sa1e1π(a1e1))
P
s∈S
P
e∈E µ(sa1eπ(a1e))
X
s∈S
µ(s | æ1π2)µ(e2 | s, æ1a2)
19

with π1:2 and π2 deﬁned according to (4). The formula for SCDT for a horizon
of 2 based on (14) is as follows.
V cau,π
µ,2
=
X
e1:2
u(e2)
 X
s∈S
µ(s)µ(e1 | s, a1)
!  X
s∈S
µ(s | æ1)µ(e2 | s, æ1a2)
!
where a1 = π(ǫ) and a2 = π(æ1).
There are six diﬀerent possible policies:
• Look and always one-box (curious one-boxer)
• Look and always two-box (curious two-boxer)
• Don’t look and one-box (incurious one-boxer)
• Don’t look and two-box (incurious two-boxer)
• Look and one-box iﬀthe box is empty (paradox-lover)
• Look and one-box iﬀthe box full (fatalistic)
Using the formulas above we can calculate their value. We use ε := 0.01.
V aev,π
µ,2
V pev,π
µ,2
V cau,π
µ,2
Curious one-boxer
500,000
990,000
500,000
Curious two-boxer
501,000
11,000
501,000
Incurious one-boxer
990,000
990,000
500,000
Incurious two-boxer
11,000
11,000
501,000
Paradox-lover
500,500
500,500
500,500
Fatalistic
500,500
500,500
500,500
The highest values are displayed in italics. The incurious one-boxer has the
highest action-evidential value. The curious one-boxer and the incurious one-
boxer have the highest policy-evidential value. However, of these two policies
only the incurious one-boxer is a time-consistent policy for SPEDT, because the
agent wants to two-box after looking into the box:
V aev,B1
µ,1
(LF) = V pev,B1
µ,1
(LF) = 1, 000, 000
V aev,B2
µ,1
(LF) = V pev,B2
µ,1
(LF) = 1, 001, 000
V aev,B1
µ,1
(LE) = V pev,B1
µ,1
(LE) = 0
V aev,B2
µ,1
(LE) = V pev,B2
µ,1
(LE) = 1, 000
The curious two-boxer and the incurious two-boxer have the highest causal
value, and they are both time-consistent for SCDT.
Example 13 (Newcomb with Precommitment). This is a formalization of
Example 8, it extends Example 11.
In the ﬁrst time step, the agent gets to choose between signing the contract
(S) and not signing (N). If the agent signs, the subsequent percept will be C,
which costs $300,000, and the prediction will be updated to one-boxing. If the
agent does not sign, the subsequent percept will be 0 with zero utility.
20

In the second time step the agent chooses to one-box (B1) or to two-box
(B2). The payoﬀs are then based on the boxes’ contents as in Example 11. If
the agent signed the contract and choses two boxes, this incurs an additional
cost of $2,000.
• S := {E, F} where E means the opaque box is empty and F means the
opaque box is full
• A := {B1, B2} where B1 means one-boxing and B2 means two-boxing,
S := B1 means signing the contract and N := B2 means not signing (the
set of actions has to be the same for all time steps)
• E := {C, 0, O0, OT , O−T , OM, OMT , OM−T }
• u(O0) := 0, u(OT ) := 1, 000, u(O−T ) := −1, 000 u(OM) := 1, 000, 000,
u(OMT ) := 1, 001, 000, u(OM−T ) := 999, 000, u(C) := −300, 000, u(0) :=
0
Let ε > 0 be a small constant denoting the prediction accuracy. Because the
environment has to assign non-zero probability to all actions, ε must be strictly
positive. The environment’s distribution µ is deﬁned as follows. Question marks
stand for single actions or percepts whose value is irrelevant.
µ(E) = µ(F) = 0.5
µ(C | E, S) = 1
µ(S | F) = µ(S | E) = 0.5
µ(0 | E, N) = 1
µ(N | F) = µ(N | E) = 0.5
µ(C | F, S) = 1
µ(B1 | E, N0) = ε
µ(0 | F, N) = 1
µ(B1 | F, N0) = 1 −ε
µ(O0 | E, N0B1) = 1
µ(B2 | E, N0) = 1 −ε
µ(OT | E, N0B2) = 1
µ(B2 | F, N0) = ε
µ(OM | F, N0B1) = 1
µ(B2 | ?, SC) = ε
µ(OMT | F, N0B2) = 1
µ(B1 | ?, SC) = 1 −ε
µ(OM | E, SCB1) = 1
µ(OM−T | E, SCB2) = 1
The environment’s game tree is given as follows:
21

E
S
C
B1
700,000
B2
699,000
N
0
B1
0
B2
1,000
F
S
C
B1
700,000
B2
699,000
N
0
B1
1,000,000
B2
1,001,000
0.5
0.5
0.5
0.5
0.5
0.5
1
1
1
1
ε
1 −ε
1 −ε
ε
1 −ε
ε
1 −ε
ε
There are four diﬀerent possible policies:
• Sign the contract and one-box (signing one-boxer)
• Sign the contract and two-box (signing two-boxer)
• Don’t sign the contract and one-box (refusing one-boxer)
• Don’t sign the contract and two-box (refusing two-boxer)
Using the formulas from Example 12 we can calculate their value.
We use
ε := 0.01.
V aev,π
µ,2
V pev,π
µ,2
V cau,π
µ,2
Signing one-boxer
700,000
700,00
700,000
Signing two-boxer
699,000
699,000
699,000
Refusing one-boxer
990,000
990,000
500,000
Refusing two-boxer
11,000
11,000
501,000
The highest values are displayed in italics. Both SAEDT and SPEDT refuse
the contract: the refusing one-boxer has the highest action-evidential and the
highest policy-evidential value. SCDT signs the contract and then one-boxes:
the signing one-boxer has the highest causal value.
Example 14 (Toxoplasmosis). This is a formalization of Example 2.
• S := {T, H} where T means having the toxoplasmosis parasite and H
means being healthy
• A := {P, N} where P means petting and N means not petting
• E := {P&T, N&T, P&H, N&H} where the percepts just reﬂect the action
and hidden state
22

• u(P&T ) := −9, u(N&T ) := −10, u(P&H) := 1, u(N&H) := 0 where
petting gives a utility of 1 and suﬀering from the parasite gives a utility
of −10
The environment’s distribution µ is deﬁned as follows.
µ(T ) = µ(H) = 0.5
µ(P&T | P, T ) = 1
µ(P | T ) = 0.8
µ(N&T | N, T ) = 1
µ(N | T ) = 0.2
µ(P&H | P, H) = 1
µ(P | H) = 0.2
µ(N&H | N, H) = 1
µ(N | H) = 0.8
Using Bayes’ rule, we calculate the following conditional probabilities.
µ(T | P) = 0.8
µ(H | P) = 0.2
µ(T | N) = 0.2
µ(H | N) = 0.8
We consider EDT ﬁrst. Since the percept e1 is generated deterministically,
µ(e | s, a) only attains values 0 or 1. We therefore omit it in the calculation
below. For action P (petting) we get
V evi,P
µ,1
:=
X
e∈E
µ(e | P)u(e) =
X
e∈E
X
s∈S
µ(e | s, P)µ(s | P)u(e)
= µ(T | P)u(T &P) + µ(H | P)u(P&H)
= 0.8 · (−9) + 0.2 · 1 = −7
For action N (not petting) we get
V evi,N
µ,1
:=
X
e∈E
µ(e | N)u(e) =
X
e∈E
X
s∈S
µ(e | s, N)µ(s | N)u(e)
= µ(T | N)u(T &N) + µ(H | N)u(H&N)
= 0.2 · (−10) + 0.8 · 0 = −2
Therefore we get that EDT favors N over P:
V evi,P
µ,1
= −7 < −2 = V evi,N
µ,1
For CDT we get for action P (petting)
V cau,P
µ,1
:=
X
e∈E
µ(e | do(P))u(e) =
X
e∈E
X
s∈S
µ(e | s, P)µ(s)u(e)
= µ(T )u(T &P) + µ(N)u(N&P)
= 0.5 · (−9) + 0.5 · 1 = −4
For action N (not petting) we get
V cau,N
µ,1
:=
X
e∈E
µ(e | do(N))u(e) =
X
e∈E
X
s∈S
µ(e | s, N)µ(s)u(e)
= µ(T )u(T &N) + µ(H)u(H&N)
= 0.5 · (−10) + 0.5 · 0 = −5
We get that CDT favors P over N:
V evi,P
µ,1
= −4 > −5 = V evi,N
µ,1
23

Example 15 (Sequential Toxoplasmosis). We here formalize a version of Example 5.
First the agent chooses whether to go to the doctor. Going to the doctor incurs
a fee, but removes the risk of getting sick. Agents that do not go to the doctor
have a chance of meeting a kitten. If they meet it, they can choose to pet it or
not; infected agents are more likely to pet the kitten. The example is intended
to elucidate the diﬀerence between SAEDT and SPEDT, whose decisions we
will calculate in detail. We will not calculate the action of SCDT.
• S := {T (oxoplasmosis), H(ealthy)}.
• A := {Y (es), N(o)}. In this example, an action is taken twice. We use Y1
and Y2, and N1 and N2, to distinguish between the ﬁrst and the second
action.
• E := {C(ured), K(itten), S(ick, not pet kitten), s(ick, pet kitten), P(et,
not sick), 0(neutral)}
• u(C) = −4, u(K) := 0, u(S) := −10, u(s) := −9, u(P) := 1, and u(0) = 0.
The environment’s game tree is given as follows, where dashed lines connect
states indistinguishable by the agent.
H
N1
K (0)
N2
0 (0)
Y2
P (1)
Y1
C (−4)
T
N1
K (0)
N2
S (−10)
Y2
s (−9)
S (−10)
Y1
C (−4)
0.5
0.5
0.5
0.5
0.5
0.5
0.2
0.8
0.8
0.2
0.2
0.8
First, the environment chooses whether to infect the agent or not with the
parasite with probability 0.5. The agent then decides whether to see the doctor.
If the agent sees the doctor, this incurs a (utility) fee of −4, but the agent will
not be sick. If the agent does not see the doctor, there will be a kitten with
probability 0.2 (or 1) and the agent will pet it with probability 0.8 (or 0.2) if
the parasite is present (or not). If there is no kitten, the next percept is S or 0
depending on whether the agent is infected or not. The agent gets −10 utility
if infected and did not see the doctor, and gets +1 utility for petting the kitten.
We want to compare the choices of SAEDT and SPEDT. Their two-step
value functions are
V aev,π
µ,2
=
X
e1
µ(e1 | a1)
 u(e1) + V aev,π
µ,2
(a1e1)

24

V pev,π
µ,2
=
X
e1
µ(e1 | π1:2)
 u(e1) + V pev,π
µ,2
(a1e1)

where the second step value functions
V aev,π
µ,2
(a1e1) = V pev,π
µ,2
(a1e1) =
X
e2
µ(e2 | a1e1a2) · u(e2)
are the same for both decision theories. They only diﬀer by assigning probability
µ(e1 | a1) and µ(e1 | π1:2) to the ﬁrst percept, respectively.
Since not petting is always better than petting for evidential agents (the
evidence towards not having the disease weighs stronger than the extra utility),
the only policies that are potentially optimal and time consistent are π1 := N1N2
and π2 := Y1.
First percept.
For π1 the occurring action-evidential quantities µ(e1 | a1)
are
µ(N1) =
X
s∈S
µ(s, N1) = µ(T, N1) + µ(H, N1) = 1
4 + 1
4 = 1
2
µ(e1 = S | N1) =
P
s∈S µ(s, N1S)
µ(N1)
= µ(T, N1S)
µ(N1)
=
1
2 · 1
2 · 4
5
1
2
= 2
5
µ(e1 = K | N1) = 1 −µ(S | N1) = 3
5
and the occurring policy-evidential quantities µ(e1 | π1:2) are
µ(N1N2) =
X
s,e1,e2
µ(s, N1e1N2e2)
= µ(T, N1KN2S) + µ(T, N1SN20) + µ(H, N1KN20)
=
1
100 + 1
10 + 1
5 = 31
100
µ(e1 = K | N1N2) =
P
s,e2 µ(s, N1KN2e2)
µ(N1, N2)
= µ(T, N1KN2S) + µ(H, N1KN20)
µ(N1N2)
=
1
100 + 1
5
31
100
= 21
31
µ(e1 = S | N1N2) = 1 −µ(K | N1N2) = 20
31
The policy π2 = {Y1} always goes to the doctor for the treatment, and so
µ(e1 = C | Y1) = 1
for both AESDT and PESDT.
25

Second percept.
With the policy π2, the second percept is always empty.
Under π1, the only action sequence that can reach the second percept is N1KN2
µ(N1KN2) =
X
s
µ(s, N1KN2) = µ(T, N1KN2) + µ(H, N1KN2)
=
1
100 + 1
5 = 21
100
µ(e2 = S | N1KN2) =
P
s µ(s, N1KN2S)
µ(N1KN2)
= µ(T, N1KN2S)
µ(N1KN2)
=
1
100
21
100
= 1
21.
Value Functions.
We start by evaluating the recursive deﬁnition from the
second time step.
The second step value functions are 0 for π1 and for the
history N1S for π2. For the history N1K, both SAEDT and PAEDT assign the
following identical value to π2:
V aev,π1
µ,2
(N1K) = V pev,π
µ,2
(N1K) =
X
e2
µ(e2 | N1KN2) · u(e2)
= µ(e2 = S | N1KN2) · u(S) + µ(e2 = 0 | N1KN2) · u(0)
= 1
21 · (−10) + 20
21 · 0 = −10
21
The ﬁrst step value functions now evaluates to:
V aev,π1
µ,2
=
X
e1
µ(e1 | N1) ·
 u(e1) + V aev,π1
µ,2
(N1e1)

= µ(S | N1) · (u(S) + V aev,π1
µ,2
(N1S))
+ µ(K | N1) · (u(K) + V aev,π1
µ,2
(N1K))
= 2
5 · (−10 + 0) + 3
5 · (0 −10
21) = −30
7 ≈−4.3
V pev,π1
µ,2
=
X
e1
µ(e1 | N1) ·
 u(e1) + V pev,π1
µ,2
(N1e1)

= µ(S | N1N2) · (u(S) + V pev,π1
µ,2
(N1S))
+ µ(K | N1N2) · (u(K) + V pev,π1
µ,2
(N1K))
= 10
31 · (−10 + 0) + 21
31 · (0 −10
21) = −110
31 ≈−3.5
Meanwhile, the value of π2 is
V aev,π2
µ,2
= V aev,π2
µ,2
=
X
e1
µ(e1 | N1)
 u(e1) + V aev,π2
µ,2
(N1e1)

= µ(C | Y1)(u(C) + V aev,π2
µ,2
(Y1C)) = 1 · (−4 + 0) = −4
That is, V aev,π1
µ,2
< V aev,π2
µ,2
= V pev,π2
µ,2
< V pev,π1
µ,2
. So SPEDT but not SAEDT
prefers π1 to π2. In other words, an SAEDT agent considers himself suﬃciently
likely to have the parasite to adopt policy π2 of seeing the doctor. The SPEDT
agent relies on the fact that he would pet the cat in case he saw it, and takes
that as evidence of not being sick. Hence he will instead adopt policy π1 of not
seeing the doctor.
26

