Auton Agent Multi-Agent Syst
DOI 10.1007/s10458-010-9126-5
Using focal point learning to improve human–machine
tacit coordination
Inon Zuckerman · Sarit Kraus · Jeffrey S. Rosenschein
The Author(s) 2010
Abstract
We consider an automated agent that needs to coordinate with a human part-
ner when communication between them is not possible or is undesirable (tacit coordination
games). Specifically, we examine situations where an agent and human attempt to coordinate
their choices among several alternatives with equivalent utilities. We use machine learn-
ing algorithms to help the agent predict human choices in these tacit coordination domains.
Experimentshaveshownthathumansareoftenabletocoordinatewithoneanotherincommu-
nication-free games, by using focal points, “prominent” solutions to coordination problems.
We integrate focal point rules into the machine learning process, by transforming raw domain
data into a new hypothesis space. We present extensive empirical results from three different
tacit coordination domains. The Focal Point Learning approach results in classiﬁers with a
40–80% higher correct classiﬁcation rate, and shorter training time, than when using regular
classiﬁers, and a 35% higher correct classiﬁcation rate than classical focal point techniques
without learning. In addition, the integration of focal points into learning algorithms results
in agents that are more robust to changes in the environment. We also present several results
describing various biases that might arise in Focal Point based coordination.
Keywords
Focal points · Human–machine interaction · Cognitive model ·
Autonomous agents · Tactic coordination
A preliminary version of this article appeared in the Proceedings of the Twentieth International Joint
Conference on Artiﬁcial Intelligence (IJCAI 2007).
Inon Zuckerman—The research was done as part of the author’s PhD research in the Department of
Computer Science, Bar-Ilan University, Ramat-Gan, Israel.
I. Zuckerman (B) · S. Kraus
The Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USA
e-mail: zukermi@cs.biu.ac.il; inonzuk@hotmail.com
S. Kraus
e-mail: sarit@cs.biu.ac.il
J. S. Rosenschein
The School of Engineering and Computer Science, The Hebrew University, Jerusalem, Israel
e-mail: jeff@cs.huji.ac.il
123

Auton Agent Multi-Agent Syst
1 Introduction
One of the central problems in multi-agent systems is the problem of coordination. Agents
often differ, as do humans, in their subjective view of the world and in their goals, and need
to coordinate their actions in a coherent manner in order to attain mutual beneﬁt. Sometimes,
achieving coherent behavior is the result of explicit communication and negotiation [34,17].
However, communication is not always possible, for reasons as varied as high communi-
cation costs, the need to avoid detection, damaged communication devices, or language
incompatibility.
Several methods have been developed for achieving coordination and cooperation without
communication, for teams of automated agents in well-deﬁned tasks. The research presented
in [8] provides a solution to the ﬂocking problem, in which robots need to follow their leader.
The robots, grouped into mobile teams, move in a two-dimensional space and cannot com-
municate with one another. A comparison of experiments with and without communication
in the retrieval task, in which agents need to scout and retrieve resources, was presented
in [1]. In [28], agents used predeﬁned social laws for achieving coordination without com-
munication in the multiagent territory exploration task. All of the above research considers
speciﬁc methods that are tailored for a single, well-deﬁned task, and for pure autonomous
agent teams (i.e., humans do not take part in the interactions).
In experimental research presented by Thomas Schelling [27], it was shown that people
are often able to successfully solve coordination-without-communication scenarios (which
he named tacit coordination games) in an impressive manner, usually with higher coordina-
tion rates than that predicted by decision theoretic analysis [4]. It appears that in many of
those games there is some sort of “prominent solution” that the players manage to agree upon
without communication, and even without knowing the identity of their coordination partner.
Those “prominent solutions” were named focal points by Schelling (and are also sometimes
referred to as Schelling points).
A classic example of focal point coordination is the solution most people choose when
asked to divide $100 into two piles, of any size; they should attempt only to match the
expected choice of some other, unseen player. More than 75% of the subjects in Schelling’s
experiments created two piles of $50 each; that solution is what Schelling dubbed a focal
point. At the same time, using decision theory would result in a random selection among the
101 possible divisions, as the (straightforward) probability distribution is uniform.
Previous coordination-without-communication studies were directed at coordinating a
team of automated agents; the main motivation for our research, however, comes from the
increasing interest in task teams that contain both humans and automated agents [9]. In such
cases, augmenting an automated agent with a mechanism that imitates focal point reasoning
in humans will allow it to better coordinate with its (human) partner.
Human–agent collaboration can take the form of physical robots or of software agents
that are working on a task with human partners ([9] provides a good survey). For exam-
ple, the “DEFACTO” system [29] used artiﬁcial agents in the ﬁre-ﬁghting domain to train
incident commanders. In the area of space exploration, NASA has explored the possibilities
of having collaborative agents assisting human astronauts in various activities [20,24,30].
Another scenario is the development of user interfaces that diverge from a limited mas-
ter-slave relationship with the user, adopting a more collaborative, task-sharing approach
in which the computer explicitly considers its user’s plans and goals, and is thus able to
coordinate various tasks [10].
One important type of natural human–machine interaction is the anticipation of move-
ment, without the need for prior explicit coordination. This movement can be physical, such
123

Auton Agent Multi-Agent Syst
as the movement of a robotic arm that is assisting a human in a construction task (e.g., a
machine helping a human weld pipes [24]). As humans naturally anticipate their partners’
choices in certain situations, we would like automated agents to also act naturally in their
interactions with humans [12]. Coordinated anticipation can also take place in virtual envi-
ronments, including online games and military simulations, where humans and automated
agents (“synthetic forces” in their terminology) can inhabit shared worlds and carry out
shared activities [13].
Regardless of the speciﬁc problem at hand, there are several general constraints implicit
in the above scenarios:
• The human partner with whom our automated agent is trying to coordinate may not always
be known ahead of time, and we want coordination strategies suitable for novel partners.
• The environment itself is not fully speciﬁed ahead of time, and may be conﬁgured some-
what randomly (although the overall domain is known, i.e., the domain elements are a
given, but not their speciﬁc arrangement).
• There is no option to “hard-wire” arbitrary coordination rules into all participants, since
we are not dealing with coordination between two centrally-designed agents.
We specifically consider environments in which a human and automated agent aspire to
communication-free coordination, and the utilities associated with coordinated choices are
equal. Clearly, if utilities for various choices differed, the agent and human could employ
game theoretic forms of analysis, such as Nash equilibria selection (e.g., [11,32]) which
might specify certain strategies. However, game theory does not address the problem of
choosing among multiple choices with equivalent utility, all other aspects being equal, in a
tacit coordination game.1
In this paper, we present an approach to augmenting the focal point mechanism in human–
agent interactions through the integration of machine learning algorithms and focal point
techniques (which we call Focal Point Learning [FPL]). The integration is done via a semi-
automatic data preprocessing technique. This preprocessing transforms the raw domain data
into a new data set that creates a new hypothesis space, consisting solely of general focal point
attributes. The transformation is done according to four general focal point rules: Firstness,
Centrality, Extremeness, and Singularity, and their intuitive interpretation in the coordination
domain.
We demonstrate that using FPL results in classiﬁers (a mapping from a coordination
problem to the choice selected by an arbitrary human coordination partner) with a 40% to
80% higher correct classiﬁcation rate, and a shorter training time, than when using regu-
lar classiﬁers, and a 35% higher rate than when using only classical focal point techniques
without applying any learning algorithm. In another series of experiments, we show that
applying these techniques can also result in agents that are more robust to changes in the
environment.
We begin by providing background on focal points in Sect. 2. In Sect. 3, we describe the
novel Focal Point Learning approach. We then describe our experimental setting in Sect. 4, its
definitions, methodology, and the domains that were used in the experiments. Next, in Sect. 5,
we discuss the robustness of our agents to dynamically changing environments. Additional
experimental results and insights on the nature of focal points is discussed in Sect. 6, and we
conclude in Sect. 7.
1 Even the question of how to choose among multiple Nash equilibria is not necessarily straightforward.
123

Auton Agent Multi-Agent Syst
2 Focal points
Focal points were introduced by Schelling in [27] as a prominent subset of solutions for tacit
coordination games, which are coordination games where communication is not possible.
In such games (also known as matching games in game theory terminology) the players
only have to agree on a possible solution, regardless of the solution itself. In other words,
they receive a reward by selecting the same solution, regardless of the solution. When their
solutions differ, both players lose and do not get any reward. A solution is said to be “focal”
(also “salient”, or “prominent”) when, despite similarity among many solutions, the players
somehow converge to this solution.
2.1 Focal point examples
To better understand the notion of focal points, we will now review several coordination tasks
that were investigated by Schelling in his original presentation [27].
The classic example, presented above, is a coordination task in which two players need to
divide a pile of 100 identical objects (e.g., 100 coins) into two piles. A player’s only concern
is that his objects should be divided in the same way as the other player’s objects, regardless
of the piles’ sizes. Schelling found that players with a strong incentive for success would
divide the pile into two identical piles of 50 objects each. The player’s reasoning process
would dictate that, as at the basic level of analysis all choices are equivalent (that would be
the expected analysis when applying a straightforward decision theoretic model), the players
must apply higher-level reasoning by focusing on some property that would distinguish a
particular choice—and at the same time, rely on the other person’s doing likewise. Here, the
property that causes the 50–50 choice to be more “prominent” than others can be regarded
as a symmetric uniqueness property.
In another example, Schelling asked his subjects to coordinate by naming a positive inte-
ger. If both players select the same number, they both get a positive reward (here again,
regardless of the number itself), otherwise, they get nothing. His results show that despite
there being an inﬁnite number of positive integers, players did manage to converge to a
few potential choices, and often coordinate. The most prominent choice in this experiment
(which got 2
5 of the answers) was the number 1. This number has an obvious property that
distinguishes it from others, as it is the smallest positive integer.
At times, physical or geographical ordering of the environment can help focus choices.
For instance, the coordination task in Fig. 1 is to check one square on a 3×3 grid board; again,
the only need is to coordinate with another player regardless of the square itself. Here, most
people manage to do better than the 1
9 predicted by straightforward mathematical analysis.
Fig. 1 3 × 3 Coordination grid
123

Auton Agent Multi-Agent Syst
Fig. 2 4 × 4 Coordination grid
Most people selected the central square as it is considered prominent according to various
subjective properties related to symmetry and centrality.
However, a small change in the environment can result in a more challenging coordination
task. Looking at Fig. 2, we can notice that now there is no prominent solution according to
the symmetry and centrality properties that were found in the previous version. However,
Schelling’s experimental results in this task suggest that subjects converged to the upper-left
square as the prominent focal point, and generally speaking most selections were on the
squares residing in the upper-left to lower-right diagonal.
The focal point phenomena can be observed in various coordination domains: ﬁnding a
meeting place at an airport, where to leave a note for a spouse, voting for the same candidate
in an election. However, the underlying idea is that the players are motivated to coordinate
and that they are doing so by a kind of higher-order reasoning: reasoning about what the
other player would reason about me.
2.2 Related work
Schelling [27], after the presentation of his experimental results, claimed that when searching
for prominent solutions, there are two main components: Logic and Imagination. Logic is
some logical explanation for a choice (for example, choosing 1 when asked to pick a positive
integer, because it is the smallest positive integer). Imagination includes the unknown pre-
disposition that makes people tend to choose “Heads” over “Tails” in a simple Heads or Tails
coordination game.
2.2.1 Game theory
The problem of selecting a choice among alternatives with equal utility values is also present
in game theory. There, interactions are often represented as normal form games, in which
a matrix is used to represent the players’ strategies, and each player’s payoffs are speciﬁed
for the combined actions played inside the matrix. This type of representation allows us to
ﬁnd dominating strategies and different sorts of equilibrium points. One example is the Nash
equilibrium [23]: two strategies S1 and S2 are said to be in Nash equilibrium if, assuming
that one agent is using strategy S1, the best the other agent can do is to use S2. When the
coordination game has a single equilibrium point one might argue that it should be selected,
but there are games where there are multiple equilibria. In Table 1 there are two equilibria:
one for strategies ac, and the other for strategies bd.
Game theory provides various solutions to cases where the payoff matrix is asymmet-
ric [11,32]; other solution concepts deal with the evolution of equilibria in games played
123

Auton Agent Multi-Agent Syst
Table 1
Normal form 2 × 2
game with two equilibrium points
2 × 2 game
Player 2
Action c
Action d
Player 1
Action a
(2, 1)
(−1, −1)
Action b
(−1, −1)
(1, 2)
Table 2
n-Action, two-player
tacit coordination game
Player 2
a2
b2
. . .
n2
Player 1
a1
(1, 1)
(0, 0)
. . .
(0, 0)
b1
(0, 0)
(1, 1)
. . .
(0, 0)
...
. . .
. . .
. . .
. . .
n1
(0, 0)
(0, 0)
(0, 0)
(1, 1)
repeatedly within a speciﬁc population [16,35,36], or which iteratively converge on a pattern
of coordinated play [5]. However, none of the above solutions from game theory addresses the
problem of solving non-repeated, multiple equilibria, symmetric coordination games without
communication, such as tacit coordination games.
A tacit coordination game for two players can be presented as the following normal form
matrix (see Table 2). In this example, each player has a set of n possible actions, labeled from
a to n (with the player number as a subscript). In this game, the players get a payoff only if
they are able to agree on an action, regardless of the action itself. The divide-100-objects focal
point example (Sect. 2.1) can be seen as a similar matrix where n = 101, for the number of
possible strategies. Yet again, while game theory does not provide a solution for such cases,
human beings are often able to do better than might be predicted by decision theory.
2.2.2 Labeling theories
Game theory lacked a formal model that would explain the ﬁndings in Schelling’s exper-
iments. Gauthier [7] was the ﬁrst who addressed this topic. He introduced the notion of
Salience when rational players are engaged in coordinated interaction. Players seek salience
in order to increase the equilibrium utility value (i.e., distinguishing one of the choices) using
additional knowledge about the situation by forming expectations about what I expect you
to expect from me. This additional knowledge is the player’s own description of the world
(which is not handled in classical game theory’s analysis), according to the way it is con-
ceived by him. The player, when making a choice, follows a principle of coordination, which
ensures that the most distinguished equilibria are selected.
Following Gauthier, Bacharach [2] introduced the notion of availability, which is the
probability that the player will conceive of certain aspects of the coordination problem. For
example, given a coordination problem where one has to select one brick from a set of eight
bricks, it is easy for the player to notice the color of the bricks (thus, this dimension will
have high availability), but it might be hard to notice that a single brick is made of a different
material (this dimension will have lower availability). Bacharach argued that the number of
123

Auton Agent Multi-Agent Syst
possible choices (or equilibria) is given according to the number of aspects the player grasps
in the coordination problem. Bacharach and Bernasconi [3] presented a variable frame theory
(VFT), where they refer to features as frames which subjectively describe the game to each
player. Frames are sets of variables that are used to conceptualize the game.
Janssen [15] continued building on Bacharach’s model, generalized it for general clas-
ses of dimensions, and showed that players in all cases would receive a higher pay-off by
following his selection principle rather than neglecting the label information.
Sugden [31] presented a different theory by showing how labels can inﬂuence decisions
in pure coordination games. He argued that the labeling of choices is beyond the conscious
control of the player, and is inﬂuenced by psychological and cultural factors. He showed that
his collective rationality principle may or may not create coordination, depending on the
labeling procedure correlation.
The economic theories presented above all give appropriate retroactive justiﬁcation to
answers for coordination games, but are not highly descriptive of human behavior, nor are
they applicable to our mixed human–agent scenario, for several reasons. First, they do not
give any consideration to social conventions and cultural factors [19,35,36].2 Second, their
analysis does not quantify the notion of availability, nor give the labeling conceived of by
players. Moreover, these theories have no explanatory power; for example, in the Heads/Tails
question, most people choose Heads. This could be explained by saying that Heads has a con-
ventional priority over Tails, but the same explanation could have been used if the majority
had picked Tails.
2.2.3 Experimental work
There has been some experimental research that has tried to establish the existence of focal
points in coordination games.
Mehtaetal.[22],inaseriesofcontrolledexperiments,managedtoverifypeople’sabilityto
coordinate their answers. In one experiment, subjects were divided into two groups. Group A
was instructed to try and coordinate their answers; Group B was instructed to give a response
without any incentive to match their partner’s choice. The questions were generally “open”
coordination questions, without a well-deﬁned set of possible answers, for example, “Choose
a year”, or “Choose a mountain”. Results showed that Group A had a significantly higher
success rate in coordinated answers than Group B, which had no incentive for coordination.
In the second experiment, the focus was on assignment games where the purpose was to
isolate some focal point selection rules that the authors hypothesized would be used in the
coordination problem: (1) closeness, (2) accession, or (3) equality. The game was as follows:
there was a board containing two squares and an arbitrary number of circles. The squares
were always located at the same position, and the circles were positioned at different places
on the board. The task was to assign each circle to a square (by painting the circles red or
blue), and to be able to coordinate your assignment with another player. The results showed
support for the hypothesis that subjects used the rules mentioned above. The authors claimed,
“It seems clear that subjects are drawing on each of the three rules to identify focal points”.
However, in an analysis that we did of each of the rules, it appears that the accession rules
had a very limited impact on the overall results, in comparison to the other rules.
Mehta published another experiment [21], in which she interviewed players, after complet-
ing various coordination games, about their behavior in the games. There were some insights
that were common to the majority of the interviewed subjects: (a) subjects tried “going into
2 We will see later that these are very important in the process of focal point discovery.
123

Auton Agent Multi-Agent Syst
the heads” of their partner, to ﬁgure out what he would do; (b) subjects tried using cultural
information shared with their partner; (c) the use of rules—most subjects followed some
rules in choosing their answer; (d) the use of “fairness” played a significant role.
Another set of experiments, which strengthens the notion of focal point usage, was done
by Van Huyck et al. [32], studying games with multiple equilibria; they checked how human
subjects make decisions under conditions of strategic uncertainty. Cooper et al. [26] experi-
mented to discover what happens when players play a non-cooperative game with multiple
Nash equilibria. His results showed that the outcome would come from the set of Nash
equilibria.
In the artiﬁcial intelligence literature, Kraus et al. [6,18] used Focal Point techniques to
coordinate between agents in communication-impoverished situations. In [18] they modeled
the process of ﬁnding focal points from domain-independent criteria using two approaches:
decision theory, and step-logic. They devised a focal point algorithm tailored to a robot ren-
dezvous coordination game (where two robots have to agree on a single object from a small
set of objects with various properties), and showed that their algorithm managed to converge
to focal points in a very high percentage of cases. However, though their approach initiated
the use of focal points in the agent-agent coordination problem, their results are not very
surprising: two agents running the same algorithm would necessarily converge to the same
solution (though the authors’ approach was not hard-wired). We consider that a major advan-
tage that automated agents could gain from using focal points is specifically when working
with human beings, who inherently seem to exhibit such reasoning capabilities.
3 Focal point learning
To enhance human–agent coordination, we would like the automated agent to have a cogni-
tive mechanism which is similar to the one that exists in human beings. Such a mechanism
would allow agents to reason and search for focal points when communication-impoverished
situations occur.
Coordination in human–agent teams can be strengthened by having agents learn how a
general human partner will make choices in a given domain. It is possible to use various
machine learning algorithms to explicitly learn the focal choices in a given game. However,
learning to classify the choices of a general human partner in tacit coordination games is
difﬁcult for the following reasons:
1. No speciﬁc function to generalize—there is no mathematical function nor behavioral
theory that predicts human choices in these games. Specifically, no function can capture
the notion that for some tacit coordination games, different human players can select
different choices.
2. Noisy data—data collected from humans in tacit coordination games tends to be very
noisy due to various social, cultural, and psychological factors that bias their answers.
When collecting data, any experience before or during the game can impact the focal
points (we will see examples of that phenomenon below).
3. Domain complexity—in complex domains, training a classiﬁer not only requires a large
set of examples, but in order to generalize an arbitrary human partner, those examples
should be taken from different sources in order to remove cultural and psychological
biases. This results in a very difﬁcult data collection task.
These difﬁculties suggest that using classical machine learning methods to build a focal
point reasoner, which works similarly to that exhibited by humans, is not an easy task. As
123

Auton Agent Multi-Agent Syst
we will see in the experimental section below, the main problem is that we want to clas-
sify a “general” human coordination partner and not a speciﬁc partner (which would be a
considerably easier task for classical machine learning algorithms).
As mentioned above, several attempts have been made to formalize focal points from a
game theoretic, human interaction point of view ([14] provides a good overview). However,
as we said, that research does not provide the practical tools necessary for use in automated
agents. In [18], Kraus et al. identiﬁed some domain-independent rules that could be used by
automated agents to identify focal points. The following rules are derived from that work,
but are adjusted and reﬁned in our presentation.3
• Centrality—this rule gives prominence to choices directly in the center of the set of
choices, either in the physical environment, or in the values of the choices. For example,
in the 3×3 grid coordination scenario (Fig. 1, above), the center square had the centrality
property as it resides directly in the center of the physical environment (both horizontally
and vertically).
• Extremeness—this rule gives prominence to choices that are extreme relative to other
choices, either in the physical environment, or in the values of the choices. For example,
when asked to select one number out of a set, the highest or smallest numbers will have
the extreme property. In a physical environment, the tallest, smallest, longest, etc., can
be named as the extreme choices.
• Firstness—this rule gives prominence to choices that physically appear ﬁrst in the set of
choices. It can be either the option closest to the agent, or the ﬁrst option in a list. For
example, when asked to select one number out of a set, the number that appears ﬁrst has
the ﬁrstness property.
• Singularity—this rule gives prominence to choices that are unique or distinguishable rel-
ative to other choices in the same set. This uniqueness can be, for example, with respect to
some physical characteristics of the options, a special arrangement, or a cultural conven-
tion. There are many examples of this rule, from a physical property such as the object’s
color or size, to some social norm which singles out one of the options.
We employ learning algorithms to help our agent discover coordination strategies. Train-
ing samples, gathered from humans playing a tacit coordination game, are used to create an
automated agent that performs well when faced with a new human partner in a newly gen-
erated environment. However, because of the aforementioned problems, applying machine
learning on raw domain data results in classiﬁers having poor performance. Instead, we use
a Focal Point Learning approach: we preprocess raw domain data, and place it into a new
representation space, based on focal point properties. Given our domain’s raw data Oi, we
apply a transformation T , such that N j = T (Oi), where i, j are the number of attributes
before and after the transformation, respectively.
The new feature space N j is created as follows: each v ∈Oi is a vector of size i rep-
resenting a game instance in the domain (world description alongside its possible choices).
The transformation T takes each vector v and creates a new vector u ∈N j, such that
j = 4×[number of choices].4 T iterates over the possible choices encoded in v, and for
each such choice computes four numerical values signifying the four focal point proper-
ties presented above. For example, given a coordination game encoded as a vector v of
size 25 that contains three choices (c1, c2, c3), the transformation T creates a new vector
u = (cc
1, ce
1, c f
1 , cs
1, cc
2, ce
2, c f
2 , cs
2, cc
3, ce
3, c f
3 , cs
3) of size 12 (3 possible choices × 4 focal
3 Kraus et al. used the following intuitive properties: Uniqueness, Uniqueness complement, centrality, and
extremeness.
4 This can be generalized to a different number of rules by taking j =[number of rules]×[number of choices].
123

Auton Agent Multi-Agent Syst
point rules), where cc/e/f/s
l
denotes the centrality/extremeness/ﬁrstness/singularity values
for choice l. Note that j might be smaller than, equal to, or greater than i, depending on the
domain and the number of rules used.
Algorithm 1: The general transformation algorithm
Input: Original coordination task Encoding
Output: Focal Point based coordination task Encoding
V [num′Choices × num′Rules];
foreach c ∈choices do
foreach r ∈Rules do
V [c × r] = ComputeRule(r,c);
end
end
return V ;
The transformation from raw domain data to the new representation in focal point space
is done semi-automatically using Algorithm 1. This linear-time algorithm is a general trans-
formation algorithm to any number of focal point rules. In order to transform raw data from
some new domain, one needs to provide a domain-speciﬁc implementation of the four gen-
eral focal point rules. There is currently no automated way to suggest the optimal set of
rules for a given problem domain, and we will not claim below that our choices are optimal.
However, due to the generic nature of the rules, this task is relatively simple, intuitive, and
suggested by the domain itself (we will see such rules in Sect. 4.3). When those rules are
implemented, the agent can itself easily carry out the transformation on all instances in the
data set.
4 The experimental setting
We designed three domains for experiments in tacit coordination. For each domain, a large
set of coordination problems was randomly generated, and the solutions to those problems
were collected from human subjects.
We used the resulting data set to train three types of agents, and compared their coordi-
nation performance (versus unknown human partners). The agent types are as follows:
1. Domain Data agent—an agent trained on the original domain data set.
2. Focal Point agent (FP agent)—an agent using focal point rules without any learning
procedure.
3. Focal Point Learning agent (FPL agent)—an agent using the Focal Point Learning
method.
In the second phase of our experiments we tested robustness to environmental changes
(Sect. 5). We took the ﬁrst domain described in Sect. 4.3, and designed two variations of
it; one variant (VSD, a Very Similar Domain) had greater similarity to the original envi-
ronment than the other variant (SD, a Similar Domain) had. Data from human subjects
operating in the two variant settings were collected. We then carried out an analysis of
automated coordination performance in the new settings, using the agents that had been
trained in the original domain. In addition to the main results, we will discuss below sev-
eral insights into the nature of focal points that came from different stages of the experi-
123

Auton Agent Multi-Agent Syst
ments, pre-experiments, and while interviewing the subjects after they participated in the
experiments.
4.1 Definitions
Deﬁnition 1 (Pure Tacit Coordination Games) Pure Tacit Coordination Games (also called
matching games) are games in which two non-communicating players get a positive payoff
only if both choose the same option. Both players have an identical set of options and the
identical incentive to succeed at coordination.
Our experiments involved pure tacit coordination games. We demonstrate the definitions
below with the following example of such a game. Two non-communicating players are faced
with the following set of numbers: {15, 18, 100, 8, 13}. Their instructions are to select a
single number of that set, where successful coordination is rewarded with $50 for each player
(regardless of the coordination choice); in the case of unsuccessful coordination, the players
receive nothing.
Obviously, in the above example, straightforward decision theory would suggest that all
ﬁve options have similar probability (p(c) = 0.2, where c is a choice), for successful coordi-
nation; from a game theoretic point of view, all ﬁve choices/actions have arbitrary labels and
an arbitrary ordering. However, we have reason to believe that using focal point reasoning, we
will be able to coordinate with a human partner with higher probability than the expected 0.2.
Deﬁnition 2 (Focality Value) Let R be the set of selection rules used in the coordination
domain, c ∈C be a possible choice in the domain, r ∈R be a speciﬁc selection rule, and
v(r, c) be its value. Then the focality value is deﬁned as:
FV (c) =

r∈R v(r, c)
|R|
.
A focality value is a quantity calculated for each possible choice in a given game, and
signiﬁes the level of prominence of that choice relative to the domain. The focality value
takes into account all of the focal point selection rules used in the coordination domain; their
speciﬁc implementation is domain dependent (e.g., what constitutes Centrality in a given
domain). Since the exact set of selection rules used by human players is unknown, this value
represents an approximation based on our characterization of the focal point rule set. In the
experiments, our FP agent will use this value to determine its classiﬁcation answer to a given
game.
Going back to our running example, we can now calculate the focality value for each of
the possible choices of the coordination problem (ﬁve choices). The ﬁrst step is to provide
an intuitive domain-dependent implementation to the suggested focal point rules:
• Centrality—will increase prominence of the central choice of that set (the number 100);
• Extremeness—will be deﬁned to increase prominence of the two extreme choices: the
smallest and largest numbers (numbers 100 and 8);
• Firstness—the intuitive implementation would be to increase the prominence of the ﬁrst
number in the list (number 15);
• Singularity—in this setting can be deﬁned using odd/even division (15 and 13), prime
numbers (only 13 is prime), or simply according to the number of digits (according to
this property, 8 and 100 are the distinguished choices).
We should choose one or more singular properties which are believed to be intuitive inter-
pretations that will be used by most humans.
123

Auton Agent Multi-Agent Syst
We can give different weight to different rules, but here we continue the example using
uniform weight for the rules. The following list will specify the FP values according to the
above rules of all choices:
FP(15) = 2—using the ﬁrstness and singularity property with the parity division.
FP(18) = 0—none of the above rules are prominent for this choice.
(100) = 3—using the centrality, extremeness and singularity rules according to the number
of digits.
FP(8) = 2—using the extremeness and singularity rules according to the number of digits.
FP(13) = 2—using singularity according to parity and prime numbers.
According to our speciﬁc implementation, a human player can have the following observa-
tions: choosing 18 is the least recommended choice as its focality value is the lowest. The
most prominent choice would be the number 100, with the highest focal value, 3. Naturally,
different interpretations as to the rules, and different weights, would result in different focal
values. However, as we see in our experimental section, the most intuitive descriptions using
those rules will assist us in focusing the answers, or will at least help us in eliminating
some of the options (e.g., in our example 18 can be easily pruned, leaving us with an easier
coordination task).
Deﬁnition 3 (Focality Difference) Let C be the set of all possible choices in the coordina-
tion domain and FV (c) be the focality value of c ∈C, max be the maximum function and
2nd_max be the second maximum function. Then the focality difference is deﬁned as:
F_Di f f (C) = max
c∈C (FV (c)) −2nd_ max
c∈C (FV (c)).
A focality difference is a function that gets a set of possible choices and determines the
difﬁculty level of the tacit coordination game. Naturally, a game with few choices that have
similar focality values is harder to solve than a game that might have more choices, but with
one of the choices much more prominent than the others.
In our example, the focality difference is F_Di f f (example) = 3 −2 = 1. This differ-
ence allows us to compare different tacit coordination games and understand which game is
easier to solve using a focal-point-based answer. The higher the focality difference is, the
easier it is to coordinate on a focal answer. Moreover, we can see that the focality difference
is not a function of the cardinality of the set of possible choices.
4.2 Methodology
For each of the experimental domains presented below, we used the same methodology. First,
we collected a large set of samples from different human players. Each such sample was a
randomly generated instance of the coordination domain; thus, there were instances that were
generated more than once and were “played” by different players.
The next step was to build machine learning classiﬁers that predict the choice selected by
most human partners. We worked with two widely used machine learning algorithms: a C4.5
decision learning tree [25], and a feed-forward back-propagation (FFBP) neural network [33].
Obviously, the different domains have different numbers of input and output neurons, thus
they require using different network architectures.
Each of these was ﬁrst trained on the raw domain data set, and then on the new prepro-
cessed data based on the focal point rules. Figure 3 describes the training stage that was done
for each of the experimental domains. As can be seen, the domain data agent is trained on
123

Auton Agent Multi-Agent Syst
Fig. 3 The training stage
Fig. 4 The testing stage
the raw experimental data, the FP agent does not undergo any training, and the FPL agent
is trained on the preprocessed data. The raw data was represented as a multi-valued feature
bit vector. Each domain feature was represented by the minimal number of bits needed to
represent all of its possible values. This simple, low-level representation helped standardize
the experimental setup with both types of classiﬁers using exactly the same domain encoding.
The transformation to focal point encoding provides focality values in terms of our low-
level focal point rules (Firstness, Singularity, Extremeness, and Centrality) for each of the
possible choices. Their values were calculated in a preprocessing stage, prior to the training
stage (and by an agent when it needs to output a prediction). In the training session, the
algorithms learn the best values and weight for each rule, as the individual impact of each
individual rule may vary across domains. It is important to note that following the transforma-
tion to the focal point encoding, we deprive the classiﬁer of any explicit domain information
during training; it trains only on the focal point information.
Finally, we compared the performance of our three agents in each domain according to
their correct classiﬁcation of the test samples. This process is described in Fig. 4, where we
can see that the test examples are fed in their original encoding to the domain data agent,
while the FP and FPL agents classify the example after it has gone through the preprocessing
123

Auton Agent Multi-Agent Syst
Fig. 5 Pick the pile game board sample
Fig. 6 Screenshot from game website
phase. For a given game instance, each of the agents outputs one of the possible choices, the
one that it predicts most people will select.
4.3 The experimental domains
We now present three experimental domains that were designed to check FPL’s performance.
We designed the coordination games with the following design principles in mind:
1. Make the domains tacit coordination games (equal utility values for all possible choices).
2. Avoid implicit biases that might occur due to psychological, cultural, and social factors
(i.e., remove possible biases).
3. Use a variety of tacit coordination problems to check the performance of focal point
learning in different domains.
4.3.1 Pick the pile game
We designed a simple and intuitive tacit coordination game that represents a simpliﬁed ver-
sion of a domain where an agent and a human partner need to agree on a possible meeting
place. The game is played on a 5-by-5 square grid. Each square of the grid can be empty,
123

Auton Agent Multi-Agent Syst
or can contain either a pile of money or the game agents (all agents are situated in the same
starting grid; see Fig. 5). Each square in the game board is colored white, yellow, or red.
The players were instructed to pick the one pile of money from the three identical piles, that
most other players, playing exactly the same game, would pick. The players were told that
the agents can make horizontal and vertical moves.
Data was collected using an Internet website (Fig. 6) which allowed players from all over
the world to participate in the game, and their answers were recorded. Upon entering the
website, each player was requested to read the instructions and was asked to play the game
only one time. The instructions speciﬁed that each player is paired with an unknown partner
and that their score would be given at the end. Each game session was constructed of 10
randomly generated instances of the domain. We enforced the one-game-per-person rule by
explicitly requesting the players to play only once, and by recording the player’s IP and
removing multiple instances of the same IP from our database.5 The call for players was
published in various AI related forums and mailing lists all over the world, and eventually
we gathered approximately 3000 game instances from over 275 different users from around
the world.
The ﬁrst step was to build a feed-forward back-propagation network on a simple encoding
of the domain. In a simple binary encoding of this domain, for encoding 25 squares with 9
possible values (4 bits) per square, we used 100 neurons for the input layer. The output layer
consisted of 3 neurons (as there are 3 piles from which to choose), where the ﬁrst neuron
represented the ﬁrst pile in the game, that are ordered horizontally from the top-left corner to
the bottom-right corner. The other network parameters, such as number of hidden neurons,
learning rate (η), and momentum constant, were set by way of trial and error to achieve the
best performance.
The transformation to the focal point space was done in the following way:
1. Centrality—Centrality was calculated as an exact bisection symmetry, thus giving a
positive value to a pile that lies directly between two other piles either horizontally,
vertically, or diagonally.
2. Singularity—the only distinguishable choice attribute is the color, thus the Singularity
of each pile was calculated according to the number of squares having the same color.
Naturally, a pile of money sitting on a red square in a board having only 4 red squares,
would have a higher degree of singularity than a pile of money sitting on a white square,
if there were 17 white squares on that board.
3. Firstness—The Firstness property was calculated as the Manhattan distance between the
agent’s square and each pile of money.
4. Extremeness—The Extremeness property was intuitively irrelevant in this domain, so
we gave it a uniform constant value.
After preprocessing the data according to the above interpretation of the focal point rules,
we built two additional agents: the focal point agent (FP) is one that selects the pile with the
highest focality value (without employing any learning procedure). The focal point learning
agent (FPL) built a new neural network after discretizing the focal point rules to a set of eight
possible values {0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1}. Now, the number of input
5 Our method was not fully secure, and could be manipulated by requesting a new IP address, or by using
other computers. However, due to the nature of the game it is reasonable to assume that most players did not
do that.
123

Auton Agent Multi-Agent Syst
neurons is 36 (4 rules × 8 discrete values (3 bits) × 3 possible choices), there are 3 output
neurons, and we will train the network on the newly transformed data.6
Example: Looking at Fig. 5, we can compute the focality value according to the above
rules. Before doing so, let us enumerate the piles as follows: pile1 is the upper right pile (on
the yellow-colored grid), pile2 is the pile on the 2nd row (counting down from above), and
pile3 is the pile on the bottom row. In addition, note that the ComputeRule function will
be denoted as v for ease of presentation. Now, neither of the piles in that example have any
Centrality property (as they are not residing on the same row, column, or diagonal); formally,
v(centrality, pile1) = v(centrality, pile2) = v(centrality, pile3) = 0. In terms of
Firstness, pile1 is 3 grid squares away from the agents, thus v( f irstness, pile1) = 0.75.
Pile2 is two grid squares away, thus v( f irstness, pile2) = 0.875, and pile3 is three grid
squares away, v( f irstness, pile3) = 0.75. The Singularity property ﬁnds piles 2 and 3 on a
white grid square; thus, as the board contains 12 white grid squares out of 25 squares overall,
their values will be v(singularity, pile2) = v(singularity, pile3) = 1 −0.48 ≈0.5.
Pile1, residing on a yellow grid square would be valued as v(singularity, pile1) = 1 −
0.28 ≈0.75.
Summing their focality values, we have FV (pile1) = 0.75, FV (pile2) = 0.6875,
FV (pile3) = 0.625. Here, the agent who is only using a focal point calculation will select
pile1, as it has the highest focality value in this game. Moreover, we can now compute the
focality difference of the game as 0.75−0.6875 = 0.0625. This quite low focality difference
suggests that this speciﬁc instance of the domain is difﬁcult to solve.
4.3.2 Candidate selection game
Players were given a list of ﬁve candidates in an election for some unknown position. The
candidates were described using the following properties and their possible values:
1. sex∈{Male, Female}
2. age∈{25, 31, 35, 42, 45}
3. height (in meters) ∈{1.71, 1.75, 1.78, 1.81, 1.85}
4. profession ∈{Doctor, Lawyer, Businessman, Engineer, Professor}
Each list was composed of ﬁve randomly generated candidates. The (pen and paper)
experiments were carried out when subjects (a total of 82 ﬁrst-year university students) were
seated in a classroom, and were told that their coordination partners were randomly selected
from experiments that took place in other classes, i.e., their partner’s identity is completely
unknown. For a candidate to be elected, it needs to get these two votes (the player’s and its
partner’s); thus, both sides need to choose the same candidate. To create the necessary moti-
vation for successful coordination, we announced a monetary reward for success.7 Figure 7
shows a sample question in the domain.
The binary encoding for building the neural network in this domain was a set of 50 input
neurons in the input layer that encoded 5 candidates, each encoded with 10 bits (1 bit for
gender, and 3 bits for each of the others). The output layer was composed of 5 output neurons,
one for each possible candidate.
The focal point transformation had the following intuitive implementation:
1. Centrality—gave a positive constant value to the third candidate in the list (which is
located in the center of the selection list).
6 The number of input neurons can be reduced, as the Extremeness property was not used in this interpretation
of the rules.
7 It is a well-known phenomenon that coordination in these games deteriorates without sufﬁcient motivation.
123

Auton Agent Multi-Agent Syst
Fig. 7 Candidate selection game sample
2. Singularity—the Singularity of a candidate was calculated according to the relative
uniqueness of each of its values (i.e., a sole female candidate in a set of males will
increase the singularity value by 1 −0.2 = 0.8).
3. Firstness—The Firstness property gave a positive constant value to the ﬁrst candidate on
the list.
4. Extremeness—The Extremeness property gave high values to properties that exhibited
extreme values in some characteristics of the candidate (for example, a candidate who
is the oldest or youngest among the set of candidates would get a higher Extremeness
value than a candidate who is not).
Example: let us now compute the rules according to the game instance presented
in Fig. 7. We will enumerate the candidates according to their order of appearance
from top to bottom. The centrality and ﬁrstness properties are intuitive, and result in
v(centrality, candidate3) = c1 and v( f irstness, candidate1) = c2, while c1, c2 > 0.
The extremeness property gave prominence to candidate2 as she is the sole oldest, and the
tallest, candidate; thus v(extremeness, candidate2) > 0. The same goes for the singularity
property,inwhichwecomputethesingularityofthevaluesexhibitedineachofthecandidate’s
properties. For instance, candidate1 is one of the three Males (hence, 0.4), one of the three
25-year-old candidates (again, 0.4), one of the two candidates of 1.75m height (0.6) and one
of the three lawyers (0.4). Taking the average, we have v(singularity, candidate1) = 0.45.
4.3.3 Shape matching game
Players were given a random set of geometric shapes, and had to mark their selected shape in
order to achieve successful coordination with an unknown partner (presented with the same
set). The seven shapes were presented in a single row and were randomized from the set of
circle, rectangle, or triangle. Questionnaires containing ten game instances were distributed
to students (78 students overall). As before, monetary prizes were guaranteed to students
with the highest coordination scores. Figure 8 shows a sample question in the domain.
This domain is the easiest among our games to represent as a simple binary encoding,
because each goal has only a single property, its type. In any game instance, each shape can
be a circle, rectangle, or triangle. Thus, the question was easier in terms of its simple binary
representation, as we needed 14 input neurons and 7 output neurons.
The focal point transformation was implemented as follows:
1. Centrality—Centrality gave additional focality value to the middle choice, and increased
the focality value of a shape which had the same shape series on both of its sides. A
longer series yielded a higher value.
2. Singularity—the Singularity of a choice was determined by the number of choices with
the same shape (for example, in a game where all shapes are circles and only a single
shape is a triangle, the triangular shape will have a high singularity value).
3. Firstness—Firstness gave a small bias to the ﬁrst shape on the left-hand side of the list.
123

Auton Agent Multi-Agent Syst
Fig. 8 Shape matching game sample
4. Extremeness—The Extremeness property gave higher focality values to the ﬁrst and last
choices in the set.
In this domain we have an example of a domain transformation in which j > i, which
means that the transformation actually increases the search space. From 14 input neurons in
the simple binary representation, we now move to a space of 42 input neurons (6 bits × 7
choices).
4.4 Results
For each of the above domains, we compared the correct classiﬁcation performance of both
C4.5 learning trees and FFBP neural network classiﬁers. As stated above, the comparison
was between a domain data agent (trained on the raw domain encoding), a focal point (FP)
agent (an untrained agent that used only focal point rules for prediction), and a focal point
learning (FPL) agent. “Correct classiﬁcation” means that the agent made the same choice as
that of the particular human player who played the same game.8
We optimized our classiﬁers’ performance by varying the network architecture and learn-
ing parameters, until attaining best results. We used a learning rate of 0.3, momentum rate
of 0.2, 1 hidden layer, random initial weights, and no biases of any sort. Before each train-
ing procedure, the data set was randomly divided into a test and a training set (a standard
33.3–66.6% division). Each instance of those sets contained the game description (either
the binary or focal point encoding) and the human answer to it. All algorithms were run
in the WEKA9 data mining software, which is a collection of machine learning algorithms
for data mining tasks. The classiﬁcation results using the neural network and the decision
tree algorithms were very close (maximum difference of 3%). Figure 9 compares the correct
classiﬁcation percentage for the agents’ classiﬁcation techniques, in each of the three exper-
imental domains. Each entry in the graph is a result averaged over ﬁve runs of each learning
algorithm (neural network and C4.5 tree), and the average of those two algorithms.
Examining the results, we see a significant improvement when using the focal point
learning approach to train classiﬁers, rather than the domain data agent (p < 0.01 in two-
proportion z-tests in all domains). In all three domains, the domain data agent is not able to
generalize sufﬁciently, thus achieving classiﬁcation rates that are only about 5–10% higher
than a random guess. Using FPL, the classiﬁcation rate improved by 40–80% above the
classiﬁcation performance of the domain data agent.10 The results also show that even the
classical FP agent, which does not employ any learning algorithm, performs better than the
domain data agent. In an additional analysis that was done on the FP agent, we saw a tendency
8 If there were multiple occurrences of a speciﬁc game instance, the choice of the majority of humans was
considered the solution.
9 http://www.cs.waikato.ac.nz/ml/weka/
10 Since even humans do not have 100% success with one another in these games, FPL is correspondingly
the more impressive.
123

Auton Agent Multi-Agent Syst
Fig. 9 Average correct classiﬁcation percentage
Fig. 10 “Pick The Pile” focality difference impact
in which the FP agent, when facing coordination problems with low focality difference, has
its performance deteriorate to that of random guesses.
Note also that in the ﬁrst domain, when using FPL instead of regular raw data learning,
the marginal increase in performance is higher than the improvement that was achieved in
the second domain (an increase of 28% vs. 22%), which is in turn higher than the marginal
increase in performance of the third domain (an increase of 22% vs. 18%). From those results,
we hypothesize that the difference in the marginal performance increase is because the ﬁrst
domain was the most complex in terms of the number of objects and their properties. As the
domain becomes more complex, there are more possibilities for human subjects to use their
own subjective rules (for example, in the Pick the Pile domain, we noticed that few people
looked at the different color patterns that were randomly created, as a decision rule for their
selected goal). As more rules are used, the data becomes harder to generalize. When an agent
is situated in a real, highly complex environment, we can expect that the marginal increase
in performance, when using FPL, will be correspondingly large.
An additional advantage of using FPL is the reduction in training time (e.g., in the Pick the
Pile domain we saw a reduction from 4h on the original data to 3min), due to the reduction
of input size. Moreover, the learning tree that was created using FPL was smaller, and can
be easily converted to a rule-based system as part of the agent’s design.
In Fig. 10 we checked the impact of the focality difference on the success rate in the “Pick
the Pile” domain. We computed the focality difference of each of the randomized instances
123

Auton Agent Multi-Agent Syst
that was played, and divided them into six groups according to their values. We then isolated
each group and checked the successful coordination rate on each of the groups. It turns out
that in the highest focality instances (>0.35) the FPL agent managed to achieve a 78.3%
successful coordination rate, and in the next group (0.3–0.35) it managed to achieve a 75%
successful coordination rate. One can also see that the successful coordination rate increased
with the focality difference.
5 Robustness to environmental changes
As seen in Sect. 4, the focal point rules proved to be helpful in handling various tacit coordi-
nation domains against an arbitrary human partner. In this section, we examine the robustness
of human–agent tacit interaction in changing environments, and analyze the robustness of
our agents to changes in the domain environment. Dynamic changes in the environment can
occur for many reasons, and having a more robust agent can become a crucial ingredient for
succeeding on various missions.
Deﬁnition 4 (Environment Similarity) Similarity between environments is calculated as the
Euclidean distance:
di j =




n

k=1
(xik −x jk)2,
where the environment vector x is constructed from the number of goals, number of attributes
per goal, number of values per attribute, and the attribute values themselves.
To check agent robustness in the face of environment changes, we took the “Pick the
Pile” domain (described in Sect. 4.3.1), and designed variants; we denote them as the simi-
lar domain, and the very similar domain. To check agent performance, we put the original
agents (i.e., the domain data and focal point learning agents that had been trained on the
original Pick the Pile version, and the regular focal point agent) in the new environments,
and compared their classiﬁcation performance.
We created two different versions of the Pick the Pile game, which had different similarity
values relative to the original version. In the ﬁrst variant (which we will denote VSD for Very
Similar Domain), we added a fourth possible value to the set of values of the color attribute
(four colors instead of three). In the second variant (which we will denote SD for Similar
Domain), in addition to the ﬁrst change, we also changed the grid structure to a 6 by 4 grid
Fig. 11 “Pick The Pile”—VSD example
123

Auton Agent Multi-Agent Syst
Fig. 12 Classiﬁcation percentage in similar environments
(instead of the original 5 by 5). Moreover, in both variants, we changed all four color values
from actual colors to various black and white texture mappings (see Fig. 11 for an example).
Additional experiments were conducted in order to collect human answers to the two
new variants of the game (85 ﬁrst-year computer science and applied mathematics students
took part). The agents that had been trained on the original environment (using the neural
network algorithm), were now asked to coordinate with an arbitrary human partner in the
new environments. Figure 12 summarizes performance comparison of the agents in each of
the new environment variants.
The prediction results on the ﬁrst variant (VSD) show that all three agents managed to
somehow cope with the new, very similar domain, and suffered only a small decrease in
performance. However, when looking at the results of the similar domain (SD), we see that
the domain data agent’s performance decreased all the way to its classiﬁcation performance’s
lower bound, that of random guessing. At the same time, our FPL agent did suffer a mild
decrease in performance (around 5%), but still managed to keep a reasonably high perfor-
mance level of around 62% (significantly better than the domain data agent, with p < 0.01
in a two-proportion z-test). We can also notice that the classical FP agent copes with the envi-
ronmental changes better than the domain data agent, with a performance level of around
45%; however, it is still low when compared to the FPL agent’s performance level.
6 Additional results and insights
6.1 Generalized pick the pile domain
We conducted an additional set of experiments in order to evaluate the algorithm’s perfor-
mance on a variation of the Pick the Pile domain that is more generalized and challenging.
In this variation, in contrast to the original Pick the Pile domain, the coordination task is not
limited to agreeing on one of the three possible piles (i.e., 33% is the expected successful
coordination rate according to decision theory), but to agree on any possible grid position.
The board was reduced to a 3 by 3 grid, and after removing the pile icons, we were left
123

Auton Agent Multi-Agent Syst
Fig. 13 Generalized pick the pile example
Fig. 14 Generalized shape matching example
with 8 possible meeting positions (the grid where the agents are located was not consid-
ered a valid meeting place), which resulted in a 12.5% expected successful coordination rate
according to standard decision theory. In addition, instead of the original color attribute, we
presented 4 new possible values on each grid square: a picture of a tree, grass, stones, or a
lake. This would implicitly cause the players to use more subjective contextual information
when needed. Figure 13 presents an instance of the new domain.
This experiment was a pen-and-paper experiment with 200 ﬁrst year students studying
computer science, applied mathematics, and engineering. Each subject received a question-
naire with 6 randomly-generated instances. As before, we compared the performance of an
agent that was trained on the original domain encoding, and a focal point learning agent that
was trained using the same rule implementations as described above for the original domain.
Our results show that this variation is very challenging to both classiﬁers, as the domain
dataagent(trainedwiththeoriginalencoding)achievedapproximately16%correctclassiﬁca-
tion, while the focal point learning agent achieved approximately 34% correct classiﬁcation.
While these rates are not as impressive as our results in the original domain, they still pro-
vide a significant improvement on the results that the original classiﬁer was able to achieve
without using the focal point learning scheme.
6.2 Generalized shape matching domain
In an additional experiment that we conducted on the Shape Matching domain, we generalized
the game as follows: instead of having a ﬁxed set of 7 shapes in each instantiation of the
domain, we randomized the number of shapes in each row in addition to their type. In the
generalized domain, each game instance can include from 3 to 7 shapes (and not always 7
shapes, as in the original version), and each such shape itself was randomized from the set of
possible shapes. Figure 14 presents an example of two game instances from a questionnaire,
where in the ﬁrst there are 4 shapes to choose from, while in the second there are 7 shapes to
choose from (just as in the original version).
The data was collected in the same way as in the original experiment (each subject played
10 randomized game instances). A comparison was made between a domain data learning
agent and a focal point learning agent, so as to evaluate the advantages gained by using
the new learning method in a variant on the domain which is more complex than the orig-
123

Auton Agent Multi-Agent Syst
inal one. When the number of randomized shapes was smaller than 7, we used a binary
value to represent a “non-existing” shape value for that input neuron. The same focal point
rules were used, with the following adjustments to the centrality and extremeness rules, in
order to consider the possibility of randomizing the number of shapes per row. (1) Cen-
trality—Centrality gave additional focality value to the middle choice, when the number
of shapes was odd. With an even number of shapes, no centrality value was given. (2)
Extremeness—The Extremeness property gave higher focality values to the ﬁrst and last
choices in the set. Those values became higher as the number of shapes increased (the
extremeness in a game with 3 shapes was lower than the extremeness in a game with 6
shapes).
Our results show that the domain data agent managed to achieve a correct classiﬁcation
rate of 37%, while the focal point learning agent managed to beat that with a 49% cor-
rect classiﬁcation rate. We also checked the performance of a regular focal point agent (not
employing any learning algorithm), and found that it performs as well as the domain data
agent, with a 38% correct classiﬁcation rate.
6.3 Culturally biased candidate selection domain
One of Schelling’s classic examples can be found in [27], where he showed his subjects a 4
by 4 grid (see Fig. 2) and instructed them as follows:
“Put a check mark in one of the sixteen squares. You win if you all succeeded in checking
the same square.”
Schelling reported that 24 out of 41 (58%) people selected the upper-left corner (which
is of course more than the 6.25% predicted by standard decision theory), and all but 3 of the
votes were distributed along the diagonal.
We conducted a similar experiment with a larger set of subjects (81 ﬁrst-year students)
and surprisingly, we had two main focal points: the ﬁrst was the upper-left corner (with 40
votes), and the second was the upper-right corner (with 32 votes). The upper-right corner
did not appear as a possible focal solution to this problem in Schelling’s report, but was
found as a strong focal point in our data. Those results seem less surprising when we add
the information that the experiment was conducted with students who write from right to
left. We hypothesize that this cultural information biased the subjects toward the upper-right
corner as an additional focal point, along with the upper-left one, which was found in the
original experiment.
In order to further explore the notion of culturally biased tacit coordination games, we
took the Candidate Selection game and designed a new, culturally biased, variation of it. In
the original version of the game, the ﬁve candidates were enumerated using numbers 1 to 5.
In the new variant, the ﬁve candidates were enumerated using family names that remained
constant and were not randomized (all other properties were still randomized). Candidate
1 was named “Brown”, candidate 2 was named “Cohen”, candidate 3 was named “Jones”,
candidate 4 was named “Mitchell”, and candidate 5 was named “White”. The names were
selected as to give a cultural bias to candidate 2 as this name is of Jewish origin (and one of
the most common family names in Israel, where the experiment took place), while all other
candidates have common American family names that we assumed have no special meaning
to most of our student subjects (Fig. 15).
We used ﬁrst and second year students to play randomized instances of the game. The
results, without even considering the candidate properties, are not surprising: while in the
123

Auton Agent Multi-Agent Syst
original version (enumerated by numbers), 16% of the students selected candidate 2, in the
biased version this number rose to 27%.
Given a tacit coordination domain and such knowledge of a cultural bias (or biases of other
forms as we will see below), we can augment the FPL with a cultural bias-based rule that
will increase the focality value of the choice (candidate 2 in our example) by some constant
amount. This increase will help the learning algorithm to break ties and differentiate towards
the biased answer when the focality difference is small. In our experiments, by including
such a bias rule in the form of a constant increase of 0.15 to singularity value of candidate 2,
we managed to increase the agent’s correct classiﬁcation rate by 9%.
6.4 Additional insights
In a series of experiments that were partly conducted to reproduce and verify the results
reported in several of Schelling’s classic experiments, and partly as a pre-experimental stage
for the domains presented above, we gained additional results that can help in deepening our
understanding of the nature of focal points.
6.4.1 Educational bias
In a pre-experimental stage of the “pick the pile” domain, we conducted pen-and-paper
experiments in a small class composed of 14 computer science graduate students. The pre-
experimental stage was meant to check whether the written instructions were comprehensible,
as we wanted to avoid answering questions out loud, due to the possibility that such answers
might bias the subjects.
The instructions stated that the subjects were to be paired with an unknown partner from
that class, and that their answers would be compared. Here again, a monetary prize was given
to the couple with the highest number of successful coordinations.
When recording their solutions, we found that there was a tendency to select piles that
are on the same column or row, even if they are further away in terms of their Manhattan
distance. Those results surprised us, as we hypothesized that the subjects, being computer
science graduates, would naturally follow the Manhattan distance measure, as the instructions
clearly state that “diagonal moves are illegal”.
After the experiment ended, we interviewed several subjects in order to understand their
selection. To our surprise, it appears that those students were working in the ﬁeld of Robotics,
and it was obvious to them that the refacing operation is an expensive operation in the robotics
domain, and should be avoided. They also added that they used that knowledge when solving
Fig. 15 Culturally biased example
123

Auton Agent Multi-Agent Syst
Table 3
“Heads or Tails?”
results
“Heads” selected
“Tails”selected
“Heads or Tails?”
23
7
“Tails or Heads?”
13
16
the coordination games, as they knew that their coordination partners were to be selected
from that group alone.
Those results show again that the more knowledge one has of one’s coordination partner,
the more it allows one to focus choices with respect to that information which is common to
both partners.
6.4.2 Firstness in “Heads or Tails?” domain
In several of our paper and pen experiments we decided to explore why, according to Schel-
ling, around 85% of the answers to the simple “Heads or Tails?” question turns out to be
“Heads”.11 Why is “Heads” an obvious focal point for a tacit coordination task based on that
question? We hypothesized, according to our suggested focal point rules, that the Firstness
property has a crucial role in focalizing “Heads”, as it appears ﬁrst in the presentation of the
question.
In order to check our hypothesis, we generated two types of questions: one being the
original formulation of the problem “Heads or Tails?”, and the other the reverse presenta-
tion, “Tails or Heads?”. In several of our previous experiments, we planted a single question
of one of the above forms into the questionnaire. Thus, several of our subjects played the
“Heads or Tails?” version, and others played with the “Tails or Heads?” version.
Overall, we collected 59 answers, and the results appear in Table 3. The results show that
on the “Heads or Tails?” question, we got results similar to those of Schelling, as only 7
people selected “Tails” out of 30 subjects (i.e., 23% in our experiment compared with Schel-
ling’s 14%). However, in the second variation, the “Tails or Heads?” presentation, 55% of
the subjects selected “Tails”, a greatly increased percentage. It might be hypothesized that
although presenting “Tails” ﬁrst made it a far more likely choice, it may not have reached the
popularity of “Heads”, since “Heads or Tails?”, with “Heads” ﬁrst, is still the more stan-
dard presentation of the problem in English (and thus the primacy of Heads is still somewhat
maintained in many players’ minds).
In any case, it is easy to see that the presentation of the question completely changed
the focality of “Heads” from a high focality answer in the original presentation, to a lower
focality answer in the new presentation. Two lessons can be learned from those results: ﬁrst,
obviously the presentation of the coordination task impacts its focality difference. Second,
as the only difference in the new version is the order of the choices, ﬁrstness is an important
property in that coordination task.
7 Conclusions and future work
In this article we have been interested in enhancing agent-human coordination possibilities
in situations where communication is not possible. When an agent (or robot) is coordinating
with a human partner, having a cognitive mechanism that mimics the reasoning process of
that human being will make the agent more robust to changes in the environment, and more
adaptable to different domains.
11 In his experiments, 36 selected Heads and only 6 Tails.
123

Auton Agent Multi-Agent Syst
We presented the Focal Point Learning (FPL) approach to building automated agents that
play tacit coordination games with general human partners in dynamic environments. The
technique makes use of learning algorithms to train agents to coordinate with general human
partners in speciﬁc domains; focal points are integrated into the learning process through the
use of focal point selection rules. Training data is preprocessed and transferred into a new
representation space, where each vector contains quantiﬁed focal point values, and these are
used to train the agent.
We created three experimental domains, and collected data to be used for training agents
to predict human coordination choices in those domains. Results showed that when trained
solely on the domain-encoded data, the classiﬁers resulted in a close-to-random correct
classiﬁcation percentage, while the FPL agents managed to achieve a significantly higher
correct classiﬁcation rate. We then created two variants of one of the domains, collected
human data for the variants, and then checked the coordination performance, in these vari-
ants, of each of the agents that had been trained in the original domain. Here again, the
FPL agents outperformed the others, and demonstrated robustness in an altered environ-
ment.
As can been seen from our experimental work, even in our simpliﬁed domains the sug-
gested FPL technique relies on having a large amount of data. Moreover, this data should
be taken from a large enough sample space to remove various biases. It might be that the
costs of applying FPL techniques for cases where there is a need to coordinate without com-
munication is not practical for all domains, as acquiring experimental data from humans
is not a simple procedure. However, in domains where human lives are at stake, such as
search-and-rescue tasks, caring for the elderly, or human-robot teams, this technique may
be very beneﬁcial. In addition, the wide array of electronic data that is available due to the
development of the internet (e.g., web searches and electronic transactions), can be used with
various data mining techniques to improve user-interface experiences or automated agent
development processes, when communication will not be available at runtime.
Future work on Focal Point Learning can be taken in several different directions. First and
foremost, more experimental work with human beings is needed to deepen our understanding
of the nature of focal points and their properties. Through that effort, we can gain a deeper
understanding of the reasoning process that underlies tacit coordination games. Additional
experimental work on the psychological component of focal points is most definitely needed.
Another direction is to take a game theoretic approach and look for a robust and formal
theory in game theoretic terms which, in contrast to existing focal point theories, will have
predictive abilities and not only the capability of retrospective analysis.
FromtheFocalPointLearningpointofview,oneobviousnextstepistoapplythetechnique
to a real-world domain, for instance, a military application where manned and unmanned
aircraft or land vehicles are cooperating to carry out some well-deﬁned task. In military appli-
cations, due to uncertainty and the dynamic nature of the environment, having a cognitive
mechanism could prove to be highly important.
To sum up, when building agents to coordinate with unfamiliar human partners, with-
out communication, machine learning classiﬁers have difﬁculty generalizing data to predict
human choices. Focal Point Learning can improve performance, and robustness in the face
of environmental change.
Acknowledgements
This research is based upon work supported in part by the U.S. Army Research
Laboratory and the U.S. Army Research Ofﬁce under grant number W911NF-08-1-0144, AFOSR grant
FA95500610405, NSF grant 0705587 and under ISF grant #1357/07. We appreciate the comments of the
anonymous reviewers, which were quite useful.
123

Auton Agent Multi-Agent Syst
References
1. Arkin, R. C. (1992). Cooperation without communication: Multiagent schema-based robot naviga-
tion. Journal of Robotic Systems, 9(3), 351–364.
2. Bacharach, M. (1993). Variable universe games. In Frontiers of game theory (pp. 255–275). Cambridge:
MIT Press.
3. Bacharach, M., & Bernasconi, M. (1997). The variable frame theory of focal points: An experimental
study. Games and Economic Behavior, 19(1), 1–45.
4. Chernoff, H., & Moses, L. (1986). Elementary decision theory. New York, NY: Dover Publications
Inc.
5. Crawford, V. P, & Haller, H. (1990). Learning how to cooperate: Optimal play in repeated coordination
games. Econometrica, 58(3), 571–595.
6. Fenster, M., Kraus, S., & Rosenschein J. S. (1998). Coordination without communication: Experimental
validation of focal point techniques. In Readings in agents (pp. 380–386). San Francisco, CA: Morgan
Kaufmann Publishers Inc.
7. Gauthier, D. (1975). Coordination. Dialogue, 14, 195–221.
8. Gervasi, V., & Prencipe, G. (2004). Coordination without communication: The case of the ﬂocking
problem. Discrete Applied Mathematics, 144(3), 324–344.
9. Goodrich, M. A., & Schultz, A. C. (2007). Human-robot interaction: A survey. Foundations and
Trends in Human-Computer Interaction, 1(3), 203–275.
10. Grosz, B. J. (2004). Beyond mice and menus. Proceedings of the American Philosophical Soci-
ety, 149(4), 529–543.
11. Harsanyi, J. C., & Selton, R. (1988). A general theory of equilibrium selection in games. Cambridge,
MA: MIT Press.
12. Hirsh, R., Graham, J., Tyree, K., Sierhuis, M., & Clancey, W. J. (2006). Intelligence for human-
assistant planetary surface robots. In A. M. Howard, & E. W. Tunstel (Eds.), Intelligence for space
robotics (pp. 261–279). Albuquerque: TSI Press.
13. Hill, R. W., Jr., Kim, Y., & Gratch, J. (2002). Anticipating where to look: Predicting the movements
of mobile agents in complex terrain. In Proceedings of the ﬁrst international joint conference on
autonomous agents and multiagent systems (AAMAS’02) (pp. 821–827). New York, NY: ACM.
14. Janssen, M. C. W. (1998). Focal points. In P. Newman (Ed.), The new palgrave of economics and the
law (pp. 150–155). London: MacMillan.
15. Janssen, M. (2001). Rationalizing focal points. Theory and Decision, 50(2), 119–148.
16. Kandori, M., Mailath, G. J., & Rob, R. (1993). Learning, mutation, and long run equilibria in
games. Econometrica, 61(1), 29–56.
17. Kraus, S. (2001) Strategic negotiation in multiagent environments. Cambridge, MA: MIT Press.
18. Kraus, S., Rosenschein, J. S., & Fenster, M. (2000). Exploiting focal points among alternative solutions:
Two approaches. Annals of Mathematics and Artiﬁcial Intelligence, 28(1–4), 187–258.
19. Lewis, D. K. (1969). Convention: A philosophical study. Cambridge: Harvard University Press.
20. Martin, C., Schreckenghost, D., Bonasso, P., Kortenkamp, D., Milam, T., & Thronesbery, C. (2003).
An environment for distributed collaboration among humans and software agents. In AAMAS ’03:
Proceedings of the second international joint conference on Autonomous agents and multiagent systems
(pp. 1062–1063). New York, NY: ACM.
21. Mehta, J. (1997). Telling tales: Actors’ accounts of their behavior in coordination games. In Focal
points: Coordination, complexity and communication in strategic contexts. Lund, Sweden.
22. Mehta, J., Starmer, C., & Sugden, R. (1994). The nature of salience: An experimental investigation
of pure coordination games. American Economic Review, 84(3), 658–673.
23. Nash, J. (1953). Two-person cooperative games. Econometrica, 21(1), 128–140.
24. Pederson, L., Kortencamp, D., Wettergreen, D., & Nourbakhsh, I. (2003). A survey of space robotics.
In 7th International symposium on artiﬁcial intelligence, robotics and automation in space (i-SAIRAS),
Nara, Japan.
25. Ross Quinlan, J. (1993). C4.5: Programs for Machine Learning. San Mateo: Morgan Kaufmann.
26. Russell, C., DeJong, D. V., Forsythe, R., & Ross, T. W. (1990). Selection criteria in coordination
games: Some experimental results. American Economic Review, 80(1), 218–233.
27. Schelling, T. (1963). The strategy of conﬂict. New York: Oxford University Press.
28. Schermerhorn, P., & Scheutz, M. (2006). Social coordination without communication in multi-agent
territory exploration tasks. In Proceedings of the ﬁfth international joint conference on autonomous
agents and multi-agent systems (pp. 654–661). Hakodate, Japan.
29. Schurr, N., Marecki, J., Lewis, J. P., Tambe, M., & Scerri, P. (2005) The defacto system: Training
tool for incident commanders. In AAAI-05 (pp. 1555–1562). Pittsburgh.
123

Auton Agent Multi-Agent Syst
30. Sierhuis, M., Bradshaw, J. M., Acquisti, R., Van Hoof, R., & Jeffers, R., (2003). Human–agent team-
work and adjustable autonomy in practice. In 7th International symposium on artiﬁcial intelligence,
robotics and automation in space (i-SAIRAS), Nara, Japan.
31. Sugden, R. (1995). A theory of focal points. Economic Journal, 105(430), 533–550.
32. Van Huyck, J. B., Battalio, R. C., & Beil, R. O. (1990). Tacit coordination games, strategic uncertainty,
and coordination failure. American Economic Review, 80(1), 234–248.
33. Werbos, P. (1974). Beyond regression: New tools for prediction and analysis in the behavioral sciences.
PhD thesis, Committee on Applied Mathematics, Harvard University, Cambridge, MA, November.
34. Wooldridge, M. J. (2009) An Introduction to MultiAgent Systems (2nd ed.). Chichester, West Sussex:
Wiley-Blackwell.
35. Young, H. P. (1993). The evolution of conventions. Econometrica, 61(1), 57–84.
36. Young, H. P. (1996). The economics of conventions. Journal of Economic Perspectives, 10(2), 105–122.
123

