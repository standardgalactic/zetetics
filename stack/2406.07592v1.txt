MambaLRP: Explaining Selective State Space Sequence Models
Farnoush Rezaei Jafari1,2
Grégoire Montavon3,2,1
Klaus-Robert Müller1,2,4,5,6
Oliver Eberle1,2
1Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany
2BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany
3Department of Mathematics and Computer Science, Freie Universität Berlin,
Arnimallee 14, 14195 Berlin, Germany
4Department of Artificial Intelligence, Korea University, Seoul 136-713, South Korea
5Max Planck Institute for Informatics, Stuhlsatzenhausweg 4, 66123 Saarbrücken, Germany
6Google DeepMind, Berlin, Germany
Correspondence to: rezaeijafari@campus.tu-berlin.de, oliver.eberle@tu-berlin.de
Abstract
Recent sequence modeling approaches using Selective State Space Sequence Models, referred
to as Mamba models, have seen a surge of interest. These models allow efficient processing
of long sequences in linear time and are rapidly being adopted in a wide range of applications
such as language modeling, demonstrating promising performance. To foster their reliable use in
real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap
by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba
architecture. Guided by the axiom of relevance conservation, we identify specific components in
the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose
MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable
relevance propagation through these components. Our proposed method is theoretically sound and
excels in achieving state-of-the-art explanation performance across a diverse range of models and
datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering
various biases and evaluating their significance. It also enables the analysis of previous speculations
regarding the long-range capabilities of Mamba models.
Keywords: Explainable AI, State Space Models, Mamba, Long-Range Dependencies
1
Introduction
Sequence modeling has demonstrated its effectiveness and versatility across a wide variety of tasks and data types,
including text, time series, genomics, audio, and computer vision [21, 72, 28, 8, 23]. Recently, there has been a
surge of interest in a new class of sequence modeling architectures, known as structured state space sequence models
(SSMs) [31, 56, 29]. This is due to their ability to process sequences in linear time, as opposed to quadratic time
required by the more established Transformer architectures [59]. The recent Mamba architecture, a prominent and
widely adopted instance of state space models, has demonstrated competitive predictive performance on a variety of
sequence modeling tasks across domains and applications [29, 39, 73, 68, 60], while scaling linearly with sequence
length.
As Mamba models, and more generally SSMs, are rapidly being adopted into real-world applications, ensuring their
transparency is crucial. This enables inspection beyond test set accuracy and uncovering various forms of biases,
including ‘Clever-Hans’ effects [35]. It is particularly important in high-risk domains such as medicine, where the
prediction behavior must be robust under real-world conditions and aligned with human understanding. The field of
Explainable AI [42, 32, 7, 49] focuses on developing faithful model explanations that attribute predictions to relevant
features and has shown success in explaining many highly nonlinear models such as convolutional networks [18], or
attention-based Transformer models [3, 2].
Explaining the predictions of Mamba models is however challenging due to their highly non-linear and recurrent
structure. A recent study [4] suggests viewing these models as attention-based models, enabling the use of attention-
based explanation methods [1, 17]. Yet, the explanations produced by attention-based techniques are often unreliable
and exposed to potential misalignment between input features and attention scores [65, 34]. As an alternative,
1
arXiv:2406.07592v1  [cs.LG]  11 Jun 2024

Figure 1: Conceptual steps involved in the design of MambaLRP. (a) Take as a starting point a basic LRP procedure,
equivalent to Gradient × Input. (b) Analyze layers in which the conservation property is violated. (c) Rework the
relevance propagation strategy at those layers to achieve conservation. The resulting MambaLRP method enables
efficient and faithful explanations.
Layer-wise Relevance Propagation (LRP) [9] decomposes the model function with the goal of explicitly identifying
the relevance of input features by applying purposely designed propagation rules at each layer. A distinguishing
feature of LRP is its adherence to a conservation axiom, which prevents the artificial amplification or suppression of
feature relevance in the backward pass. LRP has been demonstrated to produce faithful explanations across various
domains (e.g. [6, 3, 18]). Nevertheless, the peculiarities of the Mamba architecture are not addressed by the existing
LRP procedures, which may lead to the violation of the conservation property and result in unreliable explanations.
In this work, we present MambaLRP, a novel approach to integrate LRP into the Mamba architecture. By examining
the relevance propagation process across Mamba layers through the lens of conservation, we pinpoint layers within
the Mamba architecture that need to be addressed specifically. We propose a novel relevance propagation strategy
for these layers, grounded in the conservation axiom, that is theoretically sound, straightforward to implement and
computationally efficient. Through a number of quantitative evaluations, we show that the proposed MambaLRP
approach allows to robustly deliver the desired high explanatory performance, exceeding by far the performance of
various baseline explanation methods as well as a naive transposition of LRP to the Mamba architecture. We further
demonstrate the usefulness of MambaLRP in several areas: gaining concrete insights into the model’s prediction
mechanism, uncovering undesired decision strategies in image classification, identifying gender bias in language
models, and analyzing the long-range capabilities of Mamba. Our code is publicly available.1
2
Related Work
Structured State Space Sequence Models (SSMs).
Transformers [59] have emerged as the most widely used
architectures for sequence modeling. However, their computational limitations, particularly with large sequence
lengths, have restricted their applicability in modeling long sequences. Addressing these computational limitations,
recent works [30, 31] have introduced structured state space sequence models (SSMs) as an alternative approach.
SSMs are a class of sequence modeling methods, leveraging the strengths of recurrent, convolutional, and continuous-
time methods, demonstrating promising performance across various domains, including language [26, 40], image [67,
12, 44], and video [61] processing, and beyond [50, 19, 38]. A recent advancement by Gu and Dao [29] introduced
selective SSM, an enhanced data-dependent SSM with a selection mechanism that adjusts its parameters based
on the input. Built on this dynamic selection, the Mamba architecture fuses the SSM components with multilayer
perceptron (MLP) blocks. This fusion simplifies the architecture while improving its ability to handle various sequence
modeling tasks, including applications in language processing [5, 45, 62], computer vision [37, 73, 68], medical
imaging [39, 66, 27, 47, 36, 64, 63] and graphs [60, 13]. This fast adoption of SSMs and Mamba models underscores
the need for reliable explanations of their predictions.
Explainable AI and SSMs.
In efforts to explain Mamba models, Ali et al. [4] recently proposed viewing their
internal computations as an attention mechanism. This approach builds upon previous works that use attention
signal as explanation, including Attention Rollout [1] and variants thereof [17, 16]. While these approaches can
provide some insight, they inherit the limitations of using attention as an explanation [65, 34], including their inability
to capture potential misalignment between tokens and attention scores, and the limited performance in empirical
faithfulness evaluations. Alternative Explainable AI methods, not yet applied to Mamba models but in principle
1https://github.com/FarnoushRJ/MambaLRP
2

applicable to any model, include techniques using input perturbations [71, 74, 25] or leveraging gradient information
[10, 54, 58, 55, 53]. Despite their generality, these models have certain drawbacks, such as requiring multiple function
evaluations for a single explanation or being susceptible to gradient noise, resulting in subpar performance, as our
benchmark experiment will demonstrate. To tackle these challenges, we introduce MambaLRP as an efficient solution
for the computation of reliable and faithful explanations that are theoretically grounded in the axiom of relevance
conservation.
3
Background
Before delving into the details of our proposed method, we begin with a brief overview of the selective SSM
architecture, followed by an introduction to the LRP framework.
Selective SSMs (S6)
An important component within the Mamba [29] architecture is the selective SSM. It is
characterized by parameters, ¯A, ¯B, and C, and transforms a given input sequence (xt)T
t=1 into an output sequence of
the same size (yt)T
t=1 via the following equations:
ht = ¯
Atht−1 + ¯
Btxt
(1)
yt = Ctht
(2)
where the initial state h0 = 0. What distinguishes the selective SSM from the original SSM (S4) [31] is that the
evolution parameter, ¯
At, and projection parameters, ¯
Bt and Ct, are functions of the input xt. This enables dynamic
adaptation of the SSM’s parameters based on input. This dynamicity facilitates focusing on relevant information
while ignoring irrelevant details when processing a sequence.
Layer-wise Relevance Propagation
Layer-wise Relevance Propagation (LRP) [9] is an Explainable AI method that
attributes the model’s output to the input features through a single backward pass. This backward pass is specifically
designed to identify neurons relevant to the prediction. LRP assigns relevance scores to neurons in a given layer
and then propagates these scores to neurons in the preceding layer. The process continues layer by layer, starting
from the network’s output and terminating once the input features are reached. The LRP backward pass relies on
an axiom called ‘conservation’ requiring that relevance scores are preserved across layers, avoiding to artificially
amplify or suppress contributions. For example, let x and y be the input and output of some layer, respectively, and
let R(x) and R(y) represent the sum of relevance scores in the respective layers. The conservation axiom requires
that R(x) = R(y) holds true.
4
LRP for Mamba
In this work, we bring explainability, particularly LRP, to Mamba models, following the conceptual design steps, shown
in Fig. 1. We start by applying a basic LRP procedure, specifically one corresponding to Gradient × Input (GI), to the
Mamba architecture. This serves as an effective initial step for identifying layers where certain desirable explanation
properties, like relevance conservation, are violated. We analyze different layers of the Mamba architecture, derive
relevance propagation equations and test the fulfillment of the conservation property. Our analysis reveals three
components in the Mamba architecture where conservation breaks: the SiLU activation function, the selective SSM,
and the multiplicative gating of the SSM’s output. Leveraging the analysis above, we propose novel relevance
propagation strategies for these three components, which lead to a robust, faithful and computationally efficient
explanation approach, called MambaLRP.
4.1
Relevance propagation in SiLU layers
We start by examining the relevance propagation through Mamba’s SiLU activation functions. This function is
represented by the equation y = x · σ(x), where σ denotes the logistic sigmoid function.
Proposition 4.1 Applying the standard gradient propagation equations yields the following result, which relates the
relevance values before and after the activation layer:
∂f
∂xx
|{z}
R(x)
= ∂f
∂y y
|{z}
R(y)
+ ∂f
∂y · σ′(x) · x2
|
{z
}
ε
(3)
3

The derivation for Eq. 3 can be found in Appendix A.1. We observe that the conservation property, i.e. R(x) = R(y),
is violated whenever the residual term ε is non-zero. We propose to restore the conservation property in the relevance
propagation pass by locally expanding the SiLU activation function as:
y = x · [σ(x)]cst.
(4)
where [·]cst. treats the given quantity as constant. This can be implemented e.g. in PyTorch using the .detach()
function. Repeating the derivation above with this modification yields the desired conservation property, R(x) = R(y).
The explicit LRP rule associated to this LRP procedure is provided in Appendix B.
4.2
Relevance propagation in selective SSMs (S6)
xt
ht
yt
ht ¡1
yt ¡1
xt ¡1
Bt
At
Ct ¡1
Figure 2: Unfolded view of SSM, highlighting two sub-
sets of nodes, the relevance of which should be conserved
throughout relevance propagation.
The most crucial non-linear component of the Mamba ar-
chitecture is its selective SSM component. It is designed
to selectively retain or discard information throughout
the sequence by adjusting its parameters based on the
input, enabling dynamic adaptation to each token. To
facilitate the analysis, we introduce an inconsequential
modification to the original SSM by connecting Ct to
ht instead of xt. To do so, we can redefine ¯
At, ¯
Bt, and
Ct matrices as blockdiag( ¯At , 0), ( ¯Bt , I), and (Ct | 0)
respectively, such that xt becomes part of the state ht
without altering the overall functionality of the SSM.
The unfolded SSM, with the aforementioned modifica-
tion, is illustrated in Fig. 2. The complex relevance
propagation procedure in the SSM component can be
further simplified by considering two groups of units,
illustrated in red and orange in Fig. 2. In these two groups, there are no connections within units of the same group,
all the relevance propagation signals from the first group are directed towards the second group, and the second group
receives no further incoming relevance propagation signal. With these properties, these two groups should, according
to the principle of conservation, receive the same relevance scores.
Proposition 4.2 Defining θt = ( ¯At, ¯Bt, Ct−1), and working out the propagation equations between these two groups
yields the following relation:
∂f
∂xt
xt +
∂f
∂ht−1
ht−1
|
{z
}
R(xt)+R(ht−1)
= ∂f
∂ht
ht +
∂f
∂yt−1
yt−1
|
{z
}
R(ht)+R(yt−1)
+ ∂f
∂θt
∂θt
∂xt
xt + ∂f
∂θt
∂θt
∂ht−1
ht−1
|
{z
}
ε
(5)
The derivation for Eq. 5 can be found in Appendix A.2. We note that the residual term ϵ, which is typically non-zero,
violates conservation. Specifically, conservation fails due to the dependence of θ on the input. We propose to rewrite
the state-space model at each step in a way that the parameters θt appear constant, i.e.:
ht = [ ¯
At]cst.ht−1 + [ ¯
Bt]cst.xt
(6)
yt = [Ct]cst.ht
(7)
These equations can also be interpreted as viewing the selective SSM as a localized non-selective, i.e. standard, SSM.
With this modification, conservation holds between the two groups, i.e. R(xt) + R(ht−1) = R(ht) + R(yt−1). By
repeating the argument for each time step, conservation is also maintained between the input and output of the whole
SSM component. Explicit LRP rules are provided in Appendix B.
4.3
Relevance propagation in multiplicative gates
In each block within the Mamba architecture, the SSM’s output is multiplied by an input-dependent gate. In other
words, y = zA · zB, where zA = SSM(x) and zB = SiLU(Linear(x)). Assume that the locally linear expansions
introduced in Sections 4.1 and 4.2 are applied to the SSM components and SiLU activation functions, the mapping
from x to y becomes quadratic.
4

Proposition 4.3 Applying the standard gradient propagation equations establishes the following relation between
the relevance values before and after the gating operation:
∂f
∂xx
|{z}
R(x)
= ∂f
∂y y
|{z}
R(y)
+ ∂f
∂y y
|{z}
ε
(8)
The derivation for Eq. 8 and explicit LRP rules can be found in Appendix A.3 and Appendix B, respectively. In this
equation, we observe a spurious doubling of relevance in the backward pass. This can be addressed by treating half of
the output as constant:
y = 0.5 · (zA · zB) + 0.5 · [zA · zB]cst.
(9)
As for the previous examples, this ensures the conservation property R(x) = R(y). An alternative would have
been to make y linear by detaching only one of the terms in the product, as done for the SiLU activation or the
SSM component. However, the strategy of Eq. (9) better maintains the directionality given by the gradient. We
further compare these alternatives in an ablation study presented in Appendix C.5, demonstrating empirically that our
proposed approach performs better.
4.4
Additional modifications and summary
The propagation strategies developed for the Mamba-specific components complement previously proposed ap-
proaches for other layers, including propagation through RMSNorm layers [3] and convolution layers via robust
LRP-γ rules [43, 22] and their generalized variants. A summary of these additional enhancements is provided in
Appendix C.2.
Overall, our MambaLRP procedure can be implemented as a sequence of two steps:
1. Perform the detach operations of Eqs. (4), (6), (7), and (9) (as well as similar operations for RMSNorm and
convolutions).
2. Retrieve MambaLRP explanations by performing Gradient × Input on the detached model.
5
Experiments
To evaluate our proposed approach, we benchmark its effectiveness against various methods previously proposed in the
literature for interpreting neural networks. We empirically evaluate our proposed methodology using Mamba-130M
and Mamba-1.4B language models [29], which are trained on diverse text datasets. For the vision experiments, we
use the Vim-S model [73]. Moreover, we perform several ablation studies to further investigate our proposed method.
Datasets
In this study, we perform experiments on four text classification datasets, namely SST-2 [57], Medical
BIOS [24], Emotion [51], and SNLI [15]. The SST-2 dataset encompasses around 70K English movie reviews,
categorized into binary classes, representing positive and negative sentiments. The Medical BIOS dataset consists of
short biographies (10K) with five specific medical occupations as targets. The SNLI corpus (version 1.0) comprises
570k English sentence pairs, with the labels entailment, contradiction, and neutral, used for the natural language
inference (NLI) task. The Emotion dataset (20K) is a collection of English tweets, each labeled with one of six basic
emotions. For the vision experiments, we use ImageNet dataset [20] with 1.3M images and 1K classes.
Baseline methods
We compare our proposed method with several gradient-based, model-agnostic explanation
techniques: Gradient × Input (GI) [10, 54], SmoothGrad [55], and Integrated Gradients [58]. Furthermore, we
evaluate the performance of our proposed method against a naive implementation of LRP, i.e. LRP (LN-rule), where
the LRP-0 rule is used in all linear and convolution layers, along with the LN-rule [3] in normalization layers. We
also compare the performance of our proposed method with two attention-based explanation techniques, which are
Attention Rollout (AttnRoll, [1]) and Gradient × Attention Rollout (G × AttnRoll, [17]). These approaches were
initially developed to explain transformer models and have recently been extended to interpret Mamba models [4].
5.1
Conservation property
To verify the fulfillment of the conservation property, on which our method is based, we compare the network’s output
score with the sum of relevance scores attributed to the input features, for both the GI baseline and the proposed
5

MambaLRP. The analysis is performed for Mamba-130M models trained on the SST-2 and ImageNet datasets. Full
conservation is achieved if the output score equals the sum of relevance, as indicated by the blue line in Fig. 3. Our
results show that conservation is severely violated by the GI baseline, and is addressed to a large extent by MambaLRP.
Residual lack of conservation is due to the presence of biases in linear and convolution layers, which are typically
non-attributable.
Figure 3: Conservation property. The x-axis represents the sum of explanation scores across the input features and the
y-axis shows the network’s output score. Each point corresponds to one example and its proximity to the blue identity
line indicates the extent to which conservation is preserved, with closer alignment suggesting improved conservation.
5.2
Qualitative evaluation
In this section, we qualitatively examine the explanations produced by various explanation methods for Mamba-130M
and Vim-S models. Fig. 4 illustrates the explanations generated to interpret the Mamba-130M model’s prediction
on a sentence from the SST-2 dataset with negative sentiment. We note that all of the explanation methods attribute
positive scores to the word ‘disgusting’, which appears reasonable given the negative sentiment label. However, it
is notable that the explanation generated by MambaLRP is more sparse and focuses particularly on the terms ‘so’
and ‘disgusting’. In contrast, the explanations produced by the gradient-based methods and AttnRoll appear to be
quite noisy. Furthermore, we show the explanations produced to interpret the Vim-S model’s predictions on images of
Figure 4: Explanations generated for a sentence of the SST-2 dataset. Shades of red represent words that positively
influence the model’s prediction. Conversely, shades of blue reflect negative contributions. The heatmaps of attention-
based methods are constrained to non-negative values.
the ImageNet dataset in Fig. 5. Purely gradient-based explanations tend to identify unspecific noisy features, while
both attention-based approaches, AttnRoll and G × AttnRoll, are more effective at highlighting significant features.
Among these methods, MambaLRP stands out for its ability to generate explanations that are particularly focused on
key features used by the model to make a prediction. Take, for instance, the first image classified under the ‘African
elephant’ category. We can see that the explanation generated by MambaLRP not only includes all occurrences of the
‘African elephant’ object but also highlights its distinctive features, such as the tusks. In the second image labeled
‘wild boar’, despite the presence of multiple objects in the image, MambaLRP’s explanation remains focused on
the ‘wild boar’ object, disregarding other objects. Moreover, in the third instance, MambaLRP uncovers a spurious
correlation, the presence of a watermark in Chinese, influencing the model’s prediction, a subtlety overlooked or not
fully represented by other methods.
5.3
Quantitative evaluation
To quantitatively evaluate the faithfulness of explanation methods, we employ an input perturbation approach based
on ranking input features by their importance [48], which can be done using either a Most Relevant First (MoRF) or
Least Relevant First (LeRF) strategy. Ranked features are iteratively perturbed through a process known as flipping.
6

Figure 5: Explanations produced by different explanation methods for images of the ImageNet dataset. Explanations
produced by AttnRoll and G×AttnRoll are limited to non-negative values.
We monitor the resulting changes in the output logits, fc, for the predicted class c, and compute the area under the
perturbation curve. The areas under the curves for LeRF and MoRF strategies are denoted by AF
LeRF and AF
MoRF,
respectively. In contrast, the insertion method starts with a fully perturbed input and progressively restores important
features. The areas under the curves for this method are indicated by AI
MoRF and AI
LeRF, for the MoRF and LeRF
strategies, respectively. A reliable explanation method is characterized by low values of AF
MoRF or AI
LeRF, and
large values of AF
LeRF or AI
MoRF. In an effort to minimize the introduction of out-of-distribution manipulations, the
recent study by Blücher et al. [14] advocates for harnessing both insights to derive a more resilient metric. Therefore,
we follow the same strategy as [14, 2] to evaluate explanation methods. The evaluation metrics are defined as
∆AF = AF
LeRF −AF
MoRF and ∆AI = AF
MoRF −AF
LeRF. In both metrics, a higher score is preferable, as it signifies
a more accurate and reliable explanation method.
The outcomes of this analysis are represented in Table 1. MambaLRP consistently achieves highest faithfulness
scores in comparison to other baseline methods. We observe that GI struggles with noisy attributions, leading to low
faithfulness scores. However, methods like Integrated Gradients and G×AttnRoll have shown improvements in this
regard. We note that LRP (LN-rule) outperforms most methods across the majority of the text classification tasks.
Nevertheless, its performance is notably inferior compared to MambaLRP. Overall, we observe that MambaLRP
significantly outperforms all other methods by a substantial margin. In both vision and NLP experiments, attention-
based methods have shown superior performance compared to the purely gradient-based approaches. Results of the
complementary insertion experiment are presented in Appendix C.4, which consistently confirm the observations
from the flipping experiment.
Table 1: Evaluating explanation methods. Higher scores ∆AF indicate more faithful explanations.
Methods
SST-2
Med-BIOS
SNLI
Emotion
ImageNet
Mamba
130M
Mamba
1.4B
Mamba
130M
Mamba
1.4B
Mamba
130M
Mamba
1.4B
Mamba
130M
Mamba
1.4B
Vim-S
Random
-0.012
-0.106
0.044
-0.014
0.010
0.002
-0.001
0.000
0.000
GI [54]
0.078
-0.106
0.200
-0.634
-0.039
-0.039
-0.787
-0.409
0.041
SmoothGrad [55]
1.377
-0.383
1.661
-2.300
0.486
-0.687
1.808
-1.852
0.121
IG [58]
0.857
0.216
1.296
1.065
0.453
0.218
1.808
2.010
0.328
AttnRoll [4]
0.657
0.431
2.228
1.076
0.242
0.371
0.389
1.483
0.714
G × AttnRoll [4]
1.190
0.626
3.126
3.006
0.513
0.554
2.003
4.706
0.792
LRP (LN-rule, [3]) 0.877
0.961
2.217
3.456
0.673
0.656
3.079
5.199
0.647
MambaLRP (ours) 1.978
1.248
3.906
4.234
0.989
0.897
3.523
5.397
1.878
7

Runtime comparison
We report the runtimes of MambaLRP along with other methods used in this study in
Appendix C.8. As shown in Table 10, our method’s runtime is comparable to GI and can be implemented via a single
forward and backward pass. Since approaches like Integrated Gradients require multiple function evaluations, their
runtimes are considerably higher than MambaLRP.
Ablation study
In Section 4, we proposed techniques for handling different non-linear components within the
Mamba architecture. This ablation study aims to assess the significance of each technique by testing the effect of
their exclusion on faithfulness. Table 2 shows that all three modifications are essential for achieving competitive
explanation performance, with our proposed method for handling the SSM component being the most critical. Further
experiments, comparing different strategies for handling the Mamba block’s multiplicative gate, are detailed in
Appendix C.5.
Table 2: Analyzing the impact of ablating the three
proposed propagation rules on ∆AF for the compo-
nents in MambaLRP.
SiLU
SSM
Gate
SST-2
ImageNet
✓
✗
✓
0.577
0.453
✓
✓
✗
1.721
1.372
✗
✓
✓
1.943
1.794
✓
✓
✓
1.978
1.878
Table 3: Frequency of gendered words in explanations for
‘Nurse’ and ‘Surgeon’ classes of the Medical BIOS dataset
across language models.
Models
Surgeon
Nurse
GPT2-base
0.14
0.24
T5-base
0.10
0.11
RoBERTa-base
0.01
0.06
Mamba-130M
0.009
0.058
Mamba-1.4B
0.001
0.042
6
Use cases
Uncovering gender bias in Mamba.
Explanation methods serve as tools to uncover biases in pretrained vision and
language models. Using our proposed method, we examine Mamba-130M and Mamba-1.4B models, trained on the
Medical BIOS dataset, to investigate the potential presence of gender biases. Following the methodology in [24],
we use MambaLRP to identify the top-5 tokens of highest importance and to quantify the prevalence of gendered
words within these tokens. We find that the model exhibits a pronounced preference for female-gendered words
in the ‘Nurse’ class (e.g. the proportion of gender-specific words is 0.058 for females, compared to 0.0 for males
in Mamba-130M.). We also compare the results of our analysis with those achieved for the GPT2-base, T5-base,
and RoBERTa-base models as mentioned in [24]. As shown in Table 3, both Mamba models are less dependent on
gendered tokens compared to GPT2-base, T5-base, and RoBERTa-base models, with the Mamba-1.4B model showing
a further decrease in bias compared to the Mamba-130M, suggesting improvements in reducing gender bias with
increased model size.
Investigating long-range capabilities of Mamba.
The ability of SSMs to model long-range dependencies is
considered an important improvement over previous sequence models. In this use case, we analyze the extent to which
the pretrained Mamba-130M model can use information from the entire context window. We use the HotpotQA [69]
subset from the LongBench dataset [11], designed to test long context understanding. After selecting all 127 instances,
containing sequences up to 8192 tokens, we prompt the model to summarize the full paragraph by generating ten
additional tokens. Fig. 6 shows the distribution of the positional difference between a relevant token and the currently
generated token. While we observe a pronounced pattern of attributing to the last few tokens, as seen in prior language
generation studies [70, 52], the extracted explanations also identified relevant tokens across the entire context window,
as presented for one example in Fig. 6 (right). This suggests that the model is indeed capable of retrieving long-range
dependencies. We clearly see that in order to complete the sentence and assign a year to the album release date, the
model analyzes previous occurrences of chronological information and MambaLRP identifies evidence supporting the
decision for the date being ‘1972’ as relevant. This demonstrates the previously speculated long-range abilities of the
Mamba architecture [29].
Needle-in-a-haystack test.
To assess the model’s ability in retrieving relevant pieces of information from a broader
context, we perform the needle-in-a-haystack test [41]. Our test involves extracting a single passkey (the ‘needle’)
from a collection of repeated noise sentences (the ‘haystack’), as described in [33]. We run this test at eleven different
document depths with three different context lengths. We use an instruction-finetuned Mamba-2.8B model in this
experiment. To analyze the performance of the model, we introduce the explanation-based retrieval accuracy (XRA)
metric. In this approach, we first identify the positions of the top-K relevant tokens by MambaLRP, and then, calculate
the accuracy by comparing those positions to the needle’s position. As shown in Fig. 7, MambaLRP accurately
8

mid-range
long-range
Next generated token: [1972]                                                                                context: 5775 tokens
Figure 6: Analysis of the position of tokens relevant for next token generation. Left: Distribution of absolute position
of the ten most relevant tokens for the prediction of the next word. Right: Long-range dependency between tokens of
the input and the predicted next token (here: 1972).
Figure 7: Explanation-based retrieval accuracy in the needle-in-a-haystack test verifying model reliance on relevant
features for different context lengths.
captures the information used by the model to retrieve the needle. In this case, the model could accurately retrieve the
needle based on relevant information within the text. However, in more realistic and complex scenarios, the model
may depend on irrelevant data yet still generate the correct token. This issue can be analyzed using XRA but cannot
be evaluated by conventional retrieval accuracy metrics. Such cases and also further details about this experiment are
shown in Appendix C.7.
7
Discussion and conclusion
Mamba models have emerged as an efficient alternative to Transformers. However, there are limited works addressing
their interpretability [4]. To address this issue, we proposed MambaLRP within the LRP framework, specifically
tailored to the Mamba architecture and built upon the relevance conservation principle. Our evaluations across
various models and datasets confirmed that MambaLRP adheres to the conservation property and provides faithful
explanations that outperform other methods while being more computationally efficient. Moreover, we demonstrated
how MambaLRP can help to debug state-of-the-art ML models and build trust in them through various use cases.
Although our experiments focus on applying MambaLRP to vision and language models, its usefulness extends
beyond these domains. Future research can explore its potential across a broader range of applications and Mamba
architectures, providing reliable insights into sequence models.
9

Acknowledgments
This work was funded by the German Ministry for Education and Research (refs. 01IS14013A-E, 01GQ1115,
01GQ0850, 01IS18025A, 031L0207D, 01IS18037A). K.R.M. was partly supported by the Institute of Information
& Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT)
(No. 2019-0-00079, Artificial Intelligence Graduate School Program, Korea University and No. 2022-0-00984,
Development of Artificial Intelligence Technology for Personalized Plug-and-Play Explanation and Verification of
Explanation).
References
[1] S. Abnar and W. Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pages 4190–4197, Online, July 2020. Association for Computational Linguistics.
[2] R. Achtibat, S. M. V. Hatefi, M. Dreyer, A. Jain, T. Wiegand, S. Lapuschkin, and W. Samek. AttnLRP: Attention-aware
layer-wise relevance propagation for transformers. arXiv:2402.05602, 2024.
[3] A. Ali, T. Schnake, O. Eberle, G. Montavon, K.-R. Müller, and L. Wolf. XAI for transformers: Better explanations through
conservative propagation. In International Conference on Machine Learning, ICML 2022, volume 162 of Proceedings of
Machine Learning Research, pages 435–451. PMLR, 2022.
[4] A. Ali, I. Zimerman, and L. Wolf. The hidden attention of mamba models. arXiv:2403.01590, 2024.
[5] Q. Anthony, Y. Tokpanov, P. Glorioso, and B. Millidge.
BlackMamba: Mixture of experts for state-space models.
arXiv:2402.01771, 2024.
[6] L. Arras, J. Arjona-Medina, M. Widrich, G. Montavon, M. Gillhofer, K.-R. Müller, S. Hochreiter, and W. Samek. Explaining
and interpreting LSTMs. Explainable AI: Interpreting, explaining and visualizing deep learning, pages 211–238, 2019.
[7] A. B. Arrieta, N. D. Rodríguez, J. D. Ser, A. Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-Lopez, D. Molina, R. Benjamins,
R. Chatila, and F. Herrera. Explainable artificial intelligence (XAI): concepts, taxonomies, opportunities and challenges
toward responsible AI. Inf. Fusion, 58:82–115, 2020.
[8] Ž. Avsec, V. Agarwal, D. Visentin, J. R. Ledsam, A. Grabska-Barwinska, K. R. Taylor, Y. Assael, J. Jumper, P. Kohli, and
D. R. Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18
(10):1196–1203, 2021.
[9] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek. On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.
[10] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K.-R. Müller. How to explain individual classification
decisions. The Journal of Machine Learning Research, 11:1803–1831, 2010.
[11] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, Y. Dong, J. Tang, and J. Li. LongBench:
A bilingual, multitask benchmark for long context understanding. arXiv:2308.14508, 2023.
[12] E. Baron, I. Zimerman, and L. Wolf. 2-D SSM: A general spatial layer for visual transformers. arXiv:2306.06635, 2023.
[13] A. Behrouz and F. Hashemi. Graph Mamba: Towards learning on graphs with state space models. arXiv:2402.08678, 2024.
[14] S. Blücher, J. Vielhaben, and N. Strodthoff. Decoupling pixel flipping and occlusion strategy for consistent XAI benchmarks.
arXiv:2401.06654, 2024.
[15] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon,
Portugal, Sept. 2015. Association for Computational Linguistics.
[16] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder
transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 397–406, 2021.
[17] H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 782–791, 2021.
[18] P. Chormai, J. Herrmann, K.-R. Müller, and G. Montavon. Disentangled explanations of neural network predictions by
finding relevant subspaces. IEEE Trans. Pattern Anal. Mach. Intell., 2022.
[19] S. B. David, I. Zimerman, E. Nachmani, and L. Wolf. Decision S4: Efficient sequence-based rl via state spaces layers. In The
Eleventh International Conference on Learning Representations, 2022.
[20] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In
IEEE Computer Vision and Pattern Recognition (CVPR), pages 248–255, 2009.
[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language
understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics, pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
10

[22] A.-K. Dombrowski, C. J. Anders, K.-R. Müller, and P. Kessel. Towards robust explanations for deep neural networks. Pattern
Recognition, 121:108194, 2022. ISSN 0031-3203.
[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,
S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In
International Conference on Learning Representations, 2021.
[24] O. Eberle, I. Chalkidis, L. Cabello, and S. Brandl. Rather a nurse than a physician - contrastive explanations under
investigation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6907–6920. Association
for Computational Linguistics, 2023.
[25] R. C. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In 2017 IEEE International
Conference on Computer Vision (ICCV), pages 3449–3457, 2017.
[26] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Ré. Hungry hungry hippos: Towards language modeling with
state space models. arXiv:2212.14052, 2022.
[27] H. Gong, L. Kang, Y. Wang, X. Wan, and H. Li. nnMamba: 3D biomedical image segmentation, classification and landmark
detection with state space model. arXiv:2402.03526, 2024.
[28] Y. Gong, Y.-A. Chung, and J. Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pages 571–575,
2021.
[29] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752, 2023.
[30] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Ré. Combining recurrent, convolutional, and continuous-time
models with linear state space layers. Advances in Neural Information Processing Systems, 34:572–585, 2021.
[31] A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In International Conference on
Learning Representations, 2022.
[32] D. Gunning. DARPA’s explainable artificial intelligence (XAI) program. In Proceedings of the 24th International Conference
on Intelligent User Interfaces, IUI ’19, page ii. Association for Computing Machinery, 2019.
[33] C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, Y. Zhang, and B. Ginsburg. RULER: What’s the real context
size of your long-context language models? arXiv:2404.06654, 2024.
[34] S. Jain and B. C. Wallace. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics, pages 3543–3556, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics.
[35] S. Lapuschkin, S. Wäldchen, A. Binder, G. Montavon, W. Samek, and K.-R. Müller. Unmasking clever hans predictors and
assessing what machines really learn. Nature communications, 10(1):1096, 2019.
[36] J. Liu, H. Yang, H.-Y. Zhou, Y. Xi, L. Yu, Y. Yu, Y. Liang, G. Shi, S. Zhang, H. Zheng, et al. Swin-UMamba: Mamba-based
unet with imagenet-based pretraining. arXiv:2402.03302, 2024.
[37] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu. VMamba: Visual state space model. arXiv:2401.10166,
2024.
[38] C. Lu, Y. Schroecker, A. Gu, E. Parisotto, J. Foerster, S. Singh, and F. Behbahani. Structured state space models for in-context
reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.
[39] J. Ma, F. Li, and B. Wang.
U-mamba:
Enhancing long-range dependency for biomedical image segmentation.
arXiv:2401.04722, 2024.
[40] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces. arXiv:2206.13947,
2022.
[41] A. Mohtashami and M. Jaggi. Random-access infinite context length for transformers. In Advances in Neural Information
Processing Systems, 2023.
[42] G. Montavon, W. Samek, and K.-R. Müller. Methods for interpreting and understanding deep neural networks. Digital signal
processing, 73:1–15, 2018.
[43] G. Montavon, A. Binder, S. Lapuschkin, W. Samek, and K.-R. Müller. Layer-wise relevance propagation: An overview.
Explainable AI: interpreting, explaining and visualizing deep learning, pages 193–209, 2019.
[44] E. Nguyen, K. Goel, A. Gu, G. Downs, P. Shah, T. Dao, S. Baccus, and C. Ré. S4nd: Modeling images and videos as
multidimensional signals with state spaces. Advances in Neural Information Processing Systems, 35:2846–2861, 2022.
[45] M. Pióro, K. Ciebiera, K. Król, J. Ludziejewski, and S. Jaszczur. MoE-Mamba: Efficient selective state space models with
mixture of experts. arXiv:2401.04081, 2024.
[46] N. Rajani, L. Tunstall, E. Beeching, N. Lambert, A. M. Rush, and T. Wolf. No robots. https://huggingface.co/
datasets/HuggingFaceH4/no_robots, 2023.
[47] J. Ruan and S. Xiang. VM-UNet: Vision mamba UNet for medical image segmentation. arXiv:2402.02491, 2024.
[48] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K.-R. Müller. Evaluating the visualization of what a deep neural
network has learned. IEEE Transactions on Neural Networks and Learning Systems, 28(11):2660–2673, 2017.
11

[49] W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R. Müller. Explaining deep neural networks and beyond: A
review of methods and applications. Proc. IEEE, 109(3):247–278, 2021.
[50] G. Saon, A. Gupta, and X. Cui. Diagonal state space augmented transformers for speech recognition. In ICASSP 2023-2023
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE, 2023.
[51] E. Saravia, H. T. Liu, Y. Huang, J. Wu, and Y. Chen. CARER: contextualized affect representations for emotion recognition.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October
31 - November 4, 2018, pages 3687–3697. Association for Computational Linguistics, 2018.
[52] G. Sarti, N. Feldhus, L. Sickert, O. van der Wal, M. Nissim, and A. Bisazza. Inseq: An interpretability toolkit for sequence
generation models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3:
System Demonstrations), pages 421–435, Toronto, Canada, July 2023. Association for Computational Linguistics.
[53] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-CAM: Visual explanations from
deep networks via gradient-based localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pages
618–626, 2017.
[54] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating activation differences. In
Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, page 3145–3153, 2017.
[55] D. Smilkov, N. Thorat, B. Kim, F. B. Viégas, and M. Wattenberg.
SmoothGrad: removing noise by adding noise.
arXiv:1706.03825, 2017.
[56] J. T. Smith, A. Warrington, and S. Linderman. Simplified state space layers for sequence modeling. In The Eleventh
International Conference on Learning Representations, 2023.
[57] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic
compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 1631–1642. ACL, 2013.
[58] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of
Machine Learning Research, pages 3319–3328. PMLR, 2017.
[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you
need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[60] C. Wang, O. Tsepa, J. Ma, and B. Wang. Graph-Mamba: Towards long-range graph sequence modeling with selective state
spaces. arXiv:2402.00789, 2024.
[61] J. Wang, W. Zhu, P. Wang, X. Yu, L. Liu, M. Omar, and R. Hamid. Selective structured state-spaces for long-form video
understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6387–6397,
2023.
[62] J. Wang, T. Gangavarapu, J. N. Yan, and A. M. Rush. MambaByte: Token-free selective state space model. arXiv:2401.13660,
2024.
[63] Z. Wang and C. Ma. Semi-Mamba-UNet: Pixel-level contrastive cross-supervised visual mamba-based unet for semi-
supervised medical image segmentation. arXiv:2402.07245, 2024.
[64] Z. Wang, J.-Q. Zheng, Y. Zhang, G. Cui, and L. Li. Mamba-UNet: UNet-like pure visual mamba for medical image
segmentation. arXiv:2402.05079, 2024.
[65] S. Wiegreffe and Y. Pinter. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pages 11–20, Hong Kong, China, 2019. Association for Computational Linguistics.
[66] Z. Xing, T. Ye, Y. Yang, G. Liu, and L. Zhu. SegMamba: Long-range sequential modeling mamba for 3d medical image
segmentation. arXiv:2401.13560, 2024.
[67] J. N. Yan, J. Gu, and A. M. Rush. Diffusion models without attention. arXiv:2311.18257, 2023.
[68] Y. Yang, Z. Xing, and L. Zhu. Vivim: a video vision mamba for medical video object segmentation. arXiv:2401.14168, 2024.
[69] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. HotpotQA: A dataset for diverse,
explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP),
2018.
[70] K. Yin and G. Neubig. Interpreting language models with contrastive explanations. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing, pages 184–198, Abu Dhabi, United Arab Emirates, Dec. 2022.
Association for Computational Linguistics.
[71] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014: 13th
European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, pages 818–833. Springer, 2014.
[72] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang. Informer: Beyond efficient transformer for long
sequence time-series forecasting. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual
Conference, volume 35, pages 11106–11115. AAAI Press, 2021.
12

[73] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision Mamba: Efficient visual representation learning with
bidirectional state space model. arXiv:2401.09417, 2024.
[74] L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling. Visualizing deep neural network decisions: Prediction difference
analysis. In International Conference on Learning Representations, 2017.
13

A
Proofs
In the following, we provide derivations for the conservation analysis performed in Section 4.
A.1
Derivations for SiLU
We first consider the SiLU activation function. As mentioned in Section 4.1, this function is represented by the
equation y = x · σ(x), with σ being the logistic sigmoid function. By applying the standard gradient propagation
equations, we get the conservation equation:
R(x)
z}|{
∂f
∂xx = ∂f
∂y
∂y
∂xx
= ∂f
∂y · (σ(x) + xσ′(x)) · x
= ∂f
∂y · σ(x) · x + ∂f
∂y · xσ′(x) · x
= ∂f
∂y y
|{z}
R(y)
+ ∂f
∂y · σ′(x) · x2
|
{z
}
ε
(10)
A.2
Derivations for selective SSM
In Section 4.2, we introduced an inconsequential modification to the original Selective SSM architecture by connecting
the matrix Ct to the state ht instead of the input xt. The unfolded view of the SSM component with this modification
is represented in Figure 2. We can observe two subsets of nodes in this figure. The relevance scores of these two
subsets should be equal if the conservation property holds. Computing the relevance propagation equation between
these two groups, we obtain:
R(xt)+R(ht−1)
z
}|
{
∂f
∂xt
xt +
∂f
∂ht−1
ht−1 =
 ∂f
∂ht
∂+ht
∂xt
xt + ∂f
∂θt
∂θt
∂xt
xt

+
 ∂f
∂yt−1
∂+yt−1
∂ht−1
ht−1 + ∂f
∂ht
∂ht
∂ht−1
ht−1 + ∂f
∂θt
∂θt
∂ht−1
ht−1

=
 ∂f
∂ht
∂+ht
∂xt
xt + ∂f
∂ht
∂ht
∂ht−1
ht−1

+
 ∂f
∂yt−1
∂+yt−1
∂ht−1
ht−1

+
 ∂f
∂θt
∂θt
∂xt
xt + ∂f
∂θt
∂θt
∂ht−1
ht−1

= ∂f
∂ht
ht +
∂f
∂yt−1
yt−1
|
{z
}
R(ht)+R(yt−1)
+ ∂f
∂θt
∂θt
∂xt
xt + ∂f
∂θt
∂θt
∂ht−1
ht−1
|
{z
}
ε
(11)
A.3
Derivations for multiplicative gate
Mamba is composed of several blocks. In each block, the selective SSM’s output is multiplied by an input-dependent
gate. In other words, y = zAzB with zA = SSM(x) and zB = SiLU(Linear(x)). By applying the standard gradient
propagation equations, we get the conservation equation:
14

R(x)
z}|{
∂f
∂xx = ∂f
∂y
∂y
∂xx
= ∂f
∂y ·
∂zA
∂x zB + zA
∂zB
∂x

x
= ∂f
∂y ·
 zAzB + zAzB

= ∂f
∂y y
|{z}
R(y)
+ ∂f
∂y y
|{z}
ε
(12)
B
Explicit propagation rules for MambaLRP
Whereas MambaLRP is more easily implemented via the modified gradient-based approach described in the main
paper, we provide below explicit relevance propagation equations for better comparability with other works. We refer
to Sections 3 and 4 of the main paper for the definition of the notation.
B.1
SiLU
Explicit LRP rule for SiLU layers:
R(xi) = R(yi)
(13)
B.2
SSM
Using the shortcut notations aij = [At(xt)]ji, bij = [Bt(xt)]ji and cij = [Ct−1(ht−1)]ji, we can write the
propagation of relevance to the previous state space activations explicitly as:
R(h(t−1)
i
) =
X
j
h(t−1)
i
cij
P
i h(t−1)
i
cij
R(y(t−1)
j
) +
X
j
h(t−1)
i
aij
P
i h(t−1)
i
aij + P
i′ x(t)
i′ bi′j
R(h(t)
j )
(14)
and the propagation of relevance to the SSM input as:
R(x(t)
i ) =
X
j
x(t)
i bij
P
i x(t)
i bij + P
i′ h(t−1)
i′
ai′j
R(h(t)
j )
(15)
B.3
Multiplicative Gate
Explicit LRP rule for the multiplicative gate:
R([zA]i) = 0.5 · R(yi)
(16)
R([zB]i) = 0.5 · R(yi)
(17)
C
Experimental details
In this section, we provide experimental details on our experiments that allow reproducibility of our results.
C.1
Models and datasets
For the NLP experiments, we fine-tuned all parameters of the pretrained Mamba-130M and Mamba-1.4B models2 on
four text classification datasets: SST-2, SNLI, Medical BIOS, and Emotion. The data statistics can be seen in Table 5.
For the vision experiments, we used the pretrained Vim-S model3, trained on the ImageNet dataset.
2https://github.com/state-spaces/mamba
3https://github.com/hustvl/Vim
15

Training details
During training, we used a batch size of 32. To train the Mamba-1.4B model on the SNLI dataset,
a batch size of 64 is used. We employed the {EleutherAI/gpt-neox-20b}4 tokenizer. The models’ parameters were
optimized using AdamW optimizer with a learning rate set at 7e −5. Additionally, we used a linear learning rate
scheduler with an initial factor of 0.5. All models were trained for a maximum of 10 epochs, with an early stopping
mechanism in place. The top-1 accuracies of the models on each dataset are detailed in Table 4.
Table 4: The accuracies of Mamba-130M and Mamba-1.4B
models on the validation sets of four text classification
datasets.
Dataset
Mamba-130M
Mamba-1.4B
SST-2
91.97
94.15
Med-BIOS
89.10
90.30
Emotion
93.45
93.65
SNLI
89.57
91.05
Table 5: Data statistics.
Dataset
Train
Test
Validation
SST-2
68K
2K
1K
Med-BIOS
8K
1K
1K
Emotion
16K
2K
2K
SNLI
550K
10K
10K
ImageNet
1.3M
50K
100K
C.2
MambaLRP details
In this section, we begin by showing how MambaLRP can be implemented through the following algorithms. Then,
we explain the generalized LRP-γ rule, provide details regarding hyperparameters used in our implementation, and
outline the hyperparameter selection procedure.
Algorithm 1: MambaLRP in SiLU activation layers
Data: Input: x (B, L, D)
1 z ←Identity(x)
2 return z ⊙[SiLU(x) ⊘z].detach()
Algorithm 2: MambaLRP in Mamba block
Data: Input: x (B, L, D)
Data: Output: y (B, L, D)
1 x′: (B, L, E) ←SiLU(Conv1d(x))
2 g: (B, L, E) ←SiLU(Linear(x))
▷g is an input-dependent gate
3 A: (E, N) ←Parameter
4 B: (B, L, N) ←Linear(x′)
5 C: (B, L, N) ←Linear(x′)
▷C is input-dependent
6 ∆: (B, L, E) ←Softplus(Parameter + Linear(x′))
7 ¯A, ¯B: (B, L, E, N) ←discretize(∆, A, B)
▷¯A and ¯B are input-dependent
8 ySSM: (B, L, E) ←SSM( ¯A.detach(), ¯B.detach(), C.detach())(x′)
9 y′: (B, L, E) ←0.5(ySSM ⊙g) + 0.5[ySSM ⊙g].detach()
10 y: (B, L, D) ←Linear(y′)
11 return y
The following list represents the hyperparameters of the above-mentioned algorithms:
B
batch size
L
sequence length
D
hidden dimension
E
expanded hidden dimension
N
SSM dimension
Explanations generated by propagation-based methods rely on gradient computations, which can result in noisy
explanations in models with many layers. This is due to the phenomena of gradient shattering and the presence of
noisy gradients, which are more common in deep complex models [22, 2]. To mitigate this, we apply the generalized
4https://github.com/EleutherAI/gpt-neox
16

LRP-γ-rule to the convolution layers of the Vision Mamba model to improve the signal to noise ratio, thereby
enhancing explanations. The generalized LRP-γ rule is defined in Eq. 18:
R(xi) =









P
j
x+
i (wij+γw+
ij)+x−
i (wij+γw−
ij)
P
i x+
i (wij+γw+
ij)+x−
i (wij+γw−
ij)R(yj)
if
zj > 0
P
j
x+
i (wij+γw−
ij)+x−
i (wij+γw+
ij)
P
i x+
i (wij+γw−
ij)+x−
i (wij+γw+
ij)R(yj)
else
(18)
where (.)+ = max(0, .) and (.)−= min(0, .), and zj = P
i xiwij. In our experiments, the parameter γ is set to 0.25.
Our observations reveal that applying this rule to the language models does not lead to any discernible improvements.
Therefore, we use the LRP-0 rule in these models.
C.2.1
LRP composites for Vision Mamba
As mentioned in Section C.2, we apply the generalized LRP-γ rule to the convolution layers of the Vim-S model
to produce more faithful explanations. In this experiment, we justify this choice. Vision Mamba is composed of a
number of blocks and in each block, there are several linear and convolution layers, where the generalized LRP-γ rule
can be used. As can be seen in Table 6, the LRP-0 rule is sufficient to produce meaningful explanations. However, we
can perform a hyperparameter search by applying the LRP-γ rule across different layers of the model to find the most
accurate LRP composite.
Table 6: Finding the best LRP composite for Vision Mamba. The layers in which the generalized LRP-γ rule is
applied are represented with LRP-γ and the ones in which the basic LRP rule, i.e. LRP-0, is used are represented with
LRP-0.
in-proj
out-proj
conv1d
ImageNet
(∆AF ↑)
LRP-γ
LRP-γ
LRP-γ
1.7173
LRP-γ
LRP-γ
LRP-0
1.7315
LRP-0
LRP-γ
LRP-γ
1.7736
LRP-0
LRP-γ
LRP-0
1.7824
LRP-γ
LRP-0
LRP-0
1.8588
LRP-γ
LRP-0
LRP-γ
1.8646
LRP-0
LRP-0
LRP-0
1.8852
LRP-0
LRP-0
LRP-γ
1.9106
We apply the LRP-γ rule across different combinations of the input projection (in-proj), output projection (out-proj),
and convolution layers of each block. Subsequently, we perform the perturbation experiment to analyze the faithfulness
of each combination. We can observe that the best result can be achieved when the LRP-γ rule is only used in
convolution layers. In all of these combinations, the value of γ is set to 0.25.
C.3
Further details of other explanation methods
Some of the explanation methods that we used in this study have a set of hyperparameters. Table 7 provides further
details on the specific values assigned to these hyperparameters, chosen based on the values suggested in the original
papers [58, 55].
Table 7: Hyperparameters of other explanation methods. The parameters µ and σ represent the mean and standard
deviation of noise, respectively, while the parameter m denotes the sample size.
Method
Hyperparameters
SmoothGrad
µ = 0, σ = 0.15, m = 30
Integrated Gradients
m = 30
In the vision experiments, we used the original implementations 5 of the AttnRoll and G×AttnRoll methods, provided
to explain the Vim-S model. Given the unavailability of code for adapting these approaches to the language models,
5https://github.com/AmeenAli/HiddenMambaAttn/
17

namely Mamba-130M and Mamba-1.4B, we have developed our own implementation. In the vision case, the authors
obtain the final relevance map by extracting the row associated with the CLS token in the attention matrix. However,
since our language models lack a CLS token, we get the final relevance map from the row associated with the last token
in the attention matrix. This is because predictions are based on the last state in these models. For the gradient-based
methods, we use the implementations available in the Captum library6.
C.4
Additional quantitative results
The results of the insertion experiment are shown in Table 8. Similar to the flipping results, MambaLRP outperforms
all baseline methods for all the datasets and models. In the majority of the models and datasets, the G×AttnRoll
method has shown better performance compared to the pure gradient-based approaches.
Table 8: Faithfulness score ∆AI ↑. A higher score indicates more faithful explanations.
Methods
SST-2
Med-BIOS
SNLI
Emotion
ImageNet
Mamba
130M
Mamba
1.4B
Mamba
130M
Mamba
1.4B
Mamba
130M
Mamba
1.4B
Mamba
130M
Mamba
1.4B
Vim-S
Random
-0.024
0.009
0.022
0.004
-0.003
0.001
0.019
0.021
0.000
GI [54]
0.074
-0.108
0.200
-0.649
-0.043
-0.040
-0.839
-0.533
0.041
SmoothGrad [58]
1.399
-0.405
1.655
-2.295
0.497
-0.709
1.904
-1.982
0.121
IG [55]
0.880
0.223
1.312
1.065
0.465
0.224
1.909
2.181
0.328
AttnRoll [4]
0.704
0.554
2.265
1.105
0.257
0.375
0.450
1.656
0.714
G × AttnRoll [4]
1.238
0.751
3.175
3.032
0.534
0.564
2.091
4.963
0.794
LRP (LN-rule, [3]) 0.916
1.083
2.235
3.471
0.718
0.664
3.208
5.443
0.648
MambaLRP (ours) 2.038
1.379
3.933
4.250
1.024
0.917
3.640
5.650
1.878
C.5
Further ablation experiments
Comparing strategies for managing the Mamba block’s multiplicative gate:
In Section 4.3, we proposed
several strategies to mitigate conservation violation in the the Mamba block’s multiplicative gate. In this experiment,
we evaluate the proposed approaches. As can be seen in Table 9, detaching the multiplicative gate leads to lower
faithfulness scores compared to the half-relevance propagation approach. To retain conservation, an alternative
approach is to detach the SSM’s output, which limits capturing long-range dependencies, a task for which this branch
is designed for. Detaching it may result in a loss of valuable information used by the model to make predictions.
C.6
Additional qualitative results
In Section 5.2, we qualitatively evaluated the explanations produced by MambaLRP and other baseline methods. In
the following, we demonstrate further qualitative results.
C.6.1
Natural language processing
In the following figures, we represent explanations produced by MambaLRP and other baseline methods to interpret
the Mamba-130M models trained on various datasets. In the visualizations, shades of red represent words that
positively influence the model’s prediction. Conversely, shades of blue reflect negative contributions. The heatmaps
of the AttnRoll and G×AttnRolll methods are constrained to non-negative values.
C.6.2
Computer vision
In this section, we show explanations generated by MambaLRP alongside other baseline methods to interpret the
predictions of the Vim-S model on several images of the ImageNet dataset. As can be seen, explanations generated
by purely gradient-based explanation methods are very noisy. In contrast, attention-based attribution methods have
offered more focused and less noisy heatmaps. However, in the last two images labeled ‘paint brush’ and ‘flag pole’,
they could not faithfully explain the model’s predictions. Among these approaches, MambaLRP stands out with its
6https://captum.ai/
18

Table 9: Comparing the proposed strategies for managing the Mamba block’s multiplicative gate.
Strategies
SST-2
ImageNet
Detaching multiplicative gate
1.577
1.387
Half-relevance propagation
1.978
1.878
Figure 8: Explanations generated by different explanation methods for a sentence of the SNLI validation set. This
sentence belongs to the ‘entailment’ class.
Figure 9: Explanations generated by different explanation methods for a sentence of the SNLI validation set. This
sentence belongs to the ‘contradiction’ class.
Figure 10: Explanations generated by different explanation methods for a sentence of the Emotion validation set. This
sentence belongs to the ‘joy’ class.
Figure 11: Explanations generated by different explanation methods for a sentence of the Medical BIOS validation
set. This sentence belongs to the ‘nurse’ class.
19

Figure 12: Explanations produced by different explanation methods for images of the ImageNet dataset. Explanations
produced by AttnRoll and G×AttnRoll are limited to non-negative values, whereas those generated by gradient-based
techniques and MambaLRP includes both positive and negative contributions.
ability to generate sparse explanations, offering more faithful explanations of how different image patches contribute
to the final predictions.
C.7
Additional use case results
For the needle-in-a-haystack experiment in Section 6, we use a synthetic dataset 7. In this dataset, a single passkey
(the ‘needle’) is inserted at different locations within a collection of repeated noise sentences (the ‘haystack’), as
described in [33]. The dataset is composed of sequences with different context lengths. In our experiment, we use
sequences with context lengths of 512, 1024, and 2048. We use a Mamba-2.8B model 8, which is finetuned on the No
Robots dataset [46] using a context length of 2048. Then, we prompt the model to extract the passkey hidden among
irrelevant text by completing the phrase "The passkey is
".
Retrieval accuracy is a metric, which is commonly used in the needle-in-a-haystack experiment to analyze the model’s
performance. The synthetic dataset used for this experiment can be designed to include misleading information, which
may cause the model to generate the correct passkey based on incorrect evidence. In such cases, simply evaluating the
retrieval accuracy may be insufficient. This issue can also arise when dealing with more realistic haystacks. Therefore,
we introduced explanation-based retrieval accuracy (XRA) in Section 7. MambaLRP and the XRA metric designed
upon it can help to better examine the evidence the model relies on to retrieve the needle. In our experiment, we set
the value of K to 2. This is because LRP usually identifies the token immediately preceding the generated token as the
most important one and the evidence used for the passkey retrieval is usually the second most important token.
7https://huggingface.co/datasets/lvwerra/needle-llama3-16x512
8https://huggingface.co/clibrain/mamba-2.8b-chat-no_robots
20

The sample in Fig. 13 represents such scenario. In this case, the next token generated by the model is the second
part of the correct passkey (300). However, the model has incorrectly focused on the number 300 in the phrase "Pass
the key to room 6300" to generate this token. Simply looking at the retrieved token might suggest that the model
successfully retrieved the correct information. However, examining the MambaLRP’s explanation heatmaps provides
deeper insights into the model’s behavior. This helps us to debug the model more effectively and design better tests to
analyze its capabilities.
Figure 13: Detecting Clever-Hans effect in the needle-in-a-haystack test. Given the 2K context length in this example,
visualizing the entire text could be confusing. Therefore, we have removed most of the haystack from the visualization.
In this example, the model has generated the correct passkey but the generation is not based on truly relevant
information in the text.
C.8
Runtime comparison
In this section, we report the time required for each explanation method to generate its respective explanation. These
times, measured in seconds, are averaged over samples from the Medical BIOS dataset. All baseline methods are
evaluated on a single A100-40GB GPU with a batch size of 1. All methods are applied to the Mamba-130M model.
The results without fast CUDA kernels are shown in Table 10, while the results with fast CUDA kernels are presented
in Table 11. We can observe that the runtime of MambaLRP is comparable to Gradient×Input. Since algorithms like
Integrated Gradients and SmoothGrad require multiple function evaluations, their runtimes are significantly higher
than MambaLRP and Gradient×Input.
Table 10: Runtime comparison. The time needed
for each baseline method to generate its explana-
tions. The times, measured in seconds, are averaged
over the samples from the Medical BIOS dataset.
The model used in this experiment is Mamba-130M
without using fast CUDA kernels.
Methods
Runtime
Gradient × Input
0.7556
SmoothGrad
22.9772
Integrated Gradients
22.8071
AttnRoll
2.1558
G × AttnRoll
2.6661
MambaLRP
0.4345
Table 11: Runtime comparison. The time needed
for each baseline method to generate its explanations.
The times, measured in seconds, are averaged over
the samples from the Medical BIOS dataset. The
model used in this experiment is Mamba-130M using
fast CUDA kernels.
Methods
Runtime
Gradient × Input
0.0335
SmoothGrad
0.9785
Integrated Gradients
0.9742
AttnRoll
-
G × AttnRoll
-
MambaLRP
0.3063
21

