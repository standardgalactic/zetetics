Speech and Language Processing
An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition
Third Edition draft
Daniel Jurafsky
Stanford University
James H. Martin
University of Colorado at Boulder
Copyright c⃝2019. All rights reserved.
Draft of October 16, 2019. Comments and typos welcome!

Summary of Contents
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
2
Regular Expressions, Text Normalization, Edit Distance. . . . . . . . .
2
3
N-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
4
Naive Bayes and Sentiment Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . 56
5
Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6
Vector Semantics and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
7
Neural Networks and Neural Language Models . . . . . . . . . . . . . . . . . 123
8
Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
9
Sequence Processing with Recurrent Networks. . . . . . . . . . . . . . . . . . 169
10 Encoder-Decoder Models, Attention and Contextual Embeddings191
11 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
12 Constituency Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
13 Constituency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
14 Statistical Constituency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
15 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
16 Logical Representations of Sentence Meaning. . . . . . . . . . . . . . . . . . . 298
17 Computational Semantics and Semantic Parsing . . . . . . . . . . . . . . . . 324
18 Information Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
19 Word Senses and WordNet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
20 Semantic Role Labeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
21 Lexicons for Sentiment, Affect, and Connotation . . . . . . . . . . . . . . . . 394
22 Coreference Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
23 Discourse Coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .443
24 Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
25 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
26 Dialogue Systems and Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
27 Phonetics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .518
28 Speech Recognition and Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
Appendix
547
A Hidden Markov Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
2

Contents
1
Introduction
1
2
Regular Expressions, Text Normalization, Edit Distance
2
2.1
Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.2
Words
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3
Corpora
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.4
Text Normalization
. . . . . . . . . . . . . . . . . . . . . . . . .
14
2.5
Minimum Edit Distance . . . . . . . . . . . . . . . . . . . . . . .
23
2.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
27
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3
N-gram Language Models
30
3.1
N-Grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.2
Evaluating Language Models . . . . . . . . . . . . . . . . . . . .
36
3.3
Generalization and Zeros
. . . . . . . . . . . . . . . . . . . . . .
38
3.4
Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.5
Kneser-Ney Smoothing . . . . . . . . . . . . . . . . . . . . . . .
46
3.6
The Web and Stupid Backoff
. . . . . . . . . . . . . . . . . . . .
48
3.7
Advanced: Perplexity’s Relation to Entropy
. . . . . . . . . . . .
49
3.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
53
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4
Naive Bayes and Sentiment Classiﬁcation
56
4.1
Naive Bayes Classiﬁers . . . . . . . . . . . . . . . . . . . . . . .
58
4.2
Training the Naive Bayes Classiﬁer . . . . . . . . . . . . . . . . .
60
4.3
Worked example . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.4
Optimizing for Sentiment Analysis . . . . . . . . . . . . . . . . .
62
4.5
Naive Bayes for other text classiﬁcation tasks
. . . . . . . . . . .
64
4.6
Naive Bayes as a Language Model
. . . . . . . . . . . . . . . . .
65
4.7
Evaluation: Precision, Recall, F-measure . . . . . . . . . . . . . .
66
4.8
Test sets and Cross-validation . . . . . . . . . . . . . . . . . . . .
69
4.9
Statistical Signiﬁcance Testing
. . . . . . . . . . . . . . . . . . .
69
4.10
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
72
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
5
Logistic Regression
75
5.1
Classiﬁcation: the sigmoid
. . . . . . . . . . . . . . . . . . . . .
76
5.2
Learning in Logistic Regression . . . . . . . . . . . . . . . . . . .
80
5.3
The cross-entropy loss function . . . . . . . . . . . . . . . . . . .
81
5.4
Gradient Descent
. . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.5
Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
5.6
Multinomial logistic regression . . . . . . . . . . . . . . . . . . .
89
5.7
Interpreting models
. . . . . . . . . . . . . . . . . . . . . . . . .
91
5.8
Advanced: Deriving the Gradient Equation . . . . . . . . . . . . .
91
5.9
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
93
3

4
CONTENTS
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
6
Vector Semantics and Embeddings
94
6.1
Lexical Semantics . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.2
Vector Semantics
. . . . . . . . . . . . . . . . . . . . . . . . . .
98
6.3
Words and Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.4
Cosine for measuring similarity . . . . . . . . . . . . . . . . . . . 103
6.5
TF-IDF: Weighing terms in the vector
. . . . . . . . . . . . . . . 105
6.6
Applications of the tf-idf vector model . . . . . . . . . . . . . . . 107
6.7
Optional: Pointwise Mutual Information (PMI) . . . . . . . . . . . 108
6.8
Word2vec
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.9
Visualizing Embeddings . . . . . . . . . . . . . . . . . . . . . . . 115
6.10
Semantic properties of embeddings . . . . . . . . . . . . . . . . . 115
6.11
Bias and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . 117
6.12
Evaluating Vector Models . . . . . . . . . . . . . . . . . . . . . . 118
6.13
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 120
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7
Neural Networks and Neural Language Models
123
7.1
Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.2
The XOR problem . . . . . . . . . . . . . . . . . . . . . . . . . . 126
7.3
Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . 129
7.4
Training Neural Nets
. . . . . . . . . . . . . . . . . . . . . . . . 132
7.5
Neural Language Models . . . . . . . . . . . . . . . . . . . . . . 137
7.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 141
8
Part-of-Speech Tagging
143
8.1
(Mostly) English Word Classes . . . . . . . . . . . . . . . . . . . 143
8.2
The Penn Treebank Part-of-Speech Tagset
. . . . . . . . . . . . . 146
8.3
Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . 148
8.4
HMM Part-of-Speech Tagging
. . . . . . . . . . . . . . . . . . . 149
8.5
Maximum Entropy Markov Models . . . . . . . . . . . . . . . . . 159
8.6
Bidirectionality
. . . . . . . . . . . . . . . . . . . . . . . . . . . 163
8.7
Part-of-Speech Tagging for Morphological Rich Languages . . . . 164
8.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 166
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
9
Sequence Processing with Recurrent Networks
169
9.1
Simple Recurrent Neural Networks . . . . . . . . . . . . . . . . . 170
9.2
Applications of Recurrent Neural Networks
. . . . . . . . . . . . 176
9.3
Deep Networks: Stacked and Bidirectional RNNs
. . . . . . . . . 181
9.4
Managing Context in RNNs: LSTMs and GRUs . . . . . . . . . . 183
9.5
Words, Subwords and Characters . . . . . . . . . . . . . . . . . . 187
9.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 190
10 Encoder-Decoder Models, Attention and Contextual Embeddings
191
10.1
Neural Language Models and Generation Revisited
. . . . . . . . 191
10.2
Encoder-Decoder Networks . . . . . . . . . . . . . . . . . . . . . 194

CONTENTS
5
10.3
Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
10.4
Applications of Encoder-Decoder Networks
. . . . . . . . . . . . 200
10.5
Self-Attention and Transformer Networks
. . . . . . . . . . . . . 200
10.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 201
11 Machine Translation
202
12 Constituency Grammars
203
12.1
Constituency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
12.2
Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . 204
12.3
Some Grammar Rules for English . . . . . . . . . . . . . . . . . . 209
12.4
Treebanks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
12.5
Grammar Equivalence and Normal Form . . . . . . . . . . . . . . 222
12.6
Lexicalized Grammars . . . . . . . . . . . . . . . . . . . . . . . . 223
12.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 229
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
13 Constituency Parsing
232
13.1
Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
13.2
CKY Parsing: A Dynamic Programming Approach
. . . . . . . . 234
13.3
Partial Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
13.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 243
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
14 Statistical Constituency Parsing
246
14.1
Probabilistic Context-Free Grammars . . . . . . . . . . . . . . . . 246
14.2
Probabilistic CKY Parsing of PCFGs . . . . . . . . . . . . . . . . 251
14.3
Ways to Learn PCFG Rule Probabilities
. . . . . . . . . . . . . . 253
14.4
Problems with PCFGs . . . . . . . . . . . . . . . . . . . . . . . . 253
14.5
Improving PCFGs by Splitting Non-Terminals . . . . . . . . . . . 256
14.6
Probabilistic Lexicalized CFGs . . . . . . . . . . . . . . . . . . . 258
14.7
Probabilistic CCG Parsing . . . . . . . . . . . . . . . . . . . . . . 262
14.8
Evaluating Parsers . . . . . . . . . . . . . . . . . . . . . . . . . . 268
14.9
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 271
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
15 Dependency Parsing
273
15.1
Dependency Relations . . . . . . . . . . . . . . . . . . . . . . . . 274
15.2
Dependency Formalisms . . . . . . . . . . . . . . . . . . . . . . . 276
15.3
Dependency Treebanks
. . . . . . . . . . . . . . . . . . . . . . . 277
15.4
Transition-Based Dependency Parsing
. . . . . . . . . . . . . . . 278
15.5
Graph-Based Dependency Parsing
. . . . . . . . . . . . . . . . . 289
15.6
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
15.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 295
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
16 Logical Representations of Sentence Meaning
298

6
CONTENTS
16.1
Computational Desiderata for Representations . . . . . . . . . . . 299
16.2
Model-Theoretic Semantics . . . . . . . . . . . . . . . . . . . . . 301
16.3
First-Order Logic
. . . . . . . . . . . . . . . . . . . . . . . . . . 304
16.4
Event and State Representations . . . . . . . . . . . . . . . . . . . 311
16.5
Description Logics . . . . . . . . . . . . . . . . . . . . . . . . . . 316
16.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 322
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
17 Computational Semantics and Semantic Parsing
324
18 Information Extraction
325
18.1
Named Entity Recognition
. . . . . . . . . . . . . . . . . . . . . 326
18.2
Relation Extraction
. . . . . . . . . . . . . . . . . . . . . . . . . 332
18.3
Extracting Times . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
18.4
Extracting Events and their Times . . . . . . . . . . . . . . . . . . 346
18.5
Template Filling . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
18.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 352
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
19 Word Senses and WordNet
354
19.1
Word Senses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
19.2
Relations Between Senses . . . . . . . . . . . . . . . . . . . . . . 357
19.3
WordNet: A Database of Lexical Relations . . . . . . . . . . . . . 359
19.4
Word Sense Disambiguation . . . . . . . . . . . . . . . . . . . . . 362
19.5
Alternate WSD algorithms and Tasks . . . . . . . . . . . . . . . . 365
19.6
Using Thesauruses to Improve Embeddings
. . . . . . . . . . . . 368
19.7
Word Sense Induction . . . . . . . . . . . . . . . . . . . . . . . . 368
19.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 370
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
20 Semantic Role Labeling
373
20.1
Semantic Roles
. . . . . . . . . . . . . . . . . . . . . . . . . . . 374
20.2
Diathesis Alternations . . . . . . . . . . . . . . . . . . . . . . . . 375
20.3
Semantic Roles: Problems with Thematic Roles . . . . . . . . . . 376
20.4
The Proposition Bank . . . . . . . . . . . . . . . . . . . . . . . . 377
20.5
FrameNet
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
20.6
Semantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . 380
20.7
Selectional Restrictions . . . . . . . . . . . . . . . . . . . . . . . 384
20.8
Primitive Decomposition of Predicates . . . . . . . . . . . . . . . 389
20.9
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 391
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
21 Lexicons for Sentiment, Affect, and Connotation
394
21.1
Deﬁning Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . 395
21.2
Available Sentiment and Affect Lexicons . . . . . . . . . . . . . . 397
21.3
Creating Affect Lexicons by Human Labeling
. . . . . . . . . . . 398
21.4
Semi-supervised Induction of Affect Lexicons . . . . . . . . . . . 400
21.5
Supervised Learning of Word Sentiment
. . . . . . . . . . . . . . 403

CONTENTS
7
21.6
Using Lexicons for Sentiment Recognition . . . . . . . . . . . . . 408
21.7
Other tasks: Personality . . . . . . . . . . . . . . . . . . . . . . . 409
21.8
Affect Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 410
21.9
Lexicon-based methods for Entity-Centric Affect . . . . . . . . . . 411
21.10 Connotation Frames . . . . . . . . . . . . . . . . . . . . . . . . . 412
21.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 414
22 Coreference Resolution
416
22.1
Coreference Phenomena: Linguistic Background . . . . . . . . . . 419
22.2
Coreference Tasks and Datasets . . . . . . . . . . . . . . . . . . . 424
22.3
Mention Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 425
22.4
Architectures for Coreference Algorithms
. . . . . . . . . . . . . 428
22.5
Classiﬁers using hand-built features . . . . . . . . . . . . . . . . . 430
22.6
A neural mention-ranking algorithm
. . . . . . . . . . . . . . . . 431
22.7
Evaluation of Coreference Resolution . . . . . . . . . . . . . . . . 435
22.8
Entity Linking . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
22.9
Winograd Schema problems . . . . . . . . . . . . . . . . . . . . . 437
22.10 Gender Bias in Coreference . . . . . . . . . . . . . . . . . . . . . 438
22.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 440
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
23 Discourse Coherence
443
23.1
Coherence Relations . . . . . . . . . . . . . . . . . . . . . . . . . 445
23.2
Discourse Structure Parsing . . . . . . . . . . . . . . . . . . . . . 448
23.3
Centering and Entity-Based Coherence . . . . . . . . . . . . . . . 452
23.4
Representation learning models for local coherence
. . . . . . . . 456
23.5
Global Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . 458
23.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 462
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
24 Summarization
465
25 Question Answering
466
25.1
IR-based Factoid Question Answering
. . . . . . . . . . . . . . . 467
25.2
Knowledge-based Question Answering . . . . . . . . . . . . . . . 476
25.3
Using multiple information sources: IBM’s Watson
. . . . . . . . 479
25.4
Evaluation of Factoid Answers
. . . . . . . . . . . . . . . . . . . 483
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 484
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
26 Dialogue Systems and Chatbots
487
26.1
Properties of Human Conversation
. . . . . . . . . . . . . . . . . 488
26.2
Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
26.3
GUS: Simple Frame-based Dialogue Systems
. . . . . . . . . . . 498
26.4
The Dialogue-State Architecture
. . . . . . . . . . . . . . . . . . 502
26.5
Evaluating Dialogue Systems . . . . . . . . . . . . . . . . . . . . 511
26.6
Dialogue System Design . . . . . . . . . . . . . . . . . . . . . . . 512
26.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 515

8
CONTENTS
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
27 Phonetics
518
27.1
Speech Sounds and Phonetic Transcription . . . . . . . . . . . . . 518
27.2
Articulatory Phonetics . . . . . . . . . . . . . . . . . . . . . . . . 520
27.3
Prosodic Prominence: Accent, Stress and Schwa . . . . . . . . . . 525
27.4
Prosodic Structure and Tune . . . . . . . . . . . . . . . . . . . . . 526
27.5
Acoustic Phonetics and Signals . . . . . . . . . . . . . . . . . . . 529
27.6
Phonetic Resources
. . . . . . . . . . . . . . . . . . . . . . . . . 539
27.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 543
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
28 Speech Recognition and Synthesis
545
Appendix
547
A Hidden Markov Models
548
A.1
Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
A.2
The Hidden Markov Model . . . . . . . . . . . . . . . . . . . . . 549
A.3
Likelihood Computation: The Forward Algorithm . . . . . . . . . 551
A.4
Decoding: The Viterbi Algorithm . . . . . . . . . . . . . . . . . . 555
A.5
HMM Training: The Forward-Backward Algorithm . . . . . . . . 557
A.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 563
Bibliography
565
Author Index
595
Subject Index
605

CHAPTER
1
Introduction
1

2
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
CHAPTER
2
Regular
Expressions,
Text
Normalization, Edit Distance
User:
I am unhappy.
ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY
User:
I need some help, that much seems certain.
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP
User:
Perhaps I could learn to get along with my mother.
ELIZA: TELL ME MORE ABOUT YOUR FAMILY
User:
My mother takes care of me.
ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU
User:
My father.
ELIZA: YOUR FATHER
User:
You are like my father in some ways.
Weizenbaum (1966)
The dialogue above is from ELIZA, an early natural language processing system
ELIZA
that could carry on a limited conversation with a user by imitating the responses of
a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple
program that uses pattern matching to recognize phrases like “I need X” and translate
them into suitable outputs like “What would it mean to you if you got X?”. This
simple technique succeeds in this domain because ELIZA doesn’t actually need to
know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is
one of the few dialogue genres where listeners can act as if they know nothing of the
world. Eliza’s mimicry of human conversation was remarkably successful: many
people who interacted with ELIZA came to believe that it really understood them
and their problems, many continued to believe in ELIZA’s abilities even after the
program’s operation was explained to them (Weizenbaum, 1976), and even today
such chatbots are a fun diversion.
chatbots
Of course modern conversational agents are much more than a diversion; they
can answer questions, book ﬂights, or ﬁnd restaurants, functions for which they rely
on a much more sophisticated understanding of the user’s intent, as we will see in
Chapter 26. Nonetheless, the simple pattern-based methods that powered ELIZA
and other chatbots play a crucial role in natural language processing.
We’ll begin with the most important tool for describing text patterns: the regular
expression. Regular expressions can be used to specify strings we might want to
extract from a document, from transforming “I need X” in Eliza above, to deﬁning
strings like $199 or $24.99 for extracting tables of prices from a document.
We’ll then turn to a set of tasks collectively called text normalization, in which
text
normalization
regular expressions play an important part. Normalizing text means converting it
to a more convenient, standard form. For example, most of what we are going to
do with language relies on ﬁrst separating out or tokenizing words from running
text, the task of tokenization. English words are often separated from each other
tokenization
by whitespace, but whitespace is not always sufﬁcient. New York and rock ’n’ roll
are sometimes treated as large words despite the fact that they contain spaces, while
sometimes we’ll need to separate I’m into the two words I and am. For processing
tweets or texts we’ll need to tokenize emoticons like :) or hashtags like #nlproc.

2.1
•
REGULAR EXPRESSIONS
3
Some languages, like Japanese, don’t have spaces between words, so word tokeniza-
tion becomes more difﬁcult.
Another part of text normalization is lemmatization, the task of determining
lemmatization
that two words have the same root, despite their surface differences. For example,
the words sang, sung, and sings are forms of the verb sing. The word sing is the
common lemma of these words, and a lemmatizer maps from all of these to sing.
Lemmatization is essential for processing morphologically complex languages like
Arabic. Stemming refers to a simpler version of lemmatization in which we mainly
stemming
just strip sufﬁxes from the end of the word. Text normalization also includes sen-
tence segmentation: breaking up a text into individual sentences, using cues like
sentence
segmentation
periods or exclamation points.
Finally, we’ll need to compare words and other strings. We’ll introduce a metric
called edit distance that measures how similar two strings are based on the number
of edits (insertions, deletions, substitutions) it takes to change one string into the
other. Edit distance is an algorithm with applications throughout language process-
ing, from spelling correction to speech recognition to coreference resolution.
2.1
Regular Expressions
One of the unsung successes in standardization in computer science has been the
regular expression (RE), a language for specifying text search strings. This prac-
regular
expression
tical language is used in every computer language, word processor, and text pro-
cessing tools like the Unix tools grep or Emacs. Formally, a regular expression is
an algebraic notation for characterizing a set of strings. They are particularly use-
ful for searching in texts, when we have a pattern to search for and a corpus of
corpus
texts to search through. A regular expression search function will search through the
corpus, returning all texts that match the pattern. The corpus can be a single docu-
ment or a collection. For example, the Unix command-line tool grep takes a regular
expression and returns every line of the input document that matches the expression.
A search can be designed to return every match on a line, if there are more than
one, or just the ﬁrst match. In the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the ﬁrst
match. We’ll show regular expressions delimited by slashes but note that slashes are
not part of the regular expressions.
Regular expressions come in many variants. We’ll be describing extended regu-
lar expressions; different regular expression parsers may only recognize subsets of
these, or treat some expressions slightly differently. Using an online regular expres-
sion tester is a handy way to test out your expressions and explore these variations.
2.1.1
Basic Regular Expression Patterns
The simplest kind of regular expression is a sequence of simple characters. To search
for woodchuck, we type /woodchuck/. The expression /Buttercup/ matches any
string containing the substring Buttercup; grep with that expression would return the
line I’m called little Buttercup. The search string can consist of a single character
(like /!/) or a sequence of characters (like /urgl/).
Regular expressions are case sensitive; lower case /s/ is distinct from upper
case /S/ (/s/ matches a lower case s but not an upper case S). This means that
the pattern /woodchucks/ will not match the string Woodchucks. We can solve this

4
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
RE
Example Patterns Matched
/woodchucks/
“interesting links to woodchucks and lemurs”
/a/
“Mary Ann stopped by Mona’s”
/!/
“You’ve left the burglar behind again!” said Nori
Figure 2.1
Some simple regex searches.
problem with the use of the square braces [ and ]. The string of characters inside the
braces speciﬁes a disjunction of characters to match. For example, Fig. 2.2 shows
that the pattern /[wW]/ matches patterns containing either w or W.
RE
Match
Example Patterns
/[wW]oodchuck/
Woodchuck or woodchuck
“Woodchuck”
/[abc]/
‘a’, ‘b’, or ‘c’
“In uomini, in soldati”
/[1234567890]/
any digit
“plenty of 7 to 5”
Figure 2.2
The use of the brackets [] to specify a disjunction of characters.
The regular expression /[1234567890]/ speciﬁed any single digit. While such
classes of characters as digits or letters are important building blocks in expressions,
they can get awkward (e.g., it’s inconvenient to specify
/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/
to mean “any capital letter”). In cases where there is a well-deﬁned sequence asso-
ciated with a set of characters, the brackets can be used with the dash (-) to specify
any one character in a range. The pattern /[2-5]/ speciﬁes any one of the charac-
range
ters 2, 3, 4, or 5. The pattern /[b-g]/ speciﬁes one of the characters b, c, d, e, f, or
g. Some other examples are shown in Fig. 2.3.
RE
Match
Example Patterns Matched
/[A-Z]/
an upper case letter
“we should call it ‘Drenched Blossoms’ ”
/[a-z]/
a lower case letter
“my beans were impatient to be hoed!”
/[0-9]/
a single digit
“Chapter 1: Down the Rabbit Hole”
Figure 2.3
The use of the brackets [] plus the dash - to specify a range.
The square braces can also be used to specify what a single character cannot be,
by use of the caret ˆ. If the caret ˆ is the ﬁrst symbol after the open square brace [,
the resulting pattern is negated. For example, the pattern /[ˆa]/ matches any single
character (including special characters) except a. This is only true when the caret
is the ﬁrst symbol after the open square brace. If it occurs anywhere else, it usually
stands for a caret; Fig. 2.4 shows some examples.
RE
Match (single characters)
Example Patterns Matched
/[ˆA-Z]/
not an upper case letter
“Oyfn pripetchik”
/[ˆSs]/
neither ‘S’ nor ‘s’
“I have no exquisite reason for’t”
/[ˆ.]/
not a period
“our resident Djinn”
/[eˆ]/
either ‘e’ or ‘ˆ’
“look up ˆ now”
/aˆb/
the pattern ‘aˆb’
“look up aˆ b now”
Figure 2.4
The caret ˆ for negation or just to mean ˆ. See below re: the backslash for escaping the period.
How can we talk about optional elements, like an optional s in woodchuck and
woodchucks? We can’t use the square brackets, because while they allow us to say
“s or S”, they don’t allow us to say “s or nothing”. For this we use the question mark
/?/, which means “the preceding character or nothing”, as shown in Fig. 2.5.

2.1
•
REGULAR EXPRESSIONS
5
RE
Match
Example Patterns Matched
/woodchucks?/
woodchuck or woodchucks
“woodchuck”
/colou?r/
color or colour
“color”
Figure 2.5
The question mark ? marks optionality of the previous expression.
We can think of the question mark as meaning “zero or one instances of the
previous character”. That is, it’s a way of specifying how many of something that
we want, something that is very important in regular expressions. For example,
consider the language of certain sheep, which consists of strings that look like the
following:
baa!
baaa!
baaaa!
baaaaa!
...
This language consists of strings with a b, followed by at least two a’s, followed
by an exclamation point. The set of operators that allows us to say things like “some
number of as” are based on the asterisk or *, commonly called the Kleene * (gen-
Kleene *
erally pronounced “cleany star”). The Kleene star means “zero or more occurrences
of the immediately previous character or regular expression”. So /a*/ means “any
string of zero or more as”. This will match a or aaaaaa, but it will also match Off
Minor since the string Off Minor has zero a’s. So the regular expression for matching
one or more a is /aa*/, meaning one a followed by zero or more as. More complex
patterns can also be repeated. So /[ab]*/ means “zero or more a’s or b’s” (not
“zero or more right square braces”). This will match strings like aaaa or ababab or
bbbb.
For specifying multiple digits (useful for ﬁnding prices) we can extend /[0-9]/,
the regular expression for a single digit.
An integer (a string of digits) is thus
/[0-9][0-9]*/. (Why isn’t it just /[0-9]*/?)
Sometimes it’s annoying to have to write the regular expression for digits twice,
so there is a shorter way to specify “at least one” of some character. This is the
Kleene +, which means “one or more occurrences of the immediately preceding
Kleene +
character or regular expression”. Thus, the expression /[0-9]+/ is the normal way
to specify “a sequence of digits”. There are thus two ways to specify the sheep
language: /baaa*!/ or /baa+!/.
One very important special character is the period (/./), a wildcard expression
that matches any single character (except a carriage return), as shown in Fig. 2.6.
RE
Match
Example Matches
/beg.n/
any character between beg and n
begin, beg’n, begun
Figure 2.6
The use of the period . to specify any character.
The wildcard is often used together with the Kleene star to mean “any string of
characters”. For example, suppose we want to ﬁnd any line in which a particular
word, for example, aardvark, appears twice. We can specify this with the regular
expression /aardvark.*aardvark/.
Anchors are special characters that anchor regular expressions to particular places
Anchors
in a string. The most common anchors are the caret ˆ and the dollar sign $. The caret
ˆ matches the start of a line. The pattern /ˆThe/ matches the word The only at the

6
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to in-
dicate a negation inside of square brackets, and just to mean a caret. (What are the
contexts that allow grep or Python to know which function a given caret is supposed
to have?) The dollar sign $ matches the end of a line. So the pattern ␣$ is a useful
pattern for matching a space at the end of a line, and /ˆThe dog\.$/ matches a
line that contains only the phrase The dog. (We have to use the backslash here since
we want the . to mean “period” and not the wildcard.)
There are also two other anchors: \b matches a word boundary, and \B matches
a non-boundary. Thus, /\bthe\b/ matches the word the but not the word other.
More technically, a “word” for the purposes of a regular expression is deﬁned as any
sequence of digits, underscores, or letters; this is based on the deﬁnition of “words”
in programming languages. For example, /\b99\b/ will match the string 99 in
There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in
There are 299 bottles of beer on the wall (since 99 follows a number). But it will
match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore,
or letter).
2.1.2
Disjunction, Grouping, and Precedence
Suppose we need to search for texts about pets; perhaps we are particularly interested
in cats and dogs. In such a case, we might want to search for either the string cat or
the string dog. Since we can’t use the square brackets to search for “cat or dog” (why
can’t we say /[catdog]/?), we need a new operator, the disjunction operator, also
disjunction
called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or
the string dog.
Sometimes we need to use this disjunction operator in the midst of a larger se-
quence. For example, suppose I want to search for information about pet ﬁsh for
my cousin David. How can I specify both guppy and guppies? We cannot simply
say /guppy|ies/, because that would match only the strings guppy and ies. This
is because sequences like guppy take precedence over the disjunction operator |.
Precedence
To make the disjunction operator apply only to a speciﬁc pattern, we need to use the
parenthesis operators ( and ). Enclosing a pattern in parentheses makes it act like
a single character for the purposes of neighboring operators like the pipe | and the
Kleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-
tion only to apply to the sufﬁxes y and ies.
The parenthesis operator ( is also useful when we are using counters like the
Kleene*. Unlike the | operator, the Kleene* operator applies by default only to
a single character, not to a whole sequence. Suppose we want to match repeated
instances of a string. Perhaps we have a line that has column labels of the form
Column 1 Column 2 Column 3. The expression /Column␣[0-9]+␣*/ will not
match any number of columns; instead, it will match a single column followed by
any number of spaces! The star here applies only to the space ␣that precedes it,
not to the whole sequence. With the parentheses, we could write the expression
/(Column␣[0-9]+␣*)*/ to match the word Column, followed by a number and
optional spaces, the whole pattern repeated zero or more times.
This idea that one operator may take precedence over another, requiring us to
sometimes use parentheses to specify what we mean, is formalized by the operator
precedence hierarchy for regular expressions. The following table gives the order
operator
precedence
of RE operator precedence, from highest precedence to lowest precedence.

2.1
•
REGULAR EXPRESSIONS
7
Parenthesis
()
Counters
* + ? {}
Sequences and anchors
the ˆmy end$
Disjunction
|
Thus,
because
counters
have
a
higher
precedence
than
sequences,
/the*/ matches theeeee but not thethe. Because sequences have a higher prece-
dence than disjunction, /the|any/ matches the or any but not thany or theny.
Patterns can be ambiguous in another way. Consider the expression /[a-z]*/
when matching against the text once upon a time. Since /[a-z]*/ matches zero or
more letters, this expression could match nothing, or just the ﬁrst letter o, on, onc,
or once. In these cases regular expressions always match the largest string they can;
we say that patterns are greedy, expanding to cover as much of a string as they can.
greedy
There are, however, ways to enforce non-greedy matching, using another mean-
non-greedy
ing of the ? qualiﬁer. The operator *? is a Kleene star that matches as little text as
*?
possible. The operator +? is a Kleene plus that matches as little text as possible.
+?
2.1.3
A Simple Example
Suppose we wanted to write a RE to ﬁnd cases of the English article the. A simple
(but incorrect) pattern might be:
/the/
One problem is that this pattern will miss the word when it begins a sentence and
hence is capitalized (i.e., The). This might lead us to the following pattern:
/[tT]he/
But we will still incorrectly return texts with the embedded in other words (e.g.,
other or theology). So we need to specify that we want instances with a word bound-
ary on both sides:
/\b[tT]he\b/
Suppose we wanted to do this without the use of /\b/. We might want this since
/\b/ won’t treat underscores and numbers as word boundaries; but we might want
to ﬁnd the in some context where it might also have underlines or numbers nearby
(the or the25). We need to specify that we want instances in which there are no
alphabetic letters on either side of the the:
/[ˆa-zA-Z][tT]he[ˆa-zA-Z]/
But there is still one more problem with this pattern: it won’t ﬁnd the word the
when it begins a line. This is because the regular expression [ˆa-zA-Z], which
we used to avoid embedded instances of the, implies that there must be some single
(although non-alphabetic) character before the the. We can avoid this by specify-
ing that before the the we require either the beginning-of-line or a non-alphabetic
character, and the same at the end of the line:
/(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/
The process we just went through was based on ﬁxing two kinds of errors: false
positives, strings that we incorrectly matched like other or there, and false nega-
false positives
tives, strings that we incorrectly missed, like The. Addressing these two kinds of
false negatives

8
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
errors comes up again and again in implementing speech and language processing
systems. Reducing the overall error rate for an application thus involves two antag-
onistic efforts:
• Increasing precision (minimizing false positives)
• Increasing recall (minimizing false negatives)
2.1.4
A More Complex Example
Let’s try out a more signiﬁcant example of the power of REs. Suppose we want to
build an application to help a user buy a computer on the Web. The user might want
“any machine with at least 6 GHz and 500 GB of disk space for less than $1000”.
To do this kind of retrieval, we ﬁrst need to be able to look for expressions like 6
GHz or 500 GB or Mac or $999.99. In the rest of this section we’ll work out some
simple regular expressions for this task.
First, let’s complete our regular expression for prices. Here’s a regular expres-
sion for a dollar sign followed by a string of digits:
/$[0-9]+/
Note that the $ character has a different function here than the end-of-line function
we discussed earlier. Most regular expression parsers are smart enough to realize
that $ here doesn’t mean end-of-line. (As a thought experiment, think about how
regex parsers might ﬁgure out the function of $ from the context.)
Now we just need to deal with fractions of dollars. We’ll add a decimal point
and two digits afterwards:
/$[0-9]+\.[0-9][0-9]/
This pattern only allows $199.99 but not $199. We need to make the cents
optional and to make sure we’re at a word boundary:
/(ˆ|\W)$[0-9]+(\.[0-9][0-9])?\b/
One last catch! This pattern allows prices like $199999.99 which would be far
too expensive! We need to limit the dollar
/(ˆ|\W)$[0-9]{0,3}(\.[0-9][0-9])?\b/
How about disk space? We’ll need to allow for optional fractions again (5.5 GB);
note the use of ? for making the ﬁnal s optional, and the of /␣*/ to mean “zero or
more spaces” since there might always be extra spaces lying around:
/\b[0-9]+(\.[0-9]+)?␣*(GB|[Gg]igabytes?)\b/
Modifying this regular expression so that it only matches more than 500 GB is
left as an exercise for the reader.
2.1.5
More Operators
Figure 2.7 shows some aliases for common ranges, which can be used mainly to
save typing. Besides the Kleene * and Kleene + we can also use explicit numbers as
counters, by enclosing them in curly brackets. The regular expression /{3}/ means
“exactly 3 occurrences of the previous character or expression”. So /a\.{24}z/
will match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots
followed by a z).

2.1
•
REGULAR EXPRESSIONS
9
RE
Expansion
Match
First Matches
\d
[0-9]
any digit
Party␣of␣5
\D
[ˆ0-9]
any non-digit
Blue␣moon
\w
[a-zA-Z0-9_]
any alphanumeric/underscore
Daiyu
\W
[ˆ\w]
a non-alphanumeric
!!!!
\s
[␣\r\t\n\f]
whitespace (space, tab)
\S
[ˆ\s]
Non-whitespace
in␣Concord
Figure 2.7
Aliases for common sets of characters.
A range of numbers can also be speciﬁed. So /{n,m}/ speciﬁes from n to m
occurrences of the previous char or expression, and /{n,}/ means at least n occur-
rences of the previous expression. REs for counting are summarized in Fig. 2.8.
RE
Match
*
zero or more occurrences of the previous char or expression
+
one or more occurrences of the previous char or expression
?
exactly zero or one occurrence of the previous char or expression
{n}
n occurrences of the previous char or expression
{n,m}
from n to m occurrences of the previous char or expression
{n,}
at least n occurrences of the previous char or expression
{,m}
up to m occurrences of the previous char or expression
Figure 2.8
Regular expression operators for counting.
Finally, certain special characters are referred to by special notation based on the
backslash (\) (see Fig. 2.9). The most common of these are the newline character
Newline
\n and the tab character \t. To refer to characters that are special themselves (like
., *, [, and \), precede them with a backslash, (i.e., /\./, /\*/, /\[/, and /\\/).
RE
Match
First Patterns Matched
\*
an asterisk “*”
“K*A*P*L*A*N”
\.
a period “.”
“Dr. Livingston, I presume”
\?
a question mark
“Why don’t they come and lend a hand?”
\n
a newline
\t
a tab
Figure 2.9
Some characters that need to be backslashed.
2.1.6
Substitution, Capture Groups, and ELIZA
An important use of regular expressions is in substitutions. For example, the substi-
substitution
tution operator s/regexp1/pattern/ used in Python and in Unix commands like
vim or sed allows a string characterized by a regular expression to be replaced by
another string:
s/colour/color/
It is often useful to be able to refer to a particular subpart of the string matching
the ﬁrst pattern. For example, suppose we wanted to put angle brackets around all
integers in a text, for example, changing the 35 boxes to the <35> boxes. We’d
like a way to refer to the integer we’ve found so that we can easily add the brackets.
To do this, we put parentheses ( and ) around the ﬁrst pattern and use the number
operator \1 in the second pattern to refer back. Here’s how it looks:

10
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
s/([0-9]+)/<\1>/
The parenthesis and number operators can also specify that a certain string or
expression must occur twice in the text. For example, suppose we are looking for
the pattern “the Xer they were, the Xer they will be”, where we want to constrain
the two X’s to be the same string. We do this by surrounding the ﬁrst X with the
parenthesis operator, and replacing the second X with the number operator \1, as
follows:
/the (.*)er they were, the \1er they will be/
Here the \1 will be replaced by whatever string matched the ﬁrst item in paren-
theses. So this will match the bigger they were, the bigger they will be but not the
bigger they were, the faster they will be.
This use of parentheses to store a pattern in memory is called a capture group.
capture group
Every time a capture group is used (i.e., parentheses surround a pattern), the re-
sulting match is stored in a numbered register. If you match two different sets of
register
parentheses, \2 means whatever matched the second capture group. Thus
/the (.*)er they (.*), the \1er we \2/
will match the faster they ran, the faster we ran but not the faster they ran, the faster
we ate. Similarly, the third capture group is stored in \3, the fourth is \4, and so on.
Parentheses thus have a double function in regular expressions; they are used to
group terms for specifying the order in which operators should apply, and they are
used to capture something in a register. Occasionally we might want to use parenthe-
ses for grouping, but don’t want to capture the resulting pattern in a register. In that
case we use a non-capturing group, which is speciﬁed by putting the commands
non-capturing
group
?: after the open paren, in the form (?: pattern ).
/(?:some|a few) (people|cats) like some \1/
will match some cats like some cats but not some cats like some a few.
Substitutions and capture groups are very useful in implementing simple chat-
bots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian
psychologist by carrying on conversations like the following:
User1:
Men are all alike.
ELIZA1: IN WHAT WAY
User2:
They’re always bugging us about something or other.
ELIZA2: CAN YOU THINK OF A SPECIFIC EXAMPLE
User3:
Well, my boyfriend made me come here.
ELIZA3: YOUR BOYFRIEND MADE YOU COME HERE
User4:
He says I’m depressed much of the time.
ELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED
ELIZA works by having a series or cascade of regular expression substitutions
each of which matches and changes some part of the input lines. Input lines are
ﬁrst uppercased. The ﬁrst substitutions then change all instances of MY to YOUR,
and I’M to YOU ARE, and so on. The next set of substitutions matches and replaces
other patterns in the input. Here are some examples:
s/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \1/
s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \1/
s/.* all .*/IN WHAT WAY/
s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/

2.2
•
WORDS
11
Since multiple substitutions can apply to a given input, substitutions are assigned
a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we
return to the details of the ELIZA architecture in Chapter 26.
2.1.7
Lookahead Assertions
Finally, there will be times when we need to predict the future: look ahead in the
text to see if some pattern matches, but not advance the match cursor, so that we can
then deal with the pattern if it occurs.
These lookahead assertions make use of the (? syntax that we saw in the previ-
lookahead
ous section for non-capture groups. The operator (?= pattern) is true if pattern
occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator
zero-width
(?! pattern) only returns true if a pattern does not match, but again is zero-width
and doesn’t advance the cursor. Negative lookahead is commonly used when we
are parsing some complex pattern but want to rule out a special case. For example
suppose we want to match, at the beginning of a line, any single word that doesn’t
start with “Volcano”. We can use negative lookahead to do this:
/ˆ(?!Volcano)[A-Za-z]+/
2.2
Words
Before we talk about processing words, we need to decide what counts as a word.
Let’s start by looking at one particular corpus (plural corpora), a computer-readable
corpus
corpora
collection of text or speech. For example the Brown corpus is a million-word col-
lection of samples from 500 written English texts from different genres (newspa-
per, ﬁction, non-ﬁction, academic, etc.), assembled at Brown University in 1963–64
(Kuˇcera and Francis, 1967). How many words are in the following Brown sentence?
He stepped out into the hall, was delighted to encounter a water brother.
This sentence has 13 words if we don’t count punctuation marks as words, 15
if we count punctuation. Whether we treat period (“.”), comma (“,”), and so on as
words depends on the task. Punctuation is critical for ﬁnding boundaries of things
(commas, periods, colons) and for identifying some aspects of meaning (question
marks, exclamation marks, quotation marks). For some tasks, like part-of-speech
tagging or parsing or speech synthesis, we sometimes treat punctuation marks as if
they were separate words.
The Switchboard corpus of American English telephone conversations between
strangers was collected in the early 1990s; it contains 2430 conversations averaging
6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey
et al., 1992). Such corpora of spoken language don’t have punctuation but do intro-
duce other complications with regard to deﬁning words. Let’s look at one utterance
from Switchboard; an utterance is the spoken correlate of a sentence:
utterance
I do uh main- mainly business data processing
This utterance has two kinds of disﬂuencies. The broken-off word main- is
disﬂuency
called a fragment. Words like uh and um are called ﬁllers or ﬁlled pauses. Should
fragment
ﬁlled pause
we consider these to be words? Again, it depends on the application. If we are
building a speech transcription system, we might want to eventually strip out the
disﬂuencies.

12
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
But we also sometimes keep disﬂuencies around. Disﬂuencies like uh or um
are actually helpful in speech recognition in predicting the upcoming word, because
they may signal that the speaker is restarting the clause or idea, and so for speech
recognition they are treated as regular words. Because people use different disﬂu-
encies they can also be a cue to speaker identiﬁcation. In fact Clark and Fox Tree
(2002) showed that uh and um have different meanings. What do you think they are?
Are capitalized tokens like They and uncapitalized tokens like they the same
word? These are lumped together in some tasks (speech recognition), while for part-
of-speech or named-entity tagging, capitalization is a useful feature and is retained.
How about inﬂected forms like cats versus cat? These two words have the same
lemma cat but are different wordforms. A lemma is a set of lexical forms having
lemma
the same stem, the same major part-of-speech, and the same word sense. The word-
form is the full inﬂected or derived form of the word. For morphologically complex
wordform
languages like Arabic, we often need to deal with lemmatization. For many tasks in
English, however, wordforms are sufﬁcient.
How many words are there in English? To answer this question we need to
distinguish two ways of talking about words. Types are the number of distinct words
word type
in a corpus; if the set of words in the vocabulary is V, the number of types is the
vocabulary size |V|. Tokens are the total number N of running words. If we ignore
word token
punctuation, the following Brown sentence has 16 tokens and 14 types:
They picnicked by the pool, then lay back on the grass and looked at the stars.
When we speak about the number of words in the language, we are generally
referring to word types.
Corpus
Tokens = N Types = |V|
Shakespeare
884 thousand 31 thousand
Brown corpus
1 million 38 thousand
Switchboard telephone conversations
2.4 million 20 thousand
COCA
440 million
2 million
Google N-grams
1 trillion
13 million
Figure 2.10
Rough numbers of types and tokens for some English language corpora. The
largest, the Google N-grams corpus, contains 13 million types, but this count only includes
types appearing 40 or more times, so the true number would be much larger.
Fig. 2.10 shows the rough numbers of types and tokens computed from some
popular English corpora. The larger the corpora we look at, the more word types
we ﬁnd, and in fact this relationship between the number of types |V| and number
of tokens N is called Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978)
Herdan’s Law
Heaps’ Law
after its discoverers (in linguistics and information retrieval respectively). It is shown
in Eq. 2.1, where k and β are positive constants, and 0 < β < 1.
|V| = kNβ
(2.1)
The value of β depends on the corpus size and the genre, but at least for the
large corpora in Fig. 2.10, β ranges from .67 to .75. Roughly then we can say that
the vocabulary size for a text goes up signiﬁcantly faster than the square root of its
length in words.
Another measure of the number of words in the language is the number of lem-
mas instead of wordform types. Dictionaries can help in giving lemma counts; dic-
tionary entries or boldface forms are a very rough upper bound on the number of

2.3
•
CORPORA
13
lemmas (since some lemmas have multiple boldface forms). The 1989 edition of the
Oxford English Dictionary had 615,000 entries.
2.3
Corpora
Words don’t appear out of nowhere. Any particular piece of text that we study
is produced by one or more speciﬁc speakers or writers, in a speciﬁc dialect of a
speciﬁc language, at a speciﬁc time, in a speciﬁc place, for a speciﬁc function.
Perhaps the most important dimension of variation is the language. NLP algo-
rithms are most useful when they apply across many languages. The world has 7097
languages at the time of this writing, according to the online Ethnologue catalog
(Simons and Fennig, 2018). Most NLP tools tend to be developed for the ofﬁcial
languages of large industrialized nations (Chinese, English, Spanish, Arabic, etc.),
but we don’t want to limit tools to just these few languages. Furthermore, most lan-
guages also have multiple varieties, such as dialects spoken in different regions or
by different social groups. Thus, for example, if we’re processing text in African
American Vernacular English (AAVE), a dialect spoken by millions of people in the
AAVE
United States, it’s important to make use of NLP tools that function with that dialect.
Twitter posts written in AAVE make use of constructions like iont (I don’t in Stan-
dard American English (SAE)), or talmbout corresponding to SAE talking about,
SAE
both examples that inﬂuence word segmentation (Blodgett et al. 2016, Jones 2015).
It’s also quite common for speakers or writers to use multiple languages in a
single communicative act, a phenomenon called code switching.
Code switch-
code switching
ing is enormously common across the world; here are examples showing Spanish
and (transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens
et al. 2017):
(2.2) Por primera vez veo a @username actually being hateful! it was beautiful:)
[For the ﬁrst time I get to see @username actually being hateful! it was
beautiful:) ]
(2.3) dost tha or ra- hega ... dont wory ... but dherya rakhe
[“he was and will remain a friend ... don’t worry ... but have faith”]
Another dimension of variation is the genre. The text that our algorithms must
process might come from newswire, ﬁction or non-ﬁction books, scientiﬁc articles,
Wikipedia, or religious texts. It might come from spoken genres like telephone
conversations, business meetings, police body-worn cameras, medical interviews,
or transcripts of television shows or movies. It might come from work situations
like doctors’ notes, legal text, or parliamentary or congressional proceedings.
Text also reﬂects the demographic characteristics of the writer (or speaker): their
age, gender, race, socioeconomic class can all inﬂuence the linguistic properties of
the text we are processing.
And ﬁnally, time matters too. Language changes over time, and for some lan-
guages we have good corpora of texts from different historical periods.
Because language is so situated, when developing computational models for lan-
guage processing, it’s important to consider who produced the language, in what
context, for what purpose, and make sure that the models are ﬁt to the data.

14
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
2.4
Text Normalization
Before almost any natural language processing of a text, the text has to be normal-
ized. At least three tasks are commonly applied as part of any normalization process:
1. Tokenizing (segmenting) words
2. Normalizing word formats
3. Segmenting sentences
In the next sections we walk through each of these tasks.
2.4.1
Unix Tools for Crude Tokenization and Normalization
Let’s begin with an easy, if somewhat naive version of word tokenization and nor-
malization (and frequency computation) that can be accomplished for English solely
in a single UNIX command-line, inspired by Church (1994). We’ll make use of some
Unix commands: tr, used to systematically change particular characters in the in-
put; sort, which sorts input lines in alphabetical order; and uniq, which collapses
and counts adjacent identical lines.
For example let’s begin with the ‘complete words’ of Shakespeare in one textﬁle,
sh.txt. We can use tr to tokenize the words by changing every sequence of non-
alphabetic characters to a newline (’A-Za-z’ means alphabetic, the -c option com-
plements to non-alphabet, and the -s option squeezes all sequences into a single
character):
tr -sc ’A-Za-z’ ’\n’ < sh.txt
The output of this command will be:
THE
SONNETS
by
William
Shakespeare
From
fairest
creatures
We
...
Now that there is one word per line, we can sort the lines, and pass them to uniq
-c which will collapse and count them:
tr -sc ’A-Za-z’ ’\n’ < sh.txt | sort | uniq -c
with the following output:
1945 A
72 AARON
19 ABBESS
25 Aaron
6 Abate
1 Abates
5 Abbess
6 Abbey

2.4
•
TEXT NORMALIZATION
15
3 Abbot
...
Alternatively, we can collapse all the upper case to lower case:
tr -sc ’A-Za-z’ ’\n’ < sh.txt | tr A-Z a-z | sort | uniq -c
whose output is
14725 a
97 aaron
1 abaissiez
10 abandon
2 abandoned
2 abase
1 abash
14 abate
3 abated
3 abatement
...
Now we can sort again to ﬁnd the frequent words. The -n option to sort means
to sort numerically rather than alphabetically, and the -r option means to sort in
reverse order (highest-to-lowest):
tr -sc ’A-Za-z’ ’\n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r
The results show that the most frequent words in Shakespeare, as in any other
corpus, are the short function words like articles, pronouns, prepositions:
27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
12489 my
11318 that
11112 in
...
Unix tools of this sort can be very handy in building quick word count statistics
for any corpus.
2.4.2
Word Tokenization
The simple UNIX tools above were ﬁne for getting rough word statistics but more
sophisticated algorithms are generally necessary for tokenization, the task of seg-
tokenization
menting running text into words.
While the Unix command sequence just removed all the numbers and punctu-
ation, for most NLP applications we’ll need to keep these in our tokenization. We
often want to break off punctuation as a separate token; commas are a useful piece of
information for parsers, periods help indicate sentence boundaries. But we’ll often
want to keep the punctuation that occurs word internally, in examples like m.p.h,,
Ph.D., AT&T, cap’n. Special characters and numbers will need to be kept in prices

16
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
($45.55) and dates (01/02/06); we don’t want to segment that price into separate to-
kens of “45” and “55”. And there are URLs (http://www.stanford.edu), Twitter
hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).
Number expressions introduce other complications as well; while commas nor-
mally appear at word boundaries, commas are used inside numbers in English, every
three digits: 555,500.50. Languages, and hence tokenization requirements, differ
on this; many continental European languages like Spanish, French, and German, by
contrast, use a comma to mark the decimal point, and spaces (or sometimes periods)
where English puts commas, for example, 555 500,50.
A tokenizer can also be used to expand clitic contractions that are marked by
clitic
apostrophes, for example, converting what’re to the two tokens what are, and
we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only
occur when it is attached to another word. Some such contractions occur in other
alphabetic languages, including articles and pronouns in French (j’ai, l’homme).
Depending on the application, tokenization algorithms may also tokenize mul-
tiword expressions like New York or rock ’n’ roll as a single token, which re-
quires a multiword expression dictionary of some sort. Tokenization is thus inti-
mately tied up with named entity detection, the task of detecting names, dates, and
organizations (Chapter 18).
One commonly used tokenization standard is known as the Penn Treebank to-
kenization standard, used for the parsed corpora (treebanks) released by the Lin-
Penn Treebank
tokenization
guistic Data Consortium (LDC), the source of many useful datasets. This standard
separates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words to-
gether, and separates out all punctuation (to save space we’re showing visible spaces
‘ ’ between tokens, although newlines is a more common output):
Input:
"The San Francisco-based restaurant," they said,
"doesn’t charge $10".
Output: " The San Francisco-based restaurant , " they said ,
" does n’t charge $ 10 " .
In practice, since tokenization needs to be run before any other language pro-
cessing, it needs to be very fast. The standard method for tokenization is therefore
to use deterministic algorithms based on regular expressions compiled into very ef-
ﬁcient ﬁnite state automata. For example, Fig. 2.11 shows an example of a basic
regular expression that can be used to tokenize with the nltk.regexp tokenize
function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009;
http://www.nltk.org).
Carefully designed deterministic algorithms can deal with the ambiguities that
arise, such as the fact that the apostrophe needs to be tokenized differently when used
as a genitive marker (as in the book’s cover), a quotative as in ‘The other class’, she
said, or in clitics like they’re.
Word tokenization is more complex in languages like written Chinese, Japanese,
and Thai, which do not use spaces to mark potential word-boundaries.
In Chinese, for example, words are composed of characters (called hanzi in
hanzi
Chinese). Each character generally represents a single unit of meaning (called a
morpheme) and is pronounceable as a single syllable. Words are about 2.4 charac-
ters long on average. But deciding what counts as a word in Chinese is complex.
For example, consider the following sentence:
(2.4)
姚明进入总决赛
“Yao Ming reaches the ﬁnals”

2.4
•
TEXT NORMALIZATION
17
>>> text = ’That U.S.A. poster-print costs $12.40...’
>>> pattern = r’’’(?x)
# set flag to allow verbose regexps
...
([A-Z]\.)+
# abbreviations, e.g. U.S.A.
...
| \w+(-\w+)*
# words with optional internal hyphens
...
| \$?\d+(\.\d+)?%?
# currency and percentages, e.g. $12.40, 82%
...
| \.\.\.
# ellipsis
...
| [][.,;"’?():-_‘]
# these are separate tokens; includes ], [
... ’’’
>>> nltk.regexp_tokenize(text, pattern)
[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’]
Figure 2.11
A python trace of regular expression tokenization in the NLTK (Bird et al.,
2009) Python-based natural language processing toolkit, commented for readability; the (?x)
verbose ﬂag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird
et al. (2009).
As Chen et al. (2017) point out, this could be treated as 3 words (‘Chinese Treebank’
segmentation):
(2.5)
姚明
YaoMing
进入
reaches
总决赛
ﬁnals
or as 5 words (‘Peking University’ segmentation):
(2.6)
姚
Yao
明
Ming
进入
reaches
总
overall
决赛
ﬁnals
Finally, it is possible in Chinese simply to ignore words altogether and use characters
as the basic elements, treating the sentence as a series of 7 characters:
(2.7)
姚
Yao
明
Ming
进
enter
入
enter
总
overall
决
decision
赛
game
In fact, for most Chinese NLP tasks it turns out to work better to take characters
rather than words as input, since characters are at a reasonable semantic level for
most applications, and since most word standards result in a huge vocabulary with
large numbers of very rare words (Li et al., 2019).
However, for Japanese and Thai the character is too small a unit, and so algo-
rithms for word segmentation are required. These can also be useful for Chinese
word
segmentation
in the rare situations where word rather than character boundaries are required. The
standard segmentation algorithms for these languages use neural sequence mod-
els trained via supervised machine learning on hand-segmented training sets; we’ll
introduce sequence models in Chapter 8.
2.4.3
Byte-Pair Encoding for Tokenization
There is a third option to tokenizing text input. Instead of deﬁning tokens as words
(deﬁned by spaces in orthographies that have spaces, or more complex algorithms),
or as characters (as in Chinese), we can use our data to automatically tell us what size
tokens should be. Perhaps sometimes we might want tokens that are space-delimited
words (like spinach) other times it’s useful to have tokens that are larger than words
(like New York Times), and sometimes smaller than words (like the morphemes -est
or -er. A morpheme is the smallest meaning-bearing unit of a language; for example
the word unlikeliest has the morphemes un-, likely, and -est; we’ll return to this on
page 21.
One reason it’s helpful to have subword tokens is to deal with unknown words.
subword

18
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
Unknown words are particularly relevant for machine learning systems. As we will
see in the next chapter, machine learning systems often learn some facts about words
in one corpus (a training corpus) and then use these facts to make decisions about
a separate test corpus and its words. Thus if our training corpus contains, say the
words low, and lowest, but not lower, but then the word lower appears in our test
corpus, our system will not know what to do with it.
A solution to this problem is to use a kind of tokenization in which most tokens
are words, but some tokens are frequent morphemes or other subwords like -er, so
that an unseen word can be represented by combining the parts.
The simplest such algorithm is byte-pair encoding, or BPE (Sennrich et al.,
BPE
2016). Byte-pair encoding is based on a method for text compression (Gage, 1994),
but here we use it for tokenization instead. The intuition of the algorithm is to
iteratively merge frequent pairs of characters,
The algorithm begins with the set of symbols equal to the set of characters. Each
word is represented as a sequence of characters plus a special end-of-word symbol
. At each step of the algorithm, we count the number of symbol pairs, ﬁnd the
most frequent pair (‘A’, ‘B’), and replace it with the new merged symbol (‘AB’). We
continue to count and merge, creating new longer and longer character strings, until
we’ve done k merges; k is a parameter of the algorithm. The resulting symbol set
will consist of the original set of characters plus k new symbols.
The algorithm is run inside words (we don’t merge across word boundaries).
For this reason, the algorithm can take as input a dictionary of words together with
counts. Consider the following tiny input dictionary with counts for each word,
which would have the starting vocabulary of 11 letters:
dictionary
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w
2
l o w e s t
6
n e w e r
3
w i d e r
2
n e w
We ﬁrst count all pairs of symbols: the most frequent is the pair r
because
it occurs in newer (frequency of 6) and wider (frequency of 3) for a total of 9 oc-
currences. We then merge these symbols, treating r
as one symbol, and count
again:
dictionary
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, r
2
l o w e s t
6
n e w e r
3
w i d e r
2
n e w
Now the most frequent pair is e r , which we merge; our system has learned
that there should be a token for word-ﬁnal er, represented as er :
dictionary
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, r , er
2
l o w e s t
6
n e w er
3
w i d er
2
n e w
Next e w (total count of 8) get merged to ew:

2.4
•
TEXT NORMALIZATION
19
dictionary
vocabulary
5
l o w
, d, e, i, l, n, o, r, s, t, w, r , er , ew
2
l o w e s t
6
n ew er
3
w i d er
2
n ew
If we continue, the next merges are:
Merge
Current Vocabulary
(n, ew)
, d, e, i, l, n, o, r, s, t, w, r , er , ew, new
(l, o’
, d, e, i, l, n, o, r, s, t, w, r , er , ew, new, lo
(lo, w)
, d, e, i, l, n, o, r, s, t, w, r , er , ew, new, lo, low
(new, er )
, d, e, i, l, n, o, r, s, t, w, r , er , ew, new, lo, low, newer
(low,
)
, d, e, i, l, n, o, r, s, t, w, r , er , ew, new, lo, low, newer , low
When we need to tokenize a test sentence, we just run the merges we have
learned, greedily, in the order we learned them, on the test data. (Thus the fre-
quencies in the test data don’t play a role, just the frequencies in the training data).
So ﬁrst we segment each test sentence word into characters. Then we apply the ﬁrst
rule: replace every instance of r
in the test corpus with r , and then the second
rule: replace every instance of e r
in the test corpus with er , and so on. By the
end, if the test corpus contained the word n e w e r
, it would be tokenized as a
full word. But a new (unknown) word like l o w e r
would be merged into the
two tokens low er .
Of course in real algorithms BPE is run with many thousands of merges on a
very large input dictionary. The result is that most words will be represented as
full symbols, and only the very rare words (and unknown words) will have to be
represented by their parts. The full BPE learning algorithm is given in Fig. 2.12.
Wordpiece and Greedy Tokenization
There are some alternatives to byte pair encoding for inducing tokens. Like the BPE
algorithm, the wordpiece algorithm starts with some simple tokenization (such as
wordpiece
by whitespace) into rough words, and then breaks those rough word tokens into
subword tokens. The wordpiece model differs from BPE only in that the special
wordpiece
word-boundary token
appears at the beginning of words rather than at the end,
and in the way it merges pairs. Rather than merging the pairs that are most frequent,
wordpiece instead merges the pairs that minimizes the language model likelihood of
the training data. We’ll introduce these concepts in the next chapter, but to simplify,
the wordpiece model chooses the two tokens to combine that would give the training
corpus the highest probability (Wu et al., 2016).
In the wordpiece segmenter used in BERT (Devlin et al., 2019), like other word-
piece variants, an input sentence or string is ﬁrst split by some simple basic tokenizer
(like whitespace) into a series of rough word tokens. But then instead of using a
word boundary token, word-initial subwords are distinguished from those that do
not start words by marking internal subwords with special symbols ##, so that we
might split unaffable into ["un", "\#\#aff", "\#\#able"]. Then each word
token string is tokenized using a greedy longest-match-ﬁrst algorithm. This is dif-
ferent than the decoding algorithm we introduced for BPE, which runs the merges
on the test sentence in the same order they were learned from the training set.
Greedy longest-match-ﬁrst decoding is sometimes called maximum matching
maximum
matching
or MaxMatch. The maximum matching algorithm (Fig. 2.13) is given a vocabu-
lary (a learned list of wordpiece tokens) and a string and starts by pointing at the

20
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
import
re ,
c o l l e c t i o n s
def
g e t s t a t s ( vocab ) :
p a i r s =
c o l l e c t i o n s . d e f a u l t d i c t ( i n t )
f o r
word ,
f r e q
in
vocab . items ( ) :
symbols = word . s p l i t ( )
f o r
i
in
range ( len ( symbols ) −1):
p a i r s [ symbols [ i ] , symbols [ i +1]] += f r e q
r e t u r n
p a i r s
def
merge vocab ( pair ,
v i n ) :
v out = {}
bigram = re . escape ( ’
’ . j o i n ( p a i r ) )
p = re . compile ( r ’ (? <!\S ) ’ + bigram + r ’ ( ? ! \ S ) ’ )
f o r
word
in
v i n :
w out = p . sub ( ’ ’ . j o i n ( p a i r ) ,
word )
v out [ w out ] = v i n [ word ]
r e t u r n
v out
vocab = { ’ l
o w </w>’
:
5 ,
’ l
o w e s
t
</w>’
:
2 ,
’n e w e
r </w>’ : 6 ,
’w i
d e
r </w>’ : 3 ,
’n e w </w>’ :2}
num merges = 8
f o r
i
in
range ( num merges ) :
p a i r s =
g e t s t a t s ( vocab )
b e s t = max ( p a i r s ,
key= p a i r s . get )
vocab = merge vocab ( best ,
vocab )
p r i n t ( b e s t )
Figure 2.12
Python code for BPE learning algorithm from Sennrich et al. (2016).
beginning of a string. It chooses the longest token in the wordpiece vocabulary that
matches the input at the current position, and moves the pointer past that word in the
string. The algorithm is then applied again starting from the new pointer position.
function MAXMATCH(string, dictionary) returns list of tokens T
if string is empty
return empty list
for i←length(sentence) downto 1
ﬁrstword = ﬁrst i chars of sentence
remainder = rest of sentence
if InDictionary(ﬁrstword, dictionary)
return list(ﬁrstword, MaxMatch(remainder,dictionary) )
Figure 2.13
The MaxMatch (or ‘greedy longest-ﬁrst’) algorithm for word tokenization us-
ing wordpiece or other vocabularies. Assumes that all strings can be successfully tokenized
with the given dictionary.
Thus given the token intention and the dictionary:
["in", "tent","intent","##tent", "##tention", "##tion", "#ion"]
the BERT tokenizer would choose intent (because it is longer than in, and then
##ion to complete the string, resulting in the tokenization ["intent" "##ion"].
The BERT tokenizer applied to the string unwanted running will produce:
(2.8)
["un", "##want", "##ed", "runn", "##ing"]
Another tokenization algorithm is called SentencePiece (Kudo and Richardson,
SentencePiece

2.4
•
TEXT NORMALIZATION
21
2018). BPE and wordpiece both assume that we already have some initial tokeniza-
tion of words (such as by spaces, or from some initial dictionary) and so we never
tried to induce word parts across spaces. By contrast, the SentencePiece model
works from raw text; even whitespace is handled as a normal symbol. Thus it doesn’t
need an initial tokenization or word-list, and can be used in languages like Chinese
or Japanese that don’t have spaces.
2.4.4
Word Normalization, Lemmatization and Stemming
Word normalization is the task of putting words/tokens in a standard format, choos-
normalization
ing a single normal form for words with multiple forms like USA and US or uh-huh
and uhhuh. This standardization may be valuable, despite the spelling information
that is lost in the normalization process. For information retrieval or information
extraction about the US, we might want see information from documents whether
they mention the US or the USA.
Case folding is another kind of normalization. Mapping everything to lower
case folding
case means that Woodchuck and woodchuck are represented identically, which is
very helpful for generalization in many tasks, such as information retrieval or speech
recognition. For sentiment analysis and other text classiﬁcation tasks, information
extraction, and machine translation, by contrast, case can be quite helpful and case
folding is generally not done. This is because maintaining the difference between,
for example, US the country and us the pronoun can outweigh the advantage in
generalization that case folding would have provided for other words.
For many natural language processing situations we also want two morpholog-
ically different forms of a word to behave similarly. For example in web search,
someone may type the string woodchucks but a useful system might want to also
return pages that mention woodchuck with no s. This is especially common in mor-
phologically complex languages like Russian, where for example the word Moscow
has different endings in the phrases Moscow, of Moscow, to Moscow, and so on.
Lemmatization is the task of determining that two words have the same root,
despite their surface differences. The words am, are, and is have the shared lemma
be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of
these forms to the same lemma will let us ﬁnd all mentions of words in Russian like
Moscow. The lemmatized form of a sentence like He is reading detective stories
would thus be He be read detective story.
How is lemmatization done? The most sophisticated methods for lemmatization
involve complete morphological parsing of the word. Morphology is the study of
the way words are built up from smaller meaning-bearing units called morphemes.
morpheme
Two broad classes of morphemes can be distinguished: stems—the central mor-
stem
pheme of the word, supplying the main meaning— and afﬁxes—adding “additional”
afﬁx
meanings of various kinds. So, for example, the word fox consists of one morpheme
(the morpheme fox) and the word cats consists of two: the morpheme cat and the
morpheme -s. A morphological parser takes a word like cats and parses it into the
two morphemes cat and s, or a Spanish word like amaren (‘if in the future they
would love’) into the morphemes amar ‘to love’, 3PL, and future subjunctive.
The Porter Stemmer
Lemmatization algorithms can be complex. For this reason we sometimes make use
of a simpler but cruder method, which mainly consists of chopping off word-ﬁnal
afﬁxes. This naive version of morphological analysis is called stemming. One of
stemming
the most widely used stemming algorithms is the Porter (1980). The Porter stemmer
Porter stemmer

22
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
applied to the following paragraph:
This was not the map we found in Billy Bones’s chest, but
an accurate copy, complete in all things-names and heights
and soundings-with the single exception of the red crosses
and the written notes.
produces the following stemmed output:
Thi wa not the map we found in Billi Bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note
The algorithm is based on series of rewrite rules run in series, as a cascade, in
cascade
which the output of each pass is fed as input to the next pass; here is a sampling of
the rules:
ATIONAL →ATE (e.g., relational →relate)
ING →ϵ
if stem contains vowel (e.g., motoring →motor)
SSES →SS
(e.g., grasses →grass)
Detailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.)
can be found on Martin Porter’s homepage; see also the original paper (Porter, 1980).
Simple stemmers can be useful in cases where we need to collapse across differ-
ent variants of the same lemma. Nonetheless, they do tend to commit errors of both
over- and under-generalizing, as shown in the table below (Krovetz, 1993):
Errors of Commission
Errors of Omission
organization organ
European Europe
doing
doe
analysis
analyzes
numerical
numerous
noise
noisy
policy
police
sparse
sparsity
2.4.5
Sentence Segmentation
Sentence segmentation is another important step in text processing. The most use-
Sentence
segmentation
ful cues for segmenting a text into sentences are punctuation, like periods, question
marks, and exclamation points. Question marks and exclamation points are rela-
tively unambiguous markers of sentence boundaries. Periods, on the other hand, are
more ambiguous. The period character “.” is ambiguous between a sentence bound-
ary marker and a marker of abbreviations like Mr. or Inc. The previous sentence that
you just read showed an even more complex case of this ambiguity, in which the ﬁnal
period of Inc. marked both an abbreviation and the sentence boundary marker. For
this reason, sentence tokenization and word tokenization may be addressed jointly.
In general, sentence tokenization methods work by ﬁrst deciding (based on rules
or machine learning) whether a period is part of the word or is a sentence-boundary
marker. An abbreviation dictionary can help determine whether the period is part
of a commonly used abbreviation; the dictionaries can be hand-built or machine-
learned (Kiss and Strunk, 2006), as can the ﬁnal sentence splitter. In the Stan-
ford CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is
rule-based, a deterministic consequence of tokenization; a sentence ends when a
sentence-ending punctuation (., !, or ?) is not already grouped with other charac-
ters into a token (such as for an abbreviation or number), optionally followed by
additional ﬁnal quotes or brackets.

2.5
•
MINIMUM EDIT DISTANCE
23
2.5
Minimum Edit Distance
Much of natural language processing is concerned with measuring how similar two
strings are.
For example in spelling correction, the user typed some erroneous
string—let’s say graffe–and we want to know what the user meant. The user prob-
ably intended a word that is similar to graffe. Among candidate similar words,
the word giraffe, which differs by only one letter from graffe, seems intuitively
to be more similar than, say grail or graf, which differ in more letters. Another
example comes from coreference, the task of deciding whether two strings such as
the following refer to the same entity:
Stanford President John Hennessy
Stanford University President John Hennessy
Again, the fact that these two strings are very similar (differing by only one word)
seems like useful evidence for deciding that they might be coreferent.
Edit distance gives us a way to quantify both of these intuitions about string sim-
ilarity. More formally, the minimum edit distance between two strings is deﬁned
minimum edit
distance
as the minimum number of editing operations (operations like insertion, deletion,
substitution) needed to transform one string into another.
The gap between intention and execution, for example, is 5 (delete an i, substi-
tute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see
this by looking at the most important visualization for string distances, an alignment
alignment
between the two strings, shown in Fig. 2.14. Given two sequences, an alignment is
a correspondence between substrings of the two sequences. Thus, we say I aligns
with the empty string, N with E, and so on. Beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the
top string into the bottom string: d for deletion, s for substitution, i for insertion.
I N T E * N T I O N
| | | | | | | | | |
* E X E C U T I O N
d s s
i s
Figure 2.14
Representing the minimum edit distance between two strings as an alignment.
The ﬁnal row gives the operation list for converting the top string into the bottom string: d for
deletion, s for substitution, i for insertion.
We can also assign a particular cost or weight to each of these operations. The
Levenshtein distance between two sequences is the simplest weighting factor in
which each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume
that the substitution of a letter for itself, for example, t for t, has zero cost. The Lev-
enshtein distance between intention and execution is 5. Levenshtein also proposed
an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (This is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). Using this version, the Levenshtein distance between
intention and execution is 8.

24
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
2.5.1
The Minimum Edit Distance Algorithm
How do we ﬁnd the minimum edit distance? We can think of this as a search task, in
which we are searching for the shortest path—a sequence of edits—from one string
to another.
n t e n t i o n
i n t e c n t i o n
i n x e n t i o n
del
ins
subst
i n t e n t i o n
Figure 2.15
Finding the edit distance viewed as a search problem
The space of all possible edits is enormous, so we can’t search naively. However,
lots of distinct edit paths will end up in the same state (string), so rather than recom-
puting all those paths, we could just remember the shortest path to a state each time
we saw it. We can do this by using dynamic programming. Dynamic programming
dynamic
programming
is the name for a class of algorithms, ﬁrst introduced by Bellman (1957), that apply
a table-driven method to solve problems by combining solutions to sub-problems.
Some of the most commonly used algorithms in natural language processing make
use of dynamic programming, such as the Viterbi algorithm (Chapter 8) and the
CKY algorithm for parsing (Chapter 13).
The intuition of a dynamic programming problem is that a large problem can
be solved by properly combining the solutions to various sub-problems. Consider
the shortest path of transformed words that represents the minimum edit distance
between the strings intention and execution shown in Fig. 2.16.
n t e n t i o n
i n t e n t i o n
e t e n t i o n
e x e n t i o n
e x e n u t i o n
e x e c u t i o n
delete i
substitute n by e
substitute t by x
insert u
substitute n by c
Figure 2.16
Path from intention to execution.
Imagine some string (perhaps it is exention) that is in this optimal path (whatever
it is). The intuition of dynamic programming is that if exention is in the optimal
operation list, then the optimal sequence must also include the optimal path from
intention to exention. Why? If there were a shorter path from intention to exention,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequence wouldn’t be optimal, thus leading to a contradiction.
The minimum edit distance algorithm was named by Wagner and Fischer (1974)
minimum edit
distance
but independently discovered by many people (see the Historical Notes section of
Chapter 8).
Let’s ﬁrst deﬁne the minimum edit distance between two strings. Given two
strings, the source string X of length n, and target string Y of length m, we’ll deﬁne
D[i, j] as the edit distance between X[1..i] and Y[1..j], i.e., the ﬁrst i characters of X
and the ﬁrst j characters of Y. The edit distance between X and Y is thus D[n,m].

2.5
•
MINIMUM EDIT DISTANCE
25
We’ll use dynamic programming to compute D[n,m] bottom up, combining so-
lutions to subproblems. In the base case, with a source substring of length i but an
empty target string, going from i characters to 0 requires i deletes. With a target
substring of length j but an empty source going from 0 characters to j characters
requires j inserts. Having computed D[i, j] for small i, j we then compute larger
D[i, j] based on previously computed smaller values. The value of D[i, j] is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:
D[i, j] = min



D[i−1, j]+del-cost(source[i])
D[i, j −1]+ins-cost(target[j])
D[i−1, j −1]+sub-cost(source[i],target[j])
If we assume the version of Levenshtein distance in which the insertions and dele-
tions each have a cost of 1 (ins-cost(·) = del-cost(·) = 1), and substitutions have a
cost of 2 (except substitution of identical letters have zero cost), the computation for
D[i, j] becomes:
D[i, j] = min







D[i−1, j]+1
D[i, j −1]+1
D[i−1, j −1]+
 2; if source[i] ̸= target[j]
0; if source[i] = target[j]
(2.9)
The algorithm is summarized in Fig. 2.17; Fig. 2.18 shows the results of applying
the algorithm to the distance between intention and execution with the version of
Levenshtein in Eq. 2.9.
Knowing the minimum edit distance is useful for algorithms like ﬁnding poten-
tial spelling error corrections. But the edit distance algorithm is important in another
way; with a small change, it can also provide the minimum cost alignment between
two strings. Aligning two strings is useful throughout speech and language process-
ing. In speech recognition, minimum edit distance alignment is used to compute
the word error rate (Chapter 28). Alignment plays a role in machine translation, in
which sentences in a parallel corpus (a corpus with a text in two languages) need to
be matched to each other.
To extend the edit distance algorithm to produce an alignment, we can start by
visualizing an alignment as a path through the edit distance matrix. Figure 2.19
shows this path with the boldfaced cell. Each boldfaced cell represents an alignment
of a pair of letters in the two strings. If two boldfaced cells occur in the same row,
there will be an insertion in going from the source to the target; two boldfaced cells
in the same column indicate a deletion.
Figure 2.19 also shows the intuition of how to compute this alignment path. The
computation proceeds in two steps. In the ﬁrst step, we augment the minimum edit
distance algorithm to store backpointers in each cell. The backpointer from a cell
points to the previous cell (or cells) that we came from in entering the current cell.
We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have mul-
tiple backpointers because the minimum extension could have come from multiple
previous cells. In the second step, we perform a backtrace. In a backtrace, we start
backtrace
from the last cell (at the ﬁnal row and column), and follow the pointers back through
the dynamic programming matrix. Each complete path between the ﬁnal cell and the
initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the
minimum edit distance algorithm to store the pointers and compute the backtrace to
output an alignment.

26
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
function MIN-EDIT-DISTANCE(source, target) returns min-distance
n←LENGTH(source)
m←LENGTH(target)
Create a distance matrix distance[n+1,m+1]
# Initialization: the zeroth row and column is the distance from the empty string
D[0,0] = 0
for each row i from 1 to n do
D[i,0]←D[i-1,0] + del-cost(source[i])
for each column j from 1 to m do
D[0,j]←D[0, j-1] + ins-cost(target[j])
# Recurrence relation:
for each row i from 1 to n do
for each column j from 1 to m do
D[i,j]←MIN( D[i−1, j] + del-cost(source[i]),
D[i−1, j−1] + sub-cost(source[i],target[j]),
D[i, j−1] + ins-cost(target[j]))
# Termination
return D[n,m]
Figure 2.17
The minimum edit distance algorithm, an example of the class of dynamic
programming algorithms. The various costs can either be ﬁxed (e.g., ∀x,ins-cost(x) = 1)
or can be speciﬁc to the letter (to model the fact that some letters are more likely to be in-
serted than others). We assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost(x,x) = 0).
Src\Tar
#
e
x
e
c
u
t
i
o
n
#
0
1
2
3
4
5
6
7
8
9
i
1
2
3
4
5
6
7
6
7
8
n
2
3
4
5
6
7
8
7
8
7
t
3
4
5
6
7
8
7
8
9
8
e
4
3
4
5
6
7
8
9
10
9
n
5
4
5
6
7
8
9
10
11
10
t
6
5
6
7
8
9
8
9
10
11
i
7
6
7
8
9
10
9
8
9
10
o
8
7
8
9
10
11
10
9
8
9
n
9
8
9
10
11
12
11
10
9
8
Figure 2.18
Computation of minimum edit distance between intention and execution with
the algorithm of Fig. 2.17, using Levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions.
While we worked our example with simple Levenshtein distance, the algorithm
in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for
example, substitutions are more likely to happen between letters that are next to
each other on the keyboard. The Viterbi algorithm is a probabilistic extension of
minimum edit distance. Instead of computing the “minimum edit distance” between
two strings, Viterbi computes the “maximum probability alignment” of one string
with another. We’ll discuss this more in Chapter 8.

2.6
•
SUMMARY
27
#
e
x
e
c
u
t
i
o
n
#
0
←1
←2
←3
←4
←5
←6
←7
←8
←9
i
↑1
↖←↑2
↖←↑3
↖←↑4
↖←↑5
↖←↑6
↖←↑7
↖6
←7
←8
n
↑2
↖←↑3
↖←↑4
↖←↑5
↖←↑6
↖←↑7
↖←↑8
↑7
↖←↑8
↖7
t
↑3
↖←↑4
↖←↑5
↖←↑6
↖←↑7
↖←↑8
↖7
←↑8
↖←↑9
↑8
e
↑4
↖3
←4
↖←5
←6
←7
←↑8
↖←↑9
↖←↑10
↑9
n
↑5
↑4
↖←↑5
↖←↑6
↖←↑7
↖←↑8
↖←↑9
↖←↑10
↖←↑11 ↖↑10
t
↑6
↑5
↖←↑6
↖←↑7
↖←↑8
↖←↑9
↖8
←9
←10 ←↑11
i
↑7
↑6
↖←↑7
↖←↑8
↖←↑9
↖←↑10
↑9
↖8
←9
←10
o
↑8
↑7
↖←↑8
↖←↑9
↖←↑10
↖←↑11
↑10
↑9
↖8
←9
n
↑9
↑8
↖←↑9
↖←↑10
↖←↑11
↖←↑12
↑11
↑10
↑9
↖8
Figure 2.19
When entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. After the table is full we compute an alignment
(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and
following the arrows back. The sequence of bold cells represents one possible minimum cost
alignment between the two strings. Diagram design after Gusﬁeld (1997).
2.6
Summary
This chapter introduced a fundamental tool in language processing, the regular ex-
pression, and showed how to perform basic text normalization tasks including
word segmentation and normalization, sentence segmentation, and stemming.
We also introduce the important minimum edit distance algorithm for comparing
strings. Here’s a summary of the main points we covered about these ideas:
• The regular expression language is a powerful tool for pattern-matching.
• Basic operations in regular expressions include concatenation of symbols,
disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors
(ˆ, $) and precedence operators ((,)).
• Word tokenization and normalization are generally done by cascades of
simple regular expressions substitutions or ﬁnite automata.
• The Porter algorithm is a simple and efﬁcient way to do stemming, stripping
off afﬁxes. It does not have high accuracy but may be useful for some tasks.
• The minimum edit distance between two strings is the minimum number of
operations it takes to edit one into the other. Minimum edit distance can be
computed by dynamic programming, which also results in an alignment of
the two strings.
Bibliographical and Historical Notes
Kleene (1951) and (1956) ﬁrst deﬁned regular expressions and the ﬁnite automaton,
based on the McCulloch-Pitts neuron. Ken Thompson was one of the ﬁrst to build
regular expressions compilers into editors for text searching (Thompson, 1968). His
editor ed included a command “g/regular expression/p”, or Global Regular Expres-
sion Print, which later became the Unix grep utility.
Text normalization algorithms have been applied since the beginning of the
ﬁeld. One of the earliest widely-used stemmers was Lovins (1968). Stemming
was also applied early to the digital humanities, by Packard (1973), who built an
afﬁx-stripping morphological parser for Ancient Greek.
Currently a wide vari-

28
CHAPTER 2
•
REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE
ety of code for tokenization and normalization is available, such as the Stanford
Tokenizer (http://nlp.stanford.edu/software/tokenizer.shtml) or spe-
cialized tokenizers for Twitter (O’Connor et al., 2010), or for sentiment (http:
//sentiment.christopherpotts.net/tokenizing.html). See Palmer (2012)
for a survey of text preprocessing. NLTK is an essential tool that offers both useful
Python libraries (http://www.nltk.org) and textbook descriptions (Bird et al.,
2009) of many algorithms including text normalization and corpus interfaces.
For more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps
(1978), Egghe (2007) and Baayen (2001); Yasseri et al. (2012) discuss the relation-
ship with other measures of linguistic complexity. For more on edit distance, see the
excellent Gusﬁeld (1997). Our example measuring the edit distance from ‘intention’
to ‘execution’ was adapted from Kruskal (1983). There are various publicly avail-
able packages to compute edit distance, including Unix diff and the NIST sclite
program (NIST, 2005).
In his autobiography Bellman (1984) explains how he originally came up with
the term dynamic programming:
“...The 1950s were not good years for mathematical research. [the]
Secretary of Defense ...had a pathological fear and hatred of the word,
research...
I decided therefore to use the word, “programming”.
I
wanted to get across the idea that this was dynamic, this was multi-
stage... I thought, let’s ... take a word that has an absolutely precise
meaning, namely dynamic... it’s impossible to use the word, dynamic,
in a pejorative sense. Try thinking of some combination that will pos-
sibly give it a pejorative meaning. It’s impossible. Thus, I thought
dynamic programming was a good name. It was something not even a
Congressman could object to.”
Exercises
2.1
Write regular expressions for the following languages.
1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
3. the set of all strings from the alphabet a,b such that each a is immedi-
ately preceded by and immediately followed by a b;
2.2
Write regular expressions for the following languages. By “word”, we mean
an alphabetic string separated from other words by whitespace, any relevant
punctuation, line breaks, and so forth.
1. the set of all strings with two consecutive repeated words (e.g., “Hum-
bert Humbert” and “the the” but not “the bug” or “the big bug”);
2. all strings that start at the beginning of the line with an integer and that
end at the end of the line with a word;
3. all strings that have both the word grotto and the word raven in them
(but not, e.g., words like grottos that merely contain the word grotto);
4. write a pattern that places the ﬁrst word of an English sentence in a
register. Deal with punctuation.

EXERCISES
29
2.3
Implement an ELIZA-like program, using substitutions such as those described
on page 10. You might want to choose a different domain than a Rogerian psy-
chologist, although keep in mind that you would need a domain in which your
program can legitimately engage in a lot of simple repetition.
2.4
Compute the edit distance (using insertion cost 1, deletion cost 1, substitution
cost 1) of “leda” to “deal”. Show your work (using the edit distance grid).
2.5
Figure out whether drive is closer to brief or to divers and what the edit dis-
tance is to each. You may use any version of distance that you like.
2.6
Now implement a minimum edit distance algorithm and use your hand-computed
results to check your code.
2.7
Augment the minimum edit distance algorithm to output an alignment; you
will need to store pointers and add a stage to compute the backtrace.

30
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
CHAPTER
3
N-gram Language Models
“You are uniformly charming!” cried he, with a smile of associating and now
and then I bowed and they perceived a chaise and four to wish for.
Random sentence generated from a Jane Austen trigram model
Predicting is difﬁcult—especially about the future, as the old quip goes. But how
about predicting something that seems much easier, like the next few words someone
is going to say? What word, for example, is likely to follow
Please turn your homework ...
Hopefully, most of you concluded that a very likely word is in, or possibly over,
but probably not refrigerator or the. In the following sections we will formalize
this intuition by introducing models that assign a probability to each possible next
word. The same models will also serve to assign a probability to an entire sentence.
Such a model, for example, could predict that the following sequence has a much
higher probability of appearing in a text:
all of a sudden I notice three guys standing on the sidewalk
than does this same set of words in a different order:
on guys all I of notice sidewalk three a sudden standing the
Why would you want to predict upcoming words, or assign probabilities to sen-
tences? Probabilities are essential in any task in which we have to identify words in
noisy, ambiguous input, like speech recognition. For a speech recognizer to realize
that you said I will be back soonish and not I will be bassoon dish, it helps to know
that back soonish is a much more probable sequence than bassoon dish. For writing
tools like spelling correction or grammatical error correction, we need to ﬁnd and
correct errors in writing like Their are two midterms, in which There was mistyped
as Their, or Everything has improve, in which improve should have been improved.
The phrase There are will be much more probable than Their are, and has improved
than has improve, allowing us to help users by detecting and correcting these errors.
Assigning probabilities to sequences of words is also essential in machine trans-
lation. Suppose we are translating a Chinese source sentence:
他向记者
介绍了
主要内容
He to
reporters introduced main content
As part of the process we might have built the following set of potential rough
English translations:
he introduced reporters to the main contents of the statement
he briefed to reporters the main contents of the statement
he briefed reporters on the main contents of the statement

3.1
•
N-GRAMS
31
A probabilistic model of word sequences could suggest that briefed reporters on
is a more probable English phrase than briefed to reporters (which has an awkward
to after briefed) or introduced reporters to (which uses a verb that is less ﬂuent
English in this context), allowing us to correctly select the boldfaced sentence above.
Probabilities are also important for augmentative and alternative communi-
cation systems (Trnka et al. 2007, Kane et al. 2017). People often use such AAC
AAC
devices if they are physically unable or sign but can instead using eye gaze or other
speciﬁc movements to select words from a menu to be spoken by the system. Word
prediction can be used to suggest likely words for the menu.
Models that assign probabilities to sequences of words are called language mod-
els or LMs. In this chapter we introduce the simplest model that assigns probabilities
language model
LM
to sentences and sequences of words, the n-gram. An n-gram is a sequence of N
n-gram
words: a 2-gram (or bigram) is a two-word sequence of words like “please turn”,
“turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word se-
quence of words like “please turn your”, or “turn your homework”. We’ll see how
to use n-gram models to estimate the probability of the last word of an n-gram given
the previous words, and also to assign probabilities to entire sequences. In a bit of
terminological ambiguity, we usually drop the word “model”, and thus the term n-
gram is used to mean either the word sequence itself or the predictive model that
assigns it a probability. In later chapters we’ll introduce more sophisticated language
models like the RNN LMs of Chapter 9.
3.1
N-Grams
Let’s begin with the task of computing P(w|h), the probability of a word w given
some history h. Suppose the history h is “its water is so transparent that” and we
want to know the probability that the next word is the:
P(the|its water is so transparent that).
(3.1)
One way to estimate this probability is from relative frequency counts: take a
very large corpus, count the number of times we see its water is so transparent that,
and count the number of times this is followed by the. This would be answering the
question “Out of the times we saw the history h, how many times was it followed by
the word w”, as follows:
P(the|its water is so transparent that) =
C(its water is so transparent that the)
C(its water is so transparent that)
(3.2)
With a large enough corpus, such as the web, we can compute these counts and
estimate the probability from Eq. 3.2. You should pause now, go to the web, and
compute this estimate for yourself.
While this method of estimating probabilities directly from counts works ﬁne in
many cases, it turns out that even the web isn’t big enough to give us good estimates
in most cases. This is because language is creative; new sentences are created all the
time, and we won’t always be able to count entire sentences. Even simple extensions
of the example sentence may have counts of zero on the web (such as “Walden
Pond’s water is so transparent that the”; well, used to have counts of zero).

32
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
Similarly, if we wanted to know the joint probability of an entire sequence of
words like its water is so transparent, we could do it by asking “out of all possible
sequences of ﬁve words, how many of them are its water is so transparent?” We
would have to get the count of its water is so transparent and divide by the sum of
the counts of all possible ﬁve word sequences. That seems rather a lot to estimate!
For this reason, we’ll need to introduce cleverer ways of estimating the proba-
bility of a word w given a history h, or the probability of an entire word sequence W.
Let’s start with a little formalizing of notation. To represent the probability of a par-
ticular random variable Xi taking on the value “the”, or P(Xi = “the”), we will use
the simpliﬁcation P(the). We’ll represent a sequence of N words either as w1 ...wn
or wn
1 (so the expression wn−1
1
means the string w1,w2,...,wn−1). For the joint prob-
ability of each word in a sequence having a particular value P(X = w1,Y = w2,Z =
w3,...,W = wn) we’ll use P(w1,w2,...,wn).
Now how can we compute probabilities of entire sequences like P(w1,w2,...,wn)?
One thing we can do is decompose this probability using the chain rule of proba-
bility:
P(X1...Xn) = P(X1)P(X2|X1)P(X3|X2
1 )...P(Xn|Xn−1
1
)
=
n
Y
k=1
P(Xk|Xk−1
1
)
(3.3)
Applying the chain rule to words, we get
P(wn
1) = P(w1)P(w2|w1)P(w3|w2
1)...P(wn|wn−1
1
)
=
n
Y
k=1
P(wk|wk−1
1
)
(3.4)
The chain rule shows the link between computing the joint probability of a se-
quence and computing the conditional probability of a word given previous words.
Equation 3.4 suggests that we could estimate the joint probability of an entire se-
quence of words by multiplying together a number of conditional probabilities. But
using the chain rule doesn’t really seem to help us! We don’t know any way to
compute the exact probability of a word given a long sequence of preceding words,
P(wn|wn−1
1
). As we said above, we can’t just estimate by counting the number of
times every word occurs following every long string, because language is creative
and any particular context might have never occurred before!
The intuition of the n-gram model is that instead of computing the probability of
a word given its entire history, we can approximate the history by just the last few
words.
The bigram model, for example, approximates the probability of a word given
bigram
all the previous words P(wn|wn−1
1
) by using only the conditional probability of the
preceding word P(wn|wn−1). In other words, instead of computing the probability
P(the|Walden Pond’s water is so transparent that)
(3.5)
we approximate it with the probability
P(the|that)
(3.6)

3.1
•
N-GRAMS
33
When we use a bigram model to predict the conditional probability of the next
word, we are thus making the following approximation:
P(wn|wn−1
1
) ≈P(wn|wn−1)
(3.7)
The assumption that the probability of a word depends only on the previous word
is called a Markov assumption. Markov models are the class of probabilistic models
Markov
that assume we can predict the probability of some future unit without looking too
far into the past. We can generalize the bigram (which looks one word into the past)
to the trigram (which looks two words into the past) and thus to the n-gram (which
n-gram
looks n−1 words into the past).
Thus, the general equation for this n-gram approximation to the conditional
probability of the next word in a sequence is
P(wn|wn−1
1
) ≈P(wn|wn−1
n−N+1)
(3.8)
Given the bigram assumption for the probability of an individual word, we can
compute the probability of a complete word sequence by substituting Eq. 3.7 into
Eq. 3.4:
P(wn
1) ≈
n
Y
k=1
P(wk|wk−1)
(3.9)
How do we estimate these bigram or n-gram probabilities? An intuitive way to
estimate probabilities is called maximum likelihood estimation or MLE. We get
maximum
likelihood
estimation
the MLE estimate for the parameters of an n-gram model by getting counts from a
corpus, and normalizing the counts so that they lie between 0 and 1.1
normalize
For example, to compute a particular bigram probability of a word y given a
previous word x, we’ll compute the count of the bigram C(xy) and normalize by the
sum of all the bigrams that share the same ﬁrst word x:
P(wn|wn−1) =
C(wn−1wn)
P
wC(wn−1w)
(3.10)
We can simplify this equation, since the sum of all bigram counts that start with
a given word wn−1 must be equal to the unigram count for that word wn−1 (the reader
should take a moment to be convinced of this):
P(wn|wn−1) = C(wn−1wn)
C(wn−1)
(3.11)
Let’s work through an example using a mini-corpus of three sentences. We’ll
ﬁrst need to augment each sentence with a special symbol <s> at the beginning
of the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a
special end-symbol. </s>2
<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>
1
For probabilistic models, normalizing means dividing by some total count so that the resulting prob-
abilities fall legally between 0 and 1.
2
We need the end-symbol to make the bigram grammar a true probability distribution. Without an
end-symbol, the sentence probabilities for all sentences of a given length would sum to one. This model
would deﬁne an inﬁnite set of probability distributions, with one distribution per sentence length. See
Exercise 3.5.

34
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
Here are the calculations for some of the bigram probabilities from this corpus
P(I|<s>) = 2
3 = .67
P(Sam|<s>) = 1
3 = .33
P(am|I) = 2
3 = .67
P(</s>|Sam) = 1
2 = 0.5
P(Sam|am) = 1
2 = .5
P(do|I) = 1
3 = .33
For the general case of MLE n-gram parameter estimation:
P(wn|wn−1
n−N+1) = C(wn−1
n−N+1wn)
C(wn−1
n−N+1)
(3.12)
Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the
observed frequency of a particular sequence by the observed frequency of a preﬁx.
This ratio is called a relative frequency. We said above that this use of relative
relative
frequency
frequencies as a way to estimate probabilities is an example of maximum likelihood
estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood
of the training set T given the model M (i.e., P(T|M)). For example, suppose the
word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.
What is the probability that a random word selected from some other text of, say,
a million words will be the word Chinese? The MLE of its probability is
400
1000000
or .0004. Now .0004 is not the best possible estimate of the probability of Chinese
occurring in all situations; it might turn out that in some other corpus or context
Chinese is a very unlikely word. But it is the probability that makes it most likely
that Chinese will occur 400 times in a million-word corpus. We present ways to
modify the MLE estimates slightly to get better probability estimates in Section 3.4.
Let’s move on to some examples from a slightly larger corpus than our 14-word
example above. We’ll use data from the now-defunct Berkeley Restaurant Project,
a dialogue system from the last century that answered questions about a database
of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text-
normalized sample user queries (a sample of 9332 sentences is on the website):
can you tell me about any good cantonese restaurants close by
mid priced thai food is what i’m looking for
tell me about chez panisse
can you give me a listing of the kinds of food that are available
i’m looking for a good place to eat breakfast
when is caffe venezia open during the day
Figure 3.1 shows the bigram counts from a piece of a bigram grammar from the
Berkeley Restaurant Project. Note that the majority of the values are zero. In fact,
we have chosen the sample words to cohere with each other; a matrix selected from
a random set of seven words would be even more sparse.
Figure 3.2 shows the bigram probabilities after normalization (dividing each cell
in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of
unigram probabilities):
i
want to
eat
chinese food lunch spend
2533 927
2417 746 158
1093 341
278
Here are a few other useful probabilities:
P(i|<s>) = 0.25
P(english|want) = 0.0011
P(food|english) = 0.5
P(</s>|food) = 0.68
Now we can compute the probability of sentences like I want English food or
I want Chinese food by simply multiplying the appropriate bigram probabilities to-
gether, as follows:

3.1
•
N-GRAMS
35
i
want
to
eat
chinese
food
lunch
spend
i
5
827
0
9
0
0
0
2
want
2
0
608
1
6
6
5
1
to
2
0
4
686
2
0
6
211
eat
0
0
2
0
16
2
42
0
chinese
1
0
0
0
0
82
1
0
food
15
0
15
0
1
4
0
0
lunch
2
0
0
0
0
1
0
0
spend
1
0
1
0
0
0
0
0
Figure 3.1
Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-
rant Project corpus of 9332 sentences. Zero counts are in gray.
i
want
to
eat
chinese
food
lunch
spend
i
0.002
0.33
0
0.0036
0
0
0
0.00079
want
0.0022
0
0.66
0.0011
0.0065
0.0065
0.0054
0.0011
to
0.00083
0
0.0017
0.28
0.00083
0
0.0025
0.087
eat
0
0
0.0027
0
0.021
0.0027
0.056
0
chinese
0.0063
0
0
0
0
0.52
0.0063
0
food
0.014
0
0.014
0
0.00092
0.0037
0
0
lunch
0.0059
0
0
0
0
0.0029
0
0
spend
0.0036
0
0.0036
0
0
0
0
0
Figure 3.2
Bigram probabilities for eight words in the Berkeley Restaurant Project corpus
of 9332 sentences. Zero probabilities are in gray.
P(<s> i want english food </s>)
= P(i|<s>)P(want|i)P(english|want)
P(food|english)P(</s>|food)
= .25×.33×.0011×0.5×0.68
= .000031
We leave it as Exercise 3.2 to compute the probability of i want chinese food.
What kinds of linguistic phenomena are captured in these bigram statistics?
Some of the bigram probabilities above encode some facts that we think of as strictly
syntactic in nature, like the fact that what comes after eat is usually a noun or an
adjective, or that what comes after to is usually a verb. Others might be a fact about
the personal assistant task, like the high probability of sentences beginning with
the words I. And some might even be cultural rather than linguistic, like the higher
probability that people are looking for Chinese versus English food.
Some practical issues:
Although for pedagogical purposes we have only described
bigram models, in practice it’s more common to use trigram models, which con-
trigram
dition on the previous two words rather than the previous word, or 4-gram or even
4-gram
5-gram models, when there is sufﬁcient training data. Note that for these larger n-
5-gram
grams, we’ll need to assume extra context for the contexts to the left and right of the
sentence end. For example, to compute trigram probabilities at the very beginning of
the sentence, we can use two pseudo-words for the ﬁrst trigram (i.e., P(I|<s><s>).
We always represent and compute language model probabilities in log format
as log probabilities. Since probabilities are (by deﬁnition) less than or equal to
log
probabilities
1, the more probabilities we multiply together, the smaller the product becomes.
Multiplying enough n-grams together would result in numerical underﬂow. By using
log probabilities instead of raw probabilities, we get numbers that are not as small.

36
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
Adding in log space is equivalent to multiplying in linear space, so we combine log
probabilities by adding them. The result of doing all computation and storage in log
space is that we only need to convert back into probabilities if we need to report
them at the end; then we can just take the exp of the logprob:
p1 × p2 × p3 × p4 = exp(log p1 +log p2 +log p3 +log p4)
(3.13)
3.2
Evaluating Language Models
The best way to evaluate the performance of a language model is to embed it in
an application and measure how much the application improves. Such end-to-end
evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to
extrinsic
evaluation
know if a particular improvement in a component is really going to help the task
at hand. Thus, for speech recognition, we can compare the performance of two
language models by running the speech recognizer twice, once with each language
model, and seeing which gives the more accurate transcription.
Unfortunately, running big NLP systems end-to-end is often very expensive. In-
stead, it would be nice to have a metric that can be used to quickly evaluate potential
improvements in a language model. An intrinsic evaluation metric is one that mea-
intrinsic
evaluation
sures the quality of a model independent of any application.
For an intrinsic evaluation of a language model we need a test set. As with many
of the statistical models in our ﬁeld, the probabilities of an n-gram model come from
the corpus it is trained on, the training set or training corpus. We can then measure
training set
the quality of an n-gram model by its performance on some unseen data called the
test set or test corpus. We will also sometimes call test sets and other datasets that
test set
are not in our training sets held out corpora because we hold them out from the
held out
training data.
So if we are given a corpus of text and want to compare two different n-gram
models, we divide the data into training and test sets, train the parameters of both
models on the training set, and then compare how well the two trained models ﬁt the
test set.
But what does it mean to “ﬁt the test set”? The answer is simple: whichever
model assigns a higher probability to the test set—meaning it more accurately
predicts the test set—is a better model. Given two probabilistic models, the better
model is the one that has a tighter ﬁt to the test data or that better predicts the details
of the test data, and hence will assign a higher probability to the test data.
Since our evaluation metric is based on test set probability, it’s important not to
let the test sentences into the training set. Suppose we are trying to compute the
probability of a particular “test” sentence. If our test sentence is part of the training
corpus, we will mistakenly assign it an artiﬁcially high probability when it occurs
in the test set. We call this situation training on the test set. Training on the test
set introduces a bias that makes the probabilities all look too high, and causes huge
inaccuracies in perplexity, the probability-based metric we introduce below.
Sometimes we use a particular test set so often that we implicitly tune to its
characteristics. We then need a fresh test set that is truly unseen. In such cases, we
call the initial test set the development test set or, devset. How do we divide our
development
test
data into training, development, and test sets? We want our test set to be as large
as possible, since a small test set may be accidentally unrepresentative, but we also
want as much training data as possible. At the minimum, we would want to pick

3.2
•
EVALUATING LANGUAGE MODELS
37
the smallest test set that gives us enough statistical power to measure a statistically
signiﬁcant difference between two potential models. In practice, we often just divide
our data into 80% training, 10% development, and 10% test. Given a large corpus
that we want to divide into training and test, test data can either be taken from some
continuous sequence of text inside the corpus, or we can remove smaller “stripes”
of text from randomly selected parts of our corpus and combine them into a test set.
3.2.1
Perplexity
In practice we don’t use raw probability as our metric for evaluating language mod-
els, but a variant called perplexity. The perplexity (sometimes called PP for short)
perplexity
of a language model on a test set is the inverse probability of the test set, normalized
by the number of words. For a test set W = w1w2 ...wN,:
PP(W) = P(w1w2 ...wN)−1
N
(3.14)
=
N
s
1
P(w1w2 ...wN)
We can use the chain rule to expand the probability of W:
PP(W) =
N
v
u
u
t
N
Y
i=1
1
P(wi|w1 ...wi−1)
(3.15)
Thus, if we are computing the perplexity of W with a bigram language model,
we get:
PP(W) =
N
v
u
u
t
N
Y
i=1
1
P(wi|wi−1)
(3.16)
Note that because of the inverse in Eq. 3.15, the higher the conditional probabil-
ity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is
equivalent to maximizing the test set probability according to the language model.
What we generally use for word sequence in Eq. 3.15 or Eq. 3.16 is the entire se-
quence of words in some test set. Since this sequence will cross many sentence
boundaries, we need to include the begin- and end-sentence markers <s> and </s>
in the probability computation. We also need to include the end-of-sentence marker
</s> (but not the beginning-of-sentence marker <s>) in the total count of word to-
kens N.
There is another way to think about perplexity: as the weighted average branch-
ing factor of a language. The branching factor of a language is the number of possi-
ble next words that can follow any word. Consider the task of recognizing the digits
in English (zero, one, two,..., nine), given that (both in some training set and in some
test set) each of the 10 digits occurs with equal probability P = 1
10. The perplexity of
this mini-language is in fact 10. To see that, imagine a test string of digits of length
N, and assume that in the training set all the digits occurred with equal probability.
By Eq. 3.15, the perplexity will be

38
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
PP(W) = P(w1w2 ...wN)−1
N
= ( 1
10
N
)−1
N
=
1
10
−1
= 10
(3.17)
But suppose that the number zero is really frequent and occurs far more often
than other numbers. Let’s say that 0 occur 91 times in the training set, and each
of the other digits occurred 1 time each. Now we see the following test set: 0 0
0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since
most of the time the next number will be zero, which is very predictable, i.e. has
a high probability. Thus, although the branching factor is still 10, the perplexity or
weighted branching factor is smaller. We leave this exact calculation as exercise 12.
We see in Section 3.7 that perplexity is also closely related to the information-
theoretic notion of entropy.
Finally, let’s look at an example of how perplexity can be used to compare dif-
ferent n-gram models. We trained unigram, bigram, and trigram grammars on 38
million words (including start-of-sentence tokens) from the Wall Street Journal, us-
ing a 19,979 word vocabulary. We then computed the perplexity of each of these
models on a test set of 1.5 million words with Eq. 3.16. The table below shows the
perplexity of a 1.5 million word WSJ test set according to each of these grammars.
Unigram Bigram Trigram
Perplexity 962
170
109
As we see above, the more information the n-gram gives us about the word
sequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related
inversely to the likelihood of the test sequence according to the model).
Note that in computing perplexities, the n-gram model P must be constructed
without any knowledge of the test set or any prior knowledge of the vocabulary of
the test set. Any kind of knowledge of the test set can cause the perplexity to be
artiﬁcially low. The perplexity of two language models is only comparable if they
use identical vocabularies.
An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-
provement in the performance of a language processing task like speech recognition
or machine translation. Nonetheless, because perplexity often correlates with such
improvements, it is commonly used as a quick check on an algorithm. But a model’s
improvement in perplexity should always be conﬁrmed by an end-to-end evaluation
of a real task before concluding the evaluation of the model.
3.3
Generalization and Zeros
The n-gram model, like many statistical models, is dependent on the training corpus.
One implication of this is that the probabilities often encode speciﬁc facts about a
given training corpus. Another implication is that n-grams do a better and better job
of modeling the training corpus as we increase the value of N.

3.3
•
GENERALIZATION AND ZEROS
39
We can visualize both of these facts by borrowing the technique of Shannon
(1951) and Miller and Selfridge (1950) of generating random sentences from dif-
ferent n-gram models. It’s simplest to visualize how this works for the unigram
case. Imagine all the words of the English language covering the probability space
between 0 and 1, each word covering an interval proportional to its frequency. We
choose a random value between 0 and 1 and print the word whose interval includes
this chosen value. We continue choosing random numbers and generating words
until we randomly generate the sentence-ﬁnal token </s>. We can use the same
technique to generate bigrams by ﬁrst generating a random bigram that starts with
<s> (according to its bigram probability). Let’s say the second word of that bigram
is w. We next chose a random bigram starting with w (again, drawn according to its
bigram probability), and so on.
To give an intuition for the increasing power of higher-order n-grams, Fig. 3.3
shows random sentences generated from unigram, bigram, trigram, and 4-gram
models trained on Shakespeare’s works.
1
–To him swallowed confess hear both. Which. Of save on trail for are ay device and
rote life have
gram
–Hill he late speaks; or! a more to leg less ﬁrst you enter
2
–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live
king. Follow.
gram
–What means, sir. I confess she? then all sorts, he is trim, captain.
3
–Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,
’tis done.
gram
–This shall forbid it should be branded, if renown made it empty.
4
–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A
great banquet serv’d in;
gram
–It cannot be but so.
Figure 3.3
Eight sentences randomly generated from four n-grams computed from Shakespeare’s works. All
characters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected
for capitalization to improve readability.
The longer the context on which we train the model, the more coherent the sen-
tences. In the unigram sentences, there is no coherent relation between words or any
sentence-ﬁnal punctuation. The bigram sentences have some local word-to-word
coherence (especially if we consider that punctuation counts as a word). The tri-
gram and 4-gram sentences are beginning to look a lot like Shakespeare. Indeed, a
careful investigation of the 4-gram sentences shows that they look a little too much
like Shakespeare. The words It cannot be but so are directly from King John. This is
because, not to put the knock on Shakespeare, his oeuvre is not very large as corpora
go (N = 884,647,V = 29,066), and our n-gram probability matrices are ridiculously
sparse. There are V 2 = 844,000,000 possible bigrams alone, and the number of pos-
sible 4-grams is V 4 = 7×1017. Thus, once the generator has chosen the ﬁrst 4-gram
(It cannot be but), there are only ﬁve possible continuations (that, I, he, thou, and
so); indeed, for many 4-grams, there is only one continuation.
To get an idea of the dependence of a grammar on its training set, let’s look at an
n-gram grammar trained on a completely different corpus: the Wall Street Journal
(WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so
we might expect some overlap between our n-grams for the two genres. Fig. 3.4

40
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
shows sentences generated by unigram, bigram, and trigram grammars trained on
40 million words from WSJ.
1
Months the my and issue of year foreign new exchange’s september
were recession exchange new endorsed a acquire to six executives
gram
2
Last December through the way to preserve the Hudson corporation N.
B. E. C. Taylor would seem to complete the major central planners one
gram
point ﬁve percent of U. S. E. has already old M. X. corporation of living
on information such as more frequently ﬁshing to keep her
3
They also point to ninety nine point six billion dollars from two hundred
four oh six three percent of the rates of interest stores as Mexico and
gram
Brazil on market conditions
Figure 3.4
Three sentences randomly generated from three n-gram models computed from
40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-
tion as words. Output was then hand-corrected for capitalization to improve readability.
Compare these examples to the pseudo-Shakespeare in Fig. 3.3. While they both
model “English-like sentences”, there is clearly no overlap in generated sentences,
and little overlap even in small phrases. Statistical models are likely to be pretty use-
less as predictors if the training sets and the test sets are as different as Shakespeare
and WSJ.
How should we deal with this problem when we build n-gram models? One step
is to be sure to use a training corpus that has a similar genre to whatever task we are
trying to accomplish. To build a language model for translating legal documents,
we need a training corpus of legal documents. To build a language model for a
question-answering system, we need a training corpus of questions.
It is equally important to get training data in the appropriate dialect, especially
when processing social media posts or spoken transcripts. Thus tweets in AAVE
(African American Vernacular English) often use words like ﬁnna—an auxiliary
verb that marks immediate future tense —that don’t occur in other dialects, or
spellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017):
(3.18) Bored af den my phone ﬁnna die!!!
while tweets from varieties like Nigerian English have markedly different vocabu-
lary and n-gram patterns from American English (Jurgens et al., 2017):
(3.19) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u
tweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter
Matching genres and dialects is still not sufﬁcient. Our models may still be
subject to the problem of sparsity. For any n-gram that occurred a sufﬁcient number
of times, we might have a good estimate of its probability. But because any corpus is
limited, some perfectly acceptable English word sequences are bound to be missing
from it. That is, we’ll have many cases of putative “zero probability n-grams” that
should really have some non-zero probability. Consider the words that follow the
bigram denied the in the WSJ Treebank3 corpus, together with their counts:
denied the allegations:
5
denied the speculation: 2
denied the rumors:
1
denied the report:
1
But suppose our test set has phrases like:

3.3
•
GENERALIZATION AND ZEROS
41
denied the offer
denied the loan
Our model will incorrectly estimate that the P(offer|denied the) is 0!
These zeros— things that don’t ever occur in the training set but do occur in
zeros
the test set—are a problem for two reasons. First, their presence means we are
underestimating the probability of all sorts of words that might occur, which will
hurt the performance of any application we want to run on this data.
Second, if the probability of any word in the test set is 0, the entire probability
of the test set is 0. By deﬁnition, perplexity is based on the inverse probability of the
test set. Thus if some words have zero probability, we can’t compute perplexity at
all, since we can’t divide by 0!
3.3.1
Unknown Words
The previous section discussed the problem of words whose bigram probability is
zero. But what about words we simply have never seen before?
Sometimes we have a language task in which this can’t happen because we know
all the words that can occur. In such a closed vocabulary system the test set can
closed
vocabulary
only contain words from this lexicon, and there will be no unknown words. This is
a reasonable assumption in some domains, such as speech recognition or machine
translation, where we have a pronunciation dictionary or a phrase table that are ﬁxed
in advance, and so the language model can only use the words in that dictionary or
phrase table.
In other cases we have to deal with words we haven’t seen before, which we’ll
call unknown words, or out of vocabulary (OOV) words. The percentage of OOV
OOV
words that appear in the test set is called the OOV rate. An open vocabulary system
open
vocabulary
is one in which we model these potential unknown words in the test set by adding a
pseudo-word called <UNK>.
There are two common ways to train the probabilities of the unknown word
model <UNK>. The ﬁrst one is to turn the problem back into a closed vocabulary one
by choosing a ﬁxed vocabulary in advance:
1. Choose a vocabulary (word list) that is ﬁxed in advance.
2. Convert in the training set any word that is not in this set (any OOV word) to
the unknown word token <UNK> in a text normalization step.
3. Estimate the probabilities for <UNK> from its counts just like any other regular
word in the training set.
The second alternative, in situations where we don’t have a prior vocabulary in ad-
vance, is to create such a vocabulary implicitly, replacing words in the training data
by <UNK> based on their frequency. For example we can replace by <UNK> all words
that occur fewer than n times in the training set, where n is some small number, or
equivalently select a vocabulary size V in advance (say 50,000) and choose the top
V words by frequency and replace the rest by UNK. In either case we then proceed
to train the language model as before, treating <UNK> like a regular word.
The exact choice of <UNK> model does have an effect on metrics like perplexity.
A language model can achieve low perplexity by choosing a small vocabulary and
assigning the unknown word a high probability. For this reason, perplexities should
only be compared across language models with the same vocabularies (Buck et al.,
2014).

42
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
3.4
Smoothing
What do we do with words that are in our vocabulary (they are not unknown words)
but appear in a test set in an unseen context (for example they appear after a word
they never appeared after in training)? To keep a language model from assigning
zero probability to these unseen events, we’ll have to shave off a bit of probability
mass from some more frequent events and give it to the events we’ve never seen.
This modiﬁcation is called smoothing or discounting. In this section and the fol-
smoothing
discounting
lowing ones we’ll introduce a variety of ways to do smoothing: add-1 smoothing,
add-k smoothing, stupid backoff, and Kneser-Ney smoothing.
3.4.1
Laplace Smoothing
The simplest way to do smoothing is to add one to all the bigram counts, before
we normalize them into probabilities. All the counts that used to be zero will now
have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called
Laplace smoothing. Laplace smoothing does not perform well enough to be used
Laplace
smoothing
in modern n-gram models, but it usefully introduces many of the concepts that we
see in other smoothing algorithms, gives a useful baseline, and is also a practical
smoothing algorithm for other tasks like text classiﬁcation (Chapter 4).
Let’s start with the application of Laplace smoothing to unigram probabilities.
Recall that the unsmoothed maximum likelihood estimate of the unigram probability
of the word wi is its count ci normalized by the total number of word tokens N:
P(wi) = ci
N
Laplace smoothing merely adds one to each count (hence its alternate name add-
one smoothing). Since there are V words in the vocabulary and each one was incre-
add-one
mented, we also need to adjust the denominator to take into account the extra V
observations. (What happens to our P values if we don’t increase the denominator?)
PLaplace(wi) = ci +1
N +V
(3.20)
Instead of changing both the numerator and denominator, it is convenient to
describe how a smoothing algorithm affects the numerator, by deﬁning an adjusted
count c∗. This adjusted count is easier to compare directly with the MLE counts and
can be turned into a probability like an MLE count by normalizing by N. To deﬁne
this count, since we are only changing the numerator in addition to adding 1 we’ll
also need to multiply by a normalization factor
N
N+V :
c∗
i = (ci +1)
N
N +V
(3.21)
We can now turn c∗
i into a probability P∗
i by normalizing by N.
A related way to view smoothing is as discounting (lowering) some non-zero
discounting
counts in order to get the probability mass that will be assigned to the zero counts.
Thus, instead of referring to the discounted counts c∗, we might describe a smooth-
ing algorithm in terms of a relative discount dc, the ratio of the discounted counts to
discount
the original counts:

3.4
•
SMOOTHING
43
dc = c∗
c
Now that we have the intuition for the unigram case, let’s smooth our Berkeley
Restaurant Project bigrams. Figure 3.5 shows the add-one smoothed counts for the
bigrams in Fig. 3.1.
i
want
to
eat
chinese
food
lunch
spend
i
6
828
1
10
1
1
1
3
want
3
1
609
2
7
7
6
2
to
3
1
5
687
3
1
7
212
eat
1
1
3
1
17
3
43
1
chinese
2
1
1
1
1
83
2
1
food
16
1
16
1
2
5
1
1
lunch
3
1
1
1
1
2
1
1
spend
2
1
2
1
1
1
1
1
Figure 3.5
Add-one smoothed bigram counts for eight of the words (out of V = 1446) in
the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.
Figure 3.6 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2.
Recall that normal bigram probabilities are computed by normalizing each row of
counts by the unigram count:
P(wn|wn−1) = C(wn−1wn)
C(wn−1)
(3.22)
For add-one smoothed bigram counts, we need to augment the unigram count by
the number of total word types in the vocabulary V:
P∗
Laplace(wn|wn−1) =
C(wn−1wn)+1
P
w (C(wn−1w)+1) = C(wn−1wn)+1
C(wn−1)+V
(3.23)
Thus, each of the unigram counts given in the previous section will need to be
augmented by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.6.
i
want
to
eat
chinese
food
lunch
spend
i
0.0015
0.21
0.00025
0.0025
0.00025
0.00025
0.00025
0.00075
want
0.0013
0.00042
0.26
0.00084
0.0029
0.0029
0.0025
0.00084
to
0.00078
0.00026
0.0013
0.18
0.00078
0.00026
0.0018
0.055
eat
0.00046
0.00046
0.0014
0.00046
0.0078
0.0014
0.02
0.00046
chinese
0.0012
0.00062
0.00062
0.00062
0.00062
0.052
0.0012
0.00062
food
0.0063
0.00039
0.0063
0.00039
0.00079
0.002
0.00039
0.00039
lunch
0.0017
0.00056
0.00056
0.00056
0.00056
0.0011
0.00056
0.00056
spend
0.0012
0.00058
0.0012
0.00058
0.00058
0.00058
0.00058
0.00058
Figure 3.6
Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP
corpus of 9332 sentences. Previously-zero probabilities are in gray.
It is often convenient to reconstruct the count matrix so we can see how much a
smoothing algorithm has changed the original counts. These adjusted counts can be
computed by Eq. 3.24. Figure 3.7 shows the reconstructed counts.
c∗(wn−1wn) = [C(wn−1wn)+1]×C(wn−1)
C(wn−1)+V
(3.24)

44
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
i
want
to
eat
chinese
food
lunch
spend
i
3.8
527
0.64
6.4
0.64
0.64
0.64
1.9
want
1.2
0.39
238
0.78
2.7
2.7
2.3
0.78
to
1.9
0.63
3.1
430
1.9
0.63
4.4
133
eat
0.34
0.34
1
0.34
5.8
1
15
0.34
chinese
0.2
0.098
0.098
0.098
0.098
8.2
0.2
0.098
food
6.9
0.43
6.9
0.43
0.86
2.2
0.43
0.43
lunch
0.57
0.19
0.19
0.19
0.19
0.38
0.19
0.19
spend
0.32
0.16
0.32
0.16
0.16
0.16
0.16
0.16
Figure 3.7
Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus
of 9332 sentences. Previously-zero counts are in gray.
Note that add-one smoothing has made a very big change to the counts. C(want to)
changed from 609 to 238! We can see this in probability space as well: P(to|want)
decreases from .66 in the unsmoothed case to .26 in the smoothed case. Looking at
the discount d (the ratio between new and old counts) shows us how strikingly the
counts for each preﬁx word have been reduced; the discount for the bigram want to
is .39, while the discount for Chinese food is .10, a factor of 10!
The sharp change in counts and probabilities occurs because too much probabil-
ity mass is moved to all the zeros.
3.4.2
Add-k smoothing
One alternative to add-one smoothing is to move a bit less of the probability mass
from the seen to the unseen events. Instead of adding 1 to each count, we add a frac-
tional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.
add-k
P∗
Add-k(wn|wn−1) = C(wn−1wn)+k
C(wn−1)+kV
(3.25)
Add-k smoothing requires that we have a method for choosing k; this can be
done, for example, by optimizing on a devset. Although add-k is useful for some
tasks (including text classiﬁcation), it turns out that it still doesn’t work well for
language modeling, generating counts with poor variances and often inappropriate
discounts (Gale and Church, 1994).
3.4.3
Backoff and Interpolation
The discounting we have been discussing so far can help solve the problem of zero
frequency n-grams. But there is an additional source of knowledge we can draw on.
If we are trying to compute P(wn|wn−2wn−1) but we have no examples of a particular
trigram wn−2wn−1wn, we can instead estimate its probability by using the bigram
probability P(wn|wn−1). Similarly, if we don’t have counts to compute P(wn|wn−1),
we can look to the unigram P(wn).
In other words, sometimes using less context is a good thing, helping to general-
ize more for contexts that the model hasn’t learned much about. There are two ways
to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is
backoff
sufﬁcient, otherwise we use the bigram, otherwise the unigram. In other words, we
only “back off” to a lower-order n-gram if we have zero evidence for a higher-order
n-gram. By contrast, in interpolation, we always mix the probability estimates from
interpolation
all the n-gram estimators, weighing and combining the trigram, bigram, and unigram
counts.

3.4
•
SMOOTHING
45
In simple linear interpolation, we combine different order n-grams by linearly in-
terpolating all the models. Thus, we estimate the trigram probability P(wn|wn−2wn−1)
by mixing together the unigram, bigram, and trigram probabilities, each weighted
by a λ:
ˆP(wn|wn−2wn−1) = λ1P(wn|wn−2wn−1)
+λ2P(wn|wn−1)
+λ3P(wn)
(3.26)
such that the λs sum to 1:
X
i
λi = 1
(3.27)
In a slightly more sophisticated version of linear interpolation, each λ weight is
computed by conditioning on the context. This way, if we have particularly accurate
counts for a particular bigram, we assume that the counts of the trigrams based on
this bigram will be more trustworthy, so we can make the λs for those trigrams
higher and thus give that trigram more weight in the interpolation. Equation 3.28
shows the equation for interpolation with context-conditioned weights:
ˆP(wn|wn−2wn−1) = λ1(wn−1
n−2)P(wn|wn−2wn−1)
+λ2(wn−1
n−2)P(wn|wn−1)
+λ3(wn−1
n−2)P(wn)
(3.28)
How are these λ values set? Both the simple interpolation and conditional inter-
polation λs are learned from a held-out corpus. A held-out corpus is an additional
held-out
training corpus that we use to set hyperparameters like these λ values, by choosing
the λ values that maximize the likelihood of the held-out corpus. That is, we ﬁx
the n-gram probabilities and then search for the λ values that—when plugged into
Eq. 3.26—give us the highest probability of the held-out set. There are various ways
to ﬁnd this optimal set of λs. One way is to use the EM algorithm, an iterative
learning algorithm that converges on locally optimal λs (Jelinek and Mercer, 1980).
In a backoff n-gram model, if the n-gram we need has zero counts, we approxi-
mate it by backing off to the (N-1)-gram. We continue backing off until we reach a
history that has some counts.
In order for a backoff model to give a correct probability distribution, we have
to discount the higher-order n-grams to save some probability mass for the lower
discount
order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t
discounted and we just used the undiscounted MLE probability, then as soon as we
replaced an n-gram which has zero probability with a lower-order n-gram, we would
be adding probability mass, and the total probability assigned to all possible strings
by the language model would be greater than 1! In addition to this explicit discount
factor, we’ll need a function α to distribute this probability mass to the lower order
n-grams.
This kind of backoff with discounting is also called Katz backoff. In Katz back-
Katz backoff
off we rely on a discounted probability P∗if we’ve seen this n-gram before (i.e., if
we have non-zero counts). Otherwise, we recursively back off to the Katz probabil-
ity for the shorter-history (N-1)-gram. The probability for a backoff n-gram PBO is

46
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
thus computed as follows:
PBO(wn|wn−1
n−N+1) =



P∗(wn|wn−1
n−N+1),
if C(wn
n−N+1) > 0
α(wn−1
n−N+1)PBO(wn|wn−1
n−N+2),
otherwise.
(3.29)
Katz backoff is often combined with a smoothing method called Good-Turing.
Good-Turing
The combined Good-Turing backoff algorithm involves quite detailed computation
for estimating the Good-Turing smoothing and the P∗and α values.
3.5
Kneser-Ney Smoothing
One of the most commonly used and best performing n-gram smoothing methods
is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Good-
Kneser-Ney
man 1998).
Kneser-Ney has its roots in a method called absolute discounting. Recall that
discounting of the counts for frequent n-grams is necessary to save some probability
mass for the smoothing algorithm to distribute to the unseen n-grams.
To see this, we can use a clever idea from Church and Gale (1991). Consider
an n-gram that has count 4. We need to discount this count by some amount. But
how much should we discount it? Church and Gale’s clever idea was to look at a
held-out corpus and just see what the count is for all those bigrams that had count
4 in the training set. They computed a bigram grammar from 22 million words of
AP newswire and then checked the counts of each of these bigrams in another 22
million words. On average, a bigram that occurred 4 times in the ﬁrst 22 million
words occurred 3.23 times in the next 22 million words. The following table from
Church and Gale (1991) shows these counts for bigrams with c from 0 to 9:
Bigram count in Bigram count in
training set heldout set
0 0.0000270
1 0.448
2 1.25
3 2.24
4 3.23
5 4.21
6 5.23
7 6.21
8 7.21
9 8.26
Figure 3.8
For all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the
counts of these bigrams in a held-out corpus also of 22 million words.
The astute reader may have noticed that except for the held-out counts for 0
and 1, all the other bigram counts in the held-out set could be estimated pretty well
by just subtracting 0.75 from the count in the training set! Absolute discounting
Absolute
discounting
formalizes this intuition by subtracting a ﬁxed (absolute) discount d from each count.
The intuition is that since we have good estimates already for the very high counts, a
small discount d won’t affect them much. It will mainly modify the smaller counts,

3.5
•
KNESER-NEY SMOOTHING
47
for which we don’t necessarily trust the estimate anyway, and Fig. 3.8 suggests that
in practice this discount is actually a good one for bigrams with counts 2 through 9.
The equation for interpolated absolute discounting applied to bigrams:
PAbsoluteDiscounting(wi|wi−1) = C(wi−1wi)−d
P
vC(wi−1 v) +λ(wi−1)P(wi)
(3.30)
The ﬁrst term is the discounted bigram, and the second term is the unigram with
an interpolation weight λ. We could just set all the d values to .75, or we could keep
a separate discount value of 0.5 for the bigrams with counts of 1.
Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount-
ing with a more sophisticated way to handle the lower-order unigram distribution.
Consider the job of predicting the next word in this sentence, assuming we are inter-
polating a bigram and a unigram model.
I can’t see without my reading
.
The word glasses seems much more likely to follow here than, say, the word
Kong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is
more common, since Hong Kong is a very frequent word. A standard unigram model
will assign Kong a higher probability than glasses. We would like to capture the
intuition that although Kong is frequent, it is mainly only frequent in the phrase Hong
Kong, that is, after the word Hong. The word glasses has a much wider distribution.
In other words, instead of P(w), which answers the question “How likely is
w?”, we’d like to create a unigram model that we might call PCONTINUATION, which
answers the question “How likely is w to appear as a novel continuation?”. How can
we estimate this probability of seeing the word w as a novel continuation, in a new
unseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION
on the number of different contexts word w has appeared in, that is, the number of
bigram types it completes. Every bigram type was a novel continuation the ﬁrst time
it was seen. We hypothesize that words that have appeared in more contexts in the
past are more likely to appear in some new context as well. The number of times a
word w appears as a novel continuation can be expressed as:
PCONTINUATION(w) ∝|{v : C(vw) > 0}|
(3.31)
To turn this count into a probability, we normalize by the total number of word
bigram types. In summary:
PCONTINUATION(w) =
|{v : C(vw) > 0}|
|{(u′,w′) : C(u′w′) > 0}|
(3.32)
An equivalent formulation based on a different metaphor is to use the number of
word types seen to precede w (Eq. 3.31 repeated):
PCONTINUATION(w) ∝|{v : C(vw) > 0}|
(3.33)
normalized by the number of words preceding all words, as follows:
PCONTINUATION(w) =
|{v : C(vw) > 0}|
P
w′ |{v : C(vw′) > 0}|
(3.34)
A frequent word (Kong) occurring in only one context (Hong) will have a low
continuation probability.

48
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
The ﬁnal equation for Interpolated Kneser-Ney smoothing for bigrams is then:
Interpolated
Kneser-Ney
PKN(wi|wi−1) = max(C(wi−1wi)−d,0)
C(wi−1)
+λ(wi−1)PCONTINUATION(wi)
(3.35)
The λ is a normalizing constant that is used to distribute the probability mass
we’ve discounted.:
λ(wi−1) =
d
P
vC(wi−1v)|{w : C(wi−1w) > 0}|
(3.36)
The ﬁrst term,
d
P
vC(wi−1v), is the normalized discount. The second term,
|{w : C(wi−1w) > 0}|, is the number of word types that can follow wi−1 or, equiva-
lently, the number of word types that we discounted; in other words, the number of
times we applied the normalized discount.
The general recursive formulation is as follows:
PKN(wi|wi−1
i−n+1) = max(cKN(wi
i−n+1)−d,0)
P
v cKN(wi−1
i−n+1v)
+λ(wi−1
i−n+1)PKN(wi|wi−1
i−n+2) (3.37)
where the deﬁnition of the count cKN depends on whether we are counting the
highest-order n-gram being interpolated (for example trigram if we are interpolating
trigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram
if we are interpolating trigram, bigram, and unigram):
cKN(·) =
 count(·)
for the highest order
continuationcount(·)
for lower orders
(3.38)
The continuation count is the number of unique single word contexts for ·.
At the termination of the recursion, unigrams are interpolated with the uniform
distribution, where the parameter ϵ is the empty string:
PKN(w) = max(cKN(w)−d,0)
P
w′ cKN(w′)
+λ(ϵ) 1
V
(3.39)
If we want to include an unknown word <UNK>, it’s just included as a regular vo-
cabulary entry with count zero, and hence its probability will be a lambda-weighted
uniform distribution λ(ϵ)
V .
The best-performing version of Kneser-Ney smoothing is called modiﬁed Kneser-
Ney smoothing, and is due to Chen and Goodman (1998). Rather than use a single
modiﬁed
Kneser-Ney
ﬁxed discount d, modiﬁed Kneser-Ney uses three different discounts d1, d2, and
d3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and
Goodman (1998, p. 19) or Heaﬁeld et al. (2013) for the details.
3.6
The Web and Stupid Backoff
By using text from the web, it is possible to build extremely large language mod-
els. In 2006 Google released a very large set of n-gram counts, including n-grams
(1-grams through 5-grams) from all the ﬁve-word sequences that appear at least

3.7
•
ADVANCED: PERPLEXITY’S RELATION TO ENTROPY
49
40 times from 1,024,908,267,229 words of running text on the web; this includes
1,176,470,663 ﬁve-word sequences using over 13 million unique words types (Franz
and Brants, 2006). Some examples:
4-gram
Count
serve as the incoming
92
serve as the incubator
99
serve as the independent
794
serve as the index
223
serve as the indication
72
serve as the indicator
120
serve as the indicators
45
serve as the indispensable
111
serve as the indispensible
40
serve as the individual
234
Efﬁciency considerations are important when building language models that use
such large sets of n-grams. Rather than store each word as a string, it is generally
represented in memory as a 64-bit hash number, with the words themselves stored
on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte
ﬂoats), and n-grams are stored in reverse tries.
N-grams can also be shrunk by pruning, for example only storing n-grams with
counts greater than some threshold (such as the count threshold of 40 used for the
Google n-gram release) or using entropy to prune less-important n-grams (Stolcke,
1998). Another option is to build approximate language models using techniques
like Bloom ﬁlters (Talbot and Osborne 2007, Church et al. 2007). Finally, efﬁ-
Bloom ﬁlters
cient language model toolkits like KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013) use
sorted arrays, efﬁciently combine probabilities and backoffs in a single value, and
use merge sorts to efﬁciently build the probability tables in a minimal number of
passes through a large corpus.
Although with these toolkits it is possible to build web-scale language models
using full Kneser-Ney smoothing, Brants et al. (2007) show that with very large lan-
guage models a much simpler algorithm may be sufﬁcient. The algorithm is called
stupid backoff. Stupid backoff gives up the idea of trying to make the language
stupid backoff
model a true probability distribution. There is no discounting of the higher-order
probabilities. If a higher-order n-gram has a zero count, we simply backoff to a
lower order n-gram, weighed by a ﬁxed (context-independent) weight. This algo-
rithm does not produce a probability distribution, so we’ll follow Brants et al. (2007)
in referring to it as S:
S(wi|wi−1
i−k+1) =



count(wi
i−k+1)
count(wi−1
i−k+1)
if count(wi
i−k+1) > 0
λS(wi|wi−1
i−k+2)
otherwise
(3.40)
The backoff terminates in the unigram, which has probability S(w) = count(w)
N
. Brants
et al. (2007) ﬁnd that a value of 0.4 worked well for λ.
3.7
Advanced: Perplexity’s Relation to Entropy
We introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on
a test set. A better n-gram model is one that assigns a higher probability to the

50
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
test data, and perplexity is a normalized version of the probability of the test set.
The perplexity measure actually arises from the information-theoretic concept of
cross-entropy, which explains otherwise mysterious properties of perplexity (why
the inverse probability, for example?) and its relationship to entropy. Entropy is a
Entropy
measure of information. Given a random variable X ranging over whatever we are
predicting (words, letters, parts of speech, the set of which we’ll call χ) and with a
particular probability function, call it p(x), the entropy of the random variable X is:
H(X) = −
X
x∈χ
p(x)log2 p(x)
(3.41)
The log can, in principle, be computed in any base. If we use log base 2, the
resulting value of entropy will be measured in bits.
One intuitive way to think about entropy is as a lower bound on the number of
bits it would take to encode a certain decision or piece of information in the optimal
coding scheme.
Consider an example from the standard information theory textbook Cover and
Thomas (1991). Imagine that we want to place a bet on a horse race but it is too
far to go all the way to Yonkers Racetrack, so we’d like to send a short message to
the bookie to tell him which of the eight horses to bet on. One way to encode this
message is just to use the binary representation of the horse’s number as the code;
thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded
as 000. If we spend the whole day betting and each horse is coded with 3 bits, on
average we would be sending 3 bits per race.
Can we do better? Suppose that the spread is the actual distribution of the bets
placed and that we represent it as the prior probability of each horse as follows:
Horse 1
1
2
Horse 5
1
64
Horse 2
1
4
Horse 6
1
64
Horse 3
1
8
Horse 7
1
64
Horse 4
1
16
Horse 8
1
64
The entropy of the random variable X that ranges over horses gives us a lower
bound on the number of bits and is
H(X) = −
i=8
X
i=1
p(i)log p(i)
=
−1
2 log 1
2 −1
4 log 1
4 −1
8 log 1
8 −1
16 log 1
16 −4( 1
64 log 1
64 )
= 2 bits
(3.42)
A code that averages 2 bits per race can be built with short encodings for more
probable horses, and longer encodings for less probable horses. For example, we
could encode the most likely horse with the code 0, and the remaining horses as 10,
then 110, 1110, 111100, 111101, 111110, and 111111.
What if the horses are equally likely? We saw above that if we used an equal-
length binary code for the horse numbers, each horse took 3 bits to code, so the
average was 3. Is the entropy the same? In this case each horse would have a
probability of 1
8. The entropy of the choice of horses is then
H(X) = −
i=8
X
i=1
1
8 log 1
8 = −log 1
8 = 3 bits
(3.43)

3.7
•
ADVANCED: PERPLEXITY’S RELATION TO ENTROPY
51
Until now we have been computing the entropy of a single variable. But most of
what we will use entropy for involves sequences. For a grammar, for example, we
will be computing the entropy of some sequence of words W = {w0,w1,w2,...,wn}.
One way to do this is to have a variable that ranges over sequences of words. For
example we can compute the entropy of a random variable that ranges over all ﬁnite
sequences of words of length n in some language L as follows:
H(w1,w2,...,wn) = −
X
W n
1 ∈L
p(W n
1 )log p(W n
1 )
(3.44)
We could deﬁne the entropy rate (we could also think of this as the per-word
entropy rate
entropy) as the entropy of this sequence divided by the number of words:
1
nH(W n
1 ) = −1
n
X
W n
1 ∈L
p(W n
1 )log p(W n
1 )
(3.45)
But to measure the true entropy of a language, we need to consider sequences of
inﬁnite length. If we think of a language as a stochastic process L that produces a
sequence of words, and allow W to represent the sequence of words w1,...,wn, then
L’s entropy rate H(L) is deﬁned as
H(L) = lim
n→∞
1
nH(w1,w2,...,wn)
= −lim
n→∞
1
n
X
W∈L
p(w1,...,wn)log p(w1,...,wn)
(3.46)
The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and
Thomas 1991) states that if the language is regular in certain ways (to be exact, if it
is both stationary and ergodic),
H(L) = lim
n→∞−1
n log p(w1w2 ...wn)
(3.47)
That is, we can take a single sequence that is long enough instead of summing
over all possible sequences. The intuition of the Shannon-McMillan-Breiman the-
orem is that a long-enough sequence of words will contain in it many other shorter
sequences and that each of these shorter sequences will reoccur in the longer se-
quence according to their probabilities.
A stochastic process is said to be stationary if the probabilities it assigns to a
Stationary
sequence are invariant with respect to shifts in the time index. In other words, the
probability distribution for words at time t is the same as the probability distribution
at time t + 1. Markov models, and hence n-grams, are stationary. For example, in
a bigram, Pi is dependent only on Pi−1. So if we shift our time index by x, Pi+x is
still dependent on Pi+x−1. But natural language is not stationary, since as we show
in Chapter 12, the probability of upcoming words can be dependent on events that
were arbitrarily distant and time dependent. Thus, our statistical models only give
an approximation to the correct distributions and entropies of natural language.
To summarize, by making some incorrect but convenient simplifying assump-
tions, we can compute the entropy of some stochastic process by taking a very long
sample of the output and computing its average log probability.
Now we are ready to introduce cross-entropy. The cross-entropy is useful when
cross-entropy
we don’t know the actual probability distribution p that generated some data. It

52
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
allows us to use some m, which is a model of p (i.e., an approximation to p). The
cross-entropy of m on p is deﬁned by
H(p,m) = lim
n→∞−1
n
X
W∈L
p(w1,...,wn)logm(w1,...,wn)
(3.48)
That is, we draw sequences according to the probability distribution p, but sum
the log of their probabilities according to m.
Again, following the Shannon-McMillan-Breiman theorem, for a stationary er-
godic process:
H(p,m) = lim
n→∞−1
n logm(w1w2 ...wn)
(3.49)
This means that, as for entropy, we can estimate the cross-entropy of a model
m on some distribution p by taking a single sequence that is long enough instead of
summing over all possible sequences.
What makes the cross-entropy useful is that the cross-entropy H(p,m) is an up-
per bound on the entropy H(p). For any model m:
H(p) ≤H(p,m)
(3.50)
This means that we can use some simpliﬁed model m to help estimate the true en-
tropy of a sequence of symbols drawn according to probability p. The more accurate
m is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus,
the difference between H(p,m) and H(p) is a measure of how accurate a model is.
Between two models m1 and m2, the more accurate model will be the one with the
lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so
a model cannot err by underestimating the true entropy.)
We are ﬁnally ready to see the relation between perplexity and cross-entropy as
we saw it in Eq. 3.49. Cross-entropy is deﬁned in the limit, as the length of the
observed word sequence goes to inﬁnity. We will need an approximation to cross-
entropy, relying on a (sufﬁciently long) sequence of ﬁxed length. This approxima-
tion to the cross-entropy of a model M = P(wi|wi−N+1...wi−1) on a sequence of
words W is
H(W) = −1
N logP(w1w2 ...wN)
(3.51)
The perplexity of a model P on a sequence of words W is now formally deﬁned as
perplexity
the exp of this cross-entropy:
Perplexity(W) = 2H(W)
= P(w1w2 ...wN)−1
N
=
N
s
1
P(w1w2 ...wN)
=
N
v
u
u
t
N
Y
i=1
1
P(wi|w1 ...wi−1)
(3.52)

3.8
•
SUMMARY
53
3.8
Summary
This chapter introduced language modeling and the n-gram, one of the most widely
used tools in language processing.
• Language models offer a way to assign a probability to a sentence or other
sequence of words, and to predict a word from preceding words.
• n-grams are Markov models that estimate words from a ﬁxed window of pre-
vious words. n-gram probabilities can be estimated by counting in a corpus
and normalizing (the maximum likelihood estimate).
• n-gram language models are evaluated extrinsically in some task, or intrinsi-
cally using perplexity.
• The perplexity of a test set according to a language model is the geometric
mean of the inverse test set probability computed by the model.
• Smoothing algorithms provide a more sophisticated way to estimate the prob-
ability of n-grams. Commonly used smoothing algorithms for n-grams rely on
lower-order n-gram counts through backoff or interpolation.
• Both backoff and interpolation require discounting to create a probability dis-
tribution.
• Kneser-Ney smoothing makes use of the probability of a word being a novel
continuation. The interpolated Kneser-Ney smoothing algorithm mixes a
discounted probability with a lower-order continuation probability.
Bibliographical and Historical Notes
The underlying mathematics of the n-gram was ﬁrst proposed by Markov (1913),
who used what are now called Markov chains (bigrams and trigrams) to predict
whether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a con-
sonant. Markov classiﬁed 20,000 letters as V or C and computed the bigram and
trigram probability that a given letter would be a vowel given the previous one or
two letters. Shannon (1948) applied n-grams to compute approximations to English
word sequences. Based on Shannon’s work, Markov models were commonly used in
engineering, linguistic, and psychological work on modeling word sequences by the
1950s. In a series of extremely inﬂuential papers starting with Chomsky (1956) and
including Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued
that “ﬁnite-state Markov processes”, while a possibly useful engineering heuristic,
were incapable of being a complete cognitive model of human grammatical knowl-
edge. These arguments led many linguists and computational linguists to ignore
work in statistical modeling for decades.
The resurgence of n-gram models came from Jelinek and colleagues at the IBM
Thomas J. Watson Research Center, who were inﬂuenced by Shannon, and Baker
at CMU, who was inﬂuenced by the work of Baum and colleagues. Independently
these two labs successfully used n-grams in their speech recognition systems (Baker 1975b,
Jelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). A trigram model was
used in the IBM TANGORA speech recognition system in the 1970s, but the idea
was not written up until later.
Add-one smoothing derives from Laplace’s 1812 law of succession and was ﬁrst
applied as an engineering solution to the zero-frequency problem by Jeffreys (1948)

54
CHAPTER 3
•
N-GRAM LANGUAGE MODELS
based on an earlier Add-K suggestion by Johnson (1932). Problems with the add-
one algorithm are summarized in Gale and Church (1994).
A wide variety of different language modeling and smoothing techniques were
proposed in the 80s and 90s, including Good-Turing discounting—ﬁrst applied to
the n-gram smoothing at IBM by Katz (N´adas 1984, Church and Gale 1991)—
Witten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n-
gram models that used information about word classes.
class-based
n-gram
Starting in the late 1990s, Chen and Goodman produced a highly inﬂuential
series of papers with a comparison of different language models (Chen and Good-
man 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).
They performed a number of carefully controlled experiments comparing differ-
ent discounting algorithms, cache models, class-based models, and other language
model parameters. They showed the advantages of Modiﬁed Interpolated Kneser-
Ney, which has since become the standard baseline for language modeling, espe-
cially because they showed that caches and class-based models provided only minor
additional improvement. These papers are recommended for any reader with further
interest in language modeling.
Two commonly used toolkits for building language models are SRILM (Stolcke,
2002) and KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013). Both are publicly available.
SRILM offers a wider range of options and types of discounting, while KenLM is
optimized for speed and memory size, making it possible to build web-scale lan-
guage models.
The highest accuracy language models are neural network language models.
These solve a major problem with n-gram language models: the number of parame-
ters increases exponentially as the n-gram order increases, and n-grams have no way
to generalize from training to test set. Neural language models instead project words
into a continuous space in which words with similar contexts have similar represen-
tations. We’ll introduce both feedforward language models (Bengio et al. 2006,
Schwenk 2007) in Chapter 7, and recurrent language models (Mikolov, 2012) in
Chapter 9.
Exercises
3.1
Write out the equation for trigram probability estimation (modifying Eq. 3.11).
Now write out all the non-zero trigram probabilities for the I am Sam corpus
on page 33.
3.2
Calculate the probability of the sentence i want chinese food. Give two
probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on
page 35, and another using the add-1 smoothed table in Fig. 3.6. Assume the
additional add-1 smoothed probabilities P(i|<s>) = 0.19 and P(</s>|food) =
0.40.
3.3
Which of the two probabilities you computed in the previous exercise is higher,
unsmoothed or smoothed? Explain why.
3.4
We are given the following corpus, modiﬁed from the one in the chapter:
<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>

EXERCISES
55
Using a bigram language model with add-one smoothing, what is P(Sam |
am)? Include <s> and </s> in your counts just like any other token.
3.5
Suppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram
grammar on the following training corpus without using the end-symbol </s>:
<s> a b
<s> b b
<s> b a
<s> a a
Demonstrate that your bigram model does not assign a single probability dis-
tribution across all sentence lengths by showing that the sum of the probability
of the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the
sum of the probability of all possible 3 word sentences over the alphabet {a,b}
is also 1.0.
3.6
Suppose we train a trigram language model with add-one smoothing on a
given corpus. The corpus contains V word types. Express a formula for esti-
mating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),
in terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to
denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and
so on for bigrams and unigrams.
3.7
We are given the following corpus, modiﬁed from the one in the chapter:
<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>
If we use linear interpolation smoothing between a maximum-likelihood bi-
gram model and a maximum-likelihood unigram model with λ1 = 1
2 and λ2 =
1
2, what is P(Sam|am)? Include <s> and </s> in your counts just like any
other token.
3.8
Write a program to compute unsmoothed unigrams and bigrams.
3.9
Run your n-gram program on two different small corpora of your choice (you
might use email text or newsgroups). Now compare the statistics of the two
corpora. What are the differences in the most common unigrams between the
two? How about interesting differences in bigrams?
3.10 Add an option to your program to generate random sentences.
3.11 Add an option to your program to compute the perplexity of a test set.
3.12 Given a training set of 100 numbers consists of 91 zeros and 1 each of the
other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What
is the unigram perplexity?

56
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
CHAPTER
4
Naive Bayes and Sentiment
Classiﬁcation
Classiﬁcation lies at the heart of both human and machine intelligence. Deciding
what letter, word, or image has been presented to our senses, recognizing faces
or voices, sorting mail, assigning grades to homeworks; these are all examples of
assigning a category to an input. The potential challenges of this task are highlighted
by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:
(a) those that belong to the Emperor, (b) embalmed ones, (c) those that
are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray
dogs, (h) those that are included in this classiﬁcation, (i) those that
tremble as if they were mad, (j) innumerable ones, (k) those drawn with
a very ﬁne camel’s hair brush, (l) others, (m) those that have just broken
a ﬂower vase, (n) those that resemble ﬂies from a distance.
Many language processing tasks involve classiﬁcation, although luckily our classes
are much easier to deﬁne than those of Borges. In this chapter we introduce the naive
Bayes algorithm and apply it to text categorization, the task of assigning a label or
text
categorization
category to an entire text or document.
We focus on one common text categorization task, sentiment analysis, the ex-
sentiment
analysis
traction of sentiment, the positive or negative orientation that a writer expresses
toward some object. A review of a movie, book, or product on the web expresses the
author’s sentiment toward the product, while an editorial or political text expresses
sentiment toward a candidate or political action. Extracting consumer or public sen-
timent is thus relevant for ﬁelds from marketing to politics.
The simplest version of sentiment analysis is a binary classiﬁcation task, and
the words of the review provide excellent cues. Consider, for example, the follow-
ing phrases extracted from positive and negative reviews of movies and restaurants.
Words like great, richly, awesome, and pathetic, and awful and ridiculously are very
informative cues:
+ ...zany characters and richly applied satire, and some great plot twists
−It was pathetic. The worst part about it was the boxing scenes...
+ ...awesome caramel sauce and sweet toasty almonds. I love this place!
−...awful pizza and ridiculously overpriced...
Spam detection is another important commercial application, the binary clas-
spam detection
siﬁcation task of assigning an email to one of the two classes spam or not-spam.
Many lexical and other features can be used to perform this classiﬁcation. For ex-
ample you might quite reasonably be suspicious of an email containing phrases like
“online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.
Another thing we might want to know about a text is the language it’s written
in. Texts on social media, for example, can be in any number of languages and we’ll
need to apply different processing. The task of language id is thus the ﬁrst step
language id
in most language processing pipelines. Related tasks like determining a text’s au-
thor, (authorship attribution), or author characteristics like gender, age, and native
authorship
attribution

57
language are text classiﬁcation tasks that are also relevant to the digital humanities,
social sciences, and forensic linguistics.
Finally, one of the oldest tasks in text classiﬁcation is assigning a library sub-
ject category or topic label to a text. Deciding whether a research paper concerns
epidemiology or instead, perhaps, embryology, is an important component of infor-
mation retrieval. Various sets of subject categories exist, such as the MeSH (Medical
Subject Headings) thesaurus. In fact, as we will see, subject category classiﬁcation
is the task for which the naive Bayes algorithm was invented in 1961.
Classiﬁcation is essential for tasks below the level of the document as well.
We’ve already seen period disambiguation (deciding if a period is the end of a sen-
tence or part of a word), and word tokenization (deciding if a character should be
a word boundary). Even language modeling can be viewed as classiﬁcation: each
word can be thought of as a class, and so predicting the next word is classifying the
context-so-far into a class for each next word. A part-of-speech tagger (Chapter 8)
classiﬁes each occurrence of a word in a sentence as, e.g., a noun or a verb.
The goal of classiﬁcation is to take a single observation, extract some useful
features, and thereby classify the observation into one of a set of discrete classes.
One method for classifying text is to use handwritten rules. There are many areas of
language processing where handwritten rule-based classiﬁers constitute a state-of-
the-art system, or at least part of it.
Rules can be fragile, however, as situations or data change over time, and for
some tasks humans aren’t necessarily good at coming up with the rules. Most cases
of classiﬁcation in language processing are instead done via supervised machine
learning, and this will be the subject of the remainder of this chapter. In supervised
supervised
machine
learning
learning, we have a data set of input observations, each associated with some correct
output (a ‘supervision signal’). The goal of the algorithm is to learn how to map
from a new observation to a correct output.
Formally, the task of supervised classiﬁcation is to take an input x and a ﬁxed
set of output classes Y = y1,y2,...,yM and return a predicted class y ∈Y. For text
classiﬁcation, we’ll sometimes talk about c (for “class”) instead of y as our output
variable, and d (for “document”) instead of x as our input variable. In the supervised
situation we have a training set of N documents that have each been hand-labeled
with a class: (d1,c1),....,(dN,cN). Our goal is to learn a classiﬁer that is capable of
mapping from a new document d to its correct class c ∈C. A probabilistic classiﬁer
additionally will tell us the probability of the observation being in the class. This
full distribution over the classes can be useful information for downstream decisions;
avoiding making discrete decisions early on can be useful when combining systems.
Many kinds of machine learning algorithms are used to build classiﬁers. This
chapter introduces naive Bayes; the following one introduces logistic regression.
These exemplify two ways of doing classiﬁcation. Generative classiﬁers like naive
Bayes build a model of how a class could generate some input data. Given an ob-
servation, they return the class most likely to have generated the observation. Dis-
criminative classiﬁers like logistic regression instead learn what features from the
input are most useful to discriminate between the different possible classes. While
discriminative systems are often more accurate and hence more commonly used,
generative classiﬁers still have a role.

58
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
4.1
Naive Bayes Classiﬁers
In this section we introduce the multinomial naive Bayes classiﬁer, so called be-
naive Bayes
classiﬁer
cause it is a Bayesian classiﬁer that makes a simplifying (naive) assumption about
how the features interact.
The intuition of the classiﬁer is shown in Fig. 4.1. We represent a text document
as if it were a bag-of-words, that is, an unordered set of words with their position
bag-of-words
ignored, keeping only their frequency in the document. In the example in the ﬁgure,
instead of representing the word order in all the phrases like “I love this movie” and
“I would recommend it”, we simply note that the word I occurred 5 times in the
entire excerpt, the word it 6 times, the words love, recommend, and movie once, and
so on.
it
it
it
it
it
it
I
I
I
I
I
love
recommend
movie
the
the
the
the
to
to
to
and
and
and
seen
seen
yet
would
with
who
whimsical
while
whenever
times
sweet
several
scenes
satirical
romantic
of
manages
humor
have
happy
fun
friend
fairy
dialogue
but
conventions
areanyone
adventure
always
again
about
I love this movie! It's sweet, 
but with satirical humor. The 
dialogue is great and the 
adventure scenes are fun... 
It manages to be whimsical 
and romantic while laughing 
at the conventions of the 
fairy tale genre. I would 
recommend it to just about 
anyone. I've seen it several 
times, and I'm always happy 
to see it again whenever I 
have a friend who hasn't 
seen it yet!
it 
I
the
to
and
seen
yet
would
whimsical
times
sweet
satirical
adventure
genre
fairy
humor
have
great
…
6 
5
4
3
3
2
1
1
1
1
1
1
1
1
1
1
1
1
…
Figure 4.1
Intuition of the multinomial naive Bayes classiﬁer applied to a movie review. The position of the
words is ignored (the bag of words assumption) and we make use of the frequency of each word.
Naive Bayes is a probabilistic classiﬁer, meaning that for a document d, out of
all classes c ∈C the classiﬁer returns the class ˆc which has the maximum posterior
probability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our
ˆ
estimate of the correct class”.
ˆc = argmax
c∈C
P(c|d)
(4.1)
This idea of Bayesian inference has been known since the work of Bayes (1763),
Bayesian
inference
and was ﬁrst applied to text classiﬁcation by Mosteller and Wallace (1964). The
intuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. 4.1 into
other probabilities that have some useful properties. Bayes’ rule is presented in
Eq. 4.2; it gives us a way to break down any conditional probability P(x|y) into
three other probabilities:
P(x|y) = P(y|x)P(x)
P(y)
(4.2)

4.1
•
NAIVE BAYES CLASSIFIERS
59
We can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:
ˆc = argmax
c∈C
P(c|d) = argmax
c∈C
P(d|c)P(c)
P(d)
(4.3)
We can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This
is possible because we will be computing P(d|c)P(c)
P(d)
for each possible class. But P(d)
doesn’t change for each class; we are always asking about the most likely class for
the same document d, which must have the same probability P(d). Thus, we can
choose the class that maximizes this simpler formula:
ˆc = argmax
c∈C
P(c|d) = argmax
c∈C
P(d|c)P(c)
(4.4)
We thus compute the most probable class ˆc given some document d by choosing
the class which has the highest product of two probabilities: the prior probability
prior
probability
of the class P(c) and the likelihood of the document P(d|c):
likelihood
ˆc = argmax
c∈C
likelihood
z }| {
P(d|c)
prior
z}|{
P(c)
(4.5)
Without loss of generalization, we can represent a document d as a set of features
f1, f2,..., fn:
ˆc = argmax
c∈C
likelihood
z
}|
{
P(f1, f2,...., fn|c)
prior
z}|{
P(c)
(4.6)
Unfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-
plifying assumptions, estimating the probability of every possible combination of
features (for example, every possible set of words and positions) would require huge
numbers of parameters and impossibly large training sets. Naive Bayes classiﬁers
therefore make two simplifying assumptions.
The ﬁrst is the bag of words assumption discussed intuitively above: we assume
position doesn’t matter, and that the word “love” has the same effect on classiﬁcation
whether it occurs as the 1st, 20th, or last word in the document. Thus we assume
that the features f1, f2,..., fn only encode word identity and not position.
The second is commonly called the naive Bayes assumption: this is the condi-
naive Bayes
assumption
tional independence assumption that the probabilities P(fi|c) are independent given
the class c and hence can be ‘naively’ multiplied as follows:
P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)·...·P(fn|c)
(4.7)
The ﬁnal equation for the class chosen by a naive Bayes classiﬁer is thus:
cNB = argmax
c∈C
P(c)
Y
f∈F
P(f|c)
(4.8)
To apply the naive Bayes classiﬁer to text, we need to consider word positions, by
simply walking an index through every word position in the document:
positions ←all word positions in test document
cNB = argmax
c∈C
P(c)
Y
i∈positions
P(wi|c)
(4.9)

60
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
Naive Bayes calculations, like calculations for language modeling, are done in log
space, to avoid underﬂow and increase speed. Thus Eq. 4.9 is generally instead
expressed as
cNB = argmax
c∈C
logP(c)+
X
i∈positions
logP(wi|c)
(4.10)
By considering features in log space, Eq. 4.10 computes the predicted class as a lin-
ear function of input features. Classiﬁers that use a linear combination of the inputs
to make a classiﬁcation decision —like naive Bayes and also logistic regression—
are called linear classiﬁers.
linear
classiﬁers
4.2
Training the Naive Bayes Classiﬁer
How can we learn the probabilities P(c) and P(fi|c)? Let’s ﬁrst consider the max-
imum likelihood estimate. We’ll simply use the frequencies in the data. For the
document prior P(c) we ask what percentage of the documents in our training set
are in each class c. Let Nc be the number of documents in our training data with
class c and Ndoc be the total number of documents. Then:
ˆP(c) = Nc
Ndoc
(4.11)
To learn the probability P(fi|c), we’ll assume a feature is just the existence of a word
in the document’s bag of words, and so we’ll want P(wi|c), which we compute as
the fraction of times the word wi appears among all words in all documents of topic
c. We ﬁrst concatenate all documents with category c into one big “category c” text.
Then we use the frequency of wi in this concatenated document to give a maximum
likelihood estimate of the probability:
ˆP(wi|c) =
count(wi,c)
P
w∈V count(w,c)
(4.12)
Here the vocabulary V consists of the union of all the word types in all classes, not
just the words in one class c.
There is a problem, however, with maximum likelihood training. Imagine we
are trying to estimate the likelihood of the word “fantastic” given class positive, but
suppose there are no training documents that both contain the word “fantastic” and
are classiﬁed as positive. Perhaps the word “fantastic” happens to occur (sarcasti-
cally?) in the class negative. In such a case the probability for this feature will be
zero:
ˆP(“fantastic”|positive) = count(“fantastic”,positive)
P
w∈V count(w,positive)
= 0
(4.13)
But since naive Bayes naively multiplies all the feature likelihoods together, zero
probabilities in the likelihood term for any class will cause the probability of the
class to be zero, no matter the other evidence!
The simplest solution is the add-one (Laplace) smoothing introduced in Chap-
ter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing

4.2
•
TRAINING THE NAIVE BAYES CLASSIFIER
61
algorithms in language modeling, it is commonly used in naive Bayes text catego-
rization:
ˆP(wi|c) =
count(wi,c)+1
P
w∈V (count(w,c)+1) =
count(wi,c)+1
 P
w∈V count(w,c)

+|V|
(4.14)
Note once again that it is crucial that the vocabulary V consists of the union of all the
word types in all classes, not just the words in one class c (try to convince yourself
why this must be true; see the exercise at the end of the chapter).
What do we do about words that occur in our test data but are not in our vocab-
ulary at all because they did not occur in any training document in any class? The
solution for such unknown words is to ignore them—remove them from the test
unknown word
document and not include any probability for them at all.
Finally, some systems choose to completely ignore another class of words: stop
words, very frequent words like the and a. This can be done by sorting the vocabu-
stop words
lary by frequency in the training set, and deﬁning the top 10–100 vocabulary entries
as stop words, or alternatively by using one of the many pre-deﬁned stop word list
available online. Then every instance of these stop words are simply removed from
both training and test documents as if they had never occurred. In most text classi-
ﬁcation applications, however, using a stop word list doesn’t improve performance,
and so it is more common to make use of the entire vocabulary and not use a stop
word list.
Fig. 4.2 shows the ﬁnal algorithm.
function TRAIN NAIVE BAYES(D, C) returns log P(c) and log P(w|c)
for each class c ∈C
# Calculate P(c) terms
Ndoc = number of documents in D
Nc = number of documents from D in class c
logprior[c]←log Nc
Ndoc
V←vocabulary of D
bigdoc[c]←append(d) for d ∈D with class c
for each word w in V
# Calculate P(w|c) terms
count(w,c)←# of occurrences of w in bigdoc[c]
loglikelihood[w,c]←log
count(w,c) + 1
P
w′ in V (count (w′,c) + 1)
return logprior, loglikelihood, V
function TEST NAIVE BAYES(testdoc,logprior, loglikelihood, C, V) returns best c
for each class c ∈C
sum[c]←logprior[c]
for each position i in testdoc
word←testdoc[i]
if word ∈V
sum[c]←sum[c]+ loglikelihood[word,c]
return argmaxc sum[c]
Figure 4.2
The naive Bayes algorithm, using add-1 smoothing. To use add-α smoothing
instead, change the +1 to +α for loglikelihood counts in training.

62
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
4.3
Worked example
Let’s walk through an example of training and testing naive Bayes with add-one
smoothing. We’ll use a sentiment analysis domain with the two classes positive
(+) and negative (-), and take the following miniature training and test documents
simpliﬁed from actual movie reviews.
Cat
Documents
Training -
just plain boring
-
entirely predictable and lacks energy
-
no surprises and very few laughs
+
very powerful
+
the most fun ﬁlm of the summer
Test
?
predictable with no fun
The prior P(c) for the two classes is computed via Eq. 4.11 as
Nc
Ndoc :
P(−) = 3
5
P(+) = 2
5
The word with doesn’t occur in the training set, so we drop it completely (as
mentioned above, we don’t use unknown word models for naive Bayes). The like-
lihoods from the training set for the remaining three words “predictable”, “no”, and
“fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder
of the words in the training set is left as an exercise for the reader):
P(“predictable”|−) =
1+1
14+20
P(“predictable”|+) = 0+1
9+20
P(“no”|−) =
1+1
14+20
P(“no”|+) = 0+1
9+20
P(“fun”|−) =
0+1
14+20
P(“fun”|+) = 1+1
9+20
For the test sentence S = “predictable with no fun”, after removing the word ‘with’,
the chosen class, via Eq. 4.9, is therefore computed as follows:
P(−)P(S|−) = 3
5 × 2×2×1
343
= 6.1×10−5
P(+)P(S|+) = 2
5 × 1×1×2
293
= 3.2×10−5
The model thus predicts the class negative for the test sentence.
4.4
Optimizing for Sentiment Analysis
While standard naive Bayes text classiﬁcation can work well for sentiment analysis,
some small changes are generally employed that improve performance.
First, for sentiment classiﬁcation and a number of other text classiﬁcation tasks,
whether a word occurs or not seems to matter more than its frequency. Thus it
often improves performance to clip the word counts in each document at 1 (see
the end of the chapter for pointers to these results). This variant is called binary

4.4
•
OPTIMIZING FOR SENTIMENT ANALYSIS
63
multinomial naive Bayes or binary NB. The variant uses the same Eq. 4.10 except
binary NB
that for each document we remove all duplicate words before concatenating them
into the single big document. Fig. 4.3 shows an example in which a set of four
documents (shortened and text-normalized for this example) are remapped to binary,
with the modiﬁed counts shown in the table on the right. The example is worked
without add-1 smoothing to make the differences clearer. Note that the results counts
need not be 1; the word great has a count of 2 even for Binary NB, because it appears
in multiple documents.
Four original documents:
−it was pathetic the worst part was the
boxing scenes
−no plot twists or great scenes
+ and satire and great plot twists
+ great scenes great ﬁlm
After per-document binarization:
−it was pathetic the worst part boxing
scenes
−no plot twists or great scenes
+ and satire great plot twists
+ great scenes ﬁlm
NB
Binary
Counts
Counts
+
−
+
−
and
2
0
1
0
boxing
0
1
0
1
ﬁlm
1
0
1
0
great
3
1
2
1
it
0
1
0
1
no
0
1
0
1
or
0
1
0
1
part
0
1
0
1
pathetic
0
1
0
1
plot
1
1
1
1
satire
1
0
1
0
scenes
1
2
1
2
the
0
2
0
1
twists
1
1
1
1
was
0
2
0
1
worst
0
1
0
1
Figure 4.3
An example of binarization for the binary naive Bayes algorithm.
A second important addition commonly made when doing text classiﬁcation for
sentiment is to deal with negation. Consider the difference between I really like this
movie (positive) and I didn’t like this movie (negative). The negation expressed by
didn’t completely alters the inferences we draw from the predicate like. Similarly,
negation can modify a negative word to produce a positive review (don’t dismiss this
ﬁlm, doesn’t let us get bored).
A very simple baseline that is commonly used in sentiment analysis to deal with
negation is the following: during text normalization, prepend the preﬁx NOT to
every word after a token of logical negation (n’t, not, no, never) until the next punc-
tuation mark. Thus the phrase
didn’t like this movie , but I
becomes
didn’t NOT_like NOT_this NOT_movie , but I
Newly formed ‘words’ like NOT like, NOT recommend will thus occur more of-
ten in negative document and act as cues for negative sentiment, while words like
NOT bored, NOT dismiss will acquire positive associations. We will return in Chap-
ter 17 to the use of parsing to deal more accurately with the scope relationship be-
tween these negation words and the predicates they modify, but this simple baseline
works quite well in practice.
Finally, in some situations we might have insufﬁcient labeled training data to
train accurate naive Bayes classiﬁers using all words in the training set to estimate
positive and negative sentiment. In such cases we can instead derive the positive

64
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
and negative word features from sentiment lexicons, lists of words that are pre-
sentiment
lexicons
annotated with positive or negative sentiment. Four popular lexicons are the General
Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon
General
Inquirer
LIWC
of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).
For example the MPQA subjectivity lexicon has 6885 words, 2718 positive and
4912 negative, each marked for whether it is strongly or weakly biased. (Chapter 21
will discuss how these lexicons can be learned automatically.) Some samples of
positive and negative words from the MPQA lexicon include:
+ : admirable, beautiful, conﬁdent, dazzling, ecstatic, favor, glee, great
−: awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate
A common way to use lexicons in a naive Bayes classiﬁer is to add a feature
that is counted whenever a word from that lexicon occurs. Thus we might add a
feature called ‘this word occurs in the positive lexicon’, and treat all instances of
words in the lexicon as counts for that one feature, instead of counting each word
separately. Similarly, we might add as a second feature ‘this word occurs in the
negative lexicon’ of words in the negative lexicon. If we have lots of training data,
and if the test data matches the training data, using just two features won’t work as
well as using all the words. But when training data is sparse or not representative of
the test set, using dense lexicon features instead of sparse individual-word features
may generalize better.
4.5
Naive Bayes for other text classiﬁcation tasks
In the previous section we pointed out that naive Bayes doesn’t require that our
classiﬁer use all the words in the training data as features. In fact features in naive
Bayes can express any property of the input text we want.
Consider the task of spam detection, deciding if a particular piece of email is
spam detection
an example of spam (unsolicited bulk email) — and one of the ﬁrst applications of
naive Bayes to text classiﬁcation (Sahami et al., 1998).
A common solution here, rather than using all the words as individual features, is
to predeﬁne likely sets of words or phrases as features, combined these with features
that are not purely linguistic. For example the open-source SpamAssassin tool1
predeﬁnes features like the phrase “one hundred percent guaranteed”, or the feature
mentions millions of dollars, which is a regular expression that matches suspiciously
large sums of money. But it also includes features like HTML has a low ratio of
text to image area, that isn’t purely linguistic and might require some sophisticated
computation, or totally non-linguistic features about, say, the path that the email
took to arrive. More sample SpamAssassin features:
• Email subject line is all capital letters
• Contains phrases of urgency like “urgent reply”
• Email subject line contains “online pharmaceutical”
• HTML has unbalanced ”head” tags
• Claims you can be removed from the list
For other tasks, like language ID—determining what language a given piece of
language ID
text is written in—the most effective naive Bayes features are not words at all, but
byte n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie z’, ‘thei’).
1
https://spamassassin.apache.org

4.6
•
NAIVE BAYES AS A LANGUAGE MODEL
65
Because spaces count as a byte, byte n-grams can model statistics about the begin-
ning or ending of words. 2 A widely used naive Bayes system, langid.py (Lui
and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature
selection to winnow down to the most informative 7000 ﬁnal features.
Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia
text in 68 different languages were used in (Lui and Baldwin, 2011)), or newswire.
To make sure that this multilingual text correctly reﬂects different regions, dialects,
and socioeconomic classes, systems also add Twitter text in many languages geo-
tagged to many regions (important for getting world English dialects from countries
with large Anglophone populations like Nigeria or India), Bible and Quran transla-
tions, slang websites like Urban Dictionary, corpora of African American Vernacular
English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).
4.6
Naive Bayes as a Language Model
As we saw in the previous section, naive Bayes classiﬁers can use any sort of fea-
ture: dictionaries, URLs, email addresses, network features, phrases, and so on. But
if, as in the previous section, we use only individual word features, and we use all
of the words in the text (not a subset), then naive Bayes has an important similar-
ity to language modeling. Speciﬁcally, a naive Bayes model can be viewed as a
set of class-speciﬁc unigram language models, in which the model for each class
instantiates a unigram language model.
Since the likelihood features from the naive Bayes model assign a probability to
each word P(word|c), the model also assigns a probability to each sentence:
P(s|c) =
Y
i∈positions
P(wi|c)
(4.15)
Thus consider a naive Bayes model with the classes positive (+) and negative (-)
and the following model parameters:
w
P(w|+) P(w|-)
I
0.1
0.2
love 0.1
0.001
this
0.01
0.01
fun
0.05
0.005
ﬁlm 0.1
0.1
...
...
...
Each of the two columns above instantiates a language model that can assign a
probability to the sentence “I love this fun ﬁlm”:
P(”I love this fun ﬁlm”|+) = 0.1×0.1×0.01×0.05×0.1 = 0.0000005
P(”I love this fun ﬁlm”|−) = 0.2×0.001×0.01×0.005×0.1 = .0000000010
As it happens, the positive model assigns a higher probability to the sentence:
P(s|pos) > P(s|neg). Note that this is just the likelihood part of the naive Bayes
2
It’s also possible to use codepoints, which are multi-byte Unicode representations of characters in
character sets, but simply using bytes seems to work better.

66
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
model; once we multiply in the prior a full naive Bayes model might well make a
different classiﬁcation decision.
4.7
Evaluation: Precision, Recall, F-measure
To introduce the methods for evaluating text classiﬁcation, let’s ﬁrst consider some
simple binary detection tasks. For example, in spam detection, our goal is to label
every text as being in the spam category (“positive”) or not in the spam category
(“negative”). For each item (email document) we therefore need to know whether
our system called it spam or not. We also need to know whether the email is actually
spam or not, i.e. the human-deﬁned labels for each document that we are trying to
match. We will refer to these human labels as the gold labels.
gold labels
Or imagine you’re the CEO of the Delicious Pie Company and you need to know
what people are saying about your pies on social media, so you build a system that
detects tweets concerning Delicious Pie. Here the positive class is tweets about
Delicious Pie and the negative class is all other tweets.
In both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. To evaluate any system for detecting things, we start
by building a contingency table like the one shown in Fig. 4.4. Each cell labels a
contingency
table
set of possible outcomes. In the spam detection case, for example, true positives are
documents that are indeed spam (indicated by human-created gold labels) and our
system said they were spam. False negatives are documents that are indeed spam
but our system labeled as non-spam.
To the bottom right of the table is the equation for accuracy, which asks what
percentage of all the observations (for the spam or pie examples that means all emails
or tweets) our system labeled correctly. Although accuracy might seem a natural
metric, we generally don’t use it. That’s because accuracy doesn’t work well when
the classes are unbalanced (as indeed they are with spam, which is a large majority
of email, or with tweets, which are mainly not about pie).
true positive
false negative
false positive
true negative
gold positive
gold negative
system
positive
system
negative
gold standard labels
system
output
labels
recall = tp
tp+fn
precision = 
tp
tp+fp
accuracy = 
tp+tn
tp+fp+tn+fn
Figure 4.4
Contingency table
To make this more explicit, imagine that we looked at a million tweets, and
let’s say that only 100 of them are discussing their love (or hatred) for our pie,
while the other 999,900 are tweets about something completely unrelated. Imagine a
simple classiﬁer that stupidly classiﬁed every tweet as “not about pie”. This classiﬁer
would have 999,900 true negatives and only 100 false negatives for an accuracy of
999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should
be happy with this classiﬁer? But of course this fabulous ‘no pie’ classiﬁer would

4.7
•
EVALUATION: PRECISION, RECALL, F-MEASURE
67
be completely useless, since it wouldn’t ﬁnd a single one of the customer comments
we are looking for. In other words, accuracy is not a good metric when the goal is
to discover something that is rare, or at least not completely balanced in frequency,
which is a very common situation in the world.
That’s why instead of accuracy we generally turn to two other metrics: precision
and recall. Precision measures the percentage of the items that the system detected
precision
(i.e., the system labeled as positive) that are in fact positive (i.e., are positive accord-
ing to the human gold labels). Precision is deﬁned as
Precision =
true positives
true positives + false positives
Recall measures the percentage of items actually present in the input that were
recall
correctly identiﬁed by the system. Recall is deﬁned as
Recall =
true positives
true positives + false negatives
Precision and recall will help solve the problem with the useless “nothing is
pie” classiﬁer. This classiﬁer, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recall is 0/100). You should convince yourself that the precision at ﬁnding relevant
tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize
true positives: ﬁnding the things that we are supposed to be looking for.
There are many ways to deﬁne a single metric that incorporates aspects of both
precision and recall. The simplest of these combinations is the F-measure (van
F-measure
Rijsbergen, 1975) , deﬁned as:
Fβ = (β 2 +1)PR
β 2P+R
The β parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of β > 1 favor recall, while
values of β < 1 favor precision. When β = 1, precision and recall are equally bal-
anced; this is the most frequently used metric, and is called Fβ=1 or just F1:
F1
F1 = 2PR
P+R
(4.16)
F-measure comes from a weighted harmonic mean of precision and recall. The
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:
HarmonicMean(a1,a2,a3,a4,...,an) =
n
1
a1 + 1
a2 + 1
a3 +...+ 1
an
(4.17)
and hence F-measure is
F =
1
α 1
P +(1−α) 1
R
or

with β 2 = 1−α
α

F = (β 2 +1)PR
β 2P+R
(4.18)
Harmonic mean is used because it is a conservative metric; the harmonic mean of
two values is closer to the minimum of the two values than the arithmetic mean is.
Thus it weighs the lower of the two numbers more heavily.

68
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
4.7.1
More than two classes
Up to now we have been assuming text classiﬁcation tasks with only two classes.
But lots of classiﬁcation tasks in language processing have more than two classes.
For sentiment analysis we generally have 3 classes (positive, negative, neutral) and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, semantic role labeling, emotion detection, and so on.
There are two kinds of multi-class classiﬁcation tasks. In any-of or multi-label
any-of
classiﬁcation, each document or item can be assigned more than one label. We can
solve any-of classiﬁcation by building separate binary classiﬁers for each class c,
trained on positive examples labeled c and negative examples not labeled c. Given
a test document or item d, then each classiﬁer makes their decision independently,
and we may assign multiple labels to d.
More common in language processing is one-of or multinomial classiﬁcation,
one-of
multinomial
classiﬁcation
in which the classes are mutually exclusive and each document or item appears in
exactly one class. Here we again build a separate binary classiﬁer trained on positive
examples from c and negative examples from all other classes. Now given a test
document or item d, we run all the classiﬁers and choose the label from the classiﬁer
with the highest score. Consider the sample confusion matrix for a hypothetical 3-
way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.5.
8
5
10
60
urgent
normal
gold labels
system
output
recallu = 
8
8+5+3
precisionu= 
8
8+10+1
1
50
30
200
spam
urgent
normal
spam
3
recalln = recalls = 
precisionn= 
60
5+60+50
precisions= 
200
3+30+200
60
10+60+30
200
1+50+200
Figure 4.5
Confusion matrix for a three-class categorization task, showing for each pair of
classes (c1,c2), how many documents from c1 were (in)correctly assigned to c2
The matrix shows, for example, that the system mistakenly labeled 1 spam doc-
ument as urgent, and we have shown how to compute a distinct precision and recall
value for each class. In order to derive a single metric that tells us how well the
system is doing, we can combine these values in two ways. In macroaveraging, we
macroaveraging
compute the performance for each class, and then average over classes. In microav-
eraging, we collect the decisions for all classes into a single contingency table, and
microaveraging
then compute precision and recall from that table. Fig. 4.6 shows the contingency
table for each class separately, and shows the computation of microaveraged and
macroaveraged precision.
As the ﬁgure shows, a microaverage is dominated by the more frequent class (in
this case spam), since the counts are pooled. The macroaverage better reﬂects the
statistics of the smaller classes, and so is more appropriate when performance on all
the classes is equally important.

4.8
•
TEST SETS AND CROSS-VALIDATION
69
8
8
11
340
true
urgent
true
not
system
urgent
system
not
60
40
55
212
true
normal
true
not
system
normal
system
not
200
51
33
83
true
spam
true
not
system
spam
system
not
268
99
99
635
true
yes
true
no
system
yes
system
no
precision = 8+11
8
= .42
precision = 200+33
200
= .86
precision = 60+55
60
= .52
microaverage
precision
268+99
268
= .73
=
macroaverage
precision
3
.42+.52+.86
= .60
=
Pooled
Class 3: Spam
Class 2: Normal
Class 1: Urgent
Figure 4.6
Separate contingency tables for the 3 classes from the previous ﬁgure, showing the pooled contin-
gency table and the microaveraged and macroaveraged precision.
4.8
Test sets and Cross-validation
The training and testing procedure for text classiﬁcation follows what we saw with
language modeling (Section 3.2): we use the training set to train the model, then use
the development test set (also called a devset) to perhaps tune some parameters,
development
test set
devset
and in general decide what the best model is. Once we come up with what we think
is the best model, we run it on the (hitherto unseen) test set to report its performance.
While the use of a devset avoids overﬁtting the test set, having a ﬁxed training
set, devset, and test set creates another problem: in order to save lots of data for
training, the test set (or devset) might not be large enough to be representative. It
would be better if we could somehow use all our data both for training and test. We
do this by cross-validation: we randomly choose a training and test set division of
cross-validation
our data, train our classiﬁer, and then compute the error rate on the test set. Then
we repeat with a different randomly selected training set and test set. We do this
sampling process 10 times and average these 10 runs to get an average error rate.
This is called 10-fold cross-validation.
10-fold
cross-validation
The only problem with cross-validation is that because all the data is used for
testing, we need the whole corpus to be blind; we can’t examine any of the data
to suggest possible features and in general see what’s going on. But looking at the
corpus is often important for designing the system. For this reason, it is common
to create a ﬁxed training set and test set, then do 10-fold cross-validation inside
the training set, but compute error rate the normal way in the test set, as shown in
Fig. 4.7.
4.9
Statistical Signiﬁcance Testing
In building systems we are constantly comparing the performance of systems. Often
we have added some new bells and whistles to our algorithm and want to compare
the new version of the system to the unaugmented version. Or we want to compare
our algorithm to a previously published one to know which is better.
We might imagine that to compare the performance of two classiﬁers A and B
all we have to do is look at A and B’s score on the same test set—for example we
might choose to compare macro-averaged F1— and see whether it’s A or B that has

70
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
Training Iterations
1
3
4
5
2
6
7
8
9
10
Dev
Dev
Dev
Dev
Dev
Dev
Dev
Dev
Dev
Dev
Training
Training
Training
Training
Training
Training
Training
Training
Training
Training
Training
Test 
Set
Testing
Figure 4.7
10-fold cross-validation
the higher score. But just looking at this one difference isn’t good enough, because
A might have a better performance than B on a particular test set just by chance.
Let’s say we have a test set x of n observations x = x1,x2,..,xn on which A’s
performance is better than B by δ(x). How can we know if A is really better than B?
To do so we’d need to reject the null hypothesis that A isn’t really better than B and
null hypothesis
this difference δ(x) occurred purely by chance. If the null hypothesis was correct,
we would expect that if we had many test sets of size n and we measured A and B’s
performance on all of them, that on average A might accidentally still be better than
B by this amount δ(x) just by chance.
More formally, if we had a random variable X ranging over test sets, the null
hypothesis H0 expects P(δ(X) > δ(x)|H0), the probability that we’ll see similarly
big differences just by chance, to be high.
If we had all these test sets we could just measure all the δ(x′) for all the x′. If we
found that those deltas didn’t seem to be bigger than δ(x), that is, that p-value(x) was
sufﬁciently small, less than the standard thresholds of 0.05 or 0.01, then we might
reject the null hypothesis and agree that δ(x) was a sufﬁciently surprising difference
and A is really a better algorithm than B. Following Berg-Kirkpatrick et al. (2012)
we’ll refer to P(δ(X) > δ(x)|H0) as p-value(x).
In language processing we don’t generally use traditional statistical approaches
like paired t-tests to compare system outputs because most metrics are not normally
distributed, violating the assumptions of the tests. The standard approach to comput-
ing p-value(x) in natural language processing is to use non-parametric tests like the
bootstrap test (Efron and Tibshirani, 1993)— which we will describe below—or a
bootstrap test
similar test, approximate randomization (Noreen, 1989). The advantage of these
approximate
randomization
tests is that they can apply to any metric; from precision, recall, or F1 to the BLEU
metric used in machine translation.
The word bootstrapping refers to repeatedly drawing large numbers of smaller
bootstrapping
samples with replacement (called bootstrap samples) from an original larger sam-
ple. The intuition of the bootstrap test is that we can create many virtual test sets
from an observed test set by repeatedly sampling from it. The method only makes
the assumption that the sample is representative of the population.
Consider a tiny text classiﬁcation example with a test set x of 10 documents. The
ﬁrst row of Fig. 4.8 shows the results of two classiﬁers (A and B) on this test set,
with each document labeled by one of the four possibilities: (A and B both right,
both wrong, A right and B wrong, A wrong and B right); a slash through a letter

4.9
•
STATISTICAL SIGNIFICANCE TESTING
71
(B) means that that classiﬁer got the answer wrong. On the ﬁrst document both A
and B get the correct class (AB), while on the second document A got it right but B
got it wrong (AB). If we assume for simplicity that our metric is accuracy, A has an
accuracy of .70 and B of .50, so δ(x) is .20. To create each virtual test set of size
N = 10, we repeatedly (10 times) select a cell from row x with replacement. Fig. 4.8
shows a few examples.
1
2
3
4
5
6
7
8
9
10 A% B% δ()
x
AB AB AB   AB AB   AB AB AB   AB AB
.70 .50
.20
x∗(1) AB AB AB   AB   AB AB   AB AB   AB AB
.60 .60
.00
x∗(2) AB AB   AB   AB   AB AB   AB AB AB AB
.60 .70 -.10
...
x∗(b)
Figure 4.8
The bootstrap: Examples of b pseudo test sets being created from an initial true
test set x. Each pseudo test set is created by sampling n = 10 times with replacement; thus an
individual sample is a single cell, a document with its gold label and the correct or incorrect
performance of classiﬁers A and B.
Now that we have a sampling distribution, we can do statistics on how often A
has an accidental advantage. There are various ways to compute this advantage; here
we follow the version laid out in Berg-Kirkpatrick et al. (2012). We might think that
we should just ask, for each bootstrap sample x∗(i), whether A beats B by more than
δ(x). But there’s a problem: we didn’t draw these samples from a distribution with
0 mean. The x∗(i) were sampled from x, and so the expected value of δ(x∗(i)) lies
very close to δ(x). That is, about half the time A will be better than B, so we expect
A to beat B by δ(x). Instead, we want to know how often A beats these expectations
by more than δ(x). To correct for the expected success, we need to zero-center,
subtracting δ(x) from each pseudo test set. Thus we’ll be comparing for each x∗(i)
whether δ(x∗(i)) > 2δ(x). The full algorithm for the bootstrap is shown in Fig. 4.9.
It is given a test set x, a number of samples b, and counts the percentage of the b
bootstrap test sets in which δ(x∗(i)) > 2δ(x). This percentage then acts as a one-
sided empirical p-value (more sophisticated ways to get p-values from conﬁdence
intervals also exist).
function BOOTSTRAP(test set x, num of samples b) returns p-value(x)
Calculate δ(x) # how much better does algorithm A do than B on x
for i = 1 to b do
for j = 1 to n do
# Draw a bootstrap sample x∗(i) of size n
Select a member of x at random and add it to x∗(i)
Calculate δ(x∗(i))
# how much better does algorithm A do than B on x∗(i)
for each x∗(i)
s←s + 1 if δ(x∗(i)) > 2δ(x)
p-value(x) ≈s
b
# on what % of the b samples did algorithm A beat expectations?
return p-value(x)
Figure 4.9
A version of the bootstrap algorithm after Berg-Kirkpatrick et al. (2012).

72
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
4.10
Summary
This chapter introduced the naive Bayes model for classiﬁcation and applied it to
the text categorization task of sentiment analysis.
• Many language processing tasks can be viewed as tasks of classiﬁcation.
• Text categorization, in which an entire text is assigned a class from a ﬁnite set,
includes such tasks as sentiment analysis, spam detection, language identi-
ﬁcation, and authorship attribution.
• Sentiment analysis classiﬁes a text as reﬂecting the positive or negative orien-
tation (sentiment) that a writer expresses toward some object.
• Naive Bayes is a generative model that makes the bag of words assumption
(position doesn’t matter) and the conditional independence assumption (words
are conditionally independent of each other given the class)
• Naive Bayes with binarized features seems to work better for many text clas-
siﬁcation tasks.
• Classiﬁers are evaluated based on precision and recall.
• Classiﬁers are trained using distinct training, dev, and test sets, including the
use of cross-validation in the training set.
Bibliographical and Historical Notes
Multinomial naive Bayes text classiﬁcation was proposed by Maron (1961) at the
RAND Corporation for the task of assigning subject categories to journal abstracts.
His model introduced most of the features of the modern form presented here, ap-
proximating the classiﬁcation task with one-of categorization, and implementing
add-δ smoothing and information-based feature selection.
The conditional independence assumptions of naive Bayes and the idea of Bayes-
ian analysis of text seems to have arisen multiple times. The same year as Maron’s
paper, Minsky (1961) proposed a naive Bayes classiﬁer for vision and other arti-
ﬁcial intelligence problems, and Bayesian techniques were also applied to the text
classiﬁcation task of authorship attribution by Mosteller and Wallace (1963). It had
long been known that Alexander Hamilton, John Jay, and James Madison wrote
the anonymously-published Federalist papers in 1787–1788 to persuade New York
to ratify the United States Constitution. Yet although some of the 85 essays were
clearly attributable to one author or another, the authorship of 12 were in dispute
between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian
probabilistic model of the writing of Hamilton and another model on the writings
of Madison, then computed the maximum-likelihood author for each of the disputed
essays. Naive Bayes was ﬁrst applied to spam detection in Heckerman et al. (1998).
Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show
that using boolean attributes with multinomial naive Bayes works better than full
counts. Binary multinomial naive Bayes is sometimes confused with another variant
of naive Bayes that also use a binary representation of whether a term occurs in
a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead
estimates P(w|c) as the fraction of documents that contain a term, and includes a
probability for whether a term is not in a document. McCallum and Nigam (1998)
and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive

EXERCISES
73
Bayes doesn’t work as well as the multinomial algorithm for sentiment or other text
tasks.
There are a variety of sources covering the many kinds of text classiﬁcation
tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).
Stamatatos (2009) surveys authorship attribute algorithms. On language identiﬁca-
tion see Jauhiainen et al. (2018); Jaech et al. (2016) is an important early neural
system. The task of newswire indexing was often used as a test case for text classi-
ﬁcation algorithms, based on the Reuters-21578 collection of newswire articles.
See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classiﬁcation;
classiﬁcation in general is covered in machine learning textbooks (Hastie et al. 2001,
Witten and Frank 2005, Bishop 2006, Murphy 2012).
Non-parametric methods for computing statistical signiﬁcance were used ﬁrst in
NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech
recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the
bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work
has focused on issues including multiple test sets and multiple metrics (Søgaard
et al. 2014, Dror et al. 2017).
Feature selection is a method of removing features that are unlikely to generalize
well. Features are generally ranked by how informative they are about the classiﬁca-
tion decision. A very common metric, information gain, tells us how many bits of
information
gain
information the presence of the word gives us for guessing the class. Other feature
selection metrics include χ2, pointwise mutual information, and GINI index; see
Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an
introduction to feature selection.
Exercises
4.1
Assume the following likelihoods for each word being part of a positive or
negative movie review, and equal prior probabilities for each class.
pos
neg
I
0.09 0.16
always 0.07 0.06
like
0.29 0.06
foreign 0.04 0.15
ﬁlms
0.08 0.11
What class will Naive bayes assign to the sentence “I always like foreign
ﬁlms.”?
4.2
Given the following short movie reviews, each labeled with a genre, either
comedy or action:
1. fun, couple, love, love
comedy
2. fast, furious, shoot
action
3. couple, ﬂy, fast, fun, fun
comedy
4. furious, shoot, shoot, fun
action
5. ﬂy, fast, shoot, love
action
and a new document D:
fast, couple, shoot, ﬂy
compute the most likely class for D. Assume a naive Bayes classiﬁer and use
add-1 smoothing for the likelihoods.

74
CHAPTER 4
•
NAIVE BAYES AND SENTIMENT CLASSIFICATION
4.3
Train two models, multinomial naive Bayes and binarized naive Bayes, both
with add-1 smoothing, on the following document counts for key sentiment
words, with positive or negative class assigned as noted.
doc “good” “poor” “great” (class)
d1. 3
0
3
pos
d2. 0
1
2
pos
d3. 1
3
0
neg
d4. 1
5
2
neg
d5. 0
2
0
neg
Use both naive Bayes models to assign a class (pos or neg) to this sentence:
A good, good plot and great characters, but poor acting.
Do the two models agree or disagree?

CHAPTER
5
Logistic Regression
”And how do you know that these ﬁne begonias are not of equal importance?”
Hercule Poirot, in Agatha Christie’s The Mysterious Affair at Styles
Detective stories are as littered with clues as texts are with words. Yet for the
poor reader it can be challenging to know how to weigh the author’s clues in order
to make the crucial classiﬁcation task: deciding whodunnit.
In this chapter we introduce an algorithm that is admirably suited for discovering
the link between features or cues and some particular outcome: logistic regression.
logistic
regression
Indeed, logistic regression is one of the most important analytic tools in the social
and natural sciences. In natural language processing, logistic regression is the base-
line supervised machine learning algorithm for classiﬁcation, and also has a very
close relationship with neural networks. As we will see in Chapter 7, a neural net-
work can be viewed as a series of logistic regression classiﬁers stacked on top of
each other. Thus the classiﬁcation and machine learning techniques introduced here
will play an important role throughout the book.
Logistic regression can be used to classify an observation into one of two classes
(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.
Because the mathematics for the two-class case is simpler, we’ll describe this special
case of logistic regression ﬁrst in the next few sections, and then brieﬂy summarize
the use of multinomial logistic regression for more than two classes in Section 5.6.
We’ll introduce the mathematics of logistic regression in the next few sections.
But let’s begin with some high-level issues.
Generative and Discriminative Classiﬁers:
The most important difference be-
tween naive Bayes and logistic regression is that logistic regression is a discrimina-
tive classiﬁer while naive Bayes is a generative classiﬁer.
These are two very different frameworks for how
to build a machine learning model. Consider a visual
metaphor: imagine we’re trying to distinguish dog
images from cat images. A generative model would
have the goal of understanding what dogs look like
and what cats look like. You might literally ask such
a model to ‘generate’, i.e. draw, a dog. Given a test
image, the system then asks whether it’s the cat model or the dog model that better
ﬁts (is less surprised by) the image, and chooses that as its label.
A discriminative model, by contrast, is only try-
ing to learn to distinguish the classes (perhaps with-
out learning much about them). So maybe all the
dogs in the training data are wearing collars and the
cats aren’t. If that one feature neatly separates the
classes, the model is satisﬁed.
If you ask such a
model what it knows about cats all it can say is that
they don’t wear collars.

76
CHAPTER 5
•
LOGISTIC REGRESSION
More formally, recall that the naive Bayes assigns a class c to a document d not
by directly computing P(c|d) but by computing a likelihood and a prior
ˆc = argmax
c∈C
likelihood
z }| {
P(d|c)
prior
z}|{
P(c)
(5.1)
A generative model like naive Bayes makes use of this likelihood term, which
generative
model
expresses how to generate the features of a document if we knew it was of class c.
By contrast a discriminative model in this text categorization scenario attempts
discriminative
model
to directly compute P(c|d). Perhaps it will learn to assign a high weight to document
features that directly improve its ability to discriminate between possible classes,
even if it couldn’t generate an example of one of the classes.
Components of a probabilistic machine learning classiﬁer:
Like naive Bayes,
logistic regression is a probabilistic classiﬁer that makes use of supervised machine
learning. Machine learning classiﬁers require a training corpus of M input/output
pairs (x(i),y(i)). (We’ll use superscripts in parentheses to refer to individual instances
in the training set—for sentiment classiﬁcation each instance might be an individual
document to be classiﬁed). A machine learning system for classiﬁcation then has
four components:
1. A feature representation of the input. For each input observation x(i), this
will be a vector of features [x1,x2,...,xn]. We will generally refer to feature
i for input x(j) as x(j)
i , sometimes simpliﬁed as xi, but we will also see the
notation fi, fi(x), or, for multiclass classiﬁcation, fi(c,x).
2. A classiﬁcation function that computes ˆy, the estimated class, via p(y|x). In
the next section we will introduce the sigmoid and softmax tools for classiﬁ-
cation.
3. An objective function for learning, usually involving minimizing error on
training examples. We will introduce the cross-entropy loss function
4. An algorithm for optimizing the objective function. We introduce the stochas-
tic gradient descent algorithm.
Logistic regression has two phases:
training: we train the system (speciﬁcally the weights w and b) using stochastic
gradient descent and the cross-entropy loss.
test: Given a test example x we compute p(y|x) and return the higher probability
label y = 1 or y = 0.
5.1
Classiﬁcation: the sigmoid
The goal of binary logistic regression is to train a classiﬁer that can make a binary
decision about the class of a new input observation. Here we introduce the sigmoid
classiﬁer that will help us make this decision.
Consider a single input observation x, which we will represent by a vector of
features [x1,x2,...,xn] (we’ll show sample features in the next subsection). The clas-
siﬁer output y can be 1 (meaning the observation is a member of the class) or 0
(the observation is not a member of the class). We want to know the probability
P(y = 1|x) that this observation is a member of the class. So perhaps the decision

5.1
•
CLASSIFICATION: THE SIGMOID
77
is “positive sentiment” versus “negative sentiment”, the features represent counts
of words in a document, and P(y = 1|x) is the probability that the document has
positive sentiment, while and P(y = 0|x) is the probability that the document has
negative sentiment.
Logistic regression solves this task by learning, from a training set, a vector of
weights and a bias term. Each weight wi is a real number, and is associated with one
of the input features xi. The weight wi represents how important that input feature is
to the classiﬁcation decision, and can be positive (meaning the feature is associated
with the class) or negative (meaning the feature is not associated with the class).
Thus we might expect in a sentiment task the word awesome to have a high positive
weight, and abysmal to have a very negative weight. The bias term, also called the
bias term
intercept, is another real number that’s added to the weighted inputs.
intercept
To make a decision on a test instance— after we’ve learned the weights in
training— the classiﬁer ﬁrst multiplies each xi by its weight wi, sums up the weighted
features, and adds the bias term b. The resulting single number z expresses the
weighted sum of the evidence for the class.
z =
 n
X
i=1
wixi
!
+b
(5.2)
In the rest of the book we’ll represent such sums using the dot product notation from
dot product
linear algebra. The dot product of two vectors a and b, written as a·b is the sum of
the products of the corresponding elements of each vector. Thus the following is an
equivalent formation to Eq. 5.2:
z = w·x+b
(5.3)
But note that nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie
between 0 and 1. In fact, since weights are real-valued, the output might even be
negative; z ranges from −∞to ∞.
Figure 5.1
The sigmoid function y =
1
1+e−z takes a real value and maps it to the range [0,1].
It is nearly linear around 0 but outlier values get squashed toward 0 or 1.
To create a probability, we’ll pass z through the sigmoid function, σ(z). The
sigmoid
sigmoid function (named because it looks like an s) is also called the logistic func-
tion, and gives logistic regression its name. The sigmoid has the following equation,
logistic
function
shown graphically in Fig. 5.1:
y = σ(z) =
1
1+e−z
(5.4)
The sigmoid has a number of advantages; it takes a real-valued number and maps
it into the range [0,1], which is just what we want for a probability. Because it is

78
CHAPTER 5
•
LOGISTIC REGRESSION
nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier
values toward 0 or 1. And it’s differentiable, which as we’ll see in Section 5.8 will
be handy for learning.
We’re almost there. If we apply the sigmoid to the sum of the weighted features,
we get a number between 0 and 1. To make it a probability, we just need to make
sure that the two cases, p(y = 1) and p(y = 0), sum to 1. We can do this as follows:
P(y = 1) = σ(w·x+b)
=
1
1+e−(w·x+b)
P(y = 0) = 1−σ(w·x+b)
= 1−
1
1+e−(w·x+b)
=
e−(w·x+b)
1+e−(w·x+b)
(5.5)
Now we have an algorithm that given an instance x computes the probability P(y =
1|x). How do we make a decision? For a test instance x, we say yes if the probability
P(y = 1|x) is more than .5, and no otherwise. We call .5 the decision boundary:
decision
boundary
ˆy =
 1 if P(y = 1|x) > 0.5
0 otherwise
5.1.1
Example: sentiment classiﬁcation
Let’s have an example. Suppose we are doing binary sentiment classiﬁcation on
movie review text, and we would like to know whether to assign the sentiment class
+ or −to a review document doc. We’ll represent each input observation by the 6
features x1...x6 of the input shown in the following table; Fig. 5.2 shows the features
in a sample mini test document.
Var
Deﬁnition
Value in Fig. 5.2
x1
count(positive lexicon) ∈doc)
3
x2
count(negative lexicon) ∈doc)
2
x3
 1 if “no” ∈doc
0 otherwise
1
x4
count(1st and 2nd pronouns ∈doc)
3
x5
 1 if “!” ∈doc
0 otherwise
0
x6
log(word count of doc)
ln(66) = 4.19
Let’s assume for the moment that we’ve already learned a real-valued weight for
each of these features, and that the 6 weights corresponding to the 6 features are
[2.5,−5.0,−1.2,0.5,2.0,0.7], while b = 0.1. (We’ll discuss in the next section how
the weights are learned.) The weight w1, for example indicates how important a
feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to
a positive sentiment decision, while w2 tells us the importance of negative lexicon
words. Note that w1 = 2.5 is positive, while w2 = −5.0, meaning that negative words
are negatively associated with a positive sentiment decision, and are about twice as
important as positive words.

5.1
•
CLASSIFICATION: THE SIGMOID
79
It's hokey . There are virtually no surprises , and the writing is second-rate . 
So why was it so enjoyable  ? For one thing , the cast is
great . Another nice touch is the music . I was overcome with the urge to get off
the couch and start dancing .  It sucked me in , and it'll do the same to you  .
x1=3
x6=4.19
x3=1
x4=3
x5=0
x2=2
Figure 5.2
A sample mini test document showing the extracted features in the vector x.
Given these 6 features and the input review x, P(+|x) and P(−|x) can be com-
puted using Eq. 5.5:
p(+|x) = P(Y = 1|x) = σ(w·x+b)
= σ([2.5,−5.0,−1.2,0.5,2.0,0.7]·[3,2,1,3,0,4.19]+0.1)
= σ(.833)
= 0.70
(5.6)
p(−|x) = P(Y = 0|x) = 1−σ(w·x+b)
= 0.30
Logistic regression is commonly applied to all sorts of NLP tasks, and any property
of the input can be a feature. Consider the task of period disambiguation: deciding
if a period is the end of a sentence or part of a word, by classifying each period
into one of two classes EOS (end-of-sentence) and not-EOS. We might use features
like x1 below expressing that the current word is lower case and the class is EOS
(perhaps with a positive weight), or that the current word is in our abbreviations
dictionary (“Prof.”) and the class is EOS (perhaps with a negative weight). A feature
can also express a quite complex combination of properties. For example a period
following an upper case word is likely to be an EOS, but if the word itself is St. and
the previous word is capitalized, then the period is likely part of a shortening of the
word street.
x1 =
 1 if “Case(wi) = Lower”
0 otherwise
x2 =
 1 if “wi ∈AcronymDict”
0 otherwise
x3 =
 1 if “wi = St. & Case(wi−1) = Cap”
0 otherwise
Designing features:
Features are generally designed by examining the training
set with an eye to linguistic intuitions and the linguistic literature on the domain. A
careful error analysis on the training set or devset of an early version of a system
often provides insights into features.
For some tasks it is especially helpful to build complex features that are combi-
nations of more primitive features. We saw such a feature for period disambiguation
above, where a period on the word St. was less likely to be the end of the sentence
if the previous word was capitalized. For logistic regression and naive Bayes these
combination features or feature interactions have to be designed by hand.
feature
interactions

80
CHAPTER 5
•
LOGISTIC REGRESSION
For many tasks (especially when feature values can reference speciﬁc words)
we’ll need large numbers of features. Often these are created automatically via fea-
ture templates, abstract speciﬁcations of features. For example a bigram template
feature
templates
for period disambiguation might create a feature for every pair of words that occurs
before a period in the training set. Thus the feature space is sparse, since we only
have to create a feature if that n-gram exists in that position in the training set. The
feature is generally created as a hash from the string descriptions. A user description
of a feature as, “bigram(American breakfast)” is hashed into a unique integer i that
becomes the feature number fi.
In order to avoid the extensive human effort of feature design, recent research in
NLP has focused on representation learning: ways to learn features automatically
in an unsupervised way from the input. We’ll introduce methods for representation
learning in Chapter 6 and Chapter 7.
Choosing a classiﬁer
Logistic regression has a number of advantages over naive
Bayes. Naive Bayes has overly strong conditional independence assumptions. Con-
sider two features which are strongly correlated; in fact, imagine that we just add the
same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were sep-
arate, multiplying them both in, overestimating the evidence. By contrast, logistic
regression is much more robust to correlated features; if two features f1 and f2 are
perfectly correlated, regression will simply assign part of the weight to w1 and part
to w2. Thus when there are many correlated features, logistic regression will assign
a more accurate probability than naive Bayes. So logistic regression generally works
better on larger documents or datasets and is a common default.
Despite the less accurate probabilities, naive Bayes still often makes the correct
classiﬁcation decision. Furthermore, naive Bayes can work extremely well (some-
times even better than logistic regression) on very small datasets (Ng and Jordan,
2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is
easy to implement and very fast to train (there’s no optimization step). So it’s still a
reasonable approach to use in some situations.
5.2
Learning in Logistic Regression
How are the parameters of the model, the weights w and bias b, learned? Logistic
regression is an instance of supervised classiﬁcation in which we know the correct
label y (either 0 or 1) for each observation x. What the system produces via Eq. 5.5
is ˆy, the system’s estimate of the true y. We want to learn parameters (meaning w
and b) that make ˆy for each training observation as close as possible to the true y .
This requires 2 components that we foreshadowed in the introduction to the
chapter. The ﬁrst is a metric for how close the current label (ˆy) is to the true gold
label y. Rather than measure similarity, we usually talk about the opposite of this:
the distance between the system output and the gold output, and we call this distance
the loss function or the cost function. In the next section we’ll introduce the loss
loss
function that is commonly used for logistic regression and also for neural networks,
the cross-entropy loss.
The second thing we need is an optimization algorithm for iteratively updating
the weights so as to minimize this loss function. The standard algorithm for this is
gradient descent; we’ll introduce the stochastic gradient descent algorithm in the
following section.

5.3
•
THE CROSS-ENTROPY LOSS FUNCTION
81
5.3
The cross-entropy loss function
We need a loss function that expresses, for an observation x, how close the classiﬁer
output (ˆy = σ(w·x+b)) is to the correct output (y, which is 0 or 1). We’ll call this:
L(ˆy,y) = How much ˆy differs from the true y
(5.7)
We do this via a loss function that prefers the correct class labels of the train-
ing examples to be more likely. This is called conditional maximum likelihood
estimation: we choose the parameters w,b that maximize the log probability of
the true y labels in the training data given the observations x. The resulting loss
function is the negative log likelihood loss, generally called the cross-entropy loss.
cross-entropy
loss
Let’s derive this loss function, applied to a single observation x. We’d like to
learn weights that maximize the probability of the correct label p(y|x). Since there
are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can
express the probability p(y|x) that our classiﬁer produces for one observation as
the following (keeping in mind that if y=1, Eq. 5.8 simpliﬁes to ˆy; if y=0, Eq. 5.8
simpliﬁes to 1−ˆy):
p(y|x) = ˆyy (1−ˆy)1−y
(5.8)
Now we take the log of both sides. This will turn out to be handy mathematically,
and doesn’t hurt us; whatever values maximize a probability will also maximize the
log of the probability:
log p(y|x) = log

ˆyy (1−ˆy)1−y
= ylog ˆy+(1−y)log(1−ˆy)
(5.9)
Eq. 5.9 describes a log likelihood that should be maximized. In order to turn this
into loss function (something that we need to minimize), we’ll just ﬂip the sign on
Eq. 5.9. The result is the cross-entropy loss LCE:
LCE(ˆy,y) = −log p(y|x) = −[ylog ˆy+(1−y)log(1−ˆy)]
(5.10)
Finally, we can plug in the deﬁnition of ˆy = σ(w·x+b):
LCE(w,b) = −[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
(5.11)
Let’s see if this loss function does the right thing for our example from Fig. 5.2. We
want the loss to be smaller if the model’s estimate is close to correct, and bigger if
the model is confused. So ﬁrst let’s suppose the correct gold label for the sentiment
example in Fig. 5.2 is positive, i.e., y = 1. In this case our model is doing well, since
from Eq. 5.6 it indeed gave the example a higher probability of being positive (.69)
than negative (.31). If we plug σ(w·x +b) = .69 and y = 1 into Eq. 5.11, the right
side of the equation drops out, leading to the following loss:
LCE(w,b) =
−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
=
−[logσ(w·x+b)]
=
−log(.69)
=
.37

82
CHAPTER 5
•
LOGISTIC REGRESSION
By contrast, let’s pretend instead that the example in Fig. 5.2 was actually negative,
i.e. y = 0 (perhaps the reviewer went on to say “But bottom line, the movie is
terrible! I beg you not to see it!”). In this case our model is confused and we’d want
the loss to be higher. Now if we plug y = 0 and 1−σ(w·x+b) = .31 from Eq. 5.6
into Eq. 5.11, the left side of the equation drops out:
LCE(w,b) =
−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
=
−[log(1−σ(w·x+b))]
=
−log(.31)
=
1.17
Sure enough, the loss for the ﬁrst classiﬁer (.37) is less than the loss for the second
classiﬁer (1.17).
Why does minimizing this negative log probability do what we want? A per-
fect classiﬁer would assign probability 1 to the correct outcome (y=1 or y=0) and
probability 0 to the incorrect outcome. That means the higher ˆy (the closer it is to
1), the better the classiﬁer; the lower ˆy is (the closer it is to 0), the worse the clas-
siﬁer. The negative log of this probability is a convenient loss metric since it goes
from 0 (negative log of 1, no loss) to inﬁnity (negative log of 0, inﬁnite loss). This
loss function also ensures that as the probability of the correct answer is maximized,
the probability of the incorrect answer is minimized; since the two sum to one, any
increase in the probability of the correct answer is coming at the expense of the in-
correct answer. It’s called the cross-entropy loss, because Eq. 5.9 is also the formula
for the cross-entropy between the true probability distribution y and our estimated
distribution ˆy.
Now we know what we want to minimize; in the next section, we’ll see how to
ﬁnd the minimum.
5.4
Gradient Descent
Our goal with gradient descent is to ﬁnd the optimal weights: minimize the loss
function we’ve deﬁned for the model. In Eq. 5.12 below, we’ll explicitly represent
the fact that the loss function L is parameterized by the weights, which we’ll refer to
in machine learning in general as θ (in the case of logistic regression θ = w,b):
ˆθ = argmin
θ
1
m
m
X
i=1
LCE(y(i),x(i);θ)
(5.12)
How shall we ﬁnd the minimum of this (or any) loss function? Gradient descent
is a method that ﬁnds a minimum of a function by ﬁguring out in which direction
(in the space of the parameters θ) the function’s slope is rising the most steeply,
and moving in the opposite direction. The intuition is that if you are hiking in a
canyon and trying to descend most quickly down to the river at the bottom, you might
look around yourself 360 degrees, ﬁnd the direction where the ground is sloping the
steepest, and walk downhill in that direction.
For logistic regression, this loss function is conveniently convex. A convex func-
convex
tion has just one minimum; there are no local minima to get stuck in, so gradient
descent starting from any point is guaranteed to ﬁnd the minimum. (By contrast,

5.4
•
GRADIENT DESCENT
83
the loss for multi-layer neural networks is non-convex, and gradient descent may
get stuck in local minima for neural network training and never ﬁnd the global opti-
mum.)
Although the algorithm (and the concept of gradient) are designed for direction
vectors, let’s ﬁrst consider a visualization of the case where the parameter of our
system is just a single scalar w, shown in Fig. 5.3.
Given a random initialization of w at some value w1, and assuming the loss
function L happened to have the shape in Fig. 5.3, we need the algorithm to tell us
whether at the next iteration we should move left (making w2 smaller than w1) or
right (making w2 bigger than w1) to reach the minimum.
w
Loss
0
w1
wmin
slope of loss at w1 
is negative
(goal)
one step
of gradient
descent
Figure 5.3
The ﬁrst step in iteratively ﬁnding the minimum of this loss function, by moving
w in the reverse direction from the slope of the function. Since the slope is negative, we need
to move w in a positive direction, to the right. Here superscripts are used for learning steps,
so w1 means the initial value of w (which is 0), w2 at the second step, and so on.
The gradient descent algorithm answers this question by ﬁnding the gradient
gradient
of the loss function at the current point and moving in the opposite direction. The
gradient of a function of many variables is a vector pointing in the direction of the
greatest increase in a function. The gradient is a multi-variable generalization of the
slope, so for a function of one variable like the one in Fig. 5.3, we can informally
think of the gradient as the slope. The dotted line in Fig. 5.3 shows the slope of this
hypothetical loss function at point w = w1. You can see that the slope of this dotted
line is negative. Thus to ﬁnd the minimum, gradient descent tells us to go in the
opposite direction: moving w in a positive direction.
The magnitude of the amount to move in gradient descent is the value of the slope
d
dw f(x;w) weighted by a learning rate η. A higher (faster) learning rate means that
learning rate
we should move w more on each step. The change we make in our parameter is the
learning rate times the gradient (or the slope, in our single-variable example):
wt+1 = wt −η d
dw f(x;w)
(5.13)
Now let’s extend the intuition from a function of one scalar variable w to many
variables, because we don’t just want to move left or right, we want to know where
in the N-dimensional space (of the N parameters that make up θ) we should move.
The gradient is just such a vector; it expresses the directional components of the
sharpest slope along each of those N dimensions. If we’re just imagining two weight
dimensions (say for one weight w and one bias b), the gradient might be a vector with
two orthogonal components, each of which tells us how much the ground slopes in
the w dimension and in the b dimension. Fig. 5.4 shows a visualization:

84
CHAPTER 5
•
LOGISTIC REGRESSION
Cost(w,b)
w
b
Figure 5.4
Visualization of the gradient vector in two dimensions w and b.
In an actual logistic regression, the parameter vector w is much longer than 1 or
2, since the input feature vector x can be quite long, and we need a weight wi for
each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have
a component that tells us the slope with respect to that variable. Essentially we’re
asking: “How much would a small change in that variable wi inﬂuence the total loss
function L?”
In each dimension wi, we express the slope as a partial derivative
∂
∂wi of the loss
function. The gradient is then deﬁned as a vector of these partials. We’ll represent ˆy
as f(x;θ) to make the dependence on θ more obvious:
∇θL(f(x;θ),y)) =


∂
∂w1 L(f(x;θ),y)
∂
∂w2 L(f(x;θ),y)
...
∂
∂wn L(f(x;θ),y)


(5.14)
The ﬁnal equation for updating θ based on the gradient is thus
θt+1 = θt −η∇L(f(x;θ),y)
(5.15)
5.4.1
The Gradient for Logistic Regression
In order to update θ, we need a deﬁnition for the gradient ∇L(f(x;θ),y). Recall that
for logistic regression, the cross-entropy loss function is:
LCE(w,b) = −[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
(5.16)
It turns out that the derivative of this function for one observation vector x is
Eq. 5.17 (the interested reader can see Section 5.8 for the derivation of this equation):
∂LCE(w,b)
∂w j
= [σ(w·x+b)−y]xj
(5.17)
Note in Eq. 5.17 that the gradient with respect to a single weight w j represents a
very intuitive value: the difference between the true y and our estimated ˆy = σ(w ·
x+b) for that observation, multiplied by the corresponding input value xj.

5.4
•
GRADIENT DESCENT
85
5.4.2
The Stochastic Gradient Descent Algorithm
Stochastic gradient descent is an online algorithm that minimizes the loss function
by computing its gradient after each training example, and nudging θ in the right
direction (the opposite direction of the gradient). Fig. 5.5 shows the algorithm.
function STOCHASTIC GRADIENT DESCENT(L(), f(), x, y) returns θ
# where: L is the loss function
#
f is a function parameterized by θ
#
x is the set of training inputs x(1), x(2),..., x(n)
#
y is the set of training outputs (labels) y(1), y(2),..., y(n)
θ ←0
repeat til done
# see caption
For each training tuple (x(i), y(i)) (in random order)
1. Optional (for reporting):
# How are we doing on this tuple?
Compute ˆy(i) =
f(x(i);θ)
# What is our estimated output ˆy?
Compute the loss L(ˆy(i),y(i)) # How far off is ˆy(i)) from the true output y(i)?
2. g←∇θL(f(x(i);θ),y(i))
# How should we move θ to maximize loss?
3. θ ←θ −η g
# Go the other way instead
return θ
Figure 5.5
The stochastic gradient descent algorithm. Step 1 (computing the loss) is used
to report how well we are doing on the current tuple. The algorithm can terminate when it
converges (or when the gradient < ϵ), or when progress halts (for example when the loss starts
going up on a held-out set).
The learning rate η is a parameter that must be adjusted. If it’s too high, the
learner will take steps that are too large, overshooting the minimum of the loss func-
tion. If it’s too low, the learner will take steps that are too small, and take too long to
get to the minimum. It is common to begin the learning rate at a higher value, and
then slowly decrease it, so that it is a function of the iteration k of training; you will
sometimes see the notation ηk to mean the value of the learning rate at iteration k.
5.4.3
Working through an example
Let’s walk though a single step of the gradient descent algorithm. We’ll use a sim-
pliﬁed version of the example in Fig. 5.2 as it sees a single observation x, whose
correct value is y = 1 (this is a positive review), and with only two features:
x1 = 3
(count of positive lexicon words)
x2 = 2
(count of negative lexicon words)
Let’s assume the initial weights and bias in θ 0 are all set to 0, and the initial learning
rate η is 0.1:
w1 = w2 = b = 0
η = 0.1
The single update step requires that we compute the gradient, multiplied by the
learning rate
θt+1 = θt −η∇θL(f(x(i);θ),y(i))

86
CHAPTER 5
•
LOGISTIC REGRESSION
In our mini example there are three parameters, so the gradient vector has 3 dimen-
sions, for w1, w2, and b. We can compute the ﬁrst gradient as follows:
∇w,b =


∂LCE(w,b)
∂w1
∂LCE(w,b)
∂w2
∂LCE(w,b)
∂b

=


(σ(w·x+b)−y)x1
(σ(w·x+b)−y)x2
σ(w·x+b)−y

=


(σ(0)−1)x1
(σ(0)−1)x2
σ(0)−1

=


−0.5x1
−0.5x2
−0.5

=


−1.5
−1.0
−0.5


Now that we have a gradient, we compute the new parameter vector θ 1 by moving
θ 0 in the opposite direction from the gradient:
θ 2 =


w1
w2
b

−η


−1.5
−1.0
−0.5

=


.15
.1
.05


So after one step of gradient descent, the weights have shifted to be: w1 = .15,
w2 = .1, and b = .05.
Note that this observation x happened to be a positive example. We would expect
that after seeing more negative examples with high counts of negative words, that
the weight w2 would shift to have a negative value.
5.4.4
Mini-batch training
Stochastic gradient descent is called stochastic because it chooses a single random
example at a time, moving the weights so as to improve performance on that single
example. That can result in very choppy movements, so it’s common to compute the
gradient over batches of training instances rather than a single instance.
For example in batch training we compute the gradient over the entire dataset.
batch training
By seeing so many examples, batch training offers a superb estimate of which di-
rection to move the weights, at the cost of spending a lot of time processing every
single example in the training set to compute this perfect direction.
A compromise is mini-batch training: we train on a group of m examples (per-
mini-batch
haps 512, or 1024) that is less than the whole dataset. (If m is the size of the dataset,
then we are doing batch gradient descent; if m = 1, we are back to doing stochas-
tic gradient descent). Mini-batch training also has the advantage of computational
efﬁciency. The mini-batches can easily be vectorized, choosing the size of the mini-
batch based on the computational resources. This allows us to process all the exam-
ples in one mini-batch in parallel and then accumulate the loss, something that’s not
possible with individual or batch training.
We just need to deﬁne mini-batch versions of the cross-entropy loss function
we deﬁned in Section 5.3 and the gradient in Section 5.4.1. Let’s extend the cross-
entropy loss for one example from Eq. 5.10 to mini-batches of size m. We’ll continue
to use the notation that x(i) and y(i) mean the ith training features and training label,
respectively. We make the assumption that the training examples are independent:
log p(training labels) = log
m
Y
i=1
p(y(i)|x(i))
(5.18)
=
m
X
i=1
log p(y(i)|x(i))
(5.19)
= −
m
X
i=1
LCE(ˆy(i),y(i))
(5.20)

5.5
•
REGULARIZATION
87
Now the cost function for the mini-batch of m examples is the average loss for each
example:
Cost(w,b) =
1
m
m
X
i=1
LCE(ˆy(i),y(i))
= −1
m
m
X
i=1
y(i) logσ(w·x(i) +b)+(1−y(i))log

1−σ(w·x(i) +b)

(5.21)
The mini-batch gradient is the average of the individual gradients from Eq. 5.17:
∂Cost(w,b)
∂w j
=
1
m
m
X
i=1
h
σ(w·x(i) +b)−y(i)i
x(i)
j
(5.22)
5.5
Regularization
Numquam ponenda est pluralitas sine necessitate
‘Plurality should never be proposed unless needed’
William of Occam
There is a problem with learning weights that make the model perfectly match the
training data. If a feature is perfectly predictive of the outcome because it happens
to only occur in one class, it will be assigned a very high weight. The weights for
features will attempt to perfectly ﬁt details of the training set, in fact too perfectly,
modeling noisy factors that just accidentally correlate with the class. This problem is
called overﬁtting. A good model should be able to generalize well from the training
overﬁtting
generalize
data to the unseen test set, but a model that overﬁts will have poor generalization.
To avoid overﬁtting, a new regularization term R(θ) is added to the objective
regularization
function in Eq. 5.12, resulting in the following objective for a batch of m exam-
ples (slightly rewritten from Eq. 5.12 to be maximizing log probability rather than
minimizing loss, and removing the 1
m term which doesn’t affect the argmax):
ˆθ = argmax
θ
m
X
i=1
logP(y(i)|x(i))−αR(θ)
(5.23)
The new regularization term R(θ) is used to penalize large weights. Thus a setting
of the weights that matches the training data perfectly— but uses many weights with
high values to do so—will be penalized more than a setting that matches the data a
little less well, but does so using smaller weights. There are two common ways to
compute this regularization term R(θ). L2 regularization is a quadratic function of
L2
regularization
the weight values, named because it uses the (square of the) L2 norm of the weight
values. The L2 norm, ||θ||2, is the same as the Euclidean distance of the vector θ
from the origin. If θ consists of n weights, then:
R(θ) = ||θ||2
2 =
n
X
j=1
θ 2
j
(5.24)

88
CHAPTER 5
•
LOGISTIC REGRESSION
The L2 regularized objective function becomes:
ˆθ = argmax
θ
" m
X
1=i
logP(y(i)|x(i))
#
−α
n
X
j=1
θ 2
j
(5.25)
L1 regularization is a linear function of the weight values, named after the L1 norm
L1
regularization
||W||1, the sum of the absolute values of the weights, or Manhattan distance (the
Manhattan distance is the distance you’d have to walk between two points in a city
with a street grid like New York):
R(θ) = ||θ||1 =
n
X
i=1
|θi|
(5.26)
The L1 regularized objective function becomes:
ˆθ = argmax
θ
" m
X
1=i
logP(y(i)|x(i))
#
−α
n
X
j=1
|θ j|
(5.27)
These kinds of regularization come from statistics, where L1 regularization is called
lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression,
lasso
ridge
and both are commonly used in language processing. L2 regularization is easier to
optimize because of its simple derivative (the derivative of θ 2 is just 2θ), while
L1 regularization is more complex (the derivative of |θ| is non-continuous at zero).
But where L2 prefers weight vectors with many small weights, L1 prefers sparse
solutions with some larger weights but many more weights set to zero. Thus L1
regularization leads to much sparser weight vectors, that is, far fewer features.
Both L1 and L2 regularization have Bayesian interpretations as constraints on
the prior of how weights should look. L1 regularization can be viewed as a Laplace
prior on the weights. L2 regularization corresponds to assuming that weights are
distributed according to a gaussian distribution with mean µ = 0. In a gaussian
or normal distribution, the further away a value is from the mean, the lower its
probability (scaled by the variance σ). By using a gaussian prior on the weights, we
are saying that weights prefer to have the value 0. A gaussian for a weight θj is
1
q
2πσ2
j
exp
 
−(θj −µj)2
2σ2
j
!
(5.28)
If we multiply each weight by a gaussian prior on the weight, we are thus maximiz-
ing the following constraint:
ˆθ = argmax
θ
M
Y
i=1
P(y(i)|x(i))×
n
Y
j=1
1
q
2πσ2
j
exp
 
−(θj −µj)2
2σ2
j
!
(5.29)
which in log space, with µ = 0, and assuming 2σ2 = 1, corresponds to
ˆθ = argmax
θ
m
X
i=1
logP(y(i)|x(i))−α
n
X
j=1
θ 2
j
(5.30)
which is in the same form as Eq. 5.25.

5.6
•
MULTINOMIAL LOGISTIC REGRESSION
89
5.6
Multinomial logistic regression
Sometimes we need more than two classes. Perhaps we might want to do 3-way
sentiment classiﬁcation (positive, negative, or neutral). Or we could be classifying
the part of speech of a word (choosing from 10, 30, or even 50 different parts of
speech), or assigning semantic labels like the named entities or semantic relations
we will introduce in Chapter 18.
In such cases we use multinomial logistic regression, also called softmax re-
multinomial
logistic
regression
gression (or, historically, the maxent classiﬁer). In multinomial logistic regression
the target y is a variable that ranges over more than two classes; we want to know
the probability of y being in each potential class c ∈C, p(y = c|x).
The multinomial logistic classiﬁer uses a generalization of the sigmoid, called
the softmax function, to compute the probability p(y = c|x). The softmax function
softmax
takes a vector z = [z1,z2,...,zk] of k arbitrary values and maps them to a probability
distribution, with each value in the range (0,1), and all the values summing to 1.
Like the sigmoid, it is an exponential function.
For a vector z of dimensionality k, the softmax is deﬁned as:
softmax(zi) =
ezi
Pk
j=1 ezj 1 ≤i ≤k
(5.31)
The softmax of an input vector z = [z1,z2,...,zk] is thus a vector itself:
softmax(z) =
"
ez1
Pk
i=1 ezi ,
ez2
Pk
i=1 ezi ,...,
ezk
Pk
i=1 ezi
#
(5.32)
The denominator Pk
i=1 ezi is used to normalize all the values into probabilities.
Thus for example given a vector:
z = [0.6,1.1,−1.5,1.2,3.2,−1.1]
the result softmax(z) is
[0.055,0.090,0.0067,0.10,0.74,0.010]
Again like the sigmoid, the input to the softmax will be the dot product between
a weight vector w and an input vector x (plus a bias). But now we’ll need separate
weight vectors (and bias) for each of the K classes.
p(y = c|x) =
ewc ·x+bc
k
X
j=1
ew j ·x+b j
(5.33)
Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.
Thus if one of the inputs is larger than the others, it will tend to push its probability
toward 1, and suppress the probabilities of the smaller inputs.

90
CHAPTER 5
•
LOGISTIC REGRESSION
5.6.1
Features in Multinomial Logistic Regression
For multiclass classiﬁcation the input features need to be a function of both the
observation x and the candidate output class c. Thus instead of the notation xi, fi
or fi(x), when we’re discussing features we will use the notation fi(c,x), meaning
feature i for a particular class c for a given observation x.
In binary classiﬁcation, a positive weight on a feature pointed toward y=1 and
a negative weight toward y=0, but in multiclass classiﬁcation a feature could be
evidence for or against an individual class.
Let’s look at some sample features for a few NLP tasks to help understand this
perhaps unintuitive use of features that are functions of both the observation x and
the class c.
Suppose we are doing text classiﬁcation, and instead of binary classiﬁcation our
task is to assign one of the 3 classes +, −, or 0 (neutral) to a document. Now a
feature related to exclamation marks might have a negative weight for 0 documents,
and a positive weight for + or −documents:
Var
Deﬁnition
Wt
f1(0,x)
 1 if “!” ∈doc
0 otherwise
−4.5
f1(+,x)

1 if “!” ∈doc
0 otherwise
2.6
f1(−,x)
 1 if “!” ∈doc
0 otherwise
1.3
5.6.2
Learning in Multinomial Logistic Regression
Multinomial logistic regression has a slightly different loss function than binary lo-
gistic regression because it uses the softmax rather than the sigmoid classiﬁer. The
loss function for a single example x is the sum of the logs of the K output classes:
LCE(ˆy,y) = −
K
X
k=1
1{y = k}log p(y = k|x)
= −
K
X
k=1
1{y = k}log
ewk·x+bk
PK
j=1 ewj·x+b j
(5.34)
This makes use of the function 1{} which evaluates to 1 if the condition in the
brackets is true and to 0 otherwise.
The gradient for a single example turns out to be very similar to the gradient for
logistic regression, although we don’t show the derivation here. It is the difference
between the value for the true class k (which is 1) and the probability the classiﬁer
outputs for class k, weighted by the value of the input xk:
∂LCE
∂wk
= −(1{y = k}−p(y = k|x))xk
= −
 
1{y = k}−
ewk·x+bk
PK
j=1 ew j·x+b j
!
xk
(5.35)

5.7
•
INTERPRETING MODELS
91
5.7
Interpreting models
Often we want to know more than just the correct classiﬁcation of an observation.
We want to know why the classiﬁer made the decision it did. That is, we want our
decision to be interpretable. Interpretability can be hard to deﬁne strictly, but the
interpretable
core idea is that as humans we should know why our algorithms reach the conclu-
sions they do. Because the features to logistic regression are often human-designed,
one way to understand a classiﬁer’s decision is to understand the role each feature
plays in the decision. Logistic regression can be combined with statistical tests (the
likelihood ratio test, or the Wald test); investigating whether a particular feature is
signiﬁcant by one of these tests, or inspecting its magnitude (how large is the weight
w associated with the feature?) can help us interpret why the classiﬁer made the
decision it makes. This is enormously important for building transparent models.
Furthermore, in addition to its use as a classiﬁer, logistic regression in NLP and
many other ﬁelds is widely used as an analytic tool for testing hypotheses about the
effect of various explanatory variables (features). In text classiﬁcation, perhaps we
want to know if logically negative words (no, not, never) are more likely to be asso-
ciated with negative sentiment, or if negative reviews of movies are more likely to
discuss the cinematography. However, in doing so it’s necessary to control for po-
tential confounds: other factors that might inﬂuence sentiment (the movie genre, the
year it was made, perhaps the length of the review in words). Or we might be study-
ing the relationship between NLP-extracted linguistic features and non-linguistic
outcomes (hospital readmissions, political outcomes, or product sales), but need to
control for confounds (the age of the patient, the county of voting, the brand of the
product). In such cases, logistic regression allows us to test whether some feature is
associated with some outcome above and beyond the effect of other features.
5.8
Advanced: Deriving the Gradient Equation
In this section we give the derivation of the gradient of the cross-entropy loss func-
tion LCE for logistic regression. Let’s start with some quick calculus refreshers.
First, the derivative of ln(x):
d
dx ln(x) = 1
x
(5.36)
Second, the (very elegant) derivative of the sigmoid:
dσ(z)
dz
= σ(z)(1−σ(z))
(5.37)
Finally, the chain rule of derivatives. Suppose we are computing the derivative
chain rule
of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of
u(x) with respect to v(x) times the derivative of v(x) with respect to x:
d f
dx = du
dv · dv
dx
(5.38)
First, we want to know the derivative of the loss function with respect to a single
weight w j (we’ll need to compute it for each weight, and for the bias):

92
CHAPTER 5
•
LOGISTIC REGRESSION
∂LL(w,b)
∂w j
=
∂
∂w j
−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
= −
 ∂
∂w j
ylogσ(w·x+b)+ ∂
∂w j
(1−y)log[1−σ(w·x+b)]

(5.39)
Next, using the chain rule, and relying on the derivative of log:
∂LL(w,b)
∂w j
= −
y
σ(w·x+b)
∂
∂w j
σ(w·x+b)−
1−y
1−σ(w·x+b)
∂
∂w j
1−σ(w·x+b)
(5.40)
Rearranging terms:
∂LL(w,b)
∂w j
= −

y
σ(w·x+b) −
1−y
1−σ(w·x+b)
 ∂
∂w j
σ(w·x+b)
(5.41)
And now plugging in the derivative of the sigmoid, and using the chain rule one
more time, we end up with Eq. 5.42:
∂LL(w,b)
∂w j
= −

y−σ(w·x+b)
σ(w·x+b)[1−σ(w·x+b)]

σ(w·x+b)[1−σ(w·x+b)]∂(w·x+b)
∂wj
= −

y−σ(w·x+b)
σ(w·x+b)[1−σ(w·x+b)]

σ(w·x+b)[1−σ(w·x+b)]xj
= −[y−σ(w·x+b)]xj
= [σ(w·x+b)−y]xj
(5.42)
5.9
Summary
This chapter introduced the logistic regression model of classiﬁcation.
• Logistic regression is a supervised machine learning classiﬁer that extracts
real-valued features from the input, multiplies each by a weight, sums them,
and passes the sum through a sigmoid function to generate a probability. A
threshold is used to make a decision.
• Logistic regression can be used with two classes (e.g., positive and negative
sentiment) or with multiple classes (multinomial logistic regression, for ex-
ample for n-ary text classiﬁcation, part-of-speech labeling, etc.).
• Multinomial logistic regression uses the softmax function to compute proba-
bilities.
• The weights (vector w and bias b) are learned from a labeled training set via a
loss function, such as the cross-entropy loss, that must be minimized.
• Minimizing this loss function is a convex optimization problem, and iterative
algorithms like gradient descent are used to ﬁnd the optimal weights.
• Regularization is used to avoid overﬁtting.
• Logistic regression is also one of the most useful analytic tools, because of its
ability to transparently study the importance of individual features.

BIBLIOGRAPHICAL AND HISTORICAL NOTES
93
Bibliographical and Historical Notes
Logistic regression was developed in the ﬁeld of statistics, where it was used for
the analysis of binary data by the 1960s, and was particularly common in medicine
(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one
of the formal foundations of the study of linguistic variation (Sankoff and Labov,
1979).
Nonetheless, logistic regression didn’t become common in natural language pro-
cessing until the 1990s, when it seems to have appeared simultaneously from two
directions. The ﬁrst source was the neighboring ﬁelds of information retrieval and
speech processing, both of which had made use of regression, and both of which
lent many other statistical techniques to NLP. Indeed a very early use of logistic
regression for document routing was one of the ﬁrst NLP applications to use (LSI)
embeddings as word representations (Sch¨utze et al., 1995).
At the same time in the early 1990s logistic regression was developed and ap-
plied to NLP at IBM Research under the name maximum entropy modeling or
maximum
entropy
maxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-
der that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech
tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution
(Kehler, 1997b), and text classiﬁcation (Nigam et al., 1999).
More on classiﬁcation can be found in machine learning textbooks (Hastie et al. 2001,
Witten and Frank 2005, Bishop 2006, Murphy 2012).
Exercises

94
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
CHAPTER
6
Vector Semantics and Embed-
dings
The asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the
middle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt
preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene
Epoch. One of these fossils is the Smilodon, or sabre-toothed tiger, instantly rec-
ognizable by its long canines. Five million years ago or so, a completely different
sabre-tooth tiger called Thylacosmilus lived
in Argentina and other parts of South Amer-
ica. Thylacosmilus was a marsupial whereas
Smilodon was a placental mammal, but Thy-
lacosmilus had the same long upper canines
and, like Smilodon, had a protective bone
ﬂange on the lower jaw.
The similarity of
these two mammals is one of many examples
of parallel or convergent evolution, in which particular contexts or environments
lead to the evolution of very similar structures in different species (Gould, 1980).
The role of context is also important in the similarity of a less biological kind
of organism: the word. Words that occur in similar contexts tend to have similar
meanings. This link between similarity in how words are distributed and similarity
in what they mean is called the distributional hypothesis. The hypothesis was
distributional
hypothesis
ﬁrst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth
(1957), who noticed that words which are synonyms (like oculist and eye-doctor)
tended to occur in the same environment (e.g., near words like eye or examined)
with the amount of meaning difference between two words “corresponding roughly
to the amount of difference in their environments” (Harris, 1954, 157).
In this chapter we introduce vector semantics, which instantiates this linguistic
vector
semantics
hypothesis by learning representations of the meaning of words, called embeddings,
embeddings
directly from their distributions in texts. These representations are used in every
natural language processing application that makes use of meaning, and underlie the
more powerful contextualized word representations like ELMo and BERT that
we will introduce in Chapter 10.
These word representations are also the ﬁrst example in this book of repre-
sentation learning, automatically learning useful representations of the input text.
representation
learning
Finding such self-supervised ways to learn representations of the input, instead of
creating representations by hand via feature engineering, is an important focus of
NLP research (Bengio et al., 2013).
We’ll begin, however, by introducing some basic principles of word meaning,
which will motivate the vector semantic models of this chapter as well as extensions
that we’ll return to in Chapter 19, Chapter 20, and Chapter 21.

6.1
•
LEXICAL SEMANTICS
95
6.1
Lexical Semantics
How should we represent the meaning of a word? In the N-gram models we saw
in Chapter 3, and in many traditional NLP applications, our only representation of
a word is as a string of letters, or perhaps as an index in a vocabulary list. This
representation is not that different from a tradition in philosophy, perhaps you’ve
seen it in introductory logic classes, in which the meaning of words is represented
by just spelling the word with small capital letters; representing the meaning of
“dog” as DOG, and “cat” as CAT).
Representing the meaning of a word by capitalizing it is a pretty unsatisfactory
model. You might have seen the old philosophy joke:
Q: What’s the meaning of life?
A: LIFE
Surely we can do better than this! After all, we’ll want a model of word meaning
to do all sorts of things for us. It should tell us that some words have similar mean-
ings (cat is similar to dog), other words are antonyms (cold is the opposite of hot). It
should know that some words have positive connotations (happy) while others have
negative connotations (sad). It should represent the fact that the meanings of buy,
sell, and pay offer differing perspectives on the same underlying purchasing event
(If I buy something from you, you’ve probably sold it to me, and I likely paid you).
More generally, a model of word meaning should allow us to draw useful infer-
ences that will help us solve meaning-related tasks like question-answering, sum-
marization, detecting paraphrases or plagiarism, and dialogue.
In this section we summarize some of these desiderata, drawing on results in the
linguistic study of word meaning, which is called lexical semantics; we’ll return to
lexical
semantics
and expand on this list in Chapter 19.
Lemmas and Senses
Let’s start by looking at how one word (we’ll choose mouse)
might be deﬁned in a dictionary: 1
mouse (N)
1.
any of numerous small rodents...
2.
a hand-operated device that controls a cursor...
Here the form mouse is the lemma, also called the citation form. The form
lemma
citation form
mouse would also be the lemma for the word mice; dictionaries don’t have separate
deﬁnitions for inﬂected forms like mice. Similarly sing is the lemma for sing, sang,
sung. In many languages the inﬁnitive form is used as the lemma for the verb, so
Spanish dormir “to sleep” is the lemma for duermes “you sleep”. The speciﬁc forms
sung or carpets or sing or duermes are called wordforms.
wordform
As the example above shows, each lemma can have multiple meanings; the
lemma mouse can refer to the rodent or the cursor control device. We call each
of these aspects of the meaning of mouse a word sense. The fact that lemmas can
be polysemous (have multiple senses) can make interpretation difﬁcult (is someone
who types “mouse info” into a search engine looking for a pet or a tool?). Chapter 19
will discuss the problem of polysemy, and introduce word sense disambiguation,
the task of determining which sense of a word is being used in a particular context.
Synonymy
One important component of word meaning is the relationship be-
tween word senses. For example when one word has a sense whose meaning is
1
This example shortened from the online dictionary WordNet, discussed in Chapter 19.

96
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
identical to a sense of another word, or nearly identical, we say the two senses of
those two words are synonyms. Synonyms include such pairs as
synonym
couch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile
A more formal deﬁnition of synonymy (between words rather than senses) is that
two words are synonymous if they are substitutable one for the other in any sentence
without changing the truth conditions of the sentence, the situations in which the
sentence would be true. We often say in this case that the two words have the same
propositional meaning.
propositional
meaning
While substitutions between some pairs of words like car / automobile or wa-
ter / H2O are truth preserving, the words are still not identical in meaning. Indeed,
probably no two words are absolutely identical in meaning. One of the fundamen-
tal tenets of semantics, called the principle of contrast (Girard 1718, Br´eal 1897,
principle of
contrast
Clark 1987), is the assumption that a difference in linguistic form is always associ-
ated with at least some difference in meaning. For example, the word H2O is used
in scientiﬁc contexts and would be inappropriate in a hiking guide—water would be
more appropriate— and this difference in genre is part of the meaning of the word.
In practice, the word synonym is therefore commonly used to describe a relationship
of approximate or rough synonymy.
Word Similarity
While words don’t have many synonyms, most words do have
lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly
similar words. In moving from synonymy to similarity, it will be useful to shift from
talking about relations between word senses (like synonymy) to relations between
words (like similarity). Dealing with words avoids having to commit to a particular
representation of word senses, which will turn out to simplify our task.
The notion of word similarity is very useful in larger semantic tasks. Know-
similarity
ing how similar two words are can help in computing how similar the meaning of
two phrases or sentences are, a very important component of natural language un-
derstanding tasks like question answering, paraphrasing, and summarization. One
way of getting values for word similarity is to ask humans to judge how similar one
word is to another. A number of datasets have resulted from such experiments. For
example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to
10, like the examples below, which range from near-synonyms (vanish, disappear)
to pairs that scarcely seem to have anything in common (hole, agreement):
vanish
disappear
9.8
behave obey
7.3
belief
impression 5.95
muscle bone
3.65
modest ﬂexible
0.98
hole
agreement
0.3
Word Relatedness
The meaning of two words can be related in ways other than
similarity. One such class of connections is called word relatedness (Budanitsky
relatedness
and Hirst, 2006), also traditionally called word association in psychology.
association
Consider the meanings of the words coffee and cup. Coffee is not similar to cup;
they share practically no features (coffee is a plant or a beverage, while a cup is a
manufactured object with a particular shape).
But coffee and cup are clearly related; they are associated by co-participating in
an everyday event (the event of drinking coffee out of a cup). Similarly the nouns

6.1
•
LEXICAL SEMANTICS
97
scalpel and surgeon are not similar but are related eventively (a surgeon tends to
make use of a scalpel).
One common kind of relatedness between words is if they belong to the same
semantic ﬁeld. A semantic ﬁeld is a set of words which cover a particular semantic
semantic ﬁeld
domain and bear structured relations with each other.
For example, words might be related by being in the semantic ﬁeld of hospitals
(surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food,
chef), or houses (door, roof, kitchen, family, bed).
Semantic ﬁelds are also related to topic models, like Latent Dirichlet Alloca-
topic models
tion, LDA, which apply unsupervised learning on large sets of texts to induce sets of
associated words from text. Semantic ﬁelds and topic models are very useful tools
for discovering topical structure in documents.
In Chapter 19 we’ll introduce even more relations between senses, including
hypernymy or IS-A, antonymy (opposite meaning) and meronymy) (part-whole
relations).
Semantic Frames and Roles
Closely related to semantic ﬁelds is the idea of a
semantic frame. A semantic frame is a set of words that denote perspectives or
semantic frame
participants in a particular type of event. A commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
some good or service, after which the good changes hands or perhaps the service is
performed. This event can be encoded lexically by using verbs like buy (the event
from the perspective of the buyer), sell (from the perspective of the seller), pay
(focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles
(like buyer, seller, goods, money), and words in a sentence can take on these roles.
Knowing that buy and sell have this relation makes it possible for a system to
know that a sentence like Sam bought the book from Ling could be paraphrased as
Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and
Ling the seller. Being able to recognize such paraphrases is important for question
answering, and can help in shifting perspective for machine translation.
Connotation
Finally, words have affective meanings or connotations. The word
connotations
connotation has different meanings in different ﬁelds, but here we use it to mean
the aspects of a word’s meaning that are related to a writer or reader’s emotions,
sentiment, opinions, or evaluations. For example some words have positive conno-
tations (happy) while others have negative connotations (sad). Some words describe
positive evaluation (great, love) and others negative evaluation (terrible, hate). Pos-
itive or negative evaluation expressed through language is called sentiment, as we
sentiment
saw in Chapter 4, and word sentiment plays a role in important tasks like sentiment
analysis, stance detection, and many applications of natural language processing to
the language of politics and consumer reviews.
Early work on affective meaning (Osgood et al., 1957) found that words varied
along three important dimensions of affective meaning. These are now generally
called valence, arousal, and dominance, deﬁned as follows:
valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus
Thus words like happy or satisﬁed are high on valence, while unhappy or an-
noyed are low on valence. Excited or frenzied are high on arousal, while relaxed
or calm are low on arousal. Important or controlling are high on dominance, while
awed or inﬂuenced are low on dominance. Each word is thus represented by three

98
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
numbers, corresponding to its value on each of the three dimensions, like the exam-
ples below:
Valence Arousal Dominance
courageous 8.05
5.5
7.38
music
7.67
5.57
6.5
heartbreak
2.45
5.65
3.58
cub
6.71
3.95
4.24
life
6.68
5.59
5.89
Osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word’s
rating on the three scales. This revolutionary idea that word meaning word could
be represented as a point in space (e.g., that part of the meaning of heartbreak can
be represented as the point [2.45,5.65,3.58]) was the ﬁrst expression of the vector
semantics models that we introduce next.
6.2
Vector Semantics
How can we build a computational model that successfully deals with the different
aspects of word meaning we saw in the previous section (word senses, word simi-
larity and relatedness, lexical ﬁelds and frames, connotation)?
A perfect model that completely deals with each of these aspects of word mean-
ing turns out to be elusive. But the current best model, called vector semantics,
vector
semantics
draws its inspiration from linguistic and philosophical work of the 1950’s.
During that period, the philosopher Ludwig Wittgenstein, skeptical of the possi-
bility of building a completely formal theory of meaning deﬁnitions for each word,
suggested instead that “the meaning of a word is its use in the language” (Wittgen-
stein, 1953, PI 43). That is, instead of using some logical language to deﬁne each
word, we should deﬁne words by some representation of how the word was used by
actual people in speaking and understanding.
Linguists of the period like Joos (1950), Harris (1954), and Firth (1957) (the lin-
guistic distributionalists), came up with a speciﬁc idea for realizing Wittgenstein’s
intuition: deﬁne a word by its environment or distribution in language use. A word’s
distribution is the set of contexts in which it occurs, the neighboring words or gram-
matical environments. The idea is that two words that occur in very similar dis-
tributions (that occur together with very similar words) are likely to have the same
meaning.
Let’s see an example illustrating this distributionalist approach. Suppose you
didn’t know what the Cantonese word ongchoi meant, but you do see it in the fol-
lowing sentences or contexts:
(6.1) Ongchoi is delicious sauteed with garlic.
(6.2) Ongchoi is superb over rice.
(6.3) ...ongchoi leaves with salty sauces...
And furthermore let’s suppose that you had seen many of these context words
occurring in contexts like:
(6.4) ...spinach sauteed with garlic over rice...

6.2
•
VECTOR SEMANTICS
99
(6.5) ...chard stems and leaves are delicious...
(6.6) ...collard greens and other salty leafy greens
The fact that ongchoi occurs with words like rice and garlic and delicious and
salty, as do words like spinach, chard, and collard greens might suggest to the reader
that ongchoi is a leafy green similar to these other leafy greens.2
We can do the same thing computationally by just counting words in the context
of ongchoi; we’ll tend to see words like sauteed and eaten and garlic. The fact that
these words and other similar context words also occur around the word spinach or
collard greens can help us discover the similarity between these words and ongchoi.
Vector semantics thus combines two intuitions: the distributionalist intuition
(deﬁning a word by counting what other words occur in its environment), and the
vector intuition of Osgood et al. (1957) we saw in the last section on connotation:
deﬁning the meaning of a word w as a vector, a list of numbers, a point in N-
dimensional space. There are various versions of vector semantics, each deﬁning
the numbers in the vector somewhat differently, but in each case the numbers are
based in some way on counts of neighboring words.
good
nice
bad
worst
not good
wonderful
amazing
terriﬁc
dislike
worse
very good
incredibly good
fantastic
incredibly bad
now
you
i
that
with
by
to
’s
are
is
a
than
Figure 6.1
A two-dimensional (t-SNE) projection of embeddings for some words and
phrases, showing that words with similar meanings are nearby in space. The original 60-
dimensional embeddings were trained for sentiment analysis. Simpliﬁed from Li et al. (2015).
The idea of vector semantics is thus to represent a word as a point in some multi-
dimensional semantic space. Vectors for representing words are generally called
embeddings, because the word is embedded in a particular vector space. Fig. 6.1
embeddings
displays a visualization of embeddings that were learned for a sentiment analysis
task, showing the location of some selected words projected down from the original
60-dimensional space into a two dimensional space.
Notice that positive and negative words seem to be located in distinct portions of
the space (and different also from the neutral function words). This suggests one of
the great advantages of vector semantics: it offers a ﬁne-grained model of meaning
that lets us also implement word similarity (and phrase similarity). For example,
the sentiment analysis classiﬁer we saw in Chapter 4 only works if enough of the
important sentimental words that appear in the test set also appeared in the training
set. But if words were represented as embeddings, we could assign sentiment as
long as words with similar meanings as the test set words occurred in the training
2
It’s in fact Ipomoea aquatica, a relative of morning glory sometimes called water spinach in English.

100
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
set. Vector semantic models are also extremely practical because they can be learned
automatically from text without any complex labeling or supervision.
As a result of these advantages, vector models of meaning are now the standard
way to represent the meaning of words in NLP. In this chapter we’ll introduce the
two most commonly used models. First is the tf-idf model, often used as a baseline,
in which the meaning of a word is deﬁned by a simple function of the counts of
nearby words. We will see that this method results in very long vectors that are
sparse, i.e. contain mostly zeros (since most words simply never occur in the context
of others).
Then we’ll introduce the word2vec model, one of a family of models that are
ways of constructing short, dense vectors that have useful semantic properties.
We’ll also introduce the cosine, the standard way to use embeddings (vectors)
to compute functions like semantic similarity, the similarity between two words,
two sentences, or two documents, an important tool in practical applications like
question answering, summarization, or automatic essay grading.
6.3
Words and Vectors
Vector or distributional models of meaning are generally based on a co-occurrence
matrix, a way of representing how often words co-occur. This matrix can be con-
structed in various ways; let’s begin by looking at one such co-occurrence matrix, a
term-document matrix.
6.3.1
Vectors and documents
In a term-document matrix, each row represents a word in the vocabulary and each
term-document
matrix
column represents a document from some collection of documents. Fig. 6.2 shows a
small selection from a term-document matrix showing the occurrence of four words
in four plays by Shakespeare. Each cell in this matrix represents the number of times
a particular word (deﬁned by the row) occurs in a particular document (deﬁned by
the column). Thus fool appeared 58 times in Twelfth Night.
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.2
The term-document matrix for four words in four Shakespeare plays. Each cell
contains the number of times the (row) word occurs in the (column) document.
The term-document matrix of Fig. 6.2 was ﬁrst deﬁned as part of the vector
space model of information retrieval (Salton, 1971). In this model, a document is
vector space
model
represented as a count vector, a column in Fig. 6.3.
To review some basic linear algebra, a vector is, at heart, just a list or array
vector
of numbers. So As You Like It is represented as the list [1,114,36,20] and Julius
Caesar is represented as the list [7,62,1,2]. A vector space is a collection of vectors,
vector space
characterized by their dimension. In the example in Fig. 6.3, the vectors are of
dimension
dimension 4, just so they ﬁt on the page; in real term-document matrices, the vectors
representing each document would have dimensionality |V|, the vocabulary size.

6.3
•
WORDS AND VECTORS
101
The ordering of the numbers in a vector space is not arbitrary; each position
indicates a meaningful dimension on which the documents can vary. Thus the ﬁrst
dimension for both these vectors corresponds to the number of times the word battle
occurs, and we can compare each dimension, noting for example that the vectors for
As You Like It and Twelfth Night have similar values (1 and 0, respectively) for the
ﬁrst dimension.
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
1
0
7
13
good
114
80
62
89
fool
36
58
1
4
wit
20
15
2
3
Figure 6.3
The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each document is represented as a column vector of length four.
We can think of the vector for a document as identifying a point in |V|-dimensional
space; thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-
dimensional spaces are hard to draw in textbooks, Fig. 6.4 shows a visualization in
two dimensions; we’ve arbitrarily chosen the dimensions corresponding to the words
battle and fool.
5
10
15
20
25
30
5
10
Henry V [4,13]
As You Like It [36,1]
Julius Caesar [1,7]
battle
 fool
Twelfth Night [58,0]
15
40
35
40
45
50
55
60
Figure 6.4
A spatial visualization of the document vectors for the four Shakespeare play
documents, showing just two of the dimensions, corresponding to the words battle and fool.
The comedies have high values for the fool dimension and low values for the battle dimension.
Term-document matrices were originally deﬁned as a means of ﬁnding similar
documents for the task of document information retrieval. Two documents that are
similar will tend to have similar words, and if two documents have similar words
their column vectors will tend to be similar. The vectors for the comedies As You
Like It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other
(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or
Henry V [13,89,4,3]. This is clear with the raw numbers; in the ﬁrst dimension
(battle) the comedies have low numbers and the others have high numbers, and we
can see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition
more formally.
A real term-document matrix, of course, wouldn’t just have 4 rows and columns,
let alone 2. More generally, the term-document matrix has |V| rows (one for each
word type in the vocabulary) and D columns (one for each document in the collec-
tion); as we’ll see, vocabulary sizes are generally in the tens of thousands, and the
number of documents can be enormous (think about all the pages on the web).

102
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
Information retrieval (IR) is the task of ﬁnding the document d from the D
information
retrieval
documents in some collection that best matches a query q. For IR we’ll therefore also
represent a query by a vector, also of length |V|, and we’ll need a way to compare
two vectors to ﬁnd how similar they are. (Doing IR will also require efﬁcient ways
to store and manipulate these vectors by making use of the convenient fact that these
vectors are sparse, i.e., mostly zeros).
Later in the chapter we’ll introduce some of the components of this vector com-
parison process: the tf-idf term weighting, and the cosine similarity metric.
6.3.2
Words as vectors
We’ve seen that documents can be represented as vectors in a vector space. But
vector semantics can also be used to represent the meaning of words, by associating
each word with a vector.
The word vector is now a row vector rather than a column vector, and hence the
row vector
dimensions of the vector are different. The four dimensions of the vector for fool,
[36,58,1,4], correspond to the four Shakespeare plays. The same four dimensions
are used to form the vectors for the other 3 words: wit, [20,15,2,3]; battle, [1,0,7,13];
and good [114,80,62,89]. Each entry in the vector thus represents the counts of the
word’s occurrence in the document corresponding to that dimension.
For documents, we saw that similar documents had similar vectors, because sim-
ilar documents tend to have similar words. This same principle applies to words:
similar words have similar vectors because they tend to occur in similar documents.
The term-document matrix thus lets us represent the meaning of a word by the doc-
uments it tends to occur in.
However, it is most common to use a different kind of context for the dimensions
of a word’s vector representation. Rather than the term-document matrix we use the
term-term matrix, more commonly called the word-word matrix or the term-
word-word
matrix
context matrix, in which the columns are labeled by words rather than documents.
This matrix is thus of dimensionality |V|×|V| and each cell records the number of
times the row (target) word and the column (context) word co-occur in some context
in some training corpus. The context could be the document, in which case the cell
represents the number of times the two words appear in the same document. It is
most common, however, to use smaller contexts, generally a window around the
word, for example of 4 words to the left and 4 words to the right, in which case
the cell represents the number of times (in some training corpus) the column word
occurs in such a ±4 word window around the row word. For example here is one
example each of some words in their windows:
is traditionally followed by cherry
pie, a traditional dessert
often mixed, such as strawberry
rhubarb pie. Apple pie
computer peripherals and personal digital
assistants. These devices usually
a computer. This includes information available on the internet
If we then take every occurrence of each word (say strawberry) and count the con-
text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a
simpliﬁed subset of the word-word co-occurrence matrix for these four words com-
puted from the Wikipedia corpus (Davies, 2015).
Note in Fig. 6.5 that the two words cherry and strawberry are more similar to
each other (both pie and sugar tend to occur in their window) than they are to other
words like digital; conversely, digital and information are more similar to each other
than, say, to strawberry. Fig. 6.6 shows a spatial visualization.

6.4
•
COSINE FOR MEASURING SIMILARITY
103
aardvark
...
computer
data
result
pie
sugar
...
cherry
0
...
2
8
9
442
25
strawberry
0
...
0
0
1
60
19
digital
0
...
1670
1683
85
5
4
information
0
...
3325
3982
378
5
13
Figure 6.5
Co-occurrence vectors for four words in the Wikipedia corpus, showing six of
the dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in
red. Note that a real vector would have vastly more dimensions and thus be much sparser.
1000 2000 3000 4000
1000
2000
digital
 [1683,1670]
computer
 data
information
 [3982,3325] 
3000
4000
Figure 6.6
A spatial visualization of word vectors for digital and information, showing just
two of the dimensions, corresponding to the words data and computer.
Note that |V|, the length of the vector, is generally the size of the vocabulary,
usually between 10,000 and 50,000 words (using the most frequent words in the
training corpus; keeping words after about the most frequent 50,000 or so is gener-
ally not helpful). But of course since most of these numbers are zero these are sparse
vector representations, and there are efﬁcient algorithms for storing and computing
with sparse matrices.
Now that we have some intuitions, let’s move on to examine the details of com-
puting word similarity. Afterwards we’ll discuss the tf-idf method of weighting
cells.
6.4
Cosine for measuring similarity
To deﬁne similarity between two target words v and w, we need a measure for taking
two such vectors and giving a measure of vector similarity. By far the most common
similarity metric is the cosine of the angle between the vectors.
The cosine—like most measures for vector similarity used in NLP—is based on
the dot product operator from linear algebra, also called the inner product:
dot product
inner product
dot product(v,w) = v ·w =
N
X
i=1
viwi = v1w1 +v2w2 +...+vNwN
(6.7)
As we will see, most metrics for similarity between vectors are based on the dot
product. The dot product acts as a similarity metric because it will tend to be high
just when the two vectors have large values in the same dimensions. Alternatively,
vectors that have zeros in different dimensions—orthogonal vectors—will have a
dot product of 0, representing their strong dissimilarity.

104
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
This raw dot product, however, has a problem as a similarity metric: it favors
long vectors. The vector length is deﬁned as
vector length
|v| =
v
u
u
t
N
X
i=1
v2
i
(6.8)
The dot product is higher if a vector is longer, with higher values in each dimension.
More frequent words have longer vectors, since they tend to co-occur with more
words and have higher co-occurrence values with each of them. The raw dot product
thus will be higher for frequent words. But this is a problem; we’d like a similarity
metric that tells us how similar two words are regardless of their frequency.
The simplest way to modify the dot product to normalize for the vector length is
to divide the dot product by the lengths of each of the two vectors. This normalized
dot product turns out to be the same as the cosine of the angle between the two
vectors, following from the deﬁnition of the dot product between two vectors a and
b:
a·b = |a||b|cosθ
a·b
|a||b| = cosθ
(6.9)
The cosine similarity metric between two vectors v and w thus can be computed as:
cosine
cosine(v,w) = v ·w
|v||w| =
N
X
i=1
viwi
v
u
u
t
N
X
i=1
v2
i
v
u
u
t
N
X
i=1
w2
i
(6.10)
For some applications we pre-normalize each vector, by dividing it by its length,
creating a unit vector of length 1. Thus we could compute a unit vector from a by
unit vector
dividing it by |a|. For unit vectors, the dot product is the same as the cosine.
The cosine value ranges from 1 for vectors pointing in the same direction, through
0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.
But raw frequency values are non-negative, so the cosine for these vectors ranges
from 0–1.
Let’s see how the cosine computes which of the words cherry or digital is closer
in meaning to information, just using raw counts from the following shortened table:
pie
data computer
cherry
442
8
2
digital
5
1683
1670
information
5
3982
3325
cos(cherry,information) =
442∗5+8∗3982+2∗3325
√
4422 +82 +22√
52 +39822 +33252 = .017
cos(digital,information) =
5∗5+1683∗3982+1670∗3325
√
52 +16832 +16702√
52 +39822 +33252 = .996
The model decides that information is way closer to digital than it is to cherry, a
result that seems sensible. Fig. 6.7 shows a visualization.

6.5
•
TF-IDF: WEIGHING TERMS IN THE VECTOR
105
500
1000
1500
2000
2500
3000
500
digital
cherry
information
Dimension 1: ‘pie’
Dimension 2: ‘computer’
Figure 6.7
A (rough) graphical demonstration of cosine similarity, showing vectors for
three words (cherry, digital, and information) in the two dimensional space deﬁned by counts
of the words computer and pie nearby. Note that the angle between digital and information is
smaller than the angle between cherry and information. When two vectors are more similar,
the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the angle
between two vectors is smallest (0◦); the cosine of all other angles is less than 1.
6.5
TF-IDF: Weighing terms in the vector
The co-occurrence matrix in Fig. 6.5 represented each cell by the raw frequency of
the co-occurrence of two words.
It turns out, however, that simple frequency isn’t the best measure of association
between words. One problem is that raw frequency is very skewed and not very
discriminative. If we want to know what kinds of contexts are shared by cherry and
strawberry but not by digital and information, we’re not going to get good discrimi-
nation from words like the, it, or they, which occur frequently with all sorts of words
and aren’t informative about any particular word. We saw this also in Fig. 6.3 for
the Shakespeare corpus; the dimension for the word good is not very discrimina-
tive between plays; good is simply a frequent word and has roughly equivalent high
frequencies in each of the plays.
It’s a bit of a paradox. Words that occur nearby frequently (maybe pie nearby
cherry) are more important than words that only appear once or twice. Yet words
that are too frequent—ubiquitous, like the or good— are unimportant. How can we
balance these two conﬂicting constraints?
The tf-idf algorithm (the ‘-’ here is a hyphen, not a minus sign) is the product
of two terms, each term capturing one of these two intuitions:
The ﬁrst is the term frequency (Luhn, 1957): the frequency of the word t in the
term frequency
document d. We can just use the raw count as the term frequency:
tft,d = count(t,d)
(6.11)
Alternatively we can squash the raw frequency a bit, by using the log10 of the fre-
quency instead. The intuition is that a word appearing 100 times in a document
doesn’t make that word 100 times more likely to be relevant to the meaning of the
document. Because we can’t take the log of 0, we normally add 1 to the count:3
tft,d = log10(count(t,d)+1)
(6.12)
If we use log weighting, terms which occur 10 times in a document would have a
tf=2, 100 times in a document tf=3, 1000 times tf=4, and so on.
3
Or we can use this alternative: tft,d =
 1+log10 count(t,d)
if count(t,d) > 0
0
otherwise

106
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
The second factor is used to give a higher weight to words that occur only in a
few documents. Terms that are limited to a few documents are useful for discrimi-
nating those documents from the rest of the collection; terms that occur frequently
across the entire collection aren’t as helpful. The document frequency dft of a
document
frequency
term t is the number of documents it occurs in. Document frequency is not the
same as the collection frequency of a term, which is the total number of times the
word appears in the whole collection in any document. Consider in the collection of
Shakespeare’s 37 plays the two words Romeo and action. The words have identical
collection frequencies (they both occur 113 times in all the plays) but very different
document frequencies, since Romeo only occurs in a single play. If our goal is ﬁnd
documents about the romantic tribulations of Romeo, the word Romeo should be
highly weighted, but not action:
Collection Frequency Document Frequency
Romeo 113
1
action
113
31
We emphasize discriminative words like Romeo via the inverse document fre-
quency or idf term weight (Sparck Jones, 1972). The idf is deﬁned using the frac-
idf
tion N/dft, where N is the total number of documents in the collection, and dft is
the number of documents in which term t occurs. The fewer documents in which a
term occurs, the higher this weight. The lowest weight of 1 is assigned to terms that
occur in all the documents. It’s usually clear what counts as a document: in Shake-
speare we would use a play; when processing a collection of encyclopedia articles
like Wikipedia, the document is a Wikipedia page; in processing newspaper articles,
the document is a single article. Occasionally your corpus might not have appropri-
ate document divisions and you might need to break up the corpus into documents
yourself for the purposes of computing idf.
Because of the large number of documents in many collections, this measure
too is usually squashed with a log function. The resulting deﬁnition for inverse
document frequency (idf) is thus
idft = log10
 N
dft

(6.13)
Here are some idf values for some words in the Shakespeare corpus, ranging from
extremely informative words which occur in only one play like Romeo, to those that
occur in a few like salad or Falstaff, to those which are very common like fool or so
common as to be completely non-discriminative since they occur in all 37 plays like
good or sweet.4
Word
df
idf
Romeo
1
1.57
salad
2
1.27
Falstaff
4
0.967
forest
12
0.489
battle
21
0.246
wit
34
0.037
fool
36
0.012
good
37
0
sweet
37
0
4
Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of
sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).

6.6
•
APPLICATIONS OF THE TF-IDF VECTOR MODEL
107
The tf-idf weighted value wt,d for word t in document d thus combines term
tf-idf
frequency tft,d (deﬁned either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:
wt,d = tft,d ×idft
(6.14)
Fig. 6.8 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2,
using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-
sponding to the word good have now all become 0; since this word appears in every
document, the tf-idf algorithm leads it to be ignored in any comparison of the plays.
Similarly, the word fool, which appears in 36 out of the 37 plays, has a much lower
weight.
As You Like It
Twelfth Night
Julius Caesar
Henry V
battle
0.074
0
0.22
0.28
good
0
0
0
0
fool
0.019
0.021
0.0036
0.0083
wit
0.049
0.044
0.018
0.022
Figure 6.8
A tf-idf weighted term-document matrix for four words in four Shakespeare
plays, using the counts in Fig. 6.2. For example the 0.049 value for wit in As You Like It is
the product of tf = log10(20 + 1) = 1.322 and idf = .037. Note that the idf weighting has
eliminated the importance of the ubiquitous word good and vastly reduced the impact of the
almost-ubiquitous word fool.
The tf-idf weighting is the way for weighting co-occurrence matrices in infor-
mation retrieval, but also plays a role in many other aspects of natural language
processing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll look at other
weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.7.
6.6
Applications of the tf-idf vector model
In summary, the vector semantics model we’ve described so far represents a target
word as a vector with dimensions corresponding to all the words in the vocabulary
(length |V|, with vocabularies of 20,000 to 50,000), which is also sparse (most values
are zero). The values in each dimension are the frequency with which the target
word co-occurs with each neighboring context word, weighted by tf-idf. The model
computes the similarity between two words x and y by taking the cosine of their
tf-idf vectors; high cosine, high similarity. This entire model is sometimes referred
to for short as the tf-idf model, after the weighting function.
One common use for a tf-idf model is to compute word similarity, a useful tool
for tasks like ﬁnding word paraphrases, tracking changes in word meaning, or au-
tomatically discovering meanings of words in different corpora. For example, we
can ﬁnd the 10 most similar words to any target word w by computing the cosines
between w and each of the V −1 other words, sorting, and looking at the top 10.
The tf-idf vector model can also be used to decide if two documents are similar.
We represent a document by taking the vectors of all the words in the document, and
computing the centroid of all those vectors. The centroid is the multidimensional
centroid
version of the mean; the centroid of a set of vectors is a single vector that has the
minimum sum of squared distances to each of the vectors in the set. Given k word
vectors w1,w2,...,wk, the centroid document vector d is:
document
vector
d = w1 +w2 +...+wk
k
(6.15)

108
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
Given two documents, we can then compute their document vectors d1 and d2,
and estimate the similarity between the two documents by cos(d1,d2).
Document similarity is also useful for all sorts of applications; information re-
trieval, plagiarism detection, news recommender systems, and even for digital hu-
manities tasks like comparing different versions of a text to see which are similar to
each other.
6.7
Optional: Pointwise Mutual Information (PMI)
An alternative weighting function to tf-idf is called PPMI (positive pointwise mutual
information). PPMI draws on the intuition that the best way to weigh the association
between two words is to ask how much more the two words co-occur in our corpus
than we would have a priori expected them to appear by chance.
Pointwise mutual information (Fano, 1961)5 is one of the most important con-
pointwise
mutual
information
cepts in NLP. It is a measure of how often two events x and y occur, compared with
what we would expect if they were independent:
I(x,y) = log2
P(x,y)
P(x)P(y)
(6.17)
The pointwise mutual information between a target word w and a context word
c (Church and Hanks 1989, Church and Hanks 1990) is then deﬁned as:
PMI(w,c) = log2
P(w,c)
P(w)P(c)
(6.18)
The numerator tells us how often we observed the two words together (assuming
we compute probability by using the MLE). The denominator tells us how often
we would expect the two words to co-occur assuming they each occurred indepen-
dently; recall that the probability of two independent events both occurring is just
the product of the probabilities of the two events. Thus, the ratio gives us an esti-
mate of how much more the two words co-occur than we expect by chance. PMI is
a useful tool whenever we need to ﬁnd words that are strongly associated.
PMI values range from negative to positive inﬁnity. But negative PMI values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous.
To distinguish whether
two words whose individual probability is each 10−6 occur together less often than
chance, we would need to be certain that the probability of the two occurring to-
gether is signiﬁcantly different than 10−12, and this kind of granularity would require
an enormous corpus. Furthermore it’s not clear whether it’s even possible to evalu-
ate such scores of ‘unrelatedness’ with human judgments. For this reason it is more
common to use Positive PMI (called PPMI) which replaces all negative PMI values
PPMI
5
Pointwise mutual information is based on the mutual information between two random variables X
and Y, which is deﬁned as:
I(X,Y) =
X
x
X
y
P(x,y)log2
P(x,y)
P(x)P(y)
(6.16)
In a confusion of terminology, Fano used the phrase mutual information to refer to what we now call
pointwise mutual information and the phrase expectation of the mutual information for what we now call
mutual information

6.7
•
OPTIONAL: POINTWISE MUTUAL INFORMATION (PMI)
109
with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)6:
PPMI(w,c) = max(log2
P(w,c)
P(w)P(c),0)
(6.19)
More formally, let’s assume we have a co-occurrence matrix F with W rows (words)
and C columns (contexts), where fij gives the number of times word wi occurs in
context cj. This can be turned into a PPMI matrix where ppmiij gives the PPMI
value of word wi with context cj as follows:
pij =
fij
PW
i=1
PC
j=1 fi j pi∗=
PC
j=1 fi j
PW
i=1
PC
j=1 fi j p∗j =
PW
i=1 fi j
PW
i=1
PC
j=1 fi j
(6.20)
PPMIij = max(log2
pij
pi∗p∗j
,0)
(6.21)
Let’s see some PPMI calculations. We’ll use Fig. 6.9, which repeats Fig. 6.5 plus all
the count marginals, and let’s pretend for ease of calculation that these are the only
words/contexts that matter.
computer
data
result
pie
sugar
count(w)
cherry
2
8
9
442
25
486
strawberry
0
0
1
60
19
80
digital
1670
1683
85
5
4
3447
information
3325
3982
378
5
13
7703
count(context)
4997
5673
473
512
61
11716
Figure 6.9
Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,
together with the marginals, pretending for the purpose of this calculation that no other word-
s/contexts matter.
Thus for example we could compute PPMI(w=information,c=data), assuming
we pretended that Fig. 6.5 encompassed all the relevant word contexts/dimensions,
as follows:
P(w=information,c=data) =
3982
11716 = .3399
P(w=information) =
7703
11716 = .6575
P(c=data) =
5673
11716 = .4842
ppmi(information,data) = log2(.3399/(.6575∗.4842)) = .0944
Fig. 6.10 shows the joint probabilities computed from the counts in Fig. 6.9, and
Fig. 6.11 shows the PPMI values. Not surprisingly, cherry and strawberry are highly
associated with both pie and sugar, and data is mildly associated with information.
PMI has the problem of being biased toward infrequent events; very rare words
tend to have very high PMI values. One way to reduce this bias toward low frequency
events is to slightly change the computation for P(c), using a different function Pα(c)
that raises the probability of the context word to the power of α:
PPMIα(w,c) = max(log2
P(w,c)
P(w)Pα(c),0)
(6.22)
6
Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the
−∞from log(0).

110
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
p(w,context)
p(w)
computer
data
result
pie
sugar
p(w)
cherry
0.0002
0.0007
0.0008
0.0377
0.0021
0.0415
strawberry
0.0000
0.0000
0.0001
0.0051
0.0016
0.0068
digital
0.1425
0.1436
0.0073
0.0004
0.0003
0.2942
information
0.2838
0.3399
0.0323
0.0004
0.0011
0.6575
p(context)
0.4265
0.4842
0.0404
0.0437
0.0052
Figure 6.10
Replacing the counts in Fig. 6.5 with joint probabilities, showing the marginals
around the outside.
computer
data
result
pie
sugar
cherry
0
0
0
4.38
3.30
strawberry
0
0
0
4.10
5.51
digital
0.18
0.01
0
0
0
information
0.02
0.09
0.28
0
0
Figure 6.11
The PPMI matrix showing the association between words and context words,
computed from the counts in Fig. 6.10. Note that most of the 0 PPMI values are ones that had
a negative PMI; for example PMI(cherry,computer) = -6.7, meaning that cherry and computer
co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace
negative values by zero.
Pα(c) =
count(c)α
P
c count(c)α
(6.23)
Levy et al. (2015) found that a setting of α = 0.75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams described below in Eq. 6.32). This works because raising the count to α =
0.75 increases the probability assigned to rare contexts, and hence lowers their PMI
(Pα(c) > P(c) when c is rare).
Another possible solution is Laplace smoothing: Before computing PMI, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. The larger the k, the more the non-zero counts
are discounted.
6.8
Word2vec
In the previous sections we saw how to represent a word as a sparse, long vector with
dimensions corresponding to the words in the vocabulary, and whose values were tf-
idf or PPMI functions of the count of the word co-occurring with each neighboring
word. In this section we turn to an alternative method for representing a word: the
use of vectors that are short (of length perhaps 50-1000) and dense (most values are
non-zero).
It turns out that dense vectors work better in every NLP task than sparse vec-
tors. While we don’t completely understand all the reasons for this, we have some
intuitions. First, dense vectors may be more successfully included as features in
machine learning systems; for example if we use 100-dimensional word embed-
dings as features, a classiﬁer can just learn 100 weights to represent a function of
word meaning; if we instead put in a 50,000 dimensional vector, a classiﬁer would
have to learn tens of thousands of weights for each of the sparse dimensions. Sec-
ond, because they contain fewer parameters than sparse vectors of explicit counts,

6.8
•
WORD2VEC
111
dense vectors may generalize better and help avoid overﬁtting. Finally, dense vec-
tors may do a better job of capturing synonymy than sparse vectors. For example,
car and automobile are synonyms; but in a typical sparse vector representation, the
car dimension and the automobile dimension are distinct dimensions. Because the
relationship between these two dimensions is not modeled, sparse vectors may fail
to capture the similarity between a word with car as a neighbor and a word with
automobile as a neighbor.
In this section we introduce one method for very dense, short vectors, skip-
gram with negative sampling, sometimes called SGNS. The skip-gram algorithm
skip-gram
SGNS
is one of two algorithms in a software package called word2vec, and so sometimes
word2vec
the algorithm is loosely referred to as word2vec (Mikolov et al. 2013, Mikolov
et al. 2013a). The word2vec methods are fast, efﬁcient to train, and easily avail-
able online with code and pretrained embeddings. We point to other embedding
methods, like the equally popular GloVe (Pennington et al., 2014), at the end of the
chapter.
The intuition of word2vec is that instead of counting how often each word w oc-
curs near, say, apricot, we’ll instead train a classiﬁer on a binary prediction task: “Is
word w likely to show up near apricot?” We don’t actually care about this prediction
task; instead we’ll take the learned classiﬁer weights as the word embeddings.
The revolutionary intuition here is that we can just use running text as implicitly
supervised training data for such a classiﬁer; a word s that occurs near the target
word apricot acts as gold ‘correct answer’ to the question “Is word w likely to show
up near apricot?” This avoids the need for any sort of hand-labeled supervision
signal. This idea was ﬁrst proposed in the task of neural language modeling, when
Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model
(a neural network that learned to predict the next word from prior words) could just
use the next word in running text as its supervision signal, and could be used to learn
an embedding representation for each word as part of doing this prediction task.
We’ll see how to do neural networks in the next chapter, but word2vec is a
much simpler model than the neural network language model, in two ways. First,
word2vec simpliﬁes the task (making it binary classiﬁcation instead of word pre-
diction). Second, word2vec simpliﬁes the architecture (training a logistic regression
classiﬁer instead of a multi-layer neural network with hidden layers that demand
more sophisticated training algorithms). The intuition of skip-gram is:
1. Treat the target word and a neighboring context word as positive examples.
2. Randomly sample other words in the lexicon to get negative samples.
3. Use logistic regression to train a classiﬁer to distinguish those two cases.
4. Use the regression weights as the embeddings.
6.8.1
The classiﬁer
Let’s start by thinking about the classiﬁcation task, and then turn to how to train.
Imagine a sentence like the following, with a target word apricot, and assume we’re
using a window of ±2 context words:
... lemon,
a [tablespoon of apricot jam,
a] pinch ...
c1
c2
t
c3
c4
Our goal is to train a classiﬁer such that, given a tuple (t,c) of a target word
t paired with a candidate context word c (for example (apricot, jam), or perhaps

112
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
(apricot, aardvark)) it will return the probability that c is a real context word (true
for jam, false for aardvark):
P(+|t,c)
(6.24)
The probability that word c is not a real context word for t is just 1 minus
Eq. 6.24:
P(−|t,c) = 1−P(+|t,c)
(6.25)
How does the classiﬁer compute the probability P? The intuition of the skip-
gram model is to base this probability on similarity: a word is likely to occur near
the target if its embedding is similar to the target embedding. How can we compute
similarity between embeddings? Recall that two vectors are similar if they have a
high dot product (cosine, the most popular similarity metric, is just a normalized dot
product). In other words:
Similarity(t,c) ≈t ·c
(6.26)
Of course, the dot product t · c is not a probability, it’s just a number ranging from
−∞to ∞. (Recall, for that matter, that cosine isn’t a probability either). To turn the
dot product into a probability, we’ll use the logistic or sigmoid function σ(x), the
fundamental core of logistic regression:
σ(x) =
1
1+e−x
(6.27)
The probability that word c is a real context word for target word t is thus computed
as:
P(+|t,c) =
1
1+e−t·c
(6.28)
The sigmoid function just returns a number between 0 and 1, so to make it a proba-
bility we’ll need to make sure that the total probability of the two possible events (c
being a context word, and c not being a context word) sums to 1.
The probability that word c is not a real context word for t is thus:
P(−|t,c) = 1−P(+|t,c)
=
e−t·c
1+e−t·c
(6.29)
Equation 6.28 gives us the probability for one word, but we need to take account
of the multiple context words in the window. Skip-gram makes the strong but very
useful simplifying assumption that all context words are independent, allowing us to
just multiply their probabilities:
P(+|t,c1:k) =
kY
i=1
1
1+e−t·ci
(6.30)
logP(+|t,c1:k) =
k
X
i=1
log
1
1+e−t·ci
(6.31)
In summary, skip-gram trains a probabilistic classiﬁer that, given a test target word
t and its context window of k words c1:k, assigns a probability based on how similar

6.8
•
WORD2VEC
113
this context window is to the target word. The probability is based on applying the
logistic (sigmoid) function to the dot product of the embeddings of the target word
with each context word. We could thus compute this probability if only we had
embeddings for each target word and context word in the vocabulary. Let’s now turn
to learning these embeddings (which is the real goal of training this classiﬁer in the
ﬁrst place).
6.8.2
Learning skip-gram embeddings
Word2vec learns embeddings by starting with an initial set of embedding vectors
and then iteratively shifting the embedding of each word w to be more like the em-
beddings of words that occur nearby in texts, and less like the embeddings of words
that don’t occur nearby. Let’s start by considering a single piece of training data:
... lemon,
a [tablespoon of apricot jam,
a] pinch ...
c1
c2
t
c3
c4
This example has a target word t (apricot), and 4 context words in the L = ±2
window, resulting in 4 positive training instances (on the left below):
positive examples +
t
c
apricot tablespoon
apricot of
apricot jam
apricot a
negative examples -
t
c
t
c
apricot aardvark apricot seven
apricot my
apricot forever
apricot where
apricot dear
apricot coaxial
apricot if
For training a binary classiﬁer we also need negative examples. In fact skip-
gram uses more negative examples than positive examples (with the ratio between
them set by a parameter k). So for each of these (t,c) training instances we’ll create
k negative samples, each consisting of the target t plus a ‘noise word’. A noise word
is a random word from the lexicon, constrained not to be the target word t. The
right above shows the setting where k = 2, so we’ll have 2 negative examples in the
negative training set −for each positive example t,c.
The noise words are chosen according to their weighted unigram frequency
pα(w), where α is a weight. If we were sampling according to unweighted fre-
quency p(w), it would mean that with unigram probability p(“the”) we would choose
the word the as a noise word, with unigram probability p(“aardvark”) we would
choose aardvark, and so on. But in practice it is common to set α = .75, i.e. use the
weighting p
3
4 (w):
Pα(w) =
count(w)α
P
w′ count(w′)α
(6.32)
Setting α = .75 gives better performance because it gives rare noise words slightly
higher probability: for rare words, Pα(w) > P(w). To visualize this intuition, it
might help to work out the probabilities for an example with two events, P(a) = .99
and P(b) = .01:
Pα(a) =
.99.75
.99.75 +.01.75 = .97
Pα(b) =
.01.75
.99.75 +.01.75 = .03
(6.33)

114
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
Given the set of positive and negative training instances, and an initial set of em-
beddings, the goal of the learning algorithm is to adjust those embeddings such that
we
• Maximize the similarity of the target word, context word pairs (t,c) drawn
from the positive examples
• Minimize the similarity of the (t,c) pairs drawn from the negative examples.
We can express this formally over the whole training set as:
L(θ) =
X
(t,c)∈+
logP(+|t,c)+
X
(t,c)∈−
logP(−|t,c)
(6.34)
If we look at one word/context pair (t,c) with its k noise words n1...nk, the learning
objective L is:
L(θ) = logP(+|t,c)+
k
X
i=1
logP(−|t,ni)
= logσ(c·t)+
k
X
i=1
logσ(−ni ·t)
= log
1
1+e−c·t +
k
X
i=1
log
1
1+eni·t
(6.35)
That is, we want to maximize the dot product of the word with the actual context
words, and minimize the dot products of the word with the k negative sampled non-
neighbor words.
We can then use stochastic gradient descent to train to this objective, iteratively
modifying the parameters (the embeddings for each target word t and each context
word or noise word c in the vocabulary) to maximize the objective.
Note that the skip-gram model thus actually learns two separate embeddings
for each word w: the target embedding t and the context embedding c. These
target
embedding
context
embedding
embeddings are stored in two matrices, the target matrix T and the context matrix
C. So each row i of the target matrix T is the 1 × d vector embedding ti for word
i in the vocabulary V, and each column j of the context matrix C is a d × 1 vector
embedding cj for word j in V. Fig. 6.12 shows an intuition of the learning task for
the embeddings encoded in these two matrices.
Just as in logistic regression, then, the learning algorithm starts with randomly
initialized W and C matrices, and then walks through the training corpus using gra-
dient descent to move W and C so as to maximize the objective in Eq. 6.35. Thus
the matrices W and C function as the parameters θ that logistic regression is tuning.
Once the embeddings are learned, we’ll have two embeddings for each word wi:
ti and ci. We can choose to throw away the C matrix and just keep W, in which case
each word i will be represented by the vector ti.
Alternatively we can add the two embeddings together, using the summed em-
bedding ti + ci as the new d-dimensional embedding, or we can concatenate them
into an embedding of dimensionality 2d.
As with the simple count-based methods like tf-idf, the context window size L
affects the performance of skip-gram embeddings, and experiments often tune the
parameter L on a devset. One difference from the count-based methods is that for
skip-grams, the larger the window size the more computation the algorithm requires
for training (more neighboring words must be predicted).

6.9
•
VISUALIZING EMBEDDINGS
115
1
.
i
.
.
.
.
.
.
V
1.2…….j…….…….…k……V
1
.
.
.
d
T
C
1. ..    …   d
increase
similarity( apricot , jam)
ti . cj
jam
apricot
aardvark
decrease
similarity( apricot , aardvark)
ti . ck
“…apricot jam…”
neighbor word
random noise word
target word
Figure 6.12
The skip-gram model tries to shift embeddings so the target embeddings (here
for apricot) are closer to (have a higher dot product with) context embeddings for nearby
words (here jam) and further from (have a lower dot product with) context embeddings for
words that don’t occur nearby (here aardvark).
6.9
Visualizing Embeddings
“I see well in many dimensions as long as the dimensions are around two.”
The late economist Martin Shubek
Visualizing embeddings is an important goal in helping understand, apply, and
improve these models of word meaning. But how can we visualize a (for example)
100-dimensional vector?
The simplest way to visualize the meaning of a word w embedded in a space is to
list the most similar words to w by sorting the vectors for all words in the vocabulary
by their cosine with the vector for w. For example the 7 closest words to frog using
the GloVe embeddings are: frogs, toad, litoria, leptodactylidae, rana, lizard, and
eleutherodactylus (Pennington et al., 2014)
WRIST
ANKLE
SHOULDER
ARM
LEG
HAND
FOOT
HEAD
NOSE
FINGER
TOE
FACE
EAR
EYE
TOOTH
DOG
CAT
PUPPY
KITTEN
COW
MOUSE
TURTLE
OYSTER
LION
BULL
CHICAGO
ATLANTA
MONTREAL
NASHVILLE
TOKYO
CHINA
RUSSIA
AFRICA
ASIA
EUROPE
AMERICA
BRAZIL
MOSCOW
FRANCE
HAWAII
Yet another visualization method is to use a clus-
tering algorithm to show a hierarchical representa-
tion of which words are similar to others in the em-
bedding space.
The uncaptioned example on the
right uses hierarchical clustering of some embedding
vectors for nouns as a visualization method (Rohde
et al., 2006).
Probably the most common visualization method,
however, is to project the 100 dimensions of a word
down into 2 dimensions. Fig. 6.1 showed one such
visualization, as does Fig. 6.13, using a projection
method called t-SNE (van der Maaten and Hinton, 2008).
6.10
Semantic properties of embeddings
Vector semantic models have a number of parameters. One parameter that is relevant
to both sparse tf-idf vectors and dense word2vec vectors is the size of the context

116
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
window used to collect counts. This is generally between 1 and 10 words on each
side of the target word (for a total context of 3-20 words).
The choice depends on the goals of the representation. Shorter context windows
tend to lead to representations that are a bit more syntactic, since the information is
coming from immediately nearby words. When the vectors are computed from short
context windows, the most similar words to a target word w tend to be semantically
similar words with the same parts of speech. When vectors are computed from long
context windows, the highest cosine words to a target word w tend to be words that
are topically related but not similar.
For example Levy and Goldberg (2014a) showed that using skip-gram with a
window of ±2, the most similar words to the word Hogwarts (from the Harry Potter
series) were names of other ﬁctional schools: Sunnydale (from Buffy the Vampire
Slayer) or Evernight (from a vampire series). With a window of ±5, the most similar
words to Hogwarts were other words topically related to the Harry Potter series:
Dumbledore, Malfoy, and half-blood.
It’s also often useful to distinguish two kinds of similarity or association between
words (Sch¨utze and Pedersen, 1993). Two words have ﬁrst-order co-occurrence
ﬁrst-order
co-occurrence
(sometimes called syntagmatic association) if they are typically nearby each other.
Thus wrote is a ﬁrst-order associate of book or poem. Two words have second-order
co-occurrence (sometimes called paradigmatic association) if they have similar
second-order
co-occurrence
neighbors. Thus wrote is a second-order associate of words like said or remarked.
Analogy
Another semantic property of embeddings is their ability to capture re-
lational meanings. Mikolov et al. (2013b) and Levy and Goldberg (2014b) show
that the offsets between vector embeddings can capture some analogical relations
between words.
For example, the result of the expression vector(‘king’) - vec-
tor(‘man’) + vector(‘woman’) is a vector close to vector(‘queen’); the left panel
in Fig. 6.13 visualizes this, again projected down into 2 dimensions. Similarly, they
found that the expression vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) results
in a vector that is very close to vector(‘Rome’).
(a)
(b)
Figure 6.13
Relational properties of the vector space, shown by projecting vectors onto two dimensions. (a)
’king’ - ’man’ + ’woman’ is close to ’queen’ (b) offsets seem to capture comparative and superlative morphology
(Pennington et al., 2014).
Embeddings and Historical Semantics:
Embeddings can also be a useful tool
for studying how meaning changes over time, by computing multiple embedding

6.11
•
BIAS AND EMBEDDINGS
117
spaces, each from texts written in a particular time period. For example Fig. 6.14
shows a visualization of changes in meaning in English words over the last two
centuries, computed by building separate embedding spaces for each decade from
historical corpora like Google N-grams (Lin et al., 2012) and the Corpus of Histori-
cal American English (Davies, 2012).
Figure 6.14
A t-SNE visualization of the semantic change of 3 words in English using
word2vec vectors. The modern sense of each word, and the grey context words, are com-
puted from the most recent (modern) time-point embedding space. Earlier points are com-
puted from earlier historical embedding spaces. The visualizations show the changes in the
word gay from meanings related to “cheerful” or “frolicsome” to referring to homosexuality,
the development of the modern “transmission” sense of broadcast from its original sense of
sowing seeds, and the pejoration of the word awful as it shifted from meaning “full of awe”
to meaning “terrible or appalling” (Hamilton et al., 2016b).
6.11
Bias and Embeddings
In addition to their ability to learn word meaning from text, embeddings, alas, also
reproduce the implicit biases and stereotypes that were latent in the text. Recall that
embeddings model analogical relations; ‘queen’ as the closest word to ‘king’ - ‘man’
+ ‘woman’ implies the analogy man:woman::king:queen. But embedding analogies
also exhibit gender stereotypes. For example Bolukbasi et al. (2016) ﬁnd that the
closest occupation to ‘man’ - ‘computer programmer’ + ‘woman’ in word2vec em-
beddings trained on news text is ‘homemaker’, and that the embeddings similarly
suggest the analogy ‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. Algorithms that
use embeddings as part of a search for potential programmers or doctors might thus
incorrectly downweight documents with women’s names.
Embeddings also encode the implicit associations that are a property of human
reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-
ple’s associations between concepts (like ‘ﬂowers’ or ‘insects’) and attributes (like
‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with
which they label words in the various categories.7 Using such methods, people
in the United States have been shown to associate African-American names with
unpleasant words (more than European-American names), male names more with
7
Roughly speaking, if humans associate ‘ﬂowers’ with ‘pleasantness’ and ‘insects’ with ‘unpleasant-
ness’, when they are instructed to push a green button for ‘ﬂowers’ (daisy, iris, lilac) and ‘pleasant words’
(love, laughter, pleasure) and a red button for ‘insects’ (ﬂea, spider, mosquito) and ‘unpleasant words’
(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for
‘ﬂowers’ and ‘unpleasant words’ and a green button for ‘insects’ and ‘pleasant words’.

118
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
mathematics and female names with the arts, and old people’s names with unpleas-
ant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan
et al. (2017) replicated all these ﬁndings of implicit associations using GloVe vectors
and cosine similarity instead of human latencies. For example African-American
names like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words
while European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine
with pleasant words. Any embedding-aware algorithm that made use of word senti-
ment could thus lead to bias against African Americans.
Recent research focuses on ways to try to remove these kinds of biases, for ex-
ample by developing a transformation of the embedding space that removes gender
stereotypes but preserves deﬁnitional gender (Bolukbasi et al. 2016, Zhao et al. 2017)
or changing the training procedure (Zhao et al., 2018b). However, although these
sorts of debiasing may reduce bias in embeddings, they do not eliminate it (Gonen
debiasing
and Goldberg, 2019), and this remains an open problem.
Historical embeddings are also being used to measure biases in the past. Garg
et al. (2018) used embeddings from historical texts to measure the association be-
tween embeddings for occupations and embeddings for names of various ethnici-
ties or genders (for example the relative cosine similarity of women’s names versus
men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century.
They found that the cosines correlate with the empirical historical percentages of
women or ethnic groups in those occupations. Historical embeddings also repli-
cated old surveys of ethnic stereotypes; the tendency of experimental participants in
1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese
ethnicity, correlates with the cosine between Chinese last names and those adjectives
using embeddings trained on 1930s text. They also were able to document historical
gender biases, such as the fact that embeddings for adjectives related to competence
(‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than fe-
male words, and showed that this bias has been slowly decreasing since 1960. We
return in later chapters to this question about the role of bias in natural language
processing.
6.12
Evaluating Vector Models
The most important evaluation metric for vector models is extrinsic evaluation on
tasks; adding them as features into any NLP task and seeing whether this improves
performance over some other model.
Nonetheless it is useful to have intrinsic evaluations. The most common metric
is to test their performance on similarity, computing the correlation between an
algorithm’s word similarity scores and word similarity ratings assigned by humans.
WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.
SimLex-999 (Hill et al., 2015) is a more difﬁcult dataset that quantiﬁes similarity
(cup, mug) rather than relatedness (cup, coffee), and including both concrete and
abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions,
each consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: Levied is closest in meaning to:
imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these
datasets present words without context.
Slightly more realistic are intrinsic similarity tasks that include context. The

6.13
•
SUMMARY
119
Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the
Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offers richer
evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their
sentential context, while WiC gives target words in two sentential contexts that are
either in the same or different senses; see Section 19.5.3. The semantic textual sim-
ilarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of
sentence-level similarity algorithms, consisting of a set of pairs of sentences, each
pair with human-labeled similarity scores.
Another task used for evaluate is an analogy task, where the system has to solve
problems of the form a is to b as c is to d, given a, b, and c and having to ﬁnd d.
Thus given Athens is to Greece as Oslo is to
, the system must ﬁll in the word
Norway. Or more syntactically-oriented examples: given mouse, mice, and dollar
the system must return dollars. Large sets of such tuples have been created (Mikolov
et al. 2013, Mikolov et al. 2013b).
6.13
Summary
• In vector semantics, a word is modeled as a vector—a point in high-dimensional
space, also called an embedding.
• Vector semantic models fall into two classes: sparse and dense.
• In sparse models like tf-idf each dimension corresponds to a word in the vo-
cabulary V; cells in sparse models are functions of co-occurrence counts.
The term-document matrix has rows for each word (term) in the vocabulary
and a column for each document. The word-context matrix has a row for
each (target) word in the vocabulary and a column for each context term in
the vocabulary.
• The most widely used sparse weighting is tf-idf, which weights each cell by
its term frequency and inverse document frequency. PPMI (pointwise pos-
itive mutual information) is an alternative weighting scheme to tf-idf.
• Dense vector models have dimensionality 50–1000 and the dimensions are
harder to interpret. Word2vec algorithms like skip-gram are a popular and
efﬁcient way to compute dense embeddings. Skip-gram trains a logistic re-
gression classiﬁer to compute the probability that two words are ‘likely to
occur nearby in text’. This probability is computed from the dot product be-
tween the embeddings for the two words.
• Skip-gram uses stochastic gradient descent to train the classiﬁer, by learning
embeddings that have a high dot product with embeddings of words that occur
nearby and a low dot product with noise words.
• Other important embedding algorithms include GloVe, a method based on ra-
tios of word co-occurrence probabilities, and fasttext, an open-source library
for computing word embeddings by summing embeddings of the bag of char-
acter n-grams that make up a word.
• Whether using sparse or dense vectors, word and document similarities are
computed by some function of the dot product between vectors. The cosine
of two vectors—a normalized dot product—is the most popular such metric.

120
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
Bibliographical and Historical Notes
The idea of vector semantics arose out of research in the 1950s in three distinct
ﬁelds: linguistics, psychology, and computer science, each of which contributed a
fundamental aspect of the model.
The idea that meaning is related to the distribution of words in context was
widespread in linguistic theory of the 1950s, among distributionalists like Zellig
Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos
(1950) put it,
the linguist’s “meaning” of a morpheme. . . is by deﬁnition the set of conditional
probabilities of its occurrence in context with all other morphemes.
The idea that the meaning of a word might be modeled as a point in a multi-
dimensional semantic space came from psychologists like Charles E. Osgood, who
had been studying how people responded to the meaning of words by assigning val-
ues along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the
meaning of a word in general could be modeled as a point in a multidimensional
Euclidean space, and that the similarity of meaning between two words could be
modeled as the distance between these points in the space.
A ﬁnal intellectual source in the 1950s and early 1960s was the ﬁeld then called
mechanical indexing, now known as information retrieval. In what became known
mechanical
indexing
as the vector space model for information retrieval (Salton 1971, Sparck Jones 1986),
researchers demonstrated new ways to deﬁne the meaning of words in terms of vec-
tors (Switzer, 1965), and reﬁned methods for word similarity based on measures
of statistical association between words like mutual information (Giuliano, 1965)
and idf (Sparck Jones, 1972), and showed that the meaning of documents could be
represented in the same vector spaces used for words.
More distantly related is the idea of deﬁning words by a vector of discrete fea-
tures, which has a venerable history in our ﬁeld, with roots at least as far back as
Descartes and Leibniz (Wierzbicka 1992, Wierzbicka 1996). By the middle of the
20th century, beginning with the work of Hjelmslev (Hjelmslev, 1969) and ﬂeshed
out in early models of generative grammar (Katz and Fodor, 1963), the idea arose of
representing meaning with semantic features, symbols that represent some sort of
semantic
feature
primitive meaning. For example words like hen, rooster, or chick, have something
in common (they all describe chickens) and something different (their age and sex),
representable as:
hen
+female, +chicken, +adult
rooster -female, +chicken, +adult
chick
+chicken, -adult
The dimensions used by vector models of meaning to deﬁne words, however, are
only abstractly related to this idea of a small ﬁxed number of hand-built dimensions.
Nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some speciﬁc compositional aspect of meaning like
these early semantic features.
The ﬁrst use of dense vectors to model word meaning was the latent seman-
tic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent seman-
tic analysis) (Deerwester et al., 1990). In LSA singular value decomposition—
SVD— is applied to a term-document matrix (each cell weighted by log frequency
SVD
and normalized by entropy), and then the ﬁrst 300 dimensions are used as the LSA

BIBLIOGRAPHICAL AND HISTORICAL NOTES
121
embedding. Singular Value Decomposition (SVD) is a method for ﬁnding the most
important dimensions of a data set, those dimensions along which the data varies
the most. LSA was then quickly widely applied: as a cognitive model Landauer
and Dumais (1997), and tasks like spell checking (Jones and Martin, 1997), lan-
guage modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000)
morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001), and
essay grading (Rehder et al., 1998). Related models were simultaneously developed
and applied to word sense disambiguation by Sch¨utze (1992b). LSA also led to
the earliest use of embeddings to represent words in a probabilistic classiﬁer, in the
logistic regression document router of Sch¨utze et al. (1995). The idea of SVD on
the term-term matrix (rather than the term-document matrix) as a model of meaning
for NLP was proposed soon after LSA by Sch¨utze (1992b). Sch¨utze applied the
low-rank (97-dimensional) embeddings produced by SVD to the task of word sense
disambiguation, analyzed the resulting semantic space, and also suggested possible
techniques like dropping high-order dimensions. See Sch¨utze (1997a).
A number of alternative matrix models followed on from the early SVD work,
including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent
Dirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-
tion (NMF) (Lee and Seung, 1999).
By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that
neural language models could also be used to develop embeddings as part of the task
of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and
Collobert et al. (2011) then demonstrated that embeddings could play a role for rep-
resenting word meanings for a number of NLP tasks. Turian et al. (2010) compared
the value of different kinds of embeddings for different NLP tasks. Mikolov et al.
(2011) showed that recurrent neural nets could be used as language models. The
idea of simplifying the hidden layer of these neural net language models to create
the skip-gram (and also CBOW) algorithms was proposed by Mikolov et al. (2013).
The negative sampling training algorithm was proposed in Mikolov et al. (2013a).
Studies of embeddings include results showing an elegant mathematical relation-
ship between sparse and dense embeddings (Levy and Goldberg, 2014c), as well
as numerous surveys of embeddings and their parameterizations. (Bullinaria and
Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014,
Levy et al. 2015).
The most widely-used embedding model besides word2vec is GloVe (Penning-
ton et al., 2014). The name stands for Global Vectors, because the model is based on
capturing global corpus statistics. GloVe is based on ratios of probabilities from the
word-word co-occurrence matrix, combining the intuitions of count-based models
like PPMI while also capturing the linear structures used by methods like word2vec.
An extension of word2vec, fasttext (Bojanowski et al., 2017), deals with un-
fasttext
known words and sparsity in languages with rich morphology, by using subword
models. Each word in fasttext is represented as itself plus a bag of constituent n-
grams, with special boundary symbols < and > added to each word. For example,
with n = 3 the word where would be represented by the character n-grams:
<wh, whe, her, ere, re>
plus the sequence
<where>
Then a skipgram embedding is learned for each constituent n-gram, and the word
where is represented by the sum of all of the embeddings of its constituent n-grams.

122
CHAPTER 6
•
VECTOR SEMANTICS AND EMBEDDINGS
A fasttext open-source library, including pretrained embeddings for 157 languages,
is available at https://fasttext.cc.
There are many other embedding algorithms, using methods like non-negative
matrix factorization (Fyshe et al., 2015), or by converting sparse PPMI embeddings
to dense vectors by using SVD (Levy and Goldberg, 2014c).
In Chapter 10 we introduce contextual embeddings like ELMo (Peters et al.,
2018) and BERT (Devlin et al., 2019) in which the representation for a word is
contextual, a function of the entire input sentence.
See Manning et al. (2008) for a deeper understanding of the role of vectors in in-
formation retrieval, including how to compare queries with documents, more details
on tf-idf, and issues of scaling to very large datasets.
Cruse (2004) is a useful introductory linguistic text on lexical semantics.
Exercises

CHAPTER
7
Neural Networks and Neural
Language Models
“[M]achines of this character can behave in a very complicated manner when
the number of units is large.”
Alan Turing (1948) “Intelligent Machines”, page 6
Neural networks are a fundamental computational tool for language process-
ing, and a very old one. They are called neural because their origins lie in the
McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the
human neuron as a kind of computing element that could be described in terms of
propositional logic. But the modern use in language processing no longer draws on
these early biological inspirations.
Instead, a modern neural network is a network of small computing units, each
of which takes a vector of input values and produces a single output value. In this
chapter we introduce the neural net applied to classiﬁcation. The architecture we
introduce is called a feedforward network because the computation proceeds iter-
feedforward
atively from one layer of units to the next. The use of modern neural nets is often
called deep learning, because modern networks are often deep (have many layers).
deep learning
Neural networks share much of the same mathematics as logistic regression. But
neural networks are a more powerful classiﬁer than logistic regression, and indeed a
minimal neural network (technically one with a single ‘hidden layer’) can be shown
to learn any function.
Neural net classiﬁers are different from logistic regression in another way. With
logistic regression, we applied the regression classiﬁer to many different tasks by
developing many rich kinds of feature templates based on domain knowledge. When
working with neural networks, it is more common to avoid most uses of rich hand-
derived features, instead building neural networks that take raw words as inputs
and learn to induce features as part of the process of learning to classify. We saw
examples of this kind of representation learning for embeddings in Chapter 6. Nets
that are very deep are particularly good at representation learning. For that reason
deep neural nets are the right tool for large scale problems that offer sufﬁcient data
to learn features automatically.
In this chapter we’ll introduce feedforward networks as classiﬁers, and also ap-
ply them to the simple task of language modeling: assigning probabilities to word
sequences and predicting upcoming words. In subsequent chapters we’ll introduce
many other aspects of neural models, such as recurrent neural networks (Chap-
ter 9), encoder-decoder models, attention and the Transformer (Chapter 10).

124
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
7.1
Units
The building block of a neural network is a single computational unit. A unit takes
a set of real valued numbers as input, performs some computation on them, and
produces an output.
At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-
tional term in the sum called a bias term. Given a set of inputs x1...xn, a unit has
bias term
a set of corresponding weights w1...wn and a bias b, so the weighted sum z can be
represented as:
z = b+
X
i
wixi
(7.1)
Often it’s more convenient to express this weighted sum using vector notation; recall
from linear algebra that a vector is, at heart, just a list or array of numbers. Thus
vector
we’ll talk about z in terms of a weight vector w, a scalar bias b, and an input vector
x, and we’ll replace the sum with the convenient dot product:
z = w·x+b
(7.2)
As deﬁned in Eq. 7.2, z is just a real valued number.
Finally, instead of using z, a linear function of x, as the output, neural units
apply a non-linear function f to z. We will refer to the output of this function as
the activation value for the unit, a. Since we are just modeling a single unit, the
activation
activation for the node is in fact the ﬁnal output of the network, which we’ll generally
call y. So the value y is deﬁned as:
y = a = f(z)
We’ll discuss three popular non-linear functions f() below (the sigmoid, the tanh,
and the rectiﬁed linear ReLU) but it’s pedagogically convenient to start with the
sigmoid function since we saw it in Chapter 5:
sigmoid
y = σ(z) =
1
1+e−z
(7.3)
The sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output
into the range [0,1], which is useful in squashing outliers toward 0 or 1. And it’s
differentiable, which as we saw in Section 5.8 will be handy for learning.
Figure 7.1
The sigmoid function takes a real value and maps it to the range [0,1]. It is
nearly linear around 0 but outlier values get squashed toward 0 or 1.

7.1
•
UNITS
125
Substituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit:
y = σ(w·x+b) =
1
1+exp(−(w·x+b))
(7.4)
Fig. 7.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit
takes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each
value by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then
passes the resulting sum through a sigmoid function to result in a number between 0
and 1.
x1
x2
x3
y
w1
w2
w3
∑
b
σ
+1
z
a
Figure 7.2
A neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a
weight for an input clamped at +1) and producing an output y. We include some convenient
intermediate variables: the output of the summation, z, and the output of the sigmoid, a. In
this case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to
mean the ﬁnal output of the entire network, leaving a as the activation of an individual node.
Let’s walk through an example just to get an intuition. Let’s suppose we have a
unit with the following weight vector and bias:
w = [0.2,0.3,0.9]
b = 0.5
What would this unit do with the following input vector:
x = [0.5,0.6,0.1]
The resulting output y would be:
y = σ(w·x+b) =
1
1+e−(w·x+b) =
1
1+e−(.5∗.2+.6∗.3+.1∗.9+.5) = e−0.87 = .70
In practice, the sigmoid is not commonly used as an activation function. A function
that is very similar but almost always better is the tanh function shown in Fig. 7.3a;
tanh
tanh is a variant of the sigmoid that ranges from -1 to +1:
y = ez −e−z
ez +e−z
(7.5)
The simplest activation function, and perhaps the most commonly used, is the rec-
tiﬁed linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as x
ReLU
when x is positive, and 0 otherwise:
y = max(x,0)
(7.6)

126
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
(a)
(b)
Figure 7.3
The tanh and ReLU activation functions.
These activation functions have different properties that make them useful for differ-
ent language applications or network architectures. For example the rectiﬁer func-
tion has nice properties that result from it being very close to linear. In the sigmoid
or tanh functions, very high values of z result in values of y that are saturated, i.e.,
saturated
extremely close to 1, which causes problems for learning. Rectiﬁers don’t have this
problem, since the output of values close to 1 also approaches 1 in a nice gentle
linear way. By contrast, the tanh function has the nice properties of being smoothly
differentiable and mapping outlier values toward the mean.
7.2
The XOR problem
Early in the history of neural networks it was realized that the power of neural net-
works, as with the real neurons that inspired them, comes from combining these
units into larger networks.
One of the most clever demonstrations of the need for multi-layer networks was
the proof by Minsky and Papert (1969) that a single neural unit cannot compute
some very simple functions of its input. Consider the task of computing elementary
logical functions of two inputs, like AND, OR, and XOR. As a reminder, here are
the truth tables for those functions:
AND
OR
XOR
x1 x2 y
x1 x2 y
x1 x2 y
0
0
0
0
0
0
0
0
0
0
1
0
0
1
1
0
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1
1
1
1
1
0
This example was ﬁrst shown for the perceptron, which is a very simple neural
perceptron
unit that has a binary output and does not have a non-linear activation function. The
output y of a perceptron is 0 or 1, and is computed as follows (using the same weight
w, input x, and bias b as in Eq. 7.2):
y =
 0, if w·x+b ≤0
1, if w·x+b > 0
(7.7)

7.2
•
THE XOR PROBLEM
127
It’s very easy to build a perceptron that can compute the logical AND and OR
functions of its binary inputs; Fig. 7.4 shows the necessary weights.
x1
x2
+1
-1
1
1
x1
x2
+1
0
1
1
(a)
(b)
Figure 7.4
The weights w and bias b for perceptrons for computing logical functions. The
inputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied
with the bias weight b. (a) logical AND, showing weights w1 = 1 and w2 = 1 and bias weight
b = −1. (b) logical OR, showing weights w1 = 1 and w2 = 1 and bias weight b = 0. These
weights/biases are just one from an inﬁnite number of possible sets of weights and biases that
would implement the functions.
It turns out, however, that it’s not possible to build a perceptron to compute
logical XOR! (It’s worth spending a moment to give it a try!)
The intuition behind this important result relies on understanding that a percep-
tron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perception
equation, w1x1 +w2x2 +b = 0 is the equation of a line. (We can see this by putting
it in the standard linear format: x2 = −(w1/w2)x1 −b.) This line acts as a decision
boundary in two-dimensional space in which the output 0 is assigned to all inputs
decision
boundary
lying on one side of the line, and the output 1 to all input points lying on the other
side of the line. If we had more than 2 inputs, the decision boundary becomes a
hyperplane instead of a line, but the idea is the same, separating the space into two
categories.
Fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn
by one possible set of parameters for an AND and an OR classiﬁer. Notice that there
is simply no way to draw a line that separates the positive cases of XOR (01 and 10)
from the negative cases (00 and 11). We say that XOR is not a linearly separable
linearly
separable
function. Of course we could draw a boundary with a curve, or some other function,
but not a single line.
7.2.1
The solution: neural networks
While the XOR function cannot be calculated by a single perceptron, it can be cal-
culated by a layered network of units. Let’s see an example of how to do this from
Goodfellow et al. (2016) that computes XOR using two layers of ReLU-based units.
Fig. 7.6 shows a ﬁgure with the input being processed by two layers of neural units.
The middle layer (called h) has two units, and the output layer (called y) has one
unit. A set of weights and biases are shown for each ReLU that correctly computes
the XOR function.
Let’s walk through what happens with the input x = [0 0]. If we multiply each
input value by the appropriate weight, sum, and then add the bias b, we get the
vector [0 -1], and we then apply the rectiﬁed linear transformation to give the output
of the h layer as [0 0]. Now we once again multiply by the weights, sum, and add
the bias (0 in this case) resulting in the value 0. The reader should work through the
computation of the remaining 3 possible input pairs to see that the resulting y values
are 1 for the inputs [0 1] and [1 0] and 0 for [0 0] and [1 1].

128
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
0
0
1
1
x1
x2
0
0
1
1
x1
x2
0
0
1
1
x1
x2
a)  x1 AND x2
b)  x1 OR x2
c)  x1 XOR x2
?
Figure 7.5
The functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the
y axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no
way to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig
(2002).
x1
x2
h1
h2
y1
+1
1
-1
1
1
1
-2
0
1
+1
0
Figure 7.6
XOR solution after Goodfellow et al. (2016). There are three ReLU units, in
two layers; we’ve called them h1, h2 (h for “hidden layer”) and y1. As before, the numbers
on the arrows represent the weights w for each unit, and we represent the bias b as a weight
on a unit clamped to +1, with the bias weights/units in gray.
It’s also instructive to look at the intermediate results, the outputs of the two
hidden nodes h0 and h1. We showed in the previous paragraph that the h vector for
the inputs x = [0 0] was [0 0]. Fig. 7.7b shows the values of the h layer for all 4
inputs. Notice that hidden representations of the two input points x = [0 1] and x
= [1 0] (the two cases with XOR output = 1) are merged to the single point h = [1
0]. The merger makes it easy to linearly separate the positive and negative cases
of XOR. In other words, we can view the hidden layer of the network as forming a
representation for the input.
In this example we just stipulated the weights in Fig. 7.6. But for real examples
the weights for neural networks are learned automatically using the error backprop-
agation algorithm to be introduced in Section 7.4. That means the hidden layers will
learn to form useful representations. This intuition, that neural networks can auto-
matically learn useful representations of the input, is one of their key advantages,
and one that we will return to again and again in later chapters.
Note that the solution to the XOR problem requires a network of units with non-
linear activation functions. A network made up of simple linear (perceptron) units
cannot solve the XOR problem. This is because a network formed by many layers
of purely linear units can always be reduced (shown to be computationally identical

7.3
•
FEED-FORWARD NEURAL NETWORKS
129
0
0
1
1
x0
x1
a) The original x space
0
0
1
1
h0
h1
2
b) The new h space
Figure 7.7
The hidden layer forming a new representation of the input. Here is the rep-
resentation of the hidden layer, h, compared to the original input representation x. Notice
that the input point [0 1] has been collapsed with the input point [1 0], making it possible to
linearly separate the positive and negative cases of XOR. After Goodfellow et al. (2016).
to) a single layer of linear units with appropriate weights, and we’ve already shown
(visually, in Fig. 7.5) that a single unit cannot solve the XOR problem.
7.3
Feed-Forward Neural Networks
Let’s now walk through a slightly more formal presentation of the simplest kind of
neural network, the feedforward network. A feedforward network is a multilayer
feedforward
network
network in which the units are connected with no cycles; the outputs from units in
each layer are passed to units in the next higher layer, and no outputs are passed
back to lower layers. (In Chapter 9 we’ll introduce networks with cycles, called
recurrent neural networks.)
For historical reasons multilayer networks, especially feedforward networks, are
sometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,
multi-layer
perceptrons
MLP
since the units in modern multilayer networks aren’t perceptrons (perceptrons are
purely linear, but modern networks are made up of units with non-linearities like
sigmoids), but at some point the name stuck.
Simple feedforward networks have three kinds of nodes: input units, hidden
units, and output units. Fig. 7.8 shows a picture.
The input units are simply scalar values just as we saw in Fig. 7.2.
The core of the neural network is the hidden layer formed of hidden units,
hidden layer
each of which is a neural unit as described in Section 7.1, taking a weighted sum of
its inputs and then applying a non-linearity. In the standard architecture, each layer
is fully-connected, meaning that each unit in each layer takes as input the outputs
fully-connected
from all the units in the previous layer, and there is a link between every pair of units
from two adjacent layers. Thus each hidden unit sums over all the input units.
Recall that a single hidden unit has parameters w (the weight vector) and b (the
bias scalar). We represent the parameters for the entire hidden layer by combining
the weight vector wi and bias bi for each unit i into a single weight matrix W and
a single bias vector b for the whole layer (see Fig. 7.8). Each element Wij of the
weight matrix W represents the weight of the connection from the ith input unit xi to

130
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
x1
x2
h1
h2
y1
xn0
…
h3
hn1
…
+1
b
…
U
W
y2
yn2
Figure 7.8
A simple 2-layer feedforward network, with one hidden layer, one output layer,
and one input layer (the input layer is usually not counted when enumerating layers).
the jth hidden unit hj.
The advantage of using a single matrix W for the weights of the entire layer is
that now the hidden layer computation for a feedforward network can be done very
efﬁciently with simple matrix operations. In fact, the computation only has three
steps: multiplying the weight matrix by the input vector x, adding the bias vector b,
and applying the activation function g (such as the sigmoid, tanh, or ReLU activation
function deﬁned above).
The output of the hidden layer, the vector h, is thus the following, using the
sigmoid function σ:
h = σ(Wx+b)
(7.8)
Notice that we’re applying the σ function here to a vector, while in Eq. 7.3 it was
applied to a scalar. We’re thus allowing σ(·), and indeed any activation function
g(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].
Let’s introduce some constants to represent the dimensionalities of these vectors
and matrices. We’ll refer to the input layer as layer 0 of the network, and have n0
represent the number of inputs, so x is a vector of real numbers of dimension n0,
or more formally x ∈Rn0. Let’s call the hidden layer layer 1 and the output layer
layer 2. The hidden layer has dimensionality n1, so h ∈Rn1 and also b ∈Rn1 (since
each hidden unit can take a different bias value). And the weight matrix W has
dimensionality W ∈Rn1×n0.
Take a moment to convince yourself that the matrix multiplication in Eq. 7.8 will
compute the value of each hj as σ
 Pnx
i=1 wijxi +bj

.
As we saw in Section 7.2, the resulting value h (for hidden but also for hypoth-
esis) forms a representation of the input. The role of the output layer is to take
this new representation h and compute a ﬁnal output. This output could be a real-
valued number, but in many cases the goal of the network is to make some sort of
classiﬁcation decision, and so we will focus on the case of classiﬁcation.
If we are doing a binary task like sentiment classiﬁcation, we might have a single
output node, and its value y is the probability of positive versus negative sentiment.
If we are doing multinomial classiﬁcation, such as assigning a part-of-speech tag, we
might have one output node for each potential part-of-speech, whose output value
is the probability of that part-of-speech, and the values of all the output nodes must
sum to one. The output layer thus gives a probability distribution across the output

7.3
•
FEED-FORWARD NEURAL NETWORKS
131
nodes.
Let’s see how this happens. Like the hidden layer, the output layer has a weight
matrix (let’s call it U), but some models don’t include a bias vector b in the output
layer, so we’ll simplify by eliminating the bias vector in this example. The weight
matrix is multiplied by its input vector (h) to produce the intermediate output z.
z = Uh
There are n2 output nodes, so z ∈Rn2, weight matrix U has dimensionality U ∈
Rn2×n1, and element Ui j is the weight from unit j in the hidden layer to unit i in the
output layer.
However, z can’t be the output of the classiﬁer, since it’s a vector of real-valued
numbers, while what we need for classiﬁcation is a vector of probabilities. There is
a convenient function for normalizing a vector of real values, by which we mean
normalizing
converting it to a vector that encodes a probability distribution (all the numbers lie
between 0 and 1 and sum to 1): the softmax function that we saw on page 89 of
softmax
Chapter 5. For a vector z of dimensionality d, the softmax is deﬁned as:
softmax(zi) =
ezi
Pd
j=1 ezj 1 ≤i ≤d
(7.9)
Thus for example given a vector z=[0.6 1.1 -1.5 1.2 3.2 -1.1], softmax(z) is [0.055
0.090 0.0067 0.10 0.74 0.010].
You may recall that softmax was exactly what is used to create a probability
distribution from a vector of real-valued numbers (computed from summing weights
times features) in logistic regression in Chapter 5.
That means we can think of a neural network classiﬁer with one hidden layer
as building a vector h which is a hidden layer representation of the input, and then
running standard logistic regression on the features that the network develops in h.
By contrast, in Chapter 5 the features were mainly designed by hand via feature
templates. So a neural network is like logistic regression, but (a) with many layers,
since a deep neural network is like layer after layer of logistic regression classiﬁers,
and (b) rather than forming the features by feature templates, the prior layers of the
network induce the feature representations themselves.
Here are the ﬁnal equations for a feedforward network with a single hidden layer,
which takes an input vector x, outputs a probability distribution y, and is parameter-
ized by weight matrices W and U and a bias vector b:
h = σ(Wx+b)
z = Uh
y = softmax(z)
(7.10)
We’ll call this network a 2-layer network (we traditionally don’t count the input
layer when numbering layers, but do count the output layer). So by this terminology
logistic regression is a 1-layer network.
Let’s now set up some notation to make it easier to talk about deeper networks
of depth more than 2. We’ll use superscripts in square brackets to mean layer num-
bers, starting at 0 for the input layer. So W [1] will mean the weight matrix for the
(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. nj
will mean the number of units at layer j. We’ll use g(·) to stand for the activation
function, which will tend to be ReLU or tanh for intermediate layers and softmax
for output layers. We’ll use a[i] to mean the output from layer i, and z[i] to mean the

132
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
combination of weights and biases W [i]a[i−1] +b[i]. The 0th layer is for inputs, so the
inputs x we’ll refer to more generally as a[0].
Thus we can re-represent our 2-layer net from Eq. 7.10 as follows:
z[1] = W [1]a[0] +b[1]
a[1] = g[1](z[1])
z[2] = W [2]a[1] +b[2]
a[2] = g[2](z[2])
ˆy = a[2]
(7.11)
Note that with this notation, the equations for the computation done at each layer are
the same. The algorithm for computing the forward step in an n-layer feedforward
network, given the input vector a[0] is thus simply:
for i in 1..n
z[i] = W [i] a[i−1] + b[i]
a[i] = g[i](z[i])
ˆy = a[n]
The activation functions g(·) are generally different at the ﬁnal layer. Thus g[2]
might be softmax for multinomial classiﬁcation or sigmoid for binary classiﬁcation,
while ReLU or tanh might be the activation function g(·) at the internal layers.
7.4
Training Neural Nets
A feedforward neural net is an instance of supervised machine learning in which we
know the correct output y for each observation x. What the system produces, via
Eq. 7.11, is ˆy, the system’s estimate of the true y. The goal of the training procedure
is to learn parameters W [i] and b[i] for each layer i that make ˆy for each training
observation as close as possible to the true y.
In general, we do all this by drawing on the methods we introduced in Chapter 5
for logistic regression, so the reader should be comfortable with that chapter before
proceeding.
First, we’ll need a loss function that models the distance between the system
output and the gold output, and it’s common to use the loss function used for logistic
regression, the cross-entropy loss.
Second, to ﬁnd the parameters that minimize this loss function, we’ll use the
gradient descent optimization algorithm introduced in Chapter 5.
Third, gradient descent requires knowing the gradient of the loss function, the
vector that contains the partial derivative of the loss function with respect to each of
the parameters. Here is one part where learning for neural networks is more complex
than for logistic logistic regression. In logistic regression, for each observation we
could directly compute the derivative of the loss function with respect to an individ-
ual w or b. But for neural networks, with millions of parameters in many layers, it’s
much harder to see how to compute the partial derivative of some weight in layer 1
when the loss is attached to some much later layer. How do we partial out the loss
over all those intermediate layers?
The answer is the algorithm called error backpropagation or reverse differen-
tiation.

7.4
•
TRAINING NEURAL NETS
133
7.4.1
Loss function
The cross-entropy loss that is used in neural networks is the same one we saw for
cross-entropy
loss
logistic regression.
In fact, if the neural network is being used as a binary classiﬁer, with the sig-
moid at the ﬁnal layer, the loss function is exactly the same as we saw with logistic
regression in Eq. 5.10:
LCE(ˆy,y) = −log p(y|x) = −[ylog ˆy+(1−y)log(1−ˆy)]
(7.12)
What about if the neural network is being used as a multinomial classiﬁer? Let y be
a vector over the C classes representing the true output probability distribution. The
cross-entropy loss here is
LCE(ˆy,y) = −
C
X
i=1
yi log ˆyi
(7.13)
We can simplify this equation further. Assume this is a hard classiﬁcation task,
meaning that only one class is the correct one, and that there is one output unit in y
for each class. If the true class is i, then y is a vector where yi = 1 and y j = 0 ∀j ̸= i.
A vector like this, with one value=1 and the rest 0, is called a one-hot vector. Now
let ˆy be the vector output from the network. The sum in Eq. 7.13 will be 0 except
for the true class. Hence the cross-entropy loss is simply the log probability of the
correct class, and we therefore also call this the negative log likelihood loss:
negative log
likelihood loss
LCE(ˆy,y) = −log ˆyi
(7.14)
Plugging in the softmax formula from Eq. 7.9, and with K the number of classes:
LCE(ˆy,y) = −log
ezi
PK
j=1 ezj
(7.15)
7.4.2
Computing the Gradient
How do we compute the gradient of this loss function? Computing the gradient
requires the partial derivative of the loss function with respect to each parameter.
For a network with one weight layer and sigmoid output (which is what logistic
regression is), we could simply use the derivative of the loss that we used for logistic
regression in Eq. 7.16 (and derived in Section 5.8):
∂LCE(w,b)
∂w j
= (ˆy−y)x j
= (σ(w·x+b)−y)xj
(7.16)
Or for a network with one hidden layer and softmax output, we could use the deriva-
tive of the softmax loss from Eq. 5.35:
∂LCE
∂wk
= (1{y = k}−p(y = k|x))xk
=
 
1{y = k}−
ewk·x+bk
PK
j=1 ew j·x+b j
!
xk
(7.17)

134
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
But these derivatives only give correct updates for one weight layer: the last one!
For deep networks, computing the gradients for each weight is much more complex,
since we are computing the derivative with respect to weight parameters that appear
all the way back in the very early layers of the network, even though the loss is
computed only at the very end of the network.
The solution to computing this gradient is an algorithm called error backprop-
agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-
error back-
propagation
cially for neural networks, it turns out to be the same as a more general procedure
called backward differentiation, which depends on the notion of computation
graphs. Let’s see how that works in the next subsection.
7.4.3
Computation Graphs
A computation graph is a representation of the process of computing a mathematical
expression, in which the computation is broken down into separate operations, each
of which is modeled as a node in a graph.
Consider computing the function L(a,b,c) = c(a+2b). If we make each of the
component addition and multiplication operations explicit, and add names (d and e)
for the intermediate outputs, the resulting series of computations is:
d = 2∗b
e = a+d
L = c∗e
We can now represent this as a graph, with nodes for each operation, and di-
rected edges showing the outputs from each operation as the inputs to the next, as
in Fig. 7.9. The simplest use of computation graphs is to compute the value of the
function with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,
b = 1, c = −2, and we’ve shown the result of the forward pass to compute the re-
sult L(3,1,−2) = 10. In the forward pass of a computation graph, we apply each
operation left to right, passing the outputs of each computation as the input to the
next node.
e=d+a
d = 2b
L=ce
3
1
-2
e=5
d=2
L=-10
forward pass
a
b
c
Figure 7.9
Computation graph for the function L(a,b,c) = c(a+2b), with values for input
nodes a = 3, b = 1, c = −2, showing the forward pass computation of L.
7.4.4
Backward differentiation on computation graphs
The importance of the computation graph comes from the backward pass, which
is used to compute the derivatives that we’ll need for the weight update. In this
example our goal is to compute the derivative of the output function L with respect

7.4
•
TRAINING NEURAL NETS
135
to each of the input variables, i.e., ∂L
∂a, ∂L
∂b, and ∂L
∂c . The derivative ∂L
∂a, tells us how
much a small change in a affects L.
Backwards differentiation makes use of the chain rule in calculus. Suppose we
chain rule
are computing the derivative of a composite function f(x) = u(v(x)). The derivative
of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with
respect to x:
d f
dx = du
dv · dv
dx
(7.18)
The chain rule extends to more than two functions. If computing the derivative of a
composite function f(x) = u(v(w(x))), the derivative of f(x) is:
d f
dx = du
dv · dv
dw · dw
dx
(7.19)
Let’s now compute the 3 derivatives we need. Since in the computation graph
L = ce, we can directly compute the derivative ∂L
∂c :
∂L
∂c = e
(7.20)
For the other two, we’ll need to use the chain rule:
∂L
∂a = ∂L
∂e
∂e
∂a
∂L
∂b = ∂L
∂e
∂e
∂d
∂d
∂b
(7.21)
Eq. 7.21 thus requires ﬁve intermediate derivatives:
∂L
∂e , ∂L
∂c , ∂e
∂a, ∂e
∂d , and ∂d
∂b,
which are as follows (making use of the fact that the derivative of a sum is the sum
of the derivatives):
L = ce :
∂L
∂e = c, ∂L
∂c = e
e = a+d :
∂e
∂a = 1, ∂e
∂d = 1
d = 2b :
∂d
∂b = 2
In the backward pass, we compute each of these partials along each edge of the graph
from right to left, multiplying the necessary partials to result in the ﬁnal derivative
we need. Thus we begin by annotating the ﬁnal node with ∂L
∂L = 1. Moving to the
left, we then compute ∂L
∂c and ∂L
∂e , and so on, until we have annotated the graph all
the way to the input variables. The forward pass conveniently already will have
computed the values of the forward intermediate variables we need (like d and e)
to compute these derivatives. Fig. 7.10 shows the backward pass. At each node we
need to compute the local partial derivative with respect to the parent, multiply it by
the partial derivative that is being passed down from the parent, and then pass it to
the child.
Backward differentiation for a neural network
Of course computation graphs for real neural networks are much more complex.
Fig. 7.11 shows a sample computation graph for a 2-layer neural network with n0 =

136
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
e=d+a
d = 2b
L=ce
a=3
b=1
e=5
d=2
L=-10
 
∂L=1
∂L
∂L=-4
∂b
∂L=-2
∂d
a
b
c
∂L=-2
∂a
∂L=5
∂c
∂L =-2
∂e
∂L=-2
∂e
∂e =1
∂d
∂L =5
∂c
∂d =2
∂b
∂e =1
∂a
backward pass
c=-2
Figure 7.10
Computation graph for the function L(a,b,c) = c(a+2b), showing the back-
ward pass computation of ∂L
∂a , ∂L
∂b , and ∂L
∂c .
2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid
output unit for simplicity. The function that the computation graph is computing is:
z[1] = W [1]x+b[1]
a[1] = ReLU(z[1])
z[2] = W [2]a[1] +b[2]
a[2] = σ(z[2])
ˆy = a[2]
(7.22)
z[2] = 
+
a[2] = σ
 
a[1] = 
ReLU
z[1] = 
+
b[1]
*
*
*
*
x1
x2
a[1] = 
ReLU
z[1] = 
+
b[1]
*
*
w[2]
11
w[1]
11
w[1]
21
w[1]
12
w[1]
22
b[2]
w[2]
21
L (a[2],y)
Figure 7.11
Sample computation graph for a simple 2-layer neural net (= 1 hidden layer)
with two input dimensions and 2 hidden dimensions.
The weights that need updating (those for which we need to know the partial
derivative of the loss function) are shown in orange. In order to do the backward
pass, we’ll need to know the derivatives of all the functions in the graph. We already
saw in Section 5.8 the derivative of the sigmoid σ:
dσ(z)
dz
= σ(z)(1−σ(z))
(7.23)

7.5
•
NEURAL LANGUAGE MODELS
137
We’ll also need the derivatives of each of the other activation functions. The
derivative of tanh is:
d tanh(z)
dz
= 1−tanh2(z)
(7.24)
The derivative of the ReLU is
d ReLU(z)
dz
=
 0 for x < 0
1 for x ≥0
(7.25)
7.4.5
More details on learning
Optimization in neural networks is a non-convex optimization problem, more com-
plex than for logistic regression, and for that and other reasons there are many best
practices for successful learning.
For logistic regression we can initialize gradient descent with all the weights and
biases having the value 0. In neural networks, by contrast, we need to initialize the
weights with small random numbers. It’s also helpful to normalize the input values
to have 0 mean and unit variance.
Various forms of regularization are used to prevent overﬁtting. One of the most
important is dropout: randomly dropping some units and their connections from
dropout
the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning
of hyperparameters is also important. The parameters of a neural network are the
hyperparameter
weights W and biases b; those are learned by gradient descent. The hyperparameters
are things that are chosen by the algorithm designer; optimal values are tuned on a
devset rather than by gradient descent learning on the training set. Hyperparameters
include the learning rate η, the mini-batch size, the model architecture (the number
of layers, the number of hidden nodes per layer, the choice of activation functions),
how to regularize, and so on. Gradient descent itself also has many architectural
variants such as Adam (Kingma and Ba, 2015).
Finally, most modern neural networks are built using computation graph for-
malisms that make it easy and natural to do gradient computation and parallelization
onto vector-based GPUs (Graphic Processing Units). Pytorch (Paszke et al., 2017)
and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested
reader should consult a neural network textbook for further details; some sugges-
tions are at the end of the chapter.
7.5
Neural Language Models
As our ﬁrst application of neural networks, let’s consider language modeling: pre-
dicting upcoming words from prior word context.
Neural net-based language models turn out to have many advantages over the n-
gram language models of Chapter 3. Among these are that neural language models
don’t need smoothing, they can handle much longer histories, and they can general-
ize over contexts of similar words. For a training set of a given size, a neural lan-
guage model has much higher predictive accuracy than an n-gram language model.
Furthermore, neural language models underlie many of the models we’ll introduce
for tasks like machine translation, dialog, and language generation.

138
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
On the other hand, there is a cost for this improved performance: neural net
language models are strikingly slower to train than traditional language models, and
so for many tasks an n-gram language model is still the right tool.
In this chapter we’ll describe simple feedforward neural language models, ﬁrst
introduced by Bengio et al. (2003). Modern neural language models are generally
not feedforward but recurrent, using the technology that we will introduce in Chap-
ter 9.
A feedforward neural LM is a standard feedforward network that takes as input
at time t a representation of some number of previous words (wt−1,wt−2, etc.) and
outputs a probability distribution over possible next words. Thus—like the n-gram
LM—the feedforward neural LM approximates the probability of a word given the
entire prior context P(wt|wt−1
1
) by approximating based on the N previous words:
P(wt|wt−1
1
) ≈P(wt|wt−1
t−N+1)
(7.26)
In the following examples we’ll use a 4-gram example, so we’ll show a net to
estimate the probability P(wt = i|wt−1,wt−2,wt−3).
7.5.1
Embeddings
In neural language models, the prior context is represented by embeddings of the
previous words. Representing the prior context as embeddings, rather than by ex-
act words as used in n-gram language models, allows neural language models to
generalize to unseen data much better than n-gram language models. For example,
suppose we’ve seen this sentence in training:
I have to make sure when I get home to feed the cat.
but we’ve never seen the word “dog” after the words “feed the”. In our test set we
are trying to predict what comes after the preﬁx “I forgot when I got home to feed
the”.
An n-gram language model will predict “cat”, but not “dog”. But a neural LM,
which can make use of the fact that “cat” and “dog” have similar embeddings, will
be able to assign a reasonably high probability to “dog” as well as “cat”, merely
because they have similar vectors.
Let’s see how this works in practice. Let’s assume we have an embedding dic-
tionary E that gives us, for each word in our vocabulary V, the embedding for that
word, perhaps precomputed by an algorithm like word2vec from Chapter 6.
Fig. 7.12 shows a sketch of this simpliﬁed feedforward neural language model
with N=3; we have a moving window at time t with an embedding vector represent-
ing each of the 3 previous words (words wt−1, wt−2, and wt−3). These 3 vectors are
concatenated together to produce x, the input layer of a neural network whose output
is a softmax with a probability distribution over words. Thus y42, the value of output
node 42 is the probability of the next word wt being V42, the vocabulary word with
index 42.
The model shown in Fig. 7.12 is quite sufﬁcient, assuming we learn the embed-
dings separately by a method like the word2vec methods of Chapter 6. The method
of using another algorithm to learn the embedding representations we use for input
words is called pretraining. If those pretrained embeddings are sufﬁcient for your
pretraining
purposes, then this is all you need.
However, often we’d like to learn the embeddings simultaneously with training
the network. This is true when whatever task the network is designed for (sentiment

7.5
•
NEURAL LANGUAGE MODELS
139
h1
h2
y1
h3
hdh
…
…
U
W
y42
y|V|
Projection layer
1⨉3d
concatenated embeddings
for context words
Hidden layer
Output layer P(w|u)
…
in
the
hole
...
...
ground
there
lived
word 42
embedding for
word 35
embedding for 
word 9925
embedding for 
word 45180
wt-1
wt-2
wt
wt-3
dh⨉3d
1⨉dh
|V|⨉dh
P(wt=V42|wt-3,wt-2,wt-3)
1⨉|V|
Figure 7.12
A simpliﬁed view of a feedforward neural language model moving through a text. At each
timestep t the network takes the 3 context words, converts each to a d-dimensional embedding, and concatenates
the 3 embeddings together to get the 1 × Nd unit input layer x for the network. These units are multiplied by
a weight matrix W and bias vector b and then an activation function to produce a hidden layer h, which is then
multiplied by another weight matrix U. (For graphic simplicity we don’t show b in this and future pictures.)
Finally, a softmax output layer predicts at each node i the probability that the next word wt will be vocabulary
word Vi. (This picture is simpliﬁed because it assumes we just look up in an embedding dictionary E the
d-dimensional embedding vector for each word, precomputed by an algorithm like word2vec.)
classiﬁcation, or translation, or parsing) places strong constraints on what makes a
good representation.
Let’s therefore show an architecture that allows the embeddings to be learned.
To do this, we’ll add an extra layer to the network, and propagate the error all the
way back to the embedding vectors, starting with embeddings with random values
and slowly moving toward sensible representations.
For this to work at the input layer, instead of pre-trained embeddings, we’re
going to represent each of the N previous words as a one-hot vector of length |V|, i.e.,
with one dimension for each word in the vocabulary. A one-hot vector is a vector
one-hot vector
that has one element equal to 1—in the dimension corresponding to that word’s
index in the vocabulary— while all the other elements are set to zero.
Thus in a one-hot representation for the word “toothpaste”, supposing it happens
to have index 5 in the vocabulary, x5 is one and and xi = 0 ∀i ̸= 5, as shown here:
[0 0 0 0 1 0 0 ... 0 0 0 0]
1 2 3 4 5 6 7 ...
... |V|
Fig. 7.13 shows the additional layers needed to learn the embeddings during LM
training. Here the N=3 context words are represented as 3 one-hot vectors, fully
connected to the embedding layer via 3 instantiations of the embedding matrix E.
Note that we don’t want to learn separate weight matrices for mapping each of the 3
previous words to the projection layer, we want one single embedding dictionary E
that’s shared among these three. That’s because over time, many different words will
appear as wt−2 or wt−1, and we’d like to just represent each word with one vector,
whichever context position it appears in. The embedding weight matrix E thus has

140
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
h1
h2
y1
h3
hdh
…
…
U
W
y42
y|V|
Projection layer
1⨉3d
Hidden layer
Output layer 
P(w|context)
…
in
the
hole
...
...
ground
there
lived
word 42
wt-1
wt-2
wt
wt-3
dh⨉3d
1⨉dh
|V|⨉dh
P(wt=V42|wt-3,wt-2,wt-3)
1⨉|V|
Input layer
one-hot vectors
index
word 35
0 0
1
0
0
1
|V|
35
0 0
1
0
0
1
|V|
45180
0 0
1
0
0
1
|V|
9925
0
0
index 
word 9925
index 
word 45180
E
1⨉|V|
d⨉|V|
E is shared
across words
Figure 7.13
Learning all the way back to embeddings. Notice that the embedding matrix E is shared among
the 3 context words.
a row for each word, each a vector of d dimensions, and hence has dimensionality
V ×d.
Let’s walk through the forward pass of Fig. 7.13.
1. Select three embeddings from E: Given the three previous words, we look
up their indices, create 3 one-hot vectors, and then multiply each by the em-
bedding matrix E. Consider wt−3. The one-hot vector for ‘the’ (index 35) is
multiplied by the embedding matrix E, to give the ﬁrst part of the ﬁrst hidden
layer, called the projection layer. Since each row of the input matrix E is just
projection layer
an embedding for a word, and the input is a one-hot column vector xi for word
Vi, the projection layer for input w will be Exi = ei, the embedding for word i.
We now concatenate the three embeddings for the context words.
2. Multiply by W: We now multiply by W (and add b) and pass through the
rectiﬁed linear (or other) activation function to get the hidden layer h.
3. Multiply by U: h is now multiplied by U
4. Apply softmax: After the softmax, each node i in the output layer estimates
the probability P(wt = i|wt−1,wt−2,wt−3)
In summary, if we use e to represent the projection layer, formed by concate-
nating the 3 embeddings for the three context vectors, the equations for a neural
language model become:
e = (Ex1,Ex2,...,Ex)
(7.27)
h = σ(We+b)
(7.28)
z = Uh
(7.29)
y = softmax(z)
(7.30)

7.6
•
SUMMARY
141
7.5.2
Training the neural language model
To train the model, i.e. to set all the parameters θ = E,W,U,b, we do gradient
descent (Fig. 5.5), using error backpropagation on the computation graph to compute
the gradient. Training thus not only sets the weights W and U of the network, but
also as we’re predicting upcoming words, we’re learning the embeddings E for each
words that best predict upcoming words.
Generally training proceeds by taking as input a very long text, concatenating
all the sentences, starting with random weights, and then iteratively moving through
the text predicting each word wt. At each word wt, the cross-entropy (negative log
likelihood) loss is:
L = −log p(wt|wt−1,...,wt−n+1)
(7.31)
The gradient for this loss is then:
θt+1 = θt −η ∂−log p(wt|wt−1,...,wt−n+1)
∂θ
(7.32)
This gradient can be computed in any standard neural network framework which
will then backpropagate through U, W, b, E.
Training the parameters to minimize loss will result both in an algorithm for
language modeling (a word predictor) but also a new set of embeddings E that can
be used as word representations for other tasks.
7.6
Summary
• Neural networks are built out of neural units, originally inspired by human
neurons but now simply an abstract computational device.
• Each neural unit multiplies input values by a weight vector, adds a bias, and
then applies a non-linear activation function like sigmoid, tanh, or rectiﬁed
linear.
• In a fully-connected, feedforward network, each unit in layer i is connected
to each unit in layer i+1, and there are no cycles.
• The power of neural networks comes from the ability of early layers to learn
representations that can be utilized by later layers in the network.
• Neural networks are trained by optimization algorithms like gradient de-
scent.
• Error backpropagation, backward differentiation on a computation graph,
is used to compute the gradients of the loss function for a network.
• Neural language models use a neural network as a probabilistic classiﬁer, to
compute the probability of the next word given the previous n words.
• Neural language models can use pretrained embeddings, or can learn embed-
dings from scratch in the process of language modeling.
Bibliographical and Historical Notes
The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-
loch and Pitts, 1943), a simpliﬁed model of the human neuron as a kind of com-

142
CHAPTER 7
•
NEURAL NETWORKS AND NEURAL LANGUAGE MODELS
puting element that could be described in terms of propositional logic. By the late
1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and
Bernard Widrow at Stanford) developed research into neural networks; this phase
saw the development of the perceptron (Rosenblatt, 1958), and the transformation
of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).
The ﬁeld of neural networks declined after it was shown that a single percep-
tron unit was unable to model functions as simple as XOR (Minsky and Papert,
1969). While some small amount of work continued during the next two decades,
a major revival for the ﬁeld didn’t come until the 1980s, when practical tools for
building deeper networks like error backpropagation became widespread (Rumel-
hart et al., 1986). During the 1980s a wide variety of neural network and related
architectures were developed, particularly for applications in psychology and cog-
nitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986,
Rumelhart and McClelland 1986a, Elman 1990), for which the term connection-
ist or parallel distributed processing was often used (Feldman and Ballard 1982,
connectionist
Smolensky 1988). Many of the principles and techniques developed in this period
are foundational to modern work, including the ideas of distributed representations
(Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for com-
positionality (Smolensky, 1990).
By the 1990s larger neural networks began to be applied to many practical lan-
guage processing tasks as well, like handwriting recognition (LeCun et al. 1989,
LeCun et al. 1990) and speech recognition (Morgan and Bourlard 1989, Morgan
and Bourlard 1990). By the early 2000s, improvements in computer hardware and
advances in optimization and training techniques made it possible to train even larger
and deeper networks, leading to the modern term deep learning (Hinton et al. 2006,
Bengio et al. 2007). We cover more related history in Chapter 9.
There are a number of excellent books on the subject. Goldberg (2017) has a
superb and comprehensive coverage of neural networks for natural language pro-
cessing. For neural networks in general see Goodfellow et al. (2016) and Nielsen
(2015).

CHAPTER
8
Part-of-Speech Tagging
Dionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long
time ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the
linguistic knowledge of his day. This work is the source of an astonishing proportion
of modern linguistic vocabulary, including words like syntax, diphthong, clitic, and
analogy. Also included are a description of eight parts of speech: noun, verb,
parts of speech
pronoun, preposition, adverb, conjunction, participle, and article. Although earlier
scholars (including Aristotle as well as the Stoics) had their own lists of parts of
speech, it was Thrax’s set of eight that became the basis for practically all subsequent
part-of-speech descriptions of most European languages for the next 2000 years.
Schoolhouse Rock was a series of popular animated educational television clips
from the 1970s. Its Grammar Rock sequence included songs about exactly 8 parts
of speech, including the late great Bob Dorough’s Conjunction Junction:
Conjunction Junction, what’s your function?
Hooking up words and phrases and clauses...
Although the list of 8 was slightly modiﬁed from Thrax’s original, the astonishing
durability of the parts of speech through two millennia is an indicator of both the
importance and the transparency of their role in human language.1
Parts of speech (also known as POS, word classes, or syntactic categories) are
POS
useful because they reveal a lot about a word and its neighbors. Knowing whether
a word is a noun or a verb tells us about likely neighboring words (nouns are pre-
ceded by determiners and adjectives, verbs by nouns) and syntactic structure (nouns
are generally part of noun phrases), making part-of-speech tagging a key aspect of
parsing (Chapter 13). Parts of speech are useful features for labeling named entities
like people or organizations in information extraction (Chapter 18), or for corefer-
ence resolution (Chapter 22). A word’s part of speech can even play a role in speech
recognition or synthesis, e.g., the word content is pronounced CONtent when it is a
noun and conTENT when it is an adjective.
This chapter introduces parts of speech, and then introduces two algorithms for
part-of-speech tagging, the task of assigning parts of speech to words. One is
generative— Hidden Markov Model (HMM)—and one is discriminative—the Max-
imum Entropy Markov Model (MEMM). Chapter 9 then introduces a third algorithm
based on the recurrent neural network (RNN). All three have roughly equal perfor-
mance but, as we’ll see, have different tradeoffs.
8.1
(Mostly) English Word Classes
Until now we have been using part-of-speech terms like noun and verb rather freely.
In this section we give a more complete deﬁnition of these and other classes. While
word classes do have semantic tendencies—adjectives, for example, often describe
1
Nonetheless, eight isn’t very many and, as we’ll see, recent tagsets have more.

144
CHAPTER 8
•
PART-OF-SPEECH TAGGING
properties and nouns people— parts of speech are traditionally deﬁned instead based
on syntactic and morphological function, grouping words that have similar neighbor-
ing words (their distributional properties) or take similar afﬁxes (their morpholog-
ical properties).
Parts of speech can be divided into two broad supercategories: closed class types
closed class
and open class types. Closed classes are those with relatively ﬁxed membership,
open class
such as prepositions—new prepositions are rarely coined. By contrast, nouns and
verbs are open classes—new nouns and verbs like iPhone or to fax are continually
being created or borrowed. Any given speaker or corpus may have different open
class words, but all speakers of a language, and sufﬁciently large corpora, likely
share the set of closed class words. Closed class words are generally function words
function word
like of, it, and, or you, which tend to be very short, occur frequently, and often have
structuring uses in grammar.
Four major open classes occur in the languages of the world: nouns, verbs,
adjectives, and adverbs. English has all four, although not every language does.
The syntactic class noun includes the words for most people, places, or things, but
noun
others as well. Nouns include concrete terms like ship and chair, abstractions like
bandwidth and relationship, and verb-like terms like pacing as in His pacing to and
fro became quite annoying. What deﬁnes a noun in English, then, are things like its
ability to occur with determiners (a goat, its bandwidth, Plato’s Republic), to take
possessives (IBM’s annual revenue), and for most but not all nouns to occur in the
plural form (goats, abaci).
Open class nouns fall into two classes. Proper nouns, like Regina, Colorado,
proper noun
and IBM, are names of speciﬁc persons or entities. In English, they generally aren’t
preceded by articles (e.g., the book is upstairs, but Regina is upstairs). In written
English, proper nouns are usually capitalized. The other class, common nouns, are
common noun
divided in many languages, including English, into count nouns and mass nouns.
count noun
mass noun
Count nouns allow grammatical enumeration, occurring in both the singular and plu-
ral (goat/goats, relationship/relationships) and they can be counted (one goat, two
goats). Mass nouns are used when something is conceptualized as a homogeneous
group. So words like snow, salt, and communism are not counted (i.e., *two snows
or *two communisms). Mass nouns can also appear without articles where singular
count nouns cannot (Snow is white but not *Goat is white).
Verbs refer to actions and processes, including main verbs like draw, provide,
verb
and go. English verbs have inﬂections (non-third-person-sg (eat), third-person-sg
(eats), progressive (eating), past participle (eaten)). While many researchers believe
that all human languages have the categories of noun and verb, others have argued
that some languages, such as Riau Indonesian and Tongan, don’t even make this
distinction (Broschart 1997; Evans 2000; Gil 2000) .
The third open class English form is adjectives, a class that includes many terms
adjective
for properties or qualities. Most languages have adjectives for the concepts of color
(white, black), age (old, young), and value (good, bad), but there are languages
without adjectives. In Korean, for example, the words corresponding to English
adjectives act as a subclass of verbs, so what is in English an adjective “beautiful”
acts in Korean like a verb meaning “to be beautiful”.
The ﬁnal open class form, adverbs, is rather a hodge-podge in both form and
adverb
meaning. In the following all the italicized words are adverbs:
Actually, I ran home extremely quickly yesterday
What coherence the class has semantically may be solely that each of these
words can be viewed as modifying something (often verbs, hence the name “ad-

8.1
•
(MOSTLY) ENGLISH WORD CLASSES
145
verb”, but also other adverbs and entire verb phrases). Directional adverbs or loca-
tive adverbs (home, here, downhill) specify the direction or location of some action;
locative
degree adverbs (extremely, very, somewhat) specify the extent of some action, pro-
degree
cess, or property; manner adverbs (slowly, slinkily, delicately) describe the manner
manner
of some action or process; and temporal adverbs describe the time that some ac-
temporal
tion or event took place (yesterday, Monday). Because of the heterogeneous nature
of this class, some adverbs (e.g., temporal adverbs like Monday) are tagged in some
tagging schemes as nouns.
The closed classes differ more from language to language than do the open
classes. Some of the important closed classes in English include:
prepositions: on, under, over, near, by, at, from, to, with
particles: up, down, on, off, in, out, at, by
determiners: a, an, the
conjunctions: and, but, or, as, if, when
pronouns: she, who, I, others
auxiliary verbs: can, may, should, are
numerals: one, two, three, ﬁrst, second, third
Prepositions occur before noun phrases. Semantically they often indicate spatial
preposition
or temporal relations, whether literal (on it, before then, by the house) or metaphor-
ical (on time, with gusto, beside herself), but often indicate other relations as well,
like marking the agent in Hamlet was written by Shakespeare. A particle resembles
particle
a preposition or an adverb and is used in combination with a verb. Particles often
have extended meanings that aren’t quite the same as the prepositions they resemble,
as in the particle over in she turned the paper over.
A verb and a particle that act as a single syntactic and/or semantic unit are
called a phrasal verb. The meaning of phrasal verbs is often problematically non-
phrasal verb
compositional—not predictable from the distinct meanings of the verb and the par-
ticle. Thus, turn down means something like ‘reject’, rule out ‘eliminate’, ﬁnd out
‘discover’, and go on ‘continue’.
A closed class that occurs with nouns, often marking the beginning of a noun
phrase, is the determiner. One small subtype of determiners is the article: English
determiner
article
has three articles: a, an, and the. Other determiners include this and that (this chap-
ter, that page). A and an mark a noun phrase as indeﬁnite, while the can mark it
as deﬁnite; deﬁniteness is a discourse property (Chapter 23). Articles are quite fre-
quent in English; indeed, the is the most frequently occurring word in most corpora
of written English, and a and an are generally right behind.
Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc-
conjunctions
tions like and, or, and but join two elements of equal status. Subordinating conjunc-
tions are used when one of the elements has some embedded status. For example,
that in “I thought that you might like some milk” is a subordinating conjunction
that links the main clause I thought with the subordinate clause you might like some
milk. This clause is called subordinate because this entire clause is the “content” of
the main verb thought. Subordinating conjunctions like that which link a verb to its
argument in this way are also called complementizers.
complementizer
Pronouns are forms that often act as a kind of shorthand for referring to some
pronoun
noun phrase or entity or event. Personal pronouns refer to persons or entities (you,
personal
she, I, it, me, etc.). Possessive pronouns are forms of personal pronouns that in-
possessive
dicate either actual possession or more often just an abstract relation between the
person and some object (my, your, his, her, its, one’s, our, their). Wh-pronouns
wh
(what, who, whom, whoever) are used in certain question forms, or may also act as

146
CHAPTER 8
•
PART-OF-SPEECH TAGGING
complementizers (Frida, who married Diego... ).
A closed class subtype of English verbs are the auxiliary verbs. Cross-linguist-
auxiliary
ically, auxiliaries mark semantic features of a main verb: whether an action takes
place in the present, past, or future (tense), whether it is completed (aspect), whether
it is negated (polarity), and whether an action is necessary, possible, suggested, or
desired (mood). English auxiliaries include the copula verb be, the two verbs do and
copula
have, along with their inﬂected forms, as well as a class of modal verbs. Be is called
modal
a copula because it connects subjects with certain kinds of predicate nominals and
adjectives (He is a duck). The verb have can mark the perfect tenses (I have gone, I
had gone), and be is used as part of the passive (We were robbed) or progressive (We
are leaving) constructions. Modals are used to mark the mood associated with the
event depicted by the main verb: can indicates ability or possibility, may permission
or possibility, must necessity. There is also a modal use of have (e.g., I have to go).
English also has many words of more or less unique function, including inter-
jections (oh, hey, alas, uh, um), negatives (no, not), politeness markers (please,
interjection
negative
thank you), greetings (hello, goodbye), and the existential there (there are two on
the table) among others. These classes may be distinguished or lumped together as
interjections or adverbs depending on the purpose of the labeling.
8.2
The Penn Treebank Part-of-Speech Tagset
An important tagset for English is the 45-tag Penn Treebank tagset (Marcus et al.,
1993), shown in Fig. 8.1, which has been used to label many corpora. In such
labelings, parts of speech are generally represented by placing the tag after each
word, delimited by a slash:
Tag
Description
Example
Tag
Description
Example
Tag
Description Example
CC
coordinating
conjunction
and, but, or PDT
predeterminer
all, both
VBP
verb non-3sg
present
eat
CD
cardinal number
one, two
POS
possessive ending ’s
VBZ
verb 3sg pres
eats
DT
determiner
a, the
PRP
personal pronoun
I, you, he
WDT wh-determ.
which, that
EX
existential ‘there’
there
PRP$ possess. pronoun
your, one’s WP
wh-pronoun
what, who
FW
foreign word
mea culpa
RB
adverb
quickly
WP$
wh-possess.
whose
IN
preposition/
subordin-conj
of, in, by
RBR
comparative
adverb
faster
WRB wh-adverb
how, where
JJ
adjective
yellow
RBS
superlatv. adverb
fastest
$
dollar sign
$
JJR
comparative adj
bigger
RP
particle
up, off
#
pound sign
#
JJS
superlative adj
wildest
SYM
symbol
+,%, &
“
left quote
‘ or “
LS
list item marker
1, 2, One
TO
“to”
to
”
right quote
’ or ”
MD
modal
can, should UH
interjection
ah, oops
(
left paren
[, (, {, <
NN
sing or mass noun llama
VB
verb base form
eat
)
right paren
], ), }, >
NNS
noun, plural
llamas
VBD
verb past tense
ate
,
comma
,
NNP
proper noun, sing. IBM
VBG
verb gerund
eating
.
sent-end punc . ! ?
NNPS proper noun, plu.
Carolinas
VBN
verb past part.
eaten
:
sent-mid punc : ; ... – -
Figure 8.1
Penn Treebank part-of-speech tags (including punctuation).
(8.1) The/DT grand/JJ jury/NN commented/VBD on/IN a/DT number/NN of/IN
other/JJ topics/NNS ./.
(8.2) There/EX are/VBP 70/CD children/NNS there/RB

8.2
•
THE PENN TREEBANK PART-OF-SPEECH TAGSET
147
(8.3) Preliminary/JJ ﬁndings/NNS were/VBD reported/VBN in/IN today/NN
’s/POS New/NNP England/NNP Journal/NNP of/IN Medicine/NNP ./.
Example (8.1) shows the determiners the and a, the adjectives grand and other,
the common nouns jury, number, and topics, and the past tense verb commented.
Example (8.2) shows the use of the EX tag to mark the existential there construction
in English, and, for comparison, another use of there which is tagged as an adverb
(RB). Example (8.3) shows the segmentation of the possessive morpheme ’s, and a
passive construction, ‘were reported’, in which reported is tagged as a past participle
(VBN). Note that since New England Journal of Medicine is a proper noun, the
Treebank tagging chooses to mark each noun in it separately as NNP, including
journal and medicine, which might otherwise be labeled as common nouns (NN).
Corpora labeled with parts of speech are crucial training (and testing) sets for
statistical tagging algorithms. Three main tagged corpora are consistently used for
training and testing part-of-speech taggers for English. The Brown corpus is a mil-
Brown
lion words of samples from 500 written texts from different genres published in the
United States in 1961. The WSJ corpus contains a million words published in the
WSJ
Wall Street Journal in 1989. The Switchboard corpus consists of 2 million words
Switchboard
of telephone conversations collected in 1990-1991. The corpora were created by
running an automatic part-of-speech tagger on the texts and then human annotators
hand-corrected each tag.
There are some minor differences in the tagsets used by the corpora. For example
in the WSJ and Brown corpora, the single Penn tag TO is used for both the inﬁnitive
to (I like to race) and the preposition to (go to the store), while in Switchboard the
tag TO is reserved for the inﬁnitive use of to and the preposition is tagged IN:
Well/UH ,/, I/PRP ,/, I/PRP want/VBP to/TO go/VB to/IN a/DT restauran-
t/NN
Finally, there are some idiosyncracies inherent in any tagset. For example, be-
cause the Penn 45 tags were collapsed from a larger 87-tag tagset, the original
Brown tagset, some potentially useful distinctions were lost. The Penn tagset was
designed for a treebank in which sentences were parsed, and so it leaves off syntactic
information recoverable from the parse tree. Thus for example the Penn tag IN is
used for both subordinating conjunctions like if, when, unless, after:
after/IN spending/VBG a/DT day/NN at/IN the/DT beach/NN
and prepositions like in, on, after:
after/IN sunrise/NN
Words are generally tokenized before tagging.
The Penn Treebank and the
British National Corpus split contractions and the ’s-genitive from their stems:2
would/MD n’t/RB
children/NNS ’s/POS
The Treebank tagset assumes that tokenization of multipart words like New
York is done at whitespace, thus tagging. a New York City ﬁrm as a/DT New/NNP
York/NNP City/NNP ﬁrm/NN.
Another commonly used tagset, the Universal POS tag set of the Universal De-
pendencies project (Nivre et al., 2016a), is used when building systems that can tag
many languages. See Section 8.7.
2
Indeed, the Treebank tag POS is used only for ’s, which must be segmented in tokenization.

148
CHAPTER 8
•
PART-OF-SPEECH TAGGING
8.3
Part-of-Speech Tagging
Part-of-speech tagging is the process of assigning a part-of-speech marker to each
part-of-speech
tagging
word in an input text.3 The input to a tagging algorithm is a sequence of (tokenized)
words and a tagset, and the output is a sequence of tags, one per token.
Tagging is a disambiguation task; words are ambiguous —have more than one
ambiguous
possible part-of-speech—and the goal is to ﬁnd the correct tag for the situation.
For example, book can be a verb (book that ﬂight) or a noun (hand me that book).
That can be a determiner (Does that ﬂight serve dinner) or a complementizer (I
thought that your ﬂight was earlier). The goal of POS-tagging is to resolve these
ambiguity
resolution
ambiguities, choosing the proper tag for the context. How common is tag ambiguity?
Fig. 8.2 shows that most word types (85-86%) are unambiguous (Janet is always
NNP, funniest JJS, and hesitantly RB). But the ambiguous words, though accounting
for only 14-15% of the vocabulary, are very common words, and hence 55-67% of
word tokens in running text are ambiguous.4
Types:
WSJ
Brown
Unambiguous (1 tag)
44,432 (86%)
45,799 (85%)
Ambiguous
(2+ tags)
7,025 (14%)
8,050 (15%)
Tokens:
Unambiguous (1 tag)
577,421 (45%) 384,349 (33%)
Ambiguous
(2+ tags)
711,780 (55%) 786,646 (67%)
Figure 8.2
Tag ambiguity for word types in Brown and WSJ, using Treebank-3 (45-tag)
tagging. Punctuation were treated as words, and words were kept in their original case.
Some of the most ambiguous frequent words are that, back, down, put and set;
here are some examples of the 6 different parts of speech for the word back:
earnings growth took a back/JJ seat
a small building in the back/NN
a clear majority of senators back/VBP the bill
Dave began to back/VB toward the door
enable the country to buy back/RP about debt
I was twenty-one back/RB then
Nonetheless, many words are easy to disambiguate, because their different tags
aren’t equally likely. For example, a can be a determiner or the letter a, but the
determiner sense is much more likely. This idea suggests a simplistic baseline algo-
rithm for part-of-speech tagging: given an ambiguous word, choose the tag which is
most frequent in the training corpus. This is a key concept:
Most Frequent Class Baseline: Always compare a classiﬁer against a baseline at
least as good as the most frequent class baseline (assigning each token to the class
it occurred in most often in the training set).
How good is this baseline? A standard way to measure the performance of part-
of-speech taggers is accuracy: the percentage of tags correctly labeled (matching
accuracy
3
Tags are also applied to punctuation, so tagging assumes tokenizing of commas, quotation marks, etc.,
and disambiguating end-of-sentence periods from periods inside words (e.g., etc.).
4
Note the large differences across the two genres, especially in token frequency. Tags in the WSJ corpus
are less ambiguous; its focus on ﬁnancial news leads to a more limited distribution of word usages than
the diverse genres of the Brown corpus.

8.4
•
HMM PART-OF-SPEECH TAGGING
149
human labels on a test set). If we train on the WSJ training corpus and test on sec-
tions 22-24 of the same corpus the most-frequent-tag baseline achieves an accuracy
of 92.34%. By contrast, the state of the art in part-of-speech tagging on this dataset
is around 97% tag accuracy, a performance that is achievable by most algorithms
(HMMs, MEMMs, neural networks, rule-based algorithms). See Section 8.7 on
other languages and genres.
8.4
HMM Part-of-Speech Tagging
In this section we introduce the use of the Hidden Markov Model for part-of-speech
tagging. The HMM is a sequence model. A sequence model or sequence classi-
sequence model
ﬁer is a model whose job is to assign a label or class to each unit in a sequence,
thus mapping a sequence of observations to a sequence of labels. An HMM is a
probabilistic sequence model: given a sequence of units (words, letters, morphemes,
sentences, whatever), it computes a probability distribution over possible sequences
of labels and chooses the best label sequence.
8.4.1
Markov Chains
The HMM is based on augmenting the Markov chain. A Markov chain is a model
Markov chain
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. These sets can be words, or
tags, or symbols representing anything, for example the weather. A Markov chain
makes a very strong assumption that if we want to predict the future in the sequence,
all that matters is the current state. All the states before the current state have no im-
pact on the future except via the current state. It’s as if to predict tomorrow’s weather
you could examine today’s weather but you weren’t allowed to look at yesterday’s
weather.
WARM3
HOT1
COLD2
.8
.6
.1
.1
.3
.6
.1
.1
.3
charming
uniformly
are
.1
.4
.5
.5
.5
.2
.6
.2
(a)
(b)
Figure 8.3
A Markov chain for weather (a) and one for words (b), showing states and
transitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a
probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.
More formally, consider a sequence of state variables q1,q2,...,qi. A Markov
model embodies the Markov assumption on the probabilities of this sequence: that
Markov
assumption
when predicting the future, the past doesn’t matter, only the present.
Markov Assumption:
P(qi = a|q1...qi−1) = P(qi = a|qi−1)
(8.4)
Figure 8.3a shows a Markov chain for assigning a probability to a sequence of
weather events, for which the vocabulary consists of HOT, COLD, and WARM. The

150
CHAPTER 8
•
PART-OF-SPEECH TAGGING
states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. The transitions are probabilities: the values of arcs leaving a given
state must sum to 1. Figure 8.3b shows a Markov chain for assigning a probability to
a sequence of words w1...wn. This Markov chain should be familiar; in fact, it repre-
sents a bigram language model, with each edge expressing the probability p(wi|w j)!
Given the two models in Fig. 8.3, we can assign a probability to any sequence from
our vocabulary.
Formally, a Markov chain is speciﬁed by the following components:
Q = q1q2 ...qN
a set of N states
A = a11a12 ...an1 ...ann
a transition probability matrix A, each aij represent-
ing the probability of moving from state i to state j, s.t.
Pn
j=1 aij = 1 ∀i
π = π1,π2,...,πN
an initial probability distribution over states. πi is the
probability that the Markov chain will start in state i.
Some states j may have π j = 0, meaning that they cannot
be initial states. Also, Pn
i=1 πi = 1
Before you go on, use the sample probabilities in Fig. 8.3a (with π = [0.1,0.7,0.2])
to compute the probability of each of the following sequences:
(8.5) hot hot hot hot
(8.6) cold hot cold hot
What does the difference in these probabilities tell you about a real-world weather
fact encoded in Fig. 8.3a?
8.4.2
The Hidden Markov Model
A Markov chain is useful when we need to compute a probability for a sequence
of observable events. In many cases, however, the events we are interested in are
hidden: we don’t observe them directly. For example we don’t normally observe
hidden
part-of-speech tags in a text. Rather, we see words, and must infer the tags from the
word sequence. We call the tags hidden because they are not observed.
A hidden Markov model (HMM) allows us to talk about both observed events
Hidden
Markov model
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. An HMM is speciﬁed by
the following components:
Q = q1q2 ...qN
a set of N states
A = a11 ...ai j ...aNN
a transition probability matrix A, each aij representing the probability
of moving from state i to state j, s.t. PN
j=1 aij = 1 ∀i
O = o1o2 ...oT
a sequence of T observations, each one drawn from a vocabulary V =
v1,v2,...,vV
B = bi(ot)
a sequence of observation likelihoods, also called emission probabili-
ties, each expressing the probability of an observation ot being generated
from a state qi
π = π1,π2,...,πN
an initial probability distribution over states. πi is the probability that
the Markov chain will start in state i. Some states j may have πj = 0,
meaning that they cannot be initial states. Also, Pn
i=1 πi = 1

8.4
•
HMM PART-OF-SPEECH TAGGING
151
A ﬁrst-order hidden Markov model instantiates two simplifying assumptions.
First, as with a ﬁrst-order Markov chain, the probability of a particular state depends
only on the previous state:
Markov Assumption:
P(qi|q1...qi−1) = P(qi|qi−1)
(8.7)
Second, the probability of an output observation oi depends only on the state that
produced the observation qi and not on any other states or any other observations:
Output Independence: P(oi|q1 ...qi,...,qT,o1,...,oi,...,oT) = P(oi|qi)
(8.8)
8.4.3
The components of an HMM tagger
Let’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use
it to tag. An HMM has two components, the A and B probabilities.
The A matrix contains the tag transition probabilities P(ti|ti−1) which represent
the probability of a tag occurring given the previous tag. For example, modal verbs
like will are very likely to be followed by a verb in the base form, a VB, like race, so
we expect this probability to be high. We compute the maximum likelihood estimate
of this transition probability by counting, out of the times we see the ﬁrst tag in a
labeled corpus, how often the ﬁrst tag is followed by the second:
P(ti|ti−1) = C(ti−1,ti)
C(ti−1)
(8.9)
In the WSJ corpus, for example, MD occurs 13124 times of which it is followed
by VB 10471, for an MLE estimate of
P(VB|MD) = C(MD,VB)
C(MD)
= 10471
13124 = .80
(8.10)
Let’s walk through an example, seeing how these probabilities are estimated and
used in a sample tagging task, before we return to the algorithm for decoding.
In HMM tagging, the probabilities are estimated by counting on a tagged training
corpus. For this example we’ll use the tagged WSJ corpus.
The B emission probabilities, P(wi|ti), represent the probability, given a tag (say
MD), that it will be associated with a given word (say will). The MLE of the emis-
sion probability is
P(wi|ti) = C(ti,wi)
C(ti)
(8.11)
Of the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046
times:
P(will|MD) = C(MD,will)
C(MD)
= 4046
13124 = .31
(8.12)
We saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood
term is not asking “which is the most likely tag for the word will?” That would be
the posterior P(MD|will). Instead, P(will|MD) answers the slightly counterintuitive
question “If we were going to generate a MD, how likely is it that this modal would
be will?”
The A transition probabilities, and B observation likelihoods of the HMM are
illustrated in Fig. 8.4 for three states in an HMM part-of-speech tagger; the full
tagger would have one state for each tag.

152
CHAPTER 8
•
PART-OF-SPEECH TAGGING
NN3
VB1
MD2
a22
a11
a12
a21
a13
a33
a32
a23
a31
P("aardvark" | NN)
...
P(“will” | NN)
...
P("the" | NN)
...
P(“back” | NN)
...
P("zebra" | NN)
B3
P("aardvark" | VB)
...
P(“will” | VB)
...
P("the" | VB)
...
P(“back” | VB)
...
P("zebra" | VB)
B1
P("aardvark" | MD)
...
P(“will” | MD)
...
P("the" | MD)
...
P(“back” | MD)
...
P("zebra" | MD)
B2
Figure 8.4
An illustration of the two parts of an HMM representation: the A transition
probabilities used to compute the prior probability, and the B observation likelihoods that are
associated with each state, one likelihood for each possible observation word.
8.4.4
HMM tagging as decoding
For any model, such as an HMM, that contains hidden variables, the task of deter-
mining the hidden variables sequence corresponding to the sequence of observations
is called decoding. More formally,
decoding
Decoding: Given as input an HMM λ = (A,B) and a sequence of ob-
servations O = o1,o2,...,oT, ﬁnd the most probable sequence of states
Q = q1q2q3 ...qT.
For part-of-speech tagging, the goal of HMM decoding is to choose the tag se-
quence tn
1 that is most probable given the observation sequence of n words wn
1:
ˆtn
1 = argmax
tn
1
P(tn
1|wn
1)
(8.13)
The way we’ll do this in the HMM is to use Bayes’ rule to instead compute:
ˆtn
1 = argmax
tn
1
P(wn
1|tn
1)P(tn
1)
P(wn
1)
(8.14)
Furthermore, we simplify Eq. 8.14 by dropping the denominator P(wn
1):
ˆtn
1 = argmax
tn
1
P(wn
1|tn
1)P(tn
1)
(8.15)
HMM taggers make two further simplifying assumptions. The ﬁrst is that the
probability of a word appearing depends only on its own tag and is independent of
neighboring words and tags:
P(wn
1|tn
1) ≈
n
Y
i=1
P(wi|ti)
(8.16)
The second assumption, the bigram assumption, is that the probability of a tag is
dependent only on the previous tag, rather than the entire tag sequence;
P(tn
1) ≈
n
Y
i=1
P(ti|ti−1)
(8.17)

8.4
•
HMM PART-OF-SPEECH TAGGING
153
Plugging the simplifying assumptions from Eq. 8.16 and Eq. 8.17 into Eq. 8.15
results in the following equation for the most probable tag sequence from a bigram
tagger:
ˆtn
1 = argmax
tn
1
P(tn
1|wn
1) ≈argmax
tn
1
n
Y
i=1
emission
z }| {
P(wi|ti)
transition
z
}|
{
P(ti|ti−1)
(8.18)
The two parts of Eq. 8.18 correspond neatly to the B emission probability and A
transition probability that we just deﬁned above!
8.4.5
The Viterbi Algorithm
The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.5. As
Viterbi
algorithm
an instance of dynamic programming, Viterbi resembles the dynamic program-
ming minimum edit distance algorithm of Chapter 2.
function VITERBI(observations of len T,state-graph of len N) returns best-path, path-prob
create a path probability matrix viterbi[N,T]
for each state s from 1 to N do
; initialization step
viterbi[s,1]←πs ∗bs(o1)
backpointer[s,1]←0
for each time step t from 2 to T do
; recursion step
for each state s from 1 to N do
viterbi[s,t]←
N
max
s′=1 viterbi[s′,t −1] ∗as′,s ∗bs(ot)
backpointer[s,t]←
N
argmax
s′=1
viterbi[s′,t −1] ∗as′,s ∗bs(ot)
bestpathprob←
N
max
s=1
viterbi[s,T]
; termination step
bestpathpointer←
N
argmax
s=1
viterbi[s,T]
; termination step
bestpath←the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob
Figure 8.5
Viterbi algorithm for ﬁnding the optimal sequence of tags. Given an observation sequence and an
HMM λ = (A,B), the algorithm returns the state path through the HMM that assigns maximum likelihood to
the observation sequence.
The Viterbi algorithm ﬁrst sets up a probability matrix or lattice, with one col-
umn for each observation ot and one row for each state in the state graph. Each col-
umn thus has a cell for each state qi in the single combined automaton. Figure 8.6
shows an intuition of this lattice for the sentence Janet will back the bill.
Each cell of the lattice, vt(j), represents the probability that the HMM is in state
j after seeing the ﬁrst t observations and passing through the most probable state
sequence q1,...,qt−1, given the HMM λ. The value of each cell vt(j) is computed
by recursively taking the most probable path that could lead us to this cell. Formally,
each cell expresses the probability
vt(j) =
max
q1,...,qt−1 P(q1...qt−1,o1,o2 ...ot,qt = j|λ)
(8.19)
We represent the most probable path by taking the maximum over all possible
previous state sequences
max
q1,...,qt−1. Like other dynamic programming algorithms,

154
CHAPTER 8
•
PART-OF-SPEECH TAGGING
JJ
NNP
NNP
NNP
MD
MD
MD
MD
VB
VB
JJ
JJ
JJ
NN
NN
RB
RB
RB
RB
DT
DT
DT
DT
NNP
Janet
will
back
the
bill
NN
VB
MD
NN
VB
JJ
RB
NNP
DT
NN
VB
Figure 8.6
A sketch of the lattice for Janet will back the bill, showing the possible tags (qi)
for each word and highlighting the path corresponding to the correct tag sequence through the
hidden states. States (parts of speech) which have a zero probability of generating a particular
word according to the B matrix (such as the probability that a determiner DT will be realized
as Janet) are greyed out.
Viterbi ﬁlls each cell recursively. Given that we had already computed the probabil-
ity of being in every state at time t −1, we compute the Viterbi probability by taking
the most probable of the extensions of the paths that lead to the current cell. For a
given state qj at time t, the value vt(j) is computed as
vt(j) =
N
max
i=1 vt−1(i) aij bj(ot)
(8.20)
The three factors that are multiplied in Eq. 8.20 for extending the previous paths to
compute the Viterbi probability at time t are
vt−1(i)
the previous Viterbi path probability from the previous time step
ai j
the transition probability from previous state qi to current state qj
bj(ot)
the state observation likelihood of the observation symbol ot given
the current state j
8.4.6
Working through an example
Let’s tag the sentence Janet will back the bill; the goal is the correct series of tags
(see also Fig. 8.6):
(8.21) Janet/NNP will/MD back/VB the/DT bill/NN
Let the HMM be deﬁned by the two tables in Fig. 8.7 and Fig. 8.8. Figure 8.7
lists the ai j probabilities for transitioning between the hidden states (part-of-speech
tags). Figure 8.8 expresses the bi(ot) probabilities, the observation likelihoods of
words given tags. This table is (slightly simpliﬁed) from counts in the WSJ corpus.
So the word Janet only appears as an NNP, back has 4 possible parts of speech, and
the word the can appear as a determiner or as an NNP (in titles like “Somewhere
Over the Rainbow” all words are tagged as NNP).
Figure 8.9 shows a ﬂeshed-out version of the sketch we saw in Fig. 8.6, the
Viterbi lattice for computing the best hidden state sequence for the observation se-
quence Janet will back the bill.

8.4
•
HMM PART-OF-SPEECH TAGGING
155
NNP
MD
VB
JJ
NN
RB
DT
<s>
0.2767
0.0006
0.0031 0.0453 0.0449 0.0510 0.2026
NNP
0.3777
0.0110
0.0009 0.0084 0.0584 0.0090 0.0025
MD
0.0008
0.0002
0.7968 0.0005 0.0008 0.1698 0.0041
VB
0.0322
0.0005
0.0050 0.0837 0.0615 0.0514 0.2231
JJ
0.0366
0.0004
0.0001 0.0733 0.4509 0.0036 0.0036
NN
0.0096
0.0176
0.0014 0.0086 0.1216 0.0177 0.0068
RB
0.0068
0.0102
0.1011 0.1012 0.0120 0.0728 0.0479
DT
0.1147
0.0021
0.0002 0.2157 0.4744 0.0102 0.0017
Figure 8.7
The A transition probabilities P(ti|ti−1) computed from the WSJ corpus without
smoothing. Rows are labeled with the conditioning event; thus P(VB|MD) is 0.7968.
Janet
will
back
the
bill
NNP
0.000032 0
0
0.000048 0
MD
0
0.308431 0
0
0
VB
0
0.000028 0.000672 0
0.000028
JJ
0
0
0.000340 0
0
NN
0
0.000200 0.000223 0
0.002337
RB
0
0
0.010446 0
0
DT
0
0
0
0.506099 0
Figure 8.8
Observation likelihoods B computed from the WSJ corpus without smoothing,
simpliﬁed slightly.
There are N = 5 state columns. We begin in column 1 (for the word Janet) by
setting the Viterbi value in each cell to the product of the π transition probability
(the start probability for that state i, which we get from the <s> entry of Fig. 8.7),
and the observation likelihood of the word Janet given the tag for that cell. Most of
the cells in the column are zero since the word Janet cannot be any of those tags.
The reader should ﬁnd this in Fig. 8.9.
Next, each cell in the will column gets updated. For each state, we compute the
value viterbi[s,t] by taking the maximum over the extensions of all the paths from
the previous column that lead to the current cell according to Eq. 8.20. We have
shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7
values from the previous column, multiplied by the appropriate transition probabil-
ity; as it happens in this case, most of them are zero from the previous column. The
remaining value is multiplied by the relevant observation probability, and the (triv-
ial) max is taken. In this case the ﬁnal value, 2.772e-8, comes from the NNP state at
the previous column. The reader should ﬁll in the rest of the lattice in Fig. 8.9 and
backtrace to see whether or not the Viterbi algorithm returns the gold state sequence
NNP MD VB DT NN.
8.4.7
Extending the HMM Algorithm to Trigrams
Practical HMM taggers have a number of extensions of this simple model. One
important missing feature is a wider tag context. In the tagger described above the
probability of a tag depends only on the previous tag:
P(tn
1) ≈
n
Y
i=1
P(ti|ti−1)
(8.22)

156
CHAPTER 8
•
PART-OF-SPEECH TAGGING
π
P(NNP|start) 
= .28
* P(MD|MD)
= 0
*  P(MD|NNP)
.000009*.01  = 
.9e-8 
v1(2)=
.0006 x 0 = 
0
v1(1) =
 .28* .000032 
= .000009
t
MD
q2
q1
o1
Janet
bill
will
o2
o3
back
VB
JJ
v1(3)=
.0031 x 0 
= 0
v1(4)= .
045*0=0
o4
  *  P(MD|VB) 
= 0
 * P(MD|JJ)
= 0
P(VB|start) 
= .0031
P(JJ |start) =
.045
backtrace
q3
q4
the
NN
q5
RB
q6
DT
q7
v2(2) =
max * .308 =
2.772e-8
v2(5)=
max * .0002 
= .0000000001
v2(3)=
max * .000028 
=     2.5e-13
v3(6)=
max * .0104
v3(5)=
max * .
000223
v3(4)=
max * .00034
v3(3)=
max * .00067
v1(5)
v1(6)
v1(7)
v2(1)
v2(4)
v2(6)
v2(7)
backtrace
* P(RB|NN)
* P(NN|NN)
start
start
start
start
start
o5
NNP
P(MD|start) 
= .0006
Figure 8.9
The ﬁrst few entries in the individual state columns for the Viterbi algorithm. Each cell keeps the
probability of the best path so far and a pointer to the previous cell along that path. We have only ﬁlled out
columns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the
reader. After the cells are ﬁlled in, backtracing from the end state, we should be able to reconstruct the correct
state sequence NNP MD VB DT NN.
In practice we use more of the history, letting the probability of a tag depend on the
two previous tags:
P(tn
1) ≈
n
Y
i=1
P(ti|ti−1,ti−2)
(8.23)
Extending the algorithm from bigram to trigram taggers gives a small (perhaps a
half point) increase in performance, but conditioning on two previous tags instead of
one requires a signiﬁcant change to the Viterbi algorithm. For each cell, instead of
taking a max over transitions from each cell in the previous column, we have to take
a max over paths through the cells in the previous two columns, thus considering N2
rather than N hidden states at every observation.
In addition to increasing the context window, HMM taggers have a number of
other advanced features. One is to let the tagger know the location of the end of the
sentence by adding dependence on an end-of-sequence marker for tn+1. This gives
the following equation for part-of-speech tagging:
ˆtn
1 = argmax
tn
1
P(tn
1|wn
1) ≈argmax
tn
1
" n
Y
i=1
P(wi|ti)P(ti|ti−1,ti−2)
#
P(tn+1|tn)
(8.24)

8.4
•
HMM PART-OF-SPEECH TAGGING
157
In tagging any sentence with Eq. 8.24, three of the tags used in the context will
fall off the edge of the sentence, and hence will not match regular words. These tags,
t−1, t0, and tn+1, can all be set to be a single special ‘sentence boundary’ tag that is
added to the tagset, which assumes sentences boundaries have already been marked.
One problem with trigram taggers as instantiated in Eq. 8.24 is data sparsity.
Any particular sequence of tags ti−2,ti−1,ti that occurs in the test set may simply
never have occurred in the training set. That means we cannot compute the tag
trigram probability just by the maximum likelihood estimate from counts, following
Eq. 8.25:
P(ti|ti−1,ti−2) = C(ti−2,ti−1,ti)
C(ti−2,ti−1)
(8.25)
Just as we saw with language modeling, many of these counts will be zero
in any training set, and we will incorrectly predict that a given tag sequence will
never occur! What we need is a way to estimate P(ti|ti−1,ti−2) even if the sequence
ti−2,ti−1,ti never occurs in the training data.
The standard approach to solving this problem is the same interpolation idea
we saw in language modeling: estimate the probability by combining more robust,
but weaker estimators. For example, if we’ve never seen the tag sequence PRP VB
TO, and so can’t compute P(TO|PRP,VB) from this frequency, we still could rely
on the bigram probability P(TO|VB), or even the unigram probability P(TO). The
maximum likelihood estimation of each of these probabilities can be computed from
a corpus with the following counts:
Trigrams
ˆP(ti|ti−1,ti−2) = C(ti−2,ti−1,ti)
C(ti−2,ti−1)
(8.26)
Bigrams
ˆP(ti|ti−1) = C(ti−1,ti)
C(ti−1)
(8.27)
Unigrams
ˆP(ti) = C(ti)
N
(8.28)
The standard way to combine these three estimators to estimate the trigram probabil-
ity P(ti|ti−1,ti−2) is via linear interpolation. We estimate the probability P(ti|ti−1ti−2)
by a weighted sum of the unigram, bigram, and trigram probabilities:
P(ti|ti−1ti−2) = λ3 ˆP(ti|ti−1ti−2)+λ2 ˆP(ti|ti−1)+λ1 ˆP(ti)
(8.29)
We require λ1 + λ2 + λ3 = 1, ensuring that the resulting P is a probability distri-
bution. The λs are set by deleted interpolation (Jelinek and Mercer, 1980): we
deleted
interpolation
successively delete each trigram from the training corpus and choose the λs so as to
maximize the likelihood of the rest of the corpus. The deletion helps to set the λs
in such a way as to generalize to unseen data and not overﬁt. Figure 8.10 gives a
deleted interpolation algorithm for tag trigrams.
8.4.8
Beam Search
When the number of states grows very large, the vanilla Viterbi algorithm is slow.
The complexity of the algorithm is O(N2T); N (the number of states) can be large
for trigram taggers, which have to consider every previous pair of the 45 tags, re-
sulting in 453 = 91,125 computations per column. N can be even larger for other
applications of Viterbi, for example to decoding in neural networks, as we will see
in future chapters.

158
CHAPTER 8
•
PART-OF-SPEECH TAGGING
function DELETED-INTERPOLATION(corpus) returns λ1,λ2,λ3
λ1, λ2, λ3 ←0
foreach trigram t1,t2,t3 with C(t1,t2,t3) > 0
depending on the maximum of the following three values
case C(t1,t2,t3)−1
C(t1,t2)−1 : increment λ3 by C(t1,t2,t3)
case C(t2,t3)−1
C(t2)−1 : increment λ2 by C(t1,t2,t3)
case C(t3)−1
N−1 : increment λ1 by C(t1,t2,t3)
end
end
normalize λ1,λ2,λ3
return λ1,λ2,λ3
Figure 8.10
The deleted interpolation algorithm for setting the weights for combining uni-
gram, bigram, and trigram tag probabilities. If the denominator is 0 for any case, we deﬁne
the result of that case to be 0. N is the number of tokens in the corpus. After Brants (2000).
One common solution to the complexity problem is the use of beam search
beam search
decoding. In beam search, instead of keeping the entire column of states at each
time point t, we just keep the best few hypothesis at that point. At time t this requires
computing the Viterbi score for each of the N cells, sorting the scores, and keeping
only the best-scoring states. The rest are pruned out and not continued forward to
time t +1.
One way to implement beam search is to keep a ﬁxed number of states instead of
all N current states. Here the beam width β is a ﬁxed number of states. Alternatively
beam width
β can be modeled as a ﬁxed percentage of the N states, or as a probability threshold.
Figure 8.11 shows the search lattice using a beam width of 2 states.
JJ
NNP
NNP
NNP
MD
MD
MD
MD
VB
VB
JJ
JJ
JJ
NN
NN
RB
RB
RB
RB
DT
DT
DT
DT
NNP
Janet
will
back
the
bill
NN
VB
MD
NN
VB
JJ
RB
NNP
DT
NN
VB
Figure 8.11
A beam search version of Fig. 8.6, showing a beam width of 2. At each time
t, all (non-zero) states are computed, but then they are sorted and only the best 2 states are
propagated forward and the rest are pruned, shown in orange.

8.5
•
MAXIMUM ENTROPY MARKOV MODELS
159
8.4.9
Unknown Words
words people
never use —
could be
only I
know them
Ishikawa Takuboku 1885–1912
To achieve high accuracy with part-of-speech taggers, it is also important to have
a good model for dealing with unknown words. Proper names and acronyms are
unknown
words
created very often, and even new common nouns and verbs enter the language at a
surprising rate. One useful feature for distinguishing parts of speech is word shape:
words starting with capital letters are likely to be proper nouns (NNP).
But the strongest source of information for guessing the part-of-speech of un-
known words is morphology. Words that end in -s are likely to be plural nouns
(NNS), words ending with -ed tend to be past participles (VBN), words ending with
-able adjectives (JJ), and so on. We store for each ﬁnal letter sequence (for sim-
plicity referred to as word sufﬁxes) of up to 10 letters the statistics of the tag it was
associated with in training. We are thus computing for each sufﬁx of length i the
probability of the tag ti given the sufﬁx letters (Samuelsson 1993, Brants 2000):
P(ti|ln−i+1 ...ln)
(8.30)
Back-off is used to smooth these probabilities with successively shorter sufﬁxes.
Because unknown words are unlikely to be closed-class words like prepositions,
sufﬁx probabilities can be computed only for words whose training set frequency is
≤10, or only for open-class words. Separate sufﬁx tries are kept for capitalized and
uncapitalized words.
Finally, because Eq. 8.30 gives a posterior estimate p(ti|wi), we can compute
the likelihood p(wi|ti) that HMMs require by using Bayesian inversion (i.e., using
Bayes’ rule and computation of the two priors P(ti) and P(ti|ln−i+1 ...ln)).
In addition to using capitalization information for unknown words, Brants (2000)
also uses capitalization for known words by adding a capitalization feature to each
tag. Thus, instead of computing P(ti|ti−1,ti−2) as in Eq. 8.26, the algorithm com-
putes the probability P(ti,ci|ti−1,ci−1,ti−2,ci−2). This is equivalent to having a cap-
italized and uncapitalized version of each tag, doubling the size of the tagset.
Combining all these features, a trigram HMM like that of Brants (2000) has a
tagging accuracy of 96.7% on the Penn Treebank, perhaps just slightly below the
performance of the best MEMM and neural taggers.
8.5
Maximum Entropy Markov Models
While an HMM can achieve very high accuracy, we saw that it requires a number of
architectural innovations to deal with unknown words, backoff, sufﬁxes, and so on.
It would be so much easier if we could add arbitrary features directly into the model
in a clean way, but that’s hard for generative models like HMMs. Luckily, we’ve
already seen a model for doing this: the logistic regression model of Chapter 5! But
logistic regression isn’t a sequence model; it assigns a class to a single observation.
However, we could turn logistic regression into a discriminative sequence model
simply by running it on successive words, using the class assigned to the prior word

160
CHAPTER 8
•
PART-OF-SPEECH TAGGING
as a feature in the classiﬁcation of the next word. When we apply logistic regression
in this way, it’s called the maximum entropy Markov model or MEMM.5
MEMM
Let the sequence of words be W = wn
1 and the sequence of tags T = tn
1. In an
HMM to compute the best tag sequence that maximizes P(T|W) we rely on Bayes’
rule and the likelihood P(W|T):
ˆT = argmax
T
P(T|W)
= argmax
T
P(W|T)P(T)
= argmax
T
Y
i
P(wordi|tagi)
Y
i
P(tagi|tagi−1)
(8.31)
In an MEMM, by contrast, we compute the posterior P(T|W) directly, training it to
discriminate among the possible tag sequences:
ˆT = argmax
T
P(T|W)
= argmax
T
Y
i
P(ti|wi,ti−1)
(8.32)
Consider tagging just one word. A multinomial logistic regression classiﬁer could
compute the single probability P(ti|wi,ti−1) in a different way than an HMM. Fig. 8.12
shows the intuition of the difference via the direction of the arrows; HMMs compute
likelihood (observation word conditioned on tags) but MEMMs compute posterior
(tags conditioned on observation words).
will
MD
VB
DT
NN
Janet
back
the
bill
NNP
will
MD
VB
DT
NN
Janet
back
the
bill
NNP
Figure 8.12
A schematic view of the HMM (top) and MEMM (bottom) representation of
the probability computation for the correct sequence of tags for the back sentence. The HMM
computes the likelihood of the observation given the hidden state, while the MEMM computes
the posterior of each state, conditioned on the previous state and current observation.
8.5.1
Features in a MEMM
Of course we don’t build MEMMs that condition just on wi and ti−1. The reason
to use a discriminative sequence model is that it’s easier to incorporate a lot of fea-
tures.6 Figure 8.13 shows a graphical intuition of some of these additional features.
5
‘Maximum entropy model’ is an outdated name for logistic regression; see the history section.
6
Because in HMMs all computation is based on the two probabilities P(tag|tag) and P(word|tag), if
we want to include some source of knowledge into the tagging process, we must ﬁnd a way to encode
the knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of
complicated conditioning which gets harder and harder as we have more and more such features.

8.5
•
MAXIMUM ENTROPY MARKOV MODELS
161
will
MD
VB
Janet
back
the
bill
NNP
<s>
wi
wi+1
wi-1
ti-1
ti-2
wi-2
Figure 8.13
An MEMM for part-of-speech tagging showing the ability to condition on
more features.
A basic MEMM part-of-speech tagger conditions on the observation word it-
self, neighboring words, and previous tags, and various combinations, using feature
templates like the following:
templates
⟨ti,wi−2⟩,⟨ti,wi−1⟩,⟨ti,wi⟩,⟨ti,wi+1⟩,⟨ti,wi+2⟩
⟨ti,ti−1⟩,⟨ti,ti−2,ti−1⟩,
⟨ti,ti−1,wi⟩,⟨ti,wi−1,wi⟩⟨ti,wi,wi+1⟩,
(8.33)
Recall from Chapter 5 that feature templates are used to automatically populate the
set of features from every instance in the training and test set. Thus our example
Janet/NNP will/MD back/VB the/DT bill/NN, when wi is the word back, would gen-
erate the following features:
ti = VB and wi−2 = Janet
ti = VB and wi−1 = will
ti = VB and wi = back
ti = VB and wi+1 = the
ti = VB and wi+2 = bill
ti = VB and ti−1 = MD
ti = VB and ti−1 = MD and ti−2 = NNP
ti = VB and wi = back and wi+1 = the
Also necessary are features to deal with unknown words, expressing properties of
the word’s spelling or shape:
wi contains a particular preﬁx (from all preﬁxes of length ≤4)
wi contains a particular sufﬁx (from all sufﬁxes of length ≤4)
wi contains a number
wi contains an upper-case letter
wi contains a hyphen
wi is all upper case
wi’s word shape
wi’s short word shape
wi is upper case and has a digit and a dash (like CFC-12)
wi is upper case and followed within 3 words by Co., Inc., etc.
Word shape features are used to represent the abstract letter pattern of the word
word shape
by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and retaining
punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30 would
map to XXdd-dd. A second class of shorter word shape features is also used. In these
features consecutive character types are removed, so DC10-30 would be mapped to
Xd-d but I.M.F would still map to X.X.X. For example the word well-dressed would
generate the following non-zero valued feature values:

162
CHAPTER 8
•
PART-OF-SPEECH TAGGING
preﬁx(wi) = w
preﬁx(wi) = we
preﬁx(wi) = wel
preﬁx(wi) = well
sufﬁx(wi) = ssed
sufﬁx(wi) = sed
sufﬁx(wi) = ed
sufﬁx(wi) = d
has-hyphen(wi)
word-shape(wi) = xxxx-xxxxxxx
short-word-shape(wi) = x-x
Features for known words, like the templates in Eq. 8.33, are computed for every
word seen in the training set. The unknown word features can also be computed for
all words in training, or only on training words whose frequency is below some
threshold. The result of the known-word templates and word-signature features is a
very large set of features. Generally a feature cutoff is used in which features are
thrown out if they have count < 5 in the training set.
8.5.2
Decoding and Training MEMMs
The most likely sequence of tags is then computed by combining these features of
the input word wi, its neighbors within l words wi+l
i−l, and the previous k tags ti−1
i−k as
follows (using θ to refer to feature weights instead of w to avoid the confusion with
w meaning words):
ˆT = argmax
T
P(T|W)
= argmax
T
Y
i
P(ti|wi+l
i−l,ti−1
i−k )
= argmax
T
Y
i
exp

X
j
θj f j(ti,wi+l
i−l,ti−1
i−k )


X
t′∈tagset
exp

X
j
θ j f j(t′,wi+l
i−l,ti−1
i−k )


(8.34)
How should we decode to ﬁnd this optimal tag sequence ˆT? The simplest way
to turn logistic regression into a sequence model is to build a local classiﬁer that
classiﬁes each word left to right, making a hard classiﬁcation on the ﬁrst word in
the sentence, then a hard decision on the second word, and so on. This is called a
greedy decoding algorithm, because we greedily choose the best tag for each word,
greedy
as shown in Fig. 8.14.
function GREEDY SEQUENCE DECODING(words W, model P) returns tag sequence T
for i = 1 to length(W)
ˆti = argmax
t′∈T
P(t′ | wi+l
i−l,ti−1
i−k )
Figure 8.14
In greedy decoding we simply run the classiﬁer on each token, left to right,
each time making a hard decision about which is the best tag.

8.6
•
BIDIRECTIONALITY
163
The problem with the greedy algorithm is that by making a hard decision on
each word before moving on to the next word, the classiﬁer can’t use evidence from
future decisions. Although the greedy algorithm is very fast, and occasionally has
sufﬁcient accuracy to be useful, in general the hard decision causes too great a drop
in performance, and we don’t use it.
Instead we decode an MEMM with the Viterbi algorithm just as with the HMM,
Viterbi
ﬁnding the sequence of part-of-speech tags that is optimal for the whole sentence.
For example, assume that our MEMM is only conditioning on the previous tag
ti−1 and observed word wi. Concretely, this involves ﬁlling an N × T array with
the appropriate values for P(ti|ti−1,wi), maintaining backpointers as we proceed. As
with HMM Viterbi, when the table is ﬁlled, we simply follow pointers back from the
maximum value in the ﬁnal column to retrieve the desired set of labels. The requisite
changes from the HMM-style application of Viterbi have to do only with how we
ﬁll each cell. Recall from Eq. 8.20 that the recursive step of the Viterbi equation
computes the Viterbi value of time t for state j as
vt(j) =
N
max
i=1
vt−1(i)aij bj(ot); 1 ≤j ≤N,1 < t ≤T
(8.35)
which is the HMM implementation of
vt(j) =
N
max
i=1
vt−1(i) P(sj|si) P(ot|s j) 1 ≤j ≤N,1 < t ≤T
(8.36)
The MEMM requires only a slight change to this latter formula, replacing the a and
b prior and likelihood probabilities with the direct posterior:
vt(j) =
N
max
i=1
vt−1(i) P(sj|si,ot) 1 ≤j ≤N,1 < t ≤T
(8.37)
Learning in MEMMs relies on the same supervised learning algorithms we presented
for logistic regression. Given a sequence of observations, feature functions, and cor-
responding hidden states, we use gradient descent to train the weights to maximize
the log-likelihood of the training corpus.
8.6
Bidirectionality
The one problem with the MEMM and HMM models as presented is that they are
exclusively run left-to-right. While the Viterbi algorithm still allows present deci-
sions to be inﬂuenced indirectly by future decisions, it would help even more if a
decision about word wi could directly use information about future tags ti+1 and ti+2.
Adding bidirectionality has another useful advantage. MEMMs have a theoret-
ical weakness, referred to alternatively as the label bias or observation bias prob-
label bias
observation
bias
lem (Lafferty et al. 2001, Toutanova et al. 2003). These are names for situations
when one source of information is ignored because it is explained away by another
source. Consider an example from Toutanova et al. (2003), the sequence will/NN
to/TO ﬁght/VB. The tag TO is often preceded by NN but rarely by modals (MD),
and so that tendency should help predict the correct NN tag for will. But the previ-
ous transition P(twill|⟨s⟩) prefers the modal, and because P(TO|to,twill) is so close
to 1 regardless of twill the model cannot make use of the transition probability and
incorrectly chooses MD. The strong information that to must have the tag TO has ex-
plained away the presence of TO and so the model doesn’t learn the importance of

164
CHAPTER 8
•
PART-OF-SPEECH TAGGING
the previous NN tag for predicting TO. Bidirectionality helps the model by making
the link between TO available when tagging the NN.
One way to implement bidirectionality is to switch to a more powerful model
called a conditional random ﬁeld or CRF. The CRF is an undirected graphical
CRF
model, which means that it’s not computing a probability for each tag at each time
step. Instead, at each time step the CRF computes log-linear functions over a clique,
a set of relevant features. Unlike for an MEMM, these might include output features
of words in future time steps. The probability of the best sequence is similarly
computed by the Viterbi algorithm. Because a CRF normalizes probabilities over all
tag sequences, rather than over all the tags at an individual time t, training requires
computing the sum over all possible labelings, which makes CRF training quite slow.
Simpler methods can also be used; the Stanford tagger uses a bidirectional
Stanford tagger
version of the MEMM called a cyclic dependency network (Toutanova et al., 2003).
Alternatively, any sequence model can be turned into a bidirectional model by
using multiple passes. For example, the ﬁrst pass would use only part-of-speech
features from already-disambiguated words on the left. In the second pass, tags for
all words, including those on the right, can be used. Alternately, the tagger can be run
twice, once left-to-right and once right-to-left. In greedy decoding, for each word
the classiﬁer chooses the highest-scoring of the tags assigned by the left-to-right and
right-to-left classiﬁer. In Viterbi decoding, the classiﬁer chooses the higher scoring
of the two sequences (left-to-right or right-to-left). These bidirectional models lead
directly into the bi-LSTM models that we will introduce in Chapter 9 as a standard
neural sequence model.
8.7
Part-of-Speech Tagging for Morphological Rich Lan-
guages
Augmentations to tagging algorithms become necessary when dealing with lan-
guages with rich morphology like Czech, Hungarian and Turkish.
These productive word-formation processes result in a large vocabulary for these
languages: a 250,000 word token corpus of Hungarian has more than twice as many
word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while
a 10 million word token corpus of Turkish contains four times as many word types
as a similarly sized English corpus (Hakkani-T¨ur et al., 2002). Large vocabular-
ies mean many unknown words, and these unknown words cause signiﬁcant per-
formance degradations in a wide variety of languages (including Czech, Slovene,
Estonian, and Romanian) (Hajiˇc, 2000).
Highly inﬂectional languages also have much more information than English
coded in word morphology, like case (nominative, accusative, genitive) or gender
(masculine, feminine). Because this information is important for tasks like pars-
ing and coreference resolution, part-of-speech taggers for morphologically rich lan-
guages need to label words with case and gender information. Tagsets for morpho-
logically rich languages are therefore sequences of morphological tags rather than a
single primitive tag. Here’s a Turkish example, in which the word izin has three pos-
sible morphological/part-of-speech tags and meanings (Hakkani-T¨ur et al., 2002):
1. Yerdeki izin temizlenmesi gerek.
iz + Noun+A3sg+Pnon+Gen
The trace on the ﬂoor should be cleaned.
2. ¨Uzerinde parmak izin kalmis¸
iz + Noun+A3sg+P2sg+Nom

8.8
•
SUMMARY
165
Your ﬁnger print is left on (it).
3. Ic¸eri girmek ic¸in izin alman gerekiyor.
izin + Noun+A3sg+Pnon+Nom
You need permission to enter.
Using a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part-
of-speech tag greatly increases the number of parts of speech, and so tagsets can
be 4 to 10 times larger than the 50–100 tags we have seen for English. With such
large tagsets, each word needs to be morphologically analyzed to generate the list
of possible morphological tag sequences (part-of-speech tags) for the word. The
role of the tagger is then to disambiguate among these tags. This method also helps
with unknown words since morphological parsers can accept unknown stems and
still segment the afﬁxes properly.
For non-word-space languages like Chinese, word segmentation (Chapter 2) is
either applied before tagging or done jointly. Although Chinese words are on aver-
age very short (around 2.4 characters per unknown word compared with 7.7 for En-
glish) the problem of unknown words is still large. While English unknown words
tend to be proper nouns in Chinese the majority of unknown words are common
nouns and verbs because of extensive compounding. Tagging models for Chinese
use similar unknown word features to English, including character preﬁx and suf-
ﬁx features, as well as novel features like the radicals of each character in a word.
(Tseng et al., 2005).
A standard for multilingual tagging is the Universal POS tag set of the Universal
Dependencies project, which contains 16 tags plus a wide variety of features that
can be added to them to create a large tagset for any language (Nivre et al., 2016a).
8.8
Summary
This chapter introduced parts of speech and part-of-speech tagging:
• Languages generally have a small set of closed class words that are highly
frequent, ambiguous, and act as function words, and open-class words like
nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40
and 200 tags.
• Part-of-speech tagging is the process of assigning a part-of-speech label to
each of a sequence of words.
• Two common approaches to sequence modeling are a generative approach,
HMM tagging, and a discriminative approach, MEMM tagging. We will see
a third, discriminative neural approach in Chapter 9.
• The probabilities in HMM taggers are estimated by maximum likelihood es-
timation on tag-labeled training corpora. The Viterbi algorithm is used for
decoding, ﬁnding the most likely tag sequence
• Beam search is a variant of Viterbi decoding that maintains only a fraction of
high scoring states rather than all states during decoding.
• Maximum entropy Markov model or MEMM taggers train logistic regres-
sion models to pick the best tag given an observation word and its context and
the previous tags, and then use Viterbi to choose the best sequence of tags.
• Modern taggers are generally run bidirectionally.

166
CHAPTER 8
•
PART-OF-SPEECH TAGGING
Bibliographical and Historical Notes
What is probably the earliest part-of-speech tagger was part of the parser in Zellig
Harris’s Transformations and Discourse Analysis Project (TDAP), implemented be-
tween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),
although earlier systems had used part-of-speech dictionaries. TDAP used 14 hand-
written rules for part-of-speech disambiguation; the use of part-of-speech tag se-
quences and the relative frequency of tags for a word preﬁgures all modern algo-
rithms. The parser was implemented essentially as a cascade of ﬁnite-state trans-
ducers; see Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.
The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had
three components: a lexicon, a morphological analyzer, and a context disambigua-
tor. The small 1500-word lexicon listed only function words and other irregular
words. The morphological analyzer used inﬂectional and derivational sufﬁxes to as-
sign part-of-speech classes. These were run over words to produce candidate parts
of speech which were then disambiguated by a set of 500 context rules by relying on
surrounding islands of unambiguous words. For example, one rule said that between
an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-
ADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used
the same architecture as Klein and Simmons (1963), with a bigger dictionary and
more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis
and Kuˇcera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the
Brown corpus was then tagged by hand. All these early algorithms were based on
a two-stage architecture in which a dictionary was ﬁrst used to assign each word a
set of potential parts of speech, and then lists of handwritten disambiguation rules
winnowed the set down to a single part of speech per word.
Soon afterwards probabilistic architectures began to be developed. Probabili-
ties were used in tagging by Stolz et al. (1965) and a complete probabilistic tagger
with Viterbi decoding was sketched by Bahl and Mercer (1976). The Lancaster-
Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown corpus, was
tagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Marshall 1987;
Garside 1987), a probabilistic algorithm that approximated a simpliﬁed HMM tag-
ger. The algorithm used tag bigram probabilities, but instead of storing the word
likelihood of each tag, the algorithm marked tags either as rare (P(tag|word) < .01)
infrequent (P(tag|word) < .10) or normally frequent (P(tag|word) > .10).
DeRose (1988) developed a quasi-HMM algorithm, including the use of dy-
namic programming, although computing P(t|w)P(w) instead of P(w|t)P(w). The
same year, the probabilistic PARTS tagger of Church (1988), (1989) was probably
the ﬁrst implemented HMM tagger, described correctly in Church (1989), although
Church (1988) also described the computation incorrectly as P(t|w)P(w) instead
of P(w|t)P(w). Church (p.c.) explained that he had simpliﬁed for pedagogical pur-
poses because using the probability P(t|w) made the idea seem more understandable
as “storing a lexicon in an almost standard form”.
Later taggers explicitly introduced the use of the hidden Markov model (Ku-
piec 1992; Weischedel et al. 1993; Sch¨utze and Singer 1994). Merialdo (1994)
showed that fully unsupervised EM didn’t work well for the tagging task and that
reliance on hand-labeled data was important. Charniak et al. (1993) showed the im-
portance of the most frequent tag baseline; the 92.3% number we give above was
from Abney et al. (1999). See Brants (2000) for many implementation details of an
HMM tagger whose performance is still roughly close to state of the art taggers.

EXERCISES
167
Ratnaparkhi (1996) introduced the MEMM tagger, called MXPOST, and the
modern formulation is very much based on his work.
The idea of using letter sufﬁxes for unknown words is quite old; the early Klein
and Simmons (1963) system checked all ﬁnal letter sufﬁxes of lengths 1-5. The
probabilistic formulation we described for HMMs comes from Samuelsson (1993).
The unknown word features described on page 161 come mainly from (Ratnaparkhi,
1996), with augmentations from Toutanova et al. (2003) and Manning (2011).
State of the art taggers use neural algorithms like the sequence models in Chap-
ter 9 or (bidirectional) log-linear models Toutanova et al. (2003). HMM (Brants 2000;
Thede and Harper 1999) and MEMM tagger accuracies are likely just a tad lower.
An alternative modern formalism, the English Constraint Grammar systems (Karls-
son et al. 1995; Voutilainen 1995; Voutilainen 1999), uses a two-stage formalism
much like the early taggers from the 1950s and 1960s. A morphological analyzer
with tens of thousands of English word stem entries returns all parts of speech for a
word, using a large feature-based tagset. So the word occurred is tagged with the op-
tions ⟨V PCP2 SV⟩and ⟨V PAST VFIN SV⟩, meaning it can be a participle (PCP2)
for an intransitive (SV) verb, or a past (PAST) ﬁnite (VFIN) form of an intransitive
(SV) verb. A set of 3,744 constraints are then applied to the input sentence to rule
out parts of speech inconsistent with the context. For example here’s a rule for the
ambiguous word that that eliminates all tags except the ADV (adverbial intensiﬁer)
sense (this is the sense in the sentence it isn’t that odd):
ADVERBIAL-THAT RULE Given input: “that”
if (+1 A/ADV/QUANT); /* if next word is adj, adverb, or quantiﬁer */
(+2 SENT-LIM);
/* and following which is a sentence boundary, */
(NOT -1 SVOC/A); /* and the previous word is not a verb like */
/* ‘consider’ which allows adjs as object complements */
then eliminate non-ADV tags else eliminate ADV tag
Manning (2011) investigates the remaining 2.7% of errors in a high-performing
tagger, the bidirectional MEMM-style model described above (Toutanova et al.,
2003). He suggests that a third or half of these remaining errors are due to errors or
inconsistencies in the training data, a third might be solvable with richer linguistic
models, and for the remainder the task is underspeciﬁed or unclear.
Supervised tagging relies heavily on in-domain training data hand-labeled by
experts. Ways to relax this assumption include unsupervised algorithms for cluster-
ing words into part-of-speech-like classes, summarized in Christodoulopoulos et al.
(2010), and ways to combine labeled and unlabeled data, for example by co-training
(Clark et al. 2003; Søgaard 2010).
See Householder (1995) for historical notes on parts of speech, and Sampson
(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.
Exercises
8.1
Find one tagging error in each of the following sentences that are tagged with
the Penn Treebank tagset:
1. I/PRP need/VBP a/DT ﬂight/NN from/IN Atlanta/NN
2. Does/VBZ this/DT ﬂight/NN serve/VB dinner/NNS
3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP
4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN ﬂights/NNS

168
CHAPTER 8
•
PART-OF-SPEECH TAGGING
8.2
Use the Penn Treebank tagset to tag each word in the following sentences
from Damon Runyon’s short stories. You may ignore punctuation. Some of
these are quite difﬁcult; do your best.
1. It is a nice night.
2. This crap game is over a garage in Fifty-second Street...
3. ...Nobody ever takes the newspapers she sells ...
4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a
mournful voice.
5. ...I am sitting in Mindy’s restaurant putting on the geﬁllte ﬁsh, which is
a dish I am very fond of, ...
6. When a guy and a doll get to taking peeks back and forth at each other,
why there you are indeed.
8.3
Now compare your tags from the previous exercise with one or two friend’s
answers. On which words did you disagree the most? Why?
8.4
Implement the “most likely tag” baseline. Find a POS-tagged training set,
and use it to compute for each word the tag that maximizes p(t|w). You will
need to implement a simple tokenizer to deal with sentence boundaries. Start
by assuming that all unknown words are NN and compute your error rate on
known and unknown words. Now write at least ﬁve rules to do a better job of
tagging unknown words, and show the difference in error rates.
8.5
Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus.
First split the corpus into a training set and test set. From the labeled training
set, train the transition and observation probabilities of the HMM tagger di-
rectly on the hand-tagged data. Then implement the Viterbi algorithm so that
you can label an arbitrary test sentence. Now run your algorithm on the test
set. Report its error rate and compare its performance to the most frequent tag
baseline.
8.6
Do an error analysis of your tagger. Build a confusion matrix and investigate
the most frequent errors. Propose some features for improving the perfor-
mance of your tagger on these errors.

CHAPTER
9
Sequence
Processing
with
Recurrent Networks
Time will explain.
Jane Austen, Persuasion
Language is an inherently temporal phenomenon. When we comprehend and pro-
duce spoken language, we are processing continuous input streams of indeﬁnite
length. And even when dealing with written text we normally process it sequen-
tially, even though we in principle have arbitrary access to all the elements at once.
The temporal nature of language is reﬂected in the metaphors we use; we talk of
the ﬂow of conversations, news feeds, and twitter streams, all of which call out the
notion that language is a sequence that unfolds in time. This temporal nature is also
reﬂected in the algorithms we use to process language. When applied to the problem
of part-of-speech tagging, the Viterbi algorithm works its way incrementally through
its input a word at a time, taking into account information gleaned along the way.
The syntactic parsing algorithms we cover in Chapters 11, 12, and 13 operate in a
similar fashion.
In contrast, the machine learning approaches we’ve studied for sentiment analy-
sis and other classiﬁcation tasks do not have this temporal nature. These approaches
have simultaneous access to all aspects of their input. This is certainly true of feed-
forward neural networks, including their application to neural language models.
Such networks employ ﬁxed-size input vectors with associated weights to capture
all relevant aspects of an example at once. This makes it difﬁcult to deal with se-
quences of varying length, and they fail to capture important temporal aspects of
language.
We saw one work-around for these problems with the case of neural language
models. These models operate by accepting ﬁxed-sized windows of tokens as input;
sequences longer than the window size are processed by sliding windows over the
input making predictions as they go, with the end result being a sequence of pre-
dictions spanning the input. Importantly, the decision made for one window has no
impact on later decisions. Fig. 9.1, reproduced here from Chapter 7, illustrates this
approach with a window of size 3. Here, we’re predicting which word will come
next given the window the ground there. Subsequent words are predicted by sliding
the window forward one word at a time.
The sliding window approach is problematic for a number of reasons. First, it
shares the primary weakness of Markov approaches in that it limits the context from
which information can be extracted; anything outside the context window has no
impact on the decision being made. This is an issue since there are many language
tasks that require access to information that can be arbitrarily distant from the point
at which processing is happening. Second, the use of windows makes it difﬁcult for
networks to learn systematic patterns arising from phenomena like constituency. For

170
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
h1
h2
y1
h3
hdh
…
…
U
W
y42
y|V|
Projection layer
1⨉3d
concatenated embeddings
for context words
Hidden layer
Output layer P(w|u)
…
in
the
hole
...
...
ground
there
lived
word 42
embedding for
word 35
embedding for 
word 9925
embedding for 
word 45180
wt-1
wt-2
wt
wt-3
dh⨉3d
1⨉dh
|V|⨉dh
P(wt=V42|wt-3,wt-2,wt-3)
1⨉|V|
Figure 9.1
A simpliﬁed view of a feedforward neural language model moving through a text. At each time
step t the network takes the 3 context words, converts each to a d-dimensional embedding, and concatenates the
3 embeddings together to get the 1×Nd unit input layer x for the network.
example, in Fig. 9.1 the noun phrase the ground appears in two separate windows:
once, as shown, in the ﬁrst and second positions in the window, and in the preceding
step where it appears in the second and third positions, thus forcing the network to
learn two separate patterns for what should be a constituent.
The subject of this chapter is recurrent neural networks, a class of networks
recurrent
neural networks
designed to address these challenges by dealing directly with the temporal aspect of
language, allowing us to handle variable length inputs without the use of arbitrary
ﬁxed-sized windows, and providing the means to capture and exploit the temporal
nature of language.
9.1
Simple Recurrent Neural Networks
A recurrent neural network (RNN) is any network that contains a cycle within its
network connections. That is, any network where the value of a unit is directly, or
indirectly, dependent on earlier outputs as an input. While powerful, such networks
are difﬁcult to reason about and to train. However, within the general class of recur-
rent networks there are constrained architectures that have proven to be extremely
effective when applied to spoken and written language. In this section, we consider a
class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple
Elman
Networks
recurrent networks. These networks are useful in their own right and serve as the
basis for more complex approaches to be discussed later in this chapter and again in
Chapter 10 and Chapter 11. Going forward, when we use the term RNN we’ll be
referring to these simpler more constrained networks.
Fig. 9.2 illustrates the structure of a simple RNN. As with ordinary feedforward
networks, an input vector representing the current input element, xt, is multiplied by
a weight matrix and then passed through an activation function to compute an activa-

9.1
•
SIMPLE RECURRENT NEURAL NETWORKS
171
ht
yt
xt
Figure 9.2
Simple recurrent neural network after Elman (Elman, 1990). The hidden layer
includes a recurrent connection as part of its input. That is, the activation value of the hidden
layer depends on the current input as well as the activation value of the hidden layer from the
previous time step.
tion value for a layer of hidden units. This hidden layer is, in turn, used to calculate
a corresponding output, yt. In a departure from our earlier window-based approach,
sequences are processed by presenting one element at a time to the network. The
key difference from a feedforward network lies in the recurrent link shown in the
ﬁgure with the dashed line. This link augments the input to the computation at the
hidden layer with the activation value of the hidden layer from the preceding point
in time.
The hidden layer from the previous time step provides a form of memory, or
context, that encodes earlier processing and informs the decisions to be made at
later points in time. Critically, this architecture does not impose a ﬁxed-length limit
on this prior context; the context embodied in the previous hidden layer includes
information extending back to the beginning of the sequence.
Adding this temporal dimension may make RNNs appear to be more exotic than
non-recurrent architectures. But in reality, they’re not all that different. Given an
input vector and the values for the hidden layer from the previous time step, we’re
still performing the standard feedforward calculation. To see this, consider Fig. 9.3
which clariﬁes the nature of the recurrence and how it factors into the computation
at the hidden layer. The most signiﬁcant change lies in the new set of weights,
U, that connect the hidden layer from the previous time step to the current hidden
layer. These weights determine how the network should make use of past context in
calculating the output for the current input. As with the other weights in the network,
these connections are trained via backpropagation.
9.1.1
Inference in Simple RNNs
Forward inference (mapping a sequence of inputs to a sequence of outputs) in an
RNN is nearly identical to what we’ve already seen with feedforward networks. To
compute an output yt for an input xt, we need the activation value for the hidden
layer ht. To calculate this, we multiply the input xt with the weight matrix W, and
the hidden layer from the previous time step ht−1 with the weight matrix U. We
add these values together and pass them through a suitable activation function, g,
to arrive at the activation value for the current hidden layer, ht. Once we have the
values for the hidden layer, we proceed with the usual computation to generate the

172
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
U
V
W
yt
xt
ht
ht-1
Figure 9.3
Simple recurrent neural network illustrated as a feedforward network.
output vector.
ht = g(Uht−1 +Wxt)
yt = f(Vht)
In the commonly encountered case of soft classiﬁcation, computing yt consists of
a softmax computation that provides a normalized probability distribution over the
possible output classes.
yt = softmax(Vht)
The fact that the computation at time t requires the value of the hidden layer
from time t −1 mandates an incremental inference algorithm that proceeds from the
start of the sequence to the end as illustrated in Fig. 9.4. The sequential nature of
simple recurrent networks can also be seen by unrolling the network in time as is
shown in Fig. 9.5. In this ﬁgure, the various layers of units are copied for each time
step to illustrate that they will have differing values over time. However, the various
weight matrices are shared across time.
function FORWARDRNN(x, network) returns output sequence y
h0 ←0
for i←1 to LENGTH(x) do
hi ←g(U hi−1 + W xi)
yi ←f(V hi)
return y
Figure 9.4
Forward inference in a simple recurrent network. The matrices U, V and W are
shared across time, while new values for h and y are calculated with each time step.
9.1.2
Training
As with feedforward networks, we’ll use a training set, a loss function, and back-
propagation to obtain the gradients needed to adjust the weights in these recurrent
networks. As shown in Fig. 9.3, we now have 3 sets of weights to update: W, the

9.1
•
SIMPLE RECURRENT NEURAL NETWORKS
173
U
V
W
U
V
W
U
V
W
x1
x2
x3
y1
y2
y3
h1
h3
h2
h0
Figure 9.5
A simple recurrent neural network shown unrolled in time. Network layers are copied for each
time step, while the weights U, V and W are shared in common across all time steps.
weights from the input layer to the hidden layer, U, the weights from the previous
hidden layer to the current hidden layer, and ﬁnally V, the weights from the hidden
layer to the output layer.
Before going on, let’s ﬁrst review some of the notation that we introduced in
Chapter 7. Assuming a network with an input layer x and a non-linear activation
function g, a[i] refers to the activation value from a layer i, which is the result of
applying g to z[i], the weighted sum of the inputs to that layer.
Fig. 9.5 illustrates two considerations that we didn’t have to worry about with
backpropagation in feedforward networks. First, to compute the loss function for
the output at time t we need the hidden layer from time t −1. Second, the hidden
layer at time t inﬂuences both the output at time t and the hidden layer at time t +1
(and hence the output and loss at t +1). It follows from this that to assess the error
accruing to ht, we’ll need to know its inﬂuence on both the current output as well as
the ones that follow.
Consider the situation where we are examining an input/output pair at time 2 as
shown in Fig. 9.6. What do we need to compute the gradients required to update
the weights U, V, and W here? Let’s start by reviewing how we compute the gra-
dients required to update V since this computation is unchanged from feedforward
networks. To review from Chapter 7, we need to compute the derivative of the loss
function L with respect to the weights V. However, since the loss is not expressed
directly in terms of the weights, we apply the chain rule to get there indirectly.
∂L
∂V
= ∂L
∂a
∂a
∂z
∂z
∂V
The ﬁrst term on the right is the derivative of the loss function with respect to
the network output, a. The second term is the derivative of the network output with
respect to the intermediate network activation z, which is a function of the activation

174
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
function g. The ﬁnal term in our application of the chain rule is the derivative of the
network activation with respect to the weights V, which is the activation value of the
current hidden layer ht.
It’s useful here to use the ﬁrst two terms to deﬁne δ, an error term that represents
how much of the scalar loss is attributable to each of the units in the output layer.
δout = ∂L
∂a
∂a
∂z
(9.1)
δout = L′g′(z)
(9.2)
Therefore, the ﬁnal gradient we need to update the weight matrix V is just:
∂L
∂V
= δoutht
(9.3)
U
V
W
U
V
W
U
V
W
x1
x2
x3
y1
y2
y3
h1
h3
h2
h0
t1
t2
t3
Figure 9.6
The backpropagation of errors in a simple RNN ti vectors represent the targets for each element
of the sequence from the training data. The red arrows illustrate the ﬂow of backpropagated errors required to
calculate the gradients for U, V and W at time 2. The two incoming arrows converging on h2 signal that these
errors need to be summed.
Moving on, we need to compute the corresponding gradients for the weight ma-
trices W and U:
∂L
∂W and ∂L
∂U . Here we encounter the ﬁrst substantive change from
feedforward networks. The hidden state at time t contributes to the output and asso-
ciated error at time t and to the output and error at the next time step, t +1. Therefore,
the error term, δh, for the hidden layer must be the sum of the error term from the
current output and its error from the next time step.
δh = g′(z)Vδout +δnext
Given this total error term for the hidden layer, we can compute the gradients for
the weights U and W using the chain rule as we did in Chapter 7.

9.1
•
SIMPLE RECURRENT NEURAL NETWORKS
175
dL
dW
= dL
dz
dz
da
da
dW
dL
dU
= dL
dz
dz
da
da
dU
∂L
∂W
= δhxt
∂L
∂U
= δhht−1
These gradients provide us with the information needed to update the matrices U
and W.
We’re not quite done yet, we still need to assign proportional blame (compute
the error term) back to the previous hidden layer ht−1 for use in further processing.
This involves backpropagating the error from δh to ht−1 proportionally based on the
weights in U.
δnext = g′(z)Uδh
(9.4)
At this point we have all the gradients needed to perform weight updates for each of
our three sets of weights. Note that in this simple case there is no need to backprop-
agate the error through W to the input x, since the input training data is assumed to
be ﬁxed. If we wished to update our input word or character embeddings we would
backpropagate the error through to them as well.
Taken together, all of these considerations lead to a two-pass algorithm for train-
ing the weights in RNNs. In the ﬁrst pass, we perform forward inference, computing
ht, yt, and accumulating the loss at each step in time, saving the value of the hidden
layer at each step for use at the next time step. In the second phase, we process the
sequence in reverse, computing the required error terms gradients as we go, comput-
ing and saving the error term for use in the hidden layer for each step backward in
time. This general approach is commonly referred to as Backpropagation Through
Time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990).
Backpropaga-
tion Through
Time
9.1.3
Unrolled Networks as Computation Graphs
We used the unrolled network shown in Fig. 9.5 as a way to illustrate the tem-
poral nature of RNNs. However, with modern computational frameworks and ad-
equate computing resources, explicitly unrolling a recurrent network into a deep
feedforward computational graph is quite practical for word-by-word approaches to
sentence-level processing. In such an approach, we provide a template that speci-
ﬁes the basic structure of the network, including all the necessary parameters for the
input, output, and hidden layers, the weight matrices, as well as the activation and
output functions to be used. Then, when presented with a particular input sequence,
we can generate an unrolled feedforward network speciﬁc to that input, and use that
graph to perform forward inference or training via ordinary backpropagation.
For applications that involve much longer input sequences, such as speech recog-
nition, character-by-character sentence processing, or streaming of continuous in-
puts, unrolling an entire input sequence may not be feasible. In these cases, we can
unroll the input into manageable ﬁxed-length segments and treat each segment as a
distinct training item.

176
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
9.2
Applications of Recurrent Neural Networks
Recurrent neural networks have proven to be an effective approach to language mod-
eling, sequence labeling tasks such as part-of-speech tagging, as well as sequence
classiﬁcation tasks such as sentiment analysis and topic classiﬁcation. And as we’ll
see in Chapter 10 and Chapter 11, they form the basis for sequence-to-sequence
approaches to summarization, machine translation, and question answering.
9.2.1
Recurrent Neural Language Models
We’ve already seen two ways to create probabilistic language models: N-gram mod-
els and feedforward networks with sliding windows. Given a ﬁxed preceding con-
text, both attempt to predict the next word in a sequence. More formally, they com-
pute the conditional probability of the next word in a sequence given the preceding
words, P(wn|wn−1
1
).
In both approaches, the quality of a model is largely dependent on the size of
the context and how effectively the model makes use of it. Thus, both N-gram and
sliding-window neural networks are constrained by the Markov assumption embod-
ied in the following equation.
P(wn|wn−1
1
) ≈P(wn|wn−1
n−N+1)
(9.5)
That is, anything outside the preceding context of size N has no bearing on the
computation.
Recurrent neural language models process sequences a word at a time attempting
to predict the next word in a sequence by using the current word and the previous
hidden state as input (Mikolov et al., 2010). Thus, the limited context constraint
inherent in both N-gram models and sliding window approaches is avoided since the
hidden state embodies information about all of the preceding words all the way back
to the beginning of the sequence.
Forward inference in a recurrent language model proceeds as described in Sec-
tion 9.1.1. At each step the network retrieves a word embedding for the current word
as input and combines it with the hidden layer from the previous step to compute a
new hidden layer. This hidden layer is then used to generate an output layer which is
passed through a softmax layer to generate a probability distribution over the entire
vocabulary.
P(wn|wn−1
1
) = yn
(9.6)
= softmax(Vhn)
(9.7)
The probability of an entire sequence is then just the product of the probabilities of
each item in the sequence.
P(wn
1) =
n
Y
k=1
P(wk|wk−1
1
)
(9.8)
=
n
Y
k=1
yk
(9.9)

9.2
•
APPLICATIONS OF RECURRENT NEURAL NETWORKS
177
As with the approach introduced in Chapter 7, to train such a model we use a
corpus of representative text as training material. The task is to predict the next word
in a sequence given the previous words, using cross-entropy as the loss function.
Recall that the cross-entropy loss for a single example is the negative log probability
assigned to the correct class, which is the result of applying a softmax to the ﬁnal
output layer.
LCE(ˆy,y) = −log ˆyi
(9.10)
= −log
ezi
PK
j=1 ezj
(9.11)
Here, the correct class i is the word that actually comes next in the data and yi is
the probability assigned to that word, and the softmax is over the entire vocabulary,
which has size K. The weights in the network are adjusted to minimize the cross-
entropy loss over the training set via gradient descent.
Generation with Neural Language Models
As we saw with the probabilistic Shakespeare generator in Chapter 3, a fun way to
gain insight into a language model is to use Shannon’s method (Shannon, 1951) to
generate random sentences. The procedure is basically the same as that described
on 39.
• To begin, sample the ﬁrst word in the output from the softmax distribution that
results from using the beginning of sentence marker, <s>, as the ﬁrst input.
• Use the word embedding for that ﬁrst word as the input to the network at the
next time step, and then sample the next word in the same fashion.
• Continue generating until the end of sentence marker, </s>, is sampled or a
ﬁxed length limit is reached.
This technique is called autoregressive generation since the word generated at the
autoregressive
generation
each time step is conditioned on the word generated by the network at the previous
step. Fig. 9.7 illustrates this approach. In this ﬁgure, the details of the RNN’s hidden
layers and recurrent connections are hidden within the blue block.
While this is an entertaining exercise, this architecture has inspired state-of-
the-art approaches to applications such as machine translation, summarization, and
question answering. The key to these approaches is to prime the generation compo-
nent with an appropriate context. That is, instead of simply using <s> to get things
started we can provide a richer task-appropriate context. We’ll return to these more
advanced applications in Chapter 10, where we discuss encoder-decoder networks.
Finally, as we did with Shakespeare, we can move beyond informally assessing
the quality of generated output by using perplexity to objectively compare the output
to a held-out sample of the training corpus.
PP(W) =
N
v
u
u
t
N
Y
i=1
1
P(wi|wi−1)
(9.12)
The lower the perplexity, the better the model.
9.2.2
Sequence Labeling
In sequence labeling, the network’s task is to assign a label chosen from a small
ﬁxed set of labels to each element of a sequence. The canonical example of such a

178
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
In
a
<s>
RNN
hole
In
a
hole
?
Sampled Word
Softmax
Embedding
Input Word
Figure 9.7
Autoregressive generation with an RNN-based neural language model.
task is part-of-speech tagging, discussed in detail in Chapter 8. In an RNN approach
to POS tagging, inputs are word embeddings and the outputs are tag probabilities
generated by a softmax layer over the tagset, as illustrated in Fig. 9.8.
In this ﬁgure, the inputs at each time step are pre-trained word embeddings cor-
responding to the input tokens. The RNN block is an abstraction that represents
an unrolled simple recurrent network consisting of an input layer, hidden layer, and
output layer at each time step, as well as the shared U, V and W weight matrices that
comprise the network. The outputs of the network at each time step represent the
distribution over the POS tagset generated by a softmax layer.
To generate a tag sequence for a given input, we can run forward inference over
Janet
will
back
RNN
the
bill
NN
DT
VB
MD
NNP
Softmax
Argmax
Embdeddings
Words
Figure 9.8
Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained
word embeddings serve as inputs and a softmax layer provides a probability distribution over
the part-of-speech tags as output at each time step.

9.2
•
APPLICATIONS OF RECURRENT NEURAL NETWORKS
179
the input sequence and select the most likely tag from the softmax at each step. Since
we’re using a softmax layer to generate the probability distribution over the output
tagset at each time step, we will again employ the cross-entropy loss during training.
A closely related, and extremely useful, application of sequence labeling is to
ﬁnd and classify spans of text that correspond to concepts of interest in some task
domain. One example of such a task is named entity recognition — the prob-
named entity
recognition
lem of ﬁnding all the spans in a text that correspond to names of people, places or
organizations (a problem we’ll study in more detail in Chapter 18).
To use sequence labeling for a span-recognition problem, we’ll use a technique
called IOB encoding (Ramshaw and Marcus, 1995). In its simplest form, we label
any token that begins a span of interest with the label B, tokens that occur inside a
span are tagged with an I, and any tokens outside of any span of interest are labeled
O. Consider the following example:
(9.13) United
B
cancelled
O
the
O
ﬂight
O
from
O
Denver
B
to
O
San
B
Francisco.
I
Here, the spans of interest are United, Denver and San Francisco.
In applications where we are interested in more than one class of entity (e.g.,
ﬁnding and distinguishing names of people, locations, or organizations), we can
specialize the B and I tags to represent each of the more speciﬁc classes, thus ex-
panding the tagset from 3 tags to 2∗N +1, where N is the number of classes we’re
interested in. Applying this approach to the previous example results in the follow-
ing encoding.
(9.14) United
B-ORG
cancelled
O
the
O
ﬂight
O
from
O
Denver
B-LOC
to
O
San
B-LOC
Francisco.
I-LOC
Given such an encoding, we’ve reduced the span recognition task to a per-word
labeling task where the inputs are our usual word embeddings and the output consists
of a sequence of softmax distributions over the tags at each point in the sequence.
Yet another application of sequence labeling is to the problem of structure pre-
diction. Here the task is to take an input sequence and produce some kind of struc-
structure
prediction
tured output, such as a parse tree or meaning representation. One way to model
problems like this is to learn a sequence of actions, or operators, which when exe-
cuted would produce the desired structure. Therefore, instead of predicting a label
for each element of an input sequence, the network is trained to select a sequence of
actions, which when executed in sequence produce the desired output. The clearest
example of this approach is transition-based parsing which borrows the shift-reduce
paradigm from compiler construction. We’ll return to this application in Chapter 15
when we take up dependency parsing.
Viterbi and Conditional Random Fields (CRFs)
As we saw when we applied logistic regression to part-of-speech tagging, choosing
the maximum probability label for each element in a sequence independently does
not necessarily result in an optimal (or even very good) sequence of tags. In the case
of IOB tagging, it doesn’t even guarantee that the resulting sequence will be well-
formed. For example, nothing in approach described in the last section prevents an
output sequence from containing an I following an O, even though such a transition
is illegal. Similarly, when dealing with multiple classes nothing would prevent an
I-LOC tag from following a B-PER tag.

180
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
One solution to this problem is to combine the sequence of outputs from a re-
current network with an output-level language model as discussed in Chapter 8. We
can then use a variant of the Viterbi algorithm to select the most likely tag sequence.
This approach is usually implemented by adding a CRF (Lample et al., 2016a) layer
as the ﬁnal layer of recurrent network.
9.2.3
RNNs for Sequence Classiﬁcation
Another use of RNNs is to classify entire sequences rather than the tokens within
them. We’ve already encountered this task in Chapter 4 with our discussion of sen-
timent analysis. Other examples include document-level topic classiﬁcation, spam
detection, message routing for customer service applications, and deception detec-
tion. In all of these applications, sequences of text are classiﬁed as belonging to one
of a small number of categories.
To apply RNNs in this setting, the text to be classiﬁed is passed through the RNN
a word at a time generating a new hidden layer at each time step. The hidden layer
for the ﬁnal element of the text, hn, is taken to constitute a compressed representation
of the entire sequence. In the simplest approach to classiﬁcation, hn, serves as the
input to a subsequent feedforward network that chooses a class via a softmax over
the possible classes. Fig. 9.9 illustrates this approach.
x1
x2
x3
xn
RNN
hn
Softmax
Figure 9.9
Sequence classiﬁcation using a simple RNN combined with a feedforward net-
work. The ﬁnal hidden state from the RNN is used as the input to a feedforward network that
performs the classiﬁcation.
Note that in this approach there are no intermediate outputs for the words in
the sequence preceding the last element. Therefore, there are no loss terms associ-
ated with those elements. Instead, the loss function used to train the weights in the
network is based entirely on the ﬁnal text classiﬁcation task. Speciﬁcally, the out-
put from the softmax output from the feedforward classiﬁer together with a cross-
entropy loss drives the training. The error signal from the classiﬁcation is backprop-
agated all the way through the weights in the feedforward classiﬁer through, to its
input, and then through to the three sets of weights in the RNN as described earlier
in Section 9.1.2. This combination of a simple recurrent network with a feedforward

9.3
•
DEEP NETWORKS: STACKED AND BIDIRECTIONAL RNNS
181
classiﬁer is our ﬁrst example of a deep neural network. And the training regimen
that uses the loss from a downstream application to adjust the weights all the way
through the network is referred to as end-to-end training.
end-to-end
training
9.3
Deep Networks: Stacked and Bidirectional RNNs
As suggested by the sequence classiﬁcation architecture shown in Fig. 9.9, recurrent
networks are quite ﬂexible. By combining the feedforward nature of unrolled com-
putational graphs with vectors as common inputs and outputs, complex networks
can be treated as modules that can be combined in creative ways. This section intro-
duces two of the more common network architectures used in language processing
with RNNs.
9.3.1
Stacked RNNs
In our examples thus far, the inputs to our RNNs have consisted of sequences of
word or character embeddings (vectors) and the outputs have been vectors useful for
predicting words, tags or sequence labels. However, nothing prevents us from using
the entire sequence of outputs from one RNN as an input sequence to another one.
Stacked RNNs consist of multiple networks where the output of one layer serves as
Stacked RNNs
the input to a subsequent layer, as shown in Fig. 9.10.
y1
y2
y3
yn
x1
x2
x3
xn
RNN 1
RNN 3
RNN 2
Figure 9.10
Stacked recurrent networks. The output of a lower level serves as the input to
higher levels with the output of the last network serving as the ﬁnal output.
It has been demonstrated across numerous tasks that stacked RNNs can outper-
form single-layer networks. One reason for this success has to do with the network’s
ability to induce representations at differing levels of abstraction across layers. Just
as the early stages of the human visual system detect edges that are then used for
ﬁnding larger regions and shapes, the initial layers of stacked networks can induce
representations that serve as useful abstractions for further layers — representations
that might prove difﬁcult to induce in a single RNN.

182
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
The optimal number of stacked RNNs is speciﬁc to each application and to each
training set. However, as the number of stacks is increased the training costs rise
quickly.
9.3.2
Bidirectional RNNs
In a simple recurrent network, the hidden state at a given time t represents everything
the network knows about the sequence up to that point in the sequence. That is, the
hidden state at time t is the result of a function of the inputs from the start up through
time t. We can think of this as the context of the network to the left of the current
time.
hf
t
= RNNforward(xt
1)
Where h f
t corresponds to the normal hidden state at time t, and represents everything
the network has gleaned from the sequence to that point.
In many applications we have access to the entire input sequence all at once. We
might ask whether it is helpful to take advantage of the context to the right of the
current input as well. One way to recover such information is to train an RNN on an
input sequence in reverse, using exactly the same kind of networks that we’ve been
discussing. With this approach, the hidden state at time t now represents information
about the sequence to the right of the current input.
hb
t
= RNNbackward(xn
t )
Here, the hidden state hb
t represents all the information we have discerned about the
sequence from t to the end of the sequence.
Combining the forward and backward networks results in a bidirectional RNN(Schuster
bidirectional
RNN
and Paliwal, 1997). A Bi-RNN consists of two independent RNNs, one where the
input is processed from the start to the end, and the other from the end to the start.
We then combine the outputs of the two networks into a single representation that
captures both the left and right contexts of an input at each point in time.
ht = hf
t ⊕hb
t
Fig. 9.11 illustrates a bidirectional network where the outputs of the forward and
backward pass are concatenated. Other simple ways to combine the forward and
backward contexts include element-wise addition or multiplication. The output at
each step in time thus captures information to the left and to the right of the current
input. In sequence labeling applications, these concatenated outputs can serve as the
basis for a local labeling decision.
Bidirectional RNNs have also proven to be quite effective for sequence classi-
ﬁcation. Recall from Fig. 9.10, that for sequence classiﬁcation we used the ﬁnal
hidden state of the RNN as the input to a subsequent feedforward classiﬁer. A dif-
ﬁculty with this approach is that the ﬁnal state naturally reﬂects more information
about the end of the sentence than its beginning. Bidirectional RNNs provide a
simple solution to this problem; as shown in Fig. 9.12, we simply combine the ﬁnal
hidden states from the forward and backward passes and use that as input for follow-
on processing. Again, concatenation is a common approach to combining the two
outputs but element-wise summation, multiplication or averaging are also used.

9.4
•
MANAGING CONTEXT IN RNNS: LSTMS AND GRUS
183
y1
x1
x2
x3
xn
RNN 1 (Left to Right)
RNN 2 (Right to Left)
+
y2
+
y3
+
yn
+
Figure 9.11
A bidirectional RNN. Separate models are trained in the forward and backward
directions with the output of each model at each time point concatenated to represent the state
of affairs at that point in time. The box wrapped around the forward and backward network
emphasizes the modular nature of this architecture.
x1
x2
x3
xn
RNN 1 (Left to Right)
RNN 2 (Right to Left)
+
hn_forw
h1_back
Softmax
Figure 9.12
A bidirectional RNN for sequence classiﬁcation. The ﬁnal hidden units from
the forward and backward passes are combined to represent the entire sequence. This com-
bined representation serves as input to the subsequent classiﬁer.
9.4
Managing Context in RNNs: LSTMs and GRUs
In practice, it is quite difﬁcult to train RNNs for tasks that require a network to make
use of information distant from the current point of processing. Despite having
access to the entire preceding sequence, the information encoded in hidden states
tends to be fairly local, more relevant to the most recent parts of the input sequence
and recent decisions. It is often the case, however, that distant information is critical

184
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
to many language applications. To see this, consider the following example in the
context of language modeling.
(9.15) The ﬂights the airline was cancelling were full.
Assigning a high probability to was following airline is straightforward since airline
provides a strong local context for the singular agreement. However, assigning an
appropriate probability to were is quite difﬁcult, not only because the plural ﬂights is
quite distant, but also because the intervening context involves singular constituents.
Ideally, a network should be able to retain the distant information about plural ﬂights
until it is needed, while still processing the intermediate parts of the sequence cor-
rectly.
One reason for the inability of RNNs to carry forward critical information is that
the hidden layers, and, by extension, the weights that determine the values in the hid-
den layer, are being asked to perform two tasks simultaneously: provide information
useful for the current decision, and updating and carrying forward information re-
quired for future decisions.
A second difﬁculty with training SRNs arises from the need to backpropagate
the error signal back through time. Recall from Section 9.1.2 that the hidden layer
at time t contributes to the loss at the next time step since it takes part in that cal-
culation. As a result, during the backward pass of training, the hidden layers are
subject to repeated multiplications, as determined by the length of the sequence. A
frequent result of this process is that the gradients are eventually driven to zero – the
so-called vanishing gradients problem.
vanishing
gradients
To address these issues, more complex network architectures have been designed
to explicitly manage the task of maintaining relevant context over time. More specif-
ically, the network needs to learn to forget information that is no longer needed and
to remember information required for decisions still to come.
9.4.1
Long Short-Term Memory
Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997)
Long
short-term
memory
divide the context management problem into two sub-problems: removing informa-
tion no longer needed from the context, and adding information likely to be needed
for later decision making. The key to solving both problems is to learn how to man-
age this context rather than hard-coding a strategy into the architecture. LSTMs
accomplish this by ﬁrst adding an explicit context layer to the architecture (in addi-
tion to the usual recurrent hidden layer), and through the use of specialized neural
units that make use of gates to control the ﬂow of information into and out of the
units that comprise the network layers. These gates are implemented through the
use of additional weights that operate sequentially on the input, and previous hidden
layer, and previous context layers.
The gates in an LSTM share a common design pattern; each consists of a feed-
forward layer, followed by a sigmoid activation function, followed by a pointwise
multiplication with the layer being gated. The choice of the sigmoid as the activation
function arises from its tendency to push its outputs to either 0 or 1. Combining this
with a pointwise multiplication has an effect similar to that of a binary mask. Values
in the layer being gated that align with values near 1 in the mask are passed through
nearly unchanged; values corresponding to lower values are essentially erased.
The ﬁrst gate we’ll consider is the forget gate. The purpose of this gate to delete
forget gate
information from the context that is no longer needed. The forget gate computes a
weighted sum of the previous state’s hidden layer and the current input and passes

9.4
•
MANAGING CONTEXT IN RNNS: LSTMS AND GRUS
185
that through a sigmoid. This mask is then multiplied by the context vector to remove
the information from context that is no longer required.
ft = σ(Uf ht−1 +Wf xt)
kt = ct−1 ⊙ft
The next task is compute the actual information we need to extract from the
previous hidden state and current inputs — the same basic computation we’ve been
using for all our recurrent networks.
gt = tanh(Ught−1 +Wgxt)
(9.16)
Next, we generate the mask for the add gate to select the information to add to the
add gate
current context.
it = σ(Uiht−1 +Wixt)
(9.17)
jt = gt ⊙it
(9.18)
Next, we add this to the modiﬁed context vector to get our new context vector.
ct = jt +kt
(9.19)
The ﬁnal gate we’ll use is the output gate which is used to decide what informa-
output gate
tion is required for the current hidden state (as opposed to what information needs
to be preserved for future decisions).
ot = σ(Uoht−1 +Woxt)
(9.20)
ht = ot ⊙tanh(ct)
(9.21)
(9.22)
Fig. 9.13 illustrates the complete computation for a single LSTM unit. Given
the appropriate weights for the various gates, an LSTM accepts as input the context
layer, and hidden layer from the previous time step, along with the current input
vector. It then generates updated context and hidden vectors as output. The hidden
layer, ht, can be used as input to subsequent layers in a stacked RNN, or to generate
an output for the ﬁnal layer of a network.
9.4.2
Gated Recurrent Units
LSTMs introduce a considerable number of additional parameters to our recurrent
networks. We now have 8 sets of weights to learn (i.e., the U and W for each of the 4
gates within each unit), whereas with simple recurrent units we only had 2. Training
these additional parameters imposes a much signiﬁcantly higher training cost. Gated
Recurrent Units (GRUs)(Cho et al., 2014) ease this burden by dispensing with the
use of a separate context vector, and by reducing the number of gates to 2 — a reset
gate, r and an update gate, z.
rt = σ(Urht−1 +Wrxt)
(9.23)
zt = σ(Uzht−1 +Wzxt)
(9.24)

186
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
+
o
st-1
i
f
g
x t
ht-1
Y
Y
Y
st
ht
Figure 9.13
A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the
current input, x, the previous hidden state, ht−1, and the previous context, ct−1. The outputs are a new hidden
state, ht and an updated context, ct.
As with LSTMs, the use of the sigmoid in the design of these gates results in
a binary-like mask that either blocks information with values near zero or allows
information to pass through unchanged with values near one. The purpose of the
reset gate is to decide which aspects of the previous hidden state are relevant to the
current context and what can be ignored. This is accomplished by performing an
element-wise multiplication of r with the value of the previous hidden state. We
then use this masked value in computing an intermediate representation for the new
hidden state at time t.
˜ht = tanh(U(rt ⊙ht−1)+Wxt)
(9.25)
The job of the update gate z is to determine which aspects of this new state will
be used directly in the new hidden state and which aspects of the previous state need
to be preserved for future use. This is accomplished by using the values in z to
interpolate between the old hidden state and the new one.
ht = (1−zt)ht−1 +zt ˜ht
(9.26)
9.4.3
Gated Units, Layers and Networks
The neural units used in LSTMs and GRUs are obviously much more complex than
those used in basic feedforward networks. Fortunately, this complexity is encapsu-
lated within the basic processing units, allowing us to maintain modularity and to

9.5
•
WORDS, SUBWORDS AND CHARACTERS
187
h
x
xt
xt
ht-1
ht
ht
ct-1
ct
ht-1
xt
ht
ht-1
(b)
(a)
(c)
(d)
⌃
g
z
a
⌃
g
z
LSTM
Unit
GRU
a
Figure 9.14
Basic neural units used in feedforward, simple recurrent networks (SRN), long
short-term memory (LSTM) and gate recurrent units.
easily experiment with different architectures. To see this, consider Fig. 9.14 which
illustrates the inputs and outputs associated with each kind of unit.
At the far left, (a) is the basic feedforward unit where a single set of weights and
a single activation function determine its output, and when arranged in a layer there
are no connections among the units in the layer. Next, (b) represents the unit in a
simple recurrent network. Now there are two inputs and an additional set of weights
to go with it. However, there is still a single activation function and output.
The increased complexity of the LSTM (c) and GRU (d) units on the right is
encapsulated within the units themselves. The only additional external complexity
for the LSTM over the basic recurrent unit (b) is the presence of the additional
context vector as an input and output. The GRU units have the same input and
output architecture as the simple recurrent unit.
This modularity is key to the power and widespread applicability of LSTM and
GRU units. LSTM and GRU units can be substituted into any of the network ar-
chitectures described in Section 9.3. And, as with simple RNNs, multi-layered net-
works making use of gated units can be unrolled into deep feedforward networks
and trained in the usual fashion with backpropagation.
9.5
Words, Subwords and Characters
To this point, we’ve been assuming that the inputs to our networks would be word
embeddings. As we’ve seen, word-based embeddings are great at capturing dis-
tributional (syntactic and semantic) similarity between words. However, there are
drawbacks to an exclusively word-based approach:
• For some languages and applications, the lexicon is simply too large to prac-
tically represent every possible word as an embedding. Some means of com-
posing words from smaller bits is needed.
• No matter how large the lexicon, we will always encounter unknown words
due to new words entering the language, misspellings and borrowings from
other languages.

188
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
Janet
RNN
Bi-RNN
J
a
n
t
e
+
will
Bi-RNN
w
i
l
l
+
…
…
Figure 9.15
Sequence labeling RNN that accepts distributional word embeddings aug-
mented with character-level word embeddings.
• Morphological information, below the word level, is a critical source of infor-
mation for many languages and many applications. Word-based methods are
blind to such regularities.
A wide variety of alternatives to the word-based approach have been explored
over the past few years. The following are among the primary approaches that have
been tried.
• Ignore words altogether and simply use character sequences as the input to
RNNs.
• Use subword units such as those derived from byte-pair encoding or phonetic
analysis as inputs.
• Use full-blown morphological analysis to derive a linguistically motivated in-
put sequence.
Perhaps not surprisingly there is no clear one-best approach for all applications for
all languages.
One particularly successful approach combines word embeddings with embed-
dings derived from the characters that make up the words. Fig. 9.15 illustrates an
approach in the context of part-of-speech tagging. The upper part of the diagram
consists of an RNN that accepts an input sequence and outputs a softmax distribu-
tion over the tags for each element of the input. Note that this RNN can be arbitrarily
complex, consisting of stacked and/or bidirectional network layers.
The inputs to this network consist of ordinary word embeddings enriched with
character-level information. Speciﬁcally, each input consists of the concatenation
of the normal word embedding with embeddings derived from a bidirectional RNN
that accepts the character sequences for each word as input, as shown in the lower
part of the ﬁgure.
The character sequence for each word in the input is run through a bidirectional
RNN consisting of two independent RNNs — one that processes the sequence left-
to-right and the other right-to-left. As discussed in Section 9.3.2, the ﬁnal hidden
states of the left-to-right and right-to-left networks are concatenated to represent the

9.6
•
SUMMARY
189
composite character-level representation of each word. Critically, these character
embeddings are trained in the context of the overall task; the loss from the part-of-
speech softmax layer is propagated all the way back to the character embeddings.
J
a
n
e
Character Projection Layer
LSTM1
LSTM1
LSTM1
LSTM1
LSTM2
LSTM2
LSTM2
LSTM2
Right-to-left LSTM
Left-to-right LSTM
t
LSTM2
LSTM1
Concatenation
Character-Level Word Embedding
Character Embeddings
Figure 9.16
Bi-RNN accepts word character sequences and emits embeddings derived
from a forward and backward pass over the sequence. The network itself is trained in the
context of a larger end-application where the loss is propagated all the way through to the
character vector embeddings.
9.6
Summary
This chapter has introduced the concept of recurrent neural networks and how they
can be applied to language problems. Here’s a summary of the main points that we
covered:
• In simple Recurrent Neural Networks sequences are processed naturally as an
element at a time.
• The output of a neural unit at a particular point in time is based both on the
current input and value of the hidden layer from the previous time step.
• RNNs can be trained with a straightforward extension of the backpropagation
algorithm, known as backpropagation through time (BPTT).
• Common language-based applications for RNNs include:
– Probabilistic language modeling, where the model assigns a probability
to a sequence, or to the next element of a sequence given the preceding
words.
– Auto-regressive generation using a trained language model.
– Sequence labeling, where each element of a sequence is assigned a label,
as with part-of-speech tagging.
– Sequence classiﬁcation, where an entire text is assigned to a category, as
in spam detection, sentiment analysis or topic classiﬁcation.
• Simple recurrent networks often fail since it is extremely difﬁcult to success-
fully train them do to problems maintaining useful gradients over time.

190
CHAPTER 9
•
SEQUENCE PROCESSING WITH RECURRENT NETWORKS
• More complex gated architectures such as LSTMs and GRUs are designed
to overcome these issues by explicitly managing the task of deciding what to
remember and forget in their hidden and context layers.
Bibliographical and Historical Notes
Inﬂuential investigations of the kind of simple RNNs discussed here were conducted
in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in
the 1980’s. Much of this work was directed at human cognitive modeling rather than
practical NLP applications Rumelhart et al. 1986 McClelland et al. 1986. Models
using recurrence at the hidden layer in a feedforward network (Elman networks)
were introduced by Elman (1990). Similar architectures were investigated by Jordan
(1986) with a recurrence from the output layer, and Mathis and Mozer (1995) with
the addition of a recurrent context layer prior to the hidden layer. The possibility of
unrolling a recurrent network into an equivalent feedforward network is discussed
in (Rumelhart et al., 1986).
In parallel with work in cognitive modeling, RNNs were investigated extensively
in the continuous domain in the signal processing and speech communities (Giles
et al., 1994). Schuster and Paliwal (1997) introduced bidirectional RNNs and de-
scribed results on the TIMIT phoneme transcription task.
While theoretically interesting, the difﬁculty with training RNNs and manag-
ing context over long sequences impeded progress on practical applications. This
situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber
(1997). Impressive performance gains were demonstrated on tasks at the bound-
ary of signal processing and language processing including phoneme recognition
(Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and
most signiﬁcantly speech recognition (Graves et al., 2013).
Interest in applying neural networks to practical NLP problems surged with the
work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made
use of learned word embeddings, convolutional networks, and end-to-end training.
They demonstrated near state-of-the-art performance on a number of standard shared
tasks including part-of-speech tagging, chunking, named entity recognition and se-
mantic role labeling without the use of hand-engineered features.
Approaches that married LSTMs with pre-trained collections of word-embeddings
based on word2vec (Mikolov et al., 2013) and GLOVE (Pennington et al., 2014),
quickly came to dominate many common tasks: part-of-speech tagging (Ling et al.,
2015a), syntactic chunking (Søgaard and Goldberg, 2016), and named entity recog-
nition via IOB tagging Chiu and Nichols 2016, Ma and Hovy 2016, opinion mining
(Irsoy and Cardie, 2014), semantic role labeling (Zhou and Xu, 2015a) and AMR
parsing (Foland and Martin, 2016). As with the earlier surge of progress involving
statistical machine learning, these advances were made possible by the availability
of training data provided by CONLL, SemEval, and other shared tasks, as well as
shared resources such as Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer
et al., 2005).

CHAPTER
10
Encoder-Decoder Models, At-
tention, and Contextual Em-
beddings
It is all well and good to copy what one sees, but it is much better to draw only what
remains in one’s memory. This is a transformation in which imagination and memory
collaborate.
Edgar Degas
In Chapter 9 we explored recurrent neural networks along with some of their com-
mon use cases, including language modeling, contextual generation, and sequence
labeling. A common thread in these applications was the notion of transduction
— input sequences being transformed into output sequences in a one-to-one fash-
ion. Here, we’ll explore an approach that extends these models and provides much
greater ﬂexibility across a range of applications. Speciﬁcally, we’ll introduce encoder-
decoder networks, or sequence-to-sequence models, that are capable of generating
contextually appropriate, arbitrary length, output sequences. Encoder-decoder net-
works have been applied to a very wide range of applications including machine
translation, summarization, question answering, and dialogue modeling.
The key idea underlying these networks is the use of an encoder network that
takes an input sequence and creates a contextualized representation of it. This rep-
resentation is then passed to a decoder which generates a task-speciﬁc output se-
quence. The encoder and decoder networks are typically implemented with the same
architecture, often using recurrent networks of the kind we studied in Chapter 9. And
as with the deep networks introduced there, the encoder-decoder architecture allows
networks to be trained in an end-to-end fashion for each application.
10.1
Neural Language Models and Generation Revisited
To understand the design of encoder-decoder networks let’s return to neural language
models and the notion of autoregressive generation. Recall that in a simple recurrent
network, the value f the hidden state at a particular point in time is a function of the
previous hidden state and the current input; the network output is then a function of
this new hidden state.
ht = g(Uht−1 +Wxt)
yt = f(Vht)
Here, U, V, and W are weight matrices which are adjusted during training, g is a
suitable non-linear activation function such as tanh or ReLU, and in the common
case of classiﬁcation f is a softmax over the set of possible outputs. In practice,
gated networks using LSTMs or GRUs are used in place of these simple RNNs. To

192
CHAPTER 10
•
ENCODER-DECODER MODELS, ATTENTION AND CONTEXTUAL EMBEDDINGS
lived
a
there
hobbit
lived
a
hobbit
</s>
Sampled Words
Softmax
Embeddings
Preﬁx
ground
the
in
hole
Autogenerated completion
there
RNN
Figure 10.1
Using an RNN to generate the completion of an input phrase.
reﬂect this, we’ll abstract away from the details of the speciﬁc RNN being used and
simply specify the inputs on which the computation is being based. So the earlier
equations will be expressed as follows with the understanding that there is a suitable
RNN underneath.
ht = g(ht−1,xt)
yt = f(ht)
To create an RNN-based language model, we train the network to predict the
next word in a sequence using a corpus of representative text. Language models
trained in this fashion are referred to as autoregressive models. Given a trained
model, we can ask the network to generate novel sequences by ﬁrst randomly sam-
pling an appropriate word as the beginning of a sequence. We then condition the
generation of subsequent words on the hidden state from the previous time step as
well as the embedding for the word just generated, again sampling from the distri-
bution provided by the softmax. More speciﬁcally, during generation the softmax
output at each point in time provides us with the probability of every word in the
vocabulary given the preceding context, that is P(yi|y<i)∀i ∈V; we then sample a
particular word, ˆyi, from this distribution and condition subsequent generation on it.
The process continues until the end of sentence token <\s> is generated.
Now, let’s consider a simple variation on this scheme. Instead of having the
language model generate a sentence from scratch, we have it complete a sequence
given a speciﬁed preﬁx. More speciﬁcally, we ﬁrst pass the speciﬁed preﬁx through
the language model using forward inference to produce a sequence of hidden states,
ending with the hidden state corresponding to the last word of the preﬁx. We then
begin generating as we did earlier, but using the ﬁnal hidden state of the preﬁx as
our starting point. The result of this process is a novel output sequence that should
be a reasonable completion of the preﬁx input.
Fig. 10.1 illustrates this basic scheme. The portion of the network on the left
processes the provided preﬁx, while the right side executes the subsequent auto-
regressive generation. Note that the goal of the lefthand portion of the network
is to generate a series of hidden states from the given input; there are no outputs

10.1
•
NEURAL LANGUAGE MODELS AND GENERATION REVISITED
193
associated with this part of the process until we reach the end of the preﬁx.
Now, consider an ingenious extension of this idea from the world of machine
translation (MT), the task of automatically translating sentences from one language
into another. The primary resources used to train modern translation systems are
known as parallel texts, or bitexts. These are large text collections consisting of pairs
bitexts
of sentences from different languages that are translations of one another. Tradition-
ally in MT, the text being translated is referred to as the source and the translation
output is called the target.
To extend language models and autoregressive generation to machine transla-
tion, we’ll ﬁrst add an end-of-sentence marker at the end of each bitext’s source
sentence and then simply concatenate the corresponding target to it. These concate-
nated source-target pairs can now serve as training data for a combined language
model. Training proceeds as with any RNN-based language model. The network is
trained autoregressively to predict the next word in a set of sequences comprised of
the concatenated source-target bitexts, as shown in Fig. 10.2.
To translate a source text using the trained model, we run it through the network
performing forward inference to generate hidden states until we get to the end of the
source. Then we begin autoregressive generation, asking for a word in the context
of the hidden layer from the end of the source input as well as the end-of-sentence
marker. Subsequent words are conditioned on the previous hidden state and the
embedding for the last word generated.
vivait
un
</s>
hobbit
vivait
un
hobbit
</s>
Source
hobbit
a 
lived
there
Target
</s>
lived
hobbit
a
Figure 10.2
Training setup for a neural language model approach to machine translation. Source-target bi-
texts are concatenated and used to train a language model.
Early efforts using this clever approach demonstrated surprisingly good results
on standard datasets and led to a series of innovations that were the basis for net-
works discussed in the remainder of this chapter. Chapter 11 provides an in-depth
discussion of the fundamental issues in translation as well as the current state-of-
the-art approaches to MT. Here, we’ll focus on the powerful models that arose from
these early efforts.

194
CHAPTER 10
•
ENCODER-DECODER MODELS, ATTENTION AND CONTEXTUAL EMBEDDINGS
10.2
Encoder-Decoder Networks
Fig. 10.3 abstracts away from the speciﬁcs of machine translation and illustrates a
basic encoder-decoder architecture. The elements of the network on the left pro-
encoder-
decoder
cess the input sequence and comprise the encoder, the entire purpose of which is to
generate a contextualized representation of the input. In this network, this represen-
tation is embodied in the ﬁnal hidden state of the encoder, hn, which in turn feeds
into the ﬁrst hidden state of the decoder. The decoder network on the right takes
this state and autoregressively generates a sequence of outputs.
y1
y2
y3
ym
Encoder
xn
x2
x1
Decoder
hn
…
…
…
…
Figure 10.3
Basic RNN-based encoder-decoder architecture. The ﬁnal hidden state of the encoder RNN
serves as the context for the decoder in its role as h0 in the decoder RNN.
This basic architecture is consistent with the original applications of neural mod-
els to machine translation. However, it embodies a number of design choices that
are less than optimal. Among the major ones are that the encoder and the decoder
are assumed to have the same internal structure (RNNs in this case), that the ﬁnal
state of the encoder is the only context available to the decoder, and ﬁnally that
this context is only available to the decoder as its initial hidden state. Abstracting
away from these choices, we can say that encoder-decoder networks consist of three
components:
1. An encoder that accepts an input sequence, xn
1, and generates a corresponding
sequence of contextualized representations, hn
1.
2. A context vector, c, which is a function of hn
1, and conveys the essence of the
input to the decoder.
3. And a decoder, which accepts c as input and generates an arbitrary length
sequence of hidden states hm
1 , from which a corresponding sequence of output
states ym
1 , can be obtained.
Fig. 10.4 illustrates this abstracted architecture. Let’s now explore some of the pos-
sibilities for each of the components.

10.2
•
ENCODER-DECODER NETWORKS
195
y1
y2
ym
xn
x2
x1
…
Encoder
Decoder
Context
…
Figure 10.4
Basic architecture for an abstract encoder-decoder network. The context is a
function of the vector of contextualized input representations and may be used by the decoder
in a variety of ways.
Encoder
Simple RNNs, LSTMs, GRUs, convolutional networks, as well as transformer net-
works (discussed later in this chapter), can all be been employed as encoders. For
simplicity, our ﬁgures show only a single network layer for the encoder, however,
stacked architectures are the norm, where the output states from the top layer of the
stack are taken as the ﬁnal representation. A widely used encoder design makes use
of stacked Bi-LSTMs where the hidden states from top layers from the forward and
backward passes are concatenated as described in Chapter 9 to provide the contex-
tualized representations for each time step.
Decoder
For the decoder, autoregressive generation is used to produce an output sequence,
an element at a time, until an end-of-sequence marker is generated. This incremen-
tal process is guided by the context provided by the encoder as well as any items
generated for earlier states by the decoder. Again, a typical approach is to use an
LSTM or GRU-based RNN where the context consists of the ﬁnal hidden state of
the encoder, and is used to initialize the ﬁrst hidden state of the decoder. (To help
keep things straight, we’ll use the superscripts e and d where needed to distinguish
the hidden states of the encoder and the decoder.) Generation proceeds as described
earlier where each hidden state is conditioned on the previous hidden state and out-
put generated in the previous state.
c = he
n
hd
0 = c
hd
t
= g(ˆyt−1,hd
t−1)
zt = f(hd
t )
yt = softmax(zt)
Recall, that g is a stand-in for some ﬂavor of RNN and ˆyt−1 is the embedding for the
output sampled from the softmax at the previous step.
A weakness of this approach is that the context vector, c, is only directly avail-
able at the beginning of the process and its inﬂuence will wane as the output se-
quence is generated. A solution is to make the context vector c available at each step

196
CHAPTER 10
•
ENCODER-DECODER MODELS, ATTENTION AND CONTEXTUAL EMBEDDINGS
in the decoding process by adding it as a parameter to the computation of the current
hidden state.
hd
t = g(ˆyt−1,hd
t−1,c)
A common approach to the calculation of the output layer y is to base it solely
on this newly computed hidden state. While this cleanly separates the underlying
recurrence from the output generation task, it makes it difﬁcult to keep track of what
has already been generated and what hasn’t. A alternative approach is to condition
the output on both the newly generated hidden state, the output generated at the
previous state, and the encoder context.
yt = softmax(ˆyt−1,zt,c)
Finally, as shown earlier, the output y at each time consists of a softmax computa-
tion over the set of possible outputs (the vocabulary in the case of language models).
What one does with this distribution is task-dependent, but it is critical since the re-
currence depends on choosing a particular output, ˆy, from the softmax to condition
the next step in decoding. We’ve already seen several of the possible options for this.
For neural generation, where we are trying to generate novel outputs, we can sim-
ply sample from the softmax distribution. However, for applications like MT where
we’re looking for a speciﬁc output sequence, random sampling isn’t appropriate and
would likely lead to some strange output. An alternative is to choose the most likely
output at each time step by taking the argmax over the softmax output:
ˆy = argmaxP(yi|y<i)
This is easy to implement but as we’ve seen several times with sequence labeling,
independently choosing the argmax over a sequence is not a reliable way of arriving
at a good output since it doesn’t guarantee that the individual choices being made
make sense together and combine into a coherent whole. With sequence labeling we
addressed this with a CRF-layer over the output token types combined with a Viterbi-
style dynamic programming search. Unfortunately, this approach is not viable here
since the dynamic programming invariant doesn’t hold.
Beam Search
A viable alternative is to view the decoding problem as a heuristic state-space search
and systematically explore the space of possible outputs. The key to such an ap-
proach is controlling the exponential growth of the search space. To accomplish
this, we’ll use a technique called beam search. Beam search operates by combin-
Beam Search
ing a breadth-ﬁrst-search strategy with a heuristic ﬁlter that scores each option and
prunes the search space to stay within a ﬁxed-size memory footprint, called the beam
width.
At the ﬁrst step of decoding, we select the B-best options from the softmax output
y, where B is the size of the beam. Each option is scored with its corresponding
probability from the softmax output of the decoder. These initial outputs constitute
the search frontier. We’ll refer to the sequence of partial outputs generated along
these search paths as hypotheses.
At subsequent steps, each hypothesis on the frontier is extended incrementally
by being passed to distinct decoders, which again generate a softmax over the entire
vocabulary. To provide the necessary inputs for the decoders, each hypothesis must
include not only the words generated thus far but also the context vector, and the

10.2
•
ENCODER-DECODER NETWORKS
197
hidden state from the previous step. New hypotheses representing every possible
extension to the current ones are generated and added to the frontier. Each of these
new hypotheses is scored using P(yi|y<i), which is the product of the probability
of current word choice multiplied by the probability of the path that led to it. To
control the exponential growth of the frontier, it is pruned to contain only the top B
hypotheses.
This process continues until a <\s> is generated indicating that a complete can-
didate output has been found. At this point, the completed hypothesis is removed
from the frontier and the size of the beam is reduced by one. The search continues
until the beam has been reduced to 0. Leaving us with B hypotheses to consider.
Fig. 10.5 illustrates this process with a beam width of 3.
y_{i-1}
c
h_{i-1}
EOS
EOS
EOS
EOS
y_i
Decoder
Beam search with beam width = 4.
0
1
2
3
4
5
6
7
Figure 10.5
Beam decoding with a beam width of 4. At the initial step, the frontier is ﬁlled with the best 4
options from the initial state of the decoder. In a breadth-ﬁrst fashion, each state on the frontier is passed to a
decoder which computes a softmax over the entire vocabulary and attempts to enter each as a new state into the
frontier subject to the constraint that they are better than the worst state already there. As completed sequences
are discovered they are recorded and removed from the frontier and the beam width is reduced by 1.
One ﬁnal complication arises from the fact that the completed hypotheses may
have different lengths. Unfortunately, due to the probabilistic nature of our scoring
scheme, longer hypotheses will naturally look worse than shorter ones just based on
their length. This was not an issue during the earlier steps of decoding; due to the
breadth-ﬁrst nature of beam search all the hypotheses being compared had the same
length. The usual solution to this is to apply some form of length normalization to
each of the hypotheses. With normalization, we have B hypotheses and can select
the best one, or we can pass all or a subset of them on to a downstream application
with their respective scores.
Context
We’ve deﬁned the context vector c as a function of the hidden states of the encoder,
that is, c = f(hn
1). Unfortunately, the number of hidden states varies with the size of
the input, making it difﬁcult to just use them directly as a context for the decode. The
basic approach described earlier avoids this issue since c is just the ﬁnal hidden state

198
CHAPTER 10
•
ENCODER-DECODER MODELS, ATTENTION AND CONTEXTUAL EMBEDDINGS
function BEAMDECODE(c, beam width) returns best paths
y0, h0 ←0
path←()
complete paths←()
state←(c, y0, h0, path)
;initial state
frontier←⟨state⟩
;initial frontier
while frontier contains incomplete paths and beamwidth > 0
extended frontier←⟨⟩
for each state ∈frontier do
y←DECODE(state)
for each word i ∈Vocabulary do
successor←NEWSTATE(state, i, yi)
new agenda←ADDTOBEAM(successor, extended frontier, beam width)
for each state in extended frontier do
if state is complete do
complete paths←APPEND(complete paths, state)
extended frontier←REMOVE(extended frontier, state)
beam width←beam width - 1
frontier←extended frontier
return completed paths
function NEWSTATE(state, word, word prob) returns new state
function ADDTOBEAM(state, frontier, width) returns updated frontier
if LENGTH(frontier) < width then
frontier←INSERT(state, frontier)
else if SCORE(state) > SCORE(WORSTOF(frontier))
frontier←REMOVE(WORSTOF(frontier))
frontier←INSERT(state, frontier)
return frontier
Figure 10.6
Beam search decoding.
of the encoder. This approach has the advantage of being simple and of reducing the
context to a ﬁxed length vector. However, this ﬁnal hidden state inevitably is more
focused on the latter parts of input sequence, rather than the input as whole.
One solution to this problem is to use Bi-RNNs, where the context can be a
function of the end state of both the forward and backward passes. As described
in Chapter 9, a straightforward approach is to concatenate the ﬁnal states of the
forward and backward passes. An alternative is to simply sum or average the encoder
hidden states to produce a context vector. Unfortunately, this approach loses useful
information about each of the individual encoder states that might prove useful in
decoding.

10.3
•
ATTENTION
199
10.3
Attention
To overcome the deﬁciencies of these simple approaches to context, we’ll need a
mechanism that can take the entire encoder context into account, that dynamically
updates during the course of decoding, and that can be embodied in a ﬁxed-size
vector. Taken together, we’ll refer such an approach as an attention mechanism.
attention
mechanism
Our ﬁrst step is to replace the static context vector with one that is dynamically
derived from the encoder hidden states at each point during decoding. This context
vector, ci, is generated anew with each decoding step i and takes all of the encoder
hidden states into account in its derivation. We then make this context available
during decoding by conditioning the computation of the current decoder state on it,
along with the prior hidden state and the previous output generated by the decoder.
hd
i
= g(ˆyi−1,hd
i−1,ci)
The ﬁrst step in computing ci is to compute a vector of scores that capture the
relevance of each encoder hidden state to the decoder state captured in hd
i−1. That is,
at each state i during decoding we’ll compute score(hd
i−1,he
j) for each encoder state
j.
For now, let’s assume that this score provides us with a measure of how similar
the decoder hidden state is to each encoder hidden state. To implement this similarity
score, let’s begin with the straightforward approach introduced in Chapter 6 of using
the dot product between vectors.
score(hd
i−1,he
j) = hd
i−1 ·he
j
The result of the dot product is a scalar that reﬂects the degree of similarity between
the two vectors. And the vector of scores over all the encoder hidden states gives us
the relevance of each encoder state to the current step of the decoder.
While the simple dot product can be effective, it is a static measure that does not
facilitate adaptation during the course of training to ﬁt the characteristics of given
applications. A more robust similarity score can be obtained by parameterizing the
score with its own set of weights, Ws.
score(hd
i−1,he
j) = hd
t−1Wshe
j
By introducing Ws to the score, we are giving the network the ability to learn which
aspects of similarity between the decoder and encoder states are important to the
current application.
To make use of these scores, we’ll next normalize them with a softmax to create
a vector of weights, αij, that tells us the proportional relevance of each encoder
hidden state j to the current decoder state, i.
αi j = softmax(score(hd
i−1,he
j) ∀j ∈e)
=
exp(score(hd
i−1,he
j)
P
k exp(score(hd
i−1,he
k))
Finally, given the distribution in α, we can compute a ﬁxed-length context vector for
the current decoder state by taking a weighted average over all the encoder hidden
states.
ci =
X
j
αijhe
j
(10.1)

200
CHAPTER 10
•
ENCODER-DECODER MODELS, ATTENTION AND CONTEXTUAL EMBEDDINGS
With this, we ﬁnally have a ﬁxed-length context vector that takes into account
information from the entire encoder state that is dynamically update to reﬂect the
needs of the decoder at each step of decoding. Fig. 10.7 illustrates an encoder-
decoder network with attention.
yi-1
yi
Encoder
xn
x2
x1
Decoder
hn
h1
hi-1
hi
…
…
ci
…
↵i,j = softmax(hi−1, ¯hj)
…
Figure 10.7
Encoder-decoder network with attention. Computing the value for hi is based on the previous
hidden state, the previous word generated, and the current context vector ci. This context vector is derived from
the attention computation based on comparing the previous hidden state to all of the encoder hidden states.
10.4
Applications of Encoder-Decoder Networks
The addition of attention to basic encoder-decoder networks led to rapid improve-
ment in performance across a wide-range of applications including summarization,
sentence simpliﬁcation, question answering and image captioning.
10.5
Self-Attention and Transformer Networks

10.6
•
SUMMARY
201
10.6
Summary
• Encoder-decoder networks
• Attention
• Transformers
Bibliographical and Historical Notes

CHAPTER
11
Machine Translation
Placeholder
202

CHAPTER
12
Constituency Grammars
The study of grammar has an ancient pedigree; Panini’s grammar of Sanskrit was
written over two thousand years ago and is still referenced today in teaching San-
skrit. Despite this history, knowledge of grammar remains spotty at best. In this
chapter, we make a preliminary stab at addressing some of these gaps in our knowl-
edge of grammar and syntax, as well as introducing some of the formal mechanisms
that are available for capturing this knowledge in a computationally useful manner.
The word syntax comes from the Greek s´yntaxis, meaning “setting out together
syntax
or arrangement”, and refers to the way words are arranged together. We have seen
various syntactic notions in previous chapters. The regular languages introduced in
Chapter 2 offered a simple way to represent the ordering of strings of words, and
Chapter 3 showed how to compute probabilities for these word sequences. Chap-
ter 8 showed that part-of-speech categories could act as a kind of equivalence class
for words. In this chapter and the next few we introduce a variety of syntactic phe-
nomena and models for syntax that go well beyond these simpler approaches.
The bulk of this chapter is devoted to the topic of context-free grammars. Context-
free grammars are the backbone of many formal models of the syntax of natural
language (and, for that matter, of computer languages). As such, they are integral to
many computational applications, including grammar checking, semantic interpreta-
tion, dialogue understanding, and machine translation. They are powerful enough to
express sophisticated relations among the words in a sentence, yet computationally
tractable enough that efﬁcient algorithms exist for parsing sentences with them (as
we show in Chapter 13). In Chapter 14, we show that adding probability to context-
free grammars gives us a powerful model of disambiguation. And in Chapter 17 we
show how they provide a systematic framework for semantic interpretation.
The constituency grammars we introduce here, however, are not the only pos-
sible formal mechanism for modeling syntax. Chapter 15 will introduce syntactic
dependencies, an alternative model that is the core representation for dependency
parsing. Both constituency and dependency formalisms are important for language
processing.
In addition to introducing grammar formalism, this chapter also provides a brief
overview of the grammar of English. To illustrate our grammars, we have chosen
a domain that has relatively simple sentences, the Air Trafﬁc Information System
(ATIS) domain (Hemphill et al., 1990). ATIS systems were an early example of
spoken language systems for helping book airline reservations. Users try to book
ﬂights by conversing with the system, specifying constraints like I’d like to ﬂy from
Atlanta to Denver.

204
CHAPTER 12
•
CONSTITUENCY GRAMMARS
12.1
Constituency
The fundamental notion underlying the idea of constituency is that of abstraction
— groups of words behaving as single units, or constituents. A signiﬁcant part of
developing a grammar involves discovering the inventory of constituents present in
the language.
How do words group together in English? Consider the noun phrase, a sequence
noun phrase
of words surrounding at least one noun. Here are some examples of noun phrases
(thanks to Damon Runyon):
Harry the Horse
a high-class spot such as Mindy’s
the Broadway coppers
the reason he comes into the Hot Box
they
three parties from Brooklyn
What evidence do we have that these words group together (or “form constituents”)?
One piece of evidence is that they can all appear in similar syntactic environments,
for example, before a verb.
three parties from Brooklyn arrive...
a high-class spot such as Mindy’s attracts...
the Broadway coppers love...
they sit
But while the whole noun phrase can occur before a verb, this is not true of each
of the individual words that make up a noun phrase. The following are not grammat-
ical sentences of English (recall that we use an asterisk (*) to mark fragments that
are not grammatical English sentences):
*from arrive... *as attracts...
*the is...
*spot sat...
Thus, to correctly describe facts about the ordering of these words in English, we
must be able to say things like “Noun Phrases can occur before verbs”.
Other kinds of evidence for constituency come from what are called preposed or
preposed
postposed constructions. For example, the prepositional phrase on September sev-
postposed
enteenth can be placed in a number of different locations in the following examples,
including at the beginning (preposed) or at the end (postposed):
On September seventeenth, I’d like to ﬂy from Atlanta to Denver
I’d like to ﬂy on September seventeenth from Atlanta to Denver
I’d like to ﬂy from Atlanta to Denver on September seventeenth
But again, while the entire phrase can be placed differently, the individual words
making up the phrase cannot be
*On September, I’d like to ﬂy seventeenth from Atlanta to Denver
*On I’d like to ﬂy September seventeenth from Atlanta to Denver
*I’d like to ﬂy on September from Atlanta to Denver seventeenth
12.2
Context-Free Grammars
The most widely used formal system for modeling constituent structure in English
and other natural languages is the Context-Free Grammar, or CFG. Context-
CFG

12.2
•
CONTEXT-FREE GRAMMARS
205
free grammars are also called Phrase-Structure Grammars, and the formalism
is equivalent to Backus-Naur Form, or BNF. The idea of basing a grammar on
constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was
not formalized until Chomsky (1956) and, independently, Backus (1959).
A context-free grammar consists of a set of rules or productions, each of which
rules
expresses the ways that symbols of the language can be grouped and ordered to-
gether, and a lexicon of words and symbols. For example, the following productions
lexicon
express that an NP (or noun phrase) can be composed of either a ProperNoun or
NP
a determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or
more Nouns.
NP →Det Nominal
NP →ProperNoun
Nominal →Noun | Nominal Noun
Context-free rules can be hierarchically embedded, so we can combine the pre-
vious rules with others, like the following, that express facts about the lexicon:
Det →a
Det →the
Noun →ﬂight
The symbols that are used in a CFG are divided into two classes. The symbols
that correspond to words in the language (“the”, “nightclub”) are called terminal
terminal
symbols; the lexicon is the set of rules that introduce these terminal symbols. The
symbols that express abstractions over these terminals are called non-terminals. In
non-terminal
each context-free rule, the item to the right of the arrow (→) is an ordered list of one
or more terminals and non-terminals; to the left of the arrow is a single non-terminal
symbol expressing some cluster or generalization. Notice that in the lexicon, the
non-terminal associated with each word is its lexical category, or part-of-speech,
which we deﬁned in Chapter 8.
A CFG can be thought of in two ways: as a device for generating sentences
and as a device for assigning a structure to a given sentence. Viewing a CFG as a
generator, we can read the →arrow as “rewrite the symbol on the left with the string
of symbols on the right”.
So starting from the symbol:
NP
we can use our ﬁrst rule to rewrite NP as:
Det Nominal
and then rewrite Nominal as:
Det Noun
and ﬁnally rewrite these parts-of-speech as:
a ﬂight
We say the string a ﬂight can be derived from the non-terminal NP. Thus, a CFG
can be used to generate a set of strings. This sequence of rule expansions is called a
derivation of the string of words. It is common to represent a derivation by a parse
derivation
tree (commonly shown inverted with the root at the top). Figure 12.1 shows the tree
parse tree
representation of this derivation.
In the parse tree shown in Fig. 12.1, we can say that the node NP dominates
dominates
all the nodes in the tree (Det, Nom, Noun, a, ﬂight). We can say further that it
immediately dominates the nodes Det and Nom.
The formal language deﬁned by a CFG is the set of strings that are derivable
from the designated start symbol. Each grammar must have one designated start
start symbol
symbol, which is often called S. Since context-free grammars are often used to deﬁne
sentences, S is usually interpreted as the “sentence” node, and the set of strings that
are derivable from S is the set of sentences in some simpliﬁed version of English.

206
CHAPTER 12
•
CONSTITUENCY GRAMMARS
NP
Nom
Noun
ﬂight
Det
a
Figure 12.1
A parse tree for “a ﬂight”.
Let’s add a few additional rules to our inventory. The following rule expresses
the fact that a sentence can consist of a noun phrase followed by a verb phrase:
verb phrase
S →NP VP
I prefer a morning ﬂight
A verb phrase in English consists of a verb followed by assorted other things;
for example, one kind of verb phrase consists of a verb followed by a noun phrase:
VP →Verb NP
prefer a morning ﬂight
Or the verb may be followed by a noun phrase and a prepositional phrase:
VP →Verb NP PP
leave Boston in the morning
Or the verb phrase may have a verb followed by a prepositional phrase alone:
VP →Verb PP
leaving on Thursday
A prepositional phrase generally has a preposition followed by a noun phrase.
For example, a common type of prepositional phrase in the ATIS corpus is used to
indicate location or direction:
PP →Preposition NP
from Los Angeles
The NP inside a PP need not be a location; PPs are often used with times and
dates, and with other nouns as well; they can be arbitrarily complex. Here are ten
examples from the ATIS corpus:
to Seattle
on these ﬂights
in Minneapolis
about the ground transportation in Chicago
on Wednesday
of the round trip ﬂight on United Airlines
in the evening
of the AP ﬁfty seven ﬂight
on the ninth of July
with a stopover in Nashville
Figure 12.2 gives a sample lexicon, and Fig. 12.3 summarizes the grammar rules
we’ve seen so far, which we’ll call L0. Note that we can use the or-symbol | to
indicate that a non-terminal has alternate possible expansions.
We can use this grammar to generate sentences of this “ATIS-language”. We
start with S, expand it to NP VP, then choose a random expansion of NP (let’s say, to
I), and a random expansion of VP (let’s say, to Verb NP), and so on until we generate
the string I prefer a morning ﬂight. Figure 12.4 shows a parse tree that represents a
complete derivation of I prefer a morning ﬂight.
It is sometimes convenient to represent a parse tree in a more compact format
called bracketed notation; here is the bracketed representation of the parse tree of
bracketed
notation
Fig. 12.4:

12.2
•
CONTEXT-FREE GRAMMARS
207
Noun →ﬂights | breeze | trip | morning
Verb →is | prefer | like | need | want | ﬂy
Adjective →cheapest | non-stop | ﬁrst | latest
| other | direct
Pronoun →me | I | you | it
Proper-Noun →Alaska | Baltimore | Los Angeles
| Chicago | United | American
Determiner →the | a | an | this | these | that
Preposition →from | to | on | near
Conjunction →and | or | but
Figure 12.2
The lexicon for L0.
Grammar Rules
Examples
S →NP VP
I + want a morning ﬂight
NP →Pronoun
I
|
Proper-Noun
Los Angeles
|
Det Nominal
a + ﬂight
Nominal →Nominal Noun
morning + ﬂight
|
Noun
ﬂights
VP →Verb
do
|
Verb NP
want + a ﬂight
|
Verb NP PP
leave + Boston + in the morning
|
Verb PP
leaving + on Thursday
PP →Preposition NP
from + Los Angeles
Figure 12.3
The grammar for L0, with example phrases for each rule.
(12.1)
[S [NP [Pro I]] [VP [V prefer] [NP [Det a] [Nom [N morning] [Nom [N ﬂight]]]]]]
A CFG like that of L0 deﬁnes a formal language. We saw in Chapter 2 that a for-
mal language is a set of strings. Sentences (strings of words) that can be derived by a
grammar are in the formal language deﬁned by that grammar, and are called gram-
matical sentences. Sentences that cannot be derived by a given formal grammar are
grammatical
not in the language deﬁned by that grammar and are referred to as ungrammatical.
ungrammatical
This hard line between “in” and “out” characterizes all formal languages but is only
a very simpliﬁed model of how natural languages really work. This is because de-
termining whether a given sentence is part of a given natural language (say, English)
often depends on the context. In linguistics, the use of formal languages to model
natural languages is called generative grammar since the language is deﬁned by
generative
grammar
the set of possible sentences “generated” by the grammar.
12.2.1
Formal Deﬁnition of Context-Free Grammar
We conclude this section with a quick, formal description of a context-free gram-
mar and the language it generates. A context-free grammar G is deﬁned by four
parameters: N, Σ, R, S (technically this is a “4-tuple”).

208
CHAPTER 12
•
CONSTITUENCY GRAMMARS
S
VP
NP
Nom
Noun
ﬂight
Nom
Noun
morning
Det
a
Verb
prefer
NP
Pro
I
Figure 12.4
The parse tree for “I prefer a morning ﬂight” according to grammar L0.
N a set of non-terminal symbols (or variables)
Σ a set of terminal symbols (disjoint from N)
R a set of rules or productions, each of the form A →β ,
where A is a non-terminal,
β is a string of symbols from the inﬁnite set of strings (Σ∪N)∗
S
a designated start symbol and a member of N
For the remainder of the book we adhere to the following conventions when dis-
cussing the formal properties of context-free grammars (as opposed to explaining
particular facts about English or other languages).
Capital letters like A, B, and S
Non-terminals
S
The start symbol
Lower-case Greek letters like α, β, and γ
Strings drawn from (Σ∪N)∗
Lower-case Roman letters like u, v, and w
Strings of terminals
A language is deﬁned through the concept of derivation. One string derives an-
other one if it can be rewritten as the second one by some series of rule applications.
More formally, following Hopcroft and Ullman (1979),
if A →β is a production of R and α and γ are any strings in the set
(Σ∪N)∗, then we say that αAγ directly derives αβγ, or αAγ ⇒αβγ.
directly derives
Derivation is then a generalization of direct derivation:
Let α1, α2, ..., αm be strings in (Σ∪N)∗,m ≥1, such that
α1 ⇒α2,α2 ⇒α3,...,αm−1 ⇒αm
We say that α1 derives αm, or α1
∗⇒αm.
derives
We can then formally deﬁne the language LG generated by a grammar G as the
set of strings composed of terminal symbols that can be derived from the designated

12.3
•
SOME GRAMMAR RULES FOR ENGLISH
209
start symbol S.
LG = {w|w is in Σ∗and S ∗⇒w}
The problem of mapping from a string of words to its parse tree is called syn-
tactic parsing; we deﬁne algorithms for parsing in Chapter 13.
syntactic
parsing
12.3
Some Grammar Rules for English
In this section, we introduce a few more aspects of the phrase structure of English;
for consistency we will continue to focus on sentences from the ATIS domain. Be-
cause of space limitations, our discussion is necessarily limited to highlights. Read-
ers are strongly advised to consult a good reference grammar of English, such as
Huddleston and Pullum (2002).
12.3.1
Sentence-Level Constructions
In the small grammar L0, we provided only one sentence-level construction for
declarative sentences like I prefer a morning ﬂight. Among the large number of
constructions for English sentences, four are particularly common and important:
declaratives, imperatives, yes-no questions, and wh-questions.
Sentences with declarative structure have a subject noun phrase followed by
declarative
a verb phrase, like “I prefer a morning ﬂight”. Sentences with this structure have
a great number of different uses that we follow up on in Chapter 26. Here are a
number of examples from the ATIS domain:
I want a ﬂight from Ontario to Chicago
The ﬂight should be eleven a.m. tomorrow
The return ﬂight should leave at around seven p.m.
Sentences with imperative structure often begin with a verb phrase and have
imperative
no subject. They are called imperative because they are almost always used for
commands and suggestions; in the ATIS domain they are commands to the system.
Show the lowest fare
Give me Sunday’s ﬂights arriving in Las Vegas from New York City
List all ﬂights between ﬁve and seven p.m.
We can model this sentence structure with another rule for the expansion of S:
S →VP
Sentences with yes-no question structure are often (though not always) used to
yes-no question
ask questions; they begin with an auxiliary verb, followed by a subject NP, followed
by a VP. Here are some examples. Note that the third example is not a question at
all but a request; Chapter 26 discusses the uses of these question forms to perform
different pragmatic functions such as asking, requesting, or suggesting.
Do any of these ﬂights have stops?
Does American’s ﬂight eighteen twenty ﬁve serve dinner?
Can you give me the same information for United?
Here’s the rule:
S →Aux NP VP

210
CHAPTER 12
•
CONSTITUENCY GRAMMARS
The most complex sentence-level structures we examine here are the various wh-
structures. These are so named because one of their constituents is a wh-phrase, that
wh-phrase
is, one that includes a wh-word (who, whose, when, where, what, which, how, why).
wh-word
These may be broadly grouped into two classes of sentence-level structures. The
wh-subject-question structure is identical to the declarative structure, except that
the ﬁrst noun phrase contains some wh-word.
What airlines ﬂy from Burbank to Denver?
Which ﬂights depart Burbank after noon and arrive in Denver by six p.m?
Whose ﬂights serve breakfast?
Here is a rule. Exercise 12.7 discusses rules for the constituents that make up the
Wh-NP.
S →Wh-NP VP
In the wh-non-subject-question structure, the wh-phrase is not the subject of the
wh-non-subject-
question
sentence, and so the sentence includes another subject. In these types of sentences
the auxiliary appears before the subject NP, just as in the yes-no question structures.
Here is an example followed by a sample rule:
What ﬂights do you have from Burbank to Tacoma Washington?
S →Wh-NP Aux NP VP
Constructions like the wh-non-subject-question contain what are called long-
distance dependencies because the Wh-NP what ﬂights is far away from the predi-
long-distance
dependencies
cate that it is semantically related to, the main verb have in the VP. In some models
of parsing and understanding compatible with the grammar rule above, long-distance
dependencies like the relation between ﬂights and have are thought of as a semantic
relation. In such models, the job of ﬁguring out that ﬂights is the argument of have
is done during semantic interpretation. In other models of parsing, the relationship
between ﬂights and have is considered to be a syntactic relation, and the grammar is
modiﬁed to insert a small marker called a trace or empty category after the verb.
We return to such empty-category models when we introduce the Penn Treebank on
page 217.
12.3.2
Clauses and Sentences
Before we move on, we should clarify the status of the S rules in the grammars we
just described. S rules are intended to account for entire sentences that stand alone
as fundamental units of discourse. However, S can also occur on the right-hand side
of grammar rules and hence can be embedded within larger sentences. Clearly then,
there’s more to being an S than just standing alone as a unit of discourse.
What differentiates sentence constructions (i.e., the S rules) from the rest of the
grammar is the notion that they are in some sense complete. In this way they corre-
spond to the notion of a clause, which traditional grammars often describe as form-
clause
ing a complete thought. One way of making this notion of “complete thought” more
precise is to say an S is a node of the parse tree below which the main verb of the S
has all of its arguments. We deﬁne verbal arguments later, but for now let’s just see
an illustration from the tree for I prefer a morning ﬂight in Fig. 12.4 on page 208.
The verb prefer has two arguments: the subject I and the object a morning ﬂight.
One of the arguments appears below the VP node, but the other one, the subject NP,
appears only below the S node.

12.3
•
SOME GRAMMAR RULES FOR ENGLISH
211
12.3.3
The Noun Phrase
Our L0 grammar introduced three of the most frequent types of noun phrases that
occur in English: pronouns, proper nouns and the NP →Det Nominal construction.
The central focus of this section is on the last type since that is where the bulk of
the syntactic complexity resides. These noun phrases consist of a head, the central
noun in the noun phrase, along with various modiﬁers that can occur before or after
the head noun. Let’s take a close look at the various parts.
The Determiner
Noun phrases can begin with simple lexical determiners, as in the following exam-
ples:
a stop
the ﬂights
this ﬂight
those ﬂights
any ﬂights
some ﬂights
The role of the determiner in English noun phrases can also be ﬁlled by more
complex expressions, as follows:
United’s ﬂight
United’s pilot’s union
Denver’s mayor’s mother’s canceled ﬂight
In these examples, the role of the determiner is ﬁlled by a possessive expression
consisting of a noun phrase followed by an ’s as a possessive marker, as in the
following rule.
Det →NP ′s
The fact that this rule is recursive (since an NP can start with a Det) helps us model
the last two examples above, in which a sequence of possessive expressions serves
as a determiner.
Under some circumstances determiners are optional in English. For example,
determiners may be omitted if the noun they modify is plural:
(12.2) Show me ﬂights from San Francisco to Denver on weekdays
As we saw in Chapter 8, mass nouns also don’t require determination. Recall that
mass nouns often (not always) involve something that is treated like a substance
(including e.g., water and snow), don’t take the indeﬁnite article “a”, and don’t tend
to pluralize. Many abstract nouns are mass nouns (music, homework). Mass nouns
in the ATIS domain include breakfast, lunch, and dinner:
(12.3) Does this ﬂight serve dinner?
The Nominal
The nominal construction follows the determiner and contains any pre- and post-
head noun modiﬁers. As indicated in grammar L0, in its simplest form a nominal
can consist of a single noun.
Nominal →Noun
As we’ll see, this rule also provides the basis for the bottom of various recursive
rules used to capture more complex nominal constructions.

212
CHAPTER 12
•
CONSTITUENCY GRAMMARS
Before the Head Noun
A number of different kinds of word classes can appear before the head noun (the
Cardinal
numbers
“postdeterminers”) in a nominal. These include cardinal numbers, ordinal num-
bers, quantiﬁers, and adjectives. Examples of cardinal numbers:
ordinal
numbers
quantiﬁers
two friends
one stop
Ordinal numbers include ﬁrst, second, third, and so on, but also words like next,
last, past, other, and another:
the ﬁrst one
the next day
the second leg
the last ﬂight
the other American ﬂight
Some quantiﬁers (many, (a) few, several) occur only with plural count nouns:
many fares
Adjectives occur after quantiﬁers but before nouns.
a ﬁrst-class fare
a non-stop ﬂight
the longest layover
the earliest lunch ﬂight
Adjectives can also be grouped into a phrase called an adjective phrase or AP.
adjective
phrase
APs can have an adverb before the adjective (see Chapter 8 for deﬁnitions of adjec-
tives and adverbs):
the least expensive fare
After the Head Noun
A head noun can be followed by postmodiﬁers. Three kinds of nominal postmodi-
ﬁers are common in English:
prepositional phrases
all ﬂights from Cleveland
non-ﬁnite clauses
any ﬂights arriving after eleven a.m.
relative clauses
a ﬂight that serves breakfast
They are especially common in the ATIS corpus since they are used to mark the
origin and destination of ﬂights.
Here are some examples of prepositional phrase postmodiﬁers, with brackets
inserted to show the boundaries of each PP; note that two or more PPs can be strung
together within a single NP:
all ﬂights [from Cleveland] [to Newark]
arrival [in San Jose] [before seven p.m.]
a reservation [on ﬂight six oh six] [from Tampa] [to Montreal]
Here’s a new nominal rule to account for postnominal PPs:
Nominal →Nominal PP
The three most common kinds of non-ﬁnite postmodiﬁers are the gerundive (-
non-ﬁnite
ing), -ed, and inﬁnitive forms.
Gerundive postmodiﬁers are so called because they consist of a verb phrase that
gerundive
begins with the gerundive (-ing) form of the verb. Here are some examples:
any of those [leaving on Thursday]
any ﬂights [arriving after eleven a.m.]
ﬂights [arriving within thirty minutes of each other]

12.3
•
SOME GRAMMAR RULES FOR ENGLISH
213
We can deﬁne the Nominals with gerundive modiﬁers as follows, making use of
a new non-terminal GerundVP:
Nominal →Nominal GerundVP
We can make rules for GerundVP constituents by duplicating all of our VP pro-
ductions, substituting GerundV for V.
GerundVP →GerundV NP
|
GerundV PP | GerundV | GerundV NP PP
GerundV can then be deﬁned as
GerundV →being | arriving | leaving | ...
The phrases in italics below are examples of the two other common kinds of
non-ﬁnite clauses, inﬁnitives and -ed forms:
the last ﬂight to arrive in Boston
I need to have dinner served
Which is the aircraft used by this ﬂight?
A postnominal relative clause (more correctly a restrictive relative clause), is
a clause that often begins with a relative pronoun (that and who are the most com-
relative
pronoun
mon). The relative pronoun functions as the subject of the embedded verb in the
following examples:
a ﬂight that serves breakfast
ﬂights that leave in the morning
the one that leaves at ten thirty ﬁve
We might add rules like the following to deal with these:
Nominal →Nominal RelClause
RelClause →(who | that) VP
The relative pronoun may also function as the object of the embedded verb, as
in the following example; we leave for the reader the exercise of writing grammar
rules for more complex relative clauses of this kind.
the earliest American Airlines ﬂight that I can get
Various postnominal modiﬁers can be combined, as the following examples
show:
a ﬂight [from Phoenix to Detroit] [leaving Monday evening]
evening ﬂights [from Nashville to Houston] [that serve dinner]
a friend [living in Denver] [that would like to visit me in DC]
Before the Noun Phrase
Word classes that modify and appear before NPs are called predeterminers. Many
predeterminers
of these have to do with number or amount; a common predeterminer is all:
all the ﬂights
all ﬂights
all non-stop ﬂights
The example noun phrase given in Fig. 12.5 illustrates some of the complexity
that arises when these rules are combined.

214
CHAPTER 12
•
CONSTITUENCY GRAMMARS
NP
NP
Nom
GerundiveVP
leaving before 10
Nom
PP
to Tampa
Nom
PP
from Denver
Nom
Noun
ﬂights
Nom
Noun
morning
Det
the
PreDet
all
Figure 12.5
A parse tree for “all the morning ﬂights from Denver to Tampa leaving before 10”.
12.3.4
The Verb Phrase
The verb phrase consists of the verb and a number of other constituents. In the
simple rules we have built so far, these other constituents include NPs and PPs and
combinations of the two:
VP →Verb
disappear
VP →Verb NP
prefer a morning ﬂight
VP →Verb NP PP
leave Boston in the morning
VP →Verb PP
leaving on Thursday
Verb phrases can be signiﬁcantly more complicated than this. Many other kinds
of constituents, such as an entire embedded sentence, can follow the verb. These are
called sentential complements:
sentential
complements
You [VP [V said [S you had a two hundred sixty-six dollar fare]]
[VP [V Tell] [NP me] [S how to get from the airport in Philadelphia to down-
town]]
I [VP [V think [S I would like to take the nine thirty ﬂight]]
Here’s a rule for these:
VP →Verb S
Similarly, another potential constituent of the VP is another VP. This is often the
case for verbs like want, would like, try, intend, need:
I want [VP to ﬂy from Milwaukee to Orlando]
Hi, I want [VP to arrange three ﬂights]

12.3
•
SOME GRAMMAR RULES FOR ENGLISH
215
Frame
Verb
Example
/0
eat, sleep
I ate
NP
prefer, ﬁnd, leave
Find [NP the ﬂight from Pittsburgh to Boston]
NP NP
show, give
Show [NP me] [NP airlines with ﬂights from Pittsburgh]
PPfrom PPto
ﬂy, travel
I would like to ﬂy [PP from Boston] [PP to Philadelphia]
NP PPwith
help, load
Can you help [NP me] [PP with a ﬂight]
VPto
prefer, want, need
I would prefer [VPto to go by United Airlines]
VPbrst
can, would, might
I can [VPbrst go from Boston]
S
mean
Does this mean [S AA has a hub in Boston]
Figure 12.6
Subcategorization frames for a set of example verbs.
While a verb phrase can have many possible kinds of constituents, not every
verb is compatible with every verb phrase. For example, the verb want can be used
either with an NP complement (I want a ﬂight ... ) or with an inﬁnitive VP comple-
ment (I want to ﬂy to .. .). By contrast, a verb like ﬁnd cannot take this sort of VP
complement (* I found to ﬂy to Dallas).
This idea that verbs are compatible with different kinds of complements is a very
old one; traditional grammar distinguishes between transitive verbs like ﬁnd, which
transitive
take a direct object NP (I found a ﬂight), and intransitive verbs like disappear,
intransitive
which do not (*I disappeared a ﬂight).
Where traditional grammars subcategorize verbs into these two categories (tran-
subcategorize
sitive and intransitive), modern grammars distinguish as many as 100 subcategories.
We say that a verb like ﬁnd subcategorizes for an NP, and a verb like want sub-
Subcategorizes
for
categorizes for either an NP or a non-ﬁnite VP. We also call these constituents the
complements of the verb (hence our use of the term sentential complement above).
complements
So we say that want can take a VP complement. These possible sets of complements
are called the subcategorization frame for the verb. Another way of talking about
Subcategorization
frame
the relation between the verb and these other constituents is to think of the verb as
a logical predicate and the constituents as logical arguments of the predicate. So we
can think of such predicate-argument relations as FIND(I, A FLIGHT) or WANT(I, TO
FLY). We talk more about this view of verbs and arguments in Chapter 16 when we
talk about predicate calculus representations of verb semantics. Subcategorization
frames for a set of example verbs are given in Fig. 12.6.
We can capture the association between verbs and their complements by making
separate subtypes of the class Verb (e.g., Verb-with-NP-complement, Verb-with-Inf-
VP-complement, Verb-with-S-complement, and so on):
Verb-with-NP-complement →ﬁnd | leave | repeat | ...
Verb-with-S-complement →think | believe | say | ...
Verb-with-Inf-VP-complement →want | try | need | ...
Each VP rule could then be modiﬁed to require the appropriate verb subtype:
VP →Verb-with-no-complement
disappear
VP →Verb-with-NP-comp NP
prefer a morning ﬂight
VP →Verb-with-S-comp S said there were two ﬂights
A problem with this approach is the signiﬁcant increase in the number of rules
and the associated loss of generality.

216
CHAPTER 12
•
CONSTITUENCY GRAMMARS
12.3.5
Coordination
The major phrase types discussed here can be conjoined with conjunctions like and,
conjunctions
or, and but to form larger constructions of the same type. For example, a coordinate
coordinate
noun phrase can consist of two other noun phrases separated by a conjunction:
Please repeat [NP [NP the ﬂights] and [NP the costs]]
I need to know [NP [NP the aircraft] and [NP the ﬂight number]]
Here’s a rule that allows these structures:
NP →NP and NP
Note that the ability to form coordinate phrases through conjunctions is often
used as a test for constituency. Consider the following examples, which differ from
the ones given above in that they lack the second determiner.
Please repeat the [Nom [Nom ﬂights] and [Nom costs]]
I need to know the [Nom [Nom aircraft] and [Nom ﬂight number]]
The fact that these phrases can be conjoined is evidence for the presence of the
underlying Nominal constituent we have been making use of. Here’s a rule for this:
Nominal →Nominal and Nominal
The following examples illustrate conjunctions involving VPs and Ss.
What ﬂights do you have [VP [VP leaving Denver] and [VP arriving in
San Francisco]]
[S [S I’m interested in a ﬂight from Dallas to Washington] and [S I’m
also interested in going to Baltimore]]
The rules for VP and S conjunctions mirror the NP one given above.
VP →VP and VP
S →S and S
Since all the major phrase types can be conjoined in this fashion, it is also possible
to represent this conjunction fact more generally; a number of grammar formalisms
such as GPSG ((Gazdar et al., 1985)) do this using metarules such as the following:
metarules
X →X and X
This metarule simply states that any non-terminal can be conjoined with the same
non-terminal to yield a constituent of the same type. Of course, the variable X
must be designated as a variable that stands for any non-terminal rather than a non-
terminal itself.
12.4
Treebanks
Sufﬁciently robust grammars consisting of context-free grammar rules can be used
to assign a parse tree to any sentence. This means that it is possible to build a
corpus where every sentence in the collection is paired with a corresponding parse
tree. Such a syntactically annotated corpus is called a treebank. Treebanks play
treebank

12.4
•
TREEBANKS
217
an important role in parsing, as we discuss in Chapter 13, as well as in linguistic
investigations of syntactic phenomena.
A wide variety of treebanks have been created, generally through the use of
parsers (of the sort described in the next few chapters) to automatically parse each
sentence, followed by the use of humans (linguists) to hand-correct the parses. The
Penn Treebank project (whose POS tagset we introduced in Chapter 8) has pro-
Penn Treebank
duced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal cor-
pora of English, as well as treebanks in Arabic and Chinese. A number of treebanks
use the dependency representation we will introduce in Chapter 15, including many
that are part of the Universal Dependencies project (Nivre et al., 2016b).
12.4.1
Example: The Penn Treebank Project
Figure 12.7 shows sentences from the Brown and ATIS portions of the Penn Tree-
bank.1 Note the formatting differences for the part-of-speech tags; such small dif-
ferences are common and must be dealt with in processing treebanks. The Penn
Treebank part-of-speech tagset was deﬁned in Chapter 8. The use of LISP-style
parenthesized notation for trees is extremely common and resembles the bracketed
notation we saw earlier in (12.1). For those who are not familiar with it we show a
standard node-and-line tree representation in Fig. 12.8.
((S
(NP-SBJ (DT That)
(JJ cold) (, ,)
(JJ empty) (NN sky) )
(VP (VBD was)
(ADJP-PRD (JJ full)
(PP (IN of)
(NP (NN fire)
(CC and)
(NN light) ))))
(. .) ))
((S
(NP-SBJ The/DT flight/NN )
(VP should/MD
(VP arrive/VB
(PP-TMP at/IN
(NP eleven/CD a.m/RB ))
(NP-TMP tomorrow/NN )))))
(a)
(b)
Figure 12.7
Parsed sentences from the LDC Treebank3 version of the Brown (a) and ATIS
(b) corpora.
Figure 12.9 shows a tree from the Wall Street Journal. This tree shows an-
other feature of the Penn Treebanks: the use of traces (-NONE- nodes) to mark
traces
long-distance dependencies or syntactic movement. For example, quotations often
syntactic
movement
follow a quotative verb like say. But in this example, the quotation “We would have
to wait until we have collected on those assets” precedes the words he said. An
empty S containing only the node -NONE- marks the position after said where the
quotation sentence often occurs. This empty node is marked (in Treebanks II and
III) with the index 2, as is the quotation S at the beginning of the sentence. Such
co-indexing may make it easier for some parsers to recover the fact that this fronted
or topicalized quotation is the complement of the verb said. A similar -NONE- node
1
The Penn Treebank project released treebanks in multiple languages and in various stages; for ex-
ample, there were Treebank I (Marcus et al., 1993), Treebank II (Marcus et al., 1994), and Treebank III
releases of English treebanks. We use Treebank III for our examples.

218
CHAPTER 12
•
CONSTITUENCY GRAMMARS
S
.
.
VP
ADJP-PRD
PP
NP
NN
light
CC
and
NN
ﬁre
IN
of
JJ
full
VBD
was
NP-SBJ
NN
sky
JJ
empty
,
,
JJ
cold
DT
That
Figure 12.8
The tree corresponding to the Brown corpus sentence in the previous ﬁgure.
marks the fact that there is no syntactic subject right before the verb to wait; instead,
the subject is the earlier NP We. Again, they are both co-indexed with the index 1.
( (S (‘‘ ‘‘)
(S-TPC-2
(NP-SBJ-1 (PRP We) )
(VP (MD would)
(VP (VB have)
(S
(NP-SBJ (-NONE- *-1) )
(VP (TO to)
(VP (VB wait)
(SBAR-TMP (IN until)
(S
(NP-SBJ (PRP we) )
(VP (VBP have)
(VP (VBN collected)
(PP-CLR (IN on)
(NP (DT those)(NNS assets)))))))))))))
(, ,) (’’ ’’)
(NP-SBJ (PRP he) )
(VP (VBD said)
(S (-NONE- *T*-2) ))
(. .) ))
Figure 12.9
A sentence from the Wall Street Journal portion of the LDC Penn Treebank.
Note the use of the empty -NONE- nodes.
The Penn Treebank II and Treebank III releases added further information to
make it easier to recover the relationships between predicates and arguments. Cer-

12.4
•
TREEBANKS
219
Grammar
Lexicon
S →NP VP .
PRP →we | he
S →NP VP
DT →the | that | those
S →“ S ” , NP VP .
JJ →cold | empty | full
S →-NONE-
NN →sky | ﬁre | light | ﬂight | tomorrow
NP →DT NN
NNS →assets
NP →DT NNS
CC →and
NP →NN CC NN
IN →of | at | until | on
NP →CD RB
CD →eleven
NP →DT JJ , JJ NN
RB →a.m.
NP →PRP
VB →arrive | have | wait
NP →-NONE-
VBD →was | said
VP →MD VP
VBP →have
VP →VBD ADJP
VBN →collected
VP →VBD S
MD →should | would
VP →VBN PP
TO →to
VP →VB S
VP →VB SBAR
VP →VBP VP
VP →VBN PP
VP →TO VP
SBAR →IN S
ADJP →JJ PP
PP →IN NP
Figure 12.10
A sample of the CFG grammar rules and lexical entries that would be ex-
tracted from the three treebank sentences in Fig. 12.7 and Fig. 12.9.
tain phrases were marked with tags indicating the grammatical function of the phrase
(as surface subject, logical topic, cleft, non-VP predicates) its presence in particular
text categories (headlines, titles), and its semantic function (temporal phrases, lo-
cations) (Marcus et al. 1994, Bies et al. 1995). Figure 12.9 shows examples of the
-SBJ (surface subject) and -TMP (temporal phrase) tags. Figure 12.8 shows in addi-
tion the -PRD tag, which is used for predicates that are not VPs (the one in Fig. 12.8
is an ADJP). We’ll return to the topic of grammatical function when we consider
dependency grammars and parsing in Chapter 15.
12.4.2
Treebanks as Grammars
The sentences in a treebank implicitly constitute a grammar of the language repre-
sented by the corpus being annotated. For example, from the three parsed sentences
in Fig. 12.7 and Fig. 12.9, we can extract each of the CFG rules in them. For sim-
plicity, let’s strip off the rule sufﬁxes (-SBJ and so on). The resulting grammar is
shown in Fig. 12.10.
The grammar used to parse the Penn Treebank is relatively ﬂat, resulting in very
many and very long rules. For example, among the approximately 4,500 different
rules for expanding VPs are separate rules for PP sequences of any length and every
possible arrangement of verb arguments:
VP →VBD PP
VP →VBD PP PP
VP →VBD PP PP PP
VP →VBD PP PP PP PP
VP →VB ADVP PP
VP →VB PP ADVP
VP →ADVP VB PP

220
CHAPTER 12
•
CONSTITUENCY GRAMMARS
as well as even longer rules, such as
VP →VBP PP PP PP PP PP ADVP PP
which comes from the VP marked in italics:
This mostly happens because we go from football in the fall to lifting in the
winter to football again in the spring.
Some of the many thousands of NP rules include
NP →DT JJ NN
NP →DT JJ NNS
NP →DT JJ NN NN
NP →DT JJ JJ NN
NP →DT JJ CD NNS
NP →RB DT JJ NN NN
NP →RB DT JJ JJ NNS
NP →DT JJ JJ NNP NNS
NP →DT NNP NNP NNP NNP JJ NN
NP →DT JJ NNP CC JJ JJ NN NNS
NP →RB DT JJS NN NN SBAR
NP →DT VBG JJ NNP NNP CC NNP
NP →DT JJ NNS , NNS CC NN NNS NN
NP →DT JJ JJ VBG NN NNP NNP FW NNP
NP →NP JJ , JJ ‘‘ SBAR ’’ NNS
The last two of those rules, for example, come from the following two noun phrases:
[DT The] [JJ state-owned] [JJ industrial] [VBG holding] [NN company] [NNP Instituto]
[NNP Nacional] [FW de] [NNP Industria]
[NP Shearson’s] [JJ easy-to-ﬁlm], [JJ black-and-white] “[SBAR Where We Stand]”
[NNS commercials]
Viewed as a large grammar in this way, the Penn Treebank III Wall Street Journal
corpus, which contains about 1 million words, also has about 1 million non-lexical
rule tokens, consisting of about 17,500 distinct rule types.
Various facts about the treebank grammars, such as their large numbers of ﬂat
rules, pose problems for probabilistic parsing algorithms. For this reason, it is com-
mon to make various modiﬁcations to a grammar extracted from a treebank. We
discuss these further in Chapter 14.
12.4.3
Heads and Head Finding
We suggested informally earlier that syntactic constituents could be associated with
a lexical head; N is the head of an NP, V is the head of a VP. This idea of a head for
each constituent dates back to Bloomﬁeld (1914). It is central to constituent-based
grammar formalisms such as Head-Driven Phrase Structure Grammar (Pollard and
Sag, 1994), as well as the dependency-based approaches to grammar we’ll discuss
in Chapter 15. Heads and head-dependent relations have also come to play a central
role in computational linguistics with their use in probabilistic parsing (Chapter 14)
and in dependency parsing (Chapter 15).
In one simple model of lexical heads, each context-free rule is associated with
a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is
grammatically the most important. Heads are passed up the parse tree; thus, each
non-terminal in a parse tree is annotated with a single word, which is its lexical head.

12.4
•
TREEBANKS
221
S(dumped)
VP(dumped)
PP(into)
NP(bin)
NN(bin)
bin
DT(a)
a
P
into
NP(sacks)
NNS(sacks)
sacks
VBD(dumped)
dumped
NP(workers)
NNS(workers)
workers
Figure 12.11
A lexicalized tree from Collins (1999).
Figure 12.11 shows an example of such a tree from Collins (1999), in which each
non-terminal is annotated with its head.
For the generation of such a tree, each CFG rule must be augmented to identify
one right-side constituent to be the head daughter. The headword for a node is
then set to the headword of its head daughter. Choosing these head daughters is
simple for textbook examples (NN is the head of NP) but is complicated and indeed
controversial for most phrases. (Should the complementizer to or the verb be the
head of an inﬁnite verb-phrase?) Modern linguistic theories of syntax generally
include a component that deﬁnes heads (see, e.g., (Pollard and Sag, 1994)).
An alternative approach to ﬁnding a head is used in most practical computational
systems. Instead of specifying head rules in the grammar itself, heads are identiﬁed
dynamically in the context of trees for speciﬁc sentences. In other words, once
a sentence is parsed, the resulting tree is walked to decorate each node with the
appropriate head. Most current systems rely on a simple set of handwritten rules,
such as a practical one for Penn Treebank grammars given in Collins (1999) but
developed originally by Magerman (1995). For example, the rule for ﬁnding the
head of an NP is as follows (Collins, 1999, p. 238):
• If the last word is tagged POS, return last-word.
• Else search from right to left for the ﬁrst child which is an NN, NNP, NNPS, NX, POS,
or JJR.
• Else search from left to right for the ﬁrst child which is an NP.
• Else search from right to left for the ﬁrst child which is a $, ADJP, or PRN.
• Else search from right to left for the ﬁrst child which is a CD.
• Else search from right to left for the ﬁrst child which is a JJ, JJS, RB or QP.
• Else return the last word
Selected other rules from this set are shown in Fig. 12.12. For example, for VP
rules of the form VP →Y1 ··· Yn, the algorithm would start from the left of Y1 ···
Yn looking for the ﬁrst Yi of type TO; if no TOs are found, it would search for the
ﬁrst Yi of type VBD; if no VBDs are found, it would search for a VBN, and so on.
See Collins (1999) for more details.

222
CHAPTER 12
•
CONSTITUENCY GRAMMARS
Parent
Direction
Priority List
ADJP
Left
NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS
SBAR RB
ADVP
Right
RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
PRN
Left
PRT
Right
RP
QP
Left
$ IN NNS NN JJ RB DT CD NCD QP JJR JJS
S
Left
TO IN VP S SBAR ADJP UCP NP
SBAR
Left
WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG
VP
Left
TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP
Figure 12.12
Selected head rules from Collins (1999). The set of head rules is often called a head percola-
tion table.
12.5
Grammar Equivalence and Normal Form
A formal language is deﬁned as a (possibly inﬁnite) set of strings of words. This
suggests that we could ask if two grammars are equivalent by asking if they gener-
ate the same set of strings. In fact, it is possible to have two distinct context-free
grammars generate the same language.
We usually distinguish two kinds of grammar equivalence: weak equivalence
and strong equivalence. Two grammars are strongly equivalent if they generate the
same set of strings and if they assign the same phrase structure to each sentence
(allowing merely for renaming of the non-terminal symbols). Two grammars are
weakly equivalent if they generate the same set of strings but do not assign the same
phrase structure to each sentence.
It is sometimes useful to have a normal form for grammars, in which each of
normal form
the productions takes a particular form. For example, a context-free grammar is in
Chomsky normal form (CNF) (Chomsky, 1963) if it is ϵ-free and if in addition
Chomsky
normal form
each production is either of the form A →B C or A →a. That is, the right-hand side
of each rule either has two non-terminal symbols or one terminal symbol. Chomsky
normal form grammars are binary branching, that is they have binary trees (down
binary
branching
to the prelexical nodes). We make use of this binary branching property in the CKY
parsing algorithm in Chapter 13.
Any context-free grammar can be converted into a weakly equivalent Chomsky
normal form grammar. For example, a rule of the form
A →B C D
can be converted into the following two CNF rules (Exercise 12.8 asks the reader to
formulate the complete algorithm):
A →B X
X →C D
Sometimes using binary branching can actually produce smaller grammars. For
example, the sentences that might be characterized as
VP -> VBD NP PP*
are represented in the Penn Treebank by this series of rules:
VP →VBD NP PP
VP →VBD NP PP PP

12.6
•
LEXICALIZED GRAMMARS
223
VP →VBD NP PP PP PP
VP →VBD NP PP PP PP PP
...
but could also be generated by the following two-rule grammar:
VP →VBD NP PP
VP →VP PP
The generation of a symbol A with a potentially inﬁnite sequence of symbols B with
a rule of the form A →A B is known as Chomsky-adjunction.
Chomsky-
adjunction
12.6
Lexicalized Grammars
The approach to grammar presented thus far emphasizes phrase-structure rules while
minimizing the role of the lexicon.
However, as we saw in the discussions of
agreement, subcategorization, and long-distance dependencies, this approach leads
to solutions that are cumbersome at best, yielding grammars that are redundant,
hard to manage, and brittle. To overcome these issues, numerous alternative ap-
proaches have been developed that all share the common theme of making bet-
ter use of the lexicon. Among the more computationally relevant approaches are
Lexical-Functional Grammar (LFG) (Bresnan, 1982), Head-Driven Phrase Structure
Grammar (HPSG) (Pollard and Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi,
1985), and Combinatory Categorial Grammar (CCG). These approaches differ with
respect to how lexicalized they are — the degree to which they rely on the lexicon
as opposed to phrase structure rules to capture facts about the language.
The following section provides an introduction to CCG, a heavily lexicalized
approach motivated by both syntactic and semantic considerations, which we will
return to in Chapter 16. Chapter 15 discusses dependency grammars, an approach
that eliminates phrase-structure rules entirely.
12.6.1
Combinatory Categorial Grammar
In this section, we provide an overview of categorial grammar (Ajdukiewicz 1935,
categorial
grammar
Bar-Hillel 1953), an early lexicalized grammar model, as well as an important mod-
ern extension, combinatory categorial grammar, or CCG (Steedman 1996, Steed-
combinatory
categorial
grammar
man 1989, Steedman 2000).
The categorial approach consists of three major elements: a set of categories,
a lexicon that associates words with categories, and a set of rules that govern how
categories combine in context.
Categories
Categories are either atomic elements or single-argument functions that return a cat-
egory as a value when provided with a desired category as argument. More formally,
we can deﬁne C , a set of categories for a grammar as follows:
• A ⊆C , where A is a given set of atomic elements
• (X/Y), (X\Y) ∈C , if X, Y ∈C
The slash notation shown here is used to deﬁne the functions in the grammar.
It speciﬁes the type of the expected argument, the direction it is expected be found,
and the type of the result. Thus, (X/Y) is a function that seeks a constituent of type

224
CHAPTER 12
•
CONSTITUENCY GRAMMARS
Y to its right and returns a value of X; (X\Y) is the same except it seeks its argument
to the left.
The set of atomic categories is typically very small and includes familiar el-
ements such as sentences and noun phrases. Functional categories include verb
phrases and complex noun phrases among others.
The Lexicon
The lexicon in a categorial approach consists of assignments of categories to words.
These assignments can either be to atomic or functional categories, and due to lexical
ambiguity words can be assigned to multiple categories. Consider the following
sample lexical entries.
ﬂight :
N
Miami :
NP
cancel : (S\NP)/NP
Nouns and proper nouns like ﬂight and Miami are assigned to atomic categories,
reﬂecting their typical role as arguments to functions. On the other hand, a transitive
verb like cancel is assigned the category (S\NP)/NP: a function that seeks an NP on
its right and returns as its value a function with the type (S\NP). This function can,
in turn, combine with an NP on the left, yielding an S as the result. This captures the
kind of subcategorization information discussed in Section 12.3.4, however here the
information has a rich, computationally useful, internal structure.
Ditransitive verbs like give, which expect two arguments after the verb, would
have the category ((S\NP)/NP)/NP: a function that combines with an NP on its
right to yield yet another function corresponding to the transitive verb (S\NP)/NP
category such as the one given above for cancel.
Rules
The rules of a categorial grammar specify how functions and their arguments com-
bine. The following two rule templates constitute the basis for all categorial gram-
mars.
X/Y Y ⇒X
(12.4)
Y X\Y ⇒X
(12.5)
The ﬁrst rule applies a function to its argument on the right, while the second
looks to the left for its argument. We’ll refer to the ﬁrst as forward function appli-
cation, and the second as backward function application. The result of applying
either of these rules is the category speciﬁed as the value of the function being ap-
plied.
Given these rules and a simple lexicon, let’s consider an analysis of the sentence
United serves Miami. Assume that serves is a transitive verb with the category
(S\NP)/NP and that United and Miami are both simple NPs. Using both forward
and backward function application, the derivation would proceed as follows:
United
serves
Miami
NP
(S\NP)/NP
NP
>
S\NP
<
S

12.6
•
LEXICALIZED GRAMMARS
225
Categorial grammar derivations are illustrated growing down from the words,
rule applications are illustrated with a horizontal line that spans the elements in-
volved, with the type of the operation indicated at the right end of the line. In this
example, there are two function applications: one forward function application indi-
cated by the > that applies the verb serves to the NP on its right, and one backward
function application indicated by the < that applies the result of the ﬁrst to the NP
United on its left.
With the addition of another rule, the categorial approach provides a straight-
forward way to implement the coordination metarule described earlier on page 216.
Recall that English permits the coordination of two constituents of the same type,
resulting in a new constituent of the same type. The following rule provides the
mechanism to handle such examples.
X CONJ X ⇒X
(12.6)
This rule states that when two constituents of the same category are separated by a
constituent of type CONJ they can be combined into a single larger constituent of
the same type. The following derivation illustrates the use of this rule.
We
flew
to
Geneva
and
drove
to
Chamonix
NP (S\NP)/PP PP/NP
NP
CONJ (S\NP)/PP PP/NP
NP
>
>
PP
PP
>
>
S\NP
S\NP
<Φ>
S\NP
<
S
Here the two S\NP constituents are combined via the conjunction operator <Φ>
to form a larger constituent of the same type, which can then be combined with the
subject NP via backward function application.
These examples illustrate the lexical nature of the categorial grammar approach.
The grammatical facts about a language are largely encoded in the lexicon, while the
rules of the grammar are boiled down to a set of three rules. Unfortunately, the basic
categorial approach does not give us any more expressive power than we had with
traditional CFG rules; it just moves information from the grammar to the lexicon. To
move beyond these limitations CCG includes operations that operate over functions.
The ﬁrst pair of operators permit us to compose adjacent functions.
X/Y Y/Z ⇒X/Z
(12.7)
Y\Z X\Y ⇒X\Z
(12.8)
The ﬁrst rule, called forward composition, can be applied to adjacent con-
forward
composition
stituents where the ﬁrst is a function seeking an argument of type Y to its right, and
the second is a function that providesY as a result. This rule allows us to compose
these two functions into a single one with the type of the ﬁrst constituent and the
argument of the second. Although the notation is a little awkward, the second rule,
backward composition is the same, except that we’re looking to the left instead of
backward
composition
to the right for the relevant arguments. Both kinds of composition are signalled by a
B in CCG diagrams, accompanied by a < or > to indicate the direction.
The next operator is type raising. Type raising elevates simple categories to the
type raising
status of functions. More speciﬁcally, type raising takes a category and converts
it to function that seeks as an argument a function that takes the original category

226
CHAPTER 12
•
CONSTITUENCY GRAMMARS
as its argument. The following schema show two versions of type raising: one for
arguments to the right, and one for the left.
X ⇒T/(T\X)
(12.9)
X ⇒T\(T/X)
(12.10)
The category T in these rules can correspond to any of the atomic or functional
categories already present in the grammar.
A particularly useful example of type raising transforms a simple NP argument
in subject position to a function that can compose with a following VP. To see how
this works, let’s revisit our earlier example of United serves Miami. Instead of clas-
sifying United as an NP which can serve as an argument to the function attached to
serve, we can use type raising to reinvent it as a function in its own right as follows.
NP ⇒S/(S\NP)
Combining this type-raised constituent with the forward composition rule (12.7)
permits the following alternative to our previous derivation.
United
serves
Miami
NP
(S\NP)/NP
NP
>T
S/(S\NP)
>B
S/NP
>
S
By type raising United to S/(S\NP), we can compose it with the transitive verb
serves to yield the (S/NP) function needed to complete the derivation.
There are several interesting things to note about this derivation. First, is it
provides a left-to-right, word-by-word derivation that more closely mirrors the way
humans process language. This makes CCG a particularly apt framework for psy-
cholinguistic studies. Second, this derivation involves the use of an intermediate
unit of analysis, United serves, that does not correspond to a traditional constituent
in English. This ability to make use of such non-constituent elements provides CCG
with the ability to handle the coordination of phrases that are not proper constituents,
as in the following example.
(12.11) We ﬂew IcelandAir to Geneva and SwissAir to London.
Here, the segments that are being coordinated are IcelandAir to Geneva and
SwissAir to London, phrases that would not normally be considered constituents, as
can be seen in the following standard derivation for the verb phrase ﬂew IcelandAir
to Geneva.
ﬂew
IcelandAir
to
Geneva
(VP/PP)/NP
NP
PP/NP
NP
>
>
VP/PP
PP
>
VP
In this derivation, there is no single constituent that corresponds to IcelandAir
to Geneva, and hence no opportunity to make use of the <Φ> operator. Note that
complex CCG categories can can get a little cumbersome, so we’ll use VP as a
shorthand for (S\NP) in this and the following derivations.
The following alternative derivation provides the required element through the
use of both backward type raising (12.10) and backward function composition (12.8).

12.6
•
LEXICALIZED GRAMMARS
227
ﬂew
IcelandAir
to
Geneva
(VP/PP)/NP
NP
PP/NP
NP
<T
>
(VP/PP)\((VP/PP)/NP)
PP
<T
VP\(VP/PP)
<B
VP\((VP/PP)/NP)
<
VP
Applying the same analysis to SwissAir to London satisﬁes the requirements
for the <Φ> operator, yielding the following derivation for our original example
(12.11).
ﬂew
IcelandAir
to
Geneva
and
SwissAir
to
London
(VP/PP)/NP
NP
PP/NP
NP
CONJ
NP
PP/NP
NP
<T
>
<T
>
(VP/PP)\((VP/PP)/NP)
PP
(VP/PP)\((VP/PP)/NP)
PP
<T
<T
VP\(VP/PP)
VP\(VP/PP)
<
<
VP\((VP/PP)/NP)
VP\((VP/PP)/NP)
<Φ>
VP\((VP/PP)/NP)
<
VP
Finally, let’s examine how these advanced operators can be used to handle long-
distance dependencies (also referred to as syntactic movement or extraction). As
mentioned in Section 12.3.1, long-distance dependencies arise from many English
constructions including wh-questions, relative clauses, and topicalization. What
these constructions have in common is a constituent that appears somewhere dis-
tant from its usual, or expected, location. Consider the following relative clause as
an example.
the ﬂight that United diverted
Here, divert is a transitive verb that expects two NP arguments, a subject NP to its
left and a direct object NP to its right; its category is therefore (S\NP)/NP. However,
in this example the direct object the ﬂight has been “moved” to the beginning of the
clause, while the subject United remains in its normal position. What is needed is a
way to incorporate the subject argument, while dealing with the fact that the ﬂight is
not in its expected location.
The following derivation accomplishes this, again through the combined use of
type raising and function composition.
the
ﬂight
that
United
diverted
NP/N
N
(NP\NP)/(S/NP)
NP
(S\NP)/NP
>
>T
NP
S/(S\NP)
>B
S/NP
>
NP\NP
<
NP
As we saw with our earlier examples, the ﬁrst step of this derivation is type raising
United to the category S/(S\NP) allowing it to combine with diverted via forward
composition. The result of this composition is S/NP which preserves the fact that we
are still looking for an NP to ﬁll the missing direct object. The second critical piece
is the lexical category assigned to the word that: (NP\NP)/(S/NP). This function
seeks a verb phrase missing an argument to its right, and transforms it into an NP
seeking a missing element to its left, precisely where we ﬁnd the ﬂight.

228
CHAPTER 12
•
CONSTITUENCY GRAMMARS
CCGBank
As with phrase-structure approaches, treebanks play an important role in CCG-
based approaches to parsing. CCGBank (Hockenmaier and Steedman, 2007) is the
largest and most widely used CCG treebank. It was created by automatically trans-
lating phrase-structure trees from the Penn Treebank via a rule-based approach. The
method produced successful translations of over 99% of the trees in the Penn Tree-
bank resulting in 48,934 sentences paired with CCG derivations. It also provides
a lexicon of 44,000 words with over 1200 categories. Chapter 14 will discuss how
these resources can be used to train CCG parsers.
12.7
Summary
This chapter has introduced a number of fundamental concepts in syntax through
the use of context-free grammars.
• In many languages, groups of consecutive words act as a group or a con-
stituent, which can be modeled by context-free grammars (which are also
known as phrase-structure grammars).
• A context-free grammar consists of a set of rules or productions, expressed
over a set of non-terminal symbols and a set of terminal symbols. Formally,
a particular context-free language is the set of strings that can be derived
from a particular context-free grammar.
• A generative grammar is a traditional name in linguistics for a formal lan-
guage that is used to model the grammar of a natural language.
• There are many sentence-level grammatical constructions in English; declar-
ative, imperative, yes-no question, and wh-question are four common types;
these can be modeled with context-free rules.
• An English noun phrase can have determiners, numbers, quantiﬁers, and
adjective phrases preceding the head noun, which can be followed by a num-
ber of postmodiﬁers; gerundive VPs, inﬁnitives VPs, and past participial
VPs are common possibilities.
• Subjects in English agree with the main verb in person and number.
• Verbs can be subcategorized by the types of complements they expect. Sim-
ple subcategories are transitive and intransitive; most grammars include
many more categories than these.
• Treebanks of parsed sentences exist for many genres of English and for many
languages. Treebanks can be searched with tree-search tools.
• Any context-free grammar can be converted to Chomsky normal form, in
which the right-hand side of each rule has either two non-terminals or a single
terminal.
• Lexicalized grammars place more emphasis on the structure of the lexicon,
lessening the burden on pure phrase-structure rules.
• Combinatorial categorial grammar (CCG) is an important computationally
relevant lexicalized approach.

BIBLIOGRAPHICAL AND HISTORICAL NOTES
229
Bibliographical and Historical Notes
[The origin of the idea of phrasal constituency, cited in Percival (1976)]:
...den sprachlichen Ausdruck f¨ur die willk¨urliche
Gliederung einer Gesammtvorstellung in ihre
in logische Beziehung zueinander gesetzten Bestandteile
[the linguistic expression for the arbitrary division of a total idea
into its constituent parts placed in logical relations to one another]
W. Wundt
According to Percival (1976), the idea of breaking up a sentence into a hierar-
chy of constituents appeared in the V¨olkerpsychologie of the groundbreaking psy-
chologist Wilhelm Wundt (Wundt, 1900). Wundt’s idea of constituency was taken
up into linguistics by Leonard Bloomﬁeld in his early book An Introduction to the
Study of Language (Bloomﬁeld, 1914). By the time of his later book, Language
(Bloomﬁeld, 1933), what was then called “immediate-constituent analysis” was a
well-established method of syntactic study in the United States. By contrast, tra-
ditional European grammar, dating from the Classical period, deﬁned relations be-
tween words rather than constituents, and European syntacticians retained this em-
phasis on such dependency grammars, the subject of Chapter 15.
American Structuralism saw a number of speciﬁc deﬁnitions of the immediate
constituent, couched in terms of their search for a “discovery procedure”: a method-
ological algorithm for describing the syntax of a language. In general, these attempt
to capture the intuition that “The primary criterion of the immediate constituent is the
degree in which combinations behave as simple units” (Bazell, 1966, p. 284). The
most well known of the speciﬁc deﬁnitions is Harris’ idea of distributional similarity
to individual units, with the substitutability test. Essentially, the method proceeded
by breaking up a construction into constituents by attempting to substitute simple
structures for possible constituents—if a substitution of a simple form, say, man,
was substitutable in a construction for a more complex set (like intense young man),
then the form intense young man was probably a constituent. Harris’s test was the
beginning of the intuition that a constituent is a kind of equivalence class.
The ﬁrst formalization of this idea of hierarchical constituency was the phrase-
structure grammar deﬁned in Chomsky (1956) and further expanded upon (and
argued against) in Chomsky (1957) and Chomsky (1975). From this time on, most
generative linguistic theories were based at least in part on context-free grammars or
generalizations of them (such as Head-Driven Phrase Structure Grammar (Pollard
and Sag, 1994), Lexical-Functional Grammar (Bresnan, 1982), Government and
Binding (Chomsky, 1981), and Construction Grammar (Kay and Fillmore, 1999),
inter alia); many of these theories used schematic context-free templates known as
X-bar schemata, which also relied on the notion of syntactic head.
X-bar
schemata
Shortly after Chomsky’s initial work, the context-free grammar was reinvented
by Backus (1959) and independently by Naur et al. (1960) in their descriptions of
the ALGOL programming language; Backus (1996) noted that he was inﬂuenced by
the productions of Emil Post and that Naur’s work was independent of his (Backus’)
own. (Recall the discussion on page ?? of multiple invention in science.) After this
early work, a great number of computational models of natural language processing
were based on context-free grammars because of the early development of efﬁcient
algorithms to parse these grammars (see Chapter 13).

230
CHAPTER 12
•
CONSTITUENCY GRAMMARS
As we have already noted, grammars based on context-free rules are not ubiqui-
tous. Various classes of extensions to CFGs are designed speciﬁcally to handle long-
distance dependencies. We noted earlier that some grammars treat long-distance-
dependent items as being related semantically but not syntactically; the surface syn-
tax does not represent the long-distance link (Kay and Fillmore 1999, Culicover and
Jackendoff 2005). But there are alternatives.
One extended formalism is Tree Adjoining Grammar (TAG) (Joshi, 1985).
The primary TAG data structure is the tree, rather than the rule. Trees come in two
kinds: initial trees and auxiliary trees. Initial trees might, for example, represent
simple sentential structures, and auxiliary trees add recursion into a tree. Trees are
combined by two operations called substitution and adjunction. The adjunction
operation handles long-distance dependencies. See Joshi (1985) for more details.
An extension of Tree Adjoining Grammar, called Lexicalized Tree Adjoining Gram-
mars is discussed in Chapter 14. Tree Adjoining Grammar is a member of the family
of mildly context-sensitive languages.
We mentioned on page 217 another way of handling long-distance dependencies,
based on the use of empty categories and co-indexing. The Penn Treebank uses
this model, which draws (in various Treebank corpora) from the Extended Standard
Theory and Minimalism (Radford, 1997).
Readers interested in the grammar of English should get one of the three large
reference grammars of English: Huddleston and Pullum (2002), Biber et al. (1999),
and Quirk et al. (1985). Another useful reference is McCawley (1998).
There are many good introductory textbooks on syntax from different perspec-
tives. Sag et al. (2003) is an introduction to syntax from a generative perspective,
generative
focusing on the use of phrase-structure rules, uniﬁcation, and the type hierarchy in
Head-Driven Phrase Structure Grammar. Van Valin, Jr. and La Polla (1997) is an
introduction from a functional perspective, focusing on cross-linguistic data and on
functional
the functional motivation for syntactic structures.
Exercises
12.1 Draw tree structures for the following ATIS phrases:
1. Dallas
2. from Denver
3. after ﬁve p.m.
4. arriving in Washington
5. early ﬂights
6. all redeye ﬂights
7. on Thursday
8. a one-way fare
9. any delays in Denver
12.2 Draw tree structures for the following ATIS sentences:
1. Does American Airlines have a ﬂight between ﬁve a.m. and six a.m.?
2. I would like to ﬂy on American Airlines.
3. Please repeat that.
4. Does American 487 have a ﬁrst-class section?
5. I need to ﬂy between Philadelphia and Atlanta.
6. What is the fare from Atlanta to Denver?

EXERCISES
231
7. Is there an American Airlines ﬂight from Philadelphia to Dallas?
12.3 Assume a grammar that has many VP rules for different subcategorizations,
as expressed in Section 12.3.4, and differently subcategorized verb rules like
Verb-with-NP-complement. How would the rule for postnominal relative clauses
(12.4) need to be modiﬁed if we wanted to deal properly with examples like
the earliest ﬂight that you have? Recall that in such examples the pronoun
that is the object of the verb get. Your rules should allow this noun phrase but
should correctly rule out the ungrammatical S *I get.
12.4 Does your solution to the previous problem correctly model the NP the earliest
ﬂight that I can get? How about the earliest ﬂight that I think my mother
wants me to book for her? Hint: this phenomenon is called long-distance
dependency.
12.5 Write rules expressing the verbal subcategory of English auxiliaries; for ex-
ample, you might have a rule verb-with-bare-stem-VP-complement →can.
12.6 NPs like Fortune’s ofﬁce or my uncle’s marks are called possessive or genitive
possessive
genitive
noun phrases. We can model possessive noun phrases by treating the sub-NP
like Fortune’s or my uncle’s as a determiner of the following head noun. Write
grammar rules for English possessives. You may treat ’s as if it were a separate
word (i.e., as if there were always a space before ’s).
12.7 Page 210 discussed the need for a Wh-NP constituent. The simplest Wh-NP
is one of the Wh-pronouns (who, whom, whose, which). The Wh-words what
and which can be determiners: which four will you have?, what credit do you
have with the Duke? Write rules for the different types of Wh-NPs.
12.8 Write an algorithm for converting an arbitrary context-free grammar into Chom-
sky normal form.

232
CHAPTER 13
•
CONSTITUENCY PARSING
CHAPTER
13
Constituency Parsing
One morning I shot an elephant in my pajamas.
How he got into my pajamas I don’t know.
Groucho Marx, Animal Crackers, 1930
Syntactic parsing is the task of recognizing a sentence and assigning a syntactic
structure to it. This chapter focuses on the structures assigned by context-free gram-
mars of the kind described in Chapter 12. Since they are based on a purely declar-
ative formalism, context-free grammars don’t specify how the parse tree for a given
sentence should be computed. We therefore need to specify algorithms that employ
these grammars to efﬁciently produce correct trees.
Parse trees are directly useful in applications such as grammar checking in
word-processing systems: a sentence that cannot be parsed may have grammatical
errors (or at least be hard to read). More typically, however, parse trees serve as an
important intermediate stage of representation for semantic analysis (as we show in
Chapter 17) and thus play an important role in applications like question answering
and information extraction. For example, to answer the question
What books were written by British women authors before 1800?
we’ll need to know that the subject of the sentence was what books and that the by-
adjunct was British women authors to help us ﬁgure out that the user wants a list of
books (and not a list of authors).
Before presenting any algorithms, we begin by discussing how the ambiguity
arises again in this context and the problems it presents.
The section that fol-
lows then presents the Cocke-Kasami-Younger (CKY) algorithm (Kasami 1965,
Younger 1967), the standard dynamic programming approach to syntactic parsing.
Recall that we’ve already seen applications of dynamic programming algorithms in
the Minimum-Edit-Distance and Viterbi algorithms of earlier chapters. Finally, we
discuss partial parsing methods, for use in situations in which a superﬁcial syntac-
tic analysis of an input may be sufﬁcient.
13.1
Ambiguity
Ambiguity is perhaps the most serious problem faced by syntactic parsers. Chap-
ter 8 introduced the notions of part-of-speech ambiguity and part-of-speech dis-
ambiguation. Here, we introduce a new kind of ambiguity, called structural ambi-
guity, which arises from many commonly used rules in phrase-structure grammars.
structural
ambiguity
To illustrate the issues associated with structural ambiguity, we’ll make use of a new
toy grammar L1, shown in Figure 13.1, which consists of the L0 grammar from the
last chapter augmented with a few additional rules.
Structural ambiguity occurs when the grammar can assign more than one parse
to a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal

13.1
•
AMBIGUITY
233
Grammar
Lexicon
S →NP VP
Det →that | this | the | a
S →Aux NP VP
Noun →book | ﬂight | meal | money
S →VP
Verb →book | include | prefer
NP →Pronoun
Pronoun →I | she | me
NP →Proper-Noun
Proper-Noun →Houston | NWA
NP →Det Nominal
Aux →does
Nominal →Noun
Preposition →from | to | on | near | through
Nominal →Nominal Noun
Nominal →Nominal PP
VP →Verb
VP →Verb NP
VP →Verb NP PP
VP →Verb PP
VP →VP PP
PP →Preposition NP
Figure 13.1
The L1 miniature English grammar and lexicon.
S
VP
NP
Nominal
PP
in my pajamas
Nominal
Noun
elephant
Det
an
Verb
shot
NP
Pronoun
I
S
VP
PP
in my pajamas
VP
NP
Nominal
Noun
elephant
Det
an
Verb
shot
NP
Pronoun
I
Figure 13.2
Two parse trees for an ambiguous sentence. The parse on the left corresponds to the humorous
reading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which
Captain Spaulding did the shooting in his pajamas.
Crackers is ambiguous because the phrase in my pajamas can be part of the NP
headed by elephant or a part of the verb phrase headed by shot. Figure 13.2 illus-
trates these two analyses of Marx’s line using rules from L1.
Structural ambiguity, appropriately enough, comes in many forms. Two common
kinds of ambiguity are attachment ambiguity and coordination ambiguity.
A sentence has an attachment ambiguity if a particular constituent can be at-
attachment
ambiguity
tached to the parse tree at more than one place. The Groucho Marx sentence is
an example of PP-attachment ambiguity. Various kinds of adverbial phrases are
also subject to this kind of ambiguity. For instance, in the following example the
gerundive-VP ﬂying to Paris can be part of a gerundive sentence whose subject is
the Eiffel Tower or it can be an adjunct modifying the VP headed by saw:
(13.1) We saw the Eiffel Tower ﬂying to Paris.

234
CHAPTER 13
•
CONSTITUENCY PARSING
In coordination ambiguity different sets of phrases can be conjoined by a con-
coordination
ambiguity
junction like and. For example, the phrase old men and women can be bracketed as
[old [men and women]], referring to old men and old women, or as [old men] and
[women], in which case it is only the men who are old.
These ambiguities combine in complex ways in real sentences. A program that
summarized the news, for example, would need to be able to parse sentences like
the following from the Brown corpus:
(13.2) President Kennedy today pushed aside other White House business to
devote all his time and attention to working on the Berlin crisis address he
will deliver tomorrow night to the American people over nationwide
television and radio.
This sentence has a number of ambiguities, although since they are semantically
unreasonable, it requires a careful reading to see them. The last noun phrase could be
parsed [nationwide [television and radio]] or [[nationwide television] and radio].
The direct object of pushed aside should be other White House business but could
also be the bizarre phrase [other White House business to devote all his time and
attention to working] (i.e., a structure like Kennedy afﬁrmed [his intention to propose
a new budget to address the deﬁcit]). Then the phrase on the Berlin crisis address he
will deliver tomorrow night to the American people could be an adjunct modifying
the verb pushed. A PP like over nationwide television and radio could be attached
to any of the higher VPs or NPs (e.g., it could modify people or night).
The fact that there are many grammatically correct but semantically unreason-
able parses for naturally occurring sentences is an irksome problem that affects all
parsers. Ultimately, most natural language processing systems need to be able to
choose a single correct parse from the multitude of possible parses through a process
of syntactic disambiguation. Effective disambiguation algorithms require statisti-
syntactic
disambiguation
cal, semantic, and contextual knowledge sources that vary in how well they can be
integrated into parsing algorithms.
Fortunately, the CKY algorithm presented in the next section is designed to efﬁ-
ciently handle structural ambiguities of the kind we’ve been discussing. And as we’ll
see in Chapter 14, there are straightforward ways to integrate statistical techniques
into the basic CKY framework to produce highly accurate parsers.
13.2
CKY Parsing: A Dynamic Programming Approach
The previous section introduced some of the problems associated with ambiguous
grammars. Fortunately, dynamic programming provides a powerful framework for
addressing these problems, just as it did with the Minimum Edit Distance, Viterbi,
and Forward algorithms. Recall that dynamic programming approaches systemati-
cally ﬁll in tables of solutions to sub-problems. When complete, the tables contain
the solution to all the sub-problems needed to solve the problem as a whole. In
the case of syntactic parsing, these sub-problems represent parse trees for all the
constituents detected in the input.
The dynamic programming advantage arises from the context-free nature of our
grammar rules — once a constituent has been discovered in a segment of the input
we can record its presence and make it available for use in any subsequent derivation
that might require it. This provides both time and storage efﬁciencies since subtrees
can be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-

13.2
•
CKY PARSING: A DYNAMIC PROGRAMMING APPROACH
235
Younger (CKY) algorithm, the most widely used dynamic-programming based ap-
proach to parsing. Related approaches include the Earley algorithm (Earley, 1970)
and chart parsing (Kaplan 1973, Kay 1982).
13.2.1
Conversion to Chomsky Normal Form
We begin our investigation of the CKY algorithm by examining the requirement
that grammars used with it must be in Chomsky Normal Form (CNF). Recall from
Chapter 12 that grammars in CNF are restricted to rules of the form A →B C or
A →w. That is, the right-hand side of each rule must expand either to two non-
terminals or to a single terminal. Restricting a grammar to CNF does not lead to
any loss in expressiveness, since any context-free grammar can be converted into
a corresponding CNF grammar that accepts exactly the same set of strings as the
original grammar.
Let’s start with the process of converting a generic CFG into one represented in
CNF. Assuming we’re dealing with an ϵ-free grammar, there are three situations we
need to address in any generic grammar: rules that mix terminals with non-terminals
on the right-hand side, rules that have a single non-terminal on the right-hand side,
and rules in which the length of the right-hand side is greater than 2.
The remedy for rules that mix terminals and non-terminals is to simply introduce
a new dummy non-terminal that covers only the original terminal. For example, a
rule for an inﬁnitive verb phrase such as INF-VP →to VP would be replaced by the
two rules INF-VP →TO VP and TO →to.
Rules with a single non-terminal on the right are called unit productions. We
Unit
productions
can eliminate unit productions by rewriting the right-hand side of the original rules
with the right-hand side of all the non-unit production rules that they ultimately lead
to. More formally, if A ∗⇒B by a chain of one or more unit productions and B →γ
is a non-unit production in our grammar, then we add A →γ for each such rule in
the grammar and discard all the intervening unit productions. As we demonstrate
with our toy grammar, this can lead to a substantial ﬂattening of the grammar and a
consequent promotion of terminals to fairly high levels in the resulting trees.
Rules with right-hand sides longer than 2 are normalized through the introduc-
tion of new non-terminals that spread the longer sequences over several new rules.
Formally, if we have a rule like
A →B C γ
we replace the leftmost pair of non-terminals with a new non-terminal and introduce
a new production result in the following new rules:
A →X1 γ
X1 →B C
In the case of longer right-hand sides, we simply iterate this process until the of-
fending rule has been replaced by rules of length 2. The choice of replacing the
leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results
in binary rules would sufﬁce.
In our current grammar, the rule S →Aux NP VP would be replaced by the two
rules S →X1 VP and X1 →Aux NP.
The entire conversion process can be summarized as follows:
1. Copy all conforming rules to the new grammar unchanged.

236
CHAPTER 13
•
CONSTITUENCY PARSING
L1 Grammar
L1 in CNF
S →NP VP
S →NP VP
S →Aux NP VP
S →X1 VP
X1 →Aux NP
S →VP
S →book | include | prefer
S →Verb NP
S →X2 PP
S →Verb PP
S →VP PP
NP →Pronoun
NP →I | she | me
NP →Proper-Noun
NP →TWA | Houston
NP →Det Nominal
NP →Det Nominal
Nominal →Noun
Nominal →book | ﬂight | meal | money
Nominal →Nominal Noun
Nominal →Nominal Noun
Nominal →Nominal PP
Nominal →Nominal PP
VP →Verb
VP →book | include | prefer
VP →Verb NP
VP →Verb NP
VP →Verb NP PP
VP →X2 PP
X2 →Verb NP
VP →Verb PP
VP →Verb PP
VP →VP PP
VP →VP PP
PP →Preposition NP
PP →Preposition NP
Figure 13.3
L1 Grammar and its conversion to CNF. Note that although they aren’t shown
here, all the original lexical entries from L1 carry over unchanged as well.
2. Convert terminals within rules to dummy non-terminals.
3. Convert unit productions.
4. Make all rules binary and add them to new grammar.
Figure 13.3 shows the results of applying this entire conversion procedure to
the L1 grammar introduced earlier on page 233. Note that this ﬁgure doesn’t show
the original lexical rules; since these original lexical rules are already in CNF, they
all carry over unchanged to the new grammar. Figure 13.3 does, however, show
the various places where the process of eliminating unit productions has, in effect,
created new lexical rules. For example, all the original verbs have been promoted to
both VPs and to Ss in the converted grammar.
13.2.2
CKY Recognition
With our grammar now in CNF, each non-terminal node above the part-of-speech
level in a parse tree will have exactly two daughters. A two-dimensional matrix can
be used to encode the structure of an entire tree. For a sentence of length n, we will
work with the upper-triangular portion of an (n+1)×(n+1) matrix. Each cell [i, j]
in this matrix contains the set of non-terminals that represent all the constituents that
span positions i through j of the input. Since our indexing scheme begins with 0,
it’s natural to think of the indexes as pointing at the gaps between the input words
(as in 0 Book 1 that 2 ﬂight 3). It follows then that the cell that represents the entire
input resides in position [0,n] in the matrix.
Since each non-terminal entry in our table has two daughters in the parse, it fol-
lows that for each constituent represented by an entry [i, j], there must be a position
in the input, k, where it can be split into two parts such that i < k < j. Given such

13.2
•
CKY PARSING: A DYNAMIC PROGRAMMING APPROACH
237
a position k, the ﬁrst constituent [i,k] must lie to the left of entry [i, j] somewhere
along row i, and the second entry [k, j] must lie beneath it, along column j.
To make this more concrete, consider the following example with its completed
parse matrix, shown in Fig. 13.4.
(13.3) Book the ﬂight through Houston.
The superdiagonal row in the matrix contains the parts of speech for each word in
the input. The subsequent diagonals above that superdiagonal contain constituents
that cover all the spans of increasing length in the input.
Book
the 
flight 
through
Houston
S, VP, Verb, 
Nominal, 
Noun
S,VP,X2
S,VP,X2
Det
NP
NP
Nominal,
Noun
Nominal
Prep
PP
NP,
Proper-
Noun
[0,1]
[0,2]
[0,3]
[0,4]
[0,5]
[1,2]
[1,3]
[2,3]
[1,4]
[2,5]
[2,4]
[3,4]
[4,5]
[3,5]
[1,5]
Figure 13.4
Completed parse table for Book the ﬂight through Houston.
Given this setup, CKY recognition consists of ﬁlling the parse table in the right
way. To do this, we’ll proceed in a bottom-up fashion so that at the point where
we are ﬁlling any cell [i, j], the cells containing the parts that could contribute to
this entry (i.e., the cells to the left and the cells below) have already been ﬁlled.
The algorithm given in Fig. 13.5 ﬁlls the upper-triangular matrix a column at a time
working from left to right, with each column ﬁlled from bottom to top, as the right
side of Fig. 13.4 illustrates. This scheme guarantees that at each point in time we
have all the information we need (to the left, since all the columns to the left have
already been ﬁlled, and below since we’re ﬁlling bottom to top). It also mirrors on-
line parsing since ﬁlling the columns from left to right corresponds to processing
each word one at a time.
function CKY-PARSE(words, grammar) returns table
for j←from 1 to LENGTH(words) do
for all {A | A →words[j] ∈grammar}
table[j −1, j]←table[ j −1, j] ∪A
for i←from j −2 downto 0 do
for k←i+1 to j −1 do
for all {A | A →BC ∈grammar and B ∈table[i,k] and C ∈table[k, j]}
table[i,j]←table[i,j] ∪A
Figure 13.5
The CKY algorithm.

238
CHAPTER 13
•
CONSTITUENCY PARSING
...
...
[0,n]
[i,i+1]
[i,i+2]
[i,j-2]
[i,j-1]
[i+1,j]
[i+2,j]
[j-1,j]
[j-2,j]
[i,j]
...
[0,1]
[n-1, n]
Figure 13.6
All the ways to ﬁll the [i, j]th cell in the CKY table.
The outermost loop of the algorithm given in Fig. 13.5 iterates over the columns,
and the second loop iterates over the rows, from the bottom up. The purpose of the
innermost loop is to range over all the places where a substring spanning i to j in
the input might be split in two. As k ranges over the places where the string can be
split, the pairs of cells we consider move, in lockstep, to the right along row i and
down along column j. Figure 13.6 illustrates the general case of ﬁlling cell [i, j]. At
each such split, the algorithm considers whether the contents of the two cells can be
combined in a way that is sanctioned by a rule in the grammar. If such a rule exists,
the non-terminal on its left-hand side is entered into the table.
Figure 13.7 shows how the ﬁve cells of column 5 of the table are ﬁlled after the
word Houston is read. The arrows point out the two spans that are being used to add
an entry to the table. Note that the action in cell [0,5] indicates the presence of three
alternative parses for this input, one where the PP modiﬁes the ﬂight, one where
it modiﬁes the booking, and one that captures the second argument in the original
VP →Verb NP PP rule, now captured indirectly with the VP →X2 PP rule.
13.2.3
CKY Parsing
The algorithm given in Fig. 13.5 is a recognizer, not a parser; for it to succeed, it
simply has to ﬁnd an S in cell [0,n]. To turn it into a parser capable of returning all

13.2
•
CKY PARSING: A DYNAMIC PROGRAMMING APPROACH
239
Book
the 
flight 
through
Houston
S, VP, Verb, 
Nominal, 
Noun
S,VP,X2
Det
NP
Nominal,
Noun
Nominal
Prep
NP,
Proper-
Noun
[0,1]
[0,2]
[0,3]
[0,4]
[0,5]
[1,2]
[1,3]
[2,3]
[1,4]
[2,5]
[2,4]
[3,4]
[4,5]
[3,5]
[1,5]
Book
the 
flight 
through
Houston
S, VP, Verb, 
Nominal, 
Noun
S,VP,X2
Det
NP
NP
Nominal,
Noun
Prep
PP
NP,
Proper-
Noun
[0,1]
[0,2]
[0,3]
[0,4]
[0,5]
[1,2]
[1,3]
[2,3]
[1,4]
[2,5]
[2,4]
[3,4]
[4,5]
[3,5]
[1,5]
Book
the 
flight 
through
Houston
S, VP, Verb, 
Nominal, 
Noun
S,VP,X2
Det
NP
NP
Nominal,
Noun
Nominal
Prep
PP
NP,
Proper-
Noun
[0,1]
[0,2]
[0,3]
[0,4]
[0,5]
[1,2]
[1,3]
[2,3]
[1,4]
[2,5]
[2,4]
[3,4]
[4,5]
[3,5]
[1,5]
Book
the 
flight 
through
Houston
S, VP, Verb, 
Nominal, 
Noun
S,VP,X2
Det
NP
NP
Nominal,
Noun
Nominal
Prep
PP
NP,
Proper-
Noun
[0,1]
[0,2]
[0,3]
[0,4]
[0,5]
[1,2]
[1,3]
[2,3]
[1,4]
[2,5]
[2,4]
[3,4]
[4,5]
[3,5]
[1,5]
Book
the 
flight 
through
Houston
S, VP, Verb, 
Nominal, 
Noun
S,
VP,
X2
Det
NP
NP
Nominal,
Noun
Nominal
Prep
PP
NP,
Proper-
Noun
[0,1]
[0,2]
[0,3]
[0,4]
[1,2]
[1,3]
[2,3]
[1,4]
[2,5]
[2,4]
[3,4]
[4,5]
[3,5]
[1,5]
S2, VP
S3
S1,VP, X2
Figure 13.7
Filling the cells of column 5 after reading the word Houston.

240
CHAPTER 13
•
CONSTITUENCY PARSING
possible parses for a given input, we can make two simple changes to the algorithm:
the ﬁrst change is to augment the entries in the table so that each non-terminal is
paired with pointers to the table entries from which it was derived (more or less as
shown in Fig. 13.7), the second change is to permit multiple versions of the same
non-terminal to be entered into the table (again as shown in Fig. 13.7). With these
changes, the completed table contains all the possible parses for a given input. Re-
turning an arbitrary single parse consists of choosing an S from cell [0,n] and then
recursively retrieving its component constituents from the table.
Of course, returning all the parses for a given input may incur considerable cost
since an exponential number of parses may be associated with a given input. In such
cases, returning all the parses will have an unavoidable exponential cost. Looking
forward to Chapter 14, we can also think about retrieving the best parse for a given
input by further augmenting the table to contain the probabilities of each entry. Re-
trieving the most probable parse consists of running a suitably modiﬁed version of
the Viterbi algorithm from Chapter 8 over the completed parse table.
13.2.4
CKY in Practice
Finally, we should note that while the restriction to CNF does not pose a prob-
lem theoretically, it does pose some non-trivial problems in practice. Obviously, as
things stand now, our parser isn’t returning trees that are consistent with the grammar
given to us by our friendly syntacticians. In addition to making our grammar devel-
opers unhappy, the conversion to CNF will complicate any syntax-driven approach
to semantic analysis.
One approach to getting around these problems is to keep enough information
around to transform our trees back to the original grammar as a post-processing step
of the parse. This is trivial in the case of the transformation used for rules with length
greater than 2. Simply deleting the new dummy non-terminals and promoting their
daughters restores the original tree.
In the case of unit productions, it turns out to be more convenient to alter the ba-
sic CKY algorithm to handle them directly than it is to store the information needed
to recover the correct trees. Exercise 13.3 asks you to make this change. Many of
the probabilistic parsers presented in Chapter 14 use the CKY algorithm altered in
just this manner. Another solution is to adopt a more complex dynamic program-
ming solution that simply accepts arbitrary CFGs. The next section presents such an
approach.
13.3
Partial Parsing
Many language processing tasks do not require complex, complete parse trees for all
inputs. For these tasks, a partial parse, or shallow parse, of input sentences may
partial parse
shallow parse
be sufﬁcient. For example, information extraction systems generally do not extract
all the possible information from a text: they simply identify and classify the seg-
ments in a text that are likely to contain valuable information. Similarly, information
retrieval systems may index texts according to a subset of the constituents found in
them.
There are many different approaches to partial parsing.
Some make use of
cascades of ﬁnite state transducers to produce tree-like representations. These ap-
proaches typically produce ﬂatter trees than the ones we’ve been discussing in this

13.3
•
PARTIAL PARSING
241
chapter and the previous one. This ﬂatness arises from the fact that ﬁnite state trans-
ducer approaches generally defer decisions that may require semantic or contex-
tual factors, such as prepositional phrase attachments, coordination ambiguities, and
nominal compound analyses. Nevertheless, the intent is to produce parse trees that
link all the major constituents in an input.
An alternative style of partial parsing is known as chunking. Chunking is the
chunking
process of identifying and classifying the ﬂat, non-overlapping segments of a sen-
tence that constitute the basic non-recursive phrases corresponding to the major
content-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and
prepositional phrases. The task of ﬁnding all the base noun phrases in a text is
particularly common. Since chunked texts lack a hierarchical structure, a simple
bracketing notation is sufﬁcient to denote the location and the type of the chunks in
a given example:
(13.4) [NP The morning ﬂight] [PP from] [NP Denver] [VP has arrived.]
This bracketing notation makes clear the two fundamental tasks that are involved
in chunking: segmenting (ﬁnding the non-overlapping extents of the chunks) and
labeling (assigning the correct tag to the discovered chunks).
Some input words may not be part of any chunk, particularly in tasks like base
NP:
(13.5) [NP The morning ﬂight] from [NP Denver] has arrived.
What constitutes a syntactic base phrase depends on the application (and whether
the phrases come from a treebank). Nevertheless, some standard guidelines are fol-
lowed in most systems. First and foremost, base phrases of a given type do not
recursively contain any constituents of the same type. Eliminating this kind of recur-
sion leaves us with the problem of determining the boundaries of the non-recursive
phrases. In most approaches, base phrases include the headword of the phrase, along
with any pre-head material within the constituent, while crucially excluding any
post-head material. Eliminating post-head modiﬁers obviates the need to resolve at-
tachment ambiguities. This exclusion does lead to certain oddities, such as PPs and
VPs often consisting solely of their heads. Thus, our earlier example a ﬂight from
Indianapolis to Houston on NWA is reduced to the following:
(13.6) [NP a ﬂight] [PP from] [NP Indianapolis][PP to][NP Houston][PP on][NP
NWA]
13.3.1
Machine Learning-Based Approaches to Chunking
State-of-the-art approaches to chunking use supervised machine learning to train a
chunker by using annotated data as a training set and training any sequence labeler.
It’s common to model chunking as IOB tagging. In IOB tagging we introduce a tag
IOB
for the beginning (B) and inside (I) of each chunk type, and one for tokens outside
(O) any chunk. The number of tags is thus 2n + 1 tags, where n is the number
of chunk types. IOB tagging can represent exactly the same information as the
bracketed notation. The following example shows the bracketing notation of (13.4)
on page 241 reframed as a tagging task:
(13.7) The
B NP
morning
I NP
ﬂight
I NP
from
B PP
Denver
B NP
has
B VP
arrived
I VP
The same sentence with only the base-NPs tagged illustrates the role of the O tags.

242
CHAPTER 13
•
CONSTITUENCY PARSING
B_NP
I_NP
?
 
    
The
flight
from
Denver
has
arrived
 
Classifier
DT
NN
NN
IN
NNP
Corresponding feature representation
The, DT, B_NP, morning, NN, I_NP, flight, NN, from, IN, Denver, NNP
Label
I_NP
morning
Figure 13.8
A sequence model for chunking. The chunker slides a context window over the sentence, clas-
sifying words as it proceeds. At this point, the classiﬁer is attempting to label ﬂight, using features like words,
embeddings, part-of-speech tags and previously assigned chunk tags.
(13.8) The
B NP
morning
I NP
ﬂight
I NP
from
O
Denver
B NP
has
O
arrived.
O
There is no explicit encoding of the end of a chunk in IOB tagging; the end of any
chunk is implicit in any transition from an I or B to a B or O tag. This encoding
reﬂects the notion that when sequentially labeling words, it is generally easier (at
least in English) to detect the beginning of a new chunk than it is to know when a
chunk has ended.
Since annotation efforts are expensive and time consuming, chunkers usually
rely on existing treebanks like the Penn Treebank (Chapter 12), extracting syntactic
phrases from the full parse constituents of a sentence, ﬁnding the appropriate heads
and then including the material to the left of the head, ignoring the text to the right.
This is somewhat error-prone since it relies on the accuracy of the head-ﬁnding rules
described in Chapter 12.
Given a training set, any sequence model can be used. Figure 13.8 shows an
illustration of a simple feature-based model, using features like the words and parts-
of-speech within a 2 word window, and the chunk tags of the preceding inputs in the
window. In training, each training vector would consist of the values of 13 features;
the two words to the left of the decision point, their parts-of-speech and chunk tags,
the word to be tagged along with its part-of-speech, the two words that follow along
with their parts-of speech, and the correct chunk tag, in this case, I NP. During
classiﬁcation, the classiﬁer is given the same vector without the answer and assigns
the most appropriate tag from its tagset. Viterbi decoding is commonly used.
13.3.2
Chunking-System Evaluations
As with the evaluation of part-of-speech taggers, the evaluation of chunkers pro-
ceeds by comparing chunker output with gold-standard answers provided by human
annotators. However, unlike part-of-speech tagging, word-by-word accuracy mea-
sures are not appropriate. Instead, chunkers are evaluated according to the notions of

13.4
•
SUMMARY
243
precision, recall, and the F-measure that we saw in text classiﬁcation in Chapter 4,
repeated here as a quick refresher.
Precision measures the percentage of system-provided chunks that were correct.
precision
Correct here means that both the boundaries of the chunk and the chunk’s label are
correct. Precision is therefore deﬁned as
Precision: = Number of correct chunks given by system
Total number of chunks given by system
Recall measures the percentage of chunks actually present in the input that were
recall
correctly identiﬁed by the system. Recall is deﬁned as
Recall: = Number of correct chunks given by system
Total number of actual chunks in the text
The F-measure (van Rijsbergen, 1975) provides a way to combine these two
F-measure
measures into a single metric:
Fβ = (β 2 +1)PR
β 2P+R
The β parameter differentially weights the importance of recall and precision, based
perhaps on the needs of an application. Values of β > 1 favor recall, while values of
β < 1 favor precision. When β = 1, precision and recall are equally balanced; this
is sometimes called Fβ=1 or just F1:
F1 = 2PR
P+R
(13.9)
13.4
Summary
The two major ideas introduced in this chapter are those of parsing and partial
parsing. Here’s a summary of the main points we covered about these ideas:
• Structural ambiguity is a signiﬁcant problem for parsers. Common sources
of structural ambiguity include PP-attachment, coordination ambiguity,
and noun-phrase bracketing ambiguity.
• Dynamic programming parsing algorithms, such as CKY, use a table of
partial parses to efﬁciently parse ambiguous sentences.
• CKY restricts the form of the grammar to Chomsky normal form (CNF).
• Many practical problems, including information extraction problems, can be
solved without full parsing.
• Partial parsing and chunking are methods for identifying shallow syntactic
constituents in a text.
• State-of-the-art methods for partial parsing use supervised machine learning
techniques.
Bibliographical and Historical Notes
Writing about the history of compilers, Knuth notes:

244
CHAPTER 13
•
CONSTITUENCY PARSING
In this ﬁeld there has been an unusual amount of parallel discovery of
the same technique by people working independently.
Well, perhaps not unusual, since multiple discovery is the norm in science (see
page ??). But there has certainly been enough parallel publication that this his-
tory errs on the side of succinctness in giving only a characteristic early mention of
each algorithm; the interested reader should see Aho and Ullman (1972).
Bottom-up parsing seems to have been ﬁrst described by Yngve (1955), who
gave a breadth-ﬁrst, bottom-up parsing algorithm as part of an illustration of a ma-
chine translation procedure. Top-down approaches to parsing and translation were
described (presumably independently) by at least Glennie (1960), Irons (1961), and
Kuno and Oettinger (1963). Dynamic programming parsing, once again, has a his-
tory of independent discovery. According to Martin Kay (personal communica-
tion), a dynamic programming parser containing the roots of the CKY algorithm
was ﬁrst implemented by John Cocke in 1960. Later work extended and formalized
the algorithm, as well as proving its time complexity (Kay 1967, Younger 1967,
Kasami 1965). The related well-formed substring table (WFST) seems to have
WFST
been independently proposed by Kuno (1965) as a data structure that stores the re-
sults of all previous computations in the course of the parse. Based on a general-
ization of Cocke’s work, a similar data structure had been independently described
in Kay (1967) (and Kay 1973). The top-down application of dynamic programming
to parsing was described in Earley’s Ph.D. dissertation (Earley 1968, Earley 1970).
Sheil (1976) showed the equivalence of the WFST and the Earley algorithm. Norvig
(1991) shows that the efﬁciency offered by dynamic programming can be captured
in any language with a memoization function (such as in LISP) simply by wrapping
the memoization operation around a simple top-down parser.
While parsing via cascades of ﬁnite-state automata had been common in the
early history of parsing (Harris, 1962), the focus shifted to full CFG parsing quite
soon afterward. Church (1980) argued for a return to ﬁnite-state grammars as a
processing model for natural language understanding; other early ﬁnite-state parsing
models include Ejerhed (1988). Abney (1991) argued for the important practical role
of shallow parsing.
The classic reference for parsing algorithms is Aho and Ullman (1972); although
the focus of that book is on computer languages, most of the algorithms have been
applied to natural language. A good programming languages textbook such as Aho
et al. (1986) is also useful.
Exercises
13.1 Implement the algorithm to convert arbitrary context-free grammars to CNF.
Apply your program to the L1 grammar.
13.2 Implement the CKY algorithm and test it with your converted L1 grammar.
13.3 Rewrite the CKY algorithm given in Fig. 13.5 on page 237 so that it can accept
grammars that contain unit productions.
13.4 Discuss the relative advantages and disadvantages of partial versus full pars-
ing.

EXERCISES
245
13.5 Discuss how to augment a parser to deal with input that may be incorrect, for
example, containing spelling errors or mistakes arising from automatic speech
recognition.

246
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
CHAPTER
14
Statistical Constituency Pars-
ing
The characters in Damon Runyon’s short stories are willing to bet “on any propo-
sition whatever”, as Runyon says about Sky Masterson in The Idyll of Miss Sarah
Brown, from the probability of getting aces back-to-back to the odds against a man
being able to throw a peanut from second base to home plate. There is a moral
here for language processing: with enough knowledge we can ﬁgure the probability
of just about anything. The last two chapters have introduced models of syntactic
constituency structure and its parsing. Here, we show that it is possible to build
probabilistic models of syntactic knowledge and efﬁcient probabilistic parsers.
One crucial use of probabilistic parsing is to solve the problem of disambigua-
tion. Recall from Chapter 13 that sentences on average tend to be syntactically
ambiguous because of phenomena like coordination ambiguity and attachment
ambiguity. The CKY parsing algorithm can represent these ambiguities in an efﬁ-
cient way but is not equipped to resolve them. A probabilistic parser offers a solution
to the problem: compute the probability of each interpretation and choose the most
probable interpretation. The most commonly used probabilistic constituency gram-
mar formalism is the probabilistic context-free grammar (PCFG), a probabilistic
augmentation of context-free grammars in which each rule is associated with a prob-
ability. We introduce PCFGs in the next section, showing how they can be trained
on Treebank grammars and how they can be parsed with a probabilistic version of
the CKY algorithm of Chapter 13.
We then show a number of ways that we can improve on this basic probabil-
ity model (PCFGs trained on Treebank grammars), such as by modifying the set of
non-terminals (making them either more speciﬁc or more general), or adding more
sophisticated conditioning factors like subcategorization or dependencies. Heav-
ily lexicalized grammar formalisms such as Lexical-Functional Grammar (LFG)
(Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag,
1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory Categorial
Grammar (CCG) pose additional problems for probabilistic parsers. Section 14.7
introduces the task of supertagging and the use of heuristic search methods based
on the A* algorithm in the context of CCG parsing.
Finally, we describe the standard techniques and metrics for evaluating parsers.
14.1
Probabilistic Context-Free Grammars
The simplest augmentation of the context-free grammar is the Probabilistic Context-
Free Grammar (PCFG), also known as the Stochastic Context-Free Grammar
PCFG
(SCFG), ﬁrst proposed by Booth (1969). Recall that a context-free grammar G is
SCFG
deﬁned by four parameters (N, Σ, R, S); a probabilistic context-free grammar is also
deﬁned by four parameters, with a slight augmentation to each of the rules in R:

14.1
•
PROBABILISTIC CONTEXT-FREE GRAMMARS
247
N a set of non-terminal symbols (or variables)
Σ a set of terminal symbols (disjoint from N)
R a set of rules or productions, each of the form A →β [p],
where A is a non-terminal,
β is a string of symbols from the inﬁnite set of strings (Σ∪N)∗,
and p is a number between 0 and 1 expressing P(β|A)
S
a designated start symbol
That is, a PCFG differs from a standard CFG by augmenting each rule in R with
a conditional probability:
A →β [p]
(14.1)
Here p expresses the probability that the given non-terminal A will be expanded
to the sequence β. That is, p is the conditional probability of a given expansion β
given the left-hand-side (LHS) non-terminal A. We can represent this probability as
P(A →β)
or as
P(A →β|A)
or as
P(RHS|LHS)
Thus, if we consider all the possible expansions of a non-terminal, the sum of their
probabilities must be 1:
X
β
P(A →β) = 1
Figure 14.1 shows a PCFG: a probabilistic augmentation of the L1 miniature En-
glish CFG grammar and lexicon. Note that the probabilities of all of the expansions
of each non-terminal sum to 1. Also note that these probabilities were made up
for pedagogical purposes. A real grammar has a great many more rules for each
non-terminal; hence, the probabilities of any particular rule would tend to be much
smaller.
A PCFG is said to be consistent if the sum of the probabilities of all sentences
consistent
in the language equals 1. Certain kinds of recursive rules cause a grammar to be
inconsistent by causing inﬁnitely looping derivations for some sentences. For ex-
ample, a rule S →S with probability 1 would lead to lost probability mass due to
derivations that never terminate. See Booth and Thompson (1973) for more details
on consistent and inconsistent grammars.
How are PCFGs used? A PCFG can be used to estimate a number of useful
probabilities concerning a sentence and its parse tree(s), including the probability of
a particular parse tree (useful in disambiguation) and the probability of a sentence
or a piece of a sentence (useful in language modeling). Let’s see how this works.
14.1.1
PCFGs for Disambiguation
A PCFG assigns a probability to each parse tree T (i.e., each derivation) of a sen-
tence S. This attribute is useful in disambiguation. For example, consider the two
parses of the sentence “Book the dinner ﬂight” shown in Fig. 14.2. The sensible

248
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
Grammar
Lexicon
S →NP VP
[.80]
Det →that [.10] | a [.30] | the [.60]
S →Aux NP VP
[.15]
Noun →book [.10] | ﬂight [.30]
S →VP
[.05]
| meal [.05] | money [.05]
NP →Pronoun
[.35]
| ﬂight [.40] | dinner [.10]
NP →Proper-Noun
[.30]
Verb →book [.30] | include [.30]
NP →Det Nominal
[.20]
| prefer [.40]
NP →Nominal
[.15]
Pronoun →I [.40] | she [.05]
Nominal →Noun
[.75]
| me [.15] | you [.40]
Nominal →Nominal Noun [.20]
Proper-Noun →Houston [.60]
Nominal →Nominal PP
[.05]
| NWA [.40]
VP →Verb
[.35]
Aux →does [.60] | can [.40]
VP →Verb NP
[.20]
Preposition →from [.30] | to [.30]
VP →Verb NP PP
[.10]
| on [.20] | near [.15]
VP →Verb PP
[.15]
| through [.05]
VP →Verb NP NP
[.05]
VP →VP PP
[.15]
PP →Preposition NP
[1.0]
Figure 14.1
A PCFG that is a probabilistic augmentation of the L1 miniature English CFG
grammar and lexicon of Fig. 13.1. These probabilities were made up for pedagogical purposes
and are not based on a corpus (since any real corpus would have many more rules, so the true
probabilities of each rule would be much smaller).
parse on the left means “Book a ﬂight that serves dinner”. The nonsensical parse
on the right, however, would have to mean something like “Book a ﬂight on behalf
of ‘the dinner”’ just as a structurally similar sentence like “Can you book John a
ﬂight?” means something like “Can you book a ﬂight on behalf of John?”
The probability of a particular parse T is deﬁned as the product of the probabil-
ities of all the n rules used to expand each of the n non-terminal nodes in the parse
tree T, where each rule i can be expressed as LHSi →RHSi:
P(T,S) =
n
Y
i=1
P(RHSi|LHSi)
(14.2)
The resulting probability P(T,S) is both the joint probability of the parse and the
sentence and also the probability of the parse P(T). How can this be true? First, by
the deﬁnition of joint probability:
P(T,S) = P(T)P(S|T)
(14.3)
But since a parse tree includes all the words of the sentence, P(S|T) is 1. Thus,
P(T,S) = P(T)P(S|T) = P(T)
(14.4)
We can compute the probability of each of the trees in Fig. 14.2 by multiplying
the probabilities of each of the rules used in the derivation. For example, the proba-
bility of the left tree in Fig. 14.2a (call it Tleft) and the right tree (Fig. 14.2b or Tright)
can be computed as follows:
P(Tle ft) = .05∗.20∗.20∗.20∗.75∗.30∗.60∗.10∗.40 = 2.2×10−6
P(Tright) = .05∗.10∗.20∗.15∗.75∗.75∗.30∗.60∗.10∗.40 = 6.1×10−7

14.1
•
PROBABILISTIC CONTEXT-FREE GRAMMARS
249
S
VP
NP
Nominal
Noun
ﬂight
Nominal
Noun
dinner
Det
the
Verb
Book
S
VP
NP
Nominal
Noun
ﬂight
NP
Nominal
Noun
dinner
Det
the
Verb
Book
Rules
P
Rules
P
S
→VP
.05
S
→VP
.05
VP
→Verb NP
.20
VP
→Verb NP NP
.10
NP
→Det Nominal
.20
NP
→Det Nominal .20
Nominal →Nominal Noun .20
NP
→Nominal
.15
Nominal →Noun
.75
Nominal →Noun
.75
Nominal →Noun
.75
Verb
→book
.30
Verb
→book
.30
Det
→the
.60
Det
→the
.60
Noun
→dinner
.10
Noun
→dinner
.10
Noun
→ﬂight
.40
Noun
→ﬂight
.40
Figure 14.2
Two parse trees for an ambiguous sentence. The parse on the left corresponds
to the sensible meaning “Book a ﬂight that serves dinner”, while the parse on the right corre-
sponds to the nonsensical meaning “Book a ﬂight on behalf of ‘the dinner’ ”.
We can see that the left tree in Fig. 14.2 has a much higher probability than the
tree on the right. Thus, this parse would correctly be chosen by a disambiguation
algorithm that selects the parse with the highest PCFG probability.
Let’s formalize this intuition that picking the parse with the highest probability
is the correct way to do disambiguation. Consider all the possible parse trees for a
given sentence S. The string of words S is called the yield of any parse tree over S.
yield
Thus, out of all parse trees with a yield of S, the disambiguation algorithm picks the
parse tree that is most probable given S:
ˆT(S) =
argmax
Ts.t.S=yield(T)
P(T|S)
(14.5)
By deﬁnition, the probability P(T|S) can be rewritten as P(T,S)/P(S), thus leading
to
ˆT(S) =
argmax
Ts.t.S=yield(T)
P(T,S)
P(S)
(14.6)
Since we are maximizing over all parse trees for the same sentence, P(S) will be a

250
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
constant for each tree, so we can eliminate it:
ˆT(S) =
argmax
Ts.t.S=yield(T)
P(T,S)
(14.7)
Furthermore, since we showed above that P(T,S) = P(T), the ﬁnal equation for
choosing the most likely parse neatly simpliﬁes to choosing the parse with the high-
est probability:
ˆT(S) =
argmax
Ts.t.S=yield(T)
P(T)
(14.8)
14.1.2
PCFGs for Language Modeling
A second attribute of a PCFG is that it assigns a probability to the string of words
constituting a sentence. This is important in language modeling, whether for use
in speech recognition, machine translation, spelling correction, augmentative com-
munication, or other applications. The probability of an unambiguous sentence is
P(T,S) = P(T) or just the probability of the single parse tree for that sentence. The
probability of an ambiguous sentence is the sum of the probabilities of all the parse
trees for the sentence:
P(S) =
X
Ts.t.S=yield(T)
P(T,S)
(14.9)
=
X
Ts.t.S=yield(T)
P(T)
(14.10)
An additional feature of PCFGs that is useful for language modeling is their ability
to assign a probability to substrings of a sentence. For example, suppose we want
to know the probability of the next word wi in a sentence given all the words we’ve
seen so far w1,...,wi−1. The general formula for this is
P(wi|w1,w2,...,wi−1) = P(w1,w2,...,wi−1,wi)
P(w1,w2,...,wi−1)
(14.11)
We saw in Chapter 3 a simple approximation of this probability using N-grams,
conditioning on only the last word or two instead of the entire context; thus, the
bigram approximation would give us
P(wi|w1,w2,...,wi−1) ≈P(wi−1,wi)
P(wi−1)
(14.12)
But the fact that the N-gram model can only make use of a couple words of context
means it is ignoring potentially useful prediction cues. Consider predicting the word
after in the following sentence from Chelba and Jelinek (2000):
(14.13) the contract ended with a loss of 7 cents after trading as low as 9 cents
A trigram grammar must predict after from the words 7 cents, while it seems clear
that the verb ended and the subject contract would be useful predictors that a PCFG-
based parser could help us make use of. Indeed, it turns out that PCFGs allow us to
condition on the entire previous context w1,w2,...,wi−1 shown in Eq. 14.11.
In summary, this section and the previous one have shown that PCFGs can be
applied both to disambiguation in syntactic parsing and to word prediction in lan-
guage modeling. Both of these applications require that we be able to compute the
probability of parse tree T for a given sentence S. The next few sections introduce
some algorithms for computing this probability.

14.2
•
PROBABILISTIC CKY PARSING OF PCFGS
251
14.2
Probabilistic CKY Parsing of PCFGs
The parsing problem for PCFGs is to produce the most-likely parse ˆT for a given
sentence S, that is,
ˆT(S) =
argmax
Ts.t.S=yield(T)
P(T)
(14.14)
The algorithms for computing the most likely parse are simple extensions of the
standard algorithms for parsing; most modern probabilistic parsers are based on the
probabilistic CKY algorithm, ﬁrst described by Ney (1991). The probabilistic CKY
probabilistic
CKY
algorithm assumes the PCFG is in Chomsky normal form. Recall from page 222 that
in CNF, the right-hand side of each rule must expand to either two non-terminals or
to a single terminal, i.e., rules have the form A →B C, or A →w.
For the CKY algorithm, we represented each sentence as having indices between
the words. Thus, an example sentence like
(14.15) Book the ﬂight through Houston.
would assume the following indices between each word:
(14.16)
0⃝Book
1⃝the
2⃝ﬂight
3⃝through
4⃝Houston
5⃝
Using these indices, each constituent in the CKY parse tree is encoded in a
two-dimensional matrix. Speciﬁcally, for a sentence of length n and a grammar
that contains V non-terminals, we use the upper-triangular portion of an (n + 1) ×
(n + 1) matrix. For CKY, each cell table[i, j] contained a list of constituents that
could span the sequence of words from i to j. For probabilistic CKY, it’s slightly
simpler to think of the constituents in each cell as constituting a third dimension of
maximum length V. This third dimension corresponds to each non-terminal that can
be placed in this cell, and the value of the cell is then a probability for that non-
terminal/constituent rather than a list of constituents. In summary, each cell [i, j,A]
in this (n+1)×(n+1)×V matrix is the probability of a constituent of type A that
spans positions i through j of the input.
Figure 14.3 gives the probabilistic CKY algorithm.
function PROBABILISTIC-CKY(words,grammar) returns most probable parse
and its probability
for j←from 1 to LENGTH(words) do
for all { A | A →words[j] ∈grammar}
table[j −1, j,A]←P(A →words[j])
for i←from j −2 downto 0 do
for k←i+1 to j −1 do
for all { A | A →BC ∈grammar,
and table[i,k,B] > 0 and table[k, j,C] > 0 }
if (table[i,j,A] < P(A →BC) × table[i,k,B] × table[k,j,C]) then
table[i,j,A]←P(A →BC) × table[i,k,B] × table[k,j,C]
back[i,j,A]←{k,B,C}
return BUILD TREE(back[1, LENGTH(words), S]), table[1, LENGTH(words), S]
Figure 14.3
The probabilistic CKY algorithm for ﬁnding the maximum probability parse
of a string of num words words given a PCFG grammar with num rules rules in Chomsky
normal form. back is an array of backpointers used to recover the best parse. The build tree
function is left as an exercise to the reader.

252
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
Like the basic CKY algorithm in Fig. 13.5, the probabilistic CKY algorithm
requires a grammar in Chomsky normal form. Converting a probabilistic grammar
to CNF requires that we also modify the probabilities so that the probability of each
parse remains the same under the new CNF grammar. Exercise 14.2 asks you to
modify the algorithm for conversion to CNF in Chapter 13 so that it correctly handles
rule probabilities.
In practice, a generalized CKY algorithm that handles unit productions directly
is typically used. Recall that Exercise 13.3 asked you to make this change in CKY;
Exercise 14.3 asks you to extend this change to probabilistic CKY.
Let’s see an example of the probabilistic CKY chart, using the following mini-
grammar, which is already in CNF:
S
→NP VP
.80
Det
→the
.40
NP
→Det N
.30
Det
→a
.40
VP
→V NP
.20
N
→meal
.01
V
→includes .05
N
→flight .02
Given this grammar, Fig. 14.4 shows the ﬁrst steps in the probabilistic CKY
parse of the sentence “The ﬂight includes a meal”.
The
flight
[0,1]
[0,2]
[0,3]
[1,2]
[1,3]
[2,3]
Det: .40
includes
a
meal
[3,4]
[4,5]
N: .02
V: .05
NP: .30 *.40 *.02
= .0024
[0,4]
[1,4]
[2,4]
[3,5]
[2,5]
[1,5]
[0,5]
Det: .40
N: .01
Figure 14.4
The beginning of the probabilistic CKY matrix. Filling out the rest of the chart
is left as Exercise 14.4 for the reader.

14.3
•
WAYS TO LEARN PCFG RULE PROBABILITIES
253
14.3
Ways to Learn PCFG Rule Probabilities
Where do PCFG rule probabilities come from? There are two ways to learn proba-
bilities for the rules of a grammar. The simplest way is to use a treebank, a corpus
of already parsed sentences. Recall that we introduced in Chapter 12 the idea of
treebanks and the commonly used Penn Treebank (Marcus et al., 1993), a collec-
tion of parse trees in English, Chinese, and other languages that is distributed by the
Linguistic Data Consortium. Given a treebank, we can compute the probability of
each expansion of a non-terminal by counting the number of times that expansion
occurs and then normalizing.
P(α →β|α) =
Count(α →β)
P
γ Count(α →γ) = Count(α →β)
Count(α)
(14.17)
If we don’t have a treebank but we do have a (non-probabilistic) parser, we can
generate the counts we need for computing PCFG rule probabilities by ﬁrst parsing
a corpus of sentences with the parser. If sentences were unambiguous, it would be
as simple as this: parse the corpus, increment a counter for every rule in the parse,
and then normalize to get probabilities.
But wait! Since most sentences are ambiguous, that is, have multiple parses, we
don’t know which parse to count the rules in. Instead, we need to keep a separate
count for each parse of a sentence and weight each of these partial counts by the
probability of the parse it appears in. But to get these parse probabilities to weight
the rules, we need to already have a probabilistic parser.
The intuition for solving this chicken-and-egg problem is to incrementally im-
prove our estimates by beginning with a parser with equal rule probabilities, then
parse the sentence, compute a probability for each parse, use these probabilities to
weight the counts, re-estimate the rule probabilities, and so on, until our proba-
bilities converge. The standard algorithm for computing this solution is called the
inside-outside algorithm; it was proposed by Baker (1979) as a generalization of the
inside-outside
forward-backward algorithm for HMMs. Like forward-backward, inside-outside is
a special case of the Expectation Maximization (EM) algorithm, and hence has two
steps: the expectation step, and the maximization step. See Lari and Young (1990)
or Manning and Sch¨utze (1999) for more on the algorithm.
14.4
Problems with PCFGs
While probabilistic context-free grammars are a natural extension to context-free
grammars, they have two main problems as probability estimators:
Poor independence assumptions: CFG rules impose an independence assumption
on probabilities that leads to poor modeling of structural dependencies across
the parse tree.
Lack of lexical conditioning: CFG rules don’t model syntactic facts about speciﬁc
words, leading to problems with subcategorization ambiguities, preposition
attachment, and coordinate structure ambiguities.
Because of these problems, probabilistic constituent parsing models use some
augmented version of PCFGs, or modify the Treebank-based grammar in some way.

254
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
In the next few sections after discussing the problems in more detail we introduce
some of these augmentations.
14.4.1
Independence Assumptions Miss Rule Dependencies
Let’s look at these problems in more detail. Recall that in a CFG the expansion of a
non-terminal is independent of the context, that is, of the other nearby non-terminals
in the parse tree. Similarly, in a PCFG, the probability of a particular rule like
NP →Det N is also independent of the rest of the tree. By deﬁnition, the probability
of a group of independent events is the product of their probabilities. These two facts
explain why in a PCFG we compute the probability of a tree by just multiplying the
probabilities of each non-terminal expansion.
Unfortunately, this CFG independence assumption results in poor probability
estimates. This is because in English the choice of how a node expands can after all
depend on the location of the node in the parse tree. For example, in English it turns
out that NPs that are syntactic subjects are far more likely to be pronouns, and NPs
that are syntactic objects are far more likely to be non-pronominal (e.g., a proper
noun or a determiner noun sequence), as shown by these statistics for NPs in the
Switchboard corpus (Francis et al., 1999):1
Pronoun Non-Pronoun
Subject 91%
9%
Object
34%
66%
Unfortunately, there is no way to represent this contextual difference in the prob-
abilities in a PCFG. Consider two expansions of the non-terminal NP as a pronoun
or as a determiner+noun. How shall we set the probabilities of these two rules? If
we set their probabilities to their overall probability in the Switchboard corpus, the
two rules have about equal probability.
NP →DT NN .28
NP →PRP
.25
Because PCFGs don’t allow a rule probability to be conditioned on surrounding
context, this equal probability is all we get; there is no way to capture the fact that in
subject position, the probability for NP →PRP should go up to .91, while in object
position, the probability for NP →DT NN should go up to .66.
These dependencies could be captured if the probability of expanding an NP as
a pronoun (e.g., NP →PRP) versus a lexical NP (e.g., NP →DT NN) were condi-
tioned on whether the NP was a subject or an object. Section 14.5 introduces the
technique of parent annotation for adding this kind of conditioning.
14.4.2
Lack of Sensitivity to Lexical Dependencies
A second class of problems with PCFGs is their lack of sensitivity to the words in
the parse tree. Words do play a role in PCFGs since the parse probability includes
the probability of a word given a part-of-speech (e.g., from rules like V →sleep,
NN →book, etc.).
1
Distribution of subjects from 31,021 declarative sentences; distribution of objects from 7,489 sen-
tences. This tendency is caused by the use of subject position to realize the topic or old information
in a sentence (Giv´on, 1990). Pronouns are a way to talk about old information, while non-pronominal
(“lexical”) noun-phrases are often used to introduce new referents (Chapter 22).

14.4
•
PROBLEMS WITH PCFGS
255
But it turns out that lexical information is useful in other places in the grammar,
such as in resolving prepositional phrase (PP) attachment ambiguities. Since prepo-
sitional phrases in English can modify a noun phrase or a verb phrase, when a parser
ﬁnds a prepositional phrase, it must decide where to attach it into the tree. Consider
the following example:
(14.18) Workers dumped sacks into a bin.
Figure 14.5 shows two possible parse trees for this sentence; the one on the left is
the correct parse; Fig. 14.6 shows another perspective on the preposition attachment
problem, demonstrating that resolving the ambiguity in Fig. 14.5 is equivalent to
deciding whether to attach the prepositional phrase into the rest of the tree at the
NP or VP nodes; we say that the correct parse requires VP attachment, and the
VP attachment
incorrect parse implies NP attachment.
NP attachment
S
VP
PP
NP
NN
bin
DT
a
P
into
NP
NNS
sacks
VBD
dumped
NP
NNS
workers
S
VP
NP
PP
NP
NN
bin
DT
a
P
into
NP
NNS
sacks
VBD
dumped
NP
NNS
workers
Figure 14.5
Two possible parse trees for a prepositional phrase attachment ambiguity. The left parse is
the sensible one, in which “into a bin” describes the resulting location of the sacks. In the right incorrect parse,
the sacks to be dumped are the ones which are already “into a bin”, whatever that might mean.
Why doesn’t a PCFG already deal with PP attachment ambiguities? Note that
the two parse trees in Fig. 14.5 have almost exactly the same rules; they differ only
in that the left-hand parse has this rule:
VP →VBD NP PP
while the right-hand parse has these:
VP →VBD NP
NP →NP PP
Depending on how these probabilities are set, a PCFG will always either prefer
NP attachment or VP attachment. As it happens, NP attachment is slightly more
common in English, so if we trained these rule probabilities on a corpus, we might
always prefer NP attachment, causing us to misparse this sentence.
But suppose we set the probabilities to prefer the VP attachment for this sen-
tence. Now we would misparse the following, which requires NP attachment:

256
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
S
VP
NP
NNS
sacks
VBD
dumped
NP
NNS
workers
PP
NP
NN
bin
DT
a
P
into
Figure 14.6
Another view of the preposition attachment problem. Should the PP on the right attach to the
VP or NP nodes of the partial parse tree on the left?
(14.19) ﬁshermen caught tons of herring
What information in the input sentence lets us know that (14.19) requires NP
attachment while (14.18) requires VP attachment? These preferences come from
the identities of the verbs, nouns, and prepositions. The afﬁnity between the verb
dumped and the preposition into is greater than the afﬁnity between the noun sacks
and the preposition into, thus leading to VP attachment. On the other hand, in (14.19)
the afﬁnity between tons and of is greater than that between caught and of, leading to
NP attachment. Thus, to get the correct parse for these kinds of examples, we need
a model that somehow augments the PCFG probabilities to deal with these lexical
dependency statistics for different verbs and prepositions.
lexical
dependency
Coordination ambiguities are another case in which lexical dependencies are
the key to choosing the proper parse. Figure 14.7 shows an example from Collins
(1999) with two parses for the phrase dogs in houses and cats. Because dogs is
semantically a better conjunct for cats than houses (and because most dogs can’t ﬁt
inside cats), the parse [dogs in [NP houses and cats]] is intuitively unnatural and
should be dispreferred. The two parses in Fig. 14.7, however, have exactly the same
PCFG rules, and thus a PCFG will assign them the same probability.
In summary, we have shown in this section and the previous one that probabilistic
context-free grammars are incapable of modeling important structural and lexical
dependencies. In the next two sections we sketch current methods for augmenting
PCFGs to deal with both these issues.
14.5
Improving PCFGs by Splitting Non-Terminals
Let’s start with the ﬁrst of the two problems with PCFGs mentioned above: their
inability to model structural dependencies, like the fact that NPs in subject position
tend to be pronouns, whereas NPs in object position tend to have full lexical (non-
pronominal) form. How could we augment a PCFG to correctly model this fact?
One idea would be to split the NP non-terminal into two versions: one for sub-
split
jects, one for objects. Having two nodes (e.g., NPsubject and NPobject) would allow
us to correctly model their different distributional properties, since we would have

14.5
•
IMPROVING PCFGS BY SPLITTING NON-TERMINALS
257
NP
NP
Noun
cats
Conj
and
NP
PP
NP
Noun
houses
Prep
in
NP
Noun
dogs
NP
PP
NP
NP
Noun
cats
Conj
and
NP
Noun
houses
Prep
in
NP
Noun
dogs
Figure 14.7
An instance of coordination ambiguity. Although the left structure is intu-
itively the correct one, a PCFG will assign them identical probabilities since both structures
use exactly the same set of rules. After Collins (1999).
different probabilities for the rule NPsubject →PRP and the rule NPobject →PRP.
One way to implement this intuition of splits is to do parent annotation (John-
parent
annotation
son, 1998), in which we annotate each node with its parent in the parse tree. Thus,
an NP node that is the subject of the sentence and hence has parent S would be anno-
tated NPˆS, while a direct object NP whose parent is VP would be annotated NPˆVP.
Figure 14.8 shows an example of a tree produced by a grammar that parent-annotates
the phrasal non-terminals (like NP and VP).
a)
S
VP
NP
NN
ﬂight
DT
a
VBD
need
NP
PRP
I
b)
S
VPˆS
NPˆVP
NN
ﬂight
DT
a
VBD
need
NPˆS
PRP
I
Figure 14.8
A standard PCFG parse tree (a) and one which has parent annotation on the
nodes which aren’t pre-terminal (b). All the non-terminal nodes (except the pre-terminal
part-of-speech nodes) in parse (b) have been annotated with the identity of their parent.
In addition to splitting these phrasal nodes, we can also improve a PCFG by
splitting the pre-terminal part-of-speech nodes (Klein and Manning, 2003b). For ex-
ample, different kinds of adverbs (RB) tend to occur in different syntactic positions:
the most common adverbs with ADVP parents are also and now, with VP parents
n’t and not, and with NP parents only and just. Thus, adding tags like RBˆADVP,
RBˆVP, and RBˆNP can be useful in improving PCFG modeling.
Similarly, the Penn Treebank tag IN can mark a wide variety of parts-of-speech,
including subordinating conjunctions (while, as, if), complementizers (that, for),
and prepositions (of, in, from). Some of these differences can be captured by parent

258
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
annotation (subordinating conjunctions occur under S, prepositions under PP), while
others require splitting the pre-terminal nodes. Figure 14.9 shows an example from
Klein and Manning (2003b) in which even a parent-annotated grammar incorrectly
parses works as a noun in to see if advertising works. Splitting pre-terminals to allow
if to prefer a sentential complement results in the correct verbal parse.
Node-splitting is not without problems; it increases the size of the grammar and
hence reduces the amount of training data available for each grammar rule, leading
to overﬁtting. Thus, it is important to split to just the correct level of granularity for a
particular training set. While early models employed handwritten rules to try to ﬁnd
an optimal number of non-terminals (Klein and Manning, 2003b), modern models
automatically search for the optimal splits. The split and merge algorithm of Petrov
split and merge
et al. (2006), for example, starts with a simple X-bar grammar, alternately splits the
non-terminals, and merges non-terminals, ﬁnding the set of annotated nodes that
maximizes the likelihood of the training set treebank.
14.6
Probabilistic Lexicalized CFGs
The previous section showed that a simple probabilistic CKY algorithm for pars-
ing raw PCFGs can achieve extremely high parsing accuracy if the grammar rule
symbols are redesigned by automatic splits and merges.
In this section, we discuss an alternative family of models in which instead of
modifying the grammar rules, we modify the probabilistic model of the parser to
allow for lexicalized rules. The resulting family of lexicalized parsers includes the
Collins parser (Collins, 1999) and the Charniak parser (Charniak, 1997).
We saw in Section 12.4.3 that syntactic constituents could be associated with a
lexical head, and we deﬁned a lexicalized grammar in which each non-terminal
lexicalized
grammar
in the tree is annotated with its lexical head, where a rule like VP →VBD NP PP
VPˆS
VPˆVP
PPˆVP
NPˆPP
NNS
works
NN
advertising
IN
if
VB
see
TO
to
VPˆS
VPˆVP
SBARˆVP
SˆSBAR
VPˆS
VBZˆVP
works
NPˆS
NNˆNP
advertising
INˆSBAR
if
VBˆVP
see
TOˆVP
to
Figure 14.9
An incorrect parse even with a parent-annotated parse (left). The correct parse (right), was
produced by a grammar in which the pre-terminal nodes have been split, allowing the probabilistic grammar to
capture the fact that if prefers sentential complements. Adapted from Klein and Manning (2003b).

14.6
•
PROBABILISTIC LEXICALIZED CFGS
259
would be extended as
VP(dumped) →VBD(dumped) NP(sacks) PP(into)
(14.20)
In the standard type of lexicalized grammar, we actually make a further exten-
sion, which is to associate the head tag, the part-of-speech tags of the headwords,
head tag
with the non-terminal symbols as well. Each rule is thus lexicalized by both the
headword and the head tag of each constituent resulting in a format for lexicalized
rules like
VP(dumped,VBD) →VBD(dumped,VBD) NP(sacks,NNS) PP(into,P)
(14.21)
We show a lexicalized parse tree with head tags in Fig. 14.10, extended from Fig. 12.11.
TOP
S(dumped,VBD)
VP(dumped,VBD)
PP(into,P)
NP(bin,NN)
NN(bin,NN)
bin
DT(a,DT)
a
P(into,P)
into
NP(sacks,NNS)
NNS(sacks,NNS)
sacks
VBD(dumped,VBD)
dumped
NP(workers,NNS)
NNS(workers,NNS)
workers
Internal Rules
Lexical Rules
TOP
→S(dumped,VBD)
NNS(workers,NNS)
→workers
S(dumped,VBD)
→NP(workers,NNS)
VP(dumped,VBD)
VBD(dumped,VBD) →dumped
NP(workers,NNS)
→NNS(workers,NNS)
NNS(sacks,NNS)
→sacks
VP(dumped,VBD) →VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) P(into,P)
→into
PP(into,P)
→P(into,P)
NP(bin,NN)
DT(a,DT)
→a
NP(bin,NN)
→DT(a,DT)
NN(bin,NN)
NN(bin,NN)
→bin
Figure 14.10
A lexicalized tree, including head tags, for a WSJ sentence, adapted from Collins (1999). Below
we show the PCFG rules needed for this parse tree, internal rules on the left, and lexical rules on the right.
To generate such a lexicalized tree, each PCFG rule must be augmented to iden-
tify one right-hand constituent to be the head daughter. The headword for a node is
then set to the headword of its head daughter, and the head tag to the part-of-speech
tag of the headword. Recall that we gave in Fig. 12.12 a set of handwritten rules for
identifying the heads of particular constituents.
A natural way to think of a lexicalized grammar is as a parent annotation, that
is, as a simple context-free grammar with many copies of each rule, one copy for
each possible headword/head tag for each constituent. Thinking of a probabilistic
lexicalized CFG in this way would lead to the set of simple PCFG rules shown below
the tree in Fig. 14.10.
Note that Fig. 14.10 shows two kinds of rules: lexical rules, which express
lexical rules
the expansion of a pre-terminal to a word, and internal rules, which express the
internal rules

260
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
other rule expansions. We need to distinguish these kinds of rules in a lexicalized
grammar because they are associated with very different kinds of probabilities. The
lexical rules are deterministic, that is, they have probability 1.0 since a lexicalized
pre-terminal like NN(bin,NN) can only expand to the word bin. But for the internal
rules, we need to estimate probabilities.
Suppose we were to treat a probabilistic lexicalized CFG like a really big CFG
that just happened to have lots of very complex non-terminals and estimate the
probabilities for each rule from maximum likelihood estimates. Thus, according
to Eq. 14.17, the MLE estimate for the probability for the rule P(VP(dumped,VBD)
→VBD(dumped, VBD) NP(sacks,NNS) PP(into,P)) would be
Count(VP(dumped,VBD) →VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))
Count(VP(dumped,VBD))
(14.22)
But there’s no way we can get good estimates of counts like those in (14.22) because
they are so speciﬁc: we’re unlikely to see many (or even any) instances of a sentence
with a verb phrase headed by dumped that has one NP argument headed by sacks
and a PP argument headed by into. In other words, counts of fully lexicalized PCFG
rules like this will be far too sparse, and most rule probabilities will come out 0.
The idea of lexicalized parsing is to make some further independence assump-
tions to break down each rule so that we would estimate the probability
P(VP(dumped,VBD) →VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))
as the product of smaller independent probability estimates for which we could ac-
quire reasonable counts. The next section summarizes one such method, the Collins
parsing method.
14.6.1
The Collins Parser
Statistical parsers differ in exactly which independence assumptions they make.
Let’s look at the assumptions in a simpliﬁed version of the Collins parser. The ﬁrst
intuition of the Collins parser is to think of the right-hand side of every (internal)
CFG rule as consisting of a head non-terminal, together with the non-terminals to
the left of the head and the non-terminals to the right of the head. In the abstract, we
think about these rules as follows:
LHS →Ln Ln−1 ...L1 H R1 ...Rn−1 Rn
(14.23)
Since this is a lexicalized grammar, each of the symbols like L1 or R3 or H or LHS
is actually a complex symbol representing the category and its head and head tag,
like VP(dumped,VP) or NP(sacks,NNS).
Now, instead of computing a single MLE probability for this rule, we are going
to break down this rule via a neat generative story, a slight simpliﬁcation of what is
called Collins Model 1. This new generative story is that given the left-hand side,
we ﬁrst generate the head of the rule and then generate the dependents of the head,
one by one, from the inside out. Each of these steps will have its own probability.
We also add a special STOP non-terminal at the left and right edges of the rule;
this non-terminal allows the model to know when to stop generating dependents on a
given side. We generate dependents on the left side of the head until we’ve generated
STOP on the left side of the head, at which point we move to the right side of the
head and start generating dependents there until we generate STOP. So it’s as if we

14.6
•
PROBABILISTIC LEXICALIZED CFGS
261
are generating a rule augmented as follows:
P(VP(dumped,VBD) →
(14.24)
STOP VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) STOP)
Let’s see the generative story for this augmented rule. We make use of three kinds
of probabilities: PH for generating heads, PL for generating dependents on the left,
and PR for generating dependents on the right.
1. Generate the head VBD(dumped,VBD) with probability
P(H|LHS) = P(VBD(dumped,VBD) | VP(dumped,VBD))
VP(dumped,VBD)
VBD(dumped,VBD)
2. Generate the left dependent (which is STOP, since there isn’t
one) with probability
P(STOP| VP(dumped,VBD) VBD(dumped,VBD))
VP(dumped,VBD)
VBD(dumped,VBD)
STOP
3. Generate right dependent NP(sacks,NNS) with probability
Pr(NP(sacks,NNS| VP(dumped,VBD), VBD(dumped,VBD))
VP(dumped,VBD)
NP(sacks,NNS)
VBD(dumped,VBD)
STOP
4. Generate the right dependent PP(into,P) with probability
Pr(PP(into,P) | VP(dumped,VBD), VBD(dumped,VBD))
VP(dumped,VBD)
PP(into,P)
NP(sacks,NNS)
VBD(dumped,VBD)
STOP
5) Generate the right dependent STOP with probability
Pr(STOP | VP(dumped,VBD), VBD(dumped,VBD))
VP(dumped,VBD)
STOP
PP(into,P)
NP(sacks,NNS)
VBD(dumped,VBD)
STOP
In summary, the probability of this rule
P(VP(dumped,VBD) →
(14.25)
VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))
is estimated as
PH(VBD|VP, dumped) × PL(STOP|VP,VBD,dumped)
(14.26)
× PR(NP(sacks,NNS)|VP,VBD,dumped)
× PR(PP(into,P)|VP,VBD,dumped)
× PR(STOP|VP,VBD,dumped)
Each of these probabilities can be estimated from much smaller amounts of data
than the full probability in (14.25). For example, the maximum likelihood estimate
for the component probability PR(NP(sacks,NNS)|VP,VBD,dumped) is
Count(VP(dumped,VBD) with NNS(sacks) as a daughter somewhere on the right)
Count( VP(dumped,VBD) )
(14.27)

262
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
These counts are much less subject to sparsity problems than are complex counts
like those in (14.25).
More generally, if H is a head with head word hw and head tag ht, lw/lt and
rw/rt are the word/tag on the left and right respectively, and P is the parent, then the
probability of an entire rule can be expressed as follows:
1. Generate the head of the phrase H(hw,ht) with probability:
PH(H(hw,ht)|P,hw,ht)
2. Generate modiﬁers to the left of the head with total probability
n+1
Y
i=1
PL(Li(lwi,lti)|P,H,hw,ht)
such that Ln+1(lwn+1,ltn+1) = STOP, and we stop generating once we’ve gen-
erated a STOP token.
3. Generate modiﬁers to the right of the head with total probability:
n+1
Y
i=1
PR(Ri(rwi,rti)|P,H,hw,ht)
such that Rn+1(rwn+1,rtn+1) = STOP, and we stop generating once we’ve gen-
erated a STOP token.
The parsing algorithm for the Collins model is an extension of probabilistic
CKY. Extending the CKY algorithm to handle basic lexicalized probabilities is left
as Exercises 14.5 and 14.6 for the reader.
14.7
Probabilistic CCG Parsing
Lexicalized grammar frameworks such as CCG pose problems for which the phrase-
based methods we’ve been discussing are not particularly well-suited. To quickly
review, CCG consists of three major parts: a set of categories, a lexicon that asso-
ciates words with categories, and a set of rules that govern how categories combine
in context. Categories can be either atomic elements, such as S and NP, or functions
such as (S\NP)/NP which speciﬁes the transitive verb category. Rules specify how
functions, their arguments, and other functions combine. For example, the following
rule templates, forward and backward function application, specify the way that
functions apply to their arguments.
X/Y Y ⇒X
Y X\Y ⇒X
The ﬁrst rule applies a function to its argument on the right, while the second
looks to the left for its argument. The result of applying either of these rules is the
category speciﬁed as the value of the function being applied. For the purposes of
this discussion, we’ll rely on these two rules along with the forward and backward
composition rules and type-raising, as described in Chapter 12.

14.7
•
PROBABILISTIC CCG PARSING
263
14.7.1
Ambiguity in CCG
As is always the case in parsing, managing ambiguity is the key to successful CCG
parsing. The difﬁculties with CCG parsing arise from the ambiguity caused by the
large number of complex lexical categories combined with the very general nature of
the grammatical rules. To see some of the ways that ambiguity arises in a categorial
framework, consider the following example.
(14.28) United diverted the ﬂight to Reno.
Our grasp of the role of the ﬂight in this example depends on whether the prepo-
sitional phrase to Reno is taken as a modiﬁer of the ﬂight, as a modiﬁer of the entire
verb phrase, or as a potential second argument to the verb divert. In a context-free
grammar approach, this ambiguity would manifest itself as a choice among the fol-
lowing rules in the grammar.
Nominal →Nominal PP
VP →VP PP
VP →Verb NP PP
In a phrase-structure approach we would simply assign the word to to the cate-
gory P allowing it to combine with Reno to form a prepositional phrase. The sub-
sequent choice of grammar rules would then dictate the ultimate derivation. In the
categorial approach, we can associate to with distinct categories to reﬂect the ways
in which it might interact with other elements in a sentence. The fairly abstract
combinatoric rules would then sort out which derivations are possible. Therefore,
the source of ambiguity arises not from the grammar but rather from the lexicon.
Let’s see how this works by considering several possible derivations for this
example. To capture the case where the prepositional phrase to Reno modiﬁes the
ﬂight, we assign the preposition to the category (NP\NP)/NP, which gives rise to
the following derivation.
United
diverted
the
ﬂight
to
Reno
NP
(S\NP)/NP NP/N
N
(NP\NP)/NP
NP
>
>
NP
NP\NP
<
NP
>
S\NP
<
S
Here, the category assigned to to expects to ﬁnd two arguments: one to the right as
with a traditional preposition, and one to the left that corresponds to the NP to be
modiﬁed.
Alternatively, we could assign to to the category (S\S)/NP, which permits the
following derivation where to Reno modiﬁes the preceding verb phrase.
United
diverted
the
ﬂight
to
Reno
NP
(S\NP)/NP NP/N
N
(S\S)/NP
NP
>
>
NP
S\S
>
S\NP
<B
S\NP
<
S

264
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
A third possibility is to view divert as a ditransitive verb by assigning it to the
category ((S\NP)/PP)/NP, while treating to Reno as a simple prepositional phrase.
United
diverted
the
ﬂight
to
Reno
NP
((S\NP)/PP)/NP NP/N
N
PP/NP
NP
>
>
NP
PP
>
(S\NP)/PP
>
S\NP
<
S
While CCG parsers are still subject to ambiguity arising from the choice of
grammar rules, including the kind of spurious ambiguity discussed in Chapter 12,
it should be clear that the choice of lexical categories is the primary problem to be
addressed in CCG parsing.
14.7.2
CCG Parsing Frameworks
Since the rules in combinatory grammars are either binary or unary, a bottom-up,
tabular approach based on the CKY algorithm should be directly applicable to CCG
parsing. Recall from Fig. 14.3 that PCKY employs a table that records the location,
category and probability of all valid constituents discovered in the input. Given an
appropriate probability model for CCG derivations, the same kind of approach can
work for CCG parsing.
Unfortunately, the large number of lexical categories available for each word,
combined with the promiscuity of CCG’s combinatoric rules, leads to an explosion
in the number of (mostly useless) constituents added to the parsing table. The key
to managing this explosion of zombie constituents is to accurately assess and ex-
ploit the most likely lexical categories possible for each word — a process called
supertagging.
The following sections describe two approaches to CCG parsing that make use of
supertags. Section 14.7.4, presents an approach that structures the parsing process
as a heuristic search through the use of the A* algorithm. The following section
then brieﬂy describes a more traditional maximum entropy approach that manages
the search space complexity through the use of adaptive supertagging — a process
that iteratively considers more and more tags until a parse is found.
14.7.3
Supertagging
Chapter 8 introduced the task of part-of-speech tagging, the process of assigning the
correct lexical category to each word in a sentence. Supertagging is the correspond-
supertagging
ing task for highly lexicalized grammar frameworks, where the assigned tags often
dictate much of the derivation for a sentence.
CCG supertaggers rely on treebanks such as CCGbank to provide both the over-
all set of lexical categories as well as the allowable category assignments for each
word in the lexicon. CCGbank includes over 1000 lexical categories, however, in
practice, most supertaggers limit their tagsets to those tags that occur at least 10
times in the training corpus. This results in an overall total of around 425 lexical
categories available for use in the lexicon. Note that even this smaller number is
large in contrast to the 45 POS types used by the Penn Treebank tagset.

14.7
•
PROBABILISTIC CCG PARSING
265
As with traditional part-of-speech tagging, the standard approach to building a
CCG supertagger is to use supervised machine learning to build a sequence classi-
ﬁer using labeled training data. A common approach is to use the maximum entropy
Markov model (MEMM), as described in Chapter 8, to ﬁnd the most likely sequence
of tags given a sentence. The features in such a model consist of the current word
wi, its surrounding words within l words wi+l
i−l, as well as the k previously assigned
supertags ti−1
i−k . This type of model is summarized in the following equation from
Chapter 8. Training by maximizing log-likelihood of the training corpus and decod-
ing via the Viterbi algorithm are the same as described in Chapter 8.
ˆT = argmax
T
P(T|W)
= argmax
T
Y
i
P(ti|wi+l
i−l,ti−1
i−k )
= argmax
T
Y
i
exp
 X
i
wi fi(ti,wi+l
i−l,ti−1
i−k )
!
X
t′∈tagset
exp
 X
i
wi fi(t′,wi+l
i−l,ti−1
i−k )
!
(14.29)
Word and tag-based features with k and l both set to 2 provides reasonable results
given sufﬁcient training data. Additional features such as POS tags and short char-
acter sufﬁxes are also commonly used to improve performance.
Unfortunately, even with additional features the large number of possible su-
pertags combined with high per-word ambiguity leads to error rates that are too
high for practical use in a parser. More speciﬁcally, the single best tag sequence
ˆT will typically contain too many incorrect tags for effective parsing to take place.
To overcome this, we can instead return a probability distribution over the possible
supertags for each word in the input. The following table illustrates an example dis-
tribution for a simple example sentence. In this table, each column represents the
probability of each supertag for a given word in the context of the input sentence.
The “...” represent all the remaining supertags possible for each word.
United
serves
Denver
N/N: 0.4 (S\NP)/NP: 0.8
NP: 0.9
NP: 0.3
N: 0.1
N/N: 0.05
S/S: 0.1
...
...
S\S: .05
...
In a MEMM framework, the probability of the optimal tag sequence deﬁned in
Eq. 14.29 is efﬁciently computed with a suitably modiﬁed version of the Viterbi
algorithm. However, since Viterbi only ﬁnds the single best tag sequence it doesn’t
provide exactly what we need here; we need to know the probability of each pos-
sible word/tag pair. The probability of any given tag for a word is the sum of the
probabilities of all the supertag sequences that contain that tag at that location. A
table representing these values can be computed efﬁciently by using a version of the
forward-backward algorithm used for HMMs.
The same result can also be achieved through recurrent neural network (RNN)
sequence models, which have the advantage of embeddings to represent inputs and
allow representations that span the entire sentence, as opposed to size-limited sliding

266
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
windows. RNN approaches also avoid the use of high-level features, such as part of
speech tags, which is helpful since errors in tag assignment can propagate to errors
in supertags. As with the forward-backward algorithm, RNN-based methods can
provide a probability distribution over the lexical categories for each word in the
input.
14.7.4
CCG Parsing using the A* Algorithm
The A* algorithm is a heuristic search method that employs an agenda to ﬁnd an
optimal solution. Search states representing partial solutions are added to an agenda
based on a cost function, with the least-cost option being selected for further ex-
ploration at each iteration. When a state representing a complete solution is ﬁrst
selected from the agenda, it is guaranteed to be optimal and the search terminates.
The A* cost function, f(n), is used to efﬁciently guide the search to a solution.
The f-cost has two components: g(n), the exact cost of the partial solution repre-
sented by the state n, and h(n) a heuristic approximation of the cost of a solution
that makes use of n. When h(n) satisﬁes the criteria of not overestimating the actual
cost, A* will ﬁnd an optimal solution. Not surprisingly, the closer the heuristic can
get to the actual cost, the more effective A* is at ﬁnding a solution without having
to explore a signiﬁcant portion of the solution space.
When applied to parsing, search states correspond to edges representing com-
pleted constituents. As with the PCKY algorithm, edges specify a constituent’s start
and end positions, its grammatical category, and its f-cost. Here, the g component
represents the current cost of an edge and the h component represents an estimate
of the cost to complete a derivation that makes use of that edge. The use of A*
for phrase structure parsing originated with (Klein and Manning, 2003a), while the
CCG approach presented here is based on (Lewis and Steedman, 2014).
Using information from a supertagger, an agenda and a parse table are initial-
ized with states representing all the possible lexical categories for each word in the
input, along with their f-costs. The main loop removes the lowest cost edge from
the agenda and tests to see if it is a complete derivation. If it reﬂects a complete
derivation it is selected as the best solution and the loop terminates. Otherwise, new
states based on the applicable CCG rules are generated, assigned costs, and entered
into the agenda to await further processing. The loop continues until a complete
derivation is discovered, or the agenda is exhausted, indicating a failed parse. The
algorithm is given in Fig. 14.11.
Heuristic Functions
Before we can deﬁne a heuristic function for our A* search, we need to decide how
to assess the quality of CCG derivations. For the generic PCFG model, we deﬁned
the probability of a tree as the product of the probability of the rules that made up
the tree. Given CCG’s lexical nature, we’ll make the simplifying assumption that the
probability of a CCG derivation is just the product of the probability of the supertags
assigned to the words in the derivation, ignoring the rules used in the derivation.
More formally, given a sentence S and derivation D that contains supertag sequence
T, we have:
P(D,S) = P(T,S)
(14.30)
=
n
Y
i=1
P(ti|si)
(14.31)

14.7
•
PROBABILISTIC CCG PARSING
267
function CCG-ASTAR-PARSE(words) returns table or failure
supertags←SUPERTAGGER(words)
for i←from 1 to LENGTH(words) do
for all {A | (words[i], A, score) ∈supertags}
edge←MAKEEDGE(i−1,i,A,score)
table←INSERTEDGE(table,edge)
agenda←INSERTEDGE(agenda, edge)
loop do
if EMPTY?(agenda) return failure
current←POP(agenda)
if COMPLETEDPARSE?(current) return table
table←INSERTEDGE(chart, edge)
for each rule in APPLICABLERULES(edge) do
successor←APPLY(rule, edge)
if successor not ∈in agenda or chart
agenda←INSERTEDGE(agenda, successor)
else if successor ∈agenda with higher cost
agenda←REPLACEEDGE(agenda, successor)
Figure 14.11
A*-based CCG parsing.
To better ﬁt with the traditional A* approach, we’d prefer to have states scored by
a cost function where lower is better (i.e., we’re trying to minimize the cost of a
derivation). To achieve this, we’ll use negative log probabilities to score deriva-
tions; this results in the following equation, which we’ll use to score completed
CCG derivations.
P(D,S) = P(T,S)
(14.32)
=
n
X
i=1
−logP(ti|si)
(14.33)
Given this model, we can deﬁne our f-cost as follows. The f-cost of an edge is
the sum of two components: g(n), the cost of the span represented by the edge, and
h(n), the estimate of the cost to complete a derivation containing that edge (these
are often referred to as the inside and outside costs). We’ll deﬁne g(n) for an edge
using Equation 14.33. That is, it is just the sum of the costs of the supertags that
comprise the span.
For h(n), we need a score that approximates but never overestimates the actual
cost of the ﬁnal derivation. A simple heuristic that meets this requirement assumes
that each of the words in the outside span will be assigned its most probable su-
pertag. If these are the tags used in the ﬁnal derivation, then its score will equal
the heuristic. If any other tags are used in the ﬁnal derivation the f-cost will be
higher since the new tags must have higher costs, thus guaranteeing that we will not
overestimate.
Putting this all together, we arrive at the following deﬁnition of a suitable f-cost

268
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
for an edge.
f(wi,j,ti,j) = g(wi,j)+h(wi,j)
(14.34)
=
j
X
k=i
−logP(tk|wk)+
i−1
X
k=1
min
t∈tags(−logP(t|wk))+
N
X
k=j+1
min
t∈tags(−logP(t|wk))
As an example, consider an edge representing the word serves with the supertag N
in the following example.
(14.35) United serves Denver.
The g-cost for this edge is just the negative log probability of this tag, −log10(0.1),
or 1. The outside h-cost consists of the most optimistic supertag assignments for
United and Denver, which are N/N and NP respectively. The resulting f-cost for
this edge is therefore 1.443.
An Example
Fig. 14.12 shows the initial agenda and the progress of a complete parse for this
example. After initializing the agenda and the parse table with information from the
supertagger, it selects the best edge from the agenda — the entry for United with
the tag N/N and f-cost 0.591. This edge does not constitute a complete parse and is
therefore used to generate new states by applying all the relevant grammar rules. In
this case, applying forward application to United: N/N and serves: N results in the
creation of the edge United serves: N[0,2], 1.795 to the agenda.
Skipping ahead, at the third iteration an edge representing the complete deriva-
tion United serves Denver, S[0,3], .716 is added to the agenda. However, the algo-
rithm does not terminate at this point since the cost of this edge (.716) does not place
it at the top of the agenda. Instead, the edge representing Denver with the category
NP is popped. This leads to the addition of another edge to the agenda (type-raising
Denver). Only after this edge is popped and dealt with does the earlier state repre-
senting a complete derivation rise to the top of the agenda where it is popped, goal
tested, and returned as a solution.
The effectiveness of the A* approach is reﬂected in the coloring of the states
in Fig. 14.12 as well as the ﬁnal parsing table. The edges shown in blue (includ-
ing all the initial lexical category assignments not explicitly shown) reﬂect states in
the search space that never made it to the top of the agenda and, therefore, never
contributed any edges to the ﬁnal table. This is in contrast to the PCKY approach
where the parser systematically ﬁlls the parse table with all possible constituents for
all possible spans in the input, ﬁlling the table with myriad constituents that do not
contribute to the ﬁnal analysis.
14.8
Evaluating Parsers
The standard techniques for evaluating parsers and grammars are called the PAR-
SEVAL measures; they were proposed by Black et al. (1991) and were based on
the same ideas from signal-detection theory that we saw in earlier chapters. The

14.8
•
EVALUATING PARSERS
269
United serves: N[0,2]
1.795
United: N/N
.591
Denver: N/N
2.494
Denver: N
1.795
serves: N
1.494
United: S\S
1.494
United: S/S
1.1938
United: NP
.716
Denver: NP
.591
serves: (S\NP)/NP
.591
serves Denver: S\NP[1,3]
.591
United serves Denver: S[0,3]
.716
Denver: S/(S\NP)[0,1]
.591
1
2
3
4
5
6
Initial 
Agenda
Goal State
…
S: 0.716
S/NP: 0.591
United
serves
[0,1]
[0,2]
[0,3]
[1,2]
[1,3]
[2,3]
N/N: 0.591
NP: 0.716
S/S: 1.1938
S\S: 1.494
…
Denver
(S\NP)/NP: 0.591
N: 1.494
…
NP: 0.591
N: 1.795
N/N: 2.494
…
N: 1.795
Figure 14.12
Example of an A* search for the example “United serves Denver”. The circled numbers on the
blue boxes indicate the order in which the states are popped from the agenda. The costs in each state are based
on f-costs using negative log10 probabilities.
intuition of the PARSEVAL metric is to measure how much the constituents in the
hypothesis parse tree look like the constituents in a hand-labeled, gold-reference
parse. PARSEVAL thus assumes we have a human-labeled “gold standard” parse
tree for each sentence in the test set; we generally draw these gold-standard parses
from a treebank like the Penn Treebank.
Given these gold-standard reference parses for a test set, a given constituent in
a hypothesis parse Ch of a sentence s is labeled “correct” if there is a constituent in
the reference parse Cr with the same starting point, ending point, and non-terminal
symbol. We can then measure the precision and recall just as we did for chunking in
the previous chapter.

270
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
labeled recall: = # of correct constituents in hypothesis parse of s
# of correct constituents in reference parse of s
labeled precision: = # of correct constituents in hypothesis parse of s
# of total constituents in hypothesis parse of s
As with other uses of precision and recall, we often report a combination of
the two, the F-measure (van Rijsbergen, 1975), which, as we saw in Chapter 4, is
F-measure
deﬁned as:
Fβ = (β 2 +1)PR
β 2P+R
Values of β > 1 favor recall and values of β < 1 favor precision. When β = 1,
precision and recall are equally balanced; this is called Fβ=1 or just F1:
F1 = 2PR
P+R
(14.36)
We additionally use a new metric, crossing brackets, for each sentence s:
cross-brackets: the number of constituents for which the reference parse has a
bracketing such as ((A B) C) but the hypothesis parse has a bracketing such
as (A (B C)).
For comparing parsers that use different grammars, the PARSEVAL metric in-
cludes a canonicalization algorithm for removing information likely to be grammar-
speciﬁc (auxiliaries, pre-inﬁnitival “to”, etc.) and for computing a simpliﬁed score
(Black et al., 1991). The canonical implementation of the PARSEVAL metrics is
called evalb (Sekine and Collins, 1997).
evalb
Nonetheless, phrasal constituents are not always an appropriate unit for parser
evaluation. In lexically-oriented grammars, such as CCG and LFG, the ultimate goal
is to extract the appropriate predicate-argument relations or grammatical dependen-
cies, rather than a speciﬁc derivation. Such relations are also more directly relevant
to further semantic processing. For these purposes, we can use alternative evalua-
tion metrics based on the precision and recall of labeled dependencies whose labels
indicate the grammatical relations (Lin 1995, Carroll et al. 1998, Collins et al. 1999).
Finally, you might wonder why we don’t evaluate parsers by measuring how
many sentences are parsed correctly instead of measuring component accuracy in
the form of constituents or dependencies. The reason we use components is that it
gives us a more ﬁne-grained metric. This is especially true for long sentences, where
most parsers don’t get a perfect parse. If we just measured sentence accuracy, we
wouldn’t be able to distinguish between a parse that got most of the parts wrong and
one that just got one part wrong.
14.9
Summary
This chapter has sketched the basics of probabilistic parsing, concentrating on
probabilistic context-free grammars and probabilistic lexicalized context-free
grammars.
• Probabilistic grammars assign a probability to a sentence or string of words
while attempting to capture more sophisticated syntactic information than the
N-gram grammars of Chapter 3.

BIBLIOGRAPHICAL AND HISTORICAL NOTES
271
• A
probabilistic
context-free
grammar
(PCFG)
is
a
context-free
grammar in which every rule is annotated with the probability of that rule
being chosen. Each PCFG rule is treated as if it were conditionally inde-
pendent; thus, the probability of a sentence is computed by multiplying the
probabilities of each rule in the parse of the sentence.
• The probabilistic CKY (Cocke-Kasami-Younger) algorithm is a probabilistic
version of the CKY parsing algorithm. There are also probabilistic versions
of other parsers like the Earley algorithm.
• PCFG probabilities can be learned by counting in a parsed corpus or by pars-
ing a corpus. The inside-outside algorithm is a way of dealing with the fact
that the sentences being parsed are ambiguous.
• Raw PCFGs suffer from poor independence assumptions among rules and lack
of sensitivity to lexical dependencies.
• One way to deal with this problem is to split and merge non-terminals (auto-
matically or by hand).
• Probabilistic lexicalized CFGs are another solution to this problem in which
the basic PCFG model is augmented with a lexical head for each rule. The
probability of a rule can then be conditioned on the lexical head or nearby
heads.
• Parsers for lexicalized PCFGs (like the Charniak and Collins parsers) are
based on extensions to probabilistic CKY parsing.
• Parsers are evaluated with three metrics: labeled recall, labeled precision,
and cross-brackets.
Bibliographical and Historical Notes
Many of the formal properties of probabilistic context-free grammars were ﬁrst
worked out by Booth (1969) and Salomaa (1969). Baker (1979) proposed the inside-
outside algorithm for unsupervised training of PCFG probabilities, and used a CKY-
style parsing algorithm to compute inside probabilities. Jelinek and Lafferty (1991)
extended the CKY algorithm to compute probabilities for preﬁxes. Stolcke (1995)
adapted the Earley algorithm to use with PCFGs.
A number of researchers starting in the early 1990s worked on adding lexical de-
pendencies to PCFGs and on making PCFG rule probabilities more sensitive to sur-
rounding syntactic structure. For example, Schabes et al. (1988) and Schabes (1990)
presented early work on the use of heads. Many papers on the use of lexical depen-
dencies were ﬁrst presented at the DARPA Speech and Natural Language Workshop
in June 1990. A paper by Hindle and Rooth (1990) applied lexical dependencies
to the problem of attaching prepositional phrases; in the question session to a later
paper, Ken Church suggested applying this method to full parsing (Marcus, 1990).
Early work on such probabilistic CFG parsing augmented with probabilistic depen-
dency information includes Magerman and Marcus (1991), Black et al. (1992), Bod
(1993), and Jelinek et al. (1994), in addition to Collins (1996), Charniak (1997), and
Collins (1999) discussed above. Other recent PCFG parsing models include Klein
and Manning (2003a) and Petrov et al. (2006).
This early lexical probabilistic work led initially to work focused on solving
speciﬁc parsing problems like preposition-phrase attachment by using methods in-

272
CHAPTER 14
•
STATISTICAL CONSTITUENCY PARSING
cluding transformation-based learning (TBL) (Brill and Resnik, 1994), maximum
entropy (Ratnaparkhi et al., 1994), memory-based learning (Zavrel and Daelemans,
1997), log-linear models (Franz, 1997), decision trees that used semantic distance
between heads (computed from WordNet) (Stetina and Nagao, 1997), and boosting
(Abney et al., 1999). Another direction extended the lexical probabilistic parsing
work to build probabilistic formulations of grammars other than PCFGs, such as
probabilistic TAG grammar (Resnik 1992, Schabes 1992), based on the TAG gram-
mars discussed in Chapter 12, probabilistic LR parsing (Briscoe and Carroll, 1993),
and probabilistic link grammar (Lafferty et al., 1992). The supertagging approach
we saw for CCG was developed for TAG grammars (Bangalore and Joshi 1999,
Joshi and Srinivas 1994), based on the lexicalized TAG grammars of Schabes et al.
(1988).
Exercises
14.1 Implement the CKY algorithm.
14.2 Modify the algorithm for conversion to CNF from Chapter 13 to correctly
handle rule probabilities. Make sure that the resulting CNF assigns the same
total probability to each parse tree.
14.3 Recall that Exercise 13.3 asked you to update the CKY algorithm to han-
dle unit productions directly rather than converting them to CNF. Extend this
change to probabilistic CKY.
14.4 Fill out the rest of the probabilistic CKY chart in Fig. 14.4.
14.5 Sketch how the CKY algorithm would have to be augmented to handle lexi-
calized probabilities.
14.6 Implement your lexicalized extension of the CKY algorithm.
14.7 Implement the PARSEVAL metrics described in Section 14.8. Next, either
use a treebank or create your own hand-checked parsed test set. Now use your
CFG (or other) parser and grammar, parse the test set and compute labeled
recall, labeled precision, and cross-brackets.

CHAPTER
15
Dependency Parsing
The focus of the three previous chapters has been on context-free grammars and
their use in automatically generating constituent-based representations. Here we
present another family of grammar formalisms called dependency grammars that
dependency
grammars
are quite important in contemporary speech and language processing systems. In
these formalisms, phrasal constituents and phrase-structure rules do not play a direct
role. Instead, the syntactic structure of a sentence is described solely in terms of the
words (or lemmas) in a sentence and an associated set of directed binary grammatical
relations that hold among the words.
The following diagram illustrates a dependency-style analysis using the standard
graphical method favored in the dependency-parsing community.
(15.1)
I prefer the morning ﬂight through Denver
nsubj
dobj
det
nmod
nmod
case
root
Relations among the words are illustrated above the sentence with directed, la-
beled arcs from heads to dependents. We call this a typed dependency structure
typed
dependency
because the labels are drawn from a ﬁxed inventory of grammatical relations. It also
includes a root node that explicitly marks the root of the tree, the head of the entire
structure.
Figure 15.1 shows the same dependency analysis as a tree alongside its corre-
sponding phrase-structure analysis of the kind given in Chapter 12. Note the ab-
sence of nodes corresponding to phrasal constituents or lexical categories in the
dependency parse; the internal structure of the dependency parse consists solely
of directed relations between lexical items in the sentence. These relationships di-
rectly encode important information that is often buried in the more complex phrase-
structure parses. For example, the arguments to the verb prefer are directly linked to
it in the dependency structure, while their connection to the main verb is more dis-
tant in the phrase-structure tree. Similarly, morning and Denver, modiﬁers of ﬂight,
are linked to it directly in the dependency structure.
A major advantage of dependency grammars is their ability to deal with lan-
guages that are morphologically rich and have a relatively free word order. For
free word order
example, word order in Czech can be much more ﬂexible than in English; a gram-
matical object might occur before or after a location adverbial. A phrase-structure
grammar would need a separate rule for each possible place in the parse tree where
such an adverbial phrase could occur. A dependency-based approach would just
have one link type representing this particular adverbial relation. Thus, a depen-
dency grammar approach abstracts away from word-order information, representing
only the information that is necessary for the parse.
An additional practical motivation for a dependency-based approach is that the
head-dependent relations provide an approximation to the semantic relationship be-

274
CHAPTER 15
•
DEPENDENCY PARSING
prefer
ﬂight
Denver
through
morning
the
I
S
VP
NP
Nom
PP
NP
Pro
Denver
P
through
Nom
Noun
ﬂight
Nom
Noun
morning
Det
the
Verb
prefer
NP
Pro
I
Figure 15.1
A dependency-style parse alongside the corresponding constituent-based analysis for I prefer the
morning ﬂight through Denver.
tween predicates and their arguments that makes them directly useful for many ap-
plications such as coreference resolution, question answering and information ex-
traction. Constituent-based approaches to parsing provide similar information, but it
often has to be distilled from the trees via techniques such as the head-ﬁnding rules
discussed in Chapter 12.
In the following sections, we’ll discuss in more detail the inventory of relations
used in dependency parsing, as well as the formal basis for these dependency struc-
tures. We’ll then move on to discuss the dominant families of algorithms that are
used to automatically produce these structures. Finally, we’ll discuss how to eval-
uate dependency parsers and point to some of the ways they are used in language
processing applications.
15.1
Dependency Relations
The traditional linguistic notion of grammatical relation provides the basis for the
grammatical
relation
binary relations that comprise these dependency structures. The arguments to these
relations consist of a head and a dependent. We’ve already discussed the notion of
head
dependent
heads in Chapter 12 and Chapter 14 in the context of constituent structures. There,
the head word of a constituent was the central organizing word of a larger constituent
(e.g, the primary noun in a noun phrase, or verb in a verb phrase). The remaining
words in the constituent are either direct, or indirect, dependents of their head. In
dependency-based approaches, the head-dependent relationship is made explicit by
directly linking heads to the words that are immediately dependent on them, bypass-
ing the need for constituent structures.
In addition to specifying the head-dependent pairs, dependency grammars allow
us to further classify the kinds of grammatical relations, or grammatical function,
grammatical
function

15.1
•
DEPENDENCY RELATIONS
275
Clausal Argument Relations Description
NSUBJ
Nominal subject
DOBJ
Direct object
IOBJ
Indirect object
CCOMP
Clausal complement
XCOMP
Open clausal complement
Nominal Modiﬁer Relations
Description
NMOD
Nominal modiﬁer
AMOD
Adjectival modiﬁer
NUMMOD
Numeric modiﬁer
APPOS
Appositional modiﬁer
DET
Determiner
CASE
Prepositions, postpositions and other case markers
Other Notable Relations
Description
CONJ
Conjunct
CC
Coordinating conjunction
Figure 15.2
Selected dependency relations from the Universal Dependency set. (de Marn-
effe et al., 2014)
in terms of the role that the dependent plays with respect to its head. Familiar notions
such as subject, direct object and indirect object are among the kind of relations we
have in mind. In English these notions strongly correlate with, but by no means de-
termine, both position in a sentence and constituent type and are therefore somewhat
redundant with the kind of information found in phrase-structure trees. However, in
more ﬂexible languages the information encoded directly in these grammatical rela-
tions is critical since phrase-based constituent syntax provides little help.
Not surprisingly, linguists have developed taxonomies of relations that go well
beyond the familiar notions of subject and object. While there is considerable vari-
ation from theory to theory, there is enough commonality that efforts to develop a
computationally useful standard are now possible. The Universal Dependencies
Universal
Dependencies
project (Nivre et al., 2016b) provides an inventory of dependency relations that are
linguistically motivated, computationally useful, and cross-linguistically applicable.
Fig. 15.2 shows a subset of the relations from this effort. Fig. 15.3 provides some
example sentences illustrating selected relations.
The motivation for all of the relations in the Universal Dependency scheme is
beyond the scope of this chapter, but the core set of frequently used relations can be
broken into two sets: clausal relations that describe syntactic roles with respect to a
predicate (often a verb), and modiﬁer relations that categorize the ways that words
that can modify their heads.
Consider the following example sentence:
(15.2)
United canceled the morning ﬂights to Houston
nsubj
dobj
det
nmod
nmod
case
root
The clausal relations NSUBJ and DOBJ identify the subject and direct object of
the predicate cancel, while the NMOD, DET, and CASE relations denote modiﬁers of
the nouns ﬂights and Houston.

276
CHAPTER 15
•
DEPENDENCY PARSING
Relation
Examples with head and dependent
NSUBJ
United canceled the ﬂight.
DOBJ
United diverted the ﬂight to Reno.
We booked her the ﬁrst ﬂight to Miami.
IOBJ
We booked her the ﬂight to Miami.
NMOD
We took the morning ﬂight.
AMOD
Book the cheapest ﬂight.
NUMMOD
Before the storm JetBlue canceled 1000 ﬂights.
APPOS
United, a unit of UAL, matched the fares.
DET
The ﬂight was canceled.
Which ﬂight was delayed?
CONJ
We ﬂew to Denver and drove to Steamboat.
CC
We ﬂew to Denver and drove to Steamboat.
CASE
Book the ﬂight through Houston.
Figure 15.3
Examples of core Universal Dependency relations.
15.2
Dependency Formalisms
In their most general form, the dependency structures we’re discussing are simply
directed graphs. That is, structures G = (V,A) consisting of a set of vertices V, and
a set of ordered pairs of vertices A, which we’ll refer to as arcs.
For the most part we will assume that the set of vertices, V, corresponds exactly
to the set of words in a given sentence. However, they might also correspond to
punctuation, or when dealing with morphologically complex languages the set of
vertices might consist of stems and afﬁxes. The set of arcs, A, captures the head-
dependent and grammatical function relationships between the elements in V.
Further constraints on these dependency structures are speciﬁc to the underlying
grammatical theory or formalism. Among the more frequent restrictions are that the
structures must be connected, have a designated root node, and be acyclic or planar.
Of most relevance to the parsing approaches discussed in this chapter is the common,
computationally-motivated, restriction to rooted trees. That is, a dependency tree
dependency
tree
is a directed graph that satisﬁes the following constraints:
1. There is a single designated root node that has no incoming arcs.
2. With the exception of the root node, each vertex has exactly one incoming arc.
3. There is a unique path from the root node to each vertex in V.
Taken together, these constraints ensure that each word has a single head, that the
dependency structure is connected, and that there is a single root node from which
one can follow a unique directed path to each of the words in the sentence.
15.2.1
Projectivity
The notion of projectivity imposes an additional constraint that is derived from the
order of the words in the input, and is closely related to the context-free nature of
human languages discussed in Chapter 12. An arc from a head to a dependent is
said to be projective if there is a path from the head to every word that lies between
the head and the dependent in the sentence. A dependency tree is then said to be
projective if all the arcs that make it up are projective. All the dependency trees
we’ve seen thus far have been projective. There are, however, many perfectly valid

15.3
•
DEPENDENCY TREEBANKS
277
constructions which lead to non-projective trees, particularly in languages with a
relatively ﬂexible word order.
Consider the following example.
(15.3)
JetBlue canceled our ﬂight this morning which was already late
nsubj
dobj
mod
det
nmod
det
case
mod
adv
root
In this example, the arc from ﬂight to its modiﬁer was is non-projective since
there is no path from ﬂight to the intervening words this and morning. As we can
see from this diagram, projectivity (and non-projectivity) can be detected in the way
we’ve been drawing our trees. A dependency tree is projective if it can be drawn
with no crossing edges. Here there is no way to link ﬂight to its dependent was
without crossing the arc that links morning to its head.
Our concern with projectivity arises from two related issues. First, the most
widely used English dependency treebanks were automatically derived from phrase-
structure treebanks through the use of head-ﬁnding rules (Chapter 12). The trees
generated in such a fashion are guaranteed to be projective since they’re generated
from context-free grammars.
Second, there are computational limitations to the most widely used families of
parsing algorithms. The transition-based approaches discussed in Section 15.4 can
only produce projective trees, hence any sentences with non-projective structures
will necessarily contain some errors. This limitation is one of the motivations for
the more ﬂexible graph-based parsing approach described in Section 15.5.
15.3
Dependency Treebanks
As with constituent-based methods, treebanks play a critical role in the development
and evaluation of dependency parsers. Dependency treebanks have been created
using similar approaches to those discussed in Chapter 12 — having human annota-
tors directly generate dependency structures for a given corpus, or using automatic
parsers to provide an initial parse and then having annotators hand correct those
parsers. We can also use a deterministic process to translate existing constituent-
based treebanks into dependency trees through the use of head rules.
For the most part, directly annotated dependency treebanks have been created for
morphologically rich languages such as Czech, Hindi and Finnish that lend them-
selves to dependency grammar approaches, with the Prague Dependency Treebank
(Bejˇcek et al., 2013) for Czech being the most well-known effort. The major English
dependency treebanks have largely been extracted from existing resources such as
the Wall Street Journal sections of the Penn Treebank (Marcus et al., 1993). The
more recent OntoNotes project (Hovy et al. 2006, Weischedel et al. 2011) extends
this approach going beyond traditional news text to include conversational telephone
speech, weblogs, usenet newsgroups, broadcasts, and talk shows in English, Chinese
and Arabic.
The translation process from constituent to dependency structures has two sub-
tasks: identifying all the head-dependent relations in the structure and identifying
the correct dependency relations for these relations. The ﬁrst task relies heavily on

278
CHAPTER 15
•
DEPENDENCY PARSING
the use of head rules discussed in Chapter 12 ﬁrst developed for use in lexicalized
probabilistic parsers (Magerman 1994, Collins 1999, Collins 2003). Here’s a simple
and effective algorithm from Xia and Palmer (2001).
1. Mark the head child of each node in a phrase structure, using the appropriate
head rules.
2. In the dependency structure, make the head of each non-head child depend on
the head of the head-child.
When a phrase-structure parse contains additional information in the form of
grammatical relations and function tags, as in the case of the Penn Treebank, these
tags can be used to label the edges in the resulting tree. When applied to the parse
tree in Fig. 15.4, this algorithm would produce the dependency structure in exam-
ple 15.4.
(15.4)
Vinken will join the board as a nonexecutive director Nov 29
sbj
aux
dobj
clr
tmp
nmod
case
nmod
amod
num
root
The primary shortcoming of these extraction methods is that they are limited by
the information present in the original constituent trees. Among the most impor-
tant issues are the failure to integrate morphological information with the phrase-
structure trees, the inability to easily represent non-projective structures, and the
lack of internal structure to most noun-phrases, as reﬂected in the generally ﬂat
rules used in most treebank grammars. For these reasons, outside of English, most
dependency treebanks are developed directly using human annotators.
15.4
Transition-Based Dependency Parsing
Our ﬁrst approach to dependency parsing is motivated by a stack-based approach
called shift-reduce parsing originally developed for analyzing programming lan-
shift-reduce
parsing
guages (Aho and Ullman, 1972). This classic approach is simple and elegant, em-
ploying a context-free grammar, a stack, and a list of tokens to be parsed. Input
tokens are successively shifted onto the stack and the top two elements of the stack
are matched against the right-hand side of the rules in the grammar; when a match is
found the matched elements are replaced on the stack (reduced) by the non-terminal
from the left-hand side of the rule being matched. In adapting this approach for
dependency parsing, we forgo the explicit use of a grammar and alter the reduce
operation so that instead of adding a non-terminal to a parse tree, it introduces a
dependency relation between a word and its head. More speciﬁcally, the reduce ac-
tion is replaced with two possible actions: assert a head-dependent relation between
the word at the top of the stack and the word below it, or vice versa. Figure 15.5
illustrates the basic operation of such a parser.
A key element in transition-based parsing is the notion of a conﬁguration which
conﬁguration
consists of a stack, an input buffer of words, or tokens, and a set of relations rep-
resenting a dependency tree. Given this framework, the parsing process consists of

15.4
•
TRANSITION-BASED DEPENDENCY PARSING
279
S
VP
VP
NP-TMP
CD
29
NNP
Nov
PP-CLR
NP
NN
director
JJ
nonexecutive
DT
a
IN
as
NP
NN
board
DT
the
VB
join
MD
will
NP-SBJ
NNP
Vinken
S(join)
VP(join)
VP(join)
NP-TMP(29)
CD
29
NNP
Nov
PP-CLR(director)
NP(director)
NN
director
JJ
nonexecutive
DT
a
IN
as
NP(board)
NN
board
DT
the
VB
join
MD
will
NP-SBJ(Vinken)
NNP
Vinken
join
29
Nov
director
nonexecutive
a
as
board
the
will
Vinken
Figure 15.4
A phrase-structure tree from the Wall Street Journal component of the Penn Treebank 3.
a sequence of transitions through the space of possible conﬁgurations. The goal of
this process is to ﬁnd a ﬁnal conﬁguration where all the words have been accounted
for and an appropriate dependency tree has been synthesized.
To implement such a search, we’ll deﬁne a set of transition operators, which
when applied to a conﬁguration produce new conﬁgurations. Given this setup, we
can view the operation of a parser as a search through a space of conﬁgurations for
a sequence of transitions that leads from a start state to a desired goal state. At the
start of this process we create an initial conﬁguration in which the stack contains the

280
CHAPTER 15
•
DEPENDENCY PARSING
Dependency
Relations
wn
w1
w2
s2
...
s1
sn
Parser
Input buffer
Stack
Oracle
Figure 15.5
Basic transition-based parser. The parser examines the top two elements of the
stack and selects an action based on consulting an oracle that examines the current conﬁgura-
tion.
ROOT node, the word list is initialized with the set of the words or lemmatized tokens
in the sentence, and an empty set of relations is created to represent the parse. In the
ﬁnal goal state, the stack and the word list should be empty, and the set of relations
will represent the ﬁnal parse.
In the standard approach to transition-based parsing, the operators used to pro-
duce new conﬁgurations are surprisingly simple and correspond to the intuitive ac-
tions one might take in creating a dependency tree by examining the words in a
single pass over the input from left to right (Covington, 2001):
• Assign the current word as the head of some previously seen word,
• Assign some previously seen word as the head of the current word,
• Or postpone doing anything with the current word, adding it to a store for later
processing.
To make these actions more precise, we’ll create three transition operators that
will operate on the top two elements of the stack:
• LEFTARC: Assert a head-dependent relation between the word at the top of
the stack and the word directly beneath it; remove the lower word from the
stack.
• RIGHTARC: Assert a head-dependent relation between the second word on
the stack and the word at the top; remove the word at the top of the stack;
• SHIFT: Remove the word from the front of the input buffer and push it onto
the stack.
This particular set of operators implements what is known as the arc standard
arc standard
approach to transition-based parsing (Covington 2001, Nivre 2003). There are two
notable characteristics to this approach: the transition operators only assert relations
between elements at the top of the stack, and once an element has been assigned
its head it is removed from the stack and is not available for further processing.
As we’ll see, there are alternative transition systems which demonstrate different
parsing behaviors, but the arc standard approach is quite effective and is simple to
implement.

15.4
•
TRANSITION-BASED DEPENDENCY PARSING
281
To assure that these operators are used properly we’ll need to add some pre-
conditions to their use. First, since, by deﬁnition, the ROOT node cannot have any
incoming arcs, we’ll add the restriction that the LEFTARC operator cannot be ap-
plied when ROOT is the second element of the stack. Second, both reduce operators
require two elements to be on the stack to be applied. Given these transition opera-
tors and preconditions, the speciﬁcation of a transition-based parser is quite simple.
Fig. 15.6 gives the basic algorithm.
function DEPENDENCYPARSE(words) returns dependency tree
state←{[root], [words], [] } ; initial conﬁguration
while state not ﬁnal
t←ORACLE(state)
; choose a transition operator to apply
state←APPLY(t, state) ; apply it, creating a new state
return state
Figure 15.6
A generic transition-based dependency parser
At each step, the parser consults an oracle (we’ll come back to this shortly) that
provides the correct transition operator to use given the current conﬁguration. It then
applies that operator to the current conﬁguration, producing a new conﬁguration.
The process ends when all the words in the sentence have been consumed and the
ROOT node is the only element remaining on the stack.
The efﬁciency of transition-based parsers should be apparent from the algorithm.
The complexity is linear in the length of the sentence since it is based on a single left
to right pass through the words in the sentence. More speciﬁcally, each word must
ﬁrst be shifted onto the stack and then later reduced.
Note that unlike the dynamic programming and search-based approaches dis-
cussed in Chapters 12 and 13, this approach is a straightforward greedy algorithm
— the oracle provides a single choice at each step and the parser proceeds with that
choice, no other options are explored, no backtracking is employed, and a single
parse is returned in the end.
Figure 15.7 illustrates the operation of the parser with the sequence of transitions
leading to a parse for the following example.
(15.5)
Book me the morning ﬂight
iobj
dobj
det
nmod
root
Let’s consider the state of the conﬁguration at Step 2, after the word me has been
pushed onto the stack.
Stack
Word List
Relations
[root, book, me] [the, morning, ﬂight]
The correct operator to apply here is RIGHTARC which assigns book as the head of
me and pops me from the stack resulting in the following conﬁguration.
Stack
Word List
Relations
[root, book] [the, morning, ﬂight] (book →me)

282
CHAPTER 15
•
DEPENDENCY PARSING
Step
Stack
Word List
Action
Relation Added
0
[root]
[book, me, the, morning, ﬂight]
SHIFT
1
[root, book]
[me, the, morning, ﬂight]
SHIFT
2
[root, book, me]
[the, morning, ﬂight]
RIGHTARC
(book →me)
3
[root, book]
[the, morning, ﬂight]
SHIFT
4
[root, book, the]
[morning, ﬂight]
SHIFT
5
[root, book, the, morning]
[ﬂight]
SHIFT
6
[root, book, the, morning, ﬂight]
[]
LEFTARC
(morning ←ﬂight)
7
[root, book, the, ﬂight]
[]
LEFTARC
(the ←ﬂight)
8
[root, book, ﬂight]
[]
RIGHTARC
(book →ﬂight)
9
[root, book]
[]
RIGHTARC
(root →book)
10
[root]
[]
Done
Figure 15.7
Trace of a transition-based parse.
After several subsequent applications of the SHIFT and LEFTARC operators, the con-
ﬁguration in Step 6 looks like the following:
Stack
Word List
Relations
[root, book, the, morning, ﬂight]
[]
(book →me)
Here, all the remaining words have been passed onto the stack and all that is left
to do is to apply the appropriate reduce operators. In the current conﬁguration, we
employ the LEFTARC operator resulting in the following state.
Stack
Word List
Relations
[root, book, the, ﬂight]
[]
(book →me)
(morning ←ﬂight)
At this point, the parse for this sentence consists of the following structure.
(15.6)
Book me the morning ﬂight
iobj
nmod
There are several important things to note when examining sequences such as
the one in Figure 15.7. First, the sequence given is not the only one that might lead
to a reasonable parse. In general, there may be more than one path that leads to the
same result, and due to ambiguity, there may be other transition sequences that lead
to different equally valid parses.
Second, we are assuming that the oracle always provides the correct operator
at each point in the parse — an assumption that is unlikely to be true in practice.
As a result, given the greedy nature of this algorithm, incorrect choices will lead to
incorrect parses since the parser has no opportunity to go back and pursue alternative
choices. Section 15.4.2 will introduce several techniques that allow transition-based
approaches to explore the search space more fully.
Finally, for simplicity, we have illustrated this example without the labels on
the dependency relations. To produce labeled trees, we can parameterize the LEFT-
ARC and RIGHTARC operators with dependency labels, as in LEFTARC(NSUBJ) or
RIGHTARC(DOBJ). This is equivalent to expanding the set of transition operators
from our original set of three to a set that includes LEFTARC and RIGHTARC opera-
tors for each relation in the set of dependency relations being used, plus an additional
one for the SHIFT operator. This, of course, makes the job of the oracle more difﬁcult
since it now has a much larger set of operators from which to choose.

15.4
•
TRANSITION-BASED DEPENDENCY PARSING
283
15.4.1
Creating an Oracle
State-of-the-art transition-based systems use supervised machine learning methods
to train classiﬁers that play the role of the oracle. Given appropriate training data,
these methods learn a function that maps from conﬁgurations to transition operators.
As with all supervised machine learning methods, we will need access to appro-
priate training data and we will need to extract features useful for characterizing the
decisions to be made. The source for this training data will be representative tree-
banks containing dependency trees. The features will consist of many of the same
features we encountered in Chapter 8 for part-of-speech tagging, as well as those
used in Chapter 14 for statistical parsing models.
Generating Training Data
Let’s revisit the oracle from the algorithm in Fig. 15.6 to fully understand the learn-
ing problem. The oracle takes as input a conﬁguration and returns as output a tran-
sition operator. Therefore, to train a classiﬁer, we will need conﬁgurations paired
with transition operators (i.e., LEFTARC, RIGHTARC, or SHIFT). Unfortunately,
treebanks pair entire sentences with their corresponding trees, and therefore they
don’t directly provide what we need.
To generate the required training data, we will employ the oracle-based parsing
algorithm in a clever way. We will supply our oracle with the training sentences
to be parsed along with their corresponding reference parses from the treebank. To
produce training instances, we will then simulate the operation of the parser by run-
ning the algorithm and relying on a new training oracle to give us correct transition
training oracle
operators for each successive conﬁguration.
To see how this works, let’s ﬁrst review the operation of our parser. It begins with
a default initial conﬁguration where the stack contains the ROOT, the input list is just
the list of words, and the set of relations is empty. The LEFTARC and RIGHTARC
operators each add relations between the words at the top of the stack to the set of
relations being accumulated for a given sentence. Since we have a gold-standard
reference parse for each training sentence, we know which dependency relations are
valid for a given sentence. Therefore, we can use the reference parse to guide the
selection of operators as the parser steps through a sequence of conﬁgurations.
To be more precise, given a reference parse and a conﬁguration, the training
oracle proceeds as follows:
• Choose LEFTARC if it produces a correct head-dependent relation given the
reference parse and the current conﬁguration,
• Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent re-
lation given the reference parse and (2) all of the dependents of the word at
the top of the stack have already been assigned,
• Otherwise, choose SHIFT.
The restriction on selecting the RIGHTARC operator is needed to ensure that a
word is not popped from the stack, and thus lost to further processing, before all its
dependents have been assigned to it.
More formally, during training the oracle has access to the following informa-
tion:
• A current conﬁguration with a stack S and a set of dependency relations Rc
• A reference parse consisting of a set of vertices V and a set of dependency
relations Rp

284
CHAPTER 15
•
DEPENDENCY PARSING
Step
Stack
Word List
Predicted Action
0
[root]
[book, the, ﬂight, through, houston]
SHIFT
1
[root, book]
[the, ﬂight, through, houston]
SHIFT
2
[root, book, the]
[ﬂight, through, houston]
SHIFT
3
[root, book, the, ﬂight]
[through, houston]
LEFTARC
4
[root, book, ﬂight]
[through, houston]
SHIFT
5
[root, book, ﬂight, through]
[houston]
SHIFT
6
[root, book, ﬂight, through, houston]
[]
LEFTARC
7
[root, book, ﬂight, houston ]
[]
RIGHTARC
8
[root, book, ﬂight]
[]
RIGHTARC
9
[root, book]
[]
RIGHTARC
10
[root]
[]
Done
Figure 15.8
Generating training items consisting of conﬁguration/predicted action pairs by
simulating a parse with a given reference parse.
Given this information, the oracle chooses transitions as follows:
LEFTARC(r): if (S1 r S2) ∈Rp
RIGHTARC(r): if (S2 r S1) ∈Rp and ∀r′,w s.t.(S1 r′ w) ∈Rp then (S1 r′ w) ∈
Rc
SHIFT: otherwise
Let’s walk through the steps of this process with the following example as shown
in Fig. 15.8.
(15.7)
Book the ﬂight through Houston
dobj
det
nmod
case
root
At Step 1, LEFTARC is not applicable in the initial conﬁguration since it asserts
a relation, (root ←book), not in the reference answer; RIGHTARC does assert a
relation contained in the ﬁnal answer (root →book), however book has not been
attached to any of its dependents yet, so we have to defer, leaving SHIFT as the only
possible action. The same conditions hold in the next two steps. In step 3, LEFTARC
is selected to link the to its head.
Now consider the situation in Step 4.
Stack
Word buffer
Relations
[root, book, ﬂight] [through, Houston] (the ←ﬂight)
Here, we might be tempted to add a dependency relation between book and ﬂight,
which is present in the reference parse. But doing so now would prevent the later
attachment of Houston since ﬂight would have been removed from the stack. For-
tunately, the precondition on choosing RIGHTARC prevents this choice and we’re
again left with SHIFT as the only viable option. The remaining choices complete the
set of operators needed for this example.
To recap, we derive appropriate training instances consisting of conﬁguration-
transition pairs from a treebank by simulating the operation of a parser in the con-
text of a reference dependency tree. We can deterministically record correct parser
actions at each step as we progress through each training example, thereby creating
the training set we require.

15.4
•
TRANSITION-BASED DEPENDENCY PARSING
285
Features
Having generated appropriate training instances (conﬁguration-transition pairs), we
need to extract useful features from the conﬁgurations so we can train classiﬁers.
The features that are used to train transition-based systems vary by language, genre,
and the kind of classiﬁer being employed. For example, morphosyntactic features
such as case marking on subjects or direct objects may be more or less important
depending on the language being processed. That said, the basic features that we
have already seen with part-of-speech tagging and partial parsing have proven to
be useful in training dependency parsers across a wide range of languages. Word
forms, lemmas and parts of speech are all powerful features, as are the head, and
dependency relation to the head.
In the transition-based parsing framework, such features need to be extracted
from the conﬁgurations that make up the training data. Recall that conﬁgurations
consist of three elements: the stack, the buffer and the current set of relations. In
principle, any property of any or all of these elements can be represented as features
in the usual way for training. However, to avoid sparsity and encourage generaliza-
tion, it is best to focus the learning algorithm on the most useful aspects of decision
making at each point in the parsing process. The focus of feature extraction for
transition-based parsing is, therefore, on the top levels of the stack, the words near
the front of the buffer, and the dependency relations already associated with any of
those elements.
By combining simple features, such as word forms or parts of speech, with spe-
ciﬁc locations in a conﬁguration, we can employ the notion of a feature template
feature
template
that we’ve already encountered with sentiment analysis and part-of-speech tagging.
Feature templates allow us to automatically generate large numbers of speciﬁc fea-
tures from a training set. As an example, consider the following feature templates
that are based on single positions in a conﬁguration.
⟨s1.w,op⟩,⟨s2.w,op⟩⟨s1.t,op⟩,⟨s2.t,op⟩
⟨b1.w,op⟩,⟨b1.t,op⟩⟨s1.wt,op⟩
(15.8)
In these examples, individual features are denoted as location.property, where s
denotes the stack, b the word buffer, and r the set of relations. Individual properties
of locations include w for word forms, l for lemmas, and t for part-of-speech. For
example, the feature corresponding to the word form at the top of the stack would be
denoted as s1.w, and the part of speech tag at the front of the buffer b1.t. We can also
combine individual features via concatenation into more speciﬁc features that may
prove useful. For example, the feature designated by s1.wt represents the word form
concatenated with the part of speech of the word at the top of the stack. Finally, op
stands for the transition operator for the training example in question (i.e., the label
for the training instance).
Let’s consider the simple set of single-element feature templates given above
in the context of the following intermediate conﬁguration derived from a training
oracle for Example 15.2.
Stack
Word buffer
Relations
[root, canceled, ﬂights] [to Houston] (canceled →United)
(ﬂights →morning)
(ﬂights →the)
The correct transition here is SHIFT (you should convince yourself of this before

286
CHAPTER 15
•
DEPENDENCY PARSING
proceeding). The application of our set of feature templates to this conﬁguration
would result in the following set of instantiated features.
⟨s1.w = ﬂights,op = shift⟩
(15.9)
⟨s2.w = canceled,op = shift⟩
⟨s1.t = NNS,op = shift⟩
⟨s2.t = VBD,op = shift⟩
⟨b1.w = to,op = shift⟩
⟨b1.t = TO,op = shift⟩
⟨s1.wt = ﬂightsNNS,op = shift⟩
Given that the left and right arc transitions operate on the top two elements of
the stack, features that combine properties from these positions are even more useful.
For example, a feature like s1.t ◦s2.t concatenates the part of speech tag of the word
at the top of the stack with the tag of the word beneath it.
⟨s1.t ◦s2.t = NNSVBD,op = shift⟩
(15.10)
Not surprisingly, if two properties are useful then three or more should be even
better. Figure 15.9 gives a baseline set of feature templates that have been employed
(Zhang and Clark 2008, Huang and Sagae 2010, Zhang and Nivre 2011).
Note that some of these features make use of dynamic features — features such
as head words and dependency relations that have been predicted at earlier steps in
the parsing process, as opposed to features that are derived from static properties of
the input.
Source
Feature templates
One word s1.w
s1.t
s1.wt
s2.w
s2.t
s2.wt
b1.w
b1.w
b0.wt
Two word s1.w◦s2.w
s1.t ◦s2.t
s1.t ◦b1.w
s1.t ◦s2.wt
s1.w◦s2.w◦s2.t s1.w◦s1.t ◦s2.t
s1.w◦s1.t ◦s2.t
s1.w◦s1.t
Figure 15.9
Standard feature templates for training transition-based dependency parsers.
In the template speciﬁcations sn refers to a location on the stack, bn refers to a location in the
word buffer, w refers to the wordform of the input, and t refers to the part of speech of the
input.
Learning
Over the years, the dominant approaches to training transition-based dependency
parsers have been multinomial logistic regression and support vector machines, both
of which can make effective use of large numbers of sparse features of the kind
described in the last section.
More recently, neural network, or deep learning,
approaches of the kind described in Chapter 8 have been applied successfully to
transition-based parsing (Chen and Manning, 2014). These approaches eliminate the
need for complex, hand-crafted features and have been particularly effective at over-
coming the data sparsity issues normally associated with training transition-based
parsers.

15.4
•
TRANSITION-BASED DEPENDENCY PARSING
287
15.4.2
Advanced Methods in Transition-Based Parsing
The basic transition-based approach can be elaborated in a number of ways to im-
prove performance by addressing some of the most obvious ﬂaws in the approach.
Alternative Transition Systems
The arc-standard transition system described above is only one of many possible sys-
tems. A frequently used alternative is the arc eager transition system. The arc eager
arc eager
approach gets its name from its ability to assert rightward relations much sooner
than in the arc standard approach. To see this, let’s revisit the arc standard trace of
Example 15.7, repeated here.
Book the ﬂight through Houston
dobj
det
nmod
case
root
Consider the dependency relation between book and ﬂight in this analysis. As
is shown in Fig. 15.8, an arc-standard approach would assert this relation at Step 8,
despite the fact that book and ﬂight ﬁrst come together on the stack much earlier at
Step 4. The reason this relation can’t be captured at this point is due to the presence
of the post-nominal modiﬁer through Houston. In an arc-standard approach, depen-
dents are removed from the stack as soon as they are assigned their heads. If ﬂight
had been assigned book as its head in Step 4, it would no longer be available to serve
as the head of Houston.
While this delay doesn’t cause any issues in this example, in general the longer
a word has to wait to get assigned its head the more opportunities there are for
something to go awry. The arc-eager system addresses this issue by allowing words
to be attached to their heads as early as possible, before all the subsequent words
dependent on them have been seen. This is accomplished through minor changes to
the LEFTARC and RIGHTARC operators and the addition of a new REDUCE operator.
• LEFTARC: Assert a head-dependent relation between the word at the front of
the input buffer and the word at the top of the stack; pop the stack.
• RIGHTARC: Assert a head-dependent relation between the word on the top of
the stack and the word at front of the input buffer; shift the word at the front
of the input buffer to the stack.
• SHIFT: Remove the word from the front of the input buffer and push it onto
the stack.
• REDUCE: Pop the stack.
The LEFTARC and RIGHTARC operators are applied to the top of the stack and
the front of the input buffer, instead of the top two elements of the stack as in the
arc-standard approach. The RIGHTARC operator now moves the dependent to the
stack from the buffer rather than removing it, thus making it available to serve as the
head of following words. The new REDUCE operator removes the top element from
the stack. Together these changes permit a word to be eagerly assigned its head and
still allow it to serve as the head for later dependents. The trace shown in Fig. 15.10
illustrates the new decision sequence for this example.
In addition to demonstrating the arc-eager transition system, this example demon-
strates the power and ﬂexibility of the overall transition-based approach. We were
able to swap in a new transition system without having to make any changes to the

288
CHAPTER 15
•
DEPENDENCY PARSING
Step
Stack
Word List
Action
Relation Added
0
[root]
[book, the, ﬂight, through, houston]
RIGHTARC
(root →book)
1
[root, book]
[the, ﬂight, through, houston]
SHIFT
2
[root, book, the]
[ﬂight, through, houston]
LEFTARC
(the ←ﬂight)
3
[root, book]
[ﬂight, through, houston]
RIGHTARC
(book →ﬂight)
4
[root, book, ﬂight]
[through, houston]
SHIFT
5
[root, book, ﬂight, through]
[houston]
LEFTARC
(through ←houston)
6
[root, book, ﬂight]
[houston]
RIGHTARC
(ﬂight →houston)
7
[root, book, ﬂight, houston]
[]
REDUCE
8
[root, book, ﬂight]
[]
REDUCE
9
[root, book]
[]
REDUCE
10
[root]
[]
Done
Figure 15.10
A processing trace of Book the ﬂight through Houston using the arc-eager
transition operators.
underlying parsing algorithm. This ﬂexibility has led to the development of a di-
verse set of transition systems that address different aspects of syntax and semantics
including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the
generation of non-projective dependency structures (Nivre, 2009), assigning seman-
tic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages
(Bhat et al., 2017).
Beam Search
The computational efﬁciency of the transition-based approach discussed earlier de-
rives from the fact that it makes a single pass through the sentence, greedily making
decisions without considering alternatives. Of course, this is also the source of its
greatest weakness – once a decision has been made it can not be undone, even in
the face of overwhelming evidence arriving later in a sentence. Another approach
is to systematically explore alternative decision sequences, selecting the best among
those alternatives. The key problem for such a search is to manage the large number
of potential sequences. Beam search accomplishes this by combining a breadth-ﬁrst
Beam search
search strategy with a heuristic ﬁlter that prunes the search frontier to stay within a
ﬁxed-size beam width.
beam width
In applying beam search to transition-based parsing, we’ll elaborate on the al-
gorithm given in Fig. 15.6. Instead of choosing the single best transition operator
at each iteration, we’ll apply all applicable operators to each state on an agenda and
then score the resulting conﬁgurations. We then add each of these new conﬁgura-
tions to the frontier, subject to the constraint that there has to be room within the
beam. As long as the size of the agenda is within the speciﬁed beam width, we can
add new conﬁgurations to the agenda. Once the agenda reaches the limit, we only
add new conﬁgurations that are better than the worst conﬁguration on the agenda
(removing the worst element so that we stay within the limit). Finally, to insure that
we retrieve the best possible state on the agenda, the while loop continues as long as
there are non-ﬁnal states on the agenda.
The beam search approach requires a more elaborate notion of scoring than we
used with the greedy algorithm. There, we assumed that a classiﬁer trained using
supervised machine learning would serve as an oracle, selecting the best transition
operator based on features extracted from the current conﬁguration. Regardless of
the speciﬁc learning approach, this choice can be viewed as assigning a score to all
the possible transitions and picking the best one.
ˆT(c) = argmaxScore(t,c)

15.5
•
GRAPH-BASED DEPENDENCY PARSING
289
With a beam search we are now searching through the space of decision se-
quences, so it makes sense to base the score for a conﬁguration on its entire history.
More speciﬁcally, we can deﬁne the score for a new conﬁguration as the score of its
predecessor plus the score of the operator used to produce it.
ConﬁgScore(c0) = 0.0
ConﬁgScore(ci) = ConﬁgScore(ci−1)+Score(ti,ci−1)
This score is used both in ﬁltering the agenda and in selecting the ﬁnal answer.
The new beam search version of transition-based parsing is given in Fig. 15.11.
function DEPENDENCYBEAMPARSE(words, width) returns dependency tree
state←{[root], [words], [], 0.0}
;initial conﬁguration
agenda←⟨state⟩
;initial agenda
while agenda contains non-ﬁnal states
newagenda←⟨⟩
for each state ∈agenda do
for all {t | t ∈VALIDOPERATORS(state)} do
child←APPLY(t, state)
newagenda←ADDTOBEAM(child, newagenda, width)
agenda←newagenda
return BESTOF(agenda)
function ADDTOBEAM(state, agenda, width) returns updated agenda
if LENGTH(agenda) < width then
agenda←INSERT(state, agenda)
else if SCORE(state) > SCORE(WORSTOF(agenda))
agenda←REMOVE(WORSTOF(agenda))
agenda←INSERT(state, agenda)
return agenda
Figure 15.11
Beam search applied to transition-based dependency parsing.
15.5
Graph-Based Dependency Parsing
Graph-based approaches to dependency parsing search through the space of possible
trees for a given sentence for a tree (or trees) that maximize some score. These
methods encode the search space as directed graphs and employ methods drawn
from graph theory to search the space for optimal solutions. More formally, given a
sentence S we’re looking for the best dependency tree in Gs, the space of all possible
trees for that sentence, that maximizes some score.
ˆT(S) = argmax
t∈GS
score(t,S)
As with the probabilistic approaches to context-free parsing discussed in Chap-
ter 14, the overall score for a tree can be viewed as a function of the scores of the
parts of the tree. The focus of this section is on edge-factored approaches where the
edge-factored

290
CHAPTER 15
•
DEPENDENCY PARSING
score for a tree is based on the scores of the edges that comprise the tree.
score(t,S) =
X
e∈t
score(e)
There are several motivations for the use of graph-based methods. First, unlike
transition-based approaches, these methods are capable of producing non-projective
trees. Although projectivity is not a signiﬁcant issue for English, it is deﬁnitely a
problem for many of the world’s languages. A second motivation concerns parsing
accuracy, particularly with respect to longer dependencies. Empirically, transition-
based methods have high accuracy on shorter dependency relations but accuracy de-
clines signiﬁcantly as the distance between the head and dependent increases (Mc-
Donald and Nivre, 2011). Graph-based methods avoid this difﬁculty by scoring
entire trees, rather than relying on greedy local decisions.
The following section examines a widely-studied approach based on the use of a
maximum spanning tree (MST) algorithm for weighted, directed graphs. We then
maximum
spanning tree
discuss features that are typically used to score trees, as well as the methods used to
train the scoring models.
15.5.1
Parsing
The approach described here uses an efﬁcient greedy algorithm to search for optimal
spanning trees in directed graphs. Given an input sentence, it begins by constructing
a fully-connected, weighted, directed graph where the vertices are the input words
and the directed edges represent all possible head-dependent assignments. An addi-
tional ROOT node is included with outgoing edges directed at all of the other vertices.
The weights in the graph reﬂect the score for each possible head-dependent relation
as provided by a model generated from training data. Given these weights, a maxi-
mum spanning tree of this graph emanating from the ROOT represents the preferred
dependency parse for the sentence. A directed graph for the example Book that
ﬂight is shown in Fig. 15.12, with the maximum spanning tree corresponding to the
desired parse shown in blue. For ease of exposition, we’ll focus here on unlabeled
dependency parsing. Graph-based approaches to labeled parsing are discussed in
Section 15.5.3.
Before describing the algorithm it’s useful to consider two intuitions about di-
rected graphs and their spanning trees. The ﬁrst intuition begins with the fact that
every vertex in a spanning tree has exactly one incoming edge. It follows from this
that every connected component of a spanning tree will also have one incoming edge.
The second intuition is that the absolute values of the edge scores are not critical to
determining its maximum spanning tree. Instead, it is the relative weights of the
edges entering each vertex that matters. If we were to subtract a constant amount
from each edge entering a given vertex it would have no impact on the choice of
the maximum spanning tree since every possible spanning tree would decrease by
exactly the same amount.
The ﬁrst step of the algorithm itself is quite straightforward. For each vertex
in the graph, an incoming edge (representing a possible head assignment) with the
highest score is chosen. If the resulting set of edges produces a spanning tree then
we’re done. More formally, given the original fully-connected graph G = (V,E), a
subgraph T = (V,F) is a spanning tree if it has no cycles and each vertex (other than
the root) has exactly one edge entering it. If the greedy selection process produces
such a tree then it is the best possible one.

15.5
•
GRAPH-BASED DEPENDENCY PARSING
291
root
Book
that
ﬂight
12
4
4
5
6
8
7
5
7
Figure 15.12
Initial rooted, directed graph for Book that ﬂight.
Unfortunately, this approach doesn’t always lead to a tree since the set of edges
selected may contain cycles. Fortunately, in yet another case of multiple discovery,
there is a straightforward way to eliminate cycles generated during the greedy se-
lection phase. Chu and Liu (1965) and Edmonds (1967) independently developed
an approach that begins with greedy selection and follows with an elegant recursive
cleanup phase that eliminates cycles.
The cleanup phase begins by adjusting all the weights in the graph by subtracting
the score of the maximum edge entering each vertex from the score of all the edges
entering that vertex. This is where the intuitions mentioned earlier come into play.
We have scaled the values of the edges so that the weight of the edges in the cycle
have no bearing on the weight of any of the possible spanning trees. Subtracting the
value of the edge with maximum weight from each edge entering a vertex results
in a weight of zero for all of the edges selected during the greedy selection phase,
including all of the edges involved in the cycle.
Having adjusted the weights, the algorithm creates a new graph by selecting a
cycle and collapsing it into a single new node. Edges that enter or leave the cycle
are altered so that they now enter or leave the newly collapsed node. Edges that do
not touch the cycle are included and edges within the cycle are dropped.
Now, if we knew the maximum spanning tree of this new graph, we would have
what we need to eliminate the cycle. The edge of the maximum spanning tree di-
rected towards the vertex representing the collapsed cycle tells us which edge to
delete to eliminate the cycle. How do we ﬁnd the maximum spanning tree of this
new graph? We recursively apply the algorithm to the new graph. This will either
result in a spanning tree or a graph with a cycle. The recursions can continue as long
as cycles are encountered. When each recursion completes we expand the collapsed
vertex, restoring all the vertices and edges from the cycle with the exception of the
single edge to be deleted.
Putting all this together, the maximum spanning tree algorithm consists of greedy
edge selection, re-scoring of edge costs and a recursive cleanup phase when needed.
The full algorithm is shown in Fig. 15.13.
Fig. 15.14 steps through the algorithm with our Book that ﬂight example. The
ﬁrst row of the ﬁgure illustrates greedy edge selection with the edges chosen shown
in blue (corresponding to the set F in the algorithm). This results in a cycle between

292
CHAPTER 15
•
DEPENDENCY PARSING
function MAXSPANNINGTREE(G=(V,E), root,score) returns spanning tree
F←[]
T’←[]
score’←[]
for each v ∈V do
bestInEdge←argmaxe=(u,v)∈E score[e]
F←F ∪bestInEdge
for each e=(u,v) ∈E do
score’[e]←score[e] −score[bestInEdge]
if T=(V,F) is a spanning tree then return it
else
C←a cycle in F
G’←CONTRACT(G,C)
T’←MAXSPANNINGTREE(G’,root,score’)
T←EXPAND(T’, C)
return T
function CONTRACT(G,C) returns contracted graph
function EXPAND(T, C) returns expanded graph
Figure 15.13
The Chu-Liu Edmonds algorithm for ﬁnding a maximum spanning tree in a
weighted directed graph.
that and ﬂight. The scaled weights using the maximum value entering each node are
shown in the graph to the right.
Collapsing the cycle between that and ﬂight to a single node (labelled tf) and
recursing with the newly scaled costs is shown in the second row. The greedy selec-
tion step in this recursion yields a spanning tree that links root to book, as well as an
edge that links book to the contracted node. Expanding the contracted node, we can
see that this edge corresponds to the edge from book to ﬂight in the original graph.
This in turn tells us which edge to drop to eliminate the cycle
On arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)
time, where m is the number of edges and n is the number of nodes. Since this par-
ticular application of the algorithm begins by constructing a fully connected graph
m = n2 yielding a running time of O(n3). Gabow et al. (1986) present a more efﬁ-
cient implementation with a running time of O(m+nlogn).
15.5.2
Features and Training
Given a sentence, S, and a candidate tree, T, edge-factored parsing models reduce
the score for the tree to a sum of the scores of the edges that comprise the tree.
score(S,T) =
X
e∈T
score(S,e)
Each edge score can, in turn, be reduced to a weighted sum of features extracted
from it.
score(S,e) =
N
X
i=1
wi fi(S,e)

15.5
•
GRAPH-BASED DEPENDENCY PARSING
293
root
Book
tf
root
Book
that
ﬂight
0
-3
-4
-7
-1
-6
-2
root
Book
12
that
7
ﬂight
8
-4
-3
0
-2
-6
-1
-7
0
0
root
Book
0
tf
-1
0
-3
-4
-7
-1
-6
-2
root
Book
12
that
7
ﬂight
8
12
4
4
5
6
8
7
5
7
Deleted from cycle
Figure 15.14
Chu-Liu-Edmonds graph-based example for Book that ﬂight
Or more succinctly.
score(S,e) = w· f
Given this formulation, we are faced with two problems in training our parser:
identifying relevant features and ﬁnding the weights used to score those features.
The features used to train edge-factored models mirror those used in training
transition-based parsers (as shown in Fig. 15.9). This is hardly surprising since in
both cases we’re trying to capture information about the relationship between heads
and their dependents in the context of a single relation. To summarize this earlier
discussion, commonly used features include:
• Wordforms, lemmas, and parts of speech of the headword and its dependent.
• Corresponding features derived from the contexts before, after and between
the words.
• Word embeddings.
• The dependency relation itself.
• The direction of the relation (to the right or left).
• The distance from the head to the dependent.
As with transition-based approaches, pre-selected combinations of these features are
often used as well.
Given a set of features, our next problem is to learn a set of weights correspond-
ing to each. Unlike many of the learning problems discussed in earlier chapters,

294
CHAPTER 15
•
DEPENDENCY PARSING
here we are not training a model to associate training items with class labels, or
parser actions. Instead, we seek to train a model that assigns higher scores to cor-
rect trees than to incorrect ones. An effective framework for problems like this is to
use inference-based learning combined with the perceptron learning rule. In this
inference-based
learning
framework, we parse a sentence (i.e, perform inference) from the training set using
some initially random set of initial weights. If the resulting parse matches the cor-
responding tree in the training data, we do nothing to the weights. Otherwise, we
ﬁnd those features in the incorrect parse that are not present in the reference parse
and we lower their weights by a small amount based on the learning rate. We do this
incrementally for each sentence in our training data until the weights converge.
State-of-the-art algorithms in multilingual parsing are based on recurrent neural
networks (RNNs) (Zeman et al. 2017, Dozat et al. 2017).
15.5.3
Advanced Issues in Graph-Based Parsing
15.6
Evaluation
As with phrase structure-based parsing, the evaluation of dependency parsers pro-
ceeds by measuring how well they work on a test-set. An obvious metric would be
exact match (EM) — how many sentences are parsed correctly. This metric is quite
pessimistic, with most sentences being marked wrong. Such measures are not ﬁne-
grained enough to guide the development process. Our metrics need to be sensitive
enough to tell if actual improvements are being made.
For these reasons, the most common method for evaluating dependency parsers
are labeled and unlabeled attachment accuracy. Labeled attachment refers to the
proper assignment of a word to its head along with the correct dependency relation.
Unlabeled attachment simply looks at the correctness of the assigned head, ignor-
ing the dependency relation. Given a system output and a corresponding reference
parse, accuracy is simply the percentage of words in an input that are assigned the
correct head with the correct relation. These metrics are usually referred to as the
labeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we
can make use of a label accuracy score (LS), the percentage of tokens with correct
labels, ignoring where the relations are coming from.
As an example, consider the reference parse and system parse for the following
example shown in Fig. 15.15.
(15.11) Book me the ﬂight through Houston.
The system correctly ﬁnds 4 of the 6 dependency relations present in the refer-
ence parse and receives an LAS of 2/3. However, one of the 2 incorrect relations
found by the system holds between book and ﬂight, which are in a head-dependent
relation in the reference parse; the system therefore achieves a UAS of 5/6.
Beyond attachment scores, we may also be interested in how well a system is
performing on a particular kind of dependency relation, for example NSUBJ, across
a development corpus. Here we can make use of the notions of precision and recall
introduced in Chapter 8, measuring the percentage of relations labeled NSUBJ by
the system that were correct (precision), and the percentage of the NSUBJ relations
present in the development set that were in fact discovered by the system (recall).
We can employ a confusion matrix to keep track of how often each dependency type
was confused for another.

15.7
•
SUMMARY
295
Book me the
ﬂight
through Houston
Reference
obj
iobj
det
nmod
case
root
Book me the ﬂight through Houston
System
x-comp
nsubj
det
nmod
case
root
Figure 15.15
Reference and system parses for Book me the ﬂight through Houston, resulting in an LAS of
2/3 and an UAS of 5/6.
15.7
Summary
This chapter has introduced the concept of dependency grammars and dependency
parsing. Here’s a summary of the main points that we covered:
• In dependency-based approaches to syntax, the structure of a sentence is de-
scribed in terms of a set of binary relations that hold between the words in a
sentence. Larger notions of constituency are not directly encoded in depen-
dency analyses.
• The relations in a dependency structure capture the head-dependent relation-
ship among the words in a sentence.
• Dependency-based analysis provides information directly useful in further
language processing tasks including information extraction, semantic parsing
and question answering.
• Transition-based parsing systems employ a greedy stack-based algorithm to
create dependency structures.
• Graph-based methods for creating dependency structures are based on the use
of maximum spanning tree methods from graph theory.
• Both transition-based and graph-based approaches are developed using super-
vised machine learning techniques.
• Treebanks provide the data needed to train these systems. Dependency tree-
banks can be created directly by human annotators or via automatic transfor-
mation from phrase-structure treebanks.
• Evaluation of dependency parsers is based on labeled and unlabeled accuracy
scores as measured against withheld development and test corpora.
Bibliographical and Historical Notes
The dependency-based approach to grammar is much older than the relatively re-
cent phrase-structure or constituency grammars that have been the primary focus of
both theoretical and computational linguistics for years. It has its roots in the an-
cient Greek and Indian linguistic traditions. Contemporary theories of dependency
grammar all draw heavily on the work of Tesni`ere (1959). The most inﬂuential
dependency grammar frameworks include Meaning-Text Theory (MTT) (Mel’˘cuk,
1988), Word Grammar (Hudson, 1984), Functional Generative Description (FDG)
(Sgall et al., 1986). These frameworks differ along a number of dimensions in-
cluding the degree and manner in which they deal with morphological, syntactic,

296
CHAPTER 15
•
DEPENDENCY PARSING
semantic and pragmatic factors, their use of multiple layers of representation, and
the set of relations used to categorize dependency relations.
Automatic parsing using dependency grammars was ﬁrst introduced into compu-
tational linguistics by early work on machine translation at the RAND Corporation
led by David Hays. This work on dependency parsing closely paralleled work on
constituent parsing and made explicit use of grammars to guide the parsing process.
After this early period, computational work on dependency parsing remained inter-
mittent over the following decades. Notable implementations of dependency parsers
for English during this period include Link Grammar (Sleator and Temperley, 1993),
Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).
Dependency parsing saw a major resurgence in the late 1990’s with the appear-
ance of large dependency-based treebanks and the associated advent of data driven
approaches described in this chapter. Eisner (1996) developed an efﬁcient dynamic
programming approach to dependency parsing based on bilexical grammars derived
from the Penn Treebank. Covington (2001) introduced the deterministic word by
word approach underlying current transition-based approaches. Yamada and Mat-
sumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce
paradigm and the use of supervised machine learning in the form of support vector
machines to dependency parsing.
Nivre (2003) deﬁned the modern, deterministic, transition-based approach to de-
pendency parsing. Subsequent work by Nivre and his colleagues formalized and an-
alyzed the performance of numerous transition systems, training methods, and meth-
ods for dealing with non-projective language Nivre and Scholz 2004, Nivre 2006,
Nivre and Nilsson 2005, Nivre et al. 2007, Nivre 2007.
The graph-based maximum spanning tree approach to dependency parsing was
introduced by McDonald et al. 2005, McDonald et al. 2005.
The earliest source of data for training and evaluating dependency English parsers
came from the WSJ Penn Treebank (Marcus et al., 1993) described in Chapter 12.
The use of head-ﬁnding rules developed for use with probabilistic parsing facili-
tated the automatic extraction of dependency parses from phrase-based ones (Xia
and Palmer, 2001).
The long-running Prague Dependency Treebank project (Hajiˇc, 1998) is the most
signiﬁcant effort to directly annotate a corpus with multiple layers of morphological,
syntactic and semantic information. The current PDT 3.0 now contains over 1.5 M
tokens (Bejˇcek et al., 2013).
Universal Dependencies (UD) (Nivre et al., 2016b) is a project directed at cre-
ating a consistent framework for dependency treebank annotation across languages
with the goal of advancing parser development across the world’s languages. Under
the auspices of this effort, treebanks for over 30 languages have been annotated and
made available in a single consistent format. The UD annotation scheme evolved out
of several distinct efforts including Stanford dependencies (de Marneffe et al. 2006,
de Marneffe and Manning 2008, de Marneffe et al. 2014), Google’s universal part-
of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic
tagsets (Zeman, 2008). Driven in part by the UD framework, dependency treebanks
of a signiﬁcant size and quality are now available in over 30 languages (Nivre et al.,
2016b).
The Conference on Natural Language Learning (CoNLL) has conducted an in-
ﬂuential series of shared tasks related to dependency parsing over the years (Buch-
holz and Marsi 2006, Nilsson et al. 2007, Surdeanu et al. 2008a, Hajiˇc et al. 2009).
More recent evaluations have focused on parser robustness with respect to morpho-

EXERCISES
297
logically rich languages (Seddah et al., 2013), and non-canonical language forms
such as social media, texts, and spoken language (Petrov and McDonald, 2012).
Choi et al. (2015) presents a performance analysis of 10 dependency parsers across
a range of metrics, as well as DEPENDABLE, a robust parser evaluation tool.
Exercises

298
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
CHAPTER
16
Logical
Representations
of
Sentence Meaning
ISHMAEL: Surely all this is not without meaning.
Herman Melville, Moby Dick
In this chapter we introduce the idea that the meaning of linguistic expressions can
be captured in formal structures called meaning representations. Consider tasks
meaning
representations
that require some form of semantic processing, like learning to use a new piece of
software by reading the manual, deciding what to order at a restaurant by reading
a menu, or following a recipe. Accomplishing these tasks requires representations
that link the linguistic elements to the necessary non-linguistic knowledge of the
world. Reading a menu and deciding what to order, giving advice about where to
go to dinner, following a recipe, and generating new recipes all require knowledge
about food and its preparation, what people like to eat, and what restaurants are like.
Learning to use a piece of software by reading a manual, or giving advice on using
software, requires knowledge about the software and similar apps, computers, and
users in general.
In this chapter, we assume that linguistic expressions have meaning representa-
tions that are made up of the same kind of stuff that is used to represent this kind of
everyday common-sense knowledge of the world. The process whereby such repre-
sentations are created and assigned to linguistic inputs is called semantic parsing or
semantic
parsing
semantic analysis, and the entire enterprise of designing meaning representations
and associated semantic parsers is referred to as computational semantics.
computational
semantics
∃e,y Having(e)∧Haver(e,Speaker)∧HadThing(e,y)∧Car(y)
h / have-01
c / car
i / i 
arg0
arg1
(h / have-01
        arg0: (i / i)
        arg1: (c / car))
Having:
      Haver:  Speaker
      HadThing:  Car
Figure 16.1
A list of symbols, two directed graphs, and a record structure: a sampler of
meaning representations for I have a car.
Consider Fig. 16.1, which shows example meaning representations for the sen-
tence I have a car using four commonly used meaning representation languages.
The top row illustrates a sentence in First-Order Logic, covered in detail in Sec-
tion 16.3; the directed graph and its corresponding textual form is an example of
an Abstract Meaning Representation (AMR) form (Banarescu et al., 2013), and
on the right is a frame-based or slot-ﬁller representation, discussed in Section 16.5
and again in Chapter 18.

16.1
•
COMPUTATIONAL DESIDERATA FOR REPRESENTATIONS
299
While there are non-trivial differences among these approaches, they all share
the notion that a meaning representation consists of structures composed from a
set of symbols, or representational vocabulary. When appropriately arranged, these
symbol structures are taken to correspond to objects, properties of objects, and rela-
tions among objects in some state of affairs being represented or reasoned about. In
this case, all four representations make use of symbols corresponding to the speaker,
a car, and a relation denoting the possession of one by the other.
Importantly, these representations can be viewed from at least two distinct per-
spectives in all of these approaches: as representations of the meaning of the par-
ticular linguistic input I have a car, and as representations of the state of affairs in
some world. It is this dual perspective that allows these representations to be used
to link linguistic inputs to the world and to our knowledge of it.
In the next sections we give some background: our desiderata for a meaning
representation language and some guarantees that these representations will actually
do what we need them to do—provide a correspondence to the state of affairs being
represented. In Section 16.3 we introduce First-Order Logic, historically the primary
technique for investigating natural language semantics, and see in Section 16.4 how
it can be used to capture the semantics of events and states in English. Chapter 17
then introduces techniques for semantic parsing: generating these formal meaning
representations given linguistic inputs.
16.1
Computational Desiderata for Representations
Let’s consider why meaning representations are needed and what they should do for
us. To focus this discussion, let’s consider a system that gives restaurant advice to
tourists based on a knowledge base.
Veriﬁability
Consider the following simple question:
(16.1) Does Maharani serve vegetarian food?
To answer this question, we have to know what it’s asking, and know whether what
it’s asking is true of Maharini or not. veriﬁability is a system’s ability to compare
veriﬁability
the state of affairs described by a representation to the state of affairs in some world
as modeled in a knowledge base. For example we’ll need some sort of representation
like Serves(Maharani,VegetarianFood), which a system can can match against its
knowledge base of facts about particular restaurants, and if it ﬁnd a representation
matching this proposition, it can answer yes. Otherwise, it must either say No if its
knowledge of local restaurants is complete, or say that it doesn’t know if it knows
its knowledge is incomplete.
Unambiguous Representations
Semantics, like all the other domains we have studied, is subject to ambiguity.
Words and sentences have different meaning representations in different contexts.
Consider the following example:
(16.2) I wanna eat someplace that’s close to ICSI.
This sentence can either mean that the speaker wants to eat at some nearby location,
or under a Godzilla-as-speaker interpretation, the speaker may want to devour some

300
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
nearby location. The sentence is ambiguous; a single linguistic expression can have
one of two meanings. But our meaning representations itself cannot be ambiguous.
The representation of an input’s meaning should be free from any ambiguity, so that
the the system can reason over a representation that means either one thing or the
other in order to decide how to answer.
A concept closely related to ambiguity is vagueness: in which a meaning repre-
vagueness
sentation leaves some parts of the meaning underspeciﬁed. Vagueness does not give
rise to multiple representations. Consider the following request:
(16.3) I want to eat Italian food.
While Italian food may provide enough information to provide recommendations, it
is nevertheless vague as to what the user really wants to eat. A vague representation
of the meaning of this phrase may be appropriate for some purposes, while a more
speciﬁc representation may be needed for other purposes.
Canonical Form
The doctrine of canonical form says that distinct inputs that mean the same thing
canonical form
should have the same meaning representation. This approach greatly simpliﬁes rea-
soning, since systems need only deal with a single meaning representation for a
potentially wide range of expressions.
Consider the following alternative ways of expressing (16.1):
(16.4) Does Maharani have vegetarian dishes?
(16.5) Do they have vegetarian food at Maharani?
(16.6) Are vegetarian dishes served at Maharani?
(16.7) Does Maharani serve vegetarian fare?
Despite the fact these alternatives use different words and syntax, we want them
to map to a single canonical meaning representations. If they were all different,
assuming the system’s knowledge base contains only a single representation of this
fact, most of the representations wouldn’t match. We could, of course, store all
possible alternative representations of the same fact in the knowledge base, but doing
so would lead to enormous difﬁcult in keeping the knowledge base consistent.
Canonical form does complicate the task of semantic parsing. Our system must
conclude that vegetarian fare, vegetarian dishes, and vegetarian food refer to the
same thing, that having and serving are equivalent here, and that all these parse
structures still lead to the same meaning representation. Or consider this pair of
examples:
(16.8) Maharani serves vegetarian dishes.
(16.9) Vegetarian dishes are served by Maharani.
Despite the different placement of the arguments to serve, a system must still assign
Maharani and vegetarian dishes to the same roles in the two examples by draw-
ing on grammatical knowledge, such as the relationship between active and passive
sentence constructions.
Inference and Variables
What about more complex requests such as:
(16.10) Can vegetarians eat at Maharani?
This request results in the same answer as the others not because they mean the same
thing, but because there is a common-sense connection between what vegetarians eat

16.2
•
MODEL-THEORETIC SEMANTICS
301
and what vegetarian restaurants serve. This is a fact about the world. We’ll need to
connect the meaning representation of this request with this fact about the world in a
knowledge base. A system must be able to use inference—to draw valid conclusions
inference
based on the meaning representation of inputs and its background knowledge. It
must be possible for the system to draw conclusions about the truth of propositions
that are not explicitly represented in the knowledge base but that are nevertheless
logically derivable from the propositions that are present.
Now consider the following somewhat more complex request:
(16.11) I’d like to ﬁnd a restaurant where I can get vegetarian food.
This request does not make reference to any particular restaurant; the user wants in-
formation about an unknown restaurant that serves vegetarian food. Since no restau-
rants are named, simple matching is not going to work. Answering this request
requires the use of variables, using some representation like the following:
variables
Serves(x,VegetarianFood)
(16.12)
Matching succeeds only if the variable x can be replaced by some object in the
knowledge base in such a way that the entire proposition will then match. The con-
cept that is substituted for the variable can then be used to fulﬁll the user’s request.
It is critical for any meaning representation language to be able to handle these kinds
of indeﬁnite references.
Expressiveness
Finally, a meaning representation scheme must be expressive enough to handle a
wide range of subject matter, ideally any sensible natural language utterance. Al-
though this is probably too much to expect from any single representational system,
First-Order Logic, as described in Section 16.3, is expressive enough to handle quite
a lot of what needs to be represented.
16.2
Model-Theoretic Semantics
What is it about about meaning representation languages that allows them to fulﬁll
these desiderata, bridging the gap from formal representations to representations that
tell us something about some state of affairs in the world?
The answer is a model. A model is a formal construct that stands for the partic-
model
ular state of affairs in the world. Expressions in a meaning representation language
can be mapped to elements of the model, like objects, properties of objects, and
relations among objects. If the model accurately captures the facts we’re interested
in, then a consistent mapping between the meaning representation and the model
provides the bridge between meaning representation and world. Models provide a
surprisingly simple and powerful way to ground the expressions in meaning repre-
sentation languages.
First, some terminology. The vocabulary of a meaning representation consists of
two parts: the non-logical vocabulary and the logical vocabulary. The non-logical
vocabulary consists of the open-ended set of names for the objects, properties, and
non-logical
vocabulary
relations that make up the world we’re trying to represent. These appear in various
schemes as predicates, nodes, labels on links, or labels in slots in frames, The log-
ical vocabulary consists of the closed set of symbols, operators, quantiﬁers, links,
logical
vocabulary

302
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
etc., that provide the formal means for composing expressions in a given meaning
representation language.
Each element of the non-logical vocabulary must have a denotation in the model,
denotation
meaning that every element corresponds to a ﬁxed, well-deﬁned part of the model.
Let’s start with objects. The domain of a model is the set of objects that are being
domain
represented. Each distinct concept, category, or individual denotes a unique element
in the domain.
We represent properties of objects in a model by denoting the domain elements
that have the property; that is, properties denote sets. The denotation of the property
red is the set of things we think are red. Similarly, a relations among object denote a
set of ordered lists, or tuples, of domain elements that take part in the relations: the
denotation of the relation Married is set of pairs of domain objects that are married.
This approach to properties and relations is called extensional, because we deﬁne
extensional
concepts by their extension, their denotations. To summarize:
• Objects denote elements of the domain
• Properties denote sets of elements of the domain
• Relations denote sets of tuples of elements of the domain
We now need a mapping that gets us from our meaning representation to the
corresponding denotations: a function that maps from the non-logical vocabulary of
our meaning representation to the proper denotations in the model. We’ll call such
a mapping an interpretation.
interpretation
Let’s return to our restaurant advice application, and let its domain consist of
sets of restaurants, patrons, facts about the likes and dislikes of the patrons, and
facts about the restaurants such as their cuisine, typical cost, and noise level. To
begin populating our domain, D, let’s assume that we’re dealing with four patrons
designated by the non-logical symbols Matthew, Franco, Katie, and Caroline. de-
noting four unique domain elements. We’ll use the constants a,b,c and, d to stand
for these domain elements. We’re deliberately using meaningless, non-mnemonic
names for our domain elements to emphasize the fact that whatever it is that we
know about these entities has to come from the formal properties of the model and
not from the names of the symbols. Continuing, let’s assume that our application
includes three restaurants, designated as Frasca, Med, and Rio in our meaning rep-
resentation, that denote the domain elements e, f, and g. Finally, let’s assume that
we’re dealing with the three cuisines Italian, Mexican, and Eclectic, denoted by h,i,
and j in our model.
Properties like Noisy denote the subset of restaurants from our domain that are
known to be noisy. Two-place relational notions, such as which restaurants individ-
ual patrons Like, denote ordered pairs, or tuples, of the objects from the domain.
And, since we decided to represent cuisines as objects in our model, we can cap-
ture which restaurants Serve which cuisines as a set of tuples. One possible state of
affairs using this scheme is given in Fig. 16.2.
Given this simple scheme, we can ground our meaning representations by con-
sulting the appropriate denotations in the corresponding model. For example, we can
evaluate a representation claiming that Matthew likes the Rio, or that The Med serves
Italian by mapping the objects in the meaning representations to their corresponding
domain elements and mapping any links, predicates, or slots in the meaning repre-
sentation to the appropriate relations in the model. More concretely, we can verify
a representation asserting that Matthew likes Frasca by ﬁrst using our interpretation
function to map the symbol Matthew to its denotation a, Frasca to e, and the Likes
relation to the appropriate set of tuples. We then check that set of tuples for the

16.2
•
MODEL-THEORETIC SEMANTICS
303
Domain
D = {a,b,c,d,e, f,g,h,i, j}
Matthew, Franco, Katie and Caroline
a,b,c,d
Frasca, Med, Rio
e, f,g
Italian, Mexican, Eclectic
h,i, j
Properties
Noisy
Noisy = {e, f,g}
Frasca, Med, and Rio are noisy
Relations
Likes
Likes = {⟨a, f⟩,⟨c, f⟩,⟨c,g⟩,⟨b,e⟩,⟨d, f⟩,⟨d,g⟩}
Matthew likes the Med
Katie likes the Med and Rio
Franco likes Frasca
Caroline likes the Med and Rio
Serves
Serves = {⟨f, j⟩,⟨g,i⟩,⟨e,h⟩}
Med serves eclectic
Rio serves Mexican
Frasca serves Italian
Figure 16.2
A model of the restaurant world.
presence of the tuple ⟨a,e⟩. If, as it is in this case, the tuple is present in the model,
then we can conclude that Matthew likes Frasca is true; if it isn’t then we can’t.
This is all pretty straightforward—we’re using sets and operations on sets to
ground the expressions in our meaning representations. Of course, the more inter-
esting part comes when we consider more complex examples such as the following:
(16.13) Katie likes the Rio and Matthew likes the Med.
(16.14) Katie and Caroline like the same restaurants.
(16.15) Franco likes noisy, expensive restaurants.
(16.16) Not everybody likes Frasca.
Our simple scheme for grounding the meaning of representations is not adequate
for examples such as these. Plausible meaning representations for these examples
will not map directly to individual entities, properties, or relations. Instead, they
involve complications such as conjunctions, equality, quantiﬁed variables, and nega-
tions. To assess whether these statements are consistent with our model, we’ll have
to tear them apart, assess the parts, and then determine the meaning of the whole
from the meaning of the parts.
Consider the ﬁrst example above. A meaning representation for this example
will include two distinct propositions expressing the individual patron’s preferences,
conjoined with some kind of implicit or explicit conjunction operator. Our model
doesn’t have a relation that encodes pairwise preferences for all of the patrons and
restaurants in our model, nor does it need to. We know from our model that Matthew
likes the Med and separately that Katie likes the Rio (that is, the tuples ⟨a, f⟩and
⟨c,g⟩are members of the set denoted by the Likes relation). All we really need to
know is how to deal with the semantics of the conjunction operator. If we assume
the simplest possible semantics for the English word and, the whole statement is
true if it is the case that each of the components is true in our model. In this case,
both components are true since the appropriate tuples are present and therefore the
sentence as a whole is true.
What we’ve done with this example is provide a truth-conditional semantics
truth-
conditional
semantics

304
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
Formula →AtomicFormula
|
Formula Connective Formula
|
Quantiﬁer Variable,... Formula
|
¬ Formula
|
(Formula)
AtomicFormula →Predicate(Term,...)
Term →Function(Term,...)
|
Constant
|
Variable
Connective →∧| ∨| =⇒
Quantiﬁer →∀| ∃
Constant →A | VegetarianFood | Maharani···
Variable →x | y | ···
Predicate →Serves | Near | ···
Function →LocationOf | CuisineOf | ···
Figure 16.3
A context-free grammar speciﬁcation of the syntax of First-Order Logic rep-
resentations. Adapted from Russell and Norvig (2002).
for the assumed conjunction operator in some meaning representation.
That is,
we’ve provided a method for determining the truth of a complex expression from
the meanings of the parts (by consulting a model) and the meaning of an operator by
consulting a truth table. Meaning representation languages are truth-conditional to
the extent that they give a formal speciﬁcation as to how we can determine the mean-
ing of complex sentences from the meaning of their parts. In particular, we need to
know the semantics of the entire logical vocabulary of the meaning representation
scheme being used.
Note that although the details of how this happens depend on details of the par-
ticular meaning representation being used, it should be clear that assessing the truth
conditions of examples like these involves nothing beyond the simple set operations
we’ve been discussing. We return to these issues in the next section in the context of
the semantics of First-Order Logic.
16.3
First-Order Logic
First-Order Logic (FOL) is a ﬂexible, well-understood, and computationally tractable
meaning representation language that satisﬁes many of the desiderata given in Sec-
tion 16.1. It provides a sound computational basis for the veriﬁability, inference,
and expressiveness requirements, as well as a sound model-theoretic semantics.
An additional attractive feature of FOL is that it makes few speciﬁc commitments
as to how things ought to be represented, and those it does are shared by many of
the schemes mentioned earlier: the represented world consists of objects, properties
of objects, and relations among objects.
The remainder of this section introduces the basic syntax and semantics of FOL
and then describes the application of FOL to the representation of events.
16.3.1
Basic Elements of First-Order Logic
Let’s explore FOL by ﬁrst examining its various atomic elements and then showing
how they can be composed to create larger meaning representations. Figure 16.3,

16.3
•
FIRST-ORDER LOGIC
305
which provides a complete context-free grammar for the particular syntax of FOL
that we will use, is our roadmap for this section.
Let’s begin by examining the notion of a term, the FOL device for representing
term
objects. As can be seen from Fig. 16.3, FOL provides three ways to represent these
basic building blocks: constants, functions, and variables. Each of these devices can
be thought of as designating an object in the world under consideration.
Constants in FOL refer to speciﬁc objects in the world being described. Such
constant
constants are conventionally depicted as either single capitalized letters such as A
and B or single capitalized words that are often reminiscent of proper nouns such as
Maharani and Harry. Like programming language constants, FOL constants refer
to exactly one object. Objects can, however, have multiple constants that refer to
them.
Functions in FOL correspond to concepts that are often expressed in English as
function
genitives such as Frasca’s location. A FOL translation of such an expression might
look like the following.
LocationOf(Frasca)
(16.17)
FOL functions are syntactically the same as single argument predicates. It is im-
portant to remember, however, that while they have the appearance of predicates,
they are in fact terms in that they refer to unique objects. Functions provide a con-
venient way to refer to speciﬁc objects without having to associate a named constant
with them. This is particularly convenient in cases in which many named objects,
like restaurants, have a unique concept such as a location associated with them.
Variables are our ﬁnal FOL mechanism for referring to objects. Variables, de-
variable
picted as single lower-case letters, let us make assertions and draw inferences about
objects without having to make reference to any particular named object. This ability
to make statements about anonymous objects comes in two ﬂavors: making state-
ments about a particular unknown object and making statements about all the objects
in some arbitrary world of objects. We return to the topic of variables after we have
presented quantiﬁers, the elements of FOL that make variables useful.
Now that we have the means to refer to objects, we can move on to the FOL
mechanisms that are used to state relations that hold among objects. Predicates are
symbols that refer to, or name, the relations that hold among some ﬁxed number
of objects in a given domain. Returning to the example introduced informally in
Section 16.1, a reasonable FOL representation for Maharani serves vegetarian food
might look like the following formula:
Serves(Maharani,VegetarianFood)
(16.18)
This FOL sentence asserts that Serves, a two-place predicate, holds between the
objects denoted by the constants Maharani and VegetarianFood.
A somewhat different use of predicates is illustrated by the following fairly typ-
ical representation for a sentence like Maharani is a restaurant:
Restaurant(Maharani)
(16.19)
This is an example of a one-place predicate that is used, not to relate multiple objects,
but rather to assert a property of a single object. In this case, it encodes the category
membership of Maharani.
With the ability to refer to objects, to assert facts about objects, and to relate
objects to one another, we can create rudimentary composite representations. These
representations correspond to the atomic formula level in Fig. 16.3. This ability

306
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
to compose complex representations is, however, not limited to the use of single
predicates. Larger composite representations can also be put together through the
use of logical connectives. As can be seen from Fig. 16.3, logical connectives let
logical
connectives
us create larger representations by conjoining logical formulas using one of three
operators. Consider, for example, the following BERP sentence and one possible
representation for it:
(16.20) I only have ﬁve dollars and I don’t have a lot of time.
Have(Speaker,FiveDollars)∧¬Have(Speaker,LotOfTime)
(16.21)
The semantic representation for this example is built up in a straightforward way
from the semantics of the individual clauses through the use of the ∧and ¬ operators.
Note that the recursive nature of the grammar in Fig. 16.3 allows an inﬁnite number
of logical formulas to be created through the use of these connectives. Thus, as with
syntax, we can use a ﬁnite device to create an inﬁnite number of representations.
16.3.2
Variables and Quantiﬁers
We now have all the machinery necessary to return to our earlier discussion of vari-
ables. As noted above, variables are used in two ways in FOL: to refer to particular
anonymous objects and to refer generically to all objects in a collection. These two
uses are made possible through the use of operators known as quantiﬁers. The two
quantiﬁers
operators that are basic to FOL are the existential quantiﬁer, which is denoted ∃and
is pronounced as “there exists”, and the universal quantiﬁer, which is denoted ∀and
is pronounced as “for all”.
The need for an existentially quantiﬁed variable is often signaled by the presence
of an indeﬁnite noun phrase in English. Consider the following example:
(16.22) a restaurant that serves Mexican food near ICSI.
Here, reference is being made to an anonymous object of a speciﬁed category with
particular properties. The following would be a reasonable representation of the
meaning of such a phrase:
∃xRestaurant(x) ∧Serves(x,MexicanFood)
(16.23)
∧Near((LocationOf(x),LocationOf(ICSI))
The existential quantiﬁer at the head of this sentence instructs us on how to
interpret the variable x in the context of this sentence. Informally, it says that for
this sentence to be true there must be at least one object such that if we were to
substitute it for the variable x, the resulting sentence would be true. For example,
if AyCaramba is a Mexican restaurant near ICSI, then substituting AyCaramba for x
results in the following logical formula:
Restaurant(AyCaramba)∧Serves(AyCaramba,MexicanFood)
(16.24)
∧Near((LocationOf(AyCaramba),LocationOf(ICSI))
Based on the semantics of the ∧operator, this sentence will be true if all of its
three component atomic formulas are true. These in turn will be true if they are
either present in the system’s knowledge base or can be inferred from other facts in
the knowledge base.
The use of the universal quantiﬁer also has an interpretation based on substi-
tution of known objects for variables. The substitution semantics for the universal

16.3
•
FIRST-ORDER LOGIC
307
quantiﬁer takes the expression for all quite literally; the ∀operator states that for the
logical formula in question to be true, the substitution of any object in the knowledge
base for the universally quantiﬁed variable should result in a true formula. This is in
marked contrast to the ∃operator, which only insists on a single valid substitution
for the sentence to be true.
Consider the following example:
(16.25) All vegetarian restaurants serve vegetarian food.
A reasonable representation for this sentence would be something like the following:
∀xVegetarianRestaurant(x) =⇒Serves(x,VegetarianFood)
(16.26)
For this sentence to be true, every substitution of a known object for x must result in a
sentence that is true. We can divide the set of all possible substitutions into the set of
objects consisting of vegetarian restaurants and the set consisting of everything else.
Let us ﬁrst consider the case in which the substituted object actually is a vegetarian
restaurant; one such substitution would result in the following sentence:
VegetarianRestaurant(Maharani) =⇒Serves(Maharani,VegetarianFood)
(16.27)
If we assume that we know that the consequent clause
Serves(Maharani,VegetarianFood)
(16.28)
is true, then this sentence as a whole must be true. Both the antecedent and the
consequent have the value True and, therefore, according to the ﬁrst two rows of
Fig. 16.4 on page 309 the sentence itself can have the value True. This result will be
the same for all possible substitutions of Terms representing vegetarian restaurants
for x.
Remember, however, that for this sentence to be true, it must be true for all
possible substitutions. What happens when we consider a substitution from the set
of objects that are not vegetarian restaurants? Consider the substitution of a non-
vegetarian restaurant such as AyCaramba for the variable x:
VegetarianRestaurant(AyCaramba)
=⇒
Serves(AyCaramba,VegetarianFood)
Since the antecedent of the implication is False, we can determine from Fig. 16.4
that the sentence is always True, again satisfying the ∀constraint.
Note that it may still be the case that AyCaramba serves vegetarian food with-
out actually being a vegetarian restaurant. Note also that, despite our choice of
examples, there are no implied categorical restrictions on the objects that can be
substituted for x by this kind of reasoning. In other words, there is no restriction of
x to restaurants or concepts related to them. Consider the following substitution:
VegetarianRestaurant(Carburetor) =⇒Serves(Carburetor,VegetarianFood)
Here the antecedent is still false so the rule remains true under this kind of irrelevant
substitution.
To review, variables in logical formulas must be either existentially (∃) or uni-
versally (∀) quantiﬁed. To satisfy an existentially quantiﬁed variable, at least one
substitution must result in a true sentence. To satisfy a universally quantiﬁed vari-
able, all substitutions must result in true sentences.

308
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
16.3.3
Lambda Notation
The ﬁnal element we need to complete our discussion of FOL is called the lambda
notation (Church, 1940). This notation provides a way to abstract from fully speci-
lambda
notation
ﬁed FOL formulas in a way that will be particularly useful for semantic analysis. The
lambda notation extends the syntax of FOL to include expressions of the following
form:
λx.P(x)
(16.29)
Such expressions consist of the Greek symbol λ, followed by one or more variables,
followed by a FOL formula that makes use of those variables.
The usefulness of these λ-expressions is based on the ability to apply them to
logical terms to yield new FOL expressions where the formal parameter variables are
bound to the speciﬁed terms. This process is known as λ-reduction, and consists
λ -reduction
of a simple textual replacement of the λ variables and the removal of the λ. The
following expressions illustrate the application of a λ-expression to the constant A,
followed by the result of performing a λ-reduction on this expression:
λx.P(x)(A)
(16.30)
P(A)
An important and useful variation of this technique is the use of one λ-expression
as the body of another as in the following expression:
λx.λy.Near(x,y)
(16.31)
This fairly abstract expression can be glossed as the state of something being near
something else. The following expressions illustrate a single λ-application and sub-
sequent reduction with this kind of embedded λ-expression:
λx.λy.Near(x,y)(Bacaro)
(16.32)
λy.Near(Bacaro,y)
The important point here is that the resulting expression is still a λ-expression; the
ﬁrst reduction bound the variable x and removed the outer λ, thus revealing the
inner expression. As might be expected, this resulting λ-expression can, in turn,
be applied to another term to arrive at a fully speciﬁed logical formula, as in the
following:
λy.Near(Bacaro,y)(Centro)
(16.33)
Near(Bacaro,Centro)
This general technique, called currying1 (Sch¨onﬁnkel, 1924) is a way of converting
currying
a predicate with multiple arguments into a sequence of single-argument predicates.
As we show in Chapter 17, the λ-notation provides a way to incrementally gather
arguments to a predicate when they do not all appear together as daughters of the
predicate in a parse tree.
16.3.4
The Semantics of First-Order Logic
The various objects, properties, and relations represented in a FOL knowledge base
acquire their meanings by virtue of their correspondence to objects, properties, and
1
Currying is the standard term, although Heim and Kratzer (1998) present an interesting argument for
the term Sch¨onkﬁnkelization over currying, since Curry later built on Sch¨onﬁnkel’s work.

16.3
•
FIRST-ORDER LOGIC
309
relations out in the external world being modeled. We can accomplish this by em-
ploying the model-theoretic approach introduced in Section 16.2. Recall that this
approach employs simple set-theoretic notions to provide a truth-conditional map-
ping from the expressions in a meaning representation to the state of affairs being
modeled. We can apply this approach to FOL by going through all the elements in
Fig. 16.3 on page 304 and specifying how each should be accounted for.
We can start by asserting that the objects in our world, FOL terms, denote ele-
ments in a domain, and asserting that atomic formulas are captured either as sets of
domain elements for properties, or as sets of tuples of elements for relations. As an
example, consider the following:
(16.34) Centro is near Bacaro.
Capturing the meaning of this example in FOL involves identifying the Terms
and Predicates that correspond to the various grammatical elements in the sentence
and creating logical formulas that capture the relations implied by the words and
syntax of the sentence. For this example, such an effort might yield something like
the following:
Near(Centro,Bacaro)
(16.35)
The meaning of this logical formula is based on whether the domain elements de-
noted by the terms Centro and Bacaro are contained among the tuples denoted by
the relation denoted by the predicate Near in the current model.
The interpretation of formulas involving logical connectives is based on the
meanings of the components in the formulas combined with the meanings of the
connectives they contain. Figure 16.4 gives interpretations for each of the logical
operators shown in Fig. 16.3.
P
Q
¬ P
P ∧Q
P ∨Q
P =⇒Q
False
False
True
False
False
True
False
True
True
False
True
True
True
False
False
False
True
False
True
True
False
True
True
True
Figure 16.4
Truth table giving the semantics of the various logical connectives.
The semantics of the ∧(and) and ¬ (not) operators are fairly straightforward,
and are correlated with at least some of the senses of the corresponding English
terms. However, it is worth pointing out that the ∨(or) operator is not disjunctive
in the same way that the corresponding English word is, and that the =⇒(im-
plies) operator is only loosely based on any common-sense notions of implication
or causation.
The ﬁnal bit we need to address involves variables and quantiﬁers. Recall that
there are no variables in our set-based models, only elements of the domain and
relations that hold among them. We can provide a model-based account for formulas
with variables by employing the notion of a substitution introduced earlier on page
306. Formulas involving ∃are true if a substitution of terms for variables results
in a formula that is true in the model. Formulas involving ∀must be true under all
possible substitutions.
16.3.5
Inference
A meaning representation language must support inference to add valid new propo-
sitions to a knowledge base or to determine the truth of propositions not explicitly

310
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
contained within a knowledge base (Section 16.1). This section brieﬂy discusses
modus ponens, the most widely implemented inference method provided by FOL.
Modus ponens is a form of inference that corresponds to what is informally
Modus ponens
known as if-then reasoning. We can abstractly deﬁne modus ponens as follows,
where α and β should be taken as FOL formulas:
α
α =⇒β
β
(16.36)
A schema like this indicates that the formula below the line can be inferred from the
formulas above the line by some form of inference. Modus ponens states that if the
left-hand side of an implication rule is true, then the right-hand side of the rule can
be inferred. In the following discussions, we will refer to the left-hand side of an
implication as the antecedent and the right-hand side as the consequent.
For a typical use of modus ponens, consider the following example, which uses
a rule from the last section:
VegetarianRestaurant(Leaf)
∀xVegetarianRestaurant(x) =⇒Serves(x,VegetarianFood)
Serves(Leaf,VegetarianFood)
(16.37)
Here, the formula VegetarianRestaurant(Leaf) matches the antecedent of the rule,
thus allowing us to use modus ponens to conclude Serves(Leaf,VegetarianFood).
Modus ponens can be put to practical use in one of two ways: forward chaining
and backward chaining. In forward chaining systems, modus ponens is used in
forward
chaining
precisely the manner just described. As individual facts are added to the knowledge
base, modus ponens is used to ﬁre all applicable implication rules. In this kind of
arrangement, as soon as a new fact is added to the knowledge base, all applicable
implication rules are found and applied, each resulting in the addition of new facts to
the knowledge base. These new propositions in turn can be used to ﬁre implication
rules applicable to them. The process continues until no further facts can be deduced.
The forward chaining approach has the advantage that facts will be present in
the knowledge base when needed, because, in a sense all inference is performed in
advance. This can substantially reduce the time needed to answer subsequent queries
since they should all amount to simple lookups. The disadvantage of this approach
is that facts that will never be needed may be inferred and stored.
In backward chaining, modus ponens is run in reverse to prove speciﬁc propo-
backward
chaining
sitions called queries. The ﬁrst step is to see if the query formula is true by determin-
ing if it is present in the knowledge base. If it is not, then the next step is to search
for applicable implication rules present in the knowledge base. An applicable rule
is one whereby the consequent of the rule matches the query formula. If there are
any such rules, then the query can be proved if the antecedent of any one them can
be shown to be true. This can be performed recursively by backward chaining on
the antecedent as a new query. The Prolog programming language is a backward
chaining system that implements this strategy.
To see how this works, let’s assume that we have been asked to verify the truth of
the proposition Serves(Leaf,VegetarianFood), assuming the facts given above the
line in (16.37). Since this proposition is not present in the knowledge base, a search
for an applicable rule is initiated resulting in the rule given above. After substituting

16.4
•
EVENT AND STATE REPRESENTATIONS
311
the constant Leaf for the variable x, our next task is to prove the antecedent of the
rule, VegetarianRestaurant(Leaf), which, of course, is one of the facts we are given.
Note that it is critical to distinguish between reasoning by backward chaining
from queries to known facts and reasoning backwards from known consequents to
unknown antecedents. To be speciﬁc, by reasoning backwards we mean that if the
consequent of a rule is known to be true, we assume that the antecedent will be as
well. For example, let’s assume that we know that Serves(Leaf,VegetarianFood) is
true. Since this fact matches the consequent of our rule, we might reason backwards
to the conclusion that VegetarianRestaurant(Leaf).
While backward chaining is a sound method of reasoning, reasoning backwards
is an invalid, though frequently useful, form of plausible reasoning. Plausible rea-
soning from consequents to antecedents is known as abduction, and as we show in
abduction
Chapter 23, is often useful in accounting for many of the inferences people make
while analyzing extended discourses.
While forward and backward reasoning are sound, neither is complete. This
complete
means that there are valid inferences that cannot be found by systems using these
methods alone. Fortunately, there is an alternative inference technique called reso-
lution that is sound and complete. Unfortunately, inference systems based on res-
resolution
olution are far more computationally expensive than forward or backward chaining
systems. In practice, therefore, most systems use some form of chaining and place
a burden on knowledge-base developers to encode the knowledge in a fashion that
permits the necessary inferences to be drawn.
16.4
Event and State Representations
Much of the semantics that we wish to capture consists of representations of states
and events. States are conditions, or properties, that remain unchanged over an
extended period of time, and events denote changes in some state of affairs. The
representation of both states and events may involve a host of participants, props,
times and locations.
The representations for events and states that we have used thus far have con-
sisted of single predicates with as many arguments as are needed to incorporate all
the roles associated with a given example. For example, the representation for Leaf
serves vegetarian fare consists of a single predicate with arguments for the entity
doing the serving and the thing served.
Serves(Leaf,VegetarianFare)
(16.38)
This approach assumes that the predicate used to represent an event verb has the
same number of arguments as are present in the verb’s syntactic subcategorization
frame. Unfortunately, this is clearly not always the case. Consider the following
examples of the verb eat:
(16.39) I ate.
(16.40) I ate a turkey sandwich.
(16.41) I ate a turkey sandwich at my desk.
(16.42) I ate at my desk.
(16.43) I ate lunch.
(16.44) I ate a turkey sandwich for lunch.

312
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
(16.45) I ate a turkey sandwich for lunch at my desk.
Clearly, choosing the correct number of arguments for the predicate represent-
ing the meaning of eat is a tricky problem. These examples introduce ﬁve distinct
arguments, or roles, in an array of different syntactic forms, locations, and combina-
tions. Unfortunately, predicates in FOL have ﬁxed arity – they take a ﬁxed number
arity
of arguments.
To address this problem, we introduce the notion of an event variable to allow
event variable
us to make assertions about particular events. To do this, we can refactor our event
predicates to have an existentially quantiﬁed variable as their ﬁrst, and only, argu-
ment. Using this event variable, we can introduce additional predicates to represent
the other information we have about the event. These predicates take an event vari-
able as their ﬁrst argument and related FOL terms as their second argument. The
following formula illustrates this scheme with the meaning representation of 16.40
from our earlier discussion.
∃e Eating(e) ∧Eater(e,Speaker)∧Eaten(e,TurkeySandwich)
Here, the quantiﬁed variable e stands for the eating event and is used to bind the
event predicate with the core information provided via the named roles Eater and
Eaten. To handle the more complex examples, we simply add additional relations
to capture the provided information, as in the following for 16.45.
∃e Eating(e) ∧Eater(e,Speaker)∧Eaten(e,TurkeySandwich)
(16.46)
∧Meal(e,Lunch)∧Location(e,Desk)
Event representations of this sort are referred to as neo-Davidsonian event rep-
neo-
Davidsonian
resentations (Davidson 1967, Parsons 1990) after the philosopher Donald Davidson
who introduced the notion of an event variable (Davidson, 1967). To summarize, in
the neo-Davidsonian approach to event representations:
• Events are captured with predicates that take a single event variable as an
argument.
• There is no need to specify a ﬁxed number of arguments for a given FOL
predicate; rather, as many roles and ﬁllers can be glued on as are provided in
the input.
• No more roles are postulated than are mentioned in the input.
• The logical connections among closely related inputs that share the same pred-
icate are satisﬁed without the need for additional inference.
This approach still leaves us with the problem of determining the set of predi-
cates needed to represent roles associated with speciﬁc events like Eater and Eaten,
as well as more general concepts like Location and Time. We’ll return to this prob-
lem in more detail in Chapter 20.
16.4.1
Representing Time
In our discussion of events, we did not seriously address the issue of capturing the
time when the represented events are supposed to have occurred. The representation
of such information in a useful form is the domain of temporal logic. This dis-
temporal logic
cussion introduces the most basic concerns of temporal logic and brieﬂy discusses
the means by which human languages convey temporal information, which, among
other things, includes tense logic, the ways that verb tenses convey temporal infor-
tense logic

16.4
•
EVENT AND STATE REPRESENTATIONS
313
mation. A more detailed discussion of robust approaches to the representation and
analysis of temporal expressions is presented in Chapter 18.
The most straightforward theory of time holds that it ﬂows inexorably forward
and that events are associated with either points or intervals in time, as on a timeline.
We can order distinct events by situating them on the timeline; one event precedes
another if the ﬂow of time leads from the ﬁrst event to the second. Accompanying
these notions in most theories is the idea of the current moment in time. Combin-
ing this notion with the idea of a temporal ordering relationship yields the familiar
notions of past, present, and future.
Many schemes can represent this kind of temporal information. The one pre-
sented here is a fairly simple one that stays within the FOL framework of reiﬁed
events that we have been pursuing. Consider the following examples:
(16.47) I arrived in New York.
(16.48) I am arriving in New York.
(16.49) I will arrive in New York.
These sentences all refer to the same kind of event and differ solely in the tense of
the verb. In our current scheme for representing events, all three would share the
following kind of representation, which lacks any temporal information:
∃eArriving(e)∧Arriver(e,Speaker)∧Destination(e,NewYork)
(16.50)
The temporal information provided by the tense of the verbs can be exploited
by predicating additional information about the event variable e. Speciﬁcally, we
can add temporal variables representing the interval corresponding to the event, the
end point of the event, and temporal predicates relating this end point to the current
time as indicated by the tense of the verb. Such an approach yields the following
representations for our arriving examples:
∃e,i,n Arriving(e) ∧Arriver(e,Speaker)∧Destination(e,NewYork)
∧IntervalOf(e,i)∧EndPoint(i,n)∧Precedes(n,Now)
∃e,i,n Arriving(e) ∧Arriver(e,Speaker)∧Destination(e,NewYork)
∧IntervalOf(e,i)∧MemberOf(i,Now)
∃e,i,n Arriving(e) ∧Arriver(e,Speaker)∧Destination(e,NewYork)
∧IntervalOf(e,i)∧EndPoint(i,n)∧Precedes(Now,n)
This representation introduces a variable to stand for the interval of time as-
sociated with the event and a variable that stands for the end of that interval. The
two-place predicate Precedes represents the notion that the ﬁrst time-point argument
precedes the second in time; the constant Now refers to the current time. For past
events, the end point of the interval must precede the current time. Similarly, for fu-
ture events the current time must precede the end of the event. For events happening
in the present, the current time is contained within the event interval.
Unfortunately, the relation between simple verb tenses and points in time is by
no means straightforward. Consider the following examples:
(16.51) Ok, we ﬂy from San Francisco to Boston at 10.
(16.52) Flight 1390 will be at the gate an hour now.
In the ﬁrst example, the present tense of the verb ﬂy is used to refer to a future event,
while in the second the future tense is used to refer to a past event.
More complications occur when we consider some of the other verb tenses. Con-
sider the following examples:

314
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
(16.53) Flight 1902 arrived late.
(16.54) Flight 1902 had arrived late.
Although both refer to events in the past, representing them in the same way seems
wrong. The second example seems to have another unnamed event lurking in the
background (e.g., Flight 1902 had already arrived late when something else hap-
pened). To account for this phenomena, Reichenbach (1947) introduced the notion
of a reference point. In our simple temporal scheme, the current moment in time
reference point
is equated with the time of the utterance and is used as a reference point for when
the event occurred (before, at, or after). In Reichenbach’s approach, the notion of
the reference point is separated from the utterance time and the event time. The
following examples illustrate the basics of this approach:
(16.55) When Mary’s ﬂight departed, I ate lunch.
(16.56) When Mary’s ﬂight departed, I had eaten lunch.
In both of these examples, the eating event has happened in the past, that is, prior
to the utterance. However, the verb tense in the ﬁrst example indicates that the eating
event began when the ﬂight departed, while the second example indicates that the
eating was accomplished prior to the ﬂight’s departure. Therefore, in Reichenbach’s
terms the departure event speciﬁes the reference point. These facts can be accom-
modated by additional constraints relating the eating and departure events. In the
ﬁrst example, the reference point precedes the eating event, and in the second exam-
ple, the eating precedes the reference point. Figure 16.5 illustrates Reichenbach’s
approach with the primary English tenses. Exercise 16.6 asks you to represent these
examples in FOL.
Past Perfect
Simple Past
Present Perfect
Simple Future
Future Perfect
Present
E
E
E
E
R
R
U
R,E
U
R,U
U,R,E
U,R
U
Figure 16.5
Reichenbach’s approach applied to various English tenses. In these diagrams,
time ﬂows from left to right, E denotes the time of the event, R denotes the reference time,
and U denotes the time of the utterance.
This discussion has focused narrowly on the broad notions of past, present, and
future and how they are signaled by various English verb tenses. Of course, lan-
guages have many other ways to convey temporal information, including temporal
expressions:
(16.57) I’d like to go at 6:45 in the morning.
(16.58) Somewhere around noon, please.

16.4
•
EVENT AND STATE REPRESENTATIONS
315
As we show in Chapter 18, grammars for such temporal expressions are of consid-
erable practical importance to information extraction and question-answering appli-
cations.
Finally, we should note that a systematic conceptual organization is reﬂected in
examples like these. In particular, temporal expressions in English are frequently
expressed in spatial terms, as is illustrated by the various uses of at, in, somewhere,
and near in these examples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphor-
ical organizations such as these, in which one domain is systematically expressed in
terms of another, are very common in languages of the world.
16.4.2
Aspect
In the last section, we discussed ways to represent the time of an event with respect
to the time of an utterance describing it. In this section, we address the notion of
aspect, which concerns a cluster of related topics, including whether an event has
aspect
ended or is ongoing, whether it is conceptualized as happening at a point in time or
over some interval, and whether any particular state in the world comes about be-
cause of it. Based on these and related notions, event expressions have traditionally
been divided into four general classes illustrated in the following examples:
Stative: I know my departure gate.
Activity: John is ﬂying.
Accomplishment: Sally booked her ﬂight.
Achievement: She found her gate.
Although the earliest versions of this classiﬁcation were discussed by Aristotle,
the one presented here is due to Vendler (1967).
Stative expressions represent the notion of an event participant having a partic-
stative
ular property, or being in a state, at a given point in time. As such, these expressions
can be thought of as capturing an aspect of a world at a single point in time. Consider
the following ATIS examples.
(16.59) I like Flight 840 arriving at 10:06.
(16.60) I need the cheapest fare.
(16.61) I want to go ﬁrst class.
In examples like these, the event participant denoted by the subject can be seen as
experiencing something at a speciﬁc point in time. Whether or not the experiencer
was in the same state earlier or will be in the future is left unspeciﬁed.
Activity expressions describe events undertaken by a participant and have no
activity
particular end point. Unlike statives, activities are seen as occurring over some span
of time and are therefore not associated with single points in time. Consider the
following examples:
(16.62) She drove a Mazda.
(16.63) I live in Brooklyn.
These examples both specify that the subject is engaged in, or has engaged in, the
activity speciﬁed by the verb for some period of time.
The ﬁnal aspectual class, achievement expressions, is similar to accomplish-
achievement
expressions
ments in that these expressions result in a state. Consider the following:
(16.64) She found her gate.
(16.65) I reached New York.

316
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
Unlike accomplishments, achievement events are thought of as happening in an in-
stant and are not equated with any particular activity leading up to the state. To be
more speciﬁc, the events in these examples may have been preceded by extended
searching or traveling events, but the events corresponding directly to found and
reach are conceived of as points, not intervals.
Note that since both accomplishments and achievements are events that result
in a state, they are sometimes characterized as subtypes of a single aspectual class.
Members of this combined class are known as telic eventualities.
telic
eventualities
16.5
Description Logics
As noted at the beginning of this chapter, a fair number of representational schemes
have been invented to capture the meaning of linguistic utterances. It is now widely
accepted that meanings represented in these various approaches can, in principle, be
translated into equivalent statements in FOL with relative ease. The difﬁculty is that
in many of these approaches the semantics of a statement are deﬁned procedurally.
That is, the meaning arises from whatever the system that interprets it does with it.
Description logics are an effort to better specify the semantics of these earlier
structured network representations and to provide a conceptual framework that is
especially well suited to certain kinds of domain modeling. Formally, the term De-
scription Logics refers to a family of logical approaches that correspond to varying
subsets of FOL. The restrictions placed on the expressiveness of Description Logics
serve to guarantee the tractability of various critical kinds of inference. Our focus
here, however, will be on the modeling aspects of DLs rather than on computational
complexity issues.
When using Description Logics to model an application domain, the emphasis
is on the representation of knowledge about categories, individuals that belong to
those categories, and the relationships that can hold among these individuals. The
set of categories, or concepts, that make up a particular application domain is called
its terminology. The portion of a knowledge base that contains the terminology is
terminology
traditionally called the TBox; this is in contrast to the ABox that contains facts about
TBox
ABox
individuals. The terminology is typically arranged into a hierarchical organization
called an ontology that captures the subset/superset relations among the categories.
ontology
Returning to our earlier culinary domain, we represented domain concepts us-
ing unary predicates such as Restaurant(x); the DL equivalent omits the variable,
so the restaurant category is simply written as Restaurant.2 To capture the fact
that a particular domain element, such as Frasca, is a restaurant, we assert Restau-
rant(Frasca) in much the same way we would in FOL. The semantics of these
categories are speciﬁed in precisely the same way that was introduced earlier in
Section 16.2: a category like Restaurant simply denotes the set of domain elements
that are restaurants.
Once we’ve speciﬁed the categories of interest in a particular domain, the next
step is to arrange them into a hierarchical structure. There are two ways to cap-
ture the hierarchical relationships present in a terminology: we can directly assert
relations between categories that are related hierarchically, or we can provide com-
plete deﬁnitions for our concepts and then rely on inference to provide hierarchical
2
DL statements are conventionally typeset with a sans serif font. We’ll follow that convention here,
reverting to our standard mathematical notation when giving FOL equivalents of DL statements.

16.5
•
DESCRIPTION LOGICS
317
relationships. The choice between these methods hinges on the use to which the re-
sulting categories will be put and the feasibility of formulating precise deﬁnitions for
many naturally occurring categories. We’ll discuss the ﬁrst option here and return to
the notion of deﬁnitions later in this section.
To directly specify a hierarchical structure, we can assert subsumption relations
subsumption
between the appropriate concepts in a terminology. The subsumption relation is
conventionally written as C ⊑D and is read as C is subsumed by D; that is, all
members of the category C are also members of the category D. Not surprisingly, the
formal semantics of this relation are provided by a simple set relation; any domain
element that is in the set denoted by C is also in the set denoted by D.
Adding the following statements to the TBox asserts that all restaurants are com-
mercial establishments and, moreover, that there are various subtypes of restaurants.
Restaurant ⊑CommercialEstablishment
(16.66)
ItalianRestaurant ⊑Restaurant
(16.67)
ChineseRestaurant ⊑Restaurant
(16.68)
MexicanRestaurant ⊑Restaurant
(16.69)
Ontologies such as this are conventionally illustrated with diagrams such as the one
shown in Fig. 16.6, where subsumption relations are denoted by links between the
nodes representing the categories.
Restaurant
Chinese
Restaurant 
Mexican
Restaurant
Italian
Restaurant
Commercial
Establishment
Figure 16.6
A graphical network representation of a set of subsumption relations in the
restaurant domain.
Note, that it was precisely the vague nature of semantic network diagrams like
this that motivated the development of Description Logics. For example, from this
diagram we can’t tell whether the given set of categories is exhaustive or disjoint.
That is, we can’t tell if these are all the kinds of restaurants that we’ll be dealing with
in our domain or whether there might be others. We also can’t tell if an individual
restaurant must fall into only one of these categories, or if it is possible, for example,
for a restaurant to be both Italian and Chinese. The DL statements given above are
more transparent in their meaning; they simply assert a set of subsumption relations
between categories and make no claims about coverage or mutual exclusion.
If an application requires coverage and disjointness information, then such in-
formation must be made explicitly. The simplest ways to capture this kind of in-
formation is through the use of negation and disjunction operators. For example,
the following assertion would tell us that Chinese restaurants can’t also be Italian
restaurants.
ChineseRestaurant ⊑not ItalianRestaurant
(16.70)

318
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
Specifying that a set of subconcepts covers a category can be achieved with disjunc-
tion, as in the following:
Restaurant ⊑
(16.71)
(or ItalianRestaurant ChineseRestaurant MexicanRestaurant)
Having a hierarchy such as the one given in Fig. 16.6 tells us next to nothing
about the concepts in it. We certainly don’t know anything about what makes a
restaurant a restaurant, much less Italian, Chinese, or expensive. What is needed are
additional assertions about what it means to be a member of any of these categories.
In Description Logics such statements come in the form of relations between the
concepts being described and other concepts in the domain. In keeping with its
origins in structured network representations, relations in Description Logics are
typically binary and are often referred to as roles, or role-relations.
To see how such relations work, let’s consider some of the facts about restaurants
discussed earlier in the chapter. We’ll use the hasCuisine relation to capture infor-
mation as to what kinds of food restaurants serve and the hasPriceRange relation
to capture how pricey particular restaurants tend to be. We can use these relations
to say something more concrete about our various classes of restaurants. Let’s start
with our ItalianRestaurant concept. As a ﬁrst approximation, we might say some-
thing uncontroversial like Italian restaurants serve Italian cuisine. To capture these
notions, let’s ﬁrst add some new concepts to our terminology to represent various
kinds of cuisine.
MexicanCuisine ⊑Cuisine
ItalianCuisine ⊑Cuisine
ChineseCuisine ⊑Cuisine
VegetarianCuisine ⊑Cuisine
ExpensiveRestaurant ⊑Restaurant
ModerateRestaurant ⊑Restaurant
CheapRestaurant ⊑Restaurant
Next, let’s revise our earlier version of ItalianRestaurant to capture cuisine infor-
mation.
ItalianRestaurant ⊑Restaurant⊓∃hasCuisine.ItalianCuisine
(16.72)
The correct way to read this expression is that individuals in the category Italian-
Restaurant are subsumed both by the category Restaurant and by an unnamed
class deﬁned by the existential clause—the set of entities that serve Italian cuisine.
An equivalent statement in FOL would be
∀xItalianRestaurant(x) →Restaurant(x)
(16.73)
∧(∃yServes(x,y)∧ItalianCuisine(y))
This FOL translation should make it clear what the DL assertions given above do
and do not entail. In particular, they don’t say that domain entities classiﬁed as Ital-
ian restaurants can’t engage in other relations like being expensive or even serving
Chinese cuisine. And critically, they don’t say much about domain entities that we
know do serve Italian cuisine. In fact, inspection of the FOL translation makes it
clear that we cannot infer that any new entities belong to this category based on their
characteristics. The best we can do is infer new facts about restaurants that we’re
explicitly told are members of this category.

16.5
•
DESCRIPTION LOGICS
319
Of course, inferring the category membership of individuals given certain char-
acteristics is a common and critical reasoning task that we need to support. This
brings us back to the alternative approach to creating hierarchical structures in a
terminology: actually providing a deﬁnition of the categories we’re creating in the
form of necessary and sufﬁcient conditions for category membership. In this case,
we might explicitly provide a deﬁnition for ItalianRestaurant as being those restau-
rants that serve Italian cuisine, and ModerateRestaurant as being those whose
price range is moderate.
ItalianRestaurant ≡Restaurant⊓∃hasCuisine.ItalianCuisine
(16.74)
ModerateRestaurant ≡Restaurant⊓hasPriceRange.ModeratePrices
(16.75)
While our earlier statements provided necessary conditions for membership in these
categories, these statements provide both necessary and sufﬁcient conditions.
Finally, let’s now consider the superﬁcially similar case of vegetarian restaurants.
Clearly, vegetarian restaurants are those that serve vegetarian cuisine. But they don’t
merely serve vegetarian fare, that’s all they serve. We can accommodate this kind of
constraint by adding an additional restriction in the form of a universal quantiﬁer to
our earlier description of VegetarianRestaurants, as follows:
VegetarianRestaurant ≡Restaurant
(16.76)
⊓∃hasCuisine.VegetarianCuisine
⊓∀hasCuisine.VegetarianCuisine
Inference
Paralleling the focus of Description Logics on categories, relations, and individuals
is a processing focus on a restricted subset of logical inference. Rather than employ-
ing the full range of reasoning permitted by FOL, DL reasoning systems emphasize
the closely coupled problems of subsumption and instance checking.
Subsumption, as a form of inference, is the task of determining, based on the
subsumption
facts asserted in a terminology, whether a superset/subset relationship exists between
two concepts. Correspondingly, instance checking asks if an individual can be a
instance
checking
member of a particular category given the facts we know about both the individual
and the terminology. The inference mechanisms underlying subsumption and in-
stance checking go beyond simply checking for explicitly stated subsumption rela-
tions in a terminology. They must explicitly reason using the relational information
asserted about the terminology to infer appropriate subsumption and membership
relations.
Returning to our restaurant domain, let’s add a new kind of restaurant using the
following statement:
IlFornaio ⊑ModerateRestaurant⊓∃hasCuisine.ItalianCuisine
(16.77)
Given this assertion, we might ask whether the IlFornaio chain of restaurants might
be classiﬁed as an Italian restaurant or a vegetarian restaurant. More precisely, we
can pose the following questions to our reasoning system:
IlFornaio ⊑ItalianRestaurant
(16.78)
IlFornaio ⊑VegetarianRestaurant
(16.79)
The answer to the ﬁrst question is positive since IlFornaio meets the criteria we
speciﬁed for the category ItalianRestaurant: it’s a Restaurant since we explicitly

320
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
Restaurant
Chinese
Restaurant 
Mexican
Restaurant
Italian
Restaurant
Expensive
Restaurant
Cheap
Restaurant
Moderate
Restaurant
Il Fornaio
Vegetarian
Restaurant
Figure 16.7
A graphical network representation of the complete set of subsumption rela-
tions in the restaurant domain given the current set of assertions in the TBox.
classiﬁed it as a ModerateRestaurant, which is a subtype of Restaurant, and it
meets the has.Cuisine class restriction since we’ve asserted that directly.
The answer to the second question is negative. Recall, that our criteria for veg-
etarian restaurants contains two requirements: it has to serve vegetarian fare, and
that’s all it can serve. Our current deﬁnition for IlFornaio fails on both counts since
we have not asserted any relations that state that IlFornaio serves vegetarian fare,
and the relation we have asserted, hasCuisine.ItalianCuisine, contradicts the sec-
ond criteria.
A related reasoning task, based on the basic subsumption inference, is to derive
the implied hierarchy for a terminology given facts about the categories in the ter-
implied
hierarchy
minology. This task roughly corresponds to a repeated application of the subsump-
tion operator to pairs of concepts in the terminology. Given our current collection of
statements, the expanded hierarchy shown in Fig. 16.7 can be inferred. You should
convince yourself that this diagram contains all and only the subsumption links that
should be present given our current knowledge.
Instance checking is the task of determining whether a particular individual can
be classiﬁed as a member of a particular category. This process takes what is known
about a given individual, in the form of relations and explicit categorical statements,
and then compares that information with what is known about the current terminol-
ogy. It then returns a list of the most speciﬁc categories to which the individual can
belong.
As an example of a categorization problem, consider an establishment that we’re
told is a restaurant and serves Italian cuisine.
Restaurant(Gondolier)
hasCuisine(Gondolier,ItalianCuisine)
Here, we’re being told that the entity denoted by the term Gondolier is a restau-
rant and serves Italian food. Given this new information and the contents of our
current TBox, we might reasonably like to ask if this is an Italian restaurant, if it is
a vegetarian restaurant, or if it has moderate prices.
Assuming the deﬁnitional statements given earlier, we can indeed categorize
the Gondolier as an Italian restaurant. That is, the information we’ve been given
about it meets the necessary and sufﬁcient conditions required for membership in
this category. And as with the IlFornaio category, this individual fails to match the
stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also

16.6
•
SUMMARY
321
turn out to be a moderately priced restaurant, but we can’t tell at this point since
we don’t know anything about its prices. What this means is that given our current
knowledge the answer to the query ModerateRestaurant(Gondolier) would be false
since it lacks the required hasPriceRange relation.
The implementation of subsumption, instance checking, as well as other kinds of
inferences needed for practical applications, varies according to the expressivity of
the Description Logic being used. However, for a Description Logic of even modest
power, the primary implementation techniques are based on satisﬁability methods
that in turn rely on the underlying model-based semantics introduced earlier in this
chapter.
OWL and the Semantic Web
The highest-proﬁle role for Description Logics, to date, has been as a part of the
development of the Semantic Web. The Semantic Web is an ongoing effort to pro-
vide a way to formally specify the semantics of the contents of the Web (Fensel
et al., 2003). A key component of this effort involves the creation and deployment
of ontologies for various application areas of interest. The meaning representation
language used to represent this knowledge is the Web Ontology Language (OWL)
Web Ontology
Language
(McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that
corresponds roughly to the one we’ve been describing here.
16.6
Summary
This chapter has introduced the representational approach to meaning. The follow-
ing are some of the highlights of this chapter:
• A major approach to meaning in computational linguistics involves the cre-
ation of formal meaning representations that capture the meaning-related
content of linguistic inputs. These representations are intended to bridge the
gap from language to common-sense knowledge of the world.
• The frameworks that specify the syntax and semantics of these representa-
tions are called meaning representation languages. A wide variety of such
languages are used in natural language processing and artiﬁcial intelligence.
• Such representations need to be able to support the practical computational
requirements of semantic processing. Among these are the need to determine
the truth of propositions, to support unambiguous representations, to rep-
resent variables, to support inference, and to be sufﬁciently expressive.
• Human languages have a wide variety of features that are used to convey
meaning. Among the most important of these is the ability to convey a predicate-
argument structure.
• First-Order Logic is a well-understood, computationally tractable meaning
representation language that offers much of what is needed in a meaning rep-
resentation language.
• Important elements of semantic representation including states and events
can be captured in FOL.
• Semantic networks and frames can be captured within the FOL framework.
• Modern Description Logics consist of useful and computationally tractable
subsets of full First-Order Logic. The most prominent use of a description

322
CHAPTER 16
•
LOGICAL REPRESENTATIONS OF SENTENCE MEANING
logic is the Web Ontology Language (OWL), used in the speciﬁcation of the
Semantic Web.
Bibliographical and Historical Notes
The earliest computational use of declarative meaning representations in natural lan-
guage processing was in the context of question-answering systems (Green et al. 1961,
Raphael 1968, Lindsey 1963). These systems employed ad hoc representations for
the facts needed to answer questions. Questions were then translated into a form that
could be matched against facts in the knowledge base. Simmons (1965) provides an
overview of these early efforts.
Woods (1967) investigated the use of FOL-like representations in question an-
swering as a replacement for the ad hoc representations in use at the time. Woods
(1973) further developed and extended these ideas in the landmark Lunar system.
Interestingly, the representations used in Lunar had both truth-conditional and pro-
cedural semantics. Winograd (1972) employed a similar representation based on the
Micro-Planner language in his SHRDLU system.
During this same period, researchers interested in the cognitive modeling of lan-
guage and memory had been working with various forms of associative network
representations. Masterman (1957) was the ﬁrst to make computational use of a
semantic network-like knowledge representation, although semantic networks are
generally credited to Quillian (1968). A considerable amount of work in the seman-
tic network framework was carried out during this era (Norman and Rumelhart 1975,
Schank 1972, Wilks 1975c, Wilks 1975b, Kintsch 1974). It was during this period
that a number of researchers began to incorporate Fillmore’s notion of case roles
(Fillmore, 1968) into their representations. Simmons (1973) was the earliest adopter
of case roles as part of representations for natural language processing.
Detailed analyses by Woods (1975) and Brachman (1979) aimed at ﬁguring out
what semantic networks actually mean led to the development of a number of more
sophisticated network-like languages including KRL (Bobrow and Winograd, 1977)
and KL-ONE (Brachman and Schmolze, 1985). As these frameworks became more
sophisticated and well deﬁned, it became clear that they were restricted variants of
FOL coupled with specialized indexing inference procedures. A useful collection of
papers covering much of this work can be found in Brachman and Levesque (1985).
Russell and Norvig (2002) describe a modern perspective on these representational
efforts.
Linguistic efforts to assign semantic structures to natural language sentences in
the generative era began with the work of Katz and Fodor (1963). The limitations
of their simple feature-based representations and the natural ﬁt of logic to many
of the linguistic problems of the day quickly led to the adoption of a variety of
predicate-argument structures as preferred semantic representations (Lakoff 1972a,
McCawley 1968). The subsequent introduction by Montague (1973) of the truth-
conditional model-theoretic framework into linguistic theory led to a much tighter
integration between theories of formal syntax and a wide range of formal semantic
frameworks. Good introductions to Montague semantics and its role in linguistic
theory can be found in Dowty et al. (1981) and Partee (1976).
The representation of events as reiﬁed objects is due to Davidson (1967). The
approach presented here, which explicitly reiﬁes event participants, is due to Parsons

EXERCISES
323
(1990).
Most current computational approaches to temporal reasoning are based on Allen’s
notion of temporal intervals (Allen, 1984); see Chapter 18. ter Meulen (1995) pro-
vides a modern treatment of tense and aspect. Davis (1990) describes the use of FOL
to represent knowledge across a wide range of common-sense domains including
quantities, space, time, and beliefs.
A recent comprehensive treatment of logic and language can be found in van
Benthem and ter Meulen (1997). A classic semantics text is Lyons (1977). McCaw-
ley (1993) is an indispensable textbook covering a wide range of topics concerning
logic and language. Chierchia and McConnell-Ginet (1991) also broadly covers
semantic issues from a linguistic perspective. Heim and Kratzer (1998) is a more
recent text written from the perspective of current generative theory.
Exercises
16.1 Peruse your daily newspaper for three examples of ambiguous sentences or
headlines. Describe the various sources of the ambiguities.
16.2 Consider a domain in which the word coffee can refer to the following con-
cepts in a knowledge-based system: a caffeinated or decaffeinated beverage,
ground coffee used to make either kind of beverage, and the beans themselves.
Give arguments as to which of the following uses of coffee are ambiguous and
which are vague.
1. I’ve had my coffee for today.
2. Buy some coffee on your way home.
3. Please grind some more coffee.
16.3 The following rule, which we gave as a translation for Example 16.25, is not
a reasonable deﬁnition of what it means to be a vegetarian restaurant.
∀xVegetarianRestaurant(x) =⇒Serves(x,VegetarianFood)
Give a FOL rule that better deﬁnes vegetarian restaurants in terms of what they
serve.
16.4 Give FOL translations for the following sentences:
1. Vegetarians do not eat meat.
2. Not all vegetarians eat eggs.
16.5 Give a set of facts and inferences necessary to prove the following assertions:
1. McDonald’s is not a vegetarian restaurant.
2. Some vegetarians can eat at McDonald’s.
Don’t just place these facts in your knowledge base. Show that they can be
inferred from some more general facts about vegetarians and McDonald’s.
16.6 For the following sentences, give FOL translations that capture the temporal
relationships between the events.
1. When Mary’s ﬂight departed, I ate lunch.
2. When Mary’s ﬂight departed, I had eaten lunch.
16.7 On page 309, we gave the representation Near(Centro,Bacaro) as a transla-
tion for the sentence Centro is near Bacaro. In a truth-conditional semantics,
this formula is either true or false given some model. Critique this truth-
conditional approach with respect to the meaning of words like near.

CHAPTER
17
Computational Semantics and
Semantic Parsing
Placeholder
324

CHAPTER
18
Information Extraction
I am the very model of a modern Major-General,
I’ve information vegetable, animal, and mineral,
I know the kings of England, and I quote the ﬁghts historical
From Marathon to Waterloo, in order categorical...
Gilbert and Sullivan, Pirates of Penzance
Imagine that you are an analyst with an investment ﬁrm that tracks airline stocks.
You’re given the task of determining the relationship (if any) between airline an-
nouncements of fare increases and the behavior of their stocks the next day. His-
torical data about stock prices is easy to come by, but what about the airline an-
nouncements? You will need to know at least the name of the airline, the nature of
the proposed fare hike, the dates of the announcement, and possibly the response of
other airlines. Fortunately, these can be all found in news articles like this one:
Citing high fuel prices, United Airlines said Friday it has increased fares
by $6 per round trip on ﬂights to some cities also served by lower-
cost carriers. American Airlines, a unit of AMR Corp., immediately
matched the move, spokesman Tim Wagner said. United, a unit of UAL
Corp., said the increase took effect Thursday and applies to most routes
where it competes against discount carriers, such as Chicago to Dallas
and Denver to San Francisco.
This chapter presents techniques for extracting limited kinds of semantic con-
tent from text. This process of information extraction (IE), turns the unstructured
information
extraction
information embedded in texts into structured data, for example for populating a
relational database to enable further processing.
We begin with the ﬁrst step in most IE tasks, ﬁnding the proper names or named
entities in a text. The task of named entity recognition (NER) is to ﬁnd each
named entity
recognition
mention of a named entity in the text and label its type. What constitutes a named
entity type is task speciﬁc; people, places, and organizations are common, but gene
or protein names (Cohen and Demner-Fushman, 2014) or ﬁnancial asset classes
might be relevant for some tasks. Once all the named entities in a text have been
extracted, they can be linked together in sets corresponding to real-world entities,
inferring, for example, that mentions of United Airlines and United refer to the same
company. This is the joint task of coreference resolution and entity linking which
we defer til Chapter 22.
Next, we turn to the task of relation extraction: ﬁnding and classifying semantic
relation
extraction
relations among the text entities. These are often binary relations like child-of, em-
ployment, part-whole, and geospatial relations. Relation extraction has close links
to populating a relational database.
Finally, we discuss three tasks related to events. Event extraction is ﬁnding
event
extraction
events in which these entities participate, like, in our sample text, the fare increases

326
CHAPTER 18
•
INFORMATION EXTRACTION
by United and American and the reporting events said and cite. Event coreference
(Chapter 22) is needed to ﬁgure out which event mentions in a text refer to the same
event; in our running example the two instances of increase and the phrase the move
all refer to the same event.
To ﬁgure out when the events in a text happened we extract temporal expres-
sions like days of the week (Friday and Thursday), relative expressions like two
temporal
expression
days from now or next year and times such as 3:30 P.M.. These expressions must be
normalized onto speciﬁc calendar dates or times of day to situate events in time. In
temporal
normalization
our sample task, this will allow us to link Friday to the time of United’s announce-
ment, and Thursday to the previous day’s fare increase, and produce a timeline in
which United’s announcement follows the fare increase and American’s announce-
ment follows both of those events.
Finally, many texts describe recurring stereotypical events or situations. The task
of template ﬁlling is to ﬁnd such situations in documents and ﬁll in the template
template ﬁlling
slots. These slot-ﬁllers may consist of text segments extracted directly from the text,
or concepts like times, amounts, or ontology entities that have been inferred from
text elements through additional processing.
Our airline text is an example of this kind of stereotypical situation since airlines
often raise fares and then wait to see if competitors follow along. In this situa-
tion, we can identify United as a lead airline that initially raised its fares, $6 as the
amount, Thursday as the increase date, and American as an airline that followed
along, leading to a ﬁlled template like the following.
FARE-RAISE ATTEMPT:


LEAD AIRLINE:
UNITED AIRLINES
AMOUNT:
$6
EFFECTIVE DATE:
2006-10-26
FOLLOWER:
AMERICAN AIRLINES


18.1
Named Entity Recognition
The ﬁrst step in information extraction is to detect the entities in the text. A named
entity is, roughly speaking, anything that can be referred to with a proper name:
named entity
a person, a location, an organization. The term is commonly extended to include
things that aren’t entities per se, including dates, times, and other kinds of temporal
expressions, and even numerical expressions like prices. Here’s the sample text
temporal
expressions
introduced earlier with the named entities marked:
Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it
has increased fares by [MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ORG American Airlines], a
unit of [ORG AMR Corp.], immediately matched the move, spokesman
[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],
said the increase took effect [TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [LOC Chicago]
to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].
The text contains 13 mentions of named entities including 5 organizations, 4 loca-
tions, 2 times, 1 person, and 1 mention of money.
In addition to their use in extracting events and the relationship between par-
ticipants, named entities are useful for many other language processing tasks. In

18.1
•
NAMED ENTITY RECOGNITION
327
sentiment analysis we might want to know a consumer’s sentiment toward a partic-
ular entity. Entities are a useful ﬁrst stage in question answering, or for linking text
to information in structured knowledge sources like Wikipedia.
Figure 18.1 shows typical generic named entity types. Many applications will
also need to use speciﬁc entity types like proteins, genes, commercial products, or
works of art.
Type
Tag Sample Categories
Example sentences
People
PER people, characters
Turing is a giant of computer science.
Organization ORG companies, sports teams
The IPCC warned about the cyclone.
Location
LOC regions, mountains, seas
The Mt. Sanitas loop is in Sunshine Canyon.
Geo-Political
Entity
GPE countries, states, provinces
Palo Alto is raising the fees for parking.
Facility
FAC bridges, buildings, airports
Consider the Golden Gate Bridge.
Vehicles
VEH planes, trains, automobiles
It was a classic Ford Falcon.
Figure 18.1
A list of generic named entity types with the kinds of entities they refer to.
Named entity recognition means ﬁnding spans of text that constitute proper
names and then classifying the type of the entity. Recognition is difﬁcult partly be-
cause of the ambiguity of segmentation; we need to decide what’s an entity and what
isn’t, and where the boundaries are. Another difﬁculty is caused by type ambiguity.
The mention JFK can refer to a person, the airport in New York, or any number of
schools, bridges, and streets around the United States. Some examples of this kind
of cross-type confusion are given in Figures 18.2 and 18.3.
Name
Possible Categories
Washington
Person, Location, Political Entity, Organization, Vehicle
Downing St.
Location, Organization
IRA
Person, Organization, Monetary Instrument
Louis Vuitton
Person, Organization, Commercial Product
Figure 18.2
Common categorical ambiguities associated with various proper names.
[PER Washington] was born into slavery on the farm of James Burroughs.
[ORG Washington] went up 2 games to 1 in the four-game series.
Blair arrived in [LOC Washington] for what may well be his last state visit.
In June, [GPE Washington] passed a primary seatbelt law.
The [VEH Washington] had proved to be a leaky ship, every passage I made...
Figure 18.3
Examples of type ambiguities in the use of the name Washington.
18.1.1
NER as Sequence Labeling
The standard algorithm for named entity recognition is as a word-by-word sequence
labeling task, in which the assigned tags capture both the boundary and the type. A
sequence classiﬁer like an MEMM/CRF, a bi-LSTM, or a transformer is trained to
label the tokens in a text with tags that indicate the presence of particular kinds of
named entities. Consider the following simpliﬁed excerpt from our running exam-
ple.
[ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched
the move, spokesman [PER Tim Wagner] said.

328
CHAPTER 18
•
INFORMATION EXTRACTION
Figure 18.4 shows the same excerpt represented with IOB tagging. In IOB tag-
IOB
ging we introduce a tag for the beginning (B) and inside (I) of each entity type,
and one for tokens outside (O) any entity. The number of tags is thus 2n + 1 tags,
where n is the number of entity types. IOB tagging can represent exactly the same
information as the bracketed notation.
Words
IOB Label
IO Label
American
B-ORG
I-ORG
Airlines
I-ORG
I-ORG
,
O
O
a
O
O
unit
O
O
of
O
O
AMR
B-ORG
I-ORG
Corp.
I-ORG
I-ORG
,
O
O
immediately
O
O
matched
O
O
the
O
O
move
O
O
,
O
O
spokesman
O
O
Tim
B-PER
I-PER
Wagner
I-PER
I-PER
said
O
O
.
O
O
Figure 18.4
Named entity tagging as a sequence model, showing IOB and IO encodings.
We’ve also shown IO tagging, which loses some information by eliminating the
B tag. Without the B tag IO tagging is unable to distinguish between two entities of
the same type that are right next to each other. Since this situation doesn’t arise very
often (usually there is at least some punctuation or other deliminator), IO tagging
may be sufﬁcient, and has the advantage of using only n+1 tags.
In the following three sections we introduce the three standard families of al-
gorithms for NER tagging: feature based (MEMM/CRF), neural (bi-LSTM), and
rule-based.
18.1.2
A feature-based algorithm for NER
identity of wi, identity of neighboring words
embeddings for wi, embeddings for neighboring words
part of speech of wi, part of speech of neighboring words
base-phrase syntactic chunk label of wi and neighboring words
presence of wi in a gazetteer
wi contains a particular preﬁx (from all preﬁxes of length ≤4)
wi contains a particular sufﬁx (from all sufﬁxes of length ≤4)
wi is all upper case
word shape of wi, word shape of neighboring words
short word shape of wi, short word shape of neighboring words
presence of hyphen
Figure 18.5
Typical features for a feature-based NER system.

18.1
•
NAMED ENTITY RECOGNITION
329
The ﬁrst approach is to extract features and train an MEMM or CRF sequence
model of the type we saw for part-of-speech tagging in Chapter 8. Figure 18.5 lists
standard features used in such feature-based systems. We’ve seen many of these
features before in the context of part-of-speech tagging, particularly for tagging un-
known words. This is not surprising, as many unknown words are in fact named
entities. Word shape features are thus particularly important in the context of NER.
Recall that word shape features are used to represent the abstract letter pattern of
word shape
the word by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and
retaining punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30
would map to XXdd-dd. A second class of shorter word shape features is also used.
In these features consecutive character types are removed, so DC10-30 would be
mapped to Xd-d but I.M.F would still map to X.X.X. This feature by itself accounts
for a considerable part of the success of feature-based NER systems for English
news text. Shape features are also particularly important in recognizing names of
proteins and genes in biological texts.
For example the named entity token L’Occitane would generate the following
non-zero valued feature values:
preﬁx(wi) = L
sufﬁx(wi) = tane
preﬁx(wi) = L’
sufﬁx(wi) = ane
preﬁx(wi) = L’O
sufﬁx(wi) = ne
preﬁx(wi) = L’Oc
sufﬁx(wi) = e
word-shape(wi) = X’Xxxxxxxx
short-word-shape(wi) = X’Xx
A gazetteer is a list of place names, often providing millions of entries for lo-
gazetteer
cations with detailed geographical and political information.1 A related resource
is name-lists; the United States Census Bureau also provides extensive lists of ﬁrst
names and surnames derived from its decadal census in the U.S.2 Similar lists of cor-
porations, commercial products, and all manner of things biological and mineral are
also available from a variety of sources. Gazetteer and name features are typically
implemented as a binary feature for each name list. Unfortunately, such lists can
be difﬁcult to create and maintain, and their usefulness varies considerably. While
gazetteers can be quite effective, lists of persons and organizations are not always
helpful (Mikheev et al., 1999).
Feature effectiveness depends on the application, genre, media, and language.
For example, shape features, critical for English newswire texts, are of little use
with automatic speech recognition transcripts, or other non-edited or informally-
edited sources, or for languages like Chinese that don’t use orthographic case. The
features in Fig. 18.5 should therefore be thought of as only a starting point.
Figure 18.6 illustrates the result of adding part-of-speech tags, syntactic base-
phrase chunk tags, and some shape information to our earlier example.
Given such a training set, a sequence classiﬁer like an MEMM can be trained to
label new sentences. Figure 18.7 illustrates the operation of such a sequence labeler
at the point where the token Corp. is next to be labeled. If we assume a context win-
dow that includes the two preceding and following words, then the features available
to the classiﬁer are those shown in the boxed area.
1
www.geonames.org
2
www.census.gov

330
CHAPTER 18
•
INFORMATION EXTRACTION
Word
POS
Chunk
Short shape
Label
American
NNP
B-NP
Xx
B-ORG
Airlines
NNPS I-NP
Xx
I-ORG
,
,
O
,
O
a
DT
B-NP
x
O
unit
NN
I-NP
x
O
of
IN
B-PP
x
O
AMR
NNP
B-NP
X
B-ORG
Corp.
NNP
I-NP
Xx.
I-ORG
,
,
O
,
O
immediately RB
B-ADVP x
O
matched
VBD
B-VP
x
O
the
DT
B-NP
x
O
move
NN
I-NP
x
O
,
,
O
,
O
spokesman
NN
B-NP
x
O
Tim
NNP
I-NP
Xx
B-PER
Wagner
NNP
I-NP
Xx
I-PER
said
VBD
B-VP
x
O
.
,
O
.
O
Figure 18.6
Word-by-word feature encoding for NER.
Classifier
IN
NNP
NNP
RB
unit
of
a
...
x
B-PP
...
AMR
Corp.
immediately
B-NP
X
I-NP
X.
B-ADVP
x
O
B-ORG
?
...
...
,
,
O ,
matched
Figure 18.7
Named entity recognition as sequence labeling. The features available to the classiﬁer during
training and classiﬁcation are those in the boxed area.
18.1.3
A neural algorithm for NER
The standard neural algorithm for NER is based on the bi-LSTM introduced in Chap-
ter 9. Recall that in that model, word and character embeddings are computed for
input word wi. These are passed through a left-to-right LSTM and a right-to-left
LSTM, whose outputs are concatenated (or otherwise combined) to produce a sin-
gle output layer at position i. In the simplest method, this layer can then be directly
passed onto a softmax that creates a probability distribution over all NER tags, and
the most likely tag is chosen as ti.
For named entity tagging this greedy approach to decoding is insufﬁcient, since
it doesn’t allow us to impose the strong constraints neighboring tokens have on each
other (e.g., the tag I-PER must follow another I-PER or B-PER). Instead a CRF layer
is normally used on top of the bi-LSTM output, and the Viterbi decoding algorithm
is used to decode. Fig. 18.8 shows a sketch of the algorithm

18.1
•
NAMED ENTITY RECOGNITION
331
Mark
Watney
visits
Mars
LSTM1
LSTM1
LSTM1
LSTM1
LSTM2
LSTM2
LSTM2
LSTM2
Concatenation
Right-to-left LSTM
Left-to-right LSTM
B-PER
I-PER
O
B-LOC
CRF Layer
Char LSTM
Char LSTM
Char LSTM
Char LSTM
GloVe
GloVe
GloVe
GloVe
Char+GloVe
Embeddings
Figure 18.8
Putting it all together: character embeddings and words together in a bi-LSTM
sequence model. After Lample et al. (2016b).
18.1.4
Rule-based NER
While machine learned (neural or MEMM/CRF) sequence models are the norm in
academic research, commercial approaches to NER are often based on pragmatic
combinations of lists and rules, with some smaller amount of supervised machine
learning (Chiticariu et al., 2013). For example IBM System T is a text understand-
ing architecture in which a user speciﬁes complex declarative constraints for tagging
tasks in a formal query language that includes regular expressions, dictionaries, se-
mantic constraints, NLP operators, and table structures, all of which the system
compiles into an efﬁcient extractor (Chiticariu et al., 2018).
One common approach is to make repeated rule-based passes over a text, allow-
ing the results of one pass to inﬂuence the next. The stages typically ﬁrst involve
the use of rules that have extremely high precision but low recall. Subsequent stages
employ more error-prone statistical methods that take the output of the ﬁrst pass into
account.
1. First, use high-precision rules to tag unambiguous entity mentions.
2. Then, search for substring matches of the previously detected names.
3. Consult application-speciﬁc name lists to identify likely name entity mentions
from the given domain.
4. Finally, apply probabilistic sequence labeling techniques that make use of the
tags from previous stages as additional features.
The intuition behind this staged approach is twofold. First, some of the entity
mentions in a text will be more clearly indicative of a given entity’s class than others.
Second, once an unambiguous entity mention is introduced into a text, it is likely that
subsequent shortened versions will refer to the same entity (and thus the same type
of entity).
18.1.5
Evaluation of Named Entity Recognition
The familiar metrics of recall, precision, and F1 measure are used to evaluate NER
systems. Remember that recall is the ratio of the number of correctly labeled re-
sponses to the total that should have been labeled; precision is the ratio of the num-

332
CHAPTER 18
•
INFORMATION EXTRACTION
ARTIFACT
GENERAL
AFFILIATION
ORG
AFFILIATION
PART-
WHOLE
PERSON-
SOCIAL
PHYSICAL
Located
Near
Business
Family
Lasting 
Personal
Citizen-
Resident-
Ethnicity-
Religion
Org-Location-
Origin
Founder
Employment
Membership
Ownership
Student-Alum
Investor
User-Owner-Inventor-
Manufacturer
Geographical
Subsidiary
Sports-Affiliation
Figure 18.9
The 17 relations used in the ACE relation extraction task.
ber of correctly labeled responses to the total labeled; and F-measure is the harmonic
mean of the two. For named entities, the entity rather than the word is the unit of
response. Thus in the example in Fig. 18.6, the two entities Tim Wagner and AMR
Corp. and the non-entity said would each count as a single response.
The fact that named entity tagging has a segmentation component which is not
present in tasks like text categorization or part-of-speech tagging causes some prob-
lems with evaluation. For example, a system that labeled American but not American
Airlines as an organization would cause two errors, a false positive for O and a false
negative for I-ORG. In addition, using entities as the unit of response but words as
the unit of training means that there is a mismatch between the training and test
conditions.
18.2
Relation Extraction
Next on our list of tasks is to discern the relationships that exist among the detected
entities. Let’s return to our sample airline text:
Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it
has increased fares by [MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ORG American Airlines], a
unit of [ORG AMR Corp.], immediately matched the move, spokesman
[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],
said the increase took effect [TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [LOC Chicago]
to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].
The text tells us, for example, that Tim Wagner is a spokesman for American
Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR.
These binary relations are instances of more generic relations such as part-of or
employs that are fairly frequent in news-style texts. Figure 18.9 lists the 17 relations
used in the ACE relation extraction evaluations and Fig. 18.10 shows some sample
relations. We might also extract more domain-speciﬁc relation such as the notion of
an airline route. For example from this text we can conclude that United has routes
to Chicago, Dallas, Denver, and San Francisco.

18.2
•
RELATION EXTRACTION
333
Relations
Types
Examples
Physical-Located
PER-GPE
He was in Tennessee
Part-Whole-Subsidiary
ORG-ORG
XYZ, the parent company of ABC
Person-Social-Family
PER-PER
Yoko’s husband John
Org-AFF-Founder
PER-ORG
Steve Jobs, co-founder of Apple...
Figure 18.10
Semantic relations with examples and the named entity types they involve.
Domain
D = {a,b,c,d,e, f,g,h,i}
United, UAL, American Airlines, AMR
a,b,c,d
Tim Wagner
e
Chicago, Dallas, Denver, and San Francisco
f,g,h,i
Classes
United, UAL, American, and AMR are organizations
Org = {a,b,c,d}
Tim Wagner is a person
Pers = {e}
Chicago, Dallas, Denver, and San Francisco are places
Loc = { f,g,h,i}
Relations
United is a unit of UAL
PartOf = {⟨a,b⟩,⟨c,d⟩}
American is a unit of AMR
Tim Wagner works for American Airlines
OrgAff = {⟨c,e⟩}
United serves Chicago, Dallas, Denver, and San Francisco
Serves = {⟨a, f⟩,⟨a,g⟩,⟨a,h⟩,⟨a,i⟩}
Figure 18.11
A model-based view of the relations and entities in our sample text.
These relations correspond nicely to the model-theoretic notions we introduced
in Chapter 16 to ground the meanings of the logical forms. That is, a relation consists
of a set of ordered tuples over elements of a domain. In most standard information-
extraction applications, the domain elements correspond to the named entities that
occur in the text, to the underlying entities that result from co-reference resolution, or
to entities selected from a domain ontology. Figure 18.11 shows a model-based view
of the set of entities and relations that can be extracted from our running example.
Notice how this model-theoretic view subsumes the NER task as well; named entity
recognition corresponds to the identiﬁcation of a class of unary relations.
Sets of relations have been deﬁned for many other domains as well. For example
UMLS, the Uniﬁed Medical Language System from the US National Library of
Medicine has a network that deﬁnes 134 broad subject categories, entity types, and
54 relations between the entities, such as the following:
Entity
Relation
Entity
Injury
disrupts
Physiological Function
Bodily Location
location-of Biologic Function
Anatomical Structure
part-of
Organism
Pharmacologic Substance causes
Pathological Function
Pharmacologic Substance treats
Pathologic Function
Given a medical sentence like this one:
(18.1) Doppler echocardiography can be used to diagnose left anterior descending
artery stenosis in patients with type 2 diabetes
We could thus extract the UMLS relation:
Echocardiography, Doppler Diagnoses Acquired stenosis
Wikipedia also offers a large supply of relations, drawn from infoboxes, struc-
infoboxes
tured tables associated with certain Wikipedia articles. For example, the Wikipedia

334
CHAPTER 18
•
INFORMATION EXTRACTION
infobox for Stanford includes structured facts like state = "California" or
president = "Mark Tessier-Lavigne". These facts can be turned into rela-
tions like president-of or located-in. or into relations in a metalanguage called RDF
RDF
(Resource Description Framework). An RDF triple is a tuple of entity-relation-
RDF triple
entity, called a subject-predicate-object expression. Here’s a sample RDF triple:
subject
predicate object
Golden Gate Park location
San Francisco
For example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology
derived from Wikipedia containing over 2 billion RDF triples.
Another dataset
from Wikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata
Freebase
(Vrandeˇci´c and Kr¨otzsch, 2014), has relations like:
people/person/nationality
location/location/contains
WordNet or other ontologies offer useful ontological relations that express hier-
archical relations between words or concepts. For example WordNet has the is-a or
is-a
hypernym relation between classes,
hypernym
Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...
WordNet also has Instance-of relation between individuals and classes, so that for
example San Francisco is in the Instance-of relation with city. Extracting these
relations is an important step in extending or building ontologies.
There are ﬁve main classes of algorithms for relation extraction: handwritten
patterns, supervised machine learning, semi-supervised (via bootstrapping and
via distant supervision), and unsupervised. We’ll introduce each of these in the
next sections.
18.2.1
Using Patterns to Extract Relations
The earliest and still common algorithm for relation extraction is lexico-syntactic
patterns, ﬁrst developed by Hearst (1992a). Consider the following sentence:
Agar is a substance prepared from a mixture of red algae, such as Ge-
lidium, for laboratory or industrial use.
Hearst points out that most human readers will not know what Gelidium is, but that
they can readily infer that it is a kind of (a hyponym of) red algae, whatever that is.
She suggests that the following lexico-syntactic pattern
NP0 such as NP1{,NP2 ...,(and|or)NPi},i ≥1
(18.2)
implies the following semantics
∀NPi,i ≥1,hyponym(NPi,NP0)
(18.3)
allowing us to infer
hyponym(Gelidium,red algae)
(18.4)
Figure 18.12 shows ﬁve patterns Hearst (1992a, 1998) suggested for inferring
the hyponym relation; we’ve shown NPH as the parent/hyponym. Modern versions
of the pattern-based approach extend it by adding named entity constraints. For
example if our goal is to answer questions about “Who holds what ofﬁce in which
organization?”, we can use patterns like the following:

18.2
•
RELATION EXTRACTION
335
NP {, NP}* {,} (and|or) other NPH
temples, treasuries, and other important civic buildings
NPH such as {NP,}* {(or|and)} NP
red algae such as Gelidium
such NPH as {NP,}* {(or|and)} NP
such authors as Herrick, Goldsmith, and Shakespeare
NPH {,} including {NP,}* {(or|and)} NP
common-law countries, including Canada and England
NPH {,} especially {NP}* {(or|and)} NP
European countries, especially France, England, and Spain
Figure 18.12
Hand-built lexico-syntactic patterns for ﬁnding hypernyms, using {} to mark optionality
(Hearst 1992a, Hearst 1998).
PER, POSITION of ORG:
George Marshall, Secretary of State of the United States
PER (named|appointed|chose|etc.) PER Prep? POSITION
Truman appointed Marshall Secretary of State
PER [be]? (named|appointed|etc.) Prep? ORG POSITION
George Marshall was named US Secretary of State
Hand-built patterns have the advantage of high-precision and they can be tailored
to speciﬁc domains. On the other hand, they are often low-recall, and it’s a lot of
work to create them for all possible patterns.
18.2.2
Relation Extraction via Supervised Learning
Supervised machine learning approaches to relation extraction follow a scheme that
should be familiar by now. A ﬁxed set of relations and entities is chosen, a training
corpus is hand-annotated with the relations and entities, and the annotated texts are
then used to train classiﬁers to annotate an unseen test set.
The most straightforward approach has three steps, illustrated in Fig. 18.13. Step
one is to ﬁnd pairs of named entities (usually in the same sentence). In step two, a
ﬁltering classiﬁer is trained to make a binary decision as to whether a given pair of
named entities are related (by any relation). Positive examples are extracted directly
from all relations in the annotated corpus, and negative examples are generated from
within-sentence entity pairs that are not annotated with a relation. In step 3, a classi-
ﬁer is trained to assign a label to the relations that were found by step 2. The use of
the ﬁltering classiﬁer can speed up the ﬁnal classiﬁcation and also allows the use of
distinct feature-sets appropriate for each task. For each of the two classiﬁers, we can
use any of the standard classiﬁcation techniques (logistic regression, neural network,
SVM, etc.)
function FINDRELATIONS(words) returns relations
relations←nil
entities←FINDENTITIES(words)
forall entity pairs ⟨e1, e2⟩in entities do
if RELATED?(e1,e2)
relations←relations+CLASSIFYRELATION(e1,e2)
Figure 18.13
Finding and classifying the relations among entities in a text.
For the feature-based classiﬁers like logistic regression or random forests the
most important step is to identify useful features. Let’s consider features for clas-

336
CHAPTER 18
•
INFORMATION EXTRACTION
sifying the relationship between American Airlines (Mention 1, or M1) and Tim
Wagner (Mention 2, M2) from this sentence:
(18.5) American Airlines, a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
Useful word features include
• The headwords of M1 and M2 and their concatenation
Airlines
Wagner
Airlines-Wagner
• Bag-of-words and bigrams in M1 and M2
American, Airlines, Tim, Wagner, American Airlines, Tim Wagner
• Words or bigrams in particular positions
M2: -1 spokesman
M2: +1 said
• Bag of words or bigrams between M1 and M2:
a, AMR, of, immediately, matched, move, spokesman, the, unit
• Stemmed versions of the same
Embeddings can be used to represent words in any of these features. Useful named
entity features include
• Named-entity types and their concatenation
(M1: ORG, M2: PER, M1M2: ORG-PER)
• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)
M1: NAME [it or he would be PRONOUN]
M2: NAME [the company would be NOMINAL]
• Number of entities between the arguments (in this case 1, for AMR)
The syntactic structure of a sentence can also signal relationships among its
entities. Syntax is often featured by using strings representing syntactic paths: the
(dependency or constituency) path traversed through the tree in getting from one
entity to the other.
• Base syntactic chunk sequence from M1 to M2
NP NP PP VP NP NP
• Constituent paths between M1 and M2
NP ↑NP ↑S ↑S ↓NP
• Dependency-tree paths
Airlines ←sub j matched ←comp said →subj Wagner
Figure 18.14 summarizes many of the features we have discussed that could be
used for classifying the relationship between American Airlines and Tim Wagner
from our example text.
Neural models for relation extraction similarly treat the task as supervised clas-
siﬁcation. One option is to use a similar architecture as we saw for named entity
tagging: a bi-LSTM model with word embeddings as inputs and a single softmax
classiﬁcation of the sentence output as a 1-of-N relation label. Because relations
often hold between entities that are far part in a sentence (or across sentences), it
may be possible to get higher performance from algorithms like convolutional nets
(dos Santos et al., 2015) or chain or tree LSTMS (Miwa and Bansal 2016, Peng
et al. 2017).
In general, if the test set is similar enough to the training set, and if there is
enough hand-labeled data, supervised relation extraction systems can get high ac-
curacies. But labeling a large training set is extremely expensive and supervised

18.2
•
RELATION EXTRACTION
337
M1 headword
airlines (as a word token or an embedding)
M2 headword
Wagner
Word(s) before M1
NONE
Word(s) after M2
said
Bag of words between
{a, unit, of, AMR, Inc., immediately, matched, the, move, spokesman }
M1 type
ORG
M2 type
PERS
Concatenated types
ORG-PERS
Constituent path
NP ↑NP ↑S ↑S ↓NP
Base phrase path
NP →NP →PP →NP →VP →NP →NP
Typed-dependency path Airlines ←subj matched ←comp said →subj Wagner
Figure 18.14
Sample of features extracted during classiﬁcation of the <American Airlines, Tim Wagner>
tuple; M1 is the ﬁrst mention, M2 the second.
models are brittle: they don’t generalize well to different text genres. For this rea-
son, much research in relation extraction has focused on the semi-supervised and
unsupervised approaches we turn to next.
18.2.3
Semisupervised Relation Extraction via Bootstrapping
Supervised machine learning assumes that we have lots of labeled data. Unfortu-
nately, this is expensive. But suppose we just have a few high-precision seed pat-
terns, like those in Section 18.2.1, or perhaps a few seed tuples. That’s enough
seed patterns
seed tuples
to bootstrap a classiﬁer! Bootstrapping proceeds by taking the entities in the seed
bootstrapping
pair, and then ﬁnding sentences (on the web, or whatever dataset we are using) that
contain both entities. From all such sentences, we extract and generalize the context
around the entities to learn new patterns. Fig. 18.15 sketches a basic algorithm.
function BOOTSTRAP(Relation R) returns new relation tuples
tuples←Gather a set of seed tuples that have relation R
iterate
sentences←ﬁnd sentences that contain entities in tuples
patterns←generalize the context between and around entities in sentences
newpairs←use patterns to grep for more tuples
newpairs←newpairs with high conﬁdence
tuples←tuples + newpairs
return tuples
Figure 18.15
Bootstrapping from seed entity pairs to learn relations.
Suppose, for example, that we need to create a list of airline/hub pairs, and we
know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover
new patterns by ﬁnding other mentions of this relation in our corpus. We search
for the terms Ryanair, Charleroi and hub in some proximity. Perhaps we ﬁnd the
following set of sentences:
(18.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all
weekend ﬂights out of the airport.
(18.7) All ﬂights in and out of Ryanair’s Belgian hub at Charleroi airport were
grounded on Friday...

338
CHAPTER 18
•
INFORMATION EXTRACTION
(18.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000
passengers had already been affected.
From these results, we can use the context of words between the entity mentions,
the words before mention one, the word after mention two, and the named entity
types of the two mentions, and perhaps other features, to extract general patterns
such as the following:
/ [ORG], which uses [LOC] as a hub /
/ [ORG]’s hub at [LOC] /
/ [LOC] a main hub for [ORG] /
These new patterns can then be used to search for additional tuples.
Bootstrapping systems also assign conﬁdence values to new tuples to avoid se-
conﬁdence
values
mantic drift. In semantic drift, an erroneous pattern leads to the introduction of
semantic drift
erroneous tuples, which, in turn, lead to the creation of problematic patterns and the
meaning of the extracted relations ‘drifts’. Consider the following example:
(18.9) Sydney has a ferry hub at Circular Quay.
If accepted as a positive example, this expression could lead to the incorrect in-
troduction of the tuple ⟨Sydney,CircularQuay⟩. Patterns based on this tuple could
propagate further errors into the database.
Conﬁdence values for patterns are based on balancing two factors: the pattern’s
performance with respect to the current set of tuples and the pattern’s productivity
in terms of the number of matches it produces in the document collection. More
formally, given a document collection D, a current set of tuples T, and a proposed
pattern p, we need to track two factors:
• hits: the set of tuples in T that p matches while looking in D
• finds: The total set of tuples that p ﬁnds in D
The following equation balances these considerations (Riloff and Jones, 1999).
Conf RlogF(p) = hitsp
ﬁndsp
×log(ﬁndsp)
(18.10)
This metric is generally normalized to produce a probability.
We can assess the conﬁdence in a proposed new tuple by combining the evidence
supporting it from all the patterns P′ that match that tuple in D (Agichtein and
Gravano, 2000). One way to combine such evidence is the noisy-or technique.
noisy-or
Assume that a given tuple is supported by a subset of the patterns in P, each with
its own conﬁdence assessed as above. In the noisy-or model, we make two basic
assumptions. First, that for a proposed tuple to be false, all of its supporting patterns
must have been in error, and second, that the sources of their individual failures are
all independent. If we loosely treat our conﬁdence measures as probabilities, then
the probability of any individual pattern p failing is 1−Conf(p); the probability of
all of the supporting patterns for a tuple being wrong is the product of their individual
failure probabilities, leaving us with the following equation for our conﬁdence in a
new tuple.
Conf(t) = 1−
Y
p∈P′
(1−Conf(p))
(18.11)
Setting conservative conﬁdence thresholds for the acceptance of new patterns
and tuples during the bootstrapping process helps prevent the system from drifting
away from the targeted relation.

18.2
•
RELATION EXTRACTION
339
18.2.4
Distant Supervision for Relation Extraction
Although text that has been hand-labeled with relation labels is extremely expensive
to produce, there are ways to ﬁnd indirect sources of training data. The distant
supervision method of Mintz et al. (2009) combines the advantages of bootstrapping
distant
supervision
with supervised learning. Instead of just a handful of seeds, distant supervision uses
a large database to acquire a huge number of seed examples, creates lots of noisy
pattern features from all these examples and then combines them in a supervised
classiﬁer.
For example suppose we are trying to learn the place-of-birth relationship be-
tween people and their birth cities. In the seed-based approach, we might have only
5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase
have tens of thousands of examples of many relations; including over 100,000 ex-
amples of place-of-birth, (<Edwin Hubble, Marshfield>, <Albert Einstein,
Ulm>, etc.,). The next step is to run named entity taggers on large amounts of text—
Mintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences
that have two named entities that match the tuple, like the following:
...Hubble was born in Marshﬁeld...
...Einstein, born (1879), Ulm...
...Hubble’s birthplace in Marshﬁeld...
Training instances can now be extracted from this data, one training instance
for each identical tuple <relation, entity1, entity2>. Thus there will be one
training instance for each of:
<born-in, Edwin Hubble, Marshfield>
<born-in, Albert Einstein, Ulm>
<born-year, Albert Einstein, 1879>
and so on.
We can then apply feature-based or neural classiﬁcation. For feature-based clas-
siﬁcation, standard supervised relation extraction features like the named entity la-
bels of the two mentions, the words and dependency paths in between the mentions,
and neighboring words. Each tuple will have features collected from many training
instances; the feature vector for a single training instance like (<born-in,Albert
Einstein, Ulm> will have lexical and syntactic features from many different sen-
tences that mention Einstein and Ulm.
Because distant supervision has very large training sets, it is also able to use very
rich features that are conjunctions of these individual features. So we will extract
thousands of patterns that conjoin the entity types with the intervening words or
dependency paths like these:
PER was born in LOC
PER, born (XXXX), LOC
PER’s birthplace in LOC
To return to our running example, for this sentence:
(18.12) American Airlines, a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
we would learn rich conjunction features like this one:
M1 = ORG & M2 = PER & nextword=“said”& path= NP ↑NP ↑S ↑S ↓NP
The result is a supervised classiﬁer that has a huge rich set of features to use
in detecting relations. Since not every test sentence will have one of the training

340
CHAPTER 18
•
INFORMATION EXTRACTION
relations, the classiﬁer will also need to be able to label an example as no-relation.
This label is trained by randomly selecting entity pairs that do not appear in any
Freebase relation, extracting features for them, and building a feature vector for
each such tuple. The ﬁnal algorithm is sketched in Fig. 18.16.
function DISTANT SUPERVISION(Database D, Text T) returns relation classiﬁer C
foreach relation R
foreach tuple (e1,e2) of entities with relation R in D
sentences←Sentences in T that contain e1 and e2
f←Frequent features in sentences
observations←observations + new training tuple (e1, e2, f, R)
C←Train supervised classiﬁer on observations
return C
Figure 18.16
The distant supervision algorithm for relation extraction. A neural classiﬁer
might not need to use the feature set f.
Distant supervision shares advantages with each of the methods we’ve exam-
ined. Like supervised classiﬁcation, distant supervision uses a classiﬁer with lots
of features, and supervised by detailed hand-created knowledge. Like pattern-based
classiﬁers, it can make use of high-precision evidence for the relation between en-
tities. Indeed, distance supervision systems learn patterns just like the hand-built
patterns of early relation extractors. For example the is-a or hypernym extraction
system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as
distant supervision, and then learned new patterns from large amounts of text. Their
system induced exactly the original 5 template patterns of Hearst (1992a), but also
70,000 additional patterns including these four:
NPH like NP
Many hormones like leptin...
NPH called NP ...using a markup language called XHTML
NP is a NPH
Ruby is a programming language...
NP, a NPH
IBM, a company with a long...
This ability to use a large number of features simultaneously means that, un-
like the iterative expansion of patterns in seed-based systems, there’s no semantic
drift. Like unsupervised classiﬁcation, it doesn’t use a labeled training corpus of
texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very
large amounts of unlabeled data. Distant supervision also has the advantage that it
can create training tuples to be used with neural classiﬁers, where features are not
required.
But distant supervision can only help in extracting relations for which a large
enough database already exists. To extract new relations without datasets, or rela-
tions for new domains, purely unsupervised methods must be used.
18.2.5
Unsupervised Relation Extraction
The goal of unsupervised relation extraction is to extract relations from the web
when we have no labeled training data, and not even any list of relations. This task
is often called open information extraction or Open IE. In Open IE, the relations
open
information
extraction
are simply strings of words (usually beginning with a verb).
For example, the ReVerb system (Fader et al., 2011) extracts a relation from a
sentence s in 4 steps:

18.2
•
RELATION EXTRACTION
341
1. Run a part-of-speech tagger and entity chunker over s
2. For each verb in s, ﬁnd the longest sequence of words w that start with a verb
and satisfy syntactic and lexical constraints, merging adjacent matches.
3. For each phrase w, ﬁnd the nearest noun phrase x to the left which is not a
relative pronoun, wh-word or existential “there”. Find the nearest noun phrase
y to the right.
4. Assign conﬁdence c to the relation r = (x,w,y) using a conﬁdence classiﬁer
and return it.
A relation is only accepted if it meets syntactic and lexical constraints. The
syntactic constraints ensure that it is a verb-initial sequence that might also include
nouns (relations that begin with light verbs like make, have, or do often express the
core of the relation with a noun, like have a hub in):
V | VP | VW*P
V = verb particle? adv?
W = (noun | adj | adv | pron | det )
P = (prep | particle | inf. marker)
The lexical constraints are based on a dictionary D that is used to prune very rare,
long relation strings. The intuition is to eliminate candidate relations that don’t oc-
cur with sufﬁcient number of distinct argument types and so are likely to be bad
examples. The system ﬁrst runs the above relation extraction algorithm ofﬂine on
500 million web sentences and extracts a list of all the relations that occur after nor-
malizing them (removing inﬂection, auxiliary verbs, adjectives, and adverbs). Each
relation r is added to the dictionary if it occurs with at least 20 different arguments.
Fader et al. (2011) used a dictionary of 1.7 million normalized relations.
Finally, a conﬁdence value is computed for each relation using a logistic re-
gression classiﬁer. The classiﬁer is trained by taking 1000 random web sentences,
running the extractor, and hand labelling each extracted relation as correct or incor-
rect. A conﬁdence classiﬁer is then trained on this hand-labeled data, using features
of the relation and the surrounding words. Fig. 18.17 shows some sample features
used in the classiﬁcation.
(x,r,y) covers all words in s
the last preposition in r is for
the last preposition in r is on
len(s) ≤10
there is a coordinating conjunction to the left of r in s
r matches a lone V in the syntactic constraints
there is preposition to the left of x in s
there is an NP to the right of y in s
Figure 18.17
Features for the classiﬁer that assigns conﬁdence to relations extracted by the
Open Information Extraction system REVERB (Fader et al., 2011).
For example the following sentence:
(18.13) United has a hub in Chicago, which is the headquarters of United
Continental Holdings.
has the relation phrases has a hub in and is the headquarters of (it also has has and
is, but longer phrases are preferred). Step 3 ﬁnds United to the left and Chicago to
the right of has a hub in, and skips over which to ﬁnd Chicago to the left of is the
headquarters of. The ﬁnal output is:

342
CHAPTER 18
•
INFORMATION EXTRACTION
r1:
<United, has a hub in, Chicago>
r2:
<Chicago, is the headquarters of, United Continental Holdings>
The great advantage of unsupervised relation extraction is its ability to handle
a huge number of relations without having to specify them in advance. The disad-
vantage is the need to map these large sets of strings into some canonical form for
adding to databases or other knowledge sources. Current methods focus heavily on
relations expressed with verbs, and so will miss many relations that are expressed
nominally.
18.2.6
Evaluation of Relation Extraction
Supervised relation extraction systems are evaluated by using test sets with human-
annotated, gold-standard relations and computing precision, recall, and F-measure.
Labeled precision and recall require the system to classify the relation correctly,
whereas unlabeled methods simply measure a system’s ability to detect entities that
are related.
Semi-supervised and unsupervised methods are much more difﬁcult to evalu-
ate, since they extract totally new relations from the web or a large text. Because
these methods use very large amounts of text, it is generally not possible to run them
solely on a small labeled test set, and as a result it’s not possible to pre-annotate a
gold set of correct instances of relations.
For these methods it’s possible to approximate (only) precision by drawing a
random sample of relations from the output, and having a human check the accuracy
of each of these relations. Usually this approach focuses on the tuples to be extracted
from a body of text rather than on the relation mentions; systems need not detect
every mention of a relation to be scored correctly. Instead, the evaluation is based
on the set of tuples occupying the database when the system is ﬁnished. That is,
we want to know if the system can discover that Ryanair has a hub at Charleroi; we
don’t really care how many times it discovers it. The estimated precision ˆP is then
ˆP = # of correctly extracted relation tuples in the sample
total # of extracted relation tuples in the sample.
(18.14)
Another approach that gives us a little bit of information about recall is to com-
pute precision at different levels of recall. Assuming that our system is able to
rank the relations it produces (by probability, or conﬁdence) we can separately com-
pute precision for the top 1000 new relations, the top 10,000 new relations, the top
100,000, and so on. In each case we take a random sample of that set. This will
show us how the precision curve behaves as we extract more and more tuples. But
there is no way to directly evaluate recall.
18.3
Extracting Times
Times and dates are a particularly important kind of named entity that play a role
in question answering, in calendar and personal assistant applications. In order to
reason about times and dates, after we extract these temporal expressions they must
be normalized—converted to a standard format so we can reason about them. In this
section we consider both the extraction and normalization of temporal expressions.

18.3
•
EXTRACTING TIMES
343
18.3.1
Temporal Expression Extraction
Temporal expressions are those that refer to absolute points in time, relative times,
durations, and sets of these. Absolute temporal expressions are those that can be
absolute
mapped directly to calendar dates, times of day, or both. Relative temporal expres-
relative
sions map to particular times through some other reference point (as in a week from
last Tuesday). Finally, durations denote spans of time at varying levels of granular-
duration
ity (seconds, minutes, days, weeks, centuries, etc.). Figure 18.18 lists some sample
temporal expressions in each of these categories.
Absolute
Relative
Durations
April 24, 1916
yesterday
four hours
The summer of ’77
next semester
three weeks
10:15 AM
two weeks from yesterday
six days
The 3rd quarter of 2006
last quarter
the last three quarters
Figure 18.18
Examples of absolute, relational and durational temporal expressions.
Temporal expressions are grammatical constructions that have temporal lexical
triggers as their heads. Lexical triggers might be nouns, proper nouns, adjectives,
lexical triggers
and adverbs; full temporal expressions consist of their phrasal projections: noun
phrases, adjective phrases, and adverbial phrases. Figure 18.19 provides examples.
Category
Examples
Noun
morning, noon, night, winter, dusk, dawn
Proper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet
Adjective
recent, past, annual, former
Adverb
hourly, daily, monthly, yearly
Figure 18.19
Examples of temporal lexical triggers.
Let’s look at the TimeML annotation scheme, in which temporal expressions are
annotated with an XML tag, TIMEX3, and various attributes to that tag (Pustejovsky
et al. 2005, Ferro et al. 2005). The following example illustrates the basic use of this
scheme (we defer discussion of the attributes until Section 18.3.2).
A fare increase initiated <TIMEX3>last week</TIMEX3> by UAL
Corp’s United Airlines was matched by competitors over <TIMEX3>the
weekend</TIMEX3>, marking the second successful fare increase in
<TIMEX3>two weeks</TIMEX3>.
The temporal expression recognition task consists of ﬁnding the start and end of
all of the text spans that correspond to such temporal expressions. Rule-based ap-
proaches to temporal expression recognition use cascades of automata to recognize
patterns at increasing levels of complexity. Tokens are ﬁrst part-of-speech tagged,
and then larger and larger chunks are recognized from the results from previous
stages, based on patterns containing trigger words (e.g., February) or classes (e.g.,
MONTH). Figure 18.20 gives a fragment from a rule-based system.
Sequence-labeling approaches follow the same IOB scheme used for named-
entity tags, marking words that are either inside, outside or at the beginning of a
TIMEX3-delimited temporal expression with the I, O, and B tags as follows:
A
O
fare
O
increase
O
initiated
O
last
B
week
I
by
O
UAL
O
Corp’s...
O

344
CHAPTER 18
•
INFORMATION EXTRACTION
# yesterday/today/tomorrow
$string =˜ s/((($OT+the$CT+\s+)?$OT+day$CT+\s+$OT+(before|after)$CT+\s+)?$OT+$TERelDayExpr$CT+
(\s+$OT+(morning|afternoon|evening|night)$CT+)?)/<TIMEX$tever TYPE=\"DATE\">$1
<\/TIMEX$tever>/gio;
$string =˜ s/($OT+\w+$CT+\s+)<TIMEX$tever TYPE=\"DATE\"[ˆ>]*>($OT+(Today|Tonight)$CT+)
<\/TIMEX$tever>/$1$4/gso;
# this (morning/afternoon/evening)
$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+this$CT+\s*$OT+(morning|afternoon|evening)$CT+)/
<TIMEX$tever TYPE=\"DATE\">$1<\/TIMEX$tever>/gosi;
$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+last$CT+\s*$OT+night$CT+)/<TIMEX$tever
TYPE=\"DATE\">$1<\/TIMEX$tever>/gsio;
Figure 18.20
Perl fragment from the GUTime temporal tagging system in Tarsqi (Verhagen et al., 2005).
Features are extracted from the token and its context, and a statistical sequence
labeler is trained (any sequence model can be used). Figure 18.21 lists standard
features used in temporal tagging.
Feature
Explanation
Token
The target token to be labeled
Tokens in window Bag of tokens in the window around a target
Shape
Character shape features
POS
Parts of speech of target and window words
Chunk tags
Base-phrase chunk tag for target and words in a window
Lexical triggers
Presence in a list of temporal terms
Figure 18.21
Typical features used to train IOB-style temporal expression taggers.
Temporal expression recognizers are evaluated with the usual recall, precision,
and F-measures. A major difﬁculty for all of these very lexicalized approaches is
avoiding expressions that trigger false positives:
(18.15) 1984 tells the story of Winston Smith...
(18.16) ...U2’s classic Sunday Bloody Sunday
18.3.2
Temporal Normalization
Temporal normalization is the process of mapping a temporal expression to either
temporal
normalization
a speciﬁc point in time or to a duration. Points in time correspond to calendar dates,
to times of day, or both. Durations primarily consist of lengths of time but may also
include information about start and end points. Normalized times are represented
with the VALUE attribute from the ISO 8601 standard for encoding temporal values
(ISO8601, 2004). Fig. 18.22 reproduces our earlier example with the value attributes
added in.
<TIMEX3 id = ’ ’ t1 ’ ’
type =”DATE”
value =”2007−07−02”
functionInDocument=”CREATION TIME”
> July
2 , 2007 </ TIMEX3> A f a r e
i n c r e a s e
i n i t i a t e d <TIMEX3 id =” t2 ”
type =”DATE”
value =”2007−W26” anchorTimeID=” t1 ”>l a s t
week</ TIMEX3> by United
A i r l i n e s
was
matched by
c o m p e t i t o r s
over <TIMEX3 id =” t3 ”
type =”DURATION”
value =”P1WE”
anchorTimeID=” t1 ”> the
weekend </ TIMEX3> ,
marking
the
second
s u c c e s s f u l
f a r e
i n c r e a s e
in <TIMEX3 id =” t4 ”
type =”DURATION”
value =”P2W” anchorTimeID=” t1 ”> two
weeks </ TIMEX3>.
Figure 18.22
TimeML markup including normalized values for temporal expressions.
The dateline, or document date, for this text was July 2, 2007. The ISO repre-
sentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.

18.3
•
EXTRACTING TIMES
345
The encodings for the temporal expressions in our sample text all follow from this
date, and are shown here as values for the VALUE attribute.
The ﬁrst temporal expression in the text proper refers to a particular week of the
year. In the ISO standard, weeks are numbered from 01 to 53, with the ﬁrst week
of the year being the one that has the ﬁrst Thursday of the year. These weeks are
represented with the template YYYY-Wnn. The ISO week for our document date is
week 27; thus the value for last week is represented as “2007-W26”.
The next temporal expression is the weekend. ISO weeks begin on Monday;
thus, weekends occur at the end of a week and are fully contained within a single
week. Weekends are treated as durations, so the value of the VALUE attribute has
to be a length. Durations are represented according to the pattern Pnx, where n is
an integer denoting the length and x represents the unit, as in P3Y for three years
or P2D for two days. In this example, one weekend is captured as P1WE. In this
case, there is also sufﬁcient information to anchor this particular weekend as part of
a particular week. Such information is encoded in the ANCHORTIMEID attribute.
Finally, the phrase two weeks also denotes a duration captured as P2W. There is a
lot more to the various temporal annotation standards—far too much to cover here.
Figure 18.23 describes some of the basic ways that other times and durations are
represented. Consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al.
(2005) for more details.
Unit
Pattern
Sample Value
Fully speciﬁed dates
YYYY-MM-DD
1991-09-28
Weeks
YYYY-Wnn
2007-W27
Weekends
PnWE
P1WE
24-hour clock times
HH:MM:SS
11:13:45
Dates and times
YYYY-MM-DDTHH:MM:SS
1991-09-28T11:00:00
Financial quarters
Qn
1999-Q3
Figure 18.23
Sample ISO patterns for representing various times and durations.
Most current approaches to temporal normalization are rule-based (Chang and
Manning 2012, Str¨otgen and Gertz 2013). Patterns that match temporal expres-
sions are associated with semantic analysis procedures. As in the compositional
rule-to-rule approach introduced in Chapter 17, the meaning of a constituent is com-
puted from the meaning of its parts using a method speciﬁc to the constituent, al-
though here the semantic composition rules involve temporal arithmetic rather than
λ-calculus attachments.
Fully qualiﬁed date expressions contain a year, month, and day in some con-
fully qualiﬁed
ventional form. The units in the expression must be detected and then placed in the
correct place in the corresponding ISO pattern. The following pattern normalizes
expressions like April 24, 1916.
FQTE →Month Date , Year
{Year.val −Month.val −Date.val}
The non-terminals Month, Date, and Year represent constituents that have already
been recognized and assigned semantic values, accessed through the *.val notation.
The value of this FQE constituent can, in turn, be accessed as FQTE.val during
further processing.
Fully qualiﬁed temporal expressions are fairly rare in real texts. Most temporal
expressions in news articles are incomplete and are only implicitly anchored, of-
ten with respect to the dateline of the article, which we refer to as the document’s

346
CHAPTER 18
•
INFORMATION EXTRACTION
temporal anchor. The values of temporal expressions such as today, yesterday, or
temporal
anchor
tomorrow can all be computed with respect to this temporal anchor. The semantic
procedure for today simply assigns the anchor, and the attachments for tomorrow
and yesterday add a day and subtract a day from the anchor, respectively. Of course,
given the cyclic nature of our representations for months, weeks, days, and times of
day, our temporal arithmetic procedures must use modulo arithmetic appropriate to
the time unit being used.
Unfortunately, even simple expressions such as the weekend or Wednesday in-
troduce a fair amount of complexity. In our current example, the weekend clearly
refers to the weekend of the week that immediately precedes the document date. But
this won’t always be the case, as is illustrated in the following example.
(18.17) Random security checks that began yesterday at Sky Harbor will continue
at least through the weekend.
In this case, the expression the weekend refers to the weekend of the week that the
anchoring date is part of (i.e., the coming weekend). The information that signals
this meaning comes from the tense of continue, the verb governing the weekend.
Relative temporal expressions are handled with temporal arithmetic similar to
that used for today and yesterday. The document date indicates that our example
article is ISO week 27, so the expression last week normalizes to the current week
minus 1. To resolve ambiguous next and last expressions we consider the distance
from the anchoring date to the nearest unit. Next Friday can refer either to the
immediately next Friday or to the Friday following that, but the closer the document
date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such
ambiguities are handled by encoding language and domain-speciﬁc heuristics into
the temporal attachments.
18.4
Extracting Events and their Times
The task of event extraction is to identify mentions of events in texts. For the
event
extraction
purposes of this task, an event mention is any expression denoting an event or state
that can be assigned to a particular point, or interval, in time. The following markup
of the sample text on page 343 shows all the events in this text.
[EVENT Citing] high fuel prices, United Airlines [EVENT said] Fri-
day it has [EVENT increased] fares by $6 per round trip on ﬂights to
some cities also served by lower-cost carriers. American Airlines, a unit
of AMR Corp., immediately [EVENT matched] [EVENT the move],
spokesman Tim Wagner [EVENT said]. United, a unit of UAL Corp.,
[EVENT said] [EVENT the increase] took effect Thursday and [EVENT
applies] to most routes where it [EVENT competes] against discount
carriers, such as Chicago to Dallas and Denver to San Francisco.
In English, most event mentions correspond to verbs, and most verbs introduce
events. However, as we can see from our example, this is not always the case. Events
can be introduced by noun phrases, as in the move and the increase, and some verbs
fail to introduce events, as in the phrasal verb took effect, which refers to when the
event began rather than to the event itself. Similarly, light verbs such as make, take,
and have often fail to denote events; for light verbs the event is often expressed by
the nominal direct object (took a ﬂight), and these light verbs just provide a syntactic
structure for the noun’s arguments.

18.4
•
EXTRACTING EVENTS AND THEIR TIMES
347
Various versions of the event extraction task exist, depending on the goal. For
example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract
events and aspects like their aspectual and temporal properties. Events are to be
classiﬁed as actions, states, reporting events (say, report, tell, explain), perception
reporting
events
events, and so on. The aspect, tense, and modality of each event also needs to be
extracted. Thus for example the various said events in the sample text would be
annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).
Event extraction is generally modeled via supervised learning, detecting events
via sequence models with IOB tagging, and assigning event classes and attributes
with multi-class classiﬁers. Feature-based models use surface information like parts
of speech, lexical items, and verb tense information; see Fig. 18.24.
Feature
Explanation
Character afﬁxes
Character-level preﬁxes and sufﬁxes of target word
Nominalization sufﬁx
Character-level sufﬁxes for nominalizations (e.g., -tion)
Part of speech
Part of speech of the target word
Light verb
Binary feature indicating that the target is governed by a light verb
Subject syntactic category Syntactic category of the subject of the sentence
Morphological stem
Stemmed version of the target word
Verb root
Root form of the verb basis for a nominalization
WordNet hypernyms
Hypernym set for the target
Figure 18.24
Features commonly used in both rule-based and machine learning approaches to event detec-
tion.
18.4.1
Temporal Ordering of Events
With both the events and the temporal expressions in a text having been detected, the
next logical task is to use this information to ﬁt the events into a complete timeline.
Such a timeline would be useful for applications such as question answering and
summarization. This ambitious task is the subject of considerable current research
but is beyond the capabilities of current systems.
A somewhat simpler, but still useful, task is to impose a partial ordering on the
events and temporal expressions mentioned in a text. Such an ordering can provide
many of the same beneﬁts as a true timeline. An example of such a partial ordering
is the determination that the fare increase by American Airlines came after the fare
increase by United in our sample text. Determining such an ordering can be viewed
as a binary relation detection and classiﬁcation task similar to those described earlier
in Section 18.2. The temporal relation between events is classiﬁed into one of the
standard set of Allen relations shown in Fig. 18.25 (Allen, 1984), using feature-
Allen relations
based classiﬁers as in Section 18.2, trained on the TimeBank corpus with features
like words/embeddings, parse paths, tense and aspect.
The TimeBank corpus consists of text annotated with much of the information
TimeBank
we’ve been discussing throughout this section (Pustejovsky et al., 2003b). Time-
Bank 1.2 consists of 183 news articles selected from a variety of sources, including
the Penn TreeBank and PropBank collections.
Each article in the TimeBank corpus has had the temporal expressions and event
mentions in them explicitly annotated in the TimeML annotation (Pustejovsky et al.,
2003a). In addition to temporal expressions and events, the TimeML annotation
provides temporal links between events and temporal expressions that specify the
nature of the relation between them. Consider the following sample sentence and

348
CHAPTER 18
•
INFORMATION EXTRACTION
B
A
B
A
B
A
A
A
B
B
A
B
Time 
A  before B
B after  A
A overlaps B
B overlaps' A
A meets B
B meets' A
A equals B
(B equals A)
A starts B
B starts' A
A finishes B
B finishes' A
B
A during B
B during' A
A
Figure 18.25
The 13 temporal relations from Allen (1984).
<TIMEX3 tid="t57" type="DATE" value="1989-10-26"
functionInDocument="CREATION_TIME">
10/26/89
</TIMEX3>
Delta Air Lines earnings <EVENT eid="e1" class="OCCURRENCE"> soared </EVENT> 33% to a
record in
<TIMEX3 tid="t58" type="DATE" value="1989-Q1" anchorTimeID="t57"> the
fiscal first quarter </TIMEX3>, <EVENT eid="e3"
class="OCCURRENCE">bucking</EVENT>
the industry trend toward <EVENT eid="e4" class="OCCURRENCE">declining</EVENT>
profits.
Figure 18.26
Example from the TimeBank corpus.
its corresponding markup shown in Fig. 18.26, selected from one of the TimeBank
documents.
(18.18) Delta Air Lines earnings soared 33% to a record in the ﬁscal ﬁrst quarter,
bucking the industry trend toward declining proﬁts.
As annotated, this text includes three events and two temporal expressions. The
events are all in the occurrence class and are given unique identiﬁers for use in fur-
ther annotations. The temporal expressions include the creation time of the article,
which serves as the document time, and a single temporal expression within the text.
In addition to these annotations, TimeBank provides four links that capture the
temporal relations between the events and times in the text, using the Allen relations
from Fig. 18.25. The following are the within-sentence temporal relations annotated
for this example.

18.5
•
TEMPLATE FILLING
349
• Soaringe1 is included in the ﬁscal ﬁrst quartert58
• Soaringe1 is before 1989-10-26t57
• Soaringe1 is simultaneous with the buckinge3
• Declininge4 includes soaringe1
18.5
Template Filling
Many texts contain reports of events, and possibly sequences of events, that often
correspond to fairly common, stereotypical situations in the world. These abstract
situations or stories, related to what have been called scripts (Schank and Abel-
scripts
son, 1977), consist of prototypical sequences of sub-events, participants, and their
roles. The strong expectations provided by these scripts can facilitate the proper
classiﬁcation of entities, the assignment of entities into roles and relations, and most
critically, the drawing of inferences that ﬁll in things that have been left unsaid. In
their simplest form, such scripts can be represented as templates consisting of ﬁxed
templates
sets of slots that take as values slot-ﬁllers belonging to particular classes. The task
of template ﬁlling is to ﬁnd documents that invoke particular scripts and then ﬁll the
template ﬁlling
slots in the associated templates with ﬁllers extracted from the text. These slot-ﬁllers
may consist of text segments extracted directly from the text, or they may consist of
concepts that have been inferred from text elements through some additional pro-
cessing.
A ﬁlled template from our original airline story might look like the following.
FARE-RAISE ATTEMPT:


LEAD AIRLINE:
UNITED AIRLINES
AMOUNT:
$6
EFFECTIVE DATE:
2006-10-26
FOLLOWER:
AMERICAN AIRLINES


This template has four slots (LEAD AIRLINE, AMOUNT, EFFECTIVE DATE, FOL-
LOWER). The next section describes a standard sequence-labeling approach to ﬁlling
slots. Section 18.5.2 then describes an older system based on the use of cascades of
ﬁnite-state transducers and designed to address a more complex template-ﬁlling task
that current learning-based systems don’t yet address.
18.5.1
Machine Learning Approaches to Template Filling
In the standard paradigm for template ﬁlling, we are given training documents with
text spans annotated with pre-deﬁned templates and their slot ﬁllers. Our goal is to
create one template for each event in the input, ﬁlling in the slots with text spans.
The task is generally modeled by training two separate supervised systems. The
ﬁrst system decides whether the template is present in a particular sentence. This
task is called template recognition or sometimes, in a perhaps confusing bit of
template
recognition
terminology, event recognition. Template recognition can be treated as a text classi-
ﬁcation task, with features extracted from every sequence of words that was labeled
in training documents as ﬁlling any slot from the template being detected. The usual
set of features can be used: tokens, embeddings, word shapes, part-of-speech tags,
syntactic chunk tags, and named entity tags.
The second system has the job of role-ﬁller extraction. A separate classiﬁer is
role-ﬁller
extraction
trained to detect each role (LEAD-AIRLINE, AMOUNT, and so on). This can be a

350
CHAPTER 18
•
INFORMATION EXTRACTION
binary classiﬁer that is run on every noun-phrase in the parsed input sentence, or a
sequence model run over sequences of words. Each role classiﬁer is trained on the
labeled data in the training set. Again, the usual set of features can be used, but now
trained only on an individual noun phrase or the ﬁllers of a single slot.
Multiple non-identical text segments might be labeled with the same slot la-
bel. For example in our sample text, the strings United or United Airlines might be
labeled as the LEAD AIRLINE. These are not incompatible choices and the corefer-
ence resolution techniques introduced in Chapter 22 can provide a path to a solution.
A variety of annotated collections have been used to evaluate this style of ap-
proach to template ﬁlling, including sets of job announcements, conference calls for
papers, restaurant guides, and biological texts. Recent work focuses on extracting
templates in cases where there is no training data or even predeﬁned templates, by
inducing templates as sets of linked events (Chambers and Jurafsky, 2011).
18.5.2
Earlier Finite-State Template-Filling Systems
The templates above are relatively simple. But consider the task of producing a
template that contained all the information in a text like this one (Grishman and
Sundheim, 1995):
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan
with a local concern and a Japanese trading house to produce golf clubs to be
shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capital-
ized at 20 million new Taiwan dollars, will start production in January 1990
with production of 20,000 iron and “metal wood” clubs a month.
The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were
a series of U.S. government-organized information-extraction evaluations) was to
produce hierarchically linked templates describing joint ventures.
Figure 18.27
shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how
the ﬁller of the ACTIVITY slot of the TIE-UP template is itself a template with slots.
Tie-up-1
Activity-1:
RELATIONSHIP
tie-up
COMPANY
Bridgestone Sports Taiwan Co.
ENTITIES
Bridgestone Sports Co.
PRODUCT
iron and “metal wood” clubs
a local concern
START DATE DURING: January 1990
a Japanese trading house
JOINT VENTURE Bridgestone Sports Taiwan Co.
ACTIVITY
Activity-1
AMOUNT
NT$20000000
Figure 18.27
The templates produced by FASTUS given the input text on page 350.
Early systems for dealing with these complex templates were based on cascades
of transducers based on handwritten rules, as sketched in Fig. 18.28.
The ﬁrst four stages use handwritten regular expression and grammar rules to
do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and
events with a FST-based recognizer and inserts the recognized objects into the ap-
propriate slots in templates. This FST recognizer is based on hand-built regular
expressions like the following (NG indicates Noun-Group and VG Verb-Group),
which matches the ﬁrst sentence of the news story above.
NG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)
VG(Produce) NG(Product)

18.6
•
SUMMARY
351
No.
Step
Description
1
Tokens
Tokenize input stream of characters
2
Complex Words
Multiword phrases, numbers, and proper names.
3
Basic phrases
Segment sentences into noun and verb groups
4
Complex phrases
Identify complex noun groups and verb groups
5
Semantic Patterns
Identify entities and events, insert into templates.
6
Merging
Merge references to the same entity or event
Figure 18.28
Levels of processing in FASTUS (Hobbs et al., 1997). Each level extracts a
speciﬁc type of information which is then passed on to the next higher level.
The result of processing these two sentences is the ﬁve draft templates (Fig. 18.29)
that must then be merged into the single hierarchical structure shown in Fig. 18.27.
The merging algorithm, after performing coreference resolution, merges two activi-
ties that are likely to be describing the same events.
# Template/Slot
Value
1 RELATIONSHIP:
TIE-UP
ENTITIES:
Bridgestone Co., a local concern, a Japanese trading house
2 ACTIVITY:
PRODUCTION
PRODUCT:
“golf clubs”
3 RELATIONSHIP:
TIE-UP
JOINT VENTURE: “Bridgestone Sports Taiwan Co.”
AMOUNT:
NT$20000000
4 ACTIVITY:
PRODUCTION
COMPANY:
“Bridgestone Sports Taiwan Co.”
STARTDATE:
DURING: January 1990
5 ACTIVITY:
PRODUCTION
PRODUCT:
“iron and “metal wood” clubs”
Figure 18.29
The ﬁve partial templates produced by stage 5 of FASTUS. These templates
are merged in stage 6 to produce the ﬁnal template shown in Fig. 18.27 on page 350.
18.6
Summary
This chapter has explored techniques for extracting limited forms of semantic con-
tent from texts.
• Named entities can be recognized and classiﬁed by featured-based or neural
sequence labeling techniques.
• Relations among entities can be extracted by pattern-based approaches, su-
pervised learning methods when annotated training data is available, lightly
supervised bootstrapping methods when small numbers of seed tuples or
seed patterns are available, distant supervision when a database of relations
is available, and unsupervised or Open IE methods.
• Reasoning about time can be facilitated by detection and normalization of
temporal expressions through a combination of statistical learning and rule-
based methods.
• Events can be detected and ordered in time using sequence models and classi-
ﬁers trained on temporally- and event-labeled data like the TimeBank corpus.

352
CHAPTER 18
•
INFORMATION EXTRACTION
• Template-ﬁlling applications can recognize stereotypical situations in texts
and assign elements from the text to roles represented as ﬁxed sets of slots.
Bibliographical and Historical Notes
The earliest work on information extraction addressed the template-ﬁlling task in the
context of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.
government-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sund-
heim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert
et al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite inﬂuential and inspired
later systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the
MUC evaluation techniques.
Due to the difﬁculty of porting systems from one domain to another, attention
shifted to machine learning approaches.
Early supervised learning approaches to IE ( Cardie 1993, Cardie 1994, Riloff 1993,
Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acqui-
sition process, mainly for ﬁnite-state rule-based systems. Their success, and the
earlier success of HMM-based speech recognition, led to the use of sequence la-
beling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Laf-
ferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural
approaches to NER mainly follow from the pioneering results of Collobert et al.
(2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and
character-based embeddings as input followed shortly and became a standard neural
algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016b).
Neural algorithms for relation extraction often explore architectures that can
handle entities far apart in the sentence: recursive networks (Socher et al., 2012),
convolutional nets (dos Santos et al., 2015), or chain or tree LSTMS (Miwa and
Bansal 2016, Peng et al. 2017).
Progress in this area continues to be stimulated by formal evaluations with shared
benchmark datasets, including the Automatic Content Extraction (ACE) evaluations
of 2000-2007 on named entity recognition, relation extraction, and temporal ex-
pressions3, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Sur-
KBP
deanu 2013) of relation extraction tasks like slot ﬁlling (extracting attributes (‘slots’)
slot ﬁlling
like age, birthplace, and spouse for a given entity) and a series of SemEval work-
shops (Hendrickx et al., 2009).
Semisupervised relation extraction was ﬁrst proposed by Hearst (1992b), and
extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-
BALL (Agichtein and Gravano, 2000), and (Jones et al., 1999). The distant super-
vision algorithm we describe was drawn from Mintz et al. (2009), who coined the
term ‘distant supervision’, but similar ideas occurred in earlier systems like Craven
and Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data,
as well as in Snow et al. (2005) and Wu and Weld (2007). Among the many exten-
sions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open
IE systems include KNOWITALL Etzioni et al. (2005), TextRunner (Banko et al.,
2007), and REVERB (Fader et al., 2011). See Riedel et al. (2013) for a universal
schema that combines the advantages of distant supervision and Open IE.
3
www.nist.gov/speech/tests/ace/

EXERCISES
353
HeidelTime (Str¨otgen and Gertz, 2013) and SUTime (Chang and Manning, 2012)
are downloadable temporal extraction and normalization systems. The 2013 TempE-
val challenge is described in UzZaman et al. (2013); Chambers (2013) and Bethard
(2013) give typical approaches.
Exercises
18.1 Develop a set of regular expressions to recognize the character shape features
described on page 329.
18.2 The IOB labeling scheme given in this chapter isn’t the only possible one. For
example, an E tag might be added to mark the end of entities, or the B tag
can be reserved only for those situations where an ambiguity exists between
adjacent entities. Propose a new set of IOB tags for use with your NER system.
Experiment with it and compare its performance with the scheme presented
in this chapter.
18.3 Names of works of art (books, movies, video games, etc.) are quite different
from the kinds of named entities we’ve discussed in this chapter. Collect a
list of names of works of art from a particular category from a Web-based
source (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list
and give examples of ways that the names in it are likely to be problematic for
the techniques described in this chapter.
18.4 Develop an NER system speciﬁc to the category of names that you collected in
the last exercise. Evaluate your system on a collection of text likely to contain
instances of these named entities.
18.5 Acronym expansion, the process of associating a phrase with an acronym, can
be accomplished by a simple form of relational analysis. Develop a system
based on the relation analysis approaches described in this chapter to populate
a database of acronym expansions. If you focus on English Three Letter
Acronyms (TLAs) you can evaluate your system’s performance by comparing
it to Wikipedia’s TLA page.
18.6 A useful functionality in newer email and calendar applications is the ability
to associate temporal expressions connected with events in email (doctor’s
appointments, meeting planning, party invitations, etc.) with speciﬁc calendar
entries. Collect a corpus of email containing temporal expressions related to
event planning. How do these expressions compare to the kinds of expressions
commonly found in news text that we’ve been discussing in this chapter?
18.7 Acquire the CMU seminar corpus and develop a template-ﬁlling system by
using any of the techniques mentioned in Section 18.5. Analyze how well
your system performs as compared with state-of-the-art results on this corpus.

354
CHAPTER 19
•
WORD SENSES AND WORDNET
CHAPTER
19
Word Senses and WordNet
To get a single right meaning is better than a ship-load of pearls,
To resolve a single doubt is like the bottom falling off the bucket.
Yuen Mei 袁枚(1785) (translation by Arthur Waley)
Words are ambiguous: the same word can be used to mean different things. For
ambiguous
example in Chapter 6 we saw that the word “mouse” has (at least) two meanings:
(1) a small rodent, or (2) a hand-operated device to control a cursor. Or the word
“bank” can mean: (1) a ﬁnancial institution or (2) a sloping mound.
We say that the words ‘mouse’ or ‘bank’ are polysemous (from Greek ‘having
many senses’, poly- ‘many’ + sema, ‘sign, mark’).1 A sense (or word sense) is
word sense
a discrete representation of one aspect of the meaning of a word. In this chapter
we discuss word senses in more detail and introduce WordNet, a large online the-
WordNet
saurus —a database that represents word senses—with versions in many languages.
WordNet also represents relations between senses. For example, there is an IS-A
relation between dog and mammal (a dog is a kind of mammal) and a part-whole
relation between engine and car (an engine is a part of a car).
Knowing the relation between two senses can play an important role in language
understanding. Consider the antonymy relation. Two words are antonyms if they
have opposite meanings, like long and short, or up and down. Distinguishing these is
quite important for language understanding (if a user asks a dialogue agent to turn up
the music, it would be unfortunate to instead turn it down). But in fact in embedding
models like word2vec, antonyms are easily confused with each other, because often
one of the closest words in embedding space to a word (e.g., up) is its antonym (e.g.,
down). Thesauruses that represent this relationship can help!
We also introduce word sense disambiguation (WSD), the task of determining
word sense
disambiguation
which sense of a word is being used in a particular context. We’ll give supervised
and unsupervised algorithms for deciding which sense was intended in a particular
context. This task has a very long history in computational linguistics and many ap-
plications. In question answering, we can be more helpful to a user who asks about
“bat care” if we know which sense of bat is relevant. (Is the user is a vampire? or
just wants to play baseball.) And the different senses of a word often have different
translations; in Spanish the animal bat is a murci´elago while the baseball bat is a
bate, and indeed word sense algorithms may help improve MT (Pu et al., 2018).
Finally, WSD has long been used as a tool for evaluating natural language under-
standing models, and understanding how models represent different word senses is
an important analytic direction.
1
You may also see the word polysemy used in a different way, to refer only to cases where a word’s
senses have some sort of semantic relation, and use the word homonymy for cases with no relation
between the senses.

19.1
•
WORD SENSES
355
19.1
Word Senses
A sense (or word sense) is a discrete representation of one aspect of the meaning of
word sense
a word. Loosely following lexicographic tradition, we represent each sense with a
superscript: bank1 and bank2, mouse1 and mouse2. In context, it’s easy to see the
different meanings:
mouse1 : .... a mouse controlling a computer system in 1968.
mouse2 : .... a quiet animal like a mouse
bank1 : ...a bank can hold the investments in a custodial account ...
bank2 : ...as agriculture burgeons on the east bank, the river ...
19.1.1
Deﬁning Word Senses
How can we deﬁne the meaning of a word sense? We introduced in Chapter 6 the
standard computational approach of representing a word as an embedding, a point
in semantic space. The intuition of embedding models like word2vec or GloVe is
that the meaning of a word can be deﬁned by its co-occurrences, the counts of words
that often occur nearby. But that doesn’t tell us how to deﬁne the meaning of a
word sense. Contextual embeddings like ELMo or BERT go further by offering an
embedding that represents the meaning of a word in its textual context, and we’ll
see that contextual embeddings lie at the heart of modern algorithms for word sense
disambiguation.
But ﬁrst, we need to consider the alternative ways that dictionaries and the-
sauruses offer for deﬁning senses. One is based on the fact that dictionaries or the-
sauruses give textual deﬁnitions for each sense called glosses. Here are the glosses
gloss
for two senses of bank:
1.
financial institution that accepts deposits and channels
the money into lending activities
2.
sloping land (especially the slope beside a body of water)
Glosses are not a formal meaning representation; they are just written for people.
Consider the following fragments from the deﬁnitions of right, left, red, and blood
from the American Heritage Dictionary (Morris, 1985).
right adj. located nearer the right hand esp. being on the right when
facing the same direction as the observer.
left adj. located nearer to this side of the body than the right.
red n. the color of blood or a ruby.
blood n. the red liquid that circulates in the heart, arteries and veins of
animals.
Note the circularity in these deﬁnitions. The deﬁnition of right makes two direct
references to itself, and the entry for left contains an implicit self-reference in the
phrase this side of the body, which presumably means the left side. The entries for
red and blood reference each other in their deﬁnitions. For humans, such entries are
useful since the user of the dictionary has sufﬁcient grasp of these other terms.
Yet despite their circularity and lack of formal representation, glosses can still
be useful for computational modeling of senses. This is because a gloss is just a sen-
tence, and from sentences we can compute sentence embeddings that tell us some-
thing about the meaning of the sense. Dictionaries often give example sentences
along with glosses, and these can again be used to help build a sense representation.

356
CHAPTER 19
•
WORD SENSES AND WORDNET
The second way that thesauruses offer for deﬁning a sense is—like the dictionary
deﬁnitions—deﬁning a sense through its relationship with other senses. For exam-
ple, the above deﬁnitions make it clear that right and left are similar kinds of lemmas
that stand in some kind of alternation, or opposition, to one another. Similarly, we
can glean that red is a color and that blood is a liquid. Sense relations of this sort
(IS-A, or antonymy) are explicitly listed in on-line databases like WordNet. Given
a sufﬁciently large database of such relations, many applications are quite capable
of performing sophisticated semantic tasks about word senses (even if they do not
really know their right from their left).
19.1.2
How many senses do words have?
Dictionaries and thesauruses give discrete lists of senses. By contrast, embeddings
(whether static or contextual) offer a continuous high-dimensional model of meaning
that doesn’t divide up into discrete senses.
Therefore creating a thesaurus depends on criteria for deciding when the differ-
ing uses of a word should be represented with discrete senses. We might consider
two senses discrete if they have independent truth conditions, different syntactic be-
havior, and independent sense relations, or if they exhibit antagonistic meanings.
Consider the following uses of the verb serve from the WSJ corpus:
(19.1) They rarely serve red meat, preferring to prepare seafood.
(19.2) He served as U.S. ambassador to Norway in 1976 and 1977.
(19.3) He might have served his time, come out and led an upstanding life.
The serve of serving red meat and that of serving time clearly have different truth
conditions and presuppositions; the serve of serve as ambassador has the distinct
subcategorization structure serve as NP. These heuristics suggest that these are prob-
ably three distinct senses of serve. One practical technique for determining if two
senses are distinct is to conjoin two uses of a word in a single sentence; this kind
of conjunction of antagonistic readings is called zeugma. Consider the following
zeugma
examples:
(19.4) Which of those ﬂights serve breakfast?
(19.5) Does Air France serve Philadelphia?
(19.6) ?Does Air France serve breakfast and Philadelphia?
We use (?) to mark those examples that are semantically ill-formed. The oddness of
the invented third example (a case of zeugma) indicates there is no sensible way to
make a single sense of serve work for both breakfast and Philadelphia. We can use
this as evidence that serve has two different senses in this case.
Dictionaries tend to use many ﬁne-grained senses so as to capture subtle meaning
differences, a reasonable approach given that the traditional role of dictionaries is
aiding word learners. For computational purposes, we often don’t need these ﬁne
distinctions, so we often group or cluster the senses; we have already done this for
some of the examples in this chapter. Indeed, clustering examples into senses, or
senses into broader-grained categories, is an important computational task that we’ll
discuss in Section 19.7.

19.2
•
RELATIONS BETWEEN SENSES
357
19.2
Relations Between Senses
This section explores the relations between word senses, especially those that have
received signiﬁcant computational investigation like synonymy, antonymy, and hy-
pernymy.
Synonymy
We introduced in Chapter 6 the idea that when two senses of two different words
(lemmas) are identical, or nearly identical, we say the two senses are synonyms.
synonym
Synonyms include such pairs as
couch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile
And we mentioned that in practice, the word synonym is commonly used to
describe a relationship of approximate or rough synonymy. But furthermore, syn-
onymy is actually a relationship between senses rather than words. Considering the
words big and large. These may seem to be synonyms in the following sentences,
since we could swap big and large in either sentence and retain the same meaning:
(19.7) How big is that plane?
(19.8) Would I be ﬂying on a large or small plane?
But note the following sentence in which we cannot substitute large for big:
(19.9) Miss Nelson, for instance, became a kind of big sister to Benjamin.
(19.10) ?Miss Nelson, for instance, became a kind of large sister to Benjamin.
This is because the word big has a sense that means being older or grown up, while
large lacks this sense. Thus, we say that some senses of big and large are (nearly)
synonymous while other ones are not.
Antonymy
Whereas synonyms are words with identical or similar meanings, antonyms are
antonym
words with an opposite meaning, like:
long/short big/little fast/slow cold/hot dark/light
rise/fall
up/down in/out
Two senses can be antonyms if they deﬁne a binary opposition or are at opposite
ends of some scale. This is the case for long/short, fast/slow, or big/little, which are
at opposite ends of the length or size scale. Another group of antonyms, reversives,
reversives
describe change or movement in opposite directions, such as rise/fall or up/down.
Antonyms thus differ completely with respect to one aspect of their meaning—
their position on a scale or their direction—but are otherwise very similar, sharing
almost all other aspects of meaning. Thus, automatically distinguishing synonyms
from antonyms can be difﬁcult.
Taxonomic Relations
Another way word senses can be related is taxonomically. A word (or sense) is a
hyponym of another word or sense if the ﬁrst is more speciﬁc, denoting a subclass
hyponym
of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal,
and mango is a hyponym of fruit. Conversely, we say that vehicle is a hypernym of
hypernym
car, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym

358
CHAPTER 19
•
WORD SENSES AND WORDNET
and hyponym) are very similar and hence easily confused; for this reason, the word
superordinate is often used instead of hypernym.
superordinate
Superordinate vehicle fruit
furniture mammal
Subordinate
car
mango chair
dog
We can deﬁne hypernymy more formally by saying that the class denoted by the
superordinate extensionally includes the class denoted by the hyponym. Thus, the
class of animals includes as members all dogs, and the class of moving actions in-
cludes all walking actions. Hypernymy can also be deﬁned in terms of entailment.
Under this deﬁnition, a sense A is a hyponym of a sense B if everything that is A is
also B, and hence being an A entails being a B, or ∀x A(x) ⇒B(x). Hyponymy/hy-
pernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym
of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure
is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A.
IS-A
Hypernymy is useful for tasks like textual entailment or question answering;
knowing that leukemia is a type of cancer, for example, would certainly be useful in
answering questions about leukemia.
Meronymy
Another common relation is meronymy, the part-whole relation. A leg is part of a
part-whole
chair; a wheel is part of a car. We say that wheel is a meronym of car, and car is a
holonym of wheel.
Structured Polysemy
The senses of a word can also be related semantically, in which case we call the
relationship between them structured polysemy.Consider this sense bank:
structured
polysemy
(19.11) The bank is on the corner of Nassau and Witherspoon.
This sense, perhaps bank4, means something like “the building belonging to
a ﬁnancial institution”. These two kinds of senses (an organization and the build-
ing associated with an organization ) occur together for many other words as well
(school, university, hospital, etc.). Thus, there is a systematic relationship between
senses that we might represent as
BUILDING ↔ORGANIZATION
This particular subtype of polysemy relation is called metonymy. Metonymy is
metonymy
the use of one aspect of a concept or entity to refer to other aspects of the entity or
to the entity itself. We are performing metonymy when we use the phrase the White
House to refer to the administration whose ofﬁce is in the White House. Other
common examples of metonymy include the relation between the following pairings
of senses:
AUTHOR
↔WORKS OF AUTHOR
(Jane Austen wrote Emma)
(I really love Jane Austen)
FRUITTREE
↔FRUIT
(Plums have beautiful blossoms)
(I ate a preserved plum yesterday)

19.3
•
WORDNET: A DATABASE OF LEXICAL RELATIONS
359
19.3
WordNet: A Database of Lexical Relations
The most commonly used resource for sense relations in English and many other
languages is the WordNet lexical database (Fellbaum, 1998). English WordNet
WordNet
consists of three separate databases, one each for nouns and verbs and a third for
adjectives and adverbs; closed class words are not included. Each database contains
a set of lemmas, each one annotated with a set of senses. The WordNet 3.0 release
has 117,798 nouns, 11,529 verbs, 22,479 adjectives, and 4,481 adverbs. The aver-
age noun has 1.23 senses, and the average verb has 2.16 senses. WordNet can be
accessed on the Web or downloaded locally. Figure 19.1 shows the lemma entry for
the noun and adjective bass.
The noun “bass” has 8 senses in WordNet.
1. bass1 - (the lowest part of the musical range)
2. bass2, bass part1 - (the lowest part in polyphonic music)
3. bass3, basso1 - (an adult male singer with the lowest voice)
4. sea bass1, bass4 - (the lean ﬂesh of a saltwater ﬁsh of the family Serranidae)
5. freshwater bass1, bass5 - (any of various North American freshwater ﬁsh with
lean ﬂesh (especially of the genus Micropterus))
6. bass6, bass voice1, basso2 - (the lowest adult male singing voice)
7. bass7 - (the member with the lowest range of a family of musical instruments)
8. bass8 - (nontechnical name for any of numerous edible marine and
freshwater spiny-ﬁnned ﬁshes)
Figure 19.1
A portion of the WordNet 3.0 entry for the noun bass.
Note that there are eight senses for the noun and one for the adjective, each of
which has a gloss (a dictionary-style deﬁnition), a list of synonyms for the sense, and
gloss
sometimes also usage examples (shown for the adjective sense). WordNet doesn’t
represent pronunciation, so doesn’t distinguish the pronunciation [b ae s] in bass4,
bass5, and bass8 from the other senses pronounced [b ey s].
The set of near-synonyms for a WordNet sense is called a synset (for synonym
synset
set); synsets are an important primitive in WordNet. The entry for bass includes
synsets like {bass1, deep6}, or {bass6, bass voice1, basso2}. We can think of a
synset as representing a concept of the type we discussed in Chapter 16. Thus,
instead of representing concepts in logical terms, WordNet represents them as lists
of the word senses that can be used to express the concept. Here’s another synset
example:
{chump1, fool2, gull1, mark9, patsy1, fall guy1,
sucker1, soft touch1, mug2}
The gloss of this synset describes it as:
Gloss: a person who is gullible and easy to take advantage of.
Glosses are properties of a synset, so that each sense included in the synset has the
same gloss and can express this concept. Because they share glosses, synsets like
this one are the fundamental unit associated with WordNet entries, and hence it is
synsets, not wordforms, lemmas, or individual senses, that participate in most of the
lexical sense relations in WordNet.
WordNet also labels each synset with a lexicographic category drawn from a
semantic ﬁeld for example the 26 categories for nouns shown in Fig. 19.2, as well

360
CHAPTER 19
•
WORD SENSES AND WORDNET
as 15 for verbs (plus 2 for adjectives and 1 for adverbs). These categories are often
called supersenses, because they act as coarse semantic categories or groupings of
supersense
senses which can be useful when word senses are too ﬁne-grained (Ciaramita and
Johnson 2003, Ciaramita and Altun 2006). Supersenses have also been deﬁned for
adjectives (Tsvetkov et al., 2014) and prepositions (Schneider et al., 2018).
Category
Example
Category
Example
Category
Example
ACT
service
GROUP
place
PLANT
tree
ANIMAL
dog
LOCATION
area
POSSESSION price
ARTIFACT
car
MOTIVE
reason
PROCESS
process
ATTRIBUTE
quality
NATURAL EVENT
experience
QUANTITY
amount
BODY
hair
NATURAL OBJECT ﬂower
RELATION
portion
COGNITION
way
OTHER
stuff
SHAPE
square
COMMUNICATION review
PERSON
people
STATE
pain
FEELING
discomfort
PHENOMENON
result
SUBSTANCE oil
FOOD
food
TIME
day
Figure 19.2
Supersenses: 26 lexicographic categories for nouns in WordNet.
19.3.1
Sense Relations in WordNet
WordNet represents all the kinds of sense relations discussed in the previous section,
as illustrated in Fig. 19.3 and Fig. 19.4.
Relation
Also Called
Deﬁnition
Example
Hypernym
Superordinate From concepts to superordinates
breakfast1 →meal1
Hyponym
Subordinate
From concepts to subtypes
meal1 →lunch1
Instance Hypernym Instance
From instances to their concepts
Austen1 →author1
Instance Hyponym
Has-Instance
From concepts to their instances
composer1 →Bach1
Part Meronym
Has-Part
From wholes to parts
table2 →leg3
Part Holonym
Part-Of
From parts to wholes
course7 →meal1
Antonym
Semantic opposition between lemmas leader1 ⇐⇒follower1
Derivation
Lemmas w/same morphological root
destruction1 ⇐⇒destroy1
Figure 19.3
Some of the noun relations in WordNet.
Relation
Deﬁnition
Example
Hypernym
From events to superordinate events
ﬂy9 →travel5
Troponym
From events to subordinate event
walk1 →stroll1
Entails
From verbs (events) to the verbs (events) they entail
snore1 →sleep1
Antonym
Semantic opposition between lemmas
increase1 ⇐⇒decrease1
Figure 19.4
Some verb relations in WordNet.
For example WordNet represents hyponymy (page 357) by relating each synset
to its immediately more general and more speciﬁc synsets through direct hypernym
and hyponym relations. These relations can be followed to produce longer chains
of more general or more speciﬁc synsets. Figure 19.5 shows hypernym chains for
bass3 and bass7; more general synsets are shown on successively indented lines.
WordNet has two kinds of taxonomic entities: classes and instances. An instance
is an individual, a proper noun that is a unique entity. San Francisco is an instance
of city, for example. But city is a class, a hyponym of municipality and eventually

19.3
•
WORDNET: A DATABASE OF LEXICAL RELATIONS
361
bass3, basso (an adult male singer with the lowest voice)
=> singer, vocalist, vocalizer, vocaliser
=> musician, instrumentalist, player
=> performer, performing artist
=> entertainer
=> person, individual, someone...
=> organism, being
=> living thing, animate thing,
=> whole, unit
=> object, physical object
=> physical entity
=> entity
bass7 (member with the lowest range of a family of instruments)
=> musical instrument, instrument
=> device
=> instrumentality, instrumentation
=> artifact, artefact
=> whole, unit
=> object, physical object
=> physical entity
=> entity
Figure 19.5
Hyponymy chains for two separate senses of the lemma bass. Note that the
chains are completely distinct, only converging at the very abstract level whole, unit.
of location. Fig. 19.6 shows a subgraph of WordNet demonstrating many of the
relations.
Figure 19.6
WordNet viewed as a graph. Figure from Navigli (2016).

362
CHAPTER 19
•
WORD SENSES AND WORDNET
19.4
Word Sense Disambiguation
The task of selecting the correct sense for a word is called word sense disambigua-
tion, or WSD. WSD algorithms take as input a word in context and a ﬁxed inventory
word sense
disambiguation
WSD
of potential word senses and outputs the correct word sense in context.
19.4.1
WSD: The Task and Datasets
In this section we introduce the task setup for WSD, and then turn to algorithms.
The inventory of sense tags depends on the task. For sense tagging in the context
of translation from English to Spanish, the sense tag inventory for an English word
might be the set of different Spanish translations. For automatic indexing of med-
ical articles, the sense-tag inventory might be the set of MeSH (Medical Subject
Headings) thesaurus entries. Or we can use the set of senses from a resource like
WordNet, or supersenses if we want a coarser-grain set. Figure 19.4.1 shows some
such examples for the word bass.
WordNet
Spanish
WordNet
Sense
Translation
Supersense
Target Word in Context
bass4
lubina
FOOD
. . . ﬁsh as Paciﬁc salmon and striped bass and. . .
bass7
bajo
ARTIFACT
. . . play bass because he doesn’t have to solo. . .
Figure 19.7
Some possibile sense tag inventories for bass.
In some situations, we just need to disambiguate a small number of words. In
such lexical sample tasks, we have a small pre-selected set of target words and an
lexical sample
inventory of senses for each word from some lexicon. Since the set of words and the
set of senses are small, simple supervised classiﬁcation approaches work very well.
More commonly, however, we have a harder problem in which we have to dis-
ambiguate all the words in some text. In this all-words task, the system is given an
all-words
entire texts and a lexicon with an inventory of senses for each entry and we have to
disambiguate every word in the text (or sometimes just every content word). The
all-words task is similar to part-of-speech tagging, except with a much larger set of
tags since each lemma has its own set. A consequence of this larger set of tags is
data sparseness.
Supervised all-word disambiguation tasks are generally trained from a semantic
concordance, a corpus in which each open-class word in each sentence is labeled
semantic
concordance
with its word sense from a speciﬁc dictionary or thesaurus, most often WordNet.
The SemCor corpus is a subset of the Brown Corpus consisting of over 226,036
words that were manually tagged with WordNet senses (Miller et al. 1993, Landes
et al. 1998). Other sense-tagged corpora have been built for the SENSEVAL and Se-
mEval WSD tasks, such as the SENSEVAL-3 Task 1 English all-words test data with
2282 annotations (Snyder and Palmer, 2004) or the SemEval-13 Task 12 datasets.
Large semantic concordances are also available in other languages including Dutch
(Vossen et al., 2011) and German (Henrich et al., 2012).
Here’s an example from the SemCor corpus showing the WordNet sense num-
bers of the tagged words; we’ve used the standard WSD notation in which a subscript
marks the part of speech (Navigli, 2009):
(19.12) You will ﬁnd9
v that avocado1
n is1
v unlike1
j other1
j fruit1
n you have ever1
r tasted2
v
Given each noun, verb, adjective, or adverb word in the hand-labeled test set (say
fruit), the SemCor-based WSD task is to choose the correct sense from the possible

19.4
•
WORD SENSE DISAMBIGUATION
363
senses in WordNet. For fruit this would mean choosing between the correct answer
fruit1
n (the ripened reproductive body of a seed plant), and the other two senses fruit2
n
(yield; an amount of a product) and fruit3
n (the consequence of some effort or action).
Fig. 19.8 sketches the task.
an
electric
guitar
and
bass
player
stand
oﬀ
to
one
side
electric1: 
using 
electricity
electric2:  
tense
electric3: 
thrilling
guitar1 
bass1: 
low range
…
bass4: 
sea ﬁsh
… 
bass7: 
instrument
…
player1: 
in game
player2: 
musician 
player3: 
actor
…
stand1: 
upright
…
stand5: 
bear
… 
stand10: 
put 
upright
…
side1: 
relative 
region
…
side3: 
of body
… 
side11: 
slope
…
x1
y1
x2
y2
x3
y3
y4
y5
y6
x4
x5
x6
Figure 19.8
The all-words WSD task, mapping from input words (x) to WordNet senses
(y). Only nouns, verbs, adjectives, and adverbs are mapped, and note that some words (like
guitar in the example) only have one sense in WordNet. Figure inspired by Chaplot and
Salakhutdinov (2018).
WSD systems are typically evaluated intrinsically, by computing F1 against
hand-labeled sense tags in a held-out set, such as the SemCor corpus or SemEval
corpora discussed above.
A surprisingly strong baseline is simply to choose the most frequent sense for
most frequent
sense
each word from the senses in a labeled corpus (Gale et al., 1992a). For WordNet, this
corresponds to the ﬁrst sense, since senses in WordNet are generally ordered from
most frequent to least frequent based on their counts in the SemCor sense-tagged
corpus. The most frequent sense baseline can be quite accurate, and is therefore
often used as a default, to supply a word sense when a supervised algorithm has
insufﬁcient training data.
A second heuristic, called one sense per discourse is based on the work of
one sense per
discourse
Gale et al. (1992b), who noticed that a word appearing multiple times in a text or
discourse often appears with the same sense. This heuristic seems to hold better for
coarse-grained senses and particularly for cases of homonymy rather than polysemy,
so isn’t generally used as a baseline. Nonetheless various kinds of disambiguation
tasks often include some such bias toward resolving an ambiguity the same way
inside a discourse segment.
19.4.2
The WSD Algorithm: Contextual Embeddings
The best-performing WSD algorithm is a simple 1-nearest-neighbor algorithm using
contextual word embeddings, due to Melamud et al. (2016) and Peters et al. (2018).
At training time we pass each sentence in the SemCore labeled dataset through any
contextual embedding (ELMo or BERT) resulting in a contextual embedding for
each labeled token in SemCore. For each token ci of each sense c of each word, we
average the contextual representations to produce a contextual sense embedding vs

364
CHAPTER 19
•
WORD SENSES AND WORDNET
for c:
vs = 1
n
X
i
ci
(19.13)
At test time we similarly compute a contextual embedding t for the target word, and
choose its nearest neighbor sense (the sense with the highest cosine with t) from the
training set. Fig. 19.9 illustrates the model.
I  found  the  jar  empty
Neural Language Model
cI
cfound
find1
v
cthe
cjar
cempty
find9
v
find5
v
find4
v
Figure 19.9
The nearest-neighbor algorithm for WSD. In green are the contextual embed-
dings precomputed for each sense of each word; here we just show a few of the senses for
ﬁnd. A contextual embedding is computed for the target word found, and the and then the
nearest neighbor sense (in this case ﬁnd9n) would be chosen. Figure inspired by Loureiro and
Jorge (2019).
What do we do for words we haven’t seen in the sense-labeled training data?
After all, the number of senses that appear in SemCor is only a small fraction of the
words in WordNet. The simplest algorithm is to fall back to the Most Frequent Sense
baseline, i.e. taking the ﬁrst sense in WordNet. But that’s not very satisfactory.
A more powerful approach, due to Loureiro and Jorge (2019), is to impute the
missing sense embeddings, bottom-up, by using the WordNet taxonomy and super-
senses. We get a sense embedding for any higher-level node in the WordNet taxon-
omy by averaging the embeddings of its children, thus computing the embedding for
each synset as the average of its sense embeddings, the embedding for a hypernym
as the average of its synset embeddings, and the lexicographic category (supersense)
embedding as the average of the large set of synset embeddings with that category.
More formally, for each missing sense in WordNet ˆs ∈W, let the sense embeddings
for the other members of its synset be Sˆs, the hypernym-speciﬁc synset embeddings
be Hˆs, and the lexicographic (supersense-speciﬁc) synset embeddings be Lˆs. We can
then compute the sense embedding for ˆs as follows:
if
|Sˆs| > 0, vˆs = 1
|Sˆs|
X
vs,∀vs ∈Sˆs
(19.14)
else if
|Hˆs| > 0, vˆs =
1
|Hˆs|
X
vsyn,∀vsyn ∈Hˆs
(19.15)
else if
|Lˆs| > 0, vˆs =
1
|Lˆs|
X
vsyn,∀vsyn ∈Lˆs
(19.16)
Since all of the supersenses have some labeled data in SemCor, the algorithm is
guaranteed to have some representation for all possible senses by the time the al-

19.5
•
ALTERNATE WSD ALGORITHMS AND TASKS
365
gorithm backs off to the most general (supersense) information, although of course
with a very coarse model.
19.5
Alternate WSD algorithms and Tasks
19.5.1
Feature-Based WSD
Feature-based algorithms for WSD are extremely simple and function almost as
well as contextual language model algorithms. The best-performing IMS algorithm
(Zhong and Ng, 2010), augmented by embeddings (Iacobacci et al. 2016, Raganato
et al. 2017b), uses an SVM classiﬁer to choose the sense for each input word with
the following simple features of the surrounding words:
• part-of-speech tags (for a window of 3 words on each side, stopping at sen-
tence boundaries)
• collocation features of words or n-grams of lengths 1, 2, 3) at a particular
collocation
location in a window of 3 word on each side (i.e., exactly one word to the
right, or the two words starting 3 words to the left, and so on).
• weighted average of embeddings (of all words in a window of 10 words on
each side, weighted exponentially by distance)
Consider the ambiguous word bass in the following WSJ sentence:
(19.17) An electric guitar and bass player stand off to one side,
If we used a small 2-word window, a standard feature vector might include parts-of-
speech, unigram and bigram collocation features, and a weighted sum g of embed-
dings, that is:
[wi−2,POSi−2,wi−1,POSi−1,wi+1,POSi+1,wi+2,POSi+2,wi−1
i−2,
wi+2
i+1,g(E(wi−2),E(wi−1),E(wi+1),E(wi+2)]
(19.18)
would yield the following vector:
[guitar, NN, and, CC, player, NN, stand, VB, and guitar,
player stand, g(E(guitar),E(and),E(player),E(stand))]
19.5.2
The Lesk Algorithm as WSD Baseline
Generating sense labeled corpora like SemCor is quite difﬁcult and expensive. An
alternative class of WSD algorithms, knowledge-based algorithms, rely solely on
knowledge-
based
WordNet or other such resources and don’t require labeled data. While supervised
algorithms generally work better, knowledge-based methods can be used in lan-
guages or domains where thesauruses or dictionaries but not sense labeled corpora
are available.
The Lesk algorithm is the oldest and most powerful knowledge-based WSD
Lesk algorithm
method, and is a useful baseline. Lesk is really a family of algorithms that choose
the sense whose dictionary gloss or deﬁnition shares the most words with the target
word’s neighborhood. Figure 19.10 shows the simplest version of the algorithm,
often called the Simpliﬁed Lesk algorithm (Kilgarriff and Rosenzweig, 2000).
Simpliﬁed Lesk
As an example of the Lesk algorithm at work, consider disambiguating the word
bank in the following context:

366
CHAPTER 19
•
WORD SENSES AND WORDNET
function SIMPLIFIED LESK(word, sentence) returns best sense of word
best-sense←most frequent sense for word
max-overlap←0
context←set of words in sentence
for each sense in senses of word do
signature←set of words in the gloss and examples of sense
overlap←COMPUTEOVERLAP(signature, context)
if overlap > max-overlap then
max-overlap←overlap
best-sense←sense
end
return(best-sense)
Figure 19.10
The Simpliﬁed Lesk algorithm. The COMPUTEOVERLAP function returns
the number of words in common between two sets, ignoring function words or other words
on a stop list. The original Lesk algorithm deﬁnes the context in a more complex way.
(19.19) The bank can guarantee deposits will eventually cover future tuition costs
because it invests in adjustable-rate mortgage securities.
given the following two WordNet senses:
bank1
Gloss:
a ﬁnancial institution that accepts deposits and channels the
money into lending activities
Examples:
“he cashed a check at the bank”, “that bank holds the mortgage
on my home”
bank2
Gloss:
sloping land (especially the slope beside a body of water)
Examples:
“they pulled the canoe up on the bank”, “he sat on the bank of
the river and watched the currents”
Sense bank1 has two non-stopwords overlapping with the context in (19.19):
deposits and mortgage, while sense bank2 has zero words, so sense bank1 is chosen.
There are many obvious extensions to Simpliﬁed Lesk, such as weighing the
overlapping words by IDF (inverse document frequency) Chapter 6 to downweight
frequent words like function words; best performing is to use word embedding co-
sine instead of word overlap to compute the similarity between the deﬁnition and the
context (Basile et al., 2014). Modern neural extensions of Lesk use the deﬁnitions
to compute sense embeddings that can be directly used instead of SemCor-training
embeddings (Kumar et al. 2019, Luo et al. 2018a, Luo et al. 2018b).
19.5.3
Word-in-Context Evaluation
Word Sense Disambiguation is a much more ﬁne-grained evaluation of word mean-
ing than the context-free word similarity tasks we described in Chapter 6. Recall that
tasks like LexSim-999 require systems to match human judgments on the context-
free similarity between two words (how similar is cup to mug?). We can think of
WSD as a kind of contextualized similarity task, since our goal is to be able to dis-
tinguish the meaning of a word lke bass in one context (playing music) from another
context (ﬁshing).
Somewhere in between lies the word-in-context task. Here the system is given
word-in-context
two sentences, each with the same target word but in a different sentential context.
The system must decide whether the target words are used in the same sense in the

19.5
•
ALTERNATE WSD ALGORITHMS AND TASKS
367
two sentences or in a different sense. Fig. 19.11 shows sample pairs from the WiC
WiC
dataset of Pilehvar and Camacho-Collados (2019).
F There’s a lot of trash on the bed of the river —
I keep a glass of water next to my bed when I sleep
F Justify the margins — The end justiﬁes the means
T Air pollution — Open a window and let in some air
T The expanded window will give us time to catch the thieves —
You have a two-hour window of clear weather to ﬁnish working on the lawn
Figure 19.11
Positive (T) and negative (F) pairs from the WiC dataset (Pilehvar and
Camacho-Collados, 2019).
The WiC sentences are mainly taken from the example usages for senses in
WordNet. But WordNet senses are very ﬁne-grained. For this reason tasks like
word-in-context ﬁrst cluster the word senses into coarser clusters, so that the two
sentential contexts for the target word are marked as T if the two senses are in the
same cluster. WiC clusters all pairs of senses if they are ﬁrst degree connections in
the WordNet semantic graph, including sister senses, or if they belong to the same
supersense; we point to other sense clustering algorithms at the end of the chapter.
The baseline algorithm to solve the WIC task uses contextual embeddings like
BERT with a simple thesholded cosine. We ﬁrst compute the contextual embed-
dings for the target word in each of the two sentences, and then compute the cosine
between them. If it’s above a threshold tuned on a devset we respond true (the two
senses are the same) else we respond false.
19.5.4
Wikipedia as a source of training data
Datasets other than SemCor have been used for all-words WSD. One important di-
rection is to use Wikipedia as a source of sense-labeled data. When a concept is
mentioned in a Wikipedia article, the article text may contain an explicit link to the
concept’s Wikipedia page, which is named by a unique identiﬁer. This link can be
used as a sense annotation. For example, the ambiguous word bar is linked to a
different Wikipedia article depending on its meaning in context, including the page
BAR (LAW), the page BAR (MUSIC), and so on, as in the following Wikipedia
examples (Mihalcea, 2007).
In 1834, Sumner was admitted to the [[bar (law)|bar]] at the age of
twenty-three, and entered private practice in Boston.
It is danced in 3/4 time (like most waltzes), with the couple turning
approx. 180 degrees every [[bar (music)|bar]].
Jenga is a popular beer in the [[bar (establishment)|bar]]s of Thailand.
These sentences can then be added to the training data for a supervised system.
In order to use Wikipedia in this way, however, it is necessary to map from Wikipedia
concepts to whatever inventory of senses is relevant for the WSD application. Auto-
matic algorithms that map from Wikipedia to WordNet, for example, involve ﬁnding
the WordNet sense that has the greatest lexical overlap with the Wikipedia sense, by
comparing the vector of words in the WordNet synset, gloss, and related senses with
the vector of words in the Wikipedia page title, outgoing links, and page category
(Ponzetto and Navigli, 2010). The resulting mapping has been used to create Babel-
Net, a large sense-annotated resource (Navigli and Ponzetto, 2012).

368
CHAPTER 19
•
WORD SENSES AND WORDNET
19.6
Using Thesauruses to Improve Embeddings
Thesauruses have also been used to improve both static and contextual word em-
beddings. For example, static word embeddings have a problem with antonyms.
A word like expensive is often very similar in embedding cosine to its antonym
like cheap. Antonymy information from thesauruses can help solve this problem;
Fig. 19.12 shows nearest neighbors to some target words in GloVe, and the improve-
ment after one such method.
Before counterﬁtting
After counterﬁtting
east
west
north
south
eastward eastern
easterly
expensive pricey
cheaper
costly
costly
pricy
overpriced
British
American Australian Britain
Brits
London BBC
Figure 19.12
The nearest neighbors in GloVe to east, expensive, and British include
antonyms like west. The right side showing the improvement in GloVe nearest neighbors
after the counterﬁtting method (Mrkˇsi´c et al., 2016).
There are two families of solutions. The ﬁrst requires retraining: we modify the
embedding training to incorporate thesaurus relations like synonymy, antonym, or
supersenses. This can be done by modifying the static embedding loss function for
word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual
embedding training (Levine et al. 2019, Lauscher et al. 2019).
The second, for static embeddings, is more light-weight; after the embeddings
have been trained we learn a second mapping based on a thesaurus that shifts the
embeddings of words in such a way that synonyms (according to the thesaurus) are
pushed closer and antonyms further apart. Such methods are called retroﬁtting
retroﬁtting
(Faruqui et al. 2015, Lengerich et al. 2018) or counterﬁtting (Mrkˇsi´c et al., 2016).
19.7
Word Sense Induction
It is expensive and difﬁcult to build large corpora in which each word is labeled for
its word sense. For this reason, an unsupervised approach to sense disambiguation,
often called word sense induction or WSI, is an important direction. In unsu-
word sense
induction
pervised approaches, we don’t use human-deﬁned word senses. Instead, the set of
“senses” of each word is created automatically from the instances of each word in
the training set.
Most algorithms for word sense induction follow the early work of Sch¨utze
(Sch¨utze 1992b, Sch¨utze 1998) in using some sort of clustering over word embed-
dings. In training, we use three steps:
1. For each token wi of word w in a corpus, compute a context vector c.
2. Use a clustering algorithm to cluster these word-token context vectors c into
a predeﬁned number of groups or clusters. Each cluster deﬁnes a sense of w.
3. Compute the vector centroid of each cluster. Each vector centroid sj is a
sense vector representing that sense of w.
Since this is an unsupervised algorithm, we don’t have names for each of these
“senses” of w; we just refer to the jth sense of w.
To disambiguate a particular token t of w we again have three steps:

19.8
•
SUMMARY
369
1. Compute a context vector c for t.
2. Retrieve all sense vectors sj for w.
3. Assign t to the sense represented by the sense vector sj that is closest to t.
All we need is a clustering algorithm and a distance metric between vectors.
Clustering is a well-studied problem with a wide number of standard algorithms that
can be applied to inputs structured as vectors of numerical values (Duda and Hart,
1973). A frequently used technique in language applications is known as agglom-
erative clustering.
In this technique, each of the N training instances is initially
agglomerative
clustering
assigned to its own cluster. New clusters are then formed in a bottom-up fashion by
the successive merging of the two clusters that are most similar. This process con-
tinues until either a speciﬁed number of clusters is reached, or some global goodness
measure among the clusters is achieved. In cases in which the number of training
instances makes this method too expensive, random sampling can be used on the
original training set to achieve similar results.
How can we evaluate unsupervised sense disambiguation approaches? As usual,
the best way is to do extrinsic evaluation embedded in some end-to-end system; one
example used in a SemEval bakeoff is to improve search result clustering and di-
versiﬁcation (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to
map the automatically derived sense classes into a hand-labeled gold-standard set so
that we can compare a hand-labeled test set with a set labeled by our unsupervised
classiﬁer. Various such metrics have been tested, for example in the SemEval tasks
(Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013),
including cluster overlap metrics, or methods that map each sense cluster to a pre-
deﬁned sense by choosing the sense that (in some training set) has the most overlap
with the cluster. However it is fair to say that no evaluation metric for this task has
yet become standard.
19.8
Summary
This chapter has covered a wide range of issues concerning the meanings associated
with lexical items. The following are among the highlights:
• A word sense is the locus of word meaning; deﬁnitions and meaning relations
are deﬁned at the level of the word sense rather than wordforms.
• Many words are polysemous, having many senses.
• Relations between senses include synonymy, antonymy, meronymy, and
taxonomic relations hyponymy and hypernymy.
• WordNet is a large database of lexical relations for English, and WordNets
exist for a variety of languages.
• Word-sense disambiguation (WSD) is the task of determining the correct
sense of a word in context. Supervised approaches make use of a corpus
of sentences in which individual words (lexical sample task) or all words
(all-words task) are hand-labeled with senses from a resource like WordNet.
SemCor is the largest corpus with WordNet-labeled senses.
• The standard supervised algorithm for WSD is nearest neighbors with contex-
tual embeddings.
• Feature-based algorithms using parts of speech and embeddings of words in
the context of the target word also work well.

370
CHAPTER 19
•
WORD SENSES AND WORDNET
• An important baseline for WSD is the most frequent sense, equivalent, in
WordNet, to take the ﬁrst sense.
• Another baseline is a knowledge-based WSD algorithm called the Lesk al-
gorithm which chooses the sense whose dictionary deﬁnition shares the most
words with the target word’s neighborhood.
• Word sense induction is the task of learning word senses unsupervised.
Bibliographical and Historical Notes
Word sense disambiguation traces its roots to some of the earliest applications of
digital computers. The insight that underlies modern algorithms for word sense
disambiguation was ﬁrst articulated by Weaver (1955) in the context of machine
translation:
If one examines the words in a book, one at a time as through an opaque
mask with a hole in it one word wide, then it is obviously impossible
to determine, one at a time, the meaning of the words. [...] But if
one lengthens the slit in the opaque mask, until one can see not only
the central word in question but also say N words on either side, then
if N is large enough one can unambiguously decide the meaning of the
central word. [...] The practical question is : “What minimum value of
N will, at least in a tolerable fraction of cases, lead to the correct choice
of meaning for the central word?”
Other notions ﬁrst proposed in this early period include the use of a thesaurus for dis-
ambiguation (Masterman, 1957), supervised training of Bayesian models for disam-
biguation (Madhu and Lytel, 1965), and the use of clustering in word sense analysis
(Sparck Jones, 1986).
An enormous amount of work on disambiguation was conducted within the con-
text of early AI-oriented natural language processing systems. Quillian (1968) and
Quillian (1969) proposed a graph-based approach to language understanding, in
which the dictionary deﬁnition of words was represented by a network of word nodes
connected by syntactic and semantic relations. He then proposed to do sense disam-
biguation by ﬁnding the shortest path between senses in the conceptual graph. Sim-
mons (1973) is another inﬂuential early semantic network approach. Wilks proposed
one of the earliest non-discrete models with his Preference Semantics (Wilks 1975c,
Wilks 1975b, Wilks 1975a), and Small and Rieger (1982) and Riesbeck (1975) pro-
posed understanding systems based on modeling rich procedural information for
each word. Hirst’s ABSITY system (Hirst and Charniak 1982, Hirst 1987, Hirst 1988),
which used a technique called marker passing based on semantic networks, repre-
sents the most advanced system of this type. As with these largely symbolic ap-
proaches, early neural network (at the time called ‘connectionist’) approaches to
word sense disambiguation relied on small lexicons with hand-coded representa-
tions (Cottrell 1985, Kawamoto 1988).
The earliest implementation of a robust empirical approach to sense disambigua-
tion is due to Kelly and Stone (1975), who directed a team that hand-crafted a set of
disambiguation rules for 1790 ambiguous English words. Lesk (1986) was the ﬁrst
to use a machine-readable dictionary for word sense disambiguation. A collection
of work concerning WordNet can be found in Fellbaum (1998). Early work using

EXERCISES
371
dictionaries as lexical resources include Amsler’s (1981) use of the Merriam Web-
ster dictionary and Longman’s Dictionary of Contemporary English (Boguraev and
Briscoe, 1989).
Supervised approaches to disambiguation began with the use of decision trees
by Black (1988). In addition to the IMS and contextual-embedding based methods
for supervised WSD, recent supervised algorithms includes encoder-decoder models
(Raganato et al., 2017a).
The need for large amounts of annotated text in supervised methods led early
on to investigations into the use of bootstrapping methods (Hearst 1991, Yarow-
sky 1995). For example Diab and Resnik (2002) give a semi-supervised algorithm
for sense disambiguation based on aligned parallel corpora in two languages. For
example, the fact that the French word catastrophe might be translated as English
disaster in one instance and tragedy in another instance can be used to disambiguate
the senses of the two English words (i.e., to choose senses of disaster and tragedy
that are similar).
The earliest use of clustering in the study of word senses was by Sparck Jones
(1986); Pedersen and Bruce (1997), Sch¨utze (1997b), and Sch¨utze (1998) applied
distributional methods. Clustering word senses into coarse senses has also been
coarse senses
used to address the problem of dictionary senses being too ﬁne-grained (Section 19.5.3)
(Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and
de Lacalle 2003, Palmer et al. 2004, Navigli 2006, Snow et al. 2007, Pilehvar
et al. 2013). Corpora with clustered word senses for training supervised clustering
algorithms include Palmer et al. (2006) and OntoNotes (Hovy et al., 2006).
OntoNotes
Historical overviews of WSD include Agirre and Edmonds (2006) and Navigli
(2009).
See Pustejovsky (1995), Pustejovsky and Boguraev (1996), Martin (1986), and
Copestake and Briscoe (1995), inter alia, for computational approaches to the rep-
resentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in
generative
lexicon
particular his theory of the qualia structure of words, is a way of accounting for the
qualia
structure
dynamic systematic polysemy of words in context.
Exercises
19.1 Collect a small corpus of example sentences of varying lengths from any
newspaper or magazine. Using WordNet or any standard dictionary, deter-
mine how many senses there are for each of the open-class words in each sen-
tence. How many distinct combinations of senses are there for each sentence?
How does this number seem to vary with sentence length?
19.2 Using WordNet or a standard reference dictionary, tag each open-class word
in your corpus with its correct tag. Was choosing the correct sense always a
straightforward task? Report on any difﬁculties you encountered.
19.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-
ambiguation algorithm described on page 366 on the phrase Time ﬂies like an
arrow. Assume that the words are to be disambiguated one at a time, from
left to right, and that the results from earlier decisions are used later in the
process.

372
CHAPTER 19
•
WORD SENSES AND WORDNET
19.4 Build an implementation of your solution to the previous exercise. Using
WordNet, implement the original Lesk word overlap disambiguation algo-
rithm described on page ?? on the phrase Time ﬂies like an arrow.

CHAPTER
20
Semantic Role Labeling
Sometime between the 7th and 4th centuries BCE, the Indian grammarian P¯an.ini1
wrote a famous treatise on Sanskrit grammar, the As.t.¯adhy¯ay¯ı (‘8 books’), a treatise
that has been called “one of the greatest monuments of
human intelligence” (Bloomﬁeld, 1933, 11). The work
describes the linguistics of the Sanskrit language in the
form of 3959 sutras, each very efﬁciently (since it had to
be memorized!) expressing part of a formal rule system
that brilliantly preﬁgured modern mechanisms of formal
language theory (Penn and Kiparsky, 2012). One set of
rules, relevant to our discussion in this chapter, describes
the k¯arakas, semantic relationships between a verb and
noun arguments, roles like agent, instrument, or destina-
tion. P¯an.ini’s work was the earliest we know of that tried
to understand the linguistic realization of events and their participants. This task
of understanding how participants relate to events—being able to answer the ques-
tion “Who did what to whom” (and perhaps also “when and where”)—is a central
question of natural language understanding.
Let’s move forward 2.5 millennia to the present and consider the very mundane
goal of understanding text about a purchase of stock by XYZ Corporation. This
purchasing event and its participants can be described by a wide variety of surface
forms. The event can be described by a verb (sold, bought) or a noun (purchase),
and XYZ Corp can be the syntactic subject (of bought), the indirect object (of sold),
or in a genitive or noun compound relation (with the noun purchase) despite having
notionally the same role in all of them:
• XYZ corporation bought the stock.
• They sold the stock to XYZ corporation.
• The stock was bought by XYZ corporation.
• The purchase of the stock by XYZ corporation...
• The stock purchase by XYZ corporation...
In this chapter we introduce a level of representation that captures the common-
ality between these sentences: there was a purchase event, the participants were
XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic
representations , semantic roles, express the role that arguments of a predicate take
in the event, codiﬁed in databases like PropBank and FrameNet. We’ll introduce
semantic role labeling, the task of assigning roles to spans in sentences, and selec-
tional restrictions, the preferences that predicates express about their arguments,
such as the fact that the theme of eat is generally something edible.
1
Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based
on the Sanskrit grammar of Panini. Image from the Wellcome Collection.

374
CHAPTER 20
•
SEMANTIC ROLE LABELING
20.1
Semantic Roles
Consider how in Chapter 16 we represented the meaning of arguments for sentences
like these:
(20.1) Sasha broke the window.
(20.2) Pat opened the door.
A neo-Davidsonian event representation of these two sentences would be
∃e,x,y Breaking(e)∧Breaker(e,Sasha)
∧BrokenThing(e,y)∧Window(y)
∃e,x,y Opening(e)∧Opener(e,Pat)
∧OpenedThing(e,y)∧Door(y)
In this representation, the roles of the subjects of the verbs break and open are
Breaker and Opener respectively. These deep roles are speciﬁc to each event; Break-
deep roles
ing events have Breakers, Opening events have Openers, and so on.
If we are going to be able to answer questions, perform inferences, or do any
further kinds of natural language understanding of these events, we’ll need to know
a little more about the semantics of these arguments. Breakers and Openers have
something in common. They are both volitional actors, often animate, and they have
direct causal responsibility for their events.
Thematic roles are a way to capture this semantic commonality between Break-
thematic roles
ers and Eaters. We say that the subjects of both these verbs are agents. Thus, AGENT
agents
is the thematic role that represents an abstract idea such as volitional causation. Sim-
ilarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are
both prototypically inanimate objects that are affected in some way by the action.
The semantic role for these participants is theme.
theme
Thematic Role
Deﬁnition
AGENT
The volitional causer of an event
EXPERIENCER
The experiencer of an event
FORCE
The non-volitional causer of the event
THEME
The participant most directly affected by an event
RESULT
The end product of an event
CONTENT
The proposition or content of a propositional event
INSTRUMENT
An instrument used in an event
BENEFICIARY
The beneﬁciary of an event
SOURCE
The origin of the object of a transfer event
GOAL
The destination of an object of a transfer event
Figure 20.1
Some commonly used thematic roles with their deﬁnitions.
Although thematic roles are one of the oldest linguistic models, as we saw above,
their modern formulation is due to Fillmore (1968) and Gruber (1965). Although
there is no universally agreed-upon set of roles, Figs. 20.1 and 20.2 list some the-
matic roles that have been used in various computational papers, together with rough
deﬁnitions and examples. Most thematic role sets have about a dozen roles, but we’ll
see sets with smaller numbers of roles with even more abstract meanings, and sets
with very large numbers of roles that are speciﬁc to situations. We’ll use the general
term semantic roles for all sets of roles, whether small or large.
semantic roles

20.2
•
DIATHESIS ALTERNATIONS
375
Thematic Role
Example
AGENT
The waiter spilled the soup.
EXPERIENCER
John has a headache.
FORCE
The wind blows debris from the mall into our yards.
THEME
Only after Benjamin Franklin broke the ice...
RESULT
The city built a regulation-size baseball diamond...
CONTENT
Mona asked “You met Mary Ann at a supermarket?”
INSTRUMENT
He poached catﬁsh, stunning them with a shocking device...
BENEFICIARY
Whenever Ann Callahan makes hotel reservations for her boss...
SOURCE
I ﬂew in from Boston.
GOAL
I drove to Portland.
Figure 20.2
Some prototypical examples of various thematic roles.
20.2
Diathesis Alternations
The main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple inferences that aren’t possible
from the pure surface string of words, or even from the parse tree. To extend the
earlier examples, if a document says that Company A acquired Company B, we’d
like to know that this answers the query Was Company B acquired? despite the fact
that the two sentences have very different surface syntax. Similarly, this shallow
semantics might act as a useful intermediate language in machine translation.
Semantic roles thus help generalize over different surface realizations of pred-
icate arguments. For example, while the AGENT is often realized as the subject of
the sentence, in other cases the THEME can be the subject. Consider these possible
realizations of the thematic arguments of the verb break:
(20.3) John
AGENT
broke the window.
THEME
(20.4) John
AGENT
broke the window
THEME
with a rock.
INSTRUMENT
(20.5) The rock
INSTRUMENT
broke the window.
THEME
(20.6) The window
THEME
broke.
(20.7) The window
THEME
was broken by John.
AGENT
These examples suggest that break has (at least) the possible arguments AGENT,
THEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is
often called the thematic grid, θ-grid, or case frame. We can see that there are
thematic grid
case frame
(among others) the following possibilities for the realization of these arguments of
break:
AGENT/Subject, THEME/Object
AGENT/Subject,
THEME/Object,
INSTRUMENT/PPwith
INSTRUMENT/Subject,
THEME/Object
THEME/Subject
It turns out that many verbs allow their thematic roles to be realized in various
syntactic positions. For example, verbs like give can realize the THEME and GOAL
arguments in two different ways:

376
CHAPTER 20
•
SEMANTIC ROLE LABELING
(20.8)
a. Doris
AGENT
gave the book
THEME
to Cary.
GOAL
b. Doris
AGENT
gave Cary
GOAL
the book.
THEME
These multiple argument structure realizations (the fact that break can take AGENT,
INSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in
either order) are called verb alternations or diathesis alternations. The alternation
verb
alternation
we showed above for give, the dative alternation, seems to occur with particular se-
dative
alternation
mantic classes of verbs, including “verbs of future having” (advance, allocate, offer,
owe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw),
and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which
they belong (47 high-level classes, divided into 193 more speciﬁc classes) and the
various alternations in which they participate. These lists of verb classes have been
incorporated into the online resource VerbNet (Kipper et al., 2000), which links each
verb to both WordNet and FrameNet entries.
20.3
Semantic Roles: Problems with Thematic Roles
Representing meaning at the thematic role level seems like it should be useful in
dealing with complications like diathesis alternations. Yet it has proved quite difﬁ-
cult to come up with a standard set of roles, and equally difﬁcult to produce a formal
deﬁnition of roles like AGENT, THEME, or INSTRUMENT.
For example, researchers attempting to deﬁne role sets often ﬁnd they need to
fragment a role like AGENT or THEME into many speciﬁc roles. Levin and Rappa-
port Hovav (2005) summarize a number of such cases, such as the fact there seem
to be at least two kinds of INSTRUMENTS, intermediary instruments that can appear
as subjects and enabling instruments that cannot:
(20.9)
a. The cook opened the jar with the new gadget.
b. The new gadget opened the jar.
(20.10)
a. Shelly ate the sliced banana with a fork.
b. *The fork ate the sliced banana.
In addition to the fragmentation problem, there are cases in which we’d like to
reason about and generalize across semantic roles, but the ﬁnite discrete lists of roles
don’t let us do this.
Finally, it has proved difﬁcult to formally deﬁne the thematic roles. Consider the
AGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any
individual noun phrase might not exhibit all of these properties.
These problems have led to alternative semantic role models that use either
semantic role
many fewer or many more roles.
The ﬁrst of these options is to deﬁne generalized semantic roles that abstract
over the speciﬁc thematic roles. For example, PROTO-AGENT and PROTO-PATIENT
proto-agent
proto-patient
are generalized roles that express roughly agent-like and roughly patient-like mean-
ings. These roles are deﬁned, not by necessary and sufﬁcient conditions, but rather
by a set of heuristic features that accompany more agent-like or more patient-like
meanings. Thus, the more an argument displays agent-like properties (being voli-
tionally involved in the event, causing an event or a change of state in another par-
ticipant, being sentient or intentionally involved, moving) the greater the likelihood

20.4
•
THE PROPOSITION BANK
377
that the argument can be labeled a PROTO-AGENT. The more patient-like the proper-
ties (undergoing change of state, causally affected by another participant, stationary
relative to other participants, etc.), the greater the likelihood that the argument can
be labeled a PROTO-PATIENT.
The second direction is instead to deﬁne semantic roles that are speciﬁc to a
particular verb or a particular group of semantically related verbs or nouns.
In the next two sections we describe two commonly used lexical resources that
make use of these alternative versions of semantic roles. PropBank uses both proto-
roles and verb-speciﬁc semantic roles. FrameNet uses semantic roles that are spe-
ciﬁc to a general semantic idea called a frame.
20.4
The Proposition Bank
The Proposition Bank, generally referred to as PropBank, is a resource of sen-
PropBank
tences annotated with semantic roles. The English PropBank labels all the sentences
in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese
TreeBank. Because of the difﬁculty of deﬁning a universal set of thematic roles,
the semantic roles in PropBank are deﬁned with respect to an individual verb sense.
Each sense of each verb thus has a speciﬁc set of roles, which are given only numbers
rather than names: Arg0, Arg1, Arg2, and so on. In general, Arg0 represents the
PROTO-AGENT, and Arg1, the PROTO-PATIENT. The semantics of the other roles
are less consistent, often being deﬁned speciﬁcally for each verb. Nonetheless there
are some generalization; the Arg2 is often the benefactive, instrument, attribute, or
end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4
the end point.
Here are some slightly simpliﬁed PropBank entries for one sense each of the
verbs agree and fall. Such PropBank entries are called frame ﬁles; note that the
deﬁnitions in the frame ﬁle for each role (“Other entity agreeing”, “Extent, amount
fallen”) are informal glosses intended to be read by humans, rather than being formal
deﬁnitions.
(20.11) agree.01
Arg0: Agreer
Arg1: Proposition
Arg2: Other entity agreeing
Ex1:
[Arg0 The group] agreed [Arg1 it wouldn’t make an offer].
Ex2:
[ArgM-TMP Usually] [Arg0 John] agrees [Arg2 with Mary]
[Arg1 on everything].
(20.12) fall.01
Arg1: Logical subject, patient, thing falling
Arg2: Extent, amount fallen
Arg3: start point
Arg4: end point, end state of arg1
Ex1:
[Arg1 Sales] fell [Arg4 to $25 million] [Arg3 from $27 million].
Ex2:
[Arg1 The average junk bond] fell [Arg2 by 4.2%].
Note that there is no Arg0 role for fall, because the normal subject of fall is a
PROTO-PATIENT.

378
CHAPTER 20
•
SEMANTIC ROLE LABELING
The PropBank semantic roles can be useful in recovering shallow semantic in-
formation about verbal arguments. Consider the verb increase:
(20.13) increase.01 “go up incrementally”
Arg0: causer of increase
Arg1: thing increasing
Arg2: amount increased by, EXT, or MNR
Arg3: start point
Arg4: end point
A PropBank semantic role labeling would allow us to infer the commonality in
the event structures of the following three examples, that is, that in each case Big
Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing
surface forms.
(20.14) [Arg0 Big Fruit Co. ] increased [Arg1 the price of bananas].
(20.15) [Arg1 The price of bananas] was increased again [Arg0 by Big Fruit Co. ]
(20.16) [Arg1 The price of bananas] increased [Arg2 5%].
PropBank also has a number of non-numbered arguments called ArgMs, (ArgM-
TMP, ArgM-LOC, etc.) which represent modiﬁcation or adjunct meanings. These
are relatively stable across predicates, so aren’t listed with each frame ﬁle. Data
labeled with these modiﬁers can be helpful in training systems to detect temporal,
location, or directional modiﬁcation across predicates. Some of the ArgM’s include:
TMP
when?
yesterday evening, now
LOC
where?
at the museum, in San Francisco
DIR
where to/from?
down, to Bangkok
MNR
how?
clearly, with much enthusiasm
PRP/CAU why?
because ... , in response to the ruling
REC
themselves, each other
ADV
miscellaneous
PRD
secondary predication
...ate the meat raw
While PropBank focuses on verbs, a related project, NomBank (Meyers et al.,
NomBank
2004) adds annotations to noun predicates. For example the noun agreement in
Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as
the Arg2. This allows semantic role labelers to assign labels to arguments of both
verbal and nominal predicates.
20.5
FrameNet
While making inferences about the semantic commonalities across different sen-
tences with increase is useful, it would be even more useful if we could make such
inferences in many more situations, across different verbs, and also between verbs
and nouns. For example, we’d like to extract the similarity among these three sen-
tences:
(20.17) [Arg1 The price of bananas] increased [Arg2 5%].
(20.18) [Arg1 The price of bananas] rose [Arg2 5%].
(20.19) There has been a [Arg2 5%] rise [Arg1 in the price of bananas].
Note that the second example uses the different verb rise, and the third example
uses the noun rather than the verb rise. We’d like a system to recognize that the

20.5
•
FRAMENET
379
price of bananas is what went up, and that 5% is the amount it went up, no matter
whether the 5% appears as the object of the verb increased or as a nominal modiﬁer
of the noun rise.
The FrameNet project is another semantic-role-labeling project that attempts
FrameNet
to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,
Fillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank
project are speciﬁc to an individual verb, roles in the FrameNet project are speciﬁc
to a frame.
What is a frame? Consider the following set of words:
reservation, ﬂight, travel, buy, price, cost, fare, rates, meal, plane
There are many individual lexical relations of hyponymy, synonymy, and so on
between many of the words in this list. The resulting set of relations does not,
however, add up to a complete account of how these words are related. They are
clearly all deﬁned with respect to a coherent chunk of common-sense background
information concerning air travel.
We call the holistic background knowledge that unites these words a frame (Fill-
frame
more, 1985). The idea that groups of words are deﬁned with respect to some back-
ground information is widespread in artiﬁcial intelligence and cognitive science,
where besides frame we see related works like a model (Johnson-Laird, 1983), or
model
even script (Schank and Abelson, 1977).
script
A frame in FrameNet is a background knowledge structure that deﬁnes a set of
frame-speciﬁc semantic roles, called frame elements, and includes a set of predi-
frame elements
cates that use these roles. Each word evokes a frame and proﬁles some aspect of the
frame and its elements. The FrameNet dataset includes a set of frames and frame
elements, the lexical units associated with each frame, and a set of labeled exam-
ple sentences. For example, the change position on a scale frame is deﬁned as
follows:
This frame consists of words that indicate the change of an Item’s posi-
tion on a scale (the Attribute) from a starting point (Initial value) to an
end point (Final value).
Some of the semantic roles (frame elements) in the frame are deﬁned as in
Fig. 20.3. Note that these are separated into core roles, which are frame speciﬁc, and
core roles
non-core roles, which are more like the Arg-M arguments in PropBank, expressing
non-core roles
more general properties of time, location, and so on.
Here are some example sentences:
(20.20) [ITEM Oil] rose [ATTRIBUTE in price] [DIFFERENCE by 2%].
(20.21) [ITEM It] has increased [FINAL STATE to having them 1 day a month].
(20.22) [ITEM Microsoft shares] fell [FINAL VALUE to 7 5/8].
(20.23) [ITEM Colon cancer incidence] fell [DIFFERENCE by 50%] [GROUP among
men].
(20.24) a steady increase [INITIAL VALUE from 9.5] [FINAL VALUE to 14.3] [ITEM
in dividends]
(20.25) a [DIFFERENCE 5%] [ITEM dividend] increase...
Note from these example sentences that the frame includes target words like rise,
fall, and increase. In fact, the complete frame consists of the following words:

380
CHAPTER 20
•
SEMANTIC ROLE LABELING
Core Roles
ATTRIBUTE
The ATTRIBUTE is a scalar property that the ITEM possesses.
DIFFERENCE
The distance by which an ITEM changes its position on the scale.
FINAL STATE
A description that presents the ITEM’s state after the change in the ATTRIBUTE’s
value as an independent predication.
FINAL VALUE
The position on the scale where the ITEM ends up.
INITIAL STATE
A description that presents the ITEM’s state before the change in the AT-
TRIBUTE’s value as an independent predication.
INITIAL VALUE The initial position on the scale from which the ITEM moves away.
ITEM
The entity that has a position on the scale.
VALUE RANGE
A portion of the scale, typically identiﬁed by its end points, along which the
values of the ATTRIBUTE ﬂuctuate.
Some Non-Core Roles
DURATION
The length of time over which the change takes place.
SPEED
The rate of change of the VALUE.
GROUP
The GROUP in which an ITEM changes the value of an
ATTRIBUTE in a speciﬁed way.
Figure 20.3
The frame elements in the change position on a scale frame from the FrameNet Labelers
Guide (Ruppenhofer et al., 2016).
VERBS: dwindle
move
soar
escalation
shift
advance
edge
mushroom swell
explosion
tumble
climb
explode
plummet
swing
fall
decline
fall
reach
triple
ﬂuctuation ADVERBS:
decrease
ﬂuctuate rise
tumble
gain
increasingly
diminish
gain
rocket
growth
dip
grow
shift
NOUNS: hike
double
increase
skyrocket
decline
increase
drop
jump
slide
decrease
rise
FrameNet also codes relationships between frames, allowing frames to inherit
from each other, or representing relations between frames like causation (and gen-
eralizations among frame elements in different frames can be representing by inher-
itance as well). Thus, there is a Cause change of position on a scale frame that is
linked to the Change of position on a scale frame by the cause relation, but that
adds an AGENT role and is used for causative examples such as the following:
(20.26) [AGENT They] raised [ITEM the price of their soda] [DIFFERENCE by 2%].
Together, these two frames would allow an understanding system to extract the
common event semantics of all the verbal and nominal causative and non-causative
usages.
FrameNets have also been developed for many other languages including Span-
ish, German, Japanese, Portuguese, Italian, and Chinese.
20.6
Semantic Role Labeling
Semantic role labeling (sometimes shortened as SRL) is the task of automatically
semantic role
labeling
ﬁnding the semantic roles of each argument of each predicate in a sentence. Cur-
rent approaches to semantic role labeling are based on supervised machine learning,
often using the FrameNet and PropBank resources to specify what counts as a pred-
icate, deﬁne the set of roles used in the task, and provide training and test sets.

20.6
•
SEMANTIC ROLE LABELING
381
Recall that the difference between these two models of semantic roles is that
FrameNet (20.27) employs many frame-speciﬁc frame elements as roles, while Prop-
Bank (20.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speciﬁc labels, along with the more general ARGM labels. Some
examples:
(20.27)
[You]
can’t [blame]
[the program] [for being unable to identify it]
COGNIZER
TARGET
EVALUEE
REASON
(20.28)
[The San Francisco Examiner] issued
[a special edition] [yesterday]
ARG0
TARGET
ARG1
ARGM-TMP
20.6.1
A Feature-based Algorithm for Semantic Role Labeling
A simpliﬁed feature-based semantic role labeling algorithm is sketched in Fig. 20.4.
Feature-based algorithms—from the very earliest systems like (Simmons, 1973)—
begin by parsing, using broad-coverage parsers to assign a parse to the input string.
Figure 20.5 shows a parse of (20.28) above. The parse is then traversed to ﬁnd all
words that are predicates.
For each of these predicates, the algorithm examines each node in the parse
tree and uses supervised classiﬁcation to decide the semantic role (if any) it plays
for this predicate. Given a labeled training set such as PropBank or FrameNet, a
feature vector is extracted for each node, using feature templates described in the
next subsection. A 1-of-N classiﬁer is then trained to predict a semantic role for
each constituent given these features, where N is the number of potential semantic
roles plus an extra NONE role for non-role constituents. Any standard classiﬁcation
algorithms can be used. Finally, for each test sentence to be labeled, the classiﬁer is
run on each relevant constituent.
function SEMANTICROLELABEL(words) returns labeled tree
parse←PARSE(words)
for each predicate in parse do
for each node in parse do
featurevector←EXTRACTFEATURES(node, predicate, parse)
CLASSIFYNODE(node, featurevector, parse)
Figure 20.4
A generic semantic-role-labeling algorithm. CLASSIFYNODE is a 1-of-N clas-
siﬁer that assigns a semantic role (or NONE for non-role constituents), trained on labeled data
such as FrameNet or PropBank.
Instead of training a single-stage classiﬁer as in Fig. 20.5, the node-level classi-
ﬁcation task can be broken down into multiple steps:
1. Pruning: Since only a small number of the constituents in a sentence are
arguments of any given predicate, many systems use simple heuristics to prune
unlikely constituents.
2. Identiﬁcation: a binary classiﬁcation of each node as an argument to be la-
beled or a NONE.
3. Classiﬁcation: a 1-of-N classiﬁcation of all the constituents that were labeled
as arguments by the previous stage
The separation of identiﬁcation and classiﬁcation may lead to better use of fea-
tures (different features may be useful for the two tasks) or to computational efﬁ-
ciency.

382
CHAPTER 20
•
SEMANTIC ROLE LABELING
S
NP-SBJ = ARG0
VP
DT
NNP
NNP
NNP
The
San
Francisco
Examiner
VBD = TARGET
NP = ARG1
PP-TMP = ARGM-TMP
issued
DT
JJ
NN
IN
NP
a
special
edition
around
NN
NP-TMP
noon
yesterday
Figure 20.5
Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line
shows the path feature NP↑S↓VP↓VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner.
Global Optimization
The classiﬁcation algorithm of Fig. 20.5 classiﬁes each argument separately (‘lo-
cally’), making the simplifying assumption that each argument of a predicate can be
labeled independently. This assumption is false; there are interactions between argu-
ments that require a more ‘global’ assignment of labels to constituents. For example,
constituents in FrameNet and PropBank are required to be non-overlapping. More
signiﬁcantly, the semantic roles of constituents are not independent. For example
PropBank does not allow multiple identical arguments; two constituents of the same
verb cannot both be labeled ARG0 .
Role labeling systems thus often add a fourth step to deal with global consistency
across the labels in a sentence. For example, the local classiﬁers can return a list of
possible labels associated with probabilities for each constituent, and a second-pass
Viterbi decoding or re-ranking approach can be used to choose the best consensus
label. Integer linear programming (ILP) is another common way to choose a solution
that conforms best to multiple constraints.
Features for Semantic Role Labeling
Most systems use some generalization of the core set of features introduced by
Gildea and Jurafsky (2000). Common basic features templates (demonstrated on
the NP-SBJ constituent The San Francisco Examiner in Fig. 20.5) include:
• The governing predicate, in this case the verb issued. The predicate is a cru-
cial feature since labels are deﬁned only with respect to a particular predicate.
• The phrase type of the constituent, in this case, NP (or NP-SBJ). Some se-
mantic roles tend to appear as NPs, others as S or PP, and so on.
• The headword of the constituent, Examiner. The headword of a constituent
can be computed with standard head rules, such as those given in Chapter 12
in Fig. 12.12. Certain headwords (e.g., pronouns) place strong constraints on
the possible semantic roles they are likely to ﬁll.
• The headword part of speech of the constituent, NNP.
• The path in the parse tree from the constituent to the predicate. This path is
marked by the dotted line in Fig. 20.5. Following Gildea and Jurafsky (2000),
we can use a simple linear representation of the path, NP↑S↓VP↓VBD. ↑and
↓represent upward and downward movement in the tree, respectively. The

20.6
•
SEMANTIC ROLE LABELING
383
path is very useful as a compact representation of many kinds of grammatical
function relationships between the constituent and the predicate.
• The voice of the clause in which the constituent appears, in this case, active
(as contrasted with passive). Passive sentences tend to have strongly different
linkings of semantic roles to surface form than do active ones.
• The binary linear position of the constituent with respect to the predicate,
either before or after.
• The subcategorization of the predicate, the set of expected arguments that
appear in the verb phrase. We can extract this information by using the phrase-
structure rule that expands the immediate parent of the predicate; VP →VBD
NP PP for the predicate in Fig. 20.5.
• The named entity type of the constituent.
• The ﬁrst words and the last word of the constituent.
The following feature vector thus represents the ﬁrst NP in our example (recall
that most observations will have the value NONE rather than, for example, ARG0,
since most constituents in the parse tree will not bear a semantic role):
ARG0: [issued, NP, Examiner, NNP, NP↑S↓VP↓VBD, active, before, VP →NP PP,
ORG, The, Examiner]
Other features are often used in addition, such as sets of n-grams inside the
constituent, or more complex versions of the path features (the upward or downward
halves, or whether particular nodes occur in the path).
It’s also possible to use dependency parses instead of constituency parses as the
basis of features, for example using dependency parse paths instead of constituency
paths.
20.6.2
A Neural Algorithm for Semantic Role Labeling
The standard neural algorithm for semantic role labeling is based on the bi-LSTM
IOB tagger introduced in Chapter 9, which we’ve seen applied to part-of-speech
tagging and named entity tagging, among other tasks. Recall that with IOB tagging,
we have a begin and end tag for each possible role (B-ARG0, I-ARG0; B-ARG1,
I-ARG1, and so on), plus an outside tag O.
As with all the taggers, the goal is to compute the highest probability tag se-
quence ˆy, given the input sequence of words w:
ˆy = argmax
y∈T
P(y|w)
In algorithms like He et al. (2017), each input word is mapped to pre-trained em-
beddings, and also associated with an embedding for a ﬂag (0/1) variable indicating
whether that input word is the predicate. These concatenated embeddings are passed
through multiple layers of bi-directional LSTM. State-of-the-art algorithms tend to
be deeper than for POS or NER tagging, using 3 to 4 layers (6 to 8 total LSTMs).
Highway layers can be used to connect these layers as well.
Output from the last bi-LSTM can then be turned into an IOB sequence as for
POS or NER tagging. Tags can be locally optimized by taking the bi-LSTM output,
passing it through a single layer into a softmax for each word that creates a proba-
bility distribution over all SRL tags and the most likely tag for word xi is chosen as
ti, computing for each word essentially:
ˆyi = argmax
t∈tags
P(t|wi)

384
CHAPTER 20
•
SEMANTIC ROLE LABELING
The
cats
love
hats
Embeddings
LSTM1
LSTM1
LSTM1
LSTM1
LSTM2
LSTM2
LSTM2
LSTM2
Concatenation
Right-to-left LSTM
Left-to-right LSTM
Softmax
P(B-ARG0)
P(I-ARG0)
P(B-PRED)
P(B-ARG1)
0
0
1
0
word + is-predicate
Figure 20.6
A bi-LSTM approach to semantic role labeling. Most actual networks are
much deeper than shown in this ﬁgure; 3 to 4 bi-LSTM layers (6 to 8 total LSTMs) are
common. The input is a concatenation of an embedding for the input word and an embedding
of a binary variable which is 1 for the predicate to 0 for all other words. After He et al. (2017).
However, just as feature-based SRL tagging, this local approach to decoding doesn’t
exploit the global constraints between tags; a tag I-ARG0, for example, must follow
another I-ARG0 or B-ARG0.
As we saw for POS and NER tagging, there are many ways to take advantage of
these global constraints. A CRF layer can be used instead of a softmax layer on top
of the bi-LSTM output, and the Viterbi decoding algorithm can be used to decode
from the CRF.
An even simpler Viterbi decoding algorithm that may perform equally well and
doesn’t require adding CRF complexity to the training process is to start with the
simple softmax. The softmax output (the entire probability distribution over tags)
for each word is then treated as a lattice and we can do Viterbi decoding through the
lattice. The hard IOB constraints can act as the transition probabilities in the Viterbi
decoding (thus the transition from state I-ARG0 to I-ARG1 would have probabil-
ity 0). Alternatively, the training data can be used to learn bigram or trigram tag
transition probabilities as if doing HMM decoding. Fig. 20.6 shows a sketch of the
algorithm.
20.6.3
Evaluation of Semantic Role Labeling
The standard evaluation for semantic role labeling is to require that each argument
label must be assigned to the exactly correct word sequence or parse constituent, and
then compute precision, recall, and F-measure. Identiﬁcation and classiﬁcation can
also be evaluated separately. Two common datasets used for evaluation are CoNLL-
2005 (Carreras and M`arquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).
20.7
Selectional Restrictions
We turn in this section to another way to represent facts about the relationship be-
tween predicates and arguments. A selectional restriction is a semantic type con-
selectional
restriction
straint that a verb imposes on the kind of concepts that are allowed to ﬁll its argument
roles. Consider the two meanings associated with the following example:

20.7
•
SELECTIONAL RESTRICTIONS
385
(20.29) I want to eat someplace nearby.
There are two possible parses and semantic interpretations for this sentence. In
the sensible interpretation, eat is intransitive and the phrase someplace nearby is
an adjunct that gives the location of the eating event. In the nonsensical speaker-as-
Godzilla interpretation, eat is transitive and the phrase someplace nearby is the direct
object and the THEME of the eating, like the NP Malaysian food in the following
sentences:
(20.30)
I want to eat Malaysian food.
How do we know that someplace nearby isn’t the direct object in this sentence?
One useful cue is the semantic fact that the THEME of EATING events tends to be
something that is edible. This restriction placed by the verb eat on the ﬁller of its
THEME argument is a selectional restriction.
Selectional restrictions are associated with senses, not entire lexemes. We can
see this in the following examples of the lexeme serve:
(20.31)
The restaurant serves green-lipped mussels.
(20.32)
Which airlines serve Denver?
Example (20.31) illustrates the offering-food sense of serve, which ordinarily re-
stricts its THEME to be some kind of food Example (20.32) illustrates the provides a
commercial service to sense of serve, which constrains its THEME to be some type
of appropriate location.
Selectional restrictions vary widely in their speciﬁcity. The verb imagine, for
example, imposes strict requirements on its AGENT role (restricting it to humans
and other animate entities) but places very few semantic requirements on its THEME
role. A verb like diagonalize, on the other hand, places a very speciﬁc constraint
on the ﬁller of its THEME role: it has to be a matrix, while the arguments of the
adjectives odorless are restricted to concepts that could possess an odor:
(20.33) In rehearsal, I often ask the musicians to imagine a tennis game.
(20.34) Radon is an odorless gas that can’t be detected by human senses.
(20.35) To diagonalize a matrix is to ﬁnd its eigenvalues.
These examples illustrate that the set of concepts we need to represent selectional
restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.
This distinguishes selectional restrictions from other features for representing lexical
knowledge, like parts-of-speech, which are quite limited in number.
20.7.1
Representing Selectional Restrictions
One way to capture the semantics of selectional restrictions is to use and extend the
event representation of Chapter 16. Recall that the neo-Davidsonian representation
of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. Ignoring the issue of
the λ-structures and using thematic roles rather than deep event roles, the semantic
contribution of a verb like eat might look like the following:
∃e,x,y Eating(e)∧Agent(e,x)∧Theme(e,y)
With this representation, all we know about y, the ﬁller of the THEME role, is that
it is associated with an Eating event through the Theme relation. To stipulate the

386
CHAPTER 20
•
SEMANTIC ROLE LABELING
Sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich
=> snack food
=> dish
=> nutriment, nourishment, nutrition...
=> food, nutrient
=> substance
=> matter
=> physical entity
=> entity
Figure 20.7
Evidence from WordNet that hamburgers are edible.
selectional restriction that y must be something edible, we simply add a new term to
that effect:
∃e,x,y Eating(e)∧Agent(e,x)∧Theme(e,y)∧EdibleThing(y)
When a phrase like ate a hamburger is encountered, a semantic analyzer can
form the following kind of representation:
∃e,x,y Eating(e)∧Eater(e,x)∧Theme(e,y)∧EdibleThing(y)∧Hamburger(y)
This representation is perfectly reasonable since the membership of y in the category
Hamburger is consistent with its membership in the category EdibleThing, assuming
a reasonable set of facts in the knowledge base. Correspondingly, the representation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as Takeoff would be inconsistent with membership in the
category EdibleThing.
While this approach adequately captures the semantics of selectional restrictions,
there are two problems with its direct use. First, using FOL to perform the simple
task of enforcing selectional restrictions is overkill. Other, far simpler, formalisms
can do the job with far less computational cost. The second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. Unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessary to the task.
A more practical approach is to state selectional restrictions in terms of WordNet
synsets rather than as logical concepts. Each predicate simply speciﬁes a WordNet
synset as the selectional restriction on each of its arguments. A meaning representa-
tion is well-formed if the role ﬁller word is a hyponym (subordinate) of this synset.
For our ate a hamburger example, for instance, we could set the selectional
restriction on the THEME role of the verb eat to the synset {food, nutrient}, glossed
as any substance that can be metabolized by an animal to give energy and build
tissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 20.7 reveals
that hamburgers are indeed food. Again, the ﬁller of a role need not match the
restriction synset exactly; it just needs to have the synset as one of its superordinates.
We can apply this approach to the THEME roles of the verbs imagine, lift, and di-
agonalize, discussed earlier. Let us restrict imagine’s THEME to the synset {entity},
lift’s THEME to {physical entity}, and diagonalize to {matrix}. This arrangement

20.7
•
SELECTIONAL RESTRICTIONS
387
correctly permits imagine a hamburger and lift a hamburger, while also correctly
ruling out diagonalize a hamburger.
20.7.2
Selectional Preferences
In the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (Katz and Fodor 1963,
Hirst 1987). For example, the verb eat might require that its THEME argument be
[+FOOD]. Early word sense disambiguation systems used this idea to rule out senses
that violated the selectional restrictions of their governing predicates.
Very quickly, however, it became clear that these selectional restrictions were
better represented as preferences rather than strict constraints (Wilks 1975c, Wilks 1975b).
For example, selectional restriction violations (like inedible arguments of eat) often
occur in well-formed sentences, for example because they are negated (20.36), or
because selectional restrictions are overstated (20.37):
(20.36) But it fell apart in 1931, perhaps because people realized you can’t eat
gold for lunch if you’re hungry.
(20.37) In his two championship trials, Mr. Kulkarni ate glass on an empty
stomach, accompanied only by water and tea.
Modern systems for selectional preferences therefore specify the relation be-
tween a predicate and its possible arguments with soft constraints of some kind.
Selectional Association
One of the most inﬂuential has been the selectional association model of Resnik
(1993). Resnik deﬁnes the idea of selectional preference strength as the general
selectional
preference
strength
amount of information that a predicate tells us about the semantic class of its argu-
ments. For example, the verb eat tells us a lot about the semantic class of its direct
objects, since they tend to be edible. The verb be, by contrast, tells us less about
its direct objects. The selectional preference strength can be deﬁned by the differ-
ence in information between two distributions: the distribution of expected semantic
classes P(c) (how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb P(c|v) (how likely is
it that the direct object of the speciﬁc verb v will fall into semantic class c). The
greater the difference between these distributions, the more information the verb
is giving us about possible objects. The difference between these two distributions
can be quantiﬁed by relative entropy, or the Kullback-Leibler divergence (Kullback
relative entropy
and Leibler, 1951). The Kullback-Leibler or KL divergence D(P||Q) expresses the
KL divergence
difference between two probability distributions P and Q
D(P||Q) =
X
x
P(x)log P(x)
Q(x)
(20.38)
The selectional preference SR(v) uses the KL divergence to express how much in-
formation, in bits, the verb v expresses about the possible semantic class of its argu-
ment.
SR(v) = D(P(c|v)||P(c))
=
X
c
P(c|v)log P(c|v)
P(c)
(20.39)

388
CHAPTER 20
•
SEMANTIC ROLE LABELING
Resnik then deﬁnes the selectional association of a particular class and verb as the
selectional
association
relative contribution of that class to the general selectional preference of the verb:
AR(v,c) =
1
SR(v)P(c|v)log P(c|v)
P(c)
(20.40)
The selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
Resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the WordNet concepts containing the
word. The following table from Resnik (1996) shows some sample high and low
selectional associations for verbs and some WordNet semantic classes of their direct
objects.
Direct Object
Direct Object
Verb
Semantic Class Assoc
Semantic Class Assoc
read
WRITING
6.80
ACTIVITY
-.20
write
WRITING
7.26
COMMERCE
0
see
ENTITY
5.79
METHOD
-0.01
Selectional Preference via Conditional Probability
An alternative to using selectional association between a verb and the WordNet class
of its arguments is to use the conditional probability of an argument word given a
predicate verb, directly modeling the strength of association of one verb (predicate)
with one noun (argument).
The conditional probability model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. The conditional probability of an
argument noun given a verb for a particular relation P(n|v,r) can then be used as a
selectional preference metric for that pair of words (Brockmann and Lapata 2003,
Keller and Lapata 2003):
P(n|v,r) =
(
C(n,v,r)
C(v,r)
if C(n,v,r) > 0
0 otherwise
The inverse probability P(v|n,r) was found to have better performance in some cases
(Brockmann and Lapata, 2003):
P(v|n,r) =
(
C(n,v,r)
C(n,r)
if C(n,v,r) > 0
0 otherwise
An even simpler approach is to use the simple log co-occurrence frequency of
the predicate with the argument logcount(v,n,r) instead of conditional probability;
this seems to do better for extracting preferences for syntactic subjects rather than
objects (Brockmann and Lapata, 2003).
Evaluating Selectional Preferences
One way to evaluate models of selectional preferences is to use pseudowords (Gale
pseudowords
et al. 1992c, Sch¨utze 1992a). A pseudoword is an artiﬁcial word created by concate-
nating a test word in some context (say banana) with a confounder word (say door)

20.8
•
PRIMITIVE DECOMPOSITION OF PREDICATES
389
to create banana-door). The task of the system is to identify which of the two words
is the original word. To evaluate a selectional preference model (for example on the
relationship between a verb and a direct object) we take a test corpus and select all
verb tokens. For each verb token (say drive) we select the direct object (e.g., car),
concatenated with a confounder word that is its nearest neighbor, the noun with the
frequency closest to the original (say house), to make car/house). We then use the
selectional preference model to choose which of car and house are more preferred
objects of drive, and compute how often the model chooses the correct original ob-
ject (e.g., car) (Chambers and Jurafsky, 2010).
Another evaluation metric is to get human preferences for a test set of verb-
argument pairs, and have them rate their degree of plausibility. This is usually done
by using magnitude estimation, a technique from psychophysics, in which subjects
rate the plausibility of an argument proportional to a modulus item. A selectional
preference model can then be evaluated by its correlation with the human prefer-
ences (Keller and Lapata, 2003).
20.8
Primitive Decomposition of Predicates
One way of thinking about the semantic roles we have discussed through the chapter
is that they help us deﬁne the roles that arguments play in a decompositional way,
based on ﬁnite lists of thematic roles (agent, patient, instrument, proto-agent, proto-
patient, etc.). This idea of decomposing meaning into sets of primitive semantics
elements or features, called primitive decomposition or componential analysis,
componential
analysis
has been taken even further, and focused particularly on predicates.
Consider these examples of the verb kill:
(20.41) Jim killed his philodendron.
(20.42) Jim did something to cause his philodendron to become not alive.
There is a truth-conditional (‘propositional semantics’) perspective from which these
two sentences have the same meaning. Assuming this equivalence, we could repre-
sent the meaning of kill as:
(20.43) KILL(x,y) ⇔CAUSE(x, BECOME(NOT(ALIVE(y))))
thus using semantic primitives like do, cause, become not, and alive.
Indeed, one such set of potential semantic primitives has been used to account for
some of the verbal alternations discussed in Section 20.2 (Lakoff 1965, Dowty 1979).
Consider the following examples.
(20.44) John opened the door. ⇒CAUSE(John, BECOME(OPEN(door)))
(20.45) The door opened. ⇒BECOME(OPEN(door))
(20.46) The door is open. ⇒OPEN(door)
The decompositional approach asserts that a single state-like predicate associ-
ated with open underlies all of these examples. The differences among the meanings
of these examples arises from the combination of this single predicate with the prim-
itives CAUSE and BECOME.
While this approach to primitive decomposition can explain the similarity be-
tween states and actions or causative and non-causative predicates, it still relies on
having a large number of predicates like open. More radical approaches choose to

390
CHAPTER 20
•
SEMANTIC ROLE LABELING
break down these predicates as well. One such approach to verbal predicate de-
composition that played a role in early natural language understanding systems is
conceptual dependency (CD), a set of ten primitive predicates, shown in Fig. 20.8.
conceptual
dependency
Primitive
Deﬁnition
ATRANS
The abstract transfer of possession or control from one entity to
another
PTRANS
The physical transfer of an object from one location to another
MTRANS
The transfer of mental concepts between entities or within an
entity
MBUILD
The creation of new information within an entity
PROPEL
The application of physical force to move an object
MOVE
The integral movement of a body part by an animal
INGEST
The taking in of a substance by an animal
EXPEL
The expulsion of something from an animal
SPEAK
The action of producing a sound
ATTEND
The action of focusing a sense organ
Figure 20.8
A set of conceptual dependency primitives.
Below is an example sentence along with its CD representation. The verb brought
is translated into the two primitives ATRANS and PTRANS to indicate that the waiter
both physically conveyed the check to Mary and passed control of it to her. Note
that CD also associates a ﬁxed set of thematic roles with each primitive to represent
the various participants in the action.
(20.47) The waiter brought Mary the check.
∃x,y Atrans(x)∧Actor(x,Waiter)∧Object(x,Check)∧To(x,Mary)
∧Ptrans(y)∧Actor(y,Waiter)∧Object(y,Check)∧To(y,Mary)
20.9
Summary
• Semantic roles are abstract models of the role an argument plays in the event
described by the predicate.
• Thematic roles are a model of semantic roles based on a single ﬁnite list of
roles. Other semantic role models include per-verb semantic role lists and
proto-agent/proto-patient, both of which are implemented in PropBank,
and per-frame role lists, implemented in FrameNet.
• Semantic role labeling is the task of assigning semantic role labels to the
constituents of a sentence. The task is generally treated as a supervised ma-
chine learning task, with models trained on PropBank or FrameNet. Algo-
rithms generally start by parsing a sentence and then automatically tag each
parse tree node with a semantic role. Neural models map straight from words
end-to-end.
• Semantic selectional restrictions allow words (particularly predicates) to post
constraints on the semantic properties of their argument words. Selectional

BIBLIOGRAPHICAL AND HISTORICAL NOTES
391
preference models (like selectional association or simple conditional proba-
bility) allow a weight or probability to be assigned to the association between
a predicate and an argument word or class.
Bibliographical and Historical Notes
Although the idea of semantic roles dates back to P¯an.ini, they were re-introduced
into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968).
Fillmore, interestingly, had become interested in argument structure by studying
Lucien Tesni`ere’s groundbreaking ´El´ements de Syntaxe Structurale (Tesni`ere, 1959)
in which the term ‘dependency’ was introduced and the foundations were laid for
dependency grammar. Following Tesni`ere’s terminology, Fillmore ﬁrst referred to
argument roles as actants (Fillmore, 1966) but quickly switched to the term case,
(see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent,
Patient, Instrument, etc.), that could be taken on by the arguments of predicates.
Verbs would be listed in the lexicon with their case frame, the list of obligatory (or
optional) case arguments.
The idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper, more
fully-speciﬁed representations of meaning was quickly adopted in natural language
processing, and systems for extracting case frames were created for machine trans-
lation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language
understanding (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977).
General-purpose semantic role labelers were developed. The earliest ones (Sim-
mons, 1973) ﬁrst parsed a sentence by means of an ATN (Augmented Transition
Network) parser. Each verb then had a set of rules specifying how the parse should
be mapped to semantic roles. These rules mainly made reference to grammatical
functions (subject, object, complement of speciﬁc prepositions) but also checked
constituent internal features such as the animacy of head nouns. Later systems as-
signed roles from pre-built parse trees, again by using dictionaries with verb-speciﬁc
case frames (Levin 1977, Marcus 1980).
By 1977 case representation was widely used and taught in AI and NLP courses,
and was described as a standard of natural language understanding in the ﬁrst edition
of Winston’s (1977) textbook Artiﬁcial Intelligence.
In the 1980s Fillmore proposed his model of frame semantics, later describing
the intuition as follows:
“The idea behind frame semantics is that speakers are aware of possi-
bly quite complex situation types, packages of connected expectations,
that go by various names—frames, schemas, scenarios, scripts, cultural
narratives, memes—and the words in our language are understood with
such frames as their presupposed background.” (Fillmore, 2012, p. 712)
The word frame seemed to be in the air for a suite of related notions proposed at
about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as
well as related notions with other names like scripts (Schank and Abelson, 1975)
and schemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).
Fillmore was also inﬂuenced by the semantic ﬁeld theorists and by a visit to the Yale
AI lab where he took notice of the lists of slots and ﬁllers used by early information

392
CHAPTER 20
•
SEMANTIC ROLE LABELING
extraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s
Fillmore drew on these insights to begin the FrameNet corpus annotation project.
At the same time, Beth Levin drew on her early case frame dictionaries (Levin,
1977) to develop her book which summarized sets of verb classes deﬁned by shared
argument realizations (Levin, 1993). The VerbNet project built on this work (Kipper
et al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus
created by Martha Palmer and colleagues (Palmer et al., 2005).
The combination of rich linguistic annotation and corpus-based approach in-
stantiated in FrameNet and PropBank led to a revival of automatic approaches to
semantic role labeling, ﬁrst on FrameNet (Gildea and Jurafsky, 2000) and then on
PropBank data (Gildea and Palmer, 2002, inter alia). The problem ﬁrst addressed in
the 1970s by handwritten rules was thus now generally recast as one of supervised
machine learning enabled by large and consistent databases. Many popular features
used for role labeling are deﬁned in Gildea and Jurafsky (2002), Surdeanu et al.
(2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao
et al. (2009). The use of dependency rather than constituency parses was introduced
in the CoNLL-2008 shared task (Surdeanu et al., 2008b). For surveys see Palmer
et al. (2010) and M`arquez et al. (2008).
The use of neural approaches to semantic role labeling was pioneered by Col-
lobert et al. (2011), who applied a CRF on top of a convolutional net. Early work
like Foland, Jr. and Martin (2015) focused on using dependency features. Later
work eschewed syntactic features altogether; (Zhou and Xu, 2015b) introduced the
use of a stacked (6-8 layer) bi-LSTM architecture, and (He et al., 2017) showed
how to augment the bi-LSTM architecture with highway networks and also replace
the CRF with A* decoding that make it possible to apply a wide variety of global
constraints in SRL decoding.
Most semantic role labeling schemes only work within a single sentence, fo-
cusing on the object of the verbal (or nominal, in the case of NomBank) predicate.
However, in many cases, a verbal or nominal predicate may have an implicit argu-
ment: one that appears only in a contextual sentence, or perhaps not at all and must
implicit
argument
be inferred. In the two sentences This house has a new owner. The sale was ﬁnalized
10 days ago. the sale in the second sentence has no ARG1, but a reasonable reader
would infer that the Arg1 should be the house mentioned in the prior sentence. Find-
ing these arguments, implicit argument detection (sometimes shortened as iSRL)
iSRL
was introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do
et al. (2017) for more recent neural models.
To avoid the need for huge labeled training sets, unsupervised approaches for
semantic role labeling attempt to induce the set of semantic roles by clustering over
arguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier
and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev
(2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khod-
dam (2014).
Recent innovations in frame labeling include connotation frames, which mark
richer information about the argument of predicates. Connotation frames mark the
sentiment of the writer or reader toward the arguments (for example using the verb
survive in he survived a bombing expresses the writer’s sympathy toward the subject
he and negative sentiment toward the bombing. Connotation frames also mark effect
(something bad happened to x), value: (x is valuable), and mental state: (x is dis-
tressed by the event) (Rashkin et al. 2016, Rashkin et al. 2017). Connotation frames
can also mark the power differential between the arguments (using the verb implore

EXERCISES
393
means that the theme argument has greater power than the agent), and the agency
of each argument (waited is low agency). Fig. 20.9 shows a visualization from Sap
et al. (2017).
AGENT
THEME
power(AG < TH)
VERB
implore
He implored the tribunal to show mercy.
The princess waited for her prince.
AGENT
THEME
agency(AG) = -
VERB
wait
Figure 20.9
The connotation frames of Sap et al. (2017), showing that the verb implore
implies the agent has lower power than the theme (in contrast, say, with a verb like demanded),
and showing the low level of agency of the subject of waited. Figure from Sap et al. (2017).
Selectional preference has been widely studied beyond the selectional associa-
tion models of Resnik (1993) and Resnik (1996). Methods have included cluster-
ing (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic
models (S´eaghdha 2010, Ritter et al. 2010), and constraints can be expressed at the
level of words or classes (Agirre and Martinez, 2001). Selectional preferences have
also been successfully integrated into semantic role labeling (Erk 2007, Zapirain
et al. 2013, Do et al. 2017).
Exercises

394
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
CHAPTER
21
Lexicons for Sentiment, Affect,
and Connotation
“[W]e write, not with the ﬁngers, but with the whole person. The nerve which
controls the pen winds itself about every ﬁbre of our being, threads the heart,
pierces the liver.”
Virginia Woolf, Orlando
“She runs the gamut of emotions from A to B.”
Dorothy Parker, reviewing Hepburn’s performance in Little Women
“Festering’s always bad. There’s no good kind of festering.”
Adventure Time, Season 5
In this chapter we turn to tools for interpreting affective meaning, extending our
affective
study of sentiment analysis in Chapter 4. We use the word ‘affective’, following
the tradition in affective computing (Picard, 1995) to mean emotion, sentiment, per-
sonality, mood, and attitudes. Affective meaning is closely related to subjectivity,
subjectivity
the study of a speaker or writer’s evaluations, opinions, emotions, and speculations
(Wiebe et al., 1999).
How should affective meaning be deﬁned? One inﬂuential typology of affec-
tive states comes from Scherer (2000), who deﬁnes each class of affective states by
factors like its cognitive realization and time course:
We can design extractors for each of these kinds of affective states. Chapter 4
already introduced sentiment analysis, the task of extracting the positive or negative
orientation that a writer expresses in a text. This corresponds in Scherer’s typology
to the extraction of attitudes: ﬁguring out what people like or dislike, from affect-
rich texts like consumer reviews of books or movies, newspaper editorials, or public
sentiment in blogs or tweets.
Detecting emotion and moods is useful for detecting whether a student is con-
fused, engaged, or certain when interacting with a tutorial system, whether a caller
to a help line is frustrated, whether someone’s blog posts or tweets indicated depres-
sion. Detecting emotions like fear in novels, for example, could help us trace what
groups or situations are feared and how that changes over time.
Detecting different interpersonal stances can be useful when extracting infor-
mation from human-human conversations. The goal here is to detect stances like
friendliness or awkwardness in interviews or friendly conversations, for example for
summarizing meetings or ﬁnding parts of a conversation where people are especially
excited or engaged, conversational hot spots that can help in meeting summariza-
tion. Detecting the personality of a user—such as whether the user is an extrovert
or the extent to which they are open to experience— can help improve conversa-

21.1
•
DEFINING EMOTION
395
Emotion: Relatively brief episode of response to the evaluation of an external
or internal event as being of major signiﬁcance.
(angry, sad, joyful, fearful, ashamed, proud, elated, desperate)
Mood: Diffuse affect state, most pronounced as change in subjective feeling, of
low intensity but relatively long duration, often without apparent cause.
(cheerful, gloomy, irritable, listless, depressed, buoyant)
Interpersonal stance: Affective stance taken toward another person in a spe-
ciﬁc interaction, coloring the interpersonal exchange in that situation.
(distant, cold, warm, supportive, contemptuous, friendly)
Attitude: Relatively enduring, affectively colored beliefs, preferences, and pre-
dispositions towards objects or persons.
(liking, loving, hating, valuing, desiring)
Personality traits: Emotionally laden, stable personality dispositions and be-
havior tendencies, typical for a person.
(nervous, anxious, reckless, morose, hostile, jealous)
Figure 21.1
The Scherer typology of affective states (Scherer, 2000).
tional agents, which seem to work better if they match users’ personality expecta-
tions (Mairesse and Walker, 2008). And affect is important for generation as well
as recognition; synthesizing affect is important for conversational agents in various
domains, including literacy tutors such as children’s storybooks, or computer games.
In Chapter 4 we introduced the use of naive Bayes classiﬁcation to classify a
document’s sentiment. Various classiﬁers have been successfully applied to many of
these tasks, using all the words in the training set as input to a classiﬁer which then
determines the affect status of the text.
In this chapter we focus on an alternative model, in which instead of using every
word as a feature, we focus only on certain words, ones that carry particularly strong
cues to affect or sentiment. We call these lists of words affective lexicons or senti-
ment lexicons. These lexicons presuppose a fact about semantics: that words have
affective meanings or connotations. The word connotation has different meanings
connotations
in different ﬁelds, but here we use it to mean the aspects of a word’s meaning that
are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. In
addition to their ability to help determine the affective status of a text, connotation
lexicons can be useful features for other kinds of affective tasks, and for computa-
tional social science analysis.
In the next sections we introduce basic theories of emotion, show how sentiment
lexicons are a special case of emotion lexicons, and mention some useful lexicons.
We then survey three ways for building lexicons: human labeling, semi-supervised,
and supervised. Finally, we turn to other kinds of affective meaning like personality,
stance, and entity-centric affect, and introduce connotation frames.
21.1
Deﬁning Emotion
One of the most important affective classes is emotion, which Scherer (2000) deﬁnes
emotion
as a “relatively brief episode of response to the evaluation of an external or internal
event as being of major signiﬁcance”.
Detecting emotion has the potential to improve a number of language processing

396
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
tasks. Automatically detecting emotions in reviews or customer responses (anger,
dissatisfaction, trust) could help businesses recognize speciﬁc problem areas or ones
that are going well. Emotion recognition could help dialog systems like tutoring
systems detect that a student was unhappy, bored, hesitant, conﬁdent, and so on.
Emotion can play a role in medical informatics tasks like detecting depression or
suicidal intent. Detecting emotions expressed toward characters in novels might
play a role in understanding how different social groups were viewed by society at
different times.
There are two widely-held families of theories of emotion. In one family, emo-
tions are viewed as ﬁxed atomic units, limited in number, and from which others
are generated, often called basic emotions (Tomkins 1962, Plutchik 1962). Perhaps
basic emotions
most well-known of this family of theories are the 6 emotions proposed by Ekman
(see for example Ekman 1999) as a set of emotions that is likely to be universally
present in all cultures: surprise, happiness, anger, fear, disgust, sadness. Another
atomic theory is the Plutchik (1980) wheel of emotion, consisting of 8 basic emo-
tions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipa-
tion–surprise, together with the emotions derived from them, shown in Fig. 21.2.
Figure 21.2
Plutchik wheel of emotion.
The second class of emotion theories views emotion as a space in 2 or 3 di-
mensions (Russell, 1980). Most models include the two dimensions valence and
arousal, and many add a third, dominance. These can be deﬁned as:
valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus
In the next sections we’ll see lexicons for both kinds of theories of emotion.
Sentiment can be viewed as a special case of this second view of emotions as
points in space. In particular, the valence dimension, measuring how pleasant or
unpleasant a word is, is often used directly as a measure of sentiment.

21.2
•
AVAILABLE SENTIMENT AND AFFECT LEXICONS
397
21.2
Available Sentiment and Affect Lexicons
A wide variety of affect lexicons have been created and released. The most basic
lexicons label words along one dimension of semantic variability, generally called
“sentiment” or “valence”.
In the simplest lexicons this dimension is represented in a binary fashion, with
a wordlist for positive words and a wordlist for negative words. The oldest is the
General Inquirer (Stone et al., 1966), which drew on content analysis and on early
General
Inquirer
work in the cognitive psychology of word meaning (Osgood et al., 1957). The Gen-
eral Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative
words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon
(Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior
lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe,
2003) Each entry in the lexicon is hand-labeled for sentiment and also labeled for
reliability (strongly subjective or weakly subjective). The polarity lexicon of Hu
and Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product
reviews, labeled using a bootstrapping method from WordNet.
Positive
admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan-
tastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,
rejoice, relief, respect, satisfactorily, sensational, super, terriﬁc, thank, vivid, wise, won-
derful, zest
Negative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,
defective, disappointment, embarrass, fake, fear, ﬁlthy, fool, guilt, hate, idiot, inﬂict, lazy,
miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly,
vile, wicked
Figure 21.3
Some samples of words with consistent sentiment across three sentiment lexicons: the General
Inquirer (Stone et al., 1966), the MPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of
Hu and Liu (2004b).
Slightly more general than these sentiment lexicons are lexicons that assign each
word a value on all three affective dimensions. The NRC Valence, Arousal, and
Dominance (VAD) lexicon (Mohammad, 2018a) assigns valence, arousal, and dom-
inance scores to 20,000 words. Some examples are shown in Fig. 21.4.
Valence
Arousal
Dominance
vacation
.840
enraged
.962
powerful
.991
delightful
.918
party
.840
authority
.935
whistle
.653
organized
.337
saxophone
.482
consolation
.408
effortless
.120
discouraged
.0090
torture
.115
napping
.046
weak
.045
Figure 21.4
Samples of the values of selected words on the three emotional dimensions
from Mohammad (2018a).
The NRC Word-Emotion Association Lexicon, also called EmoLex (Moham-
EmoLex
mad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions deﬁned above.
The lexicon includes around 14,000 words including words from prior lexicons as
well as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for
some sample words:

398
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
Word
anger
anticipation
disgust
fear
joy
sadness
surprise
trust
positive
negative
reward
0 1
0
0
1
0 1
1 1
0
worry
0 1
0
1
0
1 0
0 0
1
tenderness 0 0
0
0
1
0 0
0 1
0
sweetheart 0 1
0
0
1
1 0
1 1
0
suddenly
0 0
0
0
0
0 1
0 0
0
thirst
0 1
0
0
0
1 1
0 0
0
garbage
0 0
1
0
0
0 0
0 0
1
For a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon
(Mohammad, 2018b) contains real-valued scores of association for anger, fear, joy,
and sadness; Fig. 21.5 shows examples.
Anger
Fear
Joy
Sadness
outraged
0.964
horror
0.923
superb
0.864
sad
0.844
violence
0.742
anguish
0.703
cheered
0.773
guilt
0.750
coup
0.578
pestilence
0.625
rainbow
0.531
unkind
0.547
oust
0.484
stressed
0.531
gesture
0.387
difﬁculties
0.421
suspicious
0.484
failing
0.531
warms
0.391
beggar
0.422
nurture
0.059
conﬁdent
0.094
hardship
.031
sing
0.017
Figure 21.5
Sample emotional intensities for words for anger, fear, joy, and sadness from
Mohammad (2018b).
LIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lex-
LIWC
icons containing over 2300 words (Pennebaker et al., 2007), designed to capture
aspects of lexical meaning relevant for social psychological tasks. In addition to
sentiment-related lexicons like ones for negative emotion (bad, weird, hate, prob-
lem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for
categories like anger, sadness, cognitive mechanisms, perception, tentative, and in-
hibition, shown in Fig. 21.6.
There are various other hand-built affective lexicons. The General Inquirer in-
cludes additional lexicons for dimensions like strong vs. weak, active vs. passive,
overstated vs. understated, as well as lexicons for categories like pleasure, pain,
virtue, vice, motivation, and cognitive orientation.
Another useful feature for various tasks is the distinction between concrete
concrete
words like banana or bathrobe and abstract words like belief and although. The
abstract
lexicon in Brysbaert et al. (2014) used crowdsourcing to assign a rating from 1 to 5
of the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5,
belief 1.19, although 1.07, and in between words like brisk a 2.5.
21.3
Creating Affect Lexicons by Human Labeling
The earliest method used to build affect lexicons, and still in common use, is to have
humans label each word. This is now most commonly done via crowdsourcing:
crowdsourcing
breaking the task into small pieces and distributing them to a large number of anno-

21.3
•
CREATING AFFECT LEXICONS BY HUMAN LABELING
399
Positive
Negative
Emotion
Emotion
Insight
Inhibition
Family
Negate
appreciat*
anger*
aware*
avoid*
brother*
aren’t
comfort*
bore*
believe
careful*
cousin*
cannot
great
cry
decid*
hesitat*
daughter*
didn’t
happy
despair*
feel
limit*
family
neither
interest
fail*
ﬁgur*
oppos*
father*
never
joy*
fear
know
prevent*
grandf*
no
perfect*
griev*
knew
reluctan*
grandm*
nobod*
please*
hate*
means
safe*
husband
none
safe*
panic*
notice*
stop
mom
nor
terriﬁc
suffers
recogni*
stubborn*
mother
nothing
value
terrify
sense
wait
niece*
nowhere
wow*
violent*
think
wary
wife
without
Figure 21.6
Samples from 5 of the 73 lexical categories in LIWC (Pennebaker et al., 2007).
The * means the previous letters are a word preﬁx and all words with that preﬁx are included
in the category.
tators. Let’s take a look at some of the methodological choices for two crowdsourced
emotion lexicons.
The NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013), labeled
emotions in two steps. To ensure that the annotators were judging the correct sense
of the word, they ﬁrst answered a multiple-choice synonym question that primed
the correct sense of the word (without requiring the annotator to read a potentially
confusing sense deﬁnition). These were created automatically using the headwords
associated with the thesaurus category of the sense in question in the Macquarie
dictionary and the headwords of 3 random distractor categories. An example:
Which word is closest in meaning (most related) to startle?
• automobile
• shake
• honesty
• entertain
For each word (e.g. startle), the annotator was then asked to rate how associated
that word is with each of the 8 emotions (joy, fear, anger, etc.). The associations
were rated on a scale of not, weakly, moderately, and strongly associated. Outlier
ratings were removed, and then each term was assigned the class chosen by the ma-
jority of the annotators, with ties broken by choosing the stronger intensity, and then
the 4 levels were mapped into a binary label for each word (no and weak mapped to
0, moderate and strong mapped to 1).
The NRC VAD Lexicon (Mohammad, 2018a) was built by selecting words and
emoticons from prior lexicons and annotating them with crowd-sourcing using best-
worst scaling (Louviere et al. 2015, Kiritchenko and Mohammad 2017). In best-
best-worst
scaling
worst scaling, annotators are given N items (usually 4) and are asked which item is
the best (highest) and which is the worst (lowest) in terms of some property. The
set of words used to decribe the ends of the scales are taken from prior literature.
For valence, for example, the raters were asked:
Q1. Which of the four words below is associated with the MOST happi-
ness / pleasure / positiveness / satisfaction / contentedness / hopefulness
OR LEAST unhappiness / annoyance / negativeness / dissatisfaction /

400
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
melancholy / despair? (Four words listed as options.)
Q2. Which of the four words below is associated with the LEAST hap-
piness / pleasure / positiveness / satisfaction / contentedness / hopeful-
ness OR MOST unhappiness / annoyance / negativeness / dissatisfaction
/ melancholy / despair? (Four words listed as options.)
The score for each word in the lexicon is the proportion of times the item was chosen
as the best (highest V/A/D) minus the proportion of times the item was chosen as the
worst (lowest V/A/D). The agreement between annotations are evaluated by split-
half reliability: split the corpus in half and compute the correlations between the
split-half
reliability
annotations in the two halves.
21.4
Semi-supervised Induction of Affect Lexicons
Another common way to learn sentiment lexicons is to start from a set of seed words
that deﬁne two poles of a semantic axis (words like good or bad), and then ﬁnd ways
to label each word w by its similarity to the two seed sets. Here we summarize two
families of seed-based semi-supervised lexicon induction algorithms, axis-based and
graph-based.
21.4.1
Semantic Axis Methods
One of the most well-known lexicon induction methods, the Turney and Littman
(2003) algorithm, is given seed words like good or bad, and then for each word w to
be labeled, measures both how similar it is to good and how different it is from bad.
Here we describe a slight extension of the algorithm due to An et al. (2018), which
is based on computing a semantic axis.
In the ﬁrst step, we choose seed words by hand. There are two methods for
dealing with the fact that the affect of a word is different in different contexts: (1)
start with a single large seed lexicon and rely on the induction algorithm to ﬁne-tune
it to the domain, or (2) choose different seed words for different genres. Hellrich
et al. (2019) suggests that for modeling affect across different historical time periods,
starting with a large modern affect dictionary is better than small seedsets tuned to be
stable across time. As an example of the second approach, Hamilton et al. (2016a)
deﬁne one set of seed words for general sentiment analysis, a different set for Twitter,
and yet another set for sentiment in ﬁnancial text:
Domain
Positive seeds
Negative seeds
General
good, lovely, excellent, fortunate, pleas-
ant, delightful, perfect, loved, love,
happy
bad, horrible, poor, unfortunate, un-
pleasant, disgusting, evil, hated, hate,
unhappy
Twitter
love,
loved,
loves,
awesome,
nice,
amazing, best, fantastic, correct, happy
hate, hated, hates, terrible, nasty, awful,
worst, horrible, wrong, sad
Finance
successful, excellent, proﬁt, beneﬁcial,
improving, improved, success, gains,
positive
negligent, loss, volatile, wrong, losses,
damages, bad, litigation, failure, down,
negative
In the second step, we compute embeddings for each of the pole words. These
embeddings can be off-the-shelf word2vec embeddings, or can be computed directly

21.4
•
SEMI-SUPERVISED INDUCTION OF AFFECT LEXICONS
401
on a speciﬁc corpus (for example using a ﬁnancial corpus if a ﬁnance lexicon is the
goal), or we can ﬁne-tune off-the-shelf embeddings to a corpus. Fine-tuning is espe-
cially important if we have a very speciﬁc genre of text but don’t have enough data
to train good embeddings. In ﬁne-tuning, we begin with off-the-shelf embeddings
like word2vec, and continue training them on the small target corpus.
Once we have embeddings for each pole word, we create an embedding that
represents each pole by taking the centroid of the embeddings of each of the seed
words; recall that the centroid is the multidimensional version of the mean. Given
a set of embeddings for the positive seed words S+ = {E(w+
1 ),E(w+
2 ),...,E(w+
n )},
and embeddings for the negative seed words S−= {E(w−
1 ),E(w−
2 ),...,E(w−
m)}, the
pole centroids are:
V+ = 1
n
n
X
1
E(w+
i )
V−= 1
m
m
X
1
E(w−
i )
(21.1)
The semantic axis deﬁned by the poles is computed just by subtracting the two vec-
tors:
Vaxis = V+ −V−
(21.2)
Vaxis, the semantic axis, is a vector in the direction of sentiment. Finally, we compute
how close each word w is to this sentiment axis, by taking the cosine between w’s
embedding and the axis vector. A higher cosine means that w is more aligned with
S+ than S−.
score(w) =
 cos(E(w),Vaxis

=
E(w)·Vaxis
∥E(w)∥∥Vaxis∥
(21.3)
If a dictionary of words with sentiment scores is sufﬁcient, we’re done! Or if we
need to group words into a positive and a negative lexicon, we can use a threshold
or other method to give us discrete lexicons.
21.4.2
Label Propagation
An alternative family of methods deﬁnes lexicons by propagating sentiment labels
on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown
(1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of
Hamilton et al. (2016a), which has four steps:
1. Deﬁne a graph: Given word embeddings, build a weighted lexical graph
by connecting each word with its k nearest neighbors (according to cosine-
similarity). The weights of the edge between words wi and w j are set as:
Ei,j = arccos

−
wi⊤wj
∥wi∥∥wj∥

.
(21.4)
2. Deﬁne a seed set: Choose positive and negative seed words.
3. Propagate polarities from the seed set: Now we perform a random walk on
this graph, starting at the seed set. In a random walk, we start at a node and

402
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
then choose a node to move to with probability proportional to the edge prob-
ability. A word’s polarity score for a seed set is proportional to the probability
of a random walk from the seed set landing on that word, (Fig. 21.7).
4. Create word scores: We walk from both positive and negative seed sets,
resulting in positive (score+(wi)) and negative (score−(wi)) label scores. We
then combine these values into a positive-polarity score as:
score+(wi) =
score+(wi)
score+(wi)+score−(wi)
(21.5)
It’s often helpful to standardize the scores to have zero mean and unit variance
within a corpus.
5. Assign conﬁdence to each score: Because sentiment scores are inﬂuenced by
the seed set, we’d like to know how much the score of a word would change if
a different seed set is used. We can use bootstrap-sampling to get conﬁdence
regions, by computing the propagation B times over random subsets of the
positive and negative seed sets (for example using B = 50 and choosing 7 of
the 10 seed words each time). The standard deviation of the bootstrap-sampled
polarity scores gives a conﬁdence measure.
idolize
love
adore
appreciate
like
ﬁnd
dislike
see
notice
disapprove
abhor
hate
loathe
despise
uncover
idolize
love
adore
appreciate
like
ﬁnd
dislike
see
notice
disapprove
abhor
hate
loathe
despise
uncover
(a)
(b)
Figure 21.7
Intuition of the SENTPROP algorithm. (a) Run random walks from the seed words. (b) Assign
polarity scores (shown here as colors green or red) based on the frequency of random walk visits.
21.4.3
Other Methods
The core of semisupervised algorithms is the metric for measuring similarity with
the seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) ap-
proaches above used embedding cosine as the distance metric: words were labeled
as positive basically if their embeddings had high cosines with positive seeds and
low cosines with negative seeds. Other methods have chosen other kinds of distance
metrics besides embedding cosine.
For example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic
cues; two adjectives are considered similar if they were frequently conjoined by and
and rarely conjoined by but. This is based on the intuition that adjectives conjoined
by the words and tend to have the same polarity; positive adjectives are generally
coordinated with positive, negative with negative:
fair and legitimate, corrupt and brutal
but less often positive adjectives coordinated with negative:
*fair and brutal, *corrupt and legitimate
By contrast, adjectives conjoined by but are likely to be of opposite polarity:

21.5
•
SUPERVISED LEARNING OF WORD SENTIMENT
403
fair but brutal
Another cue to opposite polarity comes from morphological negation (un-, im-,
-less). Adjectives with the same root but differing in a morphological negative (ad-
equate/inadequate, thoughtful/thoughtless) tend to be of opposite polarity.
Yet another method for ﬁnding words that have a similar polarity to seed words is
to make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004b).
A word’s synonyms presumably share its polarity while a word’s antonyms probably
have the opposite polarity. After a seed lexicon is built, each lexicon is updated as
follows, possibly iterated.
Lex+: Add synonyms of positive words (well) and antonyms (like ﬁne) of negative
words
Lex−: Add synonyms of negative words (awful) and antonyms (like evil) of positive
words
An extension of this algorithm assigns polarity to WordNet senses, called Senti-
WordNet (Baccianella et al., 2010). Fig. 21.8 shows some examples.
SentiWordNet
Synset
Pos
Neg
Obj
good#6
‘agreeable or pleasing’
1
0
0
respectable#2 honorable#4 good#4 estimable#2
‘deserving of esteem’
0.75
0
0.25
estimable#3 computable#1
‘may be computed or estimated’
0
0
1
sting#1 burn#4 bite#2
‘cause a sharp or stinging pain’
0
0.875 .125
acute#6
‘of critical importance and consequence’
0.625 0.125 .250
acute#4
‘of an angle; less than 90 degrees’
0
0
1
acute#1
‘having or experiencing a rapid onset and short but severe course’
0
0.5
0.5
Figure 21.8
Examples from SentiWordNet 3.0 (Baccianella et al., 2010). Note the differences between senses
of homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive
(acute#6), negative (acute#1), or neutral (acute #4).
In this algorithm, polarity is assigned to entire synsets rather than words. A
positive lexicon is built from all the synsets associated with 7 positive words, and a
negative lexicon from synsets associated with 7 negative words. A classiﬁer is then
trained from this data to take a WordNet gloss and decide if the sense being deﬁned
is positive, negative or neutral. A further step (involving a random-walk algorithm)
assigns a score to each WordNet synset for its degree of positivity, negativity, and
neutrality.
In summary, semisupervised algorithms use a human-deﬁned set of seed words
for the two poles of a dimension, and use similarity metrics like embedding cosine,
coordination, morphology, or thesaurus structure to score words by how similar they
are to the positive seeds and how dissimilar to the negative seeds.
21.5
Supervised Learning of Word Sentiment
Semi-supervised methods require only minimal human supervision (in the form of
seed sets). But sometimes a supervision signal exists in the world and can be made
use of. One such signal is the scores associated with online reviews.
The web contains an enormous number of online reviews for restaurants, movies,
books, or other products, each of which have the text of the review along with an

404
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
Movie review excerpts (IMDb)
10 A great movie. This ﬁlm is just a wonderful experience. It’s surreal, zany, witty and slapstick
all at the same time. And terriﬁc performances too.
1
This was probably the worst movie I have ever seen. The story went nowhere even though they
could have done some interesting stuff with it.
Restaurant review excerpts (Yelp)
5
The service was impeccable. The food was cooked and seasoned perfectly... The watermelon
was perfectly square ... The grilled octopus was ... mouthwatering...
2
...it took a while to get our waters, we got our entree before our starter, and we never received
silverware or napkins until we requested them...
Book review excerpts (GoodReads)
1
I am going to try and stop being deceived by eye-catching titles. I so wanted to like this book
and was so disappointed by it.
5
This book is hilarious. I would recommend it to anyone looking for a satirical read with a
romantic twist and a narrator that keeps butting in
Product review excerpts (Amazon)
5
The lid on this blender though is probably what I like the best about it... enables you to pour
into something without even taking the lid off! ... the perfect pitcher! ... works fantastic.
1
I hate this blender... It is nearly impossible to get frozen fruit and ice to turn into a smoothie...
You have to add a TON of liquid. I also wish it had a spout ...
Figure 21.9
Excerpts from some reviews from various review websites, all on a scale of 1 to 5 stars except
IMDb, which is on a scale of 1 to 10 stars.
associated review score: a value that may range from 1 star to 5 stars, or scoring 1
to 10. Fig. 21.9 shows samples extracted from restaurant, book, and movie reviews.
We can use this review score as supervision: positive words are more likely to
appear in 5-star reviews; negative words in 1-star reviews. And instead of just a
binary polarity, this kind of supervision allows us to assign a word a more complex
representation of its polarity: its distribution over stars (or other scores).
Thus in a ten-star system we could represent the sentiment of each word as a
10-tuple, each number a score representing the word’s association with that polarity
level. This association can be a raw count, or a likelihood P(w|c), or some other
function of the count, for each class c from 1 to 10.
For example, we could compute the IMDb likelihood of a word like disap-
point(ed/ing) occurring in a 1 star review by dividing the number of times disap-
point(ed/ing) occurs in 1-star reviews in the IMDb dataset (8,557) by the total num-
ber of words occurring in 1-star reviews (25,395,214), so the IMDb estimate of
P(disappointing|1) is .0003.
A slight modiﬁcation of this weighting, the normalized likelihood, can be used
as an illuminating visualization (Potts, 2011)1:
P(w|c) =
count(w,c)
P
w∈C count(w,c)
PottsScore(w) =
P(w|c)
P
c P(w|c)
(21.6)
Dividing the IMDb estimate P(disappointing|1) of .0003 by the sum of the likeli-
hood P(w|c) over all categories gives a Potts score of 0.10. The word disappointing
1
Potts shows that the normalized likelihood is an estimate of the posterior P(c|w) if we make the
incorrect but simplifying assumption that all categories c have equal probability.

21.5
•
SUPERVISED LEARNING OF WORD SENTIMENT
405
thus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The
Potts diagram (Potts, 2011) is a visualization of these word scores, representing the
Potts diagram
prior sentiment of a word as a distribution over the rating categories.
Fig. 21.10 shows the Potts diagrams for 3 positive and 3 negative scalar adjec-
tives. Note that the curve for strongly positive scalars have the shape of the letter
J, while strongly negative scalars look like a reverse J. By contrast, weakly posi-
tive and negative scalars have a hump-shape, with the maximum either below the
mean (weakly negative words like disappointing) or above the mean (weakly pos-
itive words like good). These shapes offer an illuminating typology of affective
meaning.
good
great
excellent
disappointing
bad
terrible
Positive scalars
Negative scalars
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
Figure 21.10
Potts diagrams (Potts, 2011) for positive and negative scalar adjectives, show-
ing the J-shape and reverse J-shape for strongly positive and negative adjectives, and the
hump-shape for more weakly polarized adjectives.
Fig. 21.11 shows the Potts diagrams for emphasizing and attenuating adverbs.
Note that emphatics tend to have a J-shape (most likely to occur in the most posi-
tive reviews) or a U-shape (most likely to occur in the strongly positive and nega-
tive). Attenuators all have the hump-shape, emphasizing the middle of the scale and
downplaying both extremes. The diagrams can be used both as a typology of lexical
sentiment, and also play a role in modeling sentiment compositionality.
In addition to functions like posterior P(c|w), likelihood P(w|c), or normalized
likelihood (Eq. 21.6) many other functions of the count of a word occurring with a
sentiment label have been used. We’ll introduce some of these on page 410, includ-
ing ideas like normalizing the counts per writer in Eq. 21.14.
21.5.1
Log Odds Ratio Informative Dirichlet Prior
One thing we often want to do with word polarity is to distinguish between words
that are more likely to be used in one category of texts than in another. We may, for
example, want to know the words most associated with 1 star reviews versus those
associated with 5 star reviews. These differences may not be just related to senti-
ment. We might want to ﬁnd words used more often by Democratic than Republican

406
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
Category
-0.50
-0.39
-0.28
-0.17
-0.06
0.06
0.17
0.28
0.39
0.50
0.05
0.09
0.13
totally
absolutely
utterly
somewhat
fairly
pretty
Emphatics
Attenuators
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
1  2  3  4  5  6  7  8  9  10
rating
Figure 21.11
Potts diagrams (Potts, 2011) for emphatic and attenuating adverbs.
members of Congress, or words used more often in menus of expensive restaurants
than cheap restaurants.
Given two classes of documents, to ﬁnd words more associated with one cate-
gory than another, we might choose to just compute the difference in frequencies
(is a word w more frequent in class A or class B?). Or instead of the difference in
frequencies we might want to compute the ratio of frequencies, or the log odds ratio
(the log of the ratio between the odds of the two words). Then we can sort words
by whichever of these associations with the category we use, (sorting from words
overrepresented in category A to words overrepresented in category B).
The problem with simple log-likelihood or log odds methods is that they don’t
work well for very rare words or very frequent words; for words that are very fre-
quent, all differences seem large, and for words that are very rare, no differences
seem large.
In this section we walk through the details of one solution to this problem: the
“log odds ratio informative Dirichlet prior” method of Monroe et al. (2008) that is a
particularly useful method for ﬁnding words that are statistically overrepresented in
one particular category of texts compared to another. It’s based on the idea of using
another large corpus to get a prior estimate of what we expect the frequency of each
word to be.
Let’s start with the goal: assume we want to know whether the word horrible
occurs more in corpus i or corpus j. We could compute the log likelihood ratio,
log likelihood
ratio
using f i(w) to mean the frequency of word w in corpus i, and ni to mean the total
number of words in corpus i:
llr(horrible) = log Pi(horrible)
Pj(horrible)
= logPi(horrible)−logPj(horrible)
= log fi(horrible)
ni
−log fj(horrible)
nj
(21.7)
Instead, let’s compute the log odds ratio: does horrible have higher odds in i or in
log odds ratio

21.5
•
SUPERVISED LEARNING OF WORD SENTIMENT
407
j:
lor(horrible) = log

Pi(horrible)
1−Pi(horrible)

−log

Pj(horrible)
1−Pj(horrible)

= log



fi(horrible)
ni
1−fi(horrible)
ni


−log



f j(horrible)
n j
1−fj(horrible)
nj



= log

fi(horrible)
ni −fi(horrible)

−log

f j(horrible)
nj −fj(horrible)

(21.8)
The Dirichlet intuition is to use a large background corpus to get a prior estimate of
what we expect the frequency of each word w to be. We’ll do this very simply by
adding the counts from that corpus to the numerator and denominator, so that we’re
essentially shrinking the counts toward that prior. It’s like asking how large are the
differences between i and j given what we would expect given their frequencies in
a well-estimated large background corpus.
The method estimates the difference between the frequency of word w in two
corpora i and j via the prior-modiﬁed log odds ratio for w, δ (i−j)
w
, which is estimated
as:
δ (i−j)
w
= log

f i
w +αw
ni +α0 −(f iw +αw)

−log
 
f j
w +αw
nj +α0 −(f j
w +αw)
!
(21.9)
(where ni is the size of corpus i, nj is the size of corpus j, f i
w is the count of word w
in corpus i, f j
w is the count of word w in corpus j, α0 is the size of the background
corpus, and αw is the count of word w in the background corpus.)
In addition, Monroe et al. (2008) make use of an estimate for the variance of the
log–odds–ratio:
σ2 
ˆδ (i−j)
w

≈
1
f iw +αw
+
1
f j
w +αw
(21.10)
The ﬁnal statistic for a word is then the z–score of its log–odds–ratio:
ˆδ (i−j)
w
r
σ2

ˆδ (i−j)
w

(21.11)
The Monroe et al. (2008) method thus modiﬁes the commonly used log odds ratio
in two ways: it uses the z-scores of the log odds ratio, which controls for the amount
of variance in a word’s frequency, and it uses counts from a background corpus to
provide a prior count for words.
Fig. 21.12 shows the method applied to a dataset of restaurant reviews from
Yelp, comparing the words used in 1-star reviews to the words used in 5-star reviews
(Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the
1-star reviews using negative sentiment words like worse, bad, awful and the 5-star
reviews using positive sentiment words like great, best, amazing. But there are other
illuminating differences. 1-star reviews use logical negation (no, not), while 5-star
reviews use emphatics and emphasize universality (very, highly, every, always). 1-
star reviews use ﬁrst person plurals (we, us, our) while 5 star reviews use the second
person. 1-star reviews talk about people (manager, waiter, customer) while 5-star
reviews talk about dessert and properties of expensive restaurants like courses and
atmosphere. See Jurafsky et al. (2014) for more details.

408
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
Class
Words in 1-star reviews
Class
Words in 5-star reviews
Negative
worst, rude, terrible, horrible, bad,
awful, disgusting, bland, tasteless,
gross, mediocre, overpriced, worse,
poor
Positive
great, best, love(d), delicious, amazing,
favorite, perfect, excellent, awesome,
friendly, fantastic, fresh, wonderful, in-
credible, sweet, yum(my)
Negation
no, not
Emphatics/
universals
very, highly, perfectly, deﬁnitely, abso-
lutely, everything, every, always
1Pl pro
we, us, our
2 pro
you
3 pro
she, he, her, him
Articles
a, the
Past verb
was, were, asked, told, said, did,
charged, waited, left, took
Advice
try, recommend
Sequencers after, then
Conjunct
also, as, well, with, and
Nouns
manager, waitress, waiter, customer,
customers, attitude, waste, poisoning,
money, bill, minutes
Nouns
atmosphere, dessert, chocolate, wine,
course, menu
Irrealis
modals
would, should
Auxiliaries
is/’s, can, ’ve, are
Comp
to, that
Prep, other
in, of, die, city, mouth
Figure 21.12
The top 50 words associated with one–star and ﬁve-star restaurant reviews in a Yelp dataset of
900,000 reviews, using the Monroe et al. (2008) method (Jurafsky et al., 2014).
21.6
Using Lexicons for Sentiment Recognition
In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The
lexicons we have focused on throughout the chapter so far can be used in a number
of ways to improve sentiment detection.
In the simplest case, lexicons can be used when we don’t have sufﬁcient training
data to build a supervised sentiment analyzer; it can often be expensive to have a
human assign sentiment to each document to train the supervised classiﬁer.
In such situations, lexicons can be used in a rule-based algorithm for classiﬁca-
tion. The simplest version is just to use the ratio of positive to negative words: if a
document has more positive than negative words (using the lexicon to decide the po-
larity of each word in the document), it is classiﬁed as positive. Often a threshold λ
is used, in which a document is classiﬁed as positive only if the ratio is greater than
λ. If the sentiment lexicon includes positive and negative weights for each word,
θ +
w and θ −
w , these can be used as well. Here’s a simple such sentiment algorithm:
f + =
X
w s.t. w∈positivelexicon
θ +
w count(w)
f −=
X
w s.t. w∈negativelexicon
θ −
w count(w)
sentiment =









+
if f +
f −> λ
−
if f −
f + > λ
0
otherwise.
(21.12)
If supervised training data is available, these counts computed from sentiment lex-
icons, sometimes weighted or normalized in various ways, can also be used as fea-
tures in a classiﬁer along with other lexical or non-lexical features. We return to
such algorithms in Section 21.8.

21.7
•
OTHER TASKS: PERSONALITY
409
21.7
Other tasks: Personality
Many other kinds of affective meaning can be extracted from text and speech. For
example detecting a person’s personality from their language can be useful for di-
personality
alog systems (users tend to prefer agents that match their personality), and can play
a useful role in computational social science questions like understanding how per-
sonality is related to other kinds of behavior.
Many theories of human personality are based around a small number of dimen-
sions, such as various versions of the “Big Five” dimensions (Digman, 1990):
Extroversion vs. Introversion: sociable, assertive, playful vs. aloof, reserved,
shy
Emotional stability vs. Neuroticism: calm, unemotional vs. insecure, anxious
Agreeableness vs. Disagreeableness: friendly, cooperative vs. antagonistic, fault-
ﬁnding
Conscientiousness vs. Unconscientiousness: self-disciplined, organized vs. in-
efﬁcient, careless
Openness to experience: intellectual, insightful vs. shallow, unimaginative
A few corpora of text and speech have been labeled for the personality of their
author by having the authors take a standard personality test. The essay corpus of
Pennebaker and King (1999) consists of 2,479 essays (1.9 million words) from psy-
chology students who were asked to “write whatever comes into your mind” for 20
minutes. The EAR (Electronically Activated Recorder) corpus of Mehl et al. (2006)
was created by having volunteers wear a recorder throughout the day, which ran-
domly recorded short snippets of conversation throughout the day, which were then
transcribed. The Facebook corpus of (Schwartz et al., 2013) includes 309 million
words of Facebook posts from 75,000 volunteers.
For example, here are samples from Pennebaker and King (1999) from an essay
written by someone on the neurotic end of the neurotic/emotionally stable scale,
One of my friends just barged in, and I jumped in my seat. This is crazy.
I should tell him not to do that again. I’m not that fastidious actually.
But certain things annoy me. The things that would annoy me would
actually annoy any normal human being, so I know I’m not a freak.
and someone on the emotionally stable end of the scale:
I should excel in this sport because I know how to push my body harder
than anyone I know, no matter what the test I always push my body
harder than everyone else. I want to be the best no matter what the sport
or event. I should also be good at this because I love to ride my bike.
Another kind of affective meaning is what Scherer (2000) calls interpersonal
stance, the ‘affective stance taken toward another person in a speciﬁc interaction
interpersonal
stance
coloring the interpersonal exchange’. Extracting this kind of meaning means au-
tomatically labeling participants for whether they are friendly, supportive, distant.
For example Ranganath et al. (2013) studied a corpus of speed-dates, in which par-
ticipants went on a series of 4-minute romantic dates, wearing microphones. Each
participant labeled each other for how ﬂirtatious, friendly, awkward, or assertive
they were. Ranganath et al. (2013) then used a combination of lexicons and other
features to detect these interpersonal stances from text.

410
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
21.8
Affect Recognition
Detection of emotion, personality, interactional stance, and the other kinds of af-
fective meaning described by Scherer (2000) can be done by generalizing the algo-
rithms described above for detecting sentiment.
The most common algorithms involve supervised classiﬁcation: a training set is
labeled for the affective meaning to be detected, and a classiﬁer is built using features
extracted from the training set. As with sentiment analysis, if the training set is large
enough, and the test set is sufﬁciently similar to the training set, simply using all
the words or all the bigrams as features in a powerful classiﬁer like SVM or logistic
regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose
performance is hard to beat. Thus we can treat affective meaning classiﬁcation of a
text sample as simple document classiﬁcation.
Some modiﬁcations are nonetheless often necessary for very large datasets. For
example, the Schwartz et al. (2013) study of personality, gender, and age using 700
million words of Facebook posts used only a subset of the n-grams of lengths 1-
3. Only words and phrases used by at least 1% of the subjects were included as
features, and 2-grams and 3-grams were only kept if they had sufﬁciently high PMI
(PMI greater than 2∗length, where length is the number of words):
pmi(phrase) = log
p(phrase)
Y
w∈phrase
p(w)
(21.13)
Various weights can be used for the features, including the raw count in the training
set, or some normalized probability or log probability. Schwartz et al. (2013), for
example, turn feature counts into phrase likelihoods by normalizing them by each
subject’s total word use.
p(phrase|subject) =
freq(phrase,subject)
X
phrase′∈vocab(subject)
freq(phrase′,subject)
(21.14)
If the training data is sparser, or not as similar to the test set, any of the lexicons
we’ve discussed can play a helpful role, either alone or in combination with all the
words and n-grams.
Many possible values can be used for lexicon features. The simplest is just an
indicator function, in which the value of a feature fL takes the value 1 if a particular
text has any word from the relevant lexicon L. Using the notation of Chapter 4, in
which a feature value is deﬁned for a particular output class c and document x.
fL(c,x) =
 1 if ∃w : w ∈L & w ∈x & class = c
0 otherwise
Alternatively the value of a feature fL for a particular lexicon L can be the total
number of word tokens in the document that occur in L:
fL =
X
w∈L
count(w)
For lexica in which each word is associated with a score or weight, the count can be
multiplied by a weight θ L
w:
fL =
X
w∈L
θ L
wcount(w)

21.9
•
LEXICON-BASED METHODS FOR ENTITY-CENTRIC AFFECT
411
Counts can alternatively be logged or normalized per writer as in Eq. 21.14.
However they are deﬁned, these lexicon features are then used in a supervised
classiﬁer to predict the desired affective category for the text or document. Once
a classiﬁer is trained, we can examine which lexicon features are associated with
which classes. For a classiﬁer like logistic regression the feature weight gives an
indication of how associated the feature is with the class.
Thus, for example, Mairesse and Walker (2008) found that for classifying per-
sonality, for the dimension Agreeable, the LIWC lexicons Family and Home were
positively associated while the LIWC lexicons anger and swear were negatively
associated. By contrast, Extroversion was positively associated with the Friend,
Religion and Self lexicons, and Emotional Stability was positively associated with
Sports and negatively associated with Negative Emotion.
(a)
(b)
Figure 21.13
Word clouds from Schwartz et al. (2013), showing words highly associated
with introversion (left) or extroversion (right). The size of the word represents the association
strength (the regression coefﬁcient), while the color (ranging from cold to hot) represents the
relative frequency of the word/phrase (from low to high).
In the situation in which we use all the words and phrases in the document as
potential features, we can use the resulting weights from the learned regression clas-
siﬁer as the basis of an affective lexicon. In the Extroversion/Introversion classiﬁer
of Schwartz et al. (2013), ordinary least-squares regression is used to predict the
value of a personality dimension from all the words and phrases. The resulting re-
gression coefﬁcient for each word or phrase can be used as an association value with
the predicted dimension. The word clouds in Fig. 21.13 show an example of words
associated with introversion (a) and extroversion (b).
21.9
Lexicon-based methods for Entity-Centric Affect
What if we want to get an affect score not for an entire document, but for a particular
entity in the text? The entity-centric method of Field and Tsvetkov (2019) combines
affect lexicons with contextual embeddings to assign an affect score to an entity in
text. In the context of affect about people, they relabel the Valence/Arousal/Domi-
nance dimension as Sentiment/Agency/Power. The algorithm ﬁrst trains classiﬁers
to map embeddings to scores:
1. For each word w in the training corpus:
(a) Use off-the-shelf pre-trained language models (ELMo or BERT) to ex-
tract a contextual embedding e for each instance of the word. No addi-

412
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
tional ﬁne-tuning is done.
(b) Average over the e embeddings of each instance of w to obtain a single
embedding vector for one training point w.
(c) Use the NRC VAD Lexicon to get S, A, and P scores for w.
2. Train (three) regression models on all words w to predict V, A, D scores from
a word’s average embedding.
Now given an entity mention m in a text, we assign affect scores as follows:
1. Use the same pre-trained LM to get contextual embeddings for m in context.
2. Feed this embeddings through the 3 regression models to get S, A, P scores
for the entity.
This results in a (S,A,P) tuple for a given entity mention; it get scores for the resp-
resentation of an entity in a complete document, we can run coref and average the
(S,A,P) scores for all the mentions. They ﬁnd ELMo works much better than BERT.
Fig. 21.14 shows the scores from their algorithm for characters from the movie The
Dark Knight when run on Wikpedia plot summary texts with gold coreference.
Power Score
weakly
Rachel
Dent
Gordan Batman
Joker powerfully
Sentiment Score
negative Joker
Dent
Gordan Rachel Batman positive
Agency Score
dull
Dent
Gordan Rachel Batman Joker
scary
Figure 21.14
Power (dominance), sentiment (valence) and agency (arousal) for characters
in the movie The Dark Knight computed from ELMo embeddings trained on the NRC VAD
Lexicon. Note the protagonst (Batman) and the antagonist (the Joker) have high power and
agency scores but differ in sentiment, while the love interest Rachel has low power and agency
but high sentiment.
21.10
Connotation Frames
The lexicons we’ve described so far deﬁne a word as a point in affective space. A
connotation frame, by contrast, is a lexicon that incorporates a richer kind of gram-
connotation
frame
matical structure, by combining affective lexicons with the frame semantic lexicons
of Chapter 20. The basic insight of connotation frame lexicons is that a predicate
like a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016,
Rashkin et al. 2017).
Consider sentences like:
(21.15) Country A violated the sovereignty of Country B
(21.16) the teenager ... survived the Boston Marathon bombing”

21.11
•
SUMMARY
413
By using the verb violate in (21.15), the author is expressing their sympathies with
Country B, portraying Country B as a victim, and expressing antagonism toward
the agent Country A. By contrast, in using the verb survive, the author of (21.16) is
expressing that the bombing is a negative experience, and the subject of the sentence,
the teenager, is a sympathetic character. These aspects of connotation are inherent
in the meaning of the verbs violate and survive, as shown in Fig. 21.15.
Writer
Role1
Role2
Role1 is a
sympathetic 
victim
There is
some type
of hardship
Reader
+
_
+
_
_
S(writer→role1)
S(writer→role2)
Connotation Frame for “Role1 survives Role2” 
S(role1→role2)
Writer
Role1
Role2
Role1 is the
 antagonist
Role2 is a
sympathetic
 victim
Reader
+
_
+
_
_
S(writer→role1)
S(writer→role2)
Connotation Frame for “Role1 violates Role2” 
S(role1→role2)
(a)
(b)
Figure 21.15
Connotation frames for survive and violate. (a) For survive, the writer and reader have positive
sentiment toward Role1, the subject, and negative sentiment toward Role2, the direct object. (b) For violate, the
writer and reader have positive sentiment instead toward Role2, the direct object.
The connotation frame lexicons of Rashkin et al. (2016) and Rashkin et al.
(2017) also express other connotative aspects of the predicate toward each argument,
including the effect (something bad happened to x) value: (x is valuable), and mental
state: (x is distressed by the event). Connotation frames can also mark aspects of
power and agency; see Chapter 20 (Sap et al., 2017).
Connotation frames can be built by hand (Sap et al., 2017), or they can be learned
by supervised learning (Rashkin et al., 2016), for example using hand-labeled train-
ing data to supervise classiﬁers for each of the individual relations, e.g., whether
S(writer →Role1) is + or -, and then improving accuracy via global constraints
across all relations.
21.11
Summary
• Many kinds of affective states can be distinguished, including emotions, moods,
attitudes (which include sentiment), interpersonal stance, and personality.
• Emotion can be represented by ﬁxed atomic units often called basic emo-
tions, or as points in space deﬁned by dimensions like valence and arousal.
• Words have connotational aspects related to these affective states, and this
connotational aspect of word meaning can be represented in lexicons.
• Affective lexicons can be built by hand, using crowd sourcing to label the
affective content of each word.
• Lexicons can be built with semi-supervised, bootstrapping from seed words
using similarity metrics like embedding cosine.
• Lexicons can be learned in a fully supervised manner, when a convenient
training signal can be found in the world, such as ratings assigned by users on
a review site.

414
CHAPTER 21
•
LEXICONS FOR SENTIMENT, AFFECT, AND CONNOTATION
• Words can be assigned weights in a lexicon by using various functions of word
counts in training texts, and ratio metrics like log odds ratio informative
Dirichlet prior.
• Personality is often represented as a point in 5-dimensional space.
• Affect can be detected, just like sentiment, by using standard supervised text
classiﬁcation techniques, using all the words or bigrams in a text as features.
Additional features can be drawn from counts of words in lexicons.
• Lexicons can also be used to detect affect in a rule-based classiﬁer by picking
the simple majority sentiment based on counts of words in each lexicon.
• Connotation frames express richer relations of affective meaning that a pred-
icate encodes about its arguments.
Bibliographical and Historical Notes
The idea of formally representing the subjective meaning of words began with Os-
good et al. (1957), the same pioneering study that ﬁrst proposed the vector space
model of meaning described in Chapter 6. Osgood et al. (1957) had participants rate
words on various scales, and ran factor analysis on the ratings. The most signiﬁcant
factor they uncovered was the evaluative dimension, which distinguished between
pairs like good/bad, valuable/worthless, pleasant/unpleasant. This work inﬂuenced
the development of early dictionaries of sentiment and affective meaning in the ﬁeld
of content analysis (Stone et al., 1966).
Wiebe (1994) began an inﬂuential line of work on detecting subjectivity in text,
subjectivity
beginning with the task of identifying subjective sentences and the subjective char-
acters who are described in the text as holding private states, beliefs or attitudes.
Learned sentiment lexicons such as the polarity lexicons of Hatzivassiloglou and
McKeown (1997) were shown to be a useful feature in subjectivity detection (Hatzi-
vassiloglou and Wiebe 2000, Wiebe 2000).
The term sentiment seems to have been introduced in 2001 by Das and Chen
(2001), to describe the task of measuring market sentiment by looking at the words in
stock trading message boards. In the same paper Das and Chen (2001) also proposed
the use of a sentiment lexicon. The list of words in the lexicon was created by
hand, but each word was assigned weights according to how much it discriminated
a particular class (say buy versus sell) by maximizing across-class variation and
minimizing within-class variation. The term sentiment, and the use of lexicons,
caught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) ﬁrst showed
the power of using all the words without a sentiment lexicon; see also Wang and
Manning (2012).
Most of the semi-supervised methods we describe for extending sentiment dic-
tionaries drew on the early idea that synonyms and antonyms tend to co-occur in the
same sentence. (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shep-
herd 1997). Other semi-supervised methods for learning cues to affective mean-
ing rely on information extraction techniques, like the AutoSlog pattern extractors
(Riloff and Wiebe, 2003). Graph based algorithms for sentiment were ﬁrst sug-
gested by Hatzivassiloglou and McKeown (1997), and graph propagation became
a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004,
Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by

BIBLIOGRAPHICAL AND HISTORICAL NOTES
415
ﬁltering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,
Fast et al. 2016).
Much recent work focuses on ways to learn embeddings that directly encode sen-
timent or other properties, such as the DENSIFIER algorithm of Rothe et al. (2016)
that learns to transform the embedding space to focus on sentiment (or other) infor-
mation.

416
CHAPTER 22
•
COREFERENCE RESOLUTION
CHAPTER
22
Coreference Resolution
and even Stigand, the patriotic archbishop of Canterbury, found it advisable–”’
‘Found WHAT?’ said the Duck.
‘Found IT,’ the Mouse replied rather crossly: ‘of course you know what “it”means.’
‘I know what “it”means well enough, when I ﬁnd a thing,’ said the Duck: ‘it’s gener-
ally a frog or a worm. The question is, what did the archbishop ﬁnd?’
Lewis Carroll, Alice in Wonderland
An important component of language understanding is knowing who is being talked
about in a text. Consider the following passage:
(22.1) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3
million, as the 38-year-old became the company’s president. It is widely
known that she came to Megabucks from rival Lotsabucks.
Each of the underlined phrases in this passage is used by the writer to refer to
a person named Victoria Chen. We call linguistic expressions like her or Victoria
Chen mentions or referring expressions, and the discourse entity that is referred
mention
to (Victoria Chen) the referent. (To distinguish between referring expressions and
referent
their referents, we italicize the former.)1 Two or more referring expressions that are
used to refer to the same discourse entity are said to corefer; thus, Victoria Chen
corefer
and she corefer in (22.1).
Coreference is an important component of natural language understanding. A
dialogue system that has just told the user “There is a 2pm ﬂight on United and a
4pm one on Cathay Paciﬁc” must know which ﬂight the user means by “I’ll take the
Cathay Paciﬁc ﬂight”. A question answering system that uses Wikipedia to answer a
question about where Marie Curie was born must know who she was in the sentence
“She was born in Warsaw”. And a machine translation system translating from a
language like Spanish, in which pronouns can be dropped, must use coreference
from the previous sentence to decide whether the Spanish sentence ‘“Me incanta el
conocimiento”, dice.’ should be translated as ‘“I love knowledge”, he said’, or ‘“I
love knowledge”, she said’. Indeed, this example comes from an actual news article
about a female professor and was mistranslated as “he” by Google Translate because
of inaccurate coreference resolution (Schiebinger, 2019).
Natural language understanding systems (and humans) interpret linguistic ex-
pressions with respect to a discourse model (Karttunen, 1969) shown in Fig. 22.1.
discourse
model
A discourse model is a mental model that the system (or a human hearer) builds in-
crementally as it interprets a text, containing representations of the entities referred
to in the text, as well as properties of the entities and relations among them. When a
referent is ﬁrst mentioned in a discourse, we say that a representation for it is evoked
evoked
into the model. Upon subsequent mention, this representation is accessed from the
accessed
1
As a convenient shorthand, we sometimes speak of a referring expression referring to a referent, e.g.,
saying that she refers to Victoria Chen. However, the reader should keep in mind that what we really
mean is that the speaker is performing the act of referring to Victoria Chen by uttering she.

417
V
Discourse Model
“Victoria”
“she"
corefer
refer (evoke)
refer (access)
$
Lotsabucks
Megabucks
pay
Figure 22.1
How mentions evoke and access discourse entities in a discourse model.
model.
Reference in a text to an entity that has been previously introduced into the
discourse is called anaphora, and the referring expression used is said to be an
anaphora
anaphor, or anaphoric.2 In passage (22.1), the pronouns she and her and the deﬁ-
anaphor
nite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior
mention (in this case Victoria Chen) that is called the antecedent. Not every refer-
antecedent
ring expression is an antecedent. An entity that has only a single mention in a text
(like Lotsabucks in (22.1)) is called a singleton.
singleton
In this chapter we focus on the task of of coreference resolution. Coreference
coreference
resolution
resolution is the task of determining whether two mentions corefer, by which we
mean they refer to the same entity in the discourse model (the same discourse entity).
The set of corefering expressions is often called a coreference chain or a cluster.
coreference
chain
cluster
For example, in processing (22.1), a coreference resolution algorithm would need
to ﬁnd at least four coreference chains, corresponding to the four entities in the
discourse model in Fig. 22.1.
1. {Victoria Chen, her, the 38-year-old, She}
2. {Megabucks Banking, the company, Megabucks}
3. {her pay}
4. {Lotsabucks}
Note that mentions can be nested; for example the mention her is syntactically
part of another mention, her pay, referring to a completely different discourse entity.
Coreference resolution thus comprises two tasks (although they are often per-
formed jointly): (1) identifying the mentions, and (2) clustering them into corefer-
ence chains/discourse entities.
We said that two mentions corefered if they are associated with the same dis-
course entity. But often we’d like to go further, deciding which real world entity is
associated with this discourse entity. For example, the mention Washington might
refer to the US state, or the capital city, or the person George Washington; the in-
terpretation of the sentence will of course be very different for these completely
different named entity types (Chapter 18). The task of entity linking (Ji and Gr-
entity linking
ishman, 2011) or entity resolution is the task of mapping a discourse entity to some
real-world individual.3 We usually operationalize entity linking or resolution by
2
We will follow the common NLP usage of anaphor to mean any mention that has an antecedent, rather
than the more narrow usage to mean only mentions (like pronouns) whose interpretation depends on the
antecedent (under the narrower interpretation, repeated names are not anaphors).
3
Computational linguistics/NLP thus differs in its use of the term reference from the ﬁeld of formal
semantics, which uses the words reference and coreference to describe the relation between a mention
and a real-world entity. By contrast, we follow the functional linguistics tradition in which a mention
refers to a discourse entity (Webber, 1978) and the relation between a discourse entity and the real world

418
CHAPTER 22
•
COREFERENCE RESOLUTION
mapping to an ontology: a list of entities in the world, like a gazeteer (Chapter 16).
Perhaps the most common ontology used for this task is Wikipedia; each Wikipedia
page acts as the unique id for a particular entity. Thus the entity linking task of wiki-
ﬁcation (Mihalcea and Csomai, 2007) is the task of deciding which Wikipedia page
corresponding to an individual is being referred to by a mention. But entity linking
can be done with any ontology; for example if we have an ontology of genes, we
can link mentions of genes in text to the disambiguated gene name in the ontology.
In the next sections we introduce the task of coreference resolution in more de-
tail, and offer a variety of architectures for resolution, from simple deterministic
baseline algorithms to state-of-the-art neural models.
Before turning to algorithms, however, we mention some important tasks we
will only touch on brieﬂy at the end of this chapter. First are the famous Winograd
Schema problems (so-called because they were ﬁrst pointed out by Terry Winograd
in his dissertation). These entity coreference resolution problems are designed to
be too difﬁcult to be solved by the resolution methods we describe in this chapter,
and the kind of real-world knowledge they require has made them a kind of chal-
lenge task for natural language understanding. For example, consider the task of
determining the correct antecedent of the pronoun they in the following example:
(22.2) The city council denied the demonstrators a permit because
a. they feared violence.
b. they advocated violence.
Determining the correct antecedent for the pronoun they requires understanding
that the second clause is intended as an explanation of the ﬁrst clause, and also
that city councils are perhaps more likely than demonstrators to fear violence and
that demonstrators might be more likely to advocate violence. Solving Winograd
Schema problems requires ﬁnding way to represent or discover the necessary real
world knowledge.
A problem we won’t discuss in this chapter is the related task of event corefer-
ence, deciding whether two event mentions (such as the buy and the acquisition in
event
coreference
these two sentences from the ECB+ corpus) refer to the same event:
(22.3) AMD agreed to [buy] Markham, Ontario-based ATI for around $5.4 billion
in cash and stock, the companies announced Monday.
(22.4) The [acquisition] would turn AMD into one of the world’s largest providers
of graphics chips.
Event mentions are much harder to detect than entity mentions, since they can be ver-
bal as well as nominal. Once detected, the same mention-pair and mention-ranking
models used for entities are often applied to events.
An even more complex kind of coreference is discourse deixis (Webber, 1988),
discourse deixis
in which an anaphor refers back to a discourse segment, which can be quite hard to
delimit or categorize, like the examples in (22.5) adapted from Webber (1991):
(22.5) According to Soleil, Beau just opened a restaurant
a. But that turned out to be a lie.
b. But that was false.
c. That struck me as a funny way to describe the situation.
The referent of that is a speech act (see Chapter 26) in (22.5a), a proposition in
(22.5b), and a manner of description in (22.5c). The ﬁeld awaits the development of
robust methods for interpreting most of these types of reference.
individual requires an additional step of linking.

22.1
•
COREFERENCE PHENOMENA: LINGUISTIC BACKGROUND
419
22.1
Coreference Phenomena: Linguistic Background
We now offer some linguistic background on reference phenomena. We introduce
the four types of referring expressions (deﬁnite and indeﬁnite NPs, pronouns, and
names), describe how these are used to evoke and access entities in the discourse
model, and talk about linguistic features of the anaphor/antecedent relation (like
number/gender agreement, or properties of verb semantics).
22.1.1
Types of Referring Expressions
Indeﬁnite Noun Phrases:
The most common form of indeﬁnite reference in En-
glish is marked with the determiner a (or an), but it can also be marked by a quan-
tiﬁer such as some or even the determiner this. Indeﬁnite reference generally intro-
duces into the discourse context entities that are new to the hearer.
(22.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose.
b. He had gone round one day to bring her some walnuts.
c. I saw this beautiful cauliﬂower today.
Deﬁnite Noun Phrases:
Deﬁnite reference, such as via NPs that use the English
article the, refers to an entity that is identiﬁable to the hearer. An entity can be
identiﬁable to the hearer because it has been mentioned previously in the text and
thus is already represented in the discourse model:
(22.7) It concerns a white stallion which I have sold to an ofﬁcer. But the pedigree
of the white stallion was not fully established.
Alternatively, an entity can be identiﬁable because it is contained in the hearer’s
set of beliefs about the world, or the uniqueness of the object is implied by the
description itself, in which case it evokes a representation of the referent into the
discourse model, as in (22.9):
(22.8) I read about it in the New York Times.
(22.9) Have you seen the car keys?
These last uses are quite common; more than half of deﬁnite NPs in newswire
texts are non-anaphoric, often because they are the ﬁrst time an entity is mentioned
(Poesio and Vieira 1998, Bean and Riloff 1999).
Pronouns:
Another form of deﬁnite reference is pronominalization, used for enti-
ties that are extremely salient in the discourse, (as we discuss below):
(22.10) Emma smiled and chatted as cheerfully as she could,
Pronouns can also participate in cataphora, in which they are mentioned before
cataphora
their referents are, as in (22.11).
(22.11) Even before she saw it, Dorothy had been thinking about the Emerald City
every day.
Here, the pronouns she and it both occur before their referents are introduced.
Pronouns also appear in quantiﬁed contexts in which they are considered to be
bound, as in (22.12).
bound
(22.12) Every dancer brought her left arm forward.
Under the relevant reading, her does not refer to some woman in context, but instead
behaves like a variable bound to the quantiﬁed expression every dancer. We are not
concerned with the bound interpretation of pronouns in this chapter.

420
CHAPTER 22
•
COREFERENCE RESOLUTION
In some languages, pronouns can appear as clitics attached to a word, like lo
(‘it’) in this Spanish example from AnCora (Recasens and Mart´ı, 2010):
(22.13) La intenci´on es reconocer el gran prestigio que tiene la marat´on y unirlo
con esta gran carrera.
‘The aim is to recognize the great prestige that the Marathon has and join|it
with this great race.”
Demonstrative Pronouns:
Demonstrative pronouns this and that can appear ei-
ther alone or as determiners, for instance, this ingredient, that spice:
(22.14) I just bought a copy of Thoreau’s Walden. I had bought one ﬁve years ago.
That one had been very tattered; this one was in much better condition.
Note that this NP is ambiguous; in colloquial spoken English, it can be indeﬁnite,
as in (22.6), or deﬁnite, as in (22.14).
Zero Anaphora:
Instead of using a pronoun, in some languages (including Chi-
nese, Japanese, and Italian) it is possible to have an anaphor that has no lexical
realization at all, called a zero anaphor or zero pronoun, as in the following Italian
zero anaphor
and Japanese examples from Poesio et al. (2016):
(22.15)
EN [John]i went to visit some friends. On the way [he]i bought some
wine.
IT [Giovanni]i and`o a far visita a degli amici. Per via φi compr`o del vino.
JA [John]i-wa yujin-o houmon-sita. Tochu-de φi wain-o ka-tta.
or this Chinese example:
(22.16) [我] 前一会精神上太紧张。[0] 现在比较平静了
[I] was too nervous a while ago. ... [0] am now calmer.
Zero anaphors complicate the task of mention detection in these languages.
Names:
Names (such as of people, locations, or organizations) can be used to refer
to both new and old entities in the discourse:
(22.17)
a. Miss Woodhouse certainly had not done him justice.
b. International Business Machines sought patent compensation
from Amazon; IBM had previously sued other companies.
22.1.2
Information Status
The way referring expressions are used to evoke new referents into the discourse
(introducing new information), or access old entities from the model (old informa-
tion), is called their information status or information structure. Entities can be
information
status
discourse-new or discourse-old, and indeed it is common to distinguish at least
discourse-new
discourse-old
three kinds of entities informationally (Prince, 1981a):
new NPs:
brand new NPs: these introduce entities that are discourse-new and hearer-
new like a fruit or some walnuts.
unused NPs: these introduce entities that are discourse-new but hearer-old
(like Hong Kong, Marie Curie, or the New York Times.
old NPs: also called evoked NPs, these introduce entities that already in the dis-
course model, hence are both discourse-old and hearer-old, like it in “I went
to a new restaurant. It was...”.

22.1
•
COREFERENCE PHENOMENA: LINGUISTIC BACKGROUND
421
inferrables: these introduce entities that are neither hearer-old nor discourse-old,
but the hearer can infer their existence by reasoning based on other entities
that are in the discourse. Consider the following examples:
(22.18) I went to a superb restaurant yesterday. The chef had just opened it.
(22.19) Mix ﬂour, butter and water. Knead the dough until shiny.
Neither the chef nor the dough were in the discourse model based on the ﬁrst
sentence of either example, but the reader can make a bridging inference
bridging
inference
that these entities should be added to the discourse model and associated with
the restaurant and the ingredients, based on world knowledge that restaurants
have chefs and dough is the result of mixing ﬂour and liquid (Haviland and
Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).
The form of an NP gives strong clues to its information status. We often talk
about an entity’s position on the given-new dimension, the extent to which the refer-
given-new
ent is given (salient in the discourse, easier for the hearer to call to mind, predictable
by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976,
Prince 1981b, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001)
accessible
i.e., very salient in the hearer’s mind or easy to call to mind, can be referred to with
less linguistic material. For example pronouns are used only when the referent has
a high degree of activation or salience in the discourse model.4 By contrast, less
salience
salient entities, like a new referent being introduced to the discourse, will need to be
introduced with a longer and more explicit referring expression to help the hearer
recover the referent.
Thus when an entity is ﬁrst introduced into a discourse its mentions are likely
to have full names, titles or roles, or appositive or restrictive relative clauses, as in
the introduction of our protagonist in (22.1): Victoria Chen, CFO of Megabucks
Banking. As an entity is discussed over a discourse, it becomes more salient to the
hearer and its mentions on average typically becomes shorter and less informative,
for example with a shortened name (for example Ms. Chen), a deﬁnite description
(the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change
in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b,
Reichman 1985, Fox 1993).
22.1.3
Complications: Non-Referring Expressions
Many noun phrases or other nominals are not referring expressions, although they
may bear a confusing superﬁcial resemblance. For example in some of the earliest
computational work on reference resolution, Karttunen (1969) pointed out that the
NP a car in the following example does not create a discourse referent:
(22.20) Janet doesn’t have a car.
and cannot be referred back to by anaphoric it or the car:
(22.21) *It is a Toyota.
(22.22) *The car is red.
We summarize here four common types of structures that are not counted as men-
tions in coreference tasks and hence complicate the task of mention-detection:
4
Pronouns also usually (but not always) refer to entities that were introduced no further than one or two
sentences back in the ongoing discourse, whereas deﬁnite noun phrases can often refer further back.

422
CHAPTER 22
•
COREFERENCE RESOLUTION
Appositives:
An appositional structure is a noun phrase that appears next to a
head noun phrase, describing the head. In English they often appear in commas, like
“a unit of UAL” appearing in apposition to the NP United, or CFO of Megabucks
Banking in apposition to Victoria Chen.
(22.23) Victoria Chen, CFO of Megabucks Banking, saw ...
(22.24) United, a unit of UAL, matched the fares.
Appositional NPs are not referring expressions, instead functioning as a kind of
supplementary parenthetical description of the head NP. Nonetheless, sometimes it
is useful to link these phrases to an entity they describe, and so some datasets like
ntoNotes mark appositional relationships.
Predicative and Prenominal NPs:
Predicative or attributive NPs describe prop-
erties of the head noun. In United is a unit of UAL, the NP a unit of UAL describes
a property of United, rather than referring to a distinct entity. Thus they are not
marked as mentions in coreference tasks; in our example the NPs $2.3 million and
the company’s president, are attributive, describing properties of her pay and the
38-year-old; Example (22.27) shows a Chinese example in which the predicate NP
(中国最大的城市; China’s biggest city) is not a mention.
(22.25) her pay jumped to $2.3 million
(22.26) the 38-year-old became the company’s president
(22.27) 上海是[中国最大的城市]
[Shanghai is China’s biggest city]
Expletives:
Many uses of pronouns like it in English and corresponding pronouns
in other languages are not referential. Such expletive or pleonastic cases include
expletive
it is raining, in idioms like hit it off, or in particular syntactic situations like clefts
clefts
(22.28a) or extraposition (22.28b):
(22.28)
a. It was Emma Goldman who founded Mother Earth
b. It surprised me that there was a herring hanging on her wall.
Generics:
Another kind of expression that does not refer back to an entity explic-
itly evoked in the text is generic reference. Consider (22.29).
(22.29) I love mangos. They are very tasty.
Here, they refers, not to a particular mango or set of mangos, but instead to the class
of mangos in general. The pronoun you can also be used generically:
(22.30) In July in San Francisco you have to wear a jacket.
22.1.4
Linguistic Properties of the Coreference Relation
Now that we have seen the linguistic properties of individual referring expressions
we turn to properties of the antecedent/anaphor pair. Understanding these properties
is helpful both in designing novel features and performing error analyses.
Number Agreement:
Referring expressions and their referents must generally
agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plu-
ral, and you is unspeciﬁed for number. So a plural antecedent like the chefs cannot
generally corefer with a singular anaphor like she. However, algorithms cannot
enforce number agreement too strictly. First, semantically plural entities can be re-
ferred to by either it or they:
(22.31) IBM announced a new machine translation product yesterday. They have
been working on it for 20 years.

22.1
•
COREFERENCE PHENOMENA: LINGUISTIC BACKGROUND
423
Second, singular they has become much more common, in which they is used to
singular they
describe singular individuals, often useful because they is gender neutral. Although
recently increasing, singular they is quite old, part of English for many centuries.5
Person Agreement:
English distinguishes between ﬁrst, second, and third person,
and a pronoun’s antecedent must agree with the pronoun in person. Thus a third
person pronoun (he, she, they, him, her, them, his, her, their) must have a third person
antecedent (one of the above or any other noun phrase). However, phenomena like
quotation can cause exceptions; in this example I, my, and she are coreferent:
(22.32) “I voted for Nader because he was most aligned with my values,” she said.
Gender or Noun Class Agreement:
In many languages, all nouns have grammat-
ical gender or noun class6 and pronouns generally agree with the grammatical gender
of their antecedent. In English this occurs only with third-person singular pronouns,
which distinguish between male (he, him, his), female (she, her), and nonpersonal
(it) grammatical genders. Non-binary pronouns like ze or hir may also occur in more
recent texts. Knowing which gender to associate with a name in text can be complex,
and may require world knowledge about the individual. Some examples:
(22.33) Maryam has a theorem. She is exciting. (she=Maryam, not the theorem)
(22.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)
Binding Theory Constraints:
The binding theory is a name for syntactic con-
straints on the relations between a mention and an antecedent in the same sentence
(Chomsky, 1981). Oversimplifying a bit, reﬂexive pronouns like himself and herself
reﬂexive
corefer with the subject of the most immediate clause that contains them (22.35),
whereas nonreﬂexives cannot corefer with this subject (22.36).
(22.35) Janet bought herself a bottle of ﬁsh sauce. [herself=Janet]
(22.36) Janet bought her a bottle of ﬁsh sauce. [her̸=Janet]
Recency:
Entities introduced in recent utterances tend to be more salient than
those introduced from utterances further back. Thus, in (22.37), the pronoun it is
more likely to refer to Jim’s map than the doctor’s map.
(22.37) The doctor found an old map in the captain’s chest. Jim found an even
older map hidden on the shelf. It described an island.
Grammatical Role:
Entities mentioned in subject position are more salient than
those in object position, which are in turn more salient than those mentioned in
oblique positions. Thus although the ﬁrst sentence in (22.38) and (22.39) expresses
roughly the same propositional content, the preferred referent for the pronoun he
varies with the subject—John in (22.38) and Bill in (22.39).
(22.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of
rum. [ he = Billy ]
(22.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of
rum. [ he = Jim ]
5
Here’s a bound pronoun example from Shakespeare’s Comedy of Errors: There’s not a man I meet
but doth salute me As if I were their well-acquainted friend
6
The word “gender” is generally only used for languages with 2 or 3 noun classes, like most Indo-
European languages; many languages, like the Bantu languages or Chinese, have a much larger number
of noun classes.

424
CHAPTER 22
•
COREFERENCE RESOLUTION
Verb Semantics:
Some verbs semantically emphasize one of their arguments, bi-
asing the interpretation of subsequent pronouns. Compare (22.40) and (22.41).
(22.40) John telephoned Bill. He lost the laptop.
(22.41) John criticized Bill. He lost the laptop.
These examples differ only in the verb used in the ﬁrst sentence, yet “he” in (22.40)
is typically resolved to John, whereas “he” in (22.41) is resolved to Bill. This may
be due to the link between implicit causality and saliency: the implicit cause of a
“criticizing” event is its object, whereas the implicit cause of a “telephoning” event
is its subject. In such verbs, the entity which is the implicit cause is more salient.
Selectional Restrictions:
Many other kinds of semantic knowledge can play a role
in referent preference. For example, the selectional restrictions that a verb places on
its arguments (Chapter 20) can help eliminate referents, as in (22.42).
(22.42) I ate the soup in my new bowl after cooking it for hours
There are two possible referents for it, the soup and the bowl. The verb eat, however,
requires that its direct object denote something edible, and this constraint can rule
out bowl as a possible referent.
22.2
Coreference Tasks and Datasets
We can formulate the task of coreference resolution as follows: Given a text T, ﬁnd
all entities and the coreference links between them. We evaluate our task by com-
paring the links our system creates with those in human-created gold coreference
annotations on T.
Let’s return to our coreference example, now using superscript numbers for each
coreference chain (cluster), and subscript letters for individual mentions in the clus-
ter:
(22.43) [Victoria Chen]1
a, CFO of [Megabucks Banking]2
a, saw [[her]1
b pay]3
a jump
to $2.3 million, as [the 38-year-old]1
c also became [[the company]2
b’s
president. It is widely known that [she]1
d came to [Megabucks]2
c from rival
[Lotsabucks]4
a.
Assuming example (22.43) was the entirety of the article, the chains for her pay and
Lotsabucks are singleton mentions:
1. {Victoria Chen, her, the 38-year-old, She}
2. {Megabucks Banking, the company, Megabucks}
3. { her pay}
4. { Lotsabucks}
For most coreference evaluation campaigns, the input to the system is the raw
text of articles, and systems must detect mentions and then link them into clusters.
Solving this task requires dealing with pronominal anaphora (ﬁguring out that her
refers to Victoria Chen), ﬁltering out non-referential pronouns like the pleonastic It
in It has been ten years), dealing with deﬁnite noun phrases to ﬁgure out that the
38-year-old is coreferent with Victoria Chen, and that the company is the same as
Megabucks. And we need to deal with names, to realize that Megabucks is the same
as Megabucks Banking.

22.3
•
MENTION DETECTION
425
Exactly what counts as a mention and what links are annotated differs from task
to task and dataset to dataset. For example some coreference datasets do not label
singletons, making the task much simpler. Resolvers can achieve much higher scores
on corpora without singletons, since singletons constitute the majority of mentions in
running text, and they are often hard to distinguish from non-referential NPs. Some
tasks use gold mention-detection (i.e. the system is given human-labeled mention
boundaries and the task is just to cluster these gold mentions), which eliminates the
need to detect and segment mentions from running text.
Coreference is usually evaluated by the CoNLL F1 score, which combines three
metrics: MUC, B3, and CEAFe; Section 22.7 gives the details.
Let’s mention a few characteristics of one popular coreference dataset, OntoNotes
(Pradhan et al. 2007, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task
based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese
and English coreference datasets of roughly one million words each, consisting of
newswire, magazine articles, broadcast news, broadcast conversations, web data and
conversational speech data, as well as about 300,000 words of annotated Arabic
newswire. The most important distinguishing characteristic of OntoNotes is that
it does not label singletons, simplifying the coreference task, since singletons rep-
resent 60%-70% of all entities. In other ways, it is similar to other coreference
datasets. Referring expression NPs that are coreferent are marked as mentions, but
generics and pleonastic pronouns are not marked. Appositive clauses are not marked
as separate mentions, but they are included in the mention. Thus in the NP, “Richard
Godown, president of the Industrial Biotechnology Association” the mention is the
entire phrase. Prenominal modiﬁers are annotated as separate entities only if they
are proper nouns. Thus wheat is not an entity in wheat ﬁelds, but UN is an entity in
UN policy (but not adjectives like American in American policy).
A number of corpora mark richer discourse phenomena. The ISNotes corpus
annotates a portion of OntoNotes for information status, include bridging examples
(Hou et al., 2018). The AnCora-CO coreference corpus (Recasens and Mart´ı, 2010)
contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-
CO-Ca) news data, and includes labels for complex phenomena like discourse deixis
in both languages. The ARRAU corpus (Uryupina et al., 2019) contains 350,000
words of English marking all NPs, which means singleton clusters are available.
ARRAU includes diverse genres like dialog (the TRAINS data) and ﬁction (the Pear
Stories), and has labels for bridging references, discourse deixis, generics, and am-
biguous anaphoric relations.
22.3
Mention Detection
The ﬁrst stage of coreference is mention detection: ﬁnding the spans of text that
mention
detection
constitute each mention. Mention detection algorithms are usually very liberal in
proposing candidate mentions (i.e., emphasizing recall), and only ﬁltering later. For
example many systems run parsers and named entity taggers on the text and extract
every span that is either an NP, a possessive pronoun, or a named entity.
Doing so from our sample text repeated in (22.44):
(22.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3
million, as the 38-year-old also became the company’s president. It is
widely known that she came to Megabucks from rival Lotsabucks.

426
CHAPTER 22
•
COREFERENCE RESOLUTION
might result in the following list of 13 potential mentions:
Victoria Chen
the company
CFO of Megabucks Banking the company’s president
Megabucks Banking
It
her
she
her pay
Megabucks
$2.3 million
Lotsabucks
the 38-year-old
More recent mention detection systems are even more generous; the span-based
algorithm we will describe in Section 22.6 ﬁrst extracts literally all N-gram spans of
words up to N=10. Of course recall from Section 22.1.3 that many NPs—and the
overwhelming majority of random N-gram spans—are not referring expressions.
Therefore all such mention detection systems need to eventually ﬁlter out pleonas-
tic/expletive pronouns like It above, appositives like CFO of Megabucks Banking
Inc, or predicate nominals like the company’s president or $2.3 million.
Some of this ﬁltering can be done by rules. Early rule-based systems designed
regular expressions to deal with pleonastic it, like the following rules from Lappin
and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe, know, antic-
ipate) to capture pleonastic it in “It is thought that ketchup...”, or modal adjectives
(e.g., necessary, possible, certain, important), for, e.g., “It is likely that I...”. Such
rules are sometimes used as part of modern systems:
It is Modaladjective that S
It is Modaladjective (for NP) to VP
It is Cogv-ed that S
It seems/appears/means/follows (that) S
Mention-detection rules are sometimes designed speciﬁcally for particular eval-
uation campaigns. For OntoNotes, for example, mentions are not embedded within
larger mentions, and while numeric quantities are annotated, they are rarely coref-
erential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a
common ﬁrst pass rule-based mention detection algorithm (Lee et al., 2013) is:
1. Take all NPs, possessive pronouns, and named entities.
2. Remove numeric quantities (100 dollars, 8%), mentions embedded in
larger mentions, adjectival forms of nations, and stop words (like there).
3. Remove pleonastic it based on regular expression patterns.
Rule-based systems, however, are generally insufﬁcient to deal with mention-
detection, and so modern systems incorporate some sort of learned mention detec-
tion component, such as a referentiality classiﬁer, an anaphoricity classiﬁer—
detecting whether an NP is an anaphor—or a discourse-new classiﬁer— detecting
whether a mention is discourse-new and a potential antecedent for a future anaphor.
An anaphoricity detector, for example, can draw its positive training examples
anaphoricity
detector
from any span that is labeled as an anaphoric referring expression in hand-labeled
datasets like OntoNotes, ARRAU, or AnCora. Any other NP or named entity can be
marked as a negative training example. Anaphoricity classiﬁers use features of the
candidate mention such as its head word, surrounding words, deﬁniteness, animacy,
length, position in the sentence/discourse, many of which were ﬁrst proposed in
early work by Ng and Cardie (2002a); see Section 22.5 for more on features.

22.3
•
MENTION DETECTION
427
Referentiality or anaphoricity detectors can be run as ﬁlters, in which only men-
tions that are classiﬁed as anaphoric or referential are passed on to the coreference
system. The end result of such a ﬁltering mention detection system on our example
above might be the following ﬁltered set of 9 potential mentions:
Victoria Chen
her pay
she
Megabucks Bank the 38-year-old Megabucks
her
the company
Lotsabucks
It turns out, however, that hard ﬁltering of mentions based on an anaphoricity
or referentiality classiﬁer leads to poor performance. If the anaphoricity classiﬁer
threshold is set too high, too many mentions are ﬁltered out and recall suffers. If the
classiﬁer threshold is set too low, too many pleonastic or non-referential mentions
are included and precision suffers.
The modern approach is instead to perform mention detection, anaphoricity, and
coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007,
Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),(2018)
system is based on a single end-to-end neural network that computes a score for each
mention being referential, a score for two mentions being coreference, and combines
them to make a decision, training all these scores with a single end-to-end loss. We’ll
describe this method in detail in Section 22.6. 7
Despite these advances, correctly detecting referential mentions seems to still be
an unsolved problem, since systems incorrectly marking pleonastic pronouns like
it and other non-referential NPs as coreferent is a large source of errors of mod-
ern coreference resolution systems (Kummerfeld and Klein 2013, Martschat and
Strube 2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).
Mention, referentiality, or anaphoricity detection is thus an important open area
of investigation. Other sources of knowledge may turn out to be helpful, especially
in combination with unsupervised and semisupervised algorithms, which also mit-
igate the expense of labeled datasets. In early work, for example Bean and Riloff
(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-
tracting and generalizing over the ﬁrst NPs in a text, which are guaranteed to be
non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in
the training data but never appear as gold mentions to help ﬁnd non-referential NPs.
Bergsma et al. (2008b) use web counts as a semisupervised way to augment standard
features for anaphoricity detection for English it, an important task because it is both
common and ambiguous; between a quarter and half it examples are non-anaphoric.
Consider the following two examples:
(22.45) You can make [it] in advance. [anaphoric]
(22.46) You can make [it] in Hollywood. [non-anaphoric]
The it in make it is non-anaphoric, part of the idiom make it. Bergsma et al. (2008b)
turn the context around each example into patterns, like “make * in advance” from
(22.45), and “make * in Hollywood” from (22.46). They then use Google N-grams
to enumerate all the words that can replace it in the patterns. Non-anaphoric contexts
tend to only have it in the wildcard positions, while anaphoric contexts occur with
many other NPs (for example make them in advance is just as frequent in their data
7
Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like
OntoNotes which don’t label singletons, an alternative to ﬁltering out non-referential mentions is to run
coreference resolution, and then simply delete any candidate mentions which were not corefered with
another mention. This likely doesn’t work as well as explicitly modeling referentiality, and cannot solve
the problem of detecting singletons, which is important for tasks like entity linking.

428
CHAPTER 22
•
COREFERENCE RESOLUTION
as make it in advance, but make them in Hollywood did not occur at all). These
N-gram contexts can be used as features in a supervised anaphoricity classiﬁer.
22.4
Architectures for Coreference Algorithms
Modern systems for coreference are based on supervised neural machine learning,
supervised from hand-labeled datasets like OntoNotes. In this section we overview
the various architecture of modern systems, using the categorization of Ng (2010),
which distinguishes algorithms based on whether they make each coreference deci-
sion in a way that is entity-based—representing each entity in the discourse model—
or only mention-based—considering each mention independently, and whether they
use ranking models to directly compare potential antecedents. Afterwards, we go
into more detail on one state-of-the-art algorithm in Section 22.6.
22.4.1
The Mention-Pair Architecture
We begin with the mention-pair architecture, the simplest and most inﬂuential
mention-pair
coreference architecture, which introduces many of the features of more complex
algorithms, even though other architectures perform better. The mention-pair ar-
mention-pair
chitecture is based around a classiﬁer that— as its name suggests—is given a pair
of mentions, a candidate anaphor and a candidate antecedent, and makes a binary
classiﬁcation decision: corefering or not.
Let’s consider the task of this classiﬁer for the pronoun she in our example, and
assume the slightly simpliﬁed set of potential antecedents in Fig. 22.2.
Victoria Chen
Megabucks Banking
her
her pay
the 37-year-old
she
p(coref|”Victoria Chen”,”she”)
p(coref|”Megabucks Banking”,”she”)
Figure 22.2
For each pair of a mention (like she), and a potential antecedent mention (like
Victoria Chen or her), the mention-pair classiﬁer assigns a probability of a coreference link.
For each prior mention (Victoria Chen, Megabucks Banking, her, etc.), the binary
classiﬁer computes a probability: whether or not the mention is the antecedent of
she. We want this probability to be high for actual antecedents (Victoria Chen, her,
the 38-year-old) and low for non-antecedents (Megabucks Banking, her pay).
Early classiﬁers used hand-built features (Section 22.5); more recent classiﬁers
use neural representation learning (Section 22.6)
For training, we need a heuristic for selecting training samples; since most pairs
of mentions in a document are not coreferent, selecting every pair would lead to
a massive overabundance of negative samples. The most common heuristic, from
(Soon et al., 2001), is to choose the closest antecedent as a positive example, and all
pairs in between as the negative examples. More formally, for each anaphor mention
mi we create
• one positive instance (mi,m j) where m j is the closest antecedent to mi, and

22.4
•
ARCHITECTURES FOR COREFERENCE ALGORITHMS
429
• a negative instance (mi,mk) for each mk between mj and mi
Thus for the anaphor she, we would choose (she, her) as the positive example
and no negative examples. Similarly, for the anaphor the company we would choose
(the company, Megabucks) as the positive example and (the company, she) (the com-
pany, the 38-year-old) (the company, her pay) and (the company, her) as negative
examples.
Once the classiﬁer is trained, it is applied to each test sentence in a clustering
step. For each mention i in a document, the classiﬁer considers each of the prior i−1
mentions. In closest-ﬁrst clustering (Soon et al., 2001), the classiﬁer is run right to
left (from mention i−1 down to mention 1) and the ﬁrst antecedent with probability
> .5 is linked to i. If no antecedent has probably > 0.5, no antecedent is selected for
i. In best-ﬁrst clustering, the classiﬁer is run on all i −1 antecedents and the most
probable preceding mention is chosen as the antecedent for i. The transitive closure
of the pairwise relation is taken as the cluster.
While the mention-pair model has the advantage of simplicity, it has two main
problems. First, the classiﬁer doesn’t directly compare candidate antecedents to
each other, so it’s not trained to decide, between two likely antecedents, which one
is in fact better. Second, it ignores the discourse model, looking only at mentions,
not entities. Each classiﬁer decision is made completely locally to the pair, without
being able to take into account other mentions of the same entity. The next two
models each address one of these two ﬂaws.
22.4.2
The Mention-Rank Architecture
The mention ranking model directly compares candidate antecedents to each other,
choosing the highest-scoring antecedent for each anaphor.
In early formulations, for mention i, the classiﬁer decide which of the {1,...,i−
1} prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is
in fact not anaphoric, and none of the antecedents should be chosen? Such a model
would need to run a separate anaphoricity classiﬁer on i. Instead, it turns out to be
better to jointly learn anaphoricity detection and coreference together with a single
loss (Rahman and Ng, 2009).
So in modern mention-ranking systems, for the ith mention (anaphor), we have
an associated random variable yi ranging over the values Y(i) = {1,...,i−1,ϵ}. The
value ϵ is a special dummy mention meaning that i does not have an antecedent (i.e.,
is either discourse-new and starts a new coref chain, or is non-anaphoric).
Victoria Chen
Megabucks Banking
her
her pay
the 37-year-old
she
p(”Victoria Chen”|”she”)
p(ϵ|”she”)
ϵ
One or more 
of these
should be high
All of these
should be low
}
p(”her pay”|she”)
p(”her”|she”) p(”the 37-year-old”|she”)
p(”Megabucks Banking”|she”)
}
Figure 22.3
For each candidate anaphoric mention (like she), the mention-ranking system assigns a proba-
bility distribution over all previous mentions plus the special dummy mention ϵ.
At test time, for a given mention i the model computes one softmax over all the
antecedents (plus ϵ) giving a probability for each candidate antecedent (or none).

430
CHAPTER 22
•
COREFERENCE RESOLUTION
Fig. 22.3 shows an example of the computation for the single candidate anaphor
she.
Once the antecedent is classiﬁed for each anaphor, transitive closure can be run
over the pairwise decisions to get a complete clustering.
Training is trickier in the mention-ranking model than the mention-pair model,
because for each anaphor we don’t know which of all the possible gold antecedents
to use for training. Instead, the best antecedent for each mention is latent; that
is, for each mention we have a whole cluster of legal gold antecedents to choose
from. Early work used heuristics to choose an antecedent, for example choosing the
closest antecedent as the gold antecedent and all non-antecedents in a window of
two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds
of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,
Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent
by summing over all of them, with a loss function that optimizes the likelihood of
all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the
details in Section 22.6.
Mention-ranking models can be implemented with hand-build features or with
neural representation learning (which might also incorporate some hand-built fea-
tures). we’ll explore both directions in Section 22.5 and Section 22.6.
22.4.3
Entity-based Models
Both the mention-pair and mention-ranking models make their decisions about men-
tions. By contrast, entity-based models link each mention not to a previous mention
but to a previous discourse entity (cluster of mentions).
A mention-ranking model can be turned into an entity-ranking model simply
by having the classiﬁer make its decisions over clusters of mentions rather than
individual mentions (Rahman and Ng, 2009).
For traditional feature-based models, this can be done by extracting features over
clusters. The size of a cluster is a useful features, as is its ‘shape’, which is the
list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun, so that a cluster composed of {Victoria, her, the
38-year-old} would have the shape P-Pr-D (Bj¨orkelund and Kuhn, 2014). An entity-
based model that includes a mention-pair classiﬁer can use as features aggregates of
mention-pair probabilities, for example computing the average probability of coref-
erence over all mention-pairs in the two clusters (Clark and Manning 2015).
Neural models can learn representations of clusters automatically, for example
by using an RNN over the sequence of cluster mentions to encode a state correspond-
ing to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-
resentations for pairs of clusters by pooling over learned representations of mention
pairs (Clark and Manning, 2016b).
However, although entity-based models are more expressive, the use of cluster-
level information in practice has not led to large gains in performance, so mention-
ranking models are still more commonly used.
22.5
Classiﬁers using hand-built features
Hand-designed features play an important role in coreference, whether as the sole
input to classiﬁcation in pre-neural classiﬁers, or as augmentations to the automatic

22.6
•
A NEURAL MENTION-RANKING ALGORITHM
431
representation learning used in state-of-the-art neural systems like the one we’ll de-
scribe in Section 22.6.
In this section we describe features commonly used in logistic regression, SVM,
or random forest classiﬁers for coreference resolution.
Given an anaphor mention and a potential antecedent mention, most feature
based classiﬁers make use of three types of features: (i) features of the anaphor, (ii)
features of the candidate antecedent, and (iii) features of the relationship between
the pair. Entity-based models can make additional use of two additional classes: (iv)
feature of all mentions from the antecedent’s entity cluster, and (v) features of the
relation between the anaphor and the mentions in the antecedent entity cluster.
Figure 22.4 shows a selection of commonly used features, and shows the value
that would be computed for the potential anaphor ”she” and potential antecedent
”Victoria Chen” in our example sentence, repeated below:
(22.47) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3
million, as the 38-year-old also became the company’s president. It is
widely known that she came to Megabucks from rival Lotsabucks.
Features that prior work has found to be particularly useful are exact string
match, entity headword agreement, mention distance, as well as (for pronouns) exact
attribute match and i-within-i, and (for nominals and proper names) word inclusion
and cosine. For lexical features (like head words) it is common to only use words that
appear enough times (perhaps more than 20 times), backing off to parts of speech
for rare words.
It is crucial in feature-based systems to use conjunctions of features; one exper-
iment suggested that moving from individual features in a classiﬁer to conjunctions
of multiple features increased F1 by 4 points (Lee et al., 2017a). Speciﬁc conjunc-
tions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be
conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned auto-
matically, either by using classiﬁers like decision trees or random forests ((Ng and
Cardie, 2002a), Lee et al. 2017a) or by using neural models to take raw, unconjoined
features as input, and automatically learn intermediate representations (Wiseman
et al., 2015).
Finally, some of these features can also be used in neural models as well. Neural
systems of the kind we describe in the next section make use of contextual word
embeddings, so they don’t beneﬁt from adding shallow features like string or head
match, grammatical role, or mention types. However features like mention length,
distance between mentions, or genre can complement contextual word embedding
models nicely.
22.6
A neural mention-ranking algorithm
In this section we describe the neural mention-ranking system of Lee et al. (2017b).
This end-to-end system doesn’t exactly have a separate mention-detection step. In-
stead, it considers every possible span of text up to a set length (i.e. all n-grams of
length 1,2,3...N) as a possible mention.8
8
But because this number of potential mentions makes the algorithm very slow and unwieldy (the
model’s size is O(t4) in document length) in practice various versions of the algorithm ﬁnd ways to prune
the possible mentions, essentially using a mention score as something of a mention-detector.

432
CHAPTER 22
•
COREFERENCE RESOLUTION
Features of the Anaphor or Antecedent Mention
First (last) word
Victoria/she
First or last word (or embedding) of antecedent/anaphor
Head word
Victoria/she
Head word (or head embedding) of antecedent/anaphor
Attributes
Sg-F-A-3-
PER/Sg-F-A-
3-PER
The number, gender, animacy, person, named entity type
attributes of (antecedent/anaphor)
Length
2/1
length in words of (antecedent/anaphor)
Grammatical role
Sub/Sub
The grammatical role—subject, direct object, indirect
object/PP—of (antecedent/anaphor)
Mention type
P/Pr
Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an-
tecedent/anaphor
Features of the Antecedent Entity
Entity shape
P-Pr-D
The ‘shape’ or list of types of the mentions in the
antecedent entity (cluster), i.e., sequences of (P)roper,
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.
Entity attributes
Sg-F-A-3-
PER
The number, gender, animacy, person, named entity type
attributes of the antecedent entity
Antecedent
cluster
size
3
Number of mentions in the antecedent cluster
Features of the Pair of Mentions
Longer anaphor
F
True of anaphor is longer than antecedent
Pairs of any features Victoria/she,
2/1, Sub/Sub,
P/Pr, etc .
For each individual feature, pair of type of antecedent+
type of anaphor
Sentence distance
1
The number of sentences between antecedent and anaphor
Mention distance
4
The number of mentions between antecedent and anaphor
i-within-i
F
Anaphor has i-within-i relation with antecedent
Cosine
Cosine between antecedent and anaphor embeddings
Appositive
F
True if the anaphor is in the syntactic apposition relation
to the antecedent. This can be useful even if appositives
are not mentions (to know to attach the appositive to a
preceding head)
Features of the Pair of Entities
Exact String Match
F
True if the strings of any two mentions from the antecedent
and anaphor clusters are identical.
Head Word Match
F
True if any mentions from antecedent cluster has same
headword as any mention in anaphor cluster
Word Inclusion
F
Words in antecedent cluster includes all words in anaphor
cluster
Features of the Document
Genre/source
N
The document genre— (D)ialog, (N)ews, etc,
Figure 22.4
Some common features for feature-based coreference algorithms, with values for the anaphor
“she” and potential antecedent “Victoria Chen”.
Given a document D with T words, the model considers all of the N = T(T−1)
2
text spans up to some length (in the version of Lee et al. (2018), that length is 10).
Each span i starts at word START(i) and ends at word END(i).
The task is to assign to each span i an antecedent yi, a random variable ranging
over the values Y(i) = {1,...,i −1,ϵ}; each previous span and a special dummy
token ϵ. Choosing the dummy token means that i does not have an antecedent, either

22.6
•
A NEURAL MENTION-RANKING ALGORITHM
433
because i is discourse-new and starts a new coreference chain, or because i is non-
anaphoric.
For each pair of spans i and j, the system assigns a score s(i, j) for the corefer-
ence link between span i and span j, The system then learns a distribution P(yi) over
the antecedents for span i:
P(yi) =
exp(s(i,yi))
P
y′∈Y(i) exp(s(i,yi))
(22.48)
This score s(i, j) includes three factors: m(i); whether span i is a mention; m(j);
whether span j is a mention; and c(j); whether j is the antecedent of i:
s(i, j) = m(i)+m(j)+c(i, j)
(22.49)
For the dummy antecedent ϵ, the score s(i,ϵ) is ﬁxed to 0. This way if any non-
dummy scores are positive, the model predicts the highest-scoring antecedent, but if
all the scores are negative it abstains.
The scoring functions m(i) and c(i, j) are based on a vector gi that represents
span i:
m(i) = wm ·FFNNm(gi)
(22.50)
c(i, j) = wc ·FFNNc([gi,gj,gi ◦gj,φ(i, j)])
(22.51)
The antecedent score c(i, j) takes as input a representation of the spans i and j, but
also the element-wise similarity of the two spans to each other gi ◦g j (here ◦is
element-wise multiplication). The antecedent score c also considers a feature vec-
tor φ(i, j) that encodes useful features like mention distances, and also information
about the speaker and genre.
The span representations gi themselves consist of two parts: a contextual repre-
sentation of the ﬁrst and last word in the span, and a representation of the headword
of the span. The contextual representations of the ﬁrst and last words of each span.
are computed by a standard biLSTM. The biLSTM takes as input a representation
wt for each word, based on contextual word embeddings like ELMo. (Using BERT
instead of ELMo results in even higher performance (Joshi et al., 2019)). The output
of the biLSTM for each word wt of the input is ht:
−→h t = LSTMforward(−→h t−1,wt)
←−h t = LSTMforward(←−h t+1,wt)
ht = [−→h t,←−h t]
(22.52)
The system uses independent LSTMs for each sentence.
The system uses attention (Chapter 10) over the words in the span to represent
the span’s head. As is usual with attention, the system learns a weight vector wα,
and computes its dot product with the hidden state ht transformed by a FFNN:
αt = wα · FFNNα(ht)
(22.53)
The attention score is normalized into a distribution via a softmax:
ai,t =
exp(αt)
PEND(i)
k=START(i) exp(αk)
(22.54)

434
CHAPTER 22
•
COREFERENCE RESOLUTION
And then the attention distribution is used to create a vector hATT(i) which is an
attention-weighted sum of words in span i:
hATT(i) =
END(i)
X
t=START(i)
ai,t ·wt
(22.55)
Each span i is then represented by a vector gi, a concatenation of the hidden rep-
resentations of the start and end tokens of the span, the head, and a feature vector
containing only one feature: the length of span i.
gi = [hSTART(i),hEND(i),hATT(i),φ(i)]
(22.56)
Fig. 22.5 from Lee et al. (2017b) shows the computation of the span representation
and the mention score.
General
Electric
said
the
Postal
Service
contacted
the
company
General Electric
+
Electric said the
+
the Postal Service
+
Service contacted the
+
the company
+
Mention score (m)
Span representation (g)
Span head (hatt)
Bidirectional LSTM (h)
Input word embeddings  
(ELMo)
Figure 22.5
Computation of the span representation and the mention score in the end-to-
end coreference model of Lee et al. (2017b). The model considers all spans up to a maximum
width; the ﬁgure shows a small subset of these. Figure after Lee et al. (2017b).
Fig. 22.6 shows the computation of the score s for the three possible antecedents
of the company in the example sentence from Fig. 22.5.
General Electric the Postal Service
the company
s(the company,
General Electric)
s(the company,
the Postal Service)
s(the company, ϵ) = 0
Softmax (P(yi | D))
Coreference
score (s)
Antecedent score (sa)
Mention score (sm)
Span
representation (g)
(m)
(c)
Figure 22.6
The computation of the score s for the three possible antecedents of the com-
pany in the example sentence from Fig. 22.5. Figure after Lee et al. (2017b).
At inference time, some method is generally used to prune the mentions (for
example using the mention score m as a ﬁlter to keep only the best few mentions
as a function like 0.4T of the sentence length T). Then the joint distribution of

22.7
•
EVALUATION OF COREFERENCE RESOLUTION
435
antecedents for each document is computed in a forward pass. Finally, we can then
do transitive closure on the antecedents to create a ﬁnal clustering for the document.
For training, we don’t have a single gold antecedent for each mention; instead
the coreference labeling only gives us each entire cluster of coreferent mentions, and
a mention has a latent antecedent. We therefor use a loss function that maximizes
the sum of the coreference probability of any of the legal antecedents. For a given
mention i with possible antecedents Y(i), let GOLD(i) be the set of mentions in the
gold cluster containing i. Since the set of mentions occurring before i is Y(i), the
set of mentions in that gold cluster that also occur before i is Y(i) ∩GOLD(i). We
therefore want to maximize:
X
ˆy∈Y(i)∩GOLD(i)
P(ˆy)
(22.57)
If a mention i is not in a gold cluster GOLD(i) = ϵ.
To turn this probability into a loss function, we’ll use the cross-entropy loss
function we deﬁned in Eq. 5.10 in Chapter 5, by taking the −log of the probability.
If we then sum over all mentions, we get the ﬁnal loss function for training:
L =
N
X
i=2
−log
X
ˆy∈Y(i)∩GOLD(i)
P(ˆy)
(22.58)
Fig. 22.7 shows example predictions from the model, showing the attention weights,
which Lee et al. (2017b) ﬁnd correlate with traditional semantic heads. Note that
the model gets the second example wrong, presumably because attendants and pilot
likely have nearby word embeddings.
We are looking for (a region of central Italy bordering the Adriatic Sea). (The area) is mostly
mountainous and includes Mt. Corno, the highest peak of the Apennines. (It) also includes a lot of
sheep, good clean-living, healthy sheep, and an Italian entrepreneur has an idea about how to make a
little money of them.
(The ﬂight attendants) have until 6:00 today to ratify labor concessions. (The pilots’) union and ground
crew did so yesterday.
Figure 22.7
Sample predictions from the Lee et al. (2017b) model, with one cluster per
example, showing one correct example and one mistake. Bold, parenthesized spans are men-
tions in the predicted cluster. The amount of red color on a word indicates the head-ﬁnding
attention weight ai,t in (22.54). Figure adapted from Lee et al. (2017b).
22.7
Evaluation of Coreference Resolution
We evaluate coreference algorithms model-theoretically, comparing a set of hypoth-
esis chains or clusters H produced by the system against a set of gold or reference
chains or clusters R from a human labeling, and reporting precision and recall.
However, there are a wide variety of methods for doing this comparison. In fact,
there are 5 common metrics used to evaluate coreference algorithms: the link based
MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)
metrics, the mention based B3 metric (Bagga and Baldwin, 1998), the entity based
CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and
Strube, 2016).

436
CHAPTER 22
•
COREFERENCE RESOLUTION
Let’s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)
MUC
F-measure
is based on the number of coreference links (pairs of mentions) common to H and
R. Precision is the number of common links divided by the number of links in H.
Recall is the number of common links divided by the number of links in R; This
makes MUC biased toward systems that produce large chains (and fewer entities),
and it ignores singletons, since they don’t involve links.
B3 is mention-based rather than link-based. For each mention in the reference
B3
chain, we compute a precision and recall, and then we take a weighted sum over all
N mentions in the document to compute a precision and recall for the entire task. For
a given mention i, let R be the reference chain that includes i, and H the hypothesis
chain that has i. The set of correct mentions in H is H ∩R. Precision for mention i
is thus |H∩R
|H| , and recall for mention i thus |H∩R
|R| . The total precision is the weighted
sum of the precision for mention i, weighted by a weight wi. The total recall is the
weighted sum of the recall for mention i, weighted by a weight wi. Equivalently:
Precision =
N
X
i=1
wi
# of correct mentions in hypothesis chain containing entityi
# of mentions in hypothesis chain containing entityi
Recall =
N
X
i=1
wi
# of correct mentions in hypothesis chain containing entityi
# of mentions in reference chain containing entityi
The weight wi for each entity can be set to different values to produce different
versions of the algorithm.
Following a proposal from Denis and Baldridge (2009), the CoNLL coreference
competitions were scored based on the average of MUC, CEAF-e, and B3 (Pradhan
et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns
to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed
description of the entire set of metrics; reference implementations of these should
be used rather than attempting to reimplement from scratch (Pradhan et al., 2014).
Alternative metrics have been proposed that deal with particular coreference do-
mains or tasks. For example, consider the task of resolving mentions to named
entities (persons, organizations, geopolitical entities), which might be useful for in-
formation extraction or knowledge base completion. A hypothesis chain that cor-
rectly contains all the pronouns referring to an entity, but has no version of the name
itself, or is linked with a wrong name, is not useful for this task. We might instead
want a metric that weights each mention by how informative it is (with names being
most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to
match a gold chain only if it contains at least one variant of a name (the NEC F1
metric of Agarwal et al. (2019)).
22.8
Entity Linking
The task of entity linking (Ji and Grishman, 2011), closely related to coreference,
entity linking
is to associate a mention in text with the representation of some real-world entity
in an ontology, a list of entities in the world, like a gazeteer (Chapter 16). Perhaps
the most common ontology used for this task is Wikipedia, in which each Wikipedia
page acts as the unique id for a particular entity. Thus the entity linking task of
wikiﬁcation (Mihalcea and Csomai, 2007) is the task of deciding which Wikipedia
wikiﬁcation
page corresponding to an individual is being referred to by a mention. We’ll consider

22.9
•
WINOGRAD SCHEMA PROBLEMS
437
that task for the rest of this section, but see Ling et al. (2015b) on different linking
tasks and datasets.
Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne
and Witten 2008), entity linking is done in two stages: mention detection and
mention disambiguation. A very useful feature for mention detection is what Mi-
halcea and Csomai (2007) called a key phrase: the mapping between Wikipedia
anchor texts (the hyperlinked span of text associated with a URL, like Stanford
anchor texts
University, Stanford, or Governor Stanford) and the Wikipedia page title it links
to (Stanford University, or Leland Stanford). Prebuilt dictionaries of these
anchor text/title page links are available (Spitkovsky and Chang, 2012). Mention
detection steps also often include various kinds of query expansion, for example by
doing coreference resolution on the current document. Mention disambiguation is
often done by supervised learning
Coreference can help entity linking, by giving more possible surface forms to
help link to the right Wikipedia page. But entity linking can also be used in the other
direction, to improve coreference resolution. Consider this example from Hajishirzi
et al. (2013):
(22.59) [Michael Eisner]1 and [Donald Tsang]2 announced the grand opening of
[[Hong Kong]3 Disneyland]4 yesterday. [Eisner]1 thanked [the President]2
and welcomed [fans]5 to [the park]4.
Integrating entity linking into coreference can help draw encyclopedic knowl-
edge (like the fact that Donald Tsang is a president) to help disambiguate the men-
tion the President. Ponzetto and Strube (2006) (2007) and Ratinov and Roth (2012)
showed that such attributes extracted from Wikipedia pages could be used to build
richer models of entity mentions in coreference. More recent research shows how to
do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even
jointly with named entity tagging as well (Durrett and Klein 2014).
22.9
Winograd Schema problems
From early on in the ﬁeld, researchers have noted that some cases of coreference
are quite difﬁcult, seeming to require world knowledge or sophisticated reasoning
to solve. The problem was most famously pointed out by Winograd (1972) with the
following example:
(22.60) The city council denied the demonstrators a permit because
a. they feared violence.
b. they advocated violence.
Winograd noticed that the antecedent that most readers preferred for the pro-
noun they in continuation (a) was the city council, but in (b) was the demonstrators.
He suggested that this requires understanding that the second clause is intended
as an explanation of the ﬁrst clause, and also that our cultural frames suggest that
city councils are perhaps more likely than demonstrators to fear violence and that
demonstrators might be more likely to advocate violence.
In an attempt to get the ﬁeld of NLP to focus more on methods involving world
knowledge and common sense reasoning, Levesque (2011) proposed a challenge
task called the Winograd Schema Challenge.9 The problems in the challenge task
Winograd
schema

438
CHAPTER 22
•
COREFERENCE RESOLUTION
are coreference problems designed to be easily disambiguated by the human reader,
but hopefully not solvable by simple techniques such as selectional restrictions, or
other basic word association methods.
The problems are framed as a pair of statements that differ in a single word or
phrase, and a coreference question:
(22.61) The trophy didn’t ﬁt into the suitcase because it was too large.
Question: What was too large? Answer: The trophy
(22.62) The trophy didn’t ﬁt into the suitcase because it was too small.
Question: What was too small? Answer: The suitcase
The problems have the following characteristics:
1. The problems each have two parties
2. A pronoun preferentially refers to one of the parties, but could grammatically
also refer to the other
3. A question asks which party the pronoun refers to
4. If one word in the question is changed, the human-preferred answer changes
to the other party
The kind of world knowledge that might be needed to solve the problems can
vary. In the trophy/suitcase example, it is knowledge about the physical world; that
a bigger object cannot ﬁt into a smaller object. In the original Winograd sentence,
it is stereotypes about social actors like politicians and protesters. In examples like
the following, it is knowledge about human actions like turn-taking or thanking.
(22.63) Bill passed the gameboy to John because his turn was [over/next]. Whose
turn was [over/next]? Answers: Bill/John
(22.64) Joan made sure to thank Susan for all the help she had [given/received].
Who had [given/received] help? Answers: Susan/Joan.
Although the Winograd Schema was designed to require common-sense rea-
soning, a large percentage of the original set of problem can be solved by pre-
trained language models, ﬁne-tuned on Winograd Schema sentences (Kocijan et al.,
2019). Large pre-trained language models encode an enormous amount of world or
common-sense knowledge! The current trend is therefore to propose new datasets
with increasingly difﬁcult Winograd-like coreference resolution problems like KNOWREF
(Emami et al., 2019), with examples like:
(22.65) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the
gap wasn’t all that big.
In the end, it seems likely that some combination of language modeling and knowl-
edge will prove fruitful; indeed, it seems that knowledge-based models overﬁt less
to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),
22.10
Gender Bias in Coreference
As with other aspects of language processing, coreference models exhibit gender
and other biases (Zhao et al. 2018a, Rudinger et al. 2018, Webster et al. 2018).
9
Levesque’s call was quickly followed up by Levesque et al. (2012) and Rahman and Ng (2012), a
competition at the IJCAI conference (Davis et al., 2017), and a natural language inference version of the
problem called WNLI (Wang et al., 2018).

22.10
•
GENDER BIAS IN COREFERENCE
439
For example the WinoBias dataset (Zhao et al., 2018a) uses a variant of the Wino-
grad Schema paradigm to test the extent to which coreference algorithms are biased
toward linking gendered pronouns with antecedents consistent with cultural stereo-
types. As we summarized in Chapter 6, embeddings replicate societal biases in their
training test, such as associating men with historically sterotypical male occupa-
tions like doctors, and women with stereotypical female occupations like secretaries
(Caliskan et al. 2017, Garg et al. 2018).
A WinoBias sentence contain two mentions corresponding to stereotypically-
male and stereotypically-female occupations and a gendered pronoun that must be
linked to one of them. The sentence cannot be disambiguated by the gender of the
pronoun, but a biased model might be distracted by this cue. Here is an example
sentence:
(22.66) The secretary called the physiciani and told himi about a new patient
[pro-stereotypical]
(22.67) The secretary called the physiciani and told heri about a new patient
[anti-stereotypical]
Zhao et al. (2018a) consider a coreference system to be biased if it is more accu-
rate at linking pronouns consistent with gender stereotypical occupations (e.g., him
with physician in (22.66)) than linking pronouns inconsistent with gender-stereotypical
occupations (e.g., her with physician in (22.67)). They show that coreference sys-
tems of all architectures (rule-based, feature-based machine learned, and end-to-
end-neural) all show signiﬁcant bias, performing on average 21 F1 points worse in
the anti-stereotypical cases.
One possible source of this bias is that female entities are signiﬁcantly un-
derrepresented in the OntoNotes dataset, used to train most coreference systems.
Zhao et al. (2018a) propose a way to overcome this bias: they generate a second
gender-swapped dataset in which all male entities in OntoNotes are replaced with
female ones and vice versa, and retrain coreference systems on the combined orig-
inal and swapped OntoNotes data, also using debiased GloVE embeddings (Boluk-
basi et al., 2016). The resulting coreference systems no longer exhibit bias on the
WinoBias dataset, without signiﬁcantly impacting OntoNotes coreference accuracy.
In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo
contextualized word vector representations and coref systems that use them. They
showed that retraining ELMo with data augmentation again reduces or removes bias
in coreference systems on WinoBias.
Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered
Pronoun Resolution as a tool for developing improved coreference algorithms for
gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences
with gendered ambiguous pronouns (by contrast, only 20% of the gendered pro-
nouns in the English OntoNotes training data are feminine). The examples were
created by drawing on naturally occurring sentences from Wikipedia pages to create
hard to resolve cases with two named entities of the same gender and an ambiguous
pronoun that may refer to either person (or neither), like the following:
(22.68) In May, Fujisawa joined Mari Motohashi’s rink as the team’s skip, moving
back from Karuizawa to Kitami where she had spent her junior days.
Webster et al. (2018) show that modern coreference algorithms perform signif-
icantly worse on resolving feminine pronouns than masculine pronouns in GAP.
Kurita et al. (2019) shows that a system based on BERT contextualized word repre-
sentations shows similar bias.

440
CHAPTER 22
•
COREFERENCE RESOLUTION
22.11
Summary
This chapter introduced the task of coreference resolution.
• This is the task of linking together mentions in text which corefer, i.e. refer
to the same discourse entity in the discourse model, resulting in a set of
coreference chains (also called clusters or entities).
• Mentions can be deﬁnite NPs or indeﬁnite NPs, pronouns (including zero
pronouns) or names.
• The surface form of an entity mention is linked to its information status
(new, old, or inferrable), and how accessible or salient the entity is.
• Some NPs are not referring expressions, such as pleonastic it in It is raining.
• Many corpora have human-labeled coreference annotations that can be used
for supervised learning, including OntoNotes for English, Chinese, and Ara-
bic, ARRAU for English, and AnCora for Spanish and Catalan.
• Mention detection can start with all nouns and named entities and then use
anaphoricity classiﬁers or referentiality classiﬁers to ﬁlter out non-mentions.
• Three common architectures for coreference are mention-pair, mention-rank,
and entity-based, each of which can make use of feature-based or neural clas-
siﬁers.
• Modern coreference systems tend to be end-to-end, performing mention de-
tection and coreference in a single end-to-end architecture.
• Algorithms learn representations for text spans and heads, and learn to com-
pare anaphor spans with candidate antecedent spans.
• Coreference systems are evaluated by comparing with gold entity labels using
precision/recall metrics like MUC, B3, CEAF, BLANC, or LEA.
• The Winograd Schema Challenge problems are difﬁcult coreference prob-
lems that seem to require world knowledge or sophisticated reasoning to solve.
• Coreference systems exhibit gender bias which can be evaluated using datasets
like Winobias and GAP.
Bibliographical and Historical Notes
Coreference has been part of natural language understanding since the 1970s (Woods
et al. 1972, Winograd 1972). The discourse model and the entity-centric foundation
of coreference was formulated by Karttunen (1969) (at the 3rd COLING confer-
ence), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But it
was Bonnie Webber’s (1978) dissertation and following work (Webber 1983) that
explored the model’s computational aspects, providing fundamental insights into
how entities are represented in the discourse model and the ways in which they can
license subsequent reference. Many of the examples she provided continue to chal-
lenge theories of reference to this day.
The Hobbs algorithm10 is a tree-search algorithm that was the ﬁrst in a long
Hobbs
algorithm
series of syntax-based methods for identifying reference robustly in naturally occur-
ring text. The input to the Hobbs algorithm is a pronoun to be resolved, together
10 The simpler of two algorithms presented originally in Hobbs (1978).

BIBLIOGRAPHICAL AND HISTORICAL NOTES
441
with a syntactic (constituency) parse of the sentences up to and including the cur-
rent sentence. The details of the algorithm depend on the grammar used, but can be
understand from a a simpliﬁed version due to Kehler et al. (2004) that just searches
through the list of NPs in the current and prior sentences. This simpliﬁed Hobbs
algorithm searches NPs in the following order: “(i) in the current sentence from
right-to-left, starting with the ﬁrst NP to the left of the pronoun, (ii) in the previous
sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in
the current sentence from left-to-right, starting with the ﬁrst noun group to the right
of the pronoun (for cataphora). The ﬁrst noun group that agrees with the pronoun
with respect to number, gender, and person is chosen as the antecedent” (Kehler
et al., 2004).
Lappin and Leass (1994) was an inﬂuential entity-based system that used weights
to combine syntactic and other features, extended soon after by Kennedy and Bogu-
raev (1996) whose system avoids the need for full syntactic parses.
Approximately contemporaneously centering (Grosz et al., 1995) was applied to
pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of work
followed focused on centering’s use in coreference (Kameyama 1986, Di Euge-
nio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler 1997a,
Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how centering can
be integrated with coherence-driven theories of pronoun interpretation. See Chap-
ter 23 for the use of centering in measuring discourse coherence.
Coreference competitions as part of the US DARPA-sponsored MUC confer-
ences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC-
7 corpora), and set the tone for much later work, choosing to focus exclusively
on the simplest cases of identity coreference (ignoring difﬁcult cases like bridging,
metonymy, and part-whole) and drawing the community toward supervised machine
learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE eval-
uations produced labeled coreference corpora in English, Chinese, and Arabic that
were widely used for model training and evaluation.
This DARPA work inﬂuenced the community toward supervised learning be-
ginning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy
and Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended
by Ng and Cardie (2002b), and a series of machine learning models followed over
the next 15 years. These often focused separately on pronominal anaphora resolu-
tion (Kehler et al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and
Wagstaff 1999, Ng and Cardie 2002b, Ng 2005a) and deﬁnite NP reference (Poesio
and Vieira 1998, Vieira and Poesio 2000), as well as separate anaphoricity detection
(Bean and Riloff 1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or
singleton detection (de Marneffe et al., 2015).
The move from mention-pair to mention-ranking approaches was pioneered by
Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,
then extended by Denis and Baldridge (2008) who proposed to do ranking via a soft-
max over all prior mentions. The idea of doing mention detection, anaphoricity, and
coreference jointly in a single end-to-end model grew out of the early proposal of Ng
(2005b) to use a dummy antecedent for mention-ranking, allowing ‘non-referential’
to be a choice for coreference classiﬁers, Denis and Baldridge’s (2007) joint sys-
tem combining anaphoricity classiﬁer probabilities with coreference probabilities,
the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009)
proposal to train the two models jointly with a single objective.
Simple rule-based systems for coreference returned to prominence in the 2010s,

442
CHAPTER 22
•
COREFERENCE RESOLUTION
partly because of their ability to encode entity-based features in a high-precision
way (Zhou et al. 2004, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee
et al. 2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered
from an inability to deal with the semantics necessary to correctly handle cases of
common noun coreference.
A return to supervised learning led to a number of advances in mention-ranking
models which were also extended into neural architectures, for example using re-
inforcement learning to directly optimize coreference evaluation models Clark and
Manning (2016a), doing end-to-end coreference all the way from span extraction
(Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take
advantage of global entity-level information (Clark and Manning 2016b, Wiseman
et al. 2016, Lee et al. 2018).
The coreference task as we introduced it involves a simplifying assumption that
the relationship between an anaphor and its antecedent is one of identity: the two
corefering mentions refer to the identical discourse referent. In real texts, the rela-
tionship can be more complex, where different aspects of a discourse referent can
be neutralized or refocused. For example (22.69) (Recasens et al., 2011) shows an
example of metonymy, in which the capital city Washington is used metonymically
metonymy
to refer to the US. (22.70-22.71) show other examples (Recasens et al., 2011):
(22.69) a strict interpretation of a policy requires The U.S. to notify foreign
dictators of certain coup plots ... Washington rejected the bid ...
(22.70) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New
Year. In the South, everyone was celebrating New Year; to the North, it
was a regular day.
(22.71) In France, the president is elected for a term of seven years, while in the
United States he is elected for a term of four years.
For further linguistic discussions of these complications of coreference see Puste-
jovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and
Turner (2008), Versley (2008), and Barker (2010).
Ng (2017) offers a useful compact history of machine learning models in coref-
erence resolution. There are three excellent book-length surveys of anaphora/coref-
erence resolution, covering different time periods: Hirst (1981) (early work until
about 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).
Exercises

CHAPTER
23
Discourse Coherence
And even in our wildest and most wandering reveries, nay in our very dreams,
we shall ﬁnd, if we reﬂect, that the imagination ran not altogether at adven-
tures, but that there was still a connection upheld among the different ideas,
which succeeded each other. Were the loosest and freest conversation to be
transcribed, there would immediately be transcribed, there would immediately
be observed something which connected it in all its transitions.
David Hume, An enquiry concerning human understanding, 1748
Orson Welles’ movie Citizen Kane was groundbreaking in many ways, perhaps most
notably in its structure. The story of the life of ﬁctional media magnate Charles
Foster Kane, the movie does not proceed in chronological order through Kane’s
life. Instead, the ﬁlm begins with Kane’s death (famously murmuring “Rosebud”)
and is structured around ﬂashbacks to his life inserted among scenes of a reporter
investigating his death. The novel idea that the structure of a movie does not have
to linearly follow the structure of the real timeline made apparent for 20th century
cinematography the inﬁnite possibilities and impact of different kinds of coherent
narrative structures.
But coherent structure is not just a fact about movies or works of art. Like
movies, language does not normally consist of isolated, unrelated sentences, but
instead of collocated, structured, coherent groups of sentences. We refer to such
a coherent structured group of sentences as a discourse, and we use the word co-
discourse
herence to refer to the relationship between sentences that makes real discourses
coherence
different than just random assemblages of sentences. The chapter you are now read-
ing is an example of a discourse, as is a news article, a conversation, a thread on
social media, a Wikipedia page, and your favorite novel.
What makes a discourse coherent? If you created a text by taking a random
sentences each from many different sources and pasted them together, would that
be a coherent discourse? Almost certainly not. Real discourses exhibit both local
local
coherence and global coherence. Let’s consider three ways in which real discourses
global
are locally coherent;
First, sentences or clauses in real discourses are related to nearby sentences in
systematic ways. Consider this example from Hobbs (1979):
(23.1) John took a train from Paris to Istanbul. He likes spinach.
This sequence is incoherent because it is unclear to a reader why the second
sentence follows the ﬁrst; what does liking spinach have to do with train trips? In
fact, a reader might go to some effort to try to ﬁgure out how the discourse could be
coherent; perhaps there is a French spinach shortage? The very fact that hearers try
to identify such connections suggests that human discourse comprehension involves
the need to establish this kind of coherence.
By contrast, in the following coherent example:
(23.2) Jane took a train from Paris to Istanbul. She had to attend a conference.

444
CHAPTER 23
•
DISCOURSE COHERENCE
the second sentence gives a REASON for Jane’s action in the ﬁrst sentence. Struc-
tured relationships like REASON that hold between text units are called coherence
relations, and coherent discourses are structured by many such coherence relations.
coherence
relations
Coherence relations are introduced in Section 23.1.
A second way a discourse can be locally coherent is by virtue of being “about”
someone or something. In a coherent discourse some entities are salient, and the
discourse focuses on them and doesn’t go back and forth between multiple entities.
This is called entity-based coherence. Consider the following incoherent passage,
in which the salient entity seems to wildly swing from John to Jenny to the piano
store to the living room, back to Jenny, then the piano again:
(23.3) John wanted to buy a piano for his living room.
Jenny also wanted to buy a piano.
He went to the piano store.
It was nearby.
The living room was on the second ﬂoor.
She didn’t ﬁnd anything she liked.
The piano he bought was hard to get up to that ﬂoor.
Entity-based coherence models measure this kind of coherence by tracking salient
entities across a discourse. For example Centering Theory (Grosz et al., 1995), the
Centering
Theory
most inﬂuential theory of entity-based coherence, keeps track of which entities in
the discourse model are salient at any point (salient entities are more likely to be
pronominalized or to appear in prominent syntactic positions like subject or object).
In Centering Theory, transitions between sentences that maintain the same salient
entity are considered more coherent than ones that repeatedly shift between entities.
The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly-
entity grid
used model that realizes some of the intuitions of the Centering Theory framework.
Entity-based coherence is introduced in Section 23.3.
Finally, discourses can be locally coherent by being topically coherent: nearby
topically
coherent
sentences are generally about the same topic and use the same or similar vocab-
ulary to discuss these topics. Because topically coherent discourses draw from a
single semantic ﬁeld or topic, they tend to exhibit the surface property known as
lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semanti-
lexical cohesion
cally related words in nearby sentences. For example, the fact that the words house,
chimney, garret, closet, and window— all of which belong to the same semantic
ﬁeld— appear in the two sentences in (23.4), or that they share the identical word
shingled, is a cue that the two are tied together as a discourse:
(23.4)
Before winter I built a chimney, and shingled the sides of my house...
I have thus a tight shingled and plastered house... with a garret and a
closet, a large window on each side....
In addition to the local coherence between adjacent or nearby sentences, dis-
courses also exhibit global coherence. Many genres of text are associated with
particular conventional discourse structures. Academic articles might have sections
describing the Methodology or Results. Stories might follow conventional plotlines
or motifs. Persuasive essays have a particular claim they are trying to argue for,
and an essay might express this claim together with a structured set of premises that
support the argument and demolish potential counterarguments. We’ll introduce
versions of each of these kinds of global coherence.
Why do we care about the local or global coherence of a discourse? Since co-
herence is a property of a well-written text, coherence detection plays a part in any

23.1
•
COHERENCE RELATIONS
445
task that requires measuring the quality of a text. For example coherence can help
in pedagogical tasks like essay grading or essay quality measurement that are try-
ing to grade how well-written a human essay is (Somasundaran et al., 2014; Feng
et al., 2014; Lai and Tetreault, 2018). Coherence can also help for summarization;
knowing the coherence relationship between sentences can help know how to select
information from them. Finally, detecting incoherent text may even play a role in
mental health tasks like measuring symptoms of schizophrenia or other kinds of dis-
ordered language (Ditman and Kuperberg, 2010; Elvev˚ag et al., 2007; Bedi et al.,
2015).
23.1
Coherence Relations
Recall from the introduction the difference between passages (23.5) and (23.6).
(23.5) Jane took a train from Paris to Istanbul. She likes spinach.
(23.6) Jane took a train from Paris to Istanbul. She had to attend a conference.
The reason (23.6) is more coherent is that the reader can form a connection be-
tween the two sentences, in which the second sentence provides a potential REASON
for the ﬁrst sentences. This link is harder to form for (23.5). These connections
between text spans in a discourse can be speciﬁed as a set of coherence relations.
coherence
relation
The next two sections describe two commonly used models of coherence relations
and associated corpora: Rhetorical Structure Theory (RST), and the Penn Discourse
TreeBank (PDTB).
23.1.1
Rhetorical Structure Theory
The most commonly used model of discourse organization is Rhetorical Structure
Theory (RST) (Mann and Thompson, 1987). In RST relations are deﬁned between
RST
two spans of text, generally a nucleus and a satellite. The nucleus is the unit that
nucleus
satellite
is more central to the writer’s purpose and that is interpretable independently; the
satellite is less central and generally is only interpretable with respect to the nucleus.
Some symmetric relations, however, hold between two nuclei.
Below are a few examples of RST coherence relations, with deﬁnitions adapted
from the RST Treebank Manual (Carlson and Marcu, 2001).
Reason: The nucleus is an action carried out by an animate agent and the satellite
is the reason for the nucleus.
(23.7) [NUC Jane took a train from Paris to Istanbul.] [SAT She had to attend a
conference.]
Elaboration: The satellite gives additional information or detail about the situation
presented in the nucleus.
(23.8) [NUC Dorothy was from Kansas.] [SAT She lived in the midst of the great
Kansas prairies.]
Evidence: The satellite gives additional information or detail about the situation
presented in the nucleus. The information is presented with the goal of convince the
reader to accept the information presented in the nucleus.
(23.9) [NUC Kevin must be here.] [SAT His car is parked outside.]

446
CHAPTER 23
•
DISCOURSE COHERENCE
Attribution: The satellite gives the source of attribution for an instance of reported
speech in the nucleus.
(23.10) [SAT Analysts estimated] [NUC that sales at U.S. stores declined in the
quarter, too]
List: In this multinuclear relation, a series of nuclei is given, without contrast or
explicit comparison:
(23.11) [NUC Billy Bones was the mate; ] [NUC Long John, he was quartermaster]
RST relations are traditionally represented graphically; the asymmetric Nucleus-
Satellite relation is represented with an arrow from the satellite to the nucleus:
Kevin must be here.
His car is parked outside
evidence
We can also talk about the coherence of a larger text by considering the hierar-
chical structure between coherence relations. Figure 23.1 shows the rhetorical struc-
ture of a paragraph from Marcu (2000a) for the text in (23.12) from the Scientiﬁc
American magazine.
(23.12) With its distant orbit–50 percent farther from the sun than Earth–and slim
atmospheric blanket, Mars experiences frigid weather conditions. Surface
temperatures typically average about -60 degrees Celsius (-76 degrees
Fahrenheit) at the equator and can dip to -123 degrees C near the poles. Only
the midday sun at tropical latitudes is warm enough to thaw ice on occasion,
but any liquid water formed in this way would evaporate almost instantly
because of the low atmospheric pressure.
Title
(1)
Mars
2-9
evidence
2-3
background
  (2)
WIth its 
distant orbit  
<p> -- 50
 percent 
farther from 
the sun than 
Earth -- </p> 
and slim 
atmospheric 
blanket,
(3)
Mars
experiences
frigid weather
conditions.
4-9
elaboration-additional
(4)
Surface 
temperatures
 typically average 
about -60 
degrees Celsius
 <p> (-76 degrees
Fahrenheit)</p>
 at the equator
4-5
List
(5)
and can dip
to -123
degrees C
near the
poles.
6-9
Contrast
6-7
(6)
Only the
midday sun at
tropical latitudes
is warm enough
(7)
to thaw ice
on occasion,
purpose
8-9
explanation-argumentative
(8)
but any liquid water
formed in this way 
would evaporate 
almost instantly
(9)
because of
the low
atmospheric
pressure.
Figure 23.1
A discourse tree for the Scientiﬁc American text in (23.12), from Marcu (2000a). Note that
asymmetric relations are represented with a curved arrow from the satellite to the nucleus.
The leaves in the Fig. 23.1 tree correspond to text spans of a sentence, clause or
phrase that are called elementary discourse units or EDUs in RST; these units can
EDU
also be referred to as discourse segments. Because these units may correspond to
arbitrary spans of text, determining the boundaries of an EDU is an important task

23.1
•
COHERENCE RELATIONS
447
for extracting coherence relations. Roughly speaking, one can think of discourse
segments as being analogous to constituents in sentence syntax, and indeed as we’ll
see in Section 23.2 we generally draw on parsing algorithms to infer discourse struc-
ture.
There are corpora for many discourse coherence models; the RST Discourse
TreeBank (Carlson et al., 2001) is the largest available discourse corpus. It con-
sists of 385 English language documents selected from the Penn Treebank, with full
RST parses for each one, using a large set of 78 distinct relations, grouped into 16
classes. RST treebanks exist also for Spanish, German, Basque, Dutch and Brazilian
Portuguese (Braud et al., 2017).
Now that we’ve seen examples of coherence, we can see more clearly how a
coherence relation can play a role in summarization or information extraction. For
example, the nuclei of a text presumably express more important information than
the satellites, which might be dropped in a summary.
23.1.2
Penn Discourse TreeBank (PDTB)
The Penn Discourse TreeBank (PDTB) is a second commonly used dataset that
PDTB
embodies another model of coherence relations (Miltsakaki et al., 2004; Prasad et al.,
2008, 2014). PDTB labeling is lexically grounded. Instead of asking annotators
to directly tag the coherence relation between text spans, they were given a list of
discourse connectives, words that signal discourse relations, like because, although,
discourse
connectives
when, since, or as a result. In a part of a text where these words marked a coherence
relation between two text spans, the connective and the spans were then annotated,
as in Fig. 23.13, where the phrase as a result signals a causal relationship between
what PDTB calls Arg1 (the ﬁrst two sentences, here in italics) and Arg2 (the third
sentence, here in bold).
(23.13) Jewelry displays in department stores were often cluttered and uninspired.
And the merchandise was, well, fake. As a result, marketers of faux gems
steadily lost space in department stores to more fashionable
rivals—cosmetics makers.
(23.14) In July, the Environmental Protection Agency imposed a gradual ban on
virtually all uses of asbestos. (implicit=as a result) By 1997, almost all
remaining uses of cancer-causing asbestos will be outlawed.
Not all coherence relations are marked by an explicit discourse connective, and
so the PDTB also annotates pairs of neighboring sentences with no explicit signal,
like (23.14). The annotator ﬁrst chooses the word or phrase that could have been
its signal (in this case as a result), and then labels its sense. For example for the
ambiguous discourse connective since annotators marked whether it is using a a
CAUSAL or a TEMPORAL sense.
The ﬁnal dataset contains roughly 18,000 explicit relations and 16,000 implicit
relations. Fig. 23.2 shows examples from each of the 4 major semantic classes, while
Fig. 23.3 shows the full tagset.
Unlike the RST Discourse Treebank, which integrates these pairwise coherence
relations into a global tree structure spanning an entire discourse, the PDTB does not
annotate anything above the span-pair level, making no commitment with respect to
higher-level discourse structure.
There are also treebanks using similar methods for other languages; (23.15)
shows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015).
Because Chinese has a smaller percentage of explicit discourse connectives than

448
CHAPTER 23
•
DISCOURSE COHERENCE
Class
Type
Example
TEMPORAL
SYNCHRONOUS The parishioners of St. Michael and All Angels stop to chat at
the church door, as members here always have. (Implicit while)
In the tower, ﬁve men and women pull rhythmically on ropes
attached to the same ﬁve bells that ﬁrst sounded here in 1614.
CONTINGENCY
REASON
Also unlike Mr. Ruder, Mr. Breeden appears to be in a position
to get somewhere with his agenda. (implicit=because) As a for-
mer White House aide who worked closely with Congress,
he is savvy in the ways of Washington.
COMPARISON
CONTRAST
The U.S. wants the removal of what it perceives as barriers to
investment; Japan denies there are real barriers.
EXPANSION
CONJUNCTION
Not only do the actors stand outside their characters and make
it clear they are at odds with them, but they often literally stand
on their heads.
Figure 23.2
The four high-level semantic distinctions in the PDTB sense hierarchy
Temporal
Comparison
• Asynchronous
• Contrast (Juxtaposition, Opposition)
• Synchronous (Precedence, Succession)
•Pragmatic Contrast (Juxtaposition, Opposition)
• Concession (Expectation, Contra-expectation)
• Pragmatic Concession
Contingency
Expansion
• Cause (Reason, Result)
• Exception
• Pragmatic Cause (Justiﬁcation)
• Instantiation
• Condition (Hypothetical, General, Unreal
Present/Past, Factual Present/Past)
• Restatement (Speciﬁcation, Equivalence, Generalization)
• Pragmatic Condition (Relevance, Implicit As-
sertion)
• Alternative (Conjunction, Disjunction, Chosen Alterna-
tive)
• List
Figure 23.3
The PDTB sense hierarchy. There are four top-level c¯lasses, 16 types, and 23 subtypes (not all
types have subtypes). 11 of the 16 types are commonly used for implicit argument classiﬁcation; the 5 types in
italics are too rare in implicit labeling to be used.
English (only 22% of all discourse relations are marked with explicit connectives,
compared to 47% in English), annotators labeled this corpus by directly mapping
pairs of sentences to 11 sense tags, without starting with a lexical discourse connec-
tor.
(23.15) [Conn 为] [Arg2 推动图们江地区开发] ，[Arg1 韩国捐款一百万美元
设立了图们江发展基金]
“[In order to] [Arg2 promote the development of the Tumen River region],
[Arg1 South Korea donated one million dollars to establish the Tumen
River Development Fund].”
These discourse treebanks have been used for shared tasks on multilingual dis-
course parsing (Xue et al., 2016).
23.2
Discourse Structure Parsing
Given a sequence of sentences, how can we automatically determine the coherence
relations between them? This task is often called discourse parsing (even though
discourse
parsing

23.2
•
DISCOURSE STRUCTURE PARSING
449
for PDTB we are only assigning labels to leaf spans and not building a full parse
tree as we do for RST).
23.2.1
EDU segmentation for RST parsing
RST parsing is generally done in two stages. The ﬁrst stage, EDU segmentation,
extracts the start and end of each EDU. The output of this stage would be a labeling
like the following:
(23.16) [Mr. Rambo says]e1 [that a 3.2-acre property]e2 [overlooking the San
Fernando Valley]e3 [is priced at $4 million]e4 [because the late actor Erroll
Flynn once lived there.]e5
Since EDUs roughly correspond to clauses, early models of EDU segmentation
ﬁrst ran a syntactic parser, and then post-processed the output. Modern systems
generally use neural sequence models supervised by the gold EDU segmentation in
the RST Discourse Treebank. Fig. 23.4 shows an example from Wang et al. (2018)
of a supervised architecture that uses the same biLSTM-CRF architecture we saw
for named entity tagging and semantic role labeling. Here the input sentence is
mapping to contextual word embeddings, and then passed through a biLSTM with
a CRF layer on top to produce a sequence of 0s and 1, where 1 indicates the start
of an EDU (except at the start of sentence). Muller et al. (2019) ﬁnd using BERT
contextual embeddings plus convolutional character embeddings as the input to a
similar biLSTM architecture produces highly accurate segmentations.
Mr.
Rambo
says
that
LSTM1
LSTM1
LSTM1
LSTM1
LSTM2
LSTM2
LSTM2
LSTM2
Concatenation
Right-to-left LSTM
Left-to-right LSTM
0
0
0
1
CRF Layer
Contextual
Embedding
Contextual
Embedding
Contextual
Embedding
Contextual 
Embedding
GloVe
GloVe
GloVe
GloVe
Word Representations
Figure 23.4
biLSTM-CRF for EDU segmentation. Word inputs can draw from any contex-
tual embeddings like ELMo or BERT. After Wang et al. (2018).
23.2.2
RST parsing
Tools for building RST coherence structure for a discourse have long been based on
syntactic parsing algorithms like shift-reduce parsing (Marcu, 1999). Many modern
RST parsers since Ji and Eisenstein (2014) draw on the neural syntactic parsers we
saw in Chapter 15 in Section 15.4, using representation learning to build represen-
tations for each span, and training a parser to choose the correct shift and reduce
actions based on the gold parses in the training set.
We’ll describe the shift-reduce parser of Yu et al. (2018). The parser state con-
sists of a stack and a queue, and produces this structure by taking a series of actions
on the states. Actions include:

450
CHAPTER 23
•
DISCOURSE COHERENCE
• shift: pushes the ﬁrst EDU in the queue onto the stack creating a single-node
subtree.
• reduce(l,d): merges the top two subtrees on the stack, where l is the coherence
relation label, and d is the nuclearity direction, d ∈{NN,NS,SN}.
As well as the pop root operation, to remove the ﬁnal tree from the stack.
e1
e2
e3
e4
attr
elab
elab
e1: American Telephone & Telegraph Co. said it
e2: will lay off 75 to 85 technicians here , effective Nov. 1.
e3: The workers install , maintain and repair its private branch exchanges,
e4: which are large intracompany telephone networks.
Figure 23.5
Example RST discourse tree, showing four EDUs. Figure from Yu et al. (2018).
Fig. 23.6 shows the actions the parser takes to build the structure in Fig. 23.5.
Step
Stack
Queue
Action
Relation
1
?
e1, e2, e3, e4
SH
?
2
e1
e2, e3, e4
SH
?
3
e1, e2
e3, e4
RD(attr,SN)
?
4
e1:2
e3, e4
SH
d
e1e2
5
e1:2 , e3
e4
SH
d
e1e2
6
e1:2 , e3, e4
?
RD(elab,NS)
d
e1e2
7
e1:2 , e3:4
?
RD(elab,SN)
d
e1e2, d
e3e4
8
e1:4
?
PR
d
e1e2, d
e3e4, \
e1:2e3:4
Figure 23.6
Parsing the example of Fig. 23.5 using a shift-reduce parser. Figure from Yu
et al. (2018).
The Yu et al. (2018) uses an encoder-decoder architecture, where the encoder
represents the input span of words and EDUs using a hierarchical biLSTM. The
ﬁrst biLSTM layer represents the words inside an EDU, and the second represents
the EDU sequence. Given an input sentence w1,w2,...,wm, the words can be repre-
sented as usual (by static embeddings, combinations with character embeddings or
tags, or contextual embeddings) resulting in an input word representation sequence
xw
1 ,xw
2 ,...,xw
m. The result of the word-level biLSTM is then a sequence of hw values:
hw
1 ,hw
2 ,...,hw
m = biLSTM(xw
1 ,xw
2 ,...,xw
m)
(23.17)
An EDU of span ws,ws+1,...,wt then has biLSTM output representation hw
s ,hw
s+1,...,hw
t ,
and is represented by average pooling:
xe =
1
t −s+1
tX
k=s
hw
k
(23.18)
The second layer uses this input to compute a ﬁnal representation of the sequence of
EDU representations he:
he
1,he
2,...,he
n = biLSTM(xe
1,xe
2,...,xe
n)
(23.19)
The decoder is then a feedforward network W that outputs an action o based on a
concatenation of the top three subtrees on the stack (so,s1,s2) plus the ﬁrst EDU in

23.2
•
DISCOURSE STRUCTURE PARSING
451
the queue (q0):
o = W(ht
s0,ht
s0,ht
s1,ht
s2,he
q0)
(23.20)
where the representation of the EDU on the queue he
q0 comes directly from the
encoder, and the three hidden vectors representing partial trees are computed by
average pooling over the encoder output for the EDUs in those trees:
hts =
1
j −i+1
j
X
k=i
he
k
(23.21)
Training ﬁrst maps each RST gold parse tree into a sequence of oracle actions, and
then uses the standard cross-entropy loss (with l2 regularization) to train the system
to take such actions. Give a state S and oracle action a, we ﬁrst compute the decoder
output using Eq. 23.20, apply a softmax to get probabilities:
pa =
exp(oa)
P
a′∈A exp(oa′)
(23.22)
and then computing the cross-entropy loss:
L(Θ) = −log(pa)+ λ
2 ||Θ||2
(23.23)
RST discourse parsers are evaluated on the test section of the RST Discourse Tree-
bank, either with gold EDUs or end-to-end, using the RST-Pareval metrics (Marcu,
2000b). It is standard to ﬁrst transform the gold RST trees into right-branching bi-
nary trees, and to report four metrics: trees with no labels (S for Span), labeled
with nuclei (N), with relations (R), or both (F for Full), for each metric computing
micro-averaged F1 over all spans from all documents (Marcu, 2000b; Morey et al.,
2017).
23.2.3
PDTB discourse parsing
PDTB discourse parsing, the task of detecting PDTB coherence relations between
spans, is sometimes called shallow discourse parsing because the task just involves
shallow
discourse
parsing
ﬂat relationships between text spans, rather than the full trees of RST parsing.
The set of four subtasks for PDTB discourse parsing was laid out by Lin et al.
(2014) in the ﬁrst complete system, with separate tasks for explicit (tasks 1-3) and
implicit (task 4) connectives:
1. Find the discourse connectives (disambiguating them from non-discourse uses)
2. Find the two spans for each connective
3. Label the relationship between these spans
4. Assign a relation between every adjacent pair of sentences
Many systems have been proposed for Task 4: taking a pair of adjacent sentences
as input and assign a coherence relation sense label as output. The setup often fol-
lows Lin et al. (2009) in assuming gold sentence span boundaries and assigning each
adjacent span one of the 11 second-level PDTB tags or none (removing the 5 very
rare tags of the 16 shown in italics in Fig. 23.3).
A simple but very strong algorithm for Task 4 is to represent each of the two
spans by BERT contextual embeddings and take the last layer hidden state corre-
sponding to the position of the <CLS> token, pass this through a single layer tanh
feedforward network and then a softmax for sense classiﬁcation (Nie et al., 2019).

452
CHAPTER 23
•
DISCOURSE COHERENCE
Each of the other tasks also have been addressed. Task 1 is to disambiguat-
ing discourse connectives from their non-discourse use. For example as Pitler and
Nenkova (2009) point out, the word and is a discourse connective linking the two
clauses by an elaboration/expansion relation in (23.24) while it’s a non-discourse
NP conjunction in (23.25):
(23.24) Selling picked up as previous buyers bailed out of their positions and
aggressive short sellers—anticipating further declines—moved in.
(23.25) My favorite colors are blue and green.
Similarly, once is a discourse connective indicating a temporal relation in (23.26),
but simply a non-discourse adverb meaning ‘formerly’ and modifying used in (23.27):
(23.26) The asbestos ﬁber, crocidolite, is unusually resilient once it enters the
lungs, with even brief exposures to it causing symptoms that show up
decades later, researchers said.
(23.27) A form of asbestos once used to make Kent cigarette ﬁlters has caused a
high percentage of cancer deaths among a group of workers exposed to it
more than 30 years ago, researchers reported.
Determining whether a word is a discourse connective is thus a special case
of word sense disambiguation. Early work on disambiguation showed that the 4
PDTB high-level sense classes could be disambiguated with high (94%) accuracy
used syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent
work performs the task end to end from word inputs using a biLSTM-CRF with
BIO outputs (B-CONN, I-CONN, O) (Yu et al., 2019).
For task 2, PDTB spans can be identiﬁed with the same sequence models used to
ﬁnd RST EDUs: a biLSTM sequence model with pretrained contextual embedding
(BERT) inputs (Muller et al., 2019). Simple heuristics also do pretty well as a base-
line at ﬁnding spans, since 93% of relations are either completely within a single
sentence or span two adjacent sentences, with one argument in each sentence (Biran
and McKeown, 2015).
23.3
Centering and Entity-Based Coherence
A second way a discourse can be coherent is by virtue of being “about” some entity.
This idea that at each point in the discourse some entity is salient, and a discourse
is coherent by continuing to discuss the same entity, appears early in functional lin-
guistics and the psychology of discourse (Chafe, 1976; Kintsch and Van Dijk, 1978),
and soon made its way to computational models. In this section we introduce two
models of this kind of entity-based coherence: Centering Theory (Grosz et al.,
entity-based
1995), and the entity grid model of Barzilay and Lapata (2008).
23.3.1
Centering
Centering Theory (Grosz et al., 1995) is a theory of both discourse salience and
Centering
Theory
discourse coherence. As a model of discourse salience, Centering proposes that at
any given point in the discourse one of the entities in the discourse model is salient:
it is being “centered” on. As a model of discourse coherence, Centering proposes
that discourses in which adjacent sentences CONTINUE to maintain the same salient
entity are more coherent than those which SHIFT back and forth between multiple
entities (we will see that CONTINUE and SHIFT are technical terms in the theory).

23.3
•
CENTERING AND ENTITY-BASED COHERENCE
453
The following two texts from Grosz et al. (1995) which have exactly the same
propositional content but different saliences, can help in understanding the main
Centering intuition.
(23.28)
a. John went to his favorite music store to buy a piano.
b. He had frequented the store for many years.
c. He was excited that he could ﬁnally buy a piano.
d. He arrived just as the store was closing for the day.
(23.29)
a. John went to his favorite music store to buy a piano.
b. It was a store John had frequented for many years.
c. He was excited that he could ﬁnally buy a piano.
d. It was closing just as John arrived.
While these two texts differ only in how the two entities (John and the store) are
realized in the sentences, the discourse in (23.28) is intuitively more coherent than
the one in (23.29). As Grosz et al. (1995) point out, this is because the discourse
in (23.28) is clearly about one individual, John, describing his actions and feelings.
The discourse in (23.29), by contrast, focuses ﬁrst on John, then the store, then back
to John, then to the store again. It lacks the “aboutness” of the ﬁrst discourse.
Centering Theory realizes this intuition by maintaining two representations for
each utterance Un. The backward-looking center of Un, denoted as Cb(Un), rep-
backward-
looking
center
resents the current salient entity, the one being focused on in the discourse after Un
is interpreted. The forward-looking centers of Un, denoted as Cf (Un), are a set
forward-looking
center
of potential future salient entities, the discourse entities evoked by Un any of which
could serve as Cb (the salient entity) of the following utterance, i.e. Cb(Un+1).
The set of forward-looking centers Cf (Un) are ranked according to factors like
discourse salience and grammatical role (for example subjects are higher ranked
than objects, which are higher ranked than all other grammatical roles). We call the
highest-ranked forward-looking center Cp (for “preferred center”). Cp is a kind of
prediction about what entity will be talked about next. Sometimes the next utterance
indeed talks about this entity, but sometimes another entity becomes salient instead.
We’ll use here the algorithm for centering presented in Brennan et al. (1987),
which deﬁnes four intersentential relationships between a pair of utterances Un and
Un+1 that depend on the relationship between Cb(Un+1), Cb(Un), and Cp(Un+1);
these are shown in Fig. 23.7.
Cb(Un+1) = Cb(Un)
Cb(Un+1) ̸= Cb(Un)
or undeﬁned Cb(Un)
Cb(Un+1) = Cp(Un+1)
Continue
Smooth-Shift
Cb(Un+1) ̸= Cp(Un+1)
Retain
Rough-Shift
Figure 23.7
Centering Transitions for Rule 2 from Brennan et al. (1987).
The following rules are used by the algorithm:
Rule 1: If any element of Cf (Un) is realized by a pronoun in utterance
Un+1, then Cb(Un+1) must be realized as a pronoun also.
Rule 2: Transition states are ordered. Continue is preferred to Retain is
preferred to Smooth-Shift is preferred to Rough-Shift.
Rule 1 captures the intuition that pronominalization (including zero-anaphora)
is a common way to mark discourse salience. If there are multiple pronouns in an

454
CHAPTER 23
•
DISCOURSE COHERENCE
utterance realizing entities from the previous utterance, one of these pronouns must
realize the backward center Cb; if there is only one pronoun, it must be Cb.
Rule 2 captures the intuition that discourses that continue to center the same en-
tity are more coherent than ones that repeatedly shift to other centers. The transition
table is based on two factors: whether the backward-looking center Cb is the same
from Un to Un+1 and whether this discourse entity is the one that was preferred (Cp)
from Un. If both of these hold, a CONTINUE relation, the speaker has been talking
about the same entity and is going to continue talking about that entity. In a RETAIN
relation, the speaker intends to SHIFT to a new entity in a future utterance and mean-
while places the current entity in a lower rank Cf . In a SHIFT relation, the speaker is
shifting to a new salient entity.
Let’s walk though the start of (23.28) again, repeated as (23.30), showing the
representations after each utterance is processed.
(23.30)
John went to his favorite music store to buy a piano. (U1)
He was excited that he could ﬁnally buy a piano. (U2)
He arrived just as the store was closing for the day. (U3)
It was closing just as John arrived (U4)
Using the grammatical role hierarchy to order the Cf , for sentence U1 we get:
Cf (U1): {John, music store, piano}
Cp(U1): John
Cb(U1): undeﬁned
and then for sentence U2:
Cf (U2): {John, piano}
Cp(U2): John
Cb(U2): John
Result: Continue
(Cp(U2)=Cb(U2); Cb(U1) undeﬁned)
The transition from U1 to U2 is thus a CONTINUE. Completing this example is left
as exercise (1) for the reader
23.3.2
Entity Grid model
Centering embodies a particular theory of how entity mentioning leads to coher-
ence: that salient entities appear in subject position or are pronominalized, and that
discourses are salient by means of continuing to mention the same entity in such
ways.
The entity grid model of Barzilay and Lapata (2008) is an alternative way to
entity grid
capture entity-based coherence: instead of having a top-down theory, the entity-grid
model using machine learning to induce the patterns of entity mentioning that make
a discourse more coherent.
The model is based around an entity grid, a two-dimensional array that repre-
sents the distribution of entity mentions across sentences. The rows represent sen-
tences, and the columns represent discourse entities (most versions of the entity grid
model focus just on nominal mentions). Each cell represents the possible appearance
of an entity in a sentence, and the values represent whether the entity appears and its
grammatical role. Grammatical roles are subject (S), object (O), neither (X), or ab-
sent (–); in the implementation of Barzilay and Lapata (2008), subjects of passives
are represented with O, leading to a representation with some of the characteristics
of thematic roles.

23.3
•
CENTERING AND ENTITY-BASED COHERENCE
455
Department
Trial
Microsoft
Evidence
Competitors
Markets
Products
Brands
Case
Netscape
Software
Tactics
Government
Suit
Earnings
1
S
O S X
O – – – – – – – – – – 1
2
– – O – –
X S O – – – – – – – 2
3
– –
S O – – – –
S
O O – – – – 3
4
– –
S – – – – – – – – S – – – 4
5
– – – – – – – – – – – – S O – 5
6
–
X S – – – – – – – – – – –
O 6
Figure 23.8
Part of the entity grid for the text in Fig. 23.9. Entities are listed by their head
noun; each cell represents whether an entity appears as subject (S), object (O), neither (X), or
is absent (–). Figure from Barzilay and Lapata (2008).
1 [The Justice Department]S is conducting an [anti-trust trial]O against [Microsoft Corp.]X
with [evidence]X that [the company]S is increasingly attempting to crush [competitors]O.
2 [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its own
products]S are not competitive enough to unseat [established brands]O.
3 [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring
[Netscape]O into merging [browser software]O.
4 [Microsoft]S claims [its tactics]S are commonplace and good economically.
5 [The government]S may ﬁle [a civil suit]O ruling that [conspiracy]S to curb [competition]O
through [collusion]X is [a violation of the Sherman Act]O.
6 [Microsoft]S continues to show [increased earnings]O despite [the trial]X.
Figure 23.9
A discourse with the entities marked and annotated with grammatical func-
tions. Figure from Barzilay and Lapata (2008).
Fig. 23.8 from Barzilay and Lapata (2008) shows a grid for the text shown in
Fig. 23.9. There is one row for each of the six sentences. The second column, for
the entity ‘trial’, is O – – – X, showing that the trial appears in the ﬁrst sentence as
direct object, in the last sentence as an oblique, and does not appear in the middle
sentences. The third column, for the entity Microsoft, shows that it appears as sub-
ject in sentence 1 (it also appears as the object of the preposition against, but entities
that appear multiple times are recorded with their highest-ranked grammatical func-
tion). Computing the entity grids requires extracting entities and doing coreference
resolution to cluster them into discourse entities (Chapter 22) as well as parsing the
sentences to get grammatical roles.
In the resulting grid, columns that are dense (like the column for Microsoft) in-
dicate entities that are mentioned often in the texts; sparse columns (like the column
for earnings) indicate entities that are mentioned rarely.
In the entity grid model, coherence is measured by patterns of local entity tran-
sition. For example, Department is a subject in sentence 1, and then not men-
tioned in sentence 2; this is the transition [S –]. The transitions are thus sequences
{S,O X, –}n which can be extracted as continuous cells from each column. Each
transition has a probability; the probability of [S –] in the grid from Fig. 23.8 is 0.08
(it occurs 6 times out of the 75 total transitions of length two). Fig. 23.10 shows the
distribution over transitions of length 2 for the text of Fig. 23.9 (shown as the ﬁrst
row d1), and 2 other documents.
The transitions and their probabilities can then be used as features for a machine
learning model. This model can be a text classiﬁer trained to produce human-labeled
coherence scores (for example from humans labeling each text as coherent or inco-
herent). But such data is expensive to gather. Barzilay and Lapata (2005) introduced
a simplifying innovation: coherence models can be trained by self-supervision:
trained to distinguish the natural original order of sentences in a discourse from

456
CHAPTER 23
•
DISCOURSE COHERENCE
S S
S O
S X
S –
O S
O O
O X
O –
X S
X O
X X
X –
– S
– O
– X
– –
d1
.01
.01 0
.08
.01 0
0
.09
0
0
0
.03
.05
.07
.03
.59
d2
.02
.01
.01
.02
0
.07
0
.02
.14
.14
.06
.04
.03
.07
0.1
.36
d3
.02
0
0
.03
.09 0
.09
.06
0
0
0
.05
.03
.07
.17
.39
Figure 23.10
A feature vector for representing documents using all transitions of length 2.
Document d1 is the text in Fig. 23.9. Figure from Barzilay and Lapata (2008).
a modiﬁed order (such as a randomized order). We turn to these evaluations in the
next section.
23.3.3
Evaluating Neural and Entity-based coherence
Entity-based coherence models, as well as the neural models we introduce in the
next section, are generally evaluated in one of two ways.
First, we can have humans rate the coherence of a document and train a classiﬁer
to predict these human ratings, which can be categorial (high/low, or high/mid/low)
or continuous. This is the best evaluation to use if we have some end task in mind,
like essay grading, where human raters are the correct deﬁnition of the ﬁnal label.
Alternatively, since it’s very expensive to get human labels, and we might not
yet have an end-task in mind, we can use natural texts to do self-supervision. In
self-supervision we pair up a natural discourse with a pseudo-document created by
changing the ordering. Since naturally-ordered discourses are more coherent than
random permutation (Lin et al., 2011), a successful coherence algorithm should pre-
fer the original ordering.
Self-supervision has been implemented in 3 ways. In the sentence order dis-
crimination task (Barzilay and Lapata, 2005), we compare a document to a random
permutation of its sentence. A model is considered correct for an (original, per-
muted) test pair if it ranks the original document higher. Given k documents, we can
compute n permutations, resulting in kn pairs each with one original document and
one permutation, to use in training and testing.
In the sentence insertion task (Chen et al., 2007) we take a document, remove
one of the n sentences s, and create n−1 copies of the document with s inserted into
each position. The task is to decide which of the n documents is the one with the
original ordering, distinguishing the original position for s from all other positions.
Insertion is harder than discrimination since we are comparing documents that differ
by only one sentence.
Finally, in the sentence order reconstruction task (Lapata, 2003), we take a
document, randomize the sentences, and train the model to put them back in the
correct order. Again given k documents, we can compute n permutations, resulting
in kn pairs each with one original document and one permutation, to use in training
and testing. Reordering is of course a much harder task than simple classiﬁcation.
23.4
Representation learning models for local coherence
The third kind of local coherence is topical or semantic ﬁeld coherence. Discourses
cohere by talking about the same topics and subtopics, and drawing on the same
semantic ﬁelds in doing so.
The ﬁeld was pioneered by a series of unsupervised models in the 1990s of this

23.4
•
REPRESENTATION LEARNING MODELS FOR LOCAL COHERENCE
457
kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976):
lexical cohesion
the sharing of identical or semantically related words in nearby sentences. Morris
and Hirst (1991) computed lexical chains of words (like pine, bush trees, trunk) that
occurred through a discourse and that were related in Roget’s Thesaurus (by being in
the same category, or linked categories). They showed that the number and density
of chain correlated with the topic structure. The TextTiling algorithm of Hearst
TextTiling
(1997) computed the cosine between neighboring text spans (the normalized dot
product of vectors of raw word counts), again showing that sentences or paragraph in
a subtopic have high cosine with each other, but not with sentences in a neighboring
subtopic.
A third early model, the LSA Coherence method of Foltz et al. (1998) was the
ﬁrst to use embeddings, modeling the coherence between two sentences as the co-
sine between their LSA sentence embedding vectors1, computing embeddings for a
sentence s by summing the embeddings of its words w:
sim(s,t) = cos(s,t)
= cos(
X
w∈s
w,
X
w∈t
w)
(23.31)
and deﬁning the overall coherence of a text as the average similarity over all pairs of
adjacent sentences si and si+1:
coherence(T) =
1
n−1
n−1
X
i=1
cos(si,si+1)
(23.32)
Modern neural representation-learning coherence models, beginning with Li et al.
(2014), draw on the intuitions of these early unsupervised models for learning sen-
tence representations and measuring how they change between neighboring sen-
tences. But the new models also draw on the idea pioneered by Barzilay and Lapata
(2005) of self-supervision. That is, unlike say coherence relation models, which
train on hand-labeled representations for RST or PDTB, these models are trained to
distinguish natural discourses from unnatural discourses formed by scrambling the
order of sentences, thus using representation learning to discover the features that
matter for at least the ordering aspect of coherence.
Here we present one such model, the local coherence discriminator (LCD) (Xu
et al., 2019). Like early models, LCD computes the coherence of a text as the av-
erage of coherence scores between consecutive pairs of sentences. But unlike the
early unsupervised models, LCD is a self-supervised model trained to discriminate
consecutive sentence pairs (si,si+1) in the training documents (assumed to be coher-
ent) from (constructed) incoherent pairs (si,s′). All consecutive pairs are positive
examples, and the negative (incoherent) partner for a sentence si is another sentence
uniformly sampled from the same document as si.
Fig. 23.11 describes the architecture of the model fθ, which takes a sentence
pair and returns a score, higher scores for more coherent pairs. Given an input
sentence pair s and t, the model computes sentence embeddings s and t (using any
sentence embeddings algorithm), and then concatenates four features of the pair: (1)
the concatenation of the two vectors (2) their difference s−t; (3) the absolute value
of their difference |s −t|; (4) their element-wise product s ⊙t. These are passed
through a one-layer feedforward network to output the coherence score.
1
See Chapter 6 for more on LSA embeddings; they are computed by applying SVD to the term-
document matrix (each cell weighted by log frequency and normalized by entropy), and then the ﬁrst
300 dimensions are used as the embedding.

458
CHAPTER 23
•
DISCOURSE COHERENCE
Figure 23.11
The architecture of the LCD model of document coherence, showing the
computation of the score for a pair of sentences s and t. Figure from Xu et al. (2019).
The model is trained to make this coherence score higher for real pairs than for
negative pairs. More formally, the training objective for a corpus C of documents d,
each of which consists of a list of sentences si, is:
Lθ =
X
d∈C
X
si∈d
E
p(s′|si)
[L(fθ(si,si+1), fθ(si,s′))]
(23.33)
Ep(s′|si) is the expectation with respect to the negative sampling distribution con-
ditioned on si: given a sentence si the algorithms samples a negative sentence s′
uniformly over the other sentences in the same document. L is a loss function that
takes two scores, one for a positive pair and one for a negative pair, with the goal of
encouraging f + = fθ(si,si+1) to be high and f −= fθ(si,s′)) to be low. Fig. 23.11
use the margin loss l(f +, f −) = max(0,η −f + + f −) where η is the margin hyper-
parameter.
Xu et al. (2019) also give a useful baseline algorithm that itself has quite high
performance in measuring perplexity: train an RNN language model on the data,
and compute the log likelihood of sentence si in two ways, once given the preceding
context (conditional log likelihood) and once with no context (marginal log likeli-
hood). The difference between these values tells us how much the preceding context
improved the predictability of si, a predictability measure of coherence.
While the architecture and roots of these neural models lie in the cohesion-based
idea that coherent discourses share words, semantic ﬁelds, and topics, qualitative
analysis of these models (Li and Jurafsky, 2017) suggest that neural models may
also represent coherence due to relations (for example neural models ﬁnd pairs of
sentences coherent when they have a causal or temporal relation) and entity coher-
ence (for example the models correctly assign discourse (23.28) a higher coherence
score than (23.29).
23.5
Global Coherence
A discourse must also cohere globally rather than just at the level of pairs of sen-
tences. Consider stories, for example. The narrative structure of stories is one of

23.5
•
GLOBAL COHERENCE
459
the oldest kinds of global coherence to be studied. In his inﬂuential Morphology of
the Folktale, Propp (1968) models the discourse structure of Russian folktales via
a kind of plot grammar. His model includes a set of character categories he called
dramatis personae, like Hero, Villain, Donor, or Helper, and a set of events he
called functions (like “Villain commits kidnapping”, “Donor tests Hero”, or “Hero
is pursued”) that have to occur in particular order, along with other components.
Propp shows that the plots of each of the fairy tales he studies can be represented as
a sequence of these functions, different tales choosing different subsets of functions,
but always in the same order. Indeed Lakoff (1972b) showed that Propp’s model
amounted to a discourse grammar of stories, and in recent computational work Fin-
layson (2016) demonstrates that some of these Proppian functions could be induced
from corpora of folktale texts by detecting events that have similar actions across
stories. Bamman et al. (2013) showed that generalizations over dramatis personae
could be induced from movie plot summaries on Wikipedia. Their model induced
latent personae from features like the actions the character takes (e.g., Villains stran-
gle), the actions done to them (e.g., Villains are foiled and arrested) or the descriptive
words used of them (Villains are evil).
In this section we introduce two kinds of such global discourse structure that
have been widely studied computationally. The ﬁrst is the structure of arguments:
the way people attempt to convince each other in persuasive essays by offering
claims and supporting premises. The second is somewhat related: the structure of
scientiﬁc papers, and the way authors present their goals, results, and relationship to
prior work in their papers.
23.5.1
Argumentation Structure
The ﬁrst type of global discourse structure is the structure of arguments. Analyzing
people’s argumentation computationally is often called argumentation mining.
argumentation
mining
The study of arguments dates back to Aristotle, who in his Rhetorics described
three components of a good argument: pathos (appealing to the emotions of the
pathos
listener), ethos (appealing to the speaker’s personal character), and logos (the logical
ethos
logos
structure of the argument).
Most of the discourse structure studies of argumentation have focused on logos,
particularly via building and training on annotated datasets of persuasive essays or
other arguments (Reed et al., 2008; Stab and Gurevych, 2014a; Peldszus and Stede,
2016; Habernal and Gurevych, 2017; Musi et al., 2018). Such corpora, for exam-
ple, often include annotations of argumentative components like claims (the central
claims
component of the argument that is controversial and needs support) and premises
premises
(the reasons given by the author to persuade the reader by supporting or attacking
the claim or other premises), as well as the argumentative relations between them
argumentative
relations
like SUPPORT and ATTACK.
Consider the following example of a persuasive essay from Stab and Gurevych
(2014b). The ﬁrst sentence (1) presents a claim (in bold). (2) and (3) present two
premises supporting the claim. (4) gives a premise supporting premise (3).
“(1) Museums and art galleries provide a better understanding
about arts than Internet. (2) In most museums and art galleries, de-
tailed descriptions in terms of the background, history and author are
provided. (3) Seeing an artwork online is not the same as watching it
with our own eyes, as (4) the picture online does not show the texture
or three-dimensional structure of the art, which is important to study.”

460
CHAPTER 23
•
DISCOURSE COHERENCE
Thus this example has three argumentative relations: SUPPORT(2,1), SUPPORT(3,1)
and SUPPORT(4,3). Fig. 23.12 shows the structure of a much more complex argu-
ment.
Figure 23.12
Argumentation structure of a persuasive essay. Arrows indicate argumentation relations, ei-
ther of SUPPORT (with arrowheads) or ATTACK (with circleheads); P denotes premises. Figure from Stab and
Gurevych (2017).
While argumentation mining is clearly related to rhetorical structure and other
kinds of coherence relations, arguments tend to be much less local; often a persua-
sive essay will have only a single main claim, with premises spread throughout the
text, without the local coherence we see in coherence relations.
Algorithms for detecting argumentation structure often include classiﬁers for
distinguishing claims, premises, or non-argumentation, together with relation clas-
siﬁers for deciding if two spans have the SUPPORT, ATTACK, or neither relation
(Peldszus and Stede, 2013). While these are the main focus of much computational
work, there is also preliminary efforts on annotating and detecting richer semantic
relationships (Park and Cardie, 2014; Hidey et al., 2017) such as detecting argu-
mentation schemes, larger-scale structures for argument like argument from ex-
argumentation
schemes
ample, or argument from cause to effect, or argument from consequences (Feng
and Hirst, 2011).
Another important line of research is studying how these argument structure (or
other features) are associated with the success or persuasiveness of an argument
(Habernal and Gurevych, 2016; Tan et al., 2016; Hidey et al., 2017). Indeed, while
it is Aristotle’s logos that is most related to discourse structure, Aristotle’s ethos and
pathos techniques are particularly relevant in the detection of mechanisms of this
sort of persuasion. For example scholars have investigated the linguistic realization
persuasion
of features studied by social scientists like reciprocity (people return favors), social
proof (people follow others’ choices), authority (people are inﬂuenced by those
with power), and scarcity (people value things that are scarce), all of which can
be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown
(2017) showed that these features could be combined with argumentation structure
to predict who inﬂuences whom on social media, Althoff et al. (2014) found that
linguistic models of reciprocity and authority predicted success in online requests,
while the semisupervised model of Yang et al. (2019) detected mentions of scarcity,
commitment, and social identity to predict the success of peer-to-peer lending plat-

23.6
•
SUMMARY
461
forms.
See Stede and Schneider (2018) for a comprehensive survey of argument mining.
23.5.2
The structure of scientiﬁc discourse
Scientiﬁc papers have a very speciﬁc global structure: somewhere in the course of
the paper the authors must indicate a scientiﬁc goal, develop a method for a solu-
tion, provide evidence for the solution, and compare to prior work. One popular
annotation scheme for modeling these rhetorical goals is the argumentative zoning
argumentative
zoning
model of Teufel et al. (1999) and Teufel et al. (2009), which is informed by the idea
that each scientiﬁc paper tries to make a knowledge claim about a new piece of
knowledge being added to the repository of the ﬁeld (Myers, 1992). Sentences in
a scientiﬁc paper can be assigned one of 15 tags; Fig. 23.13 shows 7 (shortened)
examples of labeled sentences.
Category
Description
Example
AIM
Statement of speciﬁc research goal, or
hypothesis of current paper
“The aim of this process is to examine the role that
training plays in the tagging process”
OWN METHOD New Knowledge claim,
own work:
methods
“In order for it to be useful for our purposes, the
following extensions must be made:”
OWN RESULTS Measurable/objective outcome of own
work
“All the curves have a generally upward trend but
always lie far below backoff (51% error rate)”
USE
Other work is used in own work
“We use the framework for the allocation and
transfer of control of Whittaker....”
GAP WEAK
Lack of solution in ﬁeld, problem with
other solutions
“Here, we will produce experimental evidence
suggesting that this simple model leads to serious
overestimates”
SUPPORT
Other work supports current work or is
supported by current work
“Work similar to that described here has been car-
ried out by Merialdo (1994), with broadly similar
conclusions.”
ANTISUPPORT
Clash with other’s results or theory; su-
periority of own work
“This result challenges the claims of...”
Figure 23.13
Examples for 7 of the 15 labels from the Argumentative Zoning labelset (Teufel et al., 2009).
Teufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientiﬁc
articles from computational linguistics and chemistry, which can be used as supervi-
sion for training standard sentence-classiﬁcation architecture to assign the 15 labels.
23.6
Summary
In this chapter we introduced local and global models for discourse coherence.
• Discourses are not arbitrary collections of sentences; they must be coherent.
Among the factors that make a discourse coherent are coherence relations
between the sentences, entity-based coherence, and topical coherence.
• Various sets of coherence relations and rhetorical relations have been pro-
posed.
The relations in Rhetorical Structure Theory (RST) hold between
spans of text and are structured into a tree. Because of this, shift-reduce
and other parsing algorithms are generally used to assign these structures.
The Penn Discourse Treebank (PDTB) labels only relations between pairs of
spans, and the labels are generally assigned by sequence models.

462
CHAPTER 23
•
DISCOURSE COHERENCE
• Entity-based coherence captures the intuition that discourses are about an
entity, and continue mentioning the entity from sentence to sentence. Cen-
tering Theory is a family of models describing how salience is modeled for
discourse entities, and hence how coherence is achieved by virtue of keeping
the same discourse entities salient over the discourse. The entity grid model
gives a more bottom-up way to compute which entity realization transitions
lead to coherence.
• Many different genres have different types of global coherence. Persuasive
essays have claims and premises that are extracted in the ﬁeld of argument
mining, scientiﬁc articles have structure related to aims, methods, results, and
comparisons.
Bibliographical and Historical Notes
Coherence relations arose from the independent development of a number of schol-
ars, including Hobbs (1979) idea that coherence relations play an inferential role for
the hearer, and the investigations by Mann and Thompson (1987) of the discourse
structure of large texts. Other approaches to coherence relations and their extrac-
tion include Segmented Discourse Representation Theory (SDRT) (Asher and Las-
SDRT
carides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi,
1988; Scha and Polanyi, 1988; Polanyi et al., 2004a, 2004b) Wolf and Gibson (2005)
argue that coherence structure includes crossed bracketings, which make it impossi-
ble to represent as a tree, and propose a graph representation instead. A compendium
of over 350 relations that have been proposed in the literature can be found in Hovy
(1990).
RST parsing was ﬁrst proposed by Marcu (1997), and early work was rule-based,
focused on discourse markers (Marcu, 2000a). The creation of the RST Discourse
TreeBank (Carlson et al. 2001, Carlson and Marcu 2001) enabled a wide variety
of machine learning algorithms, beginning with the shift-reduce parser of Marcu
(1999) that used decision trees to choose actions, and continuing with a wide variety
of machine learned parsing methods (Soricut and Marcu 2003, Sagae 2009, Hernault
et al. 2010, Feng and Hirst 2014, Surdeanu et al. 2015, Joty et al. 2015) and chunkers
(Sporleder and Lapata, 2005). Subba and Di Eugenio (2009) integrated sophisticated
semantic information into RST parsing. Ji and Eisenstein (2014) ﬁrst applied neural
models to RST parsing neural models, leading to the modern set of neural RST
models (Li et al. 2014, Li et al. 2016, Braud et al. 2017, Yu et al. 2018, inter alia) as
well as neural segmenters (Wang et al. 2018). and neural PDTB parsing models (Ji
and Eisenstein 2015, Qin et al. 2016, Qin et al. 2017).
Barzilay and Lapata (2005) pioneered the idea of self-supervision for coher-
ence: training a coherence model to distinguish true orderings of sentences from
random permutations. Li et al. (2014) ﬁrst applied this paradigm to neural sentence-
representation, and a many neural self-supervised models followed (Li and Juraf-
sky 2017,Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019)
Another aspect of global coherence is the global topic structure of a text, the way
the topics shift over the course of the document. Barzilay and Lee (2004) introduced
an HMM model for capturing topics for coherence, and later work expanded this
intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li
and Jurafsky 2017).

BIBLIOGRAPHICAL AND HISTORICAL NOTES
463
The relationship between explicit and implicit discourse connectives has been
a fruitful one for research. Marcu and Echihabi (2002) ﬁrst proposed to use sen-
tences with explicit relations to help provide training data for implicit relations, by
removing the explicit relations and trying to re-predict them as a way of improv-
ing performance on implicit connectives; this idea was reﬁned by Sporleder and
Lascarides (2005), (Pitler et al., 2009), and Rutherford and Xue (2015). This rela-
tionship can also be used as a way to create discourse-aware representations. The
DisSent algorithm (Nie et al., 2019) creates the task of predicting explicit discourse
markers between two sentences. They show that representations learned to be good
at this task also function as powerful sentence representations for other discourse
tasks.
The idea of entity-based coherence seems to have arisen in multiple ﬁelds in the
mid-1970s, in functional linguistics (Chafe, 1976), in the psychology of discourse
processing (Kintsch and Van Dijk, 1978), and in the roughly contemporaneous work
of Grosz, Sidner, Joshi, and their colleagues. Grosz (1977a) addressed the focus
of attention that conversational participants maintain as the discourse unfolds. She
deﬁned two levels of focus; entities relevant to the entire discourse were said to
be in global focus, whereas entities that are locally in focus (i.e., most central to a
particular utterance) were said to be in immediate focus. Sidner (1979, 1983) de-
scribed a method for tracking (immediate) discourse foci and their use in resolving
pronouns and demonstrative noun phrases. She made a distinction between the cur-
rent discourse focus and potential foci, which are the predecessors to the backward-
and forward-looking centers of Centering theory, respectively. The name and further
roots of the centering approach lie in papers by Joshi and Kuhn (1979) and Joshi and
Weinstein (1981), who addressed the relationship between immediate focus and the
inferences required to integrate the current utterance into the discourse model. Grosz
et al. (1983) integrated this work with the prior work of Sidner and Grosz. This led
to a manuscript on centering which, while widely circulated since 1986, remained
unpublished until Grosz et al. (1995). A collection of centering papers appears in
Walker et al. (1998). See Karamanis et al. (2004) and Poesio et al. (2004) for a
deeper exploration of centering and its parameterizations, and the History section of
Chapter 22 for more on the use of centering on coreference.
The grid model of entity-based coherence was ﬁrst proposed by Barzilay and
Lapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then
extended by them Barzilay and Lapata (2008) and others with additional features
(Elsner and Charniak 2008, (2011), Feng et al. 2014, Lin et al. 2011) a model that
projects entities into a global graph for the discourse (Guinaudeau and Strube 2013,
Mesgar and Strube 2016), and a convolutional model to capture longer-range entity
dependencies (Nguyen and Joty, 2017).
Theories of discourse coherence have also been used in algorithms for interpret-
ing discourse-level linguistic phenomena, including verb phrase ellipsis and gapping
(Asher, 1993; Kehler, 1993), and tense interpretation (Lascarides and Asher 1993,
Kehler 1994, Kehler 2000). An extensive investigation into the relationship be-
tween coherence relations and discourse connectives can be found in Knott and Dale
(1994).
Useful surveys of discourse processing and structure include Stede (2011) and
Webber et al. (2012).

464
CHAPTER 23
•
DISCOURSE COHERENCE
Exercises
23.1 Finish the Centering Theory processing of the last two utterances of (23.30),
and show how (23.29) would be processed. Does the algorithm indeed mark
(23.29) as less coherent?
23.2 Select an editorial column from your favorite newspaper, and determine the
discourse structure for a 10–20 sentence portion. What problems did you
encounter? Were you helped by superﬁcial cues the speaker included (e.g.,
discourse connectives) in any places?

CHAPTER
24
Summarization
Placeholder
465

466
CHAPTER 25
•
QUESTION ANSWERING
CHAPTER
25
Question Answering
The quest for knowledge is deeply human, and so it is not surprising that practi-
cally as soon as there were computers we were asking them questions. By the early
1960s, systems used the two major paradigms of question answering—information-
retrieval-based and knowledge-based—to answer questions about baseball statis-
tics or scientiﬁc facts. Even imaginary computers got into the act. Deep Thought,
the computer that Douglas Adams invented in The Hitchhiker’s Guide to the Galaxy,
managed to answer “the Ultimate Question Of Life, The Universe, and Everything”.1
In 2011, IBM’s Watson question-answering system won the TV game-show Jeop-
ardy! using a hybrid architecture that surpassed humans at answering questions like
WILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPAL-
ITIES OF WALLACHIA AND MOLDOVIA” INSPIRED THIS AU-
THOR’S MOST FAMOUS NOVEL2
Most question answering systems focus on factoid questions, questions that can
be answered with simple facts expressed in short texts. The answers to the questions
below can be expressed by a personal name, temporal expression, or location:
(25.1) Who founded Virgin Airlines?
(25.2) What is the average age of the onset of autism?
(25.3) Where is Apple Computer based?
In this chapter we describe the two major paradigms for factoid question an-
swering. Information-retrieval or IR-based question answering relies on the vast
quantities of textual information on the web or in collections like PubMed. Given
a user question, information retrieval techniques ﬁrst ﬁnd relevant documents and
passages. Then systems (feature-based, neural, or both) use reading comprehen-
sion algorithms to read these retrieved documents or passages and draw an answer
directly from spans of text.
In the second paradigm, knowledge-based question answering, a system in-
stead builds a semantic representation of the query, mapping What states border
Texas? to the logical representation: λx.state(x) ∧borders(x,texas), or When was
Ada Lovelace born? to the gapped relation: birth-year (Ada Lovelace, ?x).
These meaning representations are then used to query databases of facts.
Finally, large industrial systems like the DeepQA system in IBM’s Watson are
often hybrids, using both text datasets and structured knowledge bases to answer
questions. DeepQA ﬁnds many candidate answers in both knowledge bases and in
textual sources, and then scores each candidate answer using knowledge sources like
geospatial databases, taxonomical classiﬁcation, or other textual sources.
We describe IR-based approaches (including neural reading comprehension sys-
tems) in the next section, followed by sections on knowledge-based systems, on
Watson Deep QA, and a discussion of evaluation.
1
The answer was 42, but unfortunately the details of the question were never revealed.
2
The answer, of course, is ‘Who is Bram Stoker’, and the novel was Dracula.

25.1
•
IR-BASED FACTOID QUESTION ANSWERING
467
25.1
IR-based Factoid Question Answering
The goal of information retrieval based question answering is to answer a user’s
question by ﬁnding short text segments on the web or some other collection of doc-
uments. Figure 25.1 shows some sample factoid questions and their answers.
Question
Answer
Where is the Louvre Museum located?
in Paris, France
What’s the abbreviation for limited partnership?
L.P.
What are the names of Odin’s ravens?
Huginn and Muninn
What currency is used in China?
the yuan
What kind of nuts are used in marzipan?
almonds
What instrument does Max Roach play?
drums
What’s the ofﬁcial language of Algeria?
Arabic
How many pounds are there in a stone?
14
Figure 25.1
Some sample factoid questions and their answers.
Figure 25.2 shows the three phases of an IR-based factoid question-answering
system: question processing, passage retrieval and ranking, and answer extraction.
Document
Document
Document
Docume
nt
Docume
nt
Docume
nt
Docume
nt
Docume
nt
Question 
Processing
Document and Passage
Retrieval
Query 
Formulation
Answer Type 
Detection
Question
Passage 
Retrieval
Document 
Retrieval
Answer 
Extraction
Answer
passages
Indexing
Relevant
Docs
Document
Document
Document
Figure 25.2
IR-based factoid question answering has three stages: question processing, passage retrieval, and
answer processing.
25.1.1
Question Processing
The main goal of the question-processing phase is to extract the query: the keywords
passed to the IR system to match potential documents. Some systems additionally
extract further information such as:
• answer type: the entity type (person, location, time, etc.) of the answer.
• focus: the string of words in the question that is likely to be replaced by the
answer in any answer string found.
• question type: is this a deﬁnition question, a math question, a list question?
For example, for the question Which US state capital has the largest population?
the query processing might produce:
query: “US state capital has the largest population”
answer type: city
focus: state capital
In the next two sections we summarize the two most commonly used tasks, query
formulation and answer type detection.

468
CHAPTER 25
•
QUESTION ANSWERING
25.1.2
Query Formulation
Query formulation is the task of creating a query—a list of tokens— to send to an
information retrieval system to retrieve documents that might contain answer strings.
For question answering from the web, we can simply pass the entire question
to the web search engine, at most perhaps leaving out the question word (where,
when, etc.). For question answering from smaller sets of documents like corporate
information pages or Wikipedia, we still use an IR engine to index and search our
documents, generally using standard tf-idf cosine matching, but we might need to do
more processing. For example, for searching Wikipedia, it helps to compute tf-idf
over bigrams rather than unigrams in the query and document (Chen et al., 2017).
Or we might need to do query expansion, since while on the web the answer to a
question might appear in many different forms, one of which will probably match
the question, in smaller document sets an answer might appear only once. Query
expansion methods can add query terms in hopes of matching the particular form of
the answer as it appears, like adding morphological variants of the content words in
the question, or synonyms from a thesaurus.
A query formulation approach that is sometimes used for questioning the web is
to apply query reformulation rules to the query. The rules rephrase the question to
query
reformulation
make it look like a substring of possible declarative answers. The question “when
was the laser invented?” might be reformulated as “the laser was invented”; the
question “where is the Valley of the Kings?” as “the Valley of the Kings is located
in”. Here are some sample handwritten reformulation rules from Lin (2007):
(25.4) wh-word did A verb B →...A verb+ed B
(25.5) Where is A →A is located in
25.1.3
Answer Types
Some systems make use of question classiﬁcation, the task of ﬁnding the answer
question
classiﬁcation
type, the named-entity categorizing the answer. A question like “Who founded Vir-
answer type
gin Airlines?” expects an answer of type PERSON. A question like “What Canadian
city has the largest population?” expects an answer of type CITY. If we know that
the answer type for a question is a person, we can avoid examining every sentence
in the document collection, instead focusing on sentences mentioning people.
While answer types might just be the named entities like PERSON, LOCATION,
and ORGANIZATION described in Chapter 18, we can also use a larger hierarchical
set of answer types called an answer type taxonomy. Such taxonomies can be built
answer type
taxonomy
automatically, from resources like WordNet (Harabagiu et al. 2000, Pasca 2003), or
they can be designed by hand. Figure 25.4 shows one such hand-built ontology, the
Li and Roth (2005) tagset; a subset is also shown in Fig. 25.3. In this hierarchical
tagset, each question can be labeled with a coarse-grained tag like HUMAN or a ﬁne-
grained tag like HUMAN:DESCRIPTION, HUMAN:GROUP, HUMAN:IND, and so on.
The HUMAN:DESCRIPTION type is often called a BIOGRAPHY question because the
answer is required to give a brief biography of the person rather than just a name.
Question classiﬁers can be built by hand-writing rules like the following rule
from (Hovy et al., 2002) for detecting the answer type BIOGRAPHY:
(25.6) who {is | was | are | were} PERSON
Most question classiﬁers, however, are based on supervised learning, trained on
databases of questions that have been hand-labeled with an answer type (Li and
Roth, 2002). Either feature-based or neural methods can be used. Feature based

25.1
•
IR-BASED FACTOID QUESTION ANSWERING
469
NUMERIC
ABBREVIATION
ENTITY
DESCRIPTION
LOCATION
HUMAN
Li & Roth
Taxonomy
country
city
state
reason
definition
food
currency
animal
date
distance
percent
size
money
individual
title
group
expression
abbreviation
Figure 25.3
A subset of the Li and Roth (2005) answer types.
methods rely on words in the questions and their embeddings, the part-of-speech of
each word, and named entities in the questions. Often, a single word in the question
gives extra information about the answer type, and its identity is used as a feature.
This word is sometimes called the answer type word or question headword, and
may be deﬁned as the headword of the ﬁrst NP after the question’s wh-word; head-
words are indicated in boldface in the following examples:
(25.7) Which city in China has the largest number of foreign ﬁnancial companies?
(25.8) What is the state ﬂower of California?
In general, question classiﬁcation accuracies are relatively high on easy ques-
tion types like PERSON, LOCATION, and TIME questions; detecting REASON and
DESCRIPTION questions can be much harder.
25.1.4
Document and Passage Retrieval
The IR query produced from the question processing stage is sent to an IR engine,
resulting in a set of documents ranked by their relevance to the query. Because
most answer-extraction methods are designed to apply to smaller regions such as
paragraphs, QA systems next divide the top n documents into smaller passages such
passages
as sections, paragraphs, or sentences. These might be already segmented in the
source document or we might need to run a paragraph segmentation algorithm.
The simplest form of passage retrieval is then to simply pass along every pas-
passage
retrieval
sage to the answer extraction stage. A more sophisticated variant is to ﬁlter the
passages by running a named entity or answer type classiﬁcation on the retrieved
passages, discarding passages that don’t contain the answer type of the question. It’s
also possible to use supervised learning to fully rank the remaining passages, using
features like:
• The number of named entities of the right type in the passage
• The number of question keywords in the passage
• The longest exact sequence of question keywords that occurs in the passage
• The rank of the document from which the passage was extracted
• The proximity of the keywords from the original query to each other (Pasca 2003,
Monz 2004).
• The number of n-grams that overlap between the passage and the question
(Brill et al., 2002).
For question answering from the web we can instead take snippets from a Web
snippets
search engine as the passages.

470
CHAPTER 25
•
QUESTION ANSWERING
Tag
Example
ABBREVIATION
abb
What’s the abbreviation for limited partnership?
exp
What does the “c” stand for in the equation E=mc2?
DESCRIPTION
deﬁnition
What are tannins?
description
What are the words to the Canadian National anthem?
manner
How can you get rust stains out of clothing?
reason
What caused the Titanic to sink?
ENTITY
animal
What are the names of Odin’s ravens?
body
What part of your body contains the corpus callosum?
color
What colors make up a rainbow?
creative
In what book can I ﬁnd the story of Aladdin?
currency
What currency is used in China?
disease/medicine
What does Salk vaccine prevent?
event
What war involved the battle of Chapultepec?
food
What kind of nuts are used in marzipan?
instrument
What instrument does Max Roach play?
lang
What’s the ofﬁcial language of Algeria?
letter
What letter appears on the cold-water tap in Spain?
other
What is the name of King Arthur’s sword?
plant
What are some fragrant white climbing roses?
product
What is the fastest computer?
religion
What religion has the most members?
sport
What was the name of the ball game played by the Mayans?
substance
What fuel do airplanes use?
symbol
What is the chemical symbol for nitrogen?
technique
What is the best way to remove wallpaper?
term
How do you say “ Grandma” in Irish?
vehicle
What was the name of Captain Bligh’s ship?
word
What’s the singular of dice?
HUMAN
description
Who was Confucius?
group
What are the major companies that are part of Dow Jones?
ind
Who was the ﬁrst Russian astronaut to do a spacewalk?
title
What was Queen Victoria’s title regarding India?
LOCATION
city
What’s the oldest capital city in the Americas?
country
What country borders the most others?
mountain
What is the highest peak in Africa?
other
What river runs through Liverpool?
state
What states do not have state income tax?
NUMERIC
code
What is the telephone number for the University of Colorado?
count
About how many soldiers died in World War II?
date
What is the date of Boxing Day?
distance
How long was Mao’s 1930s Long March?
money
How much did a McDonald’s hamburger cost in 1963?
order
Where does Shanghai rank among world cities in population?
other
What is the population of Mexico?
period
What was the average life expectancy during the Stone Age?
percent
What fraction of a beaver’s life is spent swimming?
temp
How hot should the oven be when making Peachy Oat Mufﬁns?
speed
How fast must a spacecraft travel to escape Earth’s gravity?
size
What is the size of Argentina?
weight
How many pounds are there in a stone?
Figure 25.4
Question typology from Li and Roth (2002), (2005). Example sentences are
from their corpus of 5500 labeled questions. A question can be labeled either with a coarse-
grained tag like HUMAN or NUMERIC or with a ﬁne-grained tag like HUMAN:DESCRIPTION,
HUMAN:GROUP, HUMAN:IND, and so on.

25.1
•
IR-BASED FACTOID QUESTION ANSWERING
471
25.1.5
Answer Extraction
The ﬁnal stage of question answering is to extract a speciﬁc answer from the passage,
for example responding 29,029 feet to a question like “How tall is Mt. Everest?”.
This task is commonly modeled by span labeling: given a passage, identifying the
span of text which constitutes an answer.
span
A simple baseline algorithm for answer extraction is to run a named entity tagger
on the candidate passage and return whatever span in the passage is the correct an-
swer type. Thus, in the following examples, the underlined named entities would be
extracted from the passages as the answer to the HUMAN and DISTANCE-QUANTITY
questions:
“Who is the prime minister of India?”
Manmohan Singh, Prime Minister of India, had told left leaders that the
deal would not be renegotiated.
“How tall is Mt. Everest?”
The ofﬁcial height of Mount Everest is 29029 feet
Unfortunately, the answers to many questions, such as DEFINITION questions,
don’t tend to be of a particular named entity type. For this reason modern work on
answer extraction uses more sophisticated algorithms, generally based on supervised
learning. The next section introduces a simple feature-based classiﬁer, after which
we turn to modern neural algorithms.
25.1.6
Feature-based Answer Extraction
Supervised learning approaches to answer extraction train classiﬁers to decide if a
span or a sentence contains an answer. One obviously useful feature is the answer
type feature of the above baseline algorithm. Hand-written regular expression pat-
terns also play a role, such as the sample patterns for deﬁnition questions in Fig. 25.5.
Pattern
Question
Answer
<AP> such as <QP>
What is autism?
“, developmental disorders such as autism”
<QP>, a <AP>
What is a caldera? “the Long Valley caldera, a volcanic crater 19
miles long”
Figure 25.5
Some answer-extraction patterns using the answer phrase (AP) and question
phrase (QP) for deﬁnition questions (Pasca, 2003).
Other features in such classiﬁers include:
Answer type match: True if the candidate answer contains a phrase with the cor-
rect answer type.
Pattern match: The identity of a pattern that matches the candidate answer.
Number of matched question keywords: How many question keywords are con-
tained in the candidate answer.
Keyword distance: The distance between the candidate answer and query key-
words.
Novelty factor: True if at least one word in the candidate answer is novel, that is,
not in the query.
Apposition features: True if the candidate answer is an appositive to a phrase con-
taining many question terms. Can be approximated by the number of question
terms separated from the candidate answer through at most three words and
one comma (Pasca, 2003).

472
CHAPTER 25
•
QUESTION ANSWERING
Punctuation location: True if the candidate answer is immediately followed by a
comma, period, quotation marks, semicolon, or exclamation mark.
Sequences of question terms: The length of the longest sequence of question
terms that occurs in the candidate answer.
25.1.7
N-gram tiling answer extraction
An alternative approach to answer extraction, used solely in Web search, is based
on n-gram tiling, an approach that relies on the redundancy of the web (Brill
n-gram tiling
et al. 2002, Lin 2007). This simpliﬁed method begins with the snippets returned
from the Web search engine, produced by a reformulated query. In the ﬁrst step,
n-gram mining, every unigram, bigram, and trigram occurring in the snippet is ex-
tracted and weighted. The weight is a function of the number of snippets in which
the n-gram occurred, and the weight of the query reformulation pattern that returned
it. In the n-gram ﬁltering step, n-grams are scored by how well they match the
predicted answer type. These scores are computed by handwritten ﬁlters built for
each answer type. Finally, an n-gram tiling algorithm concatenates overlapping n-
gram fragments into longer answers. A standard greedy method is to start with the
highest-scoring candidate and try to tile each other candidate with this candidate.
The best-scoring concatenation is added to the set of candidates, the lower-scoring
candidate is removed, and the process continues until a single answer is built.
25.1.8
Neural Answer Extraction
Neural network approaches to answer extraction draw on the intuition that a question
and its answer are semantically similar in some appropriate way. As we’ll see, this
intuition can be ﬂeshed out by computing an embedding for the question and an
embedding for each token of the passage, and then selecting passage spans whose
embeddings are closest to the question embedding.
Reading Comprehension
Neural answer extractors are often designed in the context of the reading compre-
hension task. It was Hirschman et al. (1999) who ﬁrst proposed to take children’s
reading
comprehension
reading comprehension tests—pedagogical instruments in which a child is given a
passage to read and must answer questions about it—and use them to evaluate ma-
chine text comprehension algorithm. They acquired a corpus of 120 passages with 5
questions each designed for 3rd-6th grade children, built an answer extraction sys-
tem, and measured how well the answers given by their system corresponded to the
answer key from the test’s publisher.
Since then reading comprehension has become both a task in itself, as a useful
way to measure natural language understanding performance, but also as (sometimes
called the reader component of question answerers).
Reading Comprehension Datasets.
Modern reading comprehension systems tend
to use collections of questions that are designed speciﬁcally for NLP, and so are large
enough for training supervised learning systems. For example the Stanford Question
Answering Dataset (SQuAD) consists of passages from Wikipedia and associated
SQuAD
questions whose answers are spans from the passage, as well as some questions that
are designed to be unanswerable (Rajpurkar et al. 2016, Rajpurkar et al. 2018); a
total of just over 150,000 questions. Fig. 25.6 shows a (shortened) excerpt from a
SQUAD 2.0 passage together with three questions and their answer spans.

25.1
•
IR-BASED FACTOID QUESTION ANSWERING
473
Beyonc´e Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter,
record producer and actress.
Born and raised in Houston, Texas, she performed in various
singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer
of R&B girl-group Destiny’s Child. Managed by her father, Mathew Knowles, the group became
one of the world’s best-selling girl groups of all time. Their hiatus saw the release of Beyonc´e’s
debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned
ﬁve Grammy Awards and featured the Billboard Hot 100 number-one singles “Crazy in Love” and
“Baby Boy”.
Q: “In what city and state did Beyonc´e grow up?”
A: “Houston, Texas”
Q: “What areas did Beyonc´e compete in when she was growing up?”
A: “singing and dancing”
Q: “When did Beyonc´e release Dangerously in Love?”
A: “2003”
Figure 25.6
A (Wikipedia) passage from the SQuAD 2.0 dataset (Rajpurkar et al., 2018) with 3 sample
questions and the labeled answer spans.
SQuAD was built by having humans write questions for a given Wikipedia
passage and choose the answer span. Other datasets used similar techniques; the
NewsQA dataset consists of 100,000 question-answer pairs from CNN news arti-
cles. For other datasets like WikiQA the span is the entire sentence containing the
answer (Yang et al., 2015); the task of choosing a sentence rather than a smaller
answer span is sometimes called the sentence selection task.
sentence
selection
25.1.9
A bi-LSTM-based Reading Comprehension Algorithm
Neural algorithms for reading comprehension are given a question q of l tokens
q1,...,ql and a passage p of m tokens p1,..., pm. Their goal is to compute, for each
token pi the probability pstart(i) that pi is the start of the answer span, and the
probability pend(i) that pi is the end of the answer span.
Fig. 25.7 shows the architecture of the Document Reader component of the
DrQA system of Chen et al. (2017). Like most such systems, DrQA builds an
embedding for the question, builds an embedding for each token in the passage,
computes a similarity function between the question and each passage word in con-
text, and then uses the question-passage similarity scores to decide where the answer
span starts and ends.
Let’s consider the algorithm in detail, following closely the description in Chen
et al. (2017). The question is represented by a single embedding q, which is a
weighted sum of representations for each question word qi.
It is computed by
passing the series of embeddings PE(q1),...,E(ql) of question words through an
RNN (such as a bi-LSTM shown in Fig. 25.7). The resulting hidden representations
{q1,...,ql} are combined by a weighted sum
q =
X
j
b jq j
(25.9)
The weight bj is a measure of the relevance of each question word, and relies on a
learned weight vector w:
bj =
exp(w ·q j)
P
j′ exp(w ·q′
j)
(25.10)

474
CHAPTER 25
•
QUESTION ANSWERING
Beyonce’s
debut
album
LSTM1
LSTM1
LSTM1
LSTM2
LSTM2
LSTM2
GloVe
PER
NNP
When
did
Beyonce
Passage
Question
LSTM1
LSTM1
LSTM1
LSTM2
LSTM2
LSTM2
GloVe
GloVe
GloVe
…
Attention
Weighted sum
similarity
q
p2
p3
similarity
q
q
similarity
…
q-align1
GloVe
GloVe
pstart(1)
pend(1)
pstart(3)
pend(3)
…
…
…
O
NN
GloVe
GloVe
q-align2
1
0
O
NN
0
q-align3
GloVe
GloVe
Att
Att
p1
p1 p2 p3
~p1 p2 p3
~
~
q1 q2 q3
Figure 25.7
The question answering system of Chen et al. (2017), considering part of the question When did
Beyonc´e release Dangerously in Love? and the passage starting Beyonc´e’s debut album, Dangerously in Love
(2003).
To compute the passage embedding {p1,...,pm} we ﬁrst form an input represen-
tation ˜p = {˜p1,...,˜pm} by concatenating four components:
• An embedding for each word E(pi) such as from GLoVE (Pennington et al.,
2014).
• Token features like the part of speech of pi, or the named entity tag of pi, from
running POS or NER taggers.
• Exact match features representing whether the passage word pi occurred in
the question: 1(pi ∈q). Separate exact match features might be used for
lemmatized or lower-cased versions of the tokens.
• Aligned question embedding: In addition to the exact match features, many
QA systems use an attention mechanism to give a more sophisticated model of
similarity between the passage and question words, such as similar but non-
identical words like release and singles. For example a weighted similarity
P
j ai,jE(q j) can be used, where the attention weight ai,j encodes the simi-
larity between pi and each question word qj. This attention weight can be
computed as the dot product between functions α of the word embeddings of
the question and passage:
qi,j =
exp(α(E(pi))·α(E(qj)))
P
j′ exp(α(E(pi))·α(E(q′
j)))
(25.11)
α(·) can be a simple feed forward network.
We then pass ˜p through a biLSTM:
{p1,...,pm}) = RNN({˜p1,...,˜pm}
(25.12)
The result of the previous two steps is a single question embedding q and a
representation for each word in the passage {p1,...,pm}. In order to ﬁnd the an-
swer span, we can train two separate classiﬁers, one to compute for each pi the

25.1
•
IR-BASED FACTOID QUESTION ANSWERING
475
probability pstart(i) that pi is the start of the answer span, and one to compute the
probability pend(i). While the classiﬁers could just take the dot product between
the passage and question embeddings as input, it turns out to work better to learn a
more sophisticated similarity function, like a bilinear attention layer W:
pstart(i) ∝exp(piWsq)
pend(i) ∝exp(piWeq)
(25.13)
These neural answer extractors can be trained end-to-end by using datasets like
SQuAD.
25.1.10
BERT-based Question Answering
The power of contextual embeddings allow question answering models based on
BERT contextual embeddings and the transformer architecture to achieve even higher
accuracy (Fig. 25.8).
BERT
[CLS] Tok 1
Tok N
…
[SEP]
Tok 1
Tok M
…
Question
Paragraph
ECLS
E1
EN
ESEP
E1’
EM’
C
T1
TN
TSEP
T1’
TM’
…
…
…
…
S
E
Pstart1
Pend1
. .
PstartM
…
PendM
…
Figure 25.8
The BERT model for span-based question answering from reading-
comprehension-based question answering tasks. Figure after Devlin et al. (2019).
Recall from Chapter 10 that BERT represents two input strings as a sequence of
wordpiece tokens separated with a [SEP] token. The pre-trained BERT model will
produce an output token embedding T ′
i for every paragraph token i′. For span-based
question answering, we represent the question as the ﬁrst sequence and, the para-
graph as the second sequence. We’ll also need to add some structure to the output
head that will be trained in the ﬁne-tuning phase. We’ll add two new embeddings: a
span-start embedding S and a span-end embedding E. To get a span-start probability
for each output token T ′
i , we compute the dot product between S and T ′
i and then
normalize over all tokens T ′
i in the paragraph:
Pstarti =
eS·Ti
P
j eS·Tj
(25.14)
We do the analogous thing to compute a span-end probability:
Pendi =
eE·Ti
P
j eE·Tj
(25.15)

476
CHAPTER 25
•
QUESTION ANSWERING
The score of a candidate span from position i to j is S · Ti + E · Tj, and the highest
scoring span in which j ≥i is chosen is the model prediction. The training objective
for ﬁne-tuning is the sum of the log-likelihoods of the correct start and end positions
for each observation.
25.2
Knowledge-based Question Answering
While an enormous amount of information is encoded in the vast amount of text
on the web, information obviously also exists in more structured forms. We use
the term knowledge-based question answering for the idea of answering a natural
language question by mapping it to a query over a structured database. Like the text-
based paradigm for question answering, this approach dates back to the earliest days
of natural language processing, with systems like BASEBALL (Green et al., 1961)
that answered questions from a structured database of baseball games and stats.
Systems for mapping from a text string to any logical form are called seman-
tic parsers. Semantic parsers for question answering usually map either to some
version of predicate calculus or a query language like SQL or SPARQL, as in the
examples in Fig. 25.9.
Question
Logical form
When was Ada Lovelace born?
birth-year (Ada Lovelace, ?x)
What states border Texas?
λ x.state(x) ∧borders(x,texas)
What is the largest state
argmax(λx.state(x),λx.size(x))
How many people survived the sinking of
the Titanic
(count (!fb:event.disaster.survivors
fb:en.sinking of the titanic))
Figure 25.9
Sample logical forms produced by a semantic parser for question answering. These range from
simple relations like birth-year, or relations normalized to databases like Freebase, to full predicate calculus.
The logical form of the question is thus either in the form of a query or can easily
be converted into one. The database can be a full relational database, or simpler
structured databases like sets of RDF triples. Recall from Chapter 18 that an RDF
triple is a 3-tuple, a predicate with two arguments, expressing some simple relation
or proposition. Popular ontologies like Freebase (Bollacker et al., 2008) or DBpedia
(Bizer et al., 2009) have large numbers of triples derived from Wikipedia infoboxes,
the structured tables associated with certain Wikipedia articles.
The simplest formation of the knowledge-based question answering task is to
answer factoid questions that ask about one of the missing arguments in a triple.
Consider an RDF triple like the following:
subject
predicate object
Ada Lovelace birth-year 1815
This triple can be used to answer text questions like ‘When was Ada Lovelace
born?’ or ‘Who was born in 1815?’. Question answering in this paradigm requires
mapping from textual strings like ”When was ... born” to canonical relations in the
knowledge base like birth-year. We might sketch this task as:
“When was Ada Lovelace born?”
→
birth-year (Ada Lovelace, ?x)
“What is the capital of England?”
→
capital-city(?x, England)

25.2
•
KNOWLEDGE-BASED QUESTION ANSWERING
477
25.2.1
Rule-based Methods
For relations that are very frequent, it may be worthwhile to write handwritten rules
to extract relations from the question, just as we saw in Section 18.2. For example,
to extract the birth-year relation, we could write patterns that search for the question
word When, a main verb like born, and then extract the named entity argument of
the verb.
25.2.2
Supervised Methods
In some cases we have supervised data, consisting of a set of questions paired with
their correct logical form like the examples in Fig. 25.9. The task is then to take
those pairs of training tuples and produce a system that maps from new questions to
their logical forms.
Most supervised algorithms for learning to answer these simple questions about
relations ﬁrst parse the questions and then align the parse trees to the logical form.
Generally these systems bootstrap by having a small set of rules for building this
mapping, and an initial lexicon as well. For example, a system might have built-
in strings for each of the entities in the system (Texas, Ada Lovelace), and then
have simple default rules mapping fragments of the question parse tree to particular
relations:
Who V ENTITY →relation( ?x, entity)
nsubj
dobj
When V ENTITY →relation( ?x, entity)
tmod
nsubj
Then given these rules and the lexicon, a training tuple like the following:
“When was Ada Lovelace born?”
→
birth-year (Ada Lovelace, ?x)
would ﬁrst be parsed, resulting in the following mapping.
When was Ada Lovelace born →birth-year(Ada Lovelace, ?x)
tmod
nsubj
From many pairs like this, we could induce mappings between pieces of parse
fragment, such as the mapping between the parse fragment on the left and the rela-
tion on the right:
When was · born →birth-year( , ?x)
tmod
nsubj
A supervised system would thus parse each tuple in the training set and induce a
bigger set of such speciﬁc rules, allowing it to map unseen examples of “When was
X born?” questions to the birth-year relation. Rules can furthermore be associ-
ated with counts based on the number of times the rule is used to parse the training
data. Like rule counts for probabilistic grammars, these can be normalized into prob-
abilities. The probabilities can then be used to choose the highest probability parse
for sentences with multiple semantic interpretations.

478
CHAPTER 25
•
QUESTION ANSWERING
The supervised approach can be extended to deal with more complex questions
that are not just about single relations. Consider the question What is the biggest
state bordering Texas? —taken from the GeoQuery database of questions on U.S.
Geography (Zelle and Mooney, 1996)—with the semantic form: argmax(λx.state(x)∧
borders(x,texas),λx.size(x)) This question has much more complex structures than
the simple single-relation questions we considered above, such as the argmax func-
tion, the mapping of the word biggest to size and so on. Zettlemoyer and Collins
(2005) shows how more complex default rules (along with richer syntactic struc-
tures) can be used to learn to map from text sentences to more complex logical
forms. The rules take the training set’s pairings of sentence and meaning as above
and use the complex rules to break each training example down into smaller tuples
that can then be recombined to parse new sentences.
25.2.3
Dealing with Variation: Semi-Supervised Methods
Because it is difﬁcult to create training sets with questions labeled with their mean-
ing representation, supervised datasets can’t cover the wide variety of forms that
even simple factoid questions can take. For this reason most techniques for mapping
factoid questions to the canonical relations or other structures in knowledge bases
ﬁnd some way to make use of textual redundancy.
The most common source of redundancy, of course, is the web, which contains
vast numbers of textual variants expressing any relation. For this reason, most meth-
ods make some use of web text, either via semi-supervised methods like distant
supervision or unsupervised methods like open information extraction, both intro-
duced in Chapter 18. For example the REVERB open information extractor (Fader
et al., 2011) extracts billions of (subject, relation, object) triples of strings from the
web, such as (“Ada Lovelace”,“was born in”, “1815”). By aligning these strings
with a canonical knowledge source like Wikipedia, we create new relations that can
be queried while simultaneously learning to map between the words in question and
canonical relations.
To align a REVERB triple with a canonical knowledge source we ﬁrst align
the arguments and then the predicate. Recall from Chapter 22 that linking a string
like “Ada Lovelace” with a Wikipedia page is called entity linking; we thus rep-
resent the concept ‘Ada Lovelace’ by a unique identiﬁer of a Wikipedia page. If
this subject string is not associated with a unique page on Wikipedia, we can dis-
ambiguate which page is being sought, for example by using the cosine distance
between the triple string (‘Ada Lovelace was born in 1815’) and each candidate
Wikipedia page. Date strings like ‘1815’ can be turned into a normalized form using
standard tools for temporal normalization like SUTime (Chang and Manning, 2012).
Once we’ve aligned the arguments, we align the predicates. Given the Freebase
relation people.person.birthdate(ada lovelace,1815) and the string ‘Ada
Lovelace was born in 1815’, having linked Ada Lovelace and normalized 1815, we
learn the mapping between the string ‘was born in’ and the relation people.person.birthdate.
In the simplest case, this can be done by aligning the relation with the string of words
in between the arguments; more complex alignment algorithms like IBM Model 1
(Chapter 11) can be used. Then if a phrase aligns with a predicate across many
entities, it can be extracted into a lexicon for mapping questions to relations.
Here are some examples from such a resulting lexicon, produced by Berant
et al. (2013), giving many variants of phrases that align with the Freebase relation
country.capital between a country and its capital city:
Other useful sources of linguistic redundancy are paraphrase databases. For ex-

25.3
•
USING MULTIPLE INFORMATION SOURCES: IBM’S WATSON
479
capital of
capital city of
become capital of
capitol of
national capital of
ofﬁcial capital of
political capital of
administrative capital of
beautiful capital of
capitol city of
remain capital of
make capital of
political center of
bustling capital of
capital city in
cosmopolitan capital of
move its capital to
modern capital of
federal capital of
beautiful capital city of
administrative capital city of
Figure 25.10
Some phrases that align with the Freebase relation country.capital from
Berant et al. (2013).
ample the site wikianswers.com contains millions of pairs of questions that users
have tagged as having the same meaning, 18 million of which have been collected
in the PARALEX corpus (Fader et al., 2013). Here’s an example:
Q: What are the green blobs in plant cells?
Lemmatized synonyms from PARALEX:
what be the green blob in plant cell?
what be green part in plant cell?
what be the green part of a plant cell?
what be the green substance in plant cell?
what be the part of plant cell that give it green color?
what cell part do plant have that enable the plant to be give a green color?
what part of the plant cell turn it green?
part of the plant cell where the cell get it green color?
the green part in a plant be call?
the part of the plant cell that make the plant green be call?
The resulting millions of pairs of question paraphrases can be aligned to each
other using MT alignment approaches to create an MT-style phrase table for trans-
lating from question phrases to synonymous phrases. These can be used by question
answering algorithms to generate all paraphrases of a question as part of the process
of ﬁnding an answer (Fader et al. 2013, Berant and Liang 2014).
25.3
Using multiple information sources: IBM’s Watson
Of course there is no reason to limit ourselves to just text-based or knowledge-based
resources for question answering. The Watson system from IBM that won the Jeop-
ardy! challenge in 2011 is an example of a system that relies on a wide variety of
resources to answer questions.
Figure 25.11 shows the 4 stages of the DeepQA system that is the question an-
swering component of Watson.
The ﬁrst stage is question processing. The DeepQA system runs parsing, named
entity tagging, and relation extraction on the question. Then, like the text-based
systems in Section 25.1, the DeepQA system extracts the focus, the answer type
(also called the lexical answer type or LAT), and performs question classiﬁcation
and question sectioning.
Consider these Jeopardy! examples, with a category followed by a question:
Poets and Poetry: He was a bank clerk in the Yukon before he published
“Songs of a Sourdough” in 1907.
THEATRE: A new play based on this Sir Arthur Conan Doyle canine

480
CHAPTER 25
•
QUESTION ANSWERING
Document
Document
Document
(1) Question 
Processing
From Text Resources
Focus Detection
Lexical
 Answer Type 
Detection
Question
Document 
and 
Passsage 
Retrieval
passages
Document
Document
Document
Question
Classification
Parsing
Named Entity
 Tagging
Relation Extraction
Coreference
From Structured Data
Relation 
Retrieval
DBPedia
Freebase
(2) Candidate Answer Generation
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
Candidate
Answer
(3) Candidate 
Answer 
Scoring
Evidence 
Retrieval
and scoring
Answer
Extraction
Document titles
Anchor text
Text
Evidence
Sources
(4) 
Confidence
Merging 
and 
Ranking
Text
Evidence
Sources
Time from 
DBPedia
Space from 
Facebook
Answer 
Type
Answer
and
Conﬁdence
Candidate
Answer
+ 
Conﬁdence
Candidate
Answer
+ 
Conﬁdence
Candidate
Answer
+ 
Conﬁdence
Candidate
Answer
+ 
Conﬁdence
Candidate
Answer
+ 
Conﬁdence
Logistic
Regression
Answer
Ranker
Merge
Equivalent
Answers
Figure 25.11
The 4 broad stages of Watson QA: (1) Question Processing, (2) Candidate Answer Generation,
(3) Candidate Answer Scoring, and (4) Answer Merging and Conﬁdence Scoring.
classic opened on the London stage in 2007.
The questions are parsed, named entities are extracted (Sir Arthur Conan Doyle
identiﬁed as a PERSON, Yukon as a GEOPOLITICAL ENTITY, “Songs of a Sour-
dough” as a COMPOSITION), coreference is run (he is linked with clerk) and rela-
tions like the following are extracted:
authorof(focus,“Songs of a sourdough”)
publish (e1, he, “Songs of a sourdough”)
in (e2, e1, 1907)
temporallink(publish(...), 1907)
Next DeepQA extracts the question focus, shown in bold in both examples. The
focus
focus is the part of the question that co-refers with the answer, used for example to
align with a supporting passage. The focus is extracted by handwritten rules—made
possible by the relatively stylized syntax of Jeopardy! questions—such as a rule
extracting any noun phrase with determiner “this” as in the Conan Doyle example,
and rules extracting pronouns like she, he, hers, him, as in the poet example.
The lexical answer type (shown in blue above) is a word or words which tell
lexical answer
type
us something about the semantic type of the answer. Because of the wide variety
of questions in Jeopardy!, Jeopardy! uses a far larger set of answer types than the
sets for standard factoid algorithms like the one shown in Fig. 25.4. Even a large
set of named entity tags is insufﬁcient to deﬁne a set of answer types. The DeepQA
team investigated a set of 20,000 questions and found that a named entity tagger
with over 100 named entity types covered less than half the types in these questions.
Thus DeepQA extracts a wide variety of words to be answer types; roughly 5,000
lexical answer types occurred in the 20,000 questions they investigated, often with
multiple answer types in each question.
These lexical answer types are again extracted by rules: the default rule is to
choose the syntactic headword of the focus. Other rules improve this default choice.
For example additional lexical answer types can be words in the question that are
coreferent with or have a particular syntactic relation with the focus, such as head-
words of appositives or predicative nominatives of the focus. In some cases even the
Jeopardy! category can act as a lexical answer type, if it refers to a type of entity

25.3
•
USING MULTIPLE INFORMATION SOURCES: IBM’S WATSON
481
that is compatible with the other lexical answer types. Thus in the ﬁrst case above,
he, poet, and clerk are all lexical answer types. In addition to using the rules directly
as a classiﬁer, they can instead be used as features in a logistic regression classiﬁer
that can return a probability as well as a lexical answer type.
Note that answer types function quite differently in DeepQA than the purely IR-
based factoid question answerers. In the algorithm described in Section 25.1, we
determine the answer type, and then use a strict ﬁltering algorithm only considering
text strings that have exactly that type. In DeepQA, by contrast, we extract lots of
answers, unconstrained by answer type, and a set of answer types, and then in the
later ‘candidate answer scoring’ phase, we simply score how well each answer ﬁts
the answer types as one of many sources of evidence.
Finally the question is classiﬁed by type (deﬁnition question, multiple-choice,
puzzle, ﬁll-in-the-blank). This is generally done by writing pattern-matching regular
expressions over words or parse trees.
In the second candidate answer generation stage, we combine the processed
question with external documents and other knowledge sources to suggest many
candidate answers. These candidate answers can either be extracted from text docu-
ments or from structured knowledge bases.
For structured resources like DBpedia, IMDB, or the triples produced by Open
Information Extraction, we can just query these stores with the relation and the
known entity, just as we saw in Section 25.2. Thus if we have extracted the rela-
tion authorof(focus,"Songs of a sourdough"), we can query a triple store
with authorof(?x,"Songs of a sourdough") to return the correct author.
The method for extracting answers from text depends on the type of text docu-
ments. To extract answers from normal text documents we can do passage search
just as we did in Section 25.1. As we did in that section, we need to generate a query
from the question; for DeepQA this is generally done by eliminating stop words, and
then upweighting any terms which occur in any relation with the focus. For example
from this query:
MOVIE-“ING”: Robert Redford and Paul Newman starred in this depression-
era grifter ﬂick. (Answer: “The Sting”)
the following weighted query might be extracted:
(2.0 Robert Redford) (2.0 Paul Newman) star depression era grifter (1.5 ﬂick)
The query can now be passed to a standard IR system. DeepQA also makes
use of the convenient fact that the vast majority of Jeopardy! answers are the title
of a Wikipedia document. To ﬁnd these titles, we can do a second text retrieval
pass speciﬁcally on Wikipedia documents. Then instead of extracting passages from
the retrieved Wikipedia document, we directly return the titles of the highly ranked
retrieved documents as the possible answers.
Once we have a set of passages, we need to extract candidate answers. If the
document happens to be a Wikipedia page, we can just take the title, but for other
texts, like news documents, we need other approaches. Two common approaches
are to extract all anchor texts in the document (anchor text is the text between <a>
anchor texts
and </a> used to point to a URL in an HTML page), or to extract all noun phrases
in the passage that are Wikipedia document titles.
The third candidate answer scoring stage uses many sources of evidence to
score the candidates. One of the most important is the lexical answer type. DeepQA
includes a system that takes a candidate answer and a lexical answer type and returns
a score indicating whether the candidate answer can be interpreted as a subclass or

482
CHAPTER 25
•
QUESTION ANSWERING
instance of the answer type. Consider the candidate “difﬁculty swallowing” and
the lexical answer type “manifestation”. DeepQA ﬁrst matches each of these words
with possible entities in ontologies like DBpedia and WordNet. Thus the candidate
“difﬁculty swallowing” is matched with the DBpedia entity “Dysphagia”, and then
that instance is mapped to the WordNet type “Symptom”. The answer type “man-
ifestation” is mapped to the WordNet type “Condition”. The system looks for a
link of hyponymy, instance-of or synonymy between these two types; in this case a
hyponymy relation is found between “Symptom” and “Condition”.
Other scorers are based on using time and space relations extracted from DBpe-
dia or other structured databases. For example, we can extract temporal properties
of the entity (when was a person born, when died) and then compare to time expres-
sions in the question. If a time expression in the question occurs chronologically
before a person was born, that would be evidence against this person being the an-
swer to the question.
Finally, we can use text retrieval to help retrieve evidence supporting a candidate
answer. We can retrieve passages with terms matching the question, then replace the
focus in the question with the candidate answer and measure the overlapping words
or ordering of the passage with the modiﬁed question.
The output of this stage is a set of candidate answers, each with a vector of
scoring features.
The ﬁnal answer merging and scoring step ﬁrst merges candidate answers that
are equivalent. Thus if we had extracted two candidate answers J.F.K. and John F.
Kennedy, this stage would merge the two into a single candidate. Synonym dictio-
naries are a useful resource that are created by listing all anchor text strings that point
to the same Wikipedia page; such dictionaries give large numbers of synonyms for
each Wikipedia title — e.g., JFK, John F. Kennedy, John Fitzgerald Kennedy, Sena-
tor John F. Kennedy, President Kennedy, Jack Kennedy, etc. (Spitkovsky and Chang,
2012). For common nouns, we can use morphological parsing to merge candidates
which are morphological variants.
We then merge the evidence for each variant, combining the scoring feature
vectors for the merged candidates into a single vector.
Now we have a set of candidates, each with a feature vector. A classiﬁer takes
each feature vector and assigns a conﬁdence value to this candidate answer. The
classiﬁer is trained on thousands of candidate answers, each labeled for whether it
is correct or incorrect, together with their feature vectors, and learns to predict a
probability of being a correct answer. Since, in training, there are far more incorrect
answers than correct answers, we need to use one of the standard techniques for
dealing with very imbalanced data. DeepQA uses instance weighting, assigning an
instance weight of .5 for each incorrect answer example in training. The candidate
answers are then sorted by this conﬁdence value, resulting in a single best answer.3
In summary, we’ve seen in the four stages of DeepQA that it draws on the intu-
itions of both the IR-based and knowledge-based paradigms. Indeed, Watson’s ar-
chitectural innovation is its reliance on proposing a very large number of candidate
answers from both text-based and knowledge-based sources and then developing
a wide variety of evidence features for scoring these candidates—again both text-
based and knowledge-based. See the papers mentioned at the end of the chapter for
more details.
3
The merging and ranking is actually run iteratively; ﬁrst the candidates are ranked by the classiﬁer,
giving a rough ﬁrst value for each candidate answer, then that value is used to decide which of the variants
of a name to select as the merged answer, then the merged answers are re-ranked.

25.4
•
EVALUATION OF FACTOID ANSWERS
483
25.4
Evaluation of Factoid Answers
A common evaluation metric for factoid question answering, introduced in the TREC
Q/A track in 1999, is mean reciprocal rank, or MRR. MRR assumes a test set of
mean
reciprocal rank
MRR
questions that have been human-labeled with correct answers. MRR also assumes
that systems are returning a short ranked list of answers or passages containing an-
swers. Each question is then scored according to the reciprocal of the rank of the
ﬁrst correct answer. For example if the system returned ﬁve answers but the ﬁrst
three are wrong and hence the highest-ranked correct answer is ranked fourth, the
reciprocal rank score for that question would be 1
4. Questions with return sets that
do not contain any correct answers are assigned a zero. The score of a system is
then the average of the score for each question in the set. More formally, for an
evaluation of a system returning a set of ranked answers for a test set consisting of
N questions, the MRR is deﬁned as
MRR = 1
N
N
X
i=1 s.t. ranki̸=0
1
ranki
(25.16)
Reading comprehension systems on datasets like SQuAD are often evaluated
using two metrics, both ignoring punctuation and articles (a, an, the) (Rajpurkar
et al., 2016):
• Exact match: The percentage of predicted answers that match the gold answer
exactly.
• F1 score: The average overlap between predicted and gold answers. Treat the
prediction and gold as a bag of tokens, and compute F1, averaging the F1 over
all questions.
A number of test sets are available for question answering. Early systems used
the TREC QA dataset; questions and handwritten answers for TREC competitions
from 1999 to 2004 are publicly available. TriviaQA (Joshi et al., 2017) has 650K
question-answer evidence triples, from 95K hand-created question-answer pairs to-
gether with on average six supporting evidence documents collected retrospectively
from Wikipedia and the Web.
Another family of datasets starts from WEBQUESTIONS (Berant et al., 2013),
which contains 5,810 questions asked by web users, each beginning with a wh-
word and containing exactly one entity. Questions are paired with handwritten an-
swers drawn from the Freebase page of the question’s entity. WEBQUESTIONSSP
(Yih et al., 2016) augments WEBQUESTIONS with human-created semantic parses
(SPARQL queries) for those questions answerable using Freebase. COMPLEXWEB-
QUESTIONS augments the dataset with compositional and other kinds of complex
questions, resulting in 34,689 questions, along with answers, web snippets, and
SPARQL queries. (Talmor and Berant, 2018).
There are a wide variety of datasets for training and testing reading comprehen-
sion/answer extraction in addition to the SQuAD (Rajpurkar et al., 2016) and Wik-
iQA (Yang et al., 2015) datasets discussed on page 473. The NarrativeQA (Koˇcisk´y
et al., 2018) dataset, for example, has questions based on entire long documents like
books or movie scripts, while the Question Answering in Context (QuAC) dataset
(Choi et al., 2018) has 100K questions created by two crowd workers who are asking
and answering questions about a hidden Wikipedia text.

484
CHAPTER 25
•
QUESTION ANSWERING
Others take their structure from the fact that reading comprehension tasks de-
signed for children tend to be multiple choice, with the task being to choose among
the given answers. The MCTest dataset uses this structure, with 500 ﬁctional short
stories created by crowd workers with questions and multiple choice answers (Richard-
son et al., 2013). The AI2 Reasoning Challenge (ARC) (Clark et al., 2018), has
questions that are designed to be hard to answer from simple lexical methods:
Which property of a mineral can be determined just by looking at it?
(A) luster [correct] (B) mass (C) weight (D) hardness
This ARC example is difﬁcult because the correct answer luster is unlikely to cooc-
cur frequently on the web with phrases like looking at it, while the word mineral is
highly associated with the incorrect answer hardness.
Bibliographical and Historical Notes
Question answering was one of the earliest NLP tasks, and early versions of the text-
based and knowledge-based paradigms were developed by the very early 1960s. The
text-based algorithms generally relied on simple parsing of the question and of the
sentences in the document, and then looking for matches. This approach was used
very early on (Phillips, 1960) but perhaps the most complete early system, and one
that strikingly preﬁgures modern relation-based systems, was the Protosynthex sys-
tem of Simmons et al. (1964). Given a question, Protosynthex ﬁrst formed a query
from the content words in the question, and then retrieved candidate answer sen-
tences in the document, ranked by their frequency-weighted term overlap with the
question. The query and each retrieved sentence were then parsed with dependency
parsers, and the sentence whose structure best matches the question structure se-
lected. Thus the question What do worms eat? would match worms eat grass: both
have the subject worms as a dependent of eat, in the version of dependency grammar
used at the time, while birds eat worms has birds as the subject:
What do worms eat
Worms eat grass
Birds eat worms
The alternative knowledge-based paradigm was implemented in the BASEBALL
system (Green et al., 1961). This system answered questions about baseball games
like “Where did the Red Sox play on July 7” by querying a structured database of
game information. The database was stored as a kind of attribute-value matrix with
values for attributes of each game:
Month = July
Place = Boston
Day
= 7
Game Serial No.
= 96
(Team = Red Sox, Score = 5)
(Team = Yankees, Score = 3)
Each question was constituency-parsed using the algorithm of Zellig Harris’s
TDAP project at the University of Pennsylvania, essentially a cascade of ﬁnite-

BIBLIOGRAPHICAL AND HISTORICAL NOTES
485
state transducers (see the historical discussion in Joshi and Hopely 1999 and Kart-
tunen 1999). Then in a content analysis phase each word or phrase was associated
with a program that computed parts of its meaning. Thus the phrase ‘Where’ had
code to assign the semantics Place = ?, with the result that the question “Where
did the Red Sox play on July 7” was assigned the meaning
Place = ?
Team = Red Sox
Month = July
Day = 7
The question is then matched against the database to return the answer. Simmons
(1965) summarizes other early QA systems.
Another important progenitor of the knowledge-based paradigm for question-
answering is work that used predicate calculus as the meaning representation lan-
guage. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to be
LUNAR
a natural language interface to a database of chemical facts about lunar geology. It
could answer questions like Do any samples have greater than 13 percent aluminum
by parsing them into a logical form
(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16
(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))
The rise of the web brought the information-retrieval paradigm for question an-
swering to the forefront with the TREC QA track beginning in 1999, leading to a
wide variety of factoid and non-factoid systems competing in annual evaluations.
At the same time, Hirschman et al. (1999) introduced the idea of using chil-
dren’s reading comprehension tests to evaluate machine text comprehension algo-
rithms. They acquired a corpus of 120 passages with 5 questions each designed for
3rd-6th grade children, built an answer extraction system, and measured how well
the answers given by their system corresponded to the answer key from the test’s
publisher. Their algorithm focused on word overlap as a feature; later algorithms
added named entity features and more complex similarity between the question and
the answer span (Riloff and Thelen 2000, Ng et al. 2000).
Neural reading comprehension systems drew on the insight of these early sys-
tems that answer ﬁnding should focus on question-passage similarity. Many of the
architectural outlines of modern systems were laid out in the AttentiveReader (Her-
mann et al., 2015). The idea of using passage-aligned question embeddings in the
passage computation was introduced by Lee et al. (2017). Seo et al. (2017) achieves
high performance by introducing bi-directional attention ﬂow. Chen et al. (2017)
and Clark and Gardner (2018) show how to extract answers from entire documents.
The DeepQA component of the Watson system that won the Jeopardy! challenge
is described in a series of papers in volume 56 of the IBM Journal of Research and
Development; see for example Ferrucci (2012), Lally et al. (2012), Chu-Carroll et al.
(2012), Murdock et al. (2012b), Murdock et al. (2012a), Kalyanpur et al. (2012), and
Gondek et al. (2012).
Other question-answering tasks include Quiz Bowl, which has timing consid-
erations since the question can be interrupted (Boyd-Graber et al., 2018). Question
answering is also an important function of modern personal assistant dialog systems;
see Chapter 26 for more.

486
CHAPTER 25
•
QUESTION ANSWERING
Exercises

CHAPTER
26
Dialogue Systems and Chat-
bots
Les lois de la conversation sont en g´en´eral de ne s’y appesantir sur aucun ob-
jet, mais de passer l´eg`erement, sans effort et sans affectation, d’un sujet `a un
autre ; de savoir y parler de choses frivoles comme de choses s´erieuses
The rules of conversation are, in general, not to dwell on any one subject,
but to pass lightly from one to another without effort and without affectation;
to know how to speak about trivial topics as well as serious ones;
The 18th C. Encyclopedia of Diderot, start of the entry on conversation
The literature of the fantastic abounds in inanimate objects magically endowed with
sentience and the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s
Frankenstein, there is something deeply moving about creating something and then
having a chat with it. Legend has it that after ﬁnishing his
sculpture Moses, Michelangelo thought it so lifelike that
he tapped it on the knee and commanded it to speak. Per-
haps this shouldn’t be surprising. Language is the mark
of humanity and sentience, and conversation or dialogue
conversation
dialogue
is the most fundamental and specially privileged arena
of language. It is the ﬁrst kind of language we learn as
children, and for most of us, it is the kind of language
we most commonly indulge in, whether we are ordering
curry for lunch or buying spinach, participating in busi-
ness meetings or talking with our families, booking air-
line ﬂights or complaining about the weather.
This chapter introduces the fundamental algorithms of conversational agents,
conversational
agent
or dialogue systems. These programs communicate with users in natural language
dialogue system
(text, speech, or both), and fall into two classes. Task-oriented dialogue agents use
conversation with users to help complete tasks. Dialogue agents in digital assistants
(Siri, Alexa, Google Now/Home, Cortana, etc.), give directions, control appliances,
ﬁnd restaurants, or make calls. Conversational agents can answer questions on cor-
porate websites, interface with robots, and even be used for social good: DoNotPay
is a “robot lawyer” that helps people challenge incorrect parking ﬁnes, apply for
emergency housing, or claim asylum if they are refugees. By contrast, chatbots
are systems designed for extended conversations, set up to mimic the unstructured
conversations or ‘chats’ characteristic of human-human interaction, mainly for en-
tertainment, but also for practical purposes like making task-oriented agents more
natural. In Section 26.2 we’ll discuss the three major chatbot architectures : rule-
based systems, information retrieval systems, and encoder-decoder models. In Sec-
tion 26.3 we turn to task-oriented agents, introducing the frame-based architecture
(the GUS architecture) that underlies most modern task-based systems.

488
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
26.1
Properties of Human Conversation
Conversation between humans is an intricate and complex joint activity. Before we
attempt to design a conversational agent to converse with humans, it is crucial to
understand something about how humans converse with each other. Consider some
of the phenomena that occur in the conversation between a human travel agent and
a human client excerpted in Fig. 26.1.
C1:
...I need to travel in May.
A1:
And, what day in May did you want to travel?
C2:
OK uh I need to be there for a meeting that’s from the 12th to the 15th.
A2:
And you’re ﬂying into what city?
C3:
Seattle.
A3:
And what time would you like to leave Pittsburgh?
C4:
Uh hmm I don’t think there’s many options for non-stop.
A4:
Right. There’s three non-stops today.
C5:
What are they?
A5:
The ﬁrst one departs PGH at 10:00am arrives Seattle at 12:05 their time. The
second ﬂight departs PGH at 5:55pm, arrives Seattle at 8pm. And the last
ﬂight departs PGH at 8:15pm arrives Seattle at 10:28pm.
C6:
OK I’ll take the 5ish ﬂight on the night before on the 11th.
A6:
On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air ﬂight
115.
C7:
OK.
A7:
And you said returning on May 15th?
C8:
Uh, yeah, at the end of the day.
A8:
OK. There’s #two non-stops ...#
C9:
#Act...actually
#, what day of the week is the 15th?
A9:
It’s a Friday.
C10:
Uh hmm. I would consider staying there an extra day til Sunday.
A10:
OK...OK. On Sunday I have ...
Figure 26.1
Part of a phone conversation between a human travel agent (A) and human
client (C). The passages framed by # in A8 and C9 indicate overlaps in speech.
Turns
A dialogue is a sequence of turns (A1, B1, A2, and so on), each a single contribution
turn
to the dialogue (as if in a game: I take a turn, then you take a turn, then me, and so
on). A turn can consist of a sentence (like C1), although it might be as short as a
single word (C7) or as long as multiple sentences (A5).
Turn structure has important implications for spoken dialogue. A system has to
know when to stop talking; the client interrupts (in A8 and C9), so the system must
know to stop talking (and that the user might be making a correction). A system also
has to know when to start talking. For example, most of the time in conversation,
speakers start their turns almost immediately after the other speaker ﬁnishes, without
a long pause, because people are able to (most of the time) detect when the other
person is about to ﬁnish talking. Spoken dialogue systems must also detect whether
a user is done speaking, so they can process the utterance and respond. This task—
called endpointing or endpoint detection— can be quite challenging because of
endpointing
noise and because people often pause in the middle of turns.

26.1
•
PROPERTIES OF HUMAN CONVERSATION
489
Speech Acts
A key insight into conversation—due originally to the philosopher Wittgenstein
(1953) but worked out more fully by Austin (1962)—is that each utterance in a
dialogue is a kind of action being performed by the speaker. These actions are com-
monly called speech acts or dialog acts: here’s one taxonomy consisting of 4 major
speech acts
classes (Bach and Harnish, 1979):
Constatives:
committing the speaker to something’s being the case (answering, claiming,
conﬁrming, denying, disagreeing, stating)
Directives:
attempts by the speaker to get the addressee to do something (advising, ask-
ing, forbidding, inviting, ordering, requesting)
Commissives:
committing the speaker to some future course of action (promising, planning,
vowing, betting, opposing)
Acknowledgments: express the speaker’s attitude regarding the hearer with respect to some so-
cial action (apologizing, greeting, thanking, accepting an acknowledgment)
A user asking a person or a dialogue system to do something (‘Turn up the
music’) is issuing a DIRECTIVE. Asking a question that requires an answer is also
a way of issuing a DIRECTIVE: in a sense when the system says (C2) “what day in
May did you want to travel?” it’s as if the system is (very politely) commanding
the system to answer. By contrast, a user stating a constraint (like C1 ‘I need to
travel in May’) is issuing a CONSTATIVE. A user thanking the system is issuing
an ACKNOWLEDGMENT. The speech act expresses an important component of the
intention of the speaker (or writer) in saying what they said.
Grounding
A dialogue is not just a series of independent speech acts, but rather a collective act
performed by the speaker and the hearer. Like all collective acts, it’s important for
the participants to establish what they both agree on, called the common ground
common
ground
(Stalnaker, 1978). Speakers do this by grounding each other’s utterances. Ground-
grounding
ing means acknowledging that the hearer has understood the speaker; like an ACK
used to conﬁrm receipt in data communications (Clark, 1996). (People need ground-
ing for non-linguistic actions as well; the reason an elevator button lights up when
it’s pressed is to acknowledge that the elevator has indeed been called (Norman,
1988).)
Humans constantly ground each other’s utterances. We can ground by explicitly
saying “OK”, as the agent does in A8 or A10. Or we can ground by repeating what
the other person says; in utterance A1 the agent repeats “in May”, demonstrating her
understanding to the client. Or notice that when the client answers a question, the
agent begins the next question with “And”. The “And” implies that the new question
is ‘in addition’ to the old question, again indicating to the client that the agent has
successfully understood the answer to the last question.
Subdialogues and Dialogue Structure
Conversations have structure. Consider, for example, the local structure between
speech acts discussed in the ﬁeld of conversational analysis (Sacks et al., 1974).
conversational
analysis
QUESTIONS set up an expectation for an ANSWER. PROPOSALS are followed by
ACCEPTANCE (or REJECTION). COMPLIMENTS (“Nice jacket!”) often give rise to
DOWNPLAYERS (“Oh, this old thing?”). These pairs, called adjacency pairs are
adjacency pair
composed of a ﬁrst pair part and a second pair part (Schegloff, 1968), and these
expectations can help systems decide what actions to take.

490
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
However, dialogue acts aren’t always followed immediately by their second pair
part. The two parts can be separated by a side sequence (Jefferson 1972) or sub-
side sequence
dialogue. For example utterances C9 to A10 constitute a correction subdialogue
subdialogue
(Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):
C9:
#Act...actually#, what day of the week is the 15th?
A9:
It’s a Friday.
C10: Uh hmm. I would consider staying there an extra day til Sunday.
A10: OK...OK. On Sunday I have ...
The question in C9 interrupts the prior discourse, in which the agent was looking
for a May 15 return ﬂight. The agent must answer the question and also realize that
‘’I would consider staying...til Sunday” means that the client would probably like to
change their plan, and now go back to ﬁnding return ﬂights, but for the 17th.
Another side sequence is the clariﬁcation question, which can form a subdia-
logue between a REQUEST and a RESPONSE. This is especially common in dialogue
systems where speech recognition errors causes the system to have to ask for clari-
ﬁcations or repetitions like the following:
User:
What do you have going to UNKNOWN WORD on the 5th?
System:
Let’s see, going where on the 5th?
User:
Going to Hong Kong.
System:
OK, here are some ﬂights...
In addition to side-sequences, questions often have presequences, like the fol-
presequence
lowing example where a user starts with a question about the system’s capabilities
(“Can you make train reservations”) before making a request.
User:
Can you make train reservations?
System: Yes I can.
User:
Great, I’d like to reserve a seat on the 4pm train to New York.
Initiative
Sometimes a conversation is completely controlled by one participant. For example
a reporter interviewing a chef might ask questions, and the chef responds. We say
that the reporter in this case has the conversational initiative (Walker and Whittaker,
initiative
1990). In normal human-human dialogue, however, it’s more common for initiative
to shift back and forth between the participants, as they sometimes answer questions,
sometimes ask them, sometimes take the conversations in new directions, sometimes
not. You may ask me a question, and then I respond asking you to clarify something
you said, which leads the conversation in all sorts of ways. We call such interactions
mixed initiative.
Mixed initiative, while the norm for human-human conversations, is very difﬁ-
cult for dialogue systems to achieve. It’s much easier to design dialogue systems to
be passive responders. In the question answering systems we saw in Chapter 25, or
in simple search engines, the initiative lies completely with the user. In such user-
initiative systems, the user speciﬁes a query, and the systems responds. Then the
user can specify another query. Alternatively, you may have had the experience of
being stuck in a bad dialogue system that asks a question and gives you no opportu-
nity to do anything until you answer it. Such system-initiative architectures can be
very frustrating.

26.2
•
CHATBOTS
491
Inference and Implicature
Inference is also important in dialogue understanding. Consider the client’s response
C2, repeated here:
A1: And, what day in May did you want to travel?
C2: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
Notice that the client does not in fact answer the agent’s question. The client
merely mentions a meeting at a certain time. What is it that licenses the agent to
infer that the client is mentioning this meeting so as to inform the agent of the travel
dates?
The speaker seems to expect the hearer to draw certain inferences; in other
words, the speaker is communicating more information than seems to be present
in the uttered words. This kind of example was pointed out by Grice (1975, 1978)
as part of his theory of conversational implicature. Implicature means a particu-
implicature
lar class of licensed inferences. Grice proposed that what enables hearers to draw
these inferences is that conversation is guided by a set of maxims, general heuristics
that play a guiding role in the interpretation of conversational utterances. One such
maxim is the maxim of relevance which says that speakers attempt to be relevant,
relevance
they don’t just utter random speech acts. When the client mentions a meeting on the
12th, the agent reasons ‘There must be some relevance for mentioning this meeting.
What could it be?’. The agent knows that one precondition for having a meeting
(at least before Web conferencing) is being at the place where the meeting is held,
and therefore that maybe the meeting is a reason for the travel, and if so, then since
people like to arrive the day before a meeting, the agent should infer that the ﬂight
should be on the 11th.
These subtle characteristics of human conversations (turns, speech acts, ground-
ing, dialogue structure, initiative, and implicature) are among the reasons it is dif-
ﬁcult to build dialogue systems that can carry on natural conversations with humans.
Many of these challenges are active areas of dialogue systems research.
26.2
Chatbots
The simplest kinds of dialogue systems are chatbots, systems that can carry on
chatbot
extended conversations with the goal of mimicking the unstructured conversations
or ‘chats’ characteristic of informal human-human interaction.
These systems often have an entertainment value, such as Cleverbot, a simple IR-
based chatbot capable of carrying on the kinds of conversations shown in Fig. 26.2,
or Microsoft’s XiaoIce system1 (Zhou et al., 2018), a more sophisticated commercial
system that chats with people on text messaging platforms (Fig. 26.3). Yet starting
from the very ﬁrst system, ELIZA (Weizenbaum, 1966), chatbots have also been
used for practical purposes, such as testing theories of psychological counseling.
Like practically everything else in language processing, chatbot architectures
fall into two classes: rule-based systems and corpus-based systems. Rule-based
systems include the early inﬂuential ELIZA and PARRY systems. Corpus-based
systems mine large datasets of human-human conversations, which can be done by
using information retrieval (IR-based systems simply copy a human’s response from
1
XiaoIce is a somewhat confusing translation of the systems Chinese name 小冰, ‘Xiao Bing’, which
means Little Bing or Little Ice.

492
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
User:
Do you like Doctor Who?
Cleverbot:
Yes, so funny.
User:
What’s your favorite show?
Cleverbot:
I don’t have one.
User:
What’s your name?
Cleverbot:
Jason I already told you.
User:
No your real name.
Cleverbot:
Stephanie IS my real name.
User:
I thought it was Jason?
Cleverbot:
How did you come to that conclusion.
User:
Because you said so.
Cleverbot:
I have to do my homework.
Figure 26.2
Chatbots: A conversation between a user and Cleverbot, an IR-based chatbot
(Carpenter, 2017).
［人类］小冰你好
[Human]
Hello, XiaoIce.
［小冰］你好呀，你头像好有趣，感觉
在哪见过
[XiaoIce] Hello, your proﬁle pic is interesting.
Like I’ve seen it somewhere.
［人类］是吗
[Human]
Really?
［小冰］提不是哪部动画里的呀
[XiaoIce] Is it a comic character?
［人类］你怎么知道的
[Human]
How do you know that?
［小冰］你猜
[XiaoIce] Guess.
［人类］难道你是真人吗？
[Human]
Are you human?
［小冰］这个，你和我多聊聊就知道了
[XiaoIce] Well, you’ll know it after chatting with
me more.
［人类］你不是人工智能机器人么
[Human]
Aren’t you an Al chatbot?
［小冰］是又怎样，我不会冷冰冰的对
待你的
[XiaoIce] So what? I won’t be indifferent to you.
［人类］你都有什么功能啊
[Human]
What functions do you have?
［小冰］我有200多个skills，来，说出
你的需求
[XiaoIce] I have over 200 skills. Just name one
you need.
Figure 26.3
A sample Chinese conversation from the XiaoIce chatbot, together with translation from Zhou
et al. (2018).
a previous conversation) or by using a machine translation paradigm such as neural
network sequence-to-sequence systems, to learn to map from a user utterance to a
system response.
26.2.1
Rule-based chatbots: ELIZA and PARRY
ELIZA (Weizenbaum, 1966) is the most important chatbot dialogue system in the
history of the ﬁeld, and so we take some time here to describe its algorithm in de-
tail. ELIZA was designed to simulate a Rogerian psychologist, based on a branch
of clinical psychology whose methods involve drawing the patient out by reﬂecting
patient’s statements back at them. Rogerian psychology is the rare type of conver-
sation in which, as Weizenbaum points out, one can “assume the pose of knowing
almost nothing of the real world”. If a patient says “I went for a long boat ride” and
the psychiatrist says “Tell me about boats”, you don’t assume she didn’t know what
a boat is, but rather assume she had some conversational goal. Most chatbots trying
to pass the Turing test choose a domain with similar properties.
Fig. 26.4 shows the most famous ELIZA conversation, in which ELIZA seems

26.2
•
CHATBOTS
493
Men are all alike.
IN WHAT WAY
They’re always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Well my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE
He says I’m depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED
....
WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?
My father
YOUR FATHER
You are like my father in some ways.
WHAT RESEMBLANCE DO YOU SEE
You are not very aggressive but I think you don’t want me to notice that.
WHAT MAKES YOU THINK I AM NOT AGGRESSIVE
You don’t argue with me.
WHY DO YOU THINK I DON’T ARGUE WITH YOU
You are afraid of me.
DOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU
My father is afraid of everybody.
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER
Bullies.
DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR
BOYFRIEND MADE YOU COME HERE
Figure 26.4
Sample ELIZA dialogue from Weizenbaum (1966).
to simulate giving very sophisticated therapeutic advice, excerpted (deleting some
material with ...) from a full interaction given in the original paper (Weizenbaum,
1966).
As we summarized brieﬂy in Chapter 2, ELIZA worked by pattern/transform
rules like the following one:
(0 YOU 0 ME) [pattern]
->
(WHAT MAKES YOU THINK I 3 YOU) [transform]
In the ELIZA pattern, 0 means Kleene*, and in the transform rules the numbers
are the index of the constituent in the pattern. Thus the number 3 refers to the second
0 in the ﬁrst pattern. This rule would transfer
You hate me
into:
WHAT MAKES YOU THINK I HATE YOU
Each ELIZA pattern/rule is linked to a keyword that might occur in a user sen-
tence. The architecture is summarized in Fig. 26.5.
Keywords are associated with a rank, with speciﬁc words being more highly
ranked, and more general words ranking lower. Consider the following user sen-
tence:

494
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
function ELIZA GENERATOR(user sentence) returns response
Find the word w in sentence that has the highest keyword rank
if w exists
Choose the highest ranked rule r for w that matches sentence
response←Apply the transform in r to sentence
if w = ‘my’
future←Apply a transformation from the ‘memory’ rule list to sentence
Push future onto memory stack
else (no keyword applies)
either
response←Apply the transform for the NONE keyword to sentence
or
response←Pop the top response from the memory stack
return(response)
Figure 26.5
A simpliﬁed sketch of the ELIZA algorithm. The power of the algorithm
comes from the particular transforms associated with each keyword.
I know everybody laughed at me
Because it has the word “I”, this sentence could match the following rule whose
keyword is I:
(I *) -> (You say you 2)
producing:
YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU
But as Weizenbaum points out, “I” is a very general word and its keywords lead
to very general responses. Instead the keyword “everybody” is much more interest-
ing, since someone using universals like everybody or always is probably “referring
to some quite speciﬁc event or person”. Therefore, ELIZA prefers to respond with
the pattern associated with the more speciﬁc keyword everybody (implementing by
just assigning ”everybody” rank 5 and ”I” rank 0 in the lexicon), whose rule thus
transforms the sentence to:
WHO IN PARTICULAR ARE YOU THINKING OF?
If no keyword matches, ELIZA chooses a non-commital response like “PLEASE
GO ON”, “THAT’S VERY INTERESTING”, or “I SEE”.
Finally, ELIZA has a clever memory trick that accounts for the very last sen-
tence of the conversation above. Whenever the word “my” is the highest ranked
keyword, ELIZA will randomly select a transform on the MEMORY list, apply it to
the sentence, and store it on the stack:
(MEMORY MY
(0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)
(0 MY 0 = EARLIER YOU SAID YOUR 3)
(0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3
Later, if no keyword matches a sentence, ELIZA will return the top of the MEM-
ORY queue instead. 2
2
Fun fact: because of its structure as a queue, this MEMORY trick is the earliest known hierarchical
model of discourse in natural language processing.

26.2
•
CHATBOTS
495
People became deeply emotionally involved with the program. Weizenbaum
tells the story of one of his staff who would ask Weizenbaum to leave the room
when she talked with ELIZA. When Weizenbaum suggested that he might want to
store all the ELIZA conversations for later analysis, people immediately pointed
out the privacy implications, which suggested that they were having quite private
conversations with ELIZA, despite knowing that it was just software.
ELIZA’s framework is still used today; modern chatbot system tools like ALICE
are based on updated versions of ELIZA’s pattern/action architecture.
A few years after ELIZA, another chatbot with a clinical psychology focus,
PARRY (Colby et al., 1971), was used to study schizophrenia. In addition to ELIZA-
like regular expressions, the PARRY system included a model of its own mental
state, with affect variables for the agent’s levels of fear and anger; certain topics of
conversation might lead PARRY to become more angry or mistrustful. If PARRY’s
anger variable is high, he will choose from a set of “hostile” outputs. If the input
mentions his delusion topic, he will increase the value of his fear variable and then
begin to express the sequence of statements related to his delusion. Parry was the
ﬁrst known system to pass the Turing test (in 1972!); psychiatrists couldn’t distin-
guish text transcripts of interviews with PARRY from transcripts of interviews with
real paranoids (Colby et al., 1972).
26.2.2
Corpus-based chatbots
Corpus-based chatbots, instead of using hand-built rules, mine conversations of
human-human conversations, (or sometimes mine the human sides of human-machine
conversations).
These systems are enormously data-intensive; Serban et al. (2018) estimate that
training modern chatbots require hundreds of millions or even billions of words.
Many such corpora have been used, including large spoken conversational corpora
like the Switchboard corpus of American English telephone conversations (God-
frey et al., 1992) or the various CALLHOME and CALLFRIEND telephone con-
versational corpora in many languages. Many systems also train on movie dialogue,
which is available in great quantities in various corpora (Lison and Tiedemann, 2016,
inter alia), and which resembles natural conversation in many ways (Forchini, 2013).
Text from microblogging sites like Twitter (Ritter et al., 2010) or a Weibo (微博)
have also been used, or datasets of crowdworker conversations like Topical-Chat
(Gopalakrishnan et al., 2019). Many corpora also focus on speciﬁc topics, and can
be used for topical chatbots. See Serban et al. (2018) for a comprehensive summary
of available corpora. Another common technique is to extract possible responses
from non-dialogue corpora, so that a chatbot can tell stories or mention facts ac-
quired in that way.
Finally, once a chatbot has been put into practice, the turns that humans use to
respond to the chatbot can be used as additional conversational data for training.
The XiaoIce system collects and stores all human-machine conversations between
XiaoIce and its users, resulting in a a dataset of over 30 billion conversation pairs.
It’s crucial in these cases to remove personally identiﬁable information (PII); see
Section 26.6.1.
The two main architectures for corpus-based chatbots: information retrieval, and
machine learned sequence transduction. Like rule-based chatbots (but unlike frame-
based dialogue systems), most corpus-based chatbots do very little modeling of the
conversational context. Instead they tend to focus on generating a single response
turn that is appropriate given the user’s immediately previous utterance or two. For

496
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
this reason they are often called response generation systems. Corpus-based chat-
response
generation
bots thus have some similarity to question answering systems, which focus on single
responses while ignoring context or larger conversational goals.
IR-based chatbots
The principle behind information retrieval based chatbots is to respond to a user’s
turn X by repeating some appropriate turn Y from a corpus of natural (human) text
of the sort described in the prior section.
Given the corpus and the user’s sentence, IR-based systems can use any retrieval
algorithm to choose an appropriate response from the corpus. The two simplest
methods are the following:
1. Return the response to the most similar turn: Given user query q and a con-
versational corpus C, ﬁnd the turn t in C that is most similar to q (for example has
the highest cosine with q) and return the following turn, i.e. the human response to t
in C:
r = response

argmax
t∈C
qTt
||q||t||

(26.1)
The idea is that we should look for a turn that most resembles the user’s turn, and re-
turn the human response to that turn (Jafarpour et al. 2009, Leuski and Traum 2011).
2. Return the most similar turn: Given user query q and a conversational corpus
C, return the turn t in C that is most similar to q (for example has the highest cosine
with q):
r = argmax
t∈C
qTt
||q||t||
(26.2)
The idea here is to directly match the users query q with turns from C, since a good
response will often share words or semantics with the prior turn.
In each case, any similarity function can be used, such as cosines computed
either over words (weighted by tf-idf) or more commonly now, cosines over any
kind of sentence embeddings.
Although returning the response to the most similar turn seems like a more in-
tuitive algorithm, returning the most similar turn seems to work better in practice,
perhaps because selecting the response adds another layer of indirection that can
allow for more noise (Ritter et al. 2011, Wang et al. 2013).
The IR-based approach can be extended by using more features than just the
words in the q. For example using the entire conversation with the user so far
can be quite helpful when the user’s query is short (like “Yes” or “OK”). Infor-
mation about the user or sentiment or other information can also play a role. The
IR-based approach can even draw responses from narrative (non-dialogue) text. The
COBOT chatbot (Isbell et al., 2000) pioneered this approach, generating responses
by selecting sentences from a corpus that combined the Unabomber Manifesto by
Theodore Kaczynski, articles on alien abduction, the scripts of “The Big Lebowski”
and “Planet of the Apes”. Chatbots that want to generate informative turns such as
answers to user questions can use texts like Wikipedia to draw on sentences that
might contain those answers (Yan et al., 2016). XiaoIce similarly collects sentences
from public lectures and news articles and searches them using IR based on query
expansion from the user’s turn to respond to turns like “Tell me something about
Beijing” (Zhou et al., 2018).
Encoder decoder chatbots

26.2
•
CHATBOTS
497
“What ho!” I said.
“What ho!” said Motty.
“What ho! What ho!”
“What ho! What ho! What ho!”
After that it seemed rather difﬁcult to go on with the conversation.”
Wodehouse My Man Jeeves
An alternate way to use a corpus to generate dialogue is to think of response
generation as a task of transducing from the user’s prior turn to the system’s turn.
This is basically the machine learning version of Eliza; the system learns from a
corpus to transduce a question to an answer.
This idea was ﬁrst developed by using phrase-based machine translation (Ritter
et al., 2011) to translate a user turn to a system response. It quickly became clear,
however, that the task of response generation was too different from machine trans-
lation. In machine translation, words or phrases in the source and target sentences
tend to align well with each other, but in conversation, a user utterance may share no
words or phrases with a coherent response.
Instead, (roughly contemporaneously by Shang et al. 2015, Vinyals and Le 2015,
and Sordoni et al. 2015) transduction models for response generation were modeled
instead using encoder-decoder models (Chapter 11), as shown in Fig. 26.6.
How
are
you
?
I’m
fine
.
EOS
Encoding
Decoding
EOS
I’m
fine
.
Figure 26.6
An encoder decoder model for neural response generation in dialogue.
A number of modiﬁcations are required to the basic encoder-decoder model to
adapt it for the task of response generation. For example basic encoder-decoder
models have a tendency to produce predictable but repetitive and therefore dull re-
sponses like “I’m OK” or “I don’t know” that shut down the conversation. This
can be addressed by changing the objective function for seq2seq model training to a
mutual information objective, or by modifying a beam decoder to keep more diverse
responses in the beam (Li et al., 2016).
Another problem with simple encoder-decoder response generation architecture
is its inability to model the longer prior context of the conversation. This can be
done by allowing the model to see prior turns, such as by using a hierarchical model
that summarizes information over multiple prior turns (Lowe et al., 2017b).
Finally, encoder-decoder response generators focus on generating single responses,
and so don’t tend to do a good job of continuously generating responses that cohere
across multiple turns. This can be addressed by using reinforcement learning, as
well as techniques like adversarial networks, to learn to choose responses that make
the overall conversation more natural (Li et al. 2016, Li et al. 2017).
Fig. 26.7 shows some sample responses generated by a vanilla SEQ2SEQ model,
and from a model trained by an adversarial algorithm to produce responses that are
harder to distinguish from human responses (Li et al., 2017).

498
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
Input
tell me ... how long have you had this falling sickness ?
Vanilla-SEQ2SEQ
i’m not a doctor.
Adversarial
a few months, i guess .
Input
so i had the doctors test sammy ’s response to conditioning .
Vanilla-SEQ2SEQ
sammy wrote the test sammy wrote the test .
Adversarial
so he took the pills .
Input
they didn ’t have much success with this problem commander .
Vanilla-SEQ2SEQ
they ’re not the only ones who have been in the system .
Adversarial
can we ﬁnd someone else ?
Figure 26.7
Sample responses generated by a SEQ2SEQ model trained either with a vanilla
maximum likelihood objective, or adversarially trained to produce sentences that are hard for
an adversary to distinguish from human sentences (Li et al., 2017).
Evaluating Chatbots
Chatbots are generally evaluated by humans. The slot-ﬁlling evaluations used for
task-based dialogue (Section 26.5) aren’t appropriate for this task (Artstein et al.,
2009), and word-overlap metrics like BLEU for comparing a chatbot’s response to a
human response turn out to correlate very poorly with human judgments (Liu et al.,
2016). BLEU performs poorly because there are so many possible responses to any
given turn; word-overlap metrics work best when the space of responses is small and
lexically overlapping, as is the case in machine translation.
While human evaluation is therefore required for evaluating chatbots, there are
beginning to be models for automatic evaluation. The ADEM (Lowe et al., 2017a)
classiﬁer is trained on a set of responses labeled by humans with how appropriate
they are, and learns to predict this label from the dialogue context and the words in
the system response.
Another paradigm is adversarial evaluation (Bowman et al. 2016, Kannan and
adversarial
evaluation
Vinyals 2016, Li et al. 2017), inspired by the Turing test. The idea is to train a
“Turing-like” evaluator classiﬁer to distinguish between human-generated responses
and machine-generated responses. The more successful a response generation sys-
tem is at fooling this evaluator, the better the system.
26.3
GUS: Simple Frame-based Dialogue Systems
We turn now to task-based dialogue, in which a dialogue system has the goal of
helping a user solve some task like making an airplane reservation or buying a prod-
uct. In this section we introduce the very simple GUS architecture for task-based
GUS
dialogue. Although it is quite old, (it was ﬁrst introduced in 1977 in the inﬂuen-
tial GUS system for travel planning (Bobrow et al., 1977)) the GUS architecture
has been astonishingly long-lived and underlies most if not all modern commercial
digital assistants. Figure 26.8 shows a conversation with the original GUS system.
All modern task-based dialogue systems, whether the simple GUS architecture
we describe here, or the more sophisticated dialogue state architectures we turn to
dialogue state
in the following section, are based around frames. A frame is a kind of knowledge
frame
structure representing the kinds of intentions the system can extract from user sen-
tences, and consists of a collection of slots, each of which can take a set of possible
slot
values. Together this set of frames is sometimes called a domain ontology.
The set of slots in a task-based dialogue frame speciﬁes what the system needs
to know, and the ﬁller of each slot is constrained to values of a particular semantic

26.3
•
GUS: SIMPLE FRAME-BASED DIALOGUE SYSTEMS
499
GUS:
Hello. My name is GUS. I can help you plan a simple trip by air. Where do you
want to go ?
Client:
I want to go to San Diego on May 28
GUS:
What time do you want to leave?
Client:
I must be in San Diego before 10 am
GUS:
Would you like P.S.A. ﬂight 102 that arrives at 9.15 am ?
Client:
What is the next ﬂight ?
GUS:
Air California ﬂight 310 that leaves at 8.30am and arrives at 10.00am
Client:
I’ll take the ﬁrst one
GUS:
In what name should I make a reservation, please?
Client:
Dan Bobrow
GUS:
I have conﬁrmed the following ﬂight: P.S.A. ﬂight 102 on Wednesday May 28
from San Jose to San Diego that leaves at 7.30 am and arrives at 9.15 am.
GUS:
What date do you want to return on ?
Client:
On Friday in the evening.
GUS:
Would you like the ﬂight that leaves at 7.45 pm ?
Client:
That’s ﬁne.
GUS:
I have conﬁrmed the following ﬂight: P.S.A. ﬂight 307 on Friday May 30 from
San Diego to San Jose that leaves at 7.45 pm and arrives at 9.30 pm Thank you
for calling. Goodbye
Figure 26.8
The travel domain: A transcript of an actual dialogue with the GUS system of
Bobrow et al. (1977). P.S.A. and Air California were airlines of that period.
type. In the travel domain, for example, a slot might be of type city (hence take on
values like San Francisco, or Hong Kong) or of type date, airline, or time.
Slot
Type Question Template
ORIGIN CITY
city
“From what city are you leaving?”
DESTINATION CITY city
“Where are you going?”
DEPARTURE TIME
time
“When would you like to leave?”
DEPARTURE DATE
date
“What day would you like to leave?”
ARRIVAL TIME
time
“When do you want to arrive?”
ARRIVAL DATE
date
“What day would you like to arrive?”
Figure 26.9
A frame in a frame-based dialogue system, showing the type of each slot and
a question used to ﬁll the slot.
Types in GUS, as in modern frame-based dialogue agents, have hierarchical
structure; for example the date type in GUS is itself a frame with slots with types
like integer or members of sets of weekday names:
DATE
MONTH:NAME
YEAR:INTEGER
DAY:(BOUNDED-INTEGER 1 31)
WEEKDAY:(MEMBER (Sunday Monday Tuesday Wednesday
Thursday Friday Saturday))
26.3.1
Control structure for frame-based dialogue
The control architecture for frame-based dialogue systems, used in various forms
in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assistant, is
designed around the frame. The system’s goal is to ﬁll the slots in the frame with the
ﬁllers the user intends, and then perform the relevant action for the user (answering
a question, or booking a ﬂight).
To do this, the system asks questions of the user (using pre-speciﬁed question
templates associated with each slot of each frame, as shown in Fig. 26.9), ﬁlling any

500
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
slot that the user speciﬁes (we’ll describe how slot-ﬁlling works in the next section).
If a user’s response ﬁlls multiple slots, like the following:
(26.3) I want a ﬂight from San Francisco to Denver one way leaving after ﬁve
p.m. on Tuesday.
the system ﬁlls all the relevant slots, and then continues asking questions to ﬁll the
remaining slots, skipping questions associated with ﬁlled slots. The GUS architec-
ture also has condition-action rules attached to slots. For example, a rule attached
to the DESTINATION slot for the plane booking frame, once the user has speciﬁed
the destination, might automatically enter that city as the default StayLocation for
the related hotel booking frame. Or if the user speciﬁes the DESTINATION DAY for
a short trip the system could automatically enter the ARRIVAL DAY.
Many domains require multiple frames. Besides frames for car or hotel reserva-
tions, we might need frames with general route information (for questions like Which
airlines ﬂy from Boston to San Francisco?), or information about airfare practices
(for questions like Do I have to stay a speciﬁc number of days to get a decent air-
fare?). The system must be able to disambiguate which slot of which frame a given
input is supposed to ﬁll and then switch dialogue control to that frame.
Because of this need to dynamically switch control, the GUS architecture is a
production rule system. Different types of inputs cause different productions to
ﬁre, each of which can ﬂexibly ﬁll in different frames. The production rules can
then switch control according to factors such as the user’s input and some simple
dialogue history like the last question that the system asked.
Once the system has enough information it performs the necessary action (like
querying a database of ﬂights) and returns the result to the user.
26.3.2
Natural language understanding for ﬁlling slots in GUS
The goal of the natural language understanding component in the frame-based archi-
tecture is to extract three things from the user’s utterance. The ﬁrst task is domain
classiﬁcation: is this user for example talking about airlines, programming an alarm
clock, or dealing with their calendar? Of course this 1-of-n classiﬁcation tasks is
unnecessary for single-domain systems that are focused on, say, only calendar man-
agement, but multi-domain dialogue systems are the modern standard. The second
is user intent determination: what general task or goal is the user trying to accom-
intent
determination
plish? For example the task could be to Find a Movie, or Show a Flight, or Remove
a Calendar Appointment. Finally, we need to do slot ﬁlling: extract the particular
slot ﬁlling
slots and ﬁllers that the user intends the system to understand from their utterance
with respect to their intent. From a user utterance like this one:
Show me morning flights from Boston to San Francisco on Tuesday
a system might want to build a representation like:
DOMAIN:
AIR-TRAVEL
INTENT:
SHOW-FLIGHTS
ORIGIN-CITY:
Boston
ORIGIN-DATE:
Tuesday
ORIGIN-TIME:
morning
DEST-CITY:
San Francisco
while an utterance like
Wake me tomorrow at 6

26.3
•
GUS: SIMPLE FRAME-BASED DIALOGUE SYSTEMS
501
should give an intent like this:
DOMAIN:
ALARM-CLOCK
INTENT:
SET-ALARM
TIME:
2017-07-01 0600-0800
The slot-ﬁlling method used in the original GUS system, and still quite common
in industrial applications, is to use handwritten rules, often as part of the condition-
action rules attached to slots or concepts. For example we might just deﬁne a regular
expression for recognizing the SET-ALARM intent:
wake me (up) | set (the|an) alarm | get me up
Rule-based research systems like the Phoenix system (Ward and Issar, 1994)
consist of large hand-designed semantic grammars with thousands of rules. A
semantic
grammar
semantic grammar is a context-free grammar in which the left-hand side of each
rule corresponds to the semantic entities being expressed (i.e., the slot names) as in
the following fragment:
SHOW
→show me | i want | can i see|...
DEPART TIME RANGE →(after|around|before) HOUR |
morning | afternoon | evening
HOUR
→one|two|three|four...|twelve (AMPM)
FLIGHTS
→(a) ﬂight | ﬂights
AMPM
→am | pm
ORIGIN
→from CITY
DESTINATION
→to CITY
CITY
→Boston | San Francisco | Denver | Washington
Semantic grammars can be parsed by any CFG parsing algorithm (see Chap-
ter 13), resulting in a hierarchical labeling of the input string with semantic node
labels, as shown in Fig. 26.10.
S
DEPARTTIME
morning
DEPARTDATE
Tuesday
on
DESTINATION
Francisco
San
to
ORIGIN
Boston
from
FLIGHTS
ﬂights
SHOW
me
Show
Figure 26.10
A semantic grammar parse for a user sentence, using slot names as the internal parse tree nodes.
It remains only to put the ﬁllers into some sort of canonical form, for example
by normalizing dates as discussed in Chapter 18.
Many industrial dialogue systems employ the GUS architecture but use super-
vised machine learning for slot-ﬁlling instead of these kinds of rules; see Sec-
tion 26.4.2.
26.3.3
Other components of frame-based dialogue
The ASR (automatic speech recognition) component takes audio input from a phone
or other device and outputs a transcribed string of words, as discussed in Chapter 28.
The ASR component can also be made dependent on the dialogue state. For exam-
ple, if the system has just asked the user “What city are you departing from?”, the

502
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
ASR language model component can be constrained to assign high probability to
answers with city names. This can be done by training a language model on an-
swers to this question, or by hand-writing ﬁnite-state or context-free grammars for
such sentences. A language model that is completely dependent on dialogue state is
called a restrictive grammar; usually real systems mix restrictive grammars with
restrictive
grammar
more general language models.
The natural language generation module of any dialogue system produces
the utterances that the system says to the user. Frame-based systems tend to use
template-based generation, in which all or most of the words in the sentence to
template-based
generation
be uttered to the user are prespeciﬁed by the dialogue designer. Sentences created
by these templates are often called prompts. Templates might be completely ﬁxed
prompt
(like ‘Hello, how can I help you?’), or can include some variables that are ﬁlled in
by the generator, as in the following:
What time do you want to leave CITY-ORIG?
Will you return to CITY-ORIG from CITY-DEST?
It’s possible to do some simple grounding even in templated generation. Con-
sider the unnaturalness of this example from Cohen et al. (2004):
(26.4) System: Did you want to review some more of your personal proﬁle?
Caller: No.
System: What’s next?
Without an acknowledgment, the caller doesn’t know that the system has under-
stood her ‘No’. The use of Okay below adds grounding to the templated response
What’s next?, making (26.5) a much more natural response than (26.4):
(26.5) System: Did you want to review some more of your personal proﬁle?
Caller: No.
System: Okay, what’s next?
The rule-based GUS approach is very common in industrial applications. As was
true with the rule-based approach to information extraction, it has the advantage of
high precision, and if the domain is narrow enough and experts are available, can
provide sufﬁcient coverage as well. On the other hand, the handwritten rules or
grammars can be both expensive and slow to create, and handwritten rules can suffer
from recall problems.
26.4
The Dialogue-State Architecture
Modern research systems for task-based dialogue are based on a more sophisticated
version of the frame-based architecture called the dialogue-state or belief-state ar-
chitecture. Figure 26.11 shows the six components of a typical dialogue-state sys-
tem. The speech recognition and synthesis components deal with spoken language
processing; we’ll return to them in Chapter 28.
For the rest of this chapter we therefore consider the other four components,
which are part of both spoken and textual dialogue systems. These four components
are more complex than in the simple GUS systems. For example, like the GUS
systems, the dialogue-state architecture has an NLU component to extract slot ﬁllers
from the user’s utterance, but generally using machine learning rather than rules. The
dialogue state tracker maintains the current state of the dialogue (which include the
user’s most recent dialogue act, plus the entire set of slot-ﬁller constraints the user

26.4
•
THE DIALOGUE-STATE ARCHITECTURE
503
LEAVING FROM DOWNTOWN
LEAVING AT ONE P M
ARRIVING AT ONE P M
0.6
0.2
0.1
{ from: downtown }
{ depart-time: 1300 }
{ arrive-time: 1300 }
0.5
0.3
0.1
from:        CMU
to:          airport
depart-time: 1300
confirmed:   no
score:       0.10
from:        CMU
to:          airport
depart-time: 1300
confirmed:   no
score:       0.15
from:        downtown
to:          airport
depart-time: --
confirmed:   no
score:       0.65
Automatic Speech 
Recognition (ASR)
Spoken Language 
Understanding (SLU)
Dialog State 
Tracker (DST)
Dialog Policy
act:  confirm
from: downtown
FROM DOWNTOWN, 
IS THAT RIGHT?
Natural Language 
Generation (NLG)
Text to Speech (TTS)
Figure 26.11
Architecture of a dialogue-state system for task-oriented dialogue from Williams et al. (2016).
has expressed so far). The dialogue policy decides what the system should do or say
next. The dialogue policy in GUS was simple: ask questions until the frame was full
and then report back the results of some database query. But a more sophisticated
dialogue policy can help a system decide when to answer the user’s questions, when
to instead ask the user a clariﬁcation question, when to make a suggestion, and so on.
Finally, dialogue state systems have a natural language generation component. In
GUS, the sentences that the generator produced were all from pre-written templates.
But a more sophisticated generation component can condition on the exact context
to produce turns that seem much more natural.
As of the time of this writing, most commercial system are architectural hybrids,
based on GUS architecture augmented with some dialogue-state components, but
there are a wide variety of dialogue-state systems being developed in research labs.
26.4.1
Dialogue Acts
Dialogue-state systems make use of dialogue acts. Dialogue acts represent the in-
dialogue acts
teractive function of the turn or sentence, combining the idea of speech acts and
grounding into a single representation. Different types of dialogue systems require
labeling different kinds of acts, and so the tagset—deﬁning what a dialogue act is
exactly— tends to be designed for particular tasks.
Figure 26.12 shows a tagset for a restaurant recommendation system, and Fig. 26.13
shows these tags labeling a sample dialogue from the HIS system (Young et al.,
2010). This example also shows the content of each dialogue acts, which are the slot
ﬁllers being communicated. So the user might INFORM the system that they want
Italian food near a museum, or CONFIRM with the system that the price is reasonable.

504
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
Tag
Sys User
Description
HELLO(a = x,b = y,...)
✓
✓
Open a dialogue and give info a = x,b = y,...
INFORM(a = x,b = y,...)
✓
✓
Give info a = x,b = y,...
REQUEST(a,b = x,...)
✓
✓
Request value for a given b = x,...
REQALTS(a = x,...)
χ
✓
Request alternative with a = x,...
CONFIRM(a = x,b = y,...) ✓
✓
Explicitly conﬁrm a = x,b = y,...
CONFREQ(a = x,...,d)
✓
χ
Implicitly conﬁrm a = x,... and request value of d
SELECT(a = x,a = y)
✓
χ
Implicitly conﬁrm a = x,... and request value of d
AFFIRM(a = x,b = y,...)
✓
✓
Afﬁrm and give further info a = x,b = y,...
NEGATE(a = x)
χ
✓
Negate and give corrected value a = x
DENY(a = x)
χ
✓
Deny that a = x
BYE()
✓
✓
Close a dialogue
Figure 26.12
Dialogue acts used by the HIS restaurant recommendation system of Young
et al. (2010). The Sys and User columns indicate which acts are valid as system outputs and
user inputs, respectively.
Utterance
Dialogue act
U: Hi, I am looking for somewhere to eat.
hello(task = find,type=restaurant)
S: You are looking for a restaurant. What
type of food do you like?
confreq(type = restaurant, food)
U: I’d like an Italian somewhere near the
museum.
inform(food = Italian, near=museum)
S: Roma is a nice Italian restaurant near
the museum.
inform(name = "Roma", type = restaurant,
food = Italian, near = museum)
U: Is it reasonably priced?
confirm(pricerange = moderate)
S: Yes, Roma is in the moderate price
range.
affirm(name = "Roma", pricerange =
moderate)
U: What is the phone number?
request(phone)
S: The number of Roma is 385456.
inform(name = "Roma", phone = "385456")
U: Ok, thank you goodbye.
bye()
Figure 26.13
A sample dialogue from the HIS System of Young et al. (2010) using the dialogue acts in
Fig. 26.12.
26.4.2
Slot Filling
The task of slot-ﬁlling, and the simpler tasks of domain and intent classiﬁcation, are
special cases of the task of supervised semantic parsing discussed in Chapter 17, in
which we have a training set that associates each sentence with the correct set of
slots, domain, and intent.
A simple method is to train a a sequence model to map from input words repre-
sentation to slot ﬁllers, domain and intent. For example given the sentence:
I want to fly to San Francisco on Monday afternoon please
we compute a sentence representation, for example by passing the sentence through
a contextual embedding network like BERT. The resulting sentence representation
can be passed through a feedforward layer and then a simple 1-of-N classiﬁer to
determine that the domain is AIRLINE and and the intent is SHOWFLIGHT.
Our training data is sentences paired with sequences of IOB labels:
IOB
O O
O
O
O
B-DES I-DES
O
B-DEPTIME I-DEPTIME
O
I want to fly to San
Francisco on Monday
afternoon
please
Recall from Chapter 18 that in IOB tagging we introduce a tag for the beginning
(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.
The number of tags is thus 2n+1 tags, where n is the number of slots.

26.4
•
THE DIALOGUE-STATE ARCHITECTURE
505
Fig. 26.14 shows the architecture. The input is a series of words w1...wn, which
is passed through a contextual embedding model to get contextual word representa-
tions. This is followed by a feedforward layer and a softmax at each token position
over possible IOB tags, with the output is a series of IOB tags s1...sn. We can also
combine the domain-classiﬁcation and intent-extraction tasks with slot-ﬁlling sim-
ply by adding a domain concatenated with an intent as the desired output for the
ﬁnal EOS token.
San
Francisco
on
Monday
Contextual
Embeddings
Classiﬁer
B-DES
I-DES
O
B-DTIME
Output
…
d+i
<EOS>
softmax
Figure 26.14
A simple architecture for slot ﬁlling, mapping the words in the input through
contextual embeddings like BERT to an output classiﬁer layer (which can be linear or some-
thing more complex), followed by softmax to generate a series of IOB tags (and including a
ﬁnal state consisting of a domain concatenated with an intent).
Once the sequence labeler has tagged the user utterance, a ﬁller string can be
extracted for each slot from the tags (e.g., “San Francisco”), and these word strings
can then be normalized to the correct form in the ontology (perhaps the airport code
‘SFO’). This normalization can take place by using homonym dictionaries (specify-
ing, for example, that SF, SFO, and San Francisco are the same place).
In industrial contexts, machine learning-based systems for slot-ﬁlling are of-
ten bootstrapped from GUS-style rule-based systems in a semi-supervised learning
manner. A rule-based system is ﬁrst built for the domain, and a test-set is carefully
labeled. As new user utterances come in, they are paired with the labeling provided
by the rule-based system to create training tuples. A classiﬁer can then be trained
on these tuples, using the test-set to test the performance of the classiﬁer against
the rule-based system. Some heuristics can be used to eliminate errorful training
tuples, with the goal of increasing precision. As sufﬁcient training samples become
available the resulting classiﬁer can often outperform the original rule-based system
(Suendermann et al., 2009), although rule-based systems may still remain higher-
precision for dealing with complex cases like negation.
26.4.3
Dialogue State Tracking
The job of the dialogue-state tracker is to determine both the current state of the
frame (the ﬁllers of each slot), as well as the user’s most recent dialogue act. The
dialogue-state thus includes more than just the slot-ﬁllers expressed in the current
sentence; it includes the entire state of the frame at this point, summarizing all of
the user’s constraints. The following example from Mrkˇsi´c et al. (2017) shows the
required output of the dialogue state tracker after each turn:

506
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
User:
I’m looking for a cheaper restaurant
inform(price=cheap)
System: Sure. What kind - and where?
User:
Thai food, somewhere downtown
inform(price=cheap, food=Thai, area=centre)
System: The House serves cheap Thai food
User:
Where is it?
inform(price=cheap, food=Thai, area=centre); request(address)
System: The House is at 106 Regent Street
Since dialogue acts place some constraints on the slots and values, the tasks of
dialogue-act detection and slot-ﬁlling are often performed jointly. Consider the task
of determining that
I’d like Cantonese food near the Mission District
has the structure
inform(food=cantonese,area=mission).
Dialogue act interpretation—in this example choosing inform from the set of
dialogue acts for this task—is done by supervised classiﬁcation trained on hand-
labeled dialog acts, predicting the dialogue act tag based on embeddings represent-
ing the current input sentence and the prior dialogue acts.
The simplest dialogue state tracker might just take the output of a slot-ﬁlling
sequence-model (Section 26.4.2) after each sentence. Alternatively, a more complex
model can make use of the reading-comprehension architectures from Chapter 25.
For example the model of Gao et al. (2019) trains a classiﬁer for each slot to decide
whether its value is being changed in the current sentence or should be carried over
from the previous sentences. If the slot value is being changed, a span-prediction
model is used to predict the start and end of the span with the slot ﬁller.
A special case: detecting correction acts
Some dialogue acts are important because of their implications for dialogue control.
If a dialogue system misrecognizes or misunderstands an utterance, the user will
generally correct the error by repeating or reformulating the utterance. Detecting
these user correction acts is therefore quite important. Ironically, it turns out that
user correction
acts
corrections are actually harder to recognize than normal sentences! In fact, correc-
tions in one early dialogue system (the TOOT system) had double the ASR word
error rate of non-corrections (Swerts et al., 2000)! One reason for this is that speak-
ers sometimes use a speciﬁc prosodic style for corrections called hyperarticulation,
hyperarticula-
tion
in which the utterance contains exaggerated energy, duration, or F0 contours, such
as I said BAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg
et al. 2001). Even when they are not hyperarticulating, users who are frustrated
seem to speak in a way that is harder for speech recognizers (Goldberg et al., 2003).
What are the characteristics of these corrections? User corrections tend to be
either exact repetitions or repetitions with one or more words omitted, although they
may also be paraphrases of the original utterance. (Swerts et al., 2000). Detect-
ing these reformulations or correction acts can be part of the general dialogue act
detection classiﬁer. Alternatively, because the cues to these acts tend to appear in
different ways than for simple acts (like INFORM or request, we can make use of
features orthogonal to simple contextual embedding features; some typical features
are shown below (Levow 1998, Litman et al. 1999, Hirschberg et al. 2001, Bulyko
et al. 2005, Awadallah et al. 2015):

26.4
•
THE DIALOGUE-STATE ARCHITECTURE
507
features
examples
lexical
words like “no”, “correction”, “I don’t”, or even swear words, utterance length
semantic
similarity (word overlap or embedding cosine) between the candidate correc-
tion act and the user’s prior utterance
phonetic
phonetic overlap between the candidate correction act and the user’s prior ut-
terance (i.e. “WhatsApp” may be incorrectly recognized as “What’s up”)
prosodic
hyperarticulation, increases in F0 range, pause duration, and word duration,
generally normalized by the values for previous sentences
ASR
ASR conﬁdence, language model probability
26.4.4
Dialogue Policy
The goal of the dialogue policy is to decide what action the system should take next,
dialogue policy
that is, what dialogue act to generate.
More formally, at turn i in the conversation we want to predict which action Ai
to take, based on the entire dialogue state. The state could mean the entire sequence
of dialogue acts from the system (A) and from the user (U), in which case the task
would be to compute:
ˆAi = argmax
Ai∈A
P(Ai|(A1,U1,...,Ai−1,Ui−1)
(26.6)
We can simplify this by maintaining as the dialogue state mainly just the set of
slot-ﬁllers that the user has expressed, collapsing across the many different conver-
sational paths that could lead to the same set of ﬁlled slots.
Such a policy might then just condition on the current dialogue state as repre-
sented just by the current state of the frame Framei (which slots are ﬁlled and with
what) and the last turn by the system and user:
ˆAi = argmax
Ai∈A
P(Ai|Framei−1,Ai−1,Ui−1)
(26.7)
These probabilities can be estimated by a neural classiﬁer using neural representa-
tions of the slot ﬁllers (for example as spans) and the utterances (for example as
sentence embeddings computed over contextual embeddings)
More sophisticated models train the policy via reinforcement learning. To de-
cide which action to take, a reinforcement learning system gets a reward at the end
of the dialogue, and uses that reward to train a policy to take actions. For example in
the movie-recommendation dialogue system of Fazel-Zarandi et al. (2017), the ac-
tion space has only three actions: EXECUTE, CONFIRM, and ELICIT. The EXECUTE
sends a query to the database and answers the user’s question, CONFIRM clariﬁes
the intent or slot with the users (e.g., “Do you want movies directed by Christopher
Nolan?”) while ELICIT asks the user for missing information (e.g., “Which movie
are you talking about?”). The system gets a large positive reward if the dialogue sys-
tem terminates with the correct slot representation at the end, a large negative reward
if the slots are wrong, and a small negative reward for conﬁrmation and elicitation
questions to keep the system from re-conﬁrming everything.
Policy Example: Conﬁrmation and Rejection
Modern dialogue systems often make mistakes. It is therefore important for dialogue
systems to make sure that they have achieved the correct interpretation of the user’s

508
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
input. This is generally done by two methods: conﬁrming understandings with the
user and rejecting utterances that the system is likely to have misunderstood.
As we saw in the prior section, most systems introduce particular strategies and
actions related to conﬁrmation and rejection. When using the explicit conﬁrmation
explicit
conﬁrmation
strategy, a system asks the user a direct question to conﬁrm the system’s under-
standing, like the two examples below in which the system asks a (boldface) yes-no
conﬁrmation questions:
S: Which city do you want to leave from?
U: Baltimore.
S: Do you want to leave from Baltimore?
U: Yes.
U: I’d like to ﬂy from Denver Colorado to New York City on September
twenty ﬁrst in the morning on United Airlines
S: Let’s see then. I have you going from Denver Colorado to New York
on September twenty ﬁrst. Is that correct?
U: Yes
When using the implicit conﬁrmation strategy, a system instead can demon-
implicit
conﬁrmation
strate its understanding as a grounding strategy, for example repeating back the
system’s understanding as part of asking the next question, as in the two examples
below:
U:
I want to travel to Berlin
S:
When do you want to travel to Berlin?
U2: Hi I’d like to ﬂy to Seattle Tuesday Morning
A3: Traveling to Seattle on Tuesday, August eleventh in the morning.
Your full name?
Explicit and implicit conﬁrmation have complementary strengths. Explicit con-
ﬁrmation makes it easier for users to correct the system’s misrecognitions since a
user can just answer “no” to the conﬁrmation question. But explicit conﬁrmation is
awkward and increases the length of the conversation (Danieli and Gerbino 1995,
Walker et al. 1998). The explicit conﬁrmation dialogue fragments above sound non-
natural and deﬁnitely non-human; implicit conﬁrmation is much more conversation-
ally natural.
Conﬁrmation is just one kind of conversational action by which a system can
express lack of understanding. Another option is rejection, in which a system gives
rejection
the user a prompt like I’m sorry, I didn’t understand that.
Sometimes utterances are rejected multiple times. This might mean that the user
is using language that the system is unable to follow. Thus, when an utterance is
rejected, systems often follow a strategy of progressive prompting or escalating
progressive
prompting
detail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example
from Cohen et al. (2004):
System: When would you like to leave?
Caller:
Well, um, I need to be in New York in time for the ﬁrst World Series game.
System: <reject>. Sorry, I didn’t get that. Please say the month and day you’d like
to leave.
Caller:
I wanna go on October ﬁfteenth.

26.4
•
THE DIALOGUE-STATE ARCHITECTURE
509
In this example, instead of just repeating “When would you like to leave?”, the
rejection prompt gives the caller more guidance about how to formulate an utter-
ance the system will understand. These you-can-say help messages are important in
helping improve systems’ understanding performance (Bohus and Rudnicky, 2005).
If the caller’s utterance gets rejected yet again, the prompt can reﬂect this (“I still
didn’t get that”), and give the caller even more guidance.
An alternative strategy for error handling is rapid reprompting, in which the
rapid
reprompting
system rejects an utterance just by saying “I’m sorry?” or “What was that?” Only
if the caller’s utterance is rejected a second time does the system start applying
progressive prompting. Cohen et al. (2004) summarize experiments showing that
users greatly prefer rapid reprompting as a ﬁrst-level error prompt.
It is common to use rich features other than just the dialogue state representa-
tion to make policy decisions. For example, the conﬁdence that the ASR system
assigns to an utterance can be used by explicitly conﬁrming low-conﬁdence sen-
tences. As we’ll see in page ??, conﬁdence is a metric that the speech recognizer
can assign to its transcription of a sentence to indicate how conﬁdent it is in that
transcription. Conﬁdence is often computed from the acoustic log-likelihood of the
utterance (greater probability means higher conﬁdence), but prosodic features can
also be used in conﬁdence prediction. For example, utterances with large F0 ex-
cursions or longer durations, or those preceded by longer pauses, are likely to be
misrecognized (Litman et al., 2000).
Another common feature in conﬁrmation is the cost of making an error. For ex-
ample, explicit conﬁrmation is common before a ﬂight is actually booked or money
in an account is moved. Systems might have a four-tiered level of conﬁdence with
three thresholds α, β, and γ:
< α low conﬁdence
reject
≥α above the threshold
conﬁrm explicitly
≥β high conﬁdence
conﬁrm implictly
≥γ
very high conﬁdence don’t conﬁrm at all
26.4.5
Natural language generation in the dialogue-state model
Finally, once the policy has decided what speech act to generate, the natural language
generation component needs to generate the text of a response to the user.
Once a dialogue act has been decided, we need to generate the text of the re-
sponse to the user. The task of natural language generation (NLG) in the information-
state architecture is often modeled in two stages, content planning (what to say),
content
planning
and sentence realization (how to say it).
sentence
realization
Here we’ll assume content planning has been done by the dialogue policy, which
has chosen the dialogue act to generate, and chosen some attributes (slots and values)
that the planner wants to say to the user (either to give the user the answer, or as part
of a conﬁrmation strategy).
Fig. 26.15 shows some sample input/outputs for the sentence realization phase.
In the ﬁrst example, the content planner has chosen the dialogue act RECOMMEND
and some particular slots (name, neighborhood, cuisine) and their ﬁllers. The goal
of the sentence realizer is to generate a sentence like lines 1 or 2 shown in the ﬁgure,
by training on many such examples of representation/sentence pairs from a large
corpus of labeled dialogues.
Training data is hard to come by; we are unlikely to see every possible restaurants
with every possible attribute in many possible differently worded sentences. There-

510
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 Au Midi is in Midtown and serves French food.
2 There is a French restaurant in Midtown called Au Midi.
recommend(restaurant name= Loch Fyne, neighborhood = city
centre, cuisine = seafood)
3 Loch Fyne is in the City Center and serves seafood food.
4 There is a seafood restaurant in the City Centre called Loch Fyne.
Figure 26.15
Two examples of inputs to the sentence realization phase of NLG, showing
the dialogue act and attributes prespeciﬁed by the content planner. Line 1-2 and 3-4 show dif-
ferent possible output sentences to be generated by the sentence realizer. From the restaurant
recommendation system of Nayak et al. (2017).
fore it is common in sentence realization to increase the generality of the training
examples by delexicalization. Delexicalization is the process of replacing speciﬁc
delexicalization
words in the training set that represent slot values with a generic placeholder to-
ken representing the slot. Fig. 26.16 shows the result of delexicalizing the training
sentences in Fig. 26.15.
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 restaurant name is in neighborhood and serves cuisine food.
2 There is a cuisine restaurant in neighborhood called restaurant name.
Figure 26.16
Delexicalized sentences that can be used for generating many different relex-
icalized sentences. From the restaurant recommendation system of Nayak et al. (2017).
Mapping from frames to delexicalized sentences is generally done by encoder
decoder models (Wen et al. 2015a, Wen et al. 2015b, Mrkˇsi´c et al. 2017, inter alia),
trained on large hand-labeled corpora of task-oriented dialogue (Budzianowski et al.,
2018). The input to the encoder is a sequence of tokens xt that represent the dialogue
act and its arguments. Thus the attribute/value pairs decor:decent, service:good,
cuisine: null might be represented as a ﬂat sequence of tokens, each mapped to a
learned embedding wt, as shown in Fig. 26.17.
name
has
decent
service
good
name
recommend
hn
service
Figure 26.17
An encoder decoder sentence realizer mapping slots/ﬁllers to English.
The encoder reads all the input slot/value representations, produces a context
vector that is used as input to the lexical decoder, which generates an English sen-
tence Let’s suppose in this case we produce the following (delexicalized) sentence:
restaurant name has decent service
Then once we’ve generated the delexicalized string, we can use the input frame
from the content planner to relexicalize (ﬁll in the exact restaurant or neighborhood
relexicalize
or cuisine). This sentence is then relexicalized from the true values in the input
frame, resulting in the ﬁnal sentence:
Au Midi has decent service

26.5
•
EVALUATING DIALOGUE SYSTEMS
511
TTS Performance
Was the system easy to understand ?
ASR Performance
Did the system understand what you said?
Task Ease
Was it easy to ﬁnd the message/ﬂight/train you wanted?
Interaction Pace
Was the pace of interaction with the system appropriate?
User Expertise
Did you know what you could say at each point?
System Response
How often was the system sluggish and slow to reply to you?
Expected Behavior
Did the system work the way you expected it to?
Future Use
Do you think you’d use the system in the future?
Figure 26.18
User satisfaction survey, adapted from Walker et al. (2001).
Generating Clariﬁcation Questions
It’s also possible to design NLG algorithms that are speciﬁc to a particular dialogue
act. For example, consider the task of generating clariﬁcation questions, in cases
clariﬁcation
questions
where the speech recognition fails to understand some part of the user’s utterance.
While it is possible to use the generic dialogue act REJECT (“Please repeat”, or “I
don’t understand what you said”), studies of human conversations show that humans
instead use targeted clariﬁcation questions that reprise elements of the misunder-
standing (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013).
For example, in the following hypothetical example the system reprises the
words “going” and “on the 5th” to make it clear which aspect of the user’s turn
the system needs to be clariﬁed:
User:
What do you have going to UNKNOWN WORD on the 5th?
System: Going where on the 5th?
Targeted clariﬁcation questions can be created by rules (such as replacing “go-
ing to UNKNOWN WORD” with “going where”) or by building classiﬁers to guess
which slots might have been misrecognized in the sentence (Chu-Carroll and Car-
penter 1999, Stoyanchev et al. 2014, Stoyanchev and Johnston 2015).
26.5
Evaluating Dialogue Systems
Evaluation is crucial in dialogue system design. If the task is unambiguous, we can
simply measure absolute task success (did the system book the right plane ﬂight, or
put the right event on the calendar).
To get a more ﬁne-grained idea of user happiness, we can compute a user sat-
isfaction rating, having users interact with a dialogue system to perform a task and
then having them complete a questionnaire. For example, Fig. 26.18 shows sample
multiple-choice questions (Walker et al., 2001); responses are mapped into the range
of 1 to 5, and then averaged over all questions to get a total user satisfaction rating.
It is often economically infeasible to run complete user satisfaction studies after
every change in a system. For this reason, it is useful to have performance evaluation
heuristics that correlate well with human satisfaction. A number of such factors and
heuristics have been studied, often grouped into two kinds of criteria: how well the
system allows users to accomplish their goals (maximizing task success) with the
fewest problems (minimizing costs):
Task completion success:
Task success can be measured by evaluating the correctness of the total solution. For
a frame-based architecture, this might be slot error rate the percentage of slots that

512
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
were ﬁlled with the correct values:
Slot Error Rate for a Sentence = # of inserted/deleted/subsituted slots
# of total reference slots for sentence
(26.8)
For example consider a system given this sentence:
(26.9) Make an appointment with Chris at 10:30 in Gates 104
which extracted the following candidate slot structure:
Slot
Filler
PERSON Chris
TIME
11:30 a.m.
ROOM
Gates 104
Here the slot error rate is 1/3, since the TIME is wrong. Instead of error rate, slot
precision, recall, and F-score can also be used.
Interestingly, sometimes the user’s perception of whether they completed the
task is a better predictor of user satisfaction than the actual task completion success.
(Walker et al., 2001).
A perhaps more important, although less ﬁne-grained, measure of success is an
extrinsic metric like task error rate. In this case, the task error rate would quantify
how often the correct meeting was added to the calendar at the end of the interaction.
Efﬁciency cost:
Efﬁciency costs are measures of the system’s efﬁciency at helping users. This can be
measured by the total elapsed time for the dialogue in seconds, the number of total
turns or of system turns, or the total number of queries (Polifroni et al., 1992). Other
metrics include the number of system non-responses and the “turn correction ratio”:
the number of system or user turns that were used solely to correct errors divided by
the total number of turns (Danieli and Gerbino 1995, Hirschman and Pao 1993).
Quality cost:
Quality cost measures other aspects of the interactions that affect users’ perception
of the system. One such measure is the number of times the ASR system failed to re-
turn any sentence, or the number of ASR rejection prompts. Similar metrics include
the number of times the user had to barge in (interrupt the system), or the number of
time-out prompts played when the user didn’t respond quickly enough. Other qual-
ity metrics focus on how well the system understood and responded to the user. The
most important is the slot error rate described above, but other components include
the inappropriateness (verbose or ambiguous) of the system’s questions, answers,
and error messages or the correctness of each question, answer, or error message
(Zue et al. 1989, Polifroni et al. 1992).
26.6
Dialogue System Design
The user plays a more important role in dialogue systems than in most other areas of
speech and language processing, and thus the study of dialogue systems is closely
linked with the ﬁeld of Human-Computer Interaction (HCI). The design of dialogue
strategies, prompts, and error messages, is often called voice user interface design,
voice user
interface
and generally follows user-centered design principles (Gould and Lewis, 1985):

26.6
•
DIALOGUE SYSTEM DESIGN
513
1. Study the user and task:
Understand the potential users and the nature of the
task by interviews with users, investigation of similar systems, and study of related
human-human dialogues.
2. Build simulations and prototypes:
A crucial tool in building dialogue systems
is the Wizard-of-Oz system. In wizard systems, the users interact with what they
Wizard-of-Oz
system
think is a software agent but is in fact a human “wizard” disguised by a software
interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name
comes from the children’s book The Wizard of Oz (Baum, 1900), in which the wizard
turned out to be just a simulation controlled by a man behind a curtain or screen.
A Wizard-of-Oz system can be used to
test out an architecture before implementa-
tion; only the interface software and databases
need to be in place. The wizard gets input
from the user, has a graphical interface to a
database to run sample queries based on the
user utterance, and then has a way to output
sentences, either by typing them or by some
combination of selecting from a menu and
typing. The wizard’s linguistic output can be
disguised by a text-to-speech system or, more
frequently, by using text-only interactions.
The results of a Wizard-of-Oz system can
also be used as training data to train a pilot di-
alogue system. While Wizard-of-Oz systems
are very commonly used, they are not a per-
fect simulation; it is difﬁcult for the wizard to
exactly simulate the errors, limitations, or time constraints of a real system; results
of wizard studies are thus somewhat idealized, but still can provide a useful ﬁrst idea
of the domain issues.
3. Iteratively test the design on users:
An iterative design cycle with embedded
user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich
et al. 1995, Landauer 1995). For example in a famous anecdote in dialogue design
history, an early dialogue system required the user to press a key to interrupt the
system Stifelman et al. (1993). But user testing showed users barged in, which led
to a redesign of the system to recognize overlapped speech. The iterative method
is also important for designing prompts that cause the user to respond in normative
ways.
There are a number of good books on conversational interface design (Cohen
et al. 2004, Harris 2005, Pearl 2017).
26.6.1
Ethical Issues in Dialogue System Design
Ethical issues have long been understood to be crucial in the design of artiﬁcial
agents, predating the conversational agent itself. Mary Shelley’s classic discussion
of the problems of creating agents without a consideration of ethical and humanistic
concerns lies at the heart of her novel Frankenstein. One
important ethical issue has to do with bias. As we dis-
cussed in Section 6.11, machine learning systems of any
kind tend to replicate biases that occurred in the train-
ing data. This is especially relevant for chatbots, since

514
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
both IR-based and neural transduction architectures are
designed to respond by approximating the responses in
the training data.
A well-publicized instance of this occurred with Mi-
crosoft’s 2016 Tay chatbot, which was taken ofﬂine 16
Tay
hours after it went live, when it began posting messages
with racial slurs, conspiracy theories, and personal attacks. Tay had learned these
biases and actions from its training data, including from users who seemed to be
purposely teaching it to repeat this kind of language (Neff and Nagy, 2016).
Henderson et al. (2017) examined some standard dialogue datasets (drawn from
Twitter, Reddit, or movie dialogues) used to train corpus-based chatbots, measuring
bias (Hutto et al., 2015) and offensive and hate speech (Davidson et al., 2017). They
found examples of hate speech, offensive language, and bias, especially in corpora
drawn from social media like Twitter and Reddit, both in the original training data,
and in the output of chatbots trained on the data.
Another important ethical issue is privacy. Already in the ﬁrst days of ELIZA,
Weizenbaum pointed out the privacy implications of people’s revelations to the chat-
bot. Henderson et al. (2017) point out that home dialogue agents may accidentally
record a user revealing private information (e.g. “Computer, turn on the lights –an-
swers the phone –Hi, yes, my password is...”), which may then be used to train a
conversational model. They showed that when an encoder-decoder dialogue model
is trained on a standard corpus augmented with training keypairs representing pri-
vate data (e.g. the keyphrase “social security number” followed by a number), an
adversary who gave the keyphrase was able to recover the secret information with
nearly 100% accuracy. Chatbots that are trained on transcripts of human-human or
human-machine conversation must therefore anonymize personally identiﬁable in-
formation. It is the role of the Institutional Review Board (IRB) at a researcher’s
IRB
institution to review research proposals for such ethical issues.
Finally, chatbots raise important issues of gender equality. Current chatbots are
overwhelmingly given female names, likely perpetuating the stereotype of a sub-
servient female servant (Paolino, 2017). And when users use sexually harassing
language, most commercial chatbots evade or give positive responses rather than
responding in clear negative ways (Fessler, 2017).
26.7
Summary
Conversational agents are crucial speech and language processing applications that
are already widely used commercially.
• In human dialogue, speaking is a kind of action; these acts are referred to
as speech acts or dialogue acts. Speakers also attempt to achieve common
ground by acknowledging that they have understand each other. Conversation
also is characterized by turn structure and dialogue structure.
• Chatbots are conversational agents designed to mimic the appearance of in-
formal human conversation. Rule-based chatbots like ELIZA and its modern
descendants use rules to map user sentences into system responses. Corpus-
based chatbots mine logs of human conversation to learn to automatically map
user sentences into system responses.
• For task-based dialogue, most commercial dialogue systems use the GUS or

BIBLIOGRAPHICAL AND HISTORICAL NOTES
515
frame-based architecture, in which the designer speciﬁes frames consisting of
slots that the system must ﬁll by asking the user.
• The dialogue-state architecture augments the GUS frame-and-slot architec-
ture with richer representations and more sophisticated algorithms for keeping
track of user’s dialogue acts, policies for generating its own dialogue acts, and
a natural language component.
• Dialogue systems are a kind of human-computer interaction, and general HCI
principles apply in their design, including the role of the user, simulations such
as Wizard-of-Oz systems, and the importance of iterative design and testing
on real users.
Bibliographical and Historical Notes
The earliest conversational systems were chatbots like ELIZA (Weizenbaum, 1966)
and PARRY (Colby et al., 1971). ELIZA had a widespread inﬂuence on popular
perceptions of artiﬁcial intelligence, and brought up some of the ﬁrst ethical ques-
tions in natural language processing —such as the issues of privacy we discussed
above as well the role of algorithms in decision-making— leading its creator Joseph
Weizenbaum to ﬁght for social responsibility in AI and computer science in general.
Another early system, the GUS system (Bobrow et al., 1977) had by the late
1970s established the main frame-based paradigm that became the dominant indus-
trial paradigm for dialogue systems for over 30 years.
In the 1990s, stochastic models that had ﬁrst been applied to natural language
understanding began to be applied to dialogue slot ﬁlling (Miller et al. 1994, Pierac-
cini et al. 1991).
By around 2010 the GUS architecture ﬁnally began to be widely used commer-
cially in phone-based dialogue systems like Apple’s SIRI (Bellegarda, 2013) and
other digital assistants.
The rise of the web and online chatbots brought new interest in chatbots and gave
rise to corpus-based chatbot architectures around the turn of the century, ﬁrst using
information retrieval models and then in the 2010s, after the rise of deep learning,
with sequence-to-sequence models.
The idea that utterances in a conversation are a kind of action being performed
by the speaker was due originally to the philosopher Wittgenstein (1953) but worked
out more fully by Austin (1962) and his student John Searle. Various sets of speech
acts have been deﬁned over the years, and a rich linguistic and philosophical litera-
ture developed, especially focused on explaining the use of indirect speech acts.
The idea of dialogue acts draws also from a number of other sources, including
the ideas of adjacency pairs, pre-sequences, and other aspects of the international
properties of human conversation developed in the ﬁeld of conversation analysis
conversation
analysis
(see Levinson (1983) for an introduction to the ﬁeld).
This idea that acts set up strong local dialogue expectations was also preﬁgured
by Firth (1935, p. 70), in a famous quotation:
Most of the give-and-take of conversation in our everyday life is stereotyped
and very narrowly conditioned by our particular type of culture. It is a sort
of roughly prescribed social ritual, in which you generally say what the other
fellow expects you, one way or the other, to say.

516
CHAPTER 26
•
DIALOGUE SYSTEMS AND CHATBOTS
Another important research thread modeled dialogue as a kind of collaborative
behavior, including the ideas of common ground (Clark and Marshall, 1981), ref-
erence as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention
(Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980).
The dialogue-state model was also strongly informed by analytic work on the
linguistic properties of dialogue acts and on methods for their detection (Sag and
Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Good-
win 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano
et al. 2012).
Two important lines of research that we were unable to cover in the chapter fo-
cused on the computational properties of conversational structure. One line, ﬁrst
suggested by Bruce (1975), suggested that since speech acts are actions, they should
be planned like other actions, and drew on the AI planning literature (Fikes and Nils-
son, 1971). An agent seeking to ﬁnd out some information can come up with the plan
of asking the interlocutor for the information. An agent hearing an utterance can in-
terpret a speech act by running the planner “in reverse”, using inference rules to infer
from what the interlocutor said what the plan might have been. Plan-based models
of dialogue are referred to as BDI models because such planners model the beliefs,
BDI
desires, and intentions (BDI) of the agent and interlocutor. BDI models of dialogue
were ﬁrst introduced by Allen, Cohen, Perrault, and their colleagues in a number of
inﬂuential papers showing how speech acts could be generated (Cohen and Perrault,
1979) and interpreted (Perrault and Allen 1980, Allen and Perrault 1980). At the
same time, Wilensky (1983) introduced plan-based models of understanding as part
of the task of interpreting stories.
Another inﬂuential line of research focused on modeling the hierarchical struc-
ture of dialogue. Grosz’s pioneering (1977b) dissertation ﬁrst showed that “task-
oriented dialogues have a structure that closely parallels the structure of the task
being performed” (p. 27), leading to her work with Sidner and others showing how
to use similar notions of intention and plans to model discourse structure and co-
herence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the role of
intentional structure in dialogue.
The idea of applying reinforcement learning to dialogue ﬁrst came out of AT&T
and Bell Laboratories around the turn of the century with work on MDP dialogue
systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) and work on cue phrases,
prosody, and rejection and conﬁrmation. Reinforcement learning research turned
quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006,
Williams and Young 2007) applied to small slot-ﬁlling dialogue tasks,
Affect has played an important role in dialogue systems since its earliest days.
In more recent work Mairesse and Walker (2008) showed that conversational agents
are received better by users if they match users’ personality expectations. Rashkin
et al. (2019) introduced the EMPATHETICDIALOGUES dataset of 25k conversations
grounded in emotional situations, and (Lin et al., 2019) used mixtures of empathetic
listeners (MoEL), each optimized to react to particular emotions, to generate empa-
thetic responses.
[TBD: History of deep reinforcement learning here.] [TBD: surveys: Tur and
De Mori (2011), Gao et al. (2019)]
[TBD: more history here on dialogue state tracking, NLG, end-to-end neural
systems, etc]

EXERCISES
517
Exercises
26.1 Write a ﬁnite-state automaton for a dialogue manager for checking your bank
balance and withdrawing money at an automated teller machine.
26.2 A dispreferred response is a response that has the potential to make a person
dispreferred
response
uncomfortable or embarrassed in the conversational context; the most com-
mon example dispreferred responses is turning down a request. People signal
their discomfort with having to say no with surface cues (like the word well),
or via signiﬁcant silence. Try to notice the next time you or someone else
utters a dispreferred response, and write down the utterance. What are some
other cues in the response that a system might use to detect a dispreferred
response? Consider non-verbal cues like eye gaze and body gestures.
26.3 When asked a question to which they aren’t sure they know the answer, peo-
ple display their lack of conﬁdence by cues that resemble other dispreferred
responses. Try to notice some unsure answers to questions. What are some
of the cues? If you have trouble doing this, read Smith and Clark (1993) and
listen speciﬁcally for the cues they mention.
26.4 Implement a small air-travel help system based on text input. Your system
should get constraints from users about a particular ﬂight that they want to
take, expressed in natural language, and display possible ﬂights on a screen.
Make simplifying assumptions. You may build in a simple ﬂight database or
you may use a ﬂight information system on the Web as your backend.
26.5 Test your email-reading system on some potential users. Choose some of the
metrics described in Section 26.5 and evaluate your system.

CHAPTER
27
Phonetics
The characters or letters that are the basis of all the text-based methods we’ve
seen so far in this book aren’t just random symbols. They are also an amazing
scientiﬁc invention: a theoretical model of the elements that make up human speech.
The earliest independently invented writing systems (Sumerian, Chinese, Mayan)
were mainly logographic, which means one symbol representing a whole word. But
from the earliest stages we can ﬁnd, some of the symbols also represent the sounds
that make up the words. Thus, the cuneiform sign to the
right pronounced ba and meaning “ration” in Sumerian
could also function purely as the sound /ba/ in languages
that used cuneiform. Chinese writing, from its early instan-
tiations on oracle bones, also assigns phonetic meaning to many character elements.
Purely sound-based writing systems, whether syllabic (like Japanese hiragana or
katakana), alphabetic (like the Roman alphabet used in this book), or consonantal
(like Semitic writing systems), can generally be traced back to these early logo-
syllabic systems, often as two cultures came together. Thus, the Arabic, Aramaic,
Hebrew, Greek, and Roman systems all derive from a West Semitic script that is pre-
sumed to have been modiﬁed by Western Semitic mercenaries from a cursive form
of Egyptian hieroglyphs. The Japanese syllabaries were modiﬁed from a cursive
form of a set of Chinese characters that represented sounds. These Chinese charac-
ters themselves were used in Chinese to phonetically represent the Sanskrit in the
Buddhist scriptures that were brought to China in the Tang dynasty.
Whatever its origins, the idea implicit in a sound-based writing system—that
the spoken word is composed of smaller units of speech—underlies the modern
algorithms for speech recognition (transcribing acoustic waveforms into strings of
text words) and speech synthesis or text-to-speech (converting strings of text words
into acoustic waveforms).
In this chapter we introduce phonetics from a computational perspective. Pho-
phonetics
netics is the study of the speech sounds used in the languages of the world, how
they are produced by the articulators of the human vocal tract, how they are realized
acoustically, and how this acoustic realization can be digitized and processed.
27.1
Speech Sounds and Phonetic Transcription
Although a letter like ‘p’ or ‘a’ is a useful rough model of the sounds of human
speech, in speech processing we often model the pronunciation of a word instead as
a string of phones. A phone is a speech sound, represented with phonetic symbols
phone
modeled on letters in the Roman alphabet.
518

27.1
•
SPEECH SOUNDS AND PHONETIC TRANSCRIPTION
519
ARPAbet
IPA
ARPAbet
Symbol
Symbol
Word
Transcription
[p]
[p]
parsley
[p aa r s l iy]
[t]
[t]
tea
[t iy]
[k]
[k]
cook
[k uh k]
[b]
[b]
bay
[b ey]
[d]
[d]
dill
[d ih l]
[g]
[g]
garlic
[g aa r l ix k]
[m]
[m]
mint
[m ih n t]
[n]
[n]
nutmeg
[n ah t m eh g]
[ng]
[N]
baking
[b ey k ix ng]
[f]
[f]
flour
[f l aw axr]
[v]
[v]
clove
[k l ow v]
[th]
[T]
thick
[th ih k]
[dh]
[D]
those
[dh ow z]
[s]
[s]
soup
[s uw p]
[z]
[z]
eggs
[eh g z]
[sh]
[S]
squash
[s k w aa sh]
[zh]
[Z]
ambrosia
[ae m b r ow zh ax]
[ch]
[tS]
cherry
[ch eh r iy]
[jh]
[dZ]
jar
[jh aa r]
[l]
[l]
licorice
[l ih k axr ix sh]
[w]
[w]
kiwi
[k iy w iy]
[r]
[r]
rice
[r ay s]
[y]
[j]
yellow
[y eh l ow]
[h]
[h]
honey
[h ah n iy]
Figure 27.1
ARPAbet symbols for transcribing English consonants, with IPA equivalents.
This section surveys the different phones of English, focusing on American En-
glish. The International Phonetic Alphabet (IPA) is an evolving standard origi-
IPA
nally developed by the International Phonetic Association in 1888 with the goal of
transcribing the sounds of all human languages. The ARPAbet (Shoup, 1980) is a
phonetic alphabet designed for American English that uses ASCII symbols; it can be
thought of as a convenient ASCII representation of an American-English subset of
the IPA. Because the ARPAbet is common for computational modeling, we rely on it
here. Figures 27.1 and 27.2 show the ARPAbet symbols for transcribing consonants
and vowels, respectively, together with their IPA equivalents.
Many of the IPA and ARPAbet symbols are equivalent to familiar Roman let-
ters. So, for example, the ARPAbet phone [p] represents the consonant sound at the
beginning of platypus, puma, and plantain, the middle of leopard, or the end of an-
telope. In general, however, the mapping between the letters of English orthography
and phones is relatively opaque; a single letter can represent very different sounds
in different contexts. The English letter c corresponds to phone [k] in cougar [k uw
g axr], but phone [s] in cell [s eh l]. Besides appearing as c and k, the phone [k] can
appear as part of x (fox [f aa k s]), as ck (jackal [jh ae k el]) and as cc (raccoon [r ae
k uw n]). Many other languages, for example, Spanish, are much more transparent
in their sound-orthography mapping than English.

520
CHAPTER 27
•
PHONETICS
ARPAbet
IPA
ARPAbet
Symbol
Symbol
Word
Transcription
[iy]
[i]
lily
[l ih l iy]
[ih]
[I]
lily
[l ih l iy]
[ey]
[eI]
daisy
[d ey z iy]
[eh]
[E]
pen
[p eh n]
[ae]
[æ]
aster
[ae s t axr]
[aa]
[A]
poppy
[p aa p iy]
[ao]
[O]
orchid
[ao r k ix d]
[uh]
[U]
wood
[w uh d]
[ow]
[oU]
lotus
[l ow dx ax s]
[uw]
[u]
tulip
[t uw l ix p]
[ah]
[2]
buttercup
[b ah dx axr k ah p]
[er]
[Ç]
bird
[b er d]
[ay]
[aI]
iris
[ay r ix s]
[aw]
[aU]
sunﬂower
[s ah n f l aw axr]
[oy]
[oI]
soil
[s oy l]
Figure 27.2
ARPAbet symbols for American English vowels, with IPA equivalents.
27.2
Articulatory Phonetics
Articulatory phonetics is the study of how these phones are produced as the various
articulatory
phonetics
organs in the mouth, throat, and nose modify the airﬂow from the lungs.
27.2.1
The Vocal Organs
Figure 27.3 shows the organs of speech. Sound is produced by the rapid movement
of air. Humans produce most sounds in spoken languages by expelling air from the
lungs through the windpipe (technically, the trachea) and then out the mouth or
nose. As it passes through the trachea, the air passes through the larynx, commonly
known as the Adam’s apple or voice box. The larynx contains two small folds of
muscle, the vocal folds (often referred to non-technically as the vocal cords), which
can be moved together or apart. The space between these two folds is called the
glottis. If the folds are close together (but not tightly closed), they will vibrate as air
glottis
passes through them; if they are far apart, they won’t vibrate. Sounds made with the
vocal folds together and vibrating are called voiced; sounds made without this vocal
voiced sound
cord vibration are called unvoiced or voiceless. Voiced sounds include [b], [d], [g],
unvoiced sound
[v], [z], and all the English vowels, among others. Unvoiced sounds include [p], [t],
[k], [f], [s], and others.
The area above the trachea is called the vocal tract; it consists of the oral tract
and the nasal tract. After the air leaves the trachea, it can exit the body through the
mouth or the nose. Most sounds are made by air passing through the mouth. Sounds
made by air passing through the nose are called nasal sounds; nasal sounds use both
nasal
the oral and nasal tracts as resonating cavities; English nasal sounds include [m], [n],
and [ng].
Phones are divided into two main classes: consonants and vowels. Both kinds
consonant
vowel
of sounds are formed by the motion of air through the mouth, throat or nose. Con-
sonants are made by restriction or blocking of the airﬂow in some way, and can be
voiced or unvoiced. Vowels have less obstruction, are usually voiced, and are gen-
erally louder and longer-lasting than consonants. The technical use of these terms is

27.2
•
ARTICULATORY PHONETICS
521
Figure 27.3
The vocal organs, shown in side view. Drawing by Laszlo Kubinyi from Sund-
berg (1977), c⃝Scientiﬁc American, used by permission.
much like the common usage; [p], [b], [t], [d], [k], [g], [f], [v], [s], [z], [r], [l], etc.,
are consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivow-
els (such as [y] and [w]) have some of the properties of both; they are voiced like
vowels, but they are short and less syllabic like consonants.
27.2.2
Consonants: Place of Articulation
Because consonants are made by restricting the airﬂow in some way, consonants can
be distinguished by where this restriction is made: the point of maximum restriction
is called the of a consonant. Places of articulation, shown in Fig. 27.4, can be a
place of
articulation
useful way of grouping phones into equivalence classes, described below.
Labial: Consonants whose main restriction is formed by the two lips coming to-
labial
gether have a bilabial place of articulation. In English these include [p] as
in possum, [b] as in bear, and [m] as in marmot. The English labiodental

522
CHAPTER 27
•
PHONETICS
(nasal tract)
dental
bilabial
glottal
palatal
velar
alveolar
Figure 27.4
Major English places of articulation.
consonants [v] and [f] are made by pressing the bottom lip against the upper
row of teeth and letting the air ﬂow through the space in the upper teeth.
Dental: Sounds that are made by placing the tongue against the teeth are dentals.
dental
The main dentals in English are the [th] of thing and the [dh] of though, which
are made by placing the tongue behind the teeth with the tip slightly between
the teeth.
Alveolar: The alveolar ridge is the portion of the roof of the mouth just behind the
alveolar
upper teeth. Most speakers of American English make the phones [s], [z], [t],
and [d] by placing the tip of the tongue against the alveolar ridge. The word
coronal is often used to refer to both dental and alveolar.
Palatal: The roof of the mouth (the palate) rises sharply from the back of the
palatal
palate
alveolar ridge. The palato-alveolar sounds [sh] (shrimp), [ch] (china), [zh]
(Asian), and [jh] (jar) are made with the blade of the tongue against the rising
back of the alveolar ridge. The palatal sound [y] of yak is made by placing the
front of the tongue up close to the palate.
Velar: The velum, or soft palate, is a movable muscular ﬂap at the very back of the
velar
roof of the mouth. The sounds [k] (cuckoo), [g] (goose), and [N] (kingﬁsher)
are made by pressing the back of the tongue up against the velum.
Glottal: The glottal stop [q] (IPA [P]) is made by closing the glottis (by bringing
glottal
the vocal folds together).
27.2.3
Consonants: Manner of Articulation
Consonants are also distinguished by how the restriction in airﬂow is made, for ex-
ample, by a complete stoppage of air or by a partial blockage. This feature is called
the manner of articulation of a consonant. The combination of place and manner
manner of
articulation
of articulation is usually sufﬁcient to uniquely identify a consonant. Following are
the major manners of articulation for English consonants:
A stop is a consonant in which airﬂow is completely blocked for a short time.
stop
This blockage is followed by an explosive sound as the air is released. The period
of blockage is called the closure, and the explosion is called the release. English
has voiced stops like [b], [d], and [g] as well as unvoiced stops like [p], [t], and [k].
Stops are also called plosives.
The nasal sounds [n], [m], and [ng] are made by lowering the velum and allow-
nasal
ing air to pass into the nasal cavity.
In fricatives, airﬂow is constricted but not cut off completely. The turbulent
fricatives
airﬂow that results from the constriction produces a characteristic “hissing” sound.
The English labiodental fricatives [f] and [v] are produced by pressing the lower
lip against the upper teeth, allowing a restricted airﬂow between the upper teeth.

27.2
•
ARTICULATORY PHONETICS
523
The dental fricatives [th] and [dh] allow air to ﬂow around the tongue between the
teeth. The alveolar fricatives [s] and [z] are produced with the tongue against the
alveolar ridge, forcing air over the edge of the teeth. In the palato-alveolar fricatives
[sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a
groove formed in the tongue. The higher-pitched fricatives (in English [s], [z], [sh]
and [zh]) are called sibilants. Stops that are followed immediately by fricatives are
sibilants
called affricates; these include English [ch] (chicken) and [jh] (giraffe).
In approximants, the two articulators are close together but not close enough to
approximant
cause turbulent airﬂow. In English [y] (yellow), the tongue moves close to the roof
of the mouth but not close enough to cause the turbulence that would characterize a
fricative. In English [w] (wood), the back of the tongue comes close to the velum.
American [r] can be formed in at least two ways; with just the tip of the tongue
extended and close to the palate or with the whole tongue bunched up near the palate.
[l] is formed with the tip of the tongue up against the alveolar ridge or the teeth, with
one or both sides of the tongue lowered to allow air to ﬂow over it. [l] is called a
lateral sound because of the drop in the sides of the tongue.
A tap or ﬂap [dx] (or IPA [R]) is a quick motion of the tongue against the alveolar
tap
ridge. The consonant in the middle of the word lotus ([l ow dx ax s]) is a tap in most
dialects of American English; speakers of many U.K. dialects would use a [t] instead
of a tap in this word.
27.2.4
Vowels
Like consonants, vowels can be characterized by the position of the articulators as
they are made. The three most relevant parameters for vowels are what is called
vowel height, which correlates roughly with the height of the highest part of the
tongue, vowel frontness or backness, indicating whether this high point is toward
the front or back of the oral tract and whether the shape of the lips is rounded or
not. Figure 27.5 shows the position of the tongue for different vowels.
boot [uw]
closed
velum
bat [ae]
palate
beet [iy]
tongue
Figure 27.5
Positions of the tongue for three English vowels: high front [iy], low front [ae]
and high back [uw].
In the vowel [iy], for example, the highest point of the tongue is toward the
front of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is
located toward the back of the mouth. Vowels in which the tongue is raised toward
the front are called front vowels; those in which the tongue is raised toward the
Front vowel
back are called back vowels. Note that while both [ih] and [eh] are front vowels,
Back vowel
the tongue is higher for [ih] than for [eh]. Vowels in which the highest point of the
tongue is comparatively high are called high vowels; vowels with mid or low values
High vowel
of maximum tongue height are called mid vowels or low vowels, respectively.
Figure 27.6 shows a schematic characterization of the height of different vowels.
It is schematic because the abstract property height correlates only roughly with ac-

524
CHAPTER 27
•
PHONETICS
front
back
low
high
iy
ih
eh
ae
uw
uh
ax
ah
ao
aa
y uw
ey
ow
oy
ay
aw
Figure 27.6
The schematic “vowel space” for English vowels.
tual tongue positions; it is, in fact, a more accurate reﬂection of acoustic facts. Note
that the chart has two kinds of vowels: those in which tongue height is represented
as a point and those in which it is represented as a path. A vowel in which the tongue
position changes markedly during the production of the vowel is a diphthong. En-
diphthong
glish is particularly rich in diphthongs.
The second important articulatory dimension for vowels is the shape of the lips.
Certain vowels are pronounced with the lips rounded (the same lip shape used for
whistling). These rounded vowels include [uw], [ao], and [ow].
rounded vowel
27.2.5
Syllables
Consonants and vowels combine to make a syllable. A syllable is a vowel-like (or
syllable
sonorant) sound together with some of the surrounding consonants that are most
closely associated with it. The word dog has one syllable, [d aa g] (in our dialect);
the word catnip has two syllables, [k ae t] and [n ih p]. We call the vowel at the
core of a syllable the nucleus. The optional initial consonant or set of consonants
nucleus
is called the onset. If the onset has more than one consonant (as in the word strike
onset
[s t r ay k]), we say it has a complex onset. The coda is the optional consonant or
coda
sequence of consonants following the nucleus. Thus [d] is the onset of dog, and [g]
is the coda. The rime, or rhyme, is the nucleus plus coda. Figure 27.7 shows some
rime
sample syllable structures.
σ
Rime
Coda
m
Nucleus
ae
Onset
h
σ
Rime
Coda
n
Nucleus
iy
Onset
r
g
σ
Rime
Coda
z
g
Nucleus
eh
Figure 27.7
Syllable structure of ham, green, eggs. σ=syllable.

27.3
•
PROSODIC PROMINENCE: ACCENT, STRESS AND SCHWA
525
The task of automatically breaking up a word into syllables is called syllabiﬁca-
tion. Syllable structure is also closely related to the phonotactics of a language. The
syllabiﬁcation
term phonotactics means the constraints on which phones can follow each other in
phonotactics
a language. For example, English has strong constraints on what kinds of conso-
nants can appear together in an onset; the sequence [zdr], for example, cannot be a
legal English syllable onset. Phonotactics can be represented by a language model
or ﬁnite-state model of phone sequences.
27.3
Prosodic Prominence: Accent, Stress and Schwa
In a natural utterance of American English, some words sound more prominent than
prominence
others, and certain syllables in these words are also more prominent than others.
What we mean by prominence is that these words or syllables are perceptually more
salient to the listener; speakers make a word or syllable more salient in English by
saying it louder, saying it slower (so it has a longer duration), or by varying F0
during the word, making it higher or more variable.
We capture the core notion of prominence by associating a linguistic marker with
prominent words and syllables, a marker called pitch accent. Words or syllables that
pitch accent
are prominent are said to bear (be associated with) a pitch accent. Pitch accent is
thus part of the phonological description of a word in context in a spoken utterance.
Thus this utterance might be pronounced by accenting the underlined words:
(27.1) I’m a little surprised to hear it characterized as happy.
Nuclear Accent
We generally need more ﬁne-grained distinctions than just a binary distinction be-
tween accented and unaccented words. For example, the last accent in a phrase gen-
erally is perceived as being more prominent than the other accents. This prominent
last accent is called the nuclear or emphatic accent. Emphatic accents are generally
emphatic
accent
used for semantic purposes, such as marking a word as the focus of the sentence or
as contrastive or otherwise important in some way. Such emphatic words are often
written IN CAPITAL LETTERS or with **stars** around them in texts or email or
Alice in Wonderland; here’s an example from the latter:
(27.2) “I know SOMETHING interesting is sure to happen,” she said to herself.
Lexical Stress
The syllables that bear pitch accent are called accented syllables, but not every
syllable of a word can be accented. Pitch accent has to be realized on the syllable
that has lexical stress. Lexical stress is a property of the words’ pronunciation in
lexical stress
dictionaries; the syllable that has lexical stress is the one that will be louder or longer
if the word is accented. For example, the word surprised is stressed on its second
syllable, not its ﬁrst. (try stressing the other syllable by saying SURprised; hopefully
that sounds wrong to you). Thus, if the word surprised receives a pitch accent in a
sentence, it is the second syllable that will be stronger. The following example shows
underlined accented words with the stressed syllable bearing the accent (the louder,
longer syllable) in boldface:
(27.3) I’m a little surprised to hear it characterized as happy.

526
CHAPTER 27
•
PHONETICS
Stress can be marked in dictionaries in various ways. The CMU dictionary
(CMU, 1993), for example, marks each vowel with the number 0 (unstressed), 1
(stressed), or 2 (secondary stress). Thus, the word counter is listed as [K AW1 N T ER0]
and the word table as [T EY1 B AH0 L]. secondary stress is deﬁned as a level of
secondary
stress
stress lower than primary stress but higher than an unstressed vowel, as in the word
dictionary [D IH1 K SH AH0 N EH2 R IY0]. Difference in lexical stress can affect
word meaning. For example the word content can be a noun or an adjective, but have
different stressed syllables (the noun is pronounced [K AA1 N T EH0 N T] and the
adjective [K AA0 N T EH1 N T]). In IPA, on the other hand, the symbol ["] before a
syllable indicates that it has lexical stress (e.g., ["par.sli]).
Reduced Vowels and Schwa
Vowels that are unstressed can be weakened even further to reduced vowels. The
reduced vowel
most common reduced vowel is schwa ([ax]). Reduced vowels in English don’t have
schwa
their full form; the articulatory gesture isn’t as complete as for a full vowel. As a
result, the shape of the mouth is somewhat neutral; the tongue is neither particularly
high nor low. The second vowel in parakeet is a schwa: [p ae r ax k iy t].
While schwa is the most common reduced vowel, it is not the only one, at least
not in some dialects (Bolinger, 1981). Besides [ax], the ARPAbet also includes a
reduced front vowel [ix] (IPA [1]), as well as [axr], which is an r-colored schwa
(often called schwar).1 Fig. 27.8 shows these reduced vowels.
ARPAbet
IPA
ARPAbet
Symbol
Symbol
Word
Transcription
[ax]
[@]
lotus
[l ow dx ax s]
[axr]
[Ä]
heather
[h eh dh axr]
[ix]
[1]
tulip
[t uw l ix p]
Figure 27.8
Reduced vowels in American English, ARPAbet and IPA. [ax] is the reduced
vowel schwa, [ix] is the reduced vowel corresponding to [ih], and [axr] is the reduced vowel
corresponding to [er].
Not all unstressed vowels are reduced; any vowel, and diphthongs in particular,
can retain its full quality even in unstressed position. For example, the vowel [iy]
can appear in stressed position as in the word eat [iy t] or in unstressed position as
in the word carry [k ae r iy].
We have mentioned a number of potential levels of prominence: accented,
prominence
stressed, secondary stress, full vowel, and reduced vowel. It is still an open re-
search question exactly how many levels are appropriate. Very few computational
systems make use of all ﬁve of these levels, most using between one and three.
27.4
Prosodic Structure and Tune
In poetry, the word prosody refers to the study of the metrical structure of verse. In
prosody
language processing, however, we use the term prosody to mean the study of the
intonational and rhythmic aspects of language. More technically, prosody has been
deﬁned by Ladd (1996) as the “use of suprasegmental features to convey sentence-
level pragmatic meanings”. The term suprasegmental means above and beyond the
suprasegmental
1
[ix] is generally dropped in computational applications (Miller, 1998), and [ax] and [ix] are falling
together in many dialects of English (Wells, 1982, p. 167–168).

27.4
•
PROSODIC STRUCTURE AND TUNE
527
level of the segment or phone. The term refers especially to the uses of acoustic
features like F0, duration, and energy independently of the phone string.
By sentence-level pragmatic meaning, Ladd is referring to a number of kinds
of meaning that have to do with the relation between a sentence and its discourse
or external context. For example, prosody can be used to mark discourse structure
or function, like the difference between statements and questions, or the way that
a conversation is structured into segments or subdialogs. Prosody is also used to
mark saliency, such as indicating that a particular word or phrase is important or
salient. Finally, prosody is heavily used for affective and emotional meaning, such
as expressing happiness, surprise, or anger.
The kind of prosodic prominence, that we saw in the prior section is one of
the most computational studied aspects of prosody, but there are two others that we
introduce in this section: prosodic structure and tune.
27.4.1
Prosodic Structure
Spoken sentences have prosodic structure in the sense that some words seem to
group naturally together and some words seem to have a noticeable break or dis-
juncture between them. Prosodic structure is often described in terms of prosodic
phrasing, meaning that an utterance has a prosodic phrase structure in a similar way
prosodic
phrasing
to it having a syntactic phrase structure. For example, in the sentence I wanted to
go to London, but could only get tickets for France there seem to be two main in-
tonation phrases, their boundary occurring at the comma. Furthermore, in the ﬁrst
intonation
phrase
phrase, there seems to be another set of lesser prosodic phrase boundaries (often
called intermediate phrases) that split up the words as I wanted | to go | to London.
intermediate
phrase
There is also a correlation between prosodic structure and syntactic structure
(Price et al. 1991, Ostendorf and Veilleux 1994, Koehn et al. 2000).
27.4.2
Tune
Two utterances with the same prominence and phrasing patterns can still differ
prosodically by having different tunes. The tune of an utterance is the rise and
tune
fall of its F0 over time. A very obvious example of tune is the difference between
statements and yes-no questions in English. The same sentence can be said with
a ﬁnal rise in F0 to indicate a yes-no question, or a ﬁnal fall in F0 to indicate a
declarative intonation. Figure 27.9 shows the F0 track of the same words spoken as
a question or a statement. Note that the question rises at the end; this is often called
a question rise. The falling intonation of the statement is called a ﬁnal fall.
question rise
ﬁnal fall
Time (s)
0
0.922
Pitch (Hz)
50
250
you
know what
i
mean
Time (s)
0
0.912
Pitch (Hz)
50
250
you know what
i
mean
(a)
(b)
Figure 27.9
The same text read as the statement You know what I mean (on the left) and as a question You
know what I mean? (on the right). Notice that yes-no question intonation in English has a sharp ﬁnal rise in F0.
It turns out that English makes wide use of tune to express meaning. Besides this
well-known rise for yes-no questions, an English phrase containing a list of nouns

528
CHAPTER 27
•
PHONETICS
separated by commas often has a short rise called a continuation rise after each
continuation
rise
noun. Other examples include the characteristic English contours for expressing
contradiction and expressing surprise.
The mapping between meaning and tune in English is extremely complex. Con-
sider the utterance oh, really. Without varying the phrasing or stress, it is still possi-
ble to have many variants of this by varying the intonational tune. For example, we
might have an excited version oh, really! (in the context of a reply to a statement
that you’ve just won the lottery); a sceptical version oh, really?—in the context of
not being sure that the speaker is being honest; to an angry oh, really! indicating
displeasure.
Linking Tune with Prominence: ToBI
It is also possible to link models of prominence with models of tune, allowing us
to model differences between pitch accents according to the tune associated with
them.
One of the most widely used linguistic models of prosody that enables this asso-
ciation is the ToBI (Tone and Break Indices) model (Silverman et al. 1992, Beckman
ToBI
and Hirschberg 1994, Pierrehumbert 1980, Pitrelli et al. 1994). ToBI is a phono-
logical theory of intonation that models prominence, tune, and boundaries. ToBI’s
model of prominence and tunes is based on the ﬁve pitch accents and four bound-
ary tones shown in Fig. 27.10.
Pitch Accents
Boundary Tones
H*
peak accent
L-L%
“ﬁnal fall”: “declarative contour” of American
English
L*
low accent
L-H%
continuation rise
L*+H
scooped accent
H-H%
“question rise”:
cantonical yes-no question
contour
L+H*
rising peak accent
H-L%
ﬁnal level plateau (plateau because H- causes
“upstep” of following)
H+!H* step down
Figure 27.10
The accent and boundary tones labels from the ToBI transcription system for
American English intonation (Beckman and Ayers 1997, Beckman and Hirschberg 1994).
An utterance in ToBI consists of a sequence of intonational phrases, each of
which ends in one of the four boundary tones. The boundary tones represent the
boundary tone
utterance ﬁnal aspects of tune. Each word in the utterances can optionally be asso-
ciated with one of the ﬁve types of pitch accents.
Each intonational phrase consists of one or more intermediate phrase. These
phrases can also be marked with kinds of boundary tone, including the %H high
initial boundary tone, which marks a phrase that is particularly high in the speaker’s
pitch range, as well as ﬁnal phrase accents H- and L-.
In addition to accents and boundary tones, ToBI distinguishes four levels of
phrasing, labeled on a separate break index tier. The largest phrasal breaks are the
Break index
intonational phrase (break index 4) and the intermediate phrase (break index 3), dis-
cussed above. Break index 2 is used to mark a disjuncture or pause between words
that is smaller than an intermediate phrase, and 1 is used for normal phrase-medial
word boundaries.
Figure 27.11 shows the tone, orthographic, and phrasing tiers of a ToBI tran-
Tier
scription, using the Praat program. The same sentence is read with two different
tunes. In (a), the word Marianna is spoken with a high H* accent, and the sentence
has the declarative boundary tone L-L%. In (b), the word Marianna is spoken with a

27.5
•
ACOUSTIC PHONETICS AND SIGNALS
529
low L* accent and the yes-no question boundary tone H-H%. One goal of ToBI is to
express different meanings to the different type of accents. Here, the L* accent adds
a meaning of surprise to the sentence (i.e., with a connotation like ‘Are you really
saying it was Marianna?’) (Hirschberg and Pierrehumbert 1986, Steedman 2007).
H*
L–L
<SIL>
marianna
made
the
marmalade
<SIL>
1
1
1
4
Time (s)
0
1.3
L*
H–H
marianna
made
the
marmalade
1
1
1
4
Time (s)
0
1.49
(a)
(b)
Figure 27.11
The same sentence read by Mary Beckman with two different intonation patterns and tran-
scribed in ToBI. (a) Shows an H* accent and the typical American English declarative ﬁnal fall L-L%. (b)
Shows the L* accent, with the typical American English yes-no question rise H-H%.
ToBI models have been proposed for many languages (Jun, 2005), such as the
J TOBI system for Japanese (Venditti, 2005).
27.5
Acoustic Phonetics and Signals
We begin with a brief introduction to the acoustic waveform and how it is digitized
and summarize the idea of frequency analysis and spectra. This is an extremely brief
overview; the interested reader is encouraged to consult the references at the end of
the chapter.
27.5.1
Waves
Acoustic analysis is based on the sine and cosine functions. Figure 27.12 shows a
plot of a sine wave, in particular the function
y = A∗sin(2π ft)
(27.4)
where we have set the amplitude A to 1 and the frequency f to 10 cycles per second.
Recall from basic mathematics that two important characteristics of a wave are
its frequency and amplitude. The frequency is the number of times a second that
frequency
amplitude
a wave repeats itself, that is, the number of cycles. We usually measure frequency
in cycles per second. The signal in Fig. 27.12 repeats itself 5 times in .5 seconds,
hence 10 cycles per second. Cycles per second are usually called hertz (shortened
Hertz
to Hz), so the frequency in Fig. 27.12 would be described as 10 Hz. The amplitude
A of a sine wave is the maximum value on the Y axis.
The period T of the wave is deﬁned as the time it takes for one cycle to complete,
period
deﬁned as
T = 1
f
(27.5)

530
CHAPTER 27
•
PHONETICS
Time (s)
0
0.5
–1.0
1.0
0
0
0.1
0.2
0.3
0.4
0.5
Figure 27.12
A sine wave with a frequency of 10 Hz and an amplitude of 1.
In Fig. 27.12 we can see that each cycle lasts a tenth of a second; hence T = .1
seconds.
27.5.2
Speech Sound Waves
Let’s turn from hypothetical waves to sound waves. The input to a speech recog-
nizer, like the input to the human ear, is a complex series of changes in air pressure.
These changes in air pressure obviously originate with the speaker and are caused
by the speciﬁc way that air passes through the glottis and out the oral or nasal cav-
ities. We represent sound waves by plotting the change in air pressure over time.
One metaphor which sometimes helps in understanding these graphs is that of a ver-
tical plate blocking the air pressure waves (perhaps in a microphone in front of a
speaker’s mouth, or the eardrum in a hearer’s ear). The graph measures the amount
of compression or rarefaction (uncompression) of the air molecules at this plate.
Figure 27.13 shows a short segment of a waveform taken from the Switchboard
corpus of telephone speech of the vowel [iy] from someone saying “she just had a
baby”.
Time (s)
0
0.03875
–0.01697
0.02283
0
Figure 27.13
A waveform of the vowel [iy] from an utterance shown later in Fig. 27.17 on page 534. The
y-axis shows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time.
Notice that the wave repeats regularly.
Let’s explore how the digital representation of the sound wave shown in Fig. 27.13
would be constructed. The ﬁrst step in processing speech is to convert the analog
representations (ﬁrst air pressure and then analog electric signals in a microphone)
into a digital signal. This process of analog-to-digital conversion has two steps:
sampling and quantization. To sample a signal, we measure its amplitude at a
sampling
particular time; the sampling rate is the number of samples taken per second. To
accurately measure a wave, we must have at least two samples in each cycle: one
measuring the positive part of the wave and one measuring the negative part. More
than two samples per cycle increases the amplitude accuracy, but fewer than two
samples causes the frequency of the wave to be completely missed. Thus, the maxi-

27.5
•
ACOUSTIC PHONETICS AND SIGNALS
531
mum frequency wave that can be measured is one whose frequency is half the sam-
ple rate (since every cycle needs two samples). This maximum frequency for a
given sampling rate is called the Nyquist frequency. Most information in human
Nyquist
frequency
speech is in frequencies below 10,000 Hz; thus, a 20,000 Hz sampling rate would
be necessary for complete accuracy. But telephone speech is ﬁltered by the switch-
ing network, and only frequencies less than 4,000 Hz are transmitted by telephones.
Thus, an 8,000 Hz sampling rate is sufﬁcient for telephone-bandwidth speech like
the Switchboard corpus. A 16,000 Hz sampling rate (sometimes called wideband)
is often used for microphone speech.
Even an 8,000 Hz sampling rate requires 8000 amplitude measurements for each
second of speech, so it is important to store amplitude measurements efﬁciently.
They are usually stored as integers, either 8 bit (values from -128–127) or 16 bit
(values from -32768–32767). This process of representing real-valued numbers as
integers is called quantization because the difference between two integers acts as
quantization
a minimum granularity (a quantum size) and all values that are closer together than
this quantum size are represented identically.
Once data is quantized, it is stored in various formats. One parameter of these
formats is the sample rate and sample size discussed above; telephone speech is
often sampled at 8 kHz and stored as 8-bit samples, and microphone data is often
sampled at 16 kHz and stored as 16-bit samples. Another parameter of these for-
mats is the number of channels. For stereo data or for two-party conversations, we
channel
can store both channels in the same ﬁle or we can store them in separate ﬁles. A
ﬁnal parameter is individual sample storage—linearly or compressed. One common
compression format used for telephone speech is µ-law (often written u-law but still
pronounced mu-law). The intuition of log compression algorithms like µ-law is that
human hearing is more sensitive at small intensities than large ones; the log rep-
resents small values with more faithfulness at the expense of more error on large
values. The linear (unlogged) values are generally referred to as linear PCM values
PCM
(PCM stands for pulse code modulation, but never mind that). Here’s the equation
for compressing a linear PCM sample value x to 8-bit µ-law, (where µ=255 for 8
bits):
F(x) = sgn(s)log(1+ µ|s|)
log(1+ µ)
(27.6)
There are a number of standard ﬁle formats for storing the resulting digitized
waveﬁle, such as Microsoft’s .wav, Apple’s AIFF and Sun’s AU, all of which have
special headers; simple headerless “raw” ﬁles are also used. For example, the .wav
format is a subset of Microsoft’s RIFF format for multimedia ﬁles; RIFF is a general
format that can represent a series of nested chunks of data and control information.
Figure 27.14 shows a simple .wav ﬁle with a single data chunk together with its
format chunk.
Figure 27.14
Microsoft waveﬁle header format, assuming simple ﬁle with one chunk. Fol-
lowing this 44-byte header would be the data chunk.

532
CHAPTER 27
•
PHONETICS
27.5.3
Frequency and Amplitude; Pitch and Loudness
Sound waves, like all waves, can be described in terms of frequency, amplitude, and
the other characteristics that we introduced earlier for pure sine waves. In sound
waves, these are not quite as simple to measure as they were for sine waves. Let’s
consider frequency. Note in Fig. 27.13 that although not exactly a sine, the wave is
nonetheless periodic, repeating 10 times in the 38.75 milliseconds (.03875 seconds)
captured in the ﬁgure. Thus, the frequency of this segment of the wave is 10/.03875
or 258 Hz.
Where does this periodic 258 Hz wave come from? It comes from the speed
of vibration of the vocal folds; since the waveform in Fig. 27.13 is from the vowel
[iy], it is voiced. Recall that voicing is caused by regular openings and closing of
the vocal folds. When the vocal folds are open, air is pushing up through the lungs,
creating a region of high pressure. When the folds are closed, there is no pressure
from the lungs. Thus, when the vocal folds are vibrating, we expect to see regular
peaks in amplitude of the kind we see in Fig. 27.13, each major peak corresponding
to an opening of the vocal folds. The frequency of the vocal fold vibration, or the
frequency of the complex wave, is called the fundamental frequency of the wave-
fundamental
frequency
form, often abbreviated F0. We can plot F0 over time in a pitch track. Figure 27.15
F0
pitch track
shows the pitch track of a short question, “Three o’clock?” represented below the
waveform. Note the rise in F0 at the end of the question.
three
o’clock
Time (s)
0
0.544375
0 Hz
500 Hz
Figure 27.15
Pitch track of the question “Three o’clock?”, shown below the waveﬁle. Note
the rise in F0 at the end of the question. Note the lack of pitch trace during the very quiet part
(the “o’” of “o’clock”; automatic pitch tracking is based on counting the pulses in the voiced
regions, and doesn’t work if there is no voicing (or insufﬁcient sound).
The vertical axis in Fig. 27.13 measures the amount of air pressure variation;
pressure is force per unit area, measured in Pascals (Pa). A high value on the vertical
axis (a high amplitude) indicates that there is more air pressure at that point in time,
a zero value means there is normal (atmospheric) air pressure, and a negative value
means there is lower than normal air pressure (rarefaction).
In addition to this value of the amplitude at any point in time, we also often
need to know the average amplitude over some time range, to give us some idea
of how great the average displacement of air pressure is. But we can’t just take
the average of the amplitude values over a range; the positive and negative values
would (mostly) cancel out, leaving us with a number close to zero. Instead, we
generally use the RMS (root-mean-square) amplitude, which squares each number

27.5
•
ACOUSTIC PHONETICS AND SIGNALS
533
before averaging (making it positive), and then takes the square root at the end.
RMS amplitudeN
i=1 =
v
u
u
t 1
N
N
X
i=1
x2
i
(27.7)
The power of the signal is related to the square of the amplitude. If the number
Power
of samples of a sound is N, the power is
Power = 1
N
N
X
i=1
x2
i
(27.8)
Rather than power, we more often refer to the intensity of the sound, which
Intensity
normalizes the power to the human auditory threshold and is measured in dB. If P0
is the auditory threshold pressure = 2×10−5 Pa, then intensity is deﬁned as follows:
Intensity = 10log10
1
NP0
N
X
i=1
x2
i
(27.9)
Figure 27.16 shows an intensity plot for the sentence “Is it a long movie?” from
the CallHome corpus, again shown below the waveform plot.
is
it
a
long
movie?
Time (s)
0
1.1675
Figure 27.16
Intensity plot for the sentence “Is it a long movie?”. Note the intensity peaks
at each vowel and the especially high peak for the word long.
Two important perceptual properties, pitch and loudness, are related to fre-
quency and intensity. The pitch of a sound is the mental sensation, or perceptual
pitch
correlate, of fundamental frequency; in general, if a sound has a higher fundamen-
tal frequency we perceive it as having a higher pitch. We say “in general” because
the relationship is not linear, since human hearing has different acuities for different
frequencies. Roughly speaking, human pitch perception is most accurate between
100 Hz and 1000 Hz and in this range pitch correlates linearly with frequency. Hu-
man hearing represents frequencies above 1000 Hz less accurately, and above this
range, pitch correlates logarithmically with frequency. Logarithmic representation
means that the differences between high frequencies are compressed and hence not
as accurately perceived. There are various psychoacoustic models of pitch percep-
tion scales. One common model is the mel scale (Stevens et al. 1937, Stevens and
Mel

534
CHAPTER 27
•
PHONETICS
Volkmann 1940). A mel is a unit of pitch deﬁned such that pairs of sounds which
are perceptually equidistant in pitch are separated by an equal number of mels. The
mel frequency m can be computed from the raw acoustic frequency as follows:
m = 1127ln(1+
f
700)
(27.10)
As we’ll see in Chapter 28, the mel scale plays an important role in speech
recognition.
The loudness of a sound is the perceptual correlate of the power. So sounds with
higher amplitudes are perceived as louder, but again the relationship is not linear.
First of all, as we mentioned above when we deﬁned µ-law compression, humans
have greater resolution in the low-power range; the ear is more sensitive to small
power differences. Second, it turns out that there is a complex relationship between
power, frequency, and perceived loudness; sounds in certain frequency ranges are
perceived as being louder than those in other frequency ranges.
Various algorithms exist for automatically extracting F0. In a slight abuse of ter-
minology, these are called pitch extraction algorithms. The autocorrelation method
pitch extraction
of pitch extraction, for example, correlates the signal with itself at various offsets.
The offset that gives the highest correlation gives the period of the signal. Other
methods for pitch extraction are based on the cepstral features we introduce in Chap-
ter 28. There are various publicly available pitch extraction toolkits; for example,
an augmented autocorrelation pitch tracker is provided with Praat (Boersma and
Weenink, 2005).
27.5.4
Interpretation of Phones from a Waveform
Much can be learned from a visual inspection of a waveform. For example, vowels
are pretty easy to spot. Recall that vowels are voiced; another property of vowels
is that they tend to be long and are relatively loud (as we can see in the intensity
plot in Fig. 27.16). Length in time manifests itself directly on the x-axis, and loud-
ness is related to (the square of) amplitude on the y-axis. We saw in the previous
section that voicing is realized by regular peaks in amplitude of the kind we saw in
Fig. 27.13, each major peak corresponding to an opening of the vocal folds. Fig-
ure 27.17 shows the waveform of the short sentence “she just had a baby”. We have
labeled this waveform with word and phone labels. Notice that each of the six vow-
els in Fig. 27.17, [iy], [ax], [ae], [ax], [ey], [iy], all have regular amplitude peaks
indicating voicing.
she
just
had
a
baby
sh
iy
j
ax
s
h
ae
dx
ax
b
ey
b
iy
Time (s)
0
1.059
Figure 27.17
A waveform of the sentence “She just had a baby” from the Switchboard corpus (conversation
4325). The speaker is female, was 20 years old in 1991, which is approximately when the recording was made,
and speaks the South Midlands dialect of American English.

27.5
•
ACOUSTIC PHONETICS AND SIGNALS
535
For a stop consonant, which consists of a closure followed by a release, we can
often see a period of silence or near silence followed by a slight burst of amplitude.
We can see this for both of the [b]’s in baby in Fig. 27.17.
Another phone that is often quite recognizable in a waveform is a fricative. Re-
call that fricatives, especially very strident fricatives like [sh], are made when a
narrow channel for airﬂow causes noisy, turbulent air. The resulting hissy sounds
have a noisy, irregular waveform. This can be seen somewhat in Fig. 27.17; it’s even
clearer in Fig. 27.18, where we’ve magniﬁed just the ﬁrst word she.
she
sh
iy
Time (s)
0
0.257
Figure 27.18
A more detailed view of the ﬁrst word “she” extracted from the waveﬁle in Fig. 27.17. Notice
the difference between the random noise of the fricative [sh] and the regular voicing of the vowel [iy].
27.5.5
Spectra and the Frequency Domain
While some broad phonetic features (such as energy, pitch, and the presence of voic-
ing, stop closures, or fricatives) can be interpreted directly from the waveform, most
computational applications such as speech recognition (as well as human auditory
processing) are based on a different representation of the sound in terms of its com-
ponent frequencies. The insight of Fourier analysis is that every complex wave can
be represented as a sum of many sine waves of different frequencies. Consider the
waveform in Fig. 27.19. This waveform was created (in Praat) by summing two sine
waveforms, one of frequency 10 Hz and one of frequency 100 Hz.
Time (s)
0
0.5
–1
1
0
Figure 27.19
A waveform that is the sum of two sine waveforms, one of frequency 10
Hz (note ﬁve repetitions in the half-second window) and one of frequency 100 Hz, both of
amplitude 1.
We can represent these two component frequencies with a spectrum. The spec-
spectrum
trum of a signal is a representation of each of its frequency components and their
amplitudes. Figure 27.20 shows the spectrum of Fig. 27.19. Frequency in Hz is
on the x-axis and amplitude on the y-axis. Note the two spikes in the ﬁgure, one

536
CHAPTER 27
•
PHONETICS
at 10 Hz and one at 100 Hz. Thus, the spectrum is an alternative representation of
the original waveform, and we use the spectrum as a tool to study the component
frequencies of a sound wave at a particular time point.
Frequency (Hz)
1
10
100
2
20
200
5
50
Sound pressure level (dB/Hz)
40
60
80
Figure 27.20
The spectrum of the waveform in Fig. 27.19.
Let’s look now at the frequency components of a speech waveform. Figure 27.21
shows part of the waveform for the vowel [ae] of the word had, cut out from the
sentence shown in Fig. 27.17.
Time (s)
0
0.04275
–0.05554
0.04968
0
Figure 27.21
The waveform of part of the vowel [ae] from the word had cut out from the
waveform shown in Fig. 27.17.
Note that there is a complex wave that repeats about ten times in the ﬁgure; but
there is also a smaller repeated wave that repeats four times for every larger pattern
(notice the four small peaks inside each repeated wave). The complex wave has a
frequency of about 234 Hz (we can ﬁgure this out since it repeats roughly 10 times
in .0427 seconds, and 10 cycles/.0427 seconds = 234 Hz).
The smaller wave then should have a frequency of roughly four times the fre-
quency of the larger wave, or roughly 936 Hz. Then, if you look carefully, you can
see two little waves on the peak of many of the 936 Hz waves. The frequency of this
tiniest wave must be roughly twice that of the 936 Hz wave, hence 1872 Hz.
Figure 27.22 shows a smoothed spectrum for the waveform in Fig. 27.21, com-
puted with a discrete Fourier transform (DFT).
The x-axis of a spectrum shows frequency, and the y-axis shows some mea-
sure of the magnitude of each frequency component (in decibels (dB), a logarithmic
measure of amplitude that we saw earlier). Thus, Fig. 27.22 shows signiﬁcant fre-
quency components at around 930 Hz, 1860 Hz, and 3020 Hz, along with many
other lower-magnitude frequency components. These ﬁrst two components are just
what we noticed in the time domain by looking at the wave in Fig. 27.21!
Why is a spectrum useful? It turns out that these spectral peaks that are easily
visible in a spectrum are characteristic of different phones; phones have characteris-
tic spectral “signatures”. Just as chemical elements give off different wavelengths of
light when they burn, allowing us to detect elements in stars by looking at the spec-

27.5
•
ACOUSTIC PHONETICS AND SIGNALS
537
Frequency (Hz)
0
4000
Sound pressure level (dB/Hz)
–20
0
20
0
2000
4000
0
1000
2000
3000
4000
Figure 27.22
A spectrum for the vowel [ae] from the word had in the waveform of She just
had a baby in Fig. 27.17.
trum of the light, we can detect the characteristic signature of the different phones
by looking at the spectrum of a waveform. This use of spectral information is essen-
tial to both human and machine speech recognition. In human audition, the function
of the cochlea, or inner ear, is to compute a spectrum of the incoming waveform.
cochlea
Similarly, the various kinds of acoustic features used in speech recognition as the
HMM observation are all different representations of spectral information.
Let’s look at the spectrum of different vowels. Since some vowels change over
time, we’ll use a different kind of plot called a spectrogram. While a spectrum
shows the frequency components of a wave at one point in time, a spectrogram is a
spectrogram
way of envisioning how the different frequencies that make up a waveform change
over time. The x-axis shows time, as it did for the waveform, but the y-axis now
shows frequencies in hertz. The darkness of a point on a spectrogram corresponds
to the amplitude of the frequency component. Very dark points have high amplitude,
light points have low amplitude. Thus, the spectrogram is a useful way of visualizing
the three dimensions (time x frequency x amplitude).
Figure 27.23 shows spectrograms of three American English vowels, [ih], [ae],
and [ah]. Note that each vowel has a set of dark bars at various frequency bands,
slightly different bands for each vowel. Each of these represents the same kind of
spectral peak that we saw in Fig. 27.21.
Time (s)
0
2.81397
0
5000
Frequency (Hz)
Figure 27.23
Spectrograms for three American English vowels, [ih], [ae], and [uh], spoken
by the ﬁrst author.
Each dark bar (or spectral peak) is called a formant. As we discuss below, a
formant
formant is a frequency band that is particularly ampliﬁed by the vocal tract. Since
different vowels are produced with the vocal tract in different positions, they will
produce different kinds of ampliﬁcations or resonances. Let’s look at the ﬁrst two
formants, called F1 and F2. Note that F1, the dark bar closest to the bottom, is in a
different position for the three vowels; it’s low for [ih] (centered at about 470 Hz)

538
CHAPTER 27
•
PHONETICS
and somewhat higher for [ae] and [ah] (somewhere around 800 Hz). By contrast,
F2, the second dark bar from the bottom, is highest for [ih], in the middle for [ae],
and lowest for [ah].
We can see the same formants in running speech, although the reduction and
coarticulation processes make them somewhat harder to see. Figure 27.24 shows
the spectrogram of “she just had a baby”, whose waveform was shown in Fig. 27.17.
F1 and F2 (and also F3) are pretty clear for the [ax] of just, the [ae] of had, and the
[ey] of baby.
she
just
had
a
baby
sh
iy
j
ax
s
h
ae
dx
ax
b
ey
b
iy
Time (s)
0
1.059
Figure 27.24
A spectrogram of the sentence “she just had a baby” whose waveform was shown in Fig. 27.17.
We can think of a spectrogram as a collection of spectra (time slices), like Fig. 27.22 placed end to end.
What speciﬁc clues can spectral representations give for phone identiﬁcation?
First, since different vowels have their formants at characteristic places, the spectrum
can distinguish vowels from each other. We’ve seen that [ae] in the sample waveform
had formants at 930 Hz, 1860 Hz, and 3020 Hz. Consider the vowel [iy] at the
beginning of the utterance in Fig. 27.17. The spectrum for this vowel is shown in
Fig. 27.25. The ﬁrst formant of [iy] is 540 Hz, much lower than the ﬁrst formant for
[ae], and the second formant (2581 Hz) is much higher than the second formant for
[ae]. If you look carefully, you can see these formants as dark bars in Fig. 27.24 just
around 0.5 seconds.
−10
0
10
20
30
40
50
60
70
80
0
1000
2000
3000
Figure 27.25
A smoothed (LPC) spectrum for the vowel [iy] at the start of She just had a
baby. Note that the ﬁrst formant (540 Hz) is much lower than the ﬁrst formant for [ae] shown
in Fig. 27.22, and the second formant (2581 Hz) is much higher than the second formant for
[ae].
The location of the ﬁrst two formants (called F1 and F2) plays a large role in de-
termining vowel identity, although the formants still differ from speaker to speaker.

27.6
•
PHONETIC RESOURCES
539
Higher formants tend to be caused more by general characteristics of a speaker’s
vocal tract rather than by individual vowels. Formants also can be used to identify
the nasal phones [n], [m], and [ng] and the liquids [l] and [r].
27.5.6
The Source-Filter Model
Why do different vowels have different spectral signatures? As we brieﬂy mentioned
above, the formants are caused by the resonant cavities of the mouth. The source-
ﬁlter model is a way of explaining the acoustics of a sound by modeling how the
source-ﬁlter
model
pulses produced by the glottis (the source) are shaped by the vocal tract (the ﬁlter).
Let’s see how this works. Whenever we have a wave such as the vibration in air
caused by the glottal pulse, the wave also has harmonics. A harmonic is another
harmonic
wave whose frequency is a multiple of the fundamental wave. Thus, for example, a
115 Hz glottal fold vibration leads to harmonics (other waves) of 230 Hz, 345 Hz,
460 Hz, and so on on. In general, each of these waves will be weaker, that is, will
have much less amplitude than the wave at the fundamental frequency.
It turns out, however, that the vocal tract acts as a kind of ﬁlter or ampliﬁer;
indeed any cavity, such as a tube, causes waves of certain frequencies to be ampliﬁed
and others to be damped. This ampliﬁcation process is caused by the shape of the
cavity; a given shape will cause sounds of a certain frequency to resonate and hence
be ampliﬁed. Thus, by changing the shape of the cavity, we can cause different
frequencies to be ampliﬁed.
When we produce particular vowels, we are essentially changing the shape of
the vocal tract cavity by placing the tongue and the other articulators in particular
positions. The result is that different vowels cause different harmonics to be ampli-
ﬁed. So a wave of the same fundamental frequency passed through different vocal
tract positions will result in different harmonics being ampliﬁed.
We can see the result of this ampliﬁcation by looking at the relationship between
the shape of the vocal tract and the corresponding spectrum. Figure 27.26 shows
the vocal tract position for three vowels and a typical resulting spectrum. The for-
mants are places in the spectrum where the vocal tract happens to amplify particular
harmonic frequencies.
27.6
Phonetic Resources
A wide variety of phonetic resources can be drawn on for computational work. One
key set of resources are pronunciation dictionaries. Such on-line phonetic dictio-
Pronunciation
dictionary
naries give phonetic transcriptions for each word. Three commonly used on-line
dictionaries for English are the CELEX, CMUdict, and PRONLEX lexicons; for
other languages, the LDC has released pronunciation dictionaries for Egyptian Ara-
bic, German, Japanese, Korean, Mandarin, and Spanish. All these dictionaries can
be used for both speech recognition and synthesis work.
The CELEX dictionary (Baayen et al., 1995) is the most richly annotated of
the dictionaries. It includes all the words in the 1974 Oxford Advanced Learner’s
Dictionary (41,000 lemmata) and the 1978 Longman Dictionary of Contemporary
English (53,000 lemmata); in total it has pronunciations for 160,595 wordforms. Its
(British rather than American) pronunciations are transcribed with an ASCII version
of the IPA called SAM. In addition to basic phonetic information like phone strings,
syllabiﬁcation, and stress level for each syllable, each word is also annotated with

540
CHAPTER 27
•
PHONETICS
[iy]  (tea)
[ae] (cat) 
[uw]  (moo)
Frequency (Hz)
0
4000
Sound pressure level (dB/Hz)
0
20
268
2416
F1
F2
Frequency (Hz)
0
4000
Sound pressure level (dB/Hz)
0
20
903
1695
F1
F2
Frequency (Hz)
0
Sound pressure level (dB/Hz)
–20
0
295
817
F1
F2
[iy]  (tea)
[ae] (cat) 
[uw]  (moo)
Figure 27.26
Visualizing the vocal tract position as a ﬁlter: the tongue positions for three
English vowels and the resulting smoothed spectra showing F1 and F2.
morphological, part-of-speech, syntactic, and frequency information. CELEX (as
well as CMU and PRONLEX) represent three levels of stress: primary stress, sec-
ondary stress, and no stress. For example, some of the CELEX information for the
word dictionary includes multiple pronunciations (’dIk-S@n-rI and ’dIk-S@-n@-
rI, corresponding to ARPAbet [d ih k sh ax n r ih] and [d ih k sh ax n ax r ih],
respectively), together with the CV skelata for each one ([CVC][CVC][CV] and
[CVC][CV][CV][CV]), the frequency of the word, the fact that it is a noun, and its
morphological structure (diction+ary).
The free CMU Pronouncing Dictionary (CMU, 1993) has pronunciations for
about 125,000 wordforms. It uses a 39-phone ARPAbet-derived phoneme set. Tran-
scriptions are phonemic, and thus instead of marking any kind of surface reduction
like ﬂapping or reduced vowels, it marks each vowel with the number 0 (unstressed),
1 (stressed), or 2 (secondary stress). Thus, the word tiger is listed as [T AY1 G ER0],
the word table as [T EY1 B AH0 L], and the word dictionary as [D IH1 K SH AH0
N EH2 R IY0]. The dictionary is not syllabiﬁed, although the nucleus is implicitly
marked by the (numbered) vowel. Figure 27.27 shows some sample pronunciations.
ANTECEDENTS AE2 N T IH0 S IY1 D AH0 N T S
PAKISTANI
P AE2 K IH0 S T AE1 N IY0
CHANG
CH AE1 NG
TABLE
T EY1 B AH0 L
DICTIONARY
D IH1 K SH AH0 N EH2 R IY0
TROTSKY
T R AA1 T S K IY2
DINNER
D IH1 N ER0
WALTER
W AO1 L T ER0
LUNCH
L AH1 N CH
WALTZING
W AO1 L T S IH0 NG
MCFARLAND
M AH0 K F AA1 R L AH0 N D
WALTZING(2) W AO1 L S IH0 NG
Figure 27.27
Some sample pronunciations from the CMU Pronouncing Dictionary.

27.6
•
PHONETIC RESOURCES
541
The PRONLEX dictionary (LDC, 1995) was designed for speech recognition
and contains pronunciations for 90,694 wordforms. It covers all the words used in
many years of the Wall Street Journal, as well as the Switchboard Corpus. PRON-
LEX has the advantage that it includes many proper names (20,000, whereas CELEX
only has about 1000). Names are important for practical applications, and they are
both frequent and difﬁcult; we return to a discussion of deriving name pronuncia-
tions in Chapter 28.
The CMU dictionary was designed for speech recognition rather than synthesis
uses; thus, it does not specify which of the multiple pronunciations to use for syn-
thesis, does not mark syllable boundaries, and because it capitalizes the dictionary
headwords, does not distinguish between, for example, US and us (the form US has
the two pronunciations [AH1 S] and [Y UW1 EH1 S]).
The 110,000 word UNISYN dictionary, freely available for research purposes,
resolves many of these issues as it was designed speciﬁcally for synthesis (Fitt,
2002). UNISYN gives syllabiﬁcations, stress, and some morphological boundaries.
Furthermore, pronunciations in UNISYN can also be read off in any of dozens of
dialects of English, including General American, RP British, Australia, and so on.
The UNISYN uses a slightly different phone set; here are some examples:
going:
{ g * ou }.> i ng >
antecedents:
{ * a n . tˆ i . s ˜ ii . d n! t }> s >
dictionary:
{ d * i k . sh @ . n ˜ e . r ii }
Another useful resource is a phonetically annotated corpus, in which a collec-
tion of waveforms is hand-labeled with the corresponding string of phones. Three
important phonetic corpora in English are the TIMIT corpus, the Switchboard cor-
pus, and the Buckeye corpus.
The TIMIT corpus (NIST, 1990) was collected as a joint project between Texas
Instruments (TI), MIT, and SRI. It is a corpus of 6300 read sentences, with 10 sen-
tences each from 630 speakers. The 6300 sentences were drawn from a set of 2342
predesigned sentences, some selected to have particular dialect shibboleths, others to
maximize phonetic diphone coverage. Each sentence in the corpus was phonetically
hand-labeled, the sequence of phones was automatically aligned with the sentence
waveﬁle, and then the automatic phone boundaries were manually hand-corrected
(Seneff and Zue, 1988). The result is a time-aligned transcription: a transcrip-
time-aligned
transcription
tion in which each phone is associated with a start and end time in the waveform.
We showed a graphical example of a time-aligned transcription in Fig. 27.17 on
page 534.
The phoneset for TIMIT and for the Switchboard Transcription Project corpus
below, is a more detailed one than the minimal phonemic version of the ARPAbet.
In particular, these phonetic transcriptions make use of the various reduced and rare
phones mentioned in Fig. 27.1 and Fig. 27.2: the ﬂap [dx], glottal stop [q], reduced
vowels [ax], [ix], [axr], voiced allophone of [h] ([hv]), and separate phones for stop
closure ([dcl], [tcl], etc) and release ([d], [t], etc.). An example transcription is
shown in Fig. 27.28.
she
had
your
dark
suit
in
greasy
wash
water
all
year
sh iy
hv ae dcl
jh axr
dcl d aa r kcl
s ux q
en
gcl g r iy s ix
w aa sh
q w aa dx axr q
aa l
y ix axr
Figure 27.28
Phonetic transcription from the TIMIT corpus.
This transcription uses special features of
ARPAbet for narrow transcription, such as the palatalization of [d] in had, unreleased ﬁnal stop in dark, glot-
talization of ﬁnal [t] in suit to [q], and ﬂap of [t] in water. The TIMIT corpus also includes time-alignments for
each phone (not shown).

542
CHAPTER 27
•
PHONETICS
Where TIMIT is based on read speech, the more recent Switchboard Transcrip-
tion Project corpus is based on the Switchboard corpus of conversational speech.
This phonetically annotated portion consists of approximately 3.5 hours of sentences
extracted from various conversations (Greenberg et al., 1996). As with TIMIT, each
annotated utterance contains a time-aligned transcription. The Switchboard tran-
scripts are time aligned at the syllable level rather than at the phone level; thus, a
transcript consists of a sequence of syllables with the start and end time of each
syllables in the corresponding waveﬁle. Figure 27.29 shows an example from the
Switchboard Transcription Project for the phrase they’re kind of in between right
now.
0.470
0.640
0.720
0.900
0.953
1.279
1.410
1.630
dh er
k aa
n ax
v ih m
b ix
t w iy n
r ay
n aw
Figure 27.29
Phonetic transcription of the Switchboard phrase they’re kind of in between
right now. Note vowel reduction in they’re and of, coda deletion in kind and right, and re-
syllabiﬁcation (the [v] of of attaches as the onset of in). Time is given in number of seconds
from the beginning of sentence to the start of each syllable.
The Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically tran-
scribed corpus of spontaneous American speech, containing about 300,000 words
from 40 talkers. Phonetically transcribed corpora are also available for other lan-
guages, including the Kiel corpus of German and Mandarin corpora transcribed by
the Chinese Academy of Social Sciences (Li et al., 2000).
In addition to resources like dictionaries and corpora, there are many useful pho-
netic software tools. One of the most versatile is the Praat package (Boersma and
Weenink, 2005), which includes spectrum and spectrogram analysis, pitch extrac-
tion and formant analysis, and an embedded scripting language for automation.
27.7
Summary
This chapter has introduced many of the important concepts of phonetics and com-
putational phonetics.
• We can represent the pronunciation of words in terms of units called phones.
The standard system for representing phones is the International Phonetic
Alphabet or IPA. The most common computational system for transcription
of English is the ARPAbet, which conveniently uses ASCII symbols.
• Phones can be described by how they are produced articulatorily by the vocal
organs; consonants are deﬁned in terms of their place and manner of articu-
lation and voicing; vowels by their height, backness, and roundness.
• A phoneme is a generalization or abstraction over different phonetic realiza-
tions. Allophonic rules express how a phoneme is realized in a given context.
• Speech sounds can also be described acoustically. Sound waves can be de-
scribed in terms of frequency, amplitude, or their perceptual correlates, pitch
and loudness.
• The spectrum of a sound describes its different frequency components. While
some phonetic properties are recognizable from the waveform, both humans
and machines rely on spectral analysis for phone detection.
• A spectrogram is a plot of a spectrum over time. Vowels are described by
characteristic harmonics called formants.

BIBLIOGRAPHICAL AND HISTORICAL NOTES
543
• Pronunciation dictionaries are widely available and used for both speech
recognition and synthesis, including the CMU dictionary for English and CELEX
dictionaries for English, German, and Dutch. Other dictionaries are available
from the LDC.
• Phonetically transcribed corpora are a useful resource for building computa-
tional models of phone variation and reduction in natural speech.
Bibliographical and Historical Notes
The major insights of articulatory phonetics date to the linguists of 800–150 B.C.
India. They invented the concepts of place and manner of articulation, worked out
the glottal mechanism of voicing, and understood the concept of assimilation. Eu-
ropean science did not catch up with the Indian phoneticians until over 2000 years
later, in the late 19th century. The Greeks did have some rudimentary phonetic
knowledge; by the time of Plato’s Theaetetus and Cratylus, for example, they distin-
guished vowels from consonants, and stop consonants from continuants. The Stoics
developed the idea of the syllable and were aware of phonotactic constraints on pos-
sible words. An unknown Icelandic scholar of the 12th century exploited the concept
of the phoneme and proposed a phonemic writing system for Icelandic, including
diacritics for length and nasality. But his text remained unpublished until 1818 and
even then was largely unknown outside Scandinavia (Robins, 1967). The modern
era of phonetics is usually said to have begun with Sweet, who proposed what is
essentially the phoneme in his Handbook of Phonetics (1877). He also devised an
alphabet for transcription and distinguished between broad and narrow transcrip-
tion, proposing many ideas that were eventually incorporated into the IPA. Sweet
was considered the best practicing phonetician of his time; he made the ﬁrst scien-
tiﬁc recordings of languages for phonetic purposes and advanced the state of the art
of articulatory description. He was also infamously difﬁcult to get along with, a trait
that is well captured in Henry Higgins, the stage character that George Bernard Shaw
modeled after him. The phoneme was ﬁrst named by the Polish scholar Baudouin
de Courtenay, who published his theories in 1894.
Students with further interest in transcription and articulatory phonetics should
consult an introductory phonetics textbook such as Ladefoged (1993) or Clark and
Yallop (1995). Pullum and Ladusaw (1996) is a comprehensive guide to each of the
symbols and diacritics of the IPA. A good resource for details about reduction and
other phonetic processes in spoken English is Shockey (2003). Wells (1982) is the
deﬁnitive three-volume source on dialects of English.
Many of the classic insights in acoustic phonetics had been developed by the late
1950s or early 1960s; just a few highlights include techniques like the sound spectro-
graph (Koenig et al., 1946), theoretical insights like the working out of the source-
ﬁlter theory and other issues in the mapping between articulation and acoustics
((Fant, 1960), Stevens et al. 1953, Stevens and House 1955, Heinz and Stevens 1961,
Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and Barney,
1952), the understanding of the phonetic nature of stress and the use of duration and
intensity as cues (Fry, 1955), and a basic understanding of issues in phone percep-
tion (Miller and Nicely 1955,Liberman et al. 1952). Lehiste (1967) is a collection
of classic papers on acoustic phonetics. Many of the seminal papers of Gunnar Fant
have been collected in Fant (2004).
Excellent textbooks on acoustic phonetics include Johnson (2003) and Lade-
foged (1996). Coleman (2005) includes an introduction to computational processing

544
CHAPTER 27
•
PHONETICS
of acoustics as well as other speech processing issues, from a linguistic perspective.
Stevens (1998) lays out an inﬂuential theory of speech sound production. A wide
variety of books address speech from a signal processing and electrical engineering
perspective. The ones with the greatest coverage of computational phonetics issues
include Huang et al. (2001), O’Shaughnessy (2000), and Gold and Morgan (1999).
Excellent textbooks on digital signal processing are Lyons (2004) and Rabiner and
Schafer (1978).
There are a number of software packages for acoustic phonetic analysis. Proba-
bly the most widely-used one is Praat (Boersma and Weenink, 2005).
Many phonetics papers of computational interest are to be found in the Journal
of the Acoustical Society of America (JASA), Computer Speech and Language, and
Speech Communication.
Exercises
27.1 Find the mistakes in the ARPAbet transcriptions of the following words:
a. “three” [dh r i]
d. “study” [s t uh d i]
g. “slight” [s l iy t]
b. “sing” [s ih n g]
e. “though” [th ow]
c. “eyes” [ay s]
f. “planning” [p pl aa n ih ng]
27.2 Translate the pronunciations of the following color words from the IPA into
the ARPAbet (and make a note if you think you pronounce them differently
than this!):
a. [rEd]
e. [blæk]
i. [pjus]
b. [blu]
f. [waIt]
j. [toUp]
c. [grin]
g. ["OrIndZ]
d. ["jEloU]
h. ["pÇpl"]
27.3 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pro-
nunciations (each) of the words “tomato”, “potato”, and “either”. Transcribe
into the ARPAbet both pronunciations of each of these three words.
27.4 Transcribe the following words in the ARPAbet:
1. dark
2. suit
3. greasy
4. wash
5. water
27.5 Take a waveﬁle of your choice. Some examples are on the textbook website.
Download the Praat software, and use it to transcribe the waveﬁles at the word
level and into ARPAbet phones, using Praat to help you play pieces of each
waveﬁle and to look at the waveﬁle and the spectrogram.
27.6 Record yourself saying ﬁve of the English vowels: [aa], [eh], [ae], [iy], [uw].
Find F1 and F2 for each of your vowels.

CHAPTER
28
Speech Recognition and Syn-
thesis
Placeholder
545


547
Appendices

548
APPENDIX A
•
HIDDEN MARKOV MODELS
CHAPTER
A
Hidden Markov Models
Chapter 8 introduced the Hidden Markov Model and applied it to part of speech
tagging. Part of speech tagging is a fully-supervised learning task, because we have
a corpus of words labeled with the correct part-of-speech tag. But many applications
don’t have labeled data. So in this chapter, we introduce the full set of algorithms for
HMMs, including the key unsupervised learning algorithm for HMM, the Forward-
Backward algorithm. We’ll repeat some of the text from Chapter 8 for readers who
want the whole story laid out in a single chapter.
A.1
Markov Chains
The HMM is based on augmenting the Markov chain. A Markov chain is a model
Markov chain
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. These sets can be words, or
tags, or symbols representing anything, like the weather. A Markov chain makes a
very strong assumption that if we want to predict the future in the sequence, all that
matters is the current state. The states before the current state have no impact on the
future except via the current state. It’s as if to predict tomorrow’s weather you could
examine today’s weather but you weren’t allowed to look at yesterday’s weather.
WARM3
HOT1
COLD2
.8
.6
.1
.1
.3
.6
.1
.1
.3
charming
uniformly
are
.1
.4
.5
.5
.5
.2
.6
.2
(a)
(b)
Figure A.1
A Markov chain for weather (a) and one for words (b), showing states and
transitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a
probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.
More formally, consider a sequence of state variables q1,q2,...,qi. A Markov
model embodies the Markov assumption on the probabilities of this sequence: that
Markov
assumption
when predicting the future, the past doesn’t matter, only the present.
Markov Assumption:
P(qi = a|q1...qi−1) = P(qi = a|qi−1)
(A.1)
Figure A.1a shows a Markov chain for assigning a probability to a sequence of
weather events, for which the vocabulary consists of HOT, COLD, and WARM. The
states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. The transitions are probabilities: the values of arcs leaving a given

A.2
•
THE HIDDEN MARKOV MODEL
549
state must sum to 1. Figure A.1b shows a Markov chain for assigning a probabil-
ity to a sequence of words w1...wn. This Markov chain should be familiar; in fact,
it represents a bigram language model, with each edge expressing the probability
p(wi|w j)! Given the two models in Fig. A.1, we can assign a probability to any
sequence from our vocabulary.
Formally, a Markov chain is speciﬁed by the following components:
Q = q1q2 ...qN
a set of N states
A = a11a12 ...an1 ...ann
a transition probability matrix A, each aij represent-
ing the probability of moving from state i to state j, s.t.
Pn
j=1 aij = 1 ∀i
π = π1,π2,...,πN
an initial probability distribution over states. πi is the
probability that the Markov chain will start in state i.
Some states j may have π j = 0, meaning that they cannot
be initial states. Also, Pn
i=1 πi = 1
Before you go on, use the sample probabilities in Fig. A.1a (with π = [.1,.7.,2])
to compute the probability of each of the following sequences:
(A.2) hot hot hot hot
(A.3) cold hot cold hot
What does the difference in these probabilities tell you about a real-world weather
fact encoded in Fig. A.1a?
A.2
The Hidden Markov Model
A Markov chain is useful when we need to compute a probability for a sequence
of observable events. In many cases, however, the events we are interested in are
hidden: we don’t observe them directly. For example we don’t normally observe
hidden
part-of-speech tags in a text. Rather, we see words, and must infer the tags from the
word sequence. We call the tags hidden because they are not observed.
A hidden Markov model (HMM) allows us to talk about both observed events
Hidden
Markov model
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. An HMM is speciﬁed by
the following components:
Q = q1q2 ...qN
a set of N states
A = a11 ...ai j ...aNN
a transition probability matrix A, each aij representing the probability
of moving from state i to state j, s.t. PN
j=1 aij = 1 ∀i
O = o1o2 ...oT
a sequence of T observations, each one drawn from a vocabulary V =
v1,v2,...,vV
B = bi(ot)
a sequence of observation likelihoods, also called emission probabili-
ties, each expressing the probability of an observation ot being generated
from a state i
π = π1,π2,...,πN
an initial probability distribution over states. πi is the probability that
the Markov chain will start in state i. Some states j may have πj = 0,
meaning that they cannot be initial states. Also, Pn
i=1 πi = 1

550
APPENDIX A
•
HIDDEN MARKOV MODELS
A ﬁrst-order hidden Markov model instantiates two simplifying assumptions.
First, as with a ﬁrst-order Markov chain, the probability of a particular state depends
only on the previous state:
Markov Assumption:
P(qi|q1...qi−1) = P(qi|qi−1)
(A.4)
Second, the probability of an output observation oi depends only on the state that
produced the observation qi and not on any other states or any other observations:
Output Independence: P(oi|q1 ...qi,...,qT,o1,...,oi,...,oT) = P(oi|qi)
(A.5)
To exemplify these models, we’ll use a task invented by Jason Eisner (2002).
Imagine that you are a climatologist in the year 2799 studying the history of global
warming. You cannot ﬁnd any records of the weather in Baltimore, Maryland, for
the summer of 2020, but you do ﬁnd Jason Eisner’s diary, which lists how many ice
creams Jason ate every day that summer. Our goal is to use these observations to
estimate the temperature every day. We’ll simplify this weather task by assuming
there are only two kinds of days: cold (C) and hot (H). So the Eisner task is as
follows:
Given a sequence of observations O (each an integer representing the
number of ice creams eaten on a given day) ﬁnd the ‘hidden’ sequence
Q of weather states (H or C) which caused Jason to eat the ice cream.
Figure A.2 shows a sample HMM for the ice cream task. The two hidden states
(H and C) correspond to hot and cold weather, and the observations (drawn from the
alphabet O = {1,2,3}) correspond to the number of ice creams eaten by Jason on a
given day.
π = [.8,.2]
COLD2
HOT1
B2
P(1 | COLD)          .5
P(2 | COLD)    =    .4
P(3 | COLD)          .1
.5
.6
.5
.4
P(1 | HOT)          .2
P(2 | HOT)    =    .4
P(3 | HOT)          .4
B1
Figure A.2
A hidden Markov model for relating numbers of ice creams eaten by Jason (the
observations) to the weather (H or C, the hidden variables).
An inﬂuential tutorial by Rabiner (1989), based on tutorials by Jack Ferguson in
the 1960s, introduced the idea that hidden Markov models should be characterized
by three fundamental problems:
Problem 1 (Likelihood):
Given an HMM λ = (A,B) and an observation se-
quence O, determine the likelihood P(O|λ).
Problem 2 (Decoding):
Given an observation sequence O and an HMM λ =
(A,B), discover the best hidden state sequence Q.
Problem 3 (Learning):
Given an observation sequence O and the set of states
in the HMM, learn the HMM parameters A and B.
We already saw an example of Problem 2 in Chapter 8. In the next two sections
we introduce the Forward and Forward-Backward algorithms to solve Problems 1
and 3 and give more information on Problem 2

A.3
•
LIKELIHOOD COMPUTATION: THE FORWARD ALGORITHM
551
A.3
Likelihood Computation: The Forward Algorithm
Our ﬁrst problem is to compute the likelihood of a particular observation sequence.
For example, given the ice-cream eating HMM in Fig. A.2, what is the probability
of the sequence 3 1 3? More formally:
Computing Likelihood: Given an HMM λ = (A,B) and an observa-
tion sequence O, determine the likelihood P(O|λ).
For a Markov chain, where the surface observations are the same as the hidden
events, we could compute the probability of 3 1 3 just by following the states labeled
3 1 3 and multiplying the probabilities along the arcs. For a hidden Markov model,
things are not so simple. We want to determine the probability of an ice-cream
observation sequence like 3 1 3, but we don’t know what the hidden state sequence
is!
Let’s start with a slightly simpler situation. Suppose we already knew the weather
and wanted to predict how much ice cream Jason would eat. This is a useful part
of many HMM tasks. For a given hidden state sequence (e.g., hot hot cold), we can
easily compute the output likelihood of 3 1 3.
Let’s see how. First, recall that for hidden Markov models, each hidden state
produces only a single observation. Thus, the sequence of hidden states and the
sequence of observations have the same length. 1
Given this one-to-one mapping and the Markov assumptions expressed in Eq. A.4,
for a particular hidden state sequence Q = q0,q1,q2,...,qT and an observation se-
quence O = o1,o2,...,oT, the likelihood of the observation sequence is
P(O|Q) =
TY
i=1
P(oi|qi)
(A.6)
The computation of the forward probability for our ice-cream observation 3 1 3 from
one possible hidden state sequence hot hot cold is shown in Eq. A.7. Figure A.3
shows a graphic representation of this computation.
P(3 1 3|hot hot cold) = P(3|hot)×P(1|hot)×P(3|cold)
(A.7)
cold
hot
3
.4
hot
1
3
.2
.1
Figure A.3
The computation of the observation likelihood for the ice-cream events 3 1 3
given the hidden state sequence hot hot cold.
But of course, we don’t actually know what the hidden state (weather) sequence
was. We’ll need to compute the probability of ice-cream events 3 1 3 instead by
1
In a variant of HMMs called segmental HMMs (in speech recognition) or semi-HMMs (in text pro-
cessing) this one-to-one mapping between the length of the hidden state sequence and the length of the
observation sequence does not hold.

552
APPENDIX A
•
HIDDEN MARKOV MODELS
summing over all possible weather sequences, weighted by their probability. First,
let’s compute the joint probability of being in a particular weather sequence Q and
generating a particular sequence O of ice-cream events. In general, this is
P(O,Q) = P(O|Q)×P(Q) =
TY
i=1
P(oi|qi)×
TY
i=1
P(qi|qi−1)
(A.8)
The computation of the joint probability of our ice-cream observation 3 1 3 and one
possible hidden state sequence hot hot cold is shown in Eq. A.9. Figure A.4 shows
a graphic representation of this computation.
P(3 1 3,hot hot cold) = P(hot|start)×P(hot|hot)×P(cold|hot)
×P(3|hot)×P(1|hot)×P(3|cold)
(A.9)
cold
hot
3
.4
hot
.6
1
3
.4
.2
.1
Figure A.4
The computation of the joint probability of the ice-cream events 3 1 3 and the
hidden state sequence hot hot cold.
Now that we know how to compute the joint probability of the observations
with a particular hidden state sequence, we can compute the total probability of the
observations just by summing over all possible hidden state sequences:
P(O) =
X
Q
P(O,Q) =
X
Q
P(O|Q)P(Q)
(A.10)
For our particular case, we would sum over the eight 3-event sequences cold cold
cold, cold cold hot, that is,
P(3 1 3) = P(3 1 3,cold cold cold)+P(3 1 3,cold cold hot)+P(3 1 3,hot hot cold)+...
For an HMM with N hidden states and an observation sequence of T observa-
tions, there are NT possible hidden sequences. For real tasks, where N and T are
both large, NT is a very large number, so we cannot compute the total observation
likelihood by computing a separate observation likelihood for each hidden state se-
quence and then summing them.
Instead of using such an extremely exponential algorithm, we use an efﬁcient
O(N2T) algorithm called the forward algorithm. The forward algorithm is a kind
forward
algorithm
of dynamic programming algorithm, that is, an algorithm that uses a table to store
intermediate values as it builds up the probability of the observation sequence. The
forward algorithm computes the observation probability by summing over the prob-
abilities of all possible hidden state paths that could generate the observation se-
quence, but it does so efﬁciently by implicitly folding each of these paths into a
single forward trellis.
Figure A.5 shows an example of the forward trellis for computing the likelihood
of 3 1 3 given the hidden state sequence hot hot cold.

A.3
•
LIKELIHOOD COMPUTATION: THE FORWARD ALGORITHM
553
π
H
C
H
C
H
C
P(C|start) * P(3|C)
.2 * .1
P(H|H) * P(1|H)
.6 * .2
P(C|C) * P(1|C)
.5 * .5
P(C|H) * P(1|C)
.4 * .5
P(H|C) * P(1|H)
.5 * .2
P(H|start)*P(3|H)
.8 * .4
α1(2)=.32
α1(1) = .02
α2(2)= .32*.12 + .02*.1 = .0404
α2(1) = .32*.2 + .02*.25 = .069
t
C
H
q2
q1
o1
3
o2
o3
1
3
Figure A.5
The forward trellis for computing the total observation likelihood for the ice-cream events 3 1 3.
Hidden states are in circles, observations in squares. The ﬁgure shows the computation of αt(j) for two states at
two time steps. The computation in each cell follows Eq. A.12: αt(j) = PN
i=1 αt−1(i)ai jbj(ot). The resulting
probability expressed in each cell is Eq. A.11: αt(j) = P(o1,o2 ...ot,qt = j|λ).
Each cell of the forward algorithm trellis αt(j) represents the probability of be-
ing in state j after seeing the ﬁrst t observations, given the automaton λ. The value
of each cell αt(j) is computed by summing over the probabilities of every path that
could lead us to this cell. Formally, each cell expresses the following probability:
αt(j) = P(o1,o2 ...ot,qt = j|λ)
(A.11)
Here, qt = j means “the tth state in the sequence of states is state j”. We compute
this probability αt(j) by summing over the extensions of all the paths that lead to
the current cell. For a given state qj at time t, the value αt(j) is computed as
αt(j) =
N
X
i=1
αt−1(i)aijbj(ot)
(A.12)
The three factors that are multiplied in Eq. A.12 in extending the previous paths
to compute the forward probability at time t are
αt−1(i)
the previous forward path probability from the previous time step
ai j
the transition probability from previous state qi to current state qj
bj(ot)
the state observation likelihood of the observation symbol ot given
the current state j
Consider the computation in Fig. A.5 of α2(2), the forward probability of being
at time step 2 in state 2 having generated the partial observation 3 1. We compute by
extending the α probabilities from time step 1, via two paths, each extension con-
sisting of the three factors above: α1(1)×P(H|C)×P(1|H) and α1(2)×P(H|H)×
P(1|H).
Figure A.6 shows another visualization of this induction step for computing the
value in one new cell of the trellis.
We give two formal deﬁnitions of the forward algorithm: the pseudocode in
Fig. A.7 and a statement of the deﬁnitional recursion here.

554
APPENDIX A
•
HIDDEN MARKOV MODELS
ot-1
ot
a1j
a2j
aNj
a3j
bj(ot)
αt(j)= Σi αt-1(i) aij bj(ot) 
q1
q2
q3
qN
q1
qj
q2
q1
q2
ot+1
ot-2
q1
q2
q3
q3
qN
qN
αt-1(N)
αt-1(3)
αt-1(2)
αt-1(1)
αt-2(N)
αt-2(3)
αt-2(2)
αt-2(1)
Figure A.6
Visualizing the computation of a single element αt(i) in the trellis by summing
all the previous values αt−1, weighted by their transition probabilities a, and multiplying by
the observation probability bi(ot). For many applications of HMMs, many of the transition
probabilities are 0, so not all previous states will contribute to the forward probability of the
current state. Hidden states are in circles, observations in squares. Shaded nodes are included
in the probability computation for αt(i).
function FORWARD(observations of len T, state-graph of len N) returns forward-prob
create a probability matrix forward[N,T]
for each state s from 1 to N do
; initialization step
forward[s,1]←πs ∗bs(o1)
for each time step t from 2 to T do
; recursion step
for each state s from 1 to N do
forward[s,t]←
N
X
s′=1
forward[s′,t −1] ∗as′,s ∗bs(ot)
forwardprob←
N
X
s=1
forward[s,T]
; termination step
return forwardprob
Figure A.7
The forward algorithm, where forward[s,t] represents αt(s).
1. Initialization:
α1(j) = πjbj(o1) 1 ≤j ≤N
2. Recursion:
αt(j) =
N
X
i=1
αt−1(i)aijbj(ot); 1 ≤j ≤N,1 < t ≤T
3. Termination:
P(O|λ) =
N
X
i=1
αT(i)

A.4
•
DECODING: THE VITERBI ALGORITHM
555
A.4
Decoding: The Viterbi Algorithm
For any model, such as an HMM, that contains hidden variables, the task of deter-
mining which sequence of variables is the underlying source of some sequence of
observations is called the decoding task. In the ice-cream domain, given a sequence
decoding
of ice-cream observations 3 1 3 and an HMM, the task of the decoder is to ﬁnd the
best hidden weather sequence (H H H). More formally,
Decoding: Given as input an HMM λ = (A,B) and a sequence of ob-
servations O = o1,o2,...,oT, ﬁnd the most probable sequence of states
Q = q1q2q3 ...qT.
We might propose to ﬁnd the best sequence as follows: For each possible hid-
den state sequence (HHH, HHC, HCH, etc.), we could run the forward algorithm
and compute the likelihood of the observation sequence given that hidden state se-
quence. Then we could choose the hidden state sequence with the maximum obser-
vation likelihood. It should be clear from the previous section that we cannot do this
because there are an exponentially large number of state sequences.
Instead, the most common decoding algorithms for HMMs is the Viterbi algo-
rithm. Like the forward algorithm, Viterbi is a kind of dynamic programming
Viterbi
algorithm
that makes uses of a dynamic programming trellis. Viterbi also strongly resembles
another dynamic programming variant, the minimum edit distance algorithm of
Chapter 2.
π
H
C
H
C
H
C
P(C|start) * P(3|C)
.2 * .1
P(H|H) * P(1|H)
.6 * .2
P(C|C) * P(1|C)
.5 * .5
P(C|H) * P(1|C)
.4 * .5
P(H|C) * P(1|H)
.5 * .2
P(H|start)*P(3|H)
.8 * .4
v1(2)=.32
v1(1) = .02
v2(2)= max(.32*.12, .02*.10) = .038
v2(1) = max(.32*.20, .02*.25) = .064
t
C
H
q2
q1
o1
o2
o3
3
1
3
Figure A.8
The Viterbi trellis for computing the best path through the hidden state space for the ice-cream
eating events 3 1 3. Hidden states are in circles, observations in squares. White (unﬁlled) circles indicate illegal
transitions. The ﬁgure shows the computation of vt(j) for two states at two time steps. The computation in each
cell follows Eq. A.14: vt(j) = max1≤i≤N−1 vt−1(i) ai j bj(ot). The resulting probability expressed in each cell
is Eq. A.13: vt(j) = P(q0,q1,...,qt−1,o1,o2,...,ot,qt = j|λ).
Figure A.8 shows an example of the Viterbi trellis for computing the best hidden
state sequence for the observation sequence 3 1 3. The idea is to process the ob-
servation sequence left to right, ﬁlling out the trellis. Each cell of the trellis, vt(j),
represents the probability that the HMM is in state j after seeing the ﬁrst t obser-
vations and passing through the most probable state sequence q1,...,qt−1, given the

556
APPENDIX A
•
HIDDEN MARKOV MODELS
automaton λ. The value of each cell vt(j) is computed by recursively taking the
most probable path that could lead us to this cell. Formally, each cell expresses the
probability
vt(j) =
max
q1,...,qt−1 P(q1...qt−1,o1,o2 ...ot,qt = j|λ)
(A.13)
Note that we represent the most probable path by taking the maximum over all
possible previous state sequences
max
q1,...,qt−1. Like other dynamic programming algo-
rithms, Viterbi ﬁlls each cell recursively. Given that we had already computed the
probability of being in every state at time t −1, we compute the Viterbi probability
by taking the most probable of the extensions of the paths that lead to the current
cell. For a given state qj at time t, the value vt(j) is computed as
vt(j) =
N
max
i=1 vt−1(i) aij bj(ot)
(A.14)
The three factors that are multiplied in Eq. A.14 for extending the previous paths to
compute the Viterbi probability at time t are
vt−1(i)
the previous Viterbi path probability from the previous time step
ai j
the transition probability from previous state qi to current state qj
bj(ot)
the state observation likelihood of the observation symbol ot given
the current state j
function VITERBI(observations of len T,state-graph of len N) returns best-path, path-prob
create a path probability matrix viterbi[N,T]
for each state s from 1 to N do
; initialization step
viterbi[s,1]←πs ∗bs(o1)
backpointer[s,1]←0
for each time step t from 2 to T do
; recursion step
for each state s from 1 to N do
viterbi[s,t]←
N
max
s′=1 viterbi[s′,t −1] ∗as′,s ∗bs(ot)
backpointer[s,t]←
N
argmax
s′=1
viterbi[s′,t −1] ∗as′,s ∗bs(ot)
bestpathprob←
N
max
s=1
viterbi[s,T]
; termination step
bestpathpointer←
N
argmax
s=1
viterbi[s,T]
; termination step
bestpath←the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob
Figure A.9
Viterbi algorithm for ﬁnding optimal sequence of hidden states. Given an observation sequence
and an HMM λ = (A,B), the algorithm returns the state path through the HMM that assigns maximum likelihood
to the observation sequence.
Figure A.9 shows pseudocode for the Viterbi algorithm. Note that the Viterbi
algorithm is identical to the forward algorithm except that it takes the max over the
previous path probabilities whereas the forward algorithm takes the sum. Note also
that the Viterbi algorithm has one component that the forward algorithm doesn’t

A.5
•
HMM TRAINING: THE FORWARD-BACKWARD ALGORITHM
557
have: backpointers. The reason is that while the forward algorithm needs to pro-
duce an observation likelihood, the Viterbi algorithm must produce a probability and
also the most likely state sequence. We compute this best state sequence by keeping
track of the path of hidden states that led to each state, as suggested in Fig. A.10, and
then at the end backtracing the best path to the beginning (the Viterbi backtrace).
Viterbi
backtrace
π
H
C
H
C
H
C
P(C|start) * P(3|C)
.2 * .1
P(H|H) * P(1|H)
.6 * .2
P(C|C) * P(1|C)
.5 * .5
P(C|H) * P(1|C)
.4 * .5
P(H|C) * P(1|H)
.5 * .2
P(H|start)*P(3|H)
.8 * .4
v1(2)=.32
v1(1) = .02
v2(2)= max(.32*.12, .02*.10) = .038
v2(1) = max(.32*.20, .02*.25) = .064
t
C
H
q2
q1
o1
o2
o3
3
1
3
Figure A.10
The Viterbi backtrace. As we extend each path to a new state account for the next observation,
we keep a backpointer (shown with broken lines) to the best path that led us to this state.
Finally, we can give a formal deﬁnition of the Viterbi recursion as follows:
1. Initialization:
v1(j) = π jb j(o1)
1 ≤j ≤N
bt1(j) = 0
1 ≤j ≤N
2. Recursion
vt(j) =
N
max
i=1 vt−1(i)aij b j(ot); 1 ≤j ≤N,1 < t ≤T
btt(j) =
N
argmax
i=1
vt−1(i)aij bj(ot); 1 ≤j ≤N,1 < t ≤T
3. Termination:
The best score:
P∗=
N
max
i=1 vT(i)
The start of backtrace:
qT∗=
N
argmax
i=1
vT(i)
A.5
HMM Training: The Forward-Backward Algorithm
We turn to the third problem for HMMs: learning the parameters of an HMM, that
is, the A and B matrices. Formally,
Learning: Given an observation sequence O and the set of possible
states in the HMM, learn the HMM parameters A and B.

558
APPENDIX A
•
HIDDEN MARKOV MODELS
The input to such a learning algorithm would be an unlabeled sequence of ob-
servations O and a vocabulary of potential hidden states Q. Thus, for the ice cream
task, we would start with a sequence of observations O = {1,3,2,...,} and the set of
hidden states H and C.
The standard algorithm for HMM training is the forward-backward, or Baum-
Forward-
backward
Welch algorithm (Baum, 1972), a special case of the Expectation-Maximization
Baum-Welch
or EM algorithm (Dempster et al., 1977). The algorithm will let us train both the
EM
transition probabilities A and the emission probabilities B of the HMM. EM is an
iterative algorithm, computing an initial estimate for the probabilities, then using
those estimates to computing a better estimate, and so on, iteratively improving the
probabilities that it learns.
Let us begin by considering the much simpler case of training a fully visible
Markov model, we’re know both the temperature and the ice cream count for every
day. That is, imagine we see the following set of input observations and magically
knew the aligned hidden state sequences:
3
3
2
1
1
2
1
2
3
hot
hot cold
cold cold cold
cold hot hot
This would easily allow us to compute the HMM parameters just by maximum
likelihood estimation from the training data. First, we can compute π from the count
of the 3 initial hidden states:
πh = 1/3
πc = 2/3
Next we can directly compute the A matrix from the transitions, ignoring the ﬁnal
hidden states:
p(hot|hot) = 2/3
p(cold|hot) = 1/3
p(cold|cold) = 2/3
p(hot|cold) = 1/3
and the B matrix:
P(1|hot) = 0/4 = 0
p(1|cold) = 3/5 = .6
P(2|hot) = 1/4 = .25
p(2|cold = 2/5 = .4
P(3|hot) = 3/4 = .75
p(3|cold) = 0
For a real HMM, we cannot compute these counts directly from an observation
sequence since we don’t know which path of states was taken through the machine
for a given input. For example, suppose I didn’t tell you the temperature on day 2,
and you had to guess it, but you (magically) had the above probabilities, and the
temperatures on the other days. You could do some Bayesian arithmetic with all the
other probabilities to get estimates of the likely temperature on that missing day, and
use those to get expected counts for the temperatures for day 2.
But the real problem is even harder: we don’t know the counts of being in any
of the hidden states!! The Baum-Welch algorithm solves this by iteratively esti-
mating the counts. We will start with an estimate for the transition and observation
probabilities and then use these estimated probabilities to derive better and better
probabilities. And we’re going to do this by computing the forward probability for
an observation and then dividing that probability mass among all the different paths
that contributed to this forward probability.
To understand the algorithm, we need to deﬁne a useful probability related to the
forward probability and called the backward probability. The backward probabil-
backward
probability

A.5
•
HMM TRAINING: THE FORWARD-BACKWARD ALGORITHM
559
ity β is the probability of seeing the observations from time t + 1 to the end, given
that we are in state i at time t (and given the automaton λ):
βt(i) = P(ot+1,ot+2 ...oT|qt = i,λ)
(A.15)
It is computed inductively in a similar manner to the forward algorithm.
1. Initialization:
βT(i) = 1, 1 ≤i ≤N
2. Recursion
βt(i) =
N
X
j=1
aij bj(ot+1) βt+1(j), 1 ≤i ≤N,1 ≤t < T
3. Termination:
P(O|λ) =
N
X
j=1
π j bj(o1) β1(j)
Figure A.11 illustrates the backward induction step.
ot+1
ot
ai1
ai2
aiN
ai3
b1(ot+1)
βt(i)= Σj βt+1(j) aij  bj(ot+1) 
q1
q2
q3
qN
q1
qi
q2
q1
q2
ot-1
q3
qN
βt+1(N)
βt+1(3)
βt+1(2)
βt+1(1)
b2(ot+1)
b3(ot+1)
bN(ot+1)
Figure A.11
The computation of βt(i) by summing all the successive values βt+1(j)
weighted by their transition probabilities ai j and their observation probabilities b j(ot+1). Start
and end states not shown.
We are now ready to see how the forward and backward probabilities can help
compute the transition probability aij and observation probability bi(ot) from an ob-
servation sequence, even though the actual path taken through the model is hidden.
Let’s begin by seeing how to estimate ˆaij by a variant of simple maximum like-
lihood estimation:
ˆai j = expected number of transitions from state i to state j
expected number of transitions from state i
(A.16)
How do we compute the numerator? Here’s the intuition. Assume we had some
estimate of the probability that a given transition i →j was taken at a particular
point in time t in the observation sequence. If we knew this probability for each

560
APPENDIX A
•
HIDDEN MARKOV MODELS
particular time t, we could sum over all times t to estimate the total count for the
transition i →j.
More formally, let’s deﬁne the probability ξt as the probability of being in state
i at time t and state j at time t +1, given the observation sequence and of course the
model:
ξt(i, j) = P(qt = i,qt+1 = j|O,λ)
(A.17)
To compute ξt, we ﬁrst compute a probability which is similar to ξt, but differs in
including the probability of the observation; note the different conditioning of O
from Eq. A.17:
not-quite-ξt(i, j) = P(qt = i,qt+1 = j,O|λ)
(A.18)
ot+2
ot+1
αt(i)
ot-1
ot
aijbj(ot+1) 
si
sj
βt+1(j)
Figure A.12
Computation of the joint probability of being in state i at time t and state j at
time t + 1. The ﬁgure shows the various probabilities that need to be combined to produce
P(qt = i,qt+1 = j,O|λ): the α and β probabilities, the transition probability ai j and the
observation probability bj(ot+1). After Rabiner (1989) which is c⃝1989 IEEE.
Figure A.12 shows the various probabilities that go into computing not-quite-ξt:
the transition probability for the arc in question, the α probability before the arc, the
β probability after the arc, and the observation probability for the symbol just after
the arc. These four are multiplied together to produce not-quite-ξt as follows:
not-quite-ξt(i, j) = αt(i)aijbj(ot+1)βt+1(j)
(A.19)
To compute ξt from not-quite-ξt, we follow the laws of probability and divide by
P(O|λ), since
P(X|Y,Z) = P(X,Y|Z)
P(Y|Z)
(A.20)
The probability of the observation given the model is simply the forward proba-
bility of the whole utterance (or alternatively, the backward probability of the whole
utterance):
P(O|λ) =
N
X
j=1
αt(j)βt(j)
(A.21)

A.5
•
HMM TRAINING: THE FORWARD-BACKWARD ALGORITHM
561
So, the ﬁnal equation for ξt is
ξt(i, j) = αt(i)aijbj(ot+1)βt+1(j)
PN
j=1 αt(j)βt(j)
(A.22)
The expected number of transitions from state i to state j is then the sum over all
t of ξ. For our estimate of aij in Eq. A.16, we just need one more thing: the total
expected number of transitions from state i. We can get this by summing over all
transitions out of state i. Here’s the ﬁnal formula for ˆaij:
ˆaij =
PT−1
t=1 ξt(i, j)
PT−1
t=1
PN
k=1 ξt(i,k)
(A.23)
We also need a formula for recomputing the observation probability. This is the
probability of a given symbol vk from the observation vocabulary V, given a state j:
ˆb j(vk). We will do this by trying to compute
ˆbj(vk) = expected number of times in state j and observing symbol vk
expected number of times in state j
(A.24)
For this, we will need to know the probability of being in state j at time t, which
we will call γt(j):
γt(j) = P(qt = j|O,λ)
(A.25)
Once again, we will compute this by including the observation sequence in the
probability:
γt(j) = P(qt = j,O|λ)
P(O|λ)
(A.26)
ot+1
αt(j)
ot-1
ot
sj
βt(j)
Figure A.13
The computation of γt(j), the probability of being in state j at time t. Note
that γ is really a degenerate case of ξ and hence this ﬁgure is like a version of Fig. A.12 with
state i collapsed with state j. After Rabiner (1989) which is c⃝1989 IEEE.
As Fig. A.13 shows, the numerator of Eq. A.26 is just the product of the forward
probability and the backward probability:
γt(j) = αt(j)βt(j)
P(O|λ)
(A.27)

562
APPENDIX A
•
HIDDEN MARKOV MODELS
We are ready to compute b. For the numerator, we sum γt(j) for all time steps
t in which the observation ot is the symbol vk that we are interested in. For the
denominator, we sum γt(j) over all time steps t. The result is the percentage of the
times that we were in state j and saw symbol vk (the notation PT
t=1 s.t.Ot=vk means
“sum over all t for which the observation at time t was vk”):
ˆbj(vk) =
PT
t=1 s.t.Ot=vk γt(j)
PT
t=1 γt(j)
(A.28)
We now have ways in Eq. A.23 and Eq. A.28 to re-estimate the transition A and ob-
servation B probabilities from an observation sequence O, assuming that we already
have a previous estimate of A and B.
These re-estimations form the core of the iterative forward-backward algorithm.
The forward-backward algorithm (Fig. A.14) starts with some initial estimate of the
HMM parameters λ = (A,B). We then iteratively run two steps. Like other cases of
the EM (expectation-maximization) algorithm, the forward-backward algorithm has
two steps: the expectation step, or E-step, and the maximization step, or M-step.
E-step
M-step
In the E-step, we compute the expected state occupancy count γ and the expected
state transition count ξ from the earlier A and B probabilities. In the M-step, we use
γ and ξ to recompute new A and B probabilities.
function FORWARD-BACKWARD(observations of len T, output vocabulary V, hidden
state set Q) returns HMM=(A,B)
initialize A and B
iterate until convergence
E-step
γt(j) = αt(j)βt(j)
αT (qF)
∀t and j
ξt(i, j) = αt(i)aijb j(ot+1)βt+1(j)
αT (qF)
∀t, i, and j
M-step
ˆaij =
T−1
X
t=1
ξt(i, j)
T−1
X
t=1
N
X
k=1
ξt(i,k)
ˆbj(vk) =
T
X
t=1s.t. Ot=vk
γt(j)
T
X
t=1
γt(j)
return A, B
Figure A.14
The forward-backward algorithm.
Although in principle the forward-backward algorithm can do completely unsu-
pervised learning of the A and B parameters, in practice the initial conditions are
very important. For this reason the algorithm is often given extra information. For
example, for HMM-based speech recognition, the HMM structure is often set by
hand, and only the emission (B) and (non-zero) A transition probabilities are trained
from a set of observation sequences O.

A.6
•
SUMMARY
563
A.6
Summary
This chapter introduced the hidden Markov model for probabilistic sequence clas-
siﬁcation.
• Hidden Markov models (HMMs) are a way of relating a sequence of obser-
vations to a sequence of hidden classes or hidden states that explain the
observations.
• The process of discovering the sequence of hidden states, given the sequence
of observations, is known as decoding or inference. The Viterbi algorithm is
commonly used for decoding.
• The parameters of an HMM are the A transition probability matrix and the B
observation likelihood matrix. Both can be trained with the Baum-Welch or
forward-backward algorithm.
Bibliographical and Historical Notes
As we discussed in Chapter 8, Markov chains were ﬁrst used by Markov (1913)
(translation Markov 2006), to predict whether an upcoming letter in Pushkin’s Eu-
gene Onegin would be a vowel or a consonant. The hidden Markov model was de-
veloped by Baum and colleagues at the Institute for Defense Analyses in Princeton
(Baum and Petrie 1966, Baum and Eagon 1967).
The Viterbi algorithm was ﬁrst applied to speech and language processing in the
context of speech recognition by Vintsyuk (1968) but has what Kruskal (1983) calls
a “remarkable history of multiple independent discovery and publication”. Kruskal
and others give at least the following independently-discovered variants of the algo-
rithm published in four separate ﬁelds:
Citation
Field
Viterbi (1967)
information theory
Vintsyuk (1968)
speech processing
Needleman and Wunsch (1970) molecular biology
Sakoe and Chiba (1971)
speech processing
Sankoff (1972)
molecular biology
Reichert et al. (1973)
molecular biology
Wagner and Fischer (1974)
computer science
The use of the term Viterbi is now standard for the application of dynamic pro-
gramming to any kind of probabilistic maximization problem in speech and language
processing. For non-probabilistic problems (such as for minimum edit distance), the
plain term dynamic programming is often used. Forney, Jr. (1973) wrote an early
survey paper that explores the origin of the Viterbi algorithm in the context of infor-
mation and communications theory.
Our presentation of the idea that hidden Markov models should be characterized
by three fundamental problems was modeled after an inﬂuential tutorial by Rabiner
(1989), which was itself based on tutorials by Jack Ferguson of IDA in the 1960s.
Jelinek (1997) and Rabiner and Juang (1993) give very complete descriptions of the
forward-backward algorithm as applied to the speech recognition problem. Jelinek
(1997) also shows the relationship between forward-backward and EM.


Bibliography
Abadi, M., Agarwal, A., Barham, P.,
Brevdo, E., Chen, Z., Citro, C., Cor-
rado, G. S., Davis, A., Dean, J.,
Devin, M., Ghemawat, S., Goodfel-
low, I., Harp, A., Irving, G., Isard,
M., Jia, Y., Jozefowicz, R., Kaiser,
L., Kudlur, M., Levenberg, J., Man´e,
D., Monga, R., Moore, S., Murray,
D., Olah, C., Schuster, M., Shlens, J.,
Steiner, B., Sutskever, I., Talwar, K.,
Tucker, P., Vanhoucke, V., Vasudevan,
V., Vi´egas, F., Vinyals, O., Warden, P.,
Wattenberg, M., Wicke, M., Yu, Y.,
and Zheng, X. (2015).
TensorFlow:
Large-scale machine learning on het-
erogeneous systems.. Software avail-
able from tensorﬂow.org.
Abney, S. P. (1991). Parsing by chunks.
In Berwick, R. C., Abney, S. P.,
and Tenny, C. (Eds.), Principle-Based
Parsing: Computation and Psycholin-
guistics, 257–278. Kluwer.
Abney, S. P., Schapire, R. E., and
Singer, Y. (1999).
Boosting applied
to tagging and PP attachment.
In
EMNLP/VLC-99, 38–45.
Agarwal,
O.,
Subramanian,
S.,
Nenkova, A., and Roth, D. (2019).
Evaluation of named entity corefer-
ence. In Workshop on Computational
Models of Reference, Anaphora and
Coreference, 1–7.
Aggarwal, C. C. and Zhai, C. (2012).
A survey of text classiﬁcation algo-
rithms. In Aggarwal, C. C. and Zhai,
C. (Eds.), Mining text data, 163–222.
Springer.
Agichtein, E. and Gravano, L. (2000).
Snowball: Extracting relations from
large plain-text collections.
In Pro-
ceedings of the 5th ACM International
Conference on Digital Libraries.
Agirre, E. and de Lacalle, O. L. (2003).
Clustering WordNet word senses. In
RANLP 2003.
Agirre, E., Banea, C., Cardie, C., Cer,
D., Diab, M., Gonzalez-Agirre, A.,
Guo, W., Lopez-Gazpio, I., Maritx-
alar, M., Mihalcea, R., Rigau, G.,
Uria, L., and Wiebe, J. (2015). 2015
SemEval-2015 Task 2: Semantic Tex-
tual Similarity, English, Spanish and
Pilot on Interpretability. In SemEval-
15, 252–263.
Agirre, E., Diab, M., Cer, D., and
Gonzalez-Agirre, A. (2012). Semeval-
2012 task 6: A pilot on semantic tex-
tual similarity. In SemEval-12, 385–
393.
Agirre, E. and Edmonds, P. (Eds.).
(2006). Word Sense Disambiguation:
Algorithms and Applications. Kluwer.
Agirre, E. and Martinez, D. (2001).
Learning
class-to-class
selectional
preferences. In CoNLL-01.
Aho, A. V., Sethi, R., and Ullman, J. D.
(1986). Compilers: Principles, Tech-
niques, and Tools. Addison-Wesley.
Aho, A. V. and Ullman, J. D. (1972).
The Theory of Parsing, Translation,
and Compiling, Vol. 1. Prentice Hall.
Ajdukiewicz, K. (1935). Die syntaktis-
che Konnexit¨at. Studia Philosophica,
1, 1–27. English translation “Syntac-
tic Connexion” by H. Weber in Mc-
Call, S. (Ed.) 1967. Polish Logic, pp.
207–231, Oxford University Press.
Algoet, P. H. and Cover, T. M. (1988).
A sandwich proof of the Shannon-
McMillan-Breiman theorem. The An-
nals of Probability, 16(2), 899–909.
Allen, J. (1984). Towards a general the-
ory of action and time. Artiﬁcial Intel-
ligence, 23(2), 123–154.
Allen, J. and Perrault, C. R. (1980). An-
alyzing intention in utterances. Artiﬁ-
cial Intelligence, 15, 143–178.
Althoff, T., Danescu-Niculescu-Mizil,
C., and Jurafsky, D. (2014).
How
to ask for a favor: A case study on
the success of altruistic requests. In
ICWSM 2014.
Amsler, R. A. (1981). A taxonomy of
English nouns and verbs. In ACL-81,
133–138.
An, J., Kwak, H., and Ahn, Y.-Y.
(2018).
SemAxis:
A lightweight
framework to characterize domain-
speciﬁc word semantics beyond sen-
timent. In ACL 2018.
Aone, C. and Bennett, S. W. (1995).
Evaluating automated and manual
acquisition of anaphora resolution
strategies. In ACL-95, 122–129.
Ariel, M. (2001). Accessibility theory:
An overview. In Sanders, T., Schilper-
oord, J., and Spooren, W. (Eds.), Text
Representation: Linguistic and Psy-
cholinguistic Aspects,
29–87. Ben-
jamins.
Artstein, R., Gandhe, S., Gerten, J.,
Leuski, A., and Traum, D. (2009).
Semi-formal evaluation of conversa-
tional characters. In Languages: From
Formal to Natural, 22–35. Springer.
Asher, N. (1993).
Reference to Ab-
stract Objects in Discourse. Studies
in Linguistics and Philosophy (SLAP)
50, Kluwer.
Asher, N. and Lascarides, A. (2003).
Logics of Conversation.
Cambridge
University Press.
Austin, J. L. (1962). How to Do Things
with Words. Harvard University Press.
Awadallah, A. H., Kulkarni, R. G.,
Ozertem, U., and Jones, R. (2015).
Charaterizing and predicting voice
query reformulation. In CIKM-15.
Baayen, R. H. (2001). Word frequency
distributions. Springer.
Baayen, R. H., Piepenbrock, R., and
Gulikers, L. (1995). The CELEX Lexi-
cal Database (Release 2) [CD-ROM].
Linguistic Data Consortium, Univer-
sity of Pennsylvania [Distributor].
Baccianella, S., Esuli, A., and Sebas-
tiani, F. (2010).
Sentiwordnet 3.0:
An enhanced lexical resource for sen-
timent analysis and opinion mining..
In LREC-10, 2200–2204.
Bach,
K. and Harnish,
R. (1979).
Linguistic communication and speech
acts. MIT Press.
Backus, J. W. (1959).
The syntax
and semantics of the proposed in-
ternational algebraic language of the
Zurich ACM-GAMM Conference. In
Information Processing: Proceedings
of the International Conference on In-
formation Processing, Paris,
125–
132. UNESCO.
Backus, J. W. (1996).
Transcript of
question and answer session. In Wex-
elblat, R. L. (Ed.), History of Pro-
gramming Languages, p. 162. Aca-
demic Press.
Bagga, A. and Baldwin, B. (1998).
Algorithms for scoring coreference
chains. In LREC-98, 563–566.
Bahdanau, D., Cho, K., and Bengio,
Y. (2015).
Neural machine transla-
tion by jointly learning to align and
translate.
In 3rd International Con-
ference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceed-
ings.
Bahl, L. R. and Mercer, R. L. (1976).
Part of speech assignment by a sta-
tistical decision algorithm.
In Pro-
ceedings IEEE International Sympo-
sium on Information Theory, 88–89.
Bahl, L. R., Jelinek, F., and Mercer,
R. L. (1983).
A maximum likeli-
hood approach to continuous speech
recognition.
IEEE Transactions on
Pattern Analysis and Machine Intelli-
gence, 5(2), 179–190.
Baker, C. F., Fillmore, C. J., and Lowe,
J. B. (1998). The Berkeley FrameNet
project. In COLING/ACL-98, 86–90.
Baker, J. K. (1975a). The DRAGON
system – An overview. IEEE Transac-
tions on Acoustics, Speech, and Signal
Processing, ASSP-23(1), 24–29.
565

566
Bibliography
Baker, J. K. (1975b). Stochastic model-
ing for automatic speech understand-
ing.
In Reddy, D. R. (Ed.), Speech
Recognition. Academic Press.
Baker, J. K. (1979). Trainable gram-
mars for speech recognition. In Klatt,
D. H. and Wolf, J. J. (Eds.), Speech
Communication Papers for the 97th
Meeting of the Acoustical Society of
America, 547–550.
Baldridge, J., Asher, N., and Hunter,
J. (2007). Annotation for and robust
parsing of discourse structure on un-
restricted texts. Zeitschrift f¨ur Sprach-
wissenschaft, 26, 213–239.
Bamman,
D.,
O’Connor,
B.,
and
Smith, N. A. (2013). Learning latent
personas of ﬁlm characters. In ACL
2013, 352–361.
Banarescu, L., Bonial, C., Cai, S.,
Georgescu, M., Grifﬁtt, K., Herm-
jakob, U., Knight, K., Koehn, P.,
Palmer, M., and Schneider, N. (2013).
Abstract meaning representation for
sembanking. In Proceedings of the 7th
Linguistic Annotation Workshop and
Interoperability with Discourse, 178–
186.
Bangalore, S. and Joshi, A. K. (1999).
Supertagging: An approach to almost
parsing.
Computational Linguistics,
25(2), 237–265.
Banko, M., Cafarella, M., Soderland,
S., Broadhead, M., and Etzioni, O.
(2007). Open information extraction
for the web. In IJCAI, 2670–2676.
Bar-Hillel,
Y. (1953).
A quasi-
arithmetical notation for syntactic de-
scription. Language, 29, 47–58.
Barker, C. (2010). Nominals don’t pro-
vide criteria of identity. In Rathert, M.
and Alexiadou, A. (Eds.), The Seman-
tics of Nominalizations across Lan-
guages and Frameworks, 9–24. Mou-
ton de Gruyter, Berlin.
Barzilay, R. and Lapata, M. (2005).
Modeling local coherence: An entity-
based approach. In ACL-05, 141–148.
Barzilay, R. and Lapata, M. (2008).
Modeling local coherence: An entity-
based approach. Computational Lin-
guistics, 34(1), 1–34.
Barzilay, R. and Lee, L. (2004). Catch-
ing the drift:
Probabilistic content
models, with applications to gener-
ation and summarization.
In HLT-
NAACL-04, 113–120.
Basile, P., Caputo, A., and Semer-
aro,
G.
(2014).
An
enhanced
Lesk word sense disambiguation algo-
rithm through a distributional seman-
tic model.
In COLING-14,
1591–
1600.
Baum, L. E. (1972). An inequality and
associated maximization technique in
statistical estimation for probabilistic
functions of Markov processes.
In
Shisha, O. (Ed.), Inequalities III: Pro-
ceedings of the 3rd Symposium on In-
equalities, 1–8. Academic Press.
Baum, L. E. and Eagon, J. A. (1967).
An inequality with applications to
statistical estimation for probabilistic
functions of Markov processes and
to a model for ecology.
Bulletin of
the American Mathematical Society,
73(3), 360–363.
Baum, L. E. and Petrie, T. (1966).
Statistical inference for probabilis-
tic functions of ﬁnite-state Markov
chains.
Annals of Mathematical
Statistics, 37(6), 1554–1563.
Baum, L. F. (1900). The Wizard of Oz.
Available at Project Gutenberg.
Bayes, T. (1763).
An Essay Toward
Solving a Problem in the Doctrine
of Chances, Vol. 53.
Reprinted in
Facsimiles of Two Papers by Bayes,
Hafner Publishing, 1963.
Bazell, C. E. (1952/1966).
The cor-
respondence fallacy in structural lin-
guistics.
In Hamp, E. P., House-
holder,
F. W.,
and Austerlitz, R.
(Eds.), Studies by Members of the En-
glish Department, Istanbul University
(3), reprinted in Readings in Linguis-
tics II (1966), 271–298. University of
Chicago Press.
Bean, D. and Riloff, E. (1999). Corpus-
based identiﬁcation of non-anaphoric
noun phrases. In ACL-99, 373–380.
Bean, D. and Riloff, E. (2004).
Un-
supervised learning of contextual role
knowledge for coreference resolution.
In HLT-NAACL-04.
Beckman, M. E. and Ayers, G. M.
(1997).
Guidelines for ToBI la-
belling.
Unpublished manuscript,
Ohio
State
University,
http:
//www.ling.ohio-state.edu/
research/phonetics/E_ToBI/.
Beckman, M. E. and Hirschberg, J.
(1994). The ToBI annotation conven-
tions. Manuscript, Ohio State Univer-
sity.
Bedi, G., Carrillo, F., Cecchi, G. A.,
Slezak, D. F., Sigman, M., Mota,
N. B., Ribeiro, S., Javitt, D. C.,
Copelli, M., and Corcoran, C. M.
(2015).
Automated analysis of free
speech predicts psychosis onset in
high-risk youths. npj Schizophrenia,
1.
Bejˇcek, E., Hajiˇcov´a, E., Hajiˇc, J.,
J´ınov´a, P., Kettnerov´a, V., Kol´aˇrov´a,
V.,
Mikulov´a,
M.,
M´ırovsk´y,
J.,
Nedoluzhko,
A.,
Panevov´a,
J.,
Pol´akov´a,
L.,
ˇSevˇc´ıkov´a,
M.,
ˇStˇep´anek, J., and Zik´anov´a, ˇS. (2013).
Prague
dependency
treebank
3.0.
Tech. rep., Institute of Formal and
Applied Linguistics, Charles Univer-
sity in Prague.
LINDAT/CLARIN
digital library at Institute of Formal
and Applied Linguistics, Charles Uni-
versity in Prague.
Bellegarda,
J. R. (1997).
A la-
tent semantic analysis framework for
large-span language modeling.
In
Eurospeech-97.
Bellegarda, J. R. (2000).
Exploiting
latent semantic information in statis-
tical language modeling. Proceedings
of the IEEE, 89(8), 1279–1296.
Bellegarda, J. R. (2013).
Natural
language technology in mobile de-
vices:
Two grounding frameworks.
In Mobile Speech and Advanced Nat-
ural Language Solutions,
185–196.
Springer.
Bellman, R. (1957).
Dynamic Pro-
gramming.
Princeton University
Press.
Bellman, R. (1984). Eye of the Hurri-
cane: an autobiography. World Sci-
entiﬁc Singapore.
Bengio, Y., Courville, A., and Vin-
cent, P. (2013). Representation learn-
ing: A review and new perspectives.
IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 35(8),
1798–1828.
Bengio, Y., Ducharme, R., Vincent,
P., and Jauvin, C. (2003).
A neural
probabilistic language model. Journal
of machine learning research, 3(Feb),
1137–1155.
Bengio, Y., Lamblin, P., Popovici, D.,
and Larochelle, H. (2007).
Greedy
layer-wise training of deep networks.
In NIPS 2007, 153–160.
Bengio, Y., Schwenk, H., Sen´ecal, J.-
S., Morin, F., and Gauvain, J.-L.
(2006). Neural probabilistic language
models.
In Innovations in Machine
Learning, 137–186. Springer.
Bengtson, E. and Roth, D. (2008). Un-
derstanding the value of features for
coreference resolution.
In EMNLP-
08, 294–303.
Berant, J., Chou, A., Frostig, R., and
Liang, P. (2013). Semantic parsing on
freebase from question-answer pairs.
In EMNLP 2013.
Berant, J. and Liang, P. (2014). Seman-
tic parsing via paraphrasing. In ACL
2014.
Berg-Kirkpatrick, T., Burkett, D., and
Klein, D. (2012).
An empirical in-
vestigation of statistical signiﬁcance
in NLP. In EMNLP 2012, 995–1005.
Berger, A., Della Pietra, S. A., and
Della Pietra, V. J. (1996). A maximum
entropy approach to natural language
processing.
Computational Linguis-
tics, 22(1), 39–71.
Bergsma, S. and Lin, D. (2006). Boot-
strapping path-based pronoun resolu-
tion. In COLING/ACL 2006, 33–40.

Bibliography
567
Bergsma, S., Lin, D., and Goebel, R.
(2008a).
Discriminative learning of
selectional preference from unlabeled
text. In EMNLP-08, 59–68.
Bergsma, S., Lin, D., and Goebel, R.
(2008b). Distributional identiﬁcation
of non-referential pronouns. In ACL-
08, 10–18.
Bethard, S. (2013). ClearTK-TimeML:
A minimalist approach to TempEval
2013. In SemEval-13, 10–14.
Bhat, I., Bhat, R. A., Shrivastava, M.,
and Sharma, D. (2017). Joining hands:
Exploiting monolingual treebanks for
parsing of code-mixing data.
In
EACL-17, 324–330.
Biber, D., Johansson, S., Leech, G.,
Conrad, S., and Finegan, E. (1999).
Longman Grammar of Spoken and
Written English. Pearson.
Bies, A., Ferguson, M., Katz, K., and
MacIntyre, R. (1995).
Bracketing
guidelines for Treebank II style Penn
Treebank Project..
Bikel, D. M., Miller, S., Schwartz, R.,
and Weischedel, R. (1997). Nymble:
A high-performance learning name-
ﬁnder. In ANLP 1997, 194–201.
Biran, O. and McKeown, K. (2015).
PDTB discourse parsing as a tagging
task: The two taggers approach.
In
SIGDIAL 15, 96–104.
Bird, S., Klein, E., and Loper, E.
(2009). Natural Language Processing
with Python. O’Reilly.
Bisani, M. and Ney, H. (2004). Boot-
strap estimates for conﬁdence inter-
vals in ASR performance evaluation.
In ICASSP-04, Vol. I, 409–412.
Bishop, C. M. (2006). Pattern recogni-
tion and machine learning. Springer.
Bizer, C., Lehmann, J., Kobilarov, G.,
Auer, S., Becker, C., Cyganiak, R.,
and Hellmann, S. (2009). DBpedia—
A crystallization point for the Web of
Data.
Web Semantics: science, ser-
vices and agents on the world wide
web, 7(3), 154–165.
Bj¨orkelund, A. and Kuhn, J. (2014).
Learning structured perceptrons for
coreference resolution with latent an-
tecedents and non-local features.
In
ACL 2014, 47–57.
Black, E. (1988).
An experiment in
computational discrimination of En-
glish word senses. IBM Journal of Re-
search and Development, 32(2), 185–
194.
Black, E., Abney, S. P., Flickinger, D.,
Gdaniec, C., Grishman, R., Harrison,
P., Hindle, D., Ingria, R., Jelinek,
F., Klavans, J. L., Liberman, M. Y.,
Marcus, M. P., Roukos, S., Santorini,
B., and Strzalkowski, T. (1991).
A
procedure for quantitatively compar-
ing the syntactic coverage of English
grammars.
In Proceedings DARPA
Speech and Natural Language Work-
shop, 306–311.
Black, E., Jelinek, F., Lafferty, J. D.,
Magerman, D. M., Mercer, R. L., and
Roukos, S. (1992). Towards history-
based grammars: Using richer models
for probabilistic parsing. In Proceed-
ings DARPA Speech and Natural Lan-
guage Workshop, 134–139.
Blei, D. M., Ng, A. Y., and Jordan, M. I.
(2003).
Latent Dirichlet allocation.
JMLR, 3(5), 993–1022.
Blodgett,
S.
L.,
Green,
L.,
and
O’Connor, B. (2016).
Demographic
dialectal variation in social media: A
case study of African-American En-
glish. In EMNLP 2016.
Blodgett, S. L. and O’Connor, B.
(2017).
Racial disparity in natural
language processing:
A case study
of social media african-american en-
glish.
In Fairness, Accountability,
and Transparency in Machine Learn-
ing (FAT/ML) Workshop, KDD.
Bloomﬁeld, L. (1914). An Introduction
to the Study of Language. Henry Holt
and Company.
Bloomﬁeld, L. (1933). Language. Uni-
versity of Chicago Press.
Bobrow, D. G., Kaplan, R. M., Kay,
M., Norman, D. A., Thompson, H.,
and Winograd, T. (1977).
GUS, A
frame driven dialog system. Artiﬁcial
Intelligence, 8, 155–173.
Bobrow, D. G. and Norman, D. A.
(1975).
Some principles of mem-
ory schemata. In Bobrow, D. G. and
Collins, A. (Eds.), Representation and
Understanding. Academic Press.
Bobrow,
D. G. and Winograd,
T.
(1977).
An overview of KRL, a
knowledge representation language.
Cognitive Science, 1(1), 3–46.
Bod, R. (1993). Using an annotated cor-
pus as a stochastic grammar. In EACL-
93, 37–44.
Boersma, P. and Weenink, D. (2005).
Praat: doing phonetics by computer
(version 4.3.14).
[Computer pro-
gram]. Retrieved May 26, 2005, from
http://www.praat.org/.
Boguraev, B. K. and Briscoe, T. (Eds.).
(1989).
Computational Lexicogra-
phy for Natural Language Processing.
Longman.
Bohus, D. and Rudnicky, A. I. (2005).
Sorry, I didn’t catch that! — An in-
vestigation of non-understanding er-
rors and recovery strategies. In Pro-
ceedings of SIGDIAL.
Bojanowski, P., Grave, E., Joulin, A.,
and Mikolov, T. (2017).
Enriching
word vectors with subword informa-
tion. TACL, 5, 135–146.
Bolinger, D. (1981). Two kinds of vow-
els, two kinds of rhythm. Indiana Uni-
versity Linguistics Club.
Bollacker, K., Evans, C., Paritosh,
P., Sturge, T., and Taylor, J. (2008).
Freebase:
a collaboratively created
graph database for structuring human
knowledge. In SIGMOD 2008, 1247–
1250.
Bolukbasi, T., Chang, K.-W., Zou, J.,
Saligrama, V., and Kalai, A. T. (2016).
Man is to computer programmer as
woman is to homemaker? Debiasing
word embeddings. In NIPS 16, 4349–
4357.
Booth, T. L. (1969). Probabilistic rep-
resentation of formal languages.
In
IEEE Conference Record of the 1969
Tenth Annual Symposium on Switch-
ing and Automata Theory, 74–81.
Booth, T. L. and Thompson, R. A.
(1973).
Applying probability mea-
sures to abstract languages.
IEEE
Transactions on Computers, C-22(5),
442–450.
Borges, J. L. (1964).
The analytical
language of John Wilkins.
Univer-
sity of Texas Press. Trans. Ruth L. C.
Simms.
Bowman, S. R., Vilnis, L., Vinyals, O.,
Dai, A. M., Jozefowicz, R., and Ben-
gio, S. (2016). Generating sentences
from a continuous space. In CoNLL-
16, 10–21.
Boyd-Graber, J., Feng, S., and Ro-
driguez, P. (2018). Human-computer
question answering:
The case for
quizbowl. In Escalera, S. and Weimer,
M. (Eds.), The NIPS ’17 Competition:
Building Intelligent Systems. Springer.
Brachman, R. J. (1979). On the episte-
mogical status of semantic networks.
In Findler, N. V. (Ed.), Associative
Networks:
Representation and Use
of Knowledge by Computers,
3–50.
Academic Press.
Brachman, R. J. and Levesque, H. J.
(Eds.). (1985).
Readings in Knowl-
edge Representation. Morgan Kauf-
mann.
Brachman, R. J. and Schmolze, J. G.
(1985).
An overview of the KL-
ONE knowledge representation sys-
tem.
Cognitive Science, 9(2), 171–
216.
Brants, T. (2000).
TnT: A statistical
part-of-speech tagger. In ANLP 2000,
224–231.
Brants, T., Popat, A. C., Xu, P., Och,
F. J., and Dean, J. (2007). Large lan-
guage models in machine translation.
In EMNLP/CoNLL 2007.
Braud, C., Coavoux, M., and Søgaard,
A. (2017).
Cross-lingual RST dis-
course parsing.
In EACL-17,
292–
304.

568
Bibliography
Br´eal, M. (1897). Essai de S´emantique:
Science des signiﬁcations. Hachette.
Brennan, S. E., Friedman, M. W., and
Pollard, C. (1987).
A centering ap-
proach to pronouns. In ACL-87, 155–
162.
Bresnan, J. (Ed.). (1982). The Mental
Representation of Grammatical Rela-
tions. MIT Press.
Brill, E., Dumais, S. T., and Banko,
M. (2002).
An analysis of the
AskMSR question-answering system.
In EMNLP 2002, 257–264.
Brill, E. and Resnik, P. (1994).
A
rule-based approach to prepositional
phrase attachment disambiguation. In
COLING-94, 1198–1204.
Brin, S. (1998).
Extracting patterns
and relations from the World Wide
Web. In Proceedings World Wide Web
and Databases International Work-
shop, Number 1590 in LNCS, 172–
183. Springer.
Briscoe, T. and Carroll, J. (1993).
Generalized probabilistic LR parsing
of natural language (corpora) with
uniﬁcation-based grammars. Compu-
tational Linguistics, 19(1), 25–59.
Brockmann, C. and Lapata, M. (2003).
Evaluating and combining approaches
to selectional preference acquisition.
In EACL-03, 27–34.
Broschart, J. (1997). Why Tongan does
it differently. Linguistic Typology, 1,
123–165.
Bruce, B. C. (1975).
Generation as
a social action.
In Proceedings of
TINLAP-1 (Theoretical Issues in Nat-
ural Language Processing), 64–67.
Brysbaert, M., Warriner, A. B., and
Kuperman, V. (2014).
Concrete-
ness ratings for 40 thousand generally
known english word lemmas. Behav-
ior Research Methods, 46(3), 904–
911.
Buchholz, S. and Marsi, E. (2006).
Conll-x shared task on multilingual
dependency parsing.
In CoNLL-06,
149–164.
Buck, C., Heaﬁeld, K., and Van Ooyen,
B. (2014).
N-gram counts and
language models from the common
crawl. In Proceedings of LREC.
Budanitsky, A. and Hirst, G. (2006).
Evaluating WordNet-based measures
of lexical semantic relatedness. Com-
putational Linguistics, 32(1), 13–47.
Budzianowski, P., Wen, T.-H., Tseng,
B.-H., Casanueva, I., Ultes, S., Ra-
madan, O., and Gaˇsi´c, M. (2018).
MultiWOZ - a large-scale multi-
domain wizard-of-Oz dataset for task-
oriented dialogue modelling.
In
EMNLP 2018, 5016–5026.
Bullinaria, J. A. and Levy, J. P. (2007).
Extracting semantic representations
from word co-occurrence statistics: A
computational study.
Behavior re-
search methods, 39(3), 510–526.
Bullinaria, J. A. and Levy, J. P. (2012).
Extracting semantic representations
from word co-occurrence statistics:
stop-lists, stemming, and SVD.
Be-
havior research methods, 44(3), 890–
907.
Bulyko, I., Kirchhoff, K., Ostendorf,
M., and Goldberg, J. (2005). Error-
sensitive response generation in a spo-
ken language dialogue system. Speech
Communication, 45(3), 271–288.
Caliskan,
A.,
Bryson,
J. J.,
and
Narayanan, A. (2017).
Semantics
derived automatically from language
corpora contain human-like biases.
Science, 356(6334), 183–186.
Cardie, C. (1993).
A case-based ap-
proach to knowledge acquisition for
domain speciﬁc sentence analysis. In
AAAI-93, 798–803. AAAI Press.
Cardie, C. (1994).
Domain-Speciﬁc
Knowledge Acquisition for Concep-
tual
Sentence
Analysis.
Ph.D.
thesis, University of Massachusetts,
Amherst, MA. Available as CMPSCI
Technical Report 94-74.
Cardie, C. and Wagstaff, K. (1999).
Noun phrase coreference as cluster-
ing. In EMNLP/VLC-99.
Carlson, L. and Marcu, D. (2001). Dis-
course tagging manual. Tech. rep. ISI-
TR-545, ISI.
Carlson,
L.,
Marcu,
D.,
and
Okurowski, M. E. (2001). Building a
discourse-tagged corpus in the frame-
work of rhetorical structure theory. In
Proceedings of SIGDIAL.
Carpenter, R. (2017).
Cleverbot.
http://www.cleverbot.com,
accessed
2017.
Carreras, X. and M`arquez, L. (2005).
Introduction
to
the
CoNLL-2005
shared task: Semantic role labeling.
In CoNLL-05, 152–164.
Carroll, J., Briscoe, T., and Sanﬁlippo,
A. (1998). Parser evaluation: A sur-
vey and a new proposal. In LREC-98,
447–454.
Chafe, W. L. (1976).
Givenness,
contrastiveness, deﬁniteness, subjects,
topics, and point of view. In Li, C. N.
(Ed.), Subject and Topic, 25–55. Aca-
demic Press.
Chambers, N. (2013).
NavyTime:
Event and time ordering from raw
text. In SemEval-13, 73–77.
Chambers, N. and Jurafsky, D. (2010).
Improving the use of pseudo-words
for evaluating selectional preferences.
In ACL 2010, 445–453.
Chambers, N. and Jurafsky, D. (2011).
Template-based information extrac-
tion without the templates.
In ACL
2011.
Chang, A. X. and Manning, C. D.
(2012). SUTime: A library for recog-
nizing and normalizing time expres-
sions.. In LREC-12, 3735–3740.
Chang, K.-W., Samdani, R., and Roth,
D. (2013). A constrained latent vari-
able model for coreference resolution.
In EMNLP 2013, 601–612.
Chang,
K.-W.,
Samdani,
R.,
Ro-
zovskaya, A., Sammons, M., and
Roth, D. (2012). Illinois-Coref: The
UI system in the CoNLL-2012 shared
task. In CoNLL-12, 113–117.
Chaplot, D. S. and Salakhutdinov, R.
(2018). Knowledge-based word sense
disambiguation using topic models. In
AAAI-18.
Charniak, E. (1997). Statistical parsing
with a context-free grammar and word
statistics. In AAAI-97, 598–603.
Charniak, E., Hendrickson, C., Jacob-
son, N., and Perkowitz, M. (1993).
Equations for part-of-speech tagging.
In AAAI-93, 784–789. AAAI Press.
Che, W., Li, Z., Li, Y., Guo, Y., Qin,
B., and Liu, T. (2009).
Multilin-
gual dependency-based syntactic and
semantic parsing. In CoNLL-09, 49–
54.
Chelba, C. and Jelinek, F. (2000).
Structured language modeling. Com-
puter Speech and Language, 14, 283–
332.
Chen, C. and Ng, V. (2013).
Lin-
guistically aware coreference evalua-
tion metrics.
In Sixth International
Joint Conference on Natural Lan-
guage Processing, 1366–1374.
Chen, D., Fisch, A., Weston, J., and
Bordes, A. (2017). Reading wikipedia
to answer open-domain questions. In
ACL 2017.
Chen, D. and Manning, C. D. (2014).
A fast and accurate dependency parser
using neural networks..
In EMNLP
2014, 740–750.
Chen, E., Snyder, B., and Barzilay, R.
(2007).
Incremental text structuring
with online hierarchical ranking.
In
EMNLP/CoNLL 2007, 83–91.
Chen, J. N. and Chang, J. S. (1998).
Topical clustering of MRD senses
based on information retrieval tech-
niques.
Computational Linguistics,
24(1), 61–96.
Chen, S. F. and Goodman, J. (1996).
An empirical study of smoothing tech-
niques for language modeling.
In
ACL-96, 310–318.
Chen, S. F. and Goodman, J. (1998).
An empirical study of smoothing tech-
niques for language modeling. Tech.
rep. TR-10-98,
Computer Science
Group, Harvard University.

Bibliography
569
Chen, S. F. and Goodman, J. (1999).
An empirical study of smoothing tech-
niques for language modeling. Com-
puter Speech and Language, 13, 359–
394.
Chen, X., Shi, Z., Qiu, X., and Huang,
X. (2017). Adversarial multi-criteria
learning for Chinese word segmenta-
tion. In ACL 2017, 1193–1203.
Chierchia, G. and McConnell-Ginet, S.
(1991). Meaning and Grammar. MIT
Press.
Chinchor, N., Hirschman, L., and
Lewis, D. L. (1993).
Evaluating
Message Understanding systems: An
analysis of the third Message Under-
standing Conference. Computational
Linguistics, 19(3), 409–449.
Chiticariu, L., Danilevsky, M., Li, Y.,
Reiss, F., and Zhu, H. (2018). Sys-
temT: Declarative text understanding
for enterprise. In NAACL HLT 2018,
Vol. 3, 76–83.
Chiticariu, L., Li, Y., and Reiss, F. R.
(2013). Rule-Based Information Ex-
traction is Dead!
Long Live Rule-
Based Information Extraction Sys-
tems!. In EMNLP 2013, 827–832.
Chiu, J. P. C. and Nichols, E. (2016).
Named entity recognition with bidi-
rectional LSTM-CNNs.
TACL, 4,
357–370.
Cho, K., van Merri¨enboer, B., Gul-
cehre, C., Bahdanau, D., Bougares, F.,
Schwenk, H., and Bengio, Y. (2014).
Learning phrase representations using
RNN encoder–decoder for statistical
machine translation. In EMNLP 2014,
1724–1734.
Choi, E., He, H., Iyyer, M., Yatskar,
M., Yih, W.-t., Choi, Y., Liang, P., and
Zettlemoyer, L. (2018). Quac: Ques-
tion answering in context. In EMNLP
2018.
Choi, J. D. and Palmer, M. (2011a).
Getting the most out of transition-
based dependency parsing.
In ACL
2011, 687–692.
Choi, J. D. and Palmer, M. (2011b).
Transition-based semantic role label-
ing using predicate argument cluster-
ing. In Proceedings of the ACL 2011
Workshop on Relational Models of Se-
mantics, 37–45.
Choi, J. D., Tetreault, J., and Stent,
A. (2015). It depends: Dependency
parser comparison using a web-based
evaluation tool. In ACL 2015, 26–31.
Chomsky, N. (1956).
Three models
for the description of language. IRE
Transactions on Information Theory,
2(3), 113–124.
Chomsky, N. (1956/1975). The Log-
ical Structure of Linguistic Theory.
Plenum.
Chomsky, N. (1957). Syntactic Struc-
tures. Mouton, The Hague.
Chomsky, N. (1963). Formal proper-
ties of grammars.
In Luce, R. D.,
Bush, R., and Galanter, E. (Eds.),
Handbook of Mathematical Psychol-
ogy, Vol. 2, 323–418. Wiley.
Chomsky, N. (1981). Lectures on Gov-
ernment and Binding. Foris.
Christodoulopoulos, C., Goldwater,
S., and Steedman, M. (2010).
Two
decades of unsupervised POS induc-
tion: How far have we come?.
In
EMNLP-10.
Chu, Y.-J. and Liu, T.-H. (1965). On
the shortest arborescence of a directed
graph. Science Sinica, 14, 1396–1400.
Chu-Carroll, J. (1998).
A statistical
model for discourse act recognition in
dialogue interactions. In Chu-Carroll,
J. and Green, N. (Eds.), Applying
Machine Learning to Discourse Pro-
cessing. Papers from the 1998 AAAI
Spring Symposium. Tech. rep. SS-98-
01, 12–17. AAAI Press.
Chu-Carroll,
J.
and
Carberry,
S.
(1998). Collaborative response gener-
ation in planning dialogues. Compu-
tational Linguistics, 24(3), 355–400.
Chu-Carroll,
J. and Carpenter,
B.
(1999). Vector-based natural language
call routing. Computational Linguis-
tics, 25(3), 361–388.
Chu-Carroll, J., Fan, J., Boguraev,
B. K., Carmel, D., Sheinwald, D., and
Welty, C. (2012). Finding needles in
the haystack: Search and candidate
generation. IBM Journal of Research
and Development, 56(3/4), 6:1–6:12.
Church, A. (1940). A formulation of
a simple theory of types. Journal of
Symbolic Logic, 5, 56–68.
Church, K. W. (1980).
On Memory
Limitations in Natural Language Pro-
cessing
Master’s thesis, MIT.
Dis-
tributed by the Indiana University Lin-
guistics Club.
Church, K. W. (1988).
A stochastic
parts program and noun phrase parser
for unrestricted text. In ANLP 1988,
136–143.
Church, K. W. (1989).
A stochastic
parts program and noun phrase parser
for unrestricted text. In ICASSP-89,
695–698.
Church, K. W. (1994). Unix for Poets.
Slides from 2nd ELSNET Summer
School and unpublished paper ms.
Church, K. W. and Gale, W. A. (1991).
A comparison of the enhanced Good-
Turing and deleted estimation meth-
ods for estimating probabilities of En-
glish bigrams. Computer Speech and
Language, 5, 19–54.
Church, K. W. and Hanks, P. (1989).
Word association norms, mutual in-
formation, and lexicography. In ACL-
89, 76–83.
Church, K. W. and Hanks, P. (1990).
Word association norms, mutual in-
formation, and lexicography. Compu-
tational Linguistics, 16(1), 22–29.
Church, K. W., Hart, T., and Gao,
J. (2007). Compressing trigram lan-
guage models with Golomb coding. In
EMNLP/CoNLL 2007, 199–207.
Cialdini, R. B. (1984). Inﬂuence: The
psychology of persuasion. Morrow.
Ciaramita, M. and Altun, Y. (2006).
Broad-coverage sense disambiguation
and information extraction with a su-
persense sequence tagger. In EMNLP
2006, 594–602.
Ciaramita, M. and Johnson, M. (2003).
Supersense tagging of unknown nouns
in WordNet. In EMNLP-2003, 168–
175.
Clark, C. and Gardner, M. (2018).
Simple and effective multi-paragraph
reading comprehension. In ACL 2018.
Clark, E. (1987). The principle of con-
trast: A constraint on language ac-
quisition. In MacWhinney, B. (Ed.),
Mechanisms of language acquisition,
1–33. LEA.
Clark, H. H. (1996). Using Language.
Cambridge University Press.
Clark, H. H. and Fox Tree, J. E.
(2002). Using uh and um in sponta-
neous speaking.
Cognition, 84, 73–
111.
Clark, H. H. and Marshall, C. (1981).
Deﬁnite reference and mutual knowl-
edge. In Joshi, A. K., Webber, B. L.,
and Sag, I. A. (Eds.), Elements of Dis-
course Understanding, 10–63. Cam-
bridge.
Clark, H. H. and Wilkes-Gibbs, D.
(1986).
Referring as a collaborative
process. Cognition, 22, 1–39.
Clark, J. and Yallop, C. (1995). An In-
troduction to Phonetics and Phonol-
ogy (2nd Ed.). Blackwell.
Clark, K. and Manning, C. D. (2015).
Entity-centric coreference resolution
with model stacking.
In ACL 2015,
1405–1415.
Clark, K. and Manning, C. D. (2016a).
Deep
reinforcement
learning
for
mention-ranking
coreference
mod-
els. In EMNLP 2016.
Clark, K. and Manning, C. D. (2016b).
Improving coreference resolution by
learning entity-level distributed repre-
sentations. In ACL 2016.
Clark, P., Cowhey, I., Etzioni, O.,
Khot, T., Sabharwal, A., Schoenick,
C., and Tafjord, O. (2018). Think you
have solved question answering? Try
ARC, the AI2 reasoning challenge..
arXiv preprint arXiv:1803.05457.
Clark, S., Curran, J. R., and Osborne,
M. (2003). Bootstrapping pos taggers
using unlabelled data. In CoNLL-03,
49–55.

570
Bibliography
CMU (1993).
The Carnegie Mel-
lon
Pronouncing
Dictionary
v0.1.
Carnegie Mellon University.
Coccaro, N. and Jurafsky, D. (1998).
Towards better integration of semantic
predictors in statistical language mod-
eling. In ICSLP-98, 2403–2406.
Cohen, K. B. and Demner-Fushman, D.
(2014). Biomedical natural language
processing. Benjamins.
Cohen, M. H., Giangola, J. P., and
Balogh, J. (2004). Voice User Inter-
face Design. Addison-Wesley.
Cohen, P. R. and Perrault, C. R. (1979).
Elements of a plan-based theory of
speech acts. Cognitive Science, 3(3),
177–212.
Colby, K. M., Hilf, F. D., Weber, S.,
and Kraemer, H. C. (1972). Turing-
like indistinguishability tests for the
validation of a computer simulation of
paranoid processes. Artiﬁcial Intelli-
gence, 3, 199–221.
Colby, K. M., Weber, S., and Hilf, F. D.
(1971). Artiﬁcial paranoia. Artiﬁcial
Intelligence, 2(1), 1–25.
Cole, R. A., Novick, D. G., Vermeulen,
P. J. E., Sutton, S., Fanty, M., Wes-
sels, L. F. A., de Villiers, J. H., Schalk-
wyk, J., Hansen, B., and Burnett, D.
(1997). Experiments with a spoken di-
alogue system for taking the US cen-
sus. Speech Communication, 23, 243–
260.
Coleman,
J. (2005).
Introducing
Speech and Language Processing.
Cambridge University Press.
Collins, M. (1996). A new statistical
parser based on bigram lexical depen-
dencies. In ACL-96, 184–191.
Collins, M. (1999). Head-Driven Sta-
tistical Models for Natural Language
Parsing.
Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Collins, M. (2003). Head-driven statis-
tical models for natural language pars-
ing. Computational Linguistics, 29(4),
589–637.
Collins, M., Hajiˇc, J., Ramshaw, L. A.,
and Tillmann, C. (1999). A statistical
parser for Czech. In ACL-99, 505–
512.
Collobert, R. and Weston, J. (2007).
Fast semantic extraction using a novel
neural network architecture. In ACL-
07, 560–567.
Collobert, R. and Weston, J. (2008).
A uniﬁed architecture for natural lan-
guage processing: Deep neural net-
works with multitask learning.
In
ICML, 160–167.
Collobert,
R.,
Weston,
J.,
Bottou,
L., Karlen, M., Kavukcuoglu, K.,
and Kuksa,
P. (2011).
Natural
language processing (almost) from
scratch. JMLR, 12, 2493–2537.
Connolly, D., Burger, J. D., and Day,
D. S. (1994). A machine learning ap-
proach to anaphoric reference. In Pro-
ceedings of the International Confer-
ence on New Methods in Language
Processing (NeMLaP).
Copestake, A. and Briscoe, T. (1995).
Semi-productive polysemy and sense
extension.
Journal of Semantics,
12(1), 15–68.
Cottrell, G. W. (1985). A Connection-
ist Approach to Word Sense Disam-
biguation. Ph.D. thesis, University of
Rochester, Rochester, NY.
Revised
version published by Pitman, 1989.
Cover, T. M. and Thomas, J. A. (1991).
Elements of Information Theory. Wi-
ley.
Covington, M. (2001). A fundamental
algorithm for dependency parsing. In
Proceedings of the 39th Annual ACM
Southeast Conference, 95–102.
Cox, D. (1969).
Analysis of Binary
Data. Chapman and Hall, London.
Craven, M. and Kumlien, J. (1999).
Constructing
biological
knowledge
bases by extracting information from
text sources. In ISMB-99, 77–86.
Cruse, D. A. (2004). Meaning in Lan-
guage: an Introduction to Semantics
and Pragmatics.
Oxford University
Press. Second edition.
Cucerzan, S. (2007).
Large-scale
named entity disambiguation based on
Wikipedia data.
In EMNLP/CoNLL
2007, 708–716.
Culicover, P. W. and Jackendoff, R.
(2005). Simpler Syntax. Oxford Uni-
versity Press.
Dagan, I., Marcus, S., and Markovitch,
S. (1993). Contextual word similarity
and estimation from sparse data.
In
ACL-93, 164–171.
Danieli, M. and Gerbino, E. (1995).
Metrics for evaluating dialogue strate-
gies in a spoken language system. In
Proceedings of the 1995 AAAI Spring
Symposium on Empirical Methods in
Discourse Interpretation and Genera-
tion, 34–39. AAAI Press.
Das, S. R. and Chen, M. Y. (2001).
Yahoo!
for
Amazon:
Senti-
ment parsing from small talk on
the web.
EFA 2001 Barcelona
Meetings.
Available
at
SSRN:
http://ssrn.com/abstract=276189.
Davidson, D. (1967). The logical form
of action sentences.
In Rescher, N.
(Ed.), The Logic of Decision and Ac-
tion. University of Pittsburgh Press.
Davidson, T., Warmsley, D., Macy, M.,
and Weber, I. (2017). Automated hate
speech detection and the problem of
offensive language. In ICWSM 2017.
Davies, M. (2012).
Expanding hori-
zons in historical linguistics with the
400-million word Corpus of Histori-
cal American English. Corpora, 7(2),
121–157.
Davies, M. (2015).
The Wikipedia
Corpus: 4.6 million articles, 1.9 bil-
lion words. Adapted from Wikipedia.
Available online at https://www.
english-corpora.org/wiki/.
Davis, E. (1990).
Representations of
Commonsense Knowledge.
Morgan
Kaufmann.
Davis, E., Morgenstern, L., and Or-
tiz, C. L. (2017). The ﬁrst Winograd
schema challenge at IJCAI-16.
AI
Magazine, 38(3), 97–98.
de Marneffe, M.-C., Dozat, T., Sil-
veira, N., Haverinen, K., Ginter, F.,
Nivre, J., and Manning, C. D. (2014).
Universal Stanford dependencies: A
cross-linguistic typology.. In LREC,
Vol. 14, 4585–92.
de Marneffe, M.-C., MacCartney, B.,
and Manning, C. D. (2006). Gener-
ating typed dependency parses from
phrase structure parses. In LREC-06.
de Marneffe,
M.-C. and Manning,
C. D. (2008). The stanford typed de-
pendencies representation.
In Col-
ing 2008: Proceedings of the work-
shop on Cross-Framework and Cross-
Domain Parser Evaluation, 1–8.
de Marneffe, M.-C., Recasens, M., and
Potts, C. (2015). Modeling the lifes-
pan of discourse entities with applica-
tion to coreference resolution. JAIR,
52, 445–475.
Deerwester, S. C., Dumais, S. T., Fur-
nas, G. W., Harshman, R. A., Lan-
dauer, T. K., Lochbaum, K. E., and
Streeter, L. (1988). Computer infor-
mation retrieval using latent semantic
structure: US Patent 4,839,853..
Deerwester, S. C., Dumais, S. T., Lan-
dauer, T. K., Furnas, G. W., and
Harshman, R. A. (1990).
Indexing
by latent semantics analysis. JASIS,
41(6), 391–407.
DeJong, G. F. (1982).
An overview
of the FRUMP system.
In Lehn-
ert, W. G. and Ringle, M. H. (Eds.),
Strategies for Natural Language Pro-
cessing, 149–176. LEA.
Dempster, A. P., Laird, N. M., and Ru-
bin, D. B. (1977). Maximum likeli-
hood from incomplete data via the EM
algorithm. Journal of the Royal Statis-
tical Society, 39(1), 1–21.
Denis, P. and Baldridge, J. (2007).
Joint determination of anaphoricity
and coreference resolution using inte-
ger programming. In NAACL-HLT 07.
Denis, P. and Baldridge, J. (2008). Spe-
cialized models and ranking for coref-
erence resolution.
In EMNLP-08,
660–669.

Bibliography
571
Denis, P. and Baldridge, J. (2009).
Global joint models for coreference
resolution and named entity classiﬁ-
cation.
Procesamiento del Lenguaje
Natural, 42.
DeRose, S. J. (1988).
Grammatical
category disambiguation by statistical
optimization. Computational Linguis-
tics, 14, 31–39.
Devlin, J., Chang, M.-W., Lee, K., and
Toutanova, K. (2019).
BERT: Pre-
training of deep bidirectional trans-
formers for language understanding.
In NAACL HLT 2019, 4171–4186.
Di Eugenio, B. (1990). Centering the-
ory and the Italian pronominal system.
In COLING-90, 270–275.
Di Eugenio, B. (1996). The discourse
functions of Italian subjects: A center-
ing approach. In COLING-96, 352–
357.
Diab, M. and Resnik, P. (2002). An un-
supervised method for word sense tag-
ging using parallel corpora. In ACL-
02, 255–262.
Digman, J. M. (1990).
Personal-
ity structure: Emergence of the ﬁve-
factor model. Annual Review of Psy-
chology, 41(1), 417–440.
Ditman,
T. and Kuperberg,
G. R.
(2010). Building coherence: A frame-
work for exploring the breakdown
of links across clause boundaries in
schizophrenia.
Journal of neurolin-
guistics, 23(3), 254–269.
Do, Q. N. T., Bethard, S., and Moens,
M.-F. (2017). Improving implicit se-
mantic role labeling by predicting se-
mantic frame arguments. In IJCNLP-
17.
Dolan, B. (1994). Word sense ambigua-
tion: Clustering related senses.
In
COLING-94, 712–716.
dos Santos, C. N., Xiang, B., and Zhou,
B. (2015).
Classifying relations by
ranking with convolutional neural net-
works. In ACL 2015.
Dowty, D. R. (1979). Word Meaning
and Montague Grammar. D. Reidel.
Dowty, D. R., Wall, R. E., and Peters,
S. (1981). Introduction to Montague
Semantics. D. Reidel.
Dozat, T., Qi, P., and Manning, C. D.
(2017). Stanford’s graph-based neu-
ral dependency parser at the CoNLL
2017 shared task. In Proceedings of
the CoNLL 2017 Shared Task, 20–30.
Dror, R., Baumer, G., Bogomolov, M.,
and Reichart, R. (2017). Replicability
analysis for natural language process-
ing: Testing signiﬁcance with multi-
ple datasets. TACL, 5, 471––486.
Duda, R. O. and Hart, P. E. (1973). Pat-
tern Classiﬁcation and Scene Analy-
sis. John Wiley and Sons.
Durrett, G. and Klein, D. (2013). Easy
victories and uphill battles in corefer-
ence resolution. In EMNLP 2013.
Durrett, G. and Klein, D. (2014). A
joint model for entity analysis: Coref-
erence, typing, and linking. TACL, 2,
477–490.
Earley, J. (1968). An Efﬁcient Context-
Free Parsing Algorithm.
Ph.D. the-
sis, Carnegie Mellon University, Pitts-
burgh, PA.
Earley, J. (1970). An efﬁcient context-
free parsing algorithm. CACM, 6(8),
451–455.
Edmonds, J. (1967). Optimum branch-
ings. Journal of Research of the Na-
tional Bureau of Standards B, 71(4),
233–240.
Efron, B. and Tibshirani, R. J. (1993).
An introduction to the bootstrap. CRC
press.
Egghe, L. (2007). Untangling Herdan’s
law and Heaps’ law: Mathematical
and informetric arguments.
JASIST,
58(5), 702–709.
Eisner, J. (1996).
Three new proba-
bilistic models for dependency pars-
ing: An exploration. In COLING-96,
340–345.
Eisner, J. (2002).
An interactive
spreadsheet for teaching the forward-
backward algorithm. In Proceedings
of the ACL Workshop on Effective
Tools and Methodologies for Teaching
NLP and CL, 10–18.
Ejerhed, E. I. (1988). Finding clauses
in unrestricted text by ﬁnitary and
stochastic methods.
In ANLP 1988,
219–227.
Ekman, P. (1999). Basic emotions. In
Dalgleish, T. and Power, M. J. (Eds.),
Handbook of Cognition and Emotion,
45–60. Wiley.
Elman, J. L. (1990).
Finding struc-
ture in time. Cognitive science, 14(2),
179–211.
Elsner, M., Austerweil, J., and Char-
niak, E. (2007). A uniﬁed local and
global model for discourse coherence.
In NAACL-HLT 07, 436–443.
Elsner, M. and Charniak, E. (2008).
Coreference-inspired coherence mod-
eling. In ACL-08, 41–44.
Elsner, M. and Charniak, E. (2011).
Extending the entity grid with entity-
speciﬁc features. In ACL 2011, 125–
129.
Elvev˚ag,
B.,
Foltz,
P. W.,
Wein-
berger, D. R., and Goldberg, T. E.
(2007).
Quantifying incoherence in
speech:
an automated methodology
and novel application to schizophre-
nia. Schizophrenia research, 93(1-3),
304–316.
Emami, A., Trichelair, P., Trischler, A.,
Suleman, K., Schulz, H., and Cheung,
J. C. K. (2019). The KNOWREF coref-
erence corpus: Removing gender and
number cues for difﬁcult pronominal
anaphora resolution. In ACL 2019.
Erk, K. (2007). A simple, similarity-
based model for selectional prefer-
ences. In ACL-07, 216–223.
Etzioni, O., Cafarella, M., Downey, D.,
Popescu, A.-M., Shaked, T., Soder-
land, S., Weld, D. S., and Yates, A.
(2005).
Unsupervised named-entity
extraction from the web: An experi-
mental study.
Artiﬁcial Intelligence,
165(1), 91–134.
Evans, N. (2000).
Word classes in
the world’s languages. In Booij, G.,
Lehmann, C., and Mugdan, J. (Eds.),
Morphology: A Handbook on Inﬂec-
tion and Word Formation, 708–732.
Mouton.
Fader, A., Soderland, S., and Et-
zioni, O. (2011). Identifying relations
for open information extraction.
In
EMNLP-11, 1535–1545.
Fader, A., Zettlemoyer, L., and Etzioni,
O. (2013). Paraphrase-driven learning
for open question answering. In ACL
2013, 1608–1618.
Fano, R. M. (1961). Transmission of
Information: A Statistical Theory of
Communications. MIT Press.
Fant, G. M. (1960). Acoustic Theory of
Speech Production. Mouton.
Fant, G. M. (2004). Speech Acoustics
and Phonetics. Kluwer.
Faruqui, M., Dodge, J., Jauhar, S. K.,
Dyer, C., Hovy, E., and Smith, N. A.
(2015). Retroﬁtting word vectors to
semantic lexicons.
In NAACL HLT
2015, 1606–1615.
Fast, E., Chen, B., and Bernstein, M. S.
(2016). Empath: Understanding Topic
Signals in Large-Scale Text. In CHI.
Fauconnier, G. and Turner, M. (2008).
The way we think: Conceptual blend-
ing and the mind’s hidden complexi-
ties. Basic Books.
Fazel-Zarandi, M., Li, S.-W., Cao, J.,
Casale, J., Henderson, P., Whitney, D.,
and Geramifard, A. (2017). Learning
robust dialog policies in noisy envi-
ronments. In Conversational AI Work-
shop (NIPS).
Feldman, J. A. and Ballard, D. H.
(1982).
Connectionist models and
their properties. Cognitive Science, 6,
205–254.
Fellbaum, C. (Ed.). (1998). WordNet:
An Electronic Lexical Database. MIT
Press.
Feng, V. W. and Hirst, G. (2011). Clas-
sifying arguments by scheme. In ACL
2011, 987–996.

572
Bibliography
Feng, V. W. and Hirst, G. (2014).
A linear-time bottom-up discourse
parser with constraints and post-
editing. In ACL 2014, 511–521.
Feng, V. W., Lin, Z., and Hirst, G.
(2014). The impact of deep hierarchi-
cal discourse structures in the evalua-
tion of text coherence. In COLING-
14, 940–949.
Fensel, D., Hendler, J. A., Lieberman,
H., and Wahlster, W. (Eds.). (2003).
Spinning the Semantic Web: Bring the
World Wide Web to its Full Potential.
MIT Press, Cambridge, MA.
Fernandes, E. R., dos Santos, C. N.,
and Milidi´u, R. L. (2012).
Latent
structure perceptron with feature in-
duction for unrestricted coreference
resolution. In CoNLL-12, 41–48.
Ferro, L., Gerber, L., Mani, I., Sund-
heim, B., and Wilson, G. (2005).
Tides 2005 standard for the annotation
of temporal expressions. Tech. rep.,
MITRE.
Ferrucci, D. A. (2012).
Introduction
to “This is Watson”. IBM Journal of
Research and Development, 56(3/4),
1:1–1:15.
Fessler, L. (2017).
We tested bots
like Siri and Alexa to see who would
stand up to sexual harassment.
In
Quartz. Feb 22, 2017. https://qz.
com/911681/.
Field, A. and Tsvetkov, Y. (2019).
Entity-centric
contextual
affective
analysis. In ACL 2019, 2550–2560.
Fikes, R. E. and Nilsson, N. J. (1971).
STRIPS: A new approach to the ap-
plication of theorem proving to prob-
lem solving. Artiﬁcial Intelligence, 2,
189–208.
Fillmore, C. J. (1966). A proposal con-
cerning English prepositions. In Din-
neen, F. P. (Ed.), 17th annual Round
Table., Vol. 17 of Monograph Series
on Language and Linguistics, 19–34.
Georgetown University Press.
Fillmore, C. J. (1968).
The case for
case. In Bach, E. W. and Harms, R. T.
(Eds.), Universals in Linguistic The-
ory, 1–88. Holt, Rinehart & Winston.
Fillmore, C. J. (1985). Frames and the
semantics of understanding. Quaderni
di Semantica, VI(2), 222–254.
Fillmore,
C. J. (2003).
Valency
and semantic roles:
the concept of
deep structure case.
In ´Agel, V.,
Eichinger, L. M., Eroms, H. W., Hell-
wig, P., Heringer, H. J., and Lobin,
H. (Eds.), Dependenz und Valenz: Ein
internationales Handbuch der zeit-
gen¨ossischen Forschung, chap. 36,
457–475. Walter de Gruyter.
Fillmore, C. J. (2012). Encounters with
language. Computational Linguistics,
38(4), 701–718.
Fillmore, C. J. and Baker, C. F. (2009).
A frames approach to semantic anal-
ysis.
In Heine, B. and Narrog, H.
(Eds.), The Oxford Handbook of Lin-
guistic Analysis,
313–340. Oxford
University Press.
Fillmore, C. J., Johnson, C. R., and
Petruck, M. R. L. (2003). Background
to FrameNet. International journal of
lexicography, 16(3), 235–250.
Finkelstein, L., Gabrilovich, E., Ma-
tias, Y., Rivlin, E., Solan, Z., Wolf-
man, G., and Ruppin, E. (2002). Plac-
ing search in context: The concept re-
visited. ACM Transactions on Infor-
mation Systems, 20(1), 116––131.
Finlayson, M. A. (2016).
Inferring
propp’s functions from semantically
annotated text. The Journal of Ameri-
can Folklore, 129(511), 55–77.
Firth, J. R. (1935). The technique of
semantics. Transactions of the philo-
logical society, 34(1), 36–73.
Firth, J. R. (1957). A synopsis of lin-
guistic theory 1930–1955.
In Stud-
ies in Linguistic Analysis. Philological
Society. Reprinted in Palmer, F. (ed.)
1968. Selected Papers of J. R. Firth.
Longman, Harlow.
Fitt,
S. (2002).
Unisyn lexicon.
http://www.cstr.ed.ac.uk/
projects/unisyn/.
Foland, W. and Martin, J. H. (2016).
CU-NLP at semeval-2016 task 8:
AMR parsing using lstm-based recur-
rent neural networks. In Proceedings
of the 10th International Workshop on
Semantic Evaluation, 1197–1201.
Foland, Jr., W. R. and Martin, J. H.
(2015). Dependency-based semantic
role labeling using convolutional neu-
ral networks. In *SEM 2015), 279–
289.
Foltz, P. W., Kintsch, W., and Landauer,
T. K. (1998).
The measurement of
textual coherence with latent semantic
analysis. Discourse processes, 25(2-
3), 285–307.
Forchini, P. (2013).
Using movie
corpora to explore spoken Ameri-
can English:
Evidence from multi-
dimensional analysis. In Bamford, J.,
Cavalieri, S., and Diani, G. (Eds.),
Variation and Change in Spoken and
Written Discourse: Perspectives from
corpus linguistics,
123–136. Ben-
jamins.
Forney, Jr., G. D. (1973). The Viterbi
algorithm. Proceedings of the IEEE,
61(3), 268–278.
Fox, B. A. (1993). Discourse Structure
and Anaphora: Written and Conver-
sational English. Cambridge.
Francis, H. S., Gregory, M. L., and
Michaelis, L. A. (1999). Are lexical
subjects deviant?. In CLS-99. Univer-
sity of Chicago.
Francis, W. N. and Kuˇcera, H. (1982).
Frequency Analysis of English Usage.
Houghton Mifﬂin, Boston.
Franz, A. (1997).
Independence as-
sumptions considered harmful.
In
ACL/EACL-97, 182–189.
Franz,
A. and Brants,
T. (2006).
All
our
n-gram
are
belong
to
you.
http://googleresearch.
blogspot.com/2006/08/
all-our-n-gram-are-belong-to-you.
html.
Fraser, N. M. and Gilbert, G. N.
(1991).
Simulating speech systems.
Computer Speech and Language, 5,
81–99.
Fry, D. B. (1955). Duration and inten-
sity as physical correlates of linguistic
stress. JASA, 27, 765–768.
Fyshe, A., Wehbe, L., Talukdar, P. P.,
Murphy, B., and Mitchell, T. M.
(2015).
A compositional and inter-
pretable semantic space.
In NAACL
HLT 2015.
Gabow, H. N., Galil, Z., Spencer, T.,
and Tarjan, R. E. (1986). Efﬁcient al-
gorithms for ﬁnding minimum span-
ning trees in undirected and directed
graphs.
Combinatorica, 6(2), 109–
122.
Gage, P. (1994). A new algorithm for
data compression. The C Users Jour-
nal, 12(2), 23–38.
Gale, W. A. and Church, K. W. (1994).
What is wrong with adding one?.
In Oostdijk, N. and de Haan, P.
(Eds.), Corpus-Based Research into
Language, 189–198. Rodopi.
Gale, W. A., Church, K. W., and
Yarowsky, D. (1992a). Estimating up-
per and lower bounds on the perfor-
mance of word-sense disambiguation
programs. In ACL-92, 249–256.
Gale, W. A., Church, K. W., and
Yarowsky, D. (1992b).
One sense
per discourse. In Proceedings DARPA
Speech and Natural Language Work-
shop, 233–237.
Gale, W. A., Church, K. W., and
Yarowsky, D. (1992c). Work on sta-
tistical methods for word sense disam-
biguation. In Goldman, R. (Ed.), Pro-
ceedings of the 1992 AAAI Fall Sym-
posium on Probabilistic Approaches
to Natural Language.
Gao, S., Sethi, A., Aggarwal, S.,
Chung,
T.,
and Hakkani-T¨ur,
D.
(2019). Dialog state tracking: A neu-
ral reading comprehension approach.
arXiv preprint arXiv:1908.01946.
Garg, N., Schiebinger, L., Jurafsky, D.,
and Zou, J. (2018).
Word embed-
dings quantify 100 years of gender
and ethnic stereotypes.
Proceedings
of the National Academy of Sciences,
115(16), E3635–E3644.

Bibliography
573
Garside, R. (1987). The CLAWS word-
tagging system. In Garside, R., Leech,
G., and Sampson, G. (Eds.), The Com-
putational Analysis of English,
30–
41. Longman.
Garside, R., Leech, G., and McEnery,
A. (1997). Corpus Annotation. Long-
man.
Gazdar, G., Klein, E., Pullum, G. K.,
and Sag, I. A. (1985).
Generalized
Phrase Structure Grammar.
Black-
well.
Gerber, M. and Chai, J. Y. (2010). Be-
yond nombank: A study of implicit
arguments for nominal predicates. In
ACL 2010, 1583–1592.
Gil, D. (2000).
Syntactic categories,
cross-linguistic variation and univer-
sal grammar.
In Vogel, P. M. and
Comrie, B. (Eds.), Approaches to the
Typology of Word Classes, 173–216.
Mouton.
Gildea, D. and Jurafsky, D. (2000). Au-
tomatic labeling of semantic roles. In
ACL-00, 512–520.
Gildea, D. and Jurafsky, D. (2002).
Automatic labeling of semantic roles.
Computational
Linguistics,
28(3),
245–288.
Gildea, D. and Palmer, M. (2002).
The necessity of syntactic parsing for
predicate argument recognition.
In
ACL-02.
Giles,
C. L.,
Kuhn,
G. M.,
and
Williams, R. J. (1994). Dynamic re-
current neural networks: Theory and
applications.
IEEE Trans. Neural
Netw. Learning Syst., 5(2), 153–156.
Gillick, L. and Cox, S. J. (1989). Some
statistical issues in the comparison of
speech recognition algorithms.
In
ICASSP-89, 532–535.
Ginzburg, J. and Sag, I. A. (2000). In-
terrogative Investigations: the Form,
Meaning and Use of English Interrog-
atives. CSLI.
Girard, G. (1718).
La justesse de la
lange franc¸oise: ou les differentes sig-
niﬁcations des mots qui passent pour
synonymes.
Giuliano, V. E. (1965). The interpreta-
tion of word associations. In Stevens,
M. E., Giuliano, V. E., and Heil-
prin, L. B. (Eds.), Statistical Associ-
ation Methods For Mechanized Doc-
umentation. Symposium Proceedings.
Washington, D.C., USA, March 17,
1964,
25–32.
https://nvlpubs.
nist.gov/nistpubs/Legacy/MP/
nbsmiscellaneouspub269.pdf.
Giv´on, T. (1990). Syntax: A Functional
Typological Introduction. John Ben-
jamins.
Glennie, A. (1960). On the syntax ma-
chine and the construction of a univer-
sal compiler. Tech. rep. No. 2, Contr.
NR 049-141, Carnegie Mellon Uni-
versity (at the time Carnegie Institute
of Technology), Pittsburgh, PA.
Godfrey, J., Holliman, E., and Mc-
Daniel, J. (1992). SWITCHBOARD:
Telephone speech corpus for research
and development.
In ICASSP-92,
517–520.
Goffman, E. (1974). Frame analysis:
An essay on the organization of expe-
rience. Harvard University Press.
Gold,
B. and Morgan,
N. (1999).
Speech and Audio Signal Processing.
Wiley Press.
Goldberg,
J.,
Ostendorf,
M.,
and
Kirchhoff, K. (2003). The impact of
response wording in error correction
subdialogs. In ISCA Tutorial and Re-
search Workshop on Error Handling
in Spoken Dialogue Systems.
Goldberg, Y. (2017).
Neural Net-
work Methods for Natural Language
Processing, Vol. 10 of Synthesis Lec-
tures on Human Language Technolo-
gies. Morgan & Claypool.
Gondek, D. C., Lally, A., Kalyanpur,
A., Murdock, J. W., Dubou´e, P. A.,
Zhang, L., Pan, Y., Qiu, Z. M., and
Welty, C. (2012).
A framework for
merging and ranking of answers in
deepqa. IBM Journal of Research and
Development, 56(3/4), 14:1–14:12.
Gonen, H. and Goldberg, Y. (2019).
Lipstick on a pig: Debiasing methods
cover up systematic gender biases in
word embeddings but do not remove
them. In NAACL HLT 2019.
Good, M. D., Whiteside, J. A., Wixon,
D. R., and Jones, S. J. (1984). Build-
ing a user-derived interface. CACM,
27(10), 1032–1043.
Goodfellow,
I.,
Bengio,
Y.,
and
Courville, A. (2016). Deep Learning.
MIT Press.
Goodman, J. (2006). A bit of progress
in language modeling: Extended ver-
sion.
Tech. rep. MSR-TR-2001-72,
Machine Learning and Applied Statis-
tics Group, Microsoft Research, Red-
mond, WA.
Goodwin, C. (1996). Transparent vi-
sion. In Ochs, E., Schegloff, E. A.,
and Thompson, S. A. (Eds.), Interac-
tion and Grammar,
370–404. Cam-
bridge University Press.
Gopalakrishnan, K., Hedayatnia, B.,
Chen,
Q.,
Gottardi,
A.,
Kwatra,
S., Venkatesh, A., Gabriel, R., and
Hakkani-T¨ur, D. (2019). Topical-chat:
Towards knowledge-grounded open-
domain conversations..
Gould, J. D., Conti, J., and Hov-
anyecz, T. (1983). Composing letters
with a simulated listening typewriter.
CACM, 26(4), 295–308.
Gould, J. D. and Lewis, C. (1985). De-
signing for usability: Key principles
and what designers think.
CACM,
28(3), 300–311.
Gould, S. J. (1980).
The Panda’s
Thumb. Penguin Group.
Gravano,
A.,
Hirschberg,
J.,
and
Beˇnuˇs, ˇS. (2012).
Afﬁrmative cue
words in task-oriented dialogue. Com-
putational Linguistics, 38(1), 1–39.
Graves, A. (2013).
Generating se-
quences with recurrent neural net-
works..
Graves, A., Fern´andez, S., Gomez, F.,
and Schmidhuber, J. (2006).
Con-
nectionist temporal classiﬁcation: La-
belling unsegmented sequence data
with recurrent neural networks.
In
Proceedings of the 23rd International
Conference on Machine Learning,
ICML ’06, 369–376.
Graves, A., Fern´andez, S., Liwicki,
M., Bunke, H., and Schmidhuber,
J. (2007).
Unconstrained on-line
handwriting recognition with recur-
rent neural networks. In NIPS 2007,
577–584.
Graves, A., Mohamed, A., and Hin-
ton, G. E. (2013).
Speech recogni-
tion with deep recurrent neural net-
works. In IEEE International Confer-
ence on Acoustics, Speech and Signal
Processing, ICASSP, 6645–6649.
Graves, A. and Schmidhuber, J. (2005).
Framewise
phoneme
classiﬁcation
with bidirectional LSTM and other
neural network architectures. Neural
Networks, 18(5-6), 602–610.
Green, B. F., Wolf, A. K., Chomsky,
C., and Laughery, K. (1961). Base-
ball: An automatic question answerer.
In Proceedings of the Western Joint
Computer Conference 19,
219–224.
Reprinted in Grosz et al. (1986).
Greenberg, S., Ellis, D., and Hollen-
back, J. (1996). Insights into spoken
language gleaned from phonetic tran-
scription of the Switchboard corpus.
In ICSLP-96, S24–27.
Greene, B. B. and Rubin, G. M. (1971).
Automatic grammatical tagging of
English.
Department of Linguistics,
Brown University, Providence, Rhode
Island.
Greenwald, A. G., McGhee, D. E., and
Schwartz, J. L. K. (1998). Measuring
individual differences in implicit cog-
nition: the implicit association test..
Journal of personality and social psy-
chology, 74(6), 1464–1480.
Grenager, T. and Manning, C. D.
(2006). Unsupervised Discovery of a
Statistical Verb Lexicon. In EMNLP
2006.
Grice, H. P. (1975). Logic and conver-
sation. In Cole, P. and Morgan, J. L.

574
Bibliography
(Eds.), Speech Acts: Syntax and Se-
mantics Volume 3, 41–58. Academic
Press.
Grice, H. P. (1978). Further notes on
logic and conversation.
In Cole, P.
(Ed.), Pragmatics:
Syntax and Se-
mantics Volume 9,
113–127. Aca-
demic Press.
Grishman,
R.
and
Sundheim,
B.
(1995). Design of the MUC-6 eval-
uation. In MUC-6, 1–11.
Grosz, B. J. (1977a). The representa-
tion and use of focus in a system for
understanding dialogs. In IJCAI-77,
67–76. Morgan Kaufmann. Reprinted
in Grosz et al. (1986).
Grosz, B. J. (1977b). The Representa-
tion and Use of Focus in Dialogue Un-
derstanding. Ph.D. thesis, University
of California, Berkeley.
Grosz, B. J., Joshi, A. K., and We-
instein, S. (1983).
Providing a uni-
ﬁed account of deﬁnite noun phrases
in English. In ACL-83, 44–50.
Grosz, B. J., Joshi, A. K., and Wein-
stein, S. (1995). Centering: A frame-
work for modeling the local coherence
of discourse. Computational Linguis-
tics, 21(2), 203–225.
Grosz, B. J. and Sidner, C. L. (1980).
Plans for discourse. In Cohen, P. R.,
Morgan, J., and Pollack, M. E. (Eds.),
Intentions in Communication,
417–
444. MIT Press.
Gruber, J. S. (1965). Studies in Lexical
Relations. Ph.D. thesis, MIT.
Guinaudeau, C. and Strube, M. (2013).
Graph-based local coherence model-
ing. In ACL 2013, 93–103.
Guindon, R. (1988).
A multidisci-
plinary perspective on dialogue struc-
ture in user-advisor dialogues.
In
Guindon, R. (Ed.), Cognitive Sci-
ence and Its Applications for Human-
Computer
Interaction,
163–200.
Lawrence Erlbaum.
Gundel,
J. K.,
Hedberg,
N.,
and
Zacharski, R. (1993). Cognitive status
and the form of referring expressions
in discourse. Language, 69(2), 274–
307.
Gusﬁeld, D. (1997).
Algorithms on
Strings, Trees, and Sequences: Com-
puter Science and Computational Bi-
ology. Cambridge University Press.
Guyon, I. and Elisseeff, A. (2003). An
introduction to variable and feature
selection. JMLR, 3, 1157–1182.
Habernal, I. and Gurevych, I. (2016).
Which argument is more convincing?
Analyzing and predicting convincing-
ness of Web arguments using bidirec-
tional LSTM. In ACL 2016, 1589–
1599.
Habernal, I. and Gurevych, I. (2017).
Argumentation
mining
in
user-
generated web discourse.
Compu-
tational Linguistics, 43(1), 125–179.
Haghighi, A. and Klein, D. (2009).
Simple coreference resolution with
rich syntactic and semantic features.
In EMNLP-09, 1152–1161.
Hajishirzi, H., Zilles, L., Weld, D. S.,
and Zettlemoyer, L. (2013).
Joint
coreference resolution and named-
entity linking with multi-pass sieves.
In EMNLP 2013, 289–299.
Hajiˇc, J. (1998).
Building a Syn-
tactically Annotated Corpus:
The
Prague Dependency Treebank, 106–
132. Karolinum.
Hajiˇc, J. (2000).
Morphological tag-
ging: Data vs. dictionaries. In NAACL
2000. Seattle.
Hajiˇc, J., Ciaramita, M., Johansson, R.,
Kawahara, D., Mart´ı, M. A., M`arquez,
L., Meyers, A., Nivre, J., Pad´o, S.,
ˇStˇep´anek, J., Stranˇa´k, P., Surdeanu,
M., Xue, N., and Zhang, Y. (2009).
The conll-2009 shared task: Syntactic
and semantic dependencies in multi-
ple languages. In CoNLL-09, 1–18.
Hakkani-T¨ur, D., Oﬂazer, K., and T¨ur,
G. (2002). Statistical morphological
disambiguation for agglutinative lan-
guages.
Journal of Computers and
Humanities, 36(4), 381–410.
Halliday, M. A. K. and Hasan, R.
(1976). Cohesion in English. Long-
man. English Language Series, Title
No. 9.
Hamilton, W. L., Clark, K., Leskovec,
J., and Jurafsky, D. (2016a).
In-
ducing domain-speciﬁc sentiment lex-
icons from unlabeled corpora.
In
EMNLP 2016.
Hamilton, W. L., Leskovec, J., and Ju-
rafsky, D. (2016b). Diachronic word
embeddings reveal statistical laws of
semantic change. In ACL 2016.
Harabagiu, S., Pasca, M., and Maio-
rano, S. (2000).
Experiments with
open-domain textual question answer-
ing. In COLING-00.
Harris, R. A. (2005). Voice Interaction
Design: Crafting the New Conversa-
tional Speech Systems. Morgan Kauf-
mann.
Harris, Z. S. (1946). From morpheme
to utterance. Language, 22(3), 161–
183.
Harris,
Z. S. (1954).
Distribu-
tional structure. Word, 10, 146–162.
Reprinted in J. Fodor and J. Katz, The
Structure of Language, Prentice Hall,
1964 and in Z. S. Harris, Papers in
Structural and Transformational Lin-
guistics, Reidel, 1970, 775–794.
Harris, Z. S. (1962). String Analysis
of Sentence Structure. Mouton, The
Hague.
Hastie, T., Tibshirani, R. J., and Fried-
man, J. H. (2001). The Elements of
Statistical Learning. Springer.
Hatzivassiloglou, V. and McKeown, K.
(1997). Predicting the semantic orien-
tation of adjectives. In ACL/EACL-97,
174–181.
Hatzivassiloglou, V. and Wiebe, J.
(2000). Effects of adjective orienta-
tion and gradability on sentence sub-
jectivity. In COLING-00, 299–305.
Haviland, S. E. and Clark, H. H.
(1974). What’s new? Acquiring new
information as a process in compre-
hension. Journal of Verbal Learning
and Verbal Behaviour, 13, 512–521.
Hawkins, J. A. (1978).
Deﬁniteness
and indeﬁniteness: a study in refer-
ence and grammaticality prediction.
Croom Helm Ltd.
He, L., Lee, K., Lewis, M., and Zettle-
moyer, L. (2017). Deep semantic role
labeling: What works and what’s next.
In ACL 2017, 473–483.
Heaﬁeld, K. (2011).
KenLM: Faster
and smaller language model queries.
In Workshop on Statistical Machine
Translation, 187–197.
Heaﬁeld, K., Pouzyrevsky, I., Clark,
J. H., and Koehn, P. (2013). Scalable
modiﬁed Kneser-Ney language model
estimation.. In ACL 2013, 690–696.
Heaps, H. S. (1978). Information re-
trieval. Computational and theoreti-
cal aspects. Academic Press.
Hearst, M. A. (1991).
Noun homo-
graph disambiguation. In Proceedings
of the 7th Conference of the University
of Waterloo Centre for the New OED
and Text Research, 1–19.
Hearst, M. A. (1992a). Automatic ac-
quisition of hyponyms from large text
corpora. In COLING-92.
Hearst, M. A. (1992b). Automatic ac-
quisition of hyponyms from large text
corpora. In COLING-92. COLING.
Hearst, M. A. (1997).
Texttiling:
Segmenting text into multi-paragraph
subtopic passages.
Computational
Linguistics, 23, 33–64.
Hearst, M. A. (1998). Automatic dis-
covery of WordNet relations. In Fell-
baum, C. (Ed.), WordNet: An Elec-
tronic Lexical Database. MIT Press.
Heckerman, D., Horvitz, E., Sahami,
M., and Dumais, S. T. (1998).
A
bayesian approach to ﬁltering junk e-
mail. In Proceeding of AAAI-98 Work-
shop on Learning for Text Categoriza-
tion, 55–62.
Heim, I. (1982).
The semantics of
deﬁnite and indeﬁnite noun phrases.
Ph.D. thesis,
University of Mas-
sachusetts at Amherst.
Heim, I. and Kratzer, A. (1998).
Se-
mantics in a Generative Grammar.
Blackwell Publishers, Malden, MA.

Bibliography
575
Heinz, J. M. and Stevens, K. N. (1961).
On the properties of voiceless fricative
consonants. JASA, 33, 589–596.
Hellrich, J., Buechel, S., and Hahn,
U. (2019).
Modeling word emotion
in historical language: Quantity beats
supposed stability in seed word se-
lection.
In Proceedings of the 3rd
Joint SIGHUM Workshop on Compu-
tational Linguistics for Cultural Her-
itage, Social Sciences, Humanities
and Literature, 1–11.
Hemphill, C. T., Godfrey, J., and Dod-
dington, G. (1990). The ATIS spoken
language systems pilot corpus. In Pro-
ceedings DARPA Speech and Natural
Language Workshop, 96–101.
Henderson, P., Sinha, K., Angelard-
Gontier, N., Ke, N. R., Fried, G.,
Lowe, R., and Pineau, J. (2017). Ethi-
cal challenges in data-driven dialogue
systems. In AAAI/ACM AI Ethics and
Society Conference.
Hendrickx, I., Kim, S. N., Kozareva,
Z., Nakov, P., ´O S´eaghdha, D., Pad´o,
S., Pennacchiotti, M., Romano, L.,
and Szpakowicz, S. (2009). Semeval-
2010 task 8: Multi-way classiﬁcation
of semantic relations between pairs of
nominals. In Proceedings of the Work-
shop on Semantic Evaluations: Re-
cent Achievements and Future Direc-
tions, 94–99.
Hendrix, G. G., Thompson, C. W., and
Slocum, J. (1973). Language process-
ing via canonical verbs and semantic
models. In Proceedings of IJCAI-73.
Henrich, V., Hinrichs, E., and Vodola-
zova, T. (2012). WebCAGe – a web-
harvested corpus annotated with Ger-
maNet senses. In EACL-12, 387–396.
Herdan, G. (1960). Type-token mathe-
matics. The Hague, Mouton.
Hermann, K. M., Kocisky, T., Grefen-
stette, E., Espeholt, L., Kay, W., Su-
leyman, M., and Blunsom, P. (2015).
Teaching machines to read and com-
prehend. In Advances in Neural In-
formation Processing Systems, 1693–
1701.
Hernault, H., Prendinger, H., duVerle,
D. A., and Ishizuka, M. (2010). Hilda:
A discourse parser using support vec-
tor machine classiﬁcation. Dialogue
& Discourse, 1(3).
Hidey, C., Musi, E., Hwang, A., Mure-
san, S., and McKeown, K. (2017). An-
alyzing the semantic types of claims
and premises in an online persuasive
forum.
In Proceedings of the 4th
Workshop on Argument Mining, 11–
21.
Hill, F., Reichart, R., and Korhonen,
A. (2015).
Simlex-999: Evaluating
semantic models with (genuine) simi-
larity estimation. Computational Lin-
guistics, 41(4), 665–695.
Hindle, D. and Rooth, M. (1990).
Structural ambiguity and lexical re-
lations.
In Proceedings DARPA
Speech and Natural Language Work-
shop, 257–262.
Hindle, D. and Rooth, M. (1991).
Structural ambiguity and lexical rela-
tions. In ACL-91, 229–236.
Hinkelman, E. A. and Allen, J. (1989).
Two constraints on speech act ambi-
guity. In ACL-89, 212–219.
Hinton, G. E. (1986).
Learning dis-
tributed representations of concepts.
In COGSCI-86, 1–12.
Hinton, G. E., Osindero, S., and Teh,
Y.-W. (2006).
A fast learning algo-
rithm for deep belief nets.
Neural
computation, 18(7), 1527–1554.
Hinton,
G.
E.,
Srivastava,
N.,
Krizhevsky, A., Sutskever, I., and
Salakhutdinov, R. R. (2012). Improv-
ing neural networks by preventing
co-adaptation of feature detectors.
arXiv preprint arXiv:1207.0580.
Hirschberg, J., Litman, D. J., and
Swerts, M. (2001).
Identifying user
corrections automatically in spoken
dialogue systems. In NAACL 2001.
Hirschberg, J. and Pierrehumbert, J. B.
(1986).
The intonational structuring
of discourse. In ACL-86, 136–144.
Hirschman, L., Light, M., Breck, E.,
and Burger, J. D. (1999). Deep Read:
A reading comprehension system. In
ACL-99, 325–332.
Hirschman, L. and Pao, C. (1993). The
cost of errors in a spoken language
system. In EUROSPEECH-93, 1419–
1422.
Hirst, G. (1981). Anaphora in Natural
Language Understanding: A survey.
No. 119 in Lecture notes in computer
science. Springer-Verlag.
Hirst, G. (1987). Semantic Interpreta-
tion and the Resolution of Ambiguity.
Cambridge University Press.
Hirst, G. (1988). Resolving lexical am-
biguity computationally with spread-
ing activation and polaroid words. In
Small, S. L., Cottrell, G. W., and
Tanenhaus, M. K. (Eds.), Lexical Am-
biguity Resolution, 73–108. Morgan
Kaufmann.
Hirst, G. and Charniak, E. (1982).
Word sense and case slot disambigua-
tion. In AAAI-82, 95–98.
Hjelmslev, L. (1969). Prologomena to
a Theory of Language. University of
Wisconsin Press. Translated by Fran-
cis J. Whitﬁeld; original Danish edi-
tion 1943.
Hobbs, J. R. (1978).
Resolving pro-
noun references.
Lingua, 44, 311–
338. Reprinted in Grosz et al. (1986).
Hobbs, J. R. (1979).
Coherence and
coreference. Cognitive Science, 3, 67–
90.
Hobbs, J. R., Appelt, D. E., Bear, J.,
Israel, D., Kameyama, M., Stickel,
M. E., and Tyson, M. (1997). FAS-
TUS: A cascaded ﬁnite-state trans-
ducer for extracting information from
natural-language text.
In Roche, E.
and Schabes, Y. (Eds.), Finite-State
Language Processing, 383–406. MIT
Press.
Hochreiter, S. and Schmidhuber, J.
(1997).
Long short-term memory.
Neural Computation,
9(8),
1735–
1780.
Hockenmaier, J. and Steedman, M.
(2007).
Ccgbank:
a corpus of
ccg derivations and dependency struc-
tures extracted from the penn tree-
bank.
Computational Linguistics,
33(3), 355–396.
Hofmann, T. (1999). Probabilistic la-
tent semantic indexing. In SIGIR-99.
Hopcroft, J. E. and Ullman, J. D.
(1979). Introduction to Automata The-
ory, Languages, and Computation.
Addison-Wesley.
Hou, Y., Markert, K., and Strube, M.
(2018).
Unrestricted bridging res-
olution.
Computational Linguistics,
44(2), 237–284.
Householder, F. W. (1995). Dionysius
Thrax, the technai, and Sextus Empir-
icus. In Koerner, E. F. K. and Asher,
R. E. (Eds.), Concise History of the
Language Sciences, 99–103. Elsevier
Science.
Hovy, E. H. (1990). Parsimonious and
proﬂigate approaches to the question
of discourse structure relations.
In
Proceedings of the 5th International
Workshop on Natural Language Gen-
eration, 128–136.
Hovy, E. H., Hermjakob, U., and
Ravichandran, D. (2002).
A ques-
tion/answer typology with surface text
patterns. In HLT-01.
Hovy, E. H., Marcus, M. P., Palmer, M.,
Ramshaw, L. A., and Weischedel, R.
(2006). Ontonotes: The 90% solution.
In HLT-NAACL-06.
Hu, M. and Liu, B. (2004a). Mining
and summarizing customer reviews.
In KDD, 168–177.
Hu, M. and Liu, B. (2004b). Mining
and summarizing customer reviews.
In SIGKDD-04.
Huang, E. H., Socher, R., Manning,
C. D., and Ng, A. Y. (2012). Improv-
ing word representations via global
context and multiple word prototypes.
In ACL 2012, 873–882.
Huang, L. and Sagae, K. (2010). Dy-
namic programming for linear-time
incremental parsing.
In ACL 2010,
1077–1086.
Huang, X., Acero, A., and Hon, H.-W.
(2001). Spoken Language Processing:
A Guide to Theory, Algorithm, and
System Development. Prentice Hall.

576
Bibliography
Huang, Z., Xu, W., and Yu, K. (2015).
Bidirectional LSTM-CRF models for
sequence tagging.
In arXiv preprint
arXiv:1508.01991.
Huddleston, R. and Pullum, G. K.
(2002). The Cambridge Grammar of
the English Language.
Cambridge
University Press.
Hudson, R. A. (1984). Word Grammar.
Blackwell.
Huffman, S. (1996).
Learning infor-
mation extraction patterns from ex-
amples.
In Wertmer, S., Riloff, E.,
and Scheller, G. (Eds.), Connectionist,
Statistical, and Symbolic Approaches
to Learning Natural Language Pro-
cessing, 246–260. Springer.
Hutto, C. J., Folds, D., and Appling,
S. (2015). Computationally detecting
and quantifying the degree of bias in
sentence-level text of news stories. In
HUSO 2015: The First International
Conference on Human and Social An-
alytics.
Hymes, D. (1974). Ways of speaking.
In Bauman, R. and Sherzer, J. (Eds.),
Explorations in the ethnography of
speaking, 433–451. Cambridge Uni-
versity Press.
Iacobacci, I., Pilehvar, M. T., and Nav-
igli, R. (2016). Embeddings for word
sense disambiguation: An evaluation
study. In ACL 2016, 897–907.
Iida, R., Inui, K., Takamura, H., and
Matsumoto, Y. (2003).
Incorporat-
ing contextual cues in trainable mod-
els for coreference resolution.
In
EACL Workshop on The Computa-
tional Treatment of Anaphora.
Irons, E. T. (1961). A syntax directed
compiler for ALGOL 60. CACM, 4,
51–55.
Irsoy, O. and Cardie, C. (2014). Opin-
ion mining with deep recurrent neural
networks. In EMNLP 2014, 720–728.
Isbell, C. L., Kearns, M., Kormann, D.,
Singh, S., and Stone, P. (2000). Cobot
in LambdaMOO: A social statistics
agent. In AAAI/IAAI, 36–41.
ISO8601 (2004).
Data elements and
interchange
formats—information
interchange—representation of dates
and times.
Tech. rep., International
Organization for Standards (ISO).
Jackendoff, R. (1983). Semantics and
Cognition. MIT Press.
Jacobs, P. S. and Rau, L. F. (1990).
SCISOR: A system for extracting in-
formation from on-line news. CACM,
33(11), 88–97.
Jaech, A., Mulcaire, G., Hathi, S., Os-
tendorf, M., and Smith, N. A. (2016).
Hierarchical character-word models
for language identiﬁcation.
In ACL
Workshop on NLP for Social Media,
84––93.
Jafarpour, S., Burges, C. J. C., and
Ritter, A. (2009).
Filter, rank, and
transfer the knowledge: Learning to
chat. In NIPS Workshop on Advances
in Ranking.
Jauhiainen, T., Lui, M., Zampieri, M.,
Baldwin, T., and Lind´en, K. (2018).
Automatic language identiﬁcation in
texts:
A survey.
arXiv preprint
arXiv:1804.08186.
Jefferson, G. (1972). Side sequences.
In Sudnow, D. (Ed.), Studies in so-
cial interaction, 294–333. Free Press,
New York.
Jeffreys, H. (1948). Theory of Proba-
bility (2nd Ed.). Clarendon Press. Sec-
tion 3.23.
Jelinek, F. (1976). Continuous speech
recognition by statistical methods.
Proceedings of the IEEE, 64(4), 532–
557.
Jelinek, F. (1990). Self-organized lan-
guage modeling for speech recogni-
tion.
In Waibel, A. and Lee, K.-F.
(Eds.), Readings in Speech Recogni-
tion,
450–506. Morgan Kaufmann.
Originally distributed as IBM techni-
cal report in 1985.
Jelinek, F. (1997). Statistical Methods
for Speech Recognition. MIT Press.
Jelinek, F. and Lafferty, J. D. (1991).
Computation of the probability of ini-
tial substring generation by stochas-
tic context-free grammars. Computa-
tional Linguistics, 17(3), 315–323.
Jelinek, F., Lafferty, J. D., Magerman,
D. M., Mercer, R. L., Ratnaparkhi,
A., and Roukos, S. (1994). Decision
tree parsing using a hidden derivation
model.
In ARPA Human Language
Technologies Workshop, 272–277.
Jelinek, F. and Mercer, R. L. (1980).
Interpolated estimation of Markov
source parameters from sparse data.
In Gelsema, E. S. and Kanal, L. N.
(Eds.),
Proceedings,
Workshop on
Pattern Recognition in Practice, 381–
397. North Holland.
Ji, H. and Grishman, R. (2011). Knowl-
edge base population: Successful ap-
proaches and challenges.
In ACL
2011, 1148–1158.
Ji, H., Grishman, R., and Dang, H. T.
(2010).
Overview of the tac 2011
knowledge base population track. In
TAC-11.
Ji, Y. and Eisenstein, J. (2014). Rep-
resentation learning for text-level dis-
course parsing. In ACL 2014, 13–24.
Ji,
Y.
and
Eisenstein,
J.
(2015).
One vector is not enough:
Entity-
augmented distributed semantics for
discourse relations.
TACL, 3, 329–
344.
Johnson, K. (2003). Acoustic and Audi-
tory Phonetics (2nd Ed.). Blackwell.
Johnson, M. (1998). PCFG models of
linguistic tree representations. Com-
putational Linguistics, 24(4), 613–
632.
Johnson, W. E. (1932).
Probability:
deductive and inductive problems (ap-
pendix to). Mind, 41(164), 421–423.
Johnson-Laird, P. N. (1983).
Men-
tal Models. Harvard University Press,
Cambridge, MA.
Jones, M. P. and Martin, J. H. (1997).
Contextual spelling correction using
latent semantic analysis.
In ANLP
1997, 166–173.
Jones, R., McCallum, A., Nigam, K.,
and Riloff, E. (1999). Bootstrapping
for text learning tasks.
In IJCAI-99
Workshop on Text Mining: Founda-
tions, Techniques and Applications.
Jones, T. (2015).
Toward a descrip-
tion of African American Vernacular
English dialect regions using “Black
Twitter”.
American Speech, 90(4),
403–440.
Joos, M. (1950).
Description of lan-
guage design. JASA, 22, 701–708.
Jordan, M. (1986). Serial order: A par-
allel distributed processing approach.
Tech. rep. ICS Report 8604, Univer-
sity of California, San Diego.
Joshi, A. K. (1985).
Tree adjoin-
ing grammars: How much context-
sensitivity is required to provide rea-
sonable structural descriptions?.
In
Dowty, D. R., Karttunen, L., and
Zwicky, A. (Eds.), Natural Language
Parsing,
206–250. Cambridge Uni-
versity Press.
Joshi, A. K. and Hopely, P. (1999). A
parser from antiquity. In Kornai, A.
(Ed.), Extended Finite State Models of
Language, 6–15. Cambridge Univer-
sity Press.
Joshi, A. K. and Kuhn, S. (1979). Cen-
tered logic: The role of entity centered
sentence representation in natural lan-
guage inferencing. In IJCAI-79, 435–
439.
Joshi, A. K. and Srinivas, B. (1994).
Disambiguation of super parts of
speech (or supertags): Almost pars-
ing. In COLING-94, 154–160.
Joshi, A. K. and Weinstein, S. (1981).
Control of inference: Role of some as-
pects of discourse structure – center-
ing. In IJCAI-81, 385–387.
Joshi, M., Choi, E., Weld, D. S., and
Zettlemoyer, L. (2017). Triviaqa: A
large scale distantly supervised chal-
lenge dataset for reading comprehen-
sion. In ACL 2017.
Joshi, M., Levy, O., Weld, D. S., and
Zettlemoyer, L. (2019).
BERT for
coreference resolution: Baselines and
analysis. In EMNLP 2019.

Bibliography
577
Joty, S., Carenini, G., and Ng, R. T.
(2015). CODRA: A novel discrimi-
native framework for rhetorical analy-
sis. Computational Linguistics, 41(3),
385–435.
Jun, S.-A. (Ed.). (2005). Prosodic Ty-
pology and Transcription: A Uniﬁed
Approach. Oxford University Press.
Jurafsky, D. (2014). The Language of
Food. W. W. Norton, New York.
Jurafsky, D., Chahuneau, V., Rout-
ledge, B. R., and Smith, N. A. (2014).
Narrative framing of consumer sen-
timent in online restaurant reviews.
First Monday, 19(4).
Jurafsky, D., Wooters, C., Tajchman,
G., Segal, J., Stolcke, A., Fosler, E.,
and Morgan, N. (1994). The Berke-
ley restaurant project. In ICSLP-94,
2139–2142.
Jurgens, D. and Klapaftis, I. P. (2013).
Semeval-2013 task 13: Word sense
induction for graded and non-graded
senses. In *SEM, 290–299.
Jurgens, D., Tsvetkov, Y., and Juraf-
sky, D. (2017). Incorporating dialectal
variability for socially equitable lan-
guage identiﬁcation.
In ACL 2017,
51–57.
Justeson, J. S. and Katz, S. M. (1991).
Co-occurrences of antonymous adjec-
tives and their contexts.
Computa-
tional linguistics, 17(1), 1–19.
Kalchbrenner, N. and Blunsom, P.
(2013).
Recurrent continuous trans-
lation models. In Proceedings of the
2013 Conference on Empirical Meth-
ods in Natural Language Processing.
Association for Computational Lin-
guistics.
Kalyanpur, A., Boguraev, B. K., Pat-
wardhan, S., Murdock, J. W., Lally,
A., Welty, C., Prager, J. M., Coppola,
B., Fokoue-Nkoutche, A., Zhang, L.,
Pan, Y., and Qiu, Z. M. (2012). Struc-
tured data and inference in deepqa.
IBM Journal of Research and Devel-
opment, 56(3/4), 10:1–10:14.
Kameyama, M. (1986).
A property-
sharing constraint in centering.
In
ACL-86, 200–206.
Kamp, H. (1981).
A theory of truth
and semantic representation. In Groe-
nendijk, J., Janssen, T., and Stokhof,
M. (Eds.), Formal Methods in the
Study of Language, 189–222. Math-
ematical Centre, Amsterdam.
Kane, S. K., Morris, M. R., Paradiso,
A., and Campbell, J. (2017). “at times
avuncular and cantankerous, with the
reﬂexes of a mongoose”: Understand-
ing self-expression through augmen-
tative and alternative communication
devices. In CSCW 2017, 1166–1179.
Kannan, A. and Vinyals, O. (2016).
Adversarial evaluation of dialogue
models. In NIPS 2016 Workshop on
Adversarial Training.
Kaplan, R. M. (1973). A general syn-
tactic processor. In Rustin, R. (Ed.),
Natural Language Processing, 193–
241. Algorithmics Press.
Karamanis, N., Poesio, M., Mellish,
C., and Oberlander, J. (2004). Eval-
uating centering-based metrics of co-
herence for text structuring using a re-
liably annotated corpus. In ACL-04.
Karlsson, F., Voutilainen, A., Heikkil¨a,
J., and Anttila, A. (Eds.). (1995).
Constraint Grammar: A Language-
Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter.
Karttunen, L. (1969). Discourse ref-
erents. In COLING-69. Preprint No.
70.
Karttunen, L. (1999). Comments on
Joshi. In Kornai, A. (Ed.), Extended
Finite State Models of Language, 16–
18. Cambridge University Press.
Kasami, T. (1965). An efﬁcient recog-
nition and syntax analysis algorithm
for context-free languages.
Tech.
rep. AFCRL-65-758, Air Force Cam-
bridge Research Laboratory, Bedford,
MA.
Katz, J. J. and Fodor, J. A. (1963). The
structure of a semantic theory. Lan-
guage, 39, 170–210.
Kawamoto, A. H. (1988). Distributed
representations of ambiguous words
and their resolution in connectionist
networks.
In Small, S. L., Cottrell,
G. W., and Tanenhaus, M. (Eds.), Lex-
ical Ambiguity Resolution, 195–228.
Morgan Kaufman.
Kay, M. (1967).
Experiments with a
powerful parser. In Proc. 2eme Con-
ference Internationale sur le Traite-
ment Automatique des Langues.
Kay, M. (1973).
The MIND system.
In Rustin, R. (Ed.), Natural Language
Processing,
155–188. Algorithmics
Press.
Kay, M. (1982). Algorithm schemata
and data structures in syntactic pro-
cessing.
In All´en, S. (Ed.), Text
Processing: Text Analysis and Gen-
eration, Text Typology and Attribu-
tion, 327–358. Almqvist and Wiksell,
Stockholm.
Kay, P. and Fillmore, C. J. (1999).
Grammatical constructions and lin-
guistic generalizations: The What’s X
Doing Y? construction.
Language,
75(1), 1–33.
Kehler, A. (1993).
The effect of es-
tablishing coherence in ellipsis and
anaphora resolution. In ACL-93, 62–
69.
Kehler, A. (1994). Temporal relations:
Reference or discourse coherence?. In
ACL-94, 319–321.
Kehler, A. (1997a). Current theories of
centering for pronoun interpretation:
A critical evaluation. Computational
Linguistics, 23(3), 467–475.
Kehler, A. (1997b). Probabilistic coref-
erence in information extraction.
In
EMNLP 1997, 163–173.
Kehler, A. (2000).
Coherence, Ref-
erence, and the Theory of Grammar.
CSLI Publications.
Kehler, A., Appelt, D. E., Taylor,
L., and Simma, A. (2004).
The
(non)utility of predicate-argument fre-
quencies for pronoun interpretation.
In HLT-NAACL-04.
Kehler, A. and Rohde, H. (2013).
A
probabilistic
reconciliation
of
coherence-driven
and
centering-
driven theories of pronoun interpreta-
tion. Theoretical Linguistics, 39(1-2),
1–37.
Keller, F. and Lapata, M. (2003). Us-
ing the web to obtain frequencies for
unseen bigrams. Computational Lin-
guistics, 29, 459–484.
Kelly, E. F. and Stone, P. J. (1975).
Computer
Recognition
of
English
Word Senses. North-Holland.
Kennedy, C. and Boguraev, B. K.
(1996).
Anaphora for everyone:
Pronominal anaphora resolution with-
out a parser. In COLING-96, 113–
118.
Kiela, D. and Clark, S. (2014). A sys-
tematic study of semantic vector space
model parameters. In Proceedings of
the EACL 2nd Workshop on Contin-
uous Vector Space Models and their
Compositionality (CVSC), 21–30.
Kilgarriff,
A. and Rosenzweig,
J.
(2000). Framework and results for En-
glish SENSEVAL. Computers and the
Humanities, 34, 15–48.
Kim, S. M. and Hovy, E. H. (2004). De-
termining the sentiment of opinions.
In COLING-04.
Kingma, D. and Ba, J. (2015). Adam:
A method for stochastic optimization.
In ICLR 2015.
Kintsch, W. (1974). The Representa-
tion of Meaning in Memory.
Wiley,
New York.
Kintsch, W. and Van Dijk, T. A. (1978).
Toward a model of text comprehen-
sion and production.
Psychological
review, 85(5), 363–394.
Kipper, K., Dang, H. T., and Palmer,
M. (2000). Class-based construction
of a verb lexicon. In AAAI-00, 691–
696.
Kiritchenko, S. and Mohammad, S. M.
(2017). Best-worst scaling more reli-
able than rating scales: A case study
on sentiment intensity annotation. In
ACL 2017, 465–470.
Kiss, T. and Strunk, J. (2006). Unsuper-
vised multilingual sentence boundary
detection. Computational Linguistics,
32(4), 485–525.

578
Bibliography
Kleene, S. C. (1951). Representation
of events in nerve nets and ﬁnite au-
tomata.
Tech. rep. RM-704, RAND
Corporation. RAND Research Mem-
orandum.
Kleene, S. C. (1956). Representation
of events in nerve nets and ﬁnite au-
tomata. In Shannon, C. and McCarthy,
J. (Eds.), Automata Studies,
3–41.
Princeton University Press.
Klein, D. and Manning, C. D. (2001).
Parsing and hypergraphs. In IWPT-01,
123–134.
Klein, D. and Manning, C. D. (2003a).
A* parsing: Fast exact Viterbi parse
selection. In HLT-NAACL-03.
Klein, D. and Manning, C. D. (2003b).
Accurate unlexicalized parsing.
In
HLT-NAACL-03.
Klein, S. and Simmons, R. F. (1963). A
computational approach to grammati-
cal coding of English words. Journal
of the Association for Computing Ma-
chinery, 10(3), 334–347.
Kneser, R. and Ney, H. (1995).
Im-
proved backing-off for M-gram lan-
guage modeling.
In ICASSP-95,
Vol. 1, 181–184.
Knott, A. and Dale, R. (1994).
Us-
ing linguistic phenomena to motivate
a set of coherence relations. Discourse
Processes, 18(1), 35–62.
Kocijan,
V.,
Cretu,
A.-M.,
Cam-
buru,
O.-M.,
Yordanov,
Y.,
and
Lukasiewicz, T. (2019).
A surpris-
ingly robust trick for the Winograd
Schema Challenge. In ACL 2019.
Koˇcisk´y, T., Schwarz, J., Blunsom, P.,
Dyer, C., Hermann, K. M., Melis,
G., and Grefenstette, E. (2018). The
NarrativeQA reading comprehension
challenge. TACL, 6, 317–328.
Koehn, P., Abney, S. P., Hirschberg,
J., and Collins, M. (2000). Improv-
ing intonational phrasing with syntac-
tic information. In ICASSP-00, 1289–
1290.
Koenig, W., Dunn, H. K., and Lacy,
L. Y. (1946). The sound spectrograph.
JASA, 18, 19–49.
Krovetz, R. (1993). Viewing morphol-
ogy as an inference process. In SIGIR-
93, 191–202.
Kruskal, J. B. (1983). An overview of
sequence comparison. In Sankoff, D.
and Kruskal, J. B. (Eds.), Time Warps,
String Edits, and Macromolecules:
The Theory and Practice of Sequence
Comparison, 1–44. Addison-Wesley.
Kudo, T. and Matsumoto, Y. (2002).
Japanese dependency analysis using
cascaded chunking.
In CoNLL-02,
63–69.
Kudo, T. and Richardson, J. (2018).
SentencePiece:
A simple and lan-
guage independent subword tokenizer
and detokenizer for neural text pro-
cessing. In EMNLP 2018, 66–71.
Kullback, S. and Leibler, R. A. (1951).
On information and sufﬁciency. An-
nals of Mathematical Statistics, 22,
79–86.
Kumar, S., Jat, S., Saxena, K., and
Talukdar, P. (2019). Zero-shot word
sense disambiguation using sense def-
inition embeddings.
In ACL 2019,
5670–5681.
Kummerfeld, J. K. and Klein, D.
(2013). Error-driven analysis of chal-
lenges in coreference resolution.
In
EMNLP 2013, 265–277.
Kuno, S. (1965).
The predictive an-
alyzer and a path elimination tech-
nique. CACM, 8(7), 453–462.
Kuno, S. and Oettinger, A. G. (1963).
Multiple-path syntactic analyzer.
In
Popplewell, C. M. (Ed.), Informa-
tion Processing 1962: Proceedings of
the IFIP Congress 1962,
306–312.
North-Holland. Reprinted in Grosz et
al. (1986).
Kupiec, J. (1992).
Robust part-of-
speech tagging using a hidden Markov
model.
Computer Speech and Lan-
guage, 6, 225–242.
Kurita, K., Vyas, N., Pareek, A.,
Black, A. W., and Tsvetkov, Y. (2019).
Quantifying social biases in contex-
tual word representations. In 1st ACL
Workshop on Gender Bias for Natural
Language Processing.
Kuˇcera, H. and Francis, W. N. (1967).
Computational Analysis of Present-
Day American English. Brown Uni-
versity Press, Providence, RI.
Ladd, D. R. (1996).
Intonational
Phonology.
Cambridge
Studies
in Linguistics. Cambridge University
Press.
Ladefoged, P. (1993). A Course in Pho-
netics.
Harcourt Brace Jovanovich.
(3rd ed.).
Ladefoged, P. (1996).
Elements of
Acoustic Phonetics (2nd Ed.). Univer-
sity of Chicago.
Lafferty, J. D., McCallum, A., and
Pereira, F. C. N. (2001). Conditional
random ﬁelds: Probabilistic models
for segmenting and labeling sequence
data. In ICML 2001.
Lafferty, J. D., Sleator, D., and Tem-
perley, D. (1992).
Grammatical tri-
grams: A probabilistic model of link
grammar. In Proceedings of the 1992
AAAI Fall Symposium on Probabilis-
tic Approaches to Natural Language.
Lai, A. and Tetreault, J. (2018). Dis-
course coherence in the wild:
A
dataset, evaluation and methods.
In
SIGDIAL 18, 214–223.
Lakoff, G. (1965).
On the Nature of
Syntactic Irregularity.
Ph.D. thesis,
Indiana University.
Published as Ir-
regularity in Syntax. Holt, Rinehart,
and Winston, New York, 1970.
Lakoff, G. (1972a).
Linguistics and
natural logic.
In Davidson, D. and
Harman, G. (Eds.), Semantics for Nat-
ural Language, 545–665. D. Reidel.
Lakoff, G. (1972b).
Structural com-
plexity in fairy tales.
In The Study
of Man,
128–50. School of So-
cial Sciences, University of Califor-
nia, Irvine, CA.
Lakoff, G. and Johnson, M. (1980).
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Lally, A., Prager, J. M., McCord,
M. C., Boguraev, B. K., Patwardhan,
S., Fan, J., Fodor, P., and Chu-Carroll,
J. (2012).
Question analysis: How
Watson reads a clue. IBM Journal of
Research and Development, 56(3/4),
2:1–2:14.
Lample, G., Ballesteros, M., Subrama-
nian, S., Kawakami, K., and Dyer,
C. (2016a). Neural architectures for
named entity recognition. In NAACL
HLT 2016.
Lample, G., Ballesteros, M., Subrama-
nian, S., Kawakami, K., and Dyer,
C. (2016b). Neural architectures for
named entity recognition. In NAACL
HLT 2016.
Landauer, T. K. (Ed.). (1995).
The
Trouble with Computers: Usefulness,
Usability, and Productivity.
MIT
Press.
Landauer, T. K. and Dumais, S. T.
(1997). A solution to Plato’s problem:
The Latent Semantic Analysis theory
of acquisition, induction, and repre-
sentation of knowledge. Psychologi-
cal Review, 104, 211–240.
Landes, S., Leacock, C., and Tengi,
R. I. (1998).
Building seman-
tic concordances.
In Fellbaum, C.
(Ed.), WordNet: An Electronic Lexical
Database, 199–216. MIT Press.
Lang,
J. and Lapata,
M. (2014).
Similarity-driven semantic role induc-
tion via graph partitioning. Computa-
tional Linguistics, 40(3), 633–669.
Lapata, M. (2003). Probabilistic text
structuring:
Experiments with sen-
tence ordering. In ACL-03, 545–552.
Lapesa, G. and Evert, S. (2014).
A
large scale evaluation of distributional
semantic models: Parameters, interac-
tions and model selection. TACL, 2,
531–545.
Lappin, S. and Leass, H. (1994). An al-
gorithm for pronominal anaphora res-
olution.
Computational Linguistics,
20(4), 535–561.
Lari, K. and Young, S. J. (1990). The
estimation of stochastic context-free
grammars using the Inside-Outside al-
gorithm. Computer Speech and Lan-
guage, 4, 35–56.

Bibliography
579
Lascarides, A. and Asher, N. (1993).
Temporal interpretation, discourse re-
lations, and common sense entail-
ment.
Linguistics and Philosophy,
16(5), 437–493.
Lauscher, A., Vuli´c, I., Ponti, E. M.,
Korhonen, A., and Glavaˇs, G. (2019).
Informing unsupervised pretraining
with external linguistic knowledge.
arXiv preprint arXiv:1909.02339.
LDC (1995).
COMLEX English
Pronunciation Dictionary Version 0.2
(COMLEX 0.2). Linguistic Data Con-
sortium.
LeCun, Y., Boser, B., Denker, J. S.,
Henderson, D., Howard, R. E., Hub-
bard, W., and Jackel, L. D. (1989).
Backpropagation applied to handwrit-
ten zip code recognition. Neural com-
putation, 1(4), 541–551.
LeCun, Y., Boser, B. E., Denker, J. S.,
Henderson, D., Howard, R. E., Hub-
bard, W. E., and Jackel, L. D. (1990).
Handwritten digit recognition with a
back-propagation network.
In NIPS
1990, 396–404.
Lee, D. D. and Seung, H. S. (1999).
Learning the parts of objects by non-
negative matrix factorization. Nature,
401(6755), 788–791.
Lee, H., Chang, A., Peirsman, Y.,
Chambers, N., Surdeanu, M., and Ju-
rafsky, D. (2013). Deterministic coref-
erence resolution based on entity-
centric, precision-ranked rules. Com-
putational Linguistics, 39(4), 885–
916.
Lee, H., Peirsman, Y., Chang, A.,
Chambers, N., Surdeanu, M., and Ju-
rafsky, D. (2011).
Stanford’s multi-
pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task.
In CoNLL-11, 28–34.
Lee, H., Surdeanu, M., and Jurafsky, D.
(2017a).
A scaffolding approach to
coreference resolution integrating sta-
tistical and rule-based models.
Nat-
ural Language Engineering, 23(5),
733–762.
Lee, K., He, L., Lewis, M., and Zettle-
moyer, L. (2017b). End-to-end neu-
ral coreference resolution. In EMNLP
2017.
Lee, K., He, L., and Zettlemoyer, L.
(2018). Higher-order coreference res-
olution with coarse-to-ﬁne inference.
In NAACL HLT 2018.
Lee, K., Salant, S., Kwiatkowski, T.,
Parikh, A., Das, D., and Berant, J.
(2017). Learning recurrent span repre-
sentations for extractive question an-
swering. In arXiv 1611.01436.
Lehiste, I. (Ed.). (1967). Readings in
Acoustic Phonetics. MIT Press.
Lehnert, W. G., Cardie, C., Fisher, D.,
Riloff, E., and Williams, R. (1991).
Description of the CIRCUS system as
used for MUC-3.
In Sundheim, B.
(Ed.), MUC-3, 223–233.
Lemon, O., Georgila, K., Henderson,
J., and Stuttle, M. (2006).
An ISU
dialogue system exhibiting reinforce-
ment learning of dialogue policies:
Generic slot-ﬁlling in the TALK in-
car system. In EACL-06.
Lengerich, B., Maas, A., and Potts, C.
(2018). Retroﬁtting distributional em-
beddings to knowledge graphs with
functional relations.
In Proceedings
of the 27th International Conference
on Computational Linguistics, 2423–
2436.
Lesk, M. E. (1986). Automatic sense
disambiguation using machine read-
able dictionaries: How to tell a pine
cone from an ice cream cone.
In
Proceedings of the 5th International
Conference on Systems Documenta-
tion, 24–26.
Leuski, A. and Traum, D. (2011).
NPCEditor: Creating virtual human
dialogue using information retrieval
techniques. AI Magazine, 32(2), 42–
56.
Levenshtein, V. I. (1966). Binary codes
capable of correcting deletions, in-
sertions, and reversals.
Cybernetics
and Control Theory, 10(8), 707–710.
Original in Doklady Akademii Nauk
SSSR 163(4): 845–848 (1965).
Levesque, H. (2011).
The Winograd
Schema Challenge.
In Logical For-
malizations of Commonsense Reason-
ing —Papers from the AAAI 2011
Spring Symposium (SS-11-06).
Levesque, H., Davis, E., and Mor-
genstern, L. (2012).
The Winograd
Schema Challenge. In Thirteenth In-
ternational Conference on the Princi-
ples of Knowledge Representation and
Reasoning.
Levesque, H. J., Cohen, P. R., and
Nunes, J. H. T. (1990). On acting to-
gether. In AAAI-90, 94–99. Morgan
Kaufmann.
Levin, B. (1977). Mapping sentences to
case frames. Tech. rep. 167, MIT AI
Laboratory. AI Working Paper 143.
Levin, B. (1993). English Verb Classes
and Alternations: A Preliminary In-
vestigation.
University of Chicago
Press.
Levin, B. and Rappaport Hovav, M.
(2005). Argument Realization. Cam-
bridge University Press.
Levin, E., Pieraccini, R., and Eckert, W.
(2000). A stochastic model of human-
machine interaction for learning dia-
log strategies. IEEE Transactions on
Speech and Audio Processing, 8, 11–
23.
Levine, Y., Lenz, B., Dagan, O., Pad-
nos, D., Sharir, O., Shalev-Shwartz,
S., Shashua, A., and Shoham, Y.
(2019).
Sensebert:
Driving some
sense into bert.
arXiv preprint
arXiv:1908.05646.
Levinson, S. C. (1983). Conversational
Analysis, chap. 6. Cambridge Univer-
sity Press.
Levow, G.-A. (1998).
Characteriz-
ing and recognizing spoken correc-
tions in human-computer dialogue. In
COLING-ACL, 736–742.
Levy, O. and Goldberg, Y. (2014a).
Dependency-based word embeddings.
In ACL 2014.
Levy, O. and Goldberg, Y. (2014b).
Linguistic regularities in sparse and
explicit word representations.
In
CoNLL-14.
Levy, O. and Goldberg, Y. (2014c).
Neural word embedding as implicit
matrix factorization.
In NIPS 14,
2177–2185.
Levy, O., Goldberg, Y., and Dagan, I.
(2015). Improving distributional sim-
ilarity with lessons learned from word
embeddings. TACL, 3, 211–225.
Lewis, M. and Steedman, M. (2014).
A* ccg parsing with a supertag-
factored model..
In EMNLP,
990–
1000.
Li, A., Zheng, F., Byrne, W., Fung, P.,
Kamm, T., Yi, L., Song, Z., Ruhi,
U., Venkataramani, V., and Chen, X.
(2000). CASS: A phonetically tran-
scribed corpus of Mandarin sponta-
neous speech. In ICSLP-00, 485–488.
Li, J., Chen, X., Hovy, E. H., and Ju-
rafsky, D. (2015). Visualizing and un-
derstanding neural models in NLP. In
NAACL HLT 2015.
Li, J., Galley, M., Brockett, C., Gao,
J., and Dolan, B. (2016). A diversity-
promoting objective function for neu-
ral conversation models.
In NAACL
HLT 2016.
Li, J. and Jurafsky, D. (2017). Neural
net models of open-domain discourse
coherence.
In EMNLP 2017,
198–
209.
Li, J., Li, R., and Hovy, E. H. (2014).
Recursive deep models for discourse
parsing.
In EMNLP 2014,
2061–
2069.
Li, J., Monroe, W., Ritter, A., Galley,
M., Gao, J., and Jurafsky, D. (2016).
Deep reinforcement learning for dia-
logue generation. In EMNLP 2016.
Li, J., Monroe, W., Shi, T., Ritter, A.,
and Jurafsky, D. (2017).
Adversar-
ial learning for neural dialogue gener-
ation. In EMNLP 2017.
Li, Q., Li, T., and Chang, B. (2016).
Discourse
parsing
with
attention-
based hierarchical neural networks. In
EMNLP 2016, 362–371.
Li, X. and Roth, D. (2002). Learning
question classiﬁers. In COLING-02,
556–562.

580
Bibliography
Li, X. and Roth, D. (2005). Learning
question classiﬁers: The role of se-
mantic information. Journal of Nat-
ural Language Engineering, 11(4).
Li, X., Meng, Y., Sun, X., Han, Q.,
Yuan, A., and Li, J. (2019).
Is
word segmentation necessary for deep
learning of Chinese representations?.
In ACL 2019, 3242–3252.
Liberman, A. M., Delattre, P. C., and
Cooper, F. S. (1952). The role of se-
lected stimulus variables in the per-
ception of the unvoiced stop conso-
nants. American Journal of Psychol-
ogy, 65, 497–516.
Lin, D. (1995).
A dependency-based
method for evaluating broad-coverage
parsers. In IJCAI-95, 1420–1425.
Lin, D. (2003).
Dependency-based
evaluation of minipar. In Workshop on
the Evaluation of Parsing Systems.
Lin, J. (2007).
An exploration of
the principles underlying redundancy-
based factoid question answering.
ACM Transactions on Information
Systems, 25(2).
Lin,
Y.,
Michel,
J.-B.,
Lieber-
man Aiden, E., Orwant, J., Brockman,
W., and Petrov, S. (2012). Syntactic
annotations for the google books
ngram corpus.
In ACL 2012,
169–
174.
Lin, Z., Madotto, A., Shin, J., Xu, P.,
and Fung, P. (2019).
MoEL: Mix-
ture of empathetic listeners.
http:
//arxiv.org/abs/1908.07687.
Lin, Z., Kan, M.-Y., and Ng, H. T.
(2009).
Recognizing implicit dis-
course relations in the penn discourse
treebank. In EMNLP-09, 343–351.
Lin, Z., Ng, H. T., and Kan, M.-Y.
(2011). Automatically evaluating text
coherence using discourse relations.
In ACL 2011, 997–1006.
Lin, Z., Ng, H. T., and Kan, M.-Y.
(2014). A pdtb-styled end-to-end dis-
course parser. Natural Language En-
gineering, 20(2), 151–184.
Lindsey, R. (1963). Inferential mem-
ory as the basis of machines which un-
derstand natural language. In Feigen-
baum, E. and Feldman, J. (Eds.),
Computers and Thought,
217–233.
McGraw Hill.
Ling, W., Dyer, C., Black, A. W.,
Trancoso, I., Fermandez, R., Amir,
S., Marujo, L., and Lu´ıs, T. (2015a).
Finding function in form:
Compo-
sitional character models for open
vocabulary word representation.
In
EMNLP 2015, 1520–1530.
Ling, X., Singh, S., and Weld, D. S.
(2015b). Design challenges for entity
linking. TACL, 3, 315–328.
Lison, P. and Tiedemann, J. (2016).
Opensubtitles2016: Extracting large
parallel corpora from movie and tv
subtitles. In LREC-16.
Litman, D. J. (1985). Plan Recogni-
tion and Discourse Analysis: An In-
tegrated Approach for Understanding
Dialogues. Ph.D. thesis, University of
Rochester, Rochester, NY.
Litman, D. J. and Allen, J. (1987).
A plan recognition model for subdia-
logues in conversation. Cognitive Sci-
ence, 11, 163–200.
Litman, D. J., Swerts, M., and Hirsch-
berg, J. (2000). Predicting automatic
speech recognition performance using
prosodic cues. In NAACL 2000.
Litman, D. J., Walker, M. A., and
Kearns, M. (1999). Automatic detec-
tion of poor speech recognition at the
dialogue level. In ACL-99, 309–316.
Liu, B. and Zhang, L. (2012). A sur-
vey of opinion mining and sentiment
analysis. In Aggarwal, C. C. and Zhai,
C. (Eds.), Mining text data, 415–464.
Springer.
Liu, C.-W., Lowe, R. T., Serban, I. V.,
Noseworthy, M., Charlin, L., and
Pineau, J. (2016). How NOT to eval-
uate your dialogue system: An empir-
ical study of unsupervised evaluation
metrics for dialogue response genera-
tion. In EMNLP 2016.
Lochbaum, K. E., Grosz, B. J., and
Sidner, C. L. (2000). Discourse struc-
ture and intention recognition.
In
Dale, R., Moisl, H., and Somers,
H. L. (Eds.), Handbook of Natural
Language Processing. Marcel Dekker.
Logeswaran, L., Lee, H., and Radev,
D. (2018).
Sentence ordering and
coherence modeling using recurrent
neural networks. In AAAI-18.
Louis, A. and Nenkova, A. (2012). A
coherence model based on syntactic
patterns.
In EMNLP 2012,
1157–
1168.
Loureiro, D. and Jorge, A. (2019).
Language modelling makes sense:
Propagating representations through
WordNet for full-coverage word sense
disambiguation. In ACL 2019, 5682–
5691.
Louviere, J. J., Flynn, T. N., and Mar-
ley, A. A. J. (2015). Best-worst scal-
ing:
Theory, methods and applica-
tions. Cambridge University Press.
Lovins, J. B. (1968). Development of
a stemming algorithm.
Mechanical
Translation and Computational Lin-
guistics, 11(1–2), 9–13.
Lowe, R. T., Noseworthy, M., Serban,
I. V., Angelard-Gontier, N., Bengio,
Y., and Pineau, J. (2017a). Towards
an automatic Turing test: Learning to
evaluate dialogue responses. In ACL
2017.
Lowe, R. T., Pow, N., Serban, I. V.,
Charlin, L., Liu, C.-W., and Pineau,
J. (2017b).
Training end-to-end di-
alogue systems with the ubuntu dia-
logue corpus. Dialogue & Discourse,
8(1), 31–65.
Luhn, H. P. (1957).
A statistical ap-
proach to the mechanized encoding
and searching of literary information.
IBM Journal of Research and Devel-
opment, 1(4), 309–317.
Lui, M. and Baldwin, T. (2011). Cross-
domain feature selection for language
identiﬁcation. In IJCNLP-11, 553–
561.
Lui,
M. and Baldwin,
T. (2012).
langid.py:
An off-the-shelf lan-
guage identiﬁcation tool.
In ACL
2012, 25–30.
Luo, F., Liu, T., He, Z., Xia, Q., Sui,
Z., and Chang, B. (2018a). Leverag-
ing gloss knowledge in neural word
sense disambiguation by hierarchical
co-attention. In EMNLP 2018, 1402–
1411.
Luo, F., Liu, T., Xia, Q., Chang, B.,
and Sui, Z. (2018b).
Incorporating
glosses into neural word sense disam-
biguation. In ACL 2018, 2473–2482.
Luo, X. (2005). On coreference resolu-
tion performance metrics. In Proceed-
ings of HLT-EMNLP 2005, 25–32.
Luo, X. and Pradhan, S. (2016). Eval-
uation metrics.
In Poesio,
M.,
Stuckardt, R., and Versley, Y. (Eds.),
Anaphora resolution: Algorithms, re-
sources, and applications, 141–163.
Springer.
Luo, X., Pradhan, S., Recasens, M., and
Hovy, E. H. (2014). An extension of
blanc to system mentions.
In ACL
2014, Vol. 2014, p. 24.
Luong, T., Pham, H., and Manning,
C. D. (2015). Effective approaches to
attention-based neural machine trans-
lation. In Proceedings of the Confer-
ence on Empirical Methods in Natural
Language Processing, 1412–1421.
Lyons, J. (1977).
Semantics.
Cam-
bridge University Press.
Lyons, R. G. (2004).
Understanding
Digital Signal Processing.
Prentice
Hall. (2nd. ed).
Ma,
X. and Hovy,
E. H. (2016).
End-to-end sequence labeling via bi-
directional LSTM-CNNs-CRF.
In
ACL 2016.
Madhu, S. and Lytel, D. (1965).
A
ﬁgure of merit technique for the reso-
lution of non-grammatical ambiguity.
Mechanical Translation, 8(2), 9–13.
Magerman, D. M. (1994).
Natural
Language Parsing as Statistical Pat-
tern Recognition. Ph.D. thesis, Uni-
versity of Pennsylvania.

Bibliography
581
Magerman, D. M. (1995). Statistical
decision-tree models for parsing. In
ACL-95, 276–283.
Magerman, D. M. and Marcus, M. P.
(1991).
Pearl: A probabilistic chart
parser. In EACL-91.
Mairesse, F. and Walker, M. A. (2008).
Trainable generation of big-ﬁve per-
sonality styles through data-driven pa-
rameter estimation. In ACL-08.
Manandhar, S., Klapaftis, I. P., Dli-
gach, D., and Pradhan, S. (2010).
Semeval-2010 task 14: Word sense
induction & disambiguation.
In
SemEval-2010, 63–68.
Mann, W. C. and Thompson, S. A.
(1987). Rhetorical structure theory: A
theory of text organization. Tech. rep.
RS-87-190, Information Sciences In-
stitute.
Manning, C. D. (2011). Part-of-speech
tagging from 97% to 100%: Is it time
for some linguistics?.
In CICLing
2011, 171–189.
Manning, C. D., Raghavan, P., and
Sch¨utze, H. (2008).
Introduction to
Information Retrieval. Cambridge.
Manning,
C. D. and Sch¨utze,
H.
(1999).
Foundations of Statistical
Natural Language Processing.
MIT
Press.
Manning, C. D., Surdeanu, M., Bauer,
J., Finkel, J., Bethard, S., and Mc-
Closky, D. (2014).
The stanford
CoreNLP natural language processing
toolkit. In ACL 2014, 55–60.
Marcu, D. (1997). The rhetorical pars-
ing of natural language texts.
In
ACL/EACL-97.
Marcu, D. (1999).
A decision-based
approach to rhetorical parsing.
In
ACL-99, 365–372.
Marcu, D. (2000a).
The rhetori-
cal parsing of unrestricted texts: A
surface-based approach.
Computa-
tional Linguistics, 26(3), 395–448.
Marcu, D. (Ed.). (2000b).
The The-
ory and Practice of Discourse Parsing
and Summarization. MIT Press.
Marcu, D. and Echihabi, A. (2002). An
unsupervised approach to recognizing
discourse relations. In ACL-02, 368–
375.
Marcus, M. P. (1980). A Theory of Syn-
tactic Recognition for Natural Lan-
guage. MIT Press.
Marcus, M. P. (1990).
Summary
of session 9: Automatic acquisition
of linguistic structure.
In Proceed-
ings DARPA Speech and Natural Lan-
guage Workshop, 249–250.
Marcus,
M.
P.,
Kim,
G.,
Marcinkiewicz, M. A., MacIntyre,
R., Bies, A., Ferguson, M., Katz,
K., and Schasberger, B. (1994). The
Penn Treebank: Annotating predicate
argument structure.
In ARPA Hu-
man Language Technology Workshop,
114–119. Morgan Kaufmann.
Marcus, M. P., Santorini, B., and
Marcinkiewicz, M. A. (1993). Build-
ing a large annotated corpus of En-
glish: The Penn treebank. Computa-
tional Linguistics, 19(2), 313–330.
Markov, A. A. (1913).
Essai d’une
recherche statistique sur le texte du
roman “Eugene Onegin” illustrant la
liaison des epreuve en chain (‘Ex-
ample of a statistical investigation of
the text of “Eugene Onegin” illus-
trating the dependence between sam-
ples in chain’).
Izvistia Impera-
torskoi Akademii Nauk (Bulletin de
l’Acad´emie Imp´eriale des Sciences de
St.-P´etersbourg), 7, 153–162.
Markov, A. A. (2006). Classical text
in translation: A. A. Markov, an ex-
ample of statistical investigation of
the text Eugene Onegin concerning
the connection of samples in chains.
Science in Context, 19(4), 591–600.
Translated by David Link.
Maron, M. E. (1961).
Automatic
indexing:
an experimental inquiry.
Journal of the ACM (JACM), 8(3),
404–417.
M`arquez, L., Carreras, X., Litkowski,
K. C., and Stevenson, S. (2008). Se-
mantic role labeling: An introduction
to the special issue.
Computational
linguistics, 34(2), 145–159.
Marshall, I. (1983). Choice of gram-
matical word-class without GLobal
syntactic analysis: Tagging words in
the LOB corpus. Computers and the
Humanities, 17, 139–150.
Marshall, I. (1987). Tag selection us-
ing probabilistic methods.
In Gar-
side, R., Leech, G., and Sampson, G.
(Eds.), The Computational Analysis of
English, 42–56. Longman.
Martin, J. H. (1986). The acquisition of
polysemy. In ICML 1986, 198–204.
Martschat, S. and Strube, M. (2014).
Recall error analysis for coreference
resolution. In EMNLP 2014, 2070–
2081.
Martschat, S. and Strube, M. (2015).
Latent structures for coreference reso-
lution. TACL, 3, 405–418.
Masterman, M. (1957). The thesaurus
in syntax and semantics. Mechanical
Translation, 4(1), 1–2.
Mathis, D. A. and Mozer, M. C. (1995).
On the computational utility of con-
sciousness.
In Tesauro, G., Touret-
zky, D. S., and Alspector, J. (Eds.),
Advances in Neural Information Pro-
cessing Systems VII. MIT Press.
McCallum,
A.,
Freitag,
D.,
and
Pereira, F. C. N. (2000).
Maximum
entropy Markov models for informa-
tion extraction and segmentation. In
ICML 2000, 591–598.
McCallum, A. and Nigam, K. (1998).
A comparison of event models for
naive bayes text classiﬁcation.
In
AAAI/ICML-98 Workshop on Learn-
ing for Text Categorization, 41–48.
McCarthy, J. F. and Lehnert, W. G.
(1995). Using decision trees for coref-
erence resolution. In IJCAI-95, 1050–
1055.
McCawley, J. D. (1968). The role of se-
mantics in a grammar. In Bach, E. W.
and Harms, R. T. (Eds.), Universals
in Linguistic Theory, 124–169. Holt,
Rinehart & Winston.
McCawley, J. D. (1993).
Everything
that Linguists Have Always Wanted to
Know about Logic (2nd Ed.). Univer-
sity of Chicago Press, Chicago, IL.
McCawley, J. D. (1998). The Syntactic
Phenomena of English. University of
Chicago Press.
McClelland, J. L. and Elman, J. L.
(1986). The TRACE model of speech
perception. Cognitive Psychology, 18,
1–86.
McClelland, J. L., Rumelhart, D. E.,
and The PDP Research Group (1986).
Parallel Distributed Processing: Ex-
plorations in the Microstructure of
Cognition, Vol. 2: Psychological and
Biological Models.
Bradford Books
(MIT Press).
McCulloch, W. S. and Pitts, W. (1943).
A logical calculus of ideas imma-
nent in nervous activity.
Bulletin
of Mathematical Biophysics, 5, 115–
133.
Reprinted in Neurocomputing:
Foundations of Research, ed. by J. A.
Anderson and E Rosenfeld. MIT Press
1988.
McDonald,
R.,
Crammer,
K.,
and
Pereira, F. C. N. (2005).
Online
large-margin training of dependency
parsers. In ACL-05, 91–98.
McDonald, R. and Nivre, J. (2011).
Analyzing
and
integrating
depen-
dency parsers.
Computational Lin-
guistics, 37(1), 197–230.
McDonald, R., Pereira, F. C. N., Rib-
arov, K., and Hajiˇc, J. (2005). Non-
projective dependency parsing using
spanning tree algorithms.
In HLT-
EMNLP-05.
McGuiness, D. L. and van Harme-
len, F. (2004).
OWL web ontology
overview. Tech. rep. 20040210, World
Wide Web Consortium.
Mehl, M. R., Gosling, S. D., and Pen-
nebaker, J. W. (2006). Personality in
its natural habitat: manifestations and
implicit folk theories of personality in
daily life.. Journal of Personality and
Social Psychology, 90(5).
Melamud, O., Goldberger, J., and Da-
gan, I. (2016). context2vec: Learning
generic context embedding with bidi-
rectional LSTM. In CoNLL-16, 51–
61.

582
Bibliography
Mel’˘cuk, I. A. (1988).
Dependency
Syntax: Theory and Practice.
State
University of New York Press.
Merialdo, B. (1994). Tagging English
text with a probabilistic model. Com-
putational Linguistics, 20(2), 155–
172.
Mesgar, M. and Strube, M. (2016).
Lexical coherence graph modeling us-
ing word embeddings. In ACL 2016,
1414–1423.
Metsis, V., Androutsopoulos, I., and
Paliouras, G. (2006). Spam ﬁltering
with naive bayes-which naive bayes?.
In CEAS, 27–28.
Meyers, A., Reeves, R., Macleod, C.,
Szekely, R., Zielinska, V., Young,
B., and Grishman, R. (2004).
The
nombank project: An interim report.
In Proceedings of the NAACL/HLT
Workshop: Frontiers in Corpus Anno-
tation.
Mihalcea, R. (2007). Using wikipedia
for automatic word sense disambigua-
tion. In NAACL-HLT 07, 196–203.
Mihalcea, R. and Csomai, A. (2007).
Wikify!: linking documents to ency-
clopedic knowledge. In CIKM 2007,
233–242.
Mihalcea, R. and Moldovan, D. (2001).
Automatic generation of a coarse
grained WordNet.
In NAACL Work-
shop on WordNet and Other Lexical
Resources.
Mikheev, A., Moens, M., and Grover,
C. (1999). Named entity recognition
without gazetteers. In EACL-99, 1–8.
Mikolov, T. (2012).
Statistical lan-
guage models based on neural net-
works.
Ph.D. thesis, Ph. D. thesis,
Brno University of Technology.
Mikolov, T., Chen, K., Corrado, G. S.,
and Dean, J. (2013). Efﬁcient estima-
tion of word representations in vector
space. In ICLR 2013.
Mikolov, T., Karaﬁ´at, M., Burget,
L., ˇCernock`y, J., and Khudanpur, S.
(2010).
Recurrent neural network
based language model.
In INTER-
SPEECH 2010, 1045–1048.
Mikolov, T., Kombrink, S., Burget, L.,
ˇCernock`y, J. H., and Khudanpur, S.
(2011). Extensions of recurrent neural
network language model. In ICASSP-
11, 5528–5531.
Mikolov, T., Sutskever, I., Chen, K.,
Corrado, G. S., and Dean, J. (2013a).
Distributed representations of words
and phrases and their compositional-
ity. In NIPS 13, 3111–3119.
Mikolov, T., Yih, W.-t., and Zweig,
G. (2013b).
Linguistic regularities
in continuous space word representa-
tions. In NAACL HLT 2013, 746–751.
Miller, C. A. (1998).
Pronunciation
modeling in speech synthesis. Tech.
rep. IRCS 98–09, University of Penn-
sylvania Institute for Research in Cog-
nitive Science, Philadephia, PA.
Miller, G. A. and Nicely, P. E. (1955).
An analysis of perceptual confu-
sions among some English conso-
nants. JASA, 27, 338–352.
Miller, G. A. and Charles, W. G.
(1991).
Contextual correlates of se-
mantics similarity.
Language and
Cognitive Processes, 6(1), 1–28.
Miller, G. A. and Chomsky, N. (1963).
Finitary models of language users.
In Luce, R. D., Bush, R. R., and
Galanter,
E. (Eds.),
Handbook of
Mathematical Psychology,
Vol. II,
419–491. John Wiley.
Miller, G. A., Leacock, C., Tengi, R. I.,
and Bunker, R. T. (1993). A semantic
concordance.
In Proceedings ARPA
Workshop on Human Language Tech-
nology, 303–308.
Miller, G. A. and Selfridge, J. A.
(1950).
Verbal context and the re-
call of meaningful material. American
Journal of Psychology, 63, 176–185.
Miller, S., Bobrow, R. J., Ingria, R.,
and Schwartz, R. (1994).
Hidden
understanding models of natural lan-
guage. In ACL-94, 25–32.
Milne, D. and Witten, I. H. (2008).
Learning to link with wikipedia.
In
CIKM 2008, 509–518.
Miltsakaki, E., Prasad, R., Joshi, A. K.,
and Webber, B. L. (2004). The Penn
Discourse Treebank. In LREC-04.
Minsky, M. (1961). Steps toward arti-
ﬁcial intelligence. Proceedings of the
IRE, 49(1), 8–30.
Minsky, M. (1974). A framework for
representing knowledge.
Tech. rep.
306, MIT AI Laboratory. Memo 306.
Minsky, M. and Papert, S. (1969). Per-
ceptrons. MIT Press.
Mintz, M., Bills, S., Snow, R., and Ju-
rafsky, D. (2009). Distant supervision
for relation extraction without labeled
data. In ACL IJCNLP 2009.
Mitkov, R. (2002). Anaphora Resolu-
tion. Longman.
Miwa, M. and Bansal, M. (2016). End-
to-end relation extraction using lstms
on sequences and tree structures. In
ACL 2016, 1105–1116.
Mohammad, S. M. (2018a). Obtain-
ing reliable human ratings of valence,
arousal, and dominance for 20,000 en-
glish words. In ACL 2018.
Mohammad, S. M. (2018b). Word af-
fect intensities. In LREC-18.
Mohammad, S. M. and Turney, P. D.
(2013).
Crowdsourcing a word-
emotion association lexicon. Compu-
tational Intelligence, 29(3), 436–465.
Monroe, B. L., Colaresi, M. P., and
Quinn, K. M. (2008). Fightin’words:
Lexical feature selection and evalu-
ation for identifying the content of
political conﬂict.
Political Analysis,
16(4), 372–403.
Montague, R. (1973). The proper treat-
ment of quantiﬁcation in ordinary En-
glish.
In Thomason, R. (Ed.), For-
mal Philosophy: Selected Papers of
Richard Montague,
247–270. Yale
University Press, New Haven, CT.
Monz,
C. (2004).
Minimal span
weighting retrieval for question an-
swering. In SIGIR Workshop on In-
formation Retrieval for Question An-
swering, 23–30.
Moosavi, N. S. and Strube, M. (2016).
Which coreference evaluation metric
do you trust? a proposal for a link-
based entity aware metric.
In ACL
2016, 632–642.
Morey, M., Muller, P., and Asher, N.
(2017). How much progress have we
made on RST discourse parsing?
a
replication study of recent results on
the rst-dt. In EMNLP 2017.
Morgan,
A.
A.,
Hirschman,
L.,
Colosimo,
M.,
Yeh,
A. S.,
and
Colombe, J. B. (2004).
Gene name
identiﬁcation and normalization using
a model organism database.
Jour-
nal of Biomedical Informatics, 37(6),
396–410.
Morgan, N. and Bourlard, H. (1989).
Generalization and parameter estima-
tion in feedforward nets: Some exper-
iments. In Advances in neural infor-
mation processing systems, 630–637.
Morgan, N. and Bourlard, H. (1990).
Continuous speech recognition using
multilayer perceptrons with hidden
markov models. In ICASSP-90, 413–
416.
Morris, J. and Hirst, G. (1991). Lex-
ical cohesion computed by thesaural
relations as an indicator of the struc-
ture of text. Computational Linguis-
tics, 17(1), 21–48.
Morris, W. (Ed.). (1985).
Ameri-
can Heritage Dictionary (2nd College
Edition Ed.). Houghton Mifﬂin.
Mosteller,
F. and Wallace,
D. L.
(1963).
Inference in an authorship
problem: A comparative study of dis-
crimination methods applied to the au-
thorship of the disputed federalist pa-
pers. Journal of the American Statis-
tical Association, 58(302), 275–309.
Mosteller,
F. and Wallace,
D. L.
(1964).
Inference and Disputed Au-
thorship: The Federalist.
Springer-
Verlag. A second edition appeared in
1984 as Applied Bayesian and Classi-
cal Inference.
Mrkˇsi´c, N.,
´O S´eaghdha, D., Wen,
T.-H., Thomson, B., and Young, S.

Bibliography
583
(2017). Neural belief tracker: Data-
driven dialogue state tracking. In ACL
2017, 1777–1788.
Mrkˇsi´c, N., S´eaghdha, D. ´O., Thom-
son, B., Gaˇsi´c, M., Rojas-Barahona,
L. M., Su, P.-H., Vandyke, D., Wen,
T.-H., and Young, S. (2016). Counter-
ﬁtting word vectors to linguistic con-
straints. In NAACL HLT 2016, 142–
148.
Muller, P., Braud, C., and Morey, M.
(2019).
ToNy: Contextual embed-
dings for accurate multilingual dis-
course segmentation of full docu-
ments. In Proceedings of the Work-
shop on Discourse Relation Parsing
and Treebanking 2019, 115–124.
Murdock, J. W., Fan, J., Lally, A.,
Shima, H., and Boguraev, B. K.
(2012a).
Textual evidence gather-
ing and analysis.
IBM Journal of
Research and Development, 56(3/4),
8:1–8:14.
Murdock, J. W., Kalyanpur, A., Welty,
C., Fan, J., Ferrucci, D. A., Gondek,
D. C., Zhang, L., and Kanayama, H.
(2012b).
Typing candidate answers
using type coercion. IBM Journal of
Research and Development, 56(3/4),
7:1–7:13.
Murphy, K. P. (2012). Machine learn-
ing: A probabilistic perspective. MIT
press.
Musi, E., Stede, M., Kriese, L., Mure-
san, S., and Rocci, A. (2018). A multi-
layer annotated corpus of argumenta-
tive text: From argument schemes to
discourse relations. In LREC-18.
Myers, G. (1992).
“in this paper we
report...”: Speech acts and scientiﬁc
facts. Journal of Pragmatics, 17(4),
295–313.
N´adas, A. (1984). Estimation of prob-
abilities in the language model of the
IBM speech recognition system. IEEE
Transactions on Acoustics, Speech,
Signal Processing, 32(4), 859–861.
Nagata, M. and Morimoto, T. (1994).
First steps toward statistical modeling
of dialogue to predict the speech act
type of the next utterance.
Speech
Communication, 15, 193–203.
Nash-Webber, B. L. (1975). The role
of semantics in automatic speech un-
derstanding.
In Bobrow, D. G. and
Collins, A. (Eds.), Representation and
Understanding, 351–382. Academic
Press.
Naur, P., Backus, J. W., Bauer, F. L.,
Green, J., Katz, C., McCarthy, J.,
Perlis, A. J., Rutishauser, H., Samel-
son, K., Vauquois, B., Wegstein, J. H.,
van Wijnagaarden, A., and Woodger,
M. (1960). Report on the algorithmic
language ALGOL 60.
CACM, 3(5),
299–314. Revised in CACM 6:1, 1-
17, 1963.
Navigli, R. (2006). Meaningful cluster-
ing of senses helps boost word sense
disambiguation performance. In COL-
ING/ACL 2006, 105–112.
Navigli, R. (2009). Word sense disam-
biguation: A survey. ACM Computing
Surveys, 41(2).
Navigli, R. (2016). Chapter 20. ontolo-
gies.
In Mitkov, R. (Ed.), The Ox-
ford handbook of computational lin-
guistics. Oxford University Press.
Navigli, R. and Ponzetto, S. P. (2012).
BabelNet:
The automatic construc-
tion, evaluation and application of a
wide-coverage multilingual semantic
network. Artiﬁcial Intelligence, 193,
217–250.
Navigli, R. and Vannella, D. (2013).
Semeval-2013 task 11: Word sense in-
duction & disambiguation within an
end-user application. In *SEM, 193–
201.
Nayak, N., Hakkani-T¨ur, D., Walker,
M. A., and Heck, L. P. (2017). To plan
or not to plan?
discourse planning
in slot-value informed sequence to se-
quence models for language genera-
tion.. In INTERSPEECH-17, 3339–
3343.
Needleman, S. B. and Wunsch, C. D.
(1970).
A general method applica-
ble to the search for similarities in
the amino-acid sequence of two pro-
teins. Journal of Molecular Biology,
48, 443–453.
Neff, G. and Nagy, P. (2016). Talking to
bots: Symbiotic agency and the case
of Tay. International Journal of Com-
munication, 10, 4915–4931.
Ney, H. (1991).
Dynamic program-
ming parsing for context-free gram-
mars in continuous speech recogni-
tion.
IEEE Transactions on Signal
Processing, 39(2), 336–340.
Ng, A. Y. and Jordan, M. I. (2002).
On discriminative vs. generative clas-
siﬁers: A comparison of logistic re-
gression and naive bayes. In NIPS 14,
841–848.
Ng, H. T., Teo, L. H., and Kwan,
J. L. P. (2000).
A machine learn-
ing approach to answering questions
for reading comprehension tests.
In
EMNLP 2000, 124–132.
Ng, V. (2004). Learning noun phrase
anaphoricity to improve coreference
resolution:
Issues in representation
and optimization. In ACL-04.
Ng, V. (2005a). Machine learning for
coreference resolution:
From local
classiﬁcation to global ranking.
In
ACL-05.
Ng, V. (2005b). Supervised ranking for
pronoun resolution: Some recent im-
provements. In AAAI-05.
Ng, V. (2010). Supervised noun phrase
coreference research: The ﬁrst ﬁfteen
years. In ACL 2010, 1396–1411.
Ng, V. (2017).
Machine learning for
entity coreference resolution: A ret-
rospective look at two decades of re-
search. In AAAI-17.
Ng, V. and Cardie, C. (2002a). Iden-
tifying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. In COLING-02.
Ng, V. and Cardie, C. (2002b).
Im-
proving machine learning approaches
to coreference resolution. In ACL-02.
Nguyen, D. T. and Joty, S. (2017). A
neural local coherence model. In ACL
2017, 1320–1330.
Nguyen, K. A., Schulte im Walde,
S., and Vu, N. T. (2016).
Inte-
grating distributional lexical contrast
into word embeddings for antonym-
synonym distinction.
In ACL 2016,
454–459.
Nie, A., Bennett, E., and Goodman,
N. (2019).
Dissent: Learning sen-
tence representations from explicit
discourse relations.
In ACL 2019,
4497–4510.
Nielsen, J. (1992). The usability engi-
neering life cycle.
IEEE Computer,
25(3), 12–22.
Nielsen, M. A. (2015).
Neural net-
works and Deep learning. Determi-
nation Press USA.
Nigam, K., Lafferty, J. D., and McCal-
lum, A. (1999). Using maximum en-
tropy for text classiﬁcation. In IJCAI-
99 workshop on machine learning for
information ﬁltering, 61–67.
Nilsson, J., Riedel, S., and Yuret, D.
(2007). The conll 2007 shared task on
dependency parsing. In Proceedings
of the CoNLL shared task session of
EMNLP-CoNLL, 915–932. sn.
Nissim, M., Dingare, S., Carletta, J.,
and Steedman, M. (2004). An annota-
tion scheme for information status in
dialogue. In LREC-04.
NIST
(1990).
TIMIT
Acoustic-
Phonetic Continuous Speech Corpus.
National Institute of Standards and
Technology Speech Disc 1-1.1. NIST
Order No. PB91-505065.
NIST (2005). Speech recognition scor-
ing toolkit (sctk) version 2.1. http:
//www.nist.gov/speech/tools/.
Nivre, J. (2007).
Incremental non-
projective dependency parsing.
In
NAACL-HLT 07.
Nivre, J. (2003).
An efﬁcient algo-
rithm for projective dependency pars-
ing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technolo-
gies (IWPT.
Nivre, J. (2006). Inductive Dependency
Parsing. Springer.

584
Bibliography
Nivre, J. (2009). Non-projective depen-
dency parsing in expected linear time.
In ACL IJCNLP 2009, 351–359.
Nivre, J., de Marneffe, M.-C., Gin-
ter, F., Goldberg, Y., Hajiˇc, J., Man-
ning, C. D., McDonald, R., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R.,
and Zeman, D. (2016a). Universal De-
pendencies v1: A multilingual tree-
bank collection. In LREC.
Nivre, J., de Marneffe, M.-C., Gin-
ter, F., Goldberg, Y., Hajiˇc, J., Man-
ning, C. D., McDonald, R., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R.,
and Zeman, D. (2016b). Universal De-
pendencies v1: A multilingual tree-
bank collection. In LREC-16.
Nivre, J., Hall, J., Nilsson, J., Chanev,
A., Eryigit, G., K¨ubler, S., Marinov,
S., and Marsi, E. (2007). Maltparser:
A language-independent system for
data-driven dependency parsing. Nat-
ural Language Engineering, 13(02),
95–135.
Nivre,
J. and Nilsson,
J. (2005).
Pseudo-projective dependency pars-
ing. In ACL-05, 99–106.
Nivre, J. and Scholz, M. (2004). De-
terministic dependency parsing of en-
glish text. In COLING-04, p. 64.
Niwa, Y. and Nitta, Y. (1994).
Co-
occurrence vectors from corpora vs.
distance vectors from dictionaries. In
ACL-94, 304–309.
Noreen, E. W. (1989). Computer Inten-
sive Methods for Testing Hypothesis.
Wiley.
Norman, D. A. (1988). The Design of
Everyday Things. Basic Books.
Norman, D. A. and Rumelhart, D. E.
(1975).
Explorations in Cognition.
Freeman.
Norvig, P. (1991). Techniques for auto-
matic memoization with applications
to context-free parsing.
Computa-
tional Linguistics, 17(1), 91–98.
Nosek, B. A., Banaji, M. R., and Green-
wald, A. G. (2002a). Harvesting im-
plicit group attitudes and beliefs from
a demonstration web site. Group Dy-
namics: Theory, Research, and Prac-
tice, 6(1), 101.
Nosek, B. A., Banaji, M. R., and Green-
wald, A. G. (2002b).
Math=male,
me=female,
therefore
math̸=
me.
Journal of personality and social psy-
chology, 83(1), 44.
O’Connor, B., Krieger, M., and Ahn,
D. (2010). Tweetmotif: Exploratory
search and topic summarization for
twitter. In ICWSM.
Oravecz, C. and Dienes, P. (2002). Ef-
ﬁcient stochastic part-of-speech tag-
ging for Hungarian.
In LREC-02,
710–717.
Osgood, C. E., Suci, G. J., and Tan-
nenbaum, P. H. (1957). The Measure-
ment of Meaning. University of Illi-
nois Press.
O’Shaughnessy, D. (2000).
Speech
Communications:
Human and Ma-
chine. IEEE Press, New York. 2nd.
ed.
Ostendorf, M. and Veilleux, N. (1994).
A hierarchical stochastic model for
automatic
prediction
of
prosodic
boundary location.
Computational
Linguistics, 20(1), 27–54.
Packard, D. W. (1973).
Computer-
assisted morphological analysis of an-
cient Greek. In Zampolli, A. and Cal-
zolari, N. (Eds.), Computational and
Mathematical Linguistics: Proceed-
ings of the International Conference
on Computational Linguistics, 343–
355. Leo S. Olschki.
Palmer, D. (2012).
Text preprocess-
ing. In Indurkhya, N. and Damerau,
F. J. (Eds.), Handbook of Natural Lan-
guage Processing, 9–30. CRC Press.
Palmer, M., Babko-Malaya, O., and
Dang,
H. T. (2004).
Different
sense granularities for different appli-
cations.
In HLT-NAACL Workshop
on Scalable Natural Language Under-
standing, 49–56.
Palmer, M., Dang, H. T., and Fell-
baum, C. (2006). Making ﬁne-grained
and coarse-grained sense distinctions,
both manually and automatically. Nat-
ural Language Engineering, 13(2),
137–163.
Palmer, M., Gildea, D., and Xue, N.
(2010). Semantic role labeling. Syn-
thesis Lectures on Human Language
Technologies, 3(1), 1–103.
Palmer, M., Kingsbury, P., and Gildea,
D. (2005). The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1), 71–
106.
Pang, B. and Lee, L. (2008). Opinion
mining and sentiment analysis. Foun-
dations and trends in information re-
trieval, 2(1-2), 1–135.
Pang, B., Lee, L., and Vaithyanathan,
S. (2002).
Thumbs up?
Sentiment
classiﬁcation using machine learning
techniques. In EMNLP 2002, 79–86.
Paolino, J. (2017).
Google Home
vs Alexa:
Two simple user ex-
perience
design
gestures
that
delighted
a
female
user.
In
Medium.
Jan 4,
2017. https:
//medium.com/startup-grind/
google-home-vs-alexa-56e26f69ac77.
Park, J. and Cardie, C. (2014). Iden-
tifying appropriate support for propo-
sitions in online user comments.
In
Proceedings of the ﬁrst workshop on
argumentation mining, 29–38.
Parsons, T. (1990). Events in the Se-
mantics of English. MIT Press.
Partee, B. H. (Ed.). (1976). Montague
Grammar. Academic Press.
Pasca, M. (2003). Open-Domain Ques-
tion Answering from Large Text Col-
lections. CSLI.
Paszke, A., Gross, S., Chintala, S.,
Chanan, G., Yang, E., DeVito, Z., Lin,
Z., Desmaison, A., Antiga, L., and
Lerer, A. (2017). Automatic differen-
tiation in pytorch. In NIPS-W.
Pearl, C. (2017). Designing Voice User
Interfaces:
Principles of Conversa-
tional Experiences. O’Reilly.
Pedersen, T. and Bruce, R. (1997). Dis-
tinguishing word senses in untagged
text. In EMNLP 1997.
Peldszus, A. and Stede, M. (2013).
From argument diagrams to argu-
mentation mining in texts:
A sur-
vey. International Journal of Cogni-
tive Informatics and Natural Intelli-
gence (IJCINI), 7(1), 1–31.
Peldszus, A. and Stede, M. (2016).
An annotated corpus of argumentative
microtexts. In Proceedings of the 1st
European Conference on Argumenta-
tion, 801–816.
Peng,
N.,
Poon,
H.,
Quirk,
C.,
Toutanova, K., and Yih, W.-t. (2017).
Cross-sentence n-ary relation extrac-
tion with graph LSTMs.
TACL, 5,
101–115.
Penn, G. and Kiparsky, P. (2012). On
P¯an.ini and the generative capacity of
contextualized replacement systems.
In COLING-12, 943–950.
Pennebaker, J. W., Booth, R. J., and
Francis, M. E. (2007). Linguistic In-
quiry and Word Count: LIWC 2007.
Austin, TX.
Pennebaker, J. W. and King, L. A.
(1999).
Linguistic styles: language
use as an individual difference. Jour-
nal of Personality and Social Psychol-
ogy, 77(6).
Pennington, J., Socher, R., and Man-
ning, C. D. (2014).
Glove: Global
vectors for word representation.
In
EMNLP 2014, 1532–1543.
Percival,
W. K. (1976).
On the
historical source of immediate con-
stituent analysis. In McCawley, J. D.
(Ed.), Syntax and Semantics Volume
7, Notes from the Linguistic Under-
ground, 229–242. Academic Press.
Perrault, C. R. and Allen, J. (1980). A
plan-based analysis of indirect speech
acts. American Journal of Computa-
tional Linguistics, 6(3-4), 167–182.
Peters, M., Neumann, M., Iyyer, M.,
Gardner, M., Clark, C., Lee, K., and
Zettlemoyer, L. (2018).
Deep con-
textualized word representations.
In
NAACL HLT 2018, 2227–2237.

Bibliography
585
Peterson, G. E. and Barney, H. L.
(1952).
Control methods used in a
study of the vowels. JASA, 24, 175–
184.
Petrov, S., Barrett, L., Thibaux, R., and
Klein, D. (2006). Learning accurate,
compact, and interpretable tree anno-
tation. In COLING/ACL 2006, 433–
440.
Petrov, S., Das, D., and McDonald,
R. (2012). A universal part-of-speech
tagset. In LREC-12.
Petrov, S. and McDonald, R. (2012).
Overview of the 2012 shared task on
parsing the web. In Notes of the First
Workshop on Syntactic Analysis of
Non-Canonical Language (SANCL),
Vol. 59.
Phillips, A. V. (1960).
A question-
answering routine. Tech. rep. 16, MIT
AI Lab.
Picard, R. W. (1995). Affective com-
puting.
Tech. rep. 321, MIT Media
Lab Perceputal Computing Technical
Report. Revised November 26, 1995.
Pieraccini, R., Levin, E., and Lee,
C.-H. (1991).
Stochastic represen-
tation of conceptual structure in the
ATIS task.
In Proceedings DARPA
Speech and Natural Language Work-
shop, 121–124.
Pierrehumbert, J. B. (1980).
The
Phonology and Phonetics of English
Intonation. Ph.D. thesis, MIT.
Pilehvar,
M.
T.
and
Camacho-
Collados,
J. (2019).
WiC: the
word-in-context dataset for evalu-
ating context-sensitive meaning rep-
resentations.
In NAACL HLT 2019,
1267–1273.
Pilehvar, M. T., Jurgens, D., and Nav-
igli, R. (2013). Align, disambiguate
and walk:
A uniﬁed approach for
measuring semantic similarity. In ACL
2013, 1341–1351.
Pitler, E., Louis, A., and Nenkova, A.
(2009).
Automatic sense prediction
for implicit discourse relations in text.
In ACL IJCNLP 2009, 683–691.
Pitler, E. and Nenkova, A. (2009). Us-
ing syntax to disambiguate explicit
discourse connectives in text. In ACL
IJCNLP 2009, 13–16.
Pitrelli, J. F., Beckman, M. E., and
Hirschberg, J. (1994). Evaluation of
prosodic transcription labeling relia-
bility in the ToBI framework.
In
ICSLP-94, Vol. 1, 123–126.
Pitt,
M. A.,
Dilley,
L.,
Johnson,
K., Kiesling, S., Raymond, W. D.,
Hume,
E.,
and Fosler-Lussier,
E.
(2007). Buckeye corpus of conversa-
tional speech (2nd release).. Depart-
ment of Psychology, Ohio State Uni-
versity (Distributor).
Pitt, M. A., Johnson, K., Hume, E.,
Kiesling, S., and Raymond, W. D.
(2005). The buckeye corpus of con-
versational speech: Labeling conven-
tions and a test of transcriber reliabil-
ity. Speech Communication, 45, 90–
95.
Plutchik, R. (1962).
The emotions:
Facts, theories, and a new model.
Random House.
Plutchik, R. (1980).
A general psy-
choevolutionary theory of emotion. In
Plutchik, R. and Kellerman, H. (Eds.),
Emotion: Theory, Research, and Ex-
perience, Volume 1, 3–33. Academic
Press.
Poesio, M., Stevenson, R., Di Eugenio,
B., and Hitzeman, J. (2004). Center-
ing: A parametric theory and its in-
stantiations. Computational Linguis-
tics, 30(3), 309–363.
Poesio, M., Stuckardt, R., and Versley,
Y. (2016). Anaphora resolution: Al-
gorithms, resources, and applications.
Springer.
Poesio, M., Sturt, P., Artstein, R., and
Filik, R. (2006).
Underspeciﬁcation
and anaphora: Theoretical issues and
preliminary evidence. Discourse pro-
cesses, 42(2), 157–175.
Poesio, M. and Vieira, R. (1998).
A
corpus-based investigation of deﬁnite
description use. Computational Lin-
guistics, 24(2), 183–216.
Polanyi, L. (1988). A formal model of
the structure of discourse. Journal of
Pragmatics, 12.
Polanyi, L., Culy, C., van den Berg, M.,
Thione, G. L., and Ahn, D. (2004a). A
rule based approach to discourse pars-
ing. In Proceedings of SIGDIAL.
Polanyi, L., Culy, C., van den Berg,
M., Thione, G. L., and Ahn, D.
(2004b). Sentential structure and dis-
course parsing. In ACL04 Discourse
Annotation Workshop.
Polifroni, J., Hirschman, L., Seneff,
S., and Zue, V. W. (1992).
Exper-
iments in evaluating interactive spo-
ken language systems.
In Proceed-
ings DARPA Speech and Natural Lan-
guage Workshop, 28–33.
Pollard, C. and Sag, I. A. (1994).
Head-Driven Phrase Structure Gram-
mar. University of Chicago Press.
Ponzetto, S. P. and Navigli, R. (2010).
Knowledge-rich word sense disam-
biguation rivaling supervised systems.
In ACL 2010, 1522–1531.
Ponzetto, S. P. and Strube, M. (2006).
Exploiting semantic role labeling,
WordNet and Wikipedia for corefer-
ence resolution. In HLT-NAACL-06,
192–199.
Ponzetto, S. P. and Strube, M. (2007).
Knowledge derived from Wikipedia
for computing semantic relatedness.
JAIR, 30, 181–212.
Porter, M. F. (1980). An algorithm for
sufﬁx stripping. Program, 14(3), 130–
137.
Potts, C. (2011).
On the negativity
of negation.
In Li, N. and Lutz, D.
(Eds.), Proceedings of Semantics and
Linguistic Theory 20, 636–659. CLC
Publications, Ithaca, NY.
Pradhan, S., Hovy, E. H., Marcus,
M. P., Palmer, M., Ramshaw, L., and
Weischedel, R. (2007a). OntoNotes:
A uniﬁed relational semantic repre-
sentation.
In Proceedings of ICSC,
517–526.
Pradhan,
S.,
Hovy,
E. H.,
Mar-
cus, M. P., Palmer, M., Ramshaw,
L. A., and Weischedel, R. M. (2007b).
Ontonotes: a uniﬁed relational seman-
tic representation.
Int. J. Semantic
Computing, 1(4), 405–419.
Pradhan, S., Luo, X., Recasens, M.,
Hovy, E. H., Ng, V., and Strube, M.
(2014). Scoring coreference partitions
of predicted mentions: A reference
implementation. In ACL 2014.
Pradhan, S., Moschitti, A., Xue, N.,
Ng, H. T., Bj¨orkelund, A., Uryupina,
O., Zhang, Y., and Zhong, Z. (2013).
Towards robust linguistic analysis us-
ing OntoNotes. In CoNLL-13, 143–
152.
Pradhan, S., Moschitti, A., Xue, N.,
Uryupina, O., and Zhang, Y. (2012a).
CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference
in OntoNotes. In CoNLL-12.
Pradhan, S., Moschitti, A., Xue, N.,
Uryupina, O., and Zhang, Y. (2012b).
Conll-2012 shared task:
Modeling
multilingual unrestricted coreference
in OntoNotes. In CoNLL-12.
Pradhan, S., Ramshaw, L., Marcus,
M. P., Palmer, M., Weischedel, R., and
Xue, N. (2011). CoNLL-2011 shared
task: Modeling unrestricted corefer-
ence in OntoNotes. In CoNLL-11.
Pradhan,
S.,
Ramshaw,
L.,
Wei-
schedel, R., MacBride, J., and Micci-
ulla, L. (2007). Unrestricted corefer-
ence: Identifying entities and events
in OntoNotes.
In Proceedings of
ICSC 2007, 446–453.
Pradhan, S., Ward, W., Hacioglu, K.,
Martin, J. H., and Jurafsky, D. (2005).
Semantic role labeling using different
syntactic views. In ACL-05.
Prasad, R., Dinesh, N., Lee, A., Milt-
sakaki, E., Robaldo, L., Joshi, A. K.,
and Webber, B. L. (2008). The Penn
Discourse TreeBank 2.0. In LREC-08.
Prasad, R., Webber, B. L., and Joshi,
A.
(2014).
Reﬂections
on
the
Penn Discourse Treebank, compara-
ble corpora, and complementary an-
notation. Computational Linguistics,
40(4), 921–950.

586
Bibliography
Price, P. J., Ostendorf, M., Shattuck-
Hufnagel, S., and Fong, C. (1991).
The use of prosody in syntactic disam-
biguation. JASA, 90(6).
Prince, E. (1981a). Toward a taxonomy
of given-new information. In Cole, P.
(Ed.), Radical Pragmatics, 223–256.
Academic Press.
Prince, E. (1981b). Toward a taxonomy
of given-new information. In Cole, P.
(Ed.), Radical Pragmatics, 223–255.
Academic Press.
Propp, V. (1968). Morphology of the
Folktale (2nd Ed.).
University of
Texas Press. Original Russian 1928.
Translated by Laurence Scott.
Pu, X., Pappas, N., Henderson, J., and
Popescu-Belis, A. (2018). Integrating
weakly supervised word sense disam-
biguation into neural machine transla-
tion. TACL, 6, 635–649.
Pullum, G. K. and Ladusaw, W. A.
(1996). Phonetic Symbol Guide (2nd
Ed.). University of Chicago.
Purver, M. (2004). The theory and use
of clariﬁcation requests in dialogue.
Ph.D. thesis, University of London.
Pustejovsky, J. (1991). The generative
lexicon.
Computational Linguistics,
17(4).
Pustejovsky, J. (1995). The Generative
Lexicon. MIT Press.
Pustejovsky, J. and Boguraev, B. K.
(Eds.). (1996).
Lexical Semantics:
The Problem of Polysemy.
Oxford
University Press.
Pustejovsky, J., Casta˜no, J., Ingria, R.,
Saur´ı, R., Gaizauskas, R., Setzer, A.,
and Katz, G. (2003a). TimeML: ro-
bust speciﬁcation of event and tempo-
ral expressions in text. In Proceedings
of the 5th International Workshop on
Computational Semantics (IWCS-5).
Pustejovsky, J., Hanks, P., Saur´ı, R.,
See, A., Gaizauskas, R., Setzer, A.,
Radev, D., Sundheim, B., Day, D. S.,
Ferro, L., and Lazo, M. (2003b). The
TIMEBANK corpus. In Proceedings
of Corpus Linguistics 2003 Confer-
ence,
647–656.
UCREL Technical
Paper number 16.
Pustejovsky, J., Ingria, R., Saur´ı, R.,
Casta˜no, J., Littman, J., Gaizauskas,
R., Setzer, A., Katz, G., and Mani, I.
(2005).
The Speciﬁcation Language
TimeML, chap. 27. Oxford.
Qin, L., Zhang, Z., and Zhao, H.
(2016).
A stacking gated neural ar-
chitecture for implicit discourse rela-
tion classiﬁcation. In EMNLP 2016,
2263–2270.
Qin, L., Zhang, Z., Zhao, H., Hu,
Z., and Xing, E. (2017).
Adversar-
ial connective-exploiting networks for
implicit discourse relation classiﬁca-
tion. In ACL 2017, 1006–1017.
Quillian, M. R. (1968). Semantic mem-
ory. In Minsky, M. (Ed.), Semantic In-
formation Processing, 227–270. MIT
Press.
Quillian, M. R. (1969). The teachable
language comprehender:
A simula-
tion program and theory of language.
CACM, 12(8), 459–476.
Quirk, R., Greenbaum, S., Leech, G.,
and Svartvik, J. (1985).
A Compre-
hensive Grammar of the English Lan-
guage. Longman.
Rabiner, L. R. and Schafer, R. W.
(1978). Digital Processing of Speech
Signals. Prentice Hall.
Rabiner, L. R. (1989).
A tutorial
on hidden Markov models and se-
lected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2),
257–286.
Rabiner, L. R. and Juang, B. H. (1993).
Fundamentals of Speech Recognition.
Prentice Hall.
Radford, A. (1997). Syntactic Theory
and the Structure of English: A Mini-
malist Approach. Cambridge Univer-
sity Press.
Raganato, A., Bovi, C. D., and Navigli,
R. (2017a). Neural sequence learning
models for word sense disambigua-
tion. In EMNLP 2017, 1156–1167.
Raganato, A., Camacho-Collados, J.,
and Navigli, R. (2017b). Word sense
disambiguation: A uniﬁed evaluation
framework and empirical comparison.
In EACL-17, 99–110.
Raghunathan, K., Lee, H., Rangara-
jan, S., Chambers, N., Surdeanu, M.,
Jurafsky, D., and Manning, C. D.
(2010). A multi-pass sieve for coref-
erence resolution. In Proceedings of
EMNLP 2010, 492–501.
Rahman, A. and Ng, V. (2009). Super-
vised models for coreference resolu-
tion. In EMNLP-09, 968–977.
Rahman, A. and Ng, V. (2012). Re-
solving complex cases of deﬁnite pro-
nouns: the Winograd Schema chal-
lenge. In EMNLP 2012, 777–789.
Rajpurkar, P., Jia, R., and Liang, P.
(2018). Know what you don’t know:
Unanswerable questions for SQuAD.
In ACL 2018.
Rajpurkar, P., Zhang, J., Lopyrev,
K., and Liang, P. (2016).
SQuAD:
100,000+ questions for machine com-
prehension of text. In EMNLP 2016.
Ramshaw, L. A. and Marcus, M. P.
(1995).
Text
chunking
using
transformation-based learning.
In
Proceedings of the 3rd Annual Work-
shop on Very Large Corpora, 82–94.
Ranganath, R., Jurafsky, D., and Mc-
Farland, D. A. (2013).
Detecting
friendly, ﬂirtatious, awkward, and as-
sertive speech in speed-dates. Com-
puter Speech and Language, 27(1),
89–115.
Raphael, B. (1968). SIR: A computer
program for semantic information re-
trieval. In Minsky, M. (Ed.), Seman-
tic Information Processing, 33–145.
MIT Press.
Rashkin, H., Bell, E., Choi, Y., and
Volkova, S. (2017). Multilingual con-
notation frames: A case study on so-
cial media for targeted sentiment anal-
ysis and forecast. In ACL 2017, 459–
464.
Rashkin, H., Singh, S., and Choi, Y.
(2016). Connotation frames: A data-
driven investigation.
In ACL 2016,
311–321.
Rashkin, H., Smith, E. M., Li, M.,
and Boureau, Y.-L. (2019).
To-
wards empathetic open-domain con-
versation models: A new benchmark
and dataset.
In ACL 2019,
5370–
5381.
Ratinov,
L. and Roth,
D. (2012).
Learning-based
multi-sieve
co-
reference resolution with knowledge.
In EMNLP 2012, 1234–1244.
Ratnaparkhi, A. (1996).
A maxi-
mum entropy part-of-speech tagger. In
EMNLP 1996, 133–142.
Ratnaparkhi, A. (1997). A linear ob-
served time statistical parser based on
maximum entropy models. In EMNLP
1997, 1–10.
Ratnaparkhi, A., Reynar, J. C., and
Roukos, S. (1994). A maximum en-
tropy model for prepositional phrase
attachment.
In ARPA Human Lan-
guage Technologies Workshop, 250–
255.
Recasens, M. and Hovy, E. H. (2011).
BLANC: Implementing the Rand in-
dex for coreference evaluation. Nat-
ural Language Engineering, 17(4),
485–510.
Recasens, M., Hovy, E. H., and Mart´ı,
M. A. (2011). Identity, non-identity,
and near-identity:
Addressing the
complexity of coreference.
Lingua,
121(6), 1138–1152.
Recasens, M. and Mart´ı, M. A. (2010).
AnCora-CO:
Coreferentially
anno-
tated corpora for Spanish and Catalan.
Language Resources and Evaluation,
44(4), 315–345.
Reed, C., Mochales Palau, R., Rowe,
G., and Moens, M.-F. (2008).
Lan-
guage resources for studying argu-
ment. In LREC-08, 91–100.
Rehder, B., Schreiner, M. E., Wolfe,
M. B. W., Laham, D., Landauer, T. K.,
and Kintsch, W. (1998). Using Latent
Semantic Analysis to assess knowl-
edge: Some technical considerations.
Discourse Processes, 25(2-3), 337–
354.
Reichenbach, H. (1947).
Elements
of Symbolic Logic. Macmillan, New
York.

Bibliography
587
Reichert, T. A., Cohen, D. N., and
Wong, A. K. C. (1973).
An appli-
cation of information theory to ge-
netic mutations and the matching of
polypeptide sequences.
Journal of
Theoretical Biology, 42, 245–261.
Reichman, R. (1985).
Getting Com-
puters to Talk Like You and Me. MIT
Press.
Resnik, P. (1992).
Probabilistic tree-
adjoining grammar as a framework for
statistical natural language process-
ing. In COLING-92, 418–424.
Resnik, P. (1993). Semantic classes and
syntactic ambiguity.
In Proceedings
of the workshop on Human Language
Technology, 278–283.
Resnik, P. (1996).
Selectional con-
straints:
An
information-theoretic
model and its computational realiza-
tion. Cognition, 61, 127–159.
Richardson, M., Burges, C. J. C., and
Renshaw, E. (2013).
MCTest:
A
challenge dataset for the open-domain
machine comprehension of text.
In
EMNLP 2013, 193–203.
Riedel, S., Yao, L., and McCallum,
A. (2010).
Modeling relations and
their mentions without labeled text.
In Machine Learning and Knowledge
Discovery in Databases,
148–163.
Springer.
Riedel, S., Yao, L., McCallum, A., and
Marlin, B. M. (2013). Relation extrac-
tion with matrix factorization and uni-
versal schemas. In NAACL HLT 2013.
Riesbeck, C. K. (1975).
Conceptual
analysis. In Schank, R. C. (Ed.), Con-
ceptual Information Processing, 83–
156. American Elsevier, New York.
Riloff, E. (1993). Automatically con-
structing a dictionary for information
extraction tasks. In AAAI-93,
811–
816.
Riloff, E. (1996). Automatically gen-
erating extraction patterns from un-
tagged text. In AAAI-96, 117–124.
Riloff, E. and Jones, R. (1999). Learn-
ing dictionaries for information ex-
traction by multi-level bootstrapping.
In AAAI-99, 474–479.
Riloff,
E. and Schmelzenbach,
M.
(1998). An empirical approach to con-
ceptual case frame acquisition.
In
Proceedings of the Sixth Workshop on
Very Large Corpora, 49–56.
Riloff, E. and Shepherd, J. (1997). A
corpus-based approach for building
semantic lexicons. In EMNLP 1997.
Riloff, E. and Thelen, M. (2000).
A
rule-based question answering system
for reading comprehension tests.
In
Proceedings of ANLP/NAACL work-
shop on reading comprehension tests,
13–19.
Riloff, E. and Wiebe, J. (2003). Learn-
ing extraction patterns for subjective
expressions. In EMNLP 2003.
Ritter, A., Cherry, C., and Dolan, B.
(2010).
Unsupervised modeling of
twitter conversations. In HLT-NAACL.
Ritter, A., Cherry, C., and Dolan, B.
(2011). Data-driven response gener-
ation in social media. In EMNLP-11,
583–593.
Ritter, A., Etzioni, O., and Mausam
(2010).
A latent dirichlet allocation
method for selectional preferences. In
ACL 2010, 424–434.
Ritter, A., Zettlemoyer, L., Mausam,
and Etzioni, O. (2013).
Modeling
missing data in distant supervision
for information extraction.. TACL, 1,
367–378.
Robins, R. H. (1967).
A Short His-
tory of Linguistics. Indiana University
Press, Bloomington.
Rohde, D. L. T., Gonnerman, L. M.,
and Plaut, D. C. (2006). An improved
model of semantic similarity based on
lexical co-occurrence. CACM, 8, 627–
633.
Rooth, M., Riezler, S., Prescher, D.,
Carroll, G., and Beil, F. (1999). In-
ducing a semantically annotated lexi-
con via EM-based clustering. In ACL-
99, 104–111.
Rosenblatt, F. (1958).
The percep-
tron: A probabilistic model for infor-
mation storage and organization in the
brain..
Psychological review, 65(6),
386–408.
Rosenfeld, R. (1996). A maximum en-
tropy approach to adaptive statistical
language modeling. Computer Speech
and Language, 10, 187–228.
Rosenthal,
S.
and
McKeown,
K.
(2017). Detecting inﬂuencers in mul-
tiple online genres. ACM Transactions
on Internet Technology (TOIT), 17(2).
Rothe, S., Ebert, S., and Sch¨utze, H.
(2016). Ultradense Word Embeddings
by Orthogonal Transformation.
In
NAACL HLT 2016.
Roy, N., Pineau, J., and Thrun, S.
(2000).
Spoken dialog management
for robots. In ACL-00.
Rudinger,
R.,
Naradowsky,
J.,
Leonard, B., and Van Durme, B.
(2018).
Gender bias in coreference
resolution. In NAACL HLT 2018.
Rumelhart, D. E., Hinton, G. E., and
Williams, R. J. (1986). Learning in-
ternal representations by error propa-
gation. In Rumelhart, D. E. and Mc-
Clelland, J. L. (Eds.), Parallel Dis-
tributed Processing, Vol. 2, 318–362.
MIT Press.
Rumelhart, D. E. and McClelland,
J. L. (1986a).
On learning the past
tense of English verbs.
In Rumel-
hart, D. E. and McClelland, J. L.
(Eds.), Parallel Distributed Process-
ing, Vol. 2, 216–271. MIT Press.
Rumelhart, D. E. and McClelland,
J. L. (Eds.). (1986b).
Parallel Dis-
tributed Processing. MIT Press.
Rumelhart, D. E., McClelland, J. L.,
and The PDP Research Group (1986).
Parallel Distributed Processing: Ex-
plorations in the Microstructure of
Cognition, Vol. 1: Foundations. Brad-
ford Books (MIT Press).
Ruppenhofer,
J.,
Ellsworth,
M.,
Petruck, M. R. L., Johnson, C. R.,
Baker,
C. F.,
and Scheffczyk,
J.
(2016). FrameNet II: Extended theory
and practice..
Ruppenhofer,
J.,
Sporleder,
C.,
Morante, R., Baker, C. F., and Palmer,
M. (2010).
Semeval-2010 task 10:
Linking events and their participants
in discourse. In Proceedings of the 5th
International Workshop on Semantic
Evaluation, 45–50.
Russell, J. A. (1980).
A circumplex
model of affect. Journal of personality
and social psychology, 39(6), 1161–
1178.
Russell, S. and Norvig, P. (2002). Ar-
tiﬁcial Intelligence:
A Modern Ap-
proach (2nd Ed.). Prentice Hall.
Rutherford, A. and Xue, N. (2015).
Improving the inference of implicit
discourse relations via classifying ex-
plicit discourse connectives.
In
NAACL HLT 2015, 799–808.
Sacks, H., Schegloff, E. A., and Jeffer-
son, G. (1974). A simplest systematics
for the organization of turn-taking for
conversation. Language, 50(4), 696–
735.
Sag, I. A. and Liberman, M. Y. (1975).
The intonational disambiguation of
indirect speech acts. In CLS-75, 487–
498. University of Chicago.
Sag, I. A., Wasow, T., and Bender,
E. M. (Eds.). (2003). Syntactic The-
ory: A Formal Introduction.
CSLI
Publications, Stanford, CA.
Sagae, K. (2009). Analysis of discourse
structure with syntactic dependencies
and data-driven shift-reduce parsing.
In IWPT-09, 81–84.
Sahami, M., Dumais, S. T., Hecker-
man, D., and Horvitz, E. (1998). A
Bayesian approach to ﬁltering junk e-
mail. In AAAI Workshop on Learning
for Text Categorization, 98–105.
Sakoe, H. and Chiba, S. (1971). A dy-
namic programming approach to con-
tinuous speech recognition.
In Pro-
ceedings of the Seventh International
Congress on Acoustics, Vol. 3, 65–69.
Akad´emiai Kiad´o.

588
Bibliography
Salomaa, A. (1969). Probabilistic and
weighted grammars. Information and
Control, 15, 529–544.
Salton, G. (1971).
The SMART Re-
trieval System: Experiments in Auto-
matic Document Processing. Prentice
Hall.
Sampson, G. (1987). Alternative gram-
matical coding systems.
In Gar-
side, R., Leech, G., and Sampson, G.
(Eds.), The Computational Analysis of
English, 165–183. Longman.
Samuelsson, C. (1993).
Morphologi-
cal tagging based entirely on Bayesian
inference.
In 9th Nordic Confer-
ence on Computational Linguistics
NODALIDA-93. Stockholm.
Sankoff, D. (1972).
Matching se-
quences under deletion-insertion con-
straints. Proceedings of the National
Academy of Sciences, 69, 4–6.
Sankoff, D. and Labov, W. (1979). On
the uses of variable rules. Language
in society, 8(2-3), 189–222.
Sap, M., Prasettio, M. C., Holtzman,
A., Rashkin, H., and Choi, Y. (2017).
Connotation frames of power and
agency in modern ﬁlms. In EMNLP
2017, 2329–2334.
Scha, R. and Polanyi, L. (1988).
An
augmented context free grammar for
discourse. In COLING-88, 573–577.
Schabes, Y. (1990). Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Schabes, Y. (1992).
Stochastic lexi-
calized tree-adjoining grammars.
In
COLING-92, 426–433.
Schabes, Y., Abeill´e, A., and Joshi,
A. K. (1988). Parsing strategies with
‘lexicalized’ grammars: Applications
to Tree Adjoining Grammars.
In
COLING-88, 578–583.
Schank, R. C. (1972). Conceptual de-
pendency: A theory of natural lan-
guage processing. Cognitive Psychol-
ogy, 3, 552–631.
Schank, R. C. and Abelson, R. P.
(1975).
Scripts, plans, and knowl-
edge.
In Proceedings of IJCAI-75,
151–157.
Schank, R. C. and Abelson, R. P.
(1977). Scripts, Plans, Goals and Un-
derstanding. Lawrence Erlbaum.
Schegloff, E. A. (1968).
Sequencing
in conversational openings. American
Anthropologist, 70, 1075–1095.
Scherer, K. R. (2000). Psychological
models of emotion.
In Borod, J. C.
(Ed.), The neuropsychology of emo-
tion, 137–162. Oxford.
Schiebinger, L. (2019).
Machine
translation:
Analyzing
gender.
http://genderedinnovations.
stanford.edu/case-studies/
nlp.html#tabs-2.
Schneider, N., Hwang, J. D., Sriku-
mar, V., Prange, J., Blodgett, A.,
Moeller, S. R., Stern, A., Bitan, A.,
and Abend, O. (2018). Comprehen-
sive supersense disambiguation of En-
glish prepositions and possessives. In
ACL 2018, 185–196.
Schone, P. and Jurafsky, D. (2000).
Knowlege-free induction of morphol-
ogy using latent semantic analysis. In
CoNLL-00.
Schone, P. and Jurafsky, D. (2001).
Knowledge-free induction of inﬂec-
tional morphologies. In NAACL 2001.
Sch¨onﬁnkel, M. (1924).
¨Uber die
Bausteine der mathematischen Logik.
Mathematische Annalen, 92, 305–
316.
English translation appears in
From Frege to G¨odel: A Source Book
in Mathematical Logic, Harvard Uni-
versity Press, 1967.
Schuster,
M. and Paliwal,
K. K.
(1997). Bidirectional recurrent neural
networks. IEEE Transactions on Sig-
nal Processing, 45, 2673–2681.
Sch¨utze, H. (1992a). Context space. In
Goldman, R. (Ed.), Proceedings of the
1992 AAAI Fall Symposium on Prob-
abilistic Approaches to Natural Lan-
guage.
Sch¨utze, H. (1992b).
Dimensions of
meaning.
In Proceedings of Super-
computing ’92, 787–796. IEEE Press.
Sch¨utze,
H. (1997a).
Ambiguity
Resolution in Language Learning –
Computational and Cognitive Models.
CSLI, Stanford, CA.
Sch¨utze, H. (1997b). Ambiguity Reso-
lution in Language Learning: Compu-
tational and Cognitive Models. CSLI
Publications, Stanford, CA.
Sch¨utze, H. (1998).
Automatic word
sense discrimination. Computational
Linguistics, 24(1), 97–124.
Sch¨utze, H., Hull, D. A., and Pedersen,
J. (1995). A comparison of classiﬁers
and document representations for the
routing problem. In SIGIR-95, 229–
237.
Sch¨utze, H. and Pedersen, J. (1993).
A vector model for syntagmatic and
paradigmatic relatedness. In Proceed-
ings of the 9th Annual Conference of
the UW Centre for the New OED and
Text Research, 104–113.
Sch¨utze, H. and Singer, Y. (1994).
Part-of-speech tagging using a vari-
able memory Markov model. In ACL-
94, 181–187.
Schwartz, H. A., Eichstaedt, J. C.,
Kern, M. L., Dziurzynski, L., Ra-
mones, S. M., Agrawal, M., Shah, A.,
Kosinski, M., Stillwell, D., Seligman,
M. E. P., and Ungar, L. H. (2013).
Personality, gender, and age in the
language of social media: The open-
vocabulary approach. PloS one, 8(9),
e73791.
Schwenk, H. (2007). Continuous space
language models. Computer Speech &
Language, 21(3), 492–518.
S´eaghdha, D. O. (2010). Latent vari-
able models of selectional preference.
In ACL 2010, 435–444.
Seddah, D., Tsarfaty, R., K¨ubler, S.,
Candito, M., Choi, J. D., Farkas,
R., Foster, J., Goenaga, I., Gojenola,
K., Goldberg, Y., Green, S., Habash,
N., Kuhlmann, M., Maier, W., Nivre,
J.,
Przepi´orkowski,
A.,
Roth,
R.,
Seeker,
W.,
Versley,
Y.,
Vincze,
V., Woli´nski, M., Wr´oblewska, A.,
and Villemonte de la Cl´ergerie, E.
(2013). Overview of the SPMRL 2013
shared task: cross-framework evalua-
tion of parsing morphologically rich
languages.
In Proceedings of the
4th Workshop on Statistical Parsing of
Morphologically-Rich Languages.
Sekine, S. and Collins, M. (1997).
The evalb software.
http://cs.
nyu.edu/cs/projects/proteus/
evalb.
Seneff, S. and Zue, V. W. (1988).
Transcription and alignment of the
TIMIT database.
In Proceedings of
the Second Symposium on Advanced
Man-Machine Interface through Spo-
ken Language.
Sennrich, R., Haddow, B., and Birch,
A. (2016). Neural machine translation
of rare words with subword units. In
ACL 2016.
Seo, M., Kembhavi, A., Farhadi, A.,
and Hajishirzi, H. (2017).
Bidirec-
tional attention ﬂow for machine com-
prehension. In ICLR 2017.
Serban, I. V., Lowe, R., Henderson, P.,
Charlin, L., and Pineau, J. (2018). A
survey of available corpora for build-
ing data-driven dialogue systems: The
journal version.
Dialogue & Dis-
course, 9(1), 1–49.
Sgall, P., Hajiˇcov´a, E., and Panevova, J.
(1986). The Meaning of the Sentence
in its Pragmatic Aspects. Reidel.
Shang, L., Lu, Z., and Li, H. (2015).
Neural responding machine for short-
text conversation.
In ACL 2015,
1577–1586.
Shannon, C. E. (1948).
A math-
ematical theory of communication.
Bell System Technical Journal, 27(3),
379–423. Continued in the following
volume.
Shannon, C. E. (1951). Prediction and
entropy of printed English. Bell Sys-
tem Technical Journal, 30, 50–64.
Sheil, B. A. (1976). Observations on
context free parsing. SMIL: Statistical
Methods in Linguistics, 1, 71–109.
Shockey, L. (2003). Sound Patterns of
Spoken English. Blackwell.

Bibliography
589
Shoup, J. E. (1980). Phonological as-
pects of speech recognition. In Lea,
W. A. (Ed.), Trends in Speech Recog-
nition, 125–138. Prentice Hall.
Shriberg, E., Bates, R., Taylor, P., Stol-
cke, A., Jurafsky, D., Ries, K., Coc-
caro, N., Martin, R., Meteer, M., and
Van Ess-Dykema, C. (1998).
Can
prosody aid the automatic classiﬁca-
tion of dialog acts in conversational
speech?. Language and Speech (Spe-
cial Issue on Prosody and Conversa-
tion), 41(3-4), 439–487.
Sidner, C. L. (1979). Towards a com-
putational theory of deﬁnite anaphora
comprehension in English discourse.
Tech. rep. 537, MIT Artiﬁcial Intelli-
gence Laboratory, Cambridge, MA.
Sidner, C. L. (1983). Focusing in the
comprehension of deﬁnite anaphora.
In Brady, M. and Berwick, R. C.
(Eds.), Computational Models of Dis-
course, 267–330. MIT Press.
Silverman,
K.,
Beckman,
M.
E.,
Pitrelli, J. F., Ostendorf, M., Wight-
man, C. W., Price, P. J., Pierrehum-
bert, J. B., and Hirschberg, J. (1992).
ToBI: A standard for labelling English
prosody. In ICSLP-92, Vol. 2, 867–
870.
Simmons, R. F. (1965). Answering En-
glish questions by computer: A sur-
vey. CACM, 8(1), 53–70.
Simmons, R. F. (1973). Semantic net-
works:
Their computation and use
for understanding English sentences.
In Schank, R. C. and Colby, K. M.
(Eds.), Computer Models of Thought
and Language,
61–113. W.H. Free-
man and Co.
Simmons, R. F., Klein, S., and Mc-
Conlogue, K. (1964).
Indexing and
dependency logic for answering en-
glish questions. American Documen-
tation, 15(3), 196–204.
Simons, G. F. and Fennig, C. D. (2018).
Ethnologue: Languages of the world,
twenty-ﬁrst edition..
SIL Interna-
tional.
Singh, S. P., Litman, D. J., Kearns, M.,
and Walker, M. A. (2002). Optimizing
dialogue management with reinforce-
ment learning: Experiments with the
NJFun system. JAIR, 16, 105–133.
Sleator, D. and Temperley, D. (1993).
Parsing English with a link grammar.
In IWPT-93.
Small, S. L. and Rieger, C. (1982).
Parsing
and
comprehending
with
Word Experts.
In Lehnert, W. G.
and Ringle, M. H. (Eds.), Strategies
for Natural Language Processing,
89–147. Lawrence Erlbaum.
Smith, V. L. and Clark, H. H. (1993).
On the course of answering questions.
Journal of Memory and Language, 32,
25–38.
Smolensky, P. (1988). On the proper
treatment of connectionism.
Behav-
ioral and brain sciences, 11(1), 1–23.
Smolensky, P. (1990). Tensor product
variable binding and the representa-
tion of symbolic structures in connec-
tionist systems. Artiﬁcial intelligence,
46(1-2), 159–216.
Snow, R., Jurafsky, D., and Ng, A. Y.
(2005).
Learning syntactic patterns
for automatic hypernym discovery. In
Saul, L. K., Weiss, Y., and Bottou,
L. (Eds.), NIPS 17, 1297–1304. MIT
Press.
Snow, R., Prakash, S., Jurafsky, D.,
and Ng, A. Y. (2007).
Learning to
merge word senses.
In EMNLP/-
CoNLL 2007, 1005–1014.
Snyder, B. and Palmer, M. (2004).
The English all-words task.
In
SENSEVAL-3, 41–43.
Socher, R., Huval, B., Manning, C. D.,
and Ng, A. Y. (2012).
Seman-
tic compositionality through recursive
matrix-vector spaces.
In EMNLP
2012, 1201–1211.
Soderland, S., Fisher, D., Aseltine, J.,
and Lehnert, W. G. (1995).
CRYS-
TAL: Inducing a conceptual dictio-
nary. In IJCAI-95, 1134–1142.
Søgaard, A. (2010).
Simple semi-
supervised training of part-of-speech
taggers. In ACL 2010, 205–208.
Søgaard, A. and Goldberg, Y. (2016).
Deep multi-task learning with low
level tasks supervised at lower layers.
In ACL 2016.
Søgaard, A., Johannsen, A., Plank, B.,
Hovy, D., and Alonso, H. M. (2014).
What’s in a p-value in NLP?.
In
CoNLL-14.
Solorio, T., Blair, E., Maharjan, S.,
Bethard, S., Diab, M., Ghoneim, M.,
Hawwari, A., AlGhamdi, F., Hirsch-
berg, J., Chang, A., and Fung, P.
(2014). Overview for the ﬁrst shared
task on language identiﬁcation in
code-switched data. In Proceedings of
the First Workshop on Computational
Approaches to Code Switching, 62–
72.
Somasundaran, S., Burstein, J., and
Chodorow, M. (2014). Lexical chain-
ing for measuring discourse coher-
ence quality in test-taker essays.
In
COLING-14, 950–961.
Soon, W. M., Ng, H. T., and Lim, D.
C. Y. (2001).
A machine learning
approach to coreference resolution of
noun phrases. Computational Linguis-
tics, 27(4), 521–544.
Sordoni, A., Galley, M., Auli, M.,
Brockett, C., Ji, Y., Mitchell, M., Nie,
J.-Y., Gao, J., and Dolan, B. (2015).
A neural network approach to context-
sensitive generation of conversational
responses.
In NAACL HLT 2015,
196–205.
Soricut, R. and Marcu, D. (2003). Sen-
tence level discourse parsing using
syntactic and lexical information. In
HLT-NAACL-03, 149–156.
Soricut, R. and Marcu, D. (2006). Dis-
course generation using utility-trained
coherence models. In COLING/ACL
2006, 803–810.
Sparck Jones, K. (1972). A statistical
interpretation of term speciﬁcity and
its application in retrieval. Journal of
Documentation, 28(1), 11–21.
Sparck Jones, K. (1986).
Synonymy
and Semantic Classiﬁcation.
Edin-
burgh University Press, Edinburgh.
Republication of 1964 PhD Thesis.
Spitkovsky, V. I. and Chang, A. X.
(2012).
A cross-lingual dictionary
for English Wikipedia concepts.
In
LREC-12.
Sporleder,
C. and Lascarides,
A.
(2005).
Exploiting linguistic cues
to classify rhetorical relations.
In
RANLP-05.
Sporleder, C. and Lapata, M. (2005).
Discourse chunking and its applica-
tion to sentence compression. In HLT-
EMNLP-05, 257–264.
Srivastava,
N.,
Hinton,
G.
E.,
Krizhevsky, A., Sutskever, I., and
Salakhutdinov, R. R. (2014). Dropout:
a simple way to prevent neural net-
works from overﬁtting.. JMLR, 15(1),
1929–1958.
Stab, C. and Gurevych, I. (2014a).
Annotating argument components and
relations in persuasive essays.
In
COLING-14, 1501–1510.
Stab, C. and Gurevych, I. (2014b).
Identifying argumentative discourse
structures in persuasive essays.
In
EMNLP 2014, 46–56.
Stab, C. and Gurevych, I. (2017). Pars-
ing argumentation structures in per-
suasive essays.
Computational Lin-
guistics, 43(3), 619–659.
Stalnaker, R. C. (1978).
Assertion.
In Cole, P. (Ed.), Pragmatics: Syntax
and Semantics Volume 9,
315–332.
Academic Press.
Stamatatos, E. (2009).
A survey of
modern authorship attribution meth-
ods. JASIST, 60(3), 538–556.
Stede, M. (2011). Discourse process-
ing. Morgan & Claypool.
Stede, M. and Schneider, J. (2018). Ar-
gumentation Mining. Morgan & Clay-
pool.
Steedman, M. (1989).
Constituency
and coordination in a combinatory
grammar. In Baltin, M. R. and Kroch,
A. S. (Eds.), Alternative Conceptions
of Phrase Structure,
201–231. Uni-
versity of Chicago.
Steedman, M. (1996). Surface Struc-
ture and Interpretation.
MIT Press.
Linguistic Inquiry Monograph, 30.

590
Bibliography
Steedman, M. (2000).
The Syntactic
Process. The MIT Press.
Steedman, M. (2007).
Information-
structural semantics for English in-
tonation.
In Lee, C., Gordon, M.,
and B¨uring, D. (Eds.), Topic and Fo-
cus: Cross-Linguistic Perspectives on
Meaning and Intonation,
245–264.
Springer.
Stetina, J. and Nagao, M. (1997). Cor-
pus based PP attachment ambiguity
resolution with a semantic dictionary.
In Zhou, J. and Church, K. W. (Eds.),
Proceedings of the Fifth Workshop on
Very Large Corpora, 66–80.
Stevens, K. N. (1998). Acoustic Pho-
netics. MIT Press.
Stevens, K. N. and House, A. S. (1955).
Development of a quantitative de-
scription of vowel articulation. JASA,
27, 484–493.
Stevens, K. N. and House, A. S. (1961).
An acoustical theory of vowel pro-
duction and some of its implications.
Journal of Speech and Hearing Re-
search, 4, 303–320.
Stevens, K. N., Kasowski, S., and Fant,
G. M. (1953). An electrical analog of
the vocal tract. JASA, 25(4), 734–742.
Stevens, S. S. and Volkmann, J. (1940).
The relation of pitch to frequency: A
revised scale. The American Journal
of Psychology, 53(3), 329–353.
Stevens, S. S., Volkmann, J., and New-
man, E. B. (1937).
A scale for
the measurement of the psychological
magnitude pitch. JASA, 8, 185–190.
Stifelman, L. J., Arons, B., Schmandt,
C.,
and
Hulteen,
E.
A.
(1993).
VoiceNotes: A speech interface for
a hand-held voice notetaker. In Hu-
man Factors in Computing Systems:
INTERCHI ’93 Conference Proceed-
ings, 179–186.
Stolcke,
A. (1995).
An efﬁcient
probabilistic context-free parsing al-
gorithm that computes preﬁx proba-
bilities.
Computational Linguistics,
21(2), 165–202.
Stolcke, A. (1998).
Entropy-based
pruning of backoff language mod-
els.
In Proc. DARPA Broadcast
News Transcription and Understand-
ing Workshop, 270–274.
Stolcke, A. (2002).
SRILM – an ex-
tensible language modeling toolkit. In
ICSLP-02.
Stolcke, A., Ries, K., Coccaro, N.,
Shriberg, E., Bates, R., Jurafsky, D.,
Taylor, P., Martin, R., Meteer, M., and
Van Ess-Dykema, C. (2000).
Dia-
logue act modeling for automatic tag-
ging and recognition of conversational
speech.
Computational Linguistics,
26(3), 339–371.
Stolz, W. S., Tannenbaum, P. H., and
Carstensen, F. V. (1965). A stochastic
approach to the grammatical coding of
English. CACM, 8(6), 399–405.
Stone, P., Dunphry, D., Smith, M., and
Ogilvie, D. (1966). The General In-
quirer: A Computer Approach to Con-
tent Analysis. Cambridge, MA: MIT
Press.
Stoyanchev,
S.
and
Johnston,
M.
(2015). Localized error detection for
targeted clariﬁcation in a virtual assis-
tant. In ICASSP-15, 5241–5245.
Stoyanchev, S., Liu, A., and Hirsch-
berg, J. (2013).
Modelling human
clariﬁcation strategies.
In SIGDIAL
2013, 137–141.
Stoyanchev, S., Liu, A., and Hirsch-
berg, J. (2014). Towards natural clari-
ﬁcation questions in dialogue systems.
In AISB symposium on questions, dis-
course and dialogue.
Str¨otgen, J. and Gertz, M. (2013). Mul-
tilingual and cross-domain temporal
tagging.
Language Resources and
Evaluation, 47(2), 269–298.
Strube, M. and Hahn, U. (1996). Func-
tional centering.
In ACL-96,
270–
277.
Subba, R. and Di Eugenio, B. (2009).
An effective discourse parser that uses
rich linguistic information. In NAACL
HLT 2009, 566–574.
Suendermann, D., Evanini, K., Lis-
combe, J., Hunter, P., Dayanidhi, K.,
and Pieraccini, R. (2009). From rule-
based to statistical grammars: Contin-
uous improvement of large-scale spo-
ken dialog systems.
In ICASSP-09,
4713–4716.
Sundheim, B. (Ed.). (1991). Proceed-
ings of MUC-3.
Sundheim, B. (Ed.). (1992). Proceed-
ings of MUC-4.
Sundheim, B. (Ed.). (1993). Proceed-
ings of MUC-5, Baltimore, MD.
Sundheim, B. (Ed.). (1995). Proceed-
ings of MUC-6.
Surdeanu, M. (2013).
Overview of
the TAC2013 Knowledge Base Popu-
lation evaluation: English slot ﬁlling
and temporal slot ﬁlling. In TAC-13.
Surdeanu,
M.,
Harabagiu,
S.,
Williams, J., and Aarseth, P. (2003).
Using predicate-argument structures
for information extraction. In ACL-03,
8–15.
Surdeanu,
M.,
Hicks,
T.,
and
Valenzuela-Escarcega, M. A. (2015).
Two practical rhetorical structure the-
ory parsers.
In NAACL HLT 2015,
1–5.
Surdeanu, M., Johansson, R., Mey-
ers, A., M`arquez, L., and Nivre, J.
(2008a). The conll-2008 shared task
on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-08,
159–177.
Surdeanu, M., Johansson, R., Mey-
ers, A., M`arquez, L., and Nivre, J.
(2008b). The conll-2008 shared task
on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-08,
159–177.
Sutskever, I., Vinyals, O., and Le, Q. V.
(2014). Sequence to sequence learn-
ing with neural networks. In Advances
in Neural Information Processing Sys-
tems 27: Annual Conference on Neu-
ral Information Processing Systems
2014, December 8-13 2014, Montreal,
Quebec, Canada, 3104–3112.
Sweet, H. (1877). A Handbook of Pho-
netics. Clarendon Press.
Swerts, M., Litman, D. J., and Hirsch-
berg, J. (2000). Corrections in spoken
dialogue systems. In ICSLP-00.
Swier, R. and Stevenson, S. (2004).
Unsupervised semantic role labelling.
In EMNLP 2004, 95–102.
Switzer, P. (1965).
Vector images
in document retrieval.
In Stevens,
M. E., Giuliano, V. E., and Heil-
prin, L. B. (Eds.), Statistical Associ-
ation Methods For Mechanized Doc-
umentation. Symposium Proceedings.
Washington, D.C., USA, March 17,
1964, 163–171. https://nvlpubs.
nist.gov/nistpubs/Legacy/MP/
nbsmiscellaneouspub269.pdf.
Talbot, D. and Osborne, M. (2007).
Smoothed Bloom Filter Language
Models:
Tera-Scale LMs on the
Cheap.
In EMNLP/CoNLL 2007,
468–476.
Talmor, A. and Berant, J. (2018). The
web as a knowledge-base for answer-
ing complex questions.
In NAACL
HLT 2018.
Tan,
C.,
Niculae,
V.,
Danescu-
Niculescu-Mizil,
C.,
and Lee,
L.
(2016). Winning arguments: Interac-
tion dynamics and persuasion strate-
gies in good-faith online discussions.
In WWW-16, 613–624.
Tannen, D. (1979). What’s in a frame?
Surface evidence for underlying ex-
pectations. In Freedle, R. (Ed.), New
Directions in Discourse Processing,
137–181. Ablex.
ter Meulen, A. (1995).
Represent-
ing Time in Natural Language. MIT
Press.
Tesni`ere, L. (1959). ´El´ements de Syn-
taxe Structurale. Librairie C. Klinck-
sieck, Paris.
Tetreault, J. R. (2001). A corpus-based
evaluation of centering and pronoun
resolution.
Computational Linguis-
tics, 27(4), 507–520.

Bibliography
591
Teufel, S., Carletta, J., and Moens, M.
(1999).
An annotation scheme for
discourse-level argumentation in re-
search articles. In EACL-99.
Teufel, S., Siddharthan, A., and Batch-
elor, C. (2009).
Towards domain-
independent
argumentative
zoning:
Evidence from chemistry and compu-
tational linguistics.
In EMNLP-09,
1493–1502.
Thede, S. M. and Harper, M. P. (1999).
A second-order hidden Markov model
for part-of-speech tagging.
In ACL-
99, 175–182.
Thompson, K. (1968). Regular expres-
sion search algorithm. CACM, 11(6),
419–422.
Tibshirani, R. J. (1996).
Regression
shrinkage and selection via the lasso.
Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 58(1),
267–288.
Titov, I. and Khoddam, E. (2014). Un-
supervised induction of semantic roles
within a reconstruction-error mini-
mization framework. In NAACL HLT
2015.
Titov, I. and Klementiev, A. (2012).
A Bayesian approach to unsupervised
semantic role induction. In EACL-12,
12–22.
Tomkins, S. S. (1962). Affect, imagery,
consciousness: Vol. I. The positive af-
fects. Springer.
Toutanova, K., Klein, D., Manning,
C. D., and Singer, Y. (2003). Feature-
rich part-of-speech tagging with a
cyclic dependency network. In HLT-
NAACL-03.
Trichelair, P., Emami, A., Cheung, J.
C. K., Trischler, A., Suleman, K., and
Diaz, F. (2018).
On the evaluation
of common-sense reasoning in natural
language understanding. In NeurIPS
2018 Workshop on Critiquing and
Correcting Trends in Machine Learn-
ing.
Trnka, K., Yarrington, D., McCaw, J.,
McCoy, K. F., and Pennington, C.
(2007).
The effects of word predic-
tion on communication rate for AAC.
In NAACL-HLT 07, 173–176.
Tseng, H., Jurafsky, D., and Manning,
C. D. (2005). Morphological features
help POS tagging of unknown words
across language varieties. In Proceed-
ings of the 4th SIGHAN Workshop on
Chinese Language Processing.
Tsvetkov, Y., Schneider, N., Hovy, D.,
Bhatia, A., Faruqui, M., and Dyer, C.
(2014). Augmenting English adjective
senses with supersenses. In LREC-14.
Tur, G. and De Mori, R. (2011). Spoken
language understanding: Systems for
extracting semantic information from
speech. John Wiley & Sons.
Turian, J., Ratinov, L., and Bengio,
Y. (2010).
Word representations: a
simple and general method for semi-
supervised learning.
In ACL 2010,
384–394.
Turney, P. D. (2002).
Thumbs up or
thumbs down?
semantic orientation
applied to unsupervised classiﬁcation
of reviews. In ACL-02.
Turney, P. D. and Littman, M. (2003).
Measuring praise and criticism: Infer-
ence of semantic orientation from as-
sociation. ACM Transactions on Infor-
mation Systems (TOIS), 21, 315–346.
Uryupina, O., Artstein, R., Bristot, A.,
Cavicchio, F., Delogu, F., Rodriguez,
K. J., and Poesio, M. (2019). Anno-
tating a broad range of anaphoric phe-
nomena, in a variety of genres: The
ARRAU corpus.
Natural Language
Engineering, 1–34.
UzZaman, N., Llorens, H., Derczyn-
ski, L., Allen, J., Verhagen, M., and
Pustejovsky, J. (2013). Semeval-2013
task 1: Tempeval-3: Evaluating time
expressions, events, and temporal re-
lations. In SemEval-13, 1–9.
van Benthem, J. and ter Meulen, A.
(Eds.). (1997).
Handbook of Logic
and Language. MIT Press.
van Deemter,
K. and Kibble,
R.
(2000).
On coreferring:
corefer-
ence in MUC and related annotation
schemes. Computational Linguistics,
26(4), 629–637.
van der Maaten, L. and Hinton, G. E.
(2008). Visualizing high-dimensional
data using t-sne.
JMLR, 9, 2579–
2605.
van Rijsbergen, C. J. (1975). Informa-
tion Retrieval. Butterworths.
Van Valin, Jr., R. D. and La Polla, R.
(1997). Syntax: Structure, Meaning,
and Function. Cambridge University
Press.
Velikovich, L., Blair-Goldensohn, S.,
Hannan,
K.,
and
McDonald,
R.
(2010). The viability of web-derived
polarity lexicons.
In NAACL HLT
2010, 777–785.
Venditti, J. J. (2005). The j tobi model
of japanese intonation.
In Jun, S.-
A. (Ed.), Prosodic Typology and Tran-
scription: A Uniﬁed Approach. Ox-
ford University Press.
Vendler, Z. (1967). Linguistics in Phi-
losophy.
Cornell University Press,
Ithaca, NY.
Verhagen,
M.,
Gaizauskas,
R.,
Schilder, F., Hepple, M., Moszkow-
icz, J., and Pustejovsky, J. (2009).
The tempeval challenge: identifying
temporal relations in text. Language
Resources
and
Evaluation,
43(2),
161–179.
Verhagen, M., Mani, I., Sauri, R.,
Knippen, R., Jang, S. B., Littman, J.,
Rumshisky, A., Phillips, J., and Puste-
jovsky, J. (2005). Automating tempo-
ral annotation with tarsqi. In ACL-05,
81–84.
Versley, Y. (2008). Vagueness and ref-
erential ambiguity in a large-scale an-
notated corpus.
Research on Lan-
guage and Computation, 6(3-4), 333–
353.
Vieira, R. and Poesio, M. (2000). An
empirically based system for process-
ing deﬁnite descriptions.
Computa-
tional Linguistics, 26(4), 539–593.
Vilain, M., Burger, J. D., Aberdeen,
J., Connolly, D., and Hirschman, L.
(1995).
A model-theoretic corefer-
ence scoring scheme. In MUC-6.
Vintsyuk,
T. K. (1968).
Speech
discrimination by dynamic program-
ming. Cybernetics, 4(1), 52–57. Rus-
sian Kibernetika 4(1):81-88. 1968.
Vinyals, O. and Le, Q. V. (2015).
A neural conversational model.
In
Proceedings of ICML Deep Learning
Workshop.
Viterbi, A. J. (1967). Error bounds for
convolutional codes and an asymptot-
ically optimum decoding algorithm.
IEEE Transactions on Information
Theory, IT-13(2), 260–269.
Vossen,
P.,
G¨or¨og,
A.,
Laan,
F.,
Van Gompel, M., Izquierdo, R., and
Van Den Bosch, A. (2011).
Dutch-
semcor: building a semantically anno-
tated corpus for dutch. In Proceedings
of eLex, 286–296.
Voutilainen, A. (1995).
Morpholog-
ical disambiguation.
In Karlsson,
F., Voutilainen, A., Heikkil¨a, J., and
Anttila, A. (Eds.), Constraint Gram-
mar: A Language-Independent Sys-
tem for Parsing Unrestricted Text,
165–284. Mouton de Gruyter.
Voutilainen, A. (1999).
Handcrafted
rules. In van Halteren, H. (Ed.), Syn-
tactic Wordclass Tagging,
217–246.
Kluwer.
Vrandeˇci´c,
D.
and
Kr¨otzsch,
M.
(2014). Wikidata: a free collaborative
knowledge base. CACM, 57(10), 78–
85.
Wade, E., Shriberg, E., and Price,
P. J. (1992).
User behaviors affect-
ing speech recognition. In ICSLP-92,
995–998.
Wagner, R. A. and Fischer, M. J.
(1974). The string-to-string correction
problem. Journal of the Association
for Computing Machinery, 21, 168–
173.
Walker, M. A. (2000). An application
of reinforcement learning to dialogue
strategy selection in a spoken dialogue
system for email. JAIR, 12, 387–416.

592
Bibliography
Walker, M. A., Fromer, J. C., and
Narayanan, S. S. (1998).
Learning
optimal dialogue strategies: A case
study of a spoken dialogue agent for
email.
In COLING/ACL-98,
1345–
1351.
Walker, M. A., Iida, M., and Cote, S.
(1994).
Japanese discourse and the
process of centering. Computational
Linguistics, 20(2), 193–232.
Walker, M. A., Joshi, A. K., and
Prince, E. (Eds.). (1998). Centering in
Discourse. Oxford University Press.
Walker, M. A., Kamm, C. A., and Lit-
man, D. J. (2001). Towards develop-
ing general models of usability with
PARADISE. Natural Language Engi-
neering: Special Issue on Best Prac-
tice in Spoken Dialogue Systems, 6(3),
363–377.
Walker,
M. A. and Whittaker,
S.
(1990). Mixed initiative in dialogue:
An investigation into discourse seg-
mentation. In ACL-90, 70–78.
Wang, A., Singh, A., Michael, J., Hill,
F., Levy, O., and Bowman, S. R.
(2018).
Glue: A multi-task bench-
mark and analysis platform for natu-
ral language understanding. In ICLR
2017.
Wang, H., Lu, Z., Li, H., and Chen,
E. (2013). A dataset for research on
short-text conversations.. In EMNLP
2013, 935–945.
Wang, S. and Manning, C. D. (2012).
Baselines and bigrams: Simple, good
sentiment and topic classiﬁcation. In
ACL 2012, 90–94.
Wang, Y., Li, S., and Yang, J. (2018).
Toward fast and accurate neural dis-
course segmentation.
In EMNLP
2018.
Ward, W. and Issar, S. (1994).
Re-
cent improvements in the CMU spo-
ken language understanding system.
In ARPA Human Language Technolo-
gies Workshop.
Weaver, W. (1949/1955).
Transla-
tion.
In Locke, W. N. and Boothe,
A. D. (Eds.), Machine Translation
of Languages,
15–23. MIT Press.
Reprinted from a memorandum writ-
ten by Weaver in 1949.
Webber, B. L. (1978). A Formal Ap-
proach to Discourse Anaphora. Ph.D.
thesis, Harvard University.
Webber, B. L. (1983). So what can we
talk about now?.
In Brady, M. and
Berwick, R. C. (Eds.), Computational
Models of Discourse, 331–371. The
MIT Press. Reprinted in Grosz et al.
(1986).
Webber, B. L. (1991). Structure and
ostension in the interpretation of dis-
course deixis. Language and Cogni-
tive Processes, 6(2), 107–135.
Webber, B. L. and Baldwin, B. (1992).
Accommodating context change.
In
ACL-92, 96–103.
Webber, B. L., Egg, M., and Kordoni,
V. (2012).
Discourse structure and
language technology.
Natural Lan-
guage Engineering, 18(4), 437–490.
Webber, B. L. (1988).
Discourse
deixis: Reference to discourse seg-
ments. In ACL-88, 113–122.
Webster, K., Recasens, M., Axelrod,
V., and Baldridge, J. (2018). Mind the
gap: A balanced corpus of gendered
ambiguous pronouns. TACL, 6, 605–
617.
Weinschenk, S. and Barker, D. T.
(2000).
Designing Effective Speech
Interfaces. Wiley.
Weischedel, R., Hovy, E. H., Mar-
cus, M. P., Palmer, M., Belvin, R.,
Pradhan, S., Ramshaw, L. A., and
Xue, N. (2011). Ontonotes: A large
training corpus for enhanced process-
ing.
In Joseph Olive, Caitlin Chris-
tianson, J. M. (Ed.), Handbook of Nat-
ural Language Processing and Ma-
chine Translation: DARPA Global Au-
tomatic Language Exploitation, 54–
63. Springer.
Weischedel, R., Meteer, M., Schwartz,
R., Ramshaw, L. A., and Palmucci, J.
(1993).
Coping with ambiguity and
unknown words through probabilistic
models.
Computational Linguistics,
19(2), 359–382.
Weizenbaum, J. (1966). ELIZA – A
computer program for the study of
natural language communication be-
tween man and machine. CACM, 9(1),
36–45.
Weizenbaum, J. (1976).
Computer
Power and Human Reason:
From
Judgement to Calculation. W.H. Free-
man and Company.
Wells, J. C. (1982). Accents of English.
Cambridge University Press.
Wen,
T.-H.,
Gaˇsi´c,
M.,
Kim,
D.,
Mrkˇsi´c, N., Su, P.-H., Vandyke, D.,
and Young, S. J. (2015a). Stochastic
language generation in dialogue using
recurrent neural networks with convo-
lutional sentence reranking. In SIG-
DIAL 2015, 275––284.
Wen, T.-H., Gaˇsi´c, M., Mrkˇsi´c, N., Su,
P.-H., Vandyke, D., and Young, S. J.
(2015b).
Semantically conditioned
lstm-based natural language genera-
tion for spoken dialogue systems. In
EMNLP 2015.
Werbos, P. (1974). Beyond regression
: new tools for prediction and analy-
sis in the behavioral sciences /. Ph.D.
thesis, Harvard University.
Werbos, P. J. (1990). Backpropagation
through time: what it does and how
to do it.
Proceedings of the IEEE,
78(10), 1550–1560.
Widrow, B. and Hoff, M. E. (1960).
Adaptive switching circuits.
In IRE
WESCON Convention Record, Vol. 4,
96–104.
Wiebe, J. (1994).
Tracking point of
view in narrative. Computational Lin-
guistics, 20(2), 233–287.
Wiebe, J. (2000). Learning subjective
adjectives from corpora. In AAAI-00,
735–740.
Wiebe, J., Bruce, R. F., and O’Hara,
T. P. (1999). Development and use of a
gold-standard data set for subjectivity
classiﬁcations. In ACL-99, 246–253.
Wierzbicka, A. (1992).
Semantics,
Culture, and Cognition:
University
Human Concepts in Culture-Speciﬁc
Conﬁgurations.
Oxford University
Press.
Wierzbicka, A. (1996).
Semantics:
Primes and Universals. Oxford Uni-
versity Press.
Wilensky,
R.
(1983).
Planning
and Understanding:
A Computa-
tional Approach to Human Reasoning.
Addison-Wesley.
Wilks, Y. (1973).
An artiﬁcial intel-
ligence approach to machine transla-
tion.
In Schank, R. C. and Colby,
K. M. (Eds.), Computer Models of
Thought and Language,
114–151.
W.H. Freeman.
Wilks, Y. (1975a). An intelligent an-
alyzer and understander of English.
CACM, 18(5), 264–274.
Wilks, Y. (1975b). Preference seman-
tics. In Keenan, E. L. (Ed.), The For-
mal Semantics of Natural Language,
329–350. Cambridge Univ. Press.
Wilks, Y. (1975c).
A preferential,
pattern-seeking, semantics for natural
language inference. Artiﬁcial Intelli-
gence, 6(1), 53–74.
Williams, J. D., Raux, A., and Hender-
son, M. (2016). The dialog state track-
ing challenge series: A review. Dia-
logue & Discourse, 7(3), 4–33.
Williams,
J. D. and Young,
S. J.
(2007). Partially observable markov
decision processes for spoken dialog
systems. Computer Speech and Lan-
guage, 21(1), 393–422.
Wilson, T., Wiebe, J., and Hoffmann, P.
(2005). Recognizing contextual polar-
ity in phrase-level sentiment analysis.
In HLT-EMNLP-05, 347–354.
Winograd, T. (1972). Understanding
Natural Language. Academic Press.
Winston, P. H. (1977). Artiﬁcial Intel-
ligence. Addison Wesley.
Wiseman,
S.,
Rush,
A. M.,
and
Shieber, S. M. (2016).
Learning
global features for coreference resolu-
tion. In NAACL HLT 2016.

Bibliography
593
Wiseman, S., Rush, A. M., Shieber,
S. M., and Weston, J. (2015). Learn-
ing anaphoricity and antecedent rank-
ing features for coreference resolu-
tion. In ACL 2015, 1416–1426.
Witten, I. H. and Bell, T. C. (1991).
The zero-frequency problem:
Es-
timating the probabilities of novel
events in adaptive text compression.
IEEE Transactions on Information
Theory, 37(4), 1085–1094.
Witten, I. H. and Frank, E. (2005).
Data Mining:
Practical Machine
Learning Tools and Techniques (2nd
Ed.). Morgan Kaufmann.
Wittgenstein, L. (1953).
Philosoph-
ical Investigations. (Translated by
Anscombe, G.E.M.). Blackwell.
Wolf, F. and Gibson, E. (2005). Rep-
resenting discourse coherence:
A
corpus-based analysis. Computational
Linguistics, 31(2), 249–287.
Woods, W. A. (1967).
Semantics for
a Question-Answering System. Ph.D.
thesis, Harvard University.
Woods, W. A. (1973). Progress in natu-
ral language understanding.
In Pro-
ceedings of AFIPS National Confer-
ence, 441–450.
Woods, W. A. (1975).
What’s in a
link: Foundations for semantic net-
works. In Bobrow, D. G. and Collins,
A. M. (Eds.), Representation and Un-
derstanding: Studies in Cognitive Sci-
ence, 35–82. Academic Press.
Woods, W. A. (1978).
Semantics
and quantiﬁcation in natural language
question answering.
In Yovits, M.
(Ed.), Advances in Computers, 2–64.
Academic.
Woods, W. A., Kaplan, R. M., and
Nash-Webber, B. L. (1972). The lunar
sciences natural language information
system: Final report. Tech. rep. 2378,
BBN.
Woodsend, K. and Lapata, M. (2015).
Distributed representations for unsu-
pervised semantic role labeling.
In
EMNLP 2015, 2482–2491.
Wu, F. and Weld, D. S. (2007).
Au-
tonomously semantifying Wikipedia.
In CIKM-07, 41–50.
Wu, F. and Weld, D. S. (2010). Open in-
formation extraction using Wikipedia.
In ACL 2010, 118–127.
Wu,
Y.,
Schuster,
M.,
Chen,
Z.,
Le, Q. V., Norouzi, M., Macherey,
W., Krikun, M., Cao, Y., Gao, Q.,
Macherey, K., Klingner, J., Shah,
A., Johnson, M., Liu, X., Kaiser,
Ł., Gouws, S., Kato, Y., Kudo, T.,
Kazawa, H., Stevens, K., Kurian, G.,
Patil, N., Wang, W., Young, C., Smith,
J., Riesa, J., Rudnick, A., Vinyals,
O., Corrado, G. S., Hughes, M., and
Dean, J. (2016).
Google’s neural
machine translation system:
Bridg-
ing the gap between human and ma-
chine translation.
arXiv preprint
arXiv:1609.08144.
Wundt, W. (1900). V¨olkerpsychologie:
eine Untersuchung der Entwicklungs-
gesetze von Sprache, Mythus, und
Sitte. W. Engelmann, Leipzig. Band
II: Die Sprache, Zweiter Teil.
Xia, F. and Palmer, M. (2001). Convert-
ing dependency structures to phrase
structures. In HLT-01, 1–5.
Xu, P., Saghir, H., Kang, J. S., Long, T.,
Bose, A. J., Cao, Y., and Cheung, J.
C. K. (2019). A cross-domain trans-
ferable neural coherence model.
In
ACL 2019, 678–687.
Xue, N., Ng, H. T., Pradhan, S., Ruther-
ford, A., Webber, B. L., Wang, C.,
and Wang, H. (2016).
Conll 2016
shared task on multilingual shallow
discourse parsing. In Proceedings of
the CoNLL-16 shared task.
Xue, N. and Palmer, M. (2004). Cal-
ibrating features for semantic role la-
beling. In EMNLP 2004.
Yamada, H. and Matsumoto, Y. (2003).
Statistical dependency analysis with
support vector machines.
In Noord,
G. V. (Ed.), IWPT-03, 195–206.
Yan, Z., Duan, N., Bao, J.-W., Chen,
P., Zhou, M., Li, Z., and Zhou, J.
(2016). DocChat: An information re-
trieval approach for chatbot engines
using unstructured documents. In ACL
2016.
Yang, D., Chen, J., Yang, Z., Juraf-
sky, D., and Hovy, E. H. (2019).
Let’s make your request more persua-
sive: Modeling persuasive strategies
via semi-supervised neural nets on
crowdfunding platforms.
In NAACL
HLT 2019, 3620–3630.
Yang, X., Zhou, G., Su, J., and Tan,
C. L. (2003). Coreference resolution
using competition learning approach.
In ACL-03, 176–183.
Yang, Y., Yih, W.-t., and Meek, C.
(2015). Wikiqa: A challenge dataset
for open-domain question answering.
In EMNLP 2015.
Yang, Y. and Pedersen, J. (1997).
A
comparative study on feature selection
in text categorization. In ICML, 412–
420.
Yankelovich,
N.,
Levow,
G.-A.,
and Marx, M. (1995).
Designing
SpeechActs:
Issues in speech user
interfaces. In Human Factors in Com-
puting Systems: CHI ’95 Conference
Proceedings, 369–376.
Yarowsky, D. (1995).
Unsupervised
word sense disambiguation rivaling
supervised methods. In ACL-95, 189–
196.
Yasseri, T., Kornai, A., and Kert´esz, J.
(2012). A practical approach to lan-
guage complexity: a Wikipedia case
study. PloS one, 7(11).
Yih, W.-t., Richardson, M., Meek, C.,
Chang, M.-W., and Suh, J. (2016).
The value of semantic parse labeling
for knowledge base question answer-
ing. In ACL 2016, 201–206.
Yngve, V. H. (1955). Syntax and the
problem of multiple meaning.
In
Locke, W. N. and Booth, A. D. (Eds.),
Machine Translation of Languages,
208–226. MIT Press.
Young, S. J., Gaˇsi´c, M., Keizer, S.,
Mairesse, F., Schatzmann, J., Thom-
son, B., and Yu, K. (2010). The Hid-
den Information State model: A prac-
tical framework for POMDP-based
spoken dialogue management. Com-
puter Speech & Language, 24(2),
150–174.
Younger, D. H. (1967).
Recognition
and parsing of context-free languages
in time n3. Information and Control,
10, 189–208.
Yu, M. and Dredze, M. (2014). Improv-
ing lexical embeddings with semantic
knowledge. In ACL 2014, 545–550.
Yu, N., Zhang, M., and Fu, G. (2018).
Transition-based neural RST parsing
with implicit syntax features.
In
COLING-18, 559–570.
Yu, Y., Zhu, Y., Liu, Y., Liu, Y., Peng,
S., Gong, M., and Zeldes, A. (2019).
GumDrop at the DISRPT2019 shared
task: A model stacking approach to
discourse unit segmentation and con-
nective detection. In Proceedings of
the Workshop on Discourse Relation
Parsing and Treebanking 2019.
Zapirain, B., Agirre, E., M`arquez, L.,
and Surdeanu, M. (2013). Selectional
preferences for semantic role classi-
ﬁcation.
Computational Linguistics,
39(3), 631–663.
Zavrel, J. and Daelemans, W. (1997).
Memory-based learning: Using simi-
larity for smoothing. In ACL/EACL-
97, 436–443.
Zelle, J. M. and Mooney, R. J. (1996).
Learning to parse database queries us-
ing inductive logic programming. In
AAAI-96, 1050–1055.
Zeman, D. (2008).
Reusable tagset
conversion using tagset drivers..
In
LREC-08.
Zeman, D., Popel, M., Straka, M.,
Hajiˇc, J., Nivre, J., Ginter, F., Luoto-
lahti, J., Pyysalo, S., Petrov, S., Pot-
thast, M., Tyers, F. M., Badmaeva,
E., Gokirmak, M., Nedoluzhko, A.,
Cinkov´a, S., Hajic, Jr., J., Hlav´acov´a,
J., Kettnerov´a, V., Uresov´a, Z., Kan-
erva, J., Ojala, S., Missil¨a, A., Man-
ning, C. D., Schuster, S., Reddy,
S., Taji, D., Habash, N., Leung, H.,

594
Bibliography
de Marneffe, M.-C., Sanguinetti, M.,
Simi, M., Kanayama, H., de Paiva,
V., Droganova, K., Alonso, H. M.,
C¸ ¨oltekin, C¸ ., Sulubacak, U., Uszkor-
eit, H., Macketanz, V., Burchardt, A.,
Harris, K., Marheinecke, K., Rehm,
G., Kayadelen, T., Attia, M., El-
Kahky, A., Yu, Z., Pitler, E., Lert-
pradit, S., Mandl, M., Kirchner, J.,
Alcalde, H. F., Strnadov´a, J., Baner-
jee, E., Manurung, R., Stella, A., Shi-
mada, A., Kwak, S., Mendonc¸a, G.,
Lando, T., Nitisaroj, R., and Li, J.
(2017). Conll 2017 shared task: Mul-
tilingual parsing from raw text to uni-
versal dependencies. In Proceedings
of the CoNLL 2017 Shared Task: Mul-
tilingual Parsing from Raw Text to
Universal Dependencies, Vancouver,
Canada, August 3-4, 2017, 1–19.
Zettlemoyer,
L.
and
Collins,
M.
(2005).
Learning to map sentences
to logical form:
Structured classi-
ﬁcation with probabilistic categorial
grammars. In Uncertainty in Artiﬁcial
Intelligence, UAI’05, 658–666.
Zhang, R., dos Santos, C. N., Ya-
sunaga, M., Xiang, B., and Radev, D.
(2018). Neural coreference resolution
with deep biafﬁne attention by joint
mention detection and mention clus-
tering. In ACL 2018, 102–107.
Zhang,
Y.
and
Clark,
S.
(2008).
A tale of two parsers:
investigat-
ing and combining graph-based and
transition-based dependency parsing
using beam-search.
In EMNLP-08,
562–571.
Zhang,
Y.
and
Nivre,
J.
(2011).
Transition-based dependency parsing
with rich non-local features. In ACL
2011, 188–193.
Zhao, H., Chen, W., Kit, C., and Zhou,
G. (2009). Multilingual dependency
learning: A huge feature engineering
method to semantic dependency pars-
ing. In CoNLL-09, 55–60.
Zhao, J., Wang, T., Yatskar, M., Cot-
terell, R., Ordonez, V., and Chang, K.-
W. (2019). Gender bias in contextu-
alized word embeddings. In NAACL
HLT 2019, 629–634.
Zhao, J., Wang, T., Yatskar, M., Or-
donez, V., and Chang, K.-W. (2017).
Men also like shopping:
Reduc-
ing gender bias ampliﬁcation using
corpus-level constraints.
In EMNLP
2017.
Zhao, J., Wang, T., Yatskar, M., Or-
donez, V., and Chang, K.-W. (2018a).
Gender bias in coreference resolution:
Evaluation and debiasing methods. In
NAACL HLT 2018.
Zhao, J., Zhou, Y., Li, Z., Wang, W.,
and Chang, K.-W. (2018b). Learning
gender-neutral word embeddings. In
EMNLP 2018, 4847–4853.
Zheng, J., Vilnis, L., Singh, S., Choi,
J. D., and McCallum, A. (2013). Dy-
namic knowledge-base alignment for
coreference resolution. In CoNLL-13,
153–162.
Zhong, Z. and Ng, H. T. (2010).
It
makes sense: A wide-coverage word
sense disambiguation system for free
text. In ACL 2010, 78–83.
Zhou, D., Bousquet, O., Lal, T. N.,
Weston, J., and Sch¨olkopf, B. (2004).
Learning with local and global consis-
tency. In NIPS 2004.
Zhou, G., Su, J., Zhang, J., and Zhang,
M. (2005). Exploring various knowl-
edge in relation extraction. In ACL-05,
427–434.
Zhou, J. and Xu, W. (2015a). End-to-
end learning of semantic role labeling
using recurrent neural networks.
In
ACL 2015, 1127–1137.
Zhou, J. and Xu, W. (2015b). End-to-
end learning of semantic role labeling
using recurrent neural networks.
In
ACL 2015, 1127–1137.
Zhou, L., Gao, J., Li, D., and Shum,
H.-Y. (2018). The design and imple-
mentation of XiaoIce, an empathetic
social chatbot..
Zhou, L., Ticrea, M., and Hovy, E. H.
(2004).
Multi-document biography
summarization. In EMNLP 2004.
Zhou, Y. and Xue, N. (2015). The Chi-
nese Discourse TreeBank: a Chinese
corpus annotated with discourse rela-
tions. Language Resources and Eval-
uation, 49(2), 397–431.
Zhu, X. and Ghahramani, Z. (2002).
Learning from labeled and unlabeled
data with label propagation. Tech. rep.
CMU-CALD-02, CMU.
Zhu, X., Ghahramani, Z., and Lafferty,
J. (2003).
Semi-supervised learning
using gaussian ﬁelds and harmonic
functions. In ICML 2003, 912–919.
Zue, V. W., Glass, J., Goodine, D., Le-
ung, H., Phillips, M., Polifroni, J., and
Seneff, S. (1989). Preliminary eval-
uation of the VOYAGER spoken lan-
guage system. In Proceedings DARPA
Speech and Natural Language Work-
shop, 160–167.

Author Index
ˇSevˇc´ıkov´a, M., 277, 296
ˇStˇep´anek, J., 277, 296
Aarseth, P., 392
Abadi, M., 137
Abeill´e, A., 271, 272
Abelson, R. P., 349, 379,
391, 392
Abend, O., 360
Aberdeen, J., 435, 436, 441
Abney, S. P., 166, 244, 268,
270, 272, 527
Acero, A., 544
Agarwal, A., 137
Agarwal, O., 436
Aggarwal, C. C., 73
Aggarwal, S., 506, 516
Agichtein, E., 338, 352
Agirre, E., 119, 371, 393
Agrawal, M., 409–411
Ahn, D., 28, 462
Ahn, Y.-Y., 400
Aho, A. V., 244, 278
Ajdukiewicz, K., 223
Alcalde, H. F., 294
AlGhamdi, F., 13
Algoet, P. H., 51
Allen, J., 323, 347, 348,
353, 490, 516
Alonso, H. M., 73, 294
Althoff, T., 460
Altun, Y., 360
Amir, S., 190
Amsler, R. A., 371
An, J., 400
Androutsopoulos, I., 72
Angelard-Gontier, N., 498,
514
Antiga, L., 137
Anttila, A., 167, 296
Aone, C., 441
Appelt, D. E., 350–352, 441
Appling, S., 514
Ariel, M., 421
Arons, B., 513
Artstein, R., 425, 442, 498
Aseltine, J., 352
Asher, N., 451, 462, 463
Auer, S., 334, 476
Auli, M., 497
Austerweil, J., 462
Austin, J. L., 489, 515
Awadallah, A. H., 506
Axelrod, V., 438, 439
Ayers, G. M., 528
Ba, J., 137
Baayen, R. H., 28, 539
Babko-Malaya, O., 371
Baccianella, S., 403
Bach, K., 489
Backus, J. W., 205, 229
Badmaeva, E., 294
Bagga, A., 435
Bahdanau, D., 185
Bahl, L. R., 53, 166
Baker, C. F., 379, 392
Baker, J. K., 53, 253, 271
Baldridge, J., 427, 429,
430, 436, 438, 439,
441, 462
Baldwin, B., 421, 435
Baldwin, T., 65, 73
Ballard, D. H., 142
Ballesteros, M., 180, 331,
352
Balogh, J., 502, 508, 509,
513
Bamman, D., 459
Banaji, M. R., 118
Banarescu, L., 298
Banea, C., 119
Banerjee, E., 294
Bangalore, S., 272
Banko, M., 352, 469, 472
Bansal, M., 336, 352
Bao, J.-W., 496
Bar-Hillel, Y., 223
Barham, P., 137
Barker, C., 442
Barker, D. T., 508
Barney, H. L., 543
Barrett, L., 258, 271
Barzilay, R., 444, 452,
454–457, 462, 463
Basile, P., 366
Batchelor, C., 461
Bates, R., 516
Bauer, F. L., 229
Bauer, J., 22
Baum, L. E., 558, 563
Baum, L. F., 513
Baumer, G., 73
Bayes, T., 58
Bazell, C. E., 229
Bean, D., 419, 427, 441
Bear, J., 350–352
Becker, C., 334, 476
Beckman, M. E., 528
Bedi, G., 445
Beil, F., 393
Bejˇcek, E., 277, 296
Bell, E., 392, 412, 413
Bell, T. C., 54
Bellegarda, J. R., 121, 515
Bellman, R., 24, 28
Belvin, R., 277
Bender, E. M., 230
Bengio, S., 498
Bengio, Y., 54, 94, 111,
121, 127–129, 138,
142, 185, 498
Bengtson, E., 431
Bennett, E., 451, 463
Bennett, S. W., 441
Beˇnuˇs, ˇS., 516
Berant, J., 478, 479, 483,
485
Berg-Kirkpatrick, T., 70,
71, 73
Berger, A., 93
Bergsma, S., 393, 427, 441
Bernstein, M. S., 415
Bethard, S., 13, 22, 353,
392, 393
Bhat, I., 288
Bhat, R. A., 288
Bhatia, A., 360
Biber, D., 230
Bies, A., 217, 219
Bikel, D. M., 352
Bills, S., 339, 352
Biran, O., 452
Birch, A., 18, 20
Bird, S., 16, 17, 28
Bisani, M., 73
Bishop, C. M., 73, 93
Bitan, A., 360
Bizer, C., 334, 476
Bj¨orkelund, A., 384, 430
Black, A. W., 190, 439
Black, E., 268, 270, 271,
371
Blair, E., 13
Blair-Goldensohn, S., 414
Blei, D. M., 121
Blodgett, A., 360
Blodgett, S. L., 13, 40, 65
Bloomﬁeld, L., 220, 229,
373
Blunsom, P., 483, 485
Bobrow, D. G., 322, 391,
498, 499, 515
Bobrow, R. J., 515
Bod, R., 271
Boersma, P., 534, 542, 544
Bogomolov, M., 73
Boguraev, B. K., 371, 441,
485
Bohus, D., 509
Bojanowski, P., 121
Bolinger, D., 526
Bollacker, K., 334, 476
Bolukbasi, T., 117, 118,
439
Bonial, C., 298
Booth, R. J., 64, 398, 399
Booth, T. L., 246, 247, 271
Bordes, A., 468, 473, 474,
485
Borges, J. L., 56
Bose, A. J., 457, 458, 462
Boser, B., 142
Boser, B. E., 142
Bottou, L., 111, 121, 190,
352, 392
Bougares, F., 185
Boureau, Y.-L., 516
Bourlard, H., 142
Bousquet, O., 414
Bovi, C. D., 371
Bowman, S. R., 438, 498
Boyd-Graber, J., 485
Brachman, R. J., 322
Brants, T., 49, 158, 159,
166, 167
Braud, C., 447, 449, 452,
462
Br´eal, M., 96
Breck, E., 472, 485
Brennan, S. E., 441, 453
Bresnan, J., 223, 229, 246
Brevdo, E., 137
Brill, E., 272, 469, 472
Brin, S., 352
Briscoe, T., 270, 272, 371
Bristot, A., 425
Broadhead, M., 352
Brockett, C., 497
Brockman, W., 117
Brockmann, C., 388
Broschart, J., 144
Bruce, B. C., 516
Bruce, R., 371
Bruce, R. F., 394
Brysbaert, M., 398
Bryson, J. J., 118, 439
Buchholz, S., 296
Buck, C., 41
Budanitsky, A., 96
Budzianowski, P., 510
Buechel, S., 400
Bullinaria, J. A., 121
Bulyko, I., 506
Bunke, H., 190
Bunker, R. T., 362
Burchardt, A., 294
Burger, J. D., 435, 436,
441, 472, 485
Burges, C. J. C., 484, 496
Burget, L., 121, 176
Burkett, D., 70, 71, 73
Burnett, D., 513
Burstein, J., 445
Byrne, W., 542
Cafarella, M., 352
Cai, S., 298
Caliskan, A., 118, 439
Camacho-Collados, J., 119,
365, 367
Camburu, O.-M., 438
Campbell, J., 31
Candito, M., 297
Cao, J., 507
Cao, Y., 19, 457, 458, 462
Caputo, A., 366
Carberry, S., 490
Cardie, C., 119, 190, 352,
426, 431, 441, 460
Carenini, G., 462
Carletta, J., 421, 461
Carlson, L., 445, 447, 462
Carmel, D., 485
595

596
Author Index
Carpenter, B., 511
Carpenter, R., 492
Carreras, X., 384, 392
Carrillo, F., 445
Carroll, G., 393
Carroll, J., 270, 272
Carstensen, F. V., 166
Casale, J., 507
Casanueva, I., 510
Casta˜no, J., 343, 345, 347
Cavicchio, F., 425
Cecchi, G. A., 445
Cer, D., 119
ˇCernock`y, J., 176
ˇCernock`y, J. H., 121
Chafe, W. L., 421, 452, 463
Chahuneau, V., 407, 408
Chai, J. Y., 392
Chambers, N., 350, 353,
389, 426, 442
Chanan, G., 137
Chanev, A., 296
Chang, A., 13, 426, 442
Chang, A. X., 345, 353,
437, 478, 482
Chang, B., 366, 462
Chang, J. S., 371
Chang, K.-W., 117, 118,
427, 430, 438, 439
Chang, M.-W., 19, 122,
475, 483
Chaplot, D. S., 363
Charles, W. G., 414
Charlin, L., 495, 497, 498
Charniak, E., 166, 220, 258,
271, 370, 462, 463
Che, W., 392
Chelba, C., 250
Chen, B., 415
Chen, C., 436
Chen, D., 286, 468, 473,
474, 485
Chen, E., 456, 496
Chen, J., 460
Chen, J. N., 371
Chen, K., 111, 119, 121,
190
Chen, M. Y., 414
Chen, P., 496
Chen, Q., 495
Chen, S. F., 46, 48, 54
Chen, W., 392
Chen, X., 17, 99, 542
Chen, Z., 19, 137
Cherry, C., 495–497
Cheung, J. C. K., 438
Chiba, S., 563
Chierchia, G., 323
Chinchor, N., 73, 352
Chintala, S., 137
Chiticariu, L., 331
Chiu, J. P. C., 190
Cho, K., 185
Chodorow, M., 445
Choi, E., 483
Choi, J. D., 288, 297, 437
Choi, Y., 392, 393, 412,
413, 483
Chomsky, C., 322, 476, 484
Chomsky, N., 53, 205, 222,
229, 423
Chou, A., 478, 479, 483
Christodoulopoulos, C.,
167
Chu, Y.-J., 291
Chu-Carroll, J., 485, 490,
511, 516
Chung, T., 506, 516
Church, A., 308
Church, K. W., 14, 44, 46,
49, 54, 108, 109,
166, 244, 363, 388
Cialdini, R. B., 460
Ciaramita, M., 296, 360
Cinkov´a, S., 294
Citro, C., 137
Clark, C., 122, 363, 485
Clark, E., 96
Clark, H. H., 12, 421, 489,
516, 517
Clark, J., 543
Clark, J. H., 48, 49, 54
Clark, K., 400–402, 430,
442
Clark, P., 484
Clark, S., 121, 167, 286
CMU, 526, 540
Coavoux, M., 447, 462
Coccaro, N., 121, 516
Cohen, D. N., 563
Cohen, K. B., 325
Cohen, M. H., 502, 508,
509, 513
Cohen, P. R., 516
Colaresi, M. P., 406–408
Colby, K. M., 495, 515
Cole, R. A., 513
Coleman, J., 543
Collins, M., 220–222,
256–259, 270, 271,
278, 478, 527
Collobert, R., 111, 121,
190, 352, 392
Colombe, J. B., 352
Colosimo, M., 352
C¸ ¨oltekin, C¸ ., 294
Connolly, D., 435, 436, 441
Conrad, S., 230
Conti, J., 513
Cooper, F. S., 543
Copelli, M., 445
Copestake, A., 371
Coppola, B., 485
Corcoran, C. M., 445
Corrado, G. S., 19, 111,
119, 121, 190
Cote, S., 441
Cotterell, R., 439
Cottrell, G. W., 370
Courville, A., 94, 127–129,
142
Cover, T. M., 50, 51
Covington, M., 280, 296
Cowhey, I., 484
Cox, D., 93
Cox, S. J., 73
Crammer, K., 296
Craven, M., 352
Cretu, A.-M., 438
Cruse, D. A., 122
Csomai, A., 418, 436, 437
Cucerzan, S., 437
Culicover, P. W., 230
Culy, C., 462
Curran, J. R., 167
Cyganiak, R., 334, 476
Daelemans, W., 272
Dagan, I., 109, 110, 121,
363
Dagan, O., 368
Dai, A. M., 498
Dale, R., 463
Danescu-Niculescu-Mizil,
C., 460
Dang, H. T., 352, 371, 376,
392
Danieli, M., 508, 512
Danilevsky, M., 331
Das, D., 296, 485
Das, S. R., 414
Davidson, D., 312, 322
Davidson, T., 514
Davies, M., 102, 117
Davis, A., 137
Davis, E., 323, 438
Day, D. S., 347, 441
Dayanidhi, K., 505
de Courtenay, B., 543
Dean, J., 19, 49, 111, 119,
121, 137, 190
Deerwester, S. C., 120
DeJong, G. F., 352, 392
Delattre, P. C., 543
Della Pietra, S. A., 93
Della Pietra, V. J., 93
Delogu, F., 425
Demner-Fushman, D., 325
Dempster, A. P., 558
Denis, P., 427, 429, 430,
436, 441
Denker, J. S., 142
Derczynski, L., 353
DeRose, S. J., 166
Desmaison, A., 137
Devin, M., 137
DeVito, Z., 137
Devlin, J., 19, 122, 475
de Lacalle, O. L., 371
de Marneffe, M.-C., 147,
165, 217, 275, 294,
296, 441
De Mori, R., 516
de Paiva, V., 294
de Villiers, J. H., 513
Di Eugenio, B., 441
Diab, M., 13, 119, 371
Diaz, F., 438
Dienes, P., 164
Digman, J. M., 409
Dilley, L., 542
Dinesh, N., 447
Dingare, S., 421
Ditman, T., 445
Di Eugenio, B., 462, 463
Dligach, D., 369
Do, Q. N. T., 392, 393
Doddington, G., 203
Dodge, J., 368
Dolan, B., 371, 495–497
dos Santos, C. N., 336, 352,
430, 442
Downey, D., 352
Dowty, D. R., 322, 389
Dozat, T., 275, 294, 296
Dredze, M., 368
Droganova, K., 294
Dror, R., 73
Duan, N., 496
Dubou´e, P. A., 485
Ducharme, R., 111, 121,
138
Duda, R. O., 369
Dumais, S. T., 64, 72, 118,
120, 121, 469, 472
Dunn, H. K., 543
Dunphry, D., 64, 397, 414
Durrett, G., 430, 431, 437
duVerle, D. A., 462
Dyer, C., 180, 190, 331,
352, 360, 368, 483
Dziurzynski, L., 409–411
Eagon, J. A., 563
Earley, J., 235, 244
Ebert, S., 415
Echihabi, A., 463
Eckert, W., 516
Edmonds, J., 291
Edmonds, P., 371
Efron, B., 70
Egg, M., 463
Egghe, L., 28
Eichstaedt, J. C., 409–411
Eisenstein, J., 449, 462
Eisner, J., 296, 550
Ejerhed, E. I., 244
Ekman, P., 396
El-Kahky, A., 294
Elisseeff, A., 73
Ellis, D., 542
Ellsworth, M., 379, 380
Elman, J. L., 142, 170, 171,
190
Elsner, M., 462, 463
Elvev˚ag, B., 445
Emami, A., 438
Erk, K., 393
Eryigit, G., 296
Espeholt, L., 485
Esuli, A., 403
Etzioni, O., 340, 341, 352,
393, 478, 479, 484
Evanini, K., 505
Evans, C., 334, 476
Evans, N., 144
Evert, S., 121
Fader, A., 340, 341, 352,
478, 479
Fan, J., 485
Fano, R. M., 108

Author Index
597
Fant, G. M., 543
Fanty, M., 513
Farhadi, A., 485
Farkas, R., 297
Faruqui, M., 360, 368
Fast, E., 415
Fauconnier, G., 442
Fazel-Zarandi, M., 507
Feldman, J. A., 142
Fellbaum, C., 359, 370, 371
Feng, S., 485
Feng, V. W., 445, 460, 462,
463
Fennig, C. D., 13
Fensel, D., 321
Ferguson, J., 550
Ferguson, M., 217, 219
Fermandez, R., 190
Fernandes, E. R., 430
Fern´andez, S., 190
Ferro, L., 343, 345, 347
Ferrucci, D. A., 485
Fessler, L., 514
Field, A., 411
Fikes, R. E., 516
Filik, R., 442
Fillmore, C. J., 229, 230,
322, 374, 379, 391
Finegan, E., 230
Finkel, J., 22
Finkelstein, L., 118
Finlayson, M. A., 459
Firth, J. R., 94, 98, 515
Fisch, A., 468, 473, 474,
485
Fischer, M. J., 24, 563
Fisher, D., 352
Fitt, S., 541
Flickinger, D., 268, 270
Flynn, T. N., 399
Fodor, J. A., 120, 322, 387
Fodor, P., 485
Fokoue-Nkoutche, A., 485
Foland, W., 190
Foland, Jr., W. R., 392
Folds, D., 514
Foltz, P. W., 445, 457
Fong, C., 527
Forchini, P., 495
Forney, Jr., G. D., 563
Fosler, E., 34
Fosler-Lussier, E., 542
Foster, J., 297
Fox, B. A., 421
Fox Tree, J. E., 12
Francis, H. S., 254
Francis, M. E., 64, 398, 399
Francis, W. N., 11, 166
Frank, E., 73, 93
Franz, A., 49, 272
Fraser, N. M., 513
Freitag, D., 352
Fried, G., 514
Friedman, J. H., 73, 93
Friedman, M. W., 441, 453
Fromer, J. C., 508
Frostig, R., 478, 479, 483
Fry, D. B., 543
Fu, G., 449, 450, 462
Fung, P., 13, 516, 542
Furnas, G. W., 120
Fyshe, A., 122
Gabow, H. N., 292
Gabriel, R., 495
Gabrilovich, E., 118
Gage, P., 18
Gaizauskas, R., 343, 345,
347
Gale, W. A., 44, 46, 54,
363, 388
Galil, Z., 292
Galley, M., 497
Gandhe, S., 498
Gao, J., 49, 491, 492, 496,
497
Gao, Q., 19
Gao, S., 506, 516
Gardner, M., 122, 363, 485
Garg, N., 118, 439
Garside, R., 166, 167
Gaˇsi´c, M., 503, 504, 510
Gauvain, J.-L., 54, 121
Gazdar, G., 216
Gdaniec, C., 268, 270
Georgescu, M., 298
Georgila, K., 516
Geramifard, A., 507
Gerber, L., 343, 345
Gerber, M., 392
Gerbino, E., 508, 512
Gerten, J., 498
Gertz, M., 345, 353
Ghahramani, Z., 414
Ghemawat, S., 137
Ghoneim, M., 13
Giangola, J. P., 502, 508,
509, 513
Gibson, E., 462
Gil, D., 144
Gilbert, G. N., 513
Gildea, D., 190, 382, 392
Giles, C. L., 190
Gillick, L., 73
Ginter, F., 147, 165, 217,
275, 294, 296
Ginzburg, J., 511
Girard, G., 96
Giuliano, V. E., 120
Giv´on, T., 254
Glass, J., 512
Glavaˇs, G., 368
Glennie, A., 244
Godfrey, J., 11, 203, 495
Goebel, R., 393, 427
Goenaga, I., 297
Goffman, E., 391
Gojenola, K., 297
Gokirmak, M., 294
Gold, B., 544
Goldberg, J., 506
Goldberg, T. E., 445
Goldberg, Y., 110, 116,
118, 121, 122, 142,
147, 165, 190, 217,
275, 296, 297
Goldberger, J., 363
Goldwater, S., 167
Gondek, D. C., 485
Gonen, H., 118
Gong, M., 452
Gonnerman, L. M., 115
Gonzalez-Agirre, A., 119
Good, M. D., 513
Goodfellow, I., 127–129,
137, 142
Goodine, D., 512
Goodman, J., 46, 48, 54
Goodman, N., 451, 463
Goodwin, C., 516
Gopalakrishnan, K., 495
G¨or¨og, A., 362
Gosling, S. D., 409
Gottardi, A., 495
Gould, J. D., 512, 513
Gould, S. J., 94
Gouws, S., 19
Gravano, A., 516
Gravano, L., 338, 352
Grave, E., 121
Graves, A., 190
Green, B. F., 322, 476, 484
Green, J., 229
Green, L., 13, 65
Green, S., 297
Greenbaum, S., 230
Greenberg, S., 542
Greene, B. B., 166
Greenwald, A. G., 117, 118
Grefenstette, E., 483, 485
Gregory, M. L., 254
Grenager, T., 392
Grice, H. P., 491
Grifﬁtt, K., 298
Grishman, R., 268, 270,
350, 352, 378, 417,
436
Gross, S., 137
Grosz, B. J., 421, 441, 444,
452, 453, 463, 516
Grover, C., 329
Gruber, J. S., 374, 391
Guinaudeau, C., 463
Gulcehre, C., 185
Gulikers, L., 539
Gundel, J. K., 421
Guo, Y., 392
Gurevych, I., 459, 460
Gusﬁeld, D., 27, 28
Guyon, I., 73
Habash, N., 294, 297
Habernal, I., 459, 460
Hacioglu, K., 392
Haddow, B., 18, 20
Haghighi, A., 442
Hahn, U., 400, 441
Hajiˇc, J., 147, 164, 165,
217, 270, 275, 277,
294, 296
Hajiˇcov´a, E., 277, 295, 296
Hajishirzi, H., 437, 442,
485
Hakkani-T¨ur, D., 164, 495,
506, 510, 516
Hall, J., 296
Halliday, M. A. K., 444,
457
Hamilton, W. L., 117,
400–402
Han, Q., 17
Hanks, P., 108, 109, 347
Hannan, K., 414
Hansen, B., 513
Harabagiu, S., 392, 468
Harnish, R., 489
Harper, M. P., 167
Harris, K., 294
Harris, R. A., 513
Harris, Z. S., 94, 98, 166,
244
Harshman, R. A., 120
Hart, P. E., 369
Hart, T., 49
Hasan, R., 444, 457
Hastie, T., 73, 93
Hathi, S., 73
Hatzivassiloglou, V., 401,
402, 414
Haverinen, K., 275, 296
Haviland, S. E., 421
Hawkins, J. A., 421
Hawwari, A., 13
He, H., 483
He, L., 383, 384, 392, 427,
430–432, 434, 435,
442
He, Z., 366
Heaﬁeld, K., 41, 48, 49, 54
Heaps, H. S., 12, 28
Hearst, M. A., 334, 335,
340, 352, 371, 457
Heck, L. P., 510
Heckerman, D., 64, 72
Hedayatnia, B., 495
Hedberg, N., 421
Heikkil¨a, J., 167, 296
Heim, I., 308, 323, 440
Heinz, J. M., 543
Hellmann, S., 334, 476
Hellrich, J., 400
Hemphill, C. T., 203
Henderson, D., 142
Henderson, J., 354, 516
Henderson, M., 503
Henderson, P., 495, 507,
514
Hendler, J. A., 321
Hendrickson, C., 166
Hendrickx, I., 352
Hendrix, G. G., 391
Henrich, V., 362
Hepple, M., 347
Herdan, G., 12, 28
Hermann, K. M., 483, 485
Hermjakob, U., 298, 468
Hernault, H., 462
Hicks, T., 462
Hidey, C., 460
Higgins, H., 543
Hilf, F. D., 495, 515

598
Author Index
Hill, F., 96, 118, 438
Hindle, D., 268, 270, 271
Hinkelman, E. A., 516
Hinrichs, E., 362
Hinton, G. E., 115, 134,
137, 142, 175, 190
Hirschberg, J., 13, 506, 509,
511, 516, 527–529
Hirschman, L., 73, 352,
435, 436, 441, 472,
485, 512
Hirst, G., 96, 370, 387, 442,
445, 457, 460, 462,
463
Hitzeman, J., 463
Hjelmslev, L., 120
Hlav´acov´a, J., 294
Hobbs, J. R., 350–352, 440,
443, 462
Hochreiter, S., 184, 190
Hockenmaier, J., 228
Hoff, M. E., 142
Hoffmann, P., 64, 397
Hofmann, T., 121
Hollenback, J., 542
Holliman, E., 11, 495
Holtzman, A., 393, 413
Hon, H.-W., 544
Hopcroft, J. E., 208
Hopely, P., 166, 485
Horvitz, E., 64, 72
Hou, Y., 421, 425
House, A. S., 543
Householder, F. W., 167
Hovanyecz, T., 513
Hovy, D., 73, 360
Hovy, E., 368
Hovy, E. H., 99, 190, 277,
352, 371, 403, 425,
435, 436, 442, 457,
460, 462, 468
Howard, R. E., 142
Hu, M., 64, 397, 403
Hu, Z., 462
Huang, E. H., 119
Huang, L., 286
Huang, X., 17, 544
Huang, Z., 352
Hubbard, W., 142
Huddleston, R., 209, 230
Hudson, R. A., 295
Huffman, S., 352
Hughes, M., 19
Hull, D. A., 93, 121
Hulteen, E. A., 513
Hume, E., 542
Hunter, J., 462
Hunter, P., 505
Hutto, C. J., 514
Huval, B., 352
Hwang, A., 460
Hwang, J. D., 360
Hymes, D., 391
Iacobacci, I., 365
Iida, M., 441
Iida, R., 441
Ingria, R., 268, 270, 343,
345, 347, 515
Inui, K., 441
Irons, E. T., 244
Irsoy, O., 190
Irving, G., 137
Isard, M., 137
Isbell, C. L., 496
Ishizuka, M., 462
ISO8601, 344, 345
Israel, D., 350–352
Issar, S., 501
Iyyer, M., 122, 363, 483
Izquierdo, R., 362
Jackel, L. D., 142
Jackendoff, R., 230, 315
Jacobs, P. S., 352
Jacobson, N., 166
Jaech, A., 73
Jafarpour, S., 496
Jang, S. B., 344
Jat, S., 366
Jauhar, S. K., 368
Jauhiainen, T., 73
Jauvin, C., 111, 121, 138
Javitt, D. C., 445
Jefferson, G., 489, 490
Jeffreys, H., 53
Jelinek, F., 45, 53, 157, 250,
268, 270, 271, 563
Ji, H., 352, 417, 436
Ji, Y., 449, 462, 497
Jia, R., 472, 473
Jia, Y., 137
J´ınov´a, P., 277, 296
Johannsen, A., 73
Johansson, R., 296, 392
Johansson, S., 230
Johnson, C. R., 379, 380
Johnson, K., 542, 543
Johnson, M., 19, 257, 315,
360
Johnson, W. E., 54
Johnson-Laird, P. N., 379
Johnston, M., 511
Jones, M. P., 121
Jones, R., 338, 352, 506
Jones, S. J., 513
Jones, T., 13
Joos, M., 94, 98, 120
Jordan, M., 190
Jordan, M. I., 80, 121
Jorge, A., 364
Joshi, A., 447
Joshi, A. K., 166, 223, 230,
246, 271, 272, 441,
444, 447, 452, 453,
463, 485
Joshi, M., 433, 483
Joty, S., 462, 463
Joulin, A., 121
Jozefowicz, R., 137, 498
Juang, B. H., 563
Jun, S.-A., 529
Jurafsky, D., 13, 34, 40, 65,
99, 106, 117, 118,
121, 165, 339, 340,
350, 352, 371, 382,
389, 392, 400–402,
407–409, 427, 431,
439, 458, 460, 462,
497, 498, 516
Jurgens, D., 13, 40, 65, 369,
371
Justeson, J. S., 414
Kaiser, Ł., 19
Kaiser, L., 137
Kalai, A. T., 117, 118, 439
Kalyanpur, A., 485
Kameyama, M., 350–352,
441
Kamm, C. A., 511, 512
Kamm, T., 542
Kamp, H., 440
Kan, M.-Y., 451, 456, 463
Kanayama, H., 294, 485
Kane, S. K., 31
Kanerva, J., 294
Kang, J. S., 457, 458, 462
Kannan, A., 498
Kaplan, R. M., 235, 391,
440, 485, 498, 499,
515
Karaﬁ´at, M., 176
Karamanis, N., 463
Karlen, M., 111, 121, 190,
352, 392
Karlsson, F., 167, 296
Karttunen, L., 166, 416,
421, 440, 485
Kasami, T., 232, 244
Kasowski, S., 543
Kato, Y., 19
Katz, C., 229
Katz, G., 343, 345, 347
Katz, J. J., 120, 322, 387
Katz, K., 217, 219
Katz, S. M., 414
Kavukcuoglu, K., 111, 121,
190, 352, 392
Kawahara, D., 296
Kawakami, K., 180, 331,
352
Kawamoto, A. H., 370
Kay, M., 235, 244, 391,
498, 499, 515
Kay, P., 229, 230
Kay, W., 485
Kayadelen, T., 294
Kazawa, H., 19
Ke, N. R., 514
Kearns, M., 496, 506, 516
Kehler, A., 93, 441, 463
Keizer, S., 503, 504
Keller, F., 388, 389
Kelly, E. F., 370
Kembhavi, A., 485
Kennedy, C., 441
Kern, M. L., 409–411
Kert´esz, J., 28
Kettnerov´a, V., 277, 294,
296
Khoddam, E., 392
Khot, T., 484
Khudanpur, S., 121, 176
Kibble, R., 442
Kiela, D., 121
Kiesling, S., 542
Kilgarriff, A., 365
Kim, D., 510
Kim, G., 217, 219
Kim, S. M., 403
Kim, S. N., 352
King, L. A., 409
Kingma, D., 137
Kingsbury, P., 190, 392
Kintsch, W., 121, 322, 452,
457, 463
Kiparsky, P., 373
Kipper, K., 376, 392
Kirchhoff, K., 506
Kiritchenko, S., 399
Kiss, T., 22
Kit, C., 392
Klapaftis, I. P., 369
Klavans, J. L., 268, 270
Kleene, S. C., 27
Klein, D., 70, 71, 73, 163,
164, 167, 257, 258,
266, 271, 427, 430,
431, 437, 442
Klein, E., 16, 17, 28, 216
Klein, S., 166, 167, 484
Klementiev, A., 392
Klingner, J., 19
Kneser, R., 46, 47
Knight, K., 298
Knippen, R., 344
Knott, A., 463
Kobilarov, G., 334, 476
Kocijan, V., 438
Kocisky, T., 485
Koˇcisk´y, T., 483
Koehn, P., 48, 49, 54, 298,
527
Koenig, W., 543
Kol´aˇrov´a, V., 277, 296
Kombrink, S., 121
Kordoni, V., 463
Korhonen, A., 96, 118, 368
Kormann, D., 496
Kornai, A., 28
Kosinski, M., 409–411
Kozareva, Z., 352
Kraemer, H. C., 495
Kratzer, A., 308, 323
Krieger, M., 28
Kriese, L., 459
Krizhevsky, A., 137
Kr¨otzsch, M., 334
Krovetz, R., 22
Kruskal, J. B., 28, 563
K¨ubler, S., 296, 297
Kuˇcera, H., 11, 166
Kudlur, M., 137
Kudo, T., 19, 20, 296
Kuhlmann, M., 297
Kuhn, G. M., 190
Kuhn, J., 430
Kuhn, S., 463
Kuksa, P., 111, 121, 190,
352, 392

Author Index
599
Kulkarni, R. G., 506
Kullback, S., 387
Kumar, S., 366
Kumlien, J., 352
Kummerfeld, J. K., 427
Kuno, S., 244
Kuperberg, G. R., 445
Kuperman, V., 398
Kupiec, J., 166
Kurian, G., 19
Kurita, K., 439
Kwak, H., 400
Kwak, S., 294
Kwan, J. L. P., 485
Kwatra, S., 495
Kwiatkowski, T., 485
Laan, F., 362
Labov, W., 93
Lacy, L. Y., 543
Ladd, D. R., 526
Ladefoged, P., 543
Ladusaw, W. A., 543
Lafferty, J., 414
Lafferty, J. D., 93, 163, 271,
272, 352
Laham, D., 121
Lai, A., 445, 462
Laird, N. M., 558
Lakoff, G., 315, 322, 389,
459
Lal, T. N., 414
Lally, A., 485
Lamblin, P., 142
Lample, G., 180, 331, 352
Landauer, T. K., 118, 120,
121, 457, 513
Landes, S., 362
Lando, T., 294
Lang, J., 392
Lapata, M., 388, 389, 392,
444, 452, 454–457,
462, 463
Lapesa, G., 121
Lappin, S., 426, 441
Lari, K., 253
Larochelle, H., 142
Lascarides, A., 462, 463
Laughery, K., 322, 476, 484
Lauscher, A., 368
Lazo, M., 347
La Polla, R., 230
LDC., 541
Le, Q. V., 19, 497
Leacock, C., 362
Leass, H., 426, 441
LeCun, Y., 142
Lee, A., 447
Lee, C.-H., 515
Lee, D. D., 121
Lee, H., 426, 427, 431, 442,
462
Lee, K., 19, 122, 363, 383,
384, 392, 427,
430–432, 434, 435,
442, 475, 485
Lee, L., 72, 73, 414, 460,
462
Leech, G., 167, 230
Lehiste, I., 543
Lehmann, J., 334, 476
Lehnert, W. G., 352, 441
Leibler, R. A., 387
Lemon, O., 516
Lengerich, B., 368
Lenz, B., 368
Leonard, B., 438
Lerer, A., 137
Lertpradit, S., 294
Lesk, M. E., 370
Leskovec, J., 117, 400–402
Leung, H., 294, 512
Leuski, A., 496, 498
Levenberg, J., 137
Levenshtein, V. I., 23
Levesque, H., 437, 438
Levesque, H. J., 322, 516
Levin, B., 376, 391, 392
Levin, E., 515, 516
Levine, Y., 368
Levinson, S. C., 515
Levow, G.-A., 506, 508,
513
Levy, J. P., 121
Levy, O., 110, 116, 121,
122, 433, 438
Lewis, C., 512
Lewis, D. L., 73, 352
Lewis, M., 266, 383, 384,
392, 427, 430, 431,
434, 435, 442
Li, A., 542
Li, D., 491, 492, 496
Li, H., 496, 497
Li, J., 17, 99, 294, 457, 458,
462, 497, 498
Li, M., 516
Li, Q., 462
Li, R., 457, 462
Li, S., 449, 462
Li, S.-W., 507
Li, T., 462
Li, X., 17, 468, 470
Li, Y., 331, 392
Li, Z., 118, 392, 496
Liang, P., 472, 473, 478,
479, 483
Liberman, A. M., 543
Liberman, M. Y., 268, 270,
516
Lieberman, H., 321
Lieberman Aiden, E., 117
Light, M., 472, 485
Lim, D. C. Y., 428, 429,
441
Lin, D., 270, 296, 393, 427,
441
Lin, J., 468, 472
Lin, Y., 117
Lin, Z., 137, 445, 451, 456,
463, 516
Lind´en, K., 73
Lindsey, R., 322
Ling, W., 190
Ling, X., 437
Liscombe, J., 505
Lison, P., 495
Litkowski, K. C., 392
Litman, D. J., 490, 506,
509, 511, 512, 516
Littman, J., 343–345
Littman, M., 400, 402
Liu, A., 511
Liu, B., 64, 73, 397, 403
Liu, C.-W., 497, 498
Liu, T., 366, 392
Liu, T.-H., 291
Liu, Y., 452
Liwicki, M., 190
Llorens, H., 353
Lochbaum, K. E., 120, 516
Logeswaran, L., 462
Long, T., 457, 458, 462
Loper, E., 16, 17, 28
Lopez-Gazpio, I., 119
Lopyrev, K., 472, 483
Louis, A., 462, 463
Loureiro, D., 364
Louviere, J. J., 399
Lovins, J. B., 27
Lowe, J. B., 379
Lowe, R., 495, 514
Lowe, R. T., 497, 498
Lu, Z., 496, 497
Luhn, H. P., 105
Lui, M., 65, 73
Lukasiewicz, T., 438
Luo, F., 366
Luo, X., 435, 436
Luotolahti, J., 294
Lu´ıs, T., 190
Lyons, J., 323
Lyons, R. G., 544
Lytel, D., 370
Ma, X., 190, 352
Maas, A., 368
MacBride, J., 425
MacCartney, B., 296
Macherey, K., 19
Macherey, W., 19
MacIntyre, R., 217, 219
Macketanz, V., 294
Macleod, C., 378
Macy, M., 514
Madhu, S., 370
Madotto, A., 516
Magerman, D. M., 221,
271, 278
Maharjan, S., 13
Maier, W., 297
Maiorano, S., 468
Mairesse, F., 395, 411, 503,
504, 516
Manandhar, S., 369
Mandl, M., 294
Man´e, D., 137
Mani, I., 343–345
Mann, W. C., 445, 462
Manning, C. D., 22, 72, 73,
80, 111, 115, 116,
119, 121, 122, 147,
163–165, 167, 190,
217, 253, 257, 258,
266, 271, 275, 286,
294, 296, 345, 352,
353, 392, 414, 430,
442, 474, 478
Manurung, R., 294
Marcinkiewicz, M. A., 146,
217, 219, 253, 277,
296
Marcu, D., 445–447, 449,
451, 462, 463
Marcus, M. P., 146, 179,
190, 217, 219, 253,
268, 270, 271, 277,
296, 371, 391, 425,
436
Marcus, S., 109
Marheinecke, K., 294
Marinov, S., 296
Maritxalar, M., 119
Markert, K., 421, 425
Markov, A. A., 53, 563
Markovitch, S., 109
Marley, A. A. J., 399
Marlin, B. M., 352
Maron, M. E., 72
M`arquez, L., 296, 384, 392,
393
Marshall, C., 516
Marshall, I., 166
Marsi, E., 296
Mart´ı, M. A., 296, 420,
425, 442
Martin, J. H., 121, 190,
371, 392
Martin, R., 516
Martinez, D., 393
Martschat, S., 427
Marujo, L., 190
Marx, M., 508, 513
Masterman, M., 322, 370
Mathis, D. A., 190
Matias, Y., 118
Matsumoto, Y., 296, 441
Mausam, 393
Mausam., 352
McCallum, A., 72, 93, 163,
352, 437
McCarthy, J., 229
McCarthy, J. F., 441
McCaw, J., 31
McCawley, J. D., 230, 322,
323
McClelland, J. L., 142, 190
McClosky, D., 22
McConlogue, K., 484
McConnell-Ginet, S., 323
McCord, M. C., 485
McCoy, K. F., 31
McCulloch, W. S., 123, 141
McDaniel, J., 11, 495
McDonald, R., 147, 165,
217, 275, 290, 296,
297, 414
McEnery, A., 167
McFarland, D. A., 409
McGhee, D. E., 117, 118
McGuiness, D. L., 321

600
Author Index
McKeown, K., 401, 402,
414, 452, 460
Meek, C., 473, 483
Mehl, M. R., 409
Melamud, O., 363
Mel’˘cuk, I. A., 295
Melis, G., 483
Mellish, C., 463
Mendonc¸a, G., 294
Meng, Y., 17
Mercer, R. L., 45, 53, 157,
166, 271
Merialdo, B., 166
Mesgar, M., 463
Meteer, M., 166, 516
Metsis, V., 72
Meyers, A., 296, 378, 392
Micciulla, L., 425
Michael, J., 438
Michaelis, L. A., 254
Michel, J.-B., 117
Mihalcea, R., 119, 367,
371, 418, 436, 437
Mikheev, A., 329
Mikolov, T., 54, 111, 116,
119, 121, 176, 190
Mikulov´a, M., 277, 296
Milidi´u, R. L., 430
Miller, C. A., 526
Miller, G. A., 39, 53, 362,
414, 543
Miller, S., 352, 515
Milne, D., 437
Miltsakaki, E., 447
Minsky, M., 72, 126, 142,
391
Mintz, M., 339, 352
M´ırovsk´y, J., 277, 296
Missil¨a, A., 294
Mitchell, M., 497
Mitchell, T. M., 122
Mitkov, R., 442
Miwa, M., 336, 352
Mochales Palau, R., 459
Moens, M., 329, 461
Moens, M.-F., 392, 393,
459
Mohamed, A., 190
Mohammad, S. M.,
397–399
Moldovan, D., 371
Monga, R., 137
Monroe, B. L., 406–408
Monroe, W., 497, 498
Montague, R., 322
Monz, C., 469
Mooney, R. J., 478
Moore, S., 137
Moosavi, N. S., 435
Morante, R., 392
Morey, M., 449, 451, 452
Morgan, A. A., 352
Morgan, N., 34, 142, 544
Morgenstern, L., 438
Morimoto, T., 516
Morin, F., 54, 121
Morris, J., 457
Morris, M. R., 31
Morris, W., 355
Moschitti, A., 384, 425,
426, 436
Mosteller, F., 58, 72
Moszkowicz, J., 347
Mota, N. B., 445
Mozer, M. C., 190
Mrkˇsi´c, N., 368, 505, 510
Mulcaire, G., 73
Muller, P., 449, 451, 452
Murdock, J. W., 485
Muresan, S., 459, 460
Murphy, B., 122
Murphy, K. P., 73, 93
Murray, D., 137
Musi, E., 459, 460
Myers, G., 461
N´adas, A., 54
Nagao, M., 272
Nagata, M., 516
Nagy, P., 514
Nakov, P., 352
Naradowsky, J., 438
Narayanan, A., 118, 439
Narayanan, S. S., 508
Nash-Webber, B. L., 391,
440, 485
Naur, P., 229
Navigli, R., 361, 362, 365,
367, 369, 371
Nayak, N., 510
Nedoluzhko, A., 277, 294,
296
Needleman, S. B., 563
Neff, G., 514
Nenkova, A., 436, 452, 462,
463
Neumann, M., 122, 363
Newman, E. B., 533
Ney, H., 46, 47, 73, 251
Ng, A. Y., 80, 119, 121,
340, 352, 371
Ng, H. T., 365, 384, 428,
429, 441, 448, 451,
456, 463, 485
Ng, R. T., 462
Ng, V., 426–431, 436, 438,
441, 442
Nguyen, D. T., 463
Nguyen, K. A., 368
Nicely, P. E., 543
Nichols, E., 190
Niculae, V., 460
Nie, A., 451, 463
Nielsen, J., 513
Nielsen, M. A., 142
Nigam, K., 72, 93, 352
Nilsson, J., 296
Nilsson, N. J., 516
Nissim, M., 421
NIST, 28, 541
Nitisaroj, R., 294
Nitta, Y., 109
Nivre, J., 147, 165, 217,
275, 280, 286, 288,
290, 294, 296, 297,
392
Niwa, Y., 109
Noreen, E. W., 70
Norman, D. A., 322, 391,
489, 498, 499, 515
Norouzi, M., 19
Norvig, P., 128, 244, 304,
322
Nosek, B. A., 118
Noseworthy, M., 498
Novick, D. G., 513
Nunes, J. H. T., 516
O’Connor, B., 13, 28, 40,
65
O’Hara, T. P., 394
O’Shaughnessy, D., 544
Oberlander, J., 463
Och, F. J., 49
Oettinger, A. G., 244
Oﬂazer, K., 164
Ogilvie, D., 64, 397, 414
Ojala, S., 294
Okurowski, M. E., 447, 462
Olah, C., 137
Oravecz, C., 164
Ordonez, V., 118, 438, 439
Ortiz, C. L., 438
Orwant, J., 117
Osborne, M., 49, 167
Osgood, C. E., 97–99, 120,
397, 414
Osindero, S., 142
Ostendorf, M., 73, 506,
527, 528
Ozertem, U., 506
´O S´eaghdha, D., 352, 505,
510
O’Connor, B., 459
Packard, D. W., 27
Padnos, D., 368
Pad´o, S., 296, 352
Paliouras, G., 72
Paliwal, K. K., 182, 190
Palmer, D., 28
Palmer, M., 190, 277, 278,
288, 296, 298, 362,
371, 376, 392, 425,
436
Palmucci, J., 166
Pan, Y., 485
Panevova, J., 295
Panevov´a, J., 277, 296
Pang, B., 72, 73, 414
Pao, C., 512
Paolino, J., 514
Papert, S., 126, 142
Pappas, N., 354
Paradiso, A., 31
Pareek, A., 439
Parikh, A., 485
Paritosh, P., 334, 476
Park, J., 460
Parsons, T., 312, 322
Partee, B. H., 322
Pasca, M., 468, 469, 471
Paszke, A., 137
Patil, N., 19
Patwardhan, S., 485
Pearl, C., 513
Pedersen, J., 73, 93, 116,
121
Pedersen, T., 371
Peirsman, Y., 426, 442
Peldszus, A., 459, 460
Peng, N., 336, 352
Peng, S., 452
Penn, G., 373
Pennacchiotti, M., 352
Pennebaker, J. W., 64, 398,
399, 409
Pennington, C., 31
Pennington, J., 111, 115,
116, 121, 190, 474
Percival, W. K., 229
Pereira, F. C. N., 163, 296,
352
Perkowitz, M., 166
Perlis, A. J., 229
Perrault, C. R., 516
Peters, M., 122, 363
Peters, S., 322
Peterson, G. E., 543
Petrie, T., 563
Petrov, S., 117, 147, 165,
217, 258, 271, 275,
294, 296, 297
Petruck, M. R. L., 379, 380
Phillips, A. V., 484
Phillips, J., 344
Phillips, M., 512
Picard, R. W., 394
Piepenbrock, R., 539
Pieraccini, R., 505, 515,
516
Pierrehumbert, J. B., 528,
529
Pilehvar, M. T., 119, 365,
367, 371
Pineau, J., 495, 497, 498,
514, 516
Pitler, E., 294, 452, 463
Pitrelli, J. F., 528
Pitt, M. A., 542
Pitts, W., 123, 141
Plank, B., 73
Plato, 543
Plaut, D. C., 115
Plutchik, R., 396, 397
Poesio, M., 419, 420, 425,
441, 442, 463
Pol´akov´a, L., 277, 296
Polanyi, L., 462
Polifroni, J., 512
Pollard, C., 220, 221, 223,
229, 246, 441, 453
Ponti, E. M., 368
Ponzetto, S. P., 367, 437
Poon, H., 336, 352
Popat, A. C., 49
Popel, M., 294
Popescu, A.-M., 352
Popescu-Belis, A., 354
Popovici, D., 142
Porter, M. F., 21, 22
Potthast, M., 294

Author Index
601
Potts, C., 368, 404–406,
441
Pouzyrevsky, I., 48, 49, 54
Pow, N., 497
Pradhan, S., 190, 369, 384,
392, 425, 426, 435,
436, 448
Prager, J. M., 485
Prakash, S., 371
Prange, J., 360
Prasad, R., 447
Prasettio, M. C., 393, 413
Prendinger, H., 462
Prescher, D., 393
Price, P. J., 506, 527, 528
Prince, E., 420, 421, 463
Propp, V., 459
Przepi´orkowski, A., 297
Pu, X., 354
Pullum, G. K., 209, 216,
230, 543
Purver, M., 511
Pustejovsky, J., 343–345,
347, 353, 371, 442
Pyysalo, S., 147, 165, 217,
275, 294, 296
Qi, P., 294
Qin, B., 392
Qin, L., 462
Qiu, X., 17
Qiu, Z. M., 485
Quillian, M. R., 322, 370
Quinn, K. M., 406–408
Quirk, C., 336, 352
Quirk, R., 230
Rabiner, L. R., 544, 550,
560, 561, 563
Radev, D., 347, 442, 462
Radford, A., 230
Raganato, A., 365, 371
Raghavan, P., 73, 122
Raghunathan, K., 442
Rahman, A., 427, 429, 430,
438, 441
Rajpurkar, P., 472, 473, 483
Ramshaw, L., 425, 436
Ramshaw, L. A., 166, 179,
190, 270, 277, 371
Ranganath, R., 409
Rangarajan, S., 442
Raphael, B., 322
Rappaport Hovav, M., 376
Rashkin, H., 392, 393, 412,
413, 516
Ratinov, L., 121, 437
Ratnaparkhi, A., 93, 167,
271, 272
Rau, L. F., 352
Raux, A., 503
Ravichandran, D., 468
Raymond, W. D., 542
Recasens, M., 420, 425,
435, 436, 438, 439,
441, 442
Reddy, S., 294
Reed, C., 459
Reeves, R., 378
Rehder, B., 121
Rehm, G., 294
Reichart, R., 73, 96, 118
Reichenbach, H., 314
Reichert, T. A., 563
Reichman, R., 421
Reiss, F., 331
Reiss, F. R., 331
Renshaw, E., 484
Resnik, P., 272, 371, 387,
388, 393
Reynar, J. C., 272
Ribarov, K., 296
Ribeiro, S., 445
Richardson, J., 20
Richardson, M., 483, 484
Riedel, S., 296, 352
Rieger, C., 370
Ries, K., 516
Riesa, J., 19
Riesbeck, C. K., 370
Riezler, S., 393
Rigau, G., 119
Riloff, E., 338, 352, 392,
397, 414, 415, 419,
427, 441, 485
Ritter, A., 352, 393,
495–498
Rivlin, E., 118
Robaldo, L., 447
Robins, R. H., 543
Rocci, A., 459
Rodriguez, P., 485
Rohde, D. L. T., 115
Rohde, H., 441
Rojas-Barahona, L. M., 368
Romano, L., 352
Rooth, M., 271, 393
Rosenblatt, F., 142
Rosenfeld, R., 93
Rosenthal, S., 460
Rosenzweig, J., 365
Roth, D., 427, 430, 431,
436, 437, 468, 470
Roth, R., 297
Rothe, S., 415
Roukos, S., 268, 270–272
Routledge, B. R., 407, 408
Rowe, G., 459
Roy, N., 516
Rozovskaya, A., 427
Rubin, D. B., 558
Rubin, G. M., 166
Rudinger, R., 438
Rudnicky, A. I., 509
Ruhi, U., 542
Rumelhart, D. E., 134, 142,
175, 190, 322
Rumshisky, A., 344
Ruppenhofer, J., 379, 380,
392
Ruppin, E., 118
Rush, A. M., 427, 430, 431,
442
Russell, J. A., 396
Russell, S., 128, 304, 322
Rutherford, A., 448, 463
Rutishauser, H., 229
Sabharwal, A., 484
Sacks, H., 489
Sag, I. A., 216, 220, 221,
223, 229, 230, 246,
511, 516
Sagae, K., 286, 462
Saghir, H., 457, 458, 462
Sahami, M., 64, 72
Sakoe, H., 563
Salakhutdinov, R., 363
Salakhutdinov, R. R., 137
Salant, S., 485
Saligrama, V., 117, 118,
439
Salomaa, A., 271
Salton, G., 100, 120
Samdani, R., 427, 430
Samelson, K., 229
Sammons, M., 427
Sampson, G., 167
Samuelsson, C., 159, 167
Sanﬁlippo, A., 270
Sankoff, D., 93, 563
Santorini, B., 146, 217,
253, 268, 270, 277,
296
Sap, M., 393, 413
Saur´ı, R., 343, 345, 347
Sauri, R., 344
Saxena, K., 366
Scha, R., 462
Schabes, Y., 271, 272
Schafer, R. W., 544
Schalkwyk, J., 513
Schank, R. C., 322, 349,
379, 391, 392
Schapire, R. E., 166, 272
Schasberger, B., 217, 219
Schatzmann, J., 503, 504
Scheffczyk, J., 379, 380
Schegloff, E. A., 489
Scherer, K. R., 394, 395,
409, 410
Schiebinger, L., 118, 416,
439
Schilder, F., 347
Schmandt, C., 513
Schmelzenbach, M., 392
Schmidhuber, J., 184, 190
Schmolze, J. G., 322
Schneider, J., 461
Schneider, N., 298, 360
Schoenick, C., 484
Sch¨olkopf, B., 414
Scholz, M., 296
Schone, P., 121
Sch¨onﬁnkel, M., 308
Schreiner, M. E., 121
Schulte im Walde, S., 368
Schulz, H., 438
Schuster, M., 19, 137, 182,
190
Schuster, S., 294
Sch¨utze, H., 73, 93, 116,
121, 122, 166, 253,
368, 371, 388, 415
Schwartz, H. A., 409–411
Schwartz, J. L. K., 117, 118
Schwartz, R., 166, 352, 515
Schwarz, J., 483
Schwenk, H., 54, 121, 185
S´eaghdha, D. O., 393
S´eaghdha, D. ´O., 368
Sebastiani, F., 403
Seddah, D., 297
See, A., 347
Segal, J., 34
Sekine, S., 270
Selfridge, J. A., 39
Semeraro, G., 366
Sen´ecal, J.-S., 54, 121
Seneff, S., 512, 541
Sennrich, R., 18, 20
Seo, M., 485
Serban, I. V., 495, 497, 498
Sethi, A., 506, 516
Sethi, R., 244
Setzer, A., 343, 345, 347
Seung, H. S., 121
Søgaard, A., 73, 167, 190,
447, 462
Sgall, P., 295
Shah, A., 19, 409–411
Shaked, T., 352
Shalev-Shwartz, S., 368
Shang, L., 497
Shannon, C. E., 39, 53, 177
Sharir, O., 368
Sharma, D., 288
Shashua, A., 368
Shattuck-Hufnagel, S., 527
Shaw, G. B., 543
Sheil, B. A., 244
Sheinwald, D., 485
Shepherd, J., 414, 415
Shi, T., 497, 498
Shi, Z., 17
Shieber, S. M., 427, 430,
431, 442
Shima, H., 485
Shimada, A., 294
Shin, J., 516
Shlens, J., 137
Shockey, L., 543
Shoham, Y., 368
Shoup, J. E., 519
Shriberg, E., 506, 516
Shrivastava, M., 288
Shum, H.-Y., 491, 492, 496
Siddharthan, A., 461
Sidner, C. L., 463, 516
Sigman, M., 445
Silveira, N., 147, 165, 217,
275, 296
Silverman, K., 528
Simi, M., 294
Simma, A., 441
Simmons, R. F., 166, 167,
322, 370, 381, 391,
484, 485
Simons, G. F., 13
Singer, Y., 163, 164, 166,
167, 272
Singh, A., 438

602
Author Index
Singh, S., 392, 412, 413,
437, 496
Singh, S. P., 516
Sinha, K., 514
Sleator, D., 272, 296
Slezak, D. F., 445
Slocum, J., 391
Small, S. L., 370
Smith, E. M., 516
Smith, J., 19
Smith, M., 64, 397, 414
Smith, N. A., 73, 368, 407,
408, 459
Smith, V. L., 517
Smolensky, P., 142
Snow, R., 339, 340, 352,
371
Snyder, B., 362, 456
Socher, R., 111, 115, 116,
119, 121, 190, 352,
474
Soderland, S., 340, 341,
352, 478
Solan, Z., 118
Solorio, T., 13
Somasundaran, S., 445
Song, Z., 542
Soon, W. M., 428, 429, 441
Sordoni, A., 497
Soricut, R., 462
Sparck Jones, K., 106, 120,
370, 371
Spencer, T., 292
Spitkovsky, V. I., 437, 482
Sporleder, C., 392, 462, 463
Srikumar, V., 360
Srinivas, B., 272
Srivastava, N., 137
Stab, C., 459, 460
Stalnaker, R. C., 489
Stamatatos, E., 73
Stede, M., 459–461, 463
Steedman, M., 167, 223,
228, 266, 421, 529
Steiner, B., 137
Stent, A., 297
ˇStˇep´anek, J., 296
Stern, A., 360
Stetina, J., 272
Stevens, K., 19
Stevens, K. N., 543, 544
Stevens, S. S., 533
Stevenson, R., 463
Stevenson, S., 392
Stifelman, L. J., 513
Stillwell, D., 409–411
Stoics, 143
Stolcke, A., 34, 49, 54, 271,
516
Stolz, W. S., 166
Stone, P., 64, 397, 414, 496
Stone, P. J., 370
Stoyanchev, S., 511
Straka, M., 294
Stranˇa´k, P., 296
Streeter, L., 120
Strnadov´a, J., 294
Str¨otgen, J., 345, 353
Strube, M., 421, 425, 427,
435–437, 441, 463
Strunk, J., 22
Strzalkowski, T., 268, 270
Stuckardt, R., 420, 442
Sturge, T., 334, 476
Sturt, P., 442
Stuttle, M., 516
Su, J., 352, 441
Su, P.-H., 368, 510
Subba, R., 462
Subramanian, S., 180, 331,
352, 436
Suci, G. J., 97–99, 120,
397, 414
Suendermann, D., 505
Suh, J., 483
Sui, Z., 366
Suleman, K., 438
Sulubacak, U., 294
Sun, X., 17
Sundheim, B., 343, 345,
347, 350, 352
Surdeanu, M., 22, 296, 352,
392, 393, 426, 427,
431, 442, 462
Sutskever, I., 111, 121, 137
Sutton, S., 513
Svartvik, J., 230
Sweet, H., 543
Swerts, M., 506, 509
Swier, R., 392
Switzer, P., 120
Szekely, R., 378
Szpakowicz, S., 352
Tafjord, O., 484
Tajchman, G., 34
Taji, D., 294
Takamura, H., 441
Talbot, D., 49
Talmor, A., 483
Talukdar, P., 366
Talukdar, P. P., 122
Talwar, K., 137
Tan, C., 460
Tan, C. L., 441
Tannen, D., 391
Tannenbaum, P. H., 97–99,
120, 166, 397, 414
Tarjan, R. E., 292
Taylor, J., 334, 476
Taylor, L., 441
Taylor, P., 516
Teh, Y.-W., 142
Temperley, D., 272, 296
Tengi, R. I., 362
Teo, L. H., 485
ter Meulen, A., 323
Tesni`ere, L., 295, 391
Tetreault, J., 297, 445, 462
Tetreault, J. R., 441
Teufel, S., 461
The PDP Research Group,
190
Thede, S. M., 167
Thelen, M., 485
Thibaux, R., 258, 271
Thione, G. L., 462
Thomas, J. A., 50, 51
Thompson, C. W., 391
Thompson, H., 391, 498,
499, 515
Thompson, K., 27
Thompson, R. A., 247
Thompson, S. A., 445, 462
Thomson, B., 368,
503–505, 510
Thrax, D., 143
Thrun, S., 516
Tibshirani, R. J., 70, 73, 88,
93
Ticrea, M., 442
Tiedemann, J., 495
Tillmann, C., 270
Titov, I., 392
Tomkins, S. S., 396
Toutanova, K., 19, 122,
163, 164, 167, 336,
352, 475
Trancoso, I., 190
Traum, D., 496, 498
Trichelair, P., 438
Trischler, A., 438
Trnka, K., 31
Tsarfaty, R., 147, 165, 217,
275, 296, 297
Tseng, B.-H., 510
Tseng, H., 165
Tsvetkov, Y., 13, 40, 65,
360, 411, 439
Tucker, P., 137
Tur, G., 516
T¨ur, G., 164
Turian, J., 121
Turner, M., 442
Turney, P. D., 397, 399,
400, 402, 414
Tyers, F. M., 294
Tyson, M., 350–352
Ullman, J. D., 208, 244,
278
Ultes, S., 510
Ungar, L. H., 409–411
Uresov´a, Z., 294
Uria, L., 119
Uryupina, O., 425, 426, 436
Uszkoreit, H., 294
UzZaman, N., 353
Vaithyanathan, S., 72, 414
Valenzuela-Escarcega,
M. A., 462
van Benthem, J., 323
van Deemter, K., 442
Van Den Bosch, A., 362
van der Maaten, L., 115
Van Ess-Dykema, C., 516
van Harmelen, F., 321
van Merri¨enboer, B., 185
van Rijsbergen, C. J., 67,
243, 270
Van Valin, Jr., R. D., 230
van Wijnagaarden, A., 229
Vandyke, D., 368, 510
Vanhoucke, V., 137
Vannella, D., 369
van den Berg, M., 462
Van Dijk, T. A., 452, 463
Van Durme, B., 438
Van Gompel, M., 362
Van Ooyen, B., 41
Vasudevan, V., 137
Vauquois, B., 229
Veilleux, N., 527
Velikovich, L., 414
Venditti, J. J., 529
Vendler, Z., 315
Venkataramani, V., 542
Venkatesh, A., 495
Verhagen, M., 344, 347,
353
Vermeulen, P. J. E., 513
Versley, Y., 297, 420, 442
Vi´egas, F., 137
Vieira, R., 419, 441
Vilain, M., 435, 436, 441
Villemonte de la Cl´ergerie,
E., 297
Vilnis, L., 437, 498
Vincent, P., 94, 111, 121,
138
Vincze, V., 297
Vintsyuk, T. K., 563
Vinyals, O., 19, 137, 497,
498
Viterbi, A. J., 563
Vodolazova, T., 362
Volkmann, J., 533
Volkova, S., 392, 412, 413
Vossen, P., 362
Voutilainen, A., 167, 296
Vrandeˇci´c, D., 334
Vu, N. T., 368
Vuli´c, I., 368
Vyas, N., 439
Wade, E., 506
Wagner, R. A., 24, 563
Wagstaff, K., 441
Wahlster, W., 321
Walker, M. A., 395, 411,
441, 463, 490, 506,
508, 510–512, 516
Wall, R. E., 322
Wallace, D. L., 58, 72
Wang, A., 438
Wang, C., 448
Wang, H., 448, 496
Wang, S., 72, 80, 414
Wang, T., 118, 438, 439
Wang, W., 19, 118
Wang, Y., 449, 462
Ward, W., 392, 501
Warden, P., 137
Warmsley, D., 514
Warriner, A. B., 398
Wasow, T., 230
Wattenberg, M., 137
Weaver, W., 370
Webber, B. L., 417, 418,
421, 440, 447, 448,
463

Author Index
603
Weber, I., 514
Weber, S., 495, 515
Webster, K., 438, 439
Weenink, D., 534, 542, 544
Wegstein, J. H., 229
Wehbe, L., 122
Weinberger, D. R., 445
Weinschenk, S., 508
Weinstein, S., 441, 444,
452, 453, 463
Weischedel, R., 166, 277,
352, 371, 425, 436
Weischedel, R. M., 190
Weizenbaum, J., 2, 10,
491–493, 515
Weld, D. S., 352, 433, 437,
442, 483
Wells, J. C., 526, 543
Welty, C., 485
Wen, T.-H., 368, 505, 510
Werbos, P., 175
Werbos, P. J., 175
Wessels, L. F. A., 513
Weston, J., 111, 121, 190,
352, 392, 414, 427,
431, 468, 473, 474,
485
Whiteside, J. A., 513
Whitney, D., 507
Whittaker, S., 490
Wicke, M., 137
Widrow, B., 142
Wiebe, J., 64, 119, 394,
397, 414
Wierzbicka, A., 120
Wightman, C. W., 528
Wilensky, R., 516
Wilkes-Gibbs, D., 516
Wilks, Y., 322, 370, 387,
391
Williams, J., 392
Williams, J. D., 503, 516
Williams, R., 352
Williams, R. J., 134, 142,
175, 190
Wilson, G., 343, 345
Wilson, T., 64, 397
Winograd, T., 322, 391,
437, 440, 498, 499,
515
Winston, P. H., 391
Wiseman, S., 427, 430,
431, 442
Witten, I. H., 54, 73, 93,
437
Wittgenstein, L., 98, 489,
515
Wixon, D. R., 513
Wolf, A. K., 322, 476, 484
Wolf, F., 462
Wolfe, M. B. W., 121
Woli´nski, M., 297
Wong, A. K. C., 563
Woodger, M., 229
Woods, W. A., 322, 440,
485
Woodsend, K., 392
Wooters, C., 34
Wr´oblewska, A., 297
Wu, F., 352
Wu, Y., 19
Wundt, W., 205, 229
Wunsch, C. D., 563
Xia, F., 278, 296
Xia, Q., 366
Xiang, B., 336, 352, 442
Xing, E., 462
Xu, P., 49, 457, 458, 462,
516
Xu, W., 190, 352, 392
Xue, N., 277, 296, 384,
392, 425, 426, 436,
447, 448, 463
Yallop, C., 543
Yamada, H., 296
Yan, Z., 496
Yang, D., 460
Yang, E., 137
Yang, J., 449, 462
Yang, X., 441
Yang, Y., 73, 473, 483
Yang, Z., 460
Yankelovich, N., 508, 513
Yao, L., 352
Yarowsky, D., 363, 371,
388
Yarrington, D., 31
Yasseri, T., 28
Yasunaga, M., 442
Yates, A., 352
Yatskar, M., 118, 438, 439,
483
Yeh, A. S., 352
Yi, L., 542
Yih, W.-t., 116, 119, 336,
352, 473, 483
Yngve, V. H., 244
Yordanov, Y., 438
Young, B., 378
Young, C., 19
Young, S., 505, 510
Young, S. J., 253, 503, 504,
510, 516
Younger, D. H., 232, 244
Yu, K., 352, 503, 504
Yu, M., 368
Yu, N., 449, 450, 462
Yu, Y., 137, 452
Yu, Z., 294
Yuan, A., 17
Yuret, D., 296
Zacharski, R., 421
Zampieri, M., 73
Zapirain, B., 393
Zavrel, J., 272
Zeldes, A., 452
Zelle, J. M., 478
Zeman, D., 147, 165, 217,
275, 294, 296
Zettlemoyer, L., 122, 352,
363, 383, 384, 392,
427, 430–435, 437,
442, 478, 479, 483
Zhai, C., 73
Zhang, J., 352, 472, 483
Zhang, L., 73, 485
Zhang, M., 352, 449, 450,
462
Zhang, R., 442
Zhang, Y., 286, 296, 384,
425, 426, 436
Zhang, Z., 462
Zhao, H., 392, 462
Zhao, J., 118, 438, 439
Zheng, F., 542
Zheng, J., 437
Zheng, X., 137
Zhong, Z., 365, 384
Zhou, B., 336, 352
Zhou, D., 414
Zhou, G., 352, 392, 441
Zhou, J., 190, 392, 496
Zhou, L., 442, 491, 492,
496
Zhou, M., 496
Zhou, Y., 118, 447
Zhu, H., 331
Zhu, X., 414
Zhu, Y., 452
Zielinska, V., 378
Zik´anov´a, ˇS., 277, 296
Zilles, L., 437, 442
Zou, J., 117, 118, 439
Zue, V. W., 512, 541
Zweig, G., 116, 119


Subject Index
λ-reduction, 308
*?, 7
+?, 7
.wav format, 531
10-fold cross-validation, 69
→(derives), 205
ˆ, 58
* (RE Kleene *), 5
+ (RE Kleene +), 5
. (RE any character), 5
$ (RE end-of-line), 5
( (RE precedence symbol),
6
[ (RE character
disjunction), 4
\B (RE non
word-boundary), 6
\b (RE word-boundary), 6
] (RE character
disjunction), 4
ˆ (RE start-of-line), 5
[ˆ] (single-char negation),
4
∃(there exists), 306
∀(for all), 306
=⇒(implies), 309
λ-expressions, 308
λ-reduction, 308
∧(and), 306
¬ (not), 306
∨(or), 309
4-gram, 35
4-tuple, 207
5-gram, 35
A-D conversion, 530
AAC, 31
AAVE, 13
abduction, 311
ABox, 316
ABSITY, 370
absolute discounting, 46
absolute temporal
expression, 343
abstract word, 398
accent
nuclear, 525
accented syllables, 525
accessible, 421
accessing a referent, 416
accomplishment
expressions, 315
achievement expressions,
315, 315
acknowledgment speech
act, 489
activation, 124
activity expressions, 315,
315
add gate, 185
add-k, 44
add-one smoothing, 42
adjacency pairs, 489
adjective, 144, 212
adjective phrase, 212
adjunction in TAG, 230
adverb, 144
days of the week coded
as noun instead of,
145
degree, 145
directional, 145
locative, 145
manner, 145
syntactic position of, 212
temporal, 145
adversarial evaluation, 498
affective, 394
afﬁx, 21
affricate sound, 523
agent, as thematic role, 374
agglomerative clustering,
369
AIFF ﬁle, 531
ALGOL, 229
algorithm
CKY, 234
forward, 554
forward-backward, 562
inside-outside, 253
Kneser-Ney discounting,
46
Lesk, 366
minimum edit distance,
26
n-gram tiling for question
answering, 472
naive Bayes classiﬁer, 58
pointwise mutual
information, 108
probabilistic CKY, 251
semantic role labeling,
381
Simpliﬁed Lesk, 366
TextTiling, 457
unsupervised word sense
disambiguation, 368
Viterbi, 153, 555
alignment, 23
minimum cost, 25
of transcript, 541
string, 23
via minimum edit
distance, 25
all-words task in WSD, 362
Allen relations, 347
allophonic rules, 542
alveolar sound, 522
ambiguity
amount of part-of-speech
in Brown corpus,
148
attachment, 233
coordination, 233, 234,
256
in meaning
representations, 299
of referring expressions,
418
part-of-speech, 148
PCFG in, 249
prepositional phrase
attachment, 255
resolution of tag, 148
word sense, 362
American Structuralism,
229
amplitude
of a signal, 529
RMS, 532
anaphor, 417
anaphora, 417
anaphoricity detector, 426
anchor texts, 437, 481
anchors in regular
expressions, 5, 27
answer type, 468
answer type taxonomy, 468
antecedent, 417
antonym, 357
any-of, 68
AP, 212
Apple AIFF, 531
approximant sound, 523
approximate
randomization, 70
Arabic, 518
Egyptian, 539
Aramaic, 518
ARC, 484
arc eager, 287
arc standard, 280
argumentation mining, 459
argumentation schemes,
460
argumentative relations,
459
argumentative zoning, 461
Aristotle, 143, 315
arity, 312
ARPAbet, 542
article (part-of-speech), 145
articulatory phonetics, 520,
520
aspect, 315
ASR
conﬁdence, 509
association, 96
ATIS, 203
corpus, 206, 209
ATN, 391
ATRANS, 390
attachment ambiguity, 233
attention mechanism, 199
Attribution (as coherence
relation), 446
AU ﬁle, 531
augmentative
communication, 31
authorship attribution, 56
autoregressive generation,
177
auxiliary verb, 146
B3, 436
BabelNet, 367
backoff
in smoothing, 44
backprop, 134
Backpropagation Through
Time, 175
backtrace, 557
in minimum edit
distance, 25
Backus-Naur Form, 205
backward chaining, 310
backward composition, 225
backward probability, 558
backward-looking center,
453
bag of words, 58, 59
bag-of-words, 58
barge-in, 512
baseline
most frequent sense, 363
take the ﬁrst sense, 363
basic emotions, 396
batch training, 86
Bayes’ rule, 58
dropping denominator,
59, 152
Bayesian inference, 58
BDI, 516
Beam search, 288
beam search, 158, 196
beam width, 158, 288
bear pitch accent, 525
Berkeley Restaurant
Project, 34
Bernoulli naive Bayes, 72
BERT
for affect, 411
for coreference, 433
best-worst scaling, 399
bi-LSTM, 327
bias term, 77, 124
bidirectional RNN, 182
bigram, 32
bilabial, 521
binary branching, 222
binary NB, 63
binary tree, 222
bitexts, 193
bits for measuring entropy,
50
Bloom ﬁlters, 49
BNF (Backus-Naur Form),
205
bootstrap, 71
bootstrap algorithm, 71
bootstrap test, 70
bootstrapping, 70
in IE, 337
bound pronoun, 419
605

606
Subject Index
boundary tones, 528
BPE, 18
bracketed notation, 206
break index, 528
bridging inference, 421
British National Corpus
(BNC)
POS tags for phrases,
147
Brown, 147
Brown corpus, 11
original tagging of, 166
byte-pair encoding, 18
canonical form, 300
captalization
for unknown words, 159
capture group, 10
cardinal number, 212
cascade, 22
regular expression in
Eliza, 10
case
sensitivity in regular
expression search, 3
case folding, 21
case frame, 375, 391
cataphora, 419
categorial grammar, 223,
223
CD (conceptual
dependency), 390
CELEX, 539
Centering Theory, 444, 452
centroid, 107
CFG, see context-free
grammar
chain rule, 91, 135
channels in stored
waveforms, 531
character embeddings, 330
Charniak parser, 258
chatbots, 2, 491
Chinese
characters, 518
Chomsky normal form,
222, 251
Chomsky-adjunction, 223
chunking, 240, 241
CIRCUS, 352
citation form, 95
Citizen Kane, 443
CKY algorithm, 232
probabilistic, 251
claims, 459
clariﬁcation questions, 511
class-based n-gram, 54
clause, 210
clefts, 422
clitic, 16
origin of term, 143
closed class, 144
closed vocabulary, 41
closure, stop, 522
cluster, 417
clustering
in word sense
disambiguation, 371
CNF, see Chomsky normal
form
coarse senses, 371
cochlea, 537
Cocke-Kasami-Younger
algorithm, see CKY
coda, syllable, 524
code switching, 13
coherence, 443
entity-based, 452
relations, 445
cohesion
lexical, 444, 457
Collins parser, 258
collocation, 365
combinatory categorial
grammar, 223
commissive speech act, 489
common ground, 489, 516
common nouns, 144
complement, 215, 215
complementizer, 145
completeness in FOL, 311
componential analysis, 389
compression, 530
Computational Grammar
Coder (CGC), 166
computational semantics,
298
concatenation, 27
conceptual dependency, 390
concordance, semantic, 362
concrete word, 398
conditional independence,
271
conﬁdence
ASR, 509
in relation extraction, 338
conﬁdence values, 338
conﬁguration, 278
conjoined phrase, 216
conjunction, 145
conjunctions, 216
as closed class, 145
connectionist, 142
connotation frame, 412
connotation frames, 392
connotations, 97, 395
consistent, 247
consonant, 520
constants in FOL, 305
constative speech act, 489
constituency, 204
evidence for, 204
constituent, 204
Constraint Grammar, 296
Construction Grammar, 229
content planning, 509
context embedding, 114
context-free grammar, 203,
204, 207, 228
Chomsky normal form,
222
invention of, 229
multiplying probabilities,
248, 271
non-terminal symbol,
205
productions, 205
rules, 205
terminal symbol, 205
weak and strong
equivalence, 222
contingency table, 66
continuation rise, 528
conversation, 487
conversation analysis, 515
conversational agents, 487
conversational analysis, 489
conversational implicature,
491
convex, 82
coordinate noun phrase,
216
coordination ambiguity,
234, 256
copula, 146
corefer, 416
coreference chain, 417
coreference resolution, 417
gender agreement, 423
Hobbs tree search
algorithm, 440
number agreement, 422
person agreement, 423
recency preferences, 423
selectional restrictions,
424
syntactic (“binding”)
constraints, 423
verb semantics, 424
coronal sound, 522
corpora, 11
corpus, 11
ATIS, 206
BNC, 147
Brown, 11, 166
CASS phonetic of
Mandarin, 542
Kiel of German, 542
LOB, 166
regular expression
searching inside, 3
Switchboard, 11, 495,
530, 531, 541
Switchboard phonetic,
541
TimeBank, 347
TIMIT, 541
correction act detection,
506
cosine
as a similarity metric,
104
cost function, 80
count nouns, 144
counters, 27
counts
treating low as zero, 162
CRF, 164
cross-brackets, 270
cross-entropy, 51
cross-entropy loss, 81, 133
cross-validation, 69
10-fold, 69
crowdsourcing, 398
currying, 308
cycles in a wave, 529
cycles per second, 529
date
fully qualiﬁed, 345
normalization, 501
dative alternation, 376
debiasing, 118
decision boundary, 78, 127
decision tree
use in WSD, 371
declarative sentence
structure, 209
decoder, 555
decoding, 152, 555
Viterbi, 152, 555
deduction
in FOL, 310
deep
neural networks, 123
deep learning, 123
deep role, 374
deﬁnite reference, 419
degree adverb, 145
deleted interpolation, 157
delexicalization, 510
denotation, 302
dental sound, 522
dependency
grammar, 273
lexical, 256
dependency tree, 276
dependent, 274
derivation
direct (in a formal
language), 208
syntactic, 205, 205, 208,
208
description logics, 316
Det, 205
determiner, 145, 205, 211
development test set, 69
development test set
(dev-test), 36
devset, see development
test set (dev-test), 69
dialogue, 487
dialogue act
correction, 506
dialogue acts, 503
dialogue manager
design, 512
dialogue policy, 507
dialogue systems, 487
design, 512
evaluation, 511
diathesis alternation, 376
diff program, 28
digitization, 530
dimension, 100
diphthong, 524
origin of term, 143
direct derivation (in a
formal language),
208
directional adverb, 145
directive speech act, 489

Subject Index
607
disambiguation
PCFGs for, 247
role of probabilistic
parsing, 246
syntactic, 234
via PCFG, 249
discount, 42, 44, 45
discounting, 42
discourse, 443
segment, 446
discourse connectives, 447
discourse deixis, 418
discourse model, 416
discourse parsing, 448
discourse-new, 420
discourse-old, 420
discovery procedure, 229
discriminative model, 76
disﬂuency, 11
disjunction, 27
pipe in regular
expressions as, 6
square braces in regular
expression as, 4
dispreferred response, 517
distance
cosine, 104
distant supervision, 339
distributional hypothesis,
94
distributional similarity,
229
document frequency, 106
document vector, 107
domain, 302
domination in syntax, 205
dot product, 77, 103
dropout, 137
duration
temporal expression, 343
dynamic programming, 24
and parsing, 234
forward algorithm as,
552
history, 563
Viterbi as, 153, 555
E-step (expectation step) in
EM, 562
edge-factored, 289
edit distance
minimum, 24
EDU, 446
Elaboration (as coherence
relation), 445
ELIZA, 2
implementation, 10
sample conversation, 10
Elman Networks, 170
ELMo
for affect, 411
for coreference, 433
EM
Baum-Welch as, 558
E-step, 562
for deleted interpolation,
45
inside-outside in parsing,
253
M-step, 562
embedded verb, 213
embeddings, 99
character, 330
cosine for similarity, 103
GloVe, 111
skip-gram, learning, 113
sparse, 103
tf-idf, 105
word2vec, 111
emission probabilities, 150,
549
EmoLex, 397
emotion, 395
emphatic accent, 525
empty category, 210
encoder-decoder, 194
end-to-end training, 181
endpointing, 488
English
simpliﬁed grammar
rules, 206
entity grid, 454
entity linking, 417, 436,
478
entity-based coherence, 452
entropy, 50
and perplexity, 50
cross-entropy, 51
per-word, 51
rate, 51
relative, 387
error backpropagation, 134
ethos, 459
Euclidean distance
in L2 regularization, 87
Eugene Onegin, 53, 563
evalb, 270
evaluating parsers, 268
evaluation
10-fold cross-validation,
69
comparing models, 38
cross-validation, 69
development test set, 36,
69
devset, 69
devset or development
test set, 36
dialogue systems, 511
extrinsic, 36
most frequent class
baseline, 148
named entity recognition,
331
of n-gram, 36
of n-grams via
perplexity, 37
pseudoword, 388
relation extraction, 342
test set, 36
training on the test set, 36
training set, 36
unsupervised WSD, 369
WSD systems, 363
event coreference, 418
Event extraction, 325
event extraction, 346
event variable, 312
events
representation of, 311
Evidence (as coherence
relation), 445
evoking a referent, 416
existential there, 146
expansion, 206, 209
expectation step, 253
expectation step in EM, 562
Expectation-Maximization,
see EM
expletive, 422
explicit conﬁrmation, 508
expressiveness, of a
meaning
representation, 301
extraposition, 422
extrinsic evaluation, 36
F (for F-measure), 67, 243,
270
F-measure, 67, 243, 243,
270
F-measure
in NER, 331
F0, 532
factoid question, 466
false negatives, 7
false positives, 7
fasttext, 121
FASTUS, 350
feature cutoff, 162
feature interactions, 79
feature selection
information gain, 73
feature template, 285
feature templates, 80
part-of-speech tagging,
161
Federalist papers, 72
feedforward network, 129
ﬁle format, .wav, 531
ﬁlled pause, 11
ﬁller, 11
ﬁnal fall, 527
First Order Logic, see FOL
ﬁrst-order co-occurrence,
116
ﬂap (phonetic), 523
focus, 480
FOL, 298, 304
∃(there exists), 306
∀(for all), 306
=⇒(implies), 309
∧(and), 306, 309
¬ (not), 306, 309
∨(or), 309
and veriﬁability, 304
constants, 305
expressiveness of, 301,
304
functions, 305
inference in, 304
terms, 305
variables, 305
fold (in cross-validation),
69
food in NLP
ice cream, 550
forget gate, 184
formal language, 207
formant, 537
forward algorithm, 552, 553
FORWARD ALGORITHM,
554
forward chaining, 310
forward composition, 225
forward trellis, 552
forward-backward
algorithm, 558, 563
backward probability in,
558
relation to inside-outside,
253
FORWARD-BACKWARD
ALGORITHM, 562
forward-looking centers,
453
Fosler, E., see
Fosler-Lussier, E.
fragment of word, 11
frame
semantic, 379
frame elements, 379
FrameNet, 379
frames, 498
free word order, 273
Freebase, 334
frequency
of a signal, 529
fricative sound, 522
Frump, 352
fully qualiﬁed date
expressions, 345
fully-connected, 129
function word, 144, 165
functional grammar, 230
functions in FOL, 305
fundamental frequency, 532
gaussian
prior on weights, 88
gazetteer, 329
General Inquirer, 64, 397
generalize, 87
generalized semantic role,
376
generation
of sentences to test a
CFG grammar, 206
template-based, 502
generative grammar, 207
generative lexicon, 371
generative model, 76
generative syntax, 230
generator, 205
generics, 422
genitive NP, 231
German, 539
gerundive postmodiﬁer, 212
Gilbert and Sullivan, 325
given-new, 421
gloss, 359

608
Subject Index
glosses, 355
Glottal, 522
glottal stop, 522
glottis, 520
Godzilla, speaker as, 385
gold labels, 66
Good-Turing, 46
government and binding,
229
gradient, 83
Grammar
Constraint, 296
Construction, 229
Government and
Binding, 229
Head-Driven Phrase
Structure (HPSG),
220, 229
Lexical-Functional
(LFG), 229
Link, 296
Probabilistic Tree
Adjoining, 272
Tree Adjoining, 230
grammar
binary branching, 222
categorial, 223, 223
CCG, 223
checking, 232
combinatory categorial,
223
equivalence, 222
generative, 207
strong equivalence, 222
weak equivalence, 222
Grammar Rock, 143
grammatical function, 274
grammatical relation, 274
grammatical sentences, 207
greedy, 162
greedy RE patterns, 7
Greek, 518
greeting, 146
grep, 3, 3, 27
Gricean maxims, 491
grounding, 489
GUS, 498
H* pitch accent, 528
Hamilton, Alexander, 72
hanzi, 16
harmonic, 539
harmonic mean, 67
Hays, D., 296
head, 220, 274
ﬁnding, 220
in lexicalized grammar,
258
tag, 259
head tag, 259
Head-Driven Phrase
Structure Grammar
(HPSG), 220, 229
Heaps’ Law, 12
Hebrew, 518
held out, 36
held-out, 45
Herdan’s Law, 12
hertz as unit of measure,
529
hidden, 150, 549
hidden layer, 129
as representation of
input, 130
hidden units, 129
HMM, 150, 549
deleted interpolation, 157
formal deﬁnition of, 150,
549
initial distribution, 150,
549
observation likelihood,
150, 549
observations, 150, 549
simplifying assumptions
for POS tagging,
152
states, 150, 549
transition probabilities,
150, 549
trigram POS tagging, 155
Hobbs algorithm, 440
Hobbs tree search algorithm
for pronoun
resolution, 440
holonym, 358
homonymy, 354
Hungarian
part-of-speech tagging,
164
hyperarticulation, 506
hypernym, 334, 357
lexico-syntactic patterns
for, 334
hyperparameters, 137
hyponym, 357
Hz as unit of measure, 529
IBM, 53
IBM Thomas J. Watson
Research Center, 53
ice cream, 550
idf, 106
idf term weighting, 106
if then reasoning in FOL,
310
immediately dominates,
205
imperative sentence
structure, 209
implicature, 491
implicit argument, 392
implicit conﬁrmation, 508
implied hierarchy
in description logics, 320
indeﬁnite article, 211
indeﬁnite reference, 419
inference, 301
in FOL, 310
inference-based learning,
294
inﬁnitives, 215
infoboxes, 333
information
structure, 420
status, 420
information extraction (IE),
325
bootstrapping, 337
partial parsing for, 240
information gain, 73
for feature selection, 73
Information retrieval, 102
initiative, 490
inner ear, 537
inner product, 103
inside-outside algorithm,
253, 271
instance checking, 319
Institutional Review Board,
514
intensity of sound, 533
intent determination, 500
intercept, 77
interjection, 146
intermediate phrase, 527
in ToBI, 528
internal rule in a CFG
parse, 259
International Phonetic
Alphabet, 519, 542
interpersonal stance, 409
Interpolated Kneser-Ney
discounting, 46, 48
interpolation
in smoothing, 44
interpretable, 91
interpretation, 302
intonation phrases, 527
intransitive verbs, 215
intrinsic evaluation, 36
IOB, 241, 328, 504
IOB tagging
for NER, 328
for temporal expressions,
343
IPA, 519, 542
IR
idf term weighting, 106
vector space model, 100
IRB, 514
IS-A, 358
is-a, 334
ISO 8601, 344
iSRL, 392
Japanese, 518, 539
Jay, John, 72
joint intention, 516
joint probability, 248
Katz backoff, 45
KBP, 352
KenLM, 49, 54
KL divergence, 387
KL-ONE, 322
Kleene *, 5
sneakiness of matching
zero things, 5
Kleene +, 5
Kneser-Ney discounting, 46
knowledge base, 300
knowledge claim, 461
knowledge-based, 365
Korean, 539
KRL, 322
Kullback-Leibler
divergence, 387
L* pitch accent, 528
L+H* pitch accent, 528
L1 regularization, 88
L2 regularization, 87
label bias, 163
labeled precision, 270
labeled recall, 270
labial place of articulation,
521
labiodental consonants, 521
lambda notation, 308
language ID, 64
language id, 56
language model, 31
PCFG, 250
Laplace smoothing, 42
Laplace smoothing:for
PMI, 110
larynx, 520
lasso regression, 88
latent semantic analysis,
120
lateral sound, 523
LDC, 16, 253
learning rate, 83
lemma, 12, 95
versus wordform, 12
lemmatization, 3
Lesk algorithm, 365
Simpliﬁed, 365
Levenshtein distance, 23
lexical
category, 205
cohesion, 444, 457
database, 359
dependency, 256
head, 271
semantics, 95
stress, 525
trigger, in IE, 343
lexical answer type, 480
lexical dependency, 256
lexical rule
in a CFG parse, 259
lexical sample task in
WSD, 362
Lexical-Functional
Grammar (LFG),
229
lexicalized grammar, 258
lexico-syntactic pattern,
334
lexicon, 205
likelihood, 59
linear classiﬁers, 60
linear interpolation for
n-grams, 45
linearly separable, 127
Linguistic Data
Consortium, 16, 253
Linguistic Discourse
model, 462
Link Grammar, 296

Subject Index
609
List (as coherence relation),
446
LIWC, 64, 398
LM, 31
LOB corpus, 166
locative, 145
locative adverb, 145
log
why used for
probabilities, 35
why used to compress
speech, 531
log likelihood ratio, 406
log odds ratio, 406
log probabilities, 35, 35
logical connectives, 306
logical vocabulary, 301
logistic function, 77
logistic regression, 75
conditional maximum
likelihood
estimation, 81
Gaussian priors, 88
learning in, 80
regularization, 88
relation to neural
networks, 131
logos, 459
Long short-term memory,
184
long-distance dependency,
217
traces in the Penn
Treebank, 217
wh-questions, 210
lookahead in RE, 11
loss, 80
loudness, 534
LSI, see latent semantic
analysis
LSTM, 352
for NER, 330
for SRL, 383
LUNAR, 485
Lunar, 322
M-step (maximization step)
in EM, 562
machine learning
for NER, 331
textbooks, 73, 93
macroaveraging, 68
Madison, James, 72
Mandarin, 539
Manhattan distance
in L1 regularization, 88
manner adverb, 145
manner of articulation, 522
marker passing for WSD,
370
Markov, 33
assumption, 33
Markov assumption, 149,
548
Markov chain, 53, 149, 548
formal deﬁnition of, 150,
549
initial distribution, 150,
549
N-gram as, 150, 549
states, 150, 549
transition probabilities,
150, 549
Markov model, 33
formal deﬁnition of, 150,
549
history, 53
Marx, G., 232
mass nouns, 144
maxent, 93
maxim, Gricean, 491
maximization step, 253
maximization step in EM,
562
maximum entropy, 93
maximum matching, 19
maximum spanning tree,
290
MaxMatch, 19
MCTest, 484
mean reciprocal rank, 483
meaning representation,
298
as set of symbols, 299
early uses, 322
languages, 299
mechanical indexing, 120
mel
scale, 533
MEMM, 160
compared to HMM, 160
inference (decoding),
163
learning, 163
Viterbi decoding, 163
mention detection, 425
mention-pair, 428
mentions, 416
meronym, 358
meronymy, 358
MeSH (Medical Subject
Headings), 57, 362
Message Understanding
Conference, 350
metarule, 216
metonymy, 358, 442
Micro-Planner, 322
microaveraging, 68
Microsoft .wav format, 531
mini-batch, 86
minimum edit distance, 23,
23, 24, 153, 555
example of, 26
MINIMUM EDIT DISTANCE,
26
MLE
for n-grams, 33
for n-grams, intuition, 34
MLP, 129
modal verb, 146
model, 301
modiﬁed Kneser-Ney, 48
modus ponens, 310
Montague semantics, 322
morpheme, 21
Moses, Michelangelo statue
of, 487
most frequent sense, 363
MRR, 483
MUC, 350, 352
MUC F-measure, 436
multi-label classiﬁcation,
68
multi-layer perceptrons,
129
multinomial classiﬁcation,
68
multinomial logistic
regression, 89
multinomial naive Bayes,
58
multinomial naive Bayes
classiﬁer, 58
N-gram
as Markov chain, 150,
549
n-gram, 31, 33
absolute discounting, 46
add-one smoothing, 42
as approximation, 32
as generators, 39
equation for, 33
example of, 35
for Shakespeare, 39
history of, 53
interpolation, 44
Katz backoff, 45
KenLM, 49, 54
Kneser-Ney discounting,
46
logprobs in, 35
normalizing, 34
parameter estimation, 34
sensitivity to corpus, 38
smoothing, 42
SRILM, 54
test set, 36
training set, 36
unknown words, 41
n-gram
tiling, 472
naive Bayes
multinomial, 58
simplifying assumptions,
59
naive Bayes assumption, 59
naive Bayes classiﬁer
use in text categorization,
58
named entity, 326
list of types, 327
recognition, 325, 327
named entity recognition,
179
names
and gazetteers, 329
census lists, 329
NarrativeQA, 483
nasal sound, 520, 522
nasal tract, 520
negative log likelihood loss,
133
negative part-of-speech,
146
neo-Davidsonian, 312
NER, 325
neural networks
relation to logistic
regression, 131
newline character, 9
noisy-or, 338
NomBank, 378
Nominal, 205
non-capturing group, 10
non-ﬁnite postmodiﬁer, 212
non-greedy, 7
non-logical vocabulary, 301
non-terminal symbols, 205,
206
normal form, 222, 222
normalization
dates, 501
temporal, 344
word, 21
normalization of
probabilities, 33
normalized, 326
normalizing, 131
noun, 144
abstract, 144, 211
common, 144
count, 144
days of the week coded
as, 145
mass, 144, 211
proper, 144
noun phrase, 204
constituents, 205
NP, 205, 206
NP attachment, 255
nuclear accent, 525
nucleus, 445
nucleus of syllable, 524
null hypothesis, 70
numerals
as closed class, 145
Nyquist frequency, 531
o, 521
object, syntactic
frequency of pronouns
as, 254
observation bias, 163
observation likelihood
role in forward, 553
role in Viterbi, 154, 556
old information, and word
order, 254
one-hot vector, 139
one-of, 68
onset, syllable, 524
ontology, 316
OntoNotes, 371
OOV (out of vocabulary)
words, 41
OOV rate, 41
open class, 144
open information
extraction, 340
open vocabulary system

610
Subject Index
unknown words in, 41
operation list, 23
operator precedence, 6, 6
optionality
of determiners, 211
use of ? in regular
expressions for, 4
oral tract, 520
ordinal number, 212
orthography
opaque, 519
transparent, 519
output gate, 185
overﬁtting, 87
palatal sound, 522
palate, 522
palato-alveolar sound, 522
parallel distributed
processing, 142
parent annotation, 257
parse tree, 205, 208
parsed corpus, 271
parsing
ambiguity, 232
chunking, 240
CKY, 235, 251
CYK, see CKY
evaluation, 268
history, 244
partial, 240
probabilistic CKY, 251
relation to grammars,
209
shallow, 240
syntactic, 232
well-formed substring
table, 244
part-of-speech
adjective, 144
adverb, 144
as used in CFG, 205
closed class, 144, 145
greeting, 146
interjection, 146
negative, 146
noun, 144
open class, 144
particle, 145
subtle distinction
between verb and
noun, 144
usefulness of, 143
verb, 144
part-of-speech tagger
PARTS, 166
TAGGIT, 166
Part-of-speech tagging, 148
part-of-speech tagging
ambiguity and, 148
amount of ambiguity in
Brown corpus, 148
and morphological
analysis, 164
capitalization, 159
feature templates, 161
for phrases, 147
history of, 166
Hungarian, 164
Stanford tagger, 164
state of the art, 149
Turkish, 164
unknown words, 159
part-whole, 358
partial parsing, 240
particle, 145
PARTS tagger, 166
parts of speech, 143
passage retrieval, 469
passages, 469
pathos, 459
pattern, regular expression,
3
PCFG, 246
for disambiguation, 247
lack of lexical sensitivity,
254
lexicalized, 271
parse probability, 248
poor independence
assumption, 254
rule probabilities, 247
use in language
modeling, 250
PCM (Pulse Code
Modulation), 531
PDP, 142
PDTB, 447
Penn Discourse TreeBank,
447
Penn Treebank, 217
for statistical parsing,
253
POS tags for phrases,
147
tagging accuracy, 149
tagset, 146, 146
Penn Treebank
tokenization, 16
per-word entropy, 51
perceptron, 126
period of a wave, 529
perplexity, 37, 52
as weighted average
branching factor, 37
deﬁned via
cross-entropy, 52
personal pronoun, 145
personality, 409
persuasion, 460
phone, 518, 542
phoneme, 542
phonetics, 518
articulatory, 520, 520
phonotactics, 525
phrasal verb, 145
phrase-structure grammar,
205, 229
PII, 495
pipe, 6
The Pirates of Penzance,
325
pitch, 533
pitch accent, 525
ToBI, 528
pitch extraction, 534
pitch track, 532
planning
and speech acts, 516
shared plans, 516
pleonastic, 422
plosive sound, 522
plural, 211
Pointwise mutual
information, 108
politeness marker, 146
Porter stemmer, 21
POS, 143
possessive NP, 231
possessive pronoun, 145
postdeterminer, 212
postmodiﬁer, 212
postposed constructions,
204
Potts diagram, 405
power of a signal, 533
PP, 206
PPMI, 108
praat, 534, 535, 542
precedence, 6
precedence, operator, 6
Precision, 67
precision, 243
in NER, 331
predeterminer, 213
predicate, 215
predicate-argument
relations, 215
preference semantics, 370
premises, 459
preposed constructions, 204
prepositional phrase, 212
attachment, 255
constituency, 206
preposing, 204
prepositions, 145
as closed class, 145
presequences, 490
pretraining, 138
primitive decomposition,
389
principle of contrast, 96
prior probability, 59
probabilistic CKY
algorithm, 251, 251
probabilistic parsing, 251
productions, 205
progressive prompting, 508
projection layer, 140
Prolog, 310
prominence, phonetic, 526
prominent word, 525
prompts, 502
pronoun, 145
and old information, 254
as closed class, 145
bound, 419
demonstrative, 420
non-binary, 423
personal, 145
possessive, 145
wh-, 145
pronunciation dictionary,
539
CELEX, 539
CMU, 540
PRONLEX, 541
PropBank, 377
proper noun, 144
propositional meaning, 96
prosodic phrasing, 527
prosody, 526
accented syllables, 525
reduced vowels, 526
PROTO-AGENT, 376
PROTO-PATIENT, 376
pseudoword, 388
PTAG, 272
PTRANS, 390
punctuation
for numbers
cross-linguistically,
16
for sentence
segmentation, 22
part-of-speech tags, 146
stripping before
part-of-speech
tagging, 148
tokenization, 15
treated as words, 11
treated as words in LM,
40
QuAC, 483
qualia structure, 371
quantiﬁer
as part of speech, 212
semantics, 306
quantization, 531
query
reformulation in QA, 468
question
classiﬁcation, 468
factoid, 466
rise, 527
question answering
evaluation, 483
factoid questions, 466
query reformulation in,
468
range, regular expression, 4
rapid reprompting, 509
rarefaction, 530
RDF, 334
RDF triple, 334
RE
regular expression, 3
reading comprehension,
472
Reason (as coherence
relation), 445
Recall, 67
recall, 243
in NER, 331
recipe
meaning of, 298
recurrent neural networks,
170
reduced vowels, 526
reduction, phonetic, 526

Subject Index
611
reference
bound pronouns, 419
cataphora, 419
deﬁnite, 419
generics, 422
indeﬁnite, 419
reference point, 314
referent, 416
accessing of, 416
evoking of, 416
reﬂexive, 423
register in RE, 10
regression
lasso, 88
ridge, 88
regular expression, 3, 27
substitutions, 9
regularization, 87
rejection
conversation act, 508
relatedness, 96
relation extraction, 325
relative
temporal expression, 343
relative entropy, 387
relative frequency, 34
relative pronoun, 213
release, stop, 522
relevance, 491
relexicalize, 510
ReLU, 125
reporting events, 347
representation learning, 94
resolution for inference,
311
resolve, 148
response generation, 496
restrictive grammar, 502
restrictive relative clause,
213
retroﬁtting, 368
ReVerb, 340
reversives, 357
rewrite, 205
Rhetorical Structure
Theory, see RST
rhyme, syllable, 524
Riau Indonesian, 144
ridge regression, 88
rime
syllable, 524
RMS amplitude, 532
role-ﬁller extraction, 349
Rosebud, sled named, 443
rounded vowels, 524
row vector, 102
RST, 445
TreeBank, 447, 462
rules
context-free, 205
context-free, expansion,
205, 209
context-free, sample, 206
S as start symbol in CFG,
205
SAE, 13
salience, in discourse
model, 421
SAM phonetic
transcription, 539
sampling
of analog waveform, 530
rate, 530
used in clustering, 369
satellite, 445
saturated, 126
Sch¨onﬁnkelization, 308
“Schoolhouse Rock”, 143
schwa, 526
SCISOR, 352
sclite package, 28
script
Schankian, 379
scripts, 349
SDRT (Segmented
Discourse
Representation
Theory), 462
second-order
co-occurrence, 116
secondary stress, 526
seed pattern in IE, 337
seed tuples, 337
segmentation
maximum matching, 19
sentence, 22
word, 15
selectional association, 388
selectional preference
strength, 387
selectional preferences
pseudowords for
evaluation, 388
selectional restriction, 384
representing with events,
385
violations in WSD, 387
semantic concordance, 362
semantic drift in IE, 338
semantic feature, 120
semantic ﬁeld, 97
semantic frame, 97
semantic grammars, 501
semantic network
for word sense
disambiguation, 370
semantic networks
origins, 322
semantic parsing, 298
semantic relations in IE,
332
table, 333
semantic role, 374, 374,
376
Semantic role labeling, 380
semantics
lexical, 95
semivowel, 521
sense
word, 354, 355
sentence
segmentation, 22
sentence realization, 509
sentence segmentation, 3
sentence selection, 473
SentencePiece, 20
sentential complements,
214
sentiment, 97
origin of term, 414
sentiment analysis, 56
sentiment lexicons, 64
SentiWordNet, 403
sequence model, 149
SGNS, 111
Shakespeare
n-gram approximations
to, 39
shallow discourse parsing,
451
shallow parse, 240
shared plans, 516
shift-reduce parsing, 278
SHRDLU, 322
sibilant sound, 523
side sequence, 490
sigmoid, 77, 124
similarity, 96
Simpliﬁed Lesk, 365
singleton, 417
singular they, 423
skip-gram, 111
slot ﬁlling, 352, 500
slots, 498
smoothing, 42, 42
absolute discounting, 46
add-one, 42
discounting, 42
for HMM POS tagging,
157
interpolation, 44
Katz backoff, 45
Kneser-Ney discounting,
46
Laplace, 42
linear interpolation, 45
snippets, 469
softmax, 89, 131
source-ﬁlter model, 539
spam detection, 56, 64
span, 471
Spanish, 539
spectrogram, 537
spectrum, 535
speech
telephone bandwidth,
531
wideband, 531
speech acts, 489
spelling correction
use of n-grams in, 30
split, 256
split and merge, 258
split-half reliability, 400
SQuAD, 472, 483
SRILM, 54
SRL, 380
Stacked RNNs, 181
Stanford tagger, 164
start symbol, 205
state
semantic representation
of, 311
stationary stochastic
process, 51
statistical parsing, 251
stative expressions, 315
stem, 21
Stemming, 3
stemming, 21
stop (consonant), 522
stop words, 61
stress
lexical, 525
secondary, 526
strong equivalence of
grammars, 222
structural ambiguity, 232
structure prediction, 179
structured polysemy, 358
stupid backoff, 49
subcategorization
tagsets for, 215
subcategorization frame,
215
examples, 215
subcategorize for, 215
subdialogue, 490
subject, syntactic
frequency of pronouns
as, 254
in wh-questions, 210
subjectivity, 394, 414
substitutability, 229
substitution in TAG, 230
substitution operator
(regular
expressions), 9
subsumption, 317, 319
subword, 17
superordinate, 358
supersenses, 360
Supertagging, 264
supervised machine
learning, 57
suprasegmental, 526
SVD, 120
Switchboard, 147
Transcription Project,
541
Switchboard Corpus, 11,
495, 530, 531, 541
syllabiﬁcation, 525
syllable, 524
accented, 525
coda, 524
nucleus, 524
onset, 524
prominent, 525
rhyme, 524
rime, 524
synonyms, 96, 357
synset, 359
syntactic categories, 143
syntactic disambiguation,
234
syntactic movement, 217
syntax, 203
origin of term, 143

612
Subject Index
TAG, 230, 272
TAGGIT, 166
tagset
difference between Penn
Treebank and
Brown, 147
history of Penn
Treebank, 147
Penn Treebank, 146, 146
table of Penn Treebank
tags, 146
tanh, 125
tap (phonetic), 523
target embedding, 114
Tay, 514
TBox, 316
technai, 143
telephone-bandwidth
speech, 531
telic eventualities, 316
template ﬁlling, 326, 349
template recognition, 349
template, in IE, 349
template-based generation,
502
temporal adverb, 145
temporal anchor, 346
temporal expression
absolute, 343
metaphor for, 315
recognition, 326
relative, 343
temporal expressions, 326
temporal logic, 312
temporal normalization,
344
temporal reasoning, 323
tense logic, 312
term
clustering, 370, 371
in FOL, 305
term frequency, 105
term-document matrix, 100
term-term matrix, 102
terminal symbol, 205
terminology
in description logics, 316
test set, 36
development, 36
how to choose, 36
text categorization, 56
bag of words assumption,
58
naive Bayes approach, 58
unknown words, 61
text normalization, 2
part-of-speech tagging,
147
TextTiling, 457
tf-idf, 107
thematic grid, 375
thematic role, 374
and diathesis alternation,
376
examples of, 374
problems, 376
theme, 374
theme, as thematic role, 374
there, existential in English,
146
thesaurus, 370
tier
of ToBI transcript, 528
time, representation of, 312
time-aligned transcription,
541
TimeBank, 347
TIMIT, 541
ToBI, 528
boundary tones, 528
tiers, 528
tokenization, 2
maximum matching, 19
sentence, 22
word, 15
tokens, word, 12
topic (information
structure), 254
topic models, 97
trace, 210, 217
trachea, 520
training oracle, 283
training set, 36
cross-validation, 69
how to choose, 36
transcription
time-aligned, 541
Transformations and
Discourse Analysis
Project (TDAP),
166
transformer
for NER, 327
transition probability
role in forward, 553
role in Viterbi, 154, 556
transitive verbs, 215
TREC, 485
Tree Adjoining Grammar
(TAG), 230
adjunction in, 230
probabilistic, 272
substitution in, 230
treebank, 216, 253
trigram, 35
truth-conditional semantics,
303
tune, 527
continuation rise, 528
Turing test
Passed in 1972, 495
Turkish
part-of-speech tagging,
164
turn correction ratio, 512
turns, 488
type raising, 225
typed dependency structure,
273
types
word, 12
ungrammatical sentences,
207
unit production, 235
unit vector, 104
Universal Dependencies,
275
Unix, 3
<UNK>, 41
unknown words
in n-grams, 41
in part-of-speech
tagging, 159
in text categorization, 61
unvoiced sound, 520
user-centered design, 512
utterance, 11
vagueness, 300
vanishing gradients, 184
variable
existentially quantiﬁed,
307
universally quantiﬁed,
307
variables, 301
variables in FOL, 305
vector, 100, 124
vector length, 104
vector semantics, 94, 98
vector space, 100
vector space model, 100
velar sound, 522
velum, 522
verb
copula, 146
modal, 146
phrasal, 145
verb alternations, 376
verb phrase, 206, 214
Verbs, 144
veriﬁability, 299
Viterbi algorithm, 24, 153,
555
backtrace in, 557
decoding in MEMM, 163
history of, 563
VITERBI ALGORITHM,
153, 556
vocal
cords, 520
folds, 520
tract, 520
voice user interface, 512
voiced sound, 520
voiceless sound, 520
vowel, 520
back, 523
front, 523
height, 523
high, 523
low, 523
mid, 523
reduced, 526
rounded, 523
VP attachment, 255
waveﬁle format, 531
weak equivalence of
grammars, 222
Web Ontology Language,
321
WebQuestions, 483
well-formed substring
table, 244
WFST, 244
wh-non-subject-question,
210
wh-phrase, 210, 210
wh-pronoun, 145
wh-subject-questions, 210
wh-word, 210
WiC, 367
wideband speech, 531
wikiﬁcation, 436
WikiQA, 483
wildcard, regular
expression, 5
Winograd Schema, 437
Wizard-of-Oz system, 513
word
boundary, regular
expression notation,
6
closed class, 144
deﬁnition of, 11
fragment, 11
function, 144, 165
open class, 144
punctuation as, 11
tokens, 12
types, 12
word normalization, 21
word segmentation, 15, 17
word sense, 354, 355
word sense disambiguation,
362, see WSD
word sense induction, 368
word shape, 161, 329
word tokenization, 15
word-in-context, 366
word-word matrix, 102
word2vec, 111
wordform, 12
and lemma, 95
versus lemma, 12
WordNet, 359, 359
wordpiece, 19
world knowledge, 298
WSD, 362
AI-oriented efforts, 370
all-words task, 362
bootstrapping, 371
decision tree approach,
371
evaluation of, 363
history, 370
history of, 371
lexical sample task, 362
neural network
approaches, 370
robust approach, 370
supervised machine
learning, 371
unsupervised machine
learning, 368
WSI, 368
WSJ, 147
X-bar schemata, 229

Subject Index
613
yes-no questions, 209
yield, 249
Yonkers Racetrack, 50
zero anaphor, 420
zero-width, 11
zeros, 41
zeugma, 356

