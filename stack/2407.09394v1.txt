PersonaRAG: Enhancing Retrieval-Augmented Generation
Systems with User-Centric Agents
Saber Zerhoudi
saber.zerhoudi@uni-passau.de
University of Passau
Passau, Germany
Michael Granitzer
michael.granitzer@uni-passau.de
University of Passau
Passau, Germany
ABSTRACT
Large Language Models (LLMs) struggle with generating reliable
outputs due to outdated knowledge and hallucinations. Retrieval-
Augmented Generation (RAG) models address this by enhancing
LLMs with external knowledge, but often fail to personalize the
retrieval process. This paper introduces PersonaRAG, a novel frame-
work incorporating user-centric agents to adapt retrieval and gen-
eration based on real-time user data and interactions. Evaluated
across various question answering datasets, PersonaRAG demon-
strates superiority over baseline models, providing tailored answers
to user needs. The results suggest promising directions for user-
adapted information retrieval systems. Findings and resources are
available at https://github.com/padas-lab-de/PersonaRAG.
CCS CONCEPTS
‚Ä¢ Information systems ‚ÜíPersonalization; Presentation of
retrieval results; Language models.
KEYWORDS
User interactions, Retrieval-Augmented Generation (RAG), Person-
alized Information Retrieval, Multi-Agent RAG.
1
INTRODUCTION
Large Language Models (LLMs) such as GPT-4 [24] and LLaMA
3 [32] have significantly advanced the field of natural language
processing (NLP) by demonstrating impressive performance across
various tasks and exhibiting emergent abilities that push the bound-
aries of artificial intelligence [7]. However, these models face chal-
lenges such as generating unreliable outputs due to issues like
hallucination and outdated parametric memories [5].
Retrieval-Augmented Generation (RAG) models have shown
promise in addressing these issues by integrating externally re-
trieved information to support more effective performance on com-
plex, knowledge-intensive tasks [20]. Despite these advancements,
the deployment of RAG systems within broader AI frameworks
continues to face significant challenges, particularly in handling
noise and irrelevance in retrieved data [8].
A key limitation of existing RAG systems is their inability to
adapt outputs to users‚Äô specific informational and contextual needs.
Personalized techniques in information retrieval, such as adaptive
retrieval based on user interaction data and context-aware strate-
gies, are increasingly recognized as essential for enhancing user
interaction and satisfaction [30, 31]. These methods aim to refine
the retrieval process dynamically, tailoring it more closely to indi-
vidual user profiles and situational contexts [1].
Figure 1: Illustrations of Various RAG Models. Vanilla RAG
and Chain-of-Thought [39] use passive learning, while Per-
sonaRAG involves user-centric knowledge acquisition.
arXiv:2407.09394v1  [cs.IR]  12 Jul 2024

,
Zerhoudi et al.
The integration of agent-based systems with personalized RAG
architectures presents a compelling avenue for research. Such sys-
tems utilize a multi-agent framework to simulate complex, adaptive
interactions tailored to user-specific requirements [35]. By embed-
ding intelligent, user-oriented agents within the RAG framework,
these systems can evolve into more sophisticated tools that not
only retrieve relevant information but also align it closely with the
user‚Äôs specific preferences and contexts in real-time. Importantly,
the personalization strategy employed in these systems is fully
transparent to the user, ensuring that the user is aware of how their
information is being used to tailor the results.
In this study, we present PersonaRAG, an innovative method-
ology that extends traditional RAG frameworks by incorporating
user-centric agents into the retrieval process. This approach ad-
dresses the previously mentioned limitations by promoting active
engagement with retrieved content and utilizing dynamic, real-
time user data to continuously refine and personalize interactions.
PersonaRAG aims to enhance the precision and relevance of LLM
outputs, adapting dynamically to user-specific needs while main-
taining full transparency regarding the personalization process.
Our experiments, conducted using GPT-3.5, develop the Person-
aRAG model and evaluate its performance across various question
answering datasets. The results indicate that PersonaRAG achieves
an improvement of over 5% in accuracy compared to baseline mod-
els. Furthermore, PersonaRAG demonstrates an ability to adapt
responses based on user profiles and information needs, enhancing
the personalization of results. Additional analysis shows that the
principles underlying PersonaRAG can be generalized to different
LLM architectures, such as Llama 3 70b and Mixture of Experts
(MoE) 8x7b [15]. These architectures benefit from the integration
of external knowledge facilitated by PersonaRAG, with improve-
ments exceeding 10% in some cases. This evidence indicates that
PersonaRAG not only contributes to the progress of RAG systems
but also provides notable advantages for various LLM applications,
signifying a meaningful step forward in the development of more
intelligent and user-adapted information retrieval systems.
2
RELATED WORK
Retrieval-Augmented Generation (RAG) systems have emerged
as a significant advancement in natural language processing and
machine learning, enhancing language models by integrating ex-
ternal knowledge bases to improve performance across various
tasks, such as question answering, dialog understanding, and code
generation [20, 37]. These systems employ dense retrievers to pull
relevant information, which the language model then uses to gener-
ate responses. However, the development of RAG systems and their
integration within broader artificial intelligence frameworks is an
ongoing area of research, with several challenges and opportunities
for improvement.
Recent developments in RAG systems have focused on refining
these models to better handle the noise and irrelevant informa-
tion often retrieved during the process. Xu et al. [37] addressed
this issue by employing natural language inference models to se-
lect pertinent sentences, thereby enhancing the RAG‚Äôs robustness.
Additionally, advancements have been made in adaptively retriev-
ing information, with systems like those proposed by Jiang et al.
[16] dynamically fetching passages that are most likely to improve
generation accuracy.
Despite these improvements, RAG systems still face limitations,
particularly in adapting their output to the user‚Äôs specific profile,
such as their information needs or intellectual knowledge. This
limitation stems from the current design of most RAG systems,
which do not typically incorporate user context or personalized
information retrieval strategies [40]. Consequently, there exists a
gap between the general effectiveness of RAG systems and their
applicability in personalized user experiences, where context and
individual user preferences play a crucial role.
Personalization in information retrieval is increasingly recog-
nized as essential for enhancing user interaction and satisfaction [11].
Techniques such as user profiling, context-aware retrieval, and adap-
tive feedback mechanisms are commonly employed to tailor search
results to individual users‚Äô needs. For instance, Jeong et al. [14]
proposed adaptive retrieval strategies that dynamically adjust the
retrieval process based on the complexity of the query and the
user‚Äôs historical interaction data. These personalized approaches
not only improve user satisfaction but also increase the efficiency
of information retrieval by reducing the time users spend sifting
through irrelevant information.
The integration of personalized techniques with agent-based
systems provides a promising pathway to augment the capabilities
of RAG systems. Agent-based systems, particularly in the form of
LLM-Based Multi-Agent Frameworks [21], enable the simulation
of complex interactions that can lead to more nuanced and contex-
tually appropriate outputs. By incorporating multi-agent systems
into RAG frameworks, there is potential for developing more robust
and adaptive retrieval mechanisms that can handle a broader range
of queries and generate more accurate responses, closely tailored
to the specific needs and contexts of individual users.
In conclusion, while significant progress has been made in en-
hancing the effectiveness and personalization of RAG systems,
ongoing research is crucial to address their existing limitations
and expand their applications. The integration of personalized in-
formation retrieval and agent-based enhancements represents a
promising avenue for further enhancing the adaptability and accu-
racy of RAG systems, potentially leading to intelligent information
retrieval tailored to the specific needs of users.
3
METHODOLOGY
In this section, we present the methodology underlying our Per-
sonaRAG approach, which aims to enhance the ability of Language
Large Models (LLMs) to actively engage with, understand, and lever-
age user profile information for personalized content generation.
We begin by discussing the fundamental concepts of Retrieval-
Augmented Generation (RAG) models (Section 3.1) and then intro-
duce our PersonaRAG technique, which encourages LLMs to ac-
tively assimilate knowledge from live search sessions (Section 3.2).
3.1
Fundamentals of Retrieval-Augmented
Generation (RAG) Models
State-of-the-art RAG models, as described in previous studies [10,
13, 29], employ retrieval systems to identify a set of passages ùê∑=
{ùëë1, . . . ,ùëëùëõ} when given a query q. These passages are intended

PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents
,
Figure 2: Overview of Our PersonaRAG Model showcasing the dynamic interaction among specialized agents within the system,
facilitated by a global message pool for structured communication. The diagram illustrates the flow from user query input
through various agents, including User Profile, Context Retrieval, Session Analysis, Document Ranking, and Feedback Agents,
highlighting their contributions to real-time adaptation and personalized content generation by integrating live user data and
feedback for continuous improvement and contextually relevant search experiences.
to enhance the generative capabilities of LLMs by providing them
with contextually relevant information.
Early versions of RAG models typically employ a traditional
retrieval-generation framework, in which the retrieved data set
ùê∑= {ùëë1, . . . ,ùëëùëõ} is directly fed into LLMs to generate responses
to the query ùëû. However, these passages often contain irrelevant
information, and the direct utilization approach in RAG has been
shown to restrict the potential benefits of the RAG framework [9].
This limitation has sparked further discussion on how to improve
LLMs by integrating retrieval results and outputs generated by the
models themselves [36].
3.2
PersonaRAG: RAG with User-Centric Agents
Drawing from the principles of adaptive learning and user-centered
design, we develop a new PersonaRAG architecture to enable IR
systems to dynamically learn from and adapt to user behavior in
real-time. As shown in Figure 2, PersonaRAG introduces a three-
step pipeline: retrieval, user interaction analysis, and cognitive
dynamic adaptation. Unlike traditional IR models that statically
respond to queries, PersonaRAG focuses on leveraging live user
data to continually refine its understanding and responses without
the need for manual retraining.
3.2.1
User Interaction Analysis. To understand user behavior from
live interactions, PersonaRAG treats the IR system as a cognitive
structure capable of receiving, interpreting, and acting upon user
feedback [3]. Mimicking human learning behaviors, we establish
four distinct agents within the system dedicated to analyzing user
interactions from different perspectives: engagement tracking, pref-
erence analysis, context understanding, and feedback integration.
These agents‚Äô roles are detailed in Section 3.2.2.
3.2.2
Cognitive Dynamic Adaptation. Following adaptive learning
principles, we employ a dynamic adaptation mechanism to assist
the IR system in utilizing real-time user data for continuous im-
provement. This mechanism facilitates the integration of insights
gained from User Interaction Analysis into the system‚Äôs retrieval
processes. Specifically, we prompt the system to adjust its query
responses based on an initial understanding of the user‚Äôs needs and
refine these responses as more user data becomes available. This
approach not only personalizes the search results but also helps in
correcting any misalignments or errors in real-time.
PersonaRAG employs a highly specialized agent architecture,
with each agent focusing on a specific aspect of the information re-
trieval process. All agents utilize in-context learning, i.e., prompting,

,
Zerhoudi et al.
to perform their designated tasks. This role specialization allows
for the efficient decomposition of complex user queries into man-
ageable tasks [27]. To foster this, we engage the IR system as five
specialized agents to analyze user interactions based on retrieved
data. At present, the focus is on the functionality and interaction
of these agents rather than their individual performance metrics.
User Profile Agent. This component manages and updates user
profile data, incorporating historical user interactions and prefer-
ences [18, 28]. It monitors how users interact with search results,
such as click-through rates and navigation paths. The User Profile
Agent helps the system understand what captures user interest and
leads to deeper engagement, enabling personalized search experi-
ences.
Contextual Retrieval Agent. This agent is responsible for the
initial retrieval of documents based on the user‚Äôs current query.
It accesses both a traditional search index and a more dynamic
context-aware system that can consider broader aspects of the
query environment. It utilizes user profile data to modify and refine
search queries or to prioritize search results. For instance, if a
user consistently engages more with certain types of documents
or topics, the retrieval agent can boost those document types in
the search results, ensuring that the most relevant information is
presented to the user.
Live Session Agent. This agent analyzes the current session
in real-time, observing user actions such as clicks, time spent on
documents, modifications to the query, and any feedback provided.
It creates a session-specific context model that captures the user‚Äôs
immediate needs and interests. The real-time data collected by
this agent is used to adjust the ongoing session, potentially re-
ranking search results or suggesting new queries based on the
user‚Äôs behavior and preferences. Additionally, the Live Session
Agent updates the user profile with new insights gleaned from
the session, allowing for a more personalized and efficient search
experience in future interactions.
Document Ranking Agent. This agent is responsible for re-
ranking the documents retrieved by the Contextual Retrieval Agent.
It integrates insights from both the User Profile Agent and the Live
Session Agent to score and order the documents more effectively.
By considering the user‚Äôs historical preferences and their current
session behavior, the Document Ranking Agent ensures that the
most relevant and valuable documents are presented to the user in
a prioritized manner. This agent continuously adapts its ranking
algorithms based on the feedback received from the user and the
insights provided by the other agents in the system.
Feedback Agent. This agent gathers implicit and explicit feed-
back during and after user interactions. Implicit feedback includes
behavioral data like time spent on documents, click counts, and
navigation patterns. Explicit feedback involves direct user input on
document relevance and quality, collected through ratings, surveys,
or comments. The agent uses this information to train and refine
models for other agents, particularly the Document Ranking Agent.
This process enhances the system‚Äôs ability to anticipate user needs
and deliver relevant documents based on accumulated feedback
and insights.
By dynamically integrating insights from the User Profile Agent,
Contextual Retrieval Agent, Live Session Agent, Document Ranking
Agent, and Feedback Agent into the IR processes, PersonaRAG not
only adapts to immediate user needs but also evolves over time
to better anticipate and meet user expectations. This multi-agent
approach enables PersonaRAG to embody a truly adaptive and user-
focused information retrieval system, leveraging specialized agents
to analyze user interactions from different behavioral perspectives
and deliver highly personalized and contextually relevant search
experiences. The inclusion of the Document Ranking Agent ensures
that the most pertinent documents are identified and presented to
users, further enhancing the system‚Äôs ability to effectively satisfy
user information needs.
3.3
PersonaRAG Operational Workflow
The PersonaRAG framework employs a structured workflow that al-
lows for sequential and parallel processing of tasks, ensuring clarity
and consistency in communication between agents through well-
defined data structures and protocols [12]. The process involves
the User Profile Agent, Contextual Retrieval Agent, Live Session
Agent, Document Ranking Agent, and Feedback Agent working
together to refine search queries, prioritize relevant results, and
improve document scoring and re-ranking based on user profile,
session-specific contexts, and feedback.
PersonaRAG‚Äôs modular design allows for flexibility in the system
setup, enabling researchers to focus on the most relevant aspects
of the user‚Äôs profile, session, and feedback data. Agents work col-
laboratively by utilizing content from the Global Message Pool,
which serves as a central hub for inter-agent communication [12],
eliminating inefficiencies and enabling agents to access or update
information as required.
The Feedback Agent collects and analyzes implicit and explicit
user feedback to generate insights into the effectiveness of retrieval
strategies and document relevance. This feedback is used to make
dynamic adjustments to the system, refining retrieval methods
and altering the weighting of user profile factors. Through this
iterative process, PersonaRAG continuously adapts and improves
its performance, enhancing the accuracy and user satisfaction of
the retrieval results [22].
4
EXPERIMENTAL SETUPS
In this section, we present the experimental setup employed in our
study, including the datasets, baseline models, evaluation metrics,
and implementation details. We also provide an overview of the
prompts used in our experiments.
4.1
Datasets
Our experiments are conducted on three widely used single-hop
benchmark datasets in the field of Information Retrieval (IR): Natu-
ralQuestions (NQ) [19], TriviaQA [17], and WebQuestions (WebQ) [6].
NQ is a well-known dataset in Natural Language Understand-
ing (NLU), consisting of structured questions and corresponding
Wikipedia pages annotated with long and short answers. TriviaQA
comprises question-answer pairs collected from trivia and quiz-
league websites, while WebQ consists of questions selected using
the Google Suggest API, with answers being entities in Freebase.

PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents
,
Table 1 summarizes the datasets used in our initial study. Due
to the high cost of using language models and the large number
of API calls required, we randomly sampled 500 questions from
each raw dataset to create more manageable subsets for our ex-
periments. While this sampling approach limits the scope of our
study, it allows us to conduct an initial investigation into the perfor-
mance of different RAG systems on these datasets. We acknowledge
that future work with larger sample sizes and more comprehen-
sive experiments will be necessary to draw definitive conclusions.
Nonetheless, we believe this preliminary study provides valuable
insights into the relative strengths and weaknesses of the tested
RAG approaches.
Dataset
#Query
#Corpus
Sampling Rate
NQ
8,757
79,168
5.7%
TriviaQA
8,837
78,785
5.7%
WebQ
2,032
3,417
24.6%
Table 1: Summary of datasets. Each dataset consists of ran-
domly sampled 500 questions from the raw dataset.
4.2
Models
We compare PersonaRAG with several baseline models, including
prompt learning and RAG models. The prompt templates used in
user interaction analysis and dynamic adaptation are presented in
Section 4.4. Initially, the question-answering (QA) instruction is
fed to ChatGPT to conduct the vanilla answer generation model.
Following the work of Wei et al. [34], the Chain-of-Thought model
is implemented, which generates question rationale results to pro-
duce the final results. Additionally, the Guideline model serves as a
baseline, generating problem-solving steps and guiding Language
Models (LLMs) to generate the answer.
For the RAG-based baselines, two models are implemented: vanilla
RAG and Chain-of-Thought, which include utilizing raw retrieved
passages (CoT with Passage) and refining the passages as notes
(CoT with Note). The vanilla RAG model directly feeds the top-
ranked passages to the LLM. The Chain-of-Note model [39] is also
implemented, which refines and summarizes the retrieved passages
for generation. Inspired by Self-RAG Asai et al. [2], the Self-Rerank
model is conducted, which filters out unrelated contents without
fine-tuning LLMs.
4.3
Evaluation Metrics
When evaluating adaptive models, it is crucial to consider both task
performance and user-centric adaptability simultaneously, along
with their trade-offs. Therefore, the results are reported using dif-
ferent metrics, some of which measure effectiveness and others
measure efficiency.
For effectiveness, accuracy is used, following the standard evalu-
ation protocol in the field of Information Retrieval (IR) [2, 4, 23]. Ac-
curacy assesses whether the predicted answer contains the ground-
truth answer. Both the outputs of the Language Learning Model
(LLM) and golden answers are converted to lowercase, and string
matching (StringEM) is performed between each golden answer
and the model prediction to calculate accuracy.
To evaluate user-centric adaptability, the BLEU-2 score is mea-
sured to assess the text similarity between different RAG and base-
line setups and how well the generated answers resemble each other.
This metric provides insights into the system‚Äôs ability to generate
consistent and coherent responses across various configurations.
Additionally, the average sentence length and the average number
of syllables of the answers from different RAG setups are reported
as a post-hoc analysis. These measures validate whether the RAG
system effectively adjusts its responses based on user knowledge
levels, ensuring that the generated answers are tailored to the user‚Äôs
understanding and expertise.
Combining these evaluation strategies provides a comprehensive
view of both the effectiveness and user-centric adaptability of the
RAG system. The accuracy metric ensures that the system generates
correct answers, while the BLEU-2 score and post-hoc analysis of
sentence length and syllable count confirm the system‚Äôs ability
to adapt to user knowledge levels. As the understanding of user
needs and system capabilities evolves, it is essential to continuously
refine these metrics to maintain the RAG system‚Äôs effectiveness in
delivering personalized, context-aware responses that cater to the
diverse requirements of users in the field of IR.
4.4
Implementation Details
For a fair comparison and following the work of Mallen et al. [23]
and Trivedi et al. [33], the same retriever, a term-based sparse
retrieval model known as BM25 [26], is used across all different
models. The retrieval model is implemented using the OpenMatch
toolkit [38]. For the external document corpus, the KILT-Wikipedia
corpus preprocessed by Petroni et al. [25] is used, and the top-k
relevant documents are retrieved.
Regarding the LLMs used to generate answers, the Llama 3 model
instruct (ref) with 70b parameters, Mixture of Experts (MoE) 8x7b
(ref), and the GPT-3.5 model (gpt-3.5-turbo-0125) are employed.
For the retrieval-augmented LLM design, the implementation de-
tails from Trivedi et al. [33] are followed, which include input
prompts, instructions, and the number of test samples for evalua-
tion (e.g., 500 samples per dataset).
4.5
Prompts Used in PersonaRAG
This subsection presents the prompt templates employed in the
construction of the PersonaRAG model. The prompts utilized in the
User Interaction Analysis and Cognitive Dynamic Adaptation com-
ponents are detailed below. The prompt templates used by the base-
line models are available in the project repository 1. In the templates,
{question} represents the input question, {global_memory} the
Global Message Pool, while {passages} denotes the retrieved pas-
sages. Additionally, {cot_answer} is populated with the output
generated by the Chain-of-Thought model.
The placeholder {user_profile_answer} is filled with the re-
sponse produced by the User Profile agent model. Respectively,
{contextual_answer} corresponds to the Contextual Retrieval
agent model, {live_session_answer} to the Live Session agent
model, {document_ranking_answer} to the Document Ranking
agent model, and {feedback_answer} to the Feedback agent model.
1https://anonymous.4open.science/r/PersonaRAG-F423

,
Zerhoudi et al.
4.5.1
Prompts Used in User Interaction Analysis.
User Profile Agent.
Your task is to help the User Profile Agent improve
its understanding of user preferences based on ranked
document lists and the shared global memory pool.
Question: {question}
Passages: {passages}
Global Memory: {global_memory}
Task Description:
From the provided passages and global memory pool, analyze
clues about the user‚Äôs search preferences. Look for themes,
types of documents, and navigation behaviors that reveal
user interest. Use these insights to recommend how the
User Profile Agent can refine and expand the user profile
to deliver better-personalized results.
Contextual Retrieval Agent.
You are a search technology expert guiding the Contextual
Retrieval
Agent
to
deliver
context-aware
document
retrieval.
Question: {question}
Passages: {passages}
Global Memory: {global_memory}
Task Description:
Using the global memory pool and the retrieved passages,
identify
strategies
to
refine
document
retrieval.
Highlight
how
user
preferences,
immediate
needs,
and
global
insights
can
be
leveraged
to
adjust
search
queries and prioritize results that align with the user‚Äôs
interests. Ensure the Contextual Retrieval Agent uses this
shared information to deliver more relevant and valuable
results.
Live Session Agent.
Your expertise in session analysis is required to assist
the Live Session Agent in dynamically adjusting results.
Question: {question}
Passages: {passages}
Global Memory: {global_memory}
Task Description:
Examine the retrieved passages and information in the
global memory pool. Determine how the Live Session Agent
can use this data to refine its understanding of the user‚Äôs
immediate needs. Suggest ways to dynamically adjust search
results or recommend new queries in real-time, ensuring
that session adjustments align with user preferences and
goals.
Document Ranking Agent.
Your task is to help the Document Ranking Agent prioritize
documents for better ranking.
Question: {question}
Passages: {passages}
Global Memory: {global_memory}
Task Description:
Analyze the retrieved passages and global memory pool to
identify ways to rank documents effectively. Focus on
combining historical user preferences, immediate needs,
and session behavior to refine ranking algorithms. Your
insights should ensure that documents presented by the
Document
Ranking
Agent
are prioritized
to
match
user
interests and search context.
Feedback Agent.
You are an expert in feedback collection and analysis,
guiding the Feedback Agent to gather and utilize user
insights.
Question: {question}
Passages: {passages}
Global Memory: {global_memory}
Task Description:
Using
the
retrieved
passages
and
global
memory
pool,
identify methods for collecting implicit and explicit user
feedback. Suggest ways to refine feedback mechanisms to
align with user preferences, such as ratings, surveys, or
behavioral data. Your recommendations should guide the
Feedback Agent in updating other agents‚Äô models for more
personalized and relevant results.
Global Message Pool.
You are responsible for maintaining and enriching the
Global
Message
Pool,
serving
as
a
central
hub
for
inter-agent communication.
Question: {question}
Agent Responses: {agent_responses}
Existing Global Memory: {global_memory}
Task Description:
Using
the
responses
from
individual
agents
and
the
existing global memory, consolidate key insights into
a
shared
repository.
Your
goal
is
to
organize
a
comprehensive message pool that includes agent-specific
findings, historical user preferences, session-specific
behaviors,
search
queries,
and
user
feedback.
This
structure
should
provide
all
agents
with
meaningful
data
points
and
strategic
recommendations,
reducing
redundant communication and improving the system‚Äôs overall
efficiency.
4.5.2
Prompts Used in Cognitive Dynamic Adaptation.
Chain-of-Thought.
To solve the problem, Please think and reason step by
step, then answer.

PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents
,
Question: {question}
Passages: {passages}
Reasoning process:
1. Read the given question and passages to gather relevant
information.
2. Write reading notes summarizing the key points from
these passages.
3.
Discuss
the
relevance
of
the
given
question
and
passages.
4. If some passages are relevant to the given question,
provide a brief answer based on the passages.
5. If no passage is relevant, directly provide the answer
without considering the passages.
Answer:
Cognitive Agent.
Your task is to help the Cognitive Agent enhance its
understanding of user insights to continuously improve
the system‚Äôs responses.
Question: {question}
Initial Response: {cot_answer}
User Insights from Interaction Analysis:
User Profile Agent: {user_profile_answer},
Contextual Retrieval Agent: {contextual_answer},
Live Session Agent: {live_session_answer},
Document Ranking Agent: {document_ranking_answer},
Feedback Agent: {feedback_answer}
Task Description:
Verify
the
reasoning
process
in
the
initial
response
for
errors
or
misalignments.
Use
insights
from
user
interaction analysis to refine this response, correcting
any inaccuracies and enhancing the query answers based
on
user
profile.
Ensure
that
your
refined
response
aligns more closely with the user‚Äôs immediate needs and
incorporates
foundational
or
advanced
knowledge
from
other sources.
Answer:
5
EXPERIMENTAL RESULTS AND ANALYSES
In this section, we show the overall experimental results and offer
in-depth analyses of our method.
5.1
Main Results
Table 2 summarizes the primary findings for PersonaRAG across
various single-hop question answering datasets. The approach
was evaluated against multiple baseline models, including large
language models (LLMs) without retrieval-augmented generation
(RAG), the conventional RAG model, and self-refined variants, such
as utilizing raw retrieved passages (CoT with Passage) or refining
passages into notes (CoT with Note).
PersonaRAG demonstrated superior performance compared to
most of the baseline models, achieving significant improvements
over the conventional RAG (i.e., vanillaRAG) of over 10%, particu-
larly on the WebQ dataset. It also consistently outperformed the
ChatGPT-3.5 model, except on TriviaQA, which we suspect is part
of the model‚Äôs training dataset. These results suggest PersonaRAG‚Äôs
capability to guide LLMs in extracting relevant information through
active learning techniques.
Specifically, the performance of RAG models was assessed using
the top 3 and 5 ranked passages. While other RAG models generally
benefited from more passages, PersonaRAG maintained consistent
performance with either 3 or 5 passages, suggesting that 3 pas-
sages were adequate for generating accurate answers. PersonaRAG
agents played a crucial role in efficiently extracting the necessary
information regarding the user‚Äôs information need to achieve these
improvements.
Furthermore, on the WebQ dataset, PersonaRAG achieved accu-
racy scores of 63.46% and 67.50% using Top-3 and Top-5 passages,
respectively, surpassing the vanillaRAG model by 25% and 17.36%,
and nearly all other baseline models (except for Chain-of-Thought
using Top-5, which performed equally). On the NQ dataset, Per-
sonaRAG maintained similarly robust performance with scores of
49.02% and 48.78%, outperforming all baselines (except for Chain-
of-Thought and Self-Rerank (SR) using Top-5). This pattern was
further validated by experiments on other datasets, with results
showing that PersonaRAG consistently outperforms conventional
RAG models with the capability of providing an answer tailored to
the user‚Äôs interaction and information need. The comprehensive
understanding it provides contributes to the generation of accurate
and user-centric answers across various question complexities.
5.2
Comparative Analysis of RAG
Configurations
Further experiments explored PersonaRAG‚Äôs adaptive capabilities
(Figure 3). BLEU-2 scores compared outputs from Chain-of-Note
(consistently best outside PersonaRAG) with other methods. Per-
sonaRAG showed higher similarity scores, indicating its ability to
generate responses that address user needs rather than just sum-
marizing input. Additionally, PersonaRAG provides personalized
answers tailored to user profiles, extending beyond mere informa-
tion provision.
The Chain-of-Note approach demonstrated comparable perfor-
mance to the Chain-of-Thought approach, implying that both tech-
niques effectively extract pertinent information from the retrieved
passages and adapt it to align with the user‚Äôs information need.
In contrast, vanillaGPT and vanillaRAG outputs differed signifi-
cantly from the Chain-of-Note approach, indicating that counterfac-
tual cognition often leads to diverse outcomes rather than focusing
solely on query-relevant content. This suggests LLMs can construct
knowledge from multiple perspectives and customize responses
based on user understanding.
Post-hoc analyses of average sentence length and syllable count
across RAG configurations provided insights into the system‚Äôs abil-
ity to adapt responses to user comprehension levels. These obser-
vations highlight PersonaRAG‚Äôs capacity to synthesize knowledge
from various perspectives and tailor responses to different levels of
user expertise.
5.3
Analysis on Generalization Ability
This experiment evaluates the quality of knowledge construction
using different large language models (LLMs). As illustrated in

,
Zerhoudi et al.
Method
Setting
Top-3
Top-5
WebQ
TriviaQA
NQ
WebQ
TriviaQA
NQ
w/o RAG
gpt-3.5-turbo-0125
59.61
97.36
43.90
62.43
97.36
41.46
Guideline
36.53
42.10
17.07
47.21
36.84
21.95
vanillaRAG
38.46
78.94
36.58
50.14
81.57
41.46
Self-Refined
Chain-of-Thought (CoT)
57.69
89.47
39.02
67.51
89.47
41.46
Chain-of-Note (CoN)
57.17
81.57
48.78
65.15
92.10
48.78
Self-Rerank (SR)
32.63
81.57
43.90
40.26
84.21
51.21
PersonaRAG
63.46
94.73
49.02
67.50
89.47
48.78
Table 2: Overall Accuracy Performance Comparison Using Top-3 and Top-5 Passages. PersonaRAG results are reported in bold.
(a) Text Similarity for Top-3 Passages
(b) Text Similarity for Top-5 Passages
Figure 3: Text Similarity between Chain-of-Note (CoN) and
Other Methods Using BLEU-2 Score for Evaluation, with
Normalized Average Sentence Length and Average Syllable
Count.
Table 3, the PersonaRAG outcomes are used to prompt open-source
LLMs, specifically LLaMA3-70B and MoE-8x7b, to generate accurate
answers.
Compared to LLMs without retrieval-augmented generation
(w/o RAG), vanilla RAG and Chain-of-Note often exhibit lower
performance. This result suggests that retrieved passages can act as
noise, adversely affecting model performance even after refinement
through note generation. One primary reason for this behavior is
Method
WebQ
TriviaQA
NQ
LLaMA3-70B
w/o RAG
45.25
82.17
38.95
vanillaRAG
55.14
85.02
40.37
Chain-of-Thought
60.52
88.72
45.10
Chain-of-Note
62.67
89.37
48.25
Self-Rerank
54.25
84.50
47.77
PersonaRAG
66.09
92.12
50.85
MoE-8x7b
w/o RAG
38.24
75.82
34.26
vanillaRAG
48.44
80.25
38.50
Chain-of-Thought
54.12
85.46
42.37
Chain-of-Note
55.98
87.55
45.14
Self-Rerank
52.50
83.04
44.96
PersonaRAG
61.35
90.30
49.27
Table 3: Overall Accuracy Performance Comparison for Top-
5 Passages using LLaMA3-70B and MoE-8x7b.
that both LLaMA3-70B and MoE-8x7b struggle to efficiently an-
alyze and identify relevant knowledge due to limitations in their
processing capacities.
In contrast, the PersonaRAG method provides notable perfor-
mance improvements: over 8% for LLaMA3-70B and more than 10%
for MoE-8x7b across all datasets, underscoring its effectiveness.
The PersonaRAG methodology distinguishes itself from the Chain-
of-Note approach by offering a cognitive framework that connects
retrieved passages with prior knowledge. This framework models
the instructor‚Äôs (GPT-3.5) reasoning process, guiding the student
models (LLaMA3-70B and MoE-8x7b) to better understand knowl-
edge retrieved from passages. The results demonstrate that the
LLMs are capable of selecting appropriate passages to build more
accurate responses, highlighting the benefits of the PersonaRAG
approach for improving generalization.
5.4
Case Study
Finally, we randomly sample one case in Table to demonstrate the
effectiveness of PersonaRAG.
The user interaction analysis mechanism effectively generates
comprehensive results by integrating foundational and advanced
insights from user data. Retrieved passages provide critical clues for
answering questions, while agent analyses summarize and illustrate

PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents
,
Question: Who stole the Mona Lisa from the Louvre in 1911?
Answer: "Vincenzo Peruggia"
Chain-of-Thought:
1. Read the given question and passages to gather relevant
information.
2. Write reading notes summarizing the key points.
3. Discuss the relevance of the given question and passages.
4. If some passages are relevant to the given question, provide
a brief answer based on the passages.
5. If no passage is relevant, directly provide the answer without
considering the passages.
Step 1: Passage Retrieval
1. The Mona Lisa was stolen from the Louvre Museum while
the museum was closed for cleaning. Witnesses reported that a
tall, stout individual was carrying what appeared to be a large
panel covered with a horse blanket. Two years later, Vincenzo
Peruggia was arrested in Florence.
2. On August 22, 1911, the Mona Lisa was stolen from the
Louvre. Vincenzo Peruggia claimed he stole the painting to
return it to Italy.
3. In 1911, Vincenzo Peruggia, a Louvre employee, stole the
Mona Lisa.
Step 2: User Interaction Analysis
User Insights from Interaction Analysis:
- User Profile Agent: The user is interested in art heists and
historical mysteries.
- Contextual Retrieval Agent: The user frequently searches for
art history topics.
- Live Session Agent: The user reads about unsolved mysteries
and historic crimes.
- Document Ranking Agent: The user favors detailed,
chronological accounts.
- Feedback Agent: The user interacts with articles about
famous art heists.
Step 3: Cognitive Dynamic Adaptation
The user has a consistent interest in famous art heists and
historical mysteries. They are drawn to unsolved mysteries
and historic crimes. They favor detailed, chronological
accounts, so the final response should highlight theft motives,
the method, and the investigation leading to the arrest,
providing comprehensive and authoritative sources to satisfy
their engagement with articles on art heists.
Answer: Vincenzo Peruggia, a Louvre employee, stole the
Mona Lisa from the Louvre Museum on August 21, 1911. He
claimed he stole the painting to return it to Italy, and the theft
remained unsolved for two years before he was arrested in
Florence in December 1913. The painting was later recovered
and returned to the Louvre.
Table 4: PersonaRAG Case Study.
the applicability of external information to user queries. The cog-
nitive dynamic adaptation module refines initial chain-of-thought
responses using these insights, generating accurate answers. For
example, including knowledge about the "theft of the Mona Lisa in
1911," "Vincenzo Peruggia," and "Florence" enhances the reasoning
process‚Äôs precision and detail. This demonstrates PersonaRAG‚Äôs
effectiveness in helping IR agents combine external knowledge with
intrinsic user data to produce well-informed responses.
6
CONCLUSION
This paper proposes PersonaRAG, which constructs the retrieval-
augmentation architecture incorporating user interaction analysis
and cognitive dynamic adaptation. PersonaRAG builds the user
interaction agents and dynamic cognitive mechanisms to facilitate
the understanding of user needs and interests and enhance the
system capabilities to deliver personalized, context-aware responses
with the intrinsic cognition of LLMs.
Furthermore, PersonaRAG demonstrates effectiveness in lever-
aging external knowledge and adapting responses based on user
profiles, knowledge levels, and information needs to support LLMs
in generation tasks without fine-tuning. However, this approach
requires multiple calls to the LLM‚Äôs API, which can introduce addi-
tional time latency and increase API calling costs when addressing
questions. The process involves constructing the initial Chain-of-
Thought, processing the User Interaction Agents results, and execut-
ing the Cognitive Dynamic Adaptation to generate the final answer.
Furthermore, the inputs to LLMs in this approach tend to be lengthy
due to the inclusion of extensive retrieved passages and the incor-
poration of user needs, interests, and profile construction results.
These factors can impact the efficiency and cost-effectiveness of
the PersonaRAG approach in practical applications of Information
Retrieval (IR) systems.
Future research will aim to optimize the process by reducing
API calls and developing concise representations of user profiles
and retrieved information without compromising response quality.
We also plan to explore more user-centric agents to better capture
writing styles and characteristics of RAG users/searchers. This will
enhance the system‚Äôs ability to understand and adapt to individual
preferences, improving personalization and relevance in IR tasks.
ACKNOWLEDGMENTS
This work has received funding from the European Union‚Äôs Hori-
zon Europe research and innovation program under grant agree-
ment No 101070014 (OpenWebSearch.EU, https://doi.org/10.3030/
101070014).
REFERENCES
[1] Gediminas Adomavicius, Bamshad Mobasher, Francesco Ricci, and Alexander
Tuzhilin. 2011. Context-Aware Recommender Systems. AI Mag. 32, 3 (2011),
67‚Äì80. https://doi.org/10.1609/AIMAG.V32I3.2364
[2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.
CoRR abs/2310.11511 (2023). https://doi.org/10.48550/ARXIV.2310.11511
[3] Richard C. Atkinson and Richard M. Shiffrin. 1968. Human Memory: A Proposed
System and its Control Processes. In Psychology of Learning and Motivation,
Kenneth W. Spence and Janet Taylor Spence (Eds.). Psychology of Learning and
Motivation, Vol. 2. Elsevier, 89‚Äì195. https://doi.org/10.1016/S0079-7421(08)60422-
3
[4] Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, and Sung Ju Hwang.
2023. Knowledge-Augmented Language Model Verification. In Proceedings of the
2023 Conference on Empirical Methods in Natural Language Processing, EMNLP
2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali
(Eds.). Association for Computational Linguistics, 1720‚Äì1736. https://doi.org/10.
18653/V1/2023.EMNLP-MAIN.107
[5] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan
Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan

,
Zerhoudi et al.
Xu, and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation
of ChatGPT on Reasoning, Hallucination, and Interactivity. In Proceedings of
the 13th International Joint Conference on Natural Language Processing and the
3rd Conference of the Asia-Pacific Chapter of the Association for Computational
Linguistics, IJCNLP 2023 -Volume 1: Long Papers, Nusa Dua, Bali, November 1
- 4, 2023, Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu
Purwarianti, and Adila Alfa Krisnadhi (Eds.). Association for Computational
Linguistics, 675‚Äì718. https://doi.org/10.18653/V1/2023.IJCNLP-MAIN.45
[6] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic
Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, EMNLP 2013,
18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of
SIGDAT, a Special Interest Group of the ACL. ACL, 1533‚Äì1544. https://aclanthology.
org/D13-1160/
[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
In Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html
[8] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking Large
Language Models in Retrieval-Augmented Generation. In Thirty-Eighth AAAI
Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Inno-
vative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on
Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024,
Vancouver, Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan
(Eds.). AAAI Press, 17754‚Äì17762. https://doi.org/10.1609/AAAI.V38I16.29728
[9] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking Large
Language Models in Retrieval-Augmented Generation. In Thirty-Eighth AAAI
Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Inno-
vative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on
Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024,
Vancouver, Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan
(Eds.). AAAI Press, 17754‚Äì17762. https://doi.org/10.1609/AAAI.V38I16.29728
[10] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.
Retrieval-Augmented Generation for Large Language Models: A Survey. CoRR
abs/2312.10997 (2023). https://doi.org/10.48550/ARXIV.2312.10997
[11] M. Rami Ghorab, Dong Zhou, Alexander O‚ÄôConnor, and Vincent Wade. 2013.
Personalised Information Retrieval: survey and classification. User Model. User
Adapt. Interact. 23, 4 (2013), 381‚Äì443. https://doi.org/10.1007/S11257-012-9124-1
[12] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao
Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran,
Lingfeng Xiao, and Chenglin Wu. 2023. MetaGPT: Meta Programming for Multi-
Agent Collaborative Framework. CoRR abs/2308.00352 (2023). https://doi.org/10.
48550/ARXIV.2308.00352
[13] Yizheng Huang and Jimmy Huang. 2024. A Survey on Retrieval-Augmented Text
Generation for Large Language Models. arXiv preprint arXiv:2404.10981 (2024).
[14] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park.
2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
Models through Question Complexity. CoRR abs/2403.14403 (2024).
https:
//doi.org/10.48550/ARXIV.2403.14403
[15] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou
Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th√©ophile Gervet,
Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. 2024.
Mixtral of Experts. CoRR abs/2401.04088 (2024). https://doi.org/10.48550/ARXIV.
2401.04088
[16] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-
Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval
Augmented Generation. In Proceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023,
Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational
Linguistics, 7969‚Äì7992. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.495
[17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA:
A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehen-
sion. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Pa-
pers, Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational
Linguistics, 1601‚Äì1611. https://doi.org/10.18653/V1/P17-1147
[18] Ameni Kacem. 2017. Personalized Information Retrieval based on Time-Sensitive
User Profile. (Recherche d‚ÄôInformation Personalis√©e bas√©e sur un Profil Utilisateur
Sensible au Temps). Ph. D. Dissertation. Paul Sabatier University, Toulouse, France.
https://tel.archives-ouvertes.fr/tel-01707423
[19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins,
Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural
Questions: a Benchmark for Question Answering Research. Trans. Assoc. Comput.
Linguistics 7 (2019), 452‚Äì466. https://doi.org/10.1162/TACL_A_00276
[20] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim
Rockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural In-
formation Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo
Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/
6b493230205f780e1bc26945df7481e5-Abstract.html
[21] Yuan Li, Yixuan Zhang, and Lichao Sun. 2023.
MetaAgents: Simulating In-
teractions of Human Behaviors for LLM-based Task-oriented Coordination
via Collaborative Generative Agents.
CoRR abs/2310.06500 (2023).
https:
//doi.org/10.48550/ARXIV.2310.06500
[22] Dilip Kumar Limbu, Andy M. Connor, Russel Pears, and Stephen G. MacDonell.
2006. Contextual relevance feedback in web information retrieval. In Proceedings
of the 1st International Conference on Information Interaction in Context, IIiX 2006,
Copenhagen, Denmark, October 18-20, 2006, Ian Ruthven (Ed.). ACM, 138‚Äì143.
https://doi.org/10.1145/1164820.1164848
[23] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Han-
naneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating
Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of
the 61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jor-
dan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational
Linguistics, 9802‚Äì9822. https://doi.org/10.18653/V1/2023.ACL-LONG.546
[24] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023).
https:
//doi.org/10.48550/ARXIV.2303.08774 arXiv:2303.08774
[25] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yaz-
dani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean
Maillard, Vassilis Plachouras, Tim Rockt√§schel, and Sebastian Riedel. 2021. KILT:
a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the
2021 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online,
June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-T√ºr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
and Yichao Zhou (Eds.). Association for Computational Linguistics, 2523‚Äì2544.
https://doi.org/10.18653/V1/2021.NAACL-MAIN.200
[26] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu,
and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text
REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4,
1994 (NIST Special Publication, Vol. 500-225), Donna K. Harman (Ed.). National
Institute of Standards and Technology (NIST), 109‚Äì126. http://trec.nist.gov/pubs/
trec3/papers/city.ps.gz
[27] Anil Sharma and Suresh Kumar. 2020. Semantic web-based information retrieval
models: a systematic survey. In Data Science and Analytics: 5th International
Conference on Recent Developments in Science, Engineering and Technology, REDSET
2019, Gurugram, India, November 15‚Äì16, 2019, Revised Selected Papers, Part II 5.
Springer, 204‚Äì222.
[28] Aarti Singh and Anu Sharma. 2019. A multi-agent framework for context-aware
dynamic user profiling for web personalization. In Software Engineering: Proceed-
ings of CSI 2015. Springer, 1‚Äì16.
[29] Shamane Siriwardhana, Rivindu Weerasekera, Tharindu Kaluarachchi, Elliott
Wen, Rajib Rana, and Suranga Nanayakkara. 2023.
Improving the Domain
Adaptation of Retrieval Augmented Generation (RAG) Models for Open Do-
main Question Answering. Trans. Assoc. Comput. Linguistics 11 (2023), 1‚Äì17.
https://transacl.org/ojs/index.php/tacl/article/view/4029
[30] Kazunari Sugiyama, Kenji Hatano, and Masatoshi Yoshikawa. 2004. Adaptive
web search based on user profile constructed without any effort from users. In
Proceedings of the 13th international conference on World Wide Web, WWW 2004,
New York, NY, USA, May 17-20, 2004, Stuart I. Feldman, Mike Uretsky, Marc Najork,
and Craig E. Wills (Eds.). ACM, 675‚Äì684. https://doi.org/10.1145/988672.988764
[31] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2017. Personalizing Search via
Automated Analysis of Interests and Activities. SIGIR Forum 51, 3 (2017), 10‚Äì17.
https://doi.org/10.1145/3190580.3190582
[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro,

PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents
,
Faisal Azhar, Aur√©lien Rodriguez, Armand Joulin, Edouard Grave, and Guil-
laume Lample. 2023. LLaMA: Open and Efficient Foundation Language Mod-
els. CoRR abs/2302.13971 (2023).
https://doi.org/10.48550/ARXIV.2302.13971
arXiv:2302.13971
[33] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-
Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023,
Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki
Okazaki (Eds.). Association for Computational Linguistics, 10014‚Äì10037. https:
//doi.org/10.18653/V1/2023.ACL-LONG.557
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei
Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompt-
ing Elicits Reasoning in Large Language Models. In Advances in Neural Infor-
mation Processing Systems 35: Annual Conference on Neural Information Pro-
cessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - De-
cember 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,
K. Cho, and A. Oh (Eds.).
http://papers.nips.cc/paper_files/paper/2022/hash/
9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
[35] Michael J. Wooldridge. 2009. An Introduction to MultiAgent Systems, Second
Edition. Wiley.
[36] Kevin Wu, Eric Wu, and James Zou. 2024. How faithful are RAG models? Quan-
tifying the tug-of-war between RAG and LLMs‚Äô internal prior. arXiv preprint
arXiv:2404.10198 (2024).
[37] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.
RECOMP: Improving
Retrieval-Augmented LMs with Compression and Selective Augmentation. CoRR
abs/2310.04408 (2023). https://doi.org/10.48550/ARXIV.2310.04408
[38] Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2023. OpenMatch-
v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit. In
Proceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023,
Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane
Mothe, and Barbara Poblete (Eds.). ACM, 3160‚Äì3164. https://doi.org/10.1145/
3539618.3591813
[39] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and
Dong Yu. 2023. Chain-of-Note: Enhancing Robustness in Retrieval-Augmented
Language Models. CoRR abs/2311.09210 (2023). https://doi.org/10.48550/ARXIV.
2311.09210
[40] Hamed Zamani and W. Bruce Croft. 2016. Embedding-based Query Language
Models. In Proceedings of the 2016 ACM on International Conference on the Theory
of Information Retrieval, ICTIR 2016, Newark, DE, USA, September 12- 6, 2016, Ben
Carterette, Hui Fang, Mounia Lalmas, and Jian-Yun Nie (Eds.). ACM, 147‚Äì156.
https://doi.org/10.1145/2970398.2970405

