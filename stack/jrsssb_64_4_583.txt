J. R. Statist. Soc. B (2002)
64, Part 4, pp. 583–639
Bayesian measures of model complexity and ﬁt
David J. Spiegelhalter,
Medical Research Council Biostatistics Unit, Cambridge, UK
Nicola G. Best,
Imperial College School of Medicine, London, UK
Bradley P. Carlin
University of Minnesota, Minneapolis, USA
and Angelika van der Linde
University of Bremen, Germany
[Read before The Royal Statistical Society at a meeting organized by the Research
Section on Wednesday, March 13th, 2002, Professor D. Firth in the Chair]
Summary. We consider the problem of comparing complex hierarchical models in which the
number of parameters is not clearly deﬁned. Using an information theoretic argument we derive
a measure pD for the effective number of parameters in a model as the difference between
the posterior mean of the deviance and the deviance at the posterior means of the parameters
of interest. In general pD approximately corresponds to the trace of the product of Fisher’s
information and the posterior covariance, which in normal models is the trace of the ‘hat’ matrix
projecting observations onto ﬁtted values. Its properties in exponential families are explored.
The posterior mean deviance is suggested as a Bayesian measure of ﬁt or adequacy, and the
contributions of individual observations to the ﬁt and complexity can give rise to a diagnostic
plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives
a deviance information criterion for comparing models, which is related to other information
criteria and has an approximate decision theoretic justiﬁcation. The procedure is illustrated in
some examples, and comparisons are drawn with alternative Bayesian and classical proposals.
Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain
Monte Carlo analysis.
Keywords: Bayesian model comparison; Decision theory; Deviance information criterion;
Effective number of parameters; Hierarchical models; Information theory; Leverage; Markov
chain Monte Carlo methods; Model dimension
1.
Introduction
The development of Markov chain Monte Carlo (MCMC) methods has made it possible to
ﬁt increasingly large classes of models with the aim of exploring real world complexities of
data (Gilks et al., 1996). This ability naturally leads us to wish to compare alternative model
formulations with the aim of identifying a class of succinct models which appear to describe the
information in the data adequately: for example, we might ask whether we need to incorporate
Address for correspondence: David J. Spiegelhalter, Medical Research Council Biostatistics Unit, Institute of
Public Health, Robinson Way, Cambridge, CB2 2SR, UK.
E-mail: david.spiegelhalter@mrc-bsu.cam.ac.uk
2002 Royal Statistical Society
1369–7412/02/64583

584
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
a random effect to allow for overdispersion, what distributional forms to assume for responses
and random effects, and so on.
Within the classical modelling framework, model comparison generally takes place by deﬁn-
ing a measure of ﬁt, typically a deviance statistic, and complexity, the number of free parameters
in the model. Since increasing complexity is accompanied by a better ﬁt, models are compared
by trading off these two quantities and, following early work of Akaike (1973), proposals are
often formally based on minimizing a measure of expected loss on a future replicate data set:
see, for example, Efron (1986), Ripley (1996) and Burnham and Anderson (1998). A model
comparison using the Bayesian information criterion also requires the speciﬁcation of the num-
ber of parameters in each model (Kass and Raftery, 1995), but in complex hierarchical models
parameters may outnumber observations and these methods clearly cannot be directly applied
(Gelfand and Dey, 1994). The most ambitious attempts to tackle this problem appear in the
smoothing and neural network literature (Wahba, 1990; Moody, 1992; MacKay, 1995; Ripley,
1996). This paper suggests Bayesian measures of complexity and ﬁt that can be combined to
compare models of arbitrary structure.
In the next section we use an information theoretic argument to motivate a complexity mea-
sure pD for the effective number of parameters in a model, as the difference between the posterior
mean of the deviance and the deviance at the posterior estimates of the parameters of inter-
est. This quantity can be trivially obtained from an MCMC analysis and algebraic forms and
approximations are unnecessary for its use. We nevertheless investigate some of its formal prop-
erties in the following three sections: Section 3 shows that pD is approximately the trace of the
product of Fisher’s information and the posterior covariance matrix, whereas in Section 4 we
show that for normal models pD corresponds to the trace of the ‘hat’ matrix projecting observa-
tions onto ﬁtted values and we illustrate its form for various hierarchical models. Its properties
in exponential families are explored in Section 5.
The posterior mean deviance ¯D can be taken as a Bayesian measure of ﬁt or ‘adequacy’,
and Section 6 shows how in exponential family models an observation’s contributions to ¯D and
pD can be used as residual and leverage diagnostics respectively. In Section 7 we tentatively
suggest that the adequacy ¯D and complexity pD may be added to form a deviance information
criterion DIC which may be used for comparing models. We describe how this parallels the
development of non-Bayesian information criteria and provide a somewhat heuristic decision
theoretic justiﬁcation. In Section 8 we illustrate the use of this technique on some reason-
ably complex examples. Finally, Section 9 draws some conclusions concerning these proposed
techniques.
2.
The complexity of a Bayesian model
2.1.
‘Focused’ full probability models
Parametric statistical modelling of data y involves the speciﬁcation of a probability model
p.y|θ/, θ ∈Θ. For a Bayesian ‘full’ probability model, we also specify a prior distribution
p.θ/ which may give rise to a marginal distribution
p.y/ =

Θ
p.y|θ/ p.θ/ dθ:
(1)
Particular choices of p.y|θ/ and p.θ/ will be termed a model ‘focused’ on Θ. Note that we
might further parameterize our prior with unknown ‘hyperparameters’ ψ to create a hierarchical
model, so that the full probability model factorizes as

Model Complexity and Fit
585
p.y;θ;ψ/ = p.y; θ/ p.θ|ψ/ p.ψ/:
Then, depending on the parameters in focus, the model may compose the likelihood p.y|θ/ and
prior
p.θ/ =

Ψ
p.θ|ψ/ p.ψ/ dψ;
or the likelihood
p.y|ψ/ =

Θ
p.y|θ/ p.θ|ψ/ dθ
and prior p.ψ/. Both these models lead to the same marginal distribution (1) but can be consid-
ered as having different numbers of parameters. A consequence is that in hierarchical modelling
we cannot uniquely deﬁne a ‘likelihood’ or ‘model complexity’ without specifying the level of
the hierarchy that is the focus of the modelling exercise (Gelfand and Trevisani, 2002). In fact,
by focusing our models on a particular set of parameters Θ, we essentially reduce all models to
non-hierarchical structures.
For example, consider an unbalanced random-effects one-way analysis of variance (ANOVA)
focused on the group means:
yi|θi ∼N.θi;τ−1
i
/;
θi ∼N.ψ; λ−1/;
i = 1; : : : ;p:
(2)
This model could also be focused on the overall mean ψ to give
yi|ψ ∼N.ψ;τ−1
i
+ λ−1/;
in which case it could reasonably be considered as having a different complexity.
It is natural to wish to measure the complexity of a focused model, both in its own right,
say to assess the degrees of freedom of estimators, and as a contribution to model choice: for
example, criteria such as BIC (Schwarz, 1978), AIC (Akaike, 1973), TIC (Takeuchi, 1976) and
NIC (Murata et al., 1994) all trade off model ﬁt against a measure of the effective number of
parameters in the model. However, the foregoing discussion suggests that such measures of com-
plexity may not be unique and will depend on the number of parameters in focus. Furthermore,
the inclusion of a prior distribution induces a dependence between parameters that is likely
to reduce the effective dimensionality, although the degree of reduction may depend on the
data that are available. Heuristically, complexity reﬂects the ‘difﬁculty in estimation’ and hence
it seems reasonable that a measure of complexity may depend on both the prior information
concerning the parameters in focus and the speciﬁc data that are observed.
2.2.
Is there a true model?
We follow Box (1976) in believing that ‘all models are wrong, but some are useful’. However,
it can be useful to posit a ‘true’ distribution pt.Y/ of unobserved future data Y since, for any
focused model, this deﬁnes a ‘pseudotrue’ parameter value θt (Sawa, 1978) which speciﬁes a
likelihood p.Y|θt/ that minimizes the Kullback–Leibler distance Et[log{pt.Y/}=p.Y|θt/] from
pt.Y/. Having observed data y, under reasonably broad conditions (Berk, 1966; Bunke and
Milhaud, 1998) p.θ|y/ converges to θt as information on the components of θ increases. Thus
Bayesian analysis implicitly relies on p.Y|θt/ being a reasonable approximation to pt.Y/, and
we shall indicate where we make use of this ‘good model’ assumption.

586
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
2.3.
True and estimated residual information
The residual information in data y conditional on θ may be deﬁned (up to a multiplicative
constant) as −2 log{p.y|θ/} (Kullback and Leibler, 1951; Burnham and Anderson, 1998) and
can be interpreted as a measure of ‘surprise’ (Good, 1956), logarithmic penalty (Bernardo, 1979)
or uncertainty. Suppose that we have an estimator ˜θ.y/ of the pseudotrue parameter θt. Then
the excess of the true over the estimated residual information will be denoted
dΘ{y;θt; ˜θ.y/} = −2 log{p.y|θt/} + 2 log[p{y| ˜θ.y/}]:
(3)
This can be thought of as the reduction in surprise or uncertainty due to estimation, or alter-
natively the degree of ‘overﬁtting’ due to ˜θ.y/ adapting to the data y. We now argue that dΘ
may form the basis for both classical and Bayesian measures of model dimensionality, with each
approach differing in how it deals with the unknown true parameters in dΘ.
2.4.
Classical measures of model dimensionality
In a non-Bayesian likelihood-based context, we may take ˜θ.y/ to be the maximum likelihood
estimator ˆθ.y/, expand 2 log{p.y|θt/} around 2 log[p{y| ˆθ.y/}], take expectations with respect
to the unknown true sampling distribution pt.Y/ and hence show (Ripley, 1996) (page 34) that
Et[dΘ{Y;θt; ˜θ.Y/}] ≈pÅ = tr.KJ−1/;
(4)
where
J = −Et
@2 log{p.Y|θt/}
@θ2

;
K = vart
@ log{p.Y|θt/}
@θ

:
(5)
This is the measure of complexity that is used in TIC (Takeuchi, 1976). Burnham and Anderson
(1998) (page 244) pointed out that
pÅ = tr.JΣ/;
(6)
whereΣ = J−1KJ−1 isthefamiliar‘sandwich’approximationtothevariance–covariancematrix
of the ˆθ.y/ (Huber, 1967). If pt.y/ = p.y|θt/, i.e. one of the models is true, then K = J and
pÅ = p, the number of independent parameters in Θ.
For example, in a ﬁxed effect ANOVA model
yi|θi ∼N.θi;τ−1
i
/;
i = 1; : : : ;p;
with τ−1
i
s known,
dΘ{y;θt; ˆθ.y/} = 
i
τi.yi −θt
i/2;
whose expectation under pt.Y/ is pÅ = Σiτi Et.Yi −θt/2. If the model is true, Et.Yi −θt/2 = τ−1
i
and so pÅ = p.
Ripley (1996) (page 140) showed how this procedure may be extended to ‘regularized’ models
in which a speciﬁed prior term p.θ/ is introduced to form a penalized log-likelihood. Replacing
log.p/ by log{p.y|θ/} + log{p.θ/} in equations (5) yields a more general deﬁnition of pÅ that

Model Complexity and Fit
587
was derived by Moody (1992) and termed the ‘effective number of parameters’. This is the
measure of dimensionality that is used in NIC (Murata et al., 1994): the estimation of pÅ is
generally not straightforward (Ripley, 1996).
In the random-effects ANOVA example with θi ∼N.ψ; λ−1/; ψ and λ known, let ρi =
τi=.τi + λ/ be the intraclass correlation coefﬁcient in the ith group. We then obtain
pÅ = 
i
ρiτi Et.Yi −θt/2;
(7)
which becomes
pÅ = 
i
ρi
(8)
if the likelihood is true.
2.5.
A Bayesian measure of model complexity
From a Bayesian perspective, the unknown θt may be replaced by a random variable θ. Then
dΘ{y; θ; ˜θ.y/} can be estimated by its posterior expectation with respect to p.θ|y/, denoted
pD{y;Θ; ˜θ.y/} = Eθ|y[dΘ{y;θ; ˜θ.y/}]
= Eθ|y[−2 log{p.y|θ/}] + 2 log[p{y| ˜θ.y/}]:
(9)
pD{y; Θ; ˜θ.y/} is our proposal as the effective number of parameters with respect to a model with
focus Θ: we shall usually drop the arguments {y;Θ; ˜θ.y/} from the notation. In our examples
we shall generally take ˜θ.y/ = E.θ|y/ = ¯θ, the posterior mean of the parameters. However, we
note that it is not strictly necessary to use the posterior mean as an estimator of either dΘ or θ,
and the mode or median could be justiﬁed (Section 2.6).
Taking f.y/ to be some fully speciﬁed standardizing term that is a function of the data alone,
pD may be written as
pD = D.θ/ −D. ¯θ/
(10)
where
D.θ/ = −2 log{p.y|θ/} + 2 log{f.y/}:
We shall term D.θ/ the ‘Bayesian deviance’ in general and, more speciﬁcally, for members of
the exponential family with E.Y/ = µ.θ/ we shall use the saturated deviance D.θ/ obtained by
setting f.y/ = p{y|µ.θ/ = y}: see Section 8.1.
Equation (10) shows that pD can be considered as a ‘mean deviance minus the deviance of the
means’. A referee has pointed out the related argument used by Meng and Rubin (1992), who
showed that such a difference, between the average of log-likelihood ratios and the likelihood
ratio evaluated at the average (over multiple imputations) of the parameters, is the key quantity
in estimating the degrees of freedom of a test.
For example, in the random-effects ANOVA (2) with ψ and λ known,
D.θ/ = 
i
τi.yi −θi/2;

588
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
which is −2 log(likelihood) standardized by the term −2 log{f.y/} = Σi log.2π=τi/ obtained
from setting θi = yi. Now θi|y ∼N{ρiyi + .1 −ρi/ψ; ρiτ−1
i
} and hence it can be shown that the
posterior distribution of D.θ/ has the form
D.θ/ ∼ ρi χ2{1;.yi −ψ/2.1 −ρi/λ};
where χ2.a;b/ is a non-central χ2-distribution with mean a + b. Thus, since ρiλ = .1 −ρi/τi,
we have
D.θ/ =  ρi +  τi.1 −ρi/2.yi −ψ/2;
D. ¯θ/ =  τi.1 −ρi/2.yi −ψ/2;
and so
pD = 
i
ρi = 
i
τi
τi + λ:
(11)
The effective number of parameters is therefore the sum of the intraclass correlation coefﬁcients,
which essentially measures the sum of the ratios of the precision in the likelihood to the precision
in the posterior. This exactly matches Moody’s approach (8) when the model is true.
If ψ is unknown and given a uniform hyperprior we obtain a posterior distribution ψ ∼
N{¯y;.λΣρi/−1}, where ¯y = Σρiyi=Σρi. It is straightforward to show that
D.θ/ =  ρi + λ  ρi.1 −ρi/.yi −¯y/2 +  ρi.1 −ρi/=  ρi;
D. ¯θ/ = λ  ρi.1 −ρi/.yi −¯y/2;
and so pD = Σρi + Σρi.1 −ρi/=Σρi. If the groups are independent, λ = 0; ρi = 1 and pD = p.
If the groups all have the same mean, λ →∞; ρi →0 and pD →1. If all group precisions are
equal, pD = 1 + .p −1/ρ; as obtained by Hodges and Sargent (2001).
2.6.
Some observations on pD
(a) Equation (10) may be rewritten as
D.θ/ = D. ¯θ/ + pD;
(12)
whichcanbeinterpretedasaclassical‘plug-in’measureofﬁtplusameasureofcomplexity.
Thus our Bayesian measure of ﬁt, D.θ/, could perhaps be better considered as a measure
of ‘adequacy’, and we shall use these terms interchangeably. However, in Section 7.3 we
shall suggest that an additional penalty for complexity may be reasonable when making
model comparisons.
(b) Simple use of the Bayes theorem reveals the expression
pD = Eθ|y

−2 log
p.θ|y/
p.θ/

+ 2 log
p. ˜θ|y/
p. ˜θ/

;
which can be interpreted as (minus twice) the posterior estimate of the gain in information
provided by the data about θ, minus the plug-in estimate of the gain in information.

Model Complexity and Fit
589
(c) It is reasonable that the effective number of parameters in a model might depend on
the data, the choice of focus Θ and the prior information (Section 2.1). Less attractive,
perhaps, is that pD may also depend on the choice of estimator ˜θ.y/, since this can
produce a lack of invariance of pD to apparently innocuous transformations, such as
making inferences on logits instead of probabilities in Bernoulli trials. Our usual choice
of the posterior mean is largely based on the subsequent ability to investigate approximate
forms for pD (Section 3), and the positivity properties described below. A choice of, say,
posterior medians would produce a measure of model complexity that was invariant to
univariate 1–1 transformations, and we explore this possibility in Section 5.
(d) It follows from equation (10) and Jensen’s inequality that, when using the posterior mean
as an estimator ˜θ.y/;pD ⩾0 for any likelihood that is log-concave in θ, with 0 being
approached for a degenerate prior on θ. Non-log-concave likelihoods can, however, give
rise to a negative pD in certain circumstances. For example, consider a single observation
from a Cauchy distribution with deviance D.θ/ = 2 log{1 + .y −θ/2}, with a discrete
prior assigning probability 1/11 to θ = 0 and 10/11 to θ = 3. If we observe y = 0,
then the posterior probabilities are changed to 0.5 and 0.5, and so ¯θ = 1:5. Thus pD =
D.θ/ −D. ¯θ/ = log.10/ −2 log.13=4/ = log.160=169/ < 0. Our experience has been that
negative pDs indicate substantial conﬂict between the prior and data, or where the pos-
terior mean is a poor estimator (such as a symmetric bimodal distribution).
(e) The posterior distribution that is used in obtaining pD conditions on the truth of the
model, and hence pD may only be considered an appropriate measure of the complexity
of a model that reasonably describes the data. This is reﬂected in the ﬁnding that pD in
the simple ANOVA example (11) will not necessarily be approximately equivalent to the
classical pÅ (7) if the assumptions of the model are substantially inaccurate. This good
model assumption (Section 2.2) is further considered when we come to comparisons of
models (Section 7.3).
(f) ProvidedthatD.θ/isavailableinclosedform,pD maybeeasilycalculatedafteranMCMC
run by taking the sample mean of the simulated values of D.θ/, minus the plug-in estimate
of the deviance using the sample means of the simulated values of θ. No ‘small sample’
adjustment is necessary. This ease of computation should be contrasted with the frequent
difﬁculty within the classical framework with deriving the functional form of the measure
of dimensionality and its subsequent estimation.
(g) Since the complexity depends on the focus, a decision must be made whether nuisance
parameters, e.g. variances, are to be included in Θ or integrated out before specifying the
model p.y|θ/. However, such a removal of nuisance parameters may create computational
difﬁculties.
pD has been deﬁned and is trivially computable by using MCMC methods, and so strictly
speaking there is no need to explore exact forms or approximations. However, to provide insight
into the behaviour of pD, the following three sections consider the form of pD in different
situations and draw parallels with alternative suggestions: note that we are primarily concerned
with the ‘preasymptotic’ situation in which prior opinion is still inﬂuential and the likelihood
has not overwhelmed the prior.
3.
Forms for pD based on normal approximations
In Section 2.1 we argued that focused models are essentially non-hierarchical with a likelihood
p.y|θ/ and prior p.θ/. Before considering particular assumptions for these we examine the form

590
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
of pD under two general conditions: approximately normal likelihoods and negligible prior
information.
3.1.
pD assuming a normal approximation to the likelihood
We may expand D.θ/ around Eθ|y.θ/ = ¯θ to give, to second order,
D.θ/ ≈D. ¯θ/ + .θ −¯θ/T @D
@θ
¯θ
+ 1
2.θ −¯θ/T @2D
@θ2
¯θ
.θ −¯θ/;
(13)
= D. ¯θ/ −2.θ −¯θ/TL′
¯θ −.θ −¯θ/TL′′
¯θ.θ −¯θ/
(14)
where L = log{p.y|θ/} and L′ and L′′ represent ﬁrst and second derivatives with respect to θ.
This corresponds to a normal approximation to the likelihood.
Taking expectations of equation (14) with respect to the posterior distribution of θ gives
Eθ|y{D.θ/} ≈D. ¯θ/ −E[tr{.θ −¯θ/TL′′
¯θ.θ −¯θ/}]
= D. ¯θ/ −E[tr{L′′
¯θ.θ −¯θ/.θ −¯θ/T}]
= D. ¯θ/ −tr[L′′
¯θ E{.θ −¯θ/.θ −¯θ/T}]
= D. ¯θ/ + tr.−L′′
¯θV/
where V = E{.θ −¯θ/.θ −¯θ/T} is the posterior covariance matrix of θ, and −L′′
¯θ is the observed
Fisher information evaluated at the posterior mean of θ. Thus
pD ≈tr.−L′′
¯θV/;
(15)
which can be thought of as a measure of the ratio of the information in the likelihood about
the parameters as a fraction of the total information in the likelihood and the prior. We note
the parallel with the classical pÅ in equation (6).
We also note that
L′′
¯θ = Q′′
¯θ −P′′
¯θ
where Q′′ = @2 log{p.θ|y/}=@θ2 and P′′ = @2 log{p.θ/}=@θ2, and hence approximation (15) can
be written
pD ≈tr.−Q′′
¯θV/ −tr.−P′′
¯θ V/:
Under approximate posterior normality V −1 ≈−Q′′
¯θ and hence
pD ≈p −tr.−P′′
¯θ V/
(16)
where p is the cardinality of Θ.

Model Complexity and Fit
591
3.2.
pD for approximately normal likelihoods and negligible prior information
Consider a focused model in which p.θ/ is assumed to be dominated by the likelihood, either
becauseofassuminga‘ﬂat’priororbyincreasingthesamplesize.Assumethattheapproximation
θ|y ∼N. ˆθ; −L′′
ˆθ/
(17)
holds, where ¯θ = ˆθ are the maximum likelihood estimates such that L′
ˆθ = 0 (Bernardo and
Smith (1994), section 5.3). From equation (14)
D.θ/ ≈D. ˆθ/ −.θ −ˆθ/TL′′
ˆθ.θ −ˆθ/
≈D. ˆθ/ + χ2
p;
(18)
since, by approximation (17), −.θ −ˆθ/TL′′
ˆθ.θ −ˆθ/ has an approximate χ2-distribution with p
degrees of freedom.
Rearranging approximation (18) and taking expectations with respect to the posterior
distribution of θ reveals that
pD = Eθ|y{D.θ/} −D. ˆθ/ ≈p;
i.e. pD will be approximately the true number of parameters: this approximation could also be
derived by letting P′′
¯θ →0 in approximation (16). This approximate identity is illustrated in
Section 8.1.
We note in passing that we might use MCMC output to estimate the classical deviance D. ˆθ/
of any likelihood-based model by
ˆD. ˆθ/ = Eθ|y{D.θ/} −p:
(19)
Although the maximum likelihood deviance is theoretically the minimum of D over all feasible
values of θ;D. ˆθ/ will generally be very badly estimated by the sample minimum over an MCMC
run, and so the estimator given by equation (19) may be preferable.
4.
pD for normal likelihoods
In this section we illustrate the formal behaviour of pD for normal likelihoods by using exact and
approximate identities. However, it is important to keep in mind that in practice such forms are
unnecessary for computation and that pD should automatically allow for ﬁxed effects, random
effects and unknown precisions.
4.1.
The normal linear model
We consider the general hierarchical normal model described by Lindley and Smith (1972).
Suppose that
y ∼N.A1θ; C1/;
θ ∼N.A2ψ; C2/
(20)
where all matrices and vectors are of appropriate dimension, and C1 and C2 are assumed known
and θ is the focus: unknown precisions are considered in Section 4.5. Then the standardized
deviance is D.θ/ = .y−A1θ/TC−1
1 .y−A1θ/; and the posterior distribution for θ is normal with

592
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
mean ¯θ = Vb and covariance V: V and b will be left unspeciﬁed for the moment. Expressing
y −A1θ as y −A1 ¯θ + A1 ¯θ −A1θ reveals that
D.θ/ = D. ¯θ/ −2.y −A1 ¯θ/TC−1
1 A1.θ −¯θ/ + .θ −¯θ/TAT
1 C−1
1 A1.θ −¯θ/:
Taking expectations with respect to the posterior distribution of θ eliminates the middle term
and gives
¯D = D. ¯θ/ + tr.AT
1 C−1
1 A1V/;
and thus pD = tr.AT
1 C−1
1 A1V/: We note that AT
1 C−1
1 A1 is the Fisher information −L′′; V is the
posterior covariance matrix and hence
pD = tr.−L′′V/:
(21)
an exact version of approximation (15). It is also clear that in this context pD is invariant to
afﬁne transformations of θ.
If ψ is assumed known, then Lindley and Smith (1972) showed that V −1 = AT
1 C−1
1 A1 + C−1
2
and hence from equation (21)
pD = p −tr.C−1
2 V/
(22)
as an exact version of approximation (16); then 0 ⩽pD ⩽p, and p −pD is the measure of the
‘shrinkage’ of the posterior estimates towards the prior means. If .C−1
2 V/−1 = AT
1 C−1
1 A1C2 +Ip
has eigenvalues λi + 1;i = 1; : : : ;p, then
pD =
p
i=1
λi
λi + 1;
(23)
and hence the upper bound for pD is approached as the eigenvalues of C2 become large, i.e.
the prior becomes ﬂat. It can further be shown, in the case A1 = In, that pD is the sum of the
squared canonical correlations between data Y and the ‘signal’ θ.
4.2.
The ‘hat’ matrix and leverages
A revealing identity is found by noting that b = AT
1 C−1
1 y and the ﬁtted values for the data are
given by ˆy = A1 ¯θ = A1Vb = A1VAT
1 C−1
1 y. Thus the hat matrix that projects the data onto the
ﬁtted values is H = A1VAT
1 C−1
1 , and
pD = tr.AT
1 C−1
1 A1V/ = tr.A1VAT
1 C−1
1 / = tr.H/:
(24)
This identity also holds assuming that ψ is unknown with a uniform prior, in which case Lindley
and Smith (1972) showed that V −1 = AT
1 C−1
1 A1 + C−1
2
−C−1
2 A2.AT
2 C−1
2 A2/−1AT
2 C−1
2 .
The identiﬁcation of the effective number of parameters with the trace of the hat matrix
is a standard result in linear modelling and has been applied to smoothing (Wahba, 1990)
(page 63) and generalized additive models (Hastie and Tibshirani (1990), section 3.5), and is
also the conclusion of Hodges and Sargent (2001) in the context of general linear models. The
advantage of using the deviance formulation for specifying pD is that all matrix manipulation
and asymptotic approximation is avoided: see Section 4.4 for further discussion. Note that tr.H/
is the sum of terms which in regression diagnostics are identiﬁed as the individual leverages, the
inﬂuence of each observation on its ﬁtted value: we shall return to this identity in Section 6.3.

Model Complexity and Fit
593
Ye (1998) considered the independent normal model
yi ∼N.θi; τ−1/
and suggested that the effective number of parameters should be Σi hi, where
hi.θ/ = @Ey|θ. ˜θi/
@θi
:
(25)
the average sensitivity of an unspeciﬁed estimate ˜θi to a small change in yi. This is a generalization
of the trace of the hat matrix discussed above. In the context of the normal linear models, it is
straightforward to show that EY|θ. ¯θ/ = Hθ, and hence pD = tr.H/ matches Ye’s suggestion for
model complexity. Further connections with Ye (1998) are described in Section 7.2.
4.3.
Example: Laird–Ware mixed models
Laird and Ware (1982) speciﬁed the mixed normal model as
y ∼N.Xα + Zβ; C1/;
β ∼N.0;D/;
where the covariance matrices C1 and D are currently assumed known. The random effects are
β, and the ﬁxed effects are α, and placing a uniform prior on α we can write this model within
the general Lindley–Smith formulation (20) by setting θ = .α; β/;A1 = .X; Z/;ψ = 0 and C2
as a block diagonal matrix with ∞in the top left-hand block, D in the bottom right and 0
elsewhere.
We have already shown that in these circumstances pD = tr{AT
1 C−1
1 A1.AT
1 C−1
1 A1 +C−1
2 /−1},
and substituting in the appropriate entries for the Laird–Ware model gives pD = tr.VÅV −1/,
where
VÅ =

XTC−1
1 X
XTC−1
1 Z
ZTC−1
1 X
ZTC−1
1 Z
	
;
V =

XTC−1
1 X
XTC−1
1 Z
ZTC−1
1 X
ZTC−1
1 Z + D−1
	
which is the precision of the parameter estimates assuming that D−1 = 0, relative to the precision
assuming informative D.
4.4.
Frequentist approaches to model complexity: smoothing and normal non-linear
models
A common model in semiparametric regression is
y ∼N.Xα + β; τ−1C1/;
β ∼N.0;λ−1D/;
where β is a vector of length n of function values of the nonparametric part of an interpolation
spline (Wahba, 1990; van der Linde, 1995) and C1 and D are assumed known. Motivated
by the need to estimate the unknown scale factors τ−1 and λ−1, for many years the effective
number of parameters has been taken to be the trace of the hat matrix (Wahba (1990), page
63) and so, for example, ˆτ−1 is the residual sum of squares divided by the ‘effective degrees

594
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
of freedom’ n −tr.H/. In this class of models this measure of complexity coincides with pD.
Interest in regression diagnostics (Eubank, 1985; Eubank and Gunst, 1986) and cross-validation
to determine the smoothing parameter τ=λ (Wahba (1990), section 4.2) also drew attention to
the diagonal entries of the hat matrix as leverage values.
LinkstopartiallyBayesianinterpolationmodelshavebeenprovidedbyKimeldorfandWahba
(1970) and Wahba (1978, 1983) and further work built on these ideas. For example, another
large class of models can be formulated by using the following extension to the Lindley–Smith
model:
y ∼N{g.θ/;τ−1C1};
θ ∼N.A2ψ;λ−1D/
where g is a non-linear expression as found, for example, in pharmacokinetics or neural net-
works: in many situations A2ψ will be 0 and C1 and D will be identity matrices. Deﬁne
q.θ/ = .y −g.θ//TC−1
1 .y −g.θ//;
r.θ/ = .θ −A2ψ/TD−1.θ −A2ψ/
as the likelihood and prior residual variation. MacKay (1992) suggested estimating τ and λ by
maximizing the ‘type II’ likelihood p.y|λ;τ/ derived from integrating out the unknown θ from
the likelihood. Setting derivatives equal to 0 eventually reveals that
ˆτ−1 =
q. ¯θ/
n −pD
;
ˆλ−1 = r. ¯θ/
pD
;
which are the ﬁtted likelihood and prior residual variation, divided by the appropriate effective
degrees of freedom: pD = tr.H/ is the key quantity.
These results were derived by MacKay (1992) in the context of ‘regularization’ in complex
interpolation models such as neural networks, in which the parameters θ are standardized and
assumed to have independent normal priors with mean 0 and precision λ. Then expression (16)
may be written
pD ≈p −λ tr.V/:
(26)
However, MacKay’s use of approximation (26) requires the evaluation of tr.V/, whereas our
pD arises without any additional computation. We would also recommend including λ and τ in
the general MCMC estimation procedure, rather than relying on type II maximum likelihood
estimates (Ripley (1996), page 167). In this and the smoothing context a fully Bayesian analysis
requires prior distributions for τ−1 and λ−1 to be speciﬁed (van der Linde, 2000), and this will
both change the complexity of the model and require a choice of estimator of the precisions.
We shall now illustrate the form of pD in the restricted situation of unknown τ−1.
4.5.
Normal models with unknown sampling precision
Introducing unknown variances as part of the focus confronts us with the need to choose a
form for the plug-in posterior estimates. We may illustrate this issue by extending the general
hierarchical normal model (20) to the conjugate normal–gamma model with an unknown scale

Model Complexity and Fit
595
parameter τ in both the likelihood and the prior (Bernardo and Smith (1994), section 5.2.1).
Suppose that
y ∼N.A1θ;τ−1C1/;
θ ∼N.A2ψ;τ−1C2/;
(27)
and we focus on .θ;τ/. The standardized deviance is D.θ; τ/ = τ q.θ/ −n log.τ/, where
q.θ/ = .y −A1θ/TC−1
1 .y −A1θ/
is the residual variation. Then, for a currently unspeciﬁed estimator ˆτ,
pD = Eθ;τ|y.D|θ;τ/ −D. ¯θ; ˆτ/
= Eτ|y[Eθ|τ;y{τ q.θ/} −n log.τ/] −{ˆτ q. ¯θ/ −n log.ˆτ/}
= tr.H/ + q. ¯θ/.¯τ −ˆτ/ −n{log.τ/ −log.ˆτ/}
(28)
where H = AT
1 C−1
1 A1.AT
1 C−1
1 A1 + C−1
2 /−1 is the hat matrix which does not depend on τ. Thus
the additional uncertain scale parameter adds the second two terms to the complexity of the
model.
A conjugate prior τ ∼gamma.a; b/ leads to a posterior distribution τ|y ∼gamma.a + n=2;
b + S=2/, where
S = .y −A1A2ψ/T.C1 + AT
1 C2A1/−1.y −A1A2ψ/:
It remains to choose the estimator ˆτ to place in equation (28), and we shall consider two options.
Suppose that we parameterize in terms of τ and use
ˆτ = ¯τ = a + n=2
b + S=2;
making the second term in equation (28) 0. Now if X ∼gamma.a;b/, then E{log.X/}
= ψ.a/ −log.b/ where ψ is the digamma function, and so log.τ/ = ψ.a + n=2/ −log.b + S=2/.
Hence the term contributing to pD due to the unknown precision is
pD −tr.H/ = −n

ψ

a + n
2
	
−log

a + n
2
	
≈1 −2a −1
3
2a + n
using the approximation ψ.x/ ≈log.x/−1=2x−1=12x2. This term will tend to 1+1=3n as prior
information becomes negligible and hence will be close to the ‘correct’ value of 1 for moderate
sample sizes.
If we were to parameterize in terms of log.τ/ and to use ˆτ = exp{log.τ/}, the third term in
equation (28) is 0 and the second term can be shown to be 1 −O.n−1/. Thus for reasonable
sample sizes the choice of parameterization of the unknown precision will make little difference
to the measure of complexity. However, in Section 7 we shall argue that the log-scale may be
more appropriate owing to the better approximation to likelihood normality.

596
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
5.
Exponential family likelihoods
We assume that we have p groups of observations, where each of the ni observations in group i
has the same distribution. Following McCullagh and Nelder (1989), we deﬁne a one-parameter
exponential family for the jth observation in the ith group as
log{p.yij|θi;φ/} = wi{yijθi −b.θi/}=φ + c.yij; φ/;
(29)
where
µi = E.Yij|θi;φ/ = b′.θi/;
V.Yij|θi;φ/ = b′′.θi/φ=wi;
and wi is a constant. If the canonical parameterization Θ is the focus of the model, then writing
¯bi = Eθi|y{b.θi/} we easily obtain that the contribution of the ith group to the effective number
of parameters is
pΘ
Di = 2niwi{ ¯bi −b. ¯θi/}=φ:
(30)
These likelihoods highlight the issue of the lack of invariance of pD to reparameterization, since
the mean parameterization µ will give a different complexity pµ
Di. This is ﬁrst explored within
simple binomial and Poisson models with conjugate priors, and then exact and approximate
forms of pD are examined for generalized linear and generalized linear mixed models.
5.1.
Binomial likelihood with conjugate prior
In the notation of equation (29), φ = 1;wi = 1 and θ = logit.µ/ = log{µ=.1 −µ/}, and the
(unstandardized) deviance is
D.µi/ = −2yi log.µi/ −2.ni −yi/ log.1 −µi/
where yi = Σjyij. A conjugate prior µi = {1 + exp.−θi/}−1 ∼beta.a;b/ provides a posterior
µi ∼beta.a + yi;b + ni −yi/ with mean .a + yi/=.a + b + ni/. Now, if X ∼beta.a;b/, then
E{log.X/} = ψ.a/ −ψ.a + b/ and E{log.1 −X/} = ψ.b/ −ψ.a + b/ where ψ is the digamma
function, and hence it can be shown that
D.µi/ = D.θi/ = −2yi ψ.a + yi/ −2.ni −yi/ ψ.b + ni −yi/ + 2ni ψ.a + b + ni/
D. ¯µi/ = −2yi log.a + yi/ −2.ni −yi/ log.b + ni −yi/ + 2ni log.a + b + ni/
D. ¯θi/ = −2yi ψ.a + yi/ + 2yi ψ.b + ni −yi/
+ 2ni log[1 + exp{ψ.a + yi/ −ψ.b + ni −yi/}];
D.µmed
i
/ = D.θmed
i
/ = −2yi log.µmed
i
/ −2.ni −yi/ log.1 −µmed
i
/
where µmed
i
denotes the posterior median of µi.
Exact pDis are obtainable by subtraction, and Fig. 1 shows how the value of pDi depends on
the parameterization, the data and the prior. We may also gain further insight into the behaviour
of pDi by considering approximate formulae for the mean and canonical parameterizations by
using ψ.x/ ≈log.x/ −1=2x ≈log.x −1
2/. This leads to
pµ
Di ≈
yi
a + yi
+
ni −yi
b + ni −yi
−
ni
a + b + ni
; 7
pΘ
Di ≈
ni
a + b + ni −1
2
:
(31)
We make the following observations.

Model Complexity and Fit
597
Fig. 1.
Binomial likelihood—contribution of the ith group to the effective number of parameters under
various parameterizations (canonical pΘ
Di, mean pµ
Di and median pmed
Di
) as a function of the data (sample
size ni and observed proportion yi=ni) and prior (effective prior sample size a + b and prior mean a=(a + b)):
we are seeking agreement between alternative parameterizations with little dependence on data

598
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
5.1.1.
Behaviour of pD
For all three parameterizations, as the sample size in each group increases relative to the effective
prior sample size, its contribution to pDi tends towards 1.
5.1.2.
Agreement between parameterizations
The agreement between parameterizations is generally reasonable except in the situations in
which the prior sample size is 10 times that of the data. While the canonical parameterization
has pDi ≈1=11, the mean and median give increased pDi for extreme prior means.
5.1.3.
Dependence on data
With the exception of the sparse data and weak prior scenario for which the approximate formu-
lae do not hold, the canonical pΘ
Di does not depend on the data observed and is approximately
the ratio of the sample size to the effective posterior sample size. When the mean and median
forms depend on data (say when ni = 1 and a + b = 10), pDi is higher in situations of prior–data
conﬂict.
5.2.
Poisson likelihood with conjugate prior
In the notation of equation (29), φ = 1;wi = 1 and θ = log.µ/, and the (unstandardized)
deviance is D.µi/ = −2yi log.µi/ + 2niµi. A conjugate prior µi = exp.θi/ ∼gamma.a;b/ gives
a posterior µi ∼gamma.a + yi;b + ni/ with mean .a + yi/=.b + ni/. If X ∼gamma.a;b/, then
E{log.X/} = ψ.a/ −log.b/ and hence we can show that
D.µi/ = D.θi/ = −2yi{ψ.a + yi/ −log.b + ni/} + 2ni
a + yi
b + ni
;
D. ¯µi/ = −2yi{log.a + yi/ −log.b + ni/} + 2ni
a + yi
b + ni
;
D. ¯θi/ = −2yi{ψ.a + yi/ −log.b + ni/} + 2ni
exp{ψ.a + yi/}
b + ni
;
D.µmed
i
/ = D.θmed
i
/ = −2yi log.µmed
i
/ + 2niµmed
i
:
Exact pDis are obtainable by subtraction. Fig. 2 shows how the value of pDi relates to the param-
eterization, the data and the prior. Using the same approximation as previously, approximate
pDis for the mean and canonical parameterizations are
pµ
Di ≈yi=.a + yi/;
pΘ
Di ≈ni=.b + ni/:
5.2.1.
Behaviour of pDi
For all three parameterizations, as the sample size in each group increases relative to the effective
prior sample size, its contribution to pDi tends towards 1.
5.2.2.
Agreement between parameterizations
The agreement between parameterizations is best when there is no conﬂict between the prior
expectation and the data, but it can be substantial when such conﬂict is extreme. The median

Model Complexity and Fit
599
Fig. 2. Poisson likelihood—contribution of the ith group to the effective number of parameters under various
parameterizations (canonical pΘ
Di, mean pµ
Di and median pmed
Di
) as a function of the data (sample size ni
and observed total yi) and prior (mean nia=b and ‘sample size’ b)

600
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
estimator leads to a pDi that is intermediate between those derived from the canonical and mean
parameterizations.
5.2.3.
Dependence on data
Except in the situation of a single yi = 0 with weak prior information, the approximation for
the canonical pΘ
Di is very accurate and so pΘ
Di does not depend on the data observed. There can
be a substantial dependence for the mean parameterization, with pµ
Di being higher when the
prior mean underestimates the data.
5.2.4.
Conclusion
In conclusion, for both binomial and Poisson data there is reasonable agreement between the
different pDis provided that the model provides a reasonable ﬁt to the data, i.e. there is not
strong conﬂict between the prior and data. The canonical parameterization appears preferable,
both for its lack of dependence on the data and for its generally close approximation to the
invariant pDi based on a median estimator. Thus we would not normally expect the choice of
parameterization to have a strong effect, although in Section 8.3 we present an example of a
Bernoulli model where this choice does prove to be important.
5.3.
Generalized linear models with canonical link functions
Here we shall focus on the canonical parameterization in terms of θi, both for the reasons
outlined above and because its likelihood should better fulﬁl a normal approximation (Slate,
1994): related identities are available for the mean parameterization in terms of µi = µ.θi/. We
emphasize again that the approximate identities that are derived in this and the following section
are only for understanding the behaviour of pD in idealized circumstances (i.e. known precision
parameters) and are not required for computation in practical situations.
Following McCullagh and Nelder (1989) we assume that the mean µi of yij is related to a set
of covariates xi through a link function g.µi/ = xT
i α, and that g is the canonical link θ.µ/. The
second-order Taylor series expansion of D.θi/ around D. ¯θi/ yields an approximate normal distri-
bution for working observations and hence derivations of Section 3 apply. We eventually obtain
pD ≈tr{XTWX V.α|y/}
where W is diagonal with entries
Wi = wi
φ ni b′′. ¯θi/;
the generalized linear model iterated weights (McCullagh and Nelder (1989), page 40): φ is
assumed known.
Under an N.α0;C2/ prior on α, the prior contribution to the negative Hessian matrix at the
mode is just C−1
2 , so under the canonical link the approximate normal posterior has variance
V.α|y/ = .C−1
2
+ XTWX/−1;
againproducingpD asameasureoftheratioofthe‘working’likelihoodtoposteriorinformation.
5.4.
Generalized linear mixed models
We now consider the class of generalized linear mixed models with canonical link, in which
g.µi/ = xT
i α + zT
i β, where β ∼N.0;D/ (Breslow and Clayton, 1993) and D is assumed known.

Model Complexity and Fit
601
Using the same argument as for generalized linear models (Section 5.3), we ﬁnd that
pD ≈tr[.X;Z/TW.X;Z/V{.α; β/|y}] ≈tr.VÅV −1/;
where
VÅ =

XTW−1X
XTW−1Z
ZTW−1X
ZTW−1Z
	
;
V =

XTW−1X
XTW−1Z
ZTW−1X
ZTW−1Z + D−1
	
:
This matches the proposal of Lee and Nelder (1996) except their D−1 is a diagonal matrix of
the second derivatives of the prior likelihood for each random effect.
6.
Diagnostics for ﬁt and inﬂuence
6.1.
Posterior expected deviance as a Bayesian measure of ﬁt or ‘adequacy’
The posterior mean of the deviance Eθ|y{D.θ/} = D.θ/ has often been used to compare models
informally: see, for example, Dempster (1974) (reprinted as Dempster (1997a)), Raghunathan
(1988), Zeger and Karim (1991), Gilks et al. (1993) and Richardson and Green (1997). These
researchers have, however, not been explicit about whether, or how much, such a measure might
be traded off against increasing complexity of a model: Dempster (1997b) suggested plotting
log-likelihoods from MCMC runs but hesitated to dictate a model choice procedure. We shall
discuss this further in Section 7.3. In Section 2.6 we argued that D.θ/ already incorporates some
penalty for complexity and hence we use the term ‘adequacy’ and ‘Bayesian ﬁt’ interchangeably.
6.2.
Sampling theory diagnostics for lack of Bayesian ﬁt
Suppose that all aspects of the model were assumed true. Then before observing data Y our
expectation of the posterior expected deviance is
EY. ¯D/ = EY[Eθ|y{D.θ/}]
(32)
= Eθ.EY|θ[−2 log{p.Y|θ/} + 2 log{f.Y/}]/
by reversing the conditioning between Y and θ. If f.Y/ = p{Y| ˆθ.Y/} where ˆθ.Y/ is the standard
maximum likelihood estimate, then
EY|θ

−2 log

p.Y|θ/
p{Y| ˆθ.Y/}
	
is simply the expected likelihood ratio statistic for the ﬁtted values ˆθ.Y/ with respect to the true
null model θ and hence under standard conditions is approximately E.χ2
p/ = p, the dimension-
ality of θ. From equation (32) we therefore expect, if the model is true, the posterior expected
deviance (standardized by the maximized log-likelihood) to be EY. ¯D/ ≈Eθ.p/ = p, the number
of free parameters in θ. This might be appropriate for checking the overall goodness of ﬁt of the
model.
In particular, consider the one-parameter exponential family where p = n, the total sample
size. The likelihood is maximized by substituting yi for the mean of yi, and the posterior mean of
the standardized deviance has approximate sampling expectation n if the model is true. This will
be exact for normal models with known variance, but in general it will only be reliable if each
observation provides considerable information about its mean (McCullagh and Nelder (1989),

602
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
page 36). Note that comparing ¯D with n is precisely the same as comparing the ‘classical’ ﬁt
D. ¯θ/ with n −pD, the effective degrees of freedom.
It is then natural to consider the contribution Di of each observation i to the overall mean
deviance, so that
¯D = 
i
¯Di = 
i
dr2
i
where dri = ±√¯Di (with the sign given by the sign of yi −E.yi| ¯θ/) termed the Bayesian deviance
residual, deﬁned analogously to McCullagh and Nelder (1989), page 39. See Section 8.1 for an
application of this procedure.
6.3.
Leverage diagnostics
In Section 4.1 we noted that in normal linear models the contribution pDi of each observation
i to pD turned out to be its leverage, deﬁned as the relative inﬂuence that each observation has
on its own ﬁtted value. For yi conditionally independent given θ, it can be shown that
pDi = −2

Eθ|y

log
p.θ|yi/
p.θ/
 
−log
p. ¯θ|yi/
p. ¯θ/
	
which reﬂects its interpretation as the difﬁculty in estimating θ with yi.
It may be possible to exploit this interpretation in general model ﬁtting, and as a by-product
of MCMC estimation to obtain estimates of leverage for each observation. Such diagnostics are
illustrated in Section 8.1.
7.
A model comparison criterion
7.1.
Model ‘selection’
There has been a long and continuing debate about whether the issue of selecting a model as a
basis for inferences is amenable to a strict mathematical analysis using, for example, a decision
theoretic paradigm: see, for example, Key et al. (1999). Our approach here can be considered
to be semiformal. Although we believe that it is useful to have measures of ﬁt and complexity,
and to combine them into overall criteria that have some theoretical justiﬁcation, we also feel
that an overformal approach to model ‘selection’ is inappropriate since so many other features
of a model should be taken into account before using it as a basis for reporting inferences, e.g.
the robustness of its conclusions and its inherent plausibility. In addition, in many contexts it
may not be appropriate to ‘choose’ a single model. Our development closely follows that of
Section 2.
A characteristic that is common to both Bayesian and classical approaches is the concept of
an independent replicate data set Yrep, derived from the same data-generating mechanism as
gave rise to the observed data. Suppose that the loss in assigning to a set of data Y a probability
p.Y| ˜θ/ is L.Y; ˜θ/: We assume that we shall favour models p.Y| ˜θ/ for which L.Y; ˜θ/ is expected
to be small, and thus a criterion can be based on an estimate of EYrep|θt{L.Yrep; ˜θ/}.
A natural, but optimistic, estimate of this quantity is the ‘apparent’ loss L{y; ˜θ.y/} that
is suffered on repredicting the observed y that gave rise to ˜θ.y/. We follow Efron (1986) in
deﬁning the ‘optimism’ that is associated with this estimator as cΘ, where
EYrep|θt[L{Yrep; ˜θ.y/}] = L{y; ˜θ.y/} + cΘ{y; θt; ˜θ.y/}:
(33)

Model Complexity and Fit
603
Both classical and Bayesian approaches to estimating the optimism cΘ will now be examined
when assuming a logarithmic loss function L.Y; ˜θ/ = −2 log{p.Y| ˜θ/}: as in Section 2, the
classical approach attempts to estimate the sampling expectation of cΘ, whereas the Bayesian
approach is based on a direct calculation of the posterior expectation of cΘ.
7.2.
Classical criteria for model comparison
From the previous discussion, approximate forms for the expected optimism
π.θt/ = EY|θt[cΘ{Y;θt; ˜θ.Y/}]
will, from equation (33), yield criteria for a comparison of models that are based on minimizing
ˆEYrep|θt[L{Yrep; ˜θ.y/}] = L{y; ˜θ.y/} + ˆπ.θt/:
(34)
Efron (1986) derived the expression for π.θt/ for exponential families and for general loss
functions. In particular, for the logarithmic loss function, Efron showed that
πE.θt/ = 2 
i
covt. ˆYi; Yi/;
(35)
where ˆYi is the ﬁtted value arising from the estimator ˜θ: if ˜θ corresponds to maximum likelihood
estimation based on a linear predictor with p parameters, then πE.θt/ ≈2p. Hence Efron’s
result can be thought of as generalizing Akaike (1973), who sought to minimize the expected
Kullback–Leibler distance between the true and estimated predictive distribution and showed
under broad conditions that π.θt/ ≈2p.
This in turn suggests that πE=2, derived from equation (35), may be adopted as a measure
of complexity in more complex modelling situations. Ye and Wong (1998) extended the work
mentioned in Section 4.2 to show that πE=2 for exponential families can be expressed as a sum
of the average sensitivity of the ﬁtted values ˆyi to a small change in yi: this quantity is termed by
Ye and Wong the ‘generalized degrees of freedom’ when using a general estimation procedure.
In normal models with linear estimators ˆyi = ˜θi.y/ = Σj hijyj, and so π.θt/ = 2 tr.H/. Finally,
Ripley (1996) extended the analysis described in Section 2.4 to show that if the model assumed
is not true then π.θt/ ≈2pÅ, where pÅ is deﬁned in equation (4). See Burnham and Anderson
(1998) for a full and detailed review of all aspects of estimation of π.θt/.
These classical criteria for general model comparison are thus all based on equation (34)
and can all be considered as corresponding to a plug-in estimate of ﬁt, plus twice the effective
number of parameters in the model. We shall now adapt this structure to a Bayesian context.
7.3.
Bayesian criteria for model comparison
GelfandandGhosh(1998)andLaudandIbrahim(1995)bothattemptedstrictdecisiontheoretic
approaches to model choice based on expected losses on replicate data sets. Our approach is
more informal, in aiming to identify models that best explain the observed data, but with the
expectation that they are likely to minimize uncertainty about observations generated in the
same way. Thus, by analogy with the classical results described above, we propose a deviance
information criterion DIC, deﬁned as a classical estimate of ﬁt, plus twice the effective number
of parameters, to give
DIC = D. ¯θ/ + 2pD
(36)
= ¯D + pD
(37)

604
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
by deﬁnition of pD (10): equation (37) shows that DIC can also be considered as a Bayesian
measure of ﬁt or adequacy, penalized by an additional complexity term pD. From the results
in Section 3.2, we immediately see that in models with negligible prior information DIC will be
approximately equivalent to Akaike’s criterion.
An approximate decision theoretic justiﬁcation for DIC can be obtained by mimicking the
development of Ripley (1996) (page 33) and Burnham and Anderson (1998) (chapter 6). Using
the logarithmic loss function in equation (33), we obtain
cΘ{y;θt; ˜θ.y/} = EYrep|θt{Drep. ˜θ/} −D. ˜θ/
where −2 log[p{Yrep| ˜θ.y/}] is denoted Drep. ˜θ/ and so on: note in this section that D is an
unstandardized deviance .f.·/ = 1/. It is convenient to expand cΘ into the three terms
cΘ = EYrep|θt{Drep. ˜θ/ −Drep.θt/} + EYrep|θt{Drep.θt/ −D.θt/} + {D.θt/ −D. ˜θ/};
(38)
we shall denote the ﬁrst two terms by L1 and L2 respectively and, since we are taking a Bayesian
perspective, replace the true θt by a random quantity θ.
Expanding the ﬁrst term to second order gives
L1.θ; ˜θ/ ≈EYrep|θ{−2. ˜θ −θ/TL′
rep;θ −. ˜θ −θ/TL′′
rep;θ. ˜θ −θ/}
whereLrep;θ = log{p.Yrep|θ/}.SinceEYrep|θ.L′
rep;θ/ = 0fromstandardresultsforscorestatistics,
we obtain after some rearrangement
L1.θ; ˜θ/ ≈tr{Iθ. ˜θ −θ/. ˜θ −θ/T}
where Iθ = EYrep|θ.−L′′
rep;θ/ is the assumed Fisher information in Yrep, and hence also in y.
Making the good model assumption (Section 2.2), this might reasonably be approximated by
the observed information at the estimated parameters, so
L1.θ; ˜θ/ ≈tr{−L′′
˜θ. ˜θ −θ/. ˜θ −θ/T}:
(39)
Suppose that under a particular model assumption we obtain a posterior distribution p.θ|y/.
Then from approximations (38) and (39) our posterior expected optimism when adopting this
model and the estimator ˜θ is
Eθ|y.cΘ/ ≈tr[−L′′
˜θ Eθ|y{.θ −˜θ/.θ −˜θ/T}] + Eθ|y{L2.y;θ/} + Eθ|y{D.θ/ −D. ˜θ/}:
Using the posterior mean ¯θ as our estimator makes the expected optimism
Eθ|y.cΘ/ ≈tr.−L′′
¯θV/ + Eθ|y{L2.y;θ/} + pD;
(40)
where V again is deﬁned as the posterior covariance of θ, and pD = ¯D −D. ¯θ/. Now
L2.y; θ/ = EYrep|θ[−2 log{p.Yrep|θ/}] + 2 log{p.y|θ/};
and so EY[Eθ|Y{L2.Y;θ/}] = Eθ[EY|θ{L2.Y; θ/}] = 0. We have already shown in approximation
(15) that pD ≈tr.−L′′
¯θV/, and hence from expressions (33) and (40) the expected posterior loss
when adopting a particular model is
D. ¯θ/ + Eθ|y.cΘ/ ≈D. ¯θ/ + 2pD = DIC;
neglecting a term Eθ|y{L2.y; θ/} which is expected to be 0. This derivation has assumed that

Model Complexity and Fit
605
D is an unstandardized deviance: common standardization across models will leave unchanged
the property that differences in DIC are estimates of differences in expected loss in prediction.
We make the following observations concerning this admittedly heuristic justiﬁcation of DIC.
First, for the general normal linear model (20), it is straightforward to show that L2.y;θ/ =
p−.y−A1θ/TC−1
1 .y−A1θ/ where p is the dimensionality of θ, and hence for true θ has sampling
distribution p −χ2
p with mean 0 and variance 2p. This parallels the classical development in
which Ripley (1996) (page 34) pointed out that the equivalent term is O.√n/: we would hope
that this factor will tend to cancel when assessing differences in DIC, but this requires further
investigation.
Second, this development draws heavily on the approximations in Section 3 and hence
encourages parameterizations in which likelihood normality is more plausible.
Third, we are attempting to evaluate the consequences of assuming a particular model, using
an analysis that is based on that very assumption. This use of the good model assumption
(Section 2.2) argues for the use of DIC in comparing models that have already been shown to
be adequate candidates for explaining the observations.
8.
Examples
pD and DIC have already been applied by other researchers in a variety of contexts, such
as alternative models for diagnostic probabilities in screening studies (Erkanli et al., 1999),
longitudinal binary data using Markov regression models (Erkanli et al., 2001), spline models
with Bernoulli responses (Biller and Fahrmeir, 2001), multistage models for treatment usage
which combine to form a total DIC (Gelfand et al., 2000), complex spatial models for Poisson
counts (Green and Richardson, 2000), pharmacokinetic modelling (Rahman et al., 1999) and
structures of Bayesian neural networks (Vehtari and Lampinen, 1999). The following examples
illustrate the use of pD and DIC to compare alternative prior and likelihood structures.
8.1.
The spatial distribution of lip cancer in Scotland
We consider data on the rates of lip cancer in 56 districts in Scotland (Clayton and Kaldor,
1987; Breslow and Clayton, 1993). The data include observed (yi) and expected (Ei) numbers of
cases for each county i (where the expected counts are based on the age- and sex-standardized
national rate applied to the population at risk in each county) plus the ‘location’ of each county
expressed as a list (Ai) of its ni adjacent counties. We assume that the cancer counts within
each county yi follow a Poisson distribution with mean exp.θi/Ei where exp.θi/ denotes the
underlying true area-speciﬁc relative risk of lip cancer. We then consider the following set of
candidate models for θi, reﬂecting different assumptions about the between-county variation in
(log-) relative risk of lip cancer: model 1,
θi = α0;
model 2,
θi = α0 + γi;
model 3,
θi = α0 + δi;
model 4,
θi = α0 + γi + δi;
model 5,
θi = αi:

606
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
An improper uniform prior is placed on α0, independent (proper) normal priors with large
variance are speciﬁed for each αi .i = 1; : : : ;56/, γi are exchangeable random effects with a
normal prior distribution having zero mean and precision λγ, and δi are spatial random effects
with a conditional autoregressive prior (Besag, 1974) given by
δi|δ\i ∼normal
 1
ni

j∈Ai
δj;
1
niλδ
	
:
A sum-to-zero constraint is imposed on the {δi} for identiﬁability, and weakly informative
gamma(0.5,0.0005) priors are assumed for the random effects precision parameters λγ and λδ.
These ﬁve models cover the spectrum between the pooled model 1 that makes no allowance for
variation between the true risk ratios in each county and the saturated model 5 that assumes inde-
pendence between the county-speciﬁc risk ratios (essentially yielding the maximum likelihood
estimates ˆθi = log.yi=Ei/). The random-effects models 2–4 allow the county-speciﬁc relative
risks to be similar but not identical, with the autoregressive term allowing for the possibility of
spatially correlated variation.
We use the saturated deviance (McCullagh and Nelder (1989), page 34)
D.θ/ = 2 
i
[yi log{yi= exp.θi/Ei} −{yi −exp.θi/Ei}]
obtained by taking −2 log{f.y/} = −2Σi log{p.yi| ˆθi/} = 208:0 as the standardizing factor (see
Section 2.5). This allows calculation of absolute measures of ﬁt (see Section 6.2). For model
comparisons, however, it is sufﬁcient to take the standardizing factor as f.y/ = 1. For each
model we ran two independent chains of an MCMC sampler in WinBUGS (Spiegelhalter et al.,
2000) for 15000 iterations each, following a burn-in period of 5000 iterations. As suggested
by Dempster (1997b), Fig. 3 shows a kernel density smoothed plot of the resulting posterior
distributions of the deviance under each competing model. Apart from revealing the obvious
unacceptability of model 1, this clearly illustrates the difﬁculty of formally comparing posterior
deviances on the basis of such plots alone.
Deviance
0.0
0.02
0.04
0.06
0.08
0.10
20
40
60
80
100
360
380
400
Fig. 3.
Posterior distributions of the deviance for each model considered in the lip cancer example:
,
model 1; . . . . . . ., model 2; -------, model 3; – – –, model 4; — —, model 5

Model Complexity and Fit
607
Table 1.
Deviance summaries for the lip cancer data using three alternative parameterizations (mean,
canonical and median) for the plug-in deviance†
Model
¯D
D(¯µ)
pµ
D
DICµ
D(¯θ)
pθ
D
DICθ
D(med)
pmed
D
DICmed
1, pooled
381.7
380.7
1.0
382.7
380.7
1.0
382.7
380.7
1.0
382.7
2, exchangeable
61.1
18.2
42.9
104.0
17.7
43.4
104.5
17.6
43.5
104.6
3, spatial
58.3
26.6
31.7
89.9
27.1
31.2
89.5
27.2
31.1
89.3
4, exchangeable + spatial
57.9
26.1
31.8
89.7
26.5
31.4
89.3
26.6
31.3
89.2
5, saturated
55.9
0.0
55.9
111.7
3.1
52.8
108.6
1.4
54.5
110.4
†Exchangeable means an exchangeable random effect; spatial is a spatially correlated random effect.
The deviance summaries proposed in this paper are shown for the lip cancer data in Table 1:
¯D is simply the mean of the posterior samples of the saturated deviance; D. ¯µ/ is calculated by
plugging the posterior mean of µi = exp.θi/Ei into the saturated deviance; D. ¯θ/ is calculated
by plugging the posterior means of the relevant parameters (α0, αi, γi and/or δi) into the linear
predictor θi and then evaluating the saturated deviance; D.med/ is calculated by plugging the
posterior median of θi (or, equivalently, of µi) into the saturated deviance. The results are
remarkably similar for the three alternative parameterizations of the plug-in deviance. For ﬁxed
effects models we would expect from Section 3.2 that pD should be approximately the true
number of independent parameters. For the pooled model 1, pD = 1:0 as expected, whereas,
for the saturated model 5, pD ranges from 52.8 to 55.9 depending on the parameterization
that is used, which is close to the true value of 56 parameters. The models containing spatial
random effects (either with or without additional exchangeable effects) both have around 31
effective parameters, whereas the model with only exchangeable random effects has about 12
additional effective parameters. On the basis of the results of Section 5.2 comparing pD for
Poisson likelihoods with different priors, this suggests that the spatial model provides stronger
prior information than does the exchangeable model for these data.
Turning to the comparison of DIC for each model, we ﬁrst note that DIC is subject to Monte
Carlo sampling error, since it is a function of stochastic quantities generated under an MCMC
sampling scheme. Whereas computing the precise standard errors for our DIC values is a subject
of on-going research, the standard errors for the ¯D-values are readily obtained and provide a
good indication of the accuracy of DIC and pD. In any case, in several runs using different initial
values and random-number seeds for this example, the DIC and pD-estimates obtained never
varied by more than 0.5. As such, we are conﬁdent that, even allowing for Monte Carlo error,
either of models 3 or 4 is superior (in terms of DIC performance) to models 2 or 5, which are in
turn superior to model 1. A comparison of DIC for models 3 and 4 suggests that the two spatial
models are virtually indistinguishable in terms of the overall ﬁt: pragmatically, we might prefer
reporting model 3 since its DIC is only marginally greater than the more complex model 4.
Considering now the absolute measure of ﬁt suggested in Section 6.2, we compare the values
of ¯D in Table 1 with the sample size n = 56. This suggests that all models except the pooled
model 1 provide an adequate overall ﬁt to the data, and that the comparison is essentially based
on their complexity alone.
Following the discussion in Section 6, Fig. 4 shows a plot of deviance residuals dri against
leverages pDi for each of the ﬁve models considered. The broken curves marked on each plot are
of the form x2 + y = c and points lying along such a parabola will each contribute an amount
DICi = c to the overall DIC for that model. For models 2–5, parabolas are marked at values
of c = 1, 2, 5, and any data point whose contribution DICi is greater than 2 is labelled by its

608
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
deviance residual
leverage
-5
0
5
0.0
0.4
0.8
1.2
................................
.............
1
2
3
4 5 7
10
11
45
49
50
deviance residual
deviance residual
leverage
-2
-1
0
1
1
0.0
0.4
0.8
1.2
.....
..
..
...
.
.....................
....
...
...
..
..
1
2
34
53
54
55
56
leverage
-2
-1
0
0.0
0.4
0.8
1.2
.....
.
..
.
..
.
.
.
..
.
.
.
....
.
.
....
......
..
........
1
2
4
14
15
4549
50
54
55
56
deviance residual
leverage
-2
-1
0
1
0.0
0.4
0.8
1.2
......
.
..
.
..
.
.
.
..
.
.
.
....
.
.
....
......
..
.....
.
...
1
4
14
15
45
50
54
55
56
deviance residual
leverage
-2
-1
0
1
0.0
0.4
0.8
1.2
....................................................
..
(a)
(d)
(b)
(e)
(c)
Fig. 4.
Diagnostics for the lip cancer example—residuals versus leverages (the parabolas indicate contri-
butions of 1, 2 or 5 to the total DIC (apart from model 1): (a) model 1; (b) model 2; (c) model 3; (d) model 4;
(e) model 5
observation number. For model 1, parabolas are marked at c = 1, 10, 50, since the size of the
deviance residuals and individual contributions to DIC are much larger and, for clarity, only
points for which DICi is greater than 10 are marked by their observation number. Observations
55 and 56, the only districts with yi = 0, are clearly identiﬁed as potential outliers under each
of the random-effects models 2–4, as is observation 1 (the district with the highest observed
risk ratio yi=Ei). A few other observations (2, 3, 4, 53 and 54) have contributions DICi that
are just larger than 2 under model 2: with the exception of the three districts already discussed,
these ﬁve districts have the most extreme observed risk ratios and so their estimates tend to be
shrunk furthest under the exchangeable model. Observations 14, 15, 45 and 50 appear to be
outliers in models 3 and 4 which have a spatial effect, but not in the remaining models. A further
investigation reveals that the observed risk ratios in these districts are extreme compared with
those in each of their neighbouring districts. For example district 50 has only six cases compared
with 19.6 expected, whereas each of its three neighbouring districts have high observed counts
(17, 16 and 16) relative to those expected (7.8, 10.5 and 14.4). The spatial prior in models 3 and 4
causes the estimated rate in district 50 to be smoothed towards the mean of its neighbours’ rates,
thus leading to the discrepancy between observed and ﬁtted values, and since the observation still
exercises considerable weight on its ﬁtted value the leverage is high as well. However, overall we
might not consider that there is sufﬁcient evidence to cast doubt on any particular observations.
8.2.
Robust regression using the stack loss data
Spiegelhalter et al. (1996) (pages 27–29) considered a variety of error structures for the oft-
analysed stack loss data of Brownlee (1965). Here the response variable y, the amount of stack

Model Complexity and Fit
609
loss (escaping ammonia in an industrial application), is regresssed on three predictor variables:
air ﬂow x1, temperature x2 and acid concentration x3. Assuming the usual linear regression
structure
µi = β0 + β1zi1 + β2zi2 + β3zi3
where zij = .xij −¯x:j/=sd.x:j/, the standardized covariates, the presence of a few prominent
outliers among the n = 21 cases motivates a comparison of the following four error distributions:
model 1,
yi ∼normal.µi; τ−1/;
model 2,
yi ∼DE.µi; τ−1/;
model 3,
yi ∼logistic.µi; τ−1/;
model 4,
yi ∼td.µi; τ−1/
(where DE denotes the double-exponential (Laplace) distribution and td denotes Student’s t-
distribution with d degrees of freedom).
A well-known alternative to the direct ﬁtting of many symmetric but non-normal error dis-
tributions is through scale mixtures of normals (Andrews and Mallows, 1974). From page 210
of Carlin and Louis (2000), we have the alternate td-formulation model 5,
yi ∼normal

µi; 1
wiτ
	
;
wi ∼1
d χ2
d = gamma
d
2 ; d
2
	
:
Unlike our other examples the form of the likelihood changes with each model, so we must use
the full normalizing constants when computing −2 log{p.y|µ; τ/}.
Following Spiegelhalter et al. (1996) we set d = 4, and for each model we placed essentially
ﬂat priors on the βj (actually normal with mean 0 and precision 0.00001) and log.τ/ (actually
gamma(0.001,0.001) on τ) and ran the Gibbs sampler in BUGS for 5000 iterations following a
burn-in period of 1000 iterations.
Replacing τ and wi by their posterior means where necessary for the D. ¯θ/-calculation, the
resulting deviance summaries are shown in Table 2 (note that the mean parameterization and
the canonical parameterization are equivalent here, since the mean µi is a linear function of the
canonical β-parameters). Beginning with a comparison of the ﬁrst four models, the estimates of
pD are all just over 5, the correct number of parameters for this example. The DIC-values imply
that model 2 (double exponential) is best, followed by the t4-, the logistic and ﬁnally the normal
models. Clearly this order is consistent with the models’ respective abilities to accommodate
outliers.
Turning to the normal scale mixture representation for the t4-likelihood (model 5), the
pD-value is 7.6, suggesting that the wi random effects contribute only an extra 2–2.5 param-
eters. However, the model’s smaller DIC-value implies that the extra mixing parameters are

610
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
Table 2.
Deviance results for the stack loss data
Model
¯D
D(¯θ)
pD
DIC
1, normal
110.1
105.0
5.1
115.2
2, double exponential
107.9
102.3
5.6
113.5
3, logistic
109.5
104.2
5.3
114.8
4, t4
108.7
103.2
5.5
114.2
5, t4 as scale mixture
102.1
94.5
7.6
109.7
worthwhile in an overall quality-of-ﬁt sense. We emphasize that the results from models 4 and
5 need not be equal since, although they lead to the same marginal likelihood for the yi, they
correspond to different prediction problems.
Finally, plots of deviance residuals versus leverages (which are not shown) clearly identify the
observations determined to be ‘outlying’ by several previous researchers who analysed this data
set.
8.3.
Longitudinal binary observations: the six-cities study
To illustrate how the mean and canonical parameterizations (introduced in Section 5 and further
discussed in Section 9) can sometimes lead to different conclusions, our next example considers a
subset of data from the six-cities study, a longitudinal study of the health effects of air pollution:
see Fitzmaurice and Laird (1993) for the data and a likelihood-based analysis. The data consist
of repeated binary measurements yij of the wheezing status (1, yes; 0, no) of child i at time j,
i = 1; : : : ; I; j = 1; : : : ;J, for each of I = 537 children living in Stuebenville, Ohio, at J = 4
time points. We are given two predictor variables: aij, the age of child i in years at measurement
point j (7, 8, 9 or 10 years), and si, the smoking status of child i’s mother (1, yes; 0, no). Following
the Bayesian analysis of Chib and Greenberg (1998), we adopt the conditional response model
Yij ∼Bernoulli.pij/;
pij ≡Pr.Yij = 1/ = g−1.µij/;
µij = β0 + β1zij1 + β2zij2 + β3zij3 + bi;
where zijk = xijk −¯x::k;k = 1;2;3, and xij1 = aij, xij2 = si and xij3 = aijsi, a smoking–age
interaction term. The bi are individual-speciﬁc random effects, initially given an exchangeable
N.0;λ−1/ speciﬁcation, which allow for dependence between the longitudinal responses for
child i. The model choice issue here is to determine the most appropriate link function g.·/
among three candidates, namely the logit, the probit and the complementary log–log-links.
More formally, our three models are model 1,
g.pij/ = logit.pij/ = log{pij=.1 −pij/};
model 2,
g.pij/ = probit.pij/ = Φ−1.pij/;
and model 3,
g.pij/ = cloglog.pij/ = log{−log.1 −pij/}:

Model Complexity and Fit
611
Table 3.
Results for both parameterizations of the Bernoulli panel data
Model
¯D
Results for the canonical
Results for the mean
parameterization
parameterization
D(¯θ)
pD
DIC
D(¯θ)
pD
DIC
1, logit
1166.4
917.7
248.7
1415.1
997.5
168.9
1335.3
2, probit
1148.6
885.9
262.7
1411.3
989.9
158.7
1307.3
3, complementary log–log
1180.9
956.5
224.4
1405.3
1013.7
167.2
1348.1
Since the Bernoulli likelihood is unaffected by this choice, in all cases the deviance takes the
simple form
D = −2 
i;j
{yij log.pij/ + .1 −yij/ log.1 −pij/}:
Placing ﬂat priors on the βk and a gamma(0.001,0.001) prior on λ, and running the Gibbs sam-
pler for 5000 iterations following a burn-in period of 1000 iterations produces the deviance sum-
maries in Table 3 for the canonical and mean parameterizations: the canonical parameterization
constructs ¯θ as the mean of the linear predictors β and bi, and then uses the appropriate linking
transformation (logit, probit or complementary log–log) to obtain the imputed means for the pij.
The mean parameterization simply uses the means of the pij themselves when computing D. ¯θ/.
Natarajan and Kass (2000) have pointed out potential problems with the gamma(0.001,0.001)
prior on λ, but in this context the 537 random effects ensure that these ﬁndings are robust to
the choice of prior for λ.
The posterior standard deviation √λ−1 of the random effects is estimated to be 2.2 (standard
deviation 0.2), which indicates extremely high unexplained overdispersion and hence consider-
able prior–data conﬂict: this should warn us of a potential lack of robustness in our procedure.
We have a sample size of ni = 4 for each of I = 537 individuals, and an average pDi for the
canonical parameterization of around 0.4–0.5. From approximation (31), this indicates a prior
sample size a + b of around 4–6. Referring to the evidence in Fig. 1 concerning low prior and
observation sample sizes (ni = 1; a + b = 1), we might expect the mean parameterization to
display decreased complexity compared with the canonical, and this is borne out in the results.
DIC prefers the complementary log–log-link under the canonical parameterization, but the
probit link under the mean parameterization. We repeat that we prefer the canonical results
because of the improved normality of the likelihoods and their lack of dependence on observed
data: however, none of the models explain the data very well, and the lack of consensus suggests
caution in using any of the models.
9.
Discussion
Herewebrieﬂydiscussrelationshipstoothersuggestionsandgivesomeguidanceonthepractical
use of the techniques described in this paper.
9.1.
Relationship of pD and DIC to other suggestions
9.1.1.
Cross-validation
Stone (1977) showed the asymptotic equivalence of model comparison based on cross-validation

612
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
and AIC, whereas Wahba (1990) (page 52) showed how a generalized cross-validation criterion
leads to the use of n −tr.H/ as a denominator in the estimation of residual mean-squared error.
We would expect our measure of model complexity pD to be strongly related to cross-validatory
assessment, but this requires further investigation.
9.1.2.
Other predictive loss functions
Kass and Raftery (1995) criticized Akaike (1973) for using a plug-in predictive distribution as
we have done in Section 7.3, rather than the full predictive distribution obtained by integrating
out the unknown parameters. A criterion based on this predictive distribution is also invariant
to reparameterizations. Laud and Ibrahim (1995) and Gelfand and Ghosh (1998) suggested
minimizing a predictive ‘discrepancy measure’ E{d.Ynew; y/|y}; where Ynew is a draw from
the posterior predictive distribution p.Ynew|y/, and we might for instance take d.Ynew; y/ =
.Ynew −y/T.Ynew −y/. They showed that their measures also have attractive interpretations as
weighted sums of ‘goodness of ﬁt’ and ‘predictive variability penalty’ terms. However, a proper
choice of the criterion requires fairly involved analytic work, as well as several subjective choices
about the utility function that is appropriate for the problem at hand. Furthermore, the one-
way ANOVA model in Section 2.5 gives rise to a ﬁt term equivalent to D. ¯θ/, and a predictive
variability term equal to pD + p. Thus their suggestion is equivalent in this context to the
comparison by our Bayesian measure of ﬁt ¯D which, although invariant to parameterization,
does not seem to penalize complexity sufﬁciently.
In general the use of a plug-in estimate appears to ‘cost’ an extra penalty of pD.
9.1.3.
Bayes factors
Bayes factors are criteria based on a comparison of the marginal likelihoods (1) (Kass and
Raftery, 1995), and a common approximation is the Bayesian (or Schwarz) information criterion
(Schwarz, 1978), which for a model with p parameters and n observations is given by
BIC = −2 log{p.y| ˆθ/} + p log.n/:
Bernardo and Smith (1994) (chapter 6) argued that this formulation may only be appropriate
in circumstances where it was really believed that one and only one of the competing models
was in fact true, and the crucial issue was to choose this correct model, and that in other
circumstances criteria based on short-term prediction, such as cross-validation, may be more
appropriate. We support this view and refer to Han and Carlin (2001) for a review of some
of the computational and conceptual difﬁculties in using Bayes factors to compare complex
hierarchical models. Whether DIC can be justiﬁed as a basis for model averaging remains open
for investigation.
9.2.
Practical issues in using DIC
9.2.1.
Invariance
pD may be only approximately invariant to the chosen parameterization, since different ﬁtted
deviances D. ¯θ/ may arise from substituting posterior means of alternative choices of θ. The
example in Section 8.3 shows that this choice could be important with Bernoulli data.
In Section 5 we explored the use of the posterior median as an estimator leading to an invariant
pD. This has two possible disadvantages: we do not have a proof that pD will be positive and some
additional computational difﬁculty in that the full sample needs to be retained. In addition the
approximate properties based on Taylor series expansions in Section 3 may not hold, although

Model Complexity and Fit
613
this may be only of theoretical interest. Currently we recommend calculation of DIC on the basis
of several different estimators, with a preference for posterior means based on parameterizations
obeying approximate likelihood normality.
9.2.2.
Focus of analysis
As we saw in the stack loss example of Section 8.2, there may be sensitivity to apparently
innocuous restructuring of the model: this is to be expected since by making such changes we
are altering the deﬁnition of a replicate data set, and hence one would expect DIC to change.
For example, consider a model comprising a mixture of normal distributions. If this assumption
was solely to obtain a ﬂexible functional form, then the appropriate likelihood would comprise
the mixture. If, however, we were interested in the membership of individual observations,
then the likelihoods would be normal and the membership variables would contribute to the
complexity of the model. Thus the parameters in the focus of a model should ideally depend on
the purpose of the investigation, although in practice it is likely that the focus may be chosen
on computational grounds as providing likelihoods that are available in closed form.
9.2.3.
Nuisance parameters
Strictly speaking, nuisance parameters should ﬁrst be integrated out to leave a likelihood
depending solely on parameters in focus. In practice, however, parameters such as variances
are likely to be included in the focus and add to the estimated complexity: we would recommend
posterior means of log-variances as estimators.
9.2.4.
What is an important difference in DIC?
Burnham and Anderson (1998) suggested models receiving AIC within 1–2 of the ‘best’ deserve
consideration, and 3–7 have considerably less support: these rules of thumb appear to work
reasonably well for DIC. Certainly we would like to ensure that differences are not due to
Monte Carlo error: although this is straightforward for ¯D, Zhu and Carlin (2000) have explored
the difﬁculty of assessing the Monte Carlo error on DIC.
9.2.5.
Asymptotic consistency
As with AIC, DIC will not consistently select the true model from a ﬁxed set with increasing
sample sizes. We are not greatly concerned about this: we neither believe in a true model nor
would expect the list of models being considered to remain static as the sample size increased.
9.3.
Conclusion
In conclusion, our suggestions have a similar ‘information theoretic’ background to frequentist
measures of model complexity and criteria for model comparison but are based on expectations
with respect to parameters in place of sampling expectations. DIC can thus be viewed as a
Bayesian analogue of AIC, with a similar justiﬁcation but wider applicability. It is also applicable
to any class of model, involves negligible additional analytic work or Monte Carlo sampling
and appears to perform reasonably across a range of examples. We feel that pD and DIC deserve
further investigation as tools for model assessment and comparison.
Acknowledgements
We are very grateful for the generous discussion and criticism of the participants in the pro-

614
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
gramme on neural networks and machine learning that was held at the Isaac Newton Institute
for Mathematical Sciences in 1997, and to Andrew Thomas for so quickly implementing our
changing ideas into WinBUGS. NGB received partial support from Medical Research Coun-
cil grant G9803841, and BPC received partial support from National Institute of Allergy and
Infectious Diseases grant 1-R01-AI41966.
References
Akaike, H. (1973) Information theory and an extension of the maximum likelihood principle. In Proc. 2nd Int.
Symp. Information Theory (eds B. N. Petrov and F. Cs´aki), pp. 267–281. Budapest: Akad´emiai Kiad´o.
Andrews, D. F. and Mallows, C. L. (1974) Scale mixtures of normal distributions. J. R. Statist. Soc. B, 36, 99–102.
Berk, R. H. (1966) Limiting behaviour of posterior distributions when the model is incorrect. Ann. Math. Statist.,
37, 51–58.
Bernardo, J. M. (1979) Expected information as expected utility. Ann. Statist., 7, 686–690.
Bernardo, J. M. and Smith, A. F. M. (1994) Bayesian Theory. Chichester: Wiley.
Besag, J. (1974) Spatial interaction and the statistical analysis of lattice systems (with discussion). J. R. Statist.
Soc. B, 36, 192–236.
Biller, C. and Fahrmeir, L. (2001) Bayesian varying-coefﬁcient models using adaptive regression splines. Statist.
Modlng, 1, 195–211.
Box, G. E. P. (1976) Science and statistics. J. Am. Statist. Ass., 71, 791–799.
Breslow, N. E. and Clayton, D. G. (1993) Approximate inference in generalized linear mixed models. J. Am. Statist.
Ass., 88, 9–25.
Brownlee, K. A. (1965) Statistical Theory and Methodology in Science and Engineering. New York: Wiley.
Bunke, O. and Milhaud, X. (1998) Asymptotic behaviour of Bayes estimates under possibly incorrect models.
Ann. Statist., 26, 617–644.
Burnham, K. P. and Anderson, D. R. (1998) Model Selection and Inference. New York: Springer.
Carlin, B. P. and Louis, T. A. (2000) Bayes and Empirical Bayes Methods for Data Analysis, 2nd edn. Boca Raton:
Chapman and Hall–CRC Press.
Chib, S. and Greenberg, E. (1998) Analysis of multivariate probit models. Biometrika, 85, 347–361.
Clayton, D. G. and Kaldor, J. (1987) Empirical Bayes estimates of age-standardised relative risks for use in disease
mapping. Biometrics, 43, 671–681.
Dempster, A. P. (1974) The direct use of likelihood for signiﬁcance testing. In Proc. Conf. Foundational Questions
in Statistical Inference (eds O. Barndorff-Nielsen, P. Blaesild and G. Schou), pp. 335–352. Aarhus: University
of Aarhus.
(1997a) The direct use of likelihood for signiﬁcance testing. Statist. Comput., 7, 247–252.
(1997b) Commentary on the paper by Murray Aitkin, and on discussion by Mervyn Stone.Statist. Comput.,
7, 265–269.
Efron, B. (1986) How biased is the apparent error rate of a prediction rule? J. Am. Statist. Ass., 81, 461–470.
Erkanli,A.,Soyer,R.andAngold,A.(2001)Bayesiananalysesoflongitudinalbinarydatausingmarkovregression
models of unknown order. Statist. Med., 20, 755–770.
Erkanli, A., Soyer, R. and Costello, E. (1999) Bayesian inference for prevalence in longitudinal two-phase studies.
Biometrics, 55, 1145–1150.
Eubank, R. L. (1985) Diagnostics for smoothing splines. J. R. Statist. Soc. B, 47, 332–341.
Eubank, R. and Gunst, R. (1986) Diagnostics for penalized least-squares estimators. Statist. Probab. Lett., 4,
265–272.
Fitzmaurice, G. and Laird, N. (1993) A likelihood-based method for analysing longitudinal binary responses.
Biometrika, 80, 141–151.
Gelfand, A. E. and Dey, D. K. (1994) Bayesian model choice: asymptotics and exact calculations. J. R. Statist.
Soc. B, 56, 501–514.
Gelfand, A. E., Ecker, M. D., Christiansen, C., McLaughlin, T. J. and Soumerai, S. B. (2000) Conditional categor-
ical response models with application to treatment of acute myocardial infarction. Appl. Statist., 49, 171–186.
Gelfand, A. and Ghosh, S. (1998) Model choice: a minimum posterior predictive loss approach. Biometrika, 85,
1–11.
Gelfand, A. E. and Trevisani, M. (2002) Inequalities between expected marginal log likelihoods with implications
for likelihood-based model comparison. Technical Report. Department of Statistics, University of Connecticut,
Storrs.
Gilks, W. R., Richardson, S. and Spiegelhalter, D. J. (eds) (1996) Markov Chain Monte Carlo in Practice. New
York: Chapman and Hall.
Gilks, W. R., Wang, C. C., Coursaget, P. and Yvonnet, B. (1993) Random-effects models for longitudinal data
using Gibbs sampling. Biometrics, 49, 441–453.
Good, I. J. (1956) The surprise index for the multivariate normal distribution. Ann. Math. Statist., 27, 1130–1135.

Model Complexity and Fit
615
Green, P. and Richardson, S. (2002) Hidden Markov models and disease mapping. J. Am. Statist. Ass., to be
published.
Han, C. and Carlin, B. (2001) MCMC methods for computing Bayes factors: a comparative review. J. Am. Statist.
Ass., 96, 1122–1132.
Hastie, T. and Tibshirani, R. (1990) Generalized Additive Models. London: Chapman and Hall.
Hodges, J. and Sargent, D. (2001) Counting degrees of freedom in hierarchical and other richly-parameterised
models. Biometrika, 88, 367–379.
Huber, P. J. (1967) The behaviour of maximum likelihood estimates under non-standard conditions. In Proc. 5th
Berkeley Symp. Mathematical Statistics and Probability (eds L. M. LeCam and J. Neyman), vol. 1, pp. 221–233.
Berkeley: University of California Press.
Kass, R. and Raftery, A. (1995) Bayes factors and model uncertainty. J. Am. Statist. Ass., 90, 773–795.
Key, J. T., Pericchi, L. R. and Smith, A. F. M. (1999) Bayesian model choice: what and why? In Bayesian Statistics
6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 343–370. Oxford: Oxford University
Press.
Kimeldorf, G. and Wahba, G. (1970) A correspondence between Bayesian estimation on stochastic processes
and smoothing by splines. Ann. Math. Statist., 41, 495–502.
Kullback, S. and Leibler, R. A. (1951) On information and sufﬁciency. Ann. Math. Statist., 22, 79–86.
Laird, N. M. and Ware, J. H. (1982) Random effects models for longitudinal data. Biometrics, 38, 963–974.
Laud, P. W. and Ibrahim, J. G. (1995) Predictive model selection. J. R. Statist. Soc. B, 57, 247–262.
Lee, Y. and Nelder, J. A. (1996) Hierarchical generalized linear models (with discussion). J. R. Statist. Soc. B,
58, 619–678.
van der Linde, A. (1995) Splines from a Bayesian point of view. Test, 4, 63–81.
(2000) Reference priors for shrinkage and smoothing parameters. J. Statist. Planng Inf., 90, 245–274.
Lindley, D. V. and Smith, A. F. M. (1972) Bayes estimates for the linear model (with discussion). J. R. Statist.
Soc. B, 34, 1–44.
MacKay, D. J. C. (1992) Bayesian interpolation. Neur. Computn, 4, 415–447.
(1995) Probable networks and plausible predictions—a review of practical Bayesian methods for super-
vised neural networks. Netwrk Computn Neur. Syst., 6, 469–505.
McCullagh, P. and Nelder, J. (1989) Generalized Linear Models, 2nd edn. London: Chapman and Hall.
Meng, X.-L. and Rubin, D. B. (1992) Performing likelihood ratio tests with multiply imputed data sets. Bio-
metrika, 79, 103–112.
Moody, J. E. (1992) The effective number of parameters: an analysis of generalization and regularization in
nonlinear learning systems. In Advances in Neural Information Processing Systems 4 (eds J. E. Moody, S. J.
Hanson and R. P. Lippmann), pp. 847–854. San Mateo: Morgan Kaufmann.
Murata, N., Yoshizawa, S. and Amari, S. (1994) Network information criterion—determining the number of
hidden units for artiﬁcial neural network models. IEEE Trans. Neur. Netwrks, 5, 865–872.
Natarajan, R. and Kass, R. E. (2000) Reference Bayesian methods for generalised linear mixed models. J. Am.
Statist. Ass., 95, 227–237.
Raghunathan, T. E. (1988) A Bayesian model selection criterion. Technical Report. University of Washington,
Seattle.
Rahman, N. J., Wakeﬁeld, J. C., Stephens, D. A. and Falcoz, C. (1999) The Bayesian analysis of a pivotal
pharmacokinetic study. Statist. Meth. Med. Res., 8, 195–216.
Richardson, S. and Green, P. J. (1997) On Bayesian analysis of mixtures with an unknown number of components
(with discussion). J. R. Statist. Soc. B, 59, 731–792.
Ripley, B. D. (1996) Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press.
Sawa, T. (1978) Information criteria for choice of regression models: a comment. Econometrica, 46, 1273–1291.
Schwarz, G. (1978) Estimating the dimension of a model. Ann. Statist., 6, 461–466.
Slate, E. (1994) Parameterizations for natural exponential-families with quadratic variance functions. J. Am.
Statist. Ass., 89, 1471–1482.
Spiegelhalter, D. J., Thomas, A. and Best, N. G. (2000) WinBUGS Version 1.3 User Manual. Cambridge: Medical
Research Council Biostatistics Unit. (Available from http://www.mrc-bsu.cam.ac.uk/bugs.)
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Gilks, W. R. (1996) BUGS Examples Volume 1, Version 0.5
(Version ii). Cambridge: Medical Research Council Biostatistics Unit.
Stone, M. (1977) An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. J. R.
Statist. Soc. B, 39, 44–47.
Takeuchi, K. (1976) Distribution of informational statistics and a criterion for model ﬁtting (in Japanese).
Suri-Kagaku, 153, 12–18.
Vehtari, A. and Lampinen, J. (1999) Bayesian neural networks with correlated residuals. In IJCNN’99: Proc.
1999 Int. Joint Conf. Neural Networks. New York: Institute of Electrical and Electronic Engineers.
Wahba, G. (1978) Improper priors, spline smoothing and the problem of guarding against model errors in
regressions. J. R. Statist. Soc. B, 40, 364–372.
(1983) Bayesian “conﬁdence intervals” for the cross-validated smoothing spline. J. R. Statist. Soc. B, 45,
133–150.

616
D. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. van der Linde
(1990) Spline Models for Observational Data. Philadelphia: Society for Industrial and Applied Mathe-
matics.
Ye, J. (1998) On measuring and correcting the effects of data mining and model selection. J. Am. Statist. Ass.,
93, 120–131.
Ye, J. and Wong, W. (1998) Evaluation of highly complex modeling procedures with binomial and Poisson data.
Technical Report. Graduate School of Business, University of Chicago, Chicago.
Zeger, S. L. and Karim, M. R. (1991) Generalised linear models with random effects; a Gibbs sampling approach.
J. Am. Statist. Ass., 86, 79–86.
Zhu, L. and Carlin, B. (2000) Comparing hierarchical models for spatio-temporally misaligned data using the
deviance information criterion. Statist. Med., 19, 2265–2278.
Discussion on the paper by Spiegelhalter, Best, Carlin and van der Linde
S. P. Brooks .University of Cambridge/
This is a wonderful paper containing a wide array of interesting ideas. It seems to me very much like a ﬁrst
step (and in the right direction) and I am sure that it will be seen as both a focus and a source of inspiration
for future developments in this area.
As the authors point out, their pD and the deviance information criterion (DIC) statistics have al-
ready been widely used within the Bayesian literature. Given this history and in the previous absence of
a published source for these ideas, it is easy to misunderstand what pD actually does. Certainly, before
reading this paper, but having read several others which use the DIC, I thought that the pD-statistic was
a clever way of avoiding the problem that Bayesians have when it comes to calculating the number of
parameters in any hierarchical model. Essentially the problem is one of deciding which variables in the
posterior are model parameters and which are hyperparameters arising from the prior. However, pD does
not help us here and that is why we have Section 2.1 explaining that this choice is up to the reader. The
authors refer to this as choosing the ‘focus’ for the analysis. Sadly, in many cases the calculation of pD will
be impossible for the focus of primary interest since the deviance will not be available in closed from (this
includes random effects and state space models, for example), so this remains an open problem.
What pD does do is to tell you, once you have chosen your focus, how many parameters you lose (or
even gain?) by being Bayesian. The number of degrees of freedom (or parameters) in a model is clear from
the (focused) likelihood. However, by combining the likelihood with the prior we almost always impose
additional restrictions on the parameter space, effectively reducing the degrees of freedom of our model.
Take the authors’ saturated model of Section 8.1, in which parameters α1; : : :; α56 are given a prior with
some unknown mean µ and ﬁxed variance σ2. Clearly, in the limit as σ2 goes to 0, we essentially remove
the 56 individual parameters αi and effectively replace them with a single parameter µ. I guess that this is
fairly obvious with hindsight as is the case with many great ideas. None-the-less it is a credit to the authors
ﬁrstly for seeing it and, more importantly, for actually deriving a procedure for dealing with it.
This prior-induced parameter reduction can be clearly observed in Fig. 5 in which we plot the value
of pθ
D against log(σ2) both for a hyperprior µ ∼N.0; 1000/ and for µ = 0 (the authors are unclear about
which, if either, they actually use in Section 8.1). We can see that, as σ2 decreases, the effective number of
parameters decreases to either 1 or 0 depending on whether or not µ itself is a parameter, i.e. which prior
is chosen. It is interesting to note the rapid decline in pD for variances between 1 and 0.01, but what is
particularly interesting about this plot is that, as σ2 increases, pD converges to a ﬁxed maximum well
below 56, the number of parameters in the likelihood. As an experiment, if we take σ2 = 1030 or even
the Jeffreys prior for the µi, a value for pD exceeding 53.1 is never obtained (modulo Monte Carlo error).
This suggests that we automatically lose three parameters just by being Bayesian, even if we are as vague
as we could possibly be with our prior. Quoting Bernardo and Smith (1994), page 298, ‘every prior spe-
ciﬁcation has some informative posterior or predictive implications : : :. There is no “objective” prior that
represents ignorance.’ Of course, the authors’ Table 1 suggests that if we took the median as the basis for
the calculation of pD then we might obtain different results; indeed we seem to regain several parameters
this way! Unfortunately, analytic investigation of the pD-statistic is essentially limited to the case where
we take ˜θ.y/ to be the posterior mean, so we have little idea of the extent and nature of the variability
across parameterizations. This choice is likely to have a significant effect on any inference based on the
corresponding pD-statistic and further (no doubt simulation-based) investigation along these lines would
certainly be very helpful.
As well as the construction of the pD-statistic, the paper also derives a new criterion for model com-
parison labelled the DIC. The authors provide a heuristic justiﬁcation for the DIC, but there are clearly
several alternatives. One obvious extension of the usual Akaike information criterion (AIC) statistic to

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
617
p
D
Fig. 5.
Plot of pθ
D for the saturated model of Section 8.1 demonstrating its dependence on the prior variance
for the random effects:
, pD-statistic with an N.0, 1000/ hyperprior for µ: - - - - -, corresponding value
when we ﬁx µ D 0I . . . . . . . , number of parameters in the likelihood
the Bayesian context is to calculate its posterior expectation, EAIC = D.θ/+2p (rather than evaluating it
at the posterior mode under a ﬂat prior), or to take the deviance calculated at the posterior mean, i.e.
taking D.¯θ/ + 2p. Of course, as with the DIC, posterior medians, modes etc. could also be taken and
similar extensions could be applied to the corrected AIC statistic and the Bayesian information criterion
for example. Further, the number of parameters in each of these expressions might be replaced by pD to
gain even more potential criteria. Table 4 gives the posterior model probabilities and posterior-averaged
information criteria (based on p, rather than pD), including DIC, for autoregressive models of various
orders ﬁtted to the well-known lynx data (Priestley (1981), section 5.5). We note the broad agreement
between the DIC, EAIC and EAICc (as is common in my own experience and, I think, expected by the
authors), but that EBIC locates an entirely different model. We note also that the posterior model prob-
abilities correctly identify the fact that two models appear to describe the data well and it is the only
criterion to identify correctly the existence of two distinct modes in the posterior.
Given the number of approximations and assumptions that are required to obtain the DIC it can only
really be used as a broad brush technique for discriminating between obviously disparate models, in much
the same way as any of the alternative information criteria suggested above might be used. However, in
many realistic applications there may be two or more models with sufﬁciently similar DIC that it is im-
possible to choose between the two. The only sensible choice in this circumstance is to model-average (see
Section 9.1.3). Burnham and Anderson (1998), section 4.2, suggested the use of AIC weights and these
are also given in Table 4 together with the corresponding weights for the other criteria. Essentially, these
are obtained by subtracting from each AIC the value associated with the ‘best’ model and then setting
wk ∝exp{−∆AIC.k/=2}
where ∆AIC.k/ denotes the transformed AIC-value for model k. These weights are then normalized to
sum to 1 over the models under consideration.
Note the distinct differences between the weights and the posterior model probabilities given in
Table 4, suggesting that only one or the other can really make any sense. We note here that similar
comparisons have been made in the context of other examples. In the context of a log-linear contingency

618
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
Table 4.
Effective number of parameters, values of DIC and the posterior expectation of various information
criteria for ﬁtting an autoregressive model of order k (with k C 1 parameters including the error variance) to
the lynx data†
k
pD
DIC
EAIC
EBIC
EAICc
π(K = k)
wDIC
k
wEAIC
k
wEBIC
k
wEAICc
1
1.88
206.66
206.78
209.51
206.81
0.000
0.000
0.000
0.000
0.000
2
2.85
126.58
127.72
133.19
127.83
0.243
0.000
0.003
0.858
0.011
3
3.78
127.06
129.27
137.48
129.50
0.016
0.000
0.001
0.101
0.005
4
4.76
125.52
128.75
139.70
129.12
0.007
0.000
0.002
0.033
0.006
5
5.70
125.23
129.52
143.20
130.08
0.002
0.000
0.001
0.006
0.004
6
6.62
126.30
131.68
148.09
132.46
0.001
0.000
0.004
0.000
0.001
7
7.60
122.34
128.72
147.88
129.78
0.002
0.000
0.002
0.001
0.004
8
8.61
121.81
129.19
151.08
130.56
0.002
0.000
0.001
0.000
0.003
9
9.58
122.75
131.16
155.79
132.89
0.001
0.000
0.001
0.000
0.001
10
10.54
118.94
128.40
155.76
130.53
0.002
0.001
0.002
0.000
0.003
11
11.33
106.51
117.16
147.26
119.75
0.154
0.431
0.566
0.001
0.624
12
12.61
106.89
118.27
151.10
121.36
0.268
0.356
0.325
0.000
0.280
13
13.56
108.74
121.17
156.74
124.81
0.135
0.142
0.076
0.000
0.050
14
14.46
110.77
124.30
162.61
128.54
0.067
0.051
0.016
0.000
0.008
15
15.37
112.896
127.42
168.47
132.32
0.000
0.019
0.003
0.000
0.001
†Criterion entries in bold indicate the model minimizing the relevant criterion, whereas those in italics denote
alternative plausible models under the rules of thumb discussed in Section 9.2.4. Probabilities π or weights w in
bold denote the top two models in each case. Here, EAICc denotes the posterior mean of the corrected EAIC
(Burnham and Anderson, 1998), π.K = k/ the corresponding posterior model probability under a ﬂat prior across
models and the wX
k the corresponding Akaike weights (or equivalent). The posterior model probabilities were
kindly provided by Ricardo Ehlers.
table analysis, King (2001), Table 2.5, found that two models have posterior probability 0.557 and 0.057
but corresponding DIC weights of 0.062 and 0.682 respectively. Similar examples in which the DIC and
posterior model probabilities give wildly different results are provided by King and Brooks (2001). Do
the authors have any feel for why these two approaches might give such different results? Which would
they recommend be used and do they have any suggestions for alternative DIC-based weights for model
averaging which might lead to more sensible results? Surely, the only sensible approach is to calculate
posterior model probabilities via transdimensional Markov chain Monte Carlo methods. When, then, do
the authors suggest that the DIC might be used? What, in practical terms is the question that the DIC is
answering as opposed to the posterior model probabilities?
The incorporation of the DIC-statistic into WinBUGS 1.4 ensures its ultimate success, but I have grave
misgivings concerning the blind application of a ‘default’ DIC-statistic for model determination prob-
lems particularly given its heuristic derivation and the series of essentially arbitrary assumptions and
approximations on which it is based. The authors ‘recommend calculation of DIC on the basis of several
different estimators’. The option to choose different parameterizations is not available in the beta version
of WinBUGS 1.4; will it be added to later versions? What about options for the all-important choice of
focus? What do the authors suggest we do when the same parameterization is not calculable for all models
being compared? Could not the choice of parameterization for each model adversely inﬂuence the results,
particularly for models with large numbers of parameters (where a small percentage change in pD might
mean a large absolute change in the corresponding DIC)?
The paper, like any good discussion paper, leaves various other open questions. For example: why take
Eθ|y[dΘ] in equation (9) and not the mode or median; how should we decide when to take ˆθ to be the mean,
median, mode etc. as this will surely lead to different comparative results for the DIC; when is pD negative
and why; in an entirely practical sense, how does model comparison with the DIC compare with that via
posterior model probabilities and why do they differ—can both be ‘correct’ in any meaningful way? On
page 613, the authors write ‘pD and DIC deserve further investigation as tools for model assessment and
comparison’ and I would certainly agree that they do. I have very much enjoyed thinking about some
of these ideas over the past few weeks and I am very grateful to the authors for the opportunity and
motivation to do so. It therefore gives me great pleasure to propose the vote of thanks.

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
619
Jim Smith .University of Warwick, Coventry/
I shall not address technical inaccuracies but just present four foundational problems that I have with the
model selection in this paper.
(a) Bayesian models are designed to make plausible predictive statements about future observables.
The predictive implications of all the prior settings on variances in the worked examples in Section
8 are unbelievable. They do not represent carefully elicited expert judgments but the views of a vac-
uous software user. Early in Section 1 the authors state that they want to identify succinct models
‘which appear to describe the information [about wrong “true” parameter values (see Section 2.2)?]
in the data accurately’. But in a Bayesian analysis a separation between information in the data
and in the prior is artiﬁcial and inappropriate. For example where do I input extraneous data used
as the basis of my prior? When do I stop calling this data (and so include it in D.·// and instead
call it prior information? This forces the authors to use default priors.
A Bayesian analysis on behalf of a remote auditing expert (Smith, 1996) might require the selec-
tion of a prior that is robust within a class of belief of different experts (e.g. Pericchi and Walley
(1991)). Default priors can sometimes be justiﬁed for simple models. Even then, models within a
selection class need to have compatible parameterizations: see Moreno et al. (1998). However, in
examples where ‘the number of parameters outnumbers observations’—they claim their approach
addresses—default priors are unlikely to exhibit any robustness. In particular, outside the domain
of vague location estimation or separating variance estimation (discussed in Section 4), apparently
default priors can have strong inﬂuence on model implications and hence selection.
(b) Suppose that we need to select models whose predictive implications we do not believe. Surely we
should try to ensure that prior information in each model corresponds to predictive statements
that are comparable. Such issues, not addressed here, are considered by Madigan and Raftery
(1991) for simple discrete Bayesian models. But outside linear models with known variances this is
a difﬁcult problem. Furthermore it is well known that calibration is a fast function (Cooke, 1991).
In particular apparently inconsequential deviations from the features of a model ‘not in focus’
tend to dominate D.θ/ and D.θ/. A trivial example of this occurs when we plan to forecast X2
having observed an independent identically distributed X1 = 0:01 which under models M1 and
M2 have respective Gaussian distributions N.100; 10000/ and N.0; 0:001/. Then, for most priors,
model M1 is strongly preferred although its predictions about X2 are less ‘useful’ (Section 2.2).
The authors’ premise that all the models they entertain are ‘wrong’ allows these calibration issues
to bite theoretically even in the limit, unlike their asymptotically consistent rivals. The authors,
however, do no more than to acknowledge the existence of this core difﬁculty after the example in
Section 8.3.
(c)
Suppose that problems (a) and (b) do not bite. Then the ‘vector of parameters of focus’ (POF)
will have a critical inﬂuence on any ensuing inference. How in practice do we specify this? The
authors state without elaboration that this ‘should depend on the purpose of the investigation’
(Section 9.2.2). But it appears that in practice the POF is calculated on ‘computational grounds’,
their software capability driving their inference.
The high inﬂuence of the choice of the POF is illustrated in the example in Section 8.2. Here
models 4 and 5 are predictively identical but model 5 has a significantly smaller deviance infor-
mation criterion DIC than model 4. The authors conclude that ‘the extra mixing parameters are
worthwhile’: why? In what practical sense is this helpful? This example illustrates that the unguided
choice of the POF will often be inferentially critical. Incidentally in this example the order of DIC
is not (as stated) consistent with the thickness of tails of the sample distribution, the thickest-tailed
distribution being model 4.
(d) But ignoring all these difﬁculties there still remains the acknowledged choice of (re)parameteriza-
tion governing the choice of ¯θ which initially we shall assume to be the mean. Consider the case
when the POF θ is one dimensional with strictly increasing posterior distribution function F.θ|y/,
and Gµ is a distribution function of a random variable with mean µ. Then the reparameterization
of θ to φµ = G−1
µ {F.θ|y/} has E.φµ/ = µ. Thus D.¯θ/ (or D.¯φ// is arbitrary within the range of
D.·/. Thus, contrary to Section (5.1.4), the choice of parameterization of θ with non-degenerate
posterior will always be critical. But no general selection guidance is given here. In observation (c)
of Section 2.6 the authors suggest the use of the posterior median instead of the mean if this can
be calculated easily from their output: not a solution when the POF is more than one dimensional.
Even familiar transforms of marginal medians to contrasts and means or means and variances to
means and coefﬁcients of variation will not exhibit the required sorts of invariance.

620
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
There may be theoretical reasons to use DIC but I do not believe that this paper gives them. So my
suggestion to a practitioner would be: if you must use a formal selection criterion do not use DIC. I second
the vote of thanks.
The vote of thanks was passed by acclamation.
Aki Vehtari .Helsinki University of Technology/
The authors mention that the deviance information criterion DIC estimates the expected loss, with de-
viance as the loss function. This connection should be emphasized more. It should be remembered that
the estimation of the expected deviance was Akaike’s motivation for deriving the very ﬁrst information
criterion AIC (Akaike, 1973). In prediction and decision problems, it is natural to assess the predictive
ability of the model by estimating the expected utilities, as the principle of rational decisions is based on
maximizing the expected utility (Good, 1952) and the maximization of expected likelihood maximizes the
information gained (Bernardo, 1979). It is often useful to use other than likelihood-based utilities. For
example, in classiﬁcation problems it is much more meaningful for the application expert to know the
expected classiﬁcation accuracy than just the expected deviance value (Vehtari, 2001). Given an arbitrary
utility function u, it is possible to use Monte Carlo samples to estimate Eθ[¯u.θ/] and ¯u.Eθ[θ]/, and then to
compute an expected utility estimate as
¯uDIC = ¯u.Eθ[θ]/ + 2{Eθ[¯u.θ/] −¯u.Eθ[θ]/};
which is a generalization of DIC (Vehtari, 2001).
The authors also mention the known asymptotic relationship of AIC to cross-validation (CV). Equally
important is to note that the same asymptotic relationship holds also for NIC (Stone (1977), equation
(4.5)). The asymptotic relationship is not surprising, as it is known that CV can also be used to estimate
expected utilities with Bayesian justiﬁcation (Bernardo and Smith (1994), chapter 6, Vehtari (2001) and
Vehtari and Lampinen (2002a)). Below some main differences between CV and DIC are listed. See Vehtari
(2001) and Vehtari and Lampinen (2002b) for full discussion and empirical comparisons. CV can use full
predictive distributions. In the CV approach, there are no parameterization problems, as it deals directly
with predictive distributions. CV estimates the expected utility directly, but it can also be used to estimate
the effective number of parameters if desired. In the CV approach, it is easy to estimate the distributions
of the expected utility estimates, which can for example be used to determine automatically whether the
difference between two models is ‘important’. Importance sampling leave-one-out CV (Gelfand et al.,
1992; Gelfand, 1996) is computationally as light as DIC, but it seems to be numerically more unstable.
k-fold CV is very stable and reliable, but it requires k times more computation time to use. k-fold CV can
also handle ﬁnite range dependences in the data. For example, in the six-cities study, the wheezing statuses
of a single child at different ages are not independent. DIC, which assumes independence, underestimates
the expected deviance. In k-fold CV it is possible to group the dependent data and to handle independent
groups and thus to obtain better estimates (Vehtari, 2001; Vehtari and Lampinen, 2002b).
Martyn Plummer .International Agency for Research on Cancer, Lyon/
I congratulate the authors on their thought-provoking paper. I would like to offer one constructive sug-
gestion and one criticism.
Firstly, I have a proposal for a modiﬁed definition of the effective number of parameters pD. Starting
from the Kullback–Leibler information divergence between the predictive distributions at two different
values of θ
I.θ0; θ1/ = EYrep|θ0

log
p.Yrep|θ0/
p.Yrep|θ1/

;
I suggest that pD be deﬁned as the expected value of I.θ0; θ1/ when θ0 and θ1 are independent samples from
the posterior distribution of θ. This modiﬁed definition yields exactly the same expression for pD in the
normal linear model with known variance. In general, it should give a similar estimate of pD when θ has
an asymptotic normal distribution. This version of pD can also be decomposed into inﬂuence diagnostics
when the likelihood factorizes as in Section 6.3. It has the theoretical advantages of being non-negative
and co-ordinate free. A practical advantage is that pD can be estimated via Markov chain Monte Carlo
sampling using two parallel chains by taking the sample average of
log

p.Y 0
rep|θ0/
p.Y 0
rep|θ1/


Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
621
where the superscript denotes the chain to which each quantity belongs. The Monte Carlo error of this
estimate is easily calculated and the difﬁculties discussed by Zhu and Carlin (2000) can thus be avoided.
For exponential family models, I.θ0; θ1/ can be expressed in closed form and there is no need to simulate
replicate observations Yrep. When the scale parameter φ is known, the expression for pDi simpliﬁes to
pDi = niwi cov{θi; µ.θi/|Y} =φ:
This gives a surprising resolution to the problem of whether to use the canonical or mean parameterization
to estimate pD.
On a more negative note, I am not convinced by the heuristic derivation of the deviance information
criterion DIC in Section 7.3. I followed this derivation for the linear model of Section 4.1, for which it is
not necessary to make any approximations. The term with expectation 0, neglected in the ﬁnal expression,
is p −pD −D.¯θ/. Adding this to DIC gives an expected loss of p + pD which is not useful as a model
choice criterion. I am not suggesting that the use of DIC is wrong, but a formal derivation is lacking.
Mervyn Stone .University College London/
The paper is rather economical with the ‘truth’. The truth of pt.Y/ corresponds ﬁxedly to the conditions
of the experimental or observational set-up that ensures independent future replication Yrep or internal
independence of y = y = .y1; : : :; yn/ (not excluding an implicit concomitant x). For pt.Y/ ≈p.Y|θt/; θ
must parameterize a scientifically plausible family of alternative distributions of Y under those conditions
and is therefore a necessary ‘focus’ if the ‘good [true] model’ idea is to be invoked: think of tossing a bent
coin. Changing focus is not an option.
Any connection of pD with cross-validatory assessment would need truth as pt.y/ = pt.y1/: : :pt.yn/.
If l = log.p/ is an acceptable measure of predictive success, A = Σi l.yi|˜θ−i/ is a one-out estimate of
Ept.Y/[Σi l{Yi|˜θ.y/}]. Multiplied by −2, this connects with equation (33) only when the θ-model is true
with Y1; : : :; Yn independent.
Extending Stone (1977) to the posterior mode for prior p.θ/, with n large, A ≈L˜θ.y/ −Π.y/ where
Π.y/ = −tr{L′′
˜θ + l′′.˜θ/}
−1 
i
l′
˜θ.yi/l′
˜θ.yi/T
and l.θ/ = log {p.θ/}. If l′′.˜θ/ is negative definite, the typically non-negative penalty Π.y/ is smaller for
the posterior mode than for the maximum likelihood estimate. For the maximum likelihood estimate,
l′′.˜θ/ = O gives Π.y/ estimating pÅ, but the general form probably gives Ripley’s pÅ.
If Section 7.3 could be rigorously developed (the use of EY does look suspicious!), another connection
(via equation (33)) might be that DIC ≈−2A. But, since Section 7.3 invokes the ‘good model’ assumption
and small |˜θ −θ| for the Taylor series expansion (i.e. large n), such a connection would be as contrived
as that of A with the Akaike information criterion: why not stick with the pristine (nowadays calculable)
form of A—which does not need large n or truth, and which accommodates estimation of θ at the inde-
pendence level of a hierarchical Bayesian model? If sensitivity of the logarithm to negligible probabilities
is objectionable, Bayesians should be happy to substitute a subjectively preferable measure of predictive
success.
Christian P. Robert .Universit´e Paris Dauphine/ and D. M. Titterington .University of Glasgow/
A question that arises regarding this thought challenging paper was actually raised in the discussion of
Aitkin (1991), namely that the data seem to be used twice in the construction of pD. Indeed, y is used the
ﬁrst time to produce the posterior distribution π.θ|y/ and the associated estimate ˜θ.y/. The (Bayesian)
deviance criterion then computes the posterior expectation of the observed likelihood p.y|θ/,

log {p.y|θ/} π.dθ|y/ ∝

log {p.y|θ/} p.y|θ/ π.dθ/;
and thus uses y again, similarly to Aitkin’s posterior Bayes factor

p.y|θ/ π.dθ|y/:
This repeated use of y would appear to be a potential factor for overﬁtting.
It thus seems more pertinent (within the Bayesian paradigm) to follow an integrated approach along the
lines of the posterior expected deviance of Section 6.2,

622
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde

EY|θ[−2 log{p.Y|θ/} + 2 log{f.Y/}]π.dθ|y/
because this quantity would be strongly related to the posterior expected loss deﬁned by the logarithmic
deviance,
d.θ; ˜θ/ = EY|θ[log{p.Y|θ/} −log{p.Y|˜θ/}];
advocated in Robert (1996) and Dupuis and Robert (2002) as an intrinsic loss adequate for model ﬁtting.
In fact, the connection between pD, the deviance information criterion and the logarithmic deviance would
suggest the use of this loss d.θ; ˜θ/ to compute the estimate plugged in pD as the intrinsic Bayes estimator
θπ.y/ = arg min
˜θ
{Eθ|y.EY|θ[log{p.Y|θ/} −log{p.Y|˜θ/}]/}
= arg max[EY|y{p.Y|˜θ/}]
where the last expectation is computed under the predictive distribution. Not only does this make sense
because of the aforementioned connection, but it also provides an estimator that is completely invariant
to reparameterization and thus avoids the possibly difﬁcult choice of the parameterization of the problem.
(See Celeux et al. (2000) for an illustration in the set-up of mixtures.)
J. A. Nelder .Imperial College of Science, Technology and Medicine, London/
My colleague Professor Lee has made some general points connecting the subject of this paper to our
work on likelihood-based hierarchical generalized linear models. I want to make one specific point and
two general ones.
(a) Professor Dodge has shown that, of the 21 observations in the stack loss data set, only ﬁve have
not been declared to be outliers by someone! Yet there is a simple model in which no observation
appears as an outlier. It is a generalized linear model with gamma distribution, log-link and linear
predictor x2 + log.x1/Å log.x3/: This gives the following entries for Table 2 in the paper
98:3
92:1
6:2
104:5
(I am indebted to Dr Best for calculating these). It is clearly better than the existing models used
in Table 2.
(b) This example illustrates my ﬁrst general point. I believe that the time has passed when it was enough
to assume an identity link for models while allowing the distribution only to change. We should
take as our base-line set of models at least the generalized linear model class deﬁned by distribution,
link and linear predictor, with choice of scales for the covariates in the last named.
(c)
My second general point is that there is, for me, not nearly enough model checking in the paper
(I am assuming that the use of such techniques is not against the Bayesian rules). For example, if a
set of random effects is sufﬁciently large in number and the model postulates that they are normally
distributed, their estimates should be graphed to see whether they look like a sample from such a
distribution. If they look, for example, strongly bimodal, then the model must be revised.
Anthony Atkinson .London School of Economics and Political Science/
This is an interesting paper which tackles important problems. In my comments I concentrate on regression
models: the points extend to the more complicated models at the centre of the authors’ presentation.
It is stressed in Section 7.1 that information criteria assume a replication of the observations; in regres-
sion this would be with the same X-matrix. But, the simulations of Atkinson (1980) showed that, to predict
over a different region, higher values of the penalty coefﬁcient than two in equation (36) are needed. Do
the authors know of any analytical results in this area?
Information criteria for model selection are based on aggregate statistics. Fig. 4 shows an alternative
and more informative breakdown of one criterion into the contributions of individual observations than
that given by Weisberg (1981). However, it does not show the effect of the deletion of observations on
model choice. Atkinson and Riani (2000) used the forward search to analyse the stack loss data, for which
symmetrical error distributions were considered in Section 8.2. Their Fig. 4.28 shows that the square-root
transformation is the only one supported by all the data. The forward plot of residuals, Fig. 3.27, is stable,
with observations 4 and 21 outlying. This diagnostic technique complements the choice of a model using
information criteria calculated over a set of models that is too narrow.

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
623
Fig. 6.
Transformed surgical unit data: forward plot of the four added variable t-statistics: three variables
are needed in the model—x4 is not significant
An example of model choice potentially confounded by the presence of several outliers is provided by
108 observations on the survival of patients following liver surgery from Neter et al. (1996), pages 334 and
438. There are four explanatory variables. Fig. 6 shows the evolution of the added variable t-tests for the
variables during the forward search with log(survival time) as the response: the evidence for the impor-
tance of all variables except x4 increases steadily during the search. Atkinson and Riani (2002) modify
the data to produce two different effects. The forward plots of the t-tests in Fig. 7(a) show that now x1
(a)
(b)
Fig. 7.
Modiﬁed transformed surgical unit data: (a) outliers render x1 non-significant; (b) now the outliers
make x4 significant (both (a) and (b) show forward plots of added variable t-statistics)

624
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
is non-significant at the end of the search. The plot identiﬁes the group of modiﬁed observations which
have this effect on the t-test for x1. Fig. 7(b) shows the effect of a different contamination, which makes
x4 significant at the end of the search.
The use of information criteria in the selection of models is a ﬁrst step, which needs to be complemented
by diagnostic tests and plots. These examples show that the forward search is an extremely powerful tool
for this purpose. It also requires many ﬁts of the model to subsets of the data. Can it be combined with
the appreciable computations of the authors’ Markov chain Monte Carlo methods?
A. P. Dawid .University College London/
This paper should have been titled ‘Measures of Bayesian model complexity and ﬁt’, for it is the models,
not the measures, that are Bayesian. Once the ingredients of a problem have been speciﬁed, any relevant
question has a unique Bayesian answer. Bayesian methodology should focus on speciﬁcation issues or on
ways of calculating or approximating the answer. Nothing else is required.
Classical criteria overﬁt complex models, necessitating some form of penalization, and this paper lies
ﬁrmly in that tradition. But with Bayesian techniques (Kass and Raftery, 1995) overﬁtting is not a problem:
the marginal likelihood automatically penalizes model complexity without any need for further adjust-
ment. In particular, Bayesian model choice is consistent in the ‘good model’ case (Dawid, 1992a). In
Section 9.2.5 the authors brush aside the failure of their deviance information criterion procedure to
share this consistency property; but should we not seek reassurance that a procedure performs well in
those simple cases for which its performance can be readily assessed, before trusting it on more complex
problems?
I contest the view (Section 9.1.3) that likelihood is relevant only under the good model assumption: from
a decision theoretic perspective, we can always regard the ‘log-loss’ scoring rule S.p; y/ := −log{p.y/}
as a measure of the inadequacy of an assessed density p.·/ in the light of empirical data y (Dawid, 1986).
Moreover, when y is a sequence yn = .y1; : : :; yn/ of not necessarily independent or identically distributed
variables, we have
−log{p.yn/} =
n
i=1
−log{p.yi|yi−1/};
.41/
the ith term measuring the performance of the Bayesian probability forecast for yi on the basis of analysis
of earlier data only (Cowell et al. (1999), chapters 10 and 11). This representation clearly demonstrates
why unadjusted marginal likelihood offers a valid measure of model ﬁt: each ‘test’ observation yi is always
entirely disjoint from the associated ‘training’ data yi−1. If desired, we can generalize this prequential
formulation of marginal likelihood by inserting other loss functions (Dawid, 1992b) or using other model
ﬁtting methods (Skouras and Dawid, 1999). Such procedures exhibit a natural consistency property even
under model misspeciﬁcation (Dawid, 1991; Skouras and Dawid, 2000).
One place where a Bayesian might want a measure of model complexity is as a substitute for p in the
Bayes information criterion approximation to marginal likelihood, e.g. for hierarchical models. But in
such cases the definition of the sample size n can be just as problematic as that of the model dimension p.
What we need is a better substitute for the whole term p log.n/.
Andrew Lawson and Allan Clark .University of Aberdeen/
We would like to make several comments on this excellent paper.
Our prime concern here is the fact that the deviance information criterion DIC is not designed to pro-
vide a sensible measure of model complexity when the parameters in the model take the form of locations
in some R-dimensional space. In the spatial context, this could mean the locations of cluster centres or,
more generally, the components of a mixture. Clearly the averaging of parameters in these contexts is
nonsensical but is a fundamental ingredient of DIC’s penalty term D.¯θ/. Even if an alternative measure
of central tendency is used it remains inappropriate to average over conﬁgurations where locations in the
chosen space are parameters (e.g. cluster detection modelling in spatial epidemiology (McKeague and
Loiseaux, 2002; Gangnon and Clayton, 2002). In the case of the Bayes information criterion, however, it
might be possible to replace the penalty p ln.n/ by an average number of parameters (in a reversible jump
context) such as ¯p ln.n/, where p is the number of parameters and n the sample size. This would at least
approximately accommodate the varying dimension but would not require the averaging of parameters
(as compared with DIC). This was suggested in Lawson (2000).
The second point of concern is the relationship of the goodness of ﬁt to convergence of the Markov chain
Monte Carlo samplers for which DIC is designed. If posterior marginal distributions are multimodal then

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
625
the conventional convergence diagnostic will fail (as they will usually ﬁnd too much variability in individual
chains), and also DIC will average over the modes.
We are also somewhat concerned and puzzled by the results for the Scottish lip cancer data set. In
Table 1, excepting the saturated model, the largest penalty terms are for the exchangeable model and
not those with either spatial or spatial and exchangeable components. We also note that it is not strictly
appropriate to ﬁt a spatial-only model without the exchangeable component.
Finally we note that alternative approaches have recently been proposed (Plummer, 2002).
Jos´e M. Bernardo .Universitat de Val`encia/
This interesting paper discusses rather polemic issues and offers some reasonable suggestions. I shall limit
my comments to some points which could beneﬁt from further analysis.
(a) The authors point out that their proposal is not invariant under reparameterization and show that
differences may be large. The use of the median would make the result invariant in one dimension,
but it is not trivial to extend this to many dimensions. An attractive, general invariant estimator is
the intrinsic estimator obtained by minimizing the reference posterior expectation of the intrinsic
loss δ.ˆθ; θ/ (Bernardo and Suarez, 2002) deﬁned as the minimum logarithmic divergence between
p.x|ˆθ/ and p.x|θ/. Under regularity conditions and moderate or large samples, this is well approx-
imated by .E[θ|x] + M[θ|x]/=2, the average between the reference posterior mean and mode. Other
invariant estimators may be obtained by minimizing the posterior expectation of δ.ˆθ; θ/ obtained
from either a proper subjective prior or an improper prior which, as the reference prior, is obtained
from an algorithm which is invariant under reparameterization.
(b) The authors use ‘essentially ﬂat’ or ‘weakly informative’ priors, i.e. conjugate-like priors with very
small parameter values. This is dangerous and is not recommended. There is no reason to believe
that those priors are weakly informative on the parameters of interest. Indeed, these limiting proper
priors can have hidden undesirable features such as strong biases (cf. the Stein paradox). Moreover,
they may approximate a prior function which would result in an improper posterior and using a
‘vague’ proper prior in that case does not solve the problem; the answer will then typically be ex-
tremely sensitive to the hyperparameters chosen for the vague proper prior and, since the Markov
chain Monte Carlo algorithm will converge because the posteriors are guaranteed to be proper,
one might not notice anything wrong. If full, credible, subjective elicitation is not possible then one
should use formal methods to derive an appropriate reference prior.
(c)
The authors’ brief comment (in Section 9.2.4) on the calibration of the deviance information crite-
rion DIC is too short to offer guidance. With Bayes factors, we have a direct interpretation of the
numbers obtained. The Bayesian reference criterion (Bernardo, 1999) is deﬁned in terms of natural
information units (and may also be described in terms of log-odds). Is there a natural interpretation
for DIC?
(d) The important particular case of nested models is not discussed in the paper. Would the authors
comment on the behaviour on DIC in that case (and hence on their implication on precise hy-
pothesis testing)? For instance, what is DIC’s recommendation for the simple canonical problem
of testing a value for a normal mean? It seems to me that, like Akaike’s information criterion or
the Bayesian reference criterion (but not the Bayes information criterion or Bayes factors), DIC
would avoid Lindley’s paradox. Is this so?
Sujit K. Sahu .University of Southampton/
This impressive paper shows how the very complicated business of model complexity can be assessed easily
by using Markov chain Monte Carlo methods. My comments mostly concern the foundational aspects
of the methods proposed and the interrelationship of the deviance information criterion DIC and other
Bayesian model selection criteria.
The paper provides a long list of models and the associated pD, the effective number of parameters. In
each of these cases pD is interpreted nicely in terms of model quantities. However, there is an unappealing
feature of pD that I would like to point out in the discussion below.
Consider the set-up leading to equation (23). Assume further that A1 = 1; C1 = 1 and C2 = τ 2. Thus
the likelihood is N.θ; 1/ and the prior is N.0; τ 2/. Then equation (23) yields that
pD =
1
1 + 1=nτ 2 :
Assuming τ 2 to be ﬁnite it is seen that pD increases to 1 as n →∞. The unappealing point is that the

626
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
effective number of parameters is larger for larger sample sizes; conventional intuition suggests other-
wise. The number of unknowns (i.e. the effective number of parameters) should decrease as more data are
obtained under this very simple static model. In spite of the authors’ views on asymptotics or consistency,
this point deserves further explanation as it is valid even when small sample sizes are considered.
In Section 9.1 the relationship between DIC and other well-known Bayesian model selection criteria
including the Bayes factor is discussed. Although DIC is not to be viewed as a formal model choice crite-
rion (according to the authors), it is often (and it will be) used to perform model selection; see for example
the references cited by the authors. In this regard a more precise statement about the relationship between
the Bayes factor and DIC can be made. I illustrate this with the above simple example taken from the
paper.
Assume that the observation model is N.θ; 1/ and the prior for θ is N.0; τ 2/. Suppose that model 0
speciﬁes that H0 : θ = 0 and model 1 says that H1 : θ ̸= 0: I assume that both n and τ 2 are ﬁnite and thus
avoid the problems with interpretation of the Bayes factor and Lindley’s paradox. Using the Bayes factor,
model 0 will be selected if
n ¯y2 < .1 + nτ 2/log.1 + nτ 2/
nτ 2
:
In contrast, DIC selects model 0 if
n ¯y2 < .1 + nτ 2/
2
2 + nτ 2 :
Clearly, if DIC selects model 0 then the Bayes factor will also select model 0. It is also observed that the
Bayes factor allows for higher |¯y|-values without rejecting the simpler model. In effect DIC is seen to have
the much discussed poor behaviour of a conventional significance test which criticizes the simpler null
hypothesis too much and often rejects it when it should not.
Sylvia Richardson .Imperial College School of Medicine, London/
I restrict my comments on this far-reaching paper to the use of the deviance information criterion DIC
for choosing within a family of models and the behaviour of pD as a penalization.
My ﬁrst remark concerns the spatial example of Section 8. The DIC-values for the ‘spatial’ and the
‘spatial plus exchangeable’ models are nearly identical. Thus, the authors resort to external pragmatic
considerations for preferring the simpler model, while the more complex one is not penalized.
Table 5.
Performance of DIC for mixture models with different
numbers of components
Results for the following values of k:
k = 2
k = 3
k = 4
k = 5
k = 6
Bimod (n = 200)
DIC(k)
566.7
567.7
568.5
569.2
570.0
E.D|y; k/
563.4
563.7
564.1
564.5
565.0
pD
3.3
4
4.4
4.7
5
Skew (n = 200)
DIC(k)
545.5
535.9
535.5
535.7
535.8
E.D|y; k/
540.3
530.1
530.0
530.2
530.4
pD
5.2
5.8
5.5
5.5
5.4
North–south (n = 94)
DIC(k)
110.5
110.9
110.9
110.5
110.8
E.D|y; k/
94.2
91.9
89.6
87.7
86.2
pD
16.3
19.0
21.3
22.8
24.6

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
627
Fig. 8.
Predictive densities for the skew data set:       , k D 2;
, unconditional (results for k D 3, 4, 5
are superimposed)
Turning to mixture models and the comparison between models with different numbers of components,
I discuss two situations. The ﬁrst concerns simple Gaussian mixtures with an unknown number of com-
ponents; yi ∼Σk
j=1wjf.·|θj/; i = 1; : : :; n, where f.·|θj/ is Gaussian. To calculate DIC in this setting,
let us focus on mixtures as ﬂexible distributions and use the conditional density for a new observation
yÅ : g.yÅ/ = p.yÅ|y; w; θ; k/ to calculate the deviance D.g/ = −2 Σn
i=1 log{g.yi/} and take its expec-
tation over the Markov chain Monte Carlo run, conditional on k. We have pD.k/ = E{D.g/} −D.ˆgk/,
where ˆgk = p.yÅ|y; k/.
Two cases of Gaussian mixtures were simulated (one replication): a well-separated bimodal mix-
ture (bimod), 0.5 N.−1:5; 0:5/ + 0:5 N.1:5; 0:5/, and an overlapping skewed bimodal mixture (skew):
0.75 N(0, 1) + 0.25 N(1.5, 0.33), each with 200 data points.
In the clear-cut bimod case, DIC(k) is lower for k = 2, with a small incremental increase in both
E.D|y; k/ and pD as extra components are being ﬁtted (Table 5). In the more challenging skew case, the
pattern of DIC-values shows that this data set requires more than two components to be adequately ﬁtted,
but the values of DIC and pD stay surprisingly ﬂat between three and six components. Note that the pre-
dictive density plots conditional on k = 3; 4; 5 are completely superimposed (Fig. 8), indicating that more
than three components can be considered as overﬁtting the data, in the sense that they give alternative
explanations that are no better but involve increasing numbers of parameters.
The second situation is that of spatial mixture models proposed in Green and Richardson (2002) in the
context of disease mapping. DIC was calculated by focusing on area-specific risk. Referring, for exam-
ple, to the simple north–south (two-component) contrast deﬁned in that paper, we ﬁnd that DIC stays
stable as k increases, decreasing E.D|y; k/ values being compensated by increasing pD. On the basis of a
mean-square error criterion between the estimated and the underlying risk surface, a deterioration of the
ﬁt would be seen with values of 0.14, 0.15 and 0.16 for k = 2; 3; 4 respectively.
Thus pD acts as a sufﬁcient penalization only in the simplest case. In other cases, DIC does not distin-
guish between alternative ﬁts with increasing number of parameters.
Peter Green .University of Bristol/
I have two rather simple comments on this interesting, important and long-awaited paper.
The ﬁrst concerns using basic distribution theory to give a surprising new perspective on pD in the
normal case, perhaps identifying a missed opportunity in exposition.
Consider ﬁrst a decomposition of data as focus plus noise:
Y = X + Z
where X and Z are independent n-vectors, normally distributed with ﬁxed means and variances, and var(Z)

628
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
is non-singular. The deviance is
D.X/ = .Y −X/T var.Z/−1.Y −X/
and so
pD = E[D.X/|Y] −D.E[X|Y]/ = tr{var.Z/−1var.Z|Y/};
.42/
using the standard expression for the expectation of a quadratic form. Several results in the paper have
this form, possibly in disguise. However,
var.Z|Y/ = var.Z/ −cov.Z; Y/ var.Y/−1cov.Y; Z/
= var.Z/ −var.Z/ var.Y/−1var.Z/
= var.Z/ var.Y/−1{var.Y/ −var.Z/};
yielding the much more easily interpretable
pD = tr{var.Y/−1 var.X/}:
.43/
This allows a very clean derivation of examples in Sections 2.5 and 4.1–4.3. For example, in the Lindley
and Smith model we have var.Z/ = C1 and var.X/ = A1C2AT
1 , and so
pD = tr{.A1C2AT
1 + C1/−1A1C2AT
1 } = tr{AT
1 C−1
1 A1.AT
1 C−1
1 A1 + C−1
2 /−1};
as in equation (21) of the paper.
Turning now to hierarchical models, consider a decomposition into k independent terms
Y = Z1 + Z2 + : : : + Zk;
where all Zi are normal, and var.Zk/ is non-singular. These represent all the various terms of the model:
ﬁxed effects with priors, random effects with different structures, errors at various levels; again all means
and variances are ﬁxed. Then for any level l = 1; 2; : : :; k −1 we may take the sum of the ﬁrst l terms as
the focus and the rest as noise.
Version (42) of pD above is then not very promising:
pD.l/ = tr

var
	
k
i=l+1
Zi

−1
var
	
k
i=l+1
Zi
Y


;
but expression (43) gives the more compelling
pD.l/ = tr

var.Y/−1 var
	 l
i=1
Zi


:
.44/
Thus pD has generated a decomposition of the overall degrees of freedom n = Σl tr{var.Y/−1var.Zl/} into
non-negative terms attributable to the levels l = 1; 2; : : :; k, just as in frequentist nested model analysis of
variance. (We must take care with improper priors in using expression (44), and terms should be treated as
limits as precisions go to 0.) Of course, expressions (43) and (44) fail to hold with unknown variances or
with non-normal models, but the observations above do provide further motivation for accepting pD as a
measure of complexity, and suggest exploring more thoroughly its role in hierarchical models.
My second point notes that the paper has no examples with discrete ‘parameters’. Conditional distri-
butions in hierarchical models with purely categorical variables can be computed by using probability
propagation methods (Lauritzen and Spiegelhalter, 1988), avoiding Markov chain Monte Carlo methods,
so that pD is again a cheap local computation. Presumably marginal posterior modes would be used for
¯θ. Certainly this is a context where pD can be negative. Can connections be drawn with existing model
criticism criteria in probabilistic expert systems?
The following contributions were received in writing after the meeting.

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
629
Kenneth P. Burnham .US Geological Survey and Colorado State University, Fort Collins/
This paper is an impressive contribution to the literature and I congratulate the authors on their achieve-
ments therein. My comments focus on the model selection aspect of the deviance information criterion
DIC. My perspectives on model selection are given in Burnham and Anderson (2002), which has a focus
on the Akaike information criterion AIC as derived from Kullback–Leibler information theory. A lesson
that we learned was that, if the sample size n is small or the number of estimated parameters p is large
relative to n, a modiﬁed AIC should be used, such as AICc = AIC + 2p.p + 1/=.n −p −1/. I wonder
whether DIC needs such a modiﬁcation or if it really automatically adjusts for a small sample size or large
p, relative to n. This would be a useful issue for the authors to explore in detail.
At a deeper level I maintain that model selection should be multimodel inference rather than just infer-
ence based on a single best model. Thus, model selection to me has become the computation of a set of
model weights (probabilities in a Bayesian approach), based on the data and the set of models, that sum to
1. Given these weights and the ﬁtted models (or posterior distributions), model selection uncertainty can
be assessed and model-averaged inferences made. The authors clearly have this issue in mind as demon-
strated by the last sentence of Section 9.1.3. I urge them to pursue this much more general implementation
of model selection and to seek a theoretical or empirical basis for it with DIC.
There is a matter that I am confused about. The authors say ‘: : : we essentially reduce all models
to non-hierarchical structures’ (third page), and ‘Strictly speaking, nuisance parameters should ﬁrst be
integrated out : : :’ (Section 9.2.3). Does this mean that we cannot make full inferences about models with
random effects? Can DIC be applied to random-effects models? It seems so on the basis of their lip cancer
example (Section 8.1). Can I have a model with ﬁxed effects τ, random effects φ1; : : :; φk, with postulated
distribution g.φ|θ/; θ as ﬁxed effects (plus priors on all ﬁxed effects) and have my focus be all of τ; φ
and θ? Thus, I obtain shrinkage-type inferences about the φi; I do not integrate out the φ (AIC has been
adapted to this usage).
The authors make a point (page 612) that I wish to make more strongly. It will usually not be appropriate
to ‘choose’ a single model. Unfortunately, standard statistical model selection has been to select a single
model and to ignore any selection uncertainty in the subsequent inferences.
Maria DeIorio .University of Oxford/ and Christian P. Robert .Universit´e Paris Dauphine/
Amidst the wide scope of possible extensions of their paper, the authors mention the case of mixtures
k
j=1
pj f.x|θj/;
which is quite interesting, as it illustrates the versatility of the deviance information criterion DIC under
different representations of the same model.
In this set-up, if the pjs are known, the associated completed likelihood is
L{θ|.x1; z1/; : : :; .xn; zn/} ∝
n
i=1
f.xi|θzi/ =
k
j=1

i:zi=j
f.xi|θj/:
.45/
Therefore, conditional on the latent variables z = .z1; : : :; zn/, and setting the saturated deviance f.x/
to 1, deﬁne
[DIC|z] =
k
j=1

i:zi=j
.−4E[log{f.xi|θj/}|x; z} + 2 log{f.xi|ˆθj/}]/
where ˆθj = E.θj|x; z/ (under proper identiﬁability constraints; see Celeux et al. (2000)). The integrated
DIC is then
DIC1 = 
z∈Z
[DIC|z] Pr.z|x/;
where Pr.z|x/ can be approximated (Casella et al., 1999).
A second possibility is the observed DIC, DIC2, based on the observed likelihood, which does not use
the latent variables z. (We note the strong dependence of DIC on the choice of the saturated function f
and the corresponding lack of clear guidance outside exponential families. For instance, if f.xi/ goes from
the marginal density to the extreme alternative where both θ1 and θ2 are set equal to xi, DIC2 goes from
−31.71 to 166.6 in the following example.)

630
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
Table 6.
Comparison of the three different criteria DIC1, DIC2 and
DIC3 for a simulated sample of 100 observations from 0:5 N(5, 1.5)
+ 0:5 N(7.5, 8) with a conjugate prior θ1  N(4, 5) and θ2 N(8, 5),
and of DIC based on the true complete sample (x, z) and DIC for the
single-component normal model (with an N(6, 5) prior and a variance
set of 6.07)
Results for the following models:
Normal
Complete,
Integrated,
Observed,
Full,
(k = 1)
[DIC | z]
DIC1
DIC2
DIC3
DIC
465.1
413.5
462.6
457.6
447.4
∆DIC
—
−51.6
−2.5
−7.5
−17.6
pD
0.99
1.96
2.27
1.98
28.06
Fig. 9.
Histogram of the simulated data set and true density
A third possibility is the full DIC, DIC3, based on the completed likelihood (45) when it incorporate´s z
as an additional parameter, in which case the saturated deviance could be the normal standardized devi-
ance, although we still use f.x/ = 1 for comparison.
The three possibilities above lead to rather different ﬁgures, as shown by Table 6 for the simulated data
set in Fig. 9; Table 6 exhibits in addition a lack of clear domination of the mixture (k = 2) versus the
normal distribution (k = 1) (second column), except when z is set to its true value (third column) or
estimated (last column). Note that, for the full DIC, pD is far from 102; this may be because, for some
combinations of z, the likelihood is the same. (This also relates to the fact that z is not a parameter in the
classical sense.)
David Draper .University of California, Santa Cruz/
The authors of this interesting paper talk about Bayesian model assessment, comparison and ﬁt, but—if
their work is to be put seriously to practical use—the real point of the paper is Bayesian model choice: we
are encouraged to pick the model with the smallest deviance information criterion DIC among the class
of ‘good’ models (those which are ‘adequate candidates for explaining the observations’). (It is implicit
that somehow this class has been previously speciﬁed by means that are not addressed here—would the

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
631
authors comment on how this set of models is to be identiﬁed in general?) However, in the case of model
selection it would seem self-evident that to choose a model you have to say to what purpose the model will
be put, for how else will you know whether your model is sufﬁciently good? We can, perhaps, use DIC
to say that model 2 is better than model 1, and we can, perhaps, compare ¯D with ‘the number of free
parameters in θ’ to ‘check the overall goodness of ﬁt’ of model 2, but we cannot use the authors’ methods
to say whether model 2 is sufﬁciently good, because the real world definition of this concept has not been
incorporated into their methods. It seems hard to escape the fact that specifying the purpose to which a
model will be put demands a decision theoretic basis for model choice; thus (Draper, 1999) I am ﬁrmly in
the camp of Key et al. (1999).
See Draper and Fouskakis (2000) and Fouskakis and Draper (2002) for an example from health policy
that puts this approach into practice, as follows. Most attempts at variable selection in generalized
linear models conduct what might be termed a beneﬁt-only analysis, in which a subset of the available
predictors is chosen solely on the basis of predictive accuracy. However, if the purpose of the modelling is
to create a scale that will be used—in an environment of constrained costs, which is frequently the case—to
make predictions of outcome values for future observations, then the model selection process must seek
a subset of predictors which trades off predictive accuracy against data collection cost. We use stochastic
optimization methods to maximize the expected utility in a decision theoretic framework in the space of
all 2p possible subsets (for p of the order of 100), and because our predictors vary widely in how much
they cost to collect (which will also often be true in practice) we obtain subsets which are sharply different
from (and much better than) those identiﬁed by beneﬁt-only methods for performing ‘optimal’ variable
selection in regression, including DIC.
Alan E. Gelfand .Duke University, Durham/ and Matilde Trevisani .University of Trieste/
The authors’ generally informal approach motivates several remarks which we can only briefly develop
here. First, in Section 2.1, we think that better terminology would be ‘focused on p.y|θ/’ with ‘interest in
the models for θ’, as in, for example, the example in Section 8.1 where there is no θ in the likelihood for any
of the given models. Even the example in Section 8.2, where θ does not change across models, emphasizes
the focus on p.y|θ/ since f.y/ depends on the choice of p. So, here, a relative comparison of the models
depends on the choices made for the f s. Without a clear prescription for f (once we leave the exponential
family), the opportunity exists to ﬁddle the support for a model.
Though the functional form of the Bayesian deviance does not depend on p.θ/, DIC and pD will. With
the authors’ hierarchical speciﬁcation,
p.y; θ; ψ/ = p.y|θ/ p.θ|ψ/ p.ψ/;
the effective degrees of freedom will depend on p.ψ/. But, also, under this speciﬁcation, rather than p.y|θ/,
we can put a different distribution, p.y|ψ/, in focus. Again, it seems preferable not to speak in terms of
‘parameters in focus’.
Moreover, since p.y|θ/ and p.y|ψ/ have the same marginal distribution p.y/, a coherent model choice
criterion must provide the same value under either focus. Otherwise, a particular hierarchical speciﬁcation
could be given more or less support according to which distribution we focus on. But let DIC1; pD1 and
f1.y/ be associated with p.y|θ/ and DIC2; pD2 and f2.y/ with p.y|ψ/. To have DIC1 = DIC2 requires,
after some algebra, that
ln{f2.y/} −ln{f1.y/} = pD1 −pD2 + E[ln{p.y|ψ/|y}] −E[ln{p.y|θ/|y}]:
Just as the functional form of f1.y/ depends only on the form of p.y|θ/, the form for f2.y/ should
depend only on p.y|ψ/. Evidently this is not so. For instance, under the authors’ example in expression
(2), f1.y/ = 0. The above expression yields the non-intuitive choice
ln{f2.y/} =  wi + 1
2
 ln.1 −wi/ −λ var.ψ|y/  w2
i −λ
2
 w2
i {yi −E.ψ|y/}2
where wi = τi=.τi + λ/. This issue is discussed further in Gelfand and Trevisani (2002).
Jim Hodges .University of Minnesota, Minneapolis/
This is a most interesting paper, presenting a method of tremendous generality and, as a bonus, a ﬁne
survey of related methods. I can think of a dozen models for which I would like to see pD, but I shall ask
for just one: a balanced one-way random-effects model with unknown between-group precision, in which
each group has its own unknown error precision, these latter precisions being modelled as draws from,
say, a common gamma distribution with unknown parameters. Thus the precisions will be shrunk as well

632
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
as the means, and presumably the two kinds of shrinkage will affect each other. The focus could be either
the means or the precisions, or preferably both at once.
One thing is troubling: the possibility of a negative measure of complexity (Section 2.6, comment (d)).
Hodges and Sargent (2001) is linked (shackled?) to linear model theory, in which complexity is deﬁned
as the dimension of the subspace of ℜn in which the ﬁtted values lie. In our generalization, the ﬁtted
values may be restricted to ‘using’ only part of a basis vector’s dimension, because they are stochastically
constrained by higher levels of the model’s hierarchy. (Basing complexity on ﬁtted values may remove the
need to specify a focus, although, if true, this is not obvious.) In this context, zero complexity makes sense:
the ﬁtted values lie in a space of dimension 0 speciﬁed entirely by a degenerate prior. Negative complexity,
however, is uninterpretable in these terms. The authors attribute negative complexity to a poor model ﬁt,
which suggests that pD describes something more than the ﬁtted values’ complexity per se. Perhaps the
authors could comment further on this.
Youngjo Lee .Seoul National University/
It is very interesting to see the Bayesian view of Section 4.2 of Lee and Nelder (1996), which used extended
or h-likelihood and in which we introduced various test statistics. For a lack of ﬁt of the model we proposed
using the scaled deviance
Dr = −2.log{p.y|˜θt/} −log[p{y|µ.θ/ = y}]/
with degrees of freedom E.Dr/, estimated by n −tr.−L′′
˜θV/ where −L′′
˜θ = VÅ as in Sections 4.3 and 5.4 of
this paper. We considered a wider class of models, which we called hierarchical generalized linear models
(HGLMs) (see also Lee and Nelder (2001a, b)), but some of our proofs hold more widely than this, so
that, for example, Section 3.1 of this paper is summarized in our Appendix D, etc. For model complexity
the authors deﬁne in equation (9) the scaled deviance
Dm = −2[log{p.y|θ/} −log{p.y|˜θt/}]:
Dr and Dm are the scaled deviances for the residual and model respectively, whose degrees of freedom
add up to the sample size n. We are very glad that the authors have pointed out the importance of the
parameterization of θ in forming deviances. We extended the canonical parameters of Section 5 to arbi-
trary links by deﬁning the h-likelihood on a particular scale of the random parameters, namely one in
which they occur linearly in the linear predictor. In HGLMs the degrees of freedom for ﬁxed effects are
integers whereas those for random effects are fractions. Thus, a GLM has integer degrees of freedom
pm = rank.X/ because C−1
2 δ is 0 in Section 5, whereas the estimated degrees of freedom of Dm in HGLMs
are fractions. Lee and Nelder (1996) introduced the adjusted proﬁle h-likelihood eliminating θ, and this
can be used to test various structures of the dispersion parameters λ discussed in the examples of Section
8: see the model checking plots for the lip cancer data in Lee and Nelder (2001b). Lee and Nelder (2001a)
justiﬁed the simultaneous elimination of ﬁxed and random nuisance parameters. It will be interesting to
have the Bayesian view of the adjusted proﬁle h-likelihood.
Xavier de Luna .Ume˚a University/
This interesting paper presents Bayesian measures of model complexity and ﬁt which are useful at different
stages of a data analysis. My comments will focus on their use for model selection. In this respect, one
of the noticeable contributions of the paper is to propose a Bayesian analogue, the deviance information
criterion DIC, to the Akaike information criterion AIC and TIC. Both DIC and TIC are generalizations
of AIC. The former may be useful in a Bayesian data analysis, whereas the frequentist criterion TIC has
the advantage of not requiring the ‘good model’ assumption discussed by the authors.
Such ‘information-based’ criteria use measures of model complexity (denoted pÅ or pD in the paper).
It should, however, be emphasized that models can be compared without having to deﬁne and compute
their complexity. Instead, out-of-sample validation methods, such as cross-validation (Stone, 1974) or
prequential tests (Dawid, 1984) can be used in wide generality. Moreover, to use an estimate of pÅ in a
model selection criterion, some characteristics of the data-generating mechanism (DGM)—‘true model’ in
the paper—must be known. For instance, depending on the DGM either AIC-type or Bayes information
type criteria are asymptotically optimal (see Shao (1997) for a formal treatment of linear models). Thus,
when little is known about the DGM, out-of-sample validation provides a formal and general framework
to perform model selection as was presented in de Luna and Skouras (2003), in which accumulated pre-
diction errors (deﬁned with a loss function chosen in accordance with the purpose of the data analysis)

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
633
were advocated to compare and choose between different model selection strategies. When many models
are under scrutiny, out-of-sample validation may be computationally prohibitive and generally yields high
variability in the selection of a model. In such cases, different model selection strategies based on pÅ
(making—implicitly or explicitly—diverse DGM assumptions) can be applied to reduce the dimension of
the selection problem. Accumulated prediction errors can then be used to identify the best strategy while
making very few assumptions on the DGM.
Xiao-Li Meng .Harvard University, Cambridge, and University of Chicago/
The summary made me smile, for the ‘mean of the deviance −deviance of the mean’ theme once injected
a small dose of excitement into my student life. I was rather intrigued by the ‘cuteness’ of expressions
(3.4) and (3.8) of Meng and Rubin (1992), and seeing a Bayesian analogue of our likelihood ratio version
certainly brought back fond memories. My excitement back then was short lived as I quickly realized that
all I was deriving was just a masked version of a well-known variance formula. Let D.x; µ/ = .x −µ/2 be
the deviance, a case of realized discrepancy of Gelman et al. (1996); then
1
n
n
i=1
.xi −¯x/2 = D.xi; µ/ −D.¯x; µ/:
.46/
Although equation (46) is typically mentioned (with µ set to 0) for computational convenience, it is the
back-bone of the theme under quadratic or normal approximations, or more generally with log-concave
likelihoods, beyond which assumptions become much harder to justify or derive. (Obviously, equation
(46) is applicable for posterior or likelihood averaging by switching x and µ.)
Section 1 contained a small puzzle. I wondered why Ye (1998) was omitted from the list of ‘the most
ambitious attempts’, because Ye’s ‘data derivative’ perspective goes far beyond the independent normal
model cited in Section 4.2 (for example, it addresses data mining). It also provides a more original and in-
sightful justiﬁcation than normal approximations, especially considering that Markov chain Monte Carlo
sampling is most needed in cases where such approximations are deemed unacceptable.
Section 2.1 presented a bigger puzzle. The authors undoubtedly would agree that a statement like ‘In
hierarchical modelling we cannot uniquely deﬁne a “posterior” or “model complexity” without specifying
the level of the hierarchy that is the focus of the modelling exercise’ is tautological. Surely the ‘posterior’
and thus the corresponding ‘model complexity’ depend on the level or parameter(s) of interest. So why
does the statement become a meaningful motivation when the word posterior is replaced by ‘likelihood’?
There is even some irony here, because hierarchical models are models where there are unambiguous
and uncontroversial marginal likelihoods—both L.θ|y/ = p.y|θ/ and L.φ|y/ = p.y|φ/ in Section 2.1 are
likelihoods in the original sense.
Although limitations on space prevent me from describing my reactions when reading the rest, I do wish
that DIC would stick out in the dazzling AIC—TIC alphabet contest, so we would all be less compelled
to look for UIC (uniﬁed or useful information criterion?) : : :.
The authors replied later, in writing, as follows.
We thank all the contributors for their wide-ranging and provocative discussion. Our reply is organized
according to a number of recurring themes, but constraints on space mean that it is impossible to address
all the points raised. Echoing Brooks’s opening remarks, our hope is that discussants and readers will
be sufﬁciently inspired to pursue the ideas proposed in this paper and to address some of the unresolved
issues highlighted in the discussion.
Model focus and definition of deviance
Ournotionofthe‘focus’ofamodelanditsrelationshiptothepredictionproblemofinterestprovokedsome
controversy. The crucial role of the model focus is to deﬁne the (parameterization of the) likelihood, and we
appreciate Gelfand and Trevisani’s suggestion of the term ‘focus on p.y|θ/’, with interest in the structure of
θ, rather than models ‘focused on θ’. In all our examples the likelihood has been taken to be p.y|θ/ (using
the notation of Section 2.1) leading to models with a closed form likelihood but an unknown number of
effective parameters that we propose to estimate by pD. However, as Brooks points out, if the focus is on
p.y|ψ/ (i.e. integrating over the random effects θ), then in general the likelihood will no longer be available
in closed form, and other methods must be sought to evaluate p.y|ψ/: in this circumstance the number of
parameters will be the dimension of ψ or less, depending on the strength of the prior information on ψ.

634
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
Smith and others ask how the model focus should be chosen in practice. We argue that the focus is
operationalized by the prediction problem of interest. For example, if the random effects θ in a hierarchi-
cal model relate to observation units such as schools or hospitals or geographical areas, where we might
reasonably want to make future predictions for those same units, then taking p.y|θ/ as the focus is sensi-
ble. The prediction problem is then to predict a new Yi;rep conditional on the posterior estimate of θi for
that unit. However, if the random effects relate to individual people, say, then we are often interested in
population-average inference rather than subject-specific inference, so we may want to predict responses
for a new or ‘typical’ individual rather than an individual who is already in the data set. In this case, it is
appropriate to integrate over the θs and to predict Yrep for a new individual conditional on ψ, leading to
a model focused on p.y|ψ/. A crucial insight is that a predictive probability statement such as p.Yrep|y/ is
not uniquely deﬁned without specifying the level of the hierarchy that is kept ﬁxed in the prediction—this
deﬁnes the focus of the model. In summary, we feel that the issue of focus with respect to predictive model
assessment and selection is an issue in hierarchical modelling and not specifically Bayesian.
When the forms of the likelihoods differ between models being compared, it is clearly vital to be careful
that any standardizing terms that are used in the deviance are common. As observed by Smith, a compar-
ison of models with focus at different levels of the hierarchy may not be meaningful as they correspond to
different prediction problems.
Features of pD
Several discussants questioned the definition or performance of pD. As to the definition we maintain our
claim (in spite of Dawid’s comment) that it is in our models that there is a genuine Bayesian interest in
quantifying the interaction between Y and Θ in probabilistic terms. One can indeed often think of pD in
terms of dimensionality as Hodges suggests, but in general we prefer to think of it as a feature of the joint
distribution of Y and Θ. This frees it from the shackles imposed by normal linear model theory. Such a
measure of interaction or model complexity may, for example, be used to reparameterize hyperparameters
ψ to facilitate an intuitively interpretable speciﬁcation of model priors on ψ (Holmes and Denison, 1999).
Still, as suggested by Brooks, pD may turn out to be only a step towards a (better) definition of model
complexity such as that suggested by Plummer: we feel that the quantity that he proposes is intuitively
intriguing and that it may be particularly appropriate in exponential families, but we wonder about its
general validation and justiﬁcation.
Our uncertainty about whether to recommend pD as a definition or as an estimate of a quantity still
to be deﬁned makes it difﬁcult to judge proposals for an ‘improvement’. For example, using an invariant
estimator such as that proposed by Robert and Titterington or Bernardo instead of ¯θ is tempting as part
of a definition, but it takes into account only one feature of pD while destroying others such as the trace
approximation. Similarly the occurrence of a negative value of pD, typically observed if the model ﬁts
poorly, might resemble a negative estimate for a positive parameter. We take a pragmatic point of view
and look forward to theoretical progress that provides insight into why pD generally appears to work well.
Green provides a valuable insight into the interpretation of pD in the normal case, using an attractive
decomposition of the total predictive variance of the observables.
Replying to those discussants who were concerned about observing pD < n under ‘ﬂat’ priors, we re-
emphasize that pD = n was obtained theoretically only in the normal case or under normal approxima-
tions. There is no proof that pD = n for general distributions. In the case of Brooks’s illustration using the
Scottish lip cancer data, in which he shows that pD appears to ‘lose’ two or three (modulo Monte Carlo
error) parameters under such priors, we point out that two of the 56 observations in this data set are 0
with small expected values and so contribute negligibly to the Poisson deviance. We have replicated his
analysis replacing these two observations by non-zero counts, and we found that pD increases by about 2
to around 55.5.
We certainly do not recommend the unthinking use of default priors, a concern of Smith and Bernardo:
on the contrary, one of our main aims is to demonstrate how an informative prior reduces model com-
plexity. Typically a large number of parameters p relative to a small sample size n is compensated by using
an informative prior, and the deviance information criterion DIC and pD adjust accordingly without any
need for additional adjustment for small sample size (see Burnham, and Lawson and Clark’s comment on
the example in Section 8.1).
There is evidence (Daniels and Kass, 1999, 2001) that, in the absence of missing data, the use of default
priors for variance components typically has little effect on the posteriors for the main effects in a model.
Still, Smith and Bernardo observe that the ﬂat priors that may maximize pD are not necessarily weakly in-
formative, and we agree. Reference priors that are least informative in an information theoretical sense can

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
635
be easily studied in some of our examples. For example, Fig. 1 displays the performance of the beta. 1
2; 1
2/
reference prior (corresponding to a prior sample size of ni = a + b = 1) for the binomial likelihood,
and the approximation (31) indicates that pΘ
Di based on the reference prior is greater than pΘ
Di based on
the uniform beta(1, 1) prior (which has prior sample size ni = 2). Similarly for a Poisson likelihood the
reference prior π.µi/ ∝√µi yields a Γ.yi + 1
2; ni/ posterior distribution corresponding to a = 1
2; b →0.
Hence pµ
Di ≈yi=.yi + 1
2/ and pΘ
Di ≈ni=ni = 1 might be compared with the values shown in Fig. 2.
Properties of DIC
Another main part of the discussion focused on the properties and performance of DIC. Plummer doubted
the usefulness of the expected loss that DIC approximates, but he has included a standardizing constant in
the loss function which should not be present (we have made this clearer in the paper). The expected loss in
the (independent) normal linear case is then p + pD + n log.2πσ2/: this says that when comparing ‘good’
models with the same σ2s the expected loss is minimized with a degenerate prior in which no parameters
are estimated. This seems entirely reasonable, as all the models have equivalent ﬁt, and so distinction is
based on complexity alone. Of course in practice either σ2 will be estimated or σ2 will vary between models,
and hence the appropriate trade-off between ﬁt and complexity will naturally arise. A practical aspect,
related to the need for ‘good’ models in the derivation of DIC, is that the term L2 ignored by DIC will tend
to be negative with poorly ﬁtting models and hence to inﬂate DIC: the approximation of DIC to expected
loss will thus tend automatically to penalize models that are not ‘good’.
Though we agree with Brooks that owing to its heuristic derivation DIC may be considered as a ‘broad
brush technique’, we do not regard it to be as arbitrary as the alternatives that he suggests. In particular
we do not feel that terms of ‘ﬁt’ and ‘complexity’ can be arbitrarily combined, but we re-emphasize that
a measure of model complexity results from correcting overﬁt due to an approximation of the expected
loss that ‘uses the observations twice’. Similarly we would like to see a justiﬁcation of Vehtari’s estimates
of expected utilities as valid approximations generalizing DIC.
Bernardo asks for the application of DIC to nested models and hypothesis testing, in particular the
occurrence of Lindley’s paradox. This is an interesting question partially answered by the example dis-
cussed in Section 8.1 where some of the competing models are nested. The key point is that DIC is
designed to take into account priors that are concentrated on parameters which are speciﬁed in a model,
thus effectively assigning prior probability 0 to hypothetically omitted parameters (if there are remaining
parameters). Let us consider Lindley’s paradox in the following version: when comparing using the Bayes
factor ¯X ∼N.µ0; σ2=n/ with ¯X ∼N.µ; σ2=n/ where µ ∼N.µ1; τ 2/, evidence in favour of H0 : µ = µ0
becomes overwhelming as τ 2 →∞even if ¯x would cause the rejection of H0 at any arbitrary signifi-
cance level. If σ2 is known µ is the only parameter in the model. To apply DIC we compare the model
¯X ∼N.µ; σ2=n/ with prior µ ∼N.µ0; τ 2/; τ 2 →0, corresponding to H0 with the model with the same like-
lihood but prior µ ∼N.µ1; τ 2/; τ 2 →∞. Then D.µ/ = n.¯x −µ/2=σ2, D.µ/ = .n=σ2/{D.¯µ/ + var.µ|¯x/}
and pD = n=σ2 var.µ|¯x/. For τ 2 →0, pD →0; ¯µ →µ0 and DIC →D.µ0/. Similarly, for τ 2 →∞,
pD →1; ¯µ →¯x and DIC →D.¯x/ + 2 = 2. Hence the model with the ﬂat prior—the ‘alternative
hypothesis’—is favoured if D.µ0/ > 2 or |√n.¯x−µ0/=σ| > 1:414 which corresponds to a rejection of H0 at
a significance level α ≈0:16—exactly the behaviour of the Akaike information criterion. Thus Lindley’s
paradox is not observed. Similarly Sahu contrasts the prior concentrated on µ0 = 0 with an informative
prior N.0; τ 2/ which is centered at µ0, also. Thus it is reasonable to reject H0 using DIC if the data are
suitably compatible with the ‘alternative’ prior. However, we do not accept an assessment of DIC that uses
Bayes factors as a ‘gold standard’, since they are dealing with different prediction problems (see below).
Several discussants (Brooks, Bernardo, Burnham and Smith) were concerned with the lack of calibration
of DIC. However, unlike the Bayesian reference criterion (Bernardo, 1999), which is based on a Kullback–
Leibler distance and therefore a relative measure, DIC is an approximation to an absolute expected loss,
and we cannot calibrate it (externally). Correspondingly, ‘coherence’ of model choice cannot be required
in terms of equal DIC-values as Gelfand and Trevisani or Smith claim but can only be discussed in terms
of model ranking by DIC. Note, by the way, that Plummer’s alternative measure of model complexity, as
well as our pD, are deﬁned relatively, indicating that these measures might be calibrated.
Finally, we certainly do not claim that applying DIC is an exhaustive tool for model assessment.
Although we feel that our Fig. 4 is a step in the right direction, additional techniques such as those
discussed by Nelder and Atkinson are certainly needed for reﬁned analyses.
Applications
There were various comments on the interpretation of pD in the Scottish lip cancer analysis (Lawson and

636
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
Clark, and Richardson) and in mixture models (Richardson, and DeIorio and Robert). Here we tend to
think of pD as the estimable dimension of the parameter space or, alternatively, as the size of the parameter
space that is identiﬁable by the data. We repeat that the spatial model 3 in the lip cancer example (Section
8.1) provides stronger prior information than the exchangeable model 2 leading to a smaller pD. Only the
sum of the spatial and exchangeable random effects is uniquely identiﬁable in model 4 and so pD remains
virtually unchanged compared with the spatial-only model 3, thus justifying the lack of an additional
‘penalty’ for the apparently more complex model. The same is true for mixture models, where increasing
the number of components does not necessarily increase the identiﬁable parameter space. We do appreciate
the discussion of DIC in mixture models introduced by DeIorio and Robert, and by Richardson (though
Richardson does not appear to have calculated DIC as we have deﬁned it, but a different criterion based on
predictive deviances). DeIorio and Robert’s example nicely illustrates a range of possibilities for deﬁning
DIC in this case, although we re-emphasize that a comparison of models with different focus (e.g. their
DIC2 versus DIC3) may not be meaningful, and we further note that their integrated DIC (DIC1) does
not correspond to our definition of DIC.
In response to Lawson and Clark’s query about averaging ‘location’ parameters, we point to Green’s
comment concerning the calculation of pD and DIC for models with discrete parameters, and his sugges-
tion that marginal posterior modes could be used for ¯θ in this case.
We thank Nelder and Atkinson for their reﬁnements to the analysis of the stack loss data (Section
8.2). We disagree with Smith that our models 4 and 5 for these data are predictively identical since, as
already discussed, the prediction problem addressed by model 4 integrates over the random effects and
corresponds to predicting stack loss for a new chimney, whereas model 5 conditions on the random effects
and corresponds to predicting future stack loss for the 21 chimneys in the data set.
Alternatives to DIC
Several discussants (Brooks, Dawid and Sahu) feel that DIC suffers in comparison with more tradi-
tional Bayesian model selection criteria based on posterior model probabilities and Bayes factors. Here
we can only repeat that our deliberate intention was to offer an alternative to Bayes factors, which are
most suitable when the entire collection of candidate models can be speciﬁed ahead of time (the ‘M
closed’ case of Bernardo and Smith (1994)). In our practical experience, the model-building, criticism
and rebuilding process is typically an iterative ‘M open’ one in which the ultimate model collection
is rarely known ahead of time, and here DIC may emerge as more appropriate. Moreover, Bayes fac-
tors address how well the prior has predicted the observed data; this prior predictive emphasis ultimately
leads to the Lindley paradox. DIC instead addresses how well the posterior might predict future
data generated by the same mechanism that gave rise to the observed data; this posterior predictive
outlook might be considered intuitively more appealing in many practical contexts. We emphasize that
these techniques are intended to answer different questions and cannot be expected to give the same
conclusions: in any case, posterior model probabilities may be highly dependent on within- and between-
model priors, so their comparison with DIC is not straightforward. On a related point, several discussants
(Brooks, Burnham and Draper) mention the possible alternative of model averaging. We do not, however,
see any justiﬁcation for transforming DIC-values to relative probabilities, and in any case the prior
on the model space may be difﬁcult to develop, and might even reasonably be related to model com-
plexity!
Dawid wishes for a better definition of p log.n/ (instead of just p) for use in the Bayesian information
criterion (BIC) but previous work has shown that many such definitions are justiﬁable asymptotically (e.g.
Volinsky and Raftery (2000)), so this line of research does not appear promising. Regarding the suggestion
by Lawson and Clark of using ¯p log.n/ as a penalty for the BIC, this of course assumes that the number
of parameters p is a suitable measure of model complexity. But most spatial models of the type that they
refer to will involve random effects, where such use of the raw parameter count p would be inappropriate;
indeed, this is precisely the situation that pD was designed to address.
Vehtari and de Luna argue persuasively on behalf of cross-validation as an alternative to our pos-
terior predictive approach that avoids a definition of complexity. Whereas no knowledge of the data-
generating mechanism is required for cross-validation, the data-generating mechanism is necessary in a
fully Bayesian analysis. Still, cross-validation as an alternative estimation method was also used to estimate
model complexity by Efron (1986). We certainly acknowledge the potential of this approach, particularly in
comparisons of different model selection strategies. We agree with Stone concerning further investigation
of model assessment procedures in which the model is not assumed to be correct, and we refer to Konishi
and Kitagawa (1996) (whose GIC adds yet further to the alphabet).

Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
637
In conclusion, it is clear that several of the discussants feel that our pragmatic aims are muddying
otherwise pure Bayesian waters. We feel, however, that the huge increase in the use of Bayesian methods
in complex practical problems means that full elicitation of informative priors and utilities is simply not
feasible in most situations, and that reasonably simple and robust methods for prior speciﬁcation, model
criticism and model comparison are necessary. We hope that we have made a positive contribution to the
ﬁnal concern.
References in the discussion
Aitkin, M. (1991) Posterior Bayes factors (with discussion). J. R. Statist. Soc. B, 53, 111–142.
Akaike, H. (1973) Information theory and an extension of the maximum likelihood principle. In Proc. 2nd Int.
Symp. Information Theory (eds B. N. Petrov and F. Cs´aki), pp. 267–281. Budapest: Akad´emiai Kiad´o.
Atkinson, A. C. (1980) A note on the generalized information criterion for choice of a model. Biometrika, 67,
413–418.
Atkinson, A. C. and Riani, M. (2000) Robust Diagnostic Regression Analysis. New York: Springer.
(2002) Forward search added variable t tests and the effect of masked outliers on model selection and
transformation. Technical Report LSERR73. London School of Economics and Political Science, London.
Bernardo, J. M. (1979) Expected information as expected utility. Ann. Statist., 7, 686–690.
(1999) Nested hypothesis testing: the Bayesian reference criterion (with discussion). In Bayesian Statistics
6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 101–130. Oxford: Oxford University
Press.
Bernardo, J. M. and Smith, A. F. M. (1994) Bayesian Theory. New York: Wiley.
Bernardo, J. M. and Suarez, M. (2002) Intrinsic estimation. 7th Valencia Int. Meet. Bayesian Statistics, Tenerife,
June.
Burnham, K. P. and Anderson, D. R. (1998) Model Selection and Inference: a Practical Information-theoretic
Approach. New York: Springer.
(2002) Model Selection and Multimodel Inference: a Practical Information-theoretical Approach, 2nd edn.
New York: Springer.
Casella, G., Robert, C. P. and Wells, M. T. (2000) Mixture models, latent variables and partitioned importance
sampling. Technical Report. Paris.
Celeux, G., Hurn, M. and Robert, C. P. (2000) Computational and inferential difﬁculties with mixtures posterior
distribution. J. Am. Statist. Ass., 95, 957–979.
Cooke, R. M. (1991) Experts in Uncertainty. Oxford: Oxford University Press.
Cowell, R. G., Dawid, A. P., Lauritzen, S. L. and Spiegelhalter, D. J. (1999) Probabilistic Networks and Expert
Systems. New York: Springer.
Daniels, M. J. and Kass, R. E. (1999) Nonconjugate Bayesian estimation of covariance matrices and its use in
hierarchical models. J. Am. Statist. Ass., 94, 1254–1263.
(2001) Shrinkage estimators for covariance matrices. Biometrics, 57, 1173–1184.
Dawid, A. P. (1984) Statistical theory: the prequential approach. J. R. Statist. Soc. A, 147, 278–292.
(1986) Probability forecasting. In Encyclopedia of Statistical Sciences, vol. 7 (eds S. Kotz, N. L. Johnson
and C. B. Read), pp. 210–218. New York: Wiley-Interscience.
(1991) Fisherian inference in likelihood and prequential frames of reference (with discussion). J. R. Statist.
Soc. B, 53, 79–109.
(1992a) Prequential analysis, stochastic complexity and Bayesian inference (with discussion). In Bayesian
Statistics 4 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 109–125. Oxford: Oxford
University Press.
(1992b) Prequential data analysis. In Current Issues in Statistical Inference: Essays in Honor of D. Basu (eds
M. Ghosh and P. K. Pathak), pp. 113–126. Hayward: Institute of Mathematical Statistics.
Draper, D. (1999) Discussion on ‘Decision models in screening for breast cancer’ (by G. Parmigiani). In Bayesian
Statistics 6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 541–543. Oxford: Oxford
University Press.
Draper, D. and Fouskakis, D. (2000) A case study of stochastic optimization in health policy: problem formulation
and preliminary results. J. Global Optimzn, 18, 399–416.
Dupuis, J. and Robert, C. P. (2002) Model choice in qualitative regression models. J. Statist. Planng Inf., to be
published.
Efron, B. (1986) How biased is the apparent error rate of a prediction rule? J. Am. Statist. Ass., 81, 461–470.
Fouskakis, D. and Draper, D. (2002) Stochastic optimization: a review. Int. Statist. Rev., to be published.
Gangnon, R. and Clayton, M. (2002) Cluster modelling for disease rate mapping. In Spatial Cluster Modelling
(eds A. B. Lawson and D. Denison), ch. 8. New York: CRC Press.
Gelfand, A. E. (1996) Model determination using sampling-based methods. In Markov Chain Monte Carlo
in Practice (eds W. R. Gilks, S. Richardson and D. J. Spiegelhalter), pp. 145–162. London: Chapman and
Hall.

638
Discussion on the Paper by Spiegelhalter, Best, Carlin and van der Linde
Gelfand, A. E., Dey, D. K. and Chang, H. (1992) Model determination using predictive distributions with
implementation via sampling-based methods (with discussion). In Bayesian Statistics 4 (eds J. M. Bernardo,
J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 147–167. Oxford: Oxford University Press.
Gelman, A., Meng, X.-L. and Stern, H. (1996) Posterior predictive assessment of model ﬁtness via realized dis-
crepancies (with discussion). Statist. Sin., 6, 733–807.
Good, I. J. (1952) Rational decisions. J. R. Statist. Soc. B, 14, 107–114.
Green, P. and Richardson, S. (2002) Hidden Markov models and disease mapping. J. Am. Statist. Ass., to be
published.
Hodges, J. and Sargent, D. (2001) Counting degrees of freedom in hierarchical and other richly-parameterised
models. Biometrika, 88, 367–379.
Holmes, C. and Denison, D. (1999) Bayesian wavelet analysis with a model complexity prior. In Bayesian Statistics
6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 769–776. Oxford: Oxford University
Press.
Kass, R. and Raftery, A. (1995) Bayes factors and model uncertainty. J. Am. Statist. Ass., 90, 773–795.
Key, J. T., Pericchi, L. R. and Smith, A. F. M. (1999) Bayesian model choice: what and why? In Bayesian Statistics
6 (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith), pp. 343–370. Oxford: Oxford University
Press.
King, R. (2001) Bayesian model discrimination in the analysis of capture-recapture and related data. PhD Thesis.
School of Mathematics, University of Bristol, Bristol.
King, R. and Brooks, S. P. (2001) Bayesian estimation of census undercount. Biometrika, 88, 317–336.
Konishi, S. and Kitagawa, G. (1996) Generalised information criteria in model selection. Biometrika, 83, 875–890.
Lauritzen, S. L. and Spiegelhalter, D. J. (1988) Local computations with probabilities on graphical structures and
their application to expert systems (with discussion). J. R. Statist. Soc. B, 50, 157–224.
Lawson, A. B. (2000) Cluster modelling of disease incidence via rjmcmc methods: a comparative evaluation.
Statist. Med., 19, 2361–2376.
Lee, Y. and Nelder, J. A. (1996) Hierarchical generalized linear models (with discussion). J. R. Statist. Soc. B, 58,
619–678.
(2001a) Hierarchical generalized linear models: a synthesis of generalized linear models, random effect
models and structured dispersions. Biometrika, 88, 987–1006.
(2001b) Modelling and analysing correlated non-normal data. Statist. Modlng, 1, 3–16.
de Luna, X. and Skouras, K. (2003) Choosing a model selection strategy. Scand. J. Statist., to be published.
Madigan, D. and Raftery, A. E. (1991) Model selection and accounting for model uncertainty in graphical models
using Occam’s window. Technical Report 213. Department of Statistics, University of Washington, Seattle.
McKeague, I. and Loiseaux, M. (2002) Perfect sampling for point process cluster modelling. In Spatial Cluster
Modelling (eds A. B. Lawson and D. Denison), ch. 5. New York: CRC Press.
Meng, X.-L. and Rubin, D. B. (1992) Performing likelihood ratio tests with multiply imputed data sets. Biometrika,
79, 103–112.
Moreno, E., Pericchi, L. R. and Kadane, J. (1998) A robust Bayesian look at the theory of precise measure-
ment. In Decision Research from Bayesian Approaches to Normative Systems (eds J. Shantan et al.). Boston:
Kluwer.
Neter, J., Kutner, M. H., Nachtsheim, C. J. and Wasserman, W. (1996) Applied Linear Statistical Models, 4th edn.
New York: McGraw-Hill.
Pericchi, L. R. and Walley, P. (1991) Robust Bayesian credible intervals and prior ignorance. Int. Statist. Rev., 58,
1–23.
Plummer, M. (2002) Some criteria for Bayesian model choice. Preprint. (Available from http://calvin.
iarc.fr/martyn/papers/.)
Priestley, M. B. (1981) Spectral Analysis and Time Series. London: Academic Press.
Robert, C. P. (1996) Intrinsic loss functions. Theory Decsn, 40, 191–214.
Shao, J. (1997) An asymptotic theory for linear model selection. Statist. Sin., 7, 221–264.
Skouras, K. and Dawid, A. P. (1999) On efﬁcient probability forecasting systems. Biometrika, 86, 765–784.
(2000) Consistency in misspeciﬁed models. Research Report 218. Department of Statistical Science,
University College London, London. (Available from: http://www.ucl.ac.uk/Stats/research/
abs00.html#218.)
Smith, J. Q. (1996) Plausible Bayesian games. In Bayesian Statistics 5 (eds J. M. Bernardo, J. O. Berger, A. P.
Dawid and A. F. M. Smith), pp. 387–406. Oxford: Oxford University Press.
Stone, M. (1974) Cross-validatory choice and assessment of statistical predictions (with discussion). J. R. Statist.
Soc. B, 36, 111–147.
(1977) An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. J. R.
Statist. Soc. B, 36, 44–47.
Vehtari,A.(2001)Bayesianmodelassessmentandselectionusingexpectedutilities.DSc Dissertation.HelsinkiUni-
versity of Technology, Helsinki. (Available from http://lib.hut.fi/Diss/2001/isbn9512257653/.)
Vehtari, A. and Lampinen, J. (2002a) Bayesian model assessment and comparison using cross-validation predictive
densities. Neur. Computn, 14, in the press.

Discussion on the paper by Spiegelhalter, Best, Carlin and van der Linde
639
(2002b) Cross-validation, information criteria, expected utilities and the effective number of parameters.
To be published.
Volinsky, C. T. and Raftery, A. E. (2000) Bayesian information criterion for censored survival models. Biometrics,
56, 256–262.
Weisberg, S. (1981) A statistic for allocating Cp to individual cases. Technometrics, 23, 27–31.
Ye, J. (1998) On measuring and correcting the effects of data mining and model selection. J. Am. Statist. Ass., 93,
120–131.
Zhu, L. and Carlin, B. (2000) Comparing hierarchical models for spatio-temporally misaligned data using the
deviance information criterion. Statist. Med., 19, 2265–2278.

