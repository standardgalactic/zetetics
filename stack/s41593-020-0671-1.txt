Articles
https://doi.org/10.1038/s41593-020-0671-1
1Computational and Biological Learning Lab, Department of Engineering, University of Cambridge, Cambridge, UK. 2Research Institute for Signals,  
Systems and Computational Intelligence sinc(i), FICH–UNL/CONICET, Santa Fe, Argentina. 3Center for Cognitive Computation, Department of  
Cognitive Science, Central European University, Budapest, Hungary. 4These authors contributed equally: Guillaume Hennequin and Máté Lengyel. 
 ✉e-mail: recheveste@sinc.unl.edu.ar
T
he dynamics of sensory cortices exhibit a set of ubiqui-
tous features across species and experimental conditions. 
Responses vary over time and across trials even when 
the same static stimulus is presented1, and these intrinsic varia-
tions have both systematic and seemingly random components 
(so-called noise variability). The most prominent systematic 
patterns of neural activity are strong, inhibition-dominated 
transients at stimulus onset2 (or, equivalently, strong adaptation 
following stimulus onset) and stimulus-dependent population 
oscillations in the gamma band (20–80 Hz)3,4. The extent and pat-
tern of noise variability is also stimulus-dependent, whereby vari-
ability is quenched at stimulus onset1, decreases gradually with 
stimulus contrast in the primary visual cortex (V1)5 and is further 
modulated by the content of the stimulus; for example, the ori-
entation or direction of drifting gratings for cells in V1 or in the 
middle temporal visual area6.
While the mechanisms giving rise to these dynamical phenom-
ena are increasingly well understood6,7, their functional significance 
remains largely unknown and controversial. For example, cortical 
gamma oscillations have been suggested to be a substrate for bind-
ing different sources of information about a feature (known as bind-
ing by synchrony8) to mediate information routing (communication 
by synchrony9) or to enable a temporal code of spikes relative to 
the oscillation phase10. Transient overshoots have been proposed to 
carry novelty or prediction error signals11. Noise variability, when 
considered to be useful at all, has been argued to bear signatures 
of probabilistic computations5,12,13. However, it is unclear whether 
these explanations can be reconciled, as each of them only accounts 
for select aspects of the data and has been challenged by alternative 
accounts3,14–16.
Here, we present a unifying model in which all of these dynami-
cal phenomena emerge as a consequence of the efficient implemen-
tation of the same computational function: probabilistic inference. 
Probabilistic inference provides a principled solution to forming 
percepts by fusing partial and noisy information from multiple 
sources (including multiple sensory cues, modalities and forms of 
memory)17. Formally, this fusion results in a posterior distribution 
expressing the probability that relevant features in the environ-
ment that are not directly accessible to the brain (for example, the 
three-dimensional shapes of objects) may take any particular con-
figuration given information that is directly available to our senses 
(for example, photons absorbed in the retina). Behavioral evidence 
in several domains, including near-optimal performance in multi-
sensory integration, decision-making, motor control and learning, 
suggests that the brain represents posterior distributions at least 
approximately18. There have also been several proposals for how 
the neural responses of sensory cortical populations may imple-
ment these probabilistic representations5,12,19. While these models 
successfully explained important aspects of stationary response 
distributions (for example, tuning curves, Fano factors and noise 
correlations), they have so far fallen short of accounting for the rich 
intrinsic dynamics of sensory cortical areas.
To bring together dynamics (cortical-like activity patterns) and 
function (representing posterior distributions) in a principled man-
ner, we optimized a biologically constrained recurrent neural net-
work for performing sampling-based probabilistic inference5,13,20,21. 
Specifically, the objective of the network was to dynamically pro-
duce responses whose distribution matched, for each stimulus, the 
posterior distribution of a Bayesian ideal observer. The optimized 
neural circuit exhibited a number of appealing computational and 
dynamical features. Computationally, after training on a reduced 
stimulus set, the network exhibited strong forms of generalization 
by producing near-optimal response distributions to novel inputs 
that required qualitatively different responses. Furthermore, the 
network discovered out-of-equilibrium dynamics, which is a strat-
egy currently employed by modern machine-learning algorithms 
Cortical-like dynamics in recurrent circuits 
optimized for sampling-based probabilistic 
inference
Rodrigo Echeveste   1,2 ✉, Laurence Aitchison1, Guillaume Hennequin   1,4 and Máté Lengyel   1,3,4
Sensory cortices display a suite of ubiquitous dynamical features, such as ongoing noise variability, transient overshoots and 
oscillations, that have so far escaped a common, principled theoretical account. We developed a unifying model for these 
phenomena by training a recurrent excitatory–inhibitory neural circuit model of a visual cortical hypercolumn to perform 
sampling-based probabilistic inference. The optimized network displayed several key biological properties, including divi-
sive normalization and stimulus-modulated noise variability, inhibition-dominated transients at stimulus onset and strong 
gamma oscillations. These dynamical features had distinct functional roles in speeding up inferences and made predictions 
that we confirmed in novel analyses of recordings from awake monkeys. Our results suggest that the basic motifs of cortical 
dynamics emerge as a consequence of the efficient implementation of the same computational function—fast sampling-based  
inference—and predict further properties of these motifs that can be tested in future experiments.
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1138

Articles
NaTurE NEurOSciEncE
to produce samples that become statistically independent on short 
timescales22.
Biologically, the optimized circuit achieved divisive normal-
ization of its outputs and displayed marked transients at stimulus 
onset, as well as strong gamma oscillations. Both the magnitude 
of transients and the frequency of gamma oscillations scaled with 
stimulus contrast. Crucially, these dynamical phenomena did not 
emerge in control networks optimized for related objectives that 
did not require sampling-based inference. Further analyses of tran-
sients and oscillations in the optimized network revealed distinct 
functional roles for them. These analyses predicted novel proper-
ties of cortical dynamics, whereby onset transients should be tuned 
to stimuli. We confirmed this via new analyses of published V1 
recordings in awake monkeys23. In addition, our model made spe-
cific predictions about the stimulus tuning of excitatory–inhibitory 
(E–I) lags and the distribution of gamma power across the different 
modes of network dynamics. Both can be readily tested in future 
experiments.
In summary, we constructed a biologically constrained recurrent 
neural network performing sampling-based probabilistic inference 
that explained a plethora of electrophysiological observations in 
sensory cortices. Our model therefore provides a unifying theoreti-
cal account of the basic motifs of sensory cortical dynamics.
Results
Optimizing a recurrent neural circuit for probabilistic inference. 
To study neural circuit dynamics implementing probabilistic infer-
ence, we used a novel combination of two well-established, although 
hitherto unrelated, computational approaches: Bayesian ideal 
observers and the training of recurrent neural networks (Methods). 
First, we used a Bayesian ideal observer model to specify the com-
putational goal of perceptual inference in a simplified visual task. 
Performing inference requires an internal model that encapsulates 
one’s assumptions about how the inputs to be processed have been 
generated by the environment. For this, we adopted the Gaussian 
scale mixture (GSM) model (Fig. 1a), which is a generative model 
that can capture the statistics of natural image patches24. Conversely, 
inference under the GSM model can account for behavioral and 
neural data (for stationary responses) in visual perception5,25,26. The 
GSM model assumes that an image patch is generated as a linear 
combination of oriented Gabor filter-like visual features (‘projective 
fields’), each present with a different intensity (the latent variables 
of the model). The image patch is further scaled by a single global 
‘contrast’ variable. Here, we focused on modeling a single hypercol-
umn by choosing the projective fields of the GSM latent variables to 
only differ in their orientation so that they formed a ring topology 
(Fig. 1a; Extended Data Fig. 1a,b). The ideal observer was obtained 
by using a Bayesian inversion of this model. Thus, for every image 
patch taken as the sensory input, the ideal observer yielded a 
high-dimensional posterior distribution that quantifies the prob-
ability that any particular joint combination of feature intensities 
may have generated the input (Fig. 1b).
Second, to model cortical circuit dynamics, we used a canoni-
cal, rate-based stochastic recurrent neural network model: the sto-
chastic variant of the stabilized supralinear network (SSN)6,27. The 
network was constrained to exhibit the following basic biological 
features previously shown to be fundamental for cortical dynamics: 
separate but interconnected E and I populations of neurons (Fig. 
1c); supralinear (expansive) input/output functions27,28; and finite 
and stimulus-independent process noise6,29 incorporating intrinsic 
and extrinsic forms of neural variability.
We trained this network to perform sampling-based infer-
ence under the GSM. For this, we assumed a one-to-one mapping 
between latent variables and E cells such that the response (mem-
brane potential) of each E cell represented the intensity of a differ-
ent feature in the GSM model. I neurons were treated as auxiliary 
units that are not explicitly constrained by the computational objec-
tive. The network was optimized to produce distributions of excit-
atory neural activities that matched the posteriors computed by the 
GSM-based ideal observer up to second-order statistics. For each 
stimulus in a small training set, the network was required to use its 
stochastic dynamics to visit different parts of state space over time 
with a frequency proportional to the posterior distribution corre-
sponding to the same stimulus (Fig. 1d). Critically, as process noise 
in the network was stimulus-independent, the network had to use 
its recurrent dynamics to appropriately shape this variability for 
matching the target posteriors for each input. The training objective 
also included terms that encouraged fast circuit dynamics. In sum-
mary, the network had to generate fast fluctuations with the correct 
stimulus-dependent patterns of across-trial mean and covariance.
Optimizing our network was challenging because modulating 
the response variability (to match the stimulus-dependent posterior 
a
c
b
d
Stimulus
Latent 2
Latent 1
P(latents∣stimulus)
Cell 2
Cell 1
Network activity
E–I network
E/I cells
Feedforward
receptive
fields
Latents
L t
Generative model
Projective
fields
Fig. 1 | The statistical generative model and the corresponding neural 
circuit implementing sampling-based probabilistic inference. a, Sketch 
of the GSM generative model. An image patch is constructed as a linear 
combination of a fixed set of localized, oriented, Gabor filter-like features 
(projective fields, differing only in their orientations, uniformly spread 
between –90° and 90°), with stimulus-specific feature intensities (latent 
variables) drawn from a multivariate Gaussian distribution. The resulting 
image patch is scaled by a global contrast variable and corrupted by noise 
(not shown). The stimulus shown is for illustration only: the GSM model 
employed here was not sufficiently complex to generate photorealistic 
images. See Fig. 3c and Extended Data Fig. 1c for samples of the generated 
image patches. b, A two-dimensional projection of the posterior 
distribution over latent variables given a visual stimulus, computed by 
the Bayesian ideal observer under the generative model. c, An E–I neural 
network receiving an image patch as an input, filtered by feedforward 
receptive fields that are identical to the projective fields of the generative 
model in a. The activity of each E cell represents the value of one latent 
variable in the generative model. As an illustration of the ring topology of 
the network, the outgoing connections of one E–I cell pair are shown (the 
connection strength is indicated by the width of the line and ‘synapse’ 
size; see the main text for more details). d, Responses of the two E cells 
corresponding to the latent variables shown in b. The response trajectory 
samples from the corresponding posterior distribution over time given the 
same stimulus.
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1139

Articles
NaTurE NEurOSciEncE
covariances of the ideal observer model) requires strong and non-
linear recurrent interactions, but networks of strongly connected E 
neurons—especially with supralinear input/output functions—are 
prone to becoming unstable6,30. In such networks, it is nontrivial to 
find parameter regimes in which stability is preserved and there-
fore optimization can proceed6. We therefore resorted to a reduced 
‘ring’ (hypercolumn) parameterization in which recurrent connec-
tion strength between any two cells (and the covariability of their 
process noise) only depended on the angular distance between their 
preferred stimuli and their respective cell types (E or I; Extended 
Data Fig. 1d). The feedforward receptive fields of the cells were 
fixed and identical to the projective fields of the corresponding 
latent variables of the ring-structured GSM (Extended Data Fig. 1a). 
In summary, we optimized 15 parameters to match more than 6,000 
target values (means and covariances) in total.
Inference and generalization in the optimized network. In line 
with neural recordings, activity in the optimized network was highly 
variable across time and trials for both low-contrast (Fig. 2a, top) 
and high-contrast stimuli (Fig. 2a, bottom). Critically, despite the 
severely constrained parameterization of our model, the distribu-
tions of neural responses at the five training stimuli (the same image 
patch at five different contrast levels; Fig. 2b, left) closely matched 
the corresponding GSM posteriors (Fig. 2b–d, compare red to 
green). Specifically, the mean activity of neurons increased while 
the variability of their responses decreased with contrast as well as 
with the match between stimulus orientation and their preferred 
orientation. This was consistent with the behavior of the moments 
of the GSM posterior (Fig. 2c, and circles in Fig. 3a). Thus, the 
network had been successfully trained to perform sampling-based 
inference on these stimuli.
–90°
0
90°
a
b
c
d
–90°
0
90°
200 ms
0
4
8
0
4
8
0
4
8
0
4
8
0
4
8
0 4 8
4
8
4
8
1
2
–90°
0°
90°
1
2
Pref. ori.
Pref. ori.
0
5
10
uE (mV)
Stimuli
u16°
u42°
Mean uE (mV)
Posterior
Network
uE s.d. (mV)
Pref. ori.
–90°
–90°
–90°
0°
0°
0°
90°
90°
90°
Pref. ori.
Pref. ori.
Pref. ori.
–1
–0.5
0
0.5
1
Correlation
Fig. 2 | Inference and responses in the optimized network. a, Sample population activity of E cell membrane potentials uE (color) at zero (top) and high 
(bottom) contrast. The high-contrast stimulus has a dominant orientation at 0° (arrow). Neurons are ordered by preferred orientation (Pref. ori.). b, Left: 
stimuli in the training set (the shade of the frame color indicates the contrast level, with split green and red indicating that the same stimuli were used 
as input to the ideal observer and the neural network). Right: covariance ellipses (two standard deviations) of the posterior distributions of the ideal 
observer (green) and of the corresponding response distributions of the network (red). Red trajectories show sample 500-ms sequences of activities in 
the network. Projections for two representative latent variables/E cells are shown, with projective fields/receptive fields at preferred orientations 42° and 
16° (insets at the end of the axes). c, Mean (top) and standard deviation (bottom) of latent variables under the posterior distribution of the ideal observer 
(left, green) and of E cell membrane potentials uE under the stationary distribution of the network (right, red), ordered by their preferred orientation, 
for each stimulus in the training set. d, Correlation matrices of the posterior distributions of the ideal observer (left, green) and the stationary response 
distributions of the network (right, red). The line colors in c and the frame colors in d correspond to different contrast levels, using the same colors as the 
stimulus frames in b. The response moments in c and d were estimated from n = 20,000 independent samples (taken 200 ms apart). Correlations in d are 
Pearson’s correlations.
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1140

Articles
NaTurE NEurOSciEncE
We also tested the capacity of the network to represent the 
appropriate posterior distributions for novel stimuli. First, we 
confirmed that the mean and variability of network responses 
smoothly interpolated between the corresponding target moments 
for intermediate contrast levels, closely following the behavior of 
the GSM posterior (Fig. 3a, solid curves between circles). Next, we 
presented the network with 500 entirely novel image patches ran-
domly generated from the GSM (Extended Data Fig. 1c). Overall, 
both the means (Fig. 3b, top) and covariances of network responses  
(Fig. 3b, bottom) matched those of the target posteriors. This match 
4
6
a
c
b
1
2
3
5
7
0
3
6
0
0.5
1
0
0.5
1
3
5
7
0
3
6
Train
Test
2
5
2
5
2
5
2
5
2
5
2
5
–90° 0°
90°
–2
0
2
1st
2nd
3rd
–2
0
2
–2
0
2
–2
0
2
–2
0
2
–2
0
2
Mean uE (mV)
Contrast
uE s.d. (mV)
Contrast
Posterior
Network
Network
Posterior
Mean (mV)
Network
Posterior
Covariance (mV2)
Train
Stimulus
Test
Mean
Mean uE (mV)
s.d. uE (mV)
Pref. ori.
–90° 0°
90°
Pref. ori.
–90° 0°
90°
Pref. ori.
–90°
–90°
0°
0°
90°
90°
Pref. ori.
–90° 0°
90°
Pref. ori.
PCs
Correlations
Pref. ori.
–1
–0.5
0
0.5
1
Correlation
Fig. 3 | Generalization in the optimized network. a, Mean (top) and standard deviation (bottom) of latent variables (green) and stationary network 
responses (red) averaged over the population as a function of contrast. The circles and the gray dots on the x axis indicate training contrast levels. The 
network correctly generalizes to untrained contrast levels (segments between circles). b, Stationary mean (top) and covariance (bottom) during network 
activity (y axis) versus under the posterior (x axis). Each dot corresponds to the response of an individual cell (top) or cell pair (bottom) to one of the 
trained stimuli (lavender) or one of the novel, untrained stimuli in the test set (orange). c, Examples of generalization in the network. Each row corresponds 
to a different stimulus and shows the corresponding statistical moments of latent variables under the GSM posterior (green) and stationary responses 
in the network (red). As a reference, the top row shows one of the training stimuli. The bottom five rows show generalization to novel test stimuli. Left: 
example stimuli. Middle: GSM (green) and network means (red), and the first three PCs of the GSM covariance, scaled by the square root of the variance 
they explain of the GSM posterior (green) and of the network covariance (red). Right: correlation matrices of the posterior distributions of the ideal 
observer (left, green frames) and the response distributions of the network (right, red frames). The response moments in all panels were estimated from 
n = 20,000 independent samples (taken 200 ms apart). Population mean moments in a were further averaged across n = 50 E cells. Correlations in c are 
Pearson’s correlations.
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1141

Articles
NaTurE NEurOSciEncE
was similarly good for test stimuli (Fig. 3b, orange) as for training 
stimuli (Fig. 3b, lavender). Critically, while the inputs of the training 
set included a single dominant orientation, many test stimuli had a 
more complex structure, with more than one dominant orientation 
(Fig. 3c, first column). Consequently, the corresponding GSM pos-
teriors that the network was required to match became qualitatively 
different. Specifically, both the mean activity profiles across the 
population (Fig. 3c, second column) and the principal components 
(PCs) of the noise covariances (Fig. 3c, remaining columns) became 
multimodal and highly dependent on the stimulus (Fig. 3c, green; 
compare across rows). The network was able to match the required 
GSM posteriors with high accuracy even in these challenging cases 
(Fig. 3c, red). Thus, the optimized network performed approximate 
Bayesian inference over a wide array of stimuli by always sam-
pling (approximately) from the appropriate, stimulus-dependent 
high-dimensional posterior distribution of the ideal observer.
The optimized network performs fast sampling. Under 
sampling-based inference, the time it takes to accurately represent 
a posterior distribution by collecting successive samples is directly 
proportional to the timescale over which these samples are cor-
related31. In our optimized network, noise variability generated 
new, independent samples every few tens of milliseconds across 
all contrast levels. This was evident in the fast fall-off of response 
auto­correlations (Fig. 4a, colored curves), which is similar to that 
observed in sensory cortices13,32. In fact, our network was even  
faster than a disconnected network with the same membrane and 
input time constants (Fig. 4a, dashed curve), and close to the theo-
retical limit of a network of infinitely fast neurons in which sam-
pling speed is solely limited by the input time constant (Fig. 4a, 
dotted curve).
To understand how the optimized network achieved fast sam-
pling, we compared its dynamics to an algorithm known as Langevin 
sampling, which is a form of directed random walk (Methods). 
Sampling by Langevin dynamics was a relevant comparison for 
our network for two reasons. First, Langevin sampling is a popu-
lar, general-purpose algorithm in machine learning that is useful 
for benchmarking the performance of our network. Second, previ-
ous work had suggested that sampling in stochastic recurrent neural 
networks (without respecting Dale’s principle) may be implemented 
by Langevin-like dynamics33–35.
We found that for each input, Langevin dynamics was consis-
tently an order of magnitude slower than our network (Fig. 4a, 
gray curves). The slowness of Langevin dynamics is known to arise 
from one of its critical features: time-reversible dynamics36. That 
is, any time series of responses is as probable as its time-reversed 
counterpart. This was indeed reflected in temporally symmetric 
cross-correlograms (Fig. 4b, top). In contrast, our optimized network 
displayed a marked departure from time-reversibility, as evidenced 
by the strong asymmetric components in its cross-correlograms 
(Fig. 4b, bottom). This dynamical irreversibility implied that the 
activation of neurons showed sequentiality. In particular, we found 
that I cells typically lagged behind E cells. Moreover, for any cell, its 
total I input also tended to lag behind its overall E input (Fig. 4c), 
which is consistent with known electrophysiology37. Interestingly, 
this lag was smaller for cells that were most strongly driven by the 
stimulus, and this modulation became stronger with increasing 
contrast. These form testable predictions of our model.
Cortical-like dynamics in the optimized circuit. Having estab-
lished that our network represented posterior distributions via 
sampling, we compared its responses with known physiological 
properties of V1. First, firing rates in the model had a physiologi-
cally realistic dynamic range and were tuned to stimulus orienta-
tion, similar to neurons in macaque V1 (Fig. 5a, left-middle; see 
also ref. 6 and analysis of the data recorded by ref. 23). We also com-
puted spike count statistics in the network from firing rates, assum-
ing a doubly stochastic spike-generation process (Methods). The 
quenching of membrane potential variability with increasing con-
trast (Fig. 2d, bottom) gave rise to a quenching of spike count vari-
ability (quantified by the Fano factor), which was strongest at the 
preferred orientation of a cell (Fig. 5b, middle). These effects quali-
tatively matched V1 data, but were weaker (Fig. 5b, left). Moreover, 
stationary responses in the network exhibited clear signatures of 
divisive normalization38 (Extended Data Fig. 2a,b). All these results 
were expected for a network whose stationary membrane potential 
response distributions represent GSM posteriors5.
Although our optimization procedure only constrained the sta-
tionary response distributions of the network, without prescribing 
any specific dynamics, we found that the optimized network exhib-
ited realistic, cortical-like dynamics. Specifically, strong gamma 
oscillations emerged (with a peak frequency increasing with con-
trast), which was consistent with V1 recordings in awake monkeys3,4 
(Fig. 5c, left-middle). Moreover, selective clamping of either the E or 
I population abolished gamma oscillations (or stability altogether) 
(Extended Data Fig. 2c,d), which suggests that gamma oscillations 
0
0.5
1
a
b
c
=
+
=
+
–2
–1
–90°
0°
90°
Contrast
–0.4
0
0.4
Langevin
–0.4
0
0.4
–0.1
0
0.1
–1,000 0
1,000
–1,000 0
1,000
–1,000 0
1,000
–0.4
0
0.4
E–I network
–0.4
0
0.4
–0.1
0
0.1
0
100
200
–100
0
100
–100
0
100
–100
0
100
Autocorrelation
Time lag (ms)
Langevin sampler
E–I network
Feedforward network
Process noise
E–I lag (ms)
∆Orientation
0
1
Lagged cross-correlation
Total
Symmetric
Anti-symmetric
Time lag (ms)
Time lag (ms)
Time lag (ms)
E–E
I–I
E–I
Fig. 4 | Temporal correlations in the optimized network. a, Membrane potential autocorrelations (population average) in the network for increasing levels 
of stimulus contrast (from dark to pale red; same colors as in Fig. 2b–d). The autocorrelation of a purely feedforward network (with the same process 
noise) is shown for comparison (dashed black line), together with those of the process noise (dotted black line) and a collection of networks implementing 
Langevin sampling at each contrast level (from dark to light gray). b, Lagged cross-correlation (left) in the Langevin sampler (top) and in the optimized 
E–I network (bottom) decomposed into temporally symmetric (middle) and anti-symmetric components (right). Each line corresponds to a different cell 
pair (three representative pairs shown), and the color encodes the identity of participating cells (E or I; note that there is no separation of E and I cells in 
the Langevin networks). c, The lag between total E and I inputs to each E cell as a function of stimulus orientation (relative to the preferred orientation) at 
different contrast levels (indicated by the colors).
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1142

Articles
NaTurE NEurOSciEncE
arose from dynamical interactions between E and I cells (that is, 
the ‘PING’ mechanism16). The network also showed strong tran-
sient responses such that average population rates had marked 
contrast-dependent overshoots at stimulus onset, which is consis-
tent with recordings in the V1 region3 (Fig. 5d, left-middle). Finally, 
using a conductance-based approximation of our current-based 
0
10
20
0
10
20
0.7
1
1.3
0.7
1
1.3
–90°
0°
90°
10−4
10−2
100
0
50
100
40
60
20
40
0
20
40
Contrast
0
1
Contrast
0
1
2
0
0.5
1
0
10
20
0.7
1
1.3
10−4
10−2
100
0.1
1
1
10
100
0
50
100
0
20
40
Contrast
0
1
2
0
100
200
0
100
200
0
50
100
150
200
250
0
100
200
Optimized network
Firing rate (Hz)
Experiment
a
b
c
d
e
Spontaneous
Evoked
Fano factor
∆Orientation
–90°
0°
90°
∆Orientation
–90°
0°
90°
∆Orientation
Power
Frequency (Hz)
Contrast
Gamma peak (Hz)
Contrast (%)
Firing rate (Hz)
Time (ms)
0.5
1.0
2.0
Firing rate (normalized)
Time (ms)
25%
50%
100%
Conductance (nS)
Time (ms)
0
50
100
150
200
250
Time (ms)
0
50
100
150
200
250
Time (ms)
Excitatory
Inhibitory
Control
Power
Frequency (Hz)
Time (ms)
0.5
1.0
2.0
Fig. 5 | Cortical-like dynamics in the optimized network. In this figure, the left column indicates experimental data, the middle indicates the optimized 
network, and the right column indicates the control network trained to modulate its mean responses but not its variability. a,b, Mean firing rates  
(a) and Fano factors (b) of neurons as a function of stimulus orientation (relative to the preferred orientation) during spontaneous (dark red) and evoked 
activity (light orange). Experimental results show the mean ± s.e.m. (n = 99 cells). c, Peak gamma frequencies in the LFP power spectrum as a function of 
contrast. The control network and optimized network (inset) show LFP power spectra at different contrast levels (colors as in Figs. 2 and 4). Note that no 
dependence of gamma frequency is shown for the control network as there were no discernible gamma peaks in its power spectra. Experimental results 
show the mean ± s.d. (n = 14 sessions). d, Average rate responses around stimulus onset at different contrast levels (colors). e, E and I conductance 
(mean ± s.e.m., relative to baseline; see Methods for details) during a transient stimulus response. Experimental results show the mean ± s.e.m. (n = 8 
trials). Numerical results show the mean ± s.e.m. (n = 20 trials). The black bars in d and e show the stimulus period. Data in a and b reproduce analyses 
from ref. 6 of data from ref. 23 (awake macaque V1 recordings). Experimental data in c were reproduced from ref. 4 (awake macaque V1), in d reproduced 
from ref. 3 (awake macaque V1 recordings), and in e reproduced from ref. 2 (awake mouse V1 recordings).
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1143

Articles
NaTurE NEurOSciEncE
model (Methods), we found that inhibition transiently dominated 
over excitation during stimulus presentation (Fig. 5e, left-middle), 
as occured in the V1 of awake mice2.
Control networks do not show cortical-like dynamics. We next 
sought to establish whether these dynamical properties were sim-
ply due to the biological and architectural constraints imposed on 
our network or specifically due to optimizing for sampling-based 
inference. For this, we used a series of ‘control’ networks in which 
single-cell parameters (time constants and firing-rate nonlineari-
ties), overall network architecture and receptive fields were all iden-
tical to those used in the original network. Thus, these networks 
only differed in the objective for which they were optimized.
First, we confirmed that the dynamics that are characteristic of the 
originally optimized network did not emerge in randomly parameter-
ized networks without optimization, but robustly emerged after opti-
mization starting from different random initial conditions (Extended 
Data Figs. 3 and 4). Next, we optimized a control network that dif-
fered from the original network in only one critical aspect: it was only 
required to match the means of the posterior distributions, but not 
variances or covariances. Despite clear stimulus-dependent modula-
tions in the mean responses (as required by training; Fig. 5a, right), 
this network exhibited only minimal modulations of both mem-
brane potential variability (Extended Data Fig. 5a–g) and Fano fac-
tors (Fig. 5b, right; Extended Data Fig. 6). Thus, the modulations of 
response variability seen in the original network, which are a hall-
mark of sampling-based inference5, were not simply a by-product of 
nonlinear E–I dynamics. Moreover, neither gamma oscillations nor 
marked inhibition-dominated transients emerged in the control net-
work (Fig. 5c–e, right). In fact, matching both means and variances, 
but not covariances (necessary for full inference), still abolished these 
dynamical features (Extended Data Figs. 5h–n and 6). Finally, oscilla-
tions were also absent in another control network that was specifically 
optimized to modulate its mean firing rates as before but kept its Fano 
factors constant (Extended Data Figs. 6 and 7), as would be required 
by other, non-sampling-based probabilistic representations12.
These results suggest that the dynamical features observed 
in the original network emerged as a consequence of the specific 
computation for which it was optimized. Conversely, training the 
network on the original cost function but without enforcing Dale’s 
principle resulted in a substantially poorer performance and a lack 
of oscillations and transients (Extended Data Figs. 4, 6 and 8). Thus, 
achieving competent sampling performance and exhibiting realistic 
dynamics again appear to be coupled.
Oscillations improve mixing time. To study the potential func-
tional benefits of oscillations, one would ideally like to knockout 
oscillations from the network while leaving all other features of the 
dynamics intact. The complex and high dimensional dynamics of 
our network made this unfeasible. Therefore, we first studied the 
response of a single neuron to obtain an analytical understanding 
of the general role of oscillations in sampling (Fig. 6a,b). We next 
generalized this analysis to oscillations in network-wide activity 
patterns rather than single neurons, thus providing insights into the 
high-dimensional dynamics of the full network (Fig. 6c,d).
Assuming that the response of a neuron is statistically stationary 
and approximately normally distributed, it is fully characterized by 
its mean, variance and autocorrelogram. As long as this neuron is 
part of a sampling-optimized network, the mean and variance of its 
response will have to match those prescribed by the target distribu-
tion sampled by the network (Fig. 2b–d). Although the autocorre-
logram is not constrained by the target moments, it still critically 
contributes to the performance of the network. Specifically, it can 
be mathematically shown that the total area under the autocorre-
logram directly scales the ‘mixing time’; that is, the time it takes for 
the dynamics to represent the target distribution to a given precision 
(Supplementary Math Note). Therefore, to understand the specific 
role of oscillations, we compared idealized (stationary and normally 
distributed) neuronal responses, which were constructed to have 
the same mean and variance as responses in our network, but dif-
ferent autocorrelation functions (Methods).
We compared response autocorrelograms with different degrees of 
‘oscillatoriness’ (Fig. 6a, blue, orange and red), but the same envelope 
as that of the full network (Fig. 6a, inset, black dotted line; see also Fig. 
4). These oscillations substantially reduced the area under the auto-
correlogram (Fig. 6a, inset) and therefore accelerated the convergence 
of the empirical distribution of the responses to the target distribution 
(Fig. 6b; Supplementary Math Note). Importantly, oscillations will 
only decrease the area under the autocorrelogram if at least one oscil-
lation cycle fits under the envelope; that is, if the oscillation period 
is sufficiently shorter than the width of the envelope (~35 ms). This 
implied oscillation frequencies higher than 30 Hz, which is what we 
observed in the optimized network (Fig. 5c, middle).
We next studied the organization of gamma oscillations in the 
multidimensional responses of the full network. We noted that our 
results for a single neuron readily generalized to any single direc-
tion in the state space of the full network, that is, any network-wide 
activity pattern. Further analysis revealed that maximal sampling 
speed is achieved specifically when a smaller response variance 
is associated with a higher oscillation frequency (Supplementary 
Math Note). In turn, as we showed above, variability is quenched 
with increasing contrast both in our network and in the cortex  
(Figs. 2b,c, 3a and 5b). This explains why the frequency of gamma 
oscillations increased with contrast in our network after optimiza-
tion. These results suggest that contrast-modulated gamma oscil-
lations observed in the cortex3,4 may reflect a speed-optimized 
sampling strategy (Fig. 5c).
Our mathematical analyses also predicted that oscillations in an 
efficiently sampling network should be predominantly expressed 
where they matter most: in the (stimulus-dependent) network-wide 
activity patterns capturing most of the overall response variability. 
Namely, for each stimulus, we expected the strongest oscillations 
along the top PCs of the corresponding stationary covariance 
(Supplementary Math Note). This was indeed apparent in the 
power spectra of our network associated with the top ten PCs  
(Fig. 6c) and in the corresponding autocorrelograms that even 
showed negative-going lobes (Fig. 6c, inset). Specifically, there 
was a positive relationship between oscillatoriness along suc-
cessive PCs and the fraction of variance explained. This meant 
that the network oscillated more in the directions along which its 
responses had the largest variance (Fig. 6d). Note that our mea-
sure of oscillatoriness was based on autocorrelograms; therefore, 
it had no a priori dependence on response variance (Methods). 
In summary, the network used nontrivial temporal dynamics, in 
the form of contrast-dependent, pattern-selective gamma oscilla-
tions, to ensure that even short segments of its activity were suf-
ficiently representative of the posterior distribution it represented 
for each stimulus.
Transients support continual inference. The foregoing results 
showed that oscillations increase the mixing speed in the station-
ary regime; that is, once network responses have become represen-
tative of the target distribution. Complementing this, we found 
that transients in our network mitigate the other main temporal 
constraint of sampling: the ‘burn-in’ time it takes for responses to 
become representative in the first place31. We observed that in line 
with experimental data, during stimulus onset, neural responses 
tended to overshoot the corresponding stationary response levels  
(Figs. 5d and 7a). One might naively expect such transients to be det-
rimental for representing a distribution, as they clearly deviate from 
the target (represented by the steady-state responses). However, in a 
realistic setting with a changing environment, distributions need to 
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1144

Articles
NaTurE NEurOSciEncE
be continually represented, without waiting for the system to achieve 
a steady state.
To understand the role of transients in continual inference, 
we considered how a moving decoder of neural responses over a 
finite trailing time window approximated the target. As with oscil-
lations, we performed this analysis in two steps. First, to isolate 
the potential functional benefits of transients, we once again con-
sidered the response of a single idealized neuron that is part of a 
sampling-optimized network (Methods). For this idealized neuron, 
we fixed the autocorrelogram (thus controlling for oscillations) as 
well as the before-stimulus and after-stimulus onset steady-state 
means and variances to those of an actual, representative neuron in 
our network (Extended Data Fig. 9a). We then compared three ways 
in which this neuron could transition between these two steady-states 
(Fig. 7a): (1) as an upper bound on performance, instantaneously 
switching between the two steady-states (Fig. 7a, gray dashed line); 
(2) exponentially approaching the new steady-state with the charac-
teristic time constant of the cells in the network, thus lacking over-
shoots (Fig. 7a, black dashed line); and (3) undergoing overshoots as 
seen in our optimized network (Fig. 7a, red line).
We found that overshoots performed close to the upper bound, 
which was provided by instantaneous switching. In particular, 
they generated samples that allowed a substantially more accurate 
estimate of the target mean than that afforded by approaching the 
new steady-state exponentially without overshoots (Fig. 7a,b). 
These results qualitatively extended to the case when the match 
in the full distributions was considered (Extended Data Fig. 9b). 
This was because without overshoots at stimulus onset, responses 
were still sampling from the distribution corresponding to the 
baseline input. Thus, including them in the estimation of the new 
stimulus-related mean inevitably biased the estimate downwards. 
The overshoot largely compensated for this bias. Indeed, we were 
able to analytically show that optimal compensation requires 
transient overshoots at stimulus onset (Supplementary Math 
Note). This is because the continual averaging of responses for-
mally corresponds to a temporal convolution; therefore, the opti-
mal response is the deconvolution of the target with the averaging 
kernel. Under basic smoothness constraints, the deconvolution of 
a step function with such an averaging kernel yields transients 
like those we observed in the network (Extended Data Fig. 9c).
The hypothesis of increased sampling accuracy by tran-
sient compensation made the following distinct prediction 
(Supplementary Math Note): transient overshoots should scale 
with the change in steady-state responses. Indeed, our network 
0
4
8
a
b
c
d
Single neuron
Full network
–0.5
0
0.5
1
100
102
104
0
4
8
PC
–0.5
0
0.5
1
0
100
200
0
100
200
0
0.5
1
1
10
100
1,000
0
0.25
0.5
log10(power)
log10(power)
No gamma
With gamma
Strong gamma
Process noise
Autocorrelation
Time lag (ms)
Divergence (bits)
Sampling time (ms)
Frequency (Hz)
1
10
100
1,000
10
100
1,000
Frequency (Hz)
1st
10th
Autocorrelation
Time lag (ms)
Oscillatoriness
Fraction of explained variance along PC
Fig. 6 | Oscillations improve mixing time. a,b, Analyses of oscillations in the response of a single neuron. a, Power spectra of three different neural responses 
(colored lines) with identical mean and variance but different degrees of oscillatoriness. The inset shows autocorrelation functions. The black dotted line 
represents the autocorrelation of the process noise. b, Divergence between the distribution estimated from a finite sampling time (x axis) and the true 
stationary distribution for the three systems (colors as in a). c,d, Analyses of oscillations in the full network. c, Power spectra of the neural activity of the 
network along the directions of the PCs of its stationary response distribution and ordered by PC rank (colors). The inset shows autocorrelation of neural 
activity along the directions of the first and tenth PCs (colors are as in the main plot). d, The oscillatoriness of the autocorrelogram along each PC (colors as in 
c) as a function of the fraction of the total variance of responses they capture. Note that our measure of oscillatoriness is based on the relative contributions of 
an oscillatory versus a non-oscillatory component in a parametric fit to the autocorrelogram, and as such, it is invariant to the overall magnitude of fluctuations 
(which is factored out by using the autocorrelation rather than the autocovariance of responses; Methods). Error bars show the mean ± s.e.m. (n = 50 stimuli).
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1145

Articles
NaTurE NEurOSciEncE
exhibited this effect in both membrane potentials and firing 
rates (Fig. 7c,d, top). Importantly, transient overshoots there-
fore inherited the orientation tuning of stationary responses 
(Fig. 7c,d, bottom). While stimulus-onset transients have been 
widely observed3,4, their stimulus-tuning has not been analyzed. 
Therefore, we analyzed a previously published dataset of V1 
responses in awake monkeys23. In line with the predictions of 
the model, overshoot sizes were orientation-tuned (Fig. 7e, bot-
tom) and, more generally, they scaled with the change in station-
ary responses (Fig. 7e, top). Note that these results were robust 
against excluding the outliers with high firing rates, for example, 
above 60 Hz (Extended Data Fig. 9d,e).
Discussion
We have shown that a canonical neural network model6,27 produces 
cortical-like dynamics when optimized for sampling-based infer-
ence, but not when optimized for non-probabilistic objectives or 
non-sampling-based probabilistic objectives. Further controls dem-
onstrated that these dynamics were not mere by-products of the 
particular biological constraints or the optimization approach we 
a
b
c
d
e
Single neuron
Full network
Experiment
0
2
4
0
2
4
–90°
0°
90°
0
10
20
0
20
40
Preferred orthogonal
0
50
100
0
2
4
0
5
10
0
50
100
0
20
40
Preferred orthogonal
Overshoot (mV)
Steady-state difference (mV)
Membrane potential
Overshoot (mV)
∆Orientation
Overshoot (Hz)
Steady-state difference (Hz)
Firing rate
Overshoot (Hz)
Overshoot (Hz)
Steady-state difference (Hz)
Firing rate
***
Overshoot (Hz)
***
5
10
0
100
E mean (mV)
Time (ms)
Time (ms)
Overshoot
Steady-
state
difference
0
5
10
0
100
Divergence (mean) (bits)
5
10
0
100
With overshoot
Exponential
Instantaneous
Target
Running average uE (mV)
Fig. 7 | Transients support continual inference. a,b, Analyses of transients in the response of a single neuron. a, The temporal evolution of the 
mean (left) membrane potential (uE), and its running average over a 100-ms time window (right), in three different neural responses (thick lines) 
with identical autocorrelations (matched to the neural autocorrelations in the full network, Extended Data Fig. 9a; see also Fig. 6) but different 
time-dependent means (shown here) and variances (Extended Data Fig. 9a). The thin green line shows the time-varying target mean. b, The 
divergence between the target distribution at a given point in time and the distribution represented by the neural activity sampled in the preceding 
100 ms for each of the three responses (colors as in a). The mean-dependent term of the divergence is shown here, which depends on the difference 
between the target mean and the running average of samples (shown in a, right; see Extended Data Fig. 9b for the full divergence). For a and  
b, the black bars show the stimulus period. The mean and divergence were computed as an average over multiple trials (n = 10,000). c, Top: the 
overshoot magnitude versus the steady-state difference in membrane potentials (see a for legend). Each dot corresponds to the response of one 
cell to one particular stimulus. Bottom: overshoot magnitude as a function of stimulus orientation (relative to preferred orientation). d, Top: as in 
c, top, but for firing rates. Bottom: the average rate overshoot across stimuli whose orientation is aligned with the preferred orientation of cells 
(0 ± 30°) or near-orthogonal to it (90 ± 30°). e, Analyses of experimental recordings from the V1 region of awake macaques23. Top: the overshoot 
magnitude versus the steady-state difference, as in d, top. The black line shows the linear regression (±95% confidence bands); two-sided Wald 
test ***P = 3 × 5−114 (n = 1,280 cell–stimulus pairs, R2 ≃ 0.33; see also Extended Data Fig. 9d,e). Bottom: each gray dot represents the average 
rate overshoot of one cell across stimuli whose orientation is aligned with the preferred orientation of the cell (0 ± 30°) or near-orthogonal to it 
(90 ± 30°). The mean ± s.e.m. of each group is presented in black, as in d; two-sided paired t-test ***P = 6 × 10−9 (n = 80 cells d.f. = 79). For a better 
comparison with d, the y range is truncated at 50 Hz, clipping two data points. Statistical analyses used all data.
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1146

Articles
NaTurE NEurOSciEncE
adopted. Instead, they played well-defined functional roles in per-
forming inference.
The GSM model and the stochastic SSN. We used a canonical 
model of neural network dynamics (the stochastic SSN) to embody 
a set of biologically relevant constraints for cortical circuits. It was 
not trivial a priori that this model would be able to modulate its 
responses as necessary for successful sampling-based inference 
under a canonical generative model of visual image patches (the 
GSM). A hint that this might indeed be possible came from previous 
studies showing that both in the SSN6,27 and the GSM5,39, a range of 
parameters exist for which the average response or posterior mean 
monotonically increases while the variance decreases with increas-
ing stimulus strength. Empirically, we found a good quantitative 
match that went beyond this coarse, qualitative trend, with the SSN 
also capturing much of the detailed structure of the GSM posteriors. 
However, this match was not perfect: for example, the GSM poste-
riors systematically showed negative correlations of larger magni-
tude than what the network was able to express (Figs. 2d and 3b,c). 
It might be possible to achieve a more accurate match by allowing 
negative input correlations and, in general, a more flexible param-
eterization of the SSN. Indeed, once the optimization of larger-scale, 
more flexibly parameterized SSNs becomes feasible, we also expect 
them to be able to sample from richer, deeper generative models.
Function-optimized neural networks. Our approach is comple-
mentary to classical approaches for training neural network mod-
els. Previous work showed how various steady-state properties of 
cortical responses (such as receptive fields or trial-averaged activi-
ties) emerge from optimizing neural networks for some computa-
tionally well-defined objective30,40–44. Notably, our sampling-based 
computational objective required our network to modulate 
not only the mean but also the variability of its responses in a 
stimulus-dependent manner. This made the training of networks 
significantly more challenging than conventional approaches in 
training networks for deterministic targets without explicitly requir-
ing them to modulate their variability40,43,45. In return, the dynam-
ics of our network exhibited rich, stimulus-modulated patterns of 
variability. These responses captured a variety of ubiquitous features 
of the trial-by-trial behavior of cortical responses (noise variability, 
transients and oscillations) beyond the steady-state or trial-average 
properties that could be addressed by previous work.
Typically, previous network-optimization approaches aimed to 
determine the types of dynamics that arise when a task is executed 
under minimal mechanistic constraints, using a neural network as 
a universal function approximator. As a result, they yielded fun-
damental insights into the macroscopic organization of network 
dynamics (for example, the presence of line attractors45), but did 
not attempt to incorporate some of the most salient constraints on 
the detailed organization of cortical circuits. Specifically, they used 
networks that were purely feedforward40, utilized neuronal transfer 
functions that lacked the expansive nonlinearity characteristics of 
cortical neurons40,42,44,45, had no separation of E and I cells40,44,45 or 
had noiseless dynamics43.
In contrast, our goal was to study the emergence of (proba-
bilistic) computations through dynamics and to connect these 
dynamics to experimental data at (or near) single-cell resolution 
(for example, the neuron-specific and stimulus-specific reduction 
of variability or the lag between total I and E inputs in individual 
cells). This required the consideration of all the aforementioned 
biological constraints. Nevertheless, this additional realism came at 
the cost of having to limit the number of optimized parameters to 
be far lower than standard approaches with feedforward networks 
or recurrent networks for which dynamical stability is more eas-
ily achieved. While this reduced parameterization made it easier to 
find stable solutions, it was still sufficiently expressive. In particular,  
we found that our results could not have been obtained without 
optimization (Extended Data Figs. 3 and 4) or with the optimiza-
tion of other objective functions (Fig. 5; Extended Data Figs. 5–7). 
Indeed, this parameterization still included networks that were 
unstable or showed a decrease in mean responses and/or increase in 
variability with increasing stimulus strength (which is the opposite 
of what was required for matching the GSM), or were modulated 
in a non-monotonic way or only minimally altogether (Extended 
Data Fig. 3).
Neural representations of uncertainty. Our approach markedly 
differed from previous work on the neural bases of probabilistic 
inference. Previous models were typically derived using a top-down 
approach (but see ref. 43), using hand-designed network dynamics 
that explicitly mimicked specific existing approximate inference 
algorithms from machine learning based on sampling33–36,46 or other 
representations12,19,33,47. As a result, these models came with strong 
theoretical guarantees for their performance, but often offered only 
a mostly phenomenological match to neural circuit dynamics. In 
particular, they did not respect some basic biological constraints 
(for example, Dale’s principle33,35,47) or had to assume an unrealisti-
cally rapid and direct influence of stimuli on network parameters 
(for example, synaptic weights35,46). In contrast, we used a more 
bottom-up approach, starting from known constraints of cortical 
circuit organization and then optimizing the parameters of net-
works under such constraints to achieve efficient sampling-based 
probabilistic inference, without prespecifying the details of the 
dynamics that needed to be implemented. While this approach 
cannot provide formal guarantees on performance, our optimized 
network ‘discovered’ novel algorithmic motifs (oscillations and 
transients) for speeding up probabilistic inference. Although some 
of these motifs have been observed in previous work46, their func-
tion remained unclear as they were built-in by design rather than 
obtained as a result of optimization or appeared purely epiphenom-
enal. In contrast, these motifs served computationally well-defined 
functions in our network.
The dynamics of our network may also provide useful clues for 
constructing novel machine-learning algorithms. In general, the 
kind of time-irreversible, out-of-equilibrium dynamics that we 
demonstrated for our network have only recently been appreciated 
in machine learning22,36. At the same time, sampling-based inference 
algorithms using second-order dynamics with so-called ‘momen-
tum’ variables, such as Hamiltonian Monte Carlo, have long been 
known to improve sampling speed31. Indeed, it might be interesting 
to explore how much the dynamics of our network can be inter-
preted as a neural implementation of Hamiltonian Monte Carlo46. 
Nevertheless, despite such second-order dynamical systems often 
exhibiting oscillations and transient overshoots, their sampling effi-
ciency has usually been analyzed only in the more generic terms of 
the suppression of random walk-like behavior. In contrast, our anal-
yses revealed specific roles for oscillations and transients. In fact, 
the setting of continual inference that we used to demonstrate the 
benefits of transients has not been considered in machine-learning 
applications so far, although we expect it to be highly relevant for 
both biological and artificial cognition.
Cortical variability, transients and oscillations. Our work suggests 
a novel unifying function for three ubiquitous properties of sensory 
cortical responses: stimulus-modulated variability, transient over-
shoots and gamma oscillations. In previous work, these phenomena 
have traditionally been studied in isolation and ascribed separate 
functional roles that have been difficult to reconcile. In particular, 
they have not been normatively derived; that is, by starting from 
some functional objective and then optimizing that objective in a 
principled manner (but see, for example, ref. 47). For example, corti-
cal variability has most often been considered a nuisance, thereby 
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1147

Articles
NaTurE NEurOSciEncE
diminishing the accuracy of neural codes23. Theories postulating 
a functional role of variability in probabilistic computations have 
only considered the steady-state distribution of responses with-
out making specific predictions about their dynamical features5,12. 
Conversely, transient responses prominently feature as central 
ingredients of models of predictive coding, where they signal nov-
elty or deviations between predicted and observed states47. However, 
these theories did not address response variability.
Our work accounts for both transients and variability starting 
from a single principle by using only the equivalent of ‘internal 
representation neurons’48 of predictive coding but without invok-
ing specific prediction error-coding neurons. In particular, our 
model correctly predicted a specific scaling relationship between 
transients and steady-state responses, which we tested via novel 
analyses of experimental data (Fig. 7). Furthermore, our math-
ematical analysis suggested that prediction-error-like signals (more 
formally, responses that scale with the magnitude of change in the 
target distribution; Extended Data Fig. 9c) are a generic signature 
of continual inference using sampling-based dynamics, and will 
therefore not only appear at stimulus onsets but in any situation 
when predictions temporally change. A conclusive test of whether 
prediction-error-like responses in the cortex are due to this mecha-
nism or to classical predictive coding mechanisms will require more 
specific manipulations of prior expectations.
The mechanism by which gamma oscillations are generated in 
the brain, particularly whether it involves interactions between E 
and I cells (the PING mechanism) or among I cells only (the ‘ING’ 
mechanism), is a subject of current debate16. In our model, voltage 
clamping of E cells eliminated gamma oscillations (Extended Data 
Fig. 2c,d), which points to the PING mechanism. However, our net-
work only included a single I cell type and heavily constrained con-
nectivity; therefore, future work is needed to study how the precise 
mechanism of gamma generation depends on such architectural 
constraints. Studying more hierarchical or spatially extended ver-
sions of our model may also allow us to study longer-range aspects 
of gamma oscillations, such as gamma synchronization49.
Gamma oscillations have also been proposed as a substrate for a 
number of functional roles in the past with respect to how informa-
tion is encoded, combined or routed in the brain8–10,49. These puta-
tive functions need not be mutually exclusive to that played in our 
network. Nevertheless, some of these functions seem difficult to 
reconcile with specific experimental findings3,14,15,50. More generally, 
theories of gamma oscillations do not typically address transients.
Predictive coding models naturally account for transients and 
can also account for gamma oscillations11. However, it is unclear 
whether these theories would also account for properties beyond 
the mere existence of gamma oscillations. These would include 
the frequency modulation by contrast3,4 that our model repro-
duced (Fig. 5), or indeed any aspect of the ubiquitous variability 
of cortical responses, and its modulation by stimuli, which our 
model also reproduced as a core feature (Figs. 2, 3 and 5). In con-
trast, our results showed that variability, transients and gamma 
oscillations can all emerge in neural circuits from the same func-
tional objective: representing uncertainty using a time-efficient 
sampling-based code.
Online content
Any methods, additional references, Nature Research report-
ing summaries, source data, extended data, supplementary infor-
mation, acknowledgements, peer review information; details of 
author contributions and competing interests; and statements of 
data and code availability are available at https://doi.org/10.1038/
s41593-020-0671-1.
Received: 5 July 2019; Accepted: 16 June 2020;  
Published online: 10 August 2020
References
	1.	 Churchland, M. et al. Stimulus onset quenches neural variability: a 
widespread cortical phenomenon. Nat. Neurosci. 13, 369–378 (2010).
	2.	 Haider, B., Häusser, M. & Carandini, M. Inhibition dominates sensory 
responses in the awake cortex. Nature 493, 97–100 (2013).
	3.	 Ray, S. & Maunsell, J. H. Differences in gamma frequencies across visual 
cortex restrict their possible use in computation. Neuron 67, 885–896 (2010).
	4.	 Roberts, M. et al. Robust gamma coherence between macaque V1 and V2 by 
dynamic frequency matching. Neuron 78, 523–536 (2013).
	5.	 Orbán, G., Berkes, P., Fiser, J. & Lengyel, M. Neural variability and 
sampling-based probabilistic representations in the visual cortex. Neuron 92, 
530–543 (2016).
	6.	 Hennequin, G., Ahmadian, Y., Rubin, D., Lengyel, M. & Miller, K. The 
dynamical regime of sensory cortex: stable dynamics around a single 
stimulus-tuned attractor account for patterns of noise variability. Neuron 98, 
846–860 (2018).
	7.	 Buzsáki, G. & Wang, X. Mechanisms of gamma oscillations. Annu. Rev. 
Neurosci. 5, 203–225 (2012).
	8.	 Gray, C., König, P., Engel, A. & Singer, W. Oscillatory responses in cat visual 
cortex exhibit inter-columnar synchronization which reflects global stimulus 
properties. Nature 338, 334–337 (1989).
	9.	 Akam, T. & Kullmann, D. Oscillations and filtering networks support flexible 
routing of information. Neuron 67, 308–320 (2010).
	10.	Masquelier, T., Hugues, E., Deco, G. & Thorpe, S. Oscillations, phase-of-firing 
coding, and spike timing-dependent plasticity: an efficient learning scheme.  
J. Neurosci. 29, 13484–13493 (2009).
	11.	Bastos, A. et al. Canonical microcircuits for predictive coding. Neuron 76, 
695–711 (2012).
	12.	Ma, W., Beck, J., Latham, P. & Pouget, A. Bayesian inference with 
probabilistic population codes. Nat. Neurosci. 9, 1432–1438 (2006).
	13.	Berkes, P., Orbán, G., Lengyel, M. & Fiser, J. Spontaneous cortical activity 
reveals hallmarks of an optimal internal model of the environment. Science 
331, 83–87 (2011).
	14.	Shadlen, M. & Movshon, J. Synchrony unbound: a critical evaluation of the 
temporal binding hypothesis. Neuron 24, 67–77 (1999).
	15.	Thiele, A. & Stoner, G. Neuronal synchrony does not correlate with motion 
coherence in cortical area MT. Nature 421, 366–370 (2003).
	16.	Tiesinga, P. & Sejnowski, T. Cortical enlightenment: are attentional gamma 
oscillations driven by ING or PING? Neuron 63, 727–732 (2009).
	17.	Knill, D. & Richards, W. Perception as Bayesian Inference (Cambridge Univ. 
Press, 1996).
	18.	Fiser, J., Berkes, P., Orbán, G. & Lengyel, M. Statistically optimal perception 
and learning: from behavior to neural representations. Trends Cogn. Sci. 14, 
119–130 (2010).
	19.	Deneve, S., Latham, P. & Pouget, A. Efficient computation and cue integration 
with noisy population codes. Nat. Neurosci. 4, 826–831 (2001).
	20.	Haefner, R., Berkes, P. & Fiser, J. Perceptual decision-making as probabilistic 
inference by neural sampling. Neuron 90, 649–660 (2016).
	21.	Bányai, M. et al. Stimulus complexity shapes response correlations in primary 
visual cortex. Proc. Natl Acad. Sci. USA 116, 2723–2732 (2019).
	22.	Sohl-Dickstein, J., Mudigonda, M. & DeWeese, M. R. Hamiltonian Monte 
Carlo without detailed balance. in International Conference on Machine 
Learning (eds Xing, E. P. & Jebarat, T.) 719–726 (2014).
	23.	Ecker, A. et al. Decorrelated neuronal firing in cortical microcircuits. Science 
327, 584–587 (2010).
	24.	Wainwright, M. & Simoncelli, E. Scale mixtures of Gaussians and the 
statistics of natural images. Adv. Neural Inf. Proc. Syst. 12, 855–861 (2000).
	25.	Coen-Cagli, R., Kohn, A. & Schwartz, O. Flexible gating of contextual 
influences in natural vision. Nat. Neurosci. 18, 1648–1655 (2015).
	26.	Schwartz, O., Sejnowski, T. & Dayan, P. Perceptual organization in the tilt 
illusion. J. Vis. 9, 19 (2009).
	27.	Ahmadian, Y., Rubin, D. & Miller, K. Analysis of the stabilized supralinear 
network. Neural Comput. 25, 1994–2037 (2013).
	28.	Priebe, N. & Ferster, D. Inhibition, spike threshold, and stimulus selectivity in 
primary visual cortex. Neuron 57, 482–497 (2008).
	29.	van Vreeswijk, C. & Sompolinsky, H. Chaotic balanced state in a model of 
cortical circuits. Neural Comput. 10, 1321–1371 (1998).
	30.	Hennequin, G., Vogels, T. & Gerstner, W. Optimal control of transient 
dynamics in balanced networks supports generation of complex movements. 
Neuron 82, 1394–1406 (2014).
	31.	MacKay, D. Information Theory, Inference and Learning Algorithms 
(Cambridge Univ. Press, 2003).
	32.	Murray, J. et al. A hierarchy of intrinsic timescales across primate cortex. Nat. 
Neurosci. 17, 1661–1663 (2014).
	33.	Grabska-Barwinska, A., Beck, J., Pouget, A. & Latham, P. Demixing odors 
- fast inference in olfaction. Adv. Neural Inf. Proc. Syst. 26, 1968–1976 (2013).
	34.	Buesing, L., Bill, J., Nessler, B. & Maass, W. Neural dynamics as sampling: a 
model for stochastic computation in recurrent networks of spiking neurons. 
PLoS Comput. Biol. 7, e1002211 (2011).
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1148

Articles
NaTurE NEurOSciEncE
	35.	Savin, C. & Deneve, S. Spatio-temporal representations of uncertainty  
in spiking neural networks. Adv. Neural Inf. Proc. Syst. 27, 2024–2032 
(2014).
	36.	Hennequin, G., Aitchison, L. & Lengyel, M. Fast sampling-based  
inference in balanced neuronal networks. Adv. Neural Inf. Proc. Syst. 27, 
2240–2248 (2014).
	37.	Okun, M. & Lampl, I. Instantaneous correlation of excitation and  
inhibition during ongoing and sensory-evoked activities. Nat. Neurosci. 11, 
535–537 (2008).
	38.	Carandini, M. & Heeger, D. Normalization as a canonical neural 
computation. Nat. Rev. Neurosci. 13, 51–62 (2012).
	39.	Echeveste, R., Hennequin, G. & Lengyel, M. Asymptotic scaling properties of 
the posterior mean and variance in the Gaussian scale mixture model. 
Preprint at https://arxiv.org/abs/1706.00925 (2017).
	40.	Yamins, D. et al. Performance-optimized hierarchical models predict  
neural responses in higher visual cortex. Proc. Natl Acad. Sci. USA 111, 
8619–8624 (2014).
	41.	Festa, D., Hennequin, G. & Lengyel, M. Analog memories in a balanced rate- 
based network of EI neurons. Adv. Neural Inf. Proc. Syst. 27, 2231–2239 
(2014).
	42.	Song, H., Yang, G. & Wang, X. Training excitatory–inhibitory recurrent 
neural networks for cognitive tasks: a simple and flexible framework. PLoS 
Comput. Biol. 12, e1004792 (2016).
	43.	Orhan, A. & Ma, W. Efficient probabilistic inference in generic neural networks 
trained with non-probabilistic feedback. Nat. Commun. 8, 138 (2017).
	44.	Remington, E., Narain, D., Hosseini, E. & Jazayeri, M. Flexible sensorimotor 
computations through rapid reconfiguration of cortical dynamics. Neuron 98, 
1005–1019 (2018).
	45.	Mante, V., Sussillo, D., Shenoy, K. & Newsome, W. Context-dependent 
computation by recurrent dynamics in prefrontal cortex. Nature 503,  
78–84 (2013).
	46.	Aitchison, L. & Lengyel, M. The Hamiltonian brain: efficient probabilistic 
inference with excitatory–inhibitory neural circuit dynamics. PLoS Comput. 
Biol. 12, e1005186 (2016).
	47.	Rao, R. & Ballard, D. Predictive coding in the visual cortex: a functional 
interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 
79–87 (1999).
	48.	Keller, G. & Mrsic-Flogel, T. Predictive processing: a canonical cortical 
computation. Neuron 100, 424–435 (2018).
	49.	Vinck, M. & Bosman, C. More gamma more predictions: 
gamma-synchronization as a key mechanism for efficient integration of 
classical receptive field inputs with surround predictions. Front. Syst. Neurosci. 
10, 35 (2016).
	50.	Roelfsema, P., Lamme, V. & Spekreijse, H. Synchrony and covariation of 
firing rates in the primary visual cortex during contour grouping.  
Nat. Neurosci. 7, 982–991 (2004).
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in 
published maps and institutional affiliations.
© The Author(s), under exclusive licence to Springer Nature America, Inc. 2020
Nature NeuroscIence | VOL 23 | September 2020 | 1138–1149 | www.nature.com/natureneuroscience
1149

Articles
NaTurE NEurOSciEncE
Methods
Ideal observer model. Following refs. 5,25, we adopted the GSM model24 as the 
generative model of natural image patches under which V1 performs inference. 
Thus, an image patch x 2 RNx was assumed to be constructed by linearly 
combining a set of local features, the columns of A 2 RNx ´ Ny, weighted by a set of 
image patch-specific feature coefficients, y 2 RNy, and scaled by a single global (at 
the scale of the image patch) contrast variable, z 2 R, plus additive white Gaussian 
noise, which results in the following likelihood for the feature coefficients y:
xjy; z N z A y; σ2
x I


ð1Þ
where the feature coefficients were assumed to be drawn from a multivariate 
Gaussian prior distribution
y N 0; C
ð
Þ
ð2Þ
and z was assumed to be drawn from a Gamma prior: z Γ K; ϑ
ð
Þ (Supplementary 
Table 1, see also ref. 39).
To model inferences in a V1 hypercolumn, we chose the columns of A (the 
so-called projective fields of the latent variables) to be oriented as Gabor filters 
that only differed by their orientation, evenly spaced between –90° and 90° 
(four examples are shown in Fig. 1a; see also Extended Data Fig. 1a). The prior 
covariance matrix C was a circulant matrix whose elements varied smoothly as a 
function of the angular distance between the orientations of the projective fields of 
the corresponding latent variables, from positive (for similarly oriented projective 
fields) to negative (for orthogonally oriented projective fields) (Extended Data  
Fig. 1b).
The ideal observer’s posterior over the latent feature coefficients y under the 
GSM for a given image patch, x and a known contrast z, can be written as follows5:
PGSM yjx; z
ð
Þ ¼ N y; μGSM; ΣGSM


ð3Þ
with μGSM ¼ z
σ2
x
ΣGSM AT x
ð4Þ
and ΣGSM ¼
C1 þ z2
σ2
x
AT A

1
ð5Þ
In general, z would also need to be inferred. However, as z is just a single scalar 
of which the inference pools information across all pixels in the input, we 
approximated the posterior over z with a delta distribution at z*, the true value 
of z that was used to generate the input39. Thus, the final posterior over y, after 
marginalizing out the unknown z, was approximated by substituting z* into 
equation (3) as follows:
PGSM yjx
ð
Þ ’ PGSM yjx; z
ð
Þ
ð6Þ
Following ref. 5, membrane potentials, u, were taken to represent a weakly 
nonlinear function of the visual feature activations y (Supplementary Math Note) 
as follows:
ui yi
 
¼ αnl
yi þ βnl

γnl
ð7Þ
where ⌊ ⋅ ⌋ is the threshold-linear function, and αnl, βnl and γnl are the scaling, 
baseline and power, respectively, of the transformation (Supplementary Table 1; 
Supplementary Fig. 1a).
Network dynamics and architecture. Our nonlinear, stochastic E/I network 
consisted of NE excitatory and NI inhibitory neurons. Following ref. 6, we modeled 
the dynamics of each neuron i as follows:
τi
dui
dt ¼ ui tð Þ þ hi tð Þ þ
X
jWij rj tð Þ þ ηi tð Þ
ð8Þ
where ui represented the membrane potential of neuron i, τi was its membrane 
time constant, hi its feedforward input, ηi was process noise (incorporating 
intrinsic and extrinsic forms of neural variability) and Wij was the weight of the 
synapse connecting neuron j to neuron i. Firing rates ri were given by a supralinear 
transformation of the membrane potentials as follows:
ri tð Þ ¼ k ui tð Þ
b
cn
ð9Þ
where k and n are the scale and exponent, respectively, of the firing-rate 
nonlinearity (Supplementary Table 1).
We reasoned that any network performing accurate sampling-based inference 
under our ring-structured GSM would need to exhibit the same circular symmetry. 
We therefore parameterized the recurrent connectivity of the network to be 
rotationally symmetric, such that neurons were arranged into pairs of E and I 
cells around a ring according to their preferred orientations (Fig. 1c), and the 
connectivity of the network (as well as the process noise covariance, see below) was 
a smoothly decaying (circular Gaussian) function of the tuning difference between 
two cells. Specifically, each quadrant of the weight matrix (E → E, E → I, I → E and 
I → I) was defined as follows:
WXYðθi; θjÞ ¼ aXY exp cos 2 θi  θj




 1
d2
XY


ð10Þ
where X, Y ∈ {E; I} and θi = πi/NE/I was the orientation represented by the ith E/I 
neuron. Thus, we did not optimize all elements of the weight matrix, but only 
the eight free parameters aXY and dXY. We also constrained the aXY amplitudes to 
be positive for Y = E and negative for Y = I, such that the network obeyed Dale’s 
principle. This circulant parameterization implied that training the network on 
one particular stimulus–posterior pair in effect trained the network on all possible 
rotations of this pair. This reduced the size of the training set necessary to achieve 
good generalization, and therefore sped up training.
The stimulus-independent process noise (the last term in equation (8)) was 
spatially and temporally correlated to zero-mean Gaussian (for example, modeling 
inputs from other brain areas or intrinsic variability in the network) as follows:
ηðtÞ
h
i ¼ 0; ηðtÞ ηðt þ sÞT
D
E
¼ Ση exp s=τη


ð11Þ
where τη was the timescale of the process noise (Supplementary Table 1) and Ση was 
the stationary (zero-lag) covariance matrix parameterized block-wise as follows:
Ση
EE=IIðθi; θjÞ ¼ σ2
E=I exp cos 2 θi  θj




 1
d2
σ
 
!
ð12Þ
Ση
EIðθi; θjÞ ¼ ρ σE σI exp cos 2 θi  θj




 1
d2
σ
 
!
ð13Þ
which introduced four additional free parameters: σE > 0, σI > 0, ρ and dσ.
As in standard models of V1 simple cells51, the stimulus-dependent input to 
each neuron was obtained by applying a linear filter Wff to the stimulus followed by 
a static nonlinearity as follows:
hi tð Þ ¼ αh
βh þ
X
jWff
ij xj tð Þ
j
kγh
ð14Þ
where x tð Þ was the stimulus (input image patch) received at time t, and αh, βh and 
γh were the scale, baseline and exponent, respectively, of the input nonlinearity 
(Supplementary Table 1; Extended Data Fig. 1e). Given the one-to-one 
correspondence between the latent variables of the GSM and E–I neuron pairs of 
the network model (Fig. 1a,c), we determined the external input to each neuron via 
an input receptive field that was identical (up to a constant factor) to the projective 
field of the corresponding GSM latent variable, as this was suggested to be optimal 
for sampling by previous work30: Wff ¼ A A
½
T=15, where A was the same matrix 
as in the generative model (equation (1)), and [A A] denotes concatenating A with 
itself column-wise.
In summary, we optimized a total of 15 parameters: 8 describing the weight 
matrix W (equation (10)), 4 describing Ση (equations (12) and (13)) and 3 
specifying the mapping from stimuli to network inputs (equation (14)).
Computing the moments of neural responses. For every time t relative to 
stimulus onset, we denoted the across-trial moments of neural responses by
μ tð Þ ¼ u tð Þ
h
i
ð15Þ
Σ t; Δt
ð
Þ ¼
u tð Þ  μ tð Þ
ð
Þ u t þ Δt
ð
Þ  μ t þ Δt
ð
Þ
ð
ÞT
D
E
ð16Þ
where 〈 ⋅ 〉 denotes trial-averaging. To compute these moments, we employed two 
different methods. The first approach, which we refer to as the ‘stochastic method’, 
consisted of approximating the averages via sampling; that is, simulating stochastic 
network dynamics in a set of trials using the same stimulus (equations (8), (9) and 
(14)) and computing the across-trial sample mean and sample covariance at each 
time step. We used this approach in the first phase of network training and for 
obtaining results from the network once it was trained (see below).
The second approach, which we refer to as ‘assumed density filtering’ (ADF), 
used deterministic equations of motion for computing the across-trial moments. 
This approach was only used in the last phase of network training (see below). 
Based on ref. 52, the following exact differential equations were used to describe the 
evolution of μ tð Þ and Σ t; 
ð
Þ:
dμ tð Þ
dt
¼ T1 μ tð Þ þ h tð Þ þ W ν tð Þ
½

ð17Þ
dΣ t; 0
ð
Þ
dt
¼ T1 Σtð Þ
½
þ T1 Σtð Þ
½

Tþ
þJ tð Þ Σ t; 0
ð
Þ þ Σ t; 0
ð
Þ J ðtÞT
ð18Þ
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
dΣtð Þ
dt
¼  1
τη
Σtð Þ þ Ση T1 þ Σtð Þ J tð ÞT
ð19Þ
dΣ t; Δt
ð
Þ
dΔt
¼ eΔt=τη T1 Σtð Þ
½

Tþ
þΣ t; Δt
ð
Þ J t þ Δt
ð
ÞT
8Δt>0
ð20Þ
where T is the diagonal matrix of membrane time constants (Supplementary 
Table 1), ν = 〈r〉 is the average firing rate of neurons, Σ t; Δt
ð
Þ ¼ ΣT t; Δt
ð
Þ 
is the time-lagged cross-covariance of membrane potentials in the network, 
Σ¼
η u  μ
ð
ÞT
D
E
 is the instantaneous cross-covariance between membrane 
potentials and temporally correlated process noise with instantaneous covariance 
Ση (equation (11)), and
J ¼ T1
I þ W diag ∂ν
∂μ




ð21Þ
is the Jacobian of equation (17) with respect to μ. Integrating equations (17) to (20) 
in turn required evaluating some nonlinear moments of u, namely, covariances 
between membrane potentials, u, and firing rates, r. For the SSN, these moments 
can be obtained in closed form, assuming that the full joint (space–time) 
distribution of membrane potentials is Gaussian52. Thus, in contrast to the first 
(stochastic) method, which leads to unbiased, but potentially high-variance, 
estimation of the moments, the ADF method leads to zero-variance, but potentially 
biased estimates. To train the network, as described below, we combined the 
strengths of these two approaches.
Training and test stimuli and target moments. The training set (Fig. 2b) 
consisted of five image patches:
xα ¼ zα A y
ð22Þ
with α = 1, . . . , 5 and zα ∈ {0, 0.125, 0.25, 0.5, 1.0}. Therefore, these stimuli had 
the same content y—a 27°-wide Gaussian function centered around 0° (that is a 
single dominant orientation)—and differed only in their contrast (Fig. 2b). As the 
parameterization of our network was rotationally invariant (see above), such a 
stimulus was in fact representative of all image patches that could be obtained by 
rotating this patch around the center. For each training stimulus xα, we computed 
the corresponding posterior distributions over u under the GSM (equations (6)  
and (7)). We called these distributions the ‘target distributions’, and their 
corresponding means μα
tgt and covariances Σα
tgt, the ‘target moments’ (Fig. 2c,d):
μα
tgt ¼
Z
u y
ð Þ PGSM yjxα
ð
Þdy
ð23Þ
Σα
tgt ¼
Z
u y
ð Þ uT y
ð Þ PGSM yjxα
ð
Þdy  μα
tgt μα
tgt
T
ð24Þ
To test for generalization in the network, we generated a set of 500 novel image 
patches with the GSM, which were therefore not constrained to have a single 
dominant orientation (as the prior allowed multiple elements of y with different 
projective fields to be non-zero, equation (2)). To be consistent with the training 
set, we did not include additive noise in x, and added a contrast-dependent baseline 
to y so that its mean was modulated by contrast in the same way as in the training 
set. For each image patch in the test set, we also computed the corresponding 
posterior moments (equations (23) and (24)) to evaluate the test performance of 
the network.
Network training. The cost function F, which we minimized during network 
training, consisted of the following four terms for each input stimulus α in the 
training set:
F ¼ P
α ϵmean ϕα
mean

þ ϵvar ϕα
var þ
þ ϵcov ϕα
cov þ ϵslow ϕα
slow

ð25Þ
The first three terms of equation (25) penalized differences between the 
(across-trial) moments of the response distribution of the network (equations 
(15) and (16)) averaged over a finite time window ending at Tmax ¼ 500 ms after 
stimulus onset and the respective target moments of the corresponding posterior 
distributions of the ideal observer (equations (23) and (24)) as follows:
ϕα
mean ¼
Z Tmax
Tmin
μα tð Þ  μα
tgt


2
F dt
ð26Þ
ϕα
var ¼
Z Tmax
Tmin
σα tð Þ  σα
tgt


2
F dt
ð27Þ
ϕα
cov ¼
Z Tmax
Tmin
Σα t; 0
ð
Þ  Σα
tgt


2
F dt
ð28Þ
where σα tð Þ ¼ diag Σα t; 0
ð
Þ
ð
Þ and σα
tgt ¼ diag Σα
tgt


 are the response and target 
variances, respectively. The last term of equation (25) was an additional slowness 
cost, which penalized the total lagged neural response autocorrelation, given by the 
diagonal of C(τ) = corr(u(t), u(t + τ)), within a τmax = 100-ms time window:
ϕα
slow ¼
Z τmax
0
diagðCαðτÞÞ
k
k2
F dτ
ð29Þ
The coefficients ϵ controlled the relative importance of these terms (Supplementary 
Table 1). In the first control network (Fig. 5, right column; Extended Data Figs. 5a–
g and 6), we set ϵvar = ϵcov = ϵslow = 0, but kept all other meta-parameters and target 
means the same. In the second control network (Extended Data Figs. 5h–n and 6), 
we set only ϵcov = ϵslow = 0, but left ϵvar and other meta-parameters the same as in the 
original network. In the third control network (Extended Data Figs. 6 and 7,  
right), all ϵ⋯ parameters were the same as for the optimization of the original 
network, but the target covariances were modified to induce contrast-independent 
Fano factors (see below).
Optimization involved back-propagation through time53, for which we used 
automatic differentiation. We trained the network in two stages. During the first 
stage, we employed a stochastic gradient method using Ntrial = 50 trials for each 
training stimulus to estimate the corresponding moments of network responses 
(see above), and performed 250 iterations of the ADAM optimizer54. Both the 
initial conditions and the process noise of the network were re-sampled for each 
trial and iteration. Initial conditions were drawn from a Gaussian distribution 
N μ0; Σ0


 (Supplementary Table 1). Moreover, across iterations, the beginning 
of the averaging time window, Tmin in equations (26) to (28), was systematically 
changed (‘annealed’) from Tmin = 0 ms (stimulus onset) to Tmax − 50 ms. The finite 
length of the averaging window, in particular including samples immediately 
or shortly following stimulus onset, encouraged fast sampling. Thus, setting the 
explicit slowness cost ϵslow = 0 did not qualitatively affect our results (Extended Data 
Figs. 6 and 10).
In the second stage, we continued optimization using the L-BFGS-B 
optimizer55, now using the ADF method to (deterministically) compute the 
moments of the response distribution of the network (see above). We kept the 
cost-integration time window at its minimum (Tmax − Tmin = 50 ms, as reached by the 
end of the first phase). The slowness penalty cost in equation (29) was only applied 
during ADF-based optimization and, for simplicity, it was approximated using 
stationary-lagged correlations predicted by the ADF method (equation (20)) in the 
limit of temporally white process noise52.
As the cost function that we used (equation (25)) was non-convex, we checked 
the robustness of our findings by performing ten further optimization attempts 
from random initial conditions. No solutions achieved substantially lower costs, 
and those whose final cost was at least approximately as low as the network 
presented in the main text behaved qualitatively similarly (in particular, they 
showed contrast-dependent oscillations and transients; Extended Data Fig. 4). 
Nevertheless, our results should not be taken to represent a global optimum of our 
cost function.
Langevin sampling. As a comparison (Fig. 4), we also implemented Langevin 
dynamics31 to sample from the same target posteriors as those used to train the 
optimized network. As the GSM target posteriors were Gaussian (equations (3) 
and (6)), the resultant Langevin dynamics were isomorphic to that of a generic 
stochastic linear recurrent neural network:
τE _u ¼ WL u þ h þ η
ð30Þ
where, for a fair comparison, τE and η were the same time constant and process 
noise, respectively, as in our optimized network (equations (8) and (11) to (13)). 
Without loss of generality, we set the input to the network h = 0, as it would be 
completely determined by the requirement to match the response mean to the 
target mean, μtgt, but would not affect the autocorrelogram of the system, which 
was the focus of our investigation here. As variability in a linear network does not 
depend on the input (unlike in our nonlinear circuit model), we used a different WL  
to match each target covariance, Σtgt:
WL ¼ 1
γL
I 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
I þ γ2
L Ση Σ1
tgt
q


ð31Þ
where γL = 2τη/τE.
Numerical experiments after training. To obtain a reliable estimate of the 
stationary moments of neural responses to a fixed input (Figs. 2 and 3), a 
total of 20,000 independent samples (taken 200-ms apart) were drawn from 
the network, not including transients, as neural activity evolved according to 
equation (8). The neural activities in Fig. 2a show 1 s of simulated network 
activity, convolved with a 20-ms sliding window to match the effects of spike 
binning to compute average rates in experiments. The neural trajectories in 
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Fig. 2b correspond to the neural activity of two cells in the network with the 
preferred orientations 42° (ui) and 16° (uj), over a post-transient period of 
500 ms. To illustrate both the degree of modulation of the posterior covariances 
and the match between posterior and network covariances in Fig. 3c, the top 
three PCs of each posterior covariance were computed. Neural activity was then 
projected onto each PC, and the amount of variance along each direction was 
computed. The middle plots of Fig. 3c present these posterior PCs scaled by 
either the square root of the total variance along that direction in the GSM (in 
green) or in the network (in red).
The autocorrelograms in Fig. 4a were computed in 500 non-overlapping 
windows of 2 s of simulated neural activity each (subsampled at 0.4 ms) after 
stimulus onset (excluding transients), and then averaged across these windows. 
Autocorrelograms were first computed for the membrane potentials of individual 
cells and then averaged across all cells. The cross-correlograms and E–I lags 
in Fig. 4b,c were computed from a single 400-s-long simulation after stimulus 
onset, excluding transients (without subsampling). The E–I lag for each cell was 
determined as the location of the maximum in the anti-symmetric component 
of the cross-correlogram between its total E and I input. The Langevin 
samplers in Fig. 4a,b correspond to neural networks with linear, time-reversible 
dynamics, not respecting Dale’s principle, as defined by equations (30) and 
(31). Autocorrelograms and cross-correlograms for the Langevin sampler were 
computed as for the original network.
The average firing rates in Fig. 5a were computed from the same neural traces 
used in Fig. 2 to compute u moments (here, taking the average of r instead of 
u). To compute the Fano factors in Fig. 5b, we used an inhomogeneous Gamma 
process with the time-varying rate given by ri tð Þ for each neuron i, and the shape 
of the inter-spike interval distribution controlled by an additional parameter, KISI 
(Supplementary Table 1). We computed spike counts in a 100-ms window over 
500,000 independent trials. Our results were qualitatively robust to the choice 
of KISI, which primarily determined the overall magnitude of Fano factors—in 
particular KISI > 1 was needed to achieve Fano factors < 1 at high contrast—but not 
their modulation by stimuli.
The power spectra in Fig. 5c were based on simulated local field potentials 
(LFPs), computed as the (across-cell) average neural activity (membrane 
potentials) following standard approaches6 and using the same samples as the 
autocorrelograms of Fig. 4a (see above). Gamma peak frequency was identified 
as the location of the local maximum (within the gamma band, 20–80 Hz) of the 
power spectrum. The transients in Fig. 5d were computed from average firing 
rates across E cells and trials (n = 100), further averaged over a sliding 10-ms time 
window to mimic the resolution of experimental data. To account for the response 
delays observed in experimental data, we used a random delay time (truncated 
Gaussian, with 45-ms mean and 5-ms s.d.) for the feedforward input of each E–I 
cell pair in the network.
To estimate input conductance values (Fig. 5e), we equated the total 
(E or I) input current in our model to the (E or I) current in a canonical 
conductance-based model51. This gave the following expression:
gE=I
i
tð Þ 
Cm
P
j2E=I Wij rj tð Þ
τi
VE=I  ui tð Þ þ Vrest
ð
Þ


ð32Þ
where Cm is the membrane capacitance, VE/I denote the reversal potentials for E/I 
currents and Vrest is a baseline (resting) potential added to the membrane potentials 
of our model. We chose Cm = 20.0 pF, VE = 0 mV, VI = –80 mV and Vrest = –65 mV. 
The conductance traces in Fig. 5e are shown relative to their steady-state values 
during spontaneous activity and averaged across 20 trials for a single neuron with 
the preferred orientation aligned to that of the stimulus.
The autocorrelograms and power spectra of Fig. 6c were computed as in Figs. 
4a and 5c, but for the directions in the space of neural responses that corresponded 
to the first ten PCs of neural variability. To quantify oscillatoriness in neural 
responses along some direction in state space (PC (Fig. 6d) or LFP (Extended Data 
Fig. 4b)), we computed the corresponding projection of neural responses and then 
fitted the following parametric function to its autocorrelogram:
Cχ Δt
ð
Þ ¼
1  χ
ð
Þ þ χ cos 2π f Δt
ð
Þ
½
´
´
^τ
^τ^τη e Δt
j
j
^τ 
^τη
^τ^τη e Δt
j
j
^τη
h
i
ð33Þ
where ^τ and ^τη are two time constant parameters, χ ∈ [0, 1] quantifies the degree of 
oscillatoriness and f represents the dominant oscillation frequency. Fits of equation 
(33) to simulated network activity were performed using Tensorflow. The form of 
equation (33) was motivated by noting that, for χ = 0, it reduces to
C0 Δt
ð
Þ ¼
^τ
^τ  ^τη
e Δt
j
j
^τ 
^τη
^τ  ^τη
e Δt
j
j
^τη
ð34Þ
which is the autocorrelation function of the fluctuations in a single (isolated, and 
therefore not oscillating) neuron with the membrane time constant ^τ receiving 
noisy inputs with the correlation time ^τη (this can be seen by integrating equations 
(18) to (20)). More generally, for χ > 0, C0 Δt
ð
Þ (equation (34)) determines the 
envelope of Cχ Δt
ð
Þ (equation (33)):
1  2 χ
ð
Þ C0 Δt
ð
Þ≤Cχ Δt
ð
Þ≤C0 Δt
ð
Þ
ð35Þ
The overshoots in Fig. 7c,d were obtained using the same stimulus that was 
used to train the network at 0.7 contrast, and computed as the maximal across-trial 
average (n = 100) response of each E cell (membrane potential for Fig. 7c, firing 
rate for Fig. 7d), minus its stationary mean response, further averaged over 1,000 
delay configurations in our network (as for Fig. 5d, see above). Steady-state 
differences denote the magnitude of mean evoked responses of each cell with 
respect to its mean pre-stimulus response. The results in the bottom plot of Fig. 7d 
were computed by averaging the stimuli presented at the preferred orientation for 
each neuron (±30°) or orthogonal to its preferred orientation (±30°).
Analyses with Gaussian processes. For Fig. 6a,b, we considered a stationary, 
moment-matched Gaussian process (GP56) with an autocorrelation function 
that was in the same parametric form as that fitted to network responses, Cχ Δt
ð
Þ 
(equation (33)), by setting both time constants to their values characterizing the 
network (^τη ¼ ^τ ¼ τE ¼ τη ¼ 20 ms). This yielded the following form:
CGP Δt
ð
Þ ¼
1  χ
ð
Þ þ χ cos 2π f Δt
j
j
ð
Þ
½
´
´ 1 þ Δt
j
j
τE


e Δt
j
j
τE
ð36Þ
We used χ ∈ {0, 0.2, 0.7} (where χ = 0 corresponds to an isolated single neuron) 
at f = 40 Hz. We set the mean and variance of the GP to μGP = 3 and σ2
GP ¼ 4, 
respectively, without loss of generality (Supplementary Math Note).
For Fig. 7a,b and Extended Data Fig. 9, we used GPs whose stationary 
distributions all matched the same target distribution (see below) before stimulus 
onset (time t < 0), and converged to the same new target distribution after stimulus 
onset (for t ≫ 0). These GPs differed only in their transient behavior. The GP 
that most faithfully captured our optimized network was constructed by taking 
the temporal evolution of the mean and variance of an actual neuron in the full 
network around the stimulus onset, including overshoots (‘with overshoots’ in Fig. 
7a; Extended Data Fig. 9a, red). For this, we chose the neuron whose preferred 
orientation matched that of the presented stimulus. Moreover, the autocorrelogram 
of this GP was also set to match the autocorrelogram of that neuron (inset of 
Extended Data Fig. 9a, right). Two additional GPs were constructed with the same 
autocorrelogram as the first GP, but different mean and variance time courses. In 
one, the mean and variance converged exponentially with a time constant of τE 
(‘exponential’ in Fig. 7a; Extended Data Fig. 9a, dashed black line). In the other, the 
mean and variance immediately jumped at stimulus onset to their new stationary 
values (‘instantaneous’ in Fig. 7a; Extended Data Fig. 9a, dashed gray line). While 
such an instantaneous process is not realizable by any continuous dynamical 
system, it provides a useful lower bound on sampling error.
For Fig. 6b and Extended Data Fig. 9b, we used the average symmetrized 
Kullback–Leibler (SKL) divergence between the GSM target distribution, P, and 
the distribution sampled by the GP, QT over some finite time window T to measure 
the performance of these systems:
DSKL PkQT
½
¼ 1
4
E2 T
ð Þ σ2
P þ σ2
Q T
ð Þ




þ

þ σ2
Q T
ð Þ


σ2
P þ σ2
Q T
ð Þ


σ2
P  2

ð37Þ
where 〈 ⋅ 〉 denotes trial-averaging (as above), E T
ð Þ ¼ μQ T
ð Þ  μP, and μP, 
μQ T
ð Þ, σ2
P and σ2
Q T
ð Þ are the means and variances of P and QT, respectively 
(Supplementary Math Note). Figure 7b only shows the term of DSKL that depends 
on the sample mean, μQ T
ð Þ (via E2 T
ð Þ):
D
μ
SKL ¼ 1
4 E2 T
ð Þ σ2
P þ σ2
Q T
ð Þ




ð38Þ
For Fig. 6b, we used a single target distribution (matching the stationary moments 
of the GP, see above) and averaged over n = 100,000 trials of sampling from the 
GP for an increasing amount of time, T (x axis), as we also varied χ (lines). GPs 
were sampled at a dt = 0.5-ms time resolution. For Fig. 7b and Extended Data 
Fig. 9b, the target distribution was instantaneously changed at the stimulus onset 
(t = 0) from a response distribution corresponding to a zero-contrast stimulus 
(spontaneous activity) to that corresponding to a high contrast stimulus (training 
image at contrast level 0.7) (Fig. 7a; Extended Data Fig. 9b, green lines). Response 
distributions were obtained from the E cell in the original network that was tuned 
to the dominant orientation of the stimulus. At each time point t (x axis), DSKL 
(or D
μ
SKL) was computed between the momentary target distribution and the 
distribution of GP samples collected in a 100-ms-long sliding time window ending 
at t, averaging over n = 10,000 trials, for each of the three transition profiles (lines). 
GPs were sampled at a dt = 1-ms time resolution.
Experimental data analysis. Figure 7e shows novel analyses of experimental 
recordings from V1 of awake macaques during the presentation of moving gratings 
of different orientations23. Following the same procedure as in ref. 6, only cells that 
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
were significantly tuned (orientation tuning index greater than 0.75) and had an 
average evoked rate above 1 spike per second were included in the analysis. For 
each cell and each stimulus, a time-dependent firing rate trace was first obtained 
by averaging spikes across trials in a 50-ms sliding square window. From these 
traces, the steady-state difference and overshoot size (dots in Fig. 7e, top) were 
then computed as the average evoked response excluding transients (t > 160 ms 
after stimulus onset) and the maximum of the response trace during the transient 
(t < 160 ms after stimulus onset), respectively, minus the average baseline response 
(computed from the 300 ms before stimulus presentation). The results in the 
bottom plot of Fig. 7e were computed in the same way as for model neurons  
(see above).
Statistics. Sample sizes (number of trials) were chosen using the following criteria. 
First, when studying properties of the network, a large enough n was selected 
(>10,000 independent samples) such that standard errors would be smaller than 
line widths in the corresponding plots. Second, when comparing the behavior 
of the network to experimental results, the same order of magnitude for n was 
selected as in the original experiments.
Linear regressions in Fig. 7e (top) and Extended Data Fig. 9d were performed 
using the ‘linregress’ function of SciPy, which reports a two-sided P value using a 
Wald test with a t-distribution of the test statistic.
For Fig. 7e (bottom), we tested for significance in overshoot tuning using a 
two-sided paired t-test using the ‘ttest_rel’ function of SciPy. Data distribution was 
assumed to be normal, but this was not formally tested.
Randomization. No new experimental data were gathered for this paper.
Blinding. As data collection had been performed by other research groups 
considerably earlier than our study, it was effectively blind for the purposes of 
our study. Data analyses were not performed blinded to the conditions of the 
experiments.
Data exclusion. In Fig. 7e, only cells with a tuning index greater than 0.75 were 
selected. No other data were excluded in the main text. In Extended Data Fig. 9, the 
same analysis of Fig. 7e was repeated but excluding outliers to test for robustness.
Reporting Summary. Further information on research design is available in the 
Nature Research Reporting Summary linked to this article.
Data availability
All experimental data reported here have been collected by others and previously 
published. The experimental data in Fig. 5a,b reproduced analyses from ref. 6 of data 
from ref. 23 (data released in the repository of ref. 57). The experimental data in Fig. 
5c,d were captured directly from the plots in the original papers2–4. The experimental 
data in Fig. 7 are a novel analysis of the data from ref. 23, with permission from 
the authors. The code and parameters used to generate the data corresponding to 
numerical experiments in the paper have been made publicly available.
Code availability
The (Python) code and parameters for the numerical experiments are available 
at https://bitbucket.org/RSE_1987/ssn_inference_numerical_experiments. The 
(OCaml) code for the optimization procedure can be found at https://bitbucket.
org/RSE_1987/ssn_inference_optimizer.
References
	51.	Dayan, P. & Abbott, L. Theoretical Neuroscience: Computational and 
Mathematical Modeling of Neural Systems (MIT Press, 2001).
	52.	Hennequin, G. & Lengyel, M. Characterizing variability in nonlinear 
recurrent neuronal networks. Preprint at https://arxiv.org/abs/1610.03110 
(2016).
	53.	Werbos, P. Backpropagation through time: what it does and how to do it. 
Proc. IEEE 78, 1550–1560 (1990).
	54.	Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. Preprint 
at https://arxiv.org/abs/1412.6980 (2014).
	55.	Zhu, C., Byrd, R., Lu, P. & Nocedal, J. Algorithm 778: L-BFGS-B: Fortran 
subroutines for large-scale bound-constrained optimization. ACM Trans. 
Math. Software 23, 550–560 (1997).
	56.	Williams, C. & Rasmussen, C. Gaussian Processes for Machine Learning Vol. 2 
(The MIT Press, 2006).
	57.	Ecker, A. et al. State dependence of noise correlations in macaque primary 
visual cortex. Neuron 82, 235–248 (2014).
Acknowledgements
This work was supported by the Wellcome Trust (New Investigator Award 
095621/Z/11/Z and Investigator Award in Science 212262/Z/18/Z to M.L., and Seed 
Award 202111/Z/16/Z to G.H.), and the Human Frontiers Science Programme (research 
grant RGP0044/2018 to M.L.). We are grateful to A. Ecker, P. Berens, M. Bethge and A. 
Tolias for making their data publicly available; to G. Orbán, A. Bernacchia, R. Haefner 
and Y. Ahmadian for useful discussions; and to J. P. Stroud for detailed comments on the 
manuscript.
Author contributions
R.E., G.H. and M.L. designed the study. R.E. and G.H. developed the optimization 
approach. R.E. ran all numerical simulations. R.E. and G.H. analyzed the experimental 
data, and all authors performed analytical derivations. R.E., G.H. and M.L. interpreted 
results and wrote the paper, with comments from L.A.
Competing interests
The authors declare no competing interests.
Additional information
Extended data is available for this paper at https://doi.org/10.1038/s41593-020-0671-1.
Supplementary information is available for this paper at https://doi.org/10.1038/
s41593-020-0671-1.
Correspondence and requests for materials should be addressed to R.E.
Peer review information Nature Neuroscience thanks Jeff Beck, Jeffrey Erlich, and the 
other, anonymous, reviewer(s) for their contribution to the peer review of this work.
Reprints and permissions information is available at www.nature.com/reprints.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 1 | GSM and network parameters. a, Filters: projective fields of the GSM and receptive fields of the network. Each filter image shows 
the projective field of a latent variable (columns of A; Eq. (1)), which was the same as the receptive field of the corresponding E-I cell pair in the network 
(rows of Wff ¼ A A
½
T=15; Eq. (14); cf. Fig. 1a,c). b, Prior covariance in the GSM (C in Eq. (2)). c, Sample stimuli generated by the GSM, also used for  
testing the network’s generalization in Fig. 3b-c. d-e, Parameters of the optimized network. d, Recurrent weights (top: raw weights; middle: normalized 
absolute values) and process noise covariance (bottom) after training. Weights and covariances are shown for only one row (of each quadrant) of W  
(Eq. (8)) and Ση (Eq. (11)), respectively, as they are circulant. Thus, each line shows the weights connecting, or the covariance between, cells of different 
types (see legend) as a function of the difference in their preferred stimuli. As the figure shows, the connectivity profile of either E or I cells in the 
optimized network was largely independent of whether the postsynaptic cell was excitatory or inhibitory (top). Overall, recurrent E and I connections 
had similar tuning widths, with E connections being slightly more broadly tuned than I ones (middle). Nevertheless, the net E input to any one cell in the 
network was still more narrowly tuned than the net I input, due to the responses of presynaptic E cells being more narrowly tuned than those of I cells  
(not shown). The optimized network also retained a substantial amount of process noise that was larger in E than in I cells, and highly correlated 
both between the E and I cell of a pair and between cells with different tuning (up to a ~30∘ tuning difference; bottom). e, Input nonlinearity (Eq. (14)), 
converting feedforward receptive field activations ðWffxÞi into network inputs hi (black). For comparison, the distribution of inputs across all cells for the 
training set is presented in gray. As the figure shows, the optimized input transformation, capturing the nonlinear effects of upstream preprocessing of 
visual stimuli, had a threshold that was just below the distribution of receptive field outputs, ensuring that all stimulus-related information was transmitted 
in the input signal, and an exponent close to two, remarkably similar to that used by the cells of the network (cf. Eq. (9) and Supplementary Table 1).
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 2 | Divisive normalization and the mechanism underlying oscillations in the optimized network. a-b, Divisive normalization, 
or sublinear summation of neural responses, has been proposed as a canonical computation in cortical circuits38. In turn, the stabilized supralinear 
network (SSN), which formed the substrate of our optimized network, has been proposed to provide the dynamical mechanism underlying divisive 
normalization27. We thus wondered whether our optimized network also exhibited it. a, In accordance with divisive normalization, the network’s response 
to the sum of two stimuli (solid purple) was smaller than the sum of its responses to the individual stimuli (solid red/blue), and lay between the average 
(dotted purple) and the sum (dashed purple). Inset shows stimuli used in this example. b, Generic divisive normalization in the optimized network. 
We fitted a standard phenomenological model of divisive normalization (adapted from Ref. 38) to the (across-trial) mean firing-rate responses of E 
cells in the optimized network, 〈rE〉, as a function of the feedforward input h to the network (that is without regard to its recurrent dynamics; Eq. (14)): 
hriðβÞ
i
¼ b2 þ
hðβÞ
i
þ b1


ðM hðβÞÞi þ s2
h
i1
, where hðβÞ
i  and hriðβÞ
i  were the feedforward input and the average firing rate of cell i in response to stimulus β,  
respectively, and b1, b2 and s were constant parameters. The parameter matrix M was responsible for normalization, by dividing the input hi by a mixture 
of competing inputs to other neurons. M was parameterized as a symmetric circulant matrix to respect the rotational symmetry of the trained network 
(Extended Data Fig. 1). In total, our model of divisive normalization had 3 þ NE=2
ð
Þ þ 1 ¼ 29 free parameters. Model fitting was performed  
via minimization of the average squared difference between network and model rates, plus an elastic energy regularizer for neighbouring elements of 
M. Shown here is a scatter plot of neural responses to a set of 500 random stimuli (generated as the generalization dataset, Methods), predicted by the 
phenomenological model vs. produced by the actual network. Each dot corresponds to a stimulus-neuron pair. Inset shows three representative average 
response profiles across the network (dots) and the phenomenological model’s fit (lines). Note the near perfect overlap between the network and the 
phenomenological model in all three cases. The divisive normalization model also outperformed both a linear model and a model of subtractive inhibition 
(not shown). These results show comprehensively that, in line with empirical data, our trained network performed divisive normalization of its inputs 
under general conditions. c-d, Using voltage-clamp to study the mechanisms underlying oscillations in the optimized network. To determine whether 
oscillations in our network resulted from the interaction of E and I cells, or whether they arose within either of these populations alone, we conducted 
two simulated experiments. In each simulation, either of the two populations (E or I) was voltage-clamped to its (temporal) mean, as calculated from the 
original network, for each input in the training set. Thus, recurrent input from the clamped population was effectively held constant to its normal mean, but 
did not react to changes in the other population. As expected for a network in the inhibition-stabilized regime, clamping of the I cells resulted in unstable 
runaway dynamics, precluding further analysis of oscillations (not shown). c, Illustration of the E-clamping experiment in the optimized network (cf. 
Fig. 1b). For each stimulus, each E cell’s voltage was clamped to its mean voltage, μE, obtained when the network was presented with the same stimulus 
without voltage clamp. d, LFP power spectra in the network after voltage-clamping of the E population at different contrast levels (colors as in Fig. 2). 
The network remained stable, but the peak in its LFP power spectrum characteristic of gamma oscillations was no longer present (cf. Fig. 5c, inset). This 
shows that gamma oscillations in the original network required interactions between E and I cells (that is they were generated by the so-called ‘PING’ 
mechanism16).
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 3 | Random networks. The parameterization of our network was highly constrained (for example ring topology with circulant and 
symmetric weight and process noise covariance structure, 1:1 E:I ratio, fixed receptive fields). To test whether these constraints alone, without any 
optimization, were sufficient to generate the results we obtained in the optimized network, we sampled weight matrices at random, by drawing each of 
the 8 hyperparameters of the weight matrix (Eq. (10)) from an exponential distribution truncated between 0.1 and 10 times the values originally found by 
optimization. We discarded matrices that were either unstable or converged to a trivial solution (all mean rates equal 0). Less than 20% of the generated 
matrices satisfied these criteria, further confirming that optimization was non-trivial. Results for six such example random networks are shown (columns). 
a, Recurrent weights as a function of the difference in the preferred stimuli of two cells (cf. Extended Data Fig. 1d, top). Different lines are for weights 
connecting cells of different types (legend). b-c, Mean (b) and standard deviation of membrane potential responses (c), averaged over the population, as 
a function of contrast (cf. Fig. 3a). Gray dots on x-axis indicate training contrast levels. Note the wide range of behaviors displayed by these networks. For 
example, the standard deviation of responses could go up, down, or even be non-monotonic with contrast, while the range of mean rates also varied wildly. 
d-e, Mean firing rate (d) and Fano factor (e) of neurons as a function of stimulus orientation (relative to their preferred orientation) during spontaneous 
(dark red) and evoked activity (light orange; cf. Fig. 5a-b). The peak mean rate of example 3 exceeded 50 Hz and is thus shown as clipped in this figure. 
Note that mean rate tuning curves (during evoked activity) were very narrow for most networks (all but example 5), resulting in 0 Hz rates and thus 
undefined Fano factors for stimuli further away from the preferred orientation. f, LFP power spectra at different contrast levels (colors as in Fig. 2). Note 
the absence of gamma peaks (cf. Fig. 5c, inset). g, Average rate response around stimulus onset at different contrast levels (colors as in Fig. 2). Black bars 
show stimulus period. Note the absence of transients (cf. Fig. 5d). To estimate network moments in b-d, n = 20,000 independent samples (taken 200 ms 
apart) were used. Population averages (n=50 cells) were computed for b and c. Mean firing rates in panel g were computed over n = 100 trials.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 4 | Comparison of random and optimized networks: cost achieved and dynamical features. Networks are ranked in all panels in order 
of decreasing total cost achieved by them (shown in a). Random networks are those shown in Extended Data Fig. 3. The originally optimized network 
presented in the main text is indicated with ⋆, and the network optimized without enforcing Dale’s principle (Extended Data Fig. 8) is marked with †. Other 
optimized networks were studied to confirm that well-optimized networks reliably showed similar behavior. This was important because our cost function 
was highly non-convex. Therefore, any minimum our optimizer found, such as that corresponding to the originally optimized network, had no guarantee of 
being the global minimum. Therefore, we trained 10 further networks on the original cost function (Methods), starting from random initial conditions, and 
show here those whose final cost was at least approximately as low as that of the original network (9 out of 10). a, Total cost (Eq. (25)) computed for each 
of the random networks and (left) for networks that were optimized for the original cost (right). Colors indicate different components of the cost function 
(legend, see Eqs. (26)–(29) for mathematical definitions). The inset shows the optimized networks only (note different y-scale). Note that the cost 
achieved by the random networks was 1-3 orders of magnitude higher than that achieved by the optimized networks. Furthermore, none of the optimized 
networks achieved substantially lower costs than the one we presented in the main text. b-c, Oscillatoriness (b) and transient overshoot size  
(c) for each network in a. Oscillatoriness was computed by numerical fits of Eq. (33) to the autocorrelogram of the LFP generated by the network 
(Methods). Transients in population-average firing rates were quantified as the size of the overshoot normalized by the change in the steady state mean 
(see also Fig. 7a). Note that compared to the optimized networks – including the one presented in the main text –, oscillations and transients were almost 
entirely absent from random networks. Furthermore, all optimized networks had substantial oscillatoriness and transient overshoots (Extended Data Fig. 
4b-c). This suggests that the results we obtained for the originally optimized network were representative of the best achievable minima of  
the cost function.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 5 | See next page for caption.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 5 | Control networks without full variability modulation. Variability modulations are a hallmark of sampling-based inference5. To see 
whether they were also critical for our results, we optimized networks with modified cost functions, either setting both ϵvar = 0 and ϵcov = 0 (Eq. (25)), 
requiring only response means to be matched (a-g; see also Fig. 5, right column), or setting only ϵcov = 0, requiring the matching of response means and 
variances but not of covariances (h-n; see also Methods). a-b, h-i, Network parameters as in Extended Data Fig. 1d-e. Both networks developed weak 
connection weights (a and h, top), with near-identical widths for E and I inputs onto both E and I cells (a and h, middle), and an almost linear input 
transformation (b and i). c,j, Sample population activity as in Fig. 2a. d-e,k-l, Matching moments between the ideal observer and the network for training 
stimuli as in Fig. 2c-d. Extremely weak coupling in the first network (a, top) meant an essentially feed-forward architecture. Thus, its response covariance 
simply reflected its process noise covariance (compare e and a, bottom). High input correlations in the second network (h, bottom) resulted in a single, 
global mode of output fluctuations (l). f-g,m-n, Generalization to test stimuli as in Fig. 3a-b. Insets in g show GSM posterior and network response means 
for example test stimuli as in Fig. 3c. Response moments in n are shown only for training stimuli, not for test stimuli, but by distinguishing variances (blue, 
bottom) and covariances (lavender, bottom). Both networks completely failed to fit moments that they were not explicitly required to match. Thus, firing 
rate tuning curves were preserved in both networks, but Fano factors were barely modulated in the first network (Extended Data Fig. 6a-b). Critically, 
neither of these networks showed discernible oscillations or transient overshoots (Extended Data Fig. 6c-e). Response moments in d-g and k-n were 
estimated from n = 20,000 independent samples (taken 200 ms apart). Population mean moments in f and m were further averaged across n=50 E cells. 
Correlations in e and l are Pearson’s correlations.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 6 | Comparison of neural dynamics between the originally optimized network and the control networks. a-e, Dynamics of optimized 
networks as in Fig. 5a-e. The originally optimized network of the main paper (left) is compared to various control networks, from left to right: without 
slowness penalty (Extended Data Fig. 10), without covariance modulation (matching means and variances; Extended Data Fig. 5h-n), without covariance 
and variance modulation (matching means only; Extended Data Fig. 5a-g), enforcing constant Fano factors (Extended Data Fig. 7), and with Dale’s 
principle not enforced (Extended Data Fig. 8). For ease of comparison, only stimulus-dependent power spectra are shown for the optimized network of 
the main paper, without showing the dependence of gamma peak frequency on contrast (cf. Fig. 5c, middle), as most control networks had no discernible 
gamma peaks. For more details on control networks, see the captions of the corresponding figures (Extended Data Figs. 5,7,8 and 10). Response moments 
in a were estimated from n = 20,000 independent samples (taken 200 ms apart). Mean firing rates in d were computed over n = 100 trials. Panel e shows 
mean ± s.e.m. (n = 20 trials).
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 7 | Control network: enforcing constant Fano factors. Fano factors need to be specifically stimulus-independent for a class of models, 
(linear) probabilistic population codes (PPCs12), that provide a conceptually very different link between neural variability and the representation of 
uncertainty from that provided by sampling5,18, which we pursue here. Therefore, we used our optimization-based approach to directly compare the circuit 
dynamics required by PPCs to those of our originally optimized network implementing sampling. For this, we trained a further control network whose goal 
was to match the mean modulation of the control network (resulting in realistic tuning curves), while keeping Fano factors constant. We achieved this 
by devising a set of target covariances that would result (together with the target mean responses used by all other networks) in constant Fano factors – 
assuming Poisson spiking and an exponentially decaying autocorrelation function (using analytic results in Ref. 52). Training then proceeded exactly as  
for the other networks, with the same ϵ parameters in the cost function as for the originally optimized network, only employing the new covariance  
targets. a-b, Network parameters as in Extended Data Fig. 1d-e. The network made use of strong inhibitory connections (a, top), large shared process  
noise (a, bottom), and strongly modulated inputs (b). c, Sample population activity as in Fig. 2a. d-e, Matching moments between the ideal observer and 
the network for training stimuli as in Fig. 2c-d. f-g, Generalization to test stimuli as in Fig. 3a-b. Insets in g show GSM posterior and network response 
means for example test stimuli as in Fig. 3c. The network was able to match mean responses in the training set and to generalize to novel stimuli  
(d, f-g), while keeping Fano factors relatively constant as required (Extended Data Fig. 6b). For consistency with previous results, we obtained Fano factors 
by numerically simulating the same type of inhomogeneous Gamma process as in the other networks (Methods), thus violating the Poisson spiking 
assumptions under which we computed the target covariances of the network (see above) – hence the remaining small modulations of Fano factors. 
Critically, although the training procedure was identical to that used for the original network, only differing in the required variability modulation provided 
by the targets (see above), this control network displayed no gamma-band oscillations (Extended Data Fig. 6c). Inhibition-dominated transients did 
emerge, but were weaker than in the original network (Extended Data Fig. 6d,e). Response moments in d-g were estimated from n = 20,000 independent 
samples (taken 200 ms apart). Population mean moments in f were further averaged across n=50 E cells. Correlations in e are Pearson’s correlations.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 8 | Control network: Dale’s principle not enforced. In order to see how much the biological constraints we used for the optimized 
network, and in particular enforcing Dale’s principle, were necessary to achieve the performance and dynamical behavior of the original network, we 
optimized a network with the same cost function as for the original network (Eq. (25)) but without enforcing Dale’s principle. This meant that the signs of 
synaptic weights in each quadrant of the weight matrix (the aXY coefficients in Eq. (10)) were not constrained. Otherwise, optimization proceeded in the 
same way as before (Methods). The training of this network proved to be much more difficult and prone to result in unstable networks, which we avoided 
by early stopping. a-b, Network parameters as in Extended Data Fig. 1d-e. As Dale’s principle was not enforced, only notional cell types can be shown 
(legend). Nevertheless, interestingly, the network still obeyed Dale’s principle after optimization (top): all outgoing synapses of any one cell had the same 
sign. Note that the outgoing weights of the cells whose moments were constrained (Ẽ cells, whose activity is shown and analysed in c-g) were actually 
negative. Therefore, in effect, these cells became inhibitory during training. c, Sample population activity as in Fig. 2a. d-e, Matching moments between 
the ideal observer and the network for training stimuli as in Fig. 2c-d. f, Generalization to test stimuli as in Fig. 3a. g, Matching moments between the ideal 
observer and the network for training stimuli as in Fig. 3b (lavender). Note that here, unlike in Fig. 3, response moments are shown only for training stimuli, 
not for test stimuli, but by distinguishing variances (blue, bottom) and covariances (lavender, bottom). Overall, the stationary behavior of this network was 
broadly similar to that of the originally optimized network (cf. Figs. 2 and 3). Therefore, it achieved a performance that was far better than the random 
networks’ (Extended Data Fig. 4a). However, it still performed substantially worse than networks optimized with Dale’s principle enforced (Extended 
Data Fig. 4a), and its dynamics were also qualitatively different: oscillations and transient overshoots were largely absent from it (Extended Data Fig. 4b-c, 
Extended Data Fig. 6). Response moments in d-g were estimated from n = 20,000 independent samples (taken 200 ms apart). Population mean moments 
in f were further averaged across n=50 Ẽ cells. Correlations in e are Pearson’s correlations.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 9 | Further analyses of the role of transients in supporting continual inference. a-b, Analysis of transients in the response of a 
single neuron. (Left panel in a is reproduced from Fig. 7a.) a, Temporal evolution of the mean (left) membrane potential (uE), and the membrane potential 
standard deviation (right) in three different neural responses (thick lines) with identical autocorrelations (matched to neural autocorrelations in the full 
network, inset, cf. Fig. 4a) but different time-dependent means (left) and standard deviations (right). Thin green line shows the time-varying target mean 
(left) and standard deviation (right). b, Total divergence (Eq. (37), Methods) between the target distribution at a given point in time and the distribution 
represented by the neural activity sampled in the preceding 100 ms, for each of the three responses (colors as in a). In comparison, note that Fig. 7b 
only shows the mean-dependent term of the divergence (Eq. (38), Methods). Black bars in a-b show stimulus period. Mean and divergence computed 
as an average over multiple trials (n =10,000). c, Optimal response trajectories (red lines) for continual estimation of the mean of a target distribution 
(thin green lines). Different shades of red indicate optimal trajectories corresponding to three different target levels (5, 10, and 20 mV, emulating 
different contrast levels). We optimized neural response trajectories (Supplementary Eq. 39, Supplementary Math Note) so that the distance between 
their temporal average, computed using a prospective box-car filter (k(t) = 1/T for t ∈ [0, T] and 0 otherwise, with T = 20 ms), and the corresponding 
Heaviside step target signal would be minimized under a smoothness constraint (ϵsmooth = 1; Supplementary Eq. 37, Supplementary Math Note). Note the 
transient overshoot in the optimal response very closely resembling those observed in the optimized network (Figs. 5d, 7c-d): its magnitude scales with 
the value of the target mean, and it is followed by damped oscillations. Similar results (with less ringing following the overshoot) were obtained also for an 
exponentially decaying, ‘leaky’ kernel (not shown). d-e, Relationship between overshoot magnitude and steady state difference: analysis of experimental 
recordings from awake macaque V123. d, Overshoot magnitude versus steady state difference: same as Fig. 7e, but restricting the analysis to steady 
state differences below 60 Hz (to exclude outliers). Red line shows linear regression (± 95% confidence bands). The correlation between overshoot size 
and steady state difference is still significant: two-sided Wald test p = 1 × 10−89 (n = 1263 cell-stimulus pairs R2 ≃ 0.27). e, Systematically changing the 
maximal steady state difference (x-axis) used for restricting the analysis of the correlation between overshoot size and steady state difference (d and  
Fig. 7e) reveals that the correlation is robust (black line ± 95% confidence intervals) and remains highly significant (blue line, showing corresponding 
p-values; note logarithmic scale) for all but the smallest threshold (and thus smallest sample size). Horizontal dotted lines show R = 0 correlation (black) 
and p = 0.05 significance level (blue) for reference. As the maximal steady state difference increases, the number of points n considered also increases 
from n = 4 to n = 1279. Correlations in a are Pearson’s correlations. Pearson’s R values and corresponding p-values in e were obtained by linear regression 
performed as in d.
Nature NeuroscIence | www.nature.com/natureneuroscience

Articles
NaTurE NEurOSciEncE
Extended Data Fig. 10 | Control network: no explicit slowness penalty. To test whether the explicit slowness penalty in our cost function (Eq. (29)) was 
necessary for obtaining the dynamical behaviour exhibited by the optimized network of the main text, we set ϵslow = 0 in Eq. (25). a-b, Network parameters 
as in Extended Data Fig. 1d-e. c, Sample population activity as in Fig. 2a. d-e, Matching moments between the ideal observer and the network for training 
stimuli as in Fig. 2c-d. f, Generalization to test stimuli as in Fig. 3a. g, Matching moments between the ideal observer and the network for training stimuli 
as in Fig. 3b (lavender). Note that here, unlike in Fig. 3, response moments are shown only for training stimuli, not for test stimuli, but by distinguishing 
variances (blue, bottom) and covariances (lavender, bottom). Note that this network behaved largely identically to the originally optimized network (see 
also Extended Data Fig. 6). This could be attributed to the fact that our optimization implicitly encouraged fast sampling by default, simply by using a finite 
averaging window for computing average moments of network responses, and in particular by including samples immediately or shortly following stimulus 
onset (Methods). Response moments in d-g were estimated from n = 20,000 independent samples (taken 200 ms apart). Population mean moments in  
f were further averaged across n=50 E cells. Correlations in e are Pearson’s correlations.
Nature NeuroscIence | www.nature.com/natureneuroscience

1
nature research  |  reporting summary
April 2020

2
nature research  |  reporting summary
April 2020
Field-specific reporting
Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.
Life sciences
Behavioural & social sciences
 Ecological, evolutionary & environmental sciences
For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf
Life sciences study design
All studies must disclose on these points even when the disclosure is negative.
Sample size
Numerical experiments designed to show the properties of the network used large enough n such that the standard error would be smaller 
than line widths. For comparison with experimental data, the same order of magnitude for n was selected as in the original experiments.
Data exclusions
No data was excluded. In the study of transients only cells showing significant tuning were selected as is customary.
Replication
We repeated the optimization procedure 10 times, showing that our results are indeed robust.
Randomization
No experiments were conducted for this paper.
Blinding
No experiments were conducted for this paper.
Reporting for specific materials, systems and methods
We require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, 
system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. 
Materials & experimental systems
n/a Involved in the study
Antibodies
Eukaryotic cell lines
Palaeontology and archaeology
Animals and other organisms
Human research participants
Clinical data
Dual use research of concern
Methods
n/a Involved in the study
ChIP-seq
Flow cytometry
MRI-based neuroimaging

