Page 1 of 43 
A GENERALISED MATCHING DISTRIBUTION FOR THE PROBLEM OF 
COINCIDENCES 
B. O’NEILL,* Australian National University** 
WRITTEN 19 NOVEMBER 2021 
Abstract 
This paper examines the classical matching distribution arising in the “problem of coincidences”.  We 
generalise the classical matching distribution with a preliminary round of allocation where items are 
correctly matched with some fixed probability, and remaining non-matched items are allocated using 
simple random sampling without replacement.  Our generalised matching distribution is a convolution 
of the classical matching distribution and the binomial distribution.  We examine the properties of this 
latter distribution and show how its probability functions can be computes.  We also show how to use 
the distribution for matching tests and inferences of matching ability. 
PROBLEM OF COINCIDENCES; HAT-CHECK PROBLEM; SECRET-SANTA PROBLEM; MATCHING STATISTIC; 
CLASSICAL MATCHING DISTRIBUTION; GENERALISED MATCHING DISTRIBUTION; MATCHING TEST 
 
In this paper we examine a probability distribution arising in the “problem of coincidences”, 
an antique probability problem dating back to Mortmont (1708, 1713).  The problem arises in 
the classical “game of thirteen” (jeu du treize), where a dealer shuffles thirteen playing cards 
of a single suit and then turns them over in order; if no card appears in its proper place in the 
order (e.g., ascending order from ace, two, …, jack, queen, king), the dealer takes the stakes, 
but if there are any cards appearing in their proper place, the dealer pays each of the players.  
Another variation on this problem is to shuffle two groups of playing cards with the same 
numbers in different suits, and reveal one card in each group at a time, seeing if there is a 
“match” between the numbers shown on the two cards. 
 
There are many names for this probability problem, corresponding to many equivalent forms.  
Penrice (1991) notes that “[t]here seem to be as many ways of describing the problem as there 
are names for it, and each description suggests some generalization…” (p. 617).  One form of 
the problem, called the “hat-check problem”, stipulates that 𝑛 gentlemen check-in their hats at 
a restaurant, but the hat-check girl accidentally mixes them up, and returns the hats at random.  
Another variation, called the “Secret-Santa problem” considers a group of 𝑛 people who draw 
the names in the group at random to determine who will buy a present for whom.  In both cases 
we are interested in the distribution of the number of “matches” based on a fixed place for each 
item.  The mathematics of the problem of coincidences has been considered by a number of 
authors and has been of recurring interest (e.g., Anderson 1943; Feller 1968; Abramson 1973; 
 
* E-mail address: ben.oneill@hotmail.com. 
** Research School of Population Health, Australian National University, Canberra ACT 0200, Australia. 

Page 2 of 43 
Scoville 1980; Penrice 1991; Crain 1992, Knudsen and Skau 1996).  The history of the problem 
is examined in Takács (1980), and unsurprisingly, it is tied up with the historical derivation of 
the subfactorial numbers (representing the number of permutations with no matches). 
 
1. The classical matching distribution 
 
Each variant of the problem can be formulated in general terms by taking a set of 𝑛 distinct 
items (labelled here as consecutive integers).  Suppose we permute the order of these items at 
random to obtain a permutation vector 𝐱= (𝑥ଵ, … , 𝑥௡), assumed to be equiprobable over all 
permutations of 1, … , 𝑛 (i.e., simple random sampling without replacement).  We then define 
the matching statistic by the number of “fixed points” in the permutation vector: 
𝐾= ෍𝕀(𝑥௜= 𝑖)
௡
௜ୀଵ
. 
The matching statistic counts the number of items in the original vector 𝐬 that are in the same 
place after a random permutation.  The classical version of the problem focused specifically on 
the probability that there are no matches, which is ℙ(𝐾= 0).  However, it is simple to extend 
the problem to consider the full distribution of the number of matches, which fully describes 
the stochastic behaviour of the matching statistic.  This distribution (which we will call the 
“classical matching distribution”) is defined below. 
 
DEFINITION (Classical matching distribution): This is a family of discrete distributions 
characterised by the probability mass function:1 
Match(𝑘|𝑛) = 1
𝑘! ෍(−1)௜
𝑖!
௡ି௞
௜ୀ଴
𝑘= 0, … , 𝑛, 
where we have a size parameter 𝑛= 0,1,2, … , ∞.  In the special case where 𝑛= ∞ we take 
Match(𝑘|∞) ≡Pois(𝑘|1) by convention (based on limiting results shown below).  □ 
 
We begin by demonstrating that the matching statistic follows the matching distribution.  Since 
each permutation of the initial items is (by assumption) equiprobable, there are 𝑛! equiprobable 
outcomes for the permutation vector 𝐱.  The number of outcomes with 𝐾= 𝑘 can be obtained 
by choosing 𝑘 fixed values from the 𝑛 elements and then counting the number of derangements 
 
1 In the special case where 𝑛= 0 the distribution is a point-mass on 𝑘= 0, and in the special case where 𝑛= 1 
the distribution is a point-mass on 𝑘= 1. 

Page 3 of 43 
for the remaining 𝑛−𝑘 elements, which we denote as 𝒟(𝑛−𝑘).  The number of derangements 
is given by the subfactorial numbers, whose properties have been extensively studied (see e.g., 
Hassani 2003 and Hassani 2004).  Applying the fundamental principle of counting then gives: 
|{𝐱|𝐾= 𝑘}| = ቀ𝑛
𝑘ቁ× 𝒟(𝑛−𝑘). 
Consequently, under the assumption that all possible permutations are equiprobable, we have: 
ℙ(𝐾= 𝑘) = |{𝐱|𝐾= 𝑘}|
|{𝐱}|
                   
                   = 1
𝑛! × ቀ𝑛
𝑘ቁ× 𝒟(𝑛−𝑘) 
      = 1
𝑘! ∙𝒟(𝑛−𝑘)
(𝑛−𝑘)! . 
The values 𝒟(𝑛−𝑘) are the subfactorials, defined recursively by the base values 𝒟(0) = 1 
and 𝒟(1) = 0 and the recursion 𝒟(𝑛−𝑘) = (𝑛−𝑘−1)[𝒟(𝑛−𝑘−1) + 𝒟(𝑛−𝑘−2)].  
They can also be written in explicit form (in terms of the factorials) as: 
𝒟(𝑛−𝑘) = (𝑛−𝑘)! ෍(−1)௜
𝑖!
௡ି௞
௜ୀ଴
. 
Consequently, we can write the probabilities for the matching statistic as: 
ℙ(𝐾= 𝑘) = 1
𝑘! ෍(−1)௜
𝑖!
௡ି௞
௜ୀ଴
= Match(𝑘|𝑛) . 
 
In the definition of the matching distribution, we have allowed the special case 𝑛= ∞, which 
we conceive as a limiting case.  To obtain this special case we examine the limiting form of 
the distribution that occurs when 𝑛→∞.  In this case, using a well-known limit result for the 
ratio of derangements over permutations (see e.g., Hassani 2003, pp. 1-2) we get: 
lim
௡→ஶℙ(𝐾= 𝑘) = 1
𝑘! lim
௡→ஶ
𝒟(𝑛−𝑘)
(𝑛−𝑘)! = 1
𝑘! ∙1
𝑒= Pois(𝑘|1) . 
We can see that the distribution approaches the Poisson distribution with unit scale in the limit 
as 𝑛→∞.  This result has been noted in much of the literature on the problem of coincidences 
(see e.g., Penrice 1991, Crain 1992, Knudsen and Skau 1996), and it implies the asymptotic 
probability lim௡→ஶℙ(𝐾= 0) = 1 𝑒
⁄  which gives an approximate solution to the original 
problem.  In our definition of the matching distribution we have allowed 𝑛= ∞ as an explicit 
case, and we set the distribution equal to its limit in this case.  This convention has the 
advantage of giving a family of distributions that is closed under limits on its size parameter. 

Page 4 of 43 
REMARK: One noteworthy aspect of the matching statistic is that we cannot have 𝐾= 𝑛−1, 
and therefore the matching distribution always has ℙ(𝐾= 𝑛−1) = 0.  Intuitively, this reflects 
the fact that if 𝑛−1 items are matched in the permutation then the last item must also be 
matched —i.e., it is not possible to match all but one item in any permutation.  Consequently, 
the matching distribution has a strange looking support, with a missing value at 𝑛−1.  Since 
the distribution has a non-zero probability at 𝐾= 𝑛 this also means that — except in the trivial 
cases 𝑛= 0,1 where the distribution is a point-mass— the distribution is not quasi-concave 
(unimodal) and is instead slightly bimodal.  This slight bimodality of the distribution may need 
to be taken into account in some cases where the user forms a highest density region (HDR) 
for the distribution, though it is marginal even in this case.  □ 
 
2. Important properties of the classical matching distribution 
 
We will begin our examination of the classical matching distribution by deriving its moments.  
As with many discrete distributions on the non-negative integers, it is easiest to examine the 
moments by first looking at the factorial moments and then using these to obtain the raw 
moments.  Abramson (1973) has previously derived the factorial moments of the distribution 
and Feller (1968, p. 231) has derived the mean and variance of the distribution.  We give a full 
derivation of all factorial and raw moments, the moment generating function, and the central 
moments up to fourth order.  In Theorems 1-3 below we derive the form of the factorial 
moments and raw moments and the moment generating function.  As with the general form of 
the distribution, these results generalise the case of a Poisson distribution with unit scale. 
 
THEOREM 1 (Factorial moments): The factorial moments of the matching distribution are: 
𝔼((𝐾)௥) = 𝕀(𝑟≤𝑛). 
 
THEOREM 2 (Raw moments): Let 𝑆(𝑟, 𝑖) denote the Stirling numbers of the second kind.  The 
raw moments of the matching distribution are: 
𝔼(𝐾௥) =
෍
𝑆(𝑟, 𝑖)
୫୧୬(௥,௡)
௜ୀ଴
. 
In the case where 𝑟≤𝑛 this reduces to the Bell numbers: 
𝔼(𝐾௥) = 𝐵௥. 
 

Page 5 of 43 
THEOREM 3 (MGF): The moment generating function for the matching distribution is: 
𝑚௄(𝑡) = ෍(𝑒௧−1)௜
𝑖!
௡
௜ୀ଴
. 
 
The form of these moments is quite interesting in its own right.  The factorial moments of the 
distribution are unit value for all 𝑟≤𝑛 and zero thereafter.  In the special case where 𝑛= ∞ 
the distribution degenerates to the Poisson distribution with unit scale, which is known to have 
unit factorial moments and raw moments equal to the Bell numbers.  Some important moments 
of the distribution are shown in Theorem 4 below. 
 
THEOREM 4 (Central moments): Some important moments of the matching distribution are: 
          𝔼(𝐾) = ቊ 0
if  𝑛= 0
1
if  𝑛≥1
          𝕍(𝐾) = ቊ 0
if  𝑛≤1,
1
if  𝑛≥2.
    𝕊𝕜𝕖𝕨(𝐾) = ൞ 
NA
if  𝑛≤1
0
if  𝑛= 2
1
if  𝑛≥3
      𝕂𝕦𝕣𝕥(𝐾) =
⎩
⎪
⎨
⎪
⎧
 
NA
if  𝑛≤1,
1
if  𝑛= 2,
3
if  𝑛= 3,
4
if  𝑛≥4.
 
 
In the special cases where 𝑛= 0 or 𝑛= 1 the matching distribution is a point-mass on 𝐾= 0 
or 𝐾= 1 respectively, and the moments for those cases reflect this.  In the case where 𝑛= 2 
the matching distribution is equiprobable on the values 𝐾= 0 and 𝐾= 2, yielding a symmetric 
and extreme platykurtic distribution (known to be the most platykurtic distribution).  In the 
case where 𝑛= 3 the matching distribution is positively skewed and mesokurtic, and in the 
most general case where 𝑛≥4 the distribution is positively skewed and leptokurtic.  The fixed 
values of the main central moments for large 𝑛≥4, and the slowly changing nature of the 
moment generating function, suggests that the distribution does not change much with respect 
to the size parameter 𝑛 once this value is already large.  In particular, we see that the moment 
generating function has the form of a partial exponential generating function up to 𝑛, so in a 
neighbourhood of 𝑡= 0 the early terms are much larger than the later terms. 
 
In Figure 1 we plot the distribution for size values 𝑛= 1, … ,12 and confirm this fact.  This 
shows that there is very little change in the distribution once 𝑛> 6.  (At this point there is no 
discernible change in the barplot.)  To confirm this observation, we plot the SSE comparing 

Page 6 of 43 
each distribution to the limiting case 𝑛= ∞ (i.e., the Poisson distribution with unit scale) in 
Figure 2.2  From this latter plot we can confirm that the SSE diminishes rapidly, and for 𝑛> 6 
it is no greater than 6 × 10ି଺, which shows very little deviation from the limiting case.  This 
latter result can be formalised with analytical bounds on the difference between probabilities 
in the matching distribution and probabilities in the Poisson distribution with unit scale (see 
e.g., DasGupta 2005, pp. 384-385).  An interesting result is that there are alternating sign for 
these differences, though they diminish rapidly to zero (Ibid, p. 385). 
 
 
FIGURE 1: The classical matching distribution for sizes 𝑛= 1, … ,12 
 
 
2 The SSE for each size value is defined as SSE(𝑛) ≡∑
(Match(𝑘|𝑛) −Match(𝑘|∞))ଶ
ஶ
௞ୀ଴
.  The computation for 
the figure truncates this sum at a finite upper bound, but this does not make any discernible difference in the plot. 

Page 7 of 43 
 
FIGURE 2: SSE for matching distributions of sizes 𝑛= 1, … ,12 (compared to 𝑛= ∞) 
 
The explicit form for the matching distribution involves a sum of rapidly decreasing numbers 
with alternating sign.  This form is not particularly helpful for computational purposes, since 
sums of this kind commonly leads to problems of arithmetic underflow.  In order to compute 
the mass function of the distribution, it is useful to characterise this function in recursive form.  
In Theorem 5 we give a recursive form for the probabilities in the distribution.  In Theorem 6 
we establish a recursive form comparing mass values at a fixed 𝑘 as 𝑛 increases.  Both of these 
forms are simple consequences of the recursive properties of the subfactorial numbers. 
 
THEOREM 5 (Recursive form): The mass function of the matching distribution satisfies the 
recursive equations: 
Match(𝑛|𝑛) = 1
𝑛! ,                                                                                                          
Match(𝑘|𝑛) = 𝑘+ 1
𝑛−𝑘[(𝑛−𝑘−1) ∙Match(𝑘+ 1|𝑛) + (𝑘+ 2) ∙Match(𝑘+ 2|𝑛)].
 
 
COROLLARY: For a fixed size value 𝑛, let m(𝑘) ≡log Match(𝑘|𝑛) denote the log-probabilities 
for the matching distribution.  These values satisfy the recursive equations: 
m(𝑛) = −෍log(𝑖)
௡
௜ୀଵ
,                                                                                                  
m(𝑘) = log(𝑘+ 1) −log(𝑛−𝑘) + logsumexp ൬log(𝑛−𝑘−1) + m(𝑘+ 1) ,
log(𝑘+ 2) + m(𝑘+ 2)
൰.
 
 

Page 8 of 43 
THEOREM 6 (Recursive form): The mass function of the matching distribution satisfies the 
recursive equation: 
Match(𝑘|𝑛+ 1) =
𝑛−𝑘
𝑛−𝑘+ 1 ∙Match(𝑘|𝑛) +
𝑘+ 1
𝑛−𝑘+ 1 ∙Match(𝑘+ 1|𝑛) . 
Consequently, for all fixed 𝑘 we have: 
lim
௡→ஶ
Match(𝑘|𝑛+ 1)
Match(𝑘|𝑛)
= 1. 
 
For computational purposes, it is useful to compute the distribution in log-space (i.e., compute 
the log-probabilities rather than the probabilities) and then convert back to regular probability 
space at the end of the computation.  Consequently, the best way to compute the mass function 
is using the log-space recursive equations in the corollary to Theorem 5.  The stability of the 
distribution for large 𝑛 is exhibited by the recursive equation and limiting result in Theorem 6.  
We defer computation of the mass function for now, since we will first want to generalise this 
distribution to handle a broader model for matching that allows results that are “better than 
random”.  The classical matching distribution gives the baseline behaviour of the matching 
number when allocation is done at random, and so it gives a null distribution for this hypothesis, 
which can serve as a basis for hypothesis testing for better allocation.  In the next section we 
generalise the matching distribution to explicitly model allocation that is better than random. 
 
3. Generalising the matching distribution 
 
The essence of the classical problem of coincidences is that each selection of an item is assumed 
to occur via simple random sampling without replacement, such that all permutations over the 
set of items are equiprobable.  This represents the premise that the allocation of items is based 
on purely random “guesses” or “draws” from the person doing the allocation.  However, in 
certain kinds of matching problems, it is possible that the allocator may have some partial 
control over the matching.  For example, consider the game where an allocator attempts to 
match baby photos to adults, where there is one baby photo for each adult.  If the allocator is 
completely unable to match characteristics in the photos with characteristics of the adults then 
we would hypothesise that the selection is essentially just a random permutation of photos.  
However, if the allocator has some ability to match characteristics in the photos, he should tend 
to perform better than a random selection.  Indeed, testing whether allocation is “better than 
random” is one of the primary purposes of the matching distribution. 

Page 9 of 43 
Consideration of this kind of case leads to a desire to generalise the matching distribution to 
allow an additional parameter that will tend to increase the number of matches in some natural 
way.  There are many ways that one could go about seeking this generalisation.3  In this paper 
we will consider the matching mechanism as a two-step process.  In the first step, for each item, 
suppose there is a fixed probability 𝜃 that the allocator will know the correct allocation for the 
item and allocate it accordingly.  In the second step, all remaining items are allocated via a 
random permutation (i.e., via simple random sampling without replacement).  This two-step 
process can be described mathematically by letting 𝐿 denote the (random) number of items 
with known allocation, so that the number of matches is then given by: 
𝐾௡
∗= 𝐿+ 𝐾௡ି௅
𝐿 ~ Bin(𝑛, 𝜃) . 
The resulting distribution of the matching number 𝐾௡
∗ is then given by the convolution: 
Match(𝑘|𝑛, 𝜃) = ෍Bin(ℓ|𝑛, 𝜃) ∙Match(𝑘−ℓ|𝑛−ℓ)
௞
ℓୀ଴
                                                          
= ෍
𝑛!
(𝑛−ℓ)! ℓ! ∙𝜃ℓ(1 −𝜃)௡ିℓ∙
1
(𝑘−ℓ)! ∙𝒟(𝑛−𝑘)
(𝑛−𝑘)!
௞
ℓୀ଴
    
                       = 1
𝑘! ∙𝒟(𝑛−𝑘)
(𝑛−𝑘)! ෍
𝑛!
(𝑛−ℓ)! (1 −𝜃)௡ି௞∙
𝑘!
(𝑘−ℓ)! ℓ! ∙𝜃ℓ(1 −𝜃)௞ିℓ
௞
ℓୀ଴
 
= Match(𝑘|𝑛) (1 −𝜃)௡ି௞෍
𝑛!
(𝑛−ℓ)! Bin(ℓ|𝑘, 𝜃)
௞
ℓୀ଴
.        
We take this distribution to be a reasonable model of the matching process in cases where there 
may be some ability by the allocator to match the items “better than random”. 
 
DEFINITION (Generalised matching distribution): This is a family of discrete distributions 
characterised by the probability mass function: 
Match(𝑘|𝑛, 𝜃) = Match(𝑘|𝑛) (1 −𝜃)௡ି௞෍
𝑛!
(𝑛−ℓ)! Bin(ℓ|𝑘, 𝜃)
௞
ℓୀ଴
𝑘= 0, … , 𝑛, 
where we have a size parameter 𝑛∈ℕ and probability parameter 0 ≤𝜃≤1.  The generalised 
matching distribution reduces to the classical matching distribution when 𝜃= 0.  □ 
 
3 Another interesting generalisation, which we will not pursue in detail here, is to generalise using the distribution 
with moment generating function proportional to ∑
(𝜆(𝑒௧−1))௜𝑖!
⁄
௡
௜ୀ଴
, converging to the Poisson distribution as 
𝑛→∞.  Unfortunately, this only leads to a valid distribution when 0 ≤𝜆≤1, which allows a lower number of 
matches but does not allow a larger number of matches.  This is an interesting distribution, but it does not allow 
modelling of cases where allocation is “better than random” so it is unsuitable for our purposes. 

Page 10 of 43 
Unfortunately, the mass function for the generalised distribution cannot be simplified further; 
it is a rather cumbersome form that presents some computational challenges.  It is interesting 
to note that the mass function for the generalised matching distribution relates to the mass 
function for the classical case through the expectation of a falling factorial involving a binomial 
random variable.  Specifically, we have Match(𝑘|𝑛, 𝜃) = Match(𝑘|𝑛) ∙(1 −𝜃)௡ି௞∙Π(𝑘, 𝜃), 
where Π(𝑛, 𝑘, 𝜃) = 𝔼((𝑛)஻) for a random variable 𝐵 ~ Bin(𝑘, 𝜃).  The reader should note that 
the quantity Π(𝑛, 𝑘, 𝜃) is not a standard “factorial moment” for the binomial random variable, 
since those are of the form 𝔼((𝐵)௥), not 𝔼((𝑛)஻). 
 
One effective way to compute the mass function for the generalised matching distribution is to 
first compute all the mass function values for all classical matching distributions up to the 
required size (using the recursive equations in Theorem 5) and then apply the convolution 
equation  Match(𝑘|𝑛, 𝜃) = ∑
Bin(ℓ|𝑛, 𝜃) ∙Match(𝑘−ℓ|𝑛−ℓ)
௞
ℓୀ଴
 to computation the mass 
values for the generalised distribution.  It is advisable to conduct all these computations in log-
space to avoid arithmetic underflow.  Probability functions for the matching distribution are 
available in the stat.extend package in R (O’Neill and Fultz 2021).  This package gives 
standard functions dmatching, pmatching, qmatching and rmatching respectively 
for the probability mass function, cumulative distribution function, quantile function, and 
random generation function for the distribution.  (All the plots and computations in this paper 
were produced by computing the probabilities using these functions.) 
 
In the special cases where 𝑛= 0 or 𝑛= 1 the generalised matching distribution is a point-mass 
on 𝐾௡
∗= 0 or 𝐾௡
∗= 1 respectively, just as in the classical case.  For larger values the form of 
the distribution is more complicated, but it preserves the property that ℙ(𝐾௡
∗= 𝑛−1) = 0, 
reflecting the fact that it is impossible to match all but one item in any permutation.  In the 
special case where 𝜃= 0 the generalised distribution reduces down to the classical form and 
in the special case where 𝜃= 1 it denegerates down to a point-mass distribution on 𝑛.  Later 
we will show that the distribution is stochastically increasing in 𝜃, so these special cases give 
us the extremes.  In Figure 3 we plot the distribution for size values 𝑛= 1, … ,12 which shows 
some of these properties.  Once 𝑛 becomes large the distribution converges to an asymptotic 
form for the sum of a binomial random variable and a Poisson random variable with unit scale, 
and of course, for large 𝑛𝜃 the binomial part will tend to dominate the Poisson part, so the 
distribution will be close to a binomial distribution. 

Page 11 of 43 
 
FIGURE 3: The generalised matching distribution for sizes 𝑛= 1, … ,12 with 𝜃= 0.2 
 
In Theorem 7 below we derive the moment generating function of the generalised matching 
distribution.  It is possible to derive expressions for the moments using this expression or by 
applying the law of iterated expectations conditioning on the value 𝐿.  The general forms for 
the factorial moments and raw moments are messy, and not particularly illuminating.  It is also 
quite messy to obtain the higher-order moments of the distribution.  In Theorem 8 we give the 
mean and variance of the distribution, which are polynomials of the probability parameter; the 
reader can easily verify that these moments reduce down to those of the classical matching 
distribution in the case where 𝜃= 0.  In Theorem 9 we show that the generalised distribution 
obeys a central limit theorem as 𝑛→∞, so long as 0 < 𝜃< 1. 

Page 12 of 43 
THEOREM 7 (MGF): The moment generating function for the generalised distribution is: 
𝑚௄೙∗(𝑡) = ෍(𝑒௧−1)௜
𝑖!
෍ቀ𝑛
ℓቁ(𝜃𝑒௧)ℓ(1 −𝜃)௡ିℓ
௡ି௜
ℓୀ଴
௡
௜ୀ଴
. 
 
THEOREM 8 (Important central moments): The mean, variance, skewness and kurtosis of the 
generalised distribution are given respectively by: 
𝔼(𝐾௡
∗) = 1 + 𝑛𝜃−𝜃௡,                                                                                   
𝕍(𝐾௡
∗) = 1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ),                         
𝕊𝕜𝕖𝕨(𝐾௡
∗) =    ⎝
⎜⎜
⎛
1 + 𝑛𝜃(1 −3𝜃+ 2𝜃ଶ)
−𝑛(𝑛−1)
2
𝜃௡ିଶ−𝑛(2𝑛−1)𝜃௡ିଵ
+ 5𝑛ଶ−3𝑛+ 2
2
𝜃௡+ 3𝑛(𝑛+ 1)𝜃௡ାଵ−3𝑛(𝑛+ 1)𝜃௡ାଶ
−3𝑛𝜃ଶ௡ିଵ−3𝑛𝜃ଶ௡+ 6𝑛𝜃ଶ௡ାଵ−2𝜃ଷ௡
⎠
⎟⎟
⎞
[1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ)]ଷଶ
⁄
,
𝕂𝕦𝕣𝕥(𝐾௡
∗) = 3 + ⎝
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎛
1 + 𝑛𝜃−7𝑛𝜃ଶ+ 12𝑛𝜃ଷ−6𝑛𝜃ସ
−𝑛(𝑛−1)(𝑛−2)
6
𝜃௡ିଷ−3𝑛(𝑛−1)ଶ
2
𝜃௡ିଶ
−𝑛(𝑛ଶ+ 3𝑛−8)
2
𝜃௡ିଵ−49𝑛ଷ−84𝑛ଶ−61𝑛+ 6
6
𝜃௡
−2𝑛(2𝑛ଶ−3𝑛)𝜃௡ାଵ−6𝑛(𝑛ଶ+ 3𝑛+ 2)𝜃௡ାଶ
+4𝑛(𝑛+ 1)(𝑛+ 2)𝜃௡ାଷ−𝑛(5𝑛−2)𝜃ଶ௡ିଶ
−2𝑛(7𝑛−2)𝜃ଶ௡ିଵ−(𝑛ଶ−12𝑛−2)𝜃ଶ௡
+12𝑛(2𝑛+ 1)𝜃ଶ௡ାଵ−12𝑛(2𝑛+ 1)𝜃ଶ௡ାଶ
−12𝑛𝜃ଷ௡+ 24𝑛𝜃ଷ௡ାଵ−6𝜃ସ௡
⎠
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎞
[1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ)]ଶ
.
 
When 0 < 𝜃< 1 and 𝑛 is large we obtain the asymptotic equivalences: 
𝔼(𝐾௡
∗) ~ 1 + 𝑛𝜃              
𝕊𝕜𝕖𝕨(𝐾௡
∗) ~ 1 + 𝑛𝜃(1 −𝜃)(1 −2𝜃)
[1 + 𝑛𝜃(1 −𝜃)]ଷଶ
⁄
,                  
𝕍(𝐾௡
∗) ~ 1 + 𝑛𝜃(1 −𝜃)
𝕂𝕦𝕣𝕥(𝐾௡
∗) ~ 3 + 1 + 𝑛𝜃(1 −𝜃)(6𝜃ଶ−6𝜃+ 1)
[1 + 𝑛𝜃(1 −𝜃)]ଶ
.
 
This shows that the distribution is asymptotically unskewed and mesokurtic when 0 < 𝜃< 1.  
(When 𝜃= 0 we have 𝕊𝕜𝕖𝕨(𝐾௡
∗) ~ 1 and 𝕂𝕦𝕣𝕥(𝐾௡
∗) ~ 4 for the Poisson with unit rate.) 
 
THEOREM 9 (Central limit theorem): If 0 < 𝜃< 1 we get the following asymptotic result: 
ℙቆ√𝑛∙𝐾௡
∗𝑛
⁄
−1 𝑛
⁄
−𝜃
ඥ1 𝑛
⁄
+ 𝜃(1 −𝜃)
≤𝑧ቇ→Φ(𝑧)
as  𝑛→∞. 

Page 13 of 43 
It is worth noting some intuition about the central limit theorem for the generalised distribution.  
From the fact that 𝐾௡
∗= 𝐿+ 𝐾௡ି௅, if 0 < 𝜃< 1 we can see that when 𝑛→∞ the binomial 
random variable 𝐿 will come to dominate this sum and 𝐾௡
∗𝐿
⁄  will converge to unity.  Since the 
binomial distribution converges to the normal distribution (using the classical central limit 
theorem) this means that 𝐾௡
∗ also converges to a normal random variable in this limit.  The form 
of the quantity used in Theorem 8 is the standardised value of 𝐾௡
∗ using its asymptotic mean 
and variance in Theorem 8. 
 
The main value of our generalised matching distribution is that it allows us to model allocation 
that is “better than random” and is of increasing accuracy.  The case where 𝜃= 0 corresponds 
to the classical matching distribution (where items under consideration are allocated at random) 
and the case where 𝜃= 1 gives a point-mass distribution on 𝐾௡
∗= 𝑛.  Between these extremes 
it can be shown that the parameter 𝜃 has a monotonic effect on the number of matches.  To see 
this, we note that {𝐾௡|𝑛∈ℕ} is a pure-birth process, which means that 𝐾௡+ 1 ≥𝐾௡ାଵ for all 
𝑛∈ℕ.  Consequently, for all values 0 ≤ℓ< 𝑛 we have: 
∆(𝑘, ℓ) ≡ℙ(𝐾௡
∗≤𝑘|𝐿= ℓ) −ℙ(𝐾௡
∗≤𝑘|𝐿= ℓ+ 1)                             
          = ℙ(𝐿+ 𝐾௡ି௅≤𝑘|𝐿= ℓ) −ℙ(𝐿+ 𝐾௡ି௅≤𝑘|𝐿= ℓ+ 1) 
= ℙ(ℓ+ 𝐾௡ିℓ≤𝑘) −ℙ(ℓ+ 𝐾௡ିℓିଵ+ 1 ≤𝑘)            
   = ℙ(ℓ+ 𝐾௡ିℓ≤𝑘) −ℙ(ℓ+ (𝐾௡ିℓିଵ+ 1) ≤𝑘) ≥0. 
This establishes that 𝐾௡
∗ is stochastically increasing in 𝐿, and since 𝐿 is stochastically increasing 
in 𝜃 it then follows that 𝐾௡
∗ is stochastically increasing in 𝜃.  If we again exclude the extremes 
and look at the case where 0 < 𝜃< 1 the matching distribution obeys a central limit theorem 
shown in Theorem 9. 
 
Since the mass function for the generalised matching distribution is a weighted sum of binomial 
mass functions, it is a polynomial in 𝜃.  To facilitate this analysis, and some further analysis 
later in the paper, we will use the succinct notation 𝑀(𝑘, ℓ) ≡Match(𝑘−ℓ|𝑛−ℓ) to denote 
relevant values of the mass function for the classical matching distribution, which then gives: 
Match(𝑘|𝑛, 𝜃) = ෍Bin(ℓ|𝑛, 𝜃) ∙𝑀(𝑘, ℓ)
௞
ℓୀ଴
𝑀(𝑘, ℓ) =
1
(𝑘−ℓ)! ෍(−1)௜
𝑖!
௡ି௞
௜ୀ଴
. 
This mixture form for the generalised matching distribution is useful for computation.  We can 
use recursive methods to compute the classical matching distribution and then use the mixture 
result to generalise to the generalised matching distribution. 

Page 14 of 43 
4. Computing the generalised matching distribution 
 
As previously noted above, the explicit form for the mass function for the matching distribution 
involves a sum of terms with alternating sign.  Such formulae are notoriously problematic for 
computation since they often lead to arithmetic underflow problems if one tries to compute via 
the explicit form.  Moreover, it is best to compute probability functions in log-space to avoid 
arithmetic underflow from small probabilities.  Consequently, the best way to compute the 
mass function for the classical matching distribution is work in log-space using the recursive 
form for the mass function (using the corollary to Theorem 5) and then convert back to regular 
probability space at the end of the computation.  For the generalised matching distribution, we 
can compute the log-probabilities from the mass function using its convolution representation 
with the classical matching distribution and the binomial distribution.  In Algorithm 1 below 
we implement this computational method, dealing separately with some trivial special cases. 
 
ALGORITHM 1: Generalised Matching Distribution 
 
Input:  
Size parameter n (positive integer or Inf) 
 
 
Probability parameter θ (probability value) 
 
 
Logical value log (specifying whether output is log-probability) 
Output: 
Vector of probabilities/log-probabilities from the 
 
 
generalised matching distribution mass function for k = 0,…,n 
 
 
########################## Deal with some special cases ######################### 
 
#Deal with special case where n = 1 
#Distribution is a point-mass on one 
if (n = 1) {  
  LOGPROBS <- [-Inf, 0] } 
 
#Deal with special case where 1 < n < Inf and θ = 1 
#Distribution is a point-mass on n 
if (1 < n < Inf)&(θ = 1) {  
  LOGPROBS <- [-Inf, …, -Inf, 0] (vector has length = n+1) } 
 
#Deal with special case where n = Inf and θ = 0 
#Distribution is the Poisson distribution with unit rate 
if (n = Inf)&(θ = 0) {  
  LOGPROBS <- log(Pois(0:n|1)) } 
 
#Deal with special case where n = Inf and θ > 0 
#Distribution is a point-mass on infinity 
if (n = Inf)&(θ > 0) {  
  stop('Error: Distribution is a point-mass on infinity') } 
 
############################## Deal with general case ########################### 
 
#Deal with non-trivial case where 1 < n < Inf and θ < 1 
if (1 < n < Inf)&(θ < 1) { 
   

Page 15 of 43 
  #Compute the classical matching distribution by recursive method 
  BASE.LOGPROBS <- Vector indexed by k = 0,…,n  
                   (all starting values are -Inf) 
  BASE.LOGPROBS[n] <- -logfactorial(n) 
  for each i = 1,…,n { 
    k <- n-i 
    TERM1 <- log(n-k-1) + BASE.LOGPROBS[k+1] 
    TERM2 <- ifelse(k < n-1, log(k+2) + BASE.LOGPROBS[k+2], -Inf) 
    BASE.LOGPROBS[k] <- log(k+1) - log(n-k) + logsumexp(TERM1, TERM2) } 
  BASE.LOGPROBS <- BASE.LOGPROBS - logsumexp(BASE.LOGPROBS) 
   
  #Compute the matrix M 
  M <- Matrix with rows indexed by k = 0,…,n and columns indexed by l = 0,…,n 
       (all starting values are -Inf) 
  for each k = 0,…,n { 
  for each l = 0,…,k { 
    M[k, l] <- BASE.LOGPROBS[n-l, k-l] } } 
   
  #Compute the generalised matching distribution using mixture method 
  LOGPROBS <- Vector indexed by k = 0,…,n  
              (all starting values are -Inf) 
  LOGBINOM <- log(Bin(0:n|n, θ) 
  for each k = 0,…,n { 
    LOGPROBS[k] <- logsumexp(LOGBINOM + M[k, ]) } 
  LOGPROBS <- LOGPROBS - logsumexp(LOGPROBS) } 
 
#Return output 
if (log) { LOGPROBS } else { exp(LOGPROBS) } 
 
 
The computation method in Algorithm 1 is stable and accurate and allows the user to compute 
the mass function for any allowable parameter inputs within the computational power of the 
machine.4  The algorithm computes the probability mass function for the generalised matching 
distribution, but it also forms the basis for computing other probability functions, including the 
cumulative distribution function and quantile function.  In the stat.extend package this 
algorithm is used in the functions dmatching, pmatching and qmatching for the mass 
function, cumulative distribution function and quantile function.  The latter functions involve 
some further computational steps but use the same algorithm for computing the underlying log-
probabilities for the distribution.  Rather than producing the mass function (or other functions) 
over the full range 𝑘= 0, … , 𝑛 these functions are modified to instead take an arbitrary input 
vector for the argument values and compute outputs over these argument values (i.e., they are 
“vectorised” versions of the probability mass function, cumulative distribution function and 
quantile function).  (It is worth noting that Figures 1-3 above, showing the mass functions of 
the classical and generalised matching distribution, were each produced using the dmatching 
function to compute the probabilities.) 
 
4 Note that the parameter input 𝑛= ∞ and 𝜃> 0 leads to a point-mass distribution on infinity, which is not a 
proper distribution and cannot be represented in finite computation.  The algorithm gives an error message in this 
case which note the true distribution. 

Page 16 of 43 
Algorithm 1 can be extended to compute the highest-density region (HDR) for the generalised 
matching distribution by combining it with the discrete HDR algorithm presented in O’Neill 
(2021).  It is simple to create other user-friendly functions that compute the moments of the 
distribution and other aspects of interest.  The stat.extend package contains the function 
HDR.matching to compute the HDR from stipulated parameters for the distribution and a 
specified minimum coverage probability.  It also contains the function moments.matching 
which computes the first four central moments of the generalised matching function.5   
 
In the code below we generate and plot the probability functions for the generalised matching 
distribution with 𝑛= 12 and 𝜃= 0.2.  The plots are shown in Figure 4 below.  In addition to 
producing these probability plots we also compute the 95% HDR of the distribution.  (Our 
commands are shown in blue and the output is in black.) 
 
#Load library and set parameters 
library(stat.extend) 
n    <- 12 
PROB <- 0.2 
 
#Compute and plot the probability mass function 
PDF <- dmatching(0:n, size = n, prob = PROB) 
barplot(PDF, names.arg = 0:n, col = 'blue', 
        xlab = 'Value', ylab = 'Probability') 
 
#Compute and plot the cumulative distribution function 
CDF <- pmatching(0:n, size = n, prob = PROB) 
barplot(CDF, names.arg = 0:n, col = 'blue', 
        xlab = 'Value', ylab = 'Cumulative Probability') 
 
#Compute and plot the quantile function 
PROBS <- (0:100)/100 
QUANT <- qmatching(PROBS, size = n, prob = PROB) 
plot(PROBS, QUANT, pch = 16,  
     xlab = 'Probability', ylab = 'Quantile') 
 
#Compute and plot pseudo-random values 
set.seed(1) 
RAND  <- rmatching(10^4, size = n, prob = PROB) 
PROPS <- table(factor(RAND, levels = 0:n))/length(RAND) 
barplot(PROPS, col = 'red', xlab = 'Value', 
        ylab = 'Empirical Proportion') 
 
 
5 The function also has a logical option include.sd to specify whether the user wishes to include the standard 
deviation of the distribution in the output; by default this is not included. 

Page 17 of 43 
#Compute the 95% HDR of the distribution 
HDR.matching(cover.prob = 0.95, size = n, prob = PROB) 
 
        Highest Density Region (HDR) 
 
96.00% HDR for matching distribution with  
12 trials and matching probability 0.2 
Computed using discrete optimisation with  
minimum coverage probability = 95.00%  
 
1..7 
 
  
 
FIGURE 4: The generalised matching distribution with 𝑛= 12 and 𝜃= 0.2 
(top left) probability density function; (bottom left) cumulative distribution function; 
(bottom right) quantile function; (top right) proportions from random sample 
 

Page 18 of 43 
Each of these probability functions other than the random-generation function is built on the 
computational method in Algorithm 1.  For random generation from the generalised matching 
distribution we can use inverse-transformation sampling via quantile function, computed using 
the algorithm.  Alternatively, we can generate values using direct simulation of the binomial 
matches and direct simulation of random permutations for the matching part.  Interested readers 
can review code for the functions dmatching, pmatching, qmatching and rmatching 
in the stat.extend package to see how Algorithm 1 is adapted to create each probability 
function.   
 
5. Likelihood function and inference for the unknown probability parameter 
 
In applications of the matching problem, the size 𝑛 is fixed by the design of the matching game, 
but the probability parameter 𝜃 is unknown.  In a matching game, this parameter represents the 
ability of the allocator to allocate the items “better than random” with a higher probability value 
representing greater likelihood of correct allocation.  Here we will consider some methods to 
make inferences for this parameter.  We will begin by considering some point-estimators and 
then later we will look at interval estimates.  We will see that there are some essential coherence 
properties we would wish to impose on interval estimators for the parameter, which presents 
some challenges in finding an appropriate interval estimator. 
 
In order to make inferences about the parameter 𝜃 we will derive the form of the log-likelihood 
function, score function, and information function, first for a single observation and then for a 
vector of IID observations.  For purposes of numerical stability, and for other reasons, it is also 
useful to consider the problem framed in terms of the transformed parameter 𝜃⟼𝜙 given by: 
𝜃=
𝑒థ
𝑒థ+ 𝑒ିథ
𝜙= −1
2 ∙log ൬1 −𝜃
𝜃
൰. 
This transformation transforms the probability parameter 0 ≤𝜃≤1 to a value 𝜙∈ℝഥ on the 
extended real line, which means that subsequent steps use unconstrained optimisation instead 
of constrained optimisation.  For purposes of succinct statement of the functions, we will use 
the notation 𝑀(𝑘, ℓ) ≡Match(𝑘−ℓ|𝑛−ℓ) introduced above.  We begin by considering the 
case of a single observation 𝑘, corresponding to a single round of the matching game.  The size 
parameter 𝑛 is fixed by the design of the matching game (i.e., it is a known constant).  The log-
likelihood function, score function and Hessian function are shown in Theorem 10 below, and 
the corresponding functions for the transformed parameter 𝜙 are shown in the corollary. 

Page 19 of 43 
THEOREM 10 (Score function and Hessian function): For a single observation 𝑘 from the 
generalised matching distribution the log-likelihood function is: 
ℓ௞(𝜃) = log ൭෍Bin(ℓ|𝑛, 𝜃) ∙𝑀(𝑘, ℓ)
௞
ℓୀ଴
൱. 
The corresponding score function and Hessian function are given respectively by: 
𝑠௞(𝜃) =
1
Match(𝑘|𝑛, 𝜃) ෍[ℓ−𝑛𝜃] ∙Bin(ℓ|𝑛, 𝜃)
𝜃(1 −𝜃) ∙𝑀(𝑘, ℓ)
௞
ℓୀ଴
,                          
𝐻௞(𝜃) =
1
Match(𝑘|𝑛, 𝜃) ෍቎
(ℓ−1)ℓ
−2ℓ(𝑛−1)𝜃
+𝑛(𝑛−1)𝜃ଶ
቏∙Bin(ℓ|𝑛, 𝜃)
𝜃ଶ(1 −𝜃)ଶ∙𝑀(𝑘, ℓ)
௞
ℓୀ଴
−𝑠௞(𝜃)ଶ.
 
 
COROLLARY: Consider the transformed parameter 𝜃⟼𝜙 given by: 
𝜃=
𝑒థ
𝑒థ+ 𝑒ିథ
𝜙= −1
2 ∙log ൬1 −𝜃
𝜃
൰. 
The score function and Hessian function for 𝜙 are given (in terms of the parameter 𝜃) by: 
𝑠௞(𝜙) =
2
Match(𝑘|𝑛, 𝜃) ෍[ℓ−𝑛𝜃] ∙Bin(ℓ|𝑛, 𝜃) ∙𝑀(𝑘, ℓ)
௞
ℓୀ଴
,          
𝐻௞(𝜙) =
4
Match(𝑘|𝑛, 𝜃) ෍቎
(ℓ−1)ℓ
−2ℓ(𝑛−1)𝜃
+𝑛(𝑛−1)𝜃ଶ
቏∙Bin(ℓ|𝑛, 𝜃) ∙𝑀(𝑘, ℓ)
௞
ℓୀ଴
−4(𝜃−½)𝑠௞(𝜙) −𝑠௞(𝜙)ଶ.                 
 
We can write the score function and Hessian function for 𝜃 in terms of the score function and 
Hessian function for 𝜙 as follows: 
𝑠௞(𝜃) =
𝑠௞(𝜙)
2𝜃(1 −𝜃)
𝐻௞(𝜃) = 𝐻௞(𝜙) + 4(𝜃−½)𝑠௞(𝜙)
4𝜃ଶ(1 −𝜃)ଶ
. 
 
We can generalise this likelihood analysis for the case where a single player plays a matching 
game with 𝑛 objects 𝑚 times, where we assume that these are IID trials with fixed parameters 
(i.e., we assume that the player is not getting better or worse at the game as they play it more).  
Suppose that we observe the outcomes 𝒌= (𝑘ଵ, … , 𝑘௠) over these games.  Taking 𝑛 as fixed, 
the log-likelihood function, score function and Hessian function are then given by summing 
over the data points in the observed data vector. 
 

Page 20 of 43 
It is simple to obtain the MLE for an IID sample from the generalised matching distribution by 
maximising the log-likelihood function using numerical methods.  Since we have an explicit 
form for the Hessian function (and therefore the Fisher information) we can also compute the 
asymptotic standard error of the MLE and use this to obtain the standard asymptotic confidence 
interval for the unknown parameter.  Alternatively, we can use bootstrapping to obtain a set of 
values for the MLE (based on resamples of the observed data 𝒌) and use this to form a bootstrap 
confidence interval for the unknown parameter.  In either case, it is best to undertake both these 
computations in terms of the parameter 𝜙 and then transform back to obtain a corresponding 
MLE and confidence interval for 𝜃.  Using the transformed parameter 𝜙 converts the problem 
to an unconstrained optimisation, which improves numerical stability, and also ensures that the 
resulting confidence interval respects the allowable parameter range. 
 
The log-likelihood for IID data from the generalised matching distribution is unimodal, so there 
is a unique maximum likelihood estimator (MLE) for 𝜃.  In the special case where 𝑘ത௠≤1 the 
log-likelihood is monotonically decreasing in 𝜃, so the MLE is 𝜃෠୑୐୉= 0.  In the special case 
where 𝑘ത௠= 𝑛 the log-likelihood is monotonically increasing in 𝜃, so the MLE is 𝜃෠୑୐୉= 1.  
In the remaining cases the MLE is found using numerical methods.  To compute the confidence 
interval, we recommend eschewing an equal-tail interval in favour of a “moving” interval.  In 
view of the monotonicity properties of the log-likelihood, when 𝑘ത௡≤1 the lower bound of the 
interval should be at zero and when 𝑘ത௡= 𝑛 the upper bound of the interval should be at one.  
Consequently, at confidence level 1 −𝛼 we recommend using the lower tail area: 
𝛼଴= max(𝑘ത௡−1, 0)
𝑛−1
. 
Use of this lower-tail area gives a “moving” interval which interpolates between these extremes 
to give a confidence interval that (roughly) takes account of the changing shape of the log-
likelihood function.  In particular, it should give the bounds required by the monotonicity 
properties of the log-likelihood function.  In these cases, it is worth noting that the Hessian of 
the log-likelihood function vanishes at its extremes and so the standard asymptotic confidence 
interval performs poorly when the MLE for 𝜃 is near to zero or one (due to the failure of the 
underlying assumptions for that interval).  Consequently, we recommend using the bootstrap 
interval in these cases. 
 

Page 21 of 43 
The MLE and resulting confidence interval are implemented in the MLE.match function in 
the stat.extend package.  This function allows the user to compute the confidence interval 
using the standard asymptotic form or via bootstrapping.  In both cases the function uses the 
moving interval with the lower-tail area shown above.  In the code below we give an example 
where we generate a random sample of size 𝑚= 40 from the matching distribution and use 
this to estimate the probability parameter.  (Our commands are shown in blue and the output is 
shown in black.)  In this example we use the bootstrap confidence interval, which gives better 
results in cases where the true probability parameter is near its boundaries (i.e., close to zero 
or one).  As can be seen from the output, the MLE in this example is close to the true value of 
the probability parameter and the confidence interval contains the true value. 
 
#Load library and set parameters 
library(stat.extend) 
n    <- 16 
PROB <- 0.04 
 
#Generate an IID random sample 
set.seed(1) 
DATA <- rmatching(40, size = n, prob = PROB) 
 
#Generate the MLE and bootstrap confidence interval 
MLE.matching(x = DATA, size = n, 
             CI.method = 'bootstrap', conf.level = 0.99) 
 
    Maximum likelihood estimator (MLE)  
  
Data vector DATA containing 40 IID values  
from the generalised matching  
distribution with size = 16 and  
unknown probability parameter  
 
  MLE of probability parameter  
     0.038502  
 
  Maximum log-likelihood  
     -67.96092  
 
  Likelihood-per-data-point (maximised geometric mean)  
     0.182862  
 
  Bootstrap 99% CI for probability parameter  
using 1000 resamples 
    [0, 0.072687] 
 

Page 22 of 43 
The MLE is just one way that the probability parameter can be estimated.  A simpler estimator 
for the probability parameter can be obtained using the method of moments.  Recalling from 
Theorem 8 that 𝔼(𝐾௡
∗) = 1 + 𝑛𝜃−𝜃௡, and given the monotonicity properties of the likelihood 
function, the natural method-of-moments (MOM) estimator is given by the polynomial root:6 
𝜃෠௡−𝑛𝜃෠+ max(𝑘ത௡−1,0) = 0. 
For 𝑛= 1 there is no valid MOM estimator since the matching distribution does not depend 
on 𝜃 (in this case any value for 𝜃෠ satisfies the above equation).  However, for 𝑛> 1 there is a 
unique MOM estimator, and it is simple to use root-finding methods to obtain this value.7  It is 
easy to see that if 𝑘ത௡= 0 or 𝑘ത௡= 1 then 𝜃෠= 0 and if 𝑘ത௡= 𝑛 then 𝜃෠= 1.  Moreover, the 
estimate 𝜃෠ is increasing in 𝑘ത௡ for values between these two extremes.  If 𝑛 is large we have the 
approximate MOM estimator: 
𝜃෠≈max(𝑘ത௡−1,0)
𝑛−1
. 
In the code below we compute the MOM estimate and the approximate MOM estimator for the 
above example.  As can be seen, in this example the MOM estimate is close to the MLE, but 
slightly further away from the true probability parameter than this latter estimate.  Moreover, 
the approximate MOM estimate is still further from the true probability parameter. 
 
#Set the MOM function 
FUNC <- function(t) {  
  t^n - n*t + max(mean(DATA)-1, 0) } 
 
#Find the MOM estimator 
uniroot(f = FUNC, lower = 0, upper = 1)$root 
 
[1] 0.0390625 
 
#Find the approximate MOM estimator 
max(mean(DATA)-1, 0)/(n-1) 
 
[1] 0.04666667 
 
 
6 Note that we have made a slight adjustment to the method of moments since 𝔼(𝐾௡
∗) ≥1 and so the observation 
𝑘= 0 falls below the expected matches for any value of the probability parameter.  In this special case, we 
estimate 𝜃෠= 0 which is a sensible estimate given that the matching statistic is stochastically increasing in 𝜃. 
7 To see this, we observe that the MOM estimator is given by the roots 𝐹(𝜃෠) = 0 using the function: 
𝐹(𝜃) = 𝜃௡−𝑛𝜃+ max(𝑘ത௡−1,0). 
For 𝑛> 1 and 𝜃< 1 we have 𝐹ᇱ(𝜃) = −𝑛(1 −𝜃௡ିଵ) < 0 so 𝐹 is strictly decreasing.  At the boundaries of the 
parameter range we have 𝐹(0) = max(𝑘ത௡−1,0) ≥0 and 𝐹(1) = max(𝑘ത௡, 1) −𝑛≤0.  Consequently, under 
the condition that 𝑛> 1 there is a unique critical point 𝜃෠ satisfying 𝐹(𝜃෠) = 0. 

Page 23 of 43 
6. The matching test — hypothesis testing for the probability parameter 
 
Instead of generating an interval estimate for the probability parameter it is sometimes useful 
to conduct a hypothesis test on this parameter.  We will again consider the case where a player 
plays a matching game with 𝑛 objects 𝑚 times, giving rise to IID trials with fixed parameters, 
and we observe the outcomes 𝒌= (𝑘ଵ, … , 𝑘௠) over these games.  We will consider both one-
sided and two-sided tests using a simple null hypothesis positing a value 𝜃଴ for the probability 
parameter.  This encompasses a range of useful tests for the matching probability. 
 
Since the generalised matching distribution follows the monotone-likelihood ratio property, a 
reasonable test statistic for the hypothesis test is the mean number of matches over the trials, 
with a larger mean number of matches constituting evidence for a higher value of 𝜃 and a lower 
mean number of matches constituting evidence for a lower value of 𝜃.8  This makes it useful 
to compute the distribution of the total number of matches 𝑇௠= ∑
𝐾௜
௠
௜ୀଵ
 over 𝑚 trials where 
𝐾ଵ, … , 𝐾௠ ~ IID Match(𝑛, 𝜃).  Using convolutions of the generalised matching distribution it 
is fairly simple to further generalise the distribution to add a trials parameter 𝑚 and return 
the distribution of the total number of matches under that number of trials.  To this end, we will 
use the following notation to denote this more generalised distribution: 
Match(𝑡|𝑛, 𝑚, 𝜃) = ℙ(𝑇௠= 𝑡|𝑛, 𝑚, 𝜃). 
Of course, it is trivial to see that the special case where 𝑚= 1 gives the generalised matching 
distribution previously described, so we have Match(𝑡|𝑛, 1, 𝜃) = Match(𝑡|𝑛, 𝜃).  If 𝑚 is not 
too large, then we can compute the exact distribution for the total matches via convolutions of 
the generalised matching distribution.  Contrarily, if 𝑚 is large, we can rely on the central limit 
theorem to approximate the generalised matching distribution with a normal distribution over 
the appropriate support.9  We also note that there are some trivial special cases where we can 
perform exact computation easily, even when 𝑚 is large.10 
 
 
8 Another way to conduct the hypothesis test is to use the likelihood-ratio statistic for the generalised matching 
distribution.  That method is more complex than the test we will use here. 
9 For the normal approximation we use the moments in Theorem 8 and we approximate the generalised distribution 
by Match(𝑡|𝑛, 𝑚, 𝜃) ≈N(𝑡|𝑚𝔼(𝐾௡
∗), 𝑚𝕍(𝐾௡
∗)) over 𝑡= 0, … , 𝑛𝑚.  (Since it is impossible to get 𝑛−1 matches 
in a single trial the support of this generalised distribution always excludes the point 𝑡= 𝑚𝑛−1.) 
10 Some trivial special cases are noted here.  If 𝑛= 0 then the distribution is a point-mass on 𝑡= 0, if 𝑛= 1 then 
the distribution is a point-mass on 𝑡= 𝑚, if 𝜃= 1 then the distribution is a point-mass on 𝑡= 𝑛𝑚.  If 𝑛= ∞ and 
𝜃> 0 then the distribution is a point-mass on 𝑡= ∞, and if 𝑛= ∞ and 𝜃= 0 then the distribution reduces to the 
Poisson distribution Match(𝑡|∞, 𝑚, 0) = Pois(𝑡|𝑚).  In these cases we would not use the normal approximation 
to the distribution even if 𝑚 is large. 

Page 24 of 43 
The various functions for the generalised matching distribution in the stat.extend package 
actually do accommodate this generalisation.  Each function allows an input for the number of 
trials 𝑚, with default behaviour setting 𝑚= 1 if the number of trials is not specified.11  This 
distribution allows us to easily compute the p-value for any variation of the matching test.  For 
the one-sided versions of the test (testing for parameter values above/below the null value) the 
p-value is the probability that the total number of matches 𝑇௠ would be no less than/no greater 
than the total observed matches 𝑡௠.  For the two-sided version of the test (testing for parameter 
values different to the null value) the p-value is the probability that the total number of matches 
𝑇௠ takes on a value no more probable than the total observed matches 𝑡௠.  Each of these 
probabilities can easily be computed from the generalised matching distribution for 𝑇௠. 
 
The canonical version of the matching test occurs when we test whether there is evidence that 
the matching done by a player is “better than random”, which amounts to testing whether the 
probability parameter is zero.  In this case we have the hypotheses: 
𝐻଴: 𝜃= 0
𝐻୅: 𝜃> 0. 
The matching test is implemented in the matching.test function in the stat.extend 
package.  In the code below we implement the canonical matching test, and then a more specific 
one-sided matching test on the data vector previously generated from the generalised matching 
distribution.  (By way of reminder, the vector DATA consists of 𝑚= 40 independent values 
from the generalised matching distribution with size 𝑛= 16 and probability 𝜃= 0.04.)  As 
can be seen from the outcome of the two tests in this example, we find strong evidence that 
𝜃> 0 (i.e., the matching is “better than random”) but we find no evidence that 𝜃> 0.05. 
 
#Perform a matching test for matching “better than random” 
matching.test(DATA, size = 16) 
 
        Matching test 
 
data:  DATA 
mean matches = 1.625, p-value = 0.0001726 
alternative hypothesis: true prob is greater than 0 
 
 
11 This is done using the trials input in the functions.  By default, this parameter is set to one, which gives the 
generalised matching distribution for a single trial.  By default the function will compute the exact distribution 
using convolutions of the generalised matching distribution for a single trial.  In the case where 𝑚> 100 it will 
switch (by default) to the normal approximation.  The user can override this behaviour by using the approx input 
to tell the function whether or not to use the normal approximation to the distribution. 

Page 25 of 43 
#Perform a matching test with a null probability of 5% 
matching.test(DATA, size = 16, null.prob = 0.05) 
 
        Matching test 
 
data:  DATA 
mean matches = 1.625, p-value = 0.8134 
alternative hypothesis: true prob is greater than 0.05 
 
Since we use the mean number of matches as our test statistic in the matching test, this test is 
a fairly straightforward inquiry into the distribution of the total number of matches in a set of 
trials of the matching game.  It is natural to further inquire into the power of this test, to see 
how useful it is in identifying situations where we should reject a posited null value for the 
probability parameter in favour of some alternative hypothesis.  We will examine the power of 
the canonical matching test over a range of values for the size 𝑛 and the trials 𝑚.  When testing 
at significance level 𝛼 the rejection-region for the canonical matching test is the set of values 
𝑡≥𝑡∗(𝛼) where the lower bound 𝑡∗(𝛼) for the rejection-region is given by:12 
𝑡∗(𝛼) ≡min{𝑡= 0, … , 𝑛𝑚+ 1| ∑
Match(𝑟|𝑛, 𝑚, 0)
௡௠
௥ୀ௧
< 𝛼}. 
The power function for the canonical matching test is then given by: 
Power(𝜃|𝑛, 𝑚, 𝛼) = ℙ(𝑇௠≥𝑡∗(𝛼)|𝑛, 𝑚, 𝜃), 
This power function gives the probability of rejecting the null hypothesis 𝐻଴: 𝜃= 0 given the 
true value of the probability parameter.  Ideally, we would like the power to converge to one 
whenever 𝜃> 0 (i.e., whenever the alternative hypothesis is true). 
 
In Figure 5 below we plot the power functions for a 5% significance level canonical matching 
test for the parameter combinations 𝑛= 4, 6, 8, 10 and 𝑚= 1, 2, 3, 4, 5.  As can be seen, even 
for modest values for the size and trial parameters the power function increases quite rapidly 
with respect to the probability parameter 𝜃.  We can also see that for a fixed value of 𝑛𝑚 the 
test favours using a higher size parameter 𝑛 and a lower trial parameter 𝑚.  The general shape 
of the power functions in Figure 1 illustrates convergence towards full power for values 𝜃> 0 
(i.e., for all values in the alternative hypothesis) as the size and trials parameter increase.  This 
is desirable behaviour for the test, and it reflects the fact that 𝑇௠𝑛𝑚
⁄
→𝜃𝑚
⁄
 in probability 
under broad limiting conditions on either parameter. 
 
12 The special case where 𝑡∗(𝛼) = 𝑛𝑚+ 1 is a value outside the support of the distribution; this reflects the case 
where the rejection-region is empty.  This will occur in some cases where the parameters are sufficiently small, 
or the significance level is sufficiently low, such that ℙ(𝑇௠= 𝑛𝑚|𝑛, 𝑚, 0) ≥𝛼. 

Page 26 of 43 
 
FIGURE 5: Power functions for canonical matching test at significance level 𝛼= 0.05 
(axis ranges are both probability values from zero to one) 
 
It turns out that our power function will converge to this ideal power function if 𝑛→∞ or if 
𝑛> 1 and 𝑚→∞.  (In the case where 𝑛= 0 or 𝑛= 1 the generalised matching distribution 
is a point-mass distribution and the matching test has power zero.  This holds even if we take 
the limit 𝑚→∞.)  Before demonstrating this we will first show that the mean number of 
matches allows us to perfectly estimate a positive probability parameter under either of these 
limit conditions.  Applying the moment results in Theorem 8 and rules for means and variances 
of sums of IID random variables, we get: 
𝔼൬𝑇௠
𝑛𝑚൰= 1 + 𝑛𝜃−𝜃௡
𝑛𝑚
𝕍൬𝑇௠
𝑛𝑚൰= 1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ)
𝑛ଶ𝑚
. 

Page 27 of 43 
For 𝜃> 0 and under either of the stated limit conditions we have the asymptotic equivalence 
𝔼(𝑇௠𝑛𝑚
⁄
) →𝜃𝑚
⁄
 and 𝕍(𝑇௠𝑛𝑚
⁄
) →0.  Consequently, the estimator 𝑇௠𝑛𝑚
⁄
 converges in 
probability to 𝜃 under either limit condition.  This shows that 𝑇௠𝑛
⁄  is a consistent estimator 
for 𝜃 under either limit condition, but it also allows us to show that the power of the canonical 
matching test converges to one under the stated limiting conditions over all parameter values 
in the alternative hypothesis.  Under either limiting condition we have 𝑇௠𝑛𝑚
⁄
→𝜃𝑚
⁄
 which 
means that 𝑡∗(𝛼) 𝑛𝑚
⁄
→0.  Thus, for any value 𝜃> 0 we get: 
Power(𝜃|𝑛, 𝑚, 𝛼) = ℙ(𝑇௠≥𝑡∗(𝛼)|𝑛, 𝑚, 𝜃) = ℙ൬𝑇௠
𝑛𝑚≥𝑡∗(𝛼)
𝑛𝑚ฬ𝑛, 𝑚, 𝜃൰→1. 
 
One special case of the matching test that is noteworthy is the case where 𝑛= 2, such that the 
player in the matching game is trying to match a pair of objects in their correct order in each 
trial.  In this case, in a single trial there are only two possible matches — the incorrect way of 
matching the objects, which gives 𝐾= 0, and the correct way of matching the objects, which 
gives 𝐾= 2.  The player will know the correct order for the objects (not just guess it correctly 
via a random choice) if the player knows the place of at least one of the two objects, which 
occurs with probability 1 −(1 −𝜃)ଶ= 2𝜃−𝜃ଶ= 𝜃(2 −𝜃).  If the player does not know the 
correct order for the objects then the player will still get the correct match with probability ½ 
due to randomly matching the two objects.  Consequently, the probability of correctly matching 
both the objects in the matching game is: 
𝜙≡ℙ(𝐾= 2) = (2𝜃−𝜃ଶ) + ½(1 −2𝜃+ 𝜃ଶ) = 1 + 2𝜃−𝜃ଶ
2
, 
and we have the simplified distributions: 
𝐾ଵ
2 , … , 𝐾௠
2  ~ IID Bern(𝜙)
𝑇௠
2  ~ Bin(𝑚, 𝜙) . 
It is simple to show that 𝜙 is a strictly increasing function of 0 ≤𝜃≤1, which means that the 
canonical matching distribution effectively reduces to an exact binomial test of the hypotheses: 
𝐻଴: 𝜙= ½
𝐻୅: 𝜙> ½. 
In the code below we show an example of the canonical matching test for some data generated 
with 𝑛= 2 and we confirm that this gives the same p-value as the exact binomial test computed 
with the binom.test function in the stats package (R code team 2021).13  (Note that the 
“probability of success” mentioned in the latter test is the parameter 𝜙, not the parameter 𝜃.) 
 
 
13 The p-values are identical to within a small tolerance which is due to rounding error in the computations. 

Page 28 of 43 
#Generate a new IID random sample with size = 2 
set.seed(1) 
NEWDATA <- rmatching(100, size = 2, prob = PROB) 
table(NEWDATA) 
 
NEWDATA 
 0  2 
51 49 
 
#Perform a matching test for matching “better than random” 
(TEST1 <- matching.test(NEWDATA, size = 2)) 
 
        Matching test 
 
data:  NEWDATA 
mean matches = 0.98, p-value = 0.6178 
alternative hypothesis: true prob is greater than 0 
 
#Perform an exact binomial test 
SUCCESSES <- sum(NEWDATA)/2 
TRIALS    <- length(NEWDATA) 
(TEST2 <- binom.test(SUCCESSES, n = TRIALS, 
                     alternative = 'greater')) 
 
        Exact binomial test 
 
data:  SUCCESSES and TRIALS 
number of successes = 49, number of trials = 100, 
p-value = 0.6178 
alternative hypothesis: true probability  
of success is greater than 0.5 
95 percent confidence interval: 
 0.403865 1.000000 
sample estimates: 
probability of success  
                  0.49 
 
#Perform an exact binomial test 
TEST1$p.value - TEST2$p.value 
 
[1] 2.220446e-16 
 
It is possible to form an alternative matching test using the likelihood-ratio-statistic instead of 
the mean/total number of matches.  This has certain known advantages, including the fact that 
the Neyman-Pearson lemma ensures efficiency of the test.  Nevertheless, the simpler form of 
test we offer here using the mean/total number of matches as the test statistic has the advantage 
of simplicity and maintains reasonable power even for modest values of the parameters. 
 

Page 29 of 43 
7. Summary and conclusion 
 
In this paper we have examined the properties of the classical matching distribution and a useful 
generalisation of this distribution.  The classical matching distribution arises in the classical 
“problem of coincidences” — an antique problem dating back to the early eighteenth century.  
In this problem a player attempts to match the unknown order of a finite set of objects, and the 
order chosen by the player is taken to be a random permutation equivalent to simple-random-
sampling without replacement.  Our generalised distribution models the case where a player is 
first able to identify known matches for objects with a fixed probability (for each object), with 
the ordering of the remaining objects occurring “at random” via a random permutation.  This 
generalisation allows us to deal with cases of matching games where the player has some ability 
to match the objects under consideration “better than random”. 
 
We have given a comprehensive analysis of the generalised matching distribution, including 
derivation of its probability mass function, moment generating function, main central moments 
and asymptotic behaviour.  We have also developed a useful recursive algorithm to compute 
the probability mass function for the distribution and we have discussed how this algorithm 
can be extended to compute other probability functions.  Finally, we have examined estimators, 
confidence intervals and hypothesis tests for the probability parameter in the distribution, under 
the condition that the size parameter is fixed by experimental design.  All of this is implemented 
in functions in the stat.extend package for easy use by readers (see the table of available 
functions and their inputs below). 
 
Our generalisation of the matching distribution allows a non-zero matching probability, so it 
can accommodate cases where matching is done “better than random”.  This allows us to use 
matching data to make inferences about the matching probability.  In particular, we can use the 
canonical matching test to test for matching that is “better than random” and we can determine 
the power function for this test.  Our view is that this is a useful and realistic generalisation of 
the classical “problem of coincidences”.  Indeed, matching analysis has occurred in contexts 
such as the hat-check problem and the secret-Santa problem, where the nature of the objects 
(or their owners/gives) may give some clues as to their proper order.  In such cases it is useful 
to be able to model the possibility of a non-zero matching probability occurring prior to the 
random permutation of objects. 
 

Page 30 of 43 
We hope that this paper piques the reader’s interest in an interesting variation on an antique 
problem in probability theory.  We also hope that it serves to assist analysis of this problem in 
a range of realistic cases.  The generalised matching distribution we have examined in this 
paper is a natural extension of the classical matching distribution and it stands as an interesting 
and useful family of discrete distributions. 
 
Function 
Inputs 
dmatching 
x, size, trials = 1, prob = 0,  
log = FALSE, approx = FALSE 
pmatching 
x, size, trials = 1, prob = 0,  
lower.tail = TRUE, log.p = FALSE, approx = FALSE 
qmatching 
p, size, trials = 1, prob = 0,  
lower.tail = TRUE, log.p = FALSE, approx = FALSE 
rmatching 
n, size, trials = 1, prob = 0 
HDR.matching 
cover.prob, size, trials = 1, prob = 0,  
approx = FALSE 
moments.matching 
size, trials = 1, prob = 0, include.sd = FALSE 
MLE.matching 
size, prob = 0, CI.method = 'asymptotic', 
conf.level = 0.95, bootstrap.sims = 10^3 
matching.test 
x, size, approx = FALSE, 
null.prob = 0, alternative = 'greater',  
lsubfactorial 
x 
TABLE: Functions for the generalised matching distribution  
in the stat.extend package 
 
 
 

Page 31 of 43 
References 
 
ABRAMSON, L.R. (1973) Moments of a card matching distribution. The American Statistician 
27(4), p. 166. 
ANDERSON, T.W. (1943) On card matching. Annals of Mathematical Statistics 14(4), pp. 426-
435. 
CRAIN, B.R. (1992) On the matching problem in probability. Phi Mu Epsilon Journal 9(7), pp. 
448-450. 
FELLER, W. (1968) An Introduction to Probability Theory and Its Applications (Third Edition), 
Volume I. Wiley: New York. 
HASSANI, M. (2003) Derangements and applications. Journal of Integer Sequences 6, Article 
03.1.2. 
HASSANI, M. (2004) Cycles in graphs and derangements. The Mathematical Gazette 88(511), 
pp. 123-126. 
KNUDSEN, F.F. AND SKAU, I. (1996) On the asymptotic solution of a card-matching problem. 
Mathematics Magazine 69(3), pp. 190-197. 
MONTMORT, P.R. (1708) Essay d’Analyse sur les Jeux de Hazard. Revûe et augmentée de 
plusieurs Lettre: Paris. 
MONTMORT, P.R. (1713) Essay d’Analyse sur les Jeux de Hazard (Seconde Edition). Revûe et 
augmentée de plusieurs Lettre: Paris. 
MOSER, L. AND WYMAN, M. (1958) Stirling numbers of the second kind. Duke Mathematics 
Journal 25(1), pp. 29-43. 
O’NEILL, B. (2021) Smallest covering regions and highest density regions for discrete 
distributions. Computational Statistics (to appear). 
O’NEILL, B. AND FULTZ, N. (2021) stat.extend: highest density regions and other functions of 
distributions. R Package (Version 0.2.1). https://CRAN.R-project.org/package=stat.extend 
PENRICE, S.G. (1991) Derangements, permanents, and Christmas presents. The American 
Mathematical Monthly 98(7), pp. 617-620. 
R CORE TEAM. (2021) R: a language and environment for statistical computing. R Foundation 
for Statistical Computing: Vienna. https://www.R-project.org/ 
RENNIE, B.C. AND DOBSON, A.J. (1969) On Stirling numbers of the second kind. Journal of 
Combinatorial Theory 7(2), pp. 116-121. 
SCOVILLE, R. (1980) The hat-check problem. The American Mathematical Monthly 73(3), pp. 
262-265. 
TAKÁCS, L. (1966) The problem of coincidences. Archive for History of Exact Sciences 21, pp. 
229-244. 
 
 
 

Page 32 of 43 
Appendix: Proof of Theorems 
 
 
In this appendix we give proofs of the theorems in the main body of the paper.  Many of the 
proofs involve the use of properties of the subfactorial numbers (see e.g., Hassani 2003 and 
Hassani 2004). 
 
PROOF OF THEOREM 1: For all integer values 𝑟> 𝑛 we have 𝐾−𝑟+ 1 ≤0 and so we get: 
(𝐾)௥= 𝐾(𝐾−1) … (𝐾−𝑟+ 1) = 0. 
Contrarily, for the values 𝑟= 0, … , 𝑛 we can use the change of variable 𝑗= 𝑘−𝑟 to get: 
𝔼((𝐾)௥) = ෍(𝑘)௥∙Match(𝑘|𝑛)
௡
௞ୀ଴
          
        = ෍(𝑘)௥∙1
𝑘! ෍(−1)௜
𝑖!
௡ି௞
௜ୀ଴
௡
௞ୀ଴
 
        = ෍
1
(𝑘−𝑟)! ෍(−1)௜
𝑖!
௡ି௞
௜ୀ଴
௡
௞ୀ௥
 
= ෍1
𝑗! ෍(−1)௜
𝑖!
௡ି௥ି௝
௜ୀ଴
௡ି௥
௝ୀ଴
 
             = ෍Match(𝑗|𝑛−𝑟)
௡ି௥
௝ୀ଴
= 1, 
which establishes the result.  ■ 
 
PROOF OF THEOREM 2: The raw moments can be obtained from the factorial moments via: 
𝔼(𝐾௥) = ෍𝑆(𝑟, 𝑖) ∙𝔼((𝐾)௜)
௥
௜ୀ଴
, 
where the values 𝑆(𝑟, 𝑖) are the Stirling numbers of the second kind (Moser and Wyman 1958; 
Rennie and Dobson 1969).  Substituting the factorial moments from Theorem 1 gives: 
𝔼(𝐾௥) = ෍𝑆(𝑟, 𝑖) ∙𝕀(𝑖≤𝑛)
௥
௜ୀ଴
=
෍
𝑆(𝑟, 𝑖)
୫୧୬(௥,௡)
௜ୀ଴
, 
which was to be shown.  The final part of the theorem follows directly from the fact that the 
Bell numbers are sums of Stirling numbers of the second kind.  ■ 

Page 33 of 43 
PROOF OF THEOREM 3: We will prove the stipulated form using substitution of the moments.  
Using the raw moments in Theorem 2 we have: 
𝑚௄(𝑡) = 1 + ෍𝑡௥
𝑟! ∙𝔼(𝐾௥)
ஶ
௥ୀଵ
                 
             = ෍𝑡௥
𝑟! ෍𝑆(𝑟, 𝑖) ∙𝕀(𝑖≤𝑛)
௥
௜ୀ଴
ஶ
௥ୀ଴
 
               = ෍෍𝑡௥
𝑟! ∙𝑆(𝑟, 𝑖) ∙𝕀(𝑖≤𝑛)
௥
௜ୀ଴
ஶ
௥ୀ଴
 
= ෍෍𝑡௥
𝑟! ∙𝑆(𝑟, 𝑖)
ஶ
௥ୀ௜
௡
௜ୀ଴
     
= ෍(𝑒௧−1)௜
𝑖!
௡
௜ୀ଴
,           
which was to be shown.  ■ 
 
PROOF OF THEOREM 4: To establish these results we will compute the central moments from 
the raw moments.  The latter are given in Theorem 2 by the formula: 
𝔼(𝐾௥) =
෍
𝑆(𝑟, 𝑖)
୫୧୬(௥,௡)
௜ୀ଴
. 
For 𝑛= 0 we have: 
  𝔼(𝐾) = 0
𝔼(𝐾ଶ) = 0
𝔼((𝐾−𝜇)ଶ) = 0,
𝔼(𝐾ଷ) = 0
𝔼((𝐾−𝜇)ଷ) = 0,
𝔼(𝐾ସ) = 0
𝔼((𝐾−𝜇)ସ) = 0.
 
For 𝑛= 1 we have: 
  𝔼(𝐾) = 1
𝔼(𝐾ଶ) = 1
𝔼((𝐾−𝜇)ଶ) = 0,
𝔼(𝐾ଷ) = 1
𝔼((𝐾−𝜇)ଷ) = 0,
𝔼(𝐾ସ) = 1
𝔼((𝐾−𝜇)ସ) = 0.
 
For 𝑛= 2 we have: 

Page 34 of 43 
  𝔼(𝐾) = 1
𝔼(𝐾ଶ) = 2
𝔼((𝐾−𝜇)ଶ) = 1,
𝔼(𝐾ଷ) = 4
𝔼((𝐾−𝜇)ଷ) = 0,
𝔼(𝐾ସ) = 8
𝔼((𝐾−𝜇)ସ) = 1.
 
For 𝑛= 3 we have: 
  𝔼(𝐾) = 1
𝔼(𝐾ଶ) = 2
𝔼((𝐾−𝜇)ଶ) = 1,
𝔼(𝐾ଷ) = 5
𝔼((𝐾−𝜇)ଷ) = 1,
𝔼(𝐾ସ) = 14
𝔼((𝐾−𝜇)ସ) = 3.
 
For 𝑛≥4 we have: 
  𝔼(𝐾) = 1
𝔼(𝐾ଶ) = 2
𝔼((𝐾−𝜇)ଶ) = 1,
𝔼(𝐾ଷ) = 5
𝔼((𝐾−𝜇)ଷ) = 1,
𝔼(𝐾ସ) = 15
𝔼((𝐾−𝜇)ସ) = 4.
 
The raw and central moments in the theorem follow directly from these values.  ■ 
 
PROOF OF THEOREM 5: The base equation in the theorem follows by substitution: 
Match(𝑛|𝑛) =
1
(𝑛−𝑛)! 𝑛! ∙! (𝑛−𝑛) =
1
0! 𝑛! ∙! 0 = 1
𝑛! . 
To establish the recursive equation in the theorem we use the corresponding recursive equation 
for the subfactorial numbers (see e.g., Hassani 2004), which is: 
! (𝑛−𝑘) = (𝑛−𝑘−1)[! (𝑛−𝑘−1) + ! (𝑛−𝑘−2)]. 
Applying this equation gives: 
Match(𝑘|𝑛) =
1
(𝑛−𝑘)! 𝑘! ∙! (𝑛−𝑘)                                                                                        
=
1
(𝑛−𝑘)! 𝑘! ∙(𝑛−𝑘−1)[! (𝑛−𝑘−1) + ! (𝑛−𝑘−2)] 
        =   𝑘+ 1
𝑛−𝑘∙(𝑛−𝑘−1) ×
1
(𝑛−𝑘−1)! (𝑘+ 1)! ∙! (𝑛−𝑘−1) 
    + 𝑘+ 1
𝑛−𝑘∙(𝑘+ 2) ×
1
(𝑛−𝑘−2)! (𝑘+ 2)! ∙! (𝑛−𝑘−2) 
                      = 𝑘+ 1
𝑛−𝑘[(𝑛−𝑘−1) ∙Match(𝑘+ 1|𝑛) + (𝑘+ 2) ∙Match(𝑘+ 2|𝑛)], 
which was to be shown.  ■ 
 

Page 35 of 43 
PROOF OF THEOREM 6: To establish the result we use the corresponding recursive equation 
for the subfactorial numbers, which is: 
𝒟(𝑥) = (𝑥−1)[𝒟(𝑥−1) + 𝒟(𝑥−2)]. 
Applying this equation with 𝑥= 𝑛−𝑘+ 1 gives: 
Match(𝑘|𝑛+ 1) =
1
(𝑛−𝑘+ 1)! 𝑘! ∙𝒟(𝑛−𝑘+ 1)                                                            
             =
1
(𝑛−𝑘+ 1)! 𝑘! ∙(𝑛−𝑘)[𝒟(𝑛−𝑘) + 𝒟(𝑛−𝑘−1)] 
=   
𝑛−𝑘
𝑛−𝑘+ 1 ×
1
(𝑛−𝑘)! 𝑘! ∙𝒟(𝑛−𝑘)                  
              +
𝑘+ 1
𝑛−𝑘+ 1 ×
1
(𝑛−𝑘−1)! (𝑘+ 1)! ∙𝒟(𝑛−𝑘−1) 
                       =
𝑛−𝑘
𝑛−𝑘+ 1 × Match(𝑘|𝑛) +
𝑘+ 1
𝑛−𝑘+ 1 × Match(𝑘+ 1|𝑛) , 
which was to be shown.  ■ 
 
PROOF THEOREM 7: Using Theorem 3 and the relationship 𝐾௡
∗= 𝐿+ 𝐾௡ି௅ we have 
𝑚௄೙∗(𝑡) ≡𝔼(𝑒௧௄೙∗) = 𝔼(𝑒௧௅ା௧௄೙షಽ)                                             
= ෍𝔼(𝑒௧௅ା௧௄೙షಽ|𝐿= ℓ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
       
= ෍𝑒௧ℓ∙𝔼(𝑒௧௄೙షℓ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
                
= ෍𝑒௧ℓ∙𝑚௄೙షℓ(𝑡) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
                 
= ෍𝑒௧ℓ∙ቌ෍(𝑒௧−1)௜
𝑖!
௡ିℓ
௜ୀ଴
ቍ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
    
= ෍෍𝑒௧ℓ∙(𝑒௧−1)௜
𝑖!
∙Bin(ℓ|𝑛, 𝜃)
௡ିℓ
௜ୀ଴
௡
ℓୀ଴
          
   = ෍෍𝑒௧ℓ∙(𝑒௧−1)௜
𝑖!
∙ቀ𝑛
ℓቁ𝜃ℓ(1 −𝜃)௡ିℓ
௡ିℓ
௜ୀ଴
௡
ℓୀ଴
 
  = ෍෍(𝑒௧−1)௜
𝑖!
∙ቀ𝑛
ℓቁ(𝜃𝑒௧)ℓ(1 −𝜃)௡ିℓ
௡ିℓ
௜ୀ଴
௡
ℓୀ଴
 

Page 36 of 43 
 = ෍(𝑒௧−1)௜
𝑖!
෍ቀ𝑛
ℓቁ(𝜃𝑒௧)ℓ(1 −𝜃)௡ିℓ
௡ି௜
ℓୀ଴
,
௡
௜ୀ଴
 
which was to be shown. 
 
PROOF OF THEOREM 8: From Theorem 2 we have the general rule: 
𝔼(𝐾௡ିℓ
௥
) =
෍
𝑆(𝑟, 𝑖)
୫୧୬(௥,௡ିℓ)
௜ୀ଴
= ෍𝑆(𝑟, 𝑖) ∙𝕀(𝑖≤𝑛−ℓ)
௥
௜ୀ଴
             
                                           = ෍𝑆(𝑟, 𝑖) ∙𝕀(ℓ≤𝑛−𝑖)
௥
௜ୀ଴
, 
and for all 𝑟> 0 we have 𝑆(𝑟, 0) = 0 so we can then remove the first index in the sum to get: 
𝔼(𝐾௡ିℓ
௥
) = ෍𝑆(𝑟, 𝑖) ∙𝕀(ℓ≤𝑛−𝑖)
௥
௜ୀଵ
for  𝑟> 0. 
Applying this rule for 𝑟= 1, … ,4 gives the specific results: 
𝔼(𝐾௡ିℓ) = 𝕀(ℓ< 𝑛),
𝔼(𝐾௡ିℓ
ଶ
) = 𝕀(ℓ< 𝑛) + 𝕀(ℓ< 𝑛−1),
𝔼(𝐾௡ିℓ
ଷ
) = 𝕀(ℓ< 𝑛) + 3 ∙𝕀(ℓ< 𝑛−1) + 𝕀(ℓ< 𝑛−2),
𝔼(𝐾௡ିℓ
ସ
) = 𝕀(ℓ< 𝑛) + 7 ∙𝕀(ℓ< 𝑛−1) + 6 ∙𝕀(ℓ< 𝑛−2) + 𝕀(ℓ< 𝑛−3).
 
We will first use these results to obtain the first four raw moments.  Using the law of iterated 
expectation, we have: 
𝔼(𝐾௡
∗) = 𝔼(𝐿+ 𝐾௡ି௅) = 𝔼(𝔼(𝐿+ 𝐾௡ି௅|𝐿))                                      
                                      = ෍𝔼(𝐿+ 𝐾௡ି௅|𝐿= ℓ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                         = ෍𝔼(ℓ+ 𝐾௡ିℓ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                             = ෍(ℓ+ 𝕀(ℓ< 𝑛)) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
= 1 + 𝑛𝜃−𝜃௡,      
𝔼(𝐾௡
∗ଶ) = 𝔼((𝐿+ 𝐾௡ି௅)ଶ) = 𝔼(𝔼((𝐿+ 𝐾௡ି௅)ଶ|𝐿))                                        
                                            = ෍𝔼((𝐿+ 𝐾௡ି௅)ଶ|𝐿= ℓ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 

Page 37 of 43 
                               = ෍𝔼((ℓ+ 𝐾௡ିℓ)ଶ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                                               = ෍𝔼(ℓଶ+ 2ℓ𝐾௡ିℓ+ 𝐾௡ିℓ
ଶ
) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                                                         = ෍൬
ℓଶ+ 2ℓ𝕀(ℓ< 𝑛)
+𝕀(ℓ< 𝑛) + 𝕀(ℓ< 𝑛−1)൰∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                                                 = ൤
𝑛𝜃(1 −𝜃+ 𝑛𝜃) + (2𝑛𝜃−2𝑛𝜃௡)
+(1 −𝜃௡) + (1 −𝑛(1 −𝜃)𝜃௡ିଵ−𝜃௡)൨ 
                                                          = 2 + 3𝑛𝜃+ 𝑛(𝑛−1)𝜃ଶ−𝑛𝜃௡ିଵ−(𝑛+ 2)𝜃௡. 
𝔼(𝐾௡
∗ଷ) = 𝔼((𝐿+ 𝐾௡ି௅)ଷ) = 𝔼(𝔼((𝐿+ 𝐾௡ି௅)ଷ|𝐿))                                        
                                            = ෍𝔼((𝐿+ 𝐾௡ି௅)ଷ|𝐿= ℓ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                               = ෍𝔼((ℓ+ 𝐾௡ି௅)ଷ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                                                                    = ෍𝔼(ℓଷ+ 3ℓଶ𝐾௡ିℓ+ 3ℓ𝐾௡ିℓ
ଶ
+ 𝐾௡ିℓ
ଷ
) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
                    = ෍ቌ
ℓଷ+ 3ℓଶ𝕀(ℓ< 𝑛)
+3ℓ𝕀(ℓ< 𝑛) + 3ℓ𝕀(ℓ< 𝑛−1)
+𝕀(ℓ< 𝑛) + 3𝕀(ℓ< 𝑛−1) + 𝕀(ℓ< 𝑛−2)
ቍ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
= ෍൮
ℓଷ+ 3ℓଶ+ 6ℓ+ 5
−(3𝑛ଶ+ 6𝑛+ 5)𝕀(ℓ= 𝑛)
−(3𝑛+ 1)𝕀(ℓ= 𝑛−1)
−𝕀(ℓ= 𝑛−2)
൲∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
              
=
⎣
⎢
⎢
⎢
⎢
⎢
⎡𝑛𝜃(1 −3𝜃+ 3𝑛𝜃+ 2𝜃ଶ−3𝑛𝜃ଶ+ 𝑛ଶ𝜃ଶ)
+3𝑛𝜃(1 −𝜃+ 𝑛𝜃) + 6𝑛𝜃+ 5
−(3𝑛ଶ+ 6𝑛+ 5)𝜃௡
−(3𝑛+ 1)𝑛(1 −𝜃)𝜃௡ିଵ
−𝑛(𝑛−1)
2
(1 −𝜃)ଶ𝜃௡ିଶ
⎦
⎥
⎥
⎥
⎥
⎥
⎤
                   
                           = ቎
5 + 𝑛𝜃(10 −6𝜃+ 6𝑛𝜃+ 2𝜃ଶ−3𝑛𝜃ଶ+ 𝑛ଶ𝜃ଶ)
−𝑛(𝑛−1)
2
𝜃௡ିଶ−2𝑛(𝑛+ 1)𝜃௡ିଵ−ቆ5(𝑛+ 1) + 𝑛(𝑛−1)
2
ቇ𝜃௡቏, 
𝔼(𝐾௡
∗ସ) = 𝔼((𝐿+ 𝐾௡ି௅)ସ) = 𝔼(𝔼((𝐿+ 𝐾௡ି௅)ସ|𝐿))                                        
                                            = ෍𝔼((𝐿+ 𝐾௡ି௅)ସ|𝐿= ℓ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 

Page 38 of 43 
                               = ෍𝔼((ℓ+ 𝐾௡ି௅)ସ) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
= ෍𝔼(ℓସ+ 4ℓଷ𝐾௡ିℓ+ 6ℓଶ𝐾௡ିℓ
ଶ
+ 4ℓ𝐾௡ିℓ
ଷ
+ 𝐾௡ିℓ
ସ
) ∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
              
  = ෍
⎝
⎜
⎛
ℓସ+ 4ℓଷ𝕀(ℓ< 𝑛)
+6ℓଶ𝕀(ℓ< 𝑛) + 6ℓଶ𝕀(ℓ< 𝑛−1)
+4ℓ𝕀(ℓ< 𝑛) + 12ℓ𝕀(ℓ< 𝑛−1) + 4ℓ𝕀(ℓ< 𝑛−2)
𝕀(ℓ< 𝑛) + 7 ∙𝕀(ℓ< 𝑛−1)
+6 ∙𝕀(ℓ< 𝑛−2) + 𝕀(ℓ< 𝑛−3)
⎠
⎟
⎞∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
 
= ෍
⎝
⎜
⎛
ℓସ+ 4ℓଷ+ 12ℓଶ+ 20ℓ+ 15
−(4𝑛ଷ+ 12𝑛ଶ+ 20𝑛+ 15)𝕀(ℓ= 𝑛)
−(6𝑛ଶ+ 4𝑛+ 4)𝕀(ℓ= 𝑛−1)
−(4𝑛−1)𝕀(ℓ= 𝑛−2)
−𝕀(ℓ= 𝑛−3)
⎠
⎟
⎞∙Bin(ℓ|𝑛, 𝜃)
௡
ℓୀ଴
                           
=
⎣
⎢
⎢
⎢
⎢
⎢
⎢
⎡
𝑛𝜃൬
1 −7𝜃+ 7𝑛𝜃+ 12𝜃ଶ−18𝑛𝜃ଶ
+6𝑛ଶ𝜃ଶ−6𝜃ଷ+ 11𝑛𝜃ଷ−6𝑛ଶ𝜃ଷ+ 𝑛ଷ𝜃ଷ൰
+4𝑛𝜃(1 −3𝜃+ 3𝑛𝜃+ 2𝜃ଶ−3𝑛𝜃ଶ+ 𝑛ଶ𝜃ଶ)
+12𝑛𝜃(1 −𝜃+ 𝑛𝜃) + 20𝑛𝜃+ 15
−(4𝑛ଷ+ 12𝑛ଶ+ 20𝑛+ 15)𝜃௡−(6𝑛ଶ+ 4𝑛+ 4)𝑛(1 −𝜃)𝜃௡ିଵ
−(4𝑛−1) 𝑛(𝑛−1)
2
(1 −𝜃)ଶ𝜃௡ିଶ−𝑛(𝑛−1)(𝑛−2)
6
(1 −𝜃)ଷ𝜃௡ିଷ
⎦
⎥
⎥
⎥
⎥
⎥
⎥
⎤
    
=
⎣
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎡15 + 𝑛𝜃൬
37 −31𝜃+ 31𝑛𝜃+ 20𝜃ଶ−30𝑛𝜃ଶ
+10𝑛ଶ𝜃ଶ−6𝜃ଷ+ 11𝑛𝜃ଷ−6𝑛ଶ𝜃ଷ+ 𝑛ଷ𝜃ଷ൰
−(4𝑛ଷ+ 12𝑛ଶ+ 20𝑛+ 15)𝜃௡
−(6𝑛ଶ+ 4𝑛+ 4)𝑛(1 −𝜃)𝜃௡ିଵ
−(4𝑛−1) 𝑛(𝑛−1)
2
(1 −𝜃)ଶ𝜃௡ିଶ
−𝑛(𝑛−1)(𝑛−2)
6
(1 −𝜃)ଷ𝜃௡ିଷ
⎦
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎤
                              
         =
⎣
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎡
15 + 𝑛𝜃൬
37 −31𝜃+ 31𝑛𝜃+ 20𝜃ଶ−30𝑛𝜃ଶ
+10𝑛ଶ𝜃ଶ−6𝜃ଷ+ 11𝑛𝜃ଷ−6𝑛ଶ𝜃ଷ+ 𝑛ଷ𝜃ଷ൰
−𝑛(𝑛−1)(𝑛−2)
6
𝜃௡ିଷ
+ 𝑛(𝑛−1)(𝑛−2)
2
𝜃௡ିଶ−(4𝑛−1) 𝑛(𝑛−1)
2
𝜃௡ିଶ
+(4𝑛−1)𝑛(𝑛−1)𝜃௡ିଵ−(6𝑛ଶ+ 4𝑛+ 4)𝑛𝜃௡ିଵ−𝑛(𝑛−1)(𝑛−2)
2
𝜃௡ିଵ
+(6𝑛ଶ+ 4𝑛+ 4)𝑛𝜃௡−(4𝑛ଷ+ 12𝑛ଶ+ 20𝑛+ 15)𝜃௡
+ 𝑛(𝑛−1)(𝑛−2)
6
𝜃௡−(4𝑛−1) 𝑛(𝑛−1)
2
𝜃௡
⎦
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎤
 

Page 39 of 43 
       =
⎣
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎡15 + 𝑛𝜃൬
37 −31𝜃+ 31𝑛𝜃+ 20𝜃ଶ−30𝑛𝜃ଶ
+10𝑛ଶ𝜃ଶ−6𝜃ଷ+ 11𝑛𝜃ଷ−6𝑛ଶ𝜃ଷ+ 𝑛ଷ𝜃ଷ൰
−𝑛(𝑛−1)(𝑛−2)
6
𝜃௡ିଷ
−𝑛(𝑛−1)(3𝑛+ 1)
2
𝜃௡ିଶ
−𝑛(5𝑛ଶ+ 15𝑛+ 8)
2
𝜃௡ିଵ
+ 𝑛ଷ−36𝑛ଶ−97𝑛−90
6
𝜃௡
⎦
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎤
.                                    
From these raw moments we can obtain the relevant central moments, which give the moments 
in the theorem.  With a bit of algebra it can be shown that: 
𝔼(𝐾௡
∗)ଶ= ൬
1 + 2𝑛𝜃+ 𝑛ଶ𝜃ଶ
−2𝜃௡−2𝑛𝜃௡ାଵ+ 𝜃ଶ௡൰,                                      
𝔼(𝐾௡
∗)ଷ= ൭
1 + 3𝑛𝜃+ 3𝑛ଶ𝜃ଶ+ 𝑛ଷ𝜃ଷ
−3𝜃௡−6𝑛𝜃௡ାଵ−3𝑛ଶ𝜃௡ାଶ
+3𝜃ଶ௡+ 3𝑛𝜃ଶ௡ାଵ−𝜃ଷ௡
൱,                            
𝔼(𝐾௡
∗)ସ= ൮
1 + 4𝑛𝜃+ 6𝑛ଶ𝜃ଶ+ 4𝑛ଷ𝜃ଷ+ 𝑛ସ𝜃ସ
−4𝜃௡−12𝑛𝜃௡ାଵ−12𝑛ଶ𝜃௡ାଶ−4𝑛ଷ𝜃௡ାଷ
+6𝜃ଶ௡+ 12𝑛𝜃ଶ௡ାଵ+ 6𝑛ଶ𝜃ଶ௡ାଶ
−4𝜃ଷ௡−4𝑛𝜃ଷ௡ାଵ+ 𝜃ସ௡
൲.
 
The variance of the distribution is: 
𝕍(𝐾௡
∗) = 𝔼(𝐾௡
∗ଶ) −𝔼(𝐾௡
∗)ଶ                                                                                                             
= (2 + 3𝑛𝜃+ 𝑛(𝑛−1)𝜃ଶ−𝑛𝜃௡ିଵ−(𝑛+ 2)𝜃௡) −(1 + 𝑛𝜃−𝜃௡)ଶ     
              = (2 + 3𝑛𝜃+ 𝑛(𝑛−1)𝜃ଶ−𝑛𝜃௡ିଵ−(𝑛+ 2)𝜃௡) −൬
1 + 2𝑛𝜃+ 𝑛ଶ𝜃ଶ
−2𝜃௡−2𝑛𝜃௡ାଵ+ 𝜃ଶ௡൰ 
= 1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ).                                              
With a substantial amount of algebra we can establish the third and fourth central moments: 
𝔼((𝐾௡
∗−𝔼(𝐾௡
∗))ଷ) = 𝔼(𝐾௡
∗ଷ) −3𝔼(𝐾௡
∗ଶ)𝔼(𝐾௡
∗) + 2𝔼(𝐾௡
∗)ଷ                                                      
                                       = ቎
5 + 𝑛𝜃(10 −6𝜃+ 6𝑛𝜃+ 2𝜃ଶ−3𝑛𝜃ଶ+ 𝑛ଶ𝜃ଶ)
−𝑛(𝑛−1)
2
𝜃௡ିଶ−2𝑛(𝑛+ 1)𝜃௡ିଵ−ቆ5(𝑛+ 1) + 𝑛(𝑛−1)
2
ቇ𝜃௡቏ 
                                         −3(2 + 3𝑛𝜃+ 𝑛(𝑛−1)𝜃ଶ−𝑛𝜃௡ିଵ−(𝑛+ 2)𝜃௡)(1 + 𝑛𝜃−𝜃௡) 
+2(1 + 𝑛𝜃−𝜃௡)ଷ                                               

Page 40 of 43 
                           =
⎝
⎜⎜⎜
⎛
1 + 𝑛𝜃(1 −3𝜃+ 2𝜃ଶ)
−𝑛(𝑛−1)
2
𝜃௡ିଶ−𝑛(2𝑛−1)𝜃௡ିଵ
+ 5𝑛ଶ−3𝑛+ 2
2
𝜃௡+ 3𝑛(𝑛+ 1)𝜃௡ାଵ−3𝑛(𝑛+ 1)𝜃௡ାଶ
−3𝑛𝜃ଶ௡ିଵ−3𝑛𝜃ଶ௡+ 6𝑛𝜃ଶ௡ାଵ−2𝜃ଷ௡
⎠
⎟⎟⎟
⎞
, 
𝔼((𝐾௡
∗−𝔼(𝐾௡
∗))ସ) = 𝔼(𝐾௡
∗ସ) −4𝔼(𝐾௡
∗ଷ)𝔼(𝐾௡
∗) + 6𝔼(𝐾௡
∗ଶ)𝔼(𝐾௡
∗)ଶ−3𝔼(𝐾௡
∗)ସ                   
                    =
⎣
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎡15 + 𝑛𝜃൬
37 −31𝜃+ 31𝑛𝜃+ 20𝜃ଶ−30𝑛𝜃ଶ
+10𝑛ଶ𝜃ଶ−6𝜃ଷ+ 11𝑛𝜃ଷ−6𝑛ଶ𝜃ଷ+ 𝑛ଷ𝜃ଷ൰
−𝑛(𝑛−1)(𝑛−2)
6
𝜃௡ିଷ
−𝑛(𝑛−1)(3𝑛+ 1)
2
𝜃௡ିଶ
−𝑛(5𝑛ଶ+ 15𝑛+ 8)
2
𝜃௡ିଵ
+ 𝑛ଷ−36𝑛ଶ−97𝑛−90
6
𝜃௡
⎦
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎤
 
                                           −4
⎣
⎢
⎢
⎢
⎢
⎡5 + 𝑛𝜃(10 −6𝜃+ 6𝑛𝜃+ 2𝜃ଶ−3𝑛𝜃ଶ+ 𝑛ଶ𝜃ଶ)
−𝑛(𝑛−1)
2
𝜃௡ିଶ−2𝑛(𝑛+ 1)𝜃௡ିଵ
−ቆ5(𝑛+ 1) + 𝑛(𝑛−1)
2
ቇ𝜃௡
⎦
⎥
⎥
⎥
⎥
⎤
(1 + 𝑛𝜃−𝜃௡) 
                                     +6 ൬2 + 3𝑛𝜃+ 𝑛(𝑛−1)𝜃ଶ
−𝑛𝜃௡ିଵ−(𝑛+ 2)𝜃௡൰(1 + 𝑛𝜃−𝜃௡)ଶ−3(1 + 𝑛𝜃−𝜃௡)ସ 
                               = 1
6
⎝
⎜
⎜
⎜
⎜
⎜
⎜
⎛
24 + 42𝑛𝜃+ 6𝑛(3𝑛−13)𝜃ଶ
−36𝑛(𝑛−2)𝜃ଷ+ 18𝑛(𝑛−2)𝜃ସ
−𝑛(𝑛ଶ−3𝑛+ 2)𝜃௡ିଷ−9𝑛(𝑛−1)ଶ𝜃௡ିଶ
−3𝑛(𝑛ଶ+ 3𝑛+ 4)𝜃௡ିଵ−(49𝑛ଷ−48𝑛ଶ−25𝑛+ 6)𝜃௡
−12𝑛(2𝑛ଶ−3𝑛−6)𝜃௡ାଵ−36𝑛(𝑛ଶ+ 2)𝜃௡ାଶ
+24𝑛(𝑛ଶ+ 2)𝜃௡ାଷ−12𝑛(𝑛−1)𝜃ଶ௡ିଶ
−24𝑛(2𝑛−1)𝜃ଶ௡ିଵ−(60𝑛ଶ−36𝑛−12)𝜃ଶ௡
+36𝑛(2𝑛+ 1)𝜃ଶ௡ାଵ−36𝑛(2𝑛+ 1)𝜃ଶ௡ାଶ
+36𝑛𝜃ଷ௡ିଵ−36𝑛𝜃ଷ௡+ 72𝑛𝜃ଷ௡ାଵ−18𝜃ସ௡
⎠
⎟
⎟
⎟
⎟
⎟
⎟
⎞
. 
(For brevity, we have omitted a substantial number of algebraic steps in this computation.  The 
reader is invited to undertake the required algebra if they wish to do so.  It is not particularly 
illuminating work.)  We then have: 
𝕊𝕜𝕖𝕨(𝐾௡
∗) =  ⎝
⎜⎜
⎛
1 + 𝑛𝜃(1 −3𝜃+ 2𝜃ଶ)
−𝑛(𝑛−1)
2
𝜃௡ିଶ−𝑛(2𝑛−1)𝜃௡ିଵ
+ 5𝑛ଶ−3𝑛+ 2
2
𝜃௡+ 3𝑛(𝑛+ 1)𝜃௡ାଵ−3𝑛(𝑛+ 1)𝜃௡ାଶ
−3𝑛𝜃ଶ௡ିଵ−3𝑛𝜃ଶ௡+ 6𝑛𝜃ଶ௡ାଵ−2𝜃ଷ௡
⎠
⎟⎟
⎞
[1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ)]ଷଶ
⁄
,  

Page 41 of 43 
𝕂𝕦𝕣𝕥(𝐾௡
∗) = 1
6 ∙⎝
⎜
⎜
⎜
⎜
⎜
⎜
⎛
24 + 42𝑛𝜃+ 6𝑛(3𝑛−13)𝜃ଶ
−36𝑛(𝑛−2)𝜃ଷ+ 18𝑛(𝑛−2)𝜃ସ
−𝑛(𝑛−1)(𝑛−2)𝜃௡ିଷ−9𝑛(𝑛−1)ଶ𝜃௡ିଶ
−3𝑛(𝑛ଶ+ 3𝑛+ 4)𝜃௡ିଵ−(49𝑛ଷ−48𝑛ଶ−25𝑛+ 6)𝜃௡
−12𝑛(2𝑛ଶ−3𝑛−6)𝜃௡ାଵ−36𝑛(𝑛ଶ+ 2)𝜃௡ାଶ
+24𝑛(𝑛ଶ+ 2)𝜃௡ାଷ−12𝑛(𝑛−1)𝜃ଶ௡ିଶ
−24𝑛(2𝑛−1)𝜃ଶ௡ିଵ−(60𝑛ଶ−36𝑛−12)𝜃ଶ௡
+36𝑛(2𝑛+ 1)𝜃ଶ௡ାଵ−36𝑛(2𝑛+ 1)𝜃ଶ௡ାଶ
+36𝑛𝜃ଷ௡ିଵ−36𝑛𝜃ଷ௡+ 72𝑛𝜃ଷ௡ାଵ−18𝜃ସ௡
⎠
⎟
⎟
⎟
⎟
⎟
⎟
⎞
[1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ)]ଶ
 
                  = 3 + ⎝
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎛
1 + 𝑛𝜃−7𝑛𝜃ଶ+ 12𝑛𝜃ଷ−6𝑛𝜃ସ
−𝑛(𝑛−1)(𝑛−2)
6
𝜃௡ିଷ−3𝑛(𝑛−1)ଶ
2
𝜃௡ିଶ
−𝑛(𝑛ଶ+ 3𝑛−8)
2
𝜃௡ିଵ−49𝑛ଷ−84𝑛ଶ−61𝑛+ 6
6
𝜃௡
−2𝑛(2𝑛ଶ−3𝑛)𝜃௡ାଵ−6𝑛(𝑛ଶ+ 3𝑛+ 2)𝜃௡ାଶ
+4𝑛(𝑛+ 1)(𝑛+ 2)𝜃௡ାଷ−𝑛(5𝑛−2)𝜃ଶ௡ିଶ
−2𝑛(7𝑛−2)𝜃ଶ௡ିଵ−(𝑛ଶ−12𝑛−2)𝜃ଶ௡
+12𝑛(2𝑛+ 1)𝜃ଶ௡ାଵ−12𝑛(2𝑛+ 1)𝜃ଶ௡ାଶ
−12𝑛𝜃ଷ௡+ 24𝑛𝜃ଷ௡ାଵ−6𝜃ସ௡
⎠
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎞
[1 −𝜃ଶ௡+ 𝑛(𝜃−𝜃ଶ−𝜃௡ିଵ−𝜃௡+ 2𝜃௡ାଵ)]ଶ
. 
This establishes the moment results in the theorem.  The asymptotic results are obtained by 
removing all higher-order terms of the form 𝜃௡ା௞ and then simplifying the expressions with 
additional algebra.  ■ 
 
PROOF OF THEOREM 9: Using the asymptotic equivalence results in Theorem 7 we have: 
𝐾௡
∗−𝔼(𝐾௡
∗)
ඥ𝕍(𝐾௡∗)
 ~ 1
√𝑛∙𝐾௡
∗−1 −𝑛𝜃
ඥ𝜃(1 −𝜃)
= √𝑛∙𝐾௡
∗𝑛
⁄
−1 𝑛
⁄
−𝜃
ඥ𝜃(1 −𝜃)
. 
Since 𝐾௡
∗𝑛
⁄
= 𝐿𝑛
⁄
+ 𝐾௡ି௅𝑛
⁄  and 𝐾௡ି௅𝑛
⁄
→1 𝑛
⁄
→0 in probability, we can apply Slutsky’s 
theorem to obtain the asymptotic equivalence 𝐾௡
∗𝑛
⁄  ~ (𝐿+ 1) 𝑛
⁄ .  (Indeed, the asymptotic 
mean and variance shown in Theorem 8 matches the mean and variance of 𝐿+ 1.)  Since 𝐿 is 
a binomial random variable it is composed of IID Bernoulli values and the classical central 
limit theorem applies to 𝐿𝑛
⁄ .  Combining this asymptotic result with Slutsky’s theorem gives 
the result to be shown.  ■ 
 
PROOF OF THEOREM 10: The log-likelihood function in the theorem follows immediately by 
taking the logarithm of the mass function for the generalised matching distribution.  To obtain 
the score function and information function we will use the first and second-order logarithmic 
derivatives, which are: 

Page 42 of 43 
𝑑
𝑑𝜃log 𝑝(𝜃) = 𝑝′(𝜃)
𝑝(𝜃) ,                                 
𝑑ଶ
𝑑𝜃ଶlog 𝑝(𝜃) = 𝑝′′(𝜃)
𝑝(𝜃) −൬𝑑
𝑑𝜃log 𝑝(𝜃)൰
ଶ
.
 
With some algebraic manipulation, we can establish that: 
𝑑
𝑑𝜃Bin(ℓ|𝑛, 𝜃) =
𝑛!
ℓ! (𝑛−ℓ)!
𝑑
𝑑𝜃𝜃ℓ(1 −𝜃)௡ିℓ                                                       
                            =
𝑛!
ℓ! (𝑛−ℓ)! ൣℓ𝜃ℓିଵ(1 −𝜃)௡ିℓ−(𝑛−ℓ)𝜃ℓ(1 −𝜃)௡ିℓିଵ൧ 
=
𝑛!
ℓ! (𝑛−ℓ)! 𝜃ℓ(1 −𝜃)௡ିℓ൬ℓ
𝜃−𝑛−ℓ
1 −𝜃൰        
           =
𝑛!
ℓ! (𝑛−ℓ)! 𝜃ℓ(1 −𝜃)௡ିℓ∙ℓ(1 −𝜃) −(𝑛−ℓ)𝜃
𝜃(1 −𝜃)
 
=
𝑛!
ℓ! (𝑛−ℓ)! 𝜃ℓ(1 −𝜃)௡ିℓ∙ℓ−𝑛𝜃
𝜃(1 −𝜃)            
= [ℓ−𝑛𝜃] ∙Bin(ℓ|𝑛, 𝜃)
𝜃(1 −𝜃) ,                                    
𝑑ଶ
𝑑𝜃ଶBin(ℓ|𝑛, 𝜃) = 𝑑
𝑑𝜃൬Bin(ℓ|𝑛, 𝜃) ∙ℓ−𝑛𝜃
𝜃(1 −𝜃)൰                                                         
                       = ℓ−𝑛𝜃
𝜃(1 −𝜃) ∙𝑑
𝑑𝜃Bin(ℓ|𝑛, 𝜃) + Bin(ℓ|𝑛, 𝜃) ∙𝑑
𝑑𝜃
ℓ−𝑛𝜃
𝜃(1 −𝜃) 
= ℓ−𝑛𝜃
𝜃(1 −𝜃) ∙𝑑
𝑑𝜃Bin(ℓ|𝑛, 𝜃)                               
                                        + Bin(ℓ|𝑛, 𝜃) ∙−𝑛𝜃(1 −𝜃) −(ℓ−𝑛𝜃)(1 −𝜃) + (ℓ−𝑛𝜃)𝜃
𝜃ଶ(1 −𝜃)ଶ
 
                            = ℓ−𝑛𝜃
𝜃(1 −𝜃) ∙𝑑
𝑑𝜃Bin(ℓ|𝑛, 𝜃) + Bin(ℓ|𝑛, 𝜃) ∙2ℓ𝜃−ℓ−𝑛𝜃ଶ
𝜃ଶ(1 −𝜃)ଶ
 
    = [(ℓ−𝑛𝜃)ଶ+ (2ℓ𝜃−ℓ−𝑛𝜃ଶ)] Bin(ℓ|𝑛, 𝜃)
𝜃ଶ(1 −𝜃)ଶ 
                       = [(ℓଶ−2𝑛ℓ𝜃+ 𝑛ଶ𝜃ଶ) + (2ℓ𝜃−ℓ−𝑛𝜃ଶ)] Bin(ℓ|𝑛, 𝜃)
𝜃ଶ(1 −𝜃)ଶ 
                    = [ℓ(ℓ−1) −2ℓ(𝑛−1)𝜃+ 𝑛(𝑛−1)𝜃ଶ] Bin(ℓ|𝑛, 𝜃)
𝜃ଶ(1 −𝜃)ଶ. 
Substituting these derivatives into the forms for the logarithmic derivatives gives the stated 
score function and Hessian function.  The corollary is easily obtained using the transformation: 

Page 43 of 43 
𝜃=
𝑒ଶథ
𝑒ଶథ+ 1
𝜙= −1
2 ∙log ൬1 −𝜃
𝜃
൰, 
with corresponding derivatives (written in terms of 𝜃) given by: 
𝑑𝜃
𝑑𝜙= 2𝜃(1 −𝜃)
𝑑ଶ𝜃
𝑑𝜙ଶ= 8(½ −𝜃)𝜃(1 −𝜃),
𝑑𝜙
𝑑𝜃=
1
2𝜃(1 −𝜃)
𝑑ଶ𝜙
𝑑𝜃ଶ= −
½ −𝜃
𝜃ଶ(1 −𝜃)ଶ.         
 
Using the standard rules for transformations involving the gradient vector and Hessian matrix 
leads to the results stated in the corollary.  ■ 
 
 

