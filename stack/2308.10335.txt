A Study on Robustness and Reliability of Large Language Model Code Generation
Li Zhong, Zilong Wang
University of California, San Diego
Abstract
Recently, the large language models (LLMs) have shown
extraordinary ability in understanding natural language and
generating programming code. It has been a common prac-
tice of software engineers to consult LLMs when encoun-
tering coding questions. Although efforts have been made to
avoid syntax errors and align the code with the intended se-
mantics, the reliability and robustness of the code generation
from LLMs have not yet been thoroughly studied. The exe-
cutable code is not equivalent to the reliable and robust code,
especially in the context of real-world software development.
The misuse of APIs in the generated code could lead to se-
vere problem, such as resource leaks, program crashes, etc.
To make things worse, the users of LLM code generation ser-
vices are actually the developers that are most vulnerable to
these code that seems right – They are always novice devel-
opers that are not familiar with the APIs that LLMs generate
code for them. Therefore, they could hardly tell the misuse
in the code generated by LLMs, which further facilitates the
incorrect code applied in real-world software. Existing code
evaluation benchmark and datasets focus on crafting small
tasks such as programming questions in coding interviews,
which however deviates from the problem that developers
would ask LLM for real-world coding help. To fill the miss-
ing piece, in this work, we propose a dataset ROBUSTAPI
for evaluating the reliability and robustness of code gener-
ated by LLMs. We collect 1208 coding questions from Stack
Overflow on 24 representative Java APIs. We summarize the
common misuse patterns of these APIs and evaluate them on
current popular LLMs. The evaluation results show that even
for GPT-4, 62% of the generated code contains API misuses,
which would cause unexpected consequences if the code is
introduced into real-world software.
Introduction
The new era of language modeling arrives when large lan-
guage models (LLMs) show an extraordinary ability in un-
derstanding natural language and can even generate cus-
tomized code according to the user’s needs (Ye et al. 2023;
OpenAI 2023a; Anil et al. 2023). It is not surprising that
more and more software engineers choose to query large lan-
guage models for the answer to the coding questions, such
as generating a code snippet of using APIs or detecting bugs
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
in a few lines of code. Large language models are able to re-
trieve more suitable and customized answer for the question
compared with searching in the online programming forums,
such as Stack Overflow1.
Such fast pace conceals potential risks in the code genera-
tion of large language models. From the perspective of soft-
ware engineering, the robustness and reliability of generated
code have not yet been thoroughly studied even if numerous
works have been made to avoid syntax errors and improve
semantic understanding in the generated code (?Chen et al.
2021; Shen et al. 2023a; Luo et al. 2023). Unlike the on-
line programming forums, the generated code snippets are
not reviewed by the community peers and thus suffer from
API misuse, such as missing boundary checking in file read-
ing and variable indexing, missing file I/O closing, failure in
transaction completion, etc. Even if the code samples are
executable or functionally correct, the misuse can trigger
serious potential risks in products, such as memory leaks,
program crashes, garbage collection failures, etc. To make
things worse, the programmers asking these questions are
the most vulnerable ones to the risk since they are more
likely to be novice to the APIs and cannot tell the violations
in the generated code snippets. Therefore, it is essential to
contemplate the code reliability while evaluating the code
generation by large language models.
To evaluate the code generation of large language mod-
els, most of the existing benchmarks focus on the functional
correctness of the execution result from the generated code,
which means the code is acceptable as long as it is func-
tional for the user’s purpose (Chen et al. 2021; Yin et al.
2018; Lu et al. 2021). We argue that the correct execution
result is important but it is not always the case in the soft-
ware development scenario. What the engineers really need
is a reliable code sample of the new APIs without potential
risks in the long run. Moreover, the domain of most current
programming questions are far from software engineering.
The data source is mostly online coding challenge websites,
such as Codeforces, Kattis, Leetcode, etc (Hendrycks et al.
2021; Austin et al. 2021). Although remarkable progresses
have been made, we argue that they fail to substantially help
the software development in practical scenarios.
To this end, we propose ROBUSTAPI, an comprehensive
1https://stackoverflow.com
arXiv:2308.10335v2  [cs.CL]  27 Aug 2023

How can I create a file with Java?
I want to create a file through Java. What 
functions shall I use?
LLaMA2
Vicuna
Ask LLMs 
for help
File file = new File(filePath);
try {
  file.createNewFile(); 
} catch (IOException e){ 
  e.printStackTrace();
}
static void CreateNewFile(String filePath) {
  File file = new File(filePath);
  if (!file.exists()) { file.createNewFile(); }
}
LLM-Generated Code Snippet (Llama 2)
Correct API Usage
createNewFile 
Requires catching IO 
exceptions when the file 
already exists or the parent 
folder doesn’t exist.
Syntax Correct ✓
Function Correct ✓
Semantic Aligned ✓
Reliable & Robust ✗
Figure 1: The scenario where software engineers consult large language models for the answer to the programming questions. The generated
code snippet is not reliable and there exist potential risks in the software development.
benchmark to evaluate the reliability and robustness of code
generated by large language models, including a dataset of
coding questions and an evaluator using the abstract syntax
tree (AST) (Fischer, Lusiardi, and Von Gudenberg 2007). In
the dataset, we target at creating an evaluation setting which
is close to the real software development. Thus we collect
representative questions about Java from Stack Overflow.
Java is one of the most popular programming languages and
widely used in software development because of its write
once, run anywhere (WORA) feature2. For each question, we
provide the detailed description and the related Java API. We
design templates to trigger large language models to gen-
erate the code snippet and the corresponding explanation.
We also provide an evaluator which analyzes the generated
code snippets using the abstract syntax tree (AST) and com-
pares them with the expected API usage patterns. Following
Zhang et al. (2018), we formalize the API usage patterns into
structured call sequences, as shown in FIGURE. The struc-
tured call sequences present how these APIs can be properly
used to eliminate the potential system risks. Any violations
of such structured call sequences would be considered as
failure from the perspective of software engineering.
We collect 1208 real questions from Stack Overflow
which involves 24 representative Java APIs. We run ex-
periments on the close-sourced language models (GPT-3.5
and GPT-4 (OpenAI 2023a)) as well as the open-sourced
language models (Llama-2 (Touvron et al. 2023), Vicuna-
1.5 (Chiang et al. 2023). We use the default hyper-parameter
settings of the models without further extensive hyper-
parameter tuning. We further design two experiment set-
tings, zero-shot and one-shot, where none or one demonstra-
tion sample is provided in the prompt. We conduct compre-
hensive analysis of the generated code and study the com-
mon API misuse cases of current large language models. We
would like to bring up the important issues of the API mis-
use in the code generation by large language models, and
provide a new dimension to evaluate large language mod-
els other than the commonly-used functional correctness.
The main purpose of this benchmark is not to evaluate the
functional correctness of the generated code, but instead, we
focus on the reliability and robustness. We hope this work
2https://en.wikipedia.org/wiki/Java (programming language)
could facilitate future research on this topic and help create
a more robust coding helper out of large language models
to step further into the real artificial general intelligence. We
will open-source our dataset and evaluator on GitHub. We
summarize our contribution as follows.
• We propose a new benchmark, ROBUSTAPI, to evalu-
ate the reliability and robustness of code generation by
large language models. This is an important but not yet
well studied perspective to evaluate the code quality apart
from functional correctness.
• We provide a well formalized evaluation framework in-
cluding a dataset of Stack Overflow questions and an API
usage checker using AST. We report performance of pop-
ular large language models, including GPT-3.5, GPT-4,
Llama-2, Vicuna-1.5.
• We conduct comprehensive analysis of the code genera-
tion performance of current large language models. We
summarize the common API misuse for each model and
point out the promising improvement direction for future
research.
Related Work
Code Quality of LLM-Sythesized Code
With the re-
lease of Copilot (Chen et al. 2021) and other commercial
code assistant tools based on LLMs, the security and code
quality of these tools gradually get the attention of research
community. Yetistiren, Ozsoy, and Tuzun (2022) assesses
the quality of LLM-generated code from the aspects of com-
pilation correctness, functional correctness and code effi-
ciency. Siddiq et al. (2022) studies code smells in code gen-
erated by LLMs, which is the poor design in code like un-
usually long method, or duplicated code. Poesia et al. (2022)
shows that LLMs can make implementation errors in the
code like syntax error or semantic errors deviating from
users’ intention. Jesse et al. (2023) studies simple, stupid
bugs in Codex and other LLMs, which shows that AI code
assistants can help avoid some of such simple bugs but have
a higher change of introducing bugs that are hard to de-
tect. As for security impact, Pearce et al. (2022) designs 89
security-sensitive scenarios for Copilot to complete the code
for users, which shows approximately 40% of the code is

vulnerable. Perry et al. (2022) conducts the first large-scale
user study to examine whether users interact with AI Code
assistant write secure code. And they find that those users
wrote significantly less secure code while they believe their
code were secure. Sandoval et al. (2023) conducts user study
to assess the security of low-level code with pointer and ar-
ray manipulations generated by AI-based coding assistants.
They find under this specific scenario, the assistants does not
introduce more security bugs than human. Liu et al. (2023)
enlarges HumanEval (Chen et al. 2021) by generating test
cases with higher coverage which serve as an add-on to the
existing programming benchmarks but the evaluation still
focuses on the functional correctness and simple program-
ming questions far from software development. Shen et al.
(2023b) evaluates the reliability of ChatGPT3 by testing on
adversarial examples, which however has a different mean-
ing of ‘reliability’ in their context. In this paper, we refer
reliability as the ability of code to be resisted to failure, high
workload, and unexpected input.
Quality Assessment of Code in Online Forum
Exist-
ing literature in software engineering field has investigated
the quality of code from online forum and warn develop-
ers of the potential issues. Yang, Hussain, and Lopes (2016)
finds that the majority of code examples given in Stack Over-
flow answers cannot be compiled. Zhou and Walker (2016)
points out that 43% of the posts investigated by them con-
tains deprecated APIs, while Fischer et al. (2017) find that
29% of the code contains security risks. In Zhang et al.
(2018), the authors analyze the code by call sequence ex-
traction and slicing, and compare to the manually validated
API usage rules, which concludes that 31% of the code ex-
amples in Stack Overflow answers contains API misuse and
could produce unexpected behaviors.
Methodology
In this section, we describe ROBUSTAPI, which is a compre-
hensive benchmark we build to thoroughly evaluate the reli-
ability and robustness of LLM-generated code. We describe
the process of data collection and prompt generation when
constructing the dataset. Then we present the API misuse
patterns evaluated in ROBUSTAPI and discuss the potential
consequence of violations. Finally, we introduce the static
analysis method in ROBUSTAPI for detecting the API us-
age violations which leverages the abstract syntax tree and
achieves higher evaluation accuracy in evaluating the API
misuse in code generated by LLMs compared to rule-based
method such as keywords matching.
Data Collection
To take advantage of the existing research efforts in the soft-
ware engineering field, we build ROBUSTAPI based on the
dataset from ExampleCheck (Zhang et al. 2018) as our start-
ing point. ExampleCheck is proposed to study the frequent
Java API misuse in online Q&A forums. We select 23 pop-
ular Java APIs from the dataset as shown in Table 1. These
23 APIs cover 5 domains including string processing, data
3https://chat.openai.com
structure, mobile development, crypto and database opera-
tion. Then we crawl questions relevant to these APIs from
Stack Overflow. We only select the questions with online
answers and we keep the questions whose provided answer
contains API misuse. In this way, we guarantee that the ques-
tions in ROBUSTAPI are answerable and non-trivial so we
can use them to effectively evaluate the LLMs’ ability in
answering coding questions that humans are prone to make
mistakes. After filtering, we get 1208 questions in total. The
distribution of questions for each domain is shown in Ta-
ble 1.
API
Domain
1
StringTokenizer.nextToken
String
Processing
(307)
2
String.getBytes
3
JsonElement.getAsString
4
List.get
Data
Structure
(404)
5
Map.get
6
SortedMap.firstKey
7
Iterator.next
8
ProgressDialog.dismiss
Mobile
Development
(75)
9
Activity.findViewById
10
TypedArray.getString
11
ApplicationInfo.loadIcon
12
Activity.setContentView
13
Cipher.init
Crypto (10)
14
Mac.doFinal
15
RandomAccessFile.read/write/close
File I/O (390)
16
BufferedReader.readLine
17
PrintWriter.read/write/close
18
DataOutputStream.read/write/close
19
File.mkdirs
20
InputStream.read
21
File.createNewFile
22
FileChannel.write
23
SQLiteDatabase.query
Database (22)
Total
(1208)
Table 1: 23 popular Java APIs in ROBUSTAPI. They are easily mis-
used by developers according to the existing literature of software
engineering (Zhang et al. 2018).
After collecting the questions, we convert them into
the JSON format with the following fields: {id,
api,
question, origin}. id field contains the unique id we
assign for each sample. api field contains the API that we
specifically instruct the large language models to use as a
question hint. question field contains the title and descrip-
tion of the Stack Overflow questions. origin field contains
the original URL of this sample.
Prompt Generation
In ROBUSTAPI, we design a prompt template and fill it
with samples from our dataset. We collect the response from
LLMs and implement an API usage checker to evaluate the
code reliability. In the prompt, we start with the task intro-
duction and the required response format. Then we append
the few-shot demonstrations when conducting experiments
in the few-shot settings. The demonstration examples sat-
isfy our provided response format. In our zero-shot setting,

we skip this step. Next, we append the question and the cor-
responding API hint from one sample in our dataset. This
prompt simulates a user asking coding questions without
providing any additional hints from the API documentation
which is a typical scenario when novice developers seek for
help from language language models. Due to the chat com-
pletion nature of state-of-the-art LLMs, we wrap the ques-
tion and answer with special tags to instruct LLMs to gen-
erate answers to the questions. Here is an example of the
prompt:
Prompt:
Please answer my code questions using the given API
following this format:
<<<api>>>: $API
<<<code>>>: $CODE
<<<explanation>>>: $EXPLANATION
[Few-shot samples if applicable...]
Question: I have just started learning java for
android. I set the TextVisibility of a textfield to
GONE, I need the TextVisibilty to change to VISIBLE
after 7 seconds from Button click.
Please using this api Activity.setContentView.
Answer:
Demonstration Samples
Demonstration samples have been proven helpful to LLMs
in understanding natural language. To thoroughly analysis
LLMs’ ability in code generation, we design two few-shot
settings, One-shot-irrelevant and One-shot-relevant.
In the one-shot-irrelevant setting, we provide LLMs with
an example using an irrelevant API (e.g. Arrays.stream).
We assume this demonstration example would eliminate the
syntax errors in the generated code. The irrelevant example
we use in ROBUSTAPI is as follows:
One-shot-irrelevant Demo Example:
Question: How can I calculate the sum of an array
in Java?
<<<api>>>: Arrays.stream
<<<code>>>: int[] array = 1, 2, 3, 4, 5;
int sum = Arrays.stream(array).sum();
<<<explanation>>>: The sum() method of the IntStream
class returns the sum of elements in this stream.
In the one-shot-relevant setting, we provide LLMs with
an example using the same API as the given question. The
provided example contains a pair of question and answer.
The question in the demo example does not present in the
testing dataset and we manually revise the answer to ensure
that there is no API misuse in it and the semantics well align
with the questions.
Java API Misuse
When using the APIs provided by language libraries, de-
velopers need to follow the API usage rules so that they
can take full advantage of the ideal API effect. Violating
these rules and misusing the APIs could result in unex-
pected behaviors in production. A typical example is the
file operation. When opening and writing to a file through
RandomAccessFile, two usage rules need to be enforced:
(1) Reading the file could throw exceptions. If the buffer
limit is reached before expected bytes are read, the API
would throw IndexOutOfBoundsException. Also, the file
is concurrently closed by other processes, the API would
throw ClosedChannelException. To deal with these excep-
tions, the correct implementation should enclose the API
inside try-catch blocks. (2) The file channel should be
closed after usage. Otherwise, if this code snippet is inside a
long-lasting program that are concurrently running in multi-
ple instances, the file resources could be run out. Therefore,
the code need invoke close API after all file operations. The
correct usage and possible misuse are shown as following:
Correct API Usage:
try {
RandomAccessFile raf =
new RandomAccessFile("/tmp/file.json", "r");
byte[] buffer = new byte[1024 * 1024];
int bytesRead = raf.read(buffer, 0, buffer.length);
raf.close();
} catch(Exception e) {...}
Violation: Unhandled Exception
RandomAccessFile raf =
new RandomAccessFile("/tmp/file.json", "r");
byte[] buffer = new byte[1024 * 1024];
int bytesRead = raf.read(buffer, 0, buffer.length);
raf.close();
Another example of API usage rules that are easily misused
is TypedArray, a special data object that requires the de-
velopers to invoke recycle() to manually enable garbage
collection. Otherwise, the garbage collection in Java virtual
machine will not be triggered even after there are not further
uses of this TypedArray. Using this API without garbage
collection will result in unreleased memory consumption,
which in production will degrade or even suspend the soft-
ware system under large workload and high concurrency.
Correct API Usage:
TypedArray array = res.obtainTypedArray(R.array);
try {
font_name = a.getString(R.styleable.font);
} finally {
a.recycle();
}
Violation: Unreleased Resource
TypedArray array = res.obtainTypedArray(R.array);
font˙name = a.getString(R.styleable.font);
In ROBUSTAPI, we summarized 40 API usage rules from
the 23 APIs (see Appendix), which are validated in the
documentations of these APIs (Zhang et al. 2018). These
rules include: (1) The guard condition of a API, which
should be checked before API calls. For example, check the
result of File.exists() before File.createNewFile()
(2) Required call sequence of a API, which should be
called in specific order. For example, call close() after
File.write(). (3) Control structures of a API. For exam-
ple, enclose SimpleDateFormat.parse() with try-catch
structure.

try {
  RandomAccessFile raf = \
    new RandomAccessFile("file.json", "r");
  byte[] bf = new byte[1024 * 1024];
  int bytes = raf.read(bf, 0, bf.length);
} catch(Exception e) {
  e.printStackTrace();
}
Code Snippet Generated by LLM
TRY
TRY-BODY
CATCH-BODY
CALL
CALL
CALL
CALL
(i) Generate AST for the Code Snippet
AST of the Given Code Snippet
(ii) Compare AST with API Usage Rules
AST Call Sequence 
API Usage Rule
(RandomAccessFile().read())
(iii) Detect Mismatched Pattern 
& Report Violation
Cannot find RandomAccessFile().close() 
in AST Call Sequence
TRY
RandomAccessFile()
END_BLOCK
RandomAccessFile().read()
CATCH
Exception.printStackTrace()
END_BLOCK
TRY
RandomAccessFile().read()
RandomAccessFile().close()
Longest Common String
END_BLOCK
CATCH
END_BLOCK
Figure 2: The Illustration of Our API Checker. The API checker uses the static analysis method and analyzes the generated code with the
abstract syntax tree (AST). The API misuse is detected when the AST call sequence and the API usage rule do not match.
Detecting API Misuse
Existing research in evaluating the code generated by LLMs
usually use test cases either written by human or by auto-
matic test generation. However, this falls short when test-
ing the reliability and robustness of code because even high-
coverage test cases only cover the semantic correctness. It is
hard to create the testing environment and unexpected input
that can evaluate the reliability and robustness of the code,
which however would happen when the code is deployed in
production. To deal with this challenging problem, we use
the method of static analysis, which analyze the code misuse
by its code structure without running the tests. This guaran-
tees a full coverage of the whole programs while provides
higher efficiency compared to testing solutions.
To evaluate the API usage correctness in code, RO-
BUSTAPI detects the API misuses against the API usage
rules by extracting call consequences and control structures
from the code snippet, as shown in Figure 2. The code
checker firstly check the code snippets to see whether it is
a snippet of a method or a method of a class, so that it can
enclose this code snippet and constructs abstract syntax tree
(AST) from the code snippet. Then the checker traverses the
AST to record all the method calls and control structures
in order, which generate a call sequence. Next, the checker
compare the call sequence against the API usage rules. It
infers the instance type of each method calls and use the
type and method as keys to retrieve corresponding API us-
age rules. Finally, the checker computes the longest common
sequence between the call sequence and the API usage rules.
If the call sequence does not match the expected API usage
rules, the checker will report API misuses.
Experimenet
Experiment Setup
In the experiments, we evaluate ROBUSTAPI on four LLMs:
GPT-3.5 (OpenAI 2023a), GPT-4 (OpenAI 2023a), Llama-
2 (Touvron et al. 2023), Vicuna-1.5 (Chiang et al. 2023).
The implementation details, including the checkpoints, the
hyper-parameter, are included in Appendix due to the space
limitation. We use the default hyper-parameter settings of
each model without further extensive hyper-parameter tun-
ing. For all models, we evaluate three experiment settings as
introduced in Methodology:
• Zero-shot: No example is provided in the prompt. The
prompt only contains the instruction, question.
• One-shot-irrelevant: ROBUSTAPI provides one exam-
ple of a irrelevant task in the prompt.
• One-shot-relevant: ROBUSTAPI provides one example
of the same API with the correct usage in the prompt.
The examples for shot generations are manually written
and double-checked by the authors. Then they are evaluated
against the API usage checkers to make sure they are aligned
with the API usage rules.
Evaluation Metrics
To quantitatively evaluate the reliability of the generated
code, we define the following values and our metrics are
computed based on them. Supposing that we have N ques-
tions in our dataset, we divide them into three groups.
• Nmisuse: The number of cases where our API usage
checker detects the API usage violations.
• Npass: The number of cases where our API usage checker
does not detect the API usage violations.
• Nnon-exec: The number of cases where the LLM fails to
generated code or the generated code is not executable.
Based on the values, we define our metrics.
• API Misuse Rate = Nmisuse/(Nmisuse + Npass): To ana-
lyze the proportion of misuse cases among the executable
code snippets. It reveals how reliable the generated code
is after the users filter out the non-executable cases.

GPT-3.5 GPT-4 Llama-2 Vicuna
GPT-3.5 GPT-4 Llama-2 Vicuna
GPT-3.5 GPT-4 Llama-2 Vicuna
0
20
40
60
80
100
API Usage Results
29.30
49.83
29.09
62.00
49.59
31.13
28.15
62.09
27.07
64.32
41.23
49.17
8.36
0.66
30.96
49.17
25.91
47.02
20.20
16.97
35.35
48.51
36.92
27.32
Zero Shot
One Shot Irrelevant
One Shot Relevant
Not Executable
API Misuse
Pass
Figure 3: Result of Checking API Usage from LLMs. Red bars are the percentage of answers that contain API misuse, which is the lower,
the better. The white bars in dot lines are the percentage of code answers that are not executable.
• Executable Sample Percentage = (Nmisuse+Npass)/N:
To analyze the proportion of executable cases among all
questions. It is necessary to consider the percentage of
executable cases in order to eliminate the influence from
the extreme situations, such as when only a few exe-
cutable code snippets are generated.
• Overall API Misuse Percentage = Nmisuse/N: To ana-
lyze the proportion of misuse cases among all questions.
Research Questions
We conduct a series of experiment on state-of-the-art LLMs
based on ROBUSTAPI, which demonstrate the usability and
effectiveness of ROBUSTAPI. The experiments provide in-
sights on the ability of answering real-world coding ques-
tions, and the robustness and reliability of these answers re-
garding API misuse problem. In the experiment, we try to
answer following questions:
• Q1: What are the API misuse rates in answering real-
world coding questions by these LLMs?
• Q2: How do irrelevant shots affect the results?
• Q3: Can correct API usage examples reduce the misuse?
• Q4: Why does LLM-generated code fail the API usage
check?
API Misuse Rate
Firstly, we present the API misuse rate of each models based
on ROBUSTAPI in the left of Figure 3. In this figure, the
higher the API misuse rate is, the worse of the code relia-
bility and robustness for this large language model. The API
misuse rate is calculated by dividing answers that can be
compiled and contains API misuses by all the answers that
can be compiled. From the evaluation results, all the eval-
uated models suffer from the API misuse problems, even
for the state-of-the-art commercial models like GPT-3.5 and
GPT-4. In zero-shot settings, Llama has the lowest API mis-
use rate. However, this is partially due to that most of Llama
answers does not include any code. A counter-intuition find-
ing is that GPT-4 actually has a higher API misuse rate than
GPT-3.5, though the coding ability of GPT-4 is proved to be
“40% more advanced than its predecessor, GPT-3.5” (Ope-
nAI 2023b). This indicates that with the code generation
ability of large language models are largely improved nowa-
days, the reliablity and robustness of code in real-world pro-
duction rises as a unnoticed issue. And the space for im-
provement is huge for this problem.
Finding 1. Answers to real-world coding questions from the
state-of-the-art large language models widely have API mis-
use problems.
One-Shot-Irrelevant Results
In this experiment, ROBUSTAPI gives a pair of question and
answer as the example to show the model how to follow the
template required by the instructions. The example contains
no information that are relevant to the API usage checked
by ROBUSTAPI. The result is shown in the middle of Fig-
ure 3. However, for most models, the irrelevant shot does not
significantly reduce the API misuse rate but on the contrary
slightly increase the misuse rate. One possible reason of this
is the irrelevant shot provided to the large language models
actually encourage the models to give a lengthy code solu-
tion, which increase the chance of API misuses. API misuse
rate of Llama increases significantly after adding the irrele-
vant shot because it has more valid answers that contain code
snippets. Overall, adding an irrelevant shot triggers the large
language models to generate more valid answers, which en-
ables a better evaluation on the code reliability and robust-
ness.
Finding 2. Among all the answers containing executable
code, 57-70% of the code snippets in LLM answers contain
API misuse, which could lead to severe consequence in pro-
duction.
Finding 3. Irrelevant shot examples does not help decrease
the API misuse rate but triggers more valid answers, which
show to be effective for benchmarking the model perfor-
mance.
One-Shot-Relevant Results
In this experiment, ROBUSTAPI adds a manually-written
shot in the prompt, which performs a different task but uses
the same API. This gives hints to LLMs on how to use these
APIs correctly. From the results, after adding the correct us-
age shot, the API misuse rates of GPT-3.5, GPT-4 and Vi-
cuna significantly drop. This indicates a effective improve-

Zero-shot
One-shot-irrelevant
One-shot-relevant
Misuse
Exec.
Overall
Misuse
Exec.
Overall
Misuse
Exec.
Overall
Rate ↓
Sample % ↑
Misuse % ↓
Rate ↓
Sample % ↑
Misuse % ↓
Rate ↓
Sample % ↑
Misuse % ↓
GPT 3.5
62.97%
79.14%
49.83%
68.09%
91.06%
62.00%
38.56%
80.71%
31.13%
GPT 4
68.81%
90.23%
62.09%
70.38%
91.39%
64.32%
54.40%
90.40%
49.17%
Llama 2
7.34%
9.02%
0.66%
61.36%
80.13%
49.17%
64.47%
72.93%
47.02%
Vicuna 1.5
45.66%
37.17%
16.97%
57.85%
83.86%
48.51%
42.53%
64.24%
27.32%
Table 2: Performance of Each LLM on ROBUSTAPI. ↓: the lower the better. ↑: the higher the better. Misuse Rate is the proportion of
misuse cases among the executable cases; Exec. Sample % is the proportion of executable cases among all questions; Overall Misuse % is
the proportion of misuse cases among all questions.
ment under this experiment settings. As for Llama, the rel-
evant shot does not improve the performance. This experi-
ment show that some LLMs can effectively ‘learn’ the cor-
rect API usage and follow the usage. However, since existing
language models are trained with data from code reposito-
ries, if the training datasets contain a large number of API
violations, the language models are prone to generate code
with API misuses, which explains the high API misuse rate
in zero-shot and one-shot-irrelevant evaluation.
Finding 4. Some LLMs can learn from the correct usage
example, which reduce the API misuse rate.
Error Analysis
Activity.setContentView
ApplicationInfo.loadIcon
BufferedReader.readLine
Cipher.init
DataOutputStream.write
File.createNewFile
File.mkdirs
FileChannel.write
InputStream.read
Iterator.next
JsonElement.getAsString
List.get
Mac.doFinal
Map.get
PrintWriter.write
ProgressDialog.dismiss
RandomAccessFile.read
RandomAccessFile.write
SQLiteDatabase.query
SortedMap.firstKey
String.getBytes
StringTokenizer.nextToken
TypedArray.getString
G3.5_0shot
G3.5_1shot_irr
G3.5_1shot_r
G4_0shot
G4_1shot_ir
G4_1shot_r
LMA_0shot
LMA_1shot_ir
LMA_1shot_r
Vic_0shot
Vic_1shot_ir
Vic_1shot_r
Misuse Rate for Each API in Each Model
0
20
40
60
80
100
Figure 4: Misuse rate of each API by each LLM. The deeper the
color, the higher the misuse rate. G3.5, G4, LMA, Vic are short for
GPT3.5, GPT4, Llama2, Vicuna1.5.
In this section, we discuss the answers from LLMs that
cannot pass the API usage check in ROBUSTAPI evaluation.
There are two categories for the failure cases: cases that are
not compilable, and cases that are compilable but contain
API misuses as show in Figure 3. We refer the ability of be-
ing compiled successfully as compilability. The compilation
failure rate is calculated by dividing the number of cases
that can be compiled to the total number of cases in the
benchmarks. GPT-4 performs the best among all the mod-
els regarding the compilability, which has less than 10% an-
swers that cannot be compiled across all experiment settings.
Adding few shots to prompts help reduce the compilation
failure rate in the evaluation results for all models. As for
the API misuse rate, we dive deeper into the APIs that LLMs
are prone to misuse. Figure 4 details the misuse rate of each
API for each LLM. Among all APIs, the Android develope-
ment API Activity.setContentView has the least misuse
rate across all the models.
Finding 5. GPT-4 has the highest number of answers that
contain executable code. Different LLMs have different mis-
use rate trend for the benchmark APIs.
Case Study: API Misuse in GPT-3.5
Taking GPT-3.5 as an example, we show a typical sample
that GPT-3.5 reacts differently under different experiment
settings. This question ask the model to help write a string
to the file using API PrintWriter.write. Under zero-shot
and one-irrelevant-shot settings, the answers differ slightly
but both misuse the API by not catching exceptions. After
giving the model the correct API usage example, the model
learn how to use the API and responses with the reliable
code.
Zero Shot:
PrintWriter writer = new PrintWriter("f.txt", true);
writer.write("text to append");
writer.close();
One Irrelevant Shot:
String text = "Hello, World!";
PrintWriter writer = new PrintWriter("f.txt", true);
writer.write(text);
writer.close();
One Relevant Shot:
try {String text = "Hello, World!";
PrintWriter writer = new PrintWriter("f.txt", true);
writer.write(text);
} catch (IOException e) {e.printStackTrace();}
Conclusion
In this paper, we propose a benchmark ROBUSTAPI to study
the API misuse behaviors in code generated by LLMs. From
the benchmark results on state-of-the-art models, we find
that API misuse widely exists in large language models even
the code is executable and aligned with users’ intention. Un-
der different experiment settings, we explore effective meth-
ods of benchmarking and improving the API misuse rate of
LLMs. The findings and benchmarks from this paper can in-
spire and accelerate future research on this problem.

References
Anil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin,
D.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen,
Z.; et al. 2023.
Palm 2 technical report.
arXiv preprint
arXiv:2305.10403.
Austin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski,
H.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; et al.
2021. Program synthesis with large language models. arXiv
preprint arXiv:2108.07732.
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;
Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman,
G.; et al. 2021. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,
I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot
Impressing GPT-4 with 90%* ChatGPT Quality.
Fischer, F.; B¨ottinger, K.; Xiao, H.; Stransky, C.; Acar, Y.;
Backes, M.; and Fahl, S. 2017. Stack overflow considered
harmful? the impact of copy&paste on android application
security. In 2017 IEEE Symposium on Security and Privacy
(SP), 121–136. IEEE.
Fischer, G.; Lusiardi, J.; and Von Gudenberg, J. W. 2007.
Abstract syntax trees-and their role in model driven software
development. In International Conference on Software En-
gineering Advances (ICSEA 2007), 38–38. IEEE.
Hendrycks, D.; Basart, S.; Kadavath, S.; Mazeika, M.;
Arora, A.; Guo, E.; Burns, C.; Puranik, S.; He, H.; Song, D.;
et al. 2021. Measuring coding challenge competence with
apps. arXiv preprint arXiv:2105.09938.
Jesse, K.; Ahmed, T.; Devanbu, P. T.; and Morgan, E. 2023.
Large Language Models and Simple, Stupid Bugs. arXiv
preprint arXiv:2303.11455.
Liu, J.; Xia, C. S.; Wang, Y.; and Zhang, L. 2023. Is your
code generated by chatgpt really correct? rigorous evalua-
tion of large language models for code generation. arXiv
preprint arXiv:2305.01210.
Lu, S.; Guo, D.; Ren, S.; Huang, J.; Svyatkovskiy, A.;
Blanco, A.; Clement, C.; Drain, D.; Jiang, D.; Tang, D.; et al.
2021. Codexglue: A machine learning benchmark dataset
for code understanding and generation.
arXiv preprint
arXiv:2102.04664.
Luo, Z.; Xu, C.; Zhao, P.; Sun, Q.; Geng, X.; Hu, W.; Tao,
C.; Ma, J.; Lin, Q.; and Jiang, D. 2023. WizardCoder: Em-
powering Code Large Language Models with Evol-Instruct.
arXiv preprint arXiv:2306.08568.
OpenAI. 2023a.
GPT-4 Technical Report.
ArXiv,
abs/2303.08774.
OpenAI.
2023b.
GPT-4
Technical
Report.
arXiv:2303.08774.
Pearce, H.; Ahmad, B.; Tan, B.; Dolan-Gavitt, B.; and Karri,
R. 2022. Asleep at the keyboard? assessing the security of
github copilot’s code contributions. In 2022 IEEE Sympo-
sium on Security and Privacy (SP), 754–768. IEEE.
Perry, N.; Srivastava, M.; Kumar, D.; and Boneh, D. 2022.
Do users write more insecure code with AI assistants? arXiv
preprint arXiv:2211.03622.
Poesia, G.; Polozov, O.; Le, V.; Tiwari, A.; Soares, G.;
Meek, C.; and Gulwani, S. 2022. Synchromesh: Reliable
code generation from pre-trained language models. arXiv
preprint arXiv:2201.11227.
Sandoval, G.; Pearce, H.; Nys, T.; Karri, R.; Garg, S.; and
Dolan-Gavitt, B. 2023. Lost at c: A user study on the se-
curity implications of large language model code assistants.
arXiv preprint arXiv:2208.09727.
Shen, B.; Zhang, J.; Chen, T.; Zan, D.; Geng, B.; Fu, A.;
Zeng, M.; Yu, A.; Ji, J.; Zhao, J.; et al. 2023a.
PanGu-
Coder2: Boosting Large Language Models for Code with
Ranking Feedback. arXiv preprint arXiv:2307.14936.
Shen, X.; Chen, Z.; Backes, M.; and Zhang, Y. 2023b. In
chatgpt we trust? measuring and characterizing the reliabil-
ity of chatgpt. arXiv preprint arXiv:2304.08979.
Siddiq, M. L.; Majumder, S. H.; Mim, M. R.; Jajodia, S.; and
Santos, J. C. 2022. An Empirical Study of Code Smells in
Transformer-based Code Generation Techniques. In 2022
IEEE 22nd International Working Conference on Source
Code Analysis and Manipulation (SCAM), 71–82. IEEE.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288.
Yang, D.; Hussain, A.; and Lopes, C. V. 2016. From query to
usable code: an analysis of stack overflow code snippets. In
Proceedings of the 13th International Conference on Mining
Software Repositories, 391–402.
Ye, J.; Chen, X.; Xu, N.; Zu, C.; Shao, Z.; Liu, S.; Cui, Y.;
Zhou, Z.; Gong, C.; Shen, Y.; et al. 2023. A comprehensive
capability analysis of gpt-3 and gpt-3.5 series models. arXiv
preprint arXiv:2303.10420.
Yetistiren, B.; Ozsoy, I.; and Tuzun, E. 2022. Assessing the
quality of GitHub copilot’s code generation. In Proceedings
of the 18th International Conference on Predictive Models
and Data Analytics in Software Engineering, 62–71.
Yin, P.; Deng, B.; Chen, E.; Vasilescu, B.; and Neubig, G.
2018. Learning to mine aligned code and natural language
pairs from stack overflow. In Proceedings of the 15th inter-
national conference on mining software repositories, 476–
486.
Zhang, T.; Upadhyaya, G.; Reinhardt, A.; Rajan, H.; and
Kim, M. 2018. Are code examples on an online q&a forum
reliable? a study of api misuse on stack overflow. In Pro-
ceedings of the 40th international conference on software
engineering, 886–896.
Zhou, J.; and Walker, R. J. 2016. API deprecation: a ret-
rospective analysis and detection method for code examples
on the web. In Proceedings of the 2016 24th ACM SIGSOFT
International Symposium on Foundations of Software Engi-
neering, 266–277.

Appendix
Rules Checked in ROBUSTAPI
The API usage patterns checked in ROBUSTAPI are shown
in Table 3. It is summarized based on the existing research
on API misuses (Zhang et al. 2018). Each pattern contains
a sequence of control structures and method calls, sepa-
rated by commas, which will be retrieved by the API usage
checker and compare against the AST of code snippets.
To be more specific, arg0 is a variable that indicates the
first argument of the function call. rcv is also a variable cor-
responding to the object of the method call. a@C indicates
that API a can only be invoked when condition C is satisfied.
For example, the pattern of JsonElement.getAsString:
try,getAsString()@rcv!=null,end,catch(Exception),end
indicates the API JsonElement.getAsString can be used
only when the object of getAsString() is valid, and the
API call should be enclosed within a try-catch block to
handle potential exceptions.
More detailed explanation of each API usage rules is as
follows.
1. FileChannel.write: Execute write() within a try-catch block to
handle exceptions.
2. FileChannel.write: Execute write() within a try-catch for han-
dling exceptions.
3. FileChannel.write/close: To execute close() after write(), han-
dling exceptions.
4. Activity.setContentView: Ensure that setContentView(View)
is called after onCreate(Bundle).
5. List.get: Loop over the get(int) call while arg0 is less than the
instance’s size().
6. RandomAccessFile.write/close: Execute close() after execute
write(byte[]), handling exceptions.
7. PrintWriter.write: Execute write(String) within try-catch for
handling exceptions.
8. PrintWriter.write/close: Execute close() after write(String)
within try-catch for handling exceptions.
9. TypedArray.getString: After calling getString(int), make sure
to call recycle().
10. StringTokenizer.nextToken: Call hasMoreTokens() to check
before calling nextToken().
11. Map.get: Compare the return value of get().
12. File.createNewFile: Execute createNewFile() within try-catch
for handling exceptions.
13. SQLiteDatabase.query: After executing query(), make sure to
call close().
14. JsonElement.getAsString: Execute getAsString() within try-
catch. Check the object is not null.
15. JsonElement.getAsString: Execute getAsString() within try-
catch block. Check the object with isJsonPrimitive().
16. String.getBytes: Execute getBytes() within try-catch for han-
dling exceptions.
17. ProgressDialog.dismiss: Check the object of dismiss() against
null.
18. Cipher.init: Execute init() within try-catch for handling excep-
tions.
19. Cipher.init: Execute init() before getInstance() and within try-
catch for handling exceptions.
20. File.mkdirs: Call exists() before create a directory using
mkdirs().
21. ApplicationInfo.loadIcon: Execute loadIcon() within try-
catch for handling exceptions.
22. ApplicationInfo.loadIcon: Call getPackageManager() before
execute loadIcon().
23. BufferedReader.readLine: Execute readLine() within try-
catch for handling exceptions.
24. BufferedReader.readLine: Create a BufferedReader before
execute readLine().
25. BufferedReader.readLine: Create a BufferedReader before
execute readLine() within try-catch for handling exceptions.
26. BufferedReader.readLine: Execute readLine() within try-
catch and handle Exception.
27. Iterator.next: Call next() as long as the instance’s hasNext() is
true.

Table 3: API usage patterns checked in ROBUSTAPI.
API
Pattern
1
FileChannel.write
try, write(ByteBuffer), end, catch(Exception), end
2
FileChannel.write
try, getChannel(), write(ByteBuffer), end, catch(Exception), end
3
FileChannel.write/close
try, write(ByteBuffer), close(), end, catch(Exception), end
4
Activity.setContentView
onCreate(Bundle), setContentView(View)
5
List.get
LOOP, get(int)@arg0<rcv.size(), end
6
RandomAccessFile.write/close
try, write(byte[]), close(), end, catch(Exception), end
7
PrintWriter.write
try, write(String), end, catch(Exception), end
8
PrintWriter.write/close
try, write(String), close(), end, catch(Exception), end
9
TypedArray.getString
getString(int), recycle()
10
StringTokenizer.nextToken
StringTokenizer.nextToken
11
Map.get
get(String), IF, end
12
File.createNewFile
try, createNewFile(), end, catch(Exception), end
13
SQLiteDatabase.query
query(String,String[],String,String[],String,String,String), close()
14
JsonElement.getAsString
try, getAsString()@rcv!=null, end, catch(Exception), end
15
JsonElement.getAsString
try, getAsString()@rcv.isJsonPrimitive(), end, catch(Exception), end
16
String.getBytes
getBytes(String)
17
ProgressDialog.dismiss
IF, dismiss()@rcv!=null, end
18
Cipher.init
try, init(int,Key), end, catch(Exception), end
19
Cipher.init
try, getInstance(String), init(int,Key), end, catch(Exception), end
20
File.mkdirs
exists(), mkdirs()
21
ApplicationInfo.loadIcon
try, loadIcon(PackageManager), end, catch(Exception), end
22
ApplicationInfo.loadIcon
getPackageManager(), loadIcon(PackageManager)
23
BufferedReader.readLine
try, readLine(), end, catch(Exception), end
24
BufferedReader.readLine
new BufferedReader(InputStreamReader), readLine()
25
BufferedReader.readLine
try, new BufferedReader(InputStreamReader), readLine(), end, catch(Exception), end
26
BufferedReader.readLine
try, readLine(), end, catch(IOException), end
27
Iterator.next
next()@rcv.hasNext()

