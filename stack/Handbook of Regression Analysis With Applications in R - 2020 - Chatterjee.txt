Handbook of Regression Analysis
With Applications in R

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors
David J. Balding, Noel A.C. Cressie, Garrett M. Fitzmaurice, Harvey
Goldstein, Geert Molenberghs, David W. Scott, Adrian F.M. Smith, and
Ruey S. Tsay
Editors Emeriti
Vic Barnett, Ralph A. Bradley, J. Stuart Hunter, J.B. Kadane, David G.
Kendall, and Jozef L. Teugels
A complete list of the titles in this series appears at the end of this volume.

Handbook of Regression
Analysis With Applications
in R
Second Edition
Samprit Chatterjee
New York University, New York, USA
Jeffrey S. Simonoff
New York University, New York, USA

This second edition first published 2020
© 2020 John Wiley & Sons, Inc
Edition History
Wiley-Blackwell (1e, 2013)
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in
any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by
law. Advice on how to obtain permission to reuse material from this title is available at http://www.wiley.com/go/
permissions.
The right of Samprit Chatterjee and Jeffery S. Simonoff to be identified as the authors of this work has been
asserted in accordance with law.
Registered Office
John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, USA
Editorial Office
111 River Street, Hoboken, NJ 07030, USA
For details of our global editorial offices, customer services, and more information about Wiley products visit us
at www.wiley.com.
Wiley also publishes its books in a variety of electronic formats and by print-on-demand. Some content that
appears in standard print versions of this book may not be available in other formats.
Limit of Liability/Disclaimer of Warranty
While the publisher and authors have used their best efforts in preparing this work, they make no representations
or warranties with respect to the accuracy or completeness of the contents of this work and specifically disclaim all
warranties, including without limitation any implied warranties of merchantability or fitness for a particular
purpose. No warranty may be created or extended by sales representatives, written sales materials or promotional
statements for this work. The fact that an organization, website, or product is referred to in this work as a citation
and/or potential source of further information does not mean that the publisher and authors endorse the
information or services the organization, website, or product may provide or recommendations it may make. This
work is sold with the understanding that the publisher is not engaged in rendering professional services. The
advice and strategies contained herein may not be suitable for your situation. You should consult with a specialist
where appropriate. Further, readers should be aware that websites listed in this work may have changed or
disappeared between when this work was written and when it is read. Neither the publisher nor authors shall be
liable for any loss of profit or any other commercial damages, including but not limited to special, incidental,
consequential, or other damages.
Library of Congress Cataloging-in-Publication Data
Names: Chatterjee, Samprit, 1938- author. | Simonoff, Jeffrey S., author.
Title: Handbook of regression analysis with applications in R / Professor
Samprit Chatterjee, New York University, Professor Jeffrey S. Simonoff,
New York University.
Other titles: Handbook of regression analysis
Description: Second edition. | Hoboken, NJ : Wiley, 2020. | Series: Wiley
series in probability and statistics | Revised edition of: Handbook of
regression analysis. 2013. | Includes bibliographical references and
index.
Identifiers: LCCN 2020006580 (print) | LCCN 2020006581 (ebook) | ISBN
9781119392378 (hardback) | ISBN 9781119392477 (adobe pdf) | ISBN
9781119392484 (epub)
Subjects: LCSH: Regression analysis--Handbooks, manuals, etc. | R (Computer
program language)
Classification: LCC QA278.2 .C498 2020 (print) | LCC QA278.2 (ebook) |
DDC 519.5/36--dc23
LC record available at https://lccn.loc.gov/2020006580
LC ebook record available at https://lccn.loc.gov/2020006581
Cover Design: Wiley
Cover Image: © Dmitriy Rybin/Shutterstock
Set in 10.82/12pt AGaramondPro by SPi Global, Chennai, India
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

Dedicated to everyone who labors in the field
of statistics, whether they are students,
teachers, researchers, or data analysts.

Contents
Preface to the Second Edition
xv
Preface to the First Edition
xix
Part I
The Multiple Linear Regression Model
1
Multiple Linear Regression
3
1.1
Introduction
3
1.2
Concepts and Background Material
4
1.2.1
The Linear Regression Model
4
1.2.2
Estimation Using Least Squares
5
1.2.3
Assumptions
8
1.3
Methodology
9
1.3.1
Interpreting Regression Coefficients
9
1.3.2
Measuring the Strength of the Regression
Relationship
10
1.3.3
Hypothesis Tests and Confidence Intervals
for β
12
1.3.4
Fitted Values and Predictions
13
1.3.5
Checking Assumptions Using Residual Plots
14
1.4
Example — Estimating Home Prices
15
1.5
Summary
19
2
Model Building
23
2.1
Introduction
23
2.2
Concepts and Background Material
24
2.2.1
Using Hypothesis Tests to Compare Models
24
2.2.2
Collinearity
26
2.3
Methodology
29
2.3.1
Model Selection
29
2.3.2
Example — Estimating Home Prices
(continued)
31
2.4
Indicator Variables and Modeling Interactions
38
2.4.1
Example — Electronic Voting and the 2004
Presidential Election
40
2.5
Summary
46
vii

viii
CONTENTS
Part II
Addressing Violations of Assumptions
3
Diagnostics for Unusual Observations
53
3.1
Introduction
53
3.2
Concepts and Background Material
54
3.3
Methodology
56
3.3.1
Residuals and Outliers
56
3.3.2
Leverage Points
57
3.3.3
Influential Points and Cook’s Distance
58
3.4
Example — Estimating Home Prices (continued)
60
3.5
Summary
63
4
Transformations and Linearizable Models
67
4.1
Introduction
67
4.2
Concepts and Background Material: The Log-Log Model
69
4.3
Concepts and Background Material: Semilog Models
69
4.3.1
Logged Response Variable
70
4.3.2
Logged Predictor Variable
70
4.4
Example — Predicting Movie Grosses After One Week
71
4.5
Summary
77
5
Time Series Data and Autocorrelation
79
5.1
Introduction
79
5.2
Concepts and Background Material
81
5.3
Methodology: Identifying Autocorrelation
83
5.3.1
The Durbin-Watson Statistic
83
5.3.2
The Autocorrelation Function (ACF)
84
5.3.3
Residual Plots and the Runs Test
85
5.4
Methodology: Addressing Autocorrelation
86
5.4.1
Detrending and Deseasonalizing
86
5.4.2
Example — e-Commerce Retail Sales
87
5.4.3
Lagging and Differencing
93
5.4.4
Example — Stock Indexes
94
5.4.5
Generalized Least Squares (GLS):
The Cochrane-Orcutt Procedure
99
5.4.6
Example — Time Intervals Between Old Faithful
Geyser Eruptions
100
5.5
Summary
104

CONTENTS
ix
Part III
Categorical Predictors
6
Analysis of Variance
109
6.1
Introduction
109
6.2
Concepts and Background Material
110
6.2.1
One-Way ANOVA
110
6.2.2
Two-Way ANOVA
111
6.3
Methodology
113
6.3.1
Codings for Categorical Predictors
113
6.3.2
Multiple Comparisons
118
6.3.3
Levene’s Test and Weighted Least Squares
120
6.3.4
Membership in Multiple Groups
123
6.4
Example — DVD Sales of Movies
125
6.5
Higher-Way ANOVA
130
6.6
Summary
132
7
Analysis of Covariance
135
7.1
Introduction
135
7.2
Methodology
136
7.2.1
Constant Shift Models
136
7.2.2
Varying Slope Models
137
7.3
Example — International Grosses of Movies
137
7.4
Summary
142
Part IV
Non-Gaussian Regression Models
8
Logistic Regression
145
8.1
Introduction
145
8.2
Concepts and Background Material
147
8.2.1
The Logit Response Function
148
8.2.2
Bernoulli and Binomial Random Variables
149
8.2.3
Prospective and Retrospective Designs
149
8.3
Methodology
152
8.3.1
Maximum Likelihood Estimation
152
8.3.2
Inference, Model Comparison, and Model
Selection
153

x
CONTENTS
8.3.3
Goodness-of-Fit
155
8.3.4
Measures of Association and Classification
Accuracy
157
8.3.5
Diagnostics
159
8.4
Example — Smoking and Mortality
159
8.5
Example — Modeling Bankruptcy
163
8.6
Summary
168
9
Multinomial Regression
173
9.1
Introduction
173
9.2
Concepts and Background Material
174
9.2.1
Nominal Response Variable
174
9.2.2
Ordinal Response Variable
176
9.3
Methodology
178
9.3.1
Estimation
178
9.3.2
Inference, Model Comparisons, and Strength of
Fit
178
9.3.3
Lack of Fit and Violations of
Assumptions
180
9.4
Example — City Bond Ratings
180
9.5
Summary
184
10 Count Regression
187
10.1
Introduction
187
10.2
Concepts and Background Material
188
10.2.1
The Poisson Random Variable
188
10.2.2
Generalized Linear Models
189
10.3
Methodology
190
10.3.1
Estimation and Inference
190
10.3.2
Offsets
191
10.4
Overdispersion and Negative Binomial Regression
192
10.4.1
Quasi-likelihood
192
10.4.2
Negative Binomial Regression
193
10.5
Example — Unprovoked Shark Attacks in Florida
194
10.6
Other Count Regression Models
201
10.7
Poisson Regression and Weighted Least Squares
203
10.7.1
Example — International Grosses of Movies
(continued)
204
10.8
Summary
206
11 Models for Time-to-Event (Survival) Data
209
11.1
Introduction
210
11.2
Concepts and Background Material
211
11.2.1
The Nature of Survival Data
211
11.2.2
Accelerated Failure Time Models
212
11.2.3
The Proportional Hazards Model
214

CONTENTS
xi
11.3
Methodology
214
11.3.1
The Kaplan-Meier Estimator and the Log-Rank
Test
214
11.3.2
Parametric (Likelihood) Estimation
219
11.3.3
Semiparametric (Partial Likelihood)
Estimation
221
11.3.4
The Buckley-James Estimator
223
11.4
Example — The Survival of Broadway Shows
(continued)
223
11.5
Left-Truncated/Right-Censored Data and Time-Varying
Covariates
230
11.5.1
Left-Truncated/Right-Censored Data
230
11.5.2
Example — The Survival of Broadway Shows
(continued)
233
11.5.3
Time-Varying Covariates
233
11.5.4
Example — Female Heads of Government
235
11.6
Summary
238
Part V
Other Regression Models
12 Nonlinear Regression
243
12.1
Introduction
243
12.2
Concepts and Background Material
244
12.3
Methodology
246
12.3.1
Nonlinear Least Squares Estimation
246
12.3.2
Inference for Nonlinear Regression Models
247
12.4
Example — Michaelis-Menten Enzyme Kinetics
248
12.5
Summary
252
13 Models for Longitudinal and Nested Data
255
13.1
Introduction
255
13.2
Concepts and Background Material
257
13.2.1
Nested Data and ANOVA
257
13.2.2
Longitudinal Data and Time Series
258
13.2.3
Fixed Effects Versus Random Effects
259
13.3
Methodology
260
13.3.1
The Linear Mixed Effects Model
260
13.3.2
The Generalized Linear Mixed Effects Model
262
13.3.3
Generalized Estimating Equations
262
13.3.4
Nonlinear Mixed Effects Models
263
13.4
Example — Tumor Growth in a Cancer Study
264
13.5
Example — Unprovoked Shark Attacks in the United
States
269
13.6
Summary
275

xii
CONTENTS
14 Regularization Methods and Sparse Models
277
14.1
Introduction
277
14.2
Concepts and Background Material
278
14.2.1
The Bias–Variance Tradeoff
278
14.2.2
Large Numbers of Predictors and Sparsity
279
14.3
Methodology
280
14.3.1
Forward Stepwise Regression
280
14.3.2
Ridge Regression
281
14.3.3
The Lasso
281
14.3.4
Other Regularization Methods
283
14.3.5
Choosing the Regularization Parameter(s)
284
14.3.6
More Structured Regression Problems
285
14.3.7
Cautions About Regularization Methods
286
14.4
Example — Human Development Index
287
14.5
Summary
289
Part VI
Nonparametric and Semiparametric
Models
15 Smoothing and Additive Models
295
15.1
Introduction
296
15.2
Concepts and Background Material
296
15.2.1
The Bias–Variance Tradeoff
296
15.2.2
Smoothing and Local Regression
297
15.3
Methodology
298
15.3.1
Local Polynomial Regression
298
15.3.2
Choosing the Bandwidth
298
15.3.3
Smoothing Splines
299
15.3.4
Multiple Predictors, the Curse of Dimensionality, and
Additive Models
300
15.4
Example — Prices of German Used Automobiles
301
15.5
Local and Penalized Likelihood Regression
304
15.5.1
Example — The Bechdel Rule and Hollywood
Movies
305
15.6
Using Smoothing to Identify Interactions
307
15.6.1
Example — Estimating Home Prices
(continued)
308
15.7
Summary
310
16 Tree-Based Models
313
16.1
Introduction
314
16.2
Concepts and Background Material
314
16.2.1
Recursive Partitioning
314
16.2.2
Types of Trees
317

CONTENTS
xiii
16.3
Methodology
318
16.3.1
CART
318
16.3.2
Conditional Inference Trees
319
16.3.3
Ensemble Methods
320
16.4
Examples
321
16.4.1
Estimating Home Prices (continued)
321
16.4.2
Example — Courtesy in Airplane Travel
322
16.5
Trees for Other Types of Data
327
16.5.1
Trees for Nested and Longitudinal Data
327
16.5.2
Survival Trees
328
16.6
Summary
332
Bibliography
337
Index
343

Preface to the
Second Edition
The years since the first edition of this book appeared have been fast-moving
in the world of data analysis and statistics. Algorithmically-based methods
operating under the banner of machine learning, artificial intelligence, or
data science have come to the forefront of public perceptions about how to
analyze data, and more than a few pundits have predicted the demise of classic
statistical modeling.
To paraphrase Mark Twain, we believe that reports of the (impending)
death of statistical modeling in general, and regression modeling in particular,
are exaggerated. The great advantage that statistical models have over “black
box” algorithms is that in addition to effective prediction, their transparency
also provides guidance about the actual underlying process (which is crucial
for decision making), and affords the possibilities of making inferences and
distinguishing real effects from random variation based on those models.
There have been laudable attempts to encourage making machine learning
algorithms interpretable in the ways regression models are (Rudin, 2019), but
we believe that models based on statistical considerations and principles will
have a place in the analyst’s toolkit for a long time to come.
Of course, part of that usefulness comes from the ability to generalize
regression models to more complex situations, and that is the thrust of the
changes in this new edition. One thing that hasn’t changed is the philosophy
behind the book, and our recommendations on how it can be best used, and
we encourage the reader to refer to the preface to the first edition for guidance
on those points. There have been small changes to the original chapters, and
broad descriptions of those chapters can also be found in the preface to the
first edition. The five new chapters (Chapters 11, 13, 14, 15, and 16, with
the former chapter 11 on nonlinear regression moving to Chapter 12) expand
greatly on the power and applicability of regression models beyond what
was discussed in the first edition. For this reason many more references are
provided in these chapters than in the earlier ones, since some of the material
in those chapters is less established and less well-known, with much of it still
the subject of active research. In keeping with that, we do not spend much
(or any) time on issues for which there still isn’t necessarily a consensus in the
statistical community, but point to books and monographs that can help the
analyst get some perspective on that kind of material.
Chapter 11 discusses the modeling of time-to-event data, often referred
to as survival data. The response variable measures the length of time until an
event occurs, and a common complicator is that sometimes it is only known
xv

xvi
PREFACE TO THE SECOND EDITION
that a response value is greater than some number; that is, it is right-censored.
This can naturally occur, for example, in a clinical trial in which subjects
enter the study at varying times, and the event of interest has not occurred at
the end of the trial. Analysis focuses on the survival function (the probability
of surviving past a given time) and the hazard function (the instantaneous
probability of the event occurring at a given time given survival to that
time). Parametric models based on appropriate distributions like the Weibull
or log-logistic can be fit that take censoring into account. Semiparametric
models like the Cox proportional hazards model (the most commonly-used
model) and the Buckley-James estimator are also available, which weaken
distributional assumptions. Modeling can be adapted to situations where
event times are truncated, and also when there are covariates that change over
the life of the subject.
Chapter 13 extends applications to data with multiple observations for
each subject consistent with some structure from the underlying process. Such
data can take the form of nested or clustered data (such as students all in
one classroom) or longitudinal data (where a variable is measured at multiple
times for each subject). In this situation ignoring that structure results in an
induced correlation that reflects unmodeled differences between classrooms
and subjects, respectively. Mixed effects models generalize analysis of variance
(ANOVA) models and time series models to this more complicated situation.
Models with linear effects based on Gaussian distributions can be generalized
to nonlinear models, and also can be generalized to non-Gaussian distributions
through the use of generalized linear mixed effects models.
Modern data applications can involve very large (even massive) numbers of
predictors, which can cause major problems for standard regression methods.
Best subsets regression (discussed in Chapter 2) does not scale well to very
large numbers of predictors, and Chapter 14 discusses approaches that can
accomplish that. Forward stepwise regression, in which potential predictors
are stepped in one at a time, is an alternative to best subsets that scales
to massive data sets. A systematic approach to reducing the dimensionality
of a chosen regression model is through the use of regularization, in which
the usual estimation criterion is augmented with a penalty that encourages
sparsity; the most commonly-used version of this is the lasso estimator, and it
and its generalizations are discussed further.
Chapters 15 and 16 discuss methods that move away from specified
relationships between the response and the predictor to nonparametric and
semiparametric methods, in which the data are used to choose the form of
the underlying relationship. In Chapter 15 linear or (specifically specified)
nonlinear relationships are replaced with the notion of relationships taking the
form of smooth curves and surfaces. Estimation at a particular location is based
on local information; that is, the values of the response in a local neighborhood
of that location. This can be done through local versions of weighted least
squares (local polynomial estimation) or local regularization (smoothing
splines). Such methods can also be used to help identify interactions between
numerical predictors in linear regression modeling. Single predictor smoothing

PREFACE TO THE SECOND EDITION
xvii
estimators can be generalized to multiple predictors through the use of additive
functions of smooth curves. Chapter 16 focuses on an extremely flexible class of
nonparametric regression estimators, tree-based methods. Trees are based on
the notion of binary recursive partitioning. At each step a set of observations (a
node) is either split into two parts (children nodes) on the basis of the values of
a chosen variable, or is not split at all, based on encouraging homogeneity in the
children nodes. This approach provides nonparametric alternatives to linear
regression (regression trees), logistic and multinomial regression (classification
trees), accelerated failure time and proportional hazards regression (survival
trees) and mixed effects regression (longitudinal trees).
A final small change from the first edition to the second edition is in the
title, as it now includes the phrase With Applications in R. This is not really
a change, of course, as all of the analyses in the first edition were performed
using the statistics package R. Code for the output and figures in the book
can (still) be found at its associated web site at http://people.stern
.nyu.edu/jsimonof/RegressionHandbook/. As was the case in the
first edition, even though analyses are performed in R, we still refer to general
issues relevant to a data analyst in the use of statistical software even if those
issues don’t specifically apply to R.
We would like to once again thank our students and colleagues for their
encouragement and support, and in particular students for the tough questions
that have definitely affected our views on statistical modeling and by extension
this book. We would like to thank Jon Gurstelle, and later Kathleen Santoloci
and Mindy Okura-Marszycki, for approaching us with encouragement to
undertake a second edition. We would like to thank Sarah Keegan for her
patient support in bringing the book to fruition in her role as Project Editor.
We would like to thank Roni Chambers for computing assistance, and Glenn
Heller and Marc Scott for looking at earlier drafts of chapters. Finally, we
would like to thank our families for their continuing love and support.
SAMPRIT CHATTERJEE
Brooksville, Maine
JEFFREY S. SIMONOFF
New York, New York
October, 2019

Preface to the
First Edition
How to Use This Book
This book is designed to be a practical guide to regression modeling. There is
little theory here, and methodology appears in the service of the ultimate goal
of analyzing real data using appropriate regression tools. As such, the target
audience of the book includes anyone who is faced with regression data [that
is, data where there is a response variable that is being modeled as a function
of other variable(s)], and whose goal is to learn as much as possible from
that data.
The book can be used as a text for an applied regression course (indeed,
much of it is based on handouts that have been given to students in such a
course), but that is not its primary purpose; rather, it is aimed much more
broadly as a source of practical advice on how to address the problems that
come up when dealing with regression data. While a text is usually organized
in a way that makes the chapters interdependent, successively building on
each other, that is not the case here. Indeed, we encourage readers to dip into
different chapters for practical advice on specific topics as needed. The pace
of the book is faster than might typically be the case for a text. The coverage,
while at an applied level, does not shy away from sophisticated concepts. It is
distinct from, for example, Chatterjee and Hadi (2012), while also having less
theoretical focus than texts such as Greene (2011), Montgomery et al. (2012),
or Sen and Srivastava (1990).
This, however, is not a cookbook that presents a mechanical approach to
doing regression analysis. Data analysis is perhaps an art, and certainly a craft;
we believe that the goal of any data analysis book should be to help analysts
develop the skills and experience necessary to adjust to the inevitable twists
and turns that come up when analyzing real data.
We assume that the reader possesses a nodding acquaintance with regres-
sion analysis. The reader should be familiar with the basic terminology and
should have been exposed to basic regression techniques and concepts, at least
at the level of simple (one-predictor) linear regression. We also assume that
the user has access to a computer with an adequate regression package. The
material presented here is not tied to any particular software. Almost all of the
analyses described here can be performed by most standard packages, although
the ease of doing this could vary. All of the analyses presented here were
done using the free package R (R Development Core Team, 2017), which is
available for many different operating system platforms (see http://www
.R-project.org/ for more information). Code for the output and figures
xix

xx
PREFACE TO THE FIRST EDITION
in the book can be found at its associated web site at http://people
.stern.nyu.edu/jsimonof/RegressionHandbook/.
Each chapter of the book is laid out in a similar way, with most having at
least four sections of specific types. First is an introduction, where the general
issues that will be discussed in that chapter are presented. A section on concepts
and background material follows, where a discussion of the relationship of
the chapter’s material to the broader study of regression data is the focus.
This section also provides any theoretical background for the material that is
necessary. Sections on methodology follow, where the specific tools used in
the chapter are discussed. This is where relevant algorithmic details are likely
to appear. Finally, each chapter includes at least one analysis of real data using
the methods discussed in the chapter (as well as appropriate material from
earlier chapters), including both methodological and graphical analyses.
The book begins with discussion of the multiple regression model. Many
regression textbooks start with discussion of simple regression before moving
on to multiple regression. This is quite reasonable from a pedagogical point
of view, since simple regression has the great advantage of being easy to
understand graphically, but from a practical point of view simple regression
is rarely the primary tool in analysis of real data. For that reason, we start
with multiple regression, and note the simplifications that come from the
special case of a single predictor. Chapter 1 describes the basics of the multiple
regression model, including the assumptions being made, and both estimation
and inference tools, while also giving an introduction to the use of residual
plots to check assumptions.
Since it is unlikely that the first model examined will ultimately be the
final preferred model, Chapter 2 focuses on the very important areas of model
building and model selection. This includes addressing the issue of collinearity,
as well as the use of both hypothesis tests and information measures to help
choose among candidate models.
Chapters 3 through 5 study common violations of regression assumptions,
and methods available to address those model violations. Chapter 3 focuses on
unusual observations (outliers and leverage points), while Chapter 4 describes
how transformations (especially the log transformation) can often address both
nonlinearity and nonconstant variance violations. Chapter 5 is an introduction
to time series regression, and the problems caused by autocorrelation. Time
series analysis is a vast area of statistical methodology, so our goal in this
chapter is only to provide a good practical introduction to that area in the
context of regression analysis.
Chapters 6 and 7 focus on the situation where there are categorical variables
among the predictors. Chapter 6 treats analysis of variance (ANOVA) models,
which include only categorical predictors, while Chapter 7 looks at analysis of
covariance (ANCOVA) models, which include both numerical and categorical
predictors. The examination of interaction effects is a fundamental aspect of
these models, as are questions related to simultaneous comparison of many

PREFACE TO THE FIRST EDITION
xxi
groups to each other. Data of this type often exhibit nonconstant variance
related to the different subgroups in the population, and the appropriate tool
to address this issue, weighted least squares, is also a focus here.
Chapters 8 though 10 examine the situation where the nature of the
response variable is such that Gaussian-based least squares regression is no
longer appropriate. Chapter 8 focuses on logistic regression, designed for
binary response data and based on the binomial random variable. While
there are many parallels between logistic regression analysis and least squares
regression analysis, there are also issues that come up in logistic regression
that require special care. Chapter 9 uses the multinomial random variable to
generalize the models of Chapter 8 to allow for multiple categories in the
response variable, outlining models designed for response variables that either
do or do not have ordered categories. Chapter 10 focuses on response data in
the form of counts, where distributions like the Poisson and negative binomial
play a central role. The connection between all these models through the
generalized linear model framework is also exploited in this chapter.
The final chapter focuses on situations where linearity does not hold,
and a nonlinear relationship is necessary. Although these models are based on
least squares, from both an algorithmic and inferential point of view there
are strong connections with the models of Chapters 8 through 10, which we
highlight.
This Handbook can be used in several different ways. First, a reader may
use the book to find information on a specific topic. An analyst might want
additional information on, for example, logistic regression or autocorrelation.
The chapters on these (and other) topics provide the reader with this subject
matter information. As noted above, the chapters also include at least one
analysis of a data set, a clarification of computer output, and reference to
sources where additional material can be found. The chapters in the book are
to a large extent self-contained and can be consulted independently of other
chapters.
The book can also be used as a template for what we view as a reasonable
approach to data analysis in general. This is based on the cyclical paradigm
of model formulation, model fitting, model evaluation, and model updating
leading back to model (re)formulation. Statistical significance of test statistics
does not necessarily mean that an adequate model has been obtained. Further
analysis needs to be performed before the fitted model can be regarded as
an acceptable description of the data, and this book concentrates on this
important aspect of regression methodology. Detection of deficiencies of fit
is based on both testing and graphical methods, and both approaches are
highlighted here.
This preface is intended to indicate ways in which the Handbook can
be used. Our hope is that it will be a useful guide for data analysts, and will
help contribute to effective analyses. We would like to thank our students and
colleagues for their encouragement and support. We hope we have provided

xxii
PREFACE TO THE FIRST EDITION
them with a book of which they would approve. We would like to thank Steve
Quigley, Jackie Palmieri, and Amy Hendrickson for their help in bringing this
manuscript to print. We would also like to thank our families for their love
and support.
SAMPRIT CHATTERJEE
Brooksville, Maine
JEFFREY S. SIMONOFF
New York, New York
August, 2012

Part One
The Multiple Linear
Regression Model

One
Chapter
Multiple Linear Regression
1.1
Introduction
3
1.2
Concepts and Background Material
4
1.2.1
The Linear Regression Model
4
1.2.2
Estimation Using Least Squares
5
1.2.3
Assumptions
8
1.3
Methodology
9
1.3.1
Interpreting Regression Coefficients
9
1.3.2
Measuring the Strength of the Regression
Relationship
10
1.3.3
Hypothesis Tests and Confidence Intervals for β
12
1.3.4
Fitted Values and Predictions
13
1.3.5
Checking Assumptions Using Residual Plots
14
1.4
Example — Estimating Home Prices
15
1.5
Summary
19
1.1
Introduction
This is a book about regression modeling, but when we refer to regression
models, what do we mean? The regression framework can be characterized in
the following way:
1. We have one particular variable that we are interested in understanding
or modeling, such as sales of a particular product, sale price of a home, or
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
3

4
CHAPTER 1 Multiple Linear Regression
voting preference of a particular voter. This variable is called the target,
response, or dependent variable, and is usually represented by y.
2. We have a set of p other variables that we think might be useful in
predicting or modeling the target variable (the price of the product, the
competitor’s price, and so on; or the lot size, number of bedrooms, number
of bathrooms of the home, and so on; or the gender, age, income, party
membership of the voter, and so on). These are called the predicting, or
independent variables, and are usually represented by x1, x2, etc.
Typically, a regression analysis is used for one (or more) of three purposes:
1. modeling the relationship between x and y;
2. prediction of the target variable (forecasting);
3. and testing of hypotheses.
In this chapter, we introduce the basic multiple linear regression model,
and discuss how this model can be used for these three purposes. Specifically, we
discuss the interpretations of the estimates of different regression parameters,
the assumptions underlying the model, measures of the strength of the
relationship between the target and predictor variables, the construction of
tests of hypotheses and intervals related to regression parameters, and the
checking of assumptions using diagnostic plots.
1.2
Concepts and Background Material
1.2.1
THE LINEAR REGRESSION MODEL
The data consist of n observations, which are sets of observed values {x1i, x2i,
. . . , xpi, yi} that represent a random sample from a larger population. It is
assumed that these observations satisfy a linear relationship,
yi = β0 + β1x1i + · · · + βpxpi + εi,
(1.1)
where the β coefficients are unknown parameters, and the εi are random error
terms. By a linear model, it is meant that the model is linear in the parameters;
a quadratic model,
yi = β0 + β1xi + β2x2
i + εi,
paradoxically enough, is a linear model, since x and x2 are just versions of x1
and x2.
It is important to recognize that this, or any statistical model, is not
viewed as a true representation of reality; rather, the goal is that the model
be a useful representation of reality. A model can be used to explore the
relationships between variables and make accurate forecasts based on those
relationships even if it is not the “truth.” Further, any statistical model is
only temporary, representing a provisional version of views about the random
process being studied. Models can, and should, change, based on analysis using

1.2 Concepts and Background Material
5
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
••
•
•
•
•
•
•
•
•
•
•
•
2
4
6
8
10
15
20
25
30
x
y
E(y) = β0 + β1x
FIGURE 1.1: The simple linear regression model. The solid line corresponds
to the true regression line, and the dotted lines correspond to the random
errors εi.
the current model, selection among several candidate models, the acquisition
of new data, new understanding of the underlying random process, and so
on. Further, it is often the case that there are several different models that
are reasonable representations of reality. Having said this, we will sometimes
refer to the “true” model, but this should be understood as referring to the
underlying form of the currently hypothesized representation of the regression
relationship.
The special case of (1.1) with p = 1 corresponds to the simple regression
model, and is consistent with the representation in Figure 1.1. The solid line
is the true regression line, the expected value of y given the value of x. The
dotted lines are the random errors εi that account for the lack of a perfect
association between the predictor and the target variables.
1.2.2
ESTIMATION USING LEAST SQUARES
The true regression function represents the expected relationship between the
target and the predictor variables, which is unknown. A primary goal of a
regression analysis is to estimate this relationship, or equivalently, to estimate
the unknown parameters β. This requires a data-based rule, or criterion,
that will give a reasonable estimate. The standard approach is least squares
regression, where the estimates are chosen to minimize
n

i=1
[yi −(β0 + β1x1i + · · · + βpxpi)]2.
(1.2)
Figure 1.2 gives a graphical representation of least squares that is based
on Figure 1.1. Now the true regression line is represented by the gray line,

6
CHAPTER 1 Multiple Linear Regression
2
4
6
8
x
10
15
20
25
30
y
E(y) = β0 + β1x
^
^
^
•
•
•
••
• •
••
•
•
•
•
•
•
•
•
•••
•
•
•
•
•
•
•
•
•
•
FIGURE 1.2: Least squares estimation for the simple linear regression model,
using the same data as in Figure 1.1. The gray line corresponds to the true
regression line, the solid black line corresponds to the fitted least squares
line (designed to estimate the gray line), and the lengths of the dotted lines
correspond to the residuals. The sum of squared values of the lengths of the
dotted lines is minimized by the solid black line.
and the solid black line is the estimated regression line, designed to estimate
the (unknown) gray line as closely as possible. For any choice of estimated
parameters ˆβ, the estimated expected response value given the observed
predictor values equals
ˆyi = ˆβ0 + ˆβ1x1i + · · · + ˆβpxpi,
and is called the fitted value. The difference between the observed value yi
and the fitted value ˆyi is called the residual, the set of which is represented by
the signed lengths of the dotted lines in Figure 1.2. The least squares regression
line minimizes the sum of squares of the lengths of the dotted lines; that is,
the ordinary least squares (OLS) estimates minimize the sum of squares of the
residuals.
In higher dimensions (p > 1), the true and estimated regression relation-
ships correspond to planes (p = 2) or hyperplanes (p ≥3), but otherwise the
principles are the same. Figure 1.3 illustrates the case with two predictors.
The length of each vertical line corresponds to a residual (solid lines refer to
positive residuals, while dashed lines refer to negative residuals), and the (least
squares) plane that goes through the observations is chosen to minimize the
sum of squares of the residuals.

1.2 Concepts and Background Material
7
0
2
4
6
8
10
15
20
25
30
35
40
45
50
0
2 4 6
810
x2
x1
y
•
•
•
•
•
•
•
• •
•
•
•
•
•
•
•
• •
•
•
•
•
•
•
•
•
•
•
•
•
FIGURE 1.3: Least squares estimation for the multiple linear regression
model with two predictors. The plane corresponds to the fitted least squares
relationship, and the lengths of the vertical lines correspond to the residuals.
The sum of squared values of the lengths of the vertical lines is minimized by
the plane.
The linear regression model can be written compactly using matrix
notation. Define the following matrix and vectors as follows:
X =
⎛
⎜
⎝
1 x11 · · · xp1
...
...
...
1 x1n · · · xpn
⎞
⎟
⎠y =
⎛
⎜
⎝
y1...
yn
⎞
⎟
⎠β =
⎛
⎜
⎜
⎜
⎝
β0
β1...
βp
⎞
⎟
⎟
⎟
⎠ε =
⎛
⎜
⎝
ε1...
εn
⎞
⎟
⎠.
The regression model (1.1) is then
y = Xβ + ε.
(1.3)
The normal equations [which determine the minimizer of (1.2)] can be
shown (using multivariate calculus) to be
(X′X)ˆβ = X′y,
which implies that the least squares estimates satisfy
ˆβ = (X′X)−1X′y.
(1.4)
The fitted values are then
ˆy = X ˆβ = X(X′X)−1X′y ≡Hy,
(1.5)

8
CHAPTER 1 Multiple Linear Regression
where H = X(X′X)−1X′ is the so-called “hat” matrix (since it takes y to ˆy).
The residuals e = y −ˆy thus satisfy
e = y −ˆy = y −X(X′X)−1X′y = (I −X(X′X)−1X′)y,
(1.6)
or
e = (I −H)y.
1.2.3
ASSUMPTIONS
The least squares criterion will not necessarily yield sensible results unless
certain assumptions hold. One is given in (1.1) — the linear model should
be appropriate. In addition, the following assumptions are needed to justify
using least squares regression.
1. The expected value of the errors is zero (E(εi) = 0 for all i). That is, it
cannot be true that for certain observations the model is systematically
too low, while for others it is systematically too high. A violation of this
assumption will lead to difficulties in estimating β0. More importantly,
this reflects that the model does not include a necessary systematic
component, which has instead been absorbed into the error terms.
2. The variance of the errors is constant (V (εi) = σ2 for all i). That is,
it cannot be true that the strength of the model is greater for some
parts of the population (smaller σ) and less for other parts (larger σ).
This assumption of constant variance is called homoscedasticity, and its
violation (nonconstant variance) is called heteroscedasticity. A violation
of this assumption means that the least squares estimates are not as efficient
as they could be in estimating the true parameters, and better estimates are
available. More importantly, it also results in poorly calibrated confidence
and (especially) prediction intervals.
3. The errors are uncorrelated with each other. That is, it cannot be true
that knowing that the model underpredicts y (for example) for one
particular observation says anything at all about what it does for any
other observation. This violation most often occurs in data that are
ordered in time (time series data), where errors that are near each other
in time are often similar to each other (such time-related correlation
is called autocorrelation). Violation of this assumption means that the
least squares estimates are not as efficient as they could be in estimating
the true parameters, and more importantly, its presence can lead to very
misleading assessments of the strength of the regression.
4. The errors are normally distributed. This is needed if we want to construct
any confidence or prediction intervals, or hypothesis tests, which we
usually do. If this assumption is violated, hypothesis tests and confidence
and prediction intervals can be very misleading.

1.3 Methodology
9
Since violation of these assumptions can potentially lead to completely
misleading results, a fundamental part of any regression analysis is to check
them using various plots, tests, and diagnostics.
1.3
Methodology
1.3.1
INTERPRETING REGRESSION COEFFICIENTS
The least squares regression coefficients have very specific meanings. They are
often misinterpreted, so it is important to be clear on what they mean (and do
not mean). Consider first the intercept, ˆβ0.
ˆβ0: The estimated expected value of the target variable when the predictors
are all equal to zero.
Note that this might not have any physical interpretation, since a zero value for
the predictor(s) might be impossible, or might never come close to occurring
in the observed data. In that situation, it is pointless to try to interpret
this value. If all of the predictors are centered to have zero mean, then ˆβ0
necessarily equals Y , the sample mean of the target values. Note that if there
is any particular value for each predictor that is meaningful in some sense, if
each variable is centered around its particular value, then the intercept is an
estimate of E(y) when the predictors all have those meaningful values.
The estimated coefficient for the jth predictor (j = 1, . . . , p) is interpreted
in the following way:
ˆβj: The estimated expected change in the target variable associated with a one
unit change in the jth predicting variable, holding all else in the model
fixed.
There are several noteworthy aspects to this interpretation. First, note the
word associated — we cannot say that a change in the target variable is caused
by a change in the predictor, only that they are associated with each other.
That is, correlation does not imply causation.
Another key point is the phrase “holding all else in the model fixed,” the
implications of which are often ignored. Consider the following hypothetical
example. A random sample of college students at a particular university is
taken in order to understand the relationship between college grade point
average (GPA) and other variables. A model is built with college GPA as a
function of high school GPA and the standardized Scholastic Aptitude Test
(SAT), with resultant least squares fit
College GPA = 1.3 + .7 × High School GPA −.0001 × SAT.
It is tempting to say (and many people would say) that the coefficient for
SAT score has the “wrong sign,” because it says that higher values of SAT

10
CHAPTER 1 Multiple Linear Regression
are associated with lower values of college GPA. This is not correct. The
problem is that it is likely in this context that what an analyst would find
intuitive is the marginal relationship between college GPA and SAT score alone
(ignoring all else), one that we would indeed expect to be a direct (positive)
one. The regression coefficient does not say anything about that marginal
relationship. Rather, it refers to the conditional (sometimes called partial)
relationship that takes the high school GPA as fixed, which is apparently
that higher values of SAT are associated with lower values of college GPA,
holding high school GPA fixed. High school GPA and SAT are no doubt
related to each other, and it is quite likely that this relationship between
the predictors would complicate any understanding of, or intuition about,
the conditional relationship between college GPA and SAT score. Multiple
regression coefficients should not be interpreted marginally; if you really are
interested in the relationship between the target and a single predictor alone,
you should simply do a regression of the target on only that variable. This
does not mean that multiple regression coefficients are uninterpretable, only
that care is necessary when interpreting them.
Another common use of multiple regression that depends on this con-
ditional interpretation of the coefficients is to explicitly include “control”
variables in a model in order to try to account for their effect statistically. This
is particularly important in observational data (data that are not the result of a
designed experiment), since in that case, the effects of other variables cannot be
ignored as a result of random assignment in the experiment. For observational
data it is not possible to physically intervene in the experiment to “hold other
variables fixed,” but the multiple regression framework effectively allows this
to be done statistically.
Having said this, we must recognize that in many situations, it is impossible
from a practical point of view to change one predictor while holding all else
fixed. Thus, while we would like to interpret a coefficient as accounting for the
presence of other predictors in a physical sense, it is important (when dealing
with observational data in particular) to remember that linear regression is at
best only an approximation to the actual underlying random process.
1.3.2
MEASURING THE STRENGTH OF THE REGRESSION
RELATIONSHIP
The least squares estimates possess an important property:
n

i=1
(yi −Y )2 =
n

i=1
(yi −ˆyi)2 +
n

i=1
(ˆyi −Y )2.
This formula says that the variability in the target variable (the left side of
the equation, termed the corrected total sum of squares) can be split into two
mutually exclusive parts — the variability left over after doing the regression
(the first term on the right side, the residual sum of squares), and the variability
accounted for by doing the regression (the second term, the regression sum of

1.3 Methodology
11
squares). This immediately suggests the usefulness of R2 as a measure of the
strength of the regression relationship, where
R2 =
	
i(ˆyi −Y )2
	
i(yi −Y )2 ≡
Regression SS
Corrected total SS = 1 −
Residual SS
Corrected total SS.
The R2 value (also called the coefficient of determination) estimates the
population proportion of variability in y accounted for by the best linear
combination of the predictors. Values closer to 1 indicate a good deal of
predictive power of the predictors for the target variable, while values closer
to 0 indicate little predictive power. An equivalent representation of R2 is
R2 = corr(yi, ˆyi)2,
where
corr(yi, ˆyi) =
	
i(yi −Y )(ˆyi −ˆY )

	
i(yi −Y )2	
i(ˆyi −ˆY )2
is the sample correlation coefficient between y and ˆy (this correlation is called
the multiple correlation coefficient). That is, R2 is a direct measure of how
similar the observed and fitted target values are.
It can be shown that R2 is biased upwards as an estimate of the population
proportion of variability accounted for by the regression. The adjusted R2
corrects this bias, and equals
R2
a = R2 −
p
n −p −1(1 −R2).
(1.7)
It is apparent from (1.7) that unless p is large relative to n −p −1 (that is,
unless the number of predictors is large relative to the sample size), R2 and
R2
a will be close to each other, and the choice of which to use is a minor
concern. What is perhaps more interesting is the nature of R2
a as providing an
explicit tradeoff between the strength of the fit (the first term, with larger R2
corresponding to stronger fit and larger R2
a) and the complexity of the model
(the second term, with larger p corresponding to more complexity and smaller
R2
a). This tradeoff of fidelity to the data versus simplicity will be important in
the discussion of model selection in Section 2.3.1.
The only parameter left unaccounted for in the estimation scheme is the
variance of the errors σ2. An unbiased estimate is provided by the residual
mean square,
ˆσ2 =
	n
i=1 (yi −ˆyi)2
n −p −1
.
(1.8)
This estimate has a direct, but often underappreciated, use in assessing
the practical importance of the model. Does knowing x1, . . . , xp really
say anything of value about y? This isn’t a question that can be answered
completely statistically; it requires knowledge and understanding of the data
and the underlying random process (that is, it requires context). Recall that
the model assumes that the errors are normally distributed with standard

12
CHAPTER 1 Multiple Linear Regression
deviation σ. This means that, roughly speaking, 95% of the time an observed
y value falls within ±2σ of the expected response
E(y) = β0 + β1x1 + · · · + βpxp.
E(y) can be estimated for any given set of x values using
ˆy = ˆβ0 + ˆβ1x1 + · · · + ˆβpxp,
while the square root of the residual mean square (1.8), termed the standard
error of the estimate, provides an estimate of σ that can be used in constructing
this rough prediction interval ±2ˆσ.
1.3.3
HYPOTHESIS TESTS AND CONFIDENCE INTERVALS
FOR β
There are two types of hypothesis tests of immediate interest related to the
regression coefficients.
1. Do any of the predictors provide predictive power for the target variable?
This is a test of the overall significance of the regression,
H0 : β1 = · · · = βp = 0
versus
Ha : at least one βj ̸= 0,
j = 1, . . . , p.
The test of these hypotheses is the F-test,
F = Regression MS
Residual MS
≡
Regression SS/p
Residual SS/(n −p −1).
This is referenced against a null F-distribution on (p, n −p −1) degrees
of freedom.
2. Given the other variables in the model, does a particular predictor provide
additional predictive power? This corresponds to a test of the significance
of an individual coefficient,
H0 : βj = 0,
j = 1, . . . , p
versus
Ha : βj ̸= 0.
This is tested using a t-test,
tj =
ˆβj

s.e.(ˆβj)
,
which is compared to a t-distribution on n −p −1 degrees of freedom.
Other values of βj can be specified in the null hypothesis (say βj0), with
the t-statistic becoming
tj =
ˆβj −βj0

s.e.(ˆβj)
.
(1.9)

1.3 Methodology
13
The values of 
s.e.(ˆβj) are obtained as the square roots of the diagonal ele-
ments of ˆV (ˆβ) = (X′X)−1ˆσ2, where ˆσ2 is the residual mean square (1.8).
Note that for simple regression (p = 1), the hypotheses corresponding to
the overall significance of the model and the significance of the predictor
are identical,
H0 : β1 = 0
versus
Ha : β1 ̸= 0.
Given the equivalence of the sets of hypotheses, it is not surprising that
the associated tests are also equivalent; in fact, F = t2
1, and the associated
tail probabilities of the two tests are identical.
A t-test for the intercept also can be constructed as in (1.9), although this
does not refer to a hypothesis about a predictor, but rather about whether
the expected target is equal to a specified value β00 if all of the predictors
equal zero. As was noted in Section 1.3.1, this is often not physically
meaningful (and therefore of little interest), because the condition that all
predictors equal zero cannot occur, or does not come close to occurring
in the observed data.
As is always the case, a confidence interval provides an alternative way of
summarizing the degree of precision in the estimate of a regression parameter.
A 100 × (1 −α)% confidence interval for βj has the form
ˆβj ± tn−p−1
α/2

s.e.(ˆβj),
where tn−p−1
α/2
is the appropriate critical value at two-sided level α for a
t-distribution on n −p −1 degrees of freedom.
1.3.4
FITTED VALUES AND PREDICTIONS
The rough prediction interval ˆy ± 2ˆσ discussed in Section 1.3.2 is an approx-
imate 95% interval because it ignores the variability caused by the need to
estimate σ and uses only an approximate normal-based critical value. A more
accurate assessment of predictive power is provided by a prediction interval
given a particular value of x. This interval provides guidance as to how precise
ˆy0 is as a prediction of y for some particular specified value x0, where ˆy0
is determined by substituting the values x0 into the estimated regression
equation. Its width depends on both ˆσ and the position of x0 relative to the
centroid of the predictors (the point located at the means of all predictors),
since values farther from the centroid are harder to predict as precisely. Specif-
ically, for a simple regression, the estimated standard error of a predicted value
based on a value x0 of the predicting variable is

s.e.(ˆyP
0 ) = ˆσ

1 + 1
n +
(x0 −X)2
	 (xi −X)2 .

14
CHAPTER 1 Multiple Linear Regression
More generally, the variance of a predicted value is
ˆV (ˆyP
0 ) = [1 + x′
0(X′X)−1x0]ˆσ2.
(1.10)
Here x0 is taken to include a 1 in the first entry (corresponding to the intercept
in the regression model). The prediction interval is then
ˆy0 ± tn−p−1
α/2

s.e.(ˆyP
0 ),
where 
s.e.(ˆyP
0 ) =

ˆV (ˆyP
0 ).
This prediction interval should not be confused with a confidence
interval for a fitted value. The prediction interval is used to provide an
interval estimate for a prediction of y for one member of the population with a
particular value of x0; the confidence interval is used to provide an interval
estimate for the true expected value of y for all members of the population with a
particular value of x0. The corresponding standard error, termed the standard
error for a fitted value, is the square root of
ˆV (ˆyF
0 ) = x′
0(X′X)−1x0ˆσ2,
(1.11)
with corresponding confidence interval
ˆy0 ± tn−p−1
α/2

s.e.(ˆyF
0 ).
A comparison of the two estimated variances (1.10) and (1.11) shows that the
variance of the predicted value has an extra σ2 term, which corresponds to
the inherent variability in the population. Thus, the confidence interval for a
fitted value will always be narrower than the prediction interval, and is often
much narrower (especially for large samples), since increasing the sample size
will always improve estimation of the expected response value, but cannot
lessen the inherent variability in the population associated with the prediction
of the target for a single observation.
1.3.5
CHECKING ASSUMPTIONS USING RESIDUAL PLOTS
All of these tests, intervals, predictions, and so on, are based on the belief that
the assumptions of the regression model hold. Thus, it is crucially important
that these assumptions be checked. Remarkably enough, a few very simple
plots can provide much of the evidence needed to check the assumptions.
1. A plot of the residuals versus the fitted values. This plot should have no
pattern to it; that is, no structure should be apparent. Certain kinds of
structure indicate potential problems:
(a) A point (or a few points) isolated at the top or bottom, or left or
right. In addition, often the rest of the points have a noticeable “tilt”
to them. These isolated points are unusual observations and can have
a strong effect on the regression. They need to be examined carefully
and possibly removed from the data set.

1.4 Example — Estimating Home Prices
15
(b) An impression of different heights of the point cloud as the plot is
examined from left to right. This indicates potential heteroscedasticity
(nonconstant variance).
2. Plots of the residuals versus each of the predictors. Again, a plot with no
apparent structure is desired.
3. If the data set has a time structure to it, residuals should be plotted in time
order. Again, there should be no apparent pattern. If there is a cyclical
structure, this indicates that the errors are not uncorrelated, as they are
supposed to be (that is, there is potentially autocorrelation in the errors).
4. A normal plot of the residuals. This plot assesses the apparent normality
of the residuals, by plotting the observed ordered residuals on one axis
and the expected positions (under normality) of those ordered residuals
on the other. The plot should look like a straight line (roughly). Isolated
points once again represent unusual observations, while a curved line
indicates that the errors are probably not normally distributed, and tests
and intervals might not be trustworthy.
Note that all of these plots should be routinely examined in any regression
analysis, although in order to save space not all will necessarily be presented in
all of the analyses in the book.
An implicit assumption in any model that is being used for prediction
is that the future “looks like” the past; that is, it is not sufficient that these
assumptions appear to hold for the available data, as they also must continue
to hold for new data on which the estimated model is applied. Indeed, the
assumption is stronger than that, since it must be the case that the future
is exactly the same as the past, in the sense that all of the properties of the
model, including the precise values of all of the regression parameters, are the
same. This is unlikely to be exactly true, so a more realistic point of view is
that the future should be similar enough to the past so that predictions based
on the past are useful. A related point is that predictions should not be based
on extrapolation, where the predictor values are far from the values used to
build the model. Similarly, if the observations form a time series, predictions
far into the future are unlikely to be very useful.
In general, the more complex a model is, the less likely it is that all
of its characteristics will remain stable going forward, which implies that a
reasonable goal is to try to find a model that is as simple as it can be while
still accounting for the important effects in the data. This leads to questions
of model building, which is the subject of Chapter 2.
1.4
Example — Estimating Home Prices
Determining the appropriate sale price for a home is clearly of great interest
to both buyers and sellers. While this can be done in principle by examining
the prices at which other similar homes have recently sold, the well-known

16
CHAPTER 1 Multiple Linear Regression
existence of strong effects related to location means that there are likely to
be relatively few homes with the same important characteristics to make the
comparison. A solution to this problem is the use of hedonic regression models,
where the sale prices of a set of homes in a particular area are regressed on
important characteristics of the home such as the number of bedrooms, the
living area, the lot size, and so on. Academic research on this topic is plentiful,
going back to at least Wabe (1971).
This analysis is based on a sample from public data on sales of one-family
homes in the Levittown, NY area from June 2010 through May 2011.
Levittown is famous as the first planned suburban community built using
mass production methods, being aimed at former members of the military
after World War II. Most of the homes in this community were built in the
late 1940s to early 1950s, without basements and designed to make expansion
on the second floor relatively easy.
For each of the 85 houses in the sample, the number of bedrooms, number
of bathrooms, living area (in square feet), lot size (in square feet), the year
the house was built, and the property taxes are used as potential predictors
of the sale price. In any analysis the first step is to look at the data, and
Figure 1.4 gives scatter plots of sale price versus each predictor. It is apparent
that there is a positive association between sale price and each variable, other
than number of bedrooms and lot size. We also note that there are two houses
with unusually large living areas for this sample, two with unusually large
3.0
3.5
4.0
4.5
5.0
2e + 05
4e + 05
Number of bedrooms
Sale price
2e + 05
4e + 05
Sale price
2e + 05
4e + 05
Sale price
2e + 05
4e + 05
Sale price
2e + 05
4e + 05
Sale price
2e + 05
4e + 05
Sale price
1.0
1.5
2.0
2.5
3.0
Number of bathrooms
1000
1500
2000
2500
3000
Living area
6000
7000
8000
9000
10000 11000
Lot size
1948 1950 1952 1954 1956 1958 1960 1962
Year built
2000
4000
6000
8000 10000 12000 14000
Property taxes
FIGURE 1.4: Scatter plots of sale price versus each predictor for the home
price data.

1.4 Example — Estimating Home Prices
17
property taxes (these are not the same two houses), and three that were built
six or seven years later than all of the other houses in the sample.
The output below summarizes the results of a multiple regression fit.
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
-7.149e+06
3.820e+06
-1.871 0.065043 .
Bedrooms
-1.229e+04
9.347e+03
-1.315 0.192361
Bathrooms
5.170e+04
1.309e+04
3.948 0.000171 ***
Living.area
6.590e+01
1.598e+01
4.124 9.22e-05 ***
Lot.size
-8.971e-01
4.194e+00
-0.214 0.831197
Year.built
3.761e+03
1.963e+03
1.916 0.058981 .
Property.tax
1.476e+00
2.832e+00
0.521 0.603734
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 47380 on 78 degrees of freedom
Multiple R-squared: 0.5065,
Adjusted R-squared: 0.4685
F-statistic: 13.34 on 6 and 78 DF,
p-value: 2.416e-10
The overall regression is strongly statistically significant, with the tail
probability of the F-test roughly 10−10. The predictors account for roughly
50% of the variability in sale prices (R2 ≈0.5). Two of the predictors (number
of bathrooms and living area) are highly statistically significant, with tail
probabilities less than .0002, and the coefficient of the year built variable is
marginally statistically significant. The coefficients imply that given all else
in the model is held fixed, one additional bathroom in a house is associated
with an estimated expected price that is $51,700 higher; one additional square
foot of living area is associated with an estimated expected price that is $65.90
higher (given the typical value of the living area variable, a more meaningful
statement would probably be that an additional 100 square feet of living area
is associated with an estimated expected price that is $659 higher); and a house
being built one year later is associated with an estimated expected price that is
$3761 higher.
This is a situation where the distinction between a confidence interval
for a fitted value and a prediction interval (and which is of more interest to
a particular person) is clear. Consider a house with 3 bedrooms, 1 bathroom,
1050 square feet of living area, 6000 square foot lot size, built in 1948, with
$6306 in property taxes. Substituting those values into the above equation
gives an estimated expected sale price of a house with these characteristics equal
to $265,360. A buyer or a seller is interested in the sale price of one particular
house, so a prediction interval for the sale price would provide a range for
what the buyer can expect to pay and the seller expect to get. The standard
error of the estimate ˆσ = $47,380 can be used to construct a rough prediction
interval, in that roughly 95% of the time a house with these characteristics can
be expected to sell for within ±(2)(47380) = ±$94,360 of that estimated sale
price, but a more exact interval might be required. On the other hand, a home
appraiser or tax assessor is more interested in the typical (average) sale price

18
CHAPTER 1 Multiple Linear Regression
for all homes of that type in the area, so they can give a justifiable interval
estimate giving the precision of the estimate of the true expected value of the
house, so a confidence interval for the fitted value is desired.
Exact 95% intervals for a house with these characteristics can be obtained
from statistical software, and turn out to be ($167277, $363444) for the
prediction interval and ($238482, $292239) for the confidence interval. As
expected, the prediction interval is much wider than the confidence interval,
since it reflects the inherent variability in sale prices in the population of
houses; indeed, it is probably too wide to be of any practical value in this case,
but an interval with smaller coverage (that is expected to include the actual
price only 50% of the time, say) might be useful (a 50% interval in this case
would be ($231974, $298746), so a seller could be told that there is a 50/50
chance that their house will sell for a value in this range).
The validity of all of these results depends on whether the assumptions
hold. Figure 1.5 gives a scatter plot of the residuals versus the fitted values
and a normal plot of the residuals for this model fit. There is no apparent
pattern in the plot of residuals versus fitted values, and the ordered residuals
form a roughly straight line in the normal plot, so there are no apparent
violations of assumptions here. The plot of residuals versus each of the
predictors (Figure 1.6) also does not show any apparent patterns, other than
the houses with unusual living area and year being built, respectively. It would
be reasonable to omit these observations to see if they have had an effect on
the regression, but we will postpone discussion of that to Chapter 3, where
diagnostics for unusual observations are discussed in greater detail.
An obvious consideration at this point is that the models discussed here
appear to be overspecified; that is, they include variables that do not apparently
add to the predictive power of the model. As was noted earlier, this suggests
the consideration of model building, where a more appropriate (simplified)
model can be chosen, which will be discussed in Chapter 2.
250000
350000
450000
−1e + 05
0e + 00
1e + 05
−1e + 05
0e + 00
1e + 05
(a)
Fitted values
Residuals
−2
−1
0
1
2
(b)
Theoretical Quantiles
Sample Quantiles
FIGURE 1.5: Residual plots for the home price data. (a) Plot of residuals
versus fitted values. (b) Normal plot of the residuals.

1.5 Summary
19
3.0
3.5
4.0
4.5
5.0
−1e + 05
0e + 00
1e + 05
Number of bedrooms
Residuals
−1e + 05
0e + 00
1e + 05
Residuals
−1e + 05
0e + 00
1e + 05
Residuals
−1e + 05
0e + 00
1e + 05
Residuals
−1e + 05
0e + 00
1e + 05
Residuals
−1e + 05
0e + 00
1e + 05
Residuals
1.0
1.5
2.0
2.5
3.0
Number of bathrooms
1000
1500
2000
2500
3000
Living area
6000
7000
8000
9000
10000
11000
Lot size
1948 1950 1952 1954 1956 1958 1960 1962
Year built
2000
4000
6000
8000
10000 12000 14000
Property taxes
FIGURE 1.6: Scatter plots of residuals versus each predictor for the home
price data.
1.5
Summary
In this chapter we have laid out the basic structure of the linear regression
model, including the assumptions that justify the use of least squares estima-
tion. The three main goals of regression noted at the beginning of the chapter
provide a framework for an organization of the topics covered.
1. Modeling the relationship between x and y:
• the least squares estimates ˆβ summarize the expected change in y for a
given change in an x, accounting for all of the variables in the model;
• the standard error of the estimate ˆσ estimates the standard deviation of
the errors;
• R2 and R2
a estimate the proportion of variability in y accounted for by
x;
• and the confidence interval for a fitted value provides a measure of the
precision in estimating the expected target for a given set of predictor
values.

20
CHAPTER 1 Multiple Linear Regression
2. Prediction of the target variable:
• substituting specified values of x into the fitted regression model gives
an estimate of the value of the target for a new observation;
• the rough prediction interval ±2ˆσ provides a quick measure of the
limits of the ability to predict a new observation;
• and the exact prediction interval provides a more precise measure of
those limits.
3. Testing of hypotheses:
• the F-test provides a test of the statistical significance of the overall
relationship;
• the t-test for each slope coefficient testing whether the true value is zero
provides a test of whether the variable provides additional predictive
power given the other variables;
• and the t-tests can be generalized to test other hypotheses of interest
about the coefficients as well.
Since all of these methods depend on the assumptions holding, a fun-
damental part of any regression analysis is to check those assumptions. The
residual plots discussed in this chapter are a key part of that process, and
other diagnostics and tests will be discussed in future chapters that provide
additional support for that task.
KEY TERMS
Autocorrelation: Correlation between adjacent observations in a (time) series.
In the regression context it is autocorrelation of the errors that is a violation of
assumptions.
Coefficient of determination (R2): The square of the multiple correlation
coefficient, estimates the proportion of variability in the target variable that is
explained by the predictors in the linear model.
Confidence interval for a fitted value: A measure of precision of the estimate
of the expected target value for a given x.
Dependent variable: Characteristic of each member of the sample that is
being modeled. This is also known as the target or response variable.
Fitted value: The least squares estimate of the expected target value for a
particular observation obtained from the fitted regression model.
Heteroscedasticity: Unequal variance; this can refer to observed unequal
variance of the residuals or theoretical unequal variance of the errors.
Homoscedasticity: Equal variance; this can refer to observed equal variance
of the residuals or the assumed equal variance of the errors.
Independent variable(s): Characteristic(s) of each member of the sample that
could be used to model the dependent variable. These are also known as the
predicting variables.

1.5 Summary
21
Least squares: A method of estimation that minimizes the sum of squared
deviations of the observed target values from their estimated expected values.
Prediction interval: The interval estimate for the value of the target variable
for an individual member of the population using the fitted regression model.
Residual: The difference between the observed target value and the corre-
sponding fitted value.
Residual mean square: An unbiased estimate of the variance of the errors.
It is obtained by dividing the sum of squares of the residuals by (n −p −1),
where n is the number of observations and p is the number of predicting
variables.
Standard error of the estimate (ˆσ): An estimate of σ, the standard deviation
of the errors, equaling the square root of the residual mean square.

Two
Chapter
Model Building
2.1
Introduction
23
2.2
Concepts and Background Material
24
2.2.1
Using Hypothesis Tests to Compare Models
24
2.2.2
Collinearity
26
2.3
Methodology
29
2.3.1
Model Selection
29
2.3.2
Example — Estimating Home Prices (continued)
31
2.4
Indicator Variables and Modeling Interactions
38
2.4.1
Example — Electronic Voting and the 2004
Presidential Election
40
2.5
Summary
46
2.1
Introduction
All of the discussion in Chapter 1 is based on the premise that the only
model being considered is the one currently being fit. This is not a good data
analysis strategy, for several reasons.
1. Including unnecessary predictors in the model (what is sometimes called
overfitting) complicates descriptions of the process. Using such models
tends to lead to poorer predictions because of the additional unnecessary
noise. Further, a more complex representation of the true regression
relationship is less likely to remain stable enough to be useful for future
prediction than is a simpler one.
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
23

24
CHAPTER 2 Model Building
2. Omitting important effects (underfitting) reduces predictive power,
biases estimates of effects for included predictors, and results in less
understanding of the process being studied.
3. Violations of assumptions should be addressed, so that least squares
estimation is justified.
The last of these reasons is the subject of later chapters, while the first
two are discussed in this chapter. This operation of choosing among different
candidate models so as to avoid overfitting and underfitting is called model
selection.
First, we discuss the uses of hypothesis testing for model selection.
Various hypothesis tests address relevant model selection questions, but there
are also reasons why they are not sufficient for these purposes. Part of these
difficulties is the effect of correlations among the predictors, and the situation
of high correlation among the predictors (collinearity) is a particularly
challenging one.
A useful way of thinking about the tradeoffs of overfitting versus under-
fitting is as a contrast between strength of fit and simplicity. The principle
of parsimony states that a model should be as simple as possible while still
accounting for the important relationships in the data. Thus, a sensible way of
comparing models is using measures that explicitly reflect this tradeoff; such
measures are discussed in Section 2.3.1.
The chapter concludes with a discussion of techniques designed to address
the existence of well-defined subgroups in the data. In this situation, it is
often the case that the effects of a predictor on the target variable is different
in the two groups, and ways of building models to handle this are discussed in
Section 2.4.
2.2
Concepts and Background Material
2.2.1
USING HYPOTHESIS TESTS TO COMPARE MODELS
Determining whether individual regression coefficients are statistically sig-
nificant (as discussed in Section 1.3.3) is an obvious first step in deciding
whether a model is overspecified. A predictor that does not add significantly
to model fit should have an estimated slope coefficient that is not significantly
different from 0, and is thus identified by a small t-statistic. So, for example,
in the analysis of home prices in Section 1.4, the regression output on page 17
suggests removing number of bedrooms, lot size, and property taxes from the
model, as all three have insignificant t-values.
Recall that t-tests can only assess the contribution of a predictor given all of
the others in the model. When predictors are correlated with each other, t-tests
can give misleading indications of the importance of a predictor. Consider a
two-predictor situation where the predictors are each highly correlated with
the target variable, and are also highly correlated with each other. In this

2.2 Concepts and Background Material
25
situation, it is likely that the t-statistic for each predictor will be relatively
small. This is not an inappropriate result, since given one predictor the other
adds little (being highly correlated with each other, one is redundant in the
presence of the other). This means that the t-statistics are not effective in
identifying important predictors when the two variables are highly correlated.
The t-tests and F-test of Section 1.3.3 are special cases of a general
formulation that is useful for comparing certain classes of models. It might be
the case that a simpler version of a candidate model (a subset model) might
be adequate to fit the data. For example, consider taking a sample of college
students and determining their college grade point average (GPA), Scholastic
Aptitude Test (SAT) evidence-based reading and writing score (Reading),
and SAT math score (Math). The full regression model to fit to these data is
GPAi = β0 + β1Readingi + β2Mathi + εi.
Instead of considering reading and math scores separately, we could consider
whether GPA can be predicted by one variable: total SAT score, which is the
sum of Reading and Math. This subset model is
GPAi = γ0 + γ1(Reading + Math)i + εi,
with β1 = β2 ≡γ1. This equality condition is called a linear restriction,
because it defines a linear condition on the parameters of the regression model
(that is, it only involves additions, subtractions, and equalities of coefficients
and constants).
The question about whether the total SAT score is sufficient to predict
grade point average can be stated using a hypothesis test about this linear
restriction. As always, the null hypothesis gets the benefit of the doubt; in this
case, that is the simpler restricted (subset) model that the sum of Reading
and Math is adequate, since it says that only one predictor is needed, rather
than two. The alternative hypothesis is the unrestricted full model (with no
conditions on β). That is,
H0 : β1 = β2
versus
Ha : β1 ̸= β2.
These hypotheses are tested using a partial F-test. The F-statistic has the
form
F = (Residual SSsubset −Residual SSfull)/d
Residual SSfull/(n −p −1)
,
(2.1)
where n is the sample size, p is the number of predictors in the full model, and
d is the difference between the number of parameters in the full model and
the number of parameters in the subset model. This statistic is compared to
an F distribution on (d, n −p −1) degrees of freedom. So, for example, for
this GPA/SAT example, p = 2 and d = 3 −2 = 1, so the observed F-statistic
would be compared to an F distribution on (1, n −3) degrees of freedom.
Some statistical packages allow specification of the full and subset models and

26
CHAPTER 2 Model Building
will calculate the F-test, but others do not, and the statistic has to be calculated
manually based on the fits of the two models.
An alternative form for the F-test above might make clearer what is going
on here:
F =
(R2
full −R2
subset)/d
(1 −R2
full)/(n −p −1).
That is, if the strength of the fit of the full model (measured by R2) isn’t
much larger than that of the subset model, the F-statistic is small, and we do
not reject the subset model; if, on the other hand, the difference in R2 values
is large (implying that the fit of the full model is noticeably stronger), we do
reject the subset model in favor of the full model.
The F-statistic to test the overall significance of the regression is a special
case of this construction (with restriction β1 = · · · = βp = 0), as is each of the
individual t-statistics that test the significance of any variable (with restriction
βj = 0). In the latter case Fj = t2
j.
2.2.2
COLLINEARITY
Recall that the importance of a predictor can be difficult to assess using t-tests
when predictors are correlated with each other. A related issue is that of
collinearity (sometimes somewhat redundantly referred to as multicollinear-
ity), which refers to the situation when (some of) the predictors are highly
correlated with each other. The presence of predicting variables that are highly
correlated with each other can lead to instability in the regression coefficients,
increasing their standard errors, and as a result the t-statistics for the variables
can be deflated. This can be seen in Figure 2.1. The two plots refer to identical
data sets, other than the one data point that is lightly colored. Dropping
the data points down to the (x1, x2) plane makes clear the high correlation
between the predictors. The estimated regression plane changes from
ˆy = 9.906 −2.514x1 + 6.615x2
in the top plot to
ˆy = 9.748 + 9.315x1 −5.204x2
in the bottom plot; a small change in only one data point causes a major
change in the estimated regression function.
Thus, from a practical point of view, collinearity leads to two problems.
First, it can happen that the overall F-statistic is significant, yet each of the
individual t-statistics is not significant (more generally, the tail probability for
the F-test is considerably smaller than those of any of the individual coefficient
t-tests). Second, if the data are changed only slightly, the fitted regression
coefficients can change dramatically. Note that while collinearity can have a
large effect on regression coefficients and associated t-statistics, it does not
have a large effect on overall measures of fit like the overall F-test or R2, since
adding unneeded variables (whether or not they are collinear with predictors

2.2 Concepts and Background Material
27
−2
0
2
4
6
8
10
0
10
20
30
40
50
−2 0
2
4
6
8
10
x2
−2 0
2
4
6
8
10
x2
x1
x1
y
0
10
20
30
40
50
y
−2
0
2
4
6
8
10
FIGURE 2.1: Least squares estimation under collinearity. The only change
in the data sets is the lightly colored data point. The planes are the estimated
least squares fits.
already in the model) cannot increase the residual sum of squares (it can only
decrease it or leave it roughly the same).
Another problem with collinearity comes from attempting to use a fitted
regression model for prediction. As was noted in Chapter 1, simple models tend
to forecast better than more complex ones, since they make fewer assumptions
about what the future will look like. If a model exhibiting collinearity is used
for future prediction, the implicit assumption is that the relationships among
the predicting variables, as well as their relationship with the target variable,
remain the same in the future. This is less likely to be true if the predicting
variables are collinear.
How can collinearity be diagnosed? The two-predictor model
yi = β0 + β1x1i + β2x2i + εi
provides some guidance. It can be shown that in this case
var(ˆβ1) = σ2
 n

i=1
x2
1i(1 −r2
12)
−1
and
var(ˆβ2) = σ2
 n

i=1
x2
2i(1 −r2
12)
−1
,

28
CHAPTER 2 Model Building
Table 2.1: Variance
inflation caused by
correlation of predictors in
a two-predictor model.
r12
Variance
inflation
0.00
1.00
0.50
1.33
0.70
1.96
0.80
2.78
0.90
5.26
0.95
10.26
0.97
16.92
0.99
50.25
0.995
100.00
0.999
500.00
where r12 is the correlation between x1 and x2. Note that as collinearity
increases (r12 →±1), both variances tend to ∞. This effect is quantified in
Table 2.1.
This ratio describes by how much the variances of the estimated slope
coefficients are inflated due to observed collinearity relative to when the
predictors are uncorrelated. It is clear that when the correlation is high, the
variability (and hence the instability) of the estimated slopes can increase
dramatically.
A diagnostic to determine this in general is the variance inflation factor
(V IF) for each predicting variable, which is defined as
V IFj =
1
1 −R2
j
,
where R2
j is the R2 of the regression of the variable xj on the other predicting
variables. V IFj gives the proportional increase in the variance of ˆβj compared
to what it would have been if the predicting variables had been uncorrelated.
There are no formal cutoffs as to what constitutes a large V IF, but collinearity
is generally not a problem if the observed V IF satisfies
V IF < max

10,
1
1 −R2
model

,
where R2
model is the usual R2 for the regression fit. This means that either the
predictors are more related to the target variable than they are to each other, or
they are not related to each other very much. In either case coefficient estimates

2.3 Methodology
29
are not very likely to be very unstable, so collinearity is not a problem. If
collinearity is present, a simplified model should be considered, but this is
only a general guideline; sometimes two (or more) collinear predictors might
be needed in order to adequately model the target variable. In the next section
we discuss a methodology for judging the adequacy of fitted models and
comparing them.
2.3
Methodology
2.3.1
MODEL SELECTION
We saw in Section 2.2.1 that hypothesis tests can be used to compare models.
Unfortunately, there are several reasons why such tests are not adequate for the
task of choosing among a set of candidate models for the appropriate model
to use.
In addition to the effects of correlated predictors on t-tests noted earlier,
partial F-tests only can compare models that are nested (that is, where one is
a special case of the other). Comparing a model based on {x1, x3, x5} to one
based on {x2, x4}, for example, is clearly important, but is impossible using
these testing methods.
Even ignoring these issues, hypothesis tests don’t necessarily address the
question a data analyst is most interested in. With a large enough sample,
almost any estimated slope will be significantly different from zero, but
that doesn’t mean that the predictor provides additional useful predictive
power. Similarly, in small samples, important effects might not be statistically
significant at typical levels simply because of insufficient data. That is, there is
a clear distinction between statistical significance and practical importance.
In this section we discuss a strategy for determining a “best” model (or
more correctly, a set of “best” models) among a larger class of candidate
models, using objective measures designed to reflect a predictive point of
view. As a first step, it is good to explicitly identify what should not be done.
In recent years, it has become commonplace for databases to be constructed
with hundreds (or thousands) of variables and hundreds of thousands (or
millions) of observations. It is tempting to avoid issues related to choosing the
potential set of candidate models by considering all of the variables as potential
predictors in a regression model, limited only by available computing power.
This would be a mistake. If too large a set of possible predictors is considered,
it is very likely that variables will be identified as important just due to random
chance. Since they do not reflect real relationships in the population, models
based on them will predict poorly in the future, and interpretations of slope
coefficients will just be mistaken explanations of what is actually random
behavior. This sort of overfitting is known as “data dredging” and is among
the most serious dangers when analyzing data.
The set of possible models should ideally be chosen before seeing any
data based on as thorough an understanding of the underlying random

30
CHAPTER 2 Model Building
process as possible. Potential predictors should be justifiable on theoretical
grounds if at all possible. This is by necessity at least somewhat subjective,
but good basic principles exist. Potential models to consider should be based
on the scientific literature and previous relevant experiments. In particular, if
a model simply doesn’t “make sense,” it shouldn’t be considered among the
possible candidates. That does not mean that modifications and extensions
of models that are suggested by the analysis should be ignored (indeed, this
is the subject of the next three chapters), but an attempt to keep models
grounded in what is already understood about the underlying process is always
a good idea.
What do we mean by the (or a) “best” model? As was stated on page 4,
there is no “true” model, since any model is only a representation of reality
(or equivalently, the true model is too complex to be modeled usefully). Since
the goal is not to find the “true” model, but rather to find a model or set of
models that best balances fit and simplicity, any strategy used to guide model
selection should be consistent with this principle. The goal is to provide a
good predictive model that also provides useful descriptions of the process
being studied from estimated parameters.
Once a potential set of predictors is chosen, most statistical packages
include the capability to produce summary statistics for all possible regression
models using those predictors. Such algorithms (often called best subsets
algorithms) do not actually look at all possible models, but rather list statistics
for only the models with strongest fits for each number of predictors in the
model. Such a listing can then be used to determine a set of potential “best”
models to consider more closely. The most common algorithm, described in
Furnival and Wilson (1974), is based on branch and bound optimization, and
while it is much less computationally intensive than examining all possible
models, it still has a practical feasible limit of roughly 30 to 35 predictors.
In Chapter 14, we discuss model selection and fitting for (potentially much)
larger numbers of predictors.
Note that model comparisons are only sensible when based on the same
data set. Most statistical packages drop any observations that have missing data
in any of the variables in the model. If a data set has missing values scattered
over different predictors, the set of observations with complete data will change
depending on which variables are in the model being examined, and model
comparison measures will not be comparable. One way around this is to only
use observations with complete data for all variables under consideration, but
this can result in discarding a good deal of available information for any
particular model.
Tools like best subsets by their very nature are likely to be more effective
when there are a relatively small number of useful predictors that have relatively
strong effects, as opposed to a relatively large number of predictors that have
relatively weak effects. The strict present/absent choice for a predictor is
consistent with true relationships with either zero or distinctly nonzero
slopes, as opposed to many slopes that are each nonzero but also not far
from zero.

2.3 Methodology
31
2.3.2
EXAMPLE — ESTIMATING HOME PRICES (CONTINUED)
Consider again the home price data examined in Section 1.4. We repeat the
regression output from the model based on all of the predictors below:
Coefficients:
Estimate Std.Error t value Pr(>|t|)
VIF
(Intercept)
-7.149e+06 3.820e+06
-1.871 0.065043
.
Bedrooms
-1.229e+04 9.347e+03
-1.315 0.192361 1.262
Bathrooms
5.170e+04 1.309e+04
3.948 0.000171 1.420 ***
Living.area
6.590e+01 1.598e+01
4.124 9.22e-05 1.661 ***
Lot.size
-8.971e-01 4.194e+00
-0.214 0.831197 1.074
Year.built
3.761e+03 1.963e+03
1.916 0.058981 1.242 .
Property.tax
1.476e+00 2.832e+00
0.521 0.603734 1.300
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 47380 on 78 degrees of freedom
Multiple R-squared: 0.5065,
Adjusted R-squared: 0.4685
F-statistic: 13.34 on 6 and 78 DF,
p-value: 2.416e-10
This is identical to the output given earlier, except that variance infla-
tion factor (V IF) values are given for each predictor. It is apparent that
there is virtually no collinearity among these predictors (recall that 1 is the
minimum possible value of the V IF), which should make model selec-
tion more straightforward. The following output summarizes a best subsets
fitting:
P
L
r
i
Y o
B v
e p
B a i L a e
e t n o r r
d h g t . t
r r . . b y
o o a s u .
o o r i i t
Mallows
m m e z l a
Vars
R-Sq
R-Sq(adj)
Cp
AICc
S
s s a e t x
1
35.3
34.6
21.2 1849.9 52576
X
1
29.4
28.6
30.6 1857.3 54932
X
1
10.6
9.5
60.3 1877.4 61828
X
2
46.6
45.2
5.5 1835.7 48091
X X
2
38.9
37.5
17.5 1847.0 51397
X
X
2
37.8
36.3
19.3 1848.6 51870
X
X
3
49.4
47.5
3.0 1833.1 47092
X X
X
3
48.2
46.3
4.9 1835.0 47635
X X X
3
46.6
44.7
7.3 1837.5 48346
X X
X
4
50.4
48.0
3.3 1833.3 46885
X X X
X
4
49.5
47.0
4.7 1834.8 47304
X X
X X
4
49.4
46.9
5.0 1835.1 47380
X X X X

32
CHAPTER 2 Model Building
5
50.6
47.5
5.0 1835.0 47094
X X X
X X
5
50.5
47.3
5.3 1835.2 47162
X X X X X
5
49.6
46.4
6.7 1836.8 47599
X X X X X
6
50.6
46.9
7.0 1836.9 47381
X X X X X X
Output of this type provides the tools to choose among candidate models.
The output provides summary statistics for the three models with strongest fit
for each number of predictors. So, for example, the best one-predictor model is
based on Bathrooms, while the second best is based on Living.area; the
best two-predictor model is based on Bathrooms and Living.area; and
so on. The principle of parsimony noted earlier implies moving down the table
as long as the gain in fit is big enough, but no further, thereby encouraging
simplicity. A reasonable model selection strategy would not be based on only
one possible measure, but looking at all of the measures together, using various
guidelines to ultimately focus in on a few models (or only one) that best trade
off strength of fit with simplicity, for example as follows:
1. Increase the number of predictors until the R2 value levels off. Clearly, the
highest R2 for a given p cannot be smaller than that for a smaller value of
p. If R2 levels off, that implies that additional variables are not providing
much additional fit. In this case, the largest R2 values go from roughly
35% to 47% from p = 1 to p = 2, which is clearly a large gain in fit,
but beyond that more complex models do not provide much additional
fit (particularly past p = 3). Thus, this guideline suggests choosing either
p = 2 or p = 3.
2. Choose the model that maximizes the adjusted R2. Recall from
equation (1.7) that the adjusted R2 equals
R2
a = R2 −
p
n −p −1 (1 −R2).
It is apparent that R2
a explicitly trades off strength of fit (R2) versus
simplicity [the multiplier p/(n −p −1)], and can decrease if predictors
that do not add any predictive power are added to a model. Thus, it is
reasonable to not complicate a model beyond the point where its adjusted
R2 increases. For these data, R2
a is maximized at p = 4.
The fourth column in the output refers to a criterion called Mallows’ Cp
(Mallows, 1973). This criterion equals
Cp = Residual SSp
ˆσ2∗
−n + 2p + 2,
where Residual SSp is the residual sum of squares for the model being
examined, p is the number of predictors in that model, and ˆσ2
∗is the residual
mean square based on using all p∗of the candidate predicting variables. Cp is
designed to estimate the expected squared prediction error of a model. Like R2
a,
Cp explicitly trades off strength of fit versus simplicity, with two differences:
it is now small values that are desirable, and the penalty for complexity is
stronger, in that the penalty term now multiplies the number of predictors in

2.3 Methodology
33
the model by 2, rather than by 1 (which means that using R2
a will tend to lead
to more complex models than using Cp will). This suggests another model
selection rule:
3. Choose the model that minimizes Cp. In case of tied values, the simplest
model (smallest p) would be chosen. In these data, this rule implies
choosing p = 3.
An additional operational rule for the use of Cp has been suggested. When
a particular model contains all of the necessary predictors, the residual mean
square for the model should be roughly equal to σ2. Since the model that
includes all of the predictors should also include all of the necessary ones, ˆσ2
∗
should also be roughly equal to σ2. This implies that if a model includes all of
the necessary predictors, then
Cp ≈(n −p −1)σ2
σ2
−n + 2p + 2 = p + 1.
This suggests the following model selection rule:
4. Choose the simplest model such that Cp ≈p + 1 or smaller. In these data,
this rule implies choosing p = 3.
A weakness of the Cp criterion is that its value depends on the largest
set of candidate predictors (through ˆσ2
∗), which means that adding predictors
that provide no predictive power to the set of candidate models can change
the choice of best model. A general approach that avoids this is through
the use of statistical information. A detailed discussion of the determination
of information measures is beyond the scope of this book, but Burnham
and Anderson (2002) provides extensive discussion of the topic. The Akaike
Information Criterion AIC, introduced by Akaike (1973),
AIC = n log(ˆσ2) + n log[(n −p −1)/n] + 2p + 4,
(2.2)
where the log(·) function refers to natural logs, is such a measure, and it
estimates the information lost in approximating the true model by a candidate
model. It is clear from (2.2) that minimizing AIC achieves the goal of
balancing strength of fit with simplicity, and because of the 2p term in the
criterion this will result in the choice of similar models as when minimizing
Cp. It is well known that AIC has a tendency to lead to overfitting, particularly
in small samples. That is, the penalty term in AIC designed to guard against
too complicated a model is not strong enough. A modified version of AIC
that helps address this problem is the corrected AIC,
AICc = AIC + 2(p + 2)(p + 3)
n −p −3
(2.3)
(Hurvich and Tsai, 1989). Equation (2.3) shows that (especially for small
samples) models with fewer parameters will be more strongly preferred when
minimizing AICc than when minimizing AIC, providing stronger protection
against overfitting. In large samples, the two criteria are virtually identical,

34
CHAPTER 2 Model Building
but in small samples, or when considering models with a large number of
parameters, AICc is the better choice. This suggests the following model
selection rule:
5. Choose the model that minimizes AICc. In case of tied values, the
simplest model (smallest p) would be chosen. In these data, this rule
implies choosing p = 3, although the AICc value for p = 4 is virtually
identical to that of p = 3. Note that the overall level of the AICc values
is not meaningful, and should not be compared to Cp values or values for
other data sets; it is only the value for a model for a given data set relative
to the values of others for that data set that matter.
Cp, AIC, and AICc have the desirable property that they are efficient
model selection criteria. This means that in the (realistic) situation where the
set of candidate models does not include the “true” model (that is, a good
model is just viewed as a useful approximation to reality), as the sample gets
larger the error obtained in making predictions using the model chosen using
these criteria becomes indistinguishable from the error obtained using the
best possible model among all candidate models. That is, in this large-sample
predictive sense, it is as if the best approximation was known to the data
analyst. Another well-known criterion, the Bayesian Information Criterion
BIC [which substitutes log(n) × p for 2p in (2.2)], does not have this property,
but is instead a consistent criterion. Such a criterion has the property that if
the “true” model is in fact among the candidate models the criterion will
select that model with probability approaching 1 as the sample size increases.
Thus, BIC is a more natural criterion to use if the goal is to identify the
“true” predictors with nonzero slopes (which of course presumes that there
are such things as “true” predictors in a “true” model). BIC will generally
choose simpler models than AIC because of its stronger penalty (log(n) > 2
for n ≥8), and a version BICc that adjusts BIC as in (2.3) leads to even
simpler models. This supports the notion that from a predictive point of view
including a few unnecessary predictors (overfitting) is far less damaging than
is omitting necessary predictors (underfitting).
A final way of comparing models is from a directly predictive point of
view. Since a rough 95% prediction interval is ±2ˆσ, a useful model from a
predictive point of view is one with small ˆσ, suggesting choosing a model that
has small ˆσ while still being as simple as possible. That is,
6. Increase the number of predictors until ˆσ levels off. For these data (S in
the output refers to ˆσ), this implies choosing p = 3 or p = 4.
Taken together, all of these rules imply that the appropriate set of models
to consider are those with two, three, or four predictors. Typically, the
strongest model of each size (which will have highest R2, highest R2
a, lowest
Cp, lowest AICc, and lowest ˆσ, so there is no controversy as to which one
is strongest) is examined. The output on pages 31–32 provides summaries
for the top three models of each size, in case there are reasons to examine a
second- or third-best model (if, for example, a predictor in the best model is

2.3 Methodology
35
difficult or expensive to measure), but here we focus on the best model of each
size. First, here is output for the best four-predictor model.
Coefficients:
Estimate Std.Error t value Pr(>|t|)
VIF
(Intercept) -6.852e+06 3.701e+06
-1.852
0.0678
.
Bedrooms
-1.207e+04 9.212e+03
-1.310
0.1940 1.252
Bathrooms
5.303e+04 1.275e+04
4.160 7.94e-05 1.374 ***
Living.area
6.828e+01 1.460e+01
4.676 1.17e-05 1.417 ***
Year.built
3.608e+03 1.898e+03
1.901
0.0609 1.187 .
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 46890 on 80 degrees of freedom
Multiple R-squared: 0.5044,
Adjusted R-squared: 0.4796
F-statistic: 20.35 on 4 and 80 DF,
p-value: 1.356e-11
The t-statistic for number of bedrooms suggests very little evidence that it
adds anything useful given the other predictors in the model, so we consider
now the best three-predictor model. This happens to be the best four-predictor
model with the one statistically insignificant predictor omitted, but this does
not have to be the case.
Coefficients:
Estimate Std.Error t value Pr(>|t|)
VIF
(Intercept) -7.653e+06 3.666e+06
-2.087 0.039988
*
Bathrooms
5.223e+04 1.279e+04
4.084 0.000103 1.371 ***
Living.area
6.097e+01 1.355e+01
4.498 2.26e-05 1.210 ***
Year.built
4.001e+03 1.883e+03
2.125 0.036632 1.158 *
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 47090 on 81 degrees of freedom
Multiple R-squared: 0.4937,
Adjusted R-squared: 0.475
F-statistic: 26.33 on 3 and 81 DF,
p-value: 5.489e-12
Each of the predictors is statistically significant at a 0.05 level, and this
model recovers virtually all of the available fit (R2 = 49.4%, while that using
all six predictors is R2 = 50.6%), so this seems to be a reasonable model choice.
The estimated slope coefficients are very similar to those from the model using
all predictors (which is not surprising given the low collinearity in the data), so
the interpretations of the estimated coefficients on page 17 still hold to a large
extent. A plot of the residuals versus the fitted values and a normal plot of the
residuals (Figure 2.2) look fine, and similar to those for the model using all six
predictors in Figure 1.5; plots of the residuals versus each of the predictors in
the model are similar to those in Figure 1.6, so they are not repeated here.
Once a “best” model is chosen, it is tempting to use the usual inference
tools (such as t-tests and F-tests) to try to explain the process being studied.
Unfortunately, doing this while ignoring the model selection process can lead
to problems. Since the model was chosen to be best (in some sense) it will tend

36
CHAPTER 2 Model Building
250000
350000
450000
−1e + 05
0e + 00
1e + 05
−1e + 05
0e + 00
1e + 05
(a)
Fitted values
Residuals
−2
−1
0
1
2
(b)
Theoretical Quantiles
Sample Quantiles
FIGURE 2.2: Residual plots for the home price data using the best
three-predictor model. (a) Plot of residuals versus fitted values. (b) Normal
plot of the residuals.
to appear stronger than would be expected just by random chance. Conducting
inference based on the chosen model as if it was the only one examined ignores
an additional source of variability, that of actually choosing the model (model
selection based on a different sample from the same population could very
well lead to a different chosen “best” model). This is termed model selection
uncertainty. As a result of ignoring model selection uncertainty, confidence
intervals can have lower coverage than the nominal value, hypothesis tests can
reject the null too often, and prediction intervals can be too narrow for their
nominal coverage.
Identifying and correcting for this uncertainty is a difficult problem, and
an active area of research, and will be discussed further in Chapter 14. There
are, however, a few things practitioners can do. First, it is not appropriate to
emphasize too strongly the single “best” model; any model that has similar
criteria values (such as AICc or ˆσ) to those of the best model should be
recognized as being one that could easily have been chosen as best based on
a different sample from the same population, and any implications of such
a model should be viewed as being as valid as those from the best model.
Further, one should expect that p-values for the predictors included in a chosen
model are potentially smaller than they should be, so taking a conservative
attitude regarding statistical significance is appropriate. Thus, for the chosen
three-predictor model summarized on page 35, number of bathrooms and
living area are likely to correspond to real effects, but the reality of the year
built effect is more questionable.
There is a straightforward way to get a sense of the predictive power of a
chosen model if enough data are available. This can be evaluated by holding
out some data from the analysis (a holdout or validation sample), applying
the selected model from the original data to the holdout sample (based on the
previously estimated parameters, not estimates based on the new data), and
then examining the predictive performance of the model. If, for example, the

2.3 Methodology
37
250000
300000
350000
400000
450000
500000
1e + 05
2e + 05
3e + 05
4e + 05
5e + 05
6e + 05
Predicted sale price
Observed sale price
FIGURE 2.3: Plot of observed versus predicted house sale price values of
validation sample, with pointwise 95% prediction interval limits superimposed.
The dotted line corresponds to equality of observed values and predictions.
standard deviation of the errors from this prediction is not very different from
the standard error of the estimate in the original regression, chances are making
inferences based on the chosen model will not be misleading. Similarly, if
a (say) 95% prediction interval does not include roughly 95% of the new
observations, that indicates poorer-than-expected predictive performance on
new data.
Figure 2.3 illustrates a validation of the three-predictor housing price
model on a holdout sample of 20 houses. The figure is a plot of the
observed versus predicted prices, with pointwise 95% prediction interval limits
superimposed. The intervals contain 90% of the prices (18 of 20), and the
average predictive error on the new houses is only $3429 (compared to an
average observed price of more than $313,000), not suggesting the presence of
any forecasting bias in the model. Two of the houses, however, have sale prices
well below what would have been expected (more than $130,000 lower than
expected), and this is reflected in a much higher standard deviation ($66,308)
of the predictive errors than ˆσ = $47,090 from the fitted regression. If the two
outlying houses are omitted, the standard deviation of the predictive errors
is much smaller ($49,515), suggesting that while the fitted model’s predictive
performance for most houses is in line with its performance on the original
sample, there are indications that it might not predict well for the occasional
unusual house.

38
CHAPTER 2 Model Building
If validating the model on new data this way is not possible, a simple
adjustment that is helpful is to estimate the variance of the errors as
˜σ2 =
n
i=1 (yi −ˆyi)2
n −p∗−1
,
(2.4)
where ˆy is based on the chosen “best” model, and p∗is the number of
predictors in the most complex model examined, in the sense of most
predictors (Ye, 1998). Clearly, if very complex models are included among
the set of candidate models, ˜σ can be much larger than the standard error of
the estimate from the chosen model, with correspondingly wider prediction
intervals. This reinforces the benefit of limiting the set of candidate models
(and the complexity of the models in that set) from the start. In this case
˜σ = $47,987, so the effect is not that pronounced.
The adjustment of the denominator in (2.4) to account for model selection
uncertainty is just a part of the more general problem that standard degrees
of freedom calculations are no longer valid when multiple models are being
compared to each other as in the comparison of all models with a given number
of predictors in best subsets. This affects other uses of those degrees of freedom,
including the calculation of information measures like Cp, AIC, AICc, and
BIC, and thus any decisions regarding model choice. This problem becomes
progressively more serious as the number of potential predictors increases and
is the subject of active research. This will be discussed further in Chapter 14.
2.4
Indicator Variables and Modeling Interactions
It is not unusual for the observations in a sample to fall into two distinct
subgroups; for example, people are either male or female. It might be that
group membership has no relationship with the target variable (given other
predictors); such a pooled model ignores the grouping and pools the two
groups together.
On the other hand, it is clearly possible that group membership is
predictive for the target variable (for example, expected salaries differing
for men and women given other control variables could indicate gender
discrimination). Such effects can be explored easily using an indicatorvariable,
which takes on the value 0 for one group and 1 for the other (such variables
are sometimes called dummy variables or 0/1 variables). The model takes
the form
yi = β0 + β1x1i + · · · + βp−1xp−1,i + βpIi + εi,
where Ii is an indicator variable with value 1 if the observation is a member of
group and 0 otherwise. The usual interpretation of the slope still applies: βp is
the expected change in y associated with a one-unit change in I holding all else
fixed. Since I only takes on the values 0 or 1, this is equivalent to saying that
the expected target is βp higher for group members (I = 1) than nonmembers
(I = 0), holding all else fixed. This has the appealing interpretation of fitting

2.4 Indicator Variables and Modeling Interactions
39
a constant shift model, where the regression relationships for group members
and nonmembers are identical, other than being shifted up or down; that is,
yi = β0 + β1x1i + · · · + βp−1xp−1,i + εi
for nonmembers and
yi = β0 + βp + β1x1i + · · · + βp−1xp−1,i + εi
for members. The t-test for whether βp = 0 is thus a test of whether a constant
shift model (two parallel regression lines, planes, or hyperplanes) is a significant
improvement over a pooled model (one common regression line, plane, or
hyperplane).
Would two different regression relationships be better still? Say there is
only one numerical predictor x; the full model that allows for two different
regression lines is
yi = β00 + β10x1i + εi
for nonmembers (I = 0), and
yi = β01 + β11x1i + εi
for members (I = 1). The pooled model and the constant shift model can be
made to be special cases of the full model, by creating a new variable that is
the product of x and I. A regression model that includes this variable,
yi = β0 + β1x1i + β2Ii + β3x1iIi + εi,
corresponds to the two different regression lines
yi = β0 + β1x1i + εi
for nonmembers (since I = 0), implying β00 = β0 and β10 = β1 above, and
yi = β0 + β1x1i + β2 + β3x1i + εi
= (β0 + β2) + (β1 + β3)x1i + εi
for members (since I = 1), implying β01 = β0 + β2 and β11 = β1 + β3 above.
The t-test for the slope of the product variable (β3 = 0) is a test of whether
the full model (two different regression lines) is significantly better than the
constant shift model (two parallel regression lines); that is, it is a test of
parallelism. The restriction β2 = β3 = 0 defines the pooled model as a special
case of the full model, so the partial F-statistic based on (2.1),
F =
(Residual SSpooled −Residual SSfull)/2
Residual SSfull/(n −4)
on (2, n −4) degrees of freedom, provides a test comparing the pooled model
to the full model. This test is often called the Chow test (Chow, 1960) in the
economics literature.
These constructions can be easily generalized to multiple predictors, with
different variations of models obtainable. For example, a regression model with
unequal slopes for some predictors and equal slopes for others is fit by including

40
CHAPTER 2 Model Building
products of the indicator and the predictor for the ones with different slopes
and not including them for the predictors with equal slopes. Appropriate
t- and F-tests can then be constructed to make particular comparisons
of models.
A reasonable question to ask at this point is “Why bother to fit the full
model? Isn’t it just the same as fitting two separate regressions on the two
groups?” The answer is no. The full model fit above assumes that the variance
of the errors is the same (the constant variance assumption), while fitting
two separate regressions allows the variances to be different. The fitted slope
coefficients from the full model will, however, be identical to those from
two separate fits. What is gained by analyzing the data this way is the
comparison of versions of pooled, constant shift, and full models based on
group membership, including different slopes for some variables and equal
slopes for others, something that is not possible if separate regressions are fit
to the two groups.
Another way of saying that the relationship between a predictor and the
target is different for members of the two different groups is that there is
an interaction effect between the predictor and group membership on the
target. Social scientists would say that the grouping has a moderating effect
on the relationship between the predictor and the target. The fact that in the
case of a grouping variable, the interaction can be fit by multiplying the two
variables together has led to a practice that is common in some fields: to try
to represent any interaction between variables (that is, any situation where the
relationship between a predictor and the target is different for different values
of another predictor) by multiplying them together. Unfortunately, this is not
a very reasonable way to think about interactions for numerical predictors,
since there are many ways that the effect of one variable on the target can
differ depending on the value of another that have nothing to do with product
functions. See Section 15.6 for further discussion.
2.4.1
EXAMPLE — ELECTRONIC VOTING AND THE 2004
PRESIDENTIAL ELECTION
The 2000 US presidential election matching Republican George W. Bush
against Democrat Al Gore attracted worldwide attention because of its close
and controversial results, particularly in the state of Florida. The 2004 election,
pitting the incumbent Bush against John Kerry, is less discussed, but was also
controversial, in part because of the introduction of electronic voting machines
in some polling places across the country (such machines were introduced
in part because of the irregularities in paper balloting that occurred in
Florida in the 2000 election). Some of the manufacturers of electronic voting
machines were strong supporters of President Bush, and this, along with
the fact that the machines did not produce a paper trail, led to speculation
about whether the machines could be manipulated to favor one candidate
over the other.

2.4 Indicator Variables and Modeling Interactions
41
30
40
50
60
70
−2
2
6
10
(a)
2000 Bush pct.
Change in voting pct.
−2
2
6
10
Change in voting pct.
0
1
(b)
2004 electronic voting
FIGURE 2.4: Plots for the 2004 election data. (a) Plot of percentage change
in Bush vote versus 2000 Bush vote. (b) Side-by-side boxplots of percentage
change in Bush vote by whether or not the county employed electronic voting
in 2004.
This analysis is based on data from Hout et al. (2004) (see also Theus and
Urbanek, 2009). The observations are the 67 counties of Florida. Although
this is not a sample of Florida counties (it is actually a census of all of them),
these counties can be considered a sample of all of the counties in the country,
making inferences drawn about the larger population of counties based on this
set of counties meaningful. The target variable is the change in the percentage
of votes cast for Bush from 2000 to 2004 (a positive number meaning a higher
percentage in 2004). We start with the simple regression model relating the
change in Bush percentage to the percentage of votes Bush took in 2000,
with corresponding scatter plot given in the left plot of Figure 2.4. It can be
seen that most of the changes are positive, reflecting that Bush carried the
state by more than 380,000 votes in 2004, compared with the very close result
(a 537 vote margin) in 2000.
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
-2.9968
2.0253
-1.480
0.14379
Bush.pct.2000
0.1190
0.0355
3.352
0.00134 **
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 2.693 on 65 degrees of freedom
Multiple R-squared: 0.1474,
Adjusted R-squared: 0.1343
F-statistic: 11.24 on 1 and 65 DF,
p-value: 0.00134
There is a weak, but statistically significant, relationship between 2000
Bush vote and the change in vote to 2004, with counties that went more
strongly for Bush in 2000 gaining more in 2004. The constant shift model
now adds an indicator variable for whether a county used electronic voting
in 2004. The side-by-side boxplots in the right plot in Figure 2.4 show that

42
CHAPTER 2 Model Building
overall the 15 counties that used electronic voting had smaller gains for Bush
than the 52 that did not, but that of course does not take the 2000 Bush vote
into account. There are also signs of nonconstant variance, as the variability is
smaller among the counties that used electronic voting.
Coefficients:
Estimate Std. Error t value Pr(>|t|)
VIF
(Intercept)
-2.12713
2.10315
-1.011
0.31563
Bush.pct.2000
0.10804
0.03609
2.994
0.00391 1.049 **
e.Voting
-1.12840
0.80218
-1.407
0.16437 1.049
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 2.672 on 64 degrees of freedom
Multiple R-squared: 0.173,
Adjusted R-squared: 0.1471
F-statistic: 6.692 on 2 and 64 DF,
p-value: 0.002295
It can be seen that there is only weak (if any) evidence that the constant
shift model provides improved performance over the pooled model. This does
not mean that electronic voting is irrelevant, however, as it could be that two
separate (unrestricted) lines are preferred.
Coefficients:
Estimate Std.Error t value Pr(>|t|)
VIF
(Intercept)
-5.23862
2.35084
-2.228 0.029431
*
Bush.pct.2000
0.16228
0.04051
4.006 0.000166
1.44 ***
e.Voting
9.67236
4.26530
2.268 0.026787 32.26 *
Bush.2000
X e.Voting
-0.20051
0.07789
-2.574 0.012403 31.10 *
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 2.562 on 63 degrees of freedom
Multiple R-squared: 0.2517,
Adjusted R-squared: 0.2161
F-statistic: 7.063 on 3 and 63 DF,
p-value: 0.0003626
The t-test for the product variable indicates that the model with two
unrestricted lines is preferred over the model with two parallel lines. A par-
tial F-test comparing this model to the pooled model, which is F = 4.39
(p = .016), also supports two distinct lines,
Change.in.Bush.pct = −5.239 + .162 × Bush.pct.2000
for counties that did not use electronic voting in 2004, and
Change.in.Bush.pct = 4.434 −.038 × Bush.pct.2000
for counties that did use electronic voting. This is represented in Figure 2.5.
This relationship implies that in counties that did not use electronic voting
the more Republican a county was in 2000, the larger the gain for Bush in
2004, while in counties with electronic voting, the opposite pattern held true.

2.4 Indicator Variables and Modeling Interactions
43
40
50
60
70
−2
0
2
4
6
8
10
2000 Bush percentage
Change in voting percentage
e−Voting
No e−Voting
FIGURE 2.5: Regression lines for election data separated by whether the
county used electronic voting in 2004.
As can be seen from the VIFs, the predictor and the product variable are
collinear. This isn’t very surprising, since one is a function of the other, and
such collinearity is more likely to occur if one of the subgroups is much larger
than the other, or if group membership is related to the level or variability
of the predictor variable. Given that using the product variable is just a
computational construction that allows the fitting of two separate regression
lines, this is not a problem in this context.
This model is probably underspecified, as it does not include control
variables that would be expected to be related to voting percentage. Figure 2.6
gives scatter plots of the percentage change in Bush votes versus (a) the total
county voter turnouts in 2000 and (b) 2004, (c) median income, and (d)
percentage of the voters being Hispanic. None of the marginal relationships
are very strong, but in the multiple regression summarized below, median
income does seem to add important predictive power without changing the
previous relationships between change in Bush voting percentage and 2000
Bush percentage very much.
Coefficients:
Estimate Std.Error
t val P(>|t|)
VIF
(Intercept)
1.166e+00
2.55e+00
0.46
0.650
Bush.pct.2000
1.639e-01
3.69e-02
4.45 3.9e-5
1.55 ***
e.Voting
1.426e+01
4.84e+00
2.95
0.005
54.08 **
Bush.2000
X e.Voting
-2.545e-01
8.47e-02
-3.01
0.004
47.91 **

44
CHAPTER 2 Model Building
0e + 00
2e + 05
4e + 05
6e + 05
−2
0
2
4
6
8
10
(a)
2000 Turnout
Change in voting pct.
−2
0
2
4
6
8
10
Change in voting pct.
−2
0
2
4
6
8
10
Change in voting pct.
−2
0
2
4
6
8
10
Change in voting pct.
0e + 00
2e + 05
4e + 05
6e + 05
(b)
2004 Turnout
25000
35000
45000
(c)
Median income
0
10
20
30
40
50
(d)
Hispanic pct.
FIGURE 2.6: Plots for the 2004 election data. (a) Plot of percentage change
in Bush vote versus 2000 voter turnout. (b) Plot of percentage change in
Bush vote versus 2004 voter turnout. (c) Plot of percentage change in Bush
vote versus median income. (d) Plot of percentage change in Bush vote versus
percentage Hispanic voters.
Vote.turn.2000 -5.957e-06
3.10e-05
-0.19
0.848 210.66
Vote.turn.2004
1.413e-06
2.49e-05
0.06
0.955 205.81
Median.income
-1.745e-04
5.61e-05
-3.11
0.003
1.66 **
Hispan.pop.pct -4.127e-02
3.18e-02
-1.30
0.200
1.32
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 2.244 on 59 degrees of freedom
Multiple R-squared: 0.4624,
Adjusted R-squared: 0.3986
F-statistic:
7.25 on 7 and 59 DF,
p-value: 2.936e-06

2.4 Indicator Variables and Modeling Interactions
45
We could consider simplifying the model here, but often researchers prefer
to not remove control variables, even if they do not add to the fit, so that
they can be sure that the potential effect is accounted for. This is generally
not unreasonable if collinearity is not a problem, but control variables that do
not provide additional significant predictive power, but are collinear with the
variables that are of direct interest, might be worth removing so they don’t
obscure the relationships involving the more important variables. In these
data the two voter turnout variables are (not surprisingly) highly collinear,
but a potential simplification to consider (particularly given that the target
variable is the change in Bush voting percentage from 2000 to 2004) is to
consider the change in voter turnout as a predictor (the fact that the estimated
slope coefficients for 2000 and 2004 voter turnout are of opposite signs
and not very different also supports this idea). The model using change in
voter turnout is a subset of the model using 2000 and 2004 voter turnout
separately (corresponding to restriction β2004 = −β2000), so the two models
can be compared using a partial F-test. As can be seen below, the fit of the
simpler model is similar to that of the more complicated one, collinearity is no
longer a problem, and it turns out that the partial F-test (F = 0.43, p = .516)
supports that the simpler model fits well enough compared to the more
complicated model to be preferred (although voter turnout is still apparently
not important).
Coefficients:
Estimate Std.Error
t val P(>|t|)
VIF
(Intercept)
1.157e+00
2.54e+00
0.46
0.651
Bush.pct.2000
1.633e-01
3.67e-02
4.46 3.7e-05
1.55 ***
e.Voting
1.272e+01
4.20e+00
3.03
0.004 41.25 **
Bush.2000
X e.Voting
-2.297e-01
7.53e-02
-3.05
0.003 38.25 **
Change.turnout -1.223e-05
1.36e-05
-0.90
0.370
2.44
Median.income
-1.718e-04
5.57e-05
-3.08
0.003
1.65 **
Hispan.pop.pct -4.892e-02
2.94e-02
-1.66
0.102
1.14
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 2.233 on 60 degrees of freedom
Multiple R-squared: 0.4585,
Adjusted R-squared: 0.4044
F-statistic: 8.468 on 6 and 60 DF,
p-value: 1.145e-06
Residual plots given in Figure 2.7 do not indicate any obvious problems,
although the potential nonconstant variance related to whether a county used
electronic voting or not noted in Figure 2.4 is still indicated. We will not
address that issue here, but correction of nonconstant variance related to
subgroups in the data will be discussed in Section 6.3.3.

46
CHAPTER 2 Model Building
0
2
4
6
−6
−2
0
2
4
−6
−2
0
2
4
−6
−2
0
2
4
Fitted values
Residuals
−2
−1
0
1
2
Normal Q−Q Plot
Theoretical Quantiles
Sample Quantiles
30
40
50
60
70
2000 Bush pct.
Residuals
−6
−2
0
2
4
−6
−2
0
2
4
−6
−2
0
2
4
Residuals
Residuals
−6
−2
0
2
4
Residuals
Residuals
0
1
2004 electronic voting
0
40000
80000
120000
Change in turnout
25000
35000
45000
Median income
0
10
20
30
40
50
Hispanic pct.
FIGURE 2.7: Residual plots for the 2004 election data.
2.5
Summary
In this chapter, we have discussed various issues related to model building
and model selection. Such methods are important because both underfitting
(not including variables that are needed) and overfitting (including variables
that are not needed) lead to problems in interpreting the results of regression
analyses and making predictions using fitted regression models. Hypothesis
tests provide one tool for model building through formal comparisons of
models. If one model is a special case of another, defined through a linear
restriction, then a partial F-statistic provides a test of whether the more
complex model provides significantly more predictive power than does the
simpler one. One important example of a partial F-test is the standard t-test
for the significance of a slope coefficient. Another important use of partial
F-tests is in the construction of models for data where observations fall into
two distinct subgroups that allow for common (pooled) relationships over
groups, constant shift relationships that differ only in level but not in slopes,
and completely distinct and different relationships across groups.
While useful, hypothesis tests do not provide a complete tool for model
building. The problem is that a hypothesis test does not necessarily answer

2.5 Summary
47
the question that is of primary importance to a data analyst. The t-test for a
particular slope coefficient tests whether a variable adds predictive power given
the other variables in the model, but if predictors are collinear it could be that
none add anything given the others, while separately still being very important.
A related problem is that collinearity can lead to great instability in regression
coefficients and t-tests, making results difficult to interpret. Hypothesis tests
also do not distinguish between statistical significance (whether or not a true
coefficient is exactly zero) from practical importance (whether or not a model
provides the ability for an analyst to make important discoveries in the context
of how a model is used in practice).
These considerations open up a broader spectrum of tools for model
building than just hypothesis tests. Best subsets regression algorithms allow
for the quick summarization of hundreds or even thousands of potential
regression models. The underlying principle of these summaries is the principle
of parsimony, which implies the tradeoff of strength of fit versus simplicity:
that a model should only be as complex as it needs to be. Measures such as
the adjusted R2, Cp, and AICc explicitly provide this tradeoff, and are useful
tools in helping to decide when a simpler model is preferred over a more
complicated one. An effective model selection strategy uses these measures, as
well as hypothesis tests and estimated prediction intervals, to suggest a set of
potential “best” models, which can then be considered further. In doing so, it
is important to remember that the variability that comes from model selection
itself (model selection uncertainty) means that it is likely that several models
actually provide descriptions of the underlying population process that are
equally valid. One way of assessing the effects of this type of uncertainty is to
keep some of the observed data aside as a holdout sample, and then validate
the chosen fitted model(s) on that held out data.
A related point increasingly raised in recent years has been focused on issues
of replicability, or the lack thereof — the alarming tendency for supposedly
established relationships to not reappear as strongly (or at all) when new data are
examined. Much of this phenomenon comes from quite valid attempts to find
appropriate representations of relationships in a complicated world (including
those discussed here and in the next three chapters), but that doesn’t alter the
simple fact that interacting with data to make models more appropriate tends
to make things look stronger than they actually are. Replication and validation
of models (and the entire model building process) should be a fundamental
part of any exploration of a random process. Examining a problem further and
discovering that a previously-believed relationship does not replicate is not a
failure of the scientific process; in fact, it is part of the essence of it.
A valid question regarding the logistics of performing model selection
remains: what is the “correct” order in which to perform the different steps
of model selection, assumption checking, and so on? Do you omit unusual
observations first, and then try to determine the best model? Or do you work
on variable selection, and then check diagnostics based on your chosen model?
Unfortunately, there is no clear answer to this question, as neither order is
guaranteed to work. The best answer is to try it both ways and see what

48
CHAPTER 2 Model Building
happens; chances are results will be similar, and if they are not this could reveal
alternative models that are equally valid and reasonable. What is certainly true
is that if the data set is changed in any way, whether by omitting observations,
taking logs, or anything else, model selection must be explored again, as the
results previously obtained might not be appropriate for the new form of
the data.
Although best subsets algorithms and modern computing power have
made automatic model selection more feasible than it once was, they are still
limited computationally to a maximum of roughly 35 predictors. In recent
years, it has become more common for a data analyst to be faced with data sets
with hundreds or thousands of predictors, making such methods infeasible.
Recent work has focused on alternatives to least squares called regularization
methods, which can (possibly) be viewed as effectively variable selectors, and
are feasible for very large numbers of predictors. These methods are discussed
further in Chapter 14.
KEY TERMS
AICc: A modified version of the Akaike Information Criterion (AIC) that
guards against overfitting in small samples. It is used to compare models when
performing model selection.
Best subsets regression: A procedure that generates the best-fitting models
for each number of predictors in the model.
Chow test: A statistical (partial F-)test for determining whether a single
regression model can be used to describe the regression relationships when
two groups are present in the data.
Collinearity: When predictor variables in a regression fit are highly correlated
with each other.
Constant shift model: Regression models that have different intercepts but
the same slope coefficients for the predicting variables for different groups in
the data.
Indicator variable: A variable that takes on the values 0 or 1, indicating
whether a particular observation belongs to a certain group or not.
Interaction effect: When the relationship between a predictor and the target
variable differs depending on the group in which an observation falls.
Linear restriction: A linear condition on the regression coefficients that
defines a special case (subset) of a larger unrestricted model.
Mallows’ Cp: A criterion used for comparing several competing models to
each other. It is designed to estimate the expected squared prediction error of
a model.
Model selection uncertainty: The variability in results that comes from the
fact that model selection is an iterative process, arrived at after examination
of several models, and therefore the final model chosen is dependent on the

2.5 Summary
49
particular sample drawn from the population. Significance levels, confidence
intervals, etc., are not exact, as they depend on a chosen model that is itself
random. This should be recognized when interpreting results.
Overfitting: Including redundant or noninformative predictors in a fitted
regression.
Partial F-test: F-test used to compare the fit of an unrestricted model to
that of a restricted model (defined by a linear restriction), in order to see if the
restricted model is adequate to describe the relationship in the data.
Pooled model: A single model fit to the data that ignores group classification.
Replicability: The finding that a scientific experiment or modeling process
obtains a consistent result when it is repeated.
Underfitting: Omitting informative essential variables in a fitted regression.
Variance inflation factor: A statistic giving the proportional increase in the
variance of the sample regression coefficient for a particular predictor due to
the linear association of the predictor with other predictors.

Part Two
Addressing
Violations
of Assumptions

Three
Chapter
Diagnostics for Unusual
Observations
3.1
Introduction
53
3.2
Concepts and Background Material
54
3.3
Methodology
56
3.3.1
Residuals and Outliers
56
3.3.2
Leverage Points
57
3.3.3
Influential Points and Cook’s Distance
58
3.4
Example — Estimating Home Prices (continued)
60
3.5
Summary
63
3.1
Introduction
As is true of all statistical methodologies, while linear regression analysis can
be a very effective way to model data as long as the assumptions being made
are true, if they are violated least squares can potentially lead to misleading
results. The residual plots discussed in Section 1.3.5 are important tools to
check these assumptions, but their flexibility is both a strength and a weakness.
The plots can be examined for evidence of violations of assumptions without
requiring specification of the exact form of the violations, but the subjective
nature of such examination can easily lead to different data analysts having
different impressions of the validity of the underlying assumptions. Plots
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
53

54
CHAPTER 3 Diagnostics for Unusual Observations
also by definition can only provide two-dimensional views of a multivariate
regression relationship.
In this chapter (and several others to follow), we describe other tools that
can be used to identify and address potential problems with the application of
linear least squares estimation to regression problems. This chapter discusses
diagnostics for the identification of unusual observations. We will describe
both graphical and more formal uses of these diagnostics, but will not
emphasize tests of significance.
3.2
Concepts and Background Material
There are several reasons why it is important to identify unusual observations.
1. Unusual observations are sometimes simply mistakes that arise from
incorrect entry or faulty measurement of numerical values. Obviously,
these should be corrected if possible, or the observations should be omitted
if it is not possible.
2. It is often the case that a great deal of information can come from
examination of unusual observations. It is possible that the reason that an
observation apparently has a different relationship between the response
and the predictor(s) from the one that is typical for the data is that it
is different in a fundamental way from the other observations, such as
having been measured under different conditions. In such a circumstance,
it could be that the observation should have never been included in
the sample at all, or perhaps the regression model could potentially be
enriched to account for these different conditions through additional
predictors, aiding in estimation not only for that observation but others
as well. Another possibility is that an observation has a very different set
of predictor values from what is typical for the bulk of the data; this
could suggest sampling more observations with similar values of those
predictors.
3. It is important that all of the observations in a sample have similar influence
on a fitted model. It is not desirable for just a few of the observations
to have a strong influence on the fitted regression. The summary of the
relationship between the response and the predictor(s) should be based on
the bulk of the data, and not just on a small subset of it. This should hold
for not only the estimated regression coefficients, but also for measures
of its strength, and any variable selection that might be done. All of
these measures are potentially affected by unusual observations, and the
presence of such observations can lead to a misleading model for the data
if they are ignored.
It is worth saying a little more about the third reason given above. In
situations where an unusual observation is not obviously “wrong” (that is, a
point is not unusual because of a transcription error), it is sometimes argued

3.2 Concepts and Background Material
55
4
6
8
10
12
14
3
4
5
6
7
x
y
A
FIGURE 3.1: Scatter plot of data with an unusual observation (A), with the
fitted regression line with (solid line) and without (dashed line) the unusual
observation included.
that it is not appropriate to omit unusual observations from a data set, because
all of the observations in the data are “legitimate.” The argument is to keep
the data “as they really are.” This is a fundamentally incorrect attitude, as it
ignores the key goal of any statistical model, which is to describe as accurately
as possible the underlying process driving the bulk of the data. Consider
Figure 3.1. The fitted regression line that is based on all of the data (the solid
line) is obviously an extremely poor representation of what is going on in the
data — it does not in any way describe the data “as they really are,” because
of the deficiencies of least squares regression modeling (and its sensitivity
to unusual observations). A regression method that is insensitive to unusual
observations (a so-called robust method) would be affected much less by the
unusual observation, resulting in a fitted line similar to that obtained when
omitting observation A (the dashed line). This is the correct summary of the
data, but the role of A should be noted and reported in the analysis. That is, the
issue is not that there is something “wrong” with the data point; rather, the issue
is that there is something wrong with least squares regression in its sensitivity
to unusual observations. For this reason, the least squares fit with unusual
observations omitted should always be examined. If it is noticeably different

56
CHAPTER 3 Diagnostics for Unusual Observations
from that with the unusual observations included it should be reported, as it is
likely to be a better representation of the underlying relationship. When this
is done, however, the points omitted must be identified and discussed as part
of the summary of the results.
A data analyst should always examine diagnostics and report on unusual
observations. This can be done by printing them out, or (particularly if
the sample is large) displaying them in observation order (a so-called index
plot) to help make identification easier. Such a plot also has the advantage
of highlighting relative values of diagnostics (compared to the others in the
sample), since an observation that is very unusual compared to the sample as a
whole is worth investigating further even if it is not objectively “very unusual”
based on an arbitrary cutoff value.
3.3
Methodology
3.3.1
RESIDUALS AND OUTLIERS
An outlier is an observation with a response value yi that is unusual relative
to its expected value. Since the fitted value ˆyi is the best available estimate of
the expected response for the ith observation, it is natural that the residual
ei = yi −ˆyi should be the key statistic used to evaluate if an observation is an
outlier. An outlier is an observation with a large absolute residual (note that
residuals can be positive or negative). The issue is then to determine what is
meant by “large.” As was noted in equation (1.6), the residuals satisfy
e = (I −H)y,
where H = X(X′X)−1X′ is the hat matrix. Since the variance of yi satisfies
V (yi) = σ2, straightforward algebra shows that
V (e) = (I −H)σ2,
or for an individual residual,
V (ei) = σ2(1 −hii),
(3.1)
where hii is the ith diagonal element of the hat matrix. Thus, scaling the
residual by its standard deviation
e∗
i =
ei
σ

1 −hii
(3.2)
results in a standardized residual that has mean 0 and standard deviation 1.
Since the errors are assumed to be normally distributed, the residuals will also
be normally distributed, implying that the usual rules of thumb for normally
distributed random variables can be applied. For example, since only roughly
1% of the sample from a normal distribution is expected to be outside ±2.5,
standardized residuals outside ±2.5 can be flagged as potentially outlying and
examined further.

3.3 Methodology
57
The standardized residual (3.2) depends on the unknown σ, so actual
calculation requires an estimate of σ. The standard approach is to use the
standard error of the estimate ˆσ, the square root of the residual mean
square (1.8); this is sometimes called the internally studentized residual, but
is usually just referred to as the standardized residual. An alternative approach
is to base the estimate of σ on all of the observations except the ith one when
determining the ith residual, so the point will not affect the estimate of σ if it
is in fact an outlier; this is called the externally studentized residual
˜ei =
ei
ˆσ(i)

1 −hii
,
where ˆσ(i) is the standard error of the estimate based on the model omitting
the ith observation. It can be shown that
˜ei = e∗
i

n −p −2
n −p −1 −e∗2
i
.
The distribution of the externally studentized residuals is simpler than
that of the internally studentized (standardized) residuals (they follow a
tn−p−2-distribution), and will be larger in absolute value for outlying points,
but since one is a monotonic function of the other, plots using the two types of
residuals will be similar in appearance. Note that this is also true for the types
of residual plots discussed in Section 1.3.5; although the appearance of the
plots will be similar no matter which version of the residuals are plotted, given
the standard-normal scale on which they are measured, there is no reason not
to use some form of standardized residual in all residual plots.
3.3.2
LEVERAGE POINTS
One of the reasons observation A in Figure 3.1 had such a strong effect on the
fitted regression is that its value for the predictor was very different from that
of the other predictor values. This isolation in the X-space tended to draw
the fitted regression line toward it. This can be made more formal. As noted
in equation (1.5), the fitted values satisfy ˆy = Hy, where H is the hat matrix.
This can be written out explicitly for the ith fitted value as
ˆyi = hi1y1 + hi2y2 + · · · + hiiyi + · · · + hinyn.
(3.3)
Thus, the ith diagonal element of the matrix, hii, represents the potential
effect that the ith observed value yi can have on the ith fitted value ˆyi. Since
leverage points, by being isolated in the X-space, draw the regression line
(or plane or hyperplane) toward them, hii is an algebraic reflection of the
tendency that an observation has to draw the line toward it. The connection
between large values of hii and an unusual position in X-space is particularly
clear in the case of simple regression, where
hii = 1
n +
(xi −X)2

j(xj −X)2 ;

58
CHAPTER 3 Diagnostics for Unusual Observations
the farther xi is from the center of the data (as measured by the sample mean
of the x’s), the higher the leverage.
It can be shown that 0 < hii < 1 for all i, and the sum of the n leverage
values equals p + 1, where p is the number of predicting variables in the
regression. That is, the average leverage value is (p + 1)/n. A good guideline
for what constitutes a large leverage value is (2.5)(p + 1)/n. Cases with values
greater than that should be investigated as possible leverage points.
Based on equation (3.1), it is easy to see that leverage points have residuals
with less variability than residuals from non-leverage points (since hii is closer
to 1, resulting in a smaller variance of the residual). This is not surprising;
since a leverage point is characterized by a fitted value close to the observed
target value (that is, it tends to pull the fitted regression toward it), its
residual is likely to be closer to zero. Another way to see this is from the
fact that
hii +
e2
i
ˆσ2(n −p −1) ≤1,
which shows that as hii gets closer to 1, |ei| gets closer to 0.
3.3.3
INFLUENTIAL POINTS AND COOK’S DISTANCE
As described Section 3.3.2, the idea of leverage is all about the potential for
an observation to have a large effect on a fitted regression; if the observation
does not have an unusual response value, it is possible that drawing the
regression toward it will not change the estimated coefficients very much (or
at all). Figure 3.2 gives two examples of this pattern. In the top plot, the
unusual point is a leverage point, but falls almost directly on the line implied
by the rest of the data; that is, it is not an outlier. If this point is omitted,
the fitted regression line will change very little, so in the sense of effect on
the estimated coefficients the point is not influential. Note that the deletion
may have other effects; for example, the R2 and overall F-statistic with the
point included would probably be noticeably higher than those with the point
omitted, as the unusual point increases the total sum of squares  (yi −Y )2
without increasing the residual sum of squares  (yi −ˆyi)2. In the bottom
plot, the unusual point is an outlier but not a leverage point. As equation (3.3)
shows, since hii would be relatively small for this point, the observed yi
has little effect on the fitted ˆyi, so omitting it changes the fitted regression
very little.
Given these different notions of influence, it is not surprising that there
are many measures of the influence of an observation in regression analysis.
The most widely used measure is the one proposed by Cook (1977), which
measures the change in the fitted regression coefficients if a case were dropped
from the regression, relative to the inherent variability of the coefficient
estimates themselves. Cook’s distance D is also equivalent to the change in
the predicted values from the full data and the fitted value obtained by deleting

3.3 Methodology
59
4
6
8
10
12
14
2
4
6
8
12
x
y
3
4
5
6
7
4
6
8
10
x
y
FIGURE 3.2: Scatter plots of data with an unusual observation, with the
fitted regression line with (solid line) and without (dashed line) the unusual
observation included.
the observation. Cook’s distance combines the notions of outlyingness and
leverage in an appealing way, since
Di =
(e∗
i )2hii
(p + 1)(1 −hii).
(3.4)
Observations that are outliers (with large absolute standardized residual |e∗
i |)
or leverage points (with large hii) are potentially influential, and points that
are both (so-called “bad” leverage points) are the most influential. A value of
Cook’s D over 1 or so should be flagged. These points should be examined
further.
It is important to remember that Cook’s D only measures one particular
form of influence, and should not be viewed as the final judge of whether
or not a point is worth investigating. For example, outliers that have low
values of Cook’s D can still have a large effect on hypothesis tests, R2, the
standard error of the estimate, and so on. Other measures of influence have
been proposed that focus on such notions of influence; see Belsley et al. (1980)
and Chatterjee and Hadi (1988) for more discussion.
It is worth noting a weakness of all of these diagnostics. Specifically, they
are all sensitive to the so-called masking effect. This occurs when several
unusual observations are all in the same region of the (X, y) space. When
this happens, the diagnostics, which all focus on changes in the regression
when a single point is deleted, fail, since the presence of the other nearby
unusual observations means that the fitted regression changes very little if one

60
CHAPTER 3 Diagnostics for Unusual Observations
is omitted. The problem of multiple outliers in regression is a topic of ongoing
research, and typically involves defining a “clean” subset of the sample to
which potentially outlying observations are compared; see, for example, Hadi
and Simonoff (1993) and Atkinson and Riani (2000).
3.4
Example — Estimating Home Prices
(continued)
Consider again the home price data examined in Chapters 1 and 2. Regression
diagnostics for the chosen model on page 35 are given as index plots in
Figure 3.3. The guidelines given earlier for flagging unusual values are given
using dotted lines, although this is not given in the Cook’s distance plot as the
largest value is not close to 1.
0
20
40
60
80
−2
0
1
2
Standardized residuals
Index
Std. residuals
0
20
40
60
80
0.05
0.15
0.25
Diagonal elements of the hat matrix
Index
Leverage
0
20
40
60
80
0.00 0.04 0.08 0.12
Cook’s distances
Index
Cook's D
FIGURE 3.3: Index plots of diagnostics for the three-predictor regression
fit for the home price data given on page 35, with the guideline values
superimposed on the standardized residuals and leverage plots.

3.4 Example — Estimating Home Prices (continued)
61
0.05
0.10
0.15
0.20
0.25
0.00
0.02
0.04
0.06
0.08
0.10
0.12
Leverage values
Cook’s distances
FIGURE 3.4: Plot of Cook’s distance versus diagonal element of the hat
matrix for the three-predictor regression fit for the home price data given on
page 35.
None of the points are flagged as outliers or influential points (according
to Cook’s distance), but there are five leverage points flagged as unusual.
The scatter plots given in Figure 1.4 on page 16 show that these correspond
to the only two houses with living area greater than 2500 square feet (each
having living area more than 2900 square feet), and the only three houses
built after 1955 (each being built in 1961 or 1962). It is possible that the
underlying relationship could be different for houses of these types, and it is
important to see if their inclusion has had a noticeable effect on the fitted
regression. A plot of Cook’s distances versus diagonal elements of the hat
matrix (Figure 3.4) shows that not all of these leverage points would change
the estimated coefficients very much if they were omitted (one at a time),
although as noted above change in estimated coefficients is not the only
possible effect. One of those effects, in fact, is on the model selection process
discussed in Section 2.3.1; once observations are omitted this is a new data set,
and the “best” model might not be the same as it was before, so best subsets
regression needs to be rerun.
In fact, the best three-predictor model is still based on the number of
bathrooms, the living area, and the year the house was built, but there is a

62
CHAPTER 3 Diagnostics for Unusual Observations
noticeable change from the previous output (given on page 35), in that the
evidence that the year the house was built adds to the predictive power of the
model is noticeably weaker (because the standard error of the coefficient for
that variable is 50% larger than that from the model based on all of the data),
and the coefficient for living area is roughly 1.5 standard errors larger than
that from the model based on all of the data:
Coefficients:
Estimate Std. Error t value Pr(>|t|)
VIF
(Intercept) -9.545e+06
5.690e+06
-1.678
0.09754
.
Bathrooms
4.409e+04
1.405e+04
3.138
0.00242 1.600 **
Living.area
8.161e+01
1.790e+01
4.559 1.93e-05 1.352 ***
Year.built
4.964e+03
2.921e+03
1.699
0.09332 1.257 .
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 47390 on 76 degrees of freedom
Multiple R-squared: 0.4888,
Adjusted R-squared: 0.4687
F-statistic: 24.23 on 3 and 76 DF,
p-value: 4.192e-11
The apparent strength of the fit after omitting the leverage points is
less (smaller R2, larger ˆσ), but this is not unusual when omitting leverage
points, and is certainly not a reason to not prefer this model. A viable
alternative is the best two-predictor model, which removes year built as a
predictor:
Coefficients:
Estimate Std. Error t value Pr(>|t|)
VIF
(Intercept) 124450.24
26978.18
4.613 1.55e-05
***
Bathrooms
54883.41
12689.09
4.325 4.52e-05 1.274 ***
Living.area
74.29
17.59
4.224 6.51e-05 1.274 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 47970 on 77 degrees of freedom
Multiple R-squared: 0.4694,
Adjusted R-squared: 0.4556
F-statistic: 34.06 on 2 and 77 DF,
p-value: 2.532e-11
The regression diagnostics are no longer flagging any points (Figure 3.5), and
residual plots (not given) now look fine. The implications of this model are
not very different from the earlier one, but its simpler form could be useful;
for example, it might be that a model that is not based on the year the house is
built could be applicable to other neighborhoods that are similar to Levittown,
but where the houses were built at a different time.

3.5 Summary
63
0
20
40
60
80
−2
0 1 2
Standardized residuals
Index
Std. residuals
0
20
40
60
80
0.000.050.100.15
Diagonal elements of the hat matrix
Index
Leverage
0
20
40
60
80
0.000.050.100.15
Cook’s distances
Index
Cook’s D
FIGURE 3.5: Index plots of diagnostics for the two-predictor regression fit
for the home price data, with the guideline values superimposed on the
standardized residuals and leverage plots.
3.5
Summary
The identification of unusual observations is an important part of any
regression analysis. Outliers and leverage points can have a large effect on
a fitted regression, on estimated coefficients, measures of the strength of
the regression, and on the model building process itself. Further, unusual
observations can sometimes tell the data analyst as much (or more) about
the underlying random process as the other observations. They can highlight
situations in which the model could be enriched to account for their different
behavior.

64
CHAPTER 3 Diagnostics for Unusual Observations
Residual plots can often identify unusual observations, but they should
be supplemented with examination of diagnostics like the diagonal element of
the hat matrix (leverage) and Cook’s distance. Guidelines for what constitutes
an unusual value are useful, but the values should also be plotted to make
sure relative (rather than only absolute) unusualness is also made apparent.
Identification of multiple unusual values can be challenging because of the
masking effect, and is a topic of ongoing research.
KEY TERMS
Cook’s distance: A statistic that measures the change in the fitted regression
coefficients when an observation is dropped from the regression analysis,
relative to the inherent variability of the coefficients. This is also equivalent
to the change in the predicted value of an observation when it is included or
excluded in the analysis. It is used as an influence measure for a particular
observation.
Hat matrix: A matrix that contains the weights of the predictor variables
that determine the predicted values. The diagonal element hii represents the
potential effect of the ith observation on its own fitted value, and is used as a
measure of leverage.
Influential point: An observation whose deletion causes major changes in
the fitted regression. Such points exercise an undue amount of influence on
the fit, thereby distorting it. A large value of Cook’s distance is a standard
diagnostic indicator for the identification of an influential point. As is true of
all unusual observations, attention should be paid to these points.
Leverage point: An observation that is isolated in the predictor space
relative to the other observations. Leverage points draw the fitted regression
towards them, thereby potentially distorting the fitted model. They also can
have a strong effect on measures of the strength of the observed regression
relationship. A large value of diagonal of the hat matrix is a standard diagnostic
indicator for the identification of a leverage point. As is true of all unusual
observations, attention should be paid to these points.
Masking effect: The tendency in data sets when there are several unusual
observations clustered together that identification of one observation at a time
will fail because the observations hide each other.
Outlier: An observation whose response value is unusual given its predictor
variable values. A large value of absolute standardized residual is a standard
diagnostic indicator for the identification of an outlier. As is true of all unusual
observations, attention should be paid to these points.
Robust regression: A regression procedure that is not affected by extreme
(unusual) observations. This is accomplished by giving less weight to high
leverage points and outliers in the fitting procedure. Instead of minimizing
the sum of squared residuals, other functions of the residuals are minimized
that yield the desired properties.

3.5 Summary
65
Standardized residual: A residual that is standardized by scaling it using
its estimated standard deviation. The residuals can be standardized using the
estimate of the standard deviation based on the entire data set. A modification
to this is to standardize the residual using the standard deviation obtained by
omitting the observation for which the residual is being computed. This is
called the externally studentized residual. Since the two residuals are monotonic
functions of each other, residual plots using either form will have a similar
appearance.

Four
Chapter
Transformations and
Linearizable Models
4.1
Introduction
67
4.2
Concepts and Background Material: The Log-Log Model
69
4.3
Concepts and Background Material: Semilog Models
69
4.3.1
Logged Response Variable
70
4.3.2
Logged Predictor Variable
70
4.4
Example — Predicting Movie Grosses After One Week
71
4.5
Summary
77
4.1
Introduction
The linear regression models discussed thus far are only appropriate in the
situation where the relationship between the response and the predictors is at
least roughly linear. One situation that violates this assumption can be handled
easily: the possibility of polynomial relationships. For example, examination of
the data might uncover a parabolic (quadratic) relationship between x and y.
This suggests enriching the model to include both linear and quadratic terms;
that is, just fit a model that includes the two predictors x and x2. Generally
speaking it is good practice to include the linear term x in the model even
if it does not appear to add fit in the presence of the quadratic term x2, as
omitting it puts a strong restriction on the form of the parabolic curve being
fit (zero slope at x = 0). Another possibility is the situation where the context
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
67

68
CHAPTER 4 Transformations and Linearizable Models
of the problem implies the existence of an inherently nonlinear relationship
between the response and the predictors. This requires moving to nonlinear
regression methods, and is the subject of Chapter 12.
Between these two extremes are what are called linearizable models.
These correspond to response/predictor relationships that are nonlinear, but
can be changed into linear relationships through the use of transformation
(most often the logarithmic transformation). Happily, it is often the case that
taking logs of a variable can have multiple positive effects on a regression fit.
First, variables that have most of their values relatively small and positive,
but have some values much larger, so the variable covers several orders of
magnitude (that is, variables that have a distribution with a long right tail) often
become much more symmetrical when treated after taking logs. In particular,
previously indiscernible structure can become apparent when working in the
logged scale.
Another benefit of taking logs of the response variable is that it can address
certain kinds of heteroscedasticity. A situation where the variability of the
response variable reflects multiplicative (rather than additive) errors, can be
accommodated by taking logs. Consider a relationship that has the form
yi = f(xi, β) × δi,
(4.1)
where δi is an error term with E(δi) = 1 and V (δi) = γ2. In this situation the
standard deviation of yi equals f(xi, β)γ, so observations with larger response
values will also have more variability. Equation (4.1) implies that
log yi = log[f(xi, β)] + log δi ≡log[f(xi, β)] + εi;
(4.2)
if log[f(xi, β)] is a roughly linear function of the xi variables and εi has
constant variance σ2, this is reasonably represented with a linear regression
model based on log y as the response variable. In the next two sections, we
describe two relationships that satisfy this property. Note that the base of the
logarithm does not matter in equation (4.2); the most common choices are
the common logarithm (base 10) and the natural logarithm (base e).
That multiplicative relationships become additive when taking logs implies
that the standard deviation σ of the errors εi in equation (4.2) has a
multiplicative, rather than additive, interpretation. The interval ±2ˆσ is still a
rough 95% prediction interval for the response, but since that response is log y
rather than y, the correct interpretation is that y can be predicted roughly
95% of the time to within a multiplicative factor of 102ˆσ (i.e., multiplying or
dividing the estimated expected y by 102ˆσ) if common logs are used, or of e2ˆσ
if natural logs are used. Note that the intervals will be identical in the y scale
in either case, as the values of ˆσ are automatically adjusted accordingly.
A related point to this connection between logarithms and multiplicative
relationships is that money tends to operate multiplicatively rather than
additively. For example, when making an investment, people understand that
it is a proportional, rather than absolute, return that is sensible (that is, an
investor would not expect to be told something like “This investment could
yield a profit of $50,000 for you,” as it would depend on how much was

4.3 Concepts and Background Material: Semilog Models
69
invested, but would expect to be told something like “This investment could
yield a profit of 10% on your investment”). For this reason, it is often useful
to treat money variables in the logged scale.
4.2
Concepts and Background Material:
The Log-Log Model
A model often used to describe a growth process is
y = αxβ.
This corresponds to a multiplicative/multiplicative relationship, in the sense
that it is consistent with proportional changes in x being associated with
proportional changes in y. It is apparent that for this relationship multiplying x
by a constant multiplies y by a constant, as x →x′ = ax implies y →y′ =
α(ax)β = aβαxβ = aβy. This functional form is linearizable because if logs
are taken of both sides of the equation we obtain
log y = log α + β log x ≡β0 + β1 log x.
(4.3)
That is, the model is linear after logging both x and y, and is hence called the
log-log model. The log-log model has an important interpretation in terms
of demand functions. Let y above represent demand for a product, and x be
the price. The price elasticity of demand is defined as the proportional change
in demand for a proportional change in price; that is,
dy/y
dx/x = dy/dx
y/x ,
where dy/dx is the derivative of y with respect to x. Some calculus shows that
for the log-log model, the elasticity is (a constant) β, and the log-log model
is therefore sometimes called the constant elasticity model, since the slope
coefficient corresponds to an elasticity. This provides a direct interpretation
for the slope coefficient in a log-log model as the proportional change in y
associated with a proportional change in x (holding all else in the model fixed).
Thus, when fitting model (4.3), an estimated slope coefficient ˆβj implies that
a 1% change in xj is associated with an estimated ˆβj% change in y (holding
all else in the model fixed if there are other predictors).
4.3
Concepts and Background Material: Semilog
Models
Semilog models correspond to the situation where either the response variable
or a predicting variable is logged, but not both. The two situations have
fundamentally different interpretations, and so are treated separately.

70
CHAPTER 4 Transformations and Linearizable Models
4.3.1
LOGGED RESPONSE VARIABLE
Another model often used to describe a growth process is
y = αβx.
This corresponds to an additive/multiplicative relationship, in the sense that it
is consistent with additive changes in x being associated with multiplicative
changes in y. For this relationship adding a constant to x multiplies y by a
constant, as x →x′ = x + a implies y →y′ = αβx+a = βaαβx = βay. If the
predictor is time, another interpretation of this model is through the fact that
this relationship is consistent with a growth rate that is proportional to the
current level of the response. This functional form is linearizable, since if logs
are taken of both sides of the equation we obtain
log y = log α + x log β ≡β0 + β1x,
(4.4)
corresponding to a relationship where log y is linearly related to x. An
equivalent representation of this relationship is
y = exp(β0 + β1x).
(4.5)
This model is particularly appropriate, for example, for modeling the growth
of objects over time, such as the total amount of money in an investment
as a function of time, or the number of people suffering from a disease
as a function of time. Growth operates multiplicatively, but time operates
additively. In this situation, the estimated slope ˆβj is a semielasticity, and 10 ˆβj
(using common logs) or e ˆβj (using natural logs) is interpreted as the estimated
expected multiplicative change in y associated with a one unit increase in xj
holding all else in the model fixed.
It is important to remember that measures of strength of a regression
(like R2 or the overall F-statistic) for a fit using y as the response and a
fit using log y as the response are not comparable. Choosing to log or not
log the response on the basis of one or the other model fit having a higher
R2 is never the correct strategy. The response should be logged because a
multiplicative relationship is more sensible than an additive one, or because
it results in a relationship more in line with the assumptions of linear least
squares regression, not because it results in an apparently stronger fit.
4.3.2
LOGGED PREDICTOR VARIABLE
The other possibility for a semilog model is a regression model where the
response variable y is not logged, but the predictor x is. The functional
relationship this implies between y and x is
exp(y) = αxβ,
and thus corresponds to a multiplicative/additive relationship, with multiplica-
tive changes in x being associated with additive changes in y. Logging both
sides gives the relationship
y = log α + β log x ≡β0 + β1 log x.

4.4 Example — Predicting Movie Grosses After One Week
71
The slope βj in such a model is based on the usual interpretation of regression
slopes, except that adding one to log x corresponds to multiplying x by 10
(if common logs are used) or by e (if natural logs are used). That is, for
example for common logs, the model implies that multiplying xj by 10 is
associated with an estimated expected increase of ˆβj in y, holding all else in the
model fixed. Equivalently, but perhaps more meaningfully in practice, a 10%
increase in x is associated with an estimated expected increase of log(1.1) × ˆβj
in y, holding all else in the model fixed. Such a model is appropriate in the
situation where the response variable does not vary over a wide range while
the predictor does. For example, a model exploring the relationship between
a health outcome like life expectancy and a measure of wealth like gross
national income for a sample of countries is a situation where it is reasonable
to think that a proportional increase in income would be associated with an
additive change in life expectancy (implying that absolute increases in income
are “worth more” for low-income countries than they are for high-income
countries).
4.4
Example — Predicting Movie Grosses After
One Week
The movie industry is a business with a high profile, and a highly variable
revenue stream. In 2010, moviegoers spent more than $10 billion at the US
box office alone. A single movie can be the difference between tens of millions
of dollars of profits or losses for a studio in a given year. It is not surprising,
therefore, that movie studios are intensely interested in predicting revenues
from movies; the popular nature of the product results in great interest in
gross revenues from the general public as well.
The opening weekend of a movie’s release typically accounts for 35%
of the total domestic box office gross, so we would expect that the opening
weekend’s grosses would be highly predictive for total gross. In fact, this
understates the importance of the opening weekend. It is on the strength
of the opening weekend of general release that many important decisions
pertaining to a film’s ultimate financial destiny are made. Since competition
for movie screens is fierce, movie theater owners do not want to spend more
than the contractually obligatory two weeks on a film that doesn’t have “legs.”
Should a film lose its theatrical berth very quickly, chances are slim that it
will have significant play internationally (if at all), and it is less likely that
it will make it to pay-per-view, cable, or network television. This all but
guarantees that ancillary revenue streams will dry up, making a positive return
on investment virtually impossible to achieve, as ancillary deals are predicated
on domestic box office gross. Exhibitors often make the decision to keep a
film running based on the strength of its opening weekend. The ability to
predict total domestic gross after the first weekend of release is thus of great
importance.

72
CHAPTER 4 Transformations and Linearizable Models
0
20
40
60
80 100
140
0
200
400
600
Opening weekend gross
Total domestic gross
0
200
400
600
Total domestic gross
0
200
400
600
Total domestic gross
0
200
400
600
Total domestic gross
1000
2000
3000
4000
Opening screens
0
50
100
150
200
250
Production budget
20
40
60
80
100
Rotten Tomatoes rating
FIGURE 4.1: Scatter plots of total domestic gross versus opening weekend
gross (both in millions of dollars), number of opening screens, estimated
production budget (in millions of dollars), and Rotten Tomatoes rating,
respectively, for the movie grosses data.
Figure 4.1 gives plots related to an analysis based on the movies released in
the United States during 2009 that opened on more than 500 screens, which
can be viewed as a sample from the ongoing process of movie production and
release. The response variable is the total domestic (US) grosses, while potential
predictors are opening weekend gross (in millions of dollars), the number of
screens on which the movie opened, the estimated production budget when
reported (in millions of dollars), and the rating of the movie at the film review
aggregator web site Rotten Tomatoes (rottentomatoes.com). Note that
the first three predictors would certainly be available to a producer after the
opening weekend, and a general perception of the critical reaction to a movie
would be also, even if the exact Rotten Tomatoes rating might not be.
Several of the plots exhibit typical signs that the variables are better
analyzed in the logged scale: plots involving total domestic gross, opening
weekend gross, and budget all show bunching in the lower left corner, with
a gradual spreading out of the data points moving towards the upper right
corner. The relationship between total domestic gross and opening screens
also looks distinctly nonlinear.

4.4 Example — Predicting Movie Grosses After One Week
73
0.5
1.0
1.5
2.0
1.0
1.5
2.0
2.5
Logged opening weekend gross
Logged total domestic gross
1.0
1.5
2.0
2.5
Logged total domestic gross
1.0
1.5
2.0
2.5
Logged total domestic gross
1.0
1.5
2.0
2.5
Logged total domestic gross
1000
2000
3000
4000
Opening screens
−1.0
0.0 0.5 1.0 1.5 2.0 2.5
Logged production budget
20
40
60
80
100
Rotten Tomatoes rating
FIGURE 4.2: Scatter plots of logged total domestic gross versus logged open-
ing weekend gross, number of opening screens, estimated logged production
budget, and Rotten Tomatoes rating, respectively, for the movie grosses data.
All logs are base 10.
Figure 4.2 gives corresponding plots logging (base 10) the total domestic
gross, first weekend gross, and budget variables. It is apparent that the relation-
ships look much more consistent with the assumptions of linear least squares
regression. There are reasonably strong relationships with all of the variables
other than the Rotten Tomatoes rating. There is evidence of nonconstant
variance in the plot of logged total gross versus logged opening weekend
gross, and a very obviously unusual point to the left in the plot versus logged
production budget (a potential leverage point that corresponds to “The Last
House on the Left,” which had a reported production budget of only roughly
$100,000). The movie “Avatar” also shows up as unusually successful (and a
potential outlier) at the top of several of the plots.
The following output summarizes results for a regression fit of logged
total domestic gross on logged opening weekend gross, number of opening
screens, logged estimated production budget, and Rotten Tomatoes rating.
Coefficients:
Estimate Std.Err.
t val Pr(>|t|) VIF
(Intercept)
2.353e-01 4.75e-02
4.95 2.5e-06
***
Log.opening.gross 1.014e+00 4.87e-02
20.82 < 2e-16 2.93 ***

74
CHAPTER 4 Transformations and Linearizable Models
Screens
1.002e-06 2.61e-05
0.04 0.96947 3.27
Log.budget
8.657e-02 3.17e-02
2.73 0.00727 1.68 **
RT
1.619e-03 4.56e-04
3.55 0.00056 1.07 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.1191 on 115 degrees of freedom
Multiple R-squared: 0.9285,
Adjusted R-squared: 0.926
F-statistic: 373.3 on 4 and 115 DF,
p-value: < 2.2e-16
The model fit is very strong, with R2 over 92%, and a highly statistically
significant F-statistic. While logged opening weekend gross, logged budget,
and Rotten Tomatoes rating are all strongly statistically significant, given the
other predictors, the number of screens on which the movie opens does not
add any predictive power. This is consistent with the results of a best subsets
regression (output not given), which identifies the model with all of the
predictors other than number of screens as best. The output for this simplified
model is given below.
Coefficients:
Estimate Std.Err.
t val Pr(>|t|) VIF
(Intercept)
0.2358772
0.04474
5.27 6.3e-07
***
Log.opening.gross 1.0154911
0.03440
29.52 < 2e-16 1.47 ***
Log.budget
0.0870212
0.02924
2.98 0.00355 1.44 **
RT
0.0016163
0.00045
3.61 0.00046 1.04 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.1185 on 116 degrees of freedom
Multiple R-squared: 0.9285,
Adjusted R-squared: 0.9267
F-statistic: 502.1 on 3 and 116 DF,
p-value: < 2.2e-16
We first discuss the implications of the fit, although diagnostics
(Figure 4.3) indicate this might not be the final chosen model. The estimated
standard deviation of the errors ˆσ = .1185, but as was noted on page 38, this
should be adjusted to account for the model selection process. In this case the
adjustment proposed in equation (2.4) makes little difference, as ˜σ = .119.
This implies that these variables can predict total domestic gross to within a
multiplicative factor of 1.73, roughly 95% of the time (10(2)(.119) = 1.73).
Thus, it would not be surprising for a movie predicted to have a total gross
of $100 million to have an actual gross as large as $173 million or as small
as $58 million, which reflects the inherent high variability in movie grosses,
even based on a model that accounts for more than 92% of the variability in
logged grosses. The coefficients for logged opening weekend gross and logged
budget are elasticities, so they imply that holding all else fixed a 1% increase
in opening weekend gross is associated with an estimated expected 1.02%
increase in total domestic gross, and a 1% increase in production budget is
associated with an estimated expected 0.09% increase in total domestic gross,

4.4 Example — Predicting Movie Grosses After One Week
75
0
20
40
60
80
100
120
−2
0
1
2
Standardized residuals
Index
Std. residuals
0
20
40
60
80
100
120
0.0
0.2
0.4
Diagonal elements of the hat matrix
Index
Leverage
0
20
40
60
80
100
120
0.00
0.10
Cook's distances
Index
Cook's D
FIGURE 4.3: Index plots of diagnostics for the regression fit on the three-
predictor model for the movie grosses data, with guideline values superimposed
on the standardized residuals and leverage plots.
respectively. The coefficient for Rotten Tomatoes rating is a semielasticity,
implying that holding all else fixed an increase of one point in the rating is
associated with an estimated expected 0.4% increase in total domestic gross
(10.0016 = 1.004).
Regression diagnostics identify “The Last House on the Left” as an
extreme leverage point, as would be expected. After omitting this point, best
subsets regression still points to the same three-predictor model as best, and
the regression results change very little, although diagnostics (Figure 4.4) now
indicate several marginal leverage points (corresponding to “Next Day Air,”
“Ponyo,” and “The Twilight Saga: New Moon”).
Coefficients:
Estimate Std.Err.
t val Pr(>|t|) VIF
(Intercept)
0.2156550
0.04862
4.44 2.1e-05
***
Log.opening.gross 1.0006562
0.03712
26.96 < 2e-16 1.72 ***
Log.budget
0.1120680
0.03759
2.98 0.00350 1.71 **
RT
0.0015644
0.00045
3.47 0.00073 1.05 ***
---

76
CHAPTER 4 Transformations and Linearizable Models
0
20
40
60
80
100
120
−2
0.02
0.06
0
1
2
Standardized residuals
Index
0
20
40
60
80
100
120
Index
0
20
40
60
80
100
120
Index
Std. residuals
Diagonal elements of the hat matrix
Leverage
0.00
0.05
0.10
0.15
Cook's distances
Cook's D
FIGURE 4.4: Index plots of diagnostics for the three-predictor regression fit
after removing the leverage point for the movie grosses data, with guideline
values superimposed on the standardized residuals and leverage plots.
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.1185 on 115 degrees of freedom
Multiple R-squared: 0.9291,
Adjusted R-squared: 0.9273
F-statistic: 502.4 on 3 and 115 DF,
p-value: < 2.2e-16
Residual plots (Figure 4.5) illustrate more serious issues: residuals that are
somewhat right-tailed, and nonconstant variance, where movies with larger
estimated (logged) domestic grosses have higher variability. The latter issue
requires the use of weighted least squares, which will be discussed in Sections
6.3.3 and 10.7.
A natural use of this model is to forecast grosses for future movies.
Table 4.1 summarizes the results of such predictions based on this model
for movies released in January and February of 2010, giving 95% prediction
limits for total gross (obtained by antilogging the upper and lower limits of
prediction intervals based on the model for logged total domestic gross). It can

4.5 Summary
77
1.0
1.5
2.0
2.5
−2
0
1
2
3
−2
0
1
2
3
Fitted values
Std. residuals
−2
0
1
2
3
Std. residuals
−2
0
1
2
3
Std. residuals
−2
0
1
2
3
Std. residuals
−2
−1
0
1
2
Normal Q−Q Plot
Theoretical Quantiles
Sample Quantiles
0.5
1.0
1.5
2.0
Logged opening weekend gross
0.5
1.0
1.5
2.0
Logged production budget
20
40
60
80
100
Rotten Tomatoes rating
FIGURE 4.5: Residual plots for the three-predictor regression fit after remov-
ing the leverage point for the movie grosses data.
be seen that the model does a good job of predicting future grosses, with all of
the prediction intervals containing the actual total domestic gross values.
4.5
Summary
Linear least squares modeling makes several assumptions about the underlying
regression relationship in the population, which might not hold. Several viola-
tions of these assumptions, including multiplicative rather than additive errors
(which result in nonconstant variance) and certain forms of nonlinearity,
correspond to linearizable violations, in that using the target or predicting
variables (or both) in the logged scale can address them. The logged transfor-
mation is particularly useful in situations where a variable is long right-tailed.
When the target variable is logged the slope coefficients take the form of
elasticities (for logged predictors) or semielasticities (for unlogged predictors)
that have natural and intuitive interpretations in the original scale, making
this transformation a particularly attractive one in many situations.

78
CHAPTER 4 Transformations and Linearizable Models
Table 4.1: Total domestic grosses (in millions of dollars) for 2010 movies
with 95% prediction limits, based on using the model fit to 2009 data.
Movie
Lower
limit
Actual
gross
Upper
limit
Cop Out
27.88
44.88
83.03
Daybreakers
25.70
30.10
76.70
Dear John
46.17
80.01
138.26
Edge of Darkness
31.73
43.31
94.15
Extraordinary Measures
9.27
12.48
27.69
From Paris With Love
13.82
24.08
41.27
Leap Year
13.16
25.92
39.21
Legion
25.84
40.17
77.01
Percy Jackson and the Olympians
59.27
88.77
176.32
Shutter Island
82.05
128.01
244.73
The Book of Eli
60.85
94.84
180.81
The Crazies
27.56
39.12
82.50
The Tooth Fairy
21.82
60.02
65.12
The Wolf Man
59.30
62.19
178.00
Valentine’s Day
88.63
110.49
267.54
Youth in Revolt
11.57
15.29
34.63
KEY TERMS
Elasticity: The proportional change in the response corresponding to a
proportional change in a predictor. In economics this often corresponds to
a proportional change in demand of a product or service corresponding to a
proportional change in its price.
Log-log model: A relationship of the form E(y) = αxβ. This relationship
is linearizable, as the unknown parameters can be estimated from a linear
regression of log y on log x.
Semielasticity: The proportional change in the response corresponding to an
additive change in a predictor.
Semilog models: Relationships of the form y = αβx or exp(y) = αxβ. These
relationships are linearizable, as the unknown parameters can be estimated
from a linear regression of log y on x or y on log x, respectively.

Five
Chapter
Time Series Data and
Autocorrelation
5.1
Introduction
79
5.2
Concepts and Background Material
81
5.3
Methodology: Identifying Autocorrelation
83
5.3.1
The Durbin-Watson Statistic
83
5.3.2
The Autocorrelation Function (ACF)
84
5.3.3
Residual Plots and the Runs Test
85
5.4
Methodology: Addressing Autocorrelation
86
5.4.1
Detrending and Deseasonalizing
86
5.4.2
Example — e-Commerce Retail Sales
87
5.4.3
Lagging and Differencing
93
5.4.4
Example — Stock Indexes
94
5.4.5
Generalized Least Squares (GLS): The Cochrane-Orcutt
Procedure
99
5.4.6
Example — Time Intervals Between Old Faithful Geyser
Eruptions
100
5.5
Summary
104
5.1
Introduction
As was noted in Section 1.2.3, a standard assumption in regression modeling
is that the random errors εi are uncorrelated with each other. Correlation
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
79

80
CHAPTER 5 Time Series Data and Autocorrelation
in the errors represents structure in the data that has not been taken into
account. When the observations have a natural sequential order, and as a
result the correlation structure is related to that order, this correlation is called
autocorrelation. Although the issues we will discuss in this chapter can arise
in any situation where there is a natural sequential ordering to the data, we
will refer to it generically as time series data, since data ordered in time is
certainly the most common application area.
Autocorrelation occurs for several reasons. In time series data, it is often
the case that adjacent values are similar, with high values following high values
and low values following low values. Often, this is a result of the target variable
being subjected to similar external conditions from one period to the next.
Adjacent errors in economic data, which correspond to measurements from
consecutive time periods, such as days, months, or years, are often positively
correlated because of the effects of underlying economic processes that are
evolving over time. Adjacent experimental values, such as successive outputs
in a production process, can be positively correlated because they are affected
by similar short-term conditions on machinery.
Autocorrelation in time series data also can arise by omission of an
important predictor variable from the model. If the successive values of an
important omitted variable are correlated, the errors from a model that omits
this variable will tend to be autocorrelated, since the errors will reflect the effects
of the missing variable. This means that issues of model selection for time
series data can become conflated to some extent with issues of autocorrelation.
Measures designed to compare models that do not account for autocorrelation
can be misleading in the presence of autocorrelation, complicating the ability
to identify appropriate choices of variables.
In this chapter, we will discuss some of the issues related to building
regression models for time series data. We will first discuss the effects of
autocorrelation if it is ignored. We will then examine several approaches to
identifying autocorrelation, which range from one requiring strong assump-
tions (the Durbin-Watson statistic) to one related to a simple graphical
examination of residuals that requires virtually no assumptions (the runs
test). We then discuss several relatively simple approaches to accounting for
common forms of autocorrelation, including trends and seasonal effects, and
explore how values from previous time periods can be used to enrich a regres-
sion model and account for autocorrelation. We conclude with discussion
of a more sophisticated approach to handling autocorrelation that moves
past ordinary least squares estimation to estimation designed for time series
data.
It is important to note that the methods discussed here only scratch the
surface of time series analysis and modeling. It would not be at all surprising if
an analyst finds that using the methods discussed in this chapter will account
for much (or even most) of the observed autocorrelation in a data set, but
in many cases, there will still be apparent autocorrelation that requires more
complex methods. That is beyond the scope of this book, but there are many
books that discuss such methods that can be consulted, such as Cryer and
Chan (2008) and Kedem and Fokianos (2002).

5.2 Concepts and Background Material
81
5.2
Concepts and Background Material
Autocorrelation can have several problematic effects on a fitted model.
1. Least squares estimates are unbiased, but are not efficient, in the sense
that they do not have minimum variance. Since the degree to which
the OLS estimates are inefficient depends on the type and amount
of autocorrelation in both the errors and the predictor, we need to
specify a particular form of autocorrelation to explore this. A first-order
autoregressive [AR(1)] process satisfies
εi = ρεi−1 + zi,
|ρ| < 1,
(5.1)
where the zi are independent and identically distributed normally dis-
tributed random variables. The standard assumption underlying least
squares is that ρ = 0, and in that situation, the OLS estimator has the
minimum variance possible among all unbiased estimators, but if ρ ̸= 0
that is no longer the case [in this case ρ = corr(εi, εi−1)]. Figure 5.1
illustrates this point graphically for a simple regression model where the
predictor x follows an AR(1) process with parameter λ. The panels of
the plot correspond to values of λ equal to 0, 0.1, 0.3, 0.5, 0.7, and 0.9
from the lower left to the upper right, with each plot giving the ratio of
the variance of the OLS estimate of β1 to that of the unbiased estimator
with minimum variance for large samples versus the autocorrelation of
the errors ρ. It can be seen that while the OLS estimator is not very
inefficient for all λ when |ρ| < 0.3 (with plotted inefficiency close to the
horizontal line corresponding to a ratio equal to 1), for larger amounts of
autocorrelation in the errors its variance can be considerably larger than
the minimum possible value. In Section 5.4.5, we will discuss construction
of the estimator that has minimum variance for all ρ for this situation.
2. The estimates of the standard errors of the regression coefficients and
of σ2 are biased. Figure 5.2 illustrates this point. Each panel of the
plot gives the percentage bias of the usual estimate of the variance of
the OLS ˆβ1 for large samples as a function of ρ for a given λ. The
solid horizontal line at 0 corresponds to no bias, while the dashed line
corresponds to a bias of −100% (the lowest possible value). It can be
seen that when ρ and λ are nonzero with the same sign (the right side of
each panel), the estimated variance is negatively biased (often extremely
biased). This means that t-statistics will be larger than they should be
(since the square root of the estimated variance is the denominator of
the t-statistic), resulting in a spurious impression of precision. This is the
typical situation for economic data (where both ρ and λ will be positive).
If the two autocorrelation parameters are of opposite sign (the left side of
each panel), the estimated variances are positively biased, and measures of
the strength of the regression will be too small rather than too large. Note
that while there is no requirement that predictors lack autocorrelation to
justify least squares (which is reflected in the bias equaling 0 when ρ = 0
for all values of λ), autocorrelation of the predictors does have a strong

82
CHAPTER 5 Time Series Data and Autocorrelation
ρ
Inefficiency of OLS estimates
2
4
6
8
10
12
−0.5
0.0
0.5
λ : { 0 }
λ : { 0.1 }
−0.5
0.0
0.5
λ : { 0.3 }
λ : { 0.5 }
−0.5
0.0
0.5
λ : { 0.7 }
2
4
6
8
10
12
λ : { 0.9 }
FIGURE 5.1: Inefficiency of OLS estimator compared to minimum variance
estimator under first-order autoregressive errors, with first-order autoregressive
predictor; ρ = corr(εi, εi−1) and λ = corr(xi, xi−1).
effect on the consequences of autocorrelation in the errors if it exists, with
stronger effects as |λ| increases. Not only does the direction of the bias
depend on the sign of λ, the bottom left panel of the figure shows that for
this type of time series structure, there is no bias in the estimated variances
if λ = 0 for all values of ρ.
3. As a result of this bias, confidence intervals, significance tests, and
prediction intervals are no longer valid.
Given the seriousness of the autocorrelation problem, corrective action
to address it should be taken. The appropriate action depends on the source
of the autocorrelation. If autocorrelation is due to the absence of a variable
or other structure (such as seasonal effects) in the model, once the necessary
structure is included, the problem is reduced or disappears. On the other
hand, if autocorrelation is an inherent part of the population error structure,
addressing this autocorrelation requires transformation or changing the least
squares criterion.

5.3 Methodology: Identifying Autocorrelation
83
ρ
Percentage bias of estimated variance
0
200
400
600
800
−0.5
0.0
0.5
λ : { 0 }
λ : { 0.1 }
−0.5
0.0
0.5
λ : { 0.3 }
λ : { 0.5 }
−0.5
0.0
0.5
λ : { 0.7 }
0
200
400
600
800
λ : { 0.9 }
FIGURE 5.2: Percentage bias of the estimated variance of the OLS estimator
of β1 under first-order autoregressive errors, with first-order autoregressive
predictor.
5.3
Methodology: Identifying Autocorrelation
We describe three tests for detecting autocorrelation that range from strongly
parametric (based on a set of strong assumptions about the underlying error
process) to nonparametric (based on very weak assumptions).
5.3.1
THE DURBIN-WATSON STATISTIC
The Durbin-Watson statistic is the most widely used test for detecting
autocorrelation. The test is strongly parametric, being based on the assumption
that the errors follow the AR(1) process (5.1). The situation where there is
no autocorrelation (ρ = 0) corresponds to all of the standard least squares
assumptions: that is, the errors are independent and identically distributed
Gaussian random variables. Further, if autocorrelation exists, it is assumed
that it takes the specific AR(1) form. It can be shown that the AR(1) process

84
CHAPTER 5 Time Series Data and Autocorrelation
on the errors implies that the autocorrelations geometrically decay as the lag
(the gap between the observations) increases; that is,
ρk ≡corr(εi, εi−k) = ρk.
(5.2)
While the implication that errors that are farther apart are more weakly
correlated is often reasonable, the errors will often have a more complex
correlation structure than this, so the adequacy of the Durbin-Watson statistic
depends on (5.1) [and hence (5.2)] being a reasonable approximation to
reality.
In its standard form the Durbin-Watson statistic d tests the hypotheses
H0 : ρ = 0
versus
Ha : ρ > 0.
The test statistic is defined as
d =
n
i=2 (ei −ei−1)2
n
i=1 e2
i
,
where ei is the ith least squares residual. A drawback of this statistic is that it
is not pivotal; that is, its distribution (and hence critical values used to apply
it as a hypothesis test) depends on unknown parameters. Durbin and Watson
(1951) showed that the statistic is asymptotically pivotal, and further showed
that a set of two critical values {dL, dU} can be used to implement the test as
follows:
1. If d < dL, reject H0.
2. If d > dU, do not reject H0.
3. If d ∈(dL, dU), the test is inconclusive.
Critical values {dL, dU} have been tabulated by many authors and are
available on the Internet, including at the web-site for this book. Alternatively,
for large n (say n > 100), a normal approximation to the distribution of d can
be used,
z =
d
2 −1
 √n.
This form of the test shows that a value of d close to 2 is indicative of the
absence of autocorrelation. Some statistical packages provide an exact tail
probability for the test based on its null distribution (a linear combination of
χ2 variables).
Tests for negative autocorrelation are performed more rarely than for
positive autocorrelation. If a test is desired for ρ < 0, the appropriate test
statistic is 4 −d, and the procedure outlined above is then followed.
5.3.2
THE AUTOCORRELATION FUNCTION (ACF)
Use of the Durbin-Watson statistic is based on assuming an AR(1) process for
the errors, so it is important to check whether that holds. This can be done

5.3 Methodology: Identifying Autocorrelation
85
through use of an autocorrelation function (ACF) plot. In this plot estimates
of autocorrelations for a range of lags are plotted, often with approximate
95% confidence bands around 0 superimposed. The estimate of the kth-lag
autocorrelation is
ˆρk =
n
i=k+1 eiei−k
n
i=1 e2
i
.
If there is no autocorrelation, the standard error of ˆρk satisfies s.e.(ˆρk) ≈1/√n,
implying that ˆρk values greater than roughly 2/√n are significantly different
from 0 at a .05 level. Examination of the ACF plot can show if there is
any evidence of autocorrelation in the residuals, if observed autocorrelation is
consistent with an AR(1) process (by seeing if the estimated autocorrelations
follow a roughly geometric decay), or if other forms of autocorrelation (such
as seasonality) are present.
5.3.3
RESIDUAL PLOTS AND THE RUNS TEST
The presence of autocorrelation in a given set of time series data can be
detected by an examination of an index plot of the values in time order. As was
noted on page 15, a standard approach for the detection of autocorrelation
of regression errors is the corresponding index plot of the (standardized)
residuals. This is particularly helpful in the presence of underlying positive
autocorrelation of the errors, as this corresponds to a positive (negative) error
in one time period being associated with a positive (negative) error in the
next time period. In this case, the residual plot has a distinctive cyclical
pattern, where residuals of the same sign are clustered, with positive residuals
tending to follow positive residuals and negative residuals tending to follow
negative residuals. The corresponding pattern for negative autocorrelation,
with positive residuals tending to follow negative residuals (and vice versa),
also can occur, but this is difficult to see in a residual plot.
These patterns of same-signed residuals either following or not following
each other is the principle underlying a nonparametric test of autocorrelation.
The runs test formalizes the detection of residual clustering by counting the
number of runs of residuals of the same sign. For example, in a series of
residuals with signs + + + + −−−−+ + −−−−−−+ + + + + −+ −
−−−−++, there are n+ = 14 positive residuals, n−= 16 negative residuals,
and u = 9 runs (a run of 4 positive residuals, followed by a run of 4 negative
residuals, and so on). For small sample sizes, the null distribution of the number
of runs can be determined exactly on the basis of all possible permutations of
pluses and minuses, while for larger sample sizes if there is no autocorrelation,
u is roughly normally distributed with mean
μ = 2n+n−
n
+ 1
and variance
σ2 = 2n+n−(2n+n−−n)
n2(n −1)
.

86
CHAPTER 5 Time Series Data and Autocorrelation
Too few runs correspond to positive autocorrelation, while too many runs
correspond to negative autocorrelation, thus providing a test of the null
hypothesis that the errors are independent and identically distributed. The
runs test has the advantage of being a nonparametric test, not requiring any
assumptions about the underlying distribution of the errors (other than that
there is a common distribution for all errors).
5.4
Methodology: Addressing Autocorrelation
5.4.1
DETRENDING AND DESEASONALIZING
The time series literature is very extensive, and it is beyond the scope of this
book (and most routine regression analyses) to cover that material in detail.
Fortunately, it turns out that many autocorrelation problems can be addressed
to a large (if not complete) extent using relatively simple methods that should
be part of the data analyst’s toolkit. Indeed, simply including appropriate
predictors in a regression model can often take initially strong autocorrelation
in the target variable and turn it into little or no autocorrelation in the
residuals, accounting for the problem almost completely.
Temporal data (observations taken over time) often have two characteris-
tics, trend and seasonality. Trend is the general movement in the data, going
up or down over the period of observation. Since many variables naturally
grow over time (for example, family income because of inflation or national
production because of population growth), it is reasonable to incorporate
such growth into the model. This can be done in two simple ways. First,
variables (especially response variables) should be modeled in a natural scale
that is comparable across time periods if at all possible. Thus, if incomes
grow naturally because of inflation, they should be corrected for inflation
by using an appropriate price deflator so they are in constant dollars rather
than current dollars. Similarly, if production grows naturally with population,
population-corrected per capita measures should be used.
A second approach (which can still be useful even if the variables have
been rescaled as just described) is to incorporate a time trend into the model.
The trend is often modeled by incorporating a linear or quadratic term (using
time and perhaps time2 as predictors). The need for such detrending can
be assessed using the model building methods described in Chapter 2. In
situations where the growth is exponential, the semilog model described in
Section 4.3 is appropriate, with the target variable logged and time entering
the model unlogged.
Temporal data also often have a component that varies with time,
representing the effects of the season. For example, monthly sales data often
exhibit the recurring pattern of sales being higher than normal at the end
of the year (because of the Christmas season) and lower than normal at the
beginning of the year (the aftermath of the Christmas season). This sort

5.4 Methodology: Addressing Autocorrelation
87
of pattern would show up as persistent monthly patterns in monthly data,
or persistent quarterly patterns for quarterly data. It can often be identified
by examining the residuals appropriately. For example, a seasonal effect in
quarterly data can show up in an ACF plot as a significant autocorrelation at
lag 4 (and multiples of 4), since residuals that are four quarters apart follow
the persistent seasonal pattern by being one year apart; similarly, a monthly
seasonal effect can show up as a significant autocorrelation at lag 12 (and
multiples of 12). Side-by-side boxplots of residuals separated by quarter or
month also can uncover seasonal effects.
Seasonal effects can be taken into account using a set of indicator variables
as a generalization of the analysis discussed in the presence of data with two
subgroups in Section 2.4 (since quarterly data fall into four distinct subgroups,
monthly data fall into 12 distinct subgroups, and so on). An indicator variable
is defined for each subgroup (quarter or month) with one of the indicators
omitted to account for the presence of an intercept term in the model (this
approach is discussed in much more detail in Chapter 6). This procedure
accounts for systematic shifts in the target variable from seasonal effects
(that is, it corresponds to fitting a constant shift model, with the regression
(hyper)plane shifted up or down by season).
It is often the case that trend and seasonal effects are viewed as nuisance
effects, not being related to the contextual relationships of interest to the
researcher. Removing trend and seasonality from the data in these ways thus
allows a more focused examination of the structure of the data after these
effects have been taken into account.
5.4.2
EXAMPLE — E-COMMERCE RETAIL SALES
Electronic (e-)commerce is a multibillion dollar business, encompassing online
sales sites (such as Amazon) and auction sites (such as eBay); indeed, it is
hard to imagine almost any retail business not having some sort of online
sales presence. The importance of this business sector makes it important to
understand the dynamics of e-commerce sales. Figure 5.3 gives a time series
plot of the quarterly e-commerce retail sales (in millions of dollars) for the
United States from the fourth quarter of 1999 through the first quarter of 2011,
based on information from the US Census Bureau. Two characteristics of this
series are immediately apparent: sales trended upwards during the decade, and
there is a clear seasonal effect, with sales peaking in the fourth quarter and
then dropping sharply in the first quarter (this of course corresponds to the
effects of holiday shopping).
These patterns are characteristic of retail sales in general, so it is reasonable
to think that total sales could be a good predictor for e-commerce sales (this
assumes, of course, that the underlying relationships during this time period
remain at least roughly the same in the future). Figure 5.4 is a scatter plot of
e-commerce sales versus total sales, and it is clear that there is the expected
direct relationship between the two.

88
CHAPTER 5 Time Series Data and Autocorrelation
2000
2002
2004
2006
2008
2010
10000
20000
30000
40000
50000
Year
e−Commerce retail sales
FIGURE 5.3: Time series plot of quarterly U.S. e-commerce retail sales.
Output for the regression of e-commerce sales on total sales is given below.
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) -7.396e+04
8.120e+03
-9.108 1.10e-11 ***
Total.sales
1.103e-01
9.109e-03
12.104 1.35e-15 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 6270 on 44 degrees of freedom
Multiple R-squared: 0.769,
Adjusted R-squared: 0.7638
F-statistic: 146.5 on 1 and 44 DF,
p-value: 1.348e-15
There is a highly statistically significant relationship between e-commerce
sales and total sales. Unfortunately, autocorrelation is apparent in the residuals,
as Figure 5.5 shows. The time series plot of the standardized residuals in plot
(a) shows a cyclical pattern indicative of positive autocorrelation. The ACF
plot in plot (b) shows that there are significant autocorrelations at many lags,
including lags 1–4 and 8.

5.4 Methodology: Addressing Autocorrelation
89
7e+05
8e+05
9e+05
1e+06
10000
20000
30000
40000
50000
Total retail sales
e−Commerce retail sales
FIGURE 5.4: Scatter plot of e-commerce retail sales versus total retail sales.
2000
2004
2008
−1
0
1
2
(a)
(b)
Year
Standardized residuals
0
5
10
15
−0.2
0.2
0.6
Lag
ACF
FIGURE 5.5: Residual plots for the e-commerce regression fit based on total
retail sales. (a) Time series plot of standardized residuals. (b) ACF plot of
residuals, with superimposed 95% confidence limits around 0.
The jumps in the autocorrelation at lags 4 and 8 are particularly interesting,
since the quarterly nature of the data implies that these are likely to represent
seasonality. This is further supported in Figure 5.6, which is a set of side-by-side
boxplots of the standardized residuals separated by quarter. Remarkably,
however, the seasonal effect is not that sales are higher than expected in the

90
CHAPTER 5 Time Series Data and Autocorrelation
Q1
Q2
Q3
Q4
−1
0
1
2
Standardized residuals
FIGURE 5.6: Side-by-side boxplots of the standardized residuals for the
e-commerce regression fit based on total retail sales.
fourth quarter and lower than expected in the first quarter, as the original time
series plot of e-commerce sales would have implied; rather, e-commerce sales
are higher than expected in the first quarter. The reason for this is that while
the holiday seasonal effect does exist in the e-commerce sales, it is actually
weaker than the corresponding effect in the total sales; when total sales are
taken into account, first quarter e-commerce sales look better than expected
because they are not as relatively poor as those of sales overall are. This
highlights that the autocorrelation structure in the residuals (and presumably
the errors), which take predictor(s) into account, can be very different from
that in the original response variable.
This seasonality effect can potentially be accounted for by adding indicator
variables for any set of three quarters. The resultant output (based on including
indicators for the first three quarters) follows.
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) -8.538e+04
7.667e+03 -11.136 5.68e-14 ***
Total.sales
1.217e-01
8.129e-03
14.972
< 2e-16 ***
Quarter.1
7.564e+03
2.275e+03
3.325
0.00187 **
Quarter.2
-2.108e+03
2.199e+03
-0.959
0.34328
Quarter.3
-7.620e+02
2.203e+03
-0.346
0.73122
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 5241 on 41 degrees of freedom
Multiple R-squared: 0.8496,
Adjusted R-squared: 0.835
F-statistic: 57.92 on 4 and 41 DF,
p-value: 2.49e-16
The quarterly indicators add significant predictive power (the partial F-test
comparing the models with and without the seasonal indicators is F = 7.33
on (3, 41) degrees of freedom, with p = .0005, reflecting the increase in R2
from 77% to 85%). The coefficient for Quarter.1 implies that first quarter

5.4 Methodology: Addressing Autocorrelation
91
2000
2004
2008
−1
0
1
2
(a)
Year
Standardized residuals
0
5
10
15
−0.2
0.2
0.6
Lag
ACF
(b)
FIGURE 5.7: Residual plots for the e-commerce regression fit based on total
retail sales and quarterly indicators. (a) Time series plot of standardized
residuals. (b) ACF plot of residuals.
e-commerce sales are estimated to be on average $7.56 billion higher than
fourth quarter sales, given total sales are held fixed.
Unfortunately, while this has addressed the seasonality to a large extent,
it has not addressed all of the problems. Figure 5.7 shows that there is a clear
break in the residuals, with e-commerce sales higher than expected starting
with the fourth quarter of 2008. This is reflecting the relative insensitivity of
e-commerce sales to the worldwide recession that began in late 2008; while
total retail sales went down during that time, e-commerce sales continued to
go up. It is also interesting to note that the ACF plot [Figure 5.7(b)] is unable
to identify this pattern, instead pointing to an AR(1)-like slow decay of the
autocorrelations. This reinforces the importance of looking at the residuals in
different graphical ways, and not just depending on test statistics and one kind
of plot to identify problems.
The following output summarizes a regression model that adds a constant
shift corresponding to quarters during the recession. The indicator for the
recession is highly statistically significant, implying $11.9 billion higher
estimated expected e-commerce sales during the recession given the quarter
and the total retail sales.
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) -7.208e+04
2.937e+03 -24.540
< 2e-16 ***
Total.sales
1.041e-01
3.181e-03
32.721
< 2e-16 ***
Quarter.1
5.888e+03
8.432e+02
6.983 1.98e-08 ***
Quarter.2
-1.773e+03
8.090e+02
-2.191
0.0343 *
Quarter.3
-5.199e+02
8.106e+02
-0.641
0.5250
Recession
1.193e+04
7.356e+02
16.217
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 1928 on 40 degrees of freedom
Multiple R-squared: 0.9802,
Adjusted R-squared: 0.9777
F-statistic:
395 on 5 and 40 DF,
p-value: < 2.2e-16

92
CHAPTER 5 Time Series Data and Autocorrelation
At this point the R2 of the model is over 98%, and most of the
autocorrelation is accounted for (see Figure 5.8). There are three quarters
with unusually high e-commerce sales (the fourth quarters of 2007, 2009, and
2010, respectively), but otherwise the plot of standardized residuals versus
fitted values, time series plot of the standardized residuals, and normal plot
of the standardized residuals look reasonable. The ACF plot of the residuals
flags a significant autocorrelation at lag 4, suggesting some sort of seasonality
that was not accounted for by the indicators. This is quite possible, as much
more complex deseasonalizing methods (such as the X-13ARIMA-SEATS
deseasonalizing method used by the U.S. Census Bureau; see U.S. Census
Bureau, 2017) are often used with economic data. Side-by-side boxplots of
10000
20000
30000
40000
50000
−1
0
1
2
(b)
(a)
(d)
(c)
(f)
(e)
Fitted values
Standardized residuals
2000 2002 2004 2006 2008 2010
−1 0
1
2
Year
Standardized residuals
−2
−1
0
1
2
−1
0
1
2
Theoretical Quantiles
Sample Quantiles
0
5
10
15
−0.2
0.2 0.4
Lag
ACF
Q1
Q2
Q3
Q4
−1
0
1
2
Standardized residuals


0
1
−1
0
1
2
Recession
Standardized residuals
FIGURE 5.8: Residual plots for the e-commerce regression fit based on
total retail sales, quarterly indicators, and recession indicator. (a) Plot of
standardized residuals versus fitted values. (b) Time series plot of standard-
ized residuals. (c) Normal plot of standardized residuals. (d) ACF plot of
residuals. (e) Side-by-side boxplots of standardized residuals separated by
quarter. (f) Side-by-side boxplots of standardized residuals separated by pre-
or post-2008 recession.

5.4 Methodology: Addressing Autocorrelation
93
the standardized residuals suggest potential heteroscedasticity related to both
quarter (with fourth quarter residuals more variable) and recession (with
pre-recession quarters more variable); ways to address such heteroscedasticity
is the subject of Section 6.3.3.
Given the apparent non-AR(1) nature of the autocorrelation and het-
eroscedasticity the Durbin-Watson statistic is not appropriate. The runs test,
however, is (virtually) always appropriate, and in this case when applying it to
the residuals it has p = .008, reinforcing that there is still some autocorrelation
present in the residuals.
5.4.3
LAGGING AND DIFFERENCING
The values of variables from previous time periods can often be useful in
modeling the value from a current time period in time series data. This can
reflect a natural time lag of the effect of a predictor on the response variable;
for example, the Federal Reserve Bank changes interest rates in order to affect
the dynamics of the US economy, but it is expected that such effects would
take one or two quarters to be felt through the economy. Thus, it would be
natural to use a lagged value of interest rate as a predictor of gross national
income (that is, using xi−1 or xi−2 to model yi for quarterly data). Lagged
predictors are also appropriate if a regression model is intended to represent
a future forecasting relationship (rather than a description of an underlying
stochastic process), since future values of the predictors would not be available
when making forecasts about the future.
A different use of a lagged variable as a predictor is using a lagged version
of the response variable itself as a predictor; that is, using for example yi−1 as
a predictor of yi, resulting in
yi = β0 + β1x1i + · · · + βpxpi + βp+1yi−1 + εi
(5.3)
(in principle further lags, such as yi−2 or yi−3, could be used). Including
a lagged response as a predictor will often reduce autocorrelation in the
errors dramatically, as it directly models the tendency for time series values
to move in a cyclical pattern. This fundamentally changes the interpretation
of other regression coefficients, as they now represent the expected change
in the response corresponding to a one unit change in the predictor holding
the previous time period’s value of the response fixed (as well as holding
everything else in the model fixed), but from a predictive point of view can
dramatically improve the predictive power of a model while reducing the
effects of autocorrelation.
A related operation is differencing variables; that is, modeling changes
in the response value, rather than the value itself. Formally, this corresponds
to a special case of (5.3) with βp+1 = 1. Differencing a variable also can be
meaningful contextually, as in many situations, the change in the level of a
variable is more meaningful than the level itself. The example that follows
based on stock prices is such an example, since it is returns (the proportional
change in prices) that are meaningful to an investor, not prices themselves. It is

94
CHAPTER 5 Time Series Data and Autocorrelation
standard practice in time series modeling in general to difference nonstationary
series (time series where the distribution of the series changes over time) for
exactly this reason.
The Durbin-Watson statistic is not meaningful for a model using lagged
response values as a predictor, and should not be used in that situation.
5.4.4
EXAMPLE — STOCK INDEXES
The Standard & Poor’s (S&P) stock indexes are well-known value-weighted
indexes of stock prices of publicly held firms. The S&P 500 is based on 500
large capitalization firms, the S&P 400 is based on 400 mid-capitalization
firms, and the S&P 600 is based on 600 small capitalization firms. It would be
expected that such indexes would move together based on the overall health
of the economy, but it is not clear exactly what those relationships might be.
Figure 5.9 gives scatter plots of the daily S&P 500 (large-cap) index versus
the S&P 600 (small-cap) and S&P 400 (mid-cap) indexes, respectively, for all
trading days from 2006 through 2010. Clearly, there is a direct relationship
between the indexes, but the plots seem to suggest several separate regimes.
As was noted in Chapter 4, it is often the case that money data are better
analyzed in the logged scale. Figure 5.10 gives corresponding scatter plots
based on (natural) logged index values, which still have an apparent pattern of
different regimes in the series but appear to reflect more linear relationships
with less heteroscedasticity.
Regression output for the regression of logged large-cap index on logged
small-cap and logged mid-cap indexes follows.
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
1.57402
0.05949
26.458
< 2e-16 ***
log.S.P.Small.cap
1.25458
0.03835
32.715
< 2e-16 ***
log.S.P.Mid.cap
-0.27745
0.03910
-7.096 2.14e-12 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.04833 on 1256 degrees of freedom
Multiple R-squared: 0.9284,
Adjusted R-squared: 0.9283
F-statistic:
8145 on 2 and 1256 DF,
p-value: < 2.2e-16
The regression relationship is very strong, but autocorrelation is a serious
problem. Figure 5.11 illustrates the strong cyclical pattern in the standardized
residuals, consistent with a nonstationary time series that has mean value that
shifts up and down. The ACF plot is consistent with this, in that the estimated
autocorrelations of the residuals are very large, and decay very slowly. The
Durbin-Watson statistic is d = .026, which is extremely strongly statistically
significant.
Both context and the statistical results imply that differencing the data is
appropriate. As was noted on page 93, it is not stock price (or index value)

5.4 Methodology: Addressing Autocorrelation
95
200
300
400
800
1200
1600
(a)
S&P Small−cap Index
S&P Large−cap index
400
600
800
800
1200
1600
(b)
S&P Mid−cap Index
S&P Large−cap index
FIGURE 5.9: Scatter plots of daily S&P 500 (large-cap) index values from
2006 through 2010 versus (a) S&P 600 (small-cap) index values, and (b) S&P
400 (mid-cap) index values.
5.2
5.4
5.6
5.8
6.0
6.6
7.0
(a)
Logged S&P Small−cap Index
Logged S&P Large−cap index
6.0
6.2
6.4
6.6
6.8
6.6
7.0
(b)
Logged S&P Mid−cap Index
Logged S&P Large−cap index
FIGURE 5.10: Scatter plots of logged daily S&P 500 (large-cap) index values
versus (a) logged S&P 600 (small-cap) index values, and (b) logged S&P 400
(mid-cap) index values.
2006
2008
2010
−2
0
1
2
Year
(a)
(b)
Standardized residuals
0
5
10
15
20
25
30
0.0
0.4
0.8
Lag
ACF
FIGURE 5.11: Residual plots for logged large-cap index regression fit based
on logged small-cap and logged mid-cap indexes. (a) Time series plot of
standardized residuals. (b) ACF plot of residuals.

96
CHAPTER 5 Time Series Data and Autocorrelation
that matters to an investor, but rather stock return. The return is defined as
the proportional change in price, or
ri = pi −pi−1
pi−1
=
pi
pi−1
−1.
Consider now prices in the natural log scale. Differencing this variable yields
log pi −log pi−1 = log
pi
pi−1
= log

1 +
pi
pi−1
−1

= log(1 + ri);
since the return ri is usually close to 0, a Taylor series expansion yields
log(1 + ri) ≈ri (Taylor series expansions will be discussed more fully in
Section 12.3.1). That is, the differenced logged price series is roughly equal to
the return series, and for this reason the differenced logged price is referred to
as the log return in the finance and economics literature.
The slowly-decaying autocorrelations in Figure 5.11(b) are a common
symptom of the need to difference a series. We can also see this in the following
output of the regression of logged S&P large-cap price on lagged logged S&P
large-cap price:
Coefficients:
Estimate Std.Error t value Pr(>|t|)
(Intercept)
0.02699
0.017421
1.55
0.122
lag.log.S.P.Large.cap 0.99619
0.002456 405.57
<2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.01573 on 1256 degrees of freedom
(1 observation deleted due to missingness)
Multiple R-squared: 0.9924,
Adjusted R-squared: 0.9924
F-statistic: 1.645e+05 on 1 and 1256 DF,
p-value: < 2.2e-16
The slope coefficient is very close to 1, highlighting that differencing the
response variable (that is, using log returns) is an appropriate strategy. The
residuals from this regression exhibit autocorrelation [Figure 5.12(b)], and they
now reflect two typical properties of stock returns: they are long-tailed relative
to the normal distribution [Figure 5.12(c)], and they exhibit heteroscedasticity,
with periods of low variability randomly alternating with periods of high
variability [Figure 5.12(a)], with particularly high variability in late 2008.
Both of these violations of least squares are consistent with certain time series
models, such as ARCH, GARCH, and stochastic volatility models, which
are often used to model stock returns. Such models are beyond the scope of
this book, but see Gregoriou (2009) for further discussion of their use for
modeling the volatility of stock returns.
These results suggest regressing large-cap returns on small-cap and mid-cap
returns, and output for that model is given below. Note that log returns

5.4 Methodology: Addressing Autocorrelation
97
2006 2007 2008 2009 2010 2011
−6 −4 −2
0
2
4
6
(a)
(c)
Year
Standardized residuals
0
5
10
15
20
25
30
−0.10
0.00 0.05
Lag
ACF
(b)
−3
−2
−1
0
1
2
3
−6 −4 −2
0
2
4
6
Theoretical Quantiles
Sample Quantiles
FIGURE 5.12: Residual plots for the logged large-cap index regression fit
based on the lagged logged large-cap index. (a) Time series plot of standardized
residuals. (b) ACF plot of residuals. (c) Normal plot of standardized residuals.
(differenced logged prices) are used here, but results using actual returns are
virtually identical.
Coefficients:
Estimate Std.Error t value Pr(>|t|)
(Intercept)
-0.000142
0.000124
-1.14
0.25289
S.P.Small.cap.return -0.095647
0.029382
-3.26
0.00116 **
S.P.Mid.cap.return
0.965094
0.031271
30.86
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.004411 on 1255 degrees of freedom
(1 observation deleted due to missingness)
Multiple R-squared: 0.9215,
Adjusted R-squared: 0.9214
F-statistic:
7370 on 2 and 1255 DF,
p-value: < 2.2e-16
The regression is highly statistically significant, with 92% of the variability
in large-cap returns accounted for by mid-cap and small-cap returns. The rela-
tionship is strongest between large- and mid-cap returns, but the negative slope

98
CHAPTER 5 Time Series Data and Autocorrelation
of small-cap returns suggests that including them in a portfolio with mid-cap
funds could provide useful diversification. The residuals exhibit autocorre-
lation, and the heteroscedasticity and long tails noted earlier (Figure 5.13),
implying that more complex time series models are needed for these data. On
the other hand, the runs test has p = .31, not pointing to autocorrelation.
Note that measures of strength of fit like R2 and the overall F-statistic are
not comparable between models using undifferenced and differenced response
variables. The model on page 96 for logged large-cap index has R2 = 99.2%
while that for large-cap return above has R2 = 92.2%, yet the latter model
clearly reflects a stronger relationship, as seen in the much smaller value of ˆσ
(these values are comparable, since the model for log return is a special case of
a model for logged price that uses lagged logged price as a predictor and sets
the slope to 1).
−0.10
−0.05
0.00
0.05
−6
−2
2
6
(a)
Fitted values
Standardized residuals
2006
2007
2008
2009
2010
2011
−6
−2
2
6
(b)
Year
Standardized residuals
−3
−2
−1
0
1
2
3
−6
−2
2
6
(c)
Theoretical Quantiles
Sample Quantiles
0
5
10
15
20
25
30
−0.10
0.00
0.10
Lag
ACF
(d)
−0.10
−0.05
0.00
0.05
−6
−2
2
6
(e)
Small−cap log return
Standardized residuals
−0.10
−0.05
0.00
0.05
0.10
−6
−2
2
6
(f)
Mid−cap log return
Standardized residuals
FIGURE 5.13: Residual plots for the large-cap log return regression fit based
on small-cap log return and mid-cap log return. (a) Plot of standardized
residuals versus fitted values. (b) Time series plot of standardized residuals.
(c) Normal plot of standardized residuals. (d) ACF plot of residuals. (e) Scatter
plot of standardized residuals versus small-cap log returns. (f) Scatter plot of
standardized residuals versus mid-cap log returns.

5.4 Methodology: Addressing Autocorrelation
99
5.4.5
GENERALIZED LEAST SQUARES (GLS):
THE COCHRANE-ORCUTT PROCEDURE
All of the methods discussed in the previous section fundamentally change
the regression model being fit, whether it is by adding predictors (represent-
ing time trends, seasonal effects, or the lagged response) or changing the
response completely (through differencing). Sometimes the original relation-
ship hypothesized is the specific one of interest, and converting to a question
about differences (for example) is not desired. The problem is then that OLS
is an inappropriate criterion to use to fit the model since the presence of
autocorrelation is a violation of assumptions.
A solution to this problem is to use the “correct” criterion; that is,
the one for which the autocorrelation present is assumed. This defines the
generalized least squares (GLS) criterion, of which OLS is a special case.
Equivalently, the idea is to transform the target and predictor variables so that
the new variables satisfy a linear relationship based on the same parameters
but satisfying the usual regression assumptions, and then use OLS to estimate
those parameters. GLS estimation is not in general available in statistical
software, but it turns out that for one particular type of autocorrelation it can
be easily fit using OLS software. This is the essence of the Cochrane-Orcutt
procedure, introduced by Cochrane and Orcutt (1949). We describe the
algorithm for the single-predictor case, but it generalizes in a straightforward
way to multiple predictors.
The Cochrane-Orcutt procedure provides a GLS fit assuming the errors
follow an AR(1) process, as in (5.1),
yi = β0 + β1xi + εi,
εi = ρεi−1 + zi.
Now, consider the transformation to
y∗
i = yi −ρyi−1.
Substituting into the regression model yields
y∗
i = yi −ρyi−1
= β0 + β1xi + εi −ρ(β0 + β1xi−1 + εi−1)
= β0(1 −ρ) + β1(xi −ρxi−1) + εi −ρεi−1
= β0(1 −ρ) + β1(xi −ρxi−1) + zi
≡β∗
0 + β1x∗
i + zi,
where β∗
0 = β0(1 −ρ) and x∗
i = xi −ρxi−1, and the substitution in the fourth
displayed line is based on (5.1). Thus, the regression of y∗
i on x∗
i provides
estimates of β∗
0 and β1 that are appropriate (that is, they are the GLS
estimates), since the errors zi for the constructed regression are independent
and identically normally distributed. We are not interested in β∗
0, but can get
an estimate of β0 by dividing the estimate of β∗
0 by 1 −ρ.
The Cochrane-Orcutt procedure is thus as follows:
1. Determine an estimate of ρ. A good one is the entry for lag 1 in the ACF
plot of the residuals from an OLS fit (call it ˆρ).

100
CHAPTER 5 Time Series Data and Autocorrelation
2. Form the transformed variables y∗
i = yi −ˆρyi−1 and x∗
i = xi −ˆρxi−1
(do this for each of the predicting variables in a multiple regression).
3. Fit the regression of y∗
i on the x∗
i ’s using OLS. The slope estimates are
left alone; the constant term estimate is adjusted using ˆβ0 = ˆβ∗
0/(1 −ˆρ).
A rough 95% prediction interval is ˆy ± 2˜σ, where ˜σ = ˆσ/

1 −ˆρ2 and ˆσ
is the standard error of the estimate from the Cochrane-Orcutt fit.
It is important to remember that the Cochrane-Orcutt procedure is merely
a computational trick that allows a generalized least squares analysis using
ordinary least squares programs. There is no physical meaning to y∗or x∗;
they are merely tools that are used to get the GLS fit. However, since the
Cochrane-Orcutt regression mimics that GLS fit, the usual inferential tools
(F, t), residual plots, and regression diagnostics from the Cochrane-Orcutt
fit can be interpreted in the usual way, since they are the appropriate ones
from a GLS fit. The Cochrane-Orcutt procedure is not appropriate if a lagged
version of the response variable is being used as a predictor.
A variation on the Cochrane-Orcutt procedure is the Prais-Winsten
procedure (Prais and Winsten, 1954), which replaces x∗
1 and y∗
1 (which
are missing when using Cochrane-Orcutt) with x1

1 −ˆρ2 and y1

1 −ˆρ2,
respectively. Typically the results of the two approaches are very similar.
In addition, each procedure can be iterated, by successively substituting the
new estimates of β into the appropriate formulas, although usually if that is
necessary a different approach should probably be tried.
5.4.6
EXAMPLE — TIME INTERVALS BETWEEN OLD FAITHFUL
GEYSER ERUPTIONS
A geyser is a hot spring that occasionally becomes unstable and erupts hot water
and steam into the air. The Old Faithful Geyser at Yellowstone National Park
in Wyoming is probably the most famous geyser in the world. Visitors to the
park try to arrive at the geyser site to see it erupt without waiting too long; the
name of the geyser comes from the fact that eruptions follow a relatively stable
pattern. The National Park Service posts predictions of when the next eruption
will occur at the Old Faithful Visitor Education Center and online. Thus, it is
of interest to understand and predict the time interval until the next eruption.
The mechanism by which a geyser works suggests how the time to the
next eruption might be predicted. Geysers occur near active volcanic areas,
with about half of the roughly 1000 geysers in the world being in Yellowstone
National Park. The eruption of a geyser comes from surface water working
its way downwards through volcanic rock until it hits magma (molten rock).
The combination of pressure and high temperatures heats the water far above
the usual boiling temperature. The eventual boiling of the water results in
superheated water and steam spraying out through the plumbing system of
constricted fractures and fissures in the rock.
The observed duration of an eruption is subject to various random effects,
and at the Old Faithful Geyser can vary between roughly 90 seconds and five

5.4 Methodology: Addressing Autocorrelation
101
minutes. If an eruption turns out to be relatively short, less of the superheated
water is sprayed out, meaning that it will tend to take less time for the
remaining water (when mixed with new cold surface water) to be heated to
boiling point. On the other hand, in a long eruption most of the heated water
is lost, meaning that it will take longer until the next eruption. Thus, the
duration of the previous eruption should be directly related to the time interval
until the next eruption, with shorter time intervals following shorter eruptions
and longer time intervals following longer eruptions. This suggests using
regression to try to predict the time interval until the next eruption (and hence
the time at which it will occur) from the duration of the previous eruption.
Figure 5.14, a plot of time interval to the next eruption versus duration of
the previous eruption (both in minutes), shows that this is a reasonable idea.
It is based on a sample of 222 eruption duration and following inter-eruption
times taken during August 1978 and August 1979, as given in Weisberg
(1980). It should be noted that the eruption behavior of the geyser has
changed since this time, in particular because of the magnitude 6.9 earthquake
that occurred less than 200 miles away at Borah Peak, Idaho, on October
29, 1983, so this analysis does not necessarily apply today. It can be seen
that as expected there is a positive relationship between duration time and
time to the next eruption. The following regression output summarizes the
relationship.
2.0
2.5
3.0
3.5
4.0
4.5
5.0
40
50
60
70
80
90
Duration of previous eruption
Time to next eruption
FIGURE 5.14: Scatter plot of the time interval to next eruption versus the
duration of the previous eruption for eruptions of The Old Faithful Geyser.

102
CHAPTER 5 Time Series Data and Autocorrelation
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
33.9668
1.4279
23.79
<2e-16 ***
Duration
10.3582
0.3822
27.10
<2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 6.159 on 220 degrees of freedom
Multiple R-squared: 0.7695,
Adjusted R-squared: 0.7685
F-statistic: 734.6 on 1 and 220 DF,
p-value: < 2.2e-16
There is clearly a strong relationship between the duration of the previous
eruption and the time until the next eruption, with each additional minute’s
duration of the previous eruption associated with an estimated expected
increase of 10.4 minutes in the time to the next eruption. Residual plots
(Figure 5.15) suggest heteroscedasticity (with higher variability for longer
fitted time intervals between eruptions and in the second half of the data),
60
70
80
−2
−1
0
1
2
(a)
Fitted values
Standardized residuals
0
50
100
150
200
−2
−1
0
1
2
(b)
Eruption number
Standardized residuals
0
5
10
15
20
−0.2 −0.1
0.0
0.1
Lag
ACF
(c)
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
(d)
Theoretical Quantiles
Sample Quantiles
FIGURE 5.15: Residual plots for the Old Faithful Geyser time interval until
the next eruption OLS regression fit based on duration of previous eruption.
(a) Plot of standardized residuals versus fitted values. (b) Time series plot
of standardized residuals. (c) ACF plot of residuals. (d) Normal plot of
standardized residuals.

5.4 Methodology: Addressing Autocorrelation
103
and also significant negative autocorrelation at the first lag (ˆρ = −.255,
d = 2.55, p < .0001). Negative autocorrelation is less common than positive
autocorrelation, particularly for economic data, where cyclical behavior is
consistent with positive autocorrelation. In this situation, however, the geyser
eruption process makes negative autocorrelation reasonable. If an eruption
takes longer to occur than expected (a positive error from the point of view
of the regression model), the water will be heated to a higher than usual
temperature; however long the eruption then is, the hotter-than-usual water
that remains will take less time than usual to heat to the boiling point, meaning
that it will take less time to the next eruption than expected (that is, a negative
error is more likely to follow the positive error).
With the relatively small estimated autocorrelations, it is difficult to assess
the appropriateness of an AR(1) assumption for the errors, but as the first
three estimated autocorrelations decrease and change signs in the appropriate
way (from negative to positive to negative) a Cochrane-Orcutt GLS fit is not
unreasonable. The corresponding regression output is as follows:
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
45.542
1.918
23.75
<2e-16 ***
Durationstar
9.711
0.418
23.23
<2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 5.933 on 219 degrees of freedom
(1 observation deleted due to missingness)
Multiple R-squared: 0.7113,
Adjusted R-squared:
0.71
F-statistic: 539.6 on 1 and 219 DF,
p-value: < 2.2e-16
The regression relationship is still strong, with estimated regression

Interval = 36.29 + 9.711 × Duration
(after adjusting the intercept), so the GLS estimated intercept is slightly
higher than the OLS one, while the estimated slope is slightly lower.
Residual plots (Figure 5.16) still indicate heteroscedasticity, but the
autocorrelation appears to have been addressed (d = 2.05, and the runs
test has p = .21). Regression diagnostics do not indicate any problems
(the largest value of hii is .021 < (2.5)(1 + 1)/221 = .023, and the largest
Cook’s D is .02). The adjusted estimate of the standard deviation
of the errors is
˜σ = 5.933/

1 −(−.256)2 = 6.138, so a rough
95%
prediction interval would be being able to predict the time to the next
eruption to within ±(2)(6.14) = ±12.3 minutes. A rough 90% interval
would correspond to
±(1.65)(6.14) = ±10.1 minutes, and
www.nps
.gov/yell/learn/nature/oldfaithfulgeyserfaq.htm
(the
National Park Service’s Old Faithful Geyser Frequently Asked Questions web
site, accessed September 14, 2019) reports that eruptions “can be predicted
with a 90 percent confidence rate within a 10 minute variation.”

104
CHAPTER 5 Time Series Data and Autocorrelation
70
75
80
85
90
95
100
−2
−1
0
1
2
Fitted values
Standardized residuals
0
50
100
150
200
−2
−1
0
0
1
2
Eruption number
Standardized residuals
0
5
10
15
20
−0.10
0.00
0.10
Lag
ACF
(a)
−3
−2
−1
0
1
2
3
−2
−1
1
2
(b)
(c)
(d)
Theoretical Quantiles
Sample Quantiles
FIGURE 5.16: Residual plots for the Old Faithful Geyser time interval until
the next eruption Cochrane-Orcutt GLS regression fit based on the duration
of previous eruption. (a) Plot of standardized residuals versus fitted values.
(b) Time series plot of standardized residuals. (c) ACF plot of residuals.
(d) Normal plot of standardized residuals.
Note that since these data have gaps corresponding to separate days,
several cases should actually be considered missing in the Cochrane-Orcutt fit,
since the lagged duration and interval are not known for those cases. If this is
done the results do not change appreciably.
5.5
Summary
In this chapter we have examined the effects of autocorrelation and various
ways to identify it. We have also looked at simple ways of dealing with it within
the context of least squares estimation, including accounting for time trends
and seasonal effects, lagging and differencing series to account for the effects of
earlier time periods and to address nonstationarity, and applying the Cochrane-
Orcutt procedure to determine a GLS fit that is optimal for AR(1) errors.

5.5 Summary
105
As we have seen from the analyses in this chapter (where there is evidence
for further autocorrelation effects), we have only scratched the surface of time
series modeling. Deseasonalizing can require more complicated approaches
than simple constant shift models, and time series models more compli-
cated than AR(1) (such as autoregressive/moving average [ARMA], ARCH,
GARCH, and stochastic volatility models) can be appropriate. Despite this,
the simple approaches discussed here will often account for a good deal of the
autocorrelation present in regression errors, and can also serve as preliminary
steps of an analysis that remove large-scale effects to allow subtler aspects of
the data to emerge.
KEY TERMS
AR(1) process: A time series process, typically assumed for the errors when
used in regression modeling, that is defined by εi = ρεi−1 + zi, where the zi
are a set of independent and identically distributed Gaussian (normal) random
variables with mean 0 and variance σ2, and |ρ| < 1.
Autocorrelation: The correlation that exists between the observations in time
series data. The correlation between xi and xi−r is called the rth order
autocorrelation.
Autocorrelation function (ACF): A plot of the estimated autocorrelation
against the order (lag) of the autocorrelation.
Cochrane-Orcutt procedure: An algorithmic procedure used to construct the
generalized least squares fit for a linear model to regression data assumed to
have errors that follow an AR(1) process using ordinary least squares software.
This is accomplished by working with transformed data.
Differenced variable: A variable generated by taking differences of adjacent
observations; that is, for example, y2 −y1, y3 −y2, . . . , yn −yn−1.
Durbin-Watson statistic: A statistic for testing the presence of AR(1) auto-
correlation in the errors from a regression model fitting. Values close to 2
show absence of significant autocorrelation.
Lagged variable: A constructed variable whose ith value is an earlier value
of an original series, usually the (i −1)st value. Using the lagged version of
a response variable as a predictor in a regression can often help account for
autocorrelation in the errors.
Log return: The first difference of the natural log of a stock price series. It is
approximately equal to the proportional change in the price (the return).
Nonstationary time series: A time series for which the joint distribution of
{xi, xi−1, . . . , xi−m} is dependent on i for some value of m. A stationary
process is one for which the joint distribution does not depend on i, which
implies that the mean and variance of the series does not change over time.
Runs test: A nonparametric test used to assess whether a sequence of obser-
vations is random. The test is based on the number of runs of consecutive

106
CHAPTER 5 Time Series Data and Autocorrelation
observations above or below a specified value, and significance is deter-
mined based on a permutation distribution for small samples and a normal
approximation for large samples. In the regression context, the test is typically
based on runs of positive and negative residuals.
Seasonal effect: A recurring pattern in a time series linked to effects driven
by the underlying annual cycle of the series. This can be reflected in autocor-
relations at lags that are multiples of 4 in quarterly data, multiples of 12 in
monthly data, multiples of 52 in weekly data, and so on.
Time trend: A general increasing or decreasing pattern in a time series, often
reflecting smooth changes from population growth or inflation.

Part Three
Categorical Predictors

Six
Chapter
Analysis of Variance
6.1
Introduction
109
6.2
Concepts and Background Material
110
6.2.1
One-Way ANOVA
110
6.2.2
Two-Way ANOVA
111
6.3
Methodology
113
6.3.1
Codings for Categorical Predictors
113
6.3.2
Multiple Comparisons
118
6.3.3
Levene’s Test and Weighted Least Squares
120
6.3.4
Membership in Multiple Groups
123
6.4
Example — DVD Sales of Movies
125
6.5
Higher-Way ANOVA
130
6.6
Summary
132
6.1
Introduction
In the regression models examined so far, both the target and predicting
variables have been continuous, or at least effectively continuous — with one
exception. The analysis of the pooled / constant shift / full model hierarchy in
Section 2.4 recognized that the existence of two well-defined subgroups in the
data could have predictive power for the target variable. That is, a categorical
predicting variable taking on the values 0 and 1 could be used to address the
effect of being in one or the other subgroup.
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
109

110
CHAPTER 6 Analysis of Variance
A natural question is to wonder if this can be generalized to more than
two groups. For example, does knowing the educational level of a person (not
a high school graduate, high school graduate, college graduate, or postgraduate
degree) have predictive power for their annual salary? Is the return on a stock
related to the industry group of the company? Do video games of different
types have different expected sales? This is a regression question, but a special
kind of regression question; in this context, saying that group membership
has predictive power for the target is the same as saying that the average value
of the target is different for different groups. That is, this is a question of the
comparison of means.
In this and the following chapter we examine how regression models can
be extended to allow for categorical predictors that identify multiple groups
(or multiple levels of a variable). In this chapter we focus on models with
only categorical predictors, which are termed analysis of variance (ANOVA)
models. We start with one categorical predictor, one-way ANOVA, and
then generalize this to two-way ANOVA. Discussion of the methodology
used to fit such models, including two different ways of coding them as
regression models, follows. The important multiple comparisons problem
is investigated, in which the goal is to compare the different groups to each
other to determine which are actually different in terms of expected response.
Since nonconstant variance often occurs for ANOVA data, with different
groups having different variability of the errors, we then discuss weighted
least squares, the generalization of ordinary least squares designed for this
situation.
6.2
Concepts and Background Material
6.2.1
ONE-WAY ANOVA
Consider the simplest situation of one categorical predicting variable that takes
on K values. The one-way ANOVA model is
yij = μ + αi + εij, i = 1, . . . , K, j = 1, . . . , ni,
(6.1)
where yij is the value of y for the jth member of the ith group, μ is an overall
level (roughly corresponding to the overall mean), αi is the effect of being in
the ith group, εij is the error term, and ni is the number of observations that
fall in the ith group.
The α terms represent the difference in E(y) that comes from being in
any particular group relative to an overall level, since E(y) = μ + αi ≡μi for
all observations in group i. It is natural to say that αi = 0 for all i if there is no
difference between groups, but this requires a little more in terms of technical
detail. Say E(y) = 50 for all groups, corresponding to no effect related to
the categorical predictor. While this is obviously satisfied by μ = 50 and
α1 = · · · = αK = 0, it is also satisfied by μ = 40 and α1 = · · · = αK = 10;
indeed, there are an infinite number of possibilities that are consistent with

6.2 Concepts and Background Material
111
this condition. For this reason, an additional constraint must be imposed on
(6.1), that
K

i=1
αi = 0.
With this additional constraint, it is guaranteed that a situation with no group
effect will be modeled with α = 0.
It is clear that the one-way ANOVA model is linear in its parameters, and
is thus just a linear regression model based on specially-constructed predictors.
This is discussed in Section 6.3.1.
6.2.2
TWO-WAY ANOVA
Consider now a situation with two categorical predictors, one (arbitrarily
termed rows) with I levels and the other (arbitrarily termed columns) with
J levels. This usage is based on the possibility of representing the I × J
group means in the form of a table, and we will also sometimes refer to the
combination of row level i and column level j as the (i, j)th cell. A simple
generalization of model (6.1) is to add a set of parameters that correspond to
the group effects of the second predictor. The model is then
yijk = μ + αi + βj + εijk, i = 1, . . . , I, j = 1, . . . , J, k = 1, . . . , nij,
(6.2)
where yijk is the value of y for the kth member of the (i, j)th group, μ is an
overall level, αi is the row effect for the ith level of the row variable, βj is the
column effect for the jth level of the column variable, εijk is the error term,
and nij is the number of observations that fall in the (i, j)th cell. Just as was
true for model (6.1), in order to make this model identifiable the additional
constraints
I

i=1
αi =
J

j=1
βj = 0
are required. The main effects α and β have a similar interpretation to the
effect α in the one-way ANOVA model (6.1), except in an analogous way to
slope coefficients in a multiple regression versus a simple regression. In (6.2)
each parameter corresponds to an effect holding the level of the other variable
fixed. Thus, the model implies that (for example) the difference in expected
response between an observation with row level i and one with row level i′
equals αi −αi′ no matter which column level the observations come from, as
long as they come from the same column level. It is clear that the notion of a
(single) overall row effect and (single) overall column effect is meaningful in
this context.
It is easy to imagine, however, a situation where the effect on y of being
in one row category versus another row category differs depending on the
column category (or equivalently, the effect of being in one column category
versus another column category differs depending on the row category). This

112
CHAPTER 6 Analysis of Variance
is an interaction effect in the same way that the existence of different slopes
for a predictor depending on group membership discussed in Section 2.4 was
an interaction effect. This corresponds to an extension of model (6.2) to
yijk = μ + αi + βj + (αβ)ij + εijk, i = 1, . . . , I, j = 1, . . . , J,
k = 1, . . . , nij,
(6.3)
where the (αβ) terms correspond to the existence of different row (column)
effects for different columns (rows). The constraints
I

i=1
(αβ)ij =
J

j=1
(αβ)ij = 0
make the model identifiable, and clearly
(αβ)11 = · · · = (αβ)IJ = 0
corresponds to a lack of the presence of an interaction effect.
The (αβ) parameters themselves do not necessarily lend themselves to
easy description of how the simple additivity of (6.2) is violated, but such a
description is easily available through the use of an interaction plot. In such
a plot the observed group means for each (i, j) combination are plotted in
the form of separate lines connecting the means for each row, or alternatively
separate lines connecting the means for each column. If model (6.2) holds (that
is, there is no interaction effect), the differences between expected response
values for each row (column) are the same for each column (row), implying
that the lines in the plot corresponding to each row (column) will be parallel.
This is illustrated in the top plot of Figure 6.1. The relative position of
the lines for the different columns shows that the column effect (which is the
same for all rows) corresponds to increasing expected response going from
levels 1 to 2 to 3, and the shape of those lines shows that the row effect (which
is the same for all columns) corresponds to lowest expected response for level
1 and highest expect response for level 2.
This can be contrasted with the bottom plot in Figure 6.1, which is
consistent with the existence of an interaction effect. In this case there is
no single “row effect” or “column effect,” since the row effect is different
depending on the column level, and the column effect is different depending
on the row level, resulting in lines that are not parallel in the plot. Specifically,
in this case, the row effect for column level 1 implies an ordering in expected
response from low to high of row levels {1, 3, 2}, for column level 2 an
ordering of row levels {3, 2, 1}, and for column level 3 an ordering of row
levels {2, 1, 3}.
Of course, in an interaction plot based on sample means one would expect
to see some evidence of an apparent interaction even if the true model was
(6.2) because of random fluctuation, so the interaction plot should not be
viewed as a way to choose between models (6.2) and (6.3), but rather as a
way to describe the interaction effect if it is decided that one exists. Choosing
between the models would be done using the usual tools of regression model

6.3 Methodology
113
20
0
20
40
60
30
40
50
60
Without an interaction effect
Rows
Mean response
Row 1
Row 2
Row 3
   Columns
Col. 1
Col. 2
Col. 3
With an interaction effect
Rows
Mean response
Row 1
Row 2
Row 3
   Columns
Col. 1
Col. 2
Col. 3
FIGURE 6.1: Interaction plots based on expected responses in situations
without (top plot) and with (bottom plot) an interaction effect.
selection, which requires the ability to represent these models as regression
models. This is discussed in the next section.
6.3
Methodology
6.3.1
CODINGS FOR CATEGORICAL PREDICTORS
Given that a categorical predictor with K levels is just a generalization from
one with two levels, it seems natural that it can be incorporated into a
regression model in a corresponding way using indicator variables. This is
in fact the case. Just as was true when there were two levels of a categorical
variable and only one indicator variable was used to account for that effect, in
the situation with K levels of a categorical variable, K −1 indicator variables
are used to account for that effect. Any one of the K possible indicator
variables can be omitted, but the choice affects the interpretation of the slope
coefficients.
Consider Table 6.1. The top table summarizes the implications of fitting
a one-way ANOVA model (6.1) using indicator variables where the indicator
for the Kth level is omitted; that is,
yij = β0 + β1I1ij + · · · + βK−1IK−1,ij + εij.
As is apparent from the table, in this situation the intercept β0 equals
the expected response when an observation comes from level K, the level

114
CHAPTER 6 Analysis of Variance
Table 6.1: Indicator variable codings for a one-way ANOVA fit omitting the
variable for level K (top table) and level 1 (bottom table), respectively.
Level
I1
I2
IK−2
IK−1
Expected response
Level 1
1
0
0
0
β0 + β1
Level 2
0
1
0
0
β0 + β2
...
· · ·
...
Level K −2
0
0
1
0
β0 + βK−2
Level K −1
0
0
0
1
β0 + βK−1
Level K
0
0
0
0
β0
Level
I2
I3
IK−1
IK
Expected response
Level 1
0
0
0
0
β0
Level 2
1
0
0
0
β0 + β2
Level 3
0
1
0
0
β0 + β3
...
· · ·
...
Level K −1
0
0
1
0
β0 + βK−1
Level K
0
0
0
1
β0 + βK
for which the indicator was omitted. A slope coefficient βm (say) then
represents the difference in expected response between level m and level K
for m = 1, . . . , K −1. That is, level K is a reference group, and the slope
coefficients represent expected deviations from that reference group. If the
omitted level was instead level 1, the model is
yij = β0 + β2I2ij + · · · + βKIKij + εij.
As is summarized in the bottom table of Table 6.1, the reference group now
becomes level 1, and all of the coefficients change accordingly (even though
the expected responses do not change). The statistical significance of the effect
of the categorical predictor (that is, the test of α = 0) corresponds to a test of
all of the slopes of the indicator variables equaling 0, and is thus tested using
the usual overall F-test.
This indicator variable formulation is a very reasonable one when there is a
natural reference group in the context of the problem (for example, in a clinical
trial situation where one level corresponds to the control group and the others
are experimental treatment groups), but it is not an obvious choice when there
is no natural reference group. Indeed, if there is no natural reference group,
examining slope coefficients based on indicator variables artificially forces one
level into the role of “reference” group in a way that could be completely
inappropriate. Further, an indicator variable-based fit does not correspond
to the ANOVA model (6.1). To achieve that fit requires a different type of

6.3 Methodology
115
Table 6.2: Effect codings for a one-way ANOVA fit omitting the variable for
level K (top table) and level 1 (bottom table), respectively.
Level
E1
E2
EK−2
EK−1
Expected response
Level 1
1
0
0
0
β0 + β1
Level 2
0
1
0
0
β0 + β2
...
· · ·
...
Level K −2
0
0
1
0
β0 + βK−2
Level K −1
0
0
0
1
β0 + βK−1
Level K
−1
−1
−1
−1
β0 −β1 −· · · −βK−1
(≡β0 + βK)
Level
E2
E3
EK−1
EK
Expected response
Level 1
−1
−1
−1
−1
β0 −β2 −· · · −βK
(≡β0 + β1)
Level 2
1
0
0
0
β0 + β2
Level 3
0
1
0
0
β0 + β3
...
· · ·
...
Level K −1
0
0
1
0
β0 + βK−1
Level K
0
0
0
1
β0 + βK
variable, called an effect coding. Effect codings look very much like indicator
variables, except for the values corresponding to the level for the variable that
is omitted. While for indicator variables observations from the omitted level
have the value 0 for every variable, for effect codings observations from the
omitted level have the value −1 for every variable. It is simple to construct
these variables from a set of indicator variables, as each effect coding variable
just equals the indicator variable for the omitted level subtracted from the
corresponding indicator variable.
This is laid out in Table 6.2. In the top plot the Kth level has its variable
omitted, so any observations from that level take on the value −1 for every
other variable (that is, the mth effect coding satisfies Em = Im −IK), and
the regression being fit is based on
yij = β0 + β1E1ij + · · · + βK−1EK−1,ij + εij.
This results in an expected response for level K of β0 −β1 −· · · −βK−1,
rather than β0 (as was the case for indicator variables). The average of the
expected responses over all levels (the average of the values in the last column)
is clearly β0, which shows that in this formulation β0 is an overall level of the
response, not the expected response for a reference group. That is, it is μ from

116
CHAPTER 6 Analysis of Variance
the ANOVA model (6.1). A slope coefficient βm represents the difference
between the expected response for level m and this overall level μ; that is,
βm = αm for m = 1, . . . , K −1 in (6.1). Finally, since the α terms sum to 0,
the expected response for level K is implicitly μ + αK, as implied by (6.1).
The key point here is that unlike when indicator variables are used, the
interpretation of all of the coefficients using effect codings is the same no
matter which level has its variable omitted. The bottom table in Table 6.2
demonstrates this, as all of the expected responses have the same form as in
the top table, even though in this case it was the variable for the first level that
was omitted. For this reason, it is perfectly appropriate to report all K + 1 of
the estimated coefficients (K slopes along with the intercept) to summarize
the model, even though there are only K parameters being estimated; the
constraint that makes the model identifiable (
iαi = 0) is implicitly satisfied
by the estimated coefficients. The existence of an effect is again tested using
the overall F-test, which will be the same no matter which variable is omitted
(and the same as that when using indicator variables).
The equivalence of the indicator variable and effect coding variable
representations of categorical predictors requires that all K −1 variables
be included for each. Omitting individual underlying indicator or effect
coding variables is generally inadvisable, as this corresponds to merging
different categories together in potentially unintuitive ways, with very different
implications for different indicator variable or effect coding representations.
Further, such a merging of categories treats categories with estimated responses
that are not statistically significantly different from each other as being equal
to each other, a potentially dangerous confusion of statistical significance with
practical importance.
The two-way ANOVA model without an interaction (6.2) is fit based
on a regression using the I −1 indicators or effect codings for rows and the
J −1 indicators or effect codings for columns. Statistical significance of the
row (column) effect is based on the partial F-test comparing the fit using
all of the variables to the one omitting the variables corresponding to rows
(columns). In this context these tests are sometimes referred to as being based
on “Type III” sums of squares, and are the same type of partial F-tests (and
t-tests) discussed earlier. Fitted values are directly available from (6.2) and the
estimated coefficients from the regression fit using effect codings, since

E(yij) = ˆμ + ˆαi + ˆβj
(note that the β terms above refer to the column parameters in the two-way
ANOVA model (6.2), not the parameters from the one-way ANOVA
regression formulation summarized in Tables 6.1 and 6.2). The underly-
ing components of these fitted values, ˆμ + ˆαi and ˆμ + ˆβj, are called the
least squares means for rows and columns, respectively, as they estimate the
expected response for each row (column) taking the column (row) effect into
account.
The two-way ANOVA model with an interaction (6.3) also can be fit
as a regression, but this requires the construction of indicator variables or

6.3 Methodology
117
effect codings for the interaction term. This is accomplished by calculating
each of the (I −1)(J −1) pairwise products of a row variable and a column
variable (that is, IR1 × IC1, IR1 × IC2, etc., or ER1 × EC1, ER1 × EC2, etc.,
where the R (C) subscript refers to those being variables associated with row
(column) levels). The statistical significance of the interaction effect is tested
using the partial F-test comparing model (6.3) to model (6.2). Under model
(6.3) the fitted value for any observation from the (i, j)th cell is the mean
response for all observations from the (i, j)th cell, Y ij·.
An interesting question concerns the order in which ANOVA models
should be examined; that is, should an analyst “build up” from a model with
only main effects to one including an interaction, or should one “simplify
down” from a model with an interaction to one with only main effects. There
are reasonable arguments in favor of either approach. A model that includes
only main effects is analogous to a regression model on numerical predictors
(where variable effects are constant given the values of the other predictors),
and building up from that is consistent with the pooled model / constant
shift model / full model hierarchy described in Section 2.4 (which will be
generalized in Chapter 7 to allow for categorical variables with more than
two levels). Adding an interaction effect then involves exploring whether it
is possible to improve upon the simpler main effects model, and this can be
tested using the corresponding partial F-test.
On the other hand, in many situations main effects are fairly obvious
and therefore relatively uninteresting, and it is interaction effects that are the
focus. In that situation discussing main effects is at best uninteresting and at
worst potentially misleading, since in the presence of an interaction “the” row
effect or “the” column effect is no longer meaningful (an interaction means
that there are different row effects for different columns, and different column
effects for different rows). So, for example, if the partial F-test for columns
based a model using only main effects is insignificant, this could potentially
encourage the analyst to think that the column variable has no relationship
with the target variable and should be completely omitted from the analysis,
when in fact it turns out that the row-column interaction effect is very strongly
predictive. This possibility suggests starting with the model that includes the
interaction effect, and using the partial F-test for it to decide if a simpler
model with only main effects is adequate. Then, if that is the case, the main
effects model could be fit, and row and column effect partial F-tests could be
used to decide if the model can be simplified further.
What is certainly true is that when fitting a model that includes an
interaction effect, the partial F-tests for the main effects do not correspond
to testing meaningful hypotheses, and should not be examined. This does not
mean that a general pattern of means for rows or columns must be ignored;
for example, based on the bottom interaction plot of Figure 6.1 it would be
reasonable for a data analyst to report that observations from column level
3 have generally the highest expected response, those from column level 1
have moderate expected response, and those from column level 2 have lowest
expected response. What should not be done in a model that includes the

118
CHAPTER 6 Analysis of Variance
interaction, however, is to appeal to the partial F-test for the column main
effect to justify this statement. Further, a model that includes an interaction
should include the corresponding main effects, since otherwise the simple
interpretability of the interaction is lost (in fact, many statistical packages will
refuse to fit a two-way ANOVA model that includes an interaction effect
without including the corresponding main effects).
The situation where the number of observations nij is the same for all
cells is termed a balanced design, and has several advantages over unbalanced
designs. In a balanced design, the effects are orthogonal to each other; that
is there is a perfect lack of collinearity between the codings that define one
effect with those that define another effect. This implies that the variability
accounted for by any effect (as measured by the change in residual sum of
squares when including or excluding the effect) is the same, no matter which
other effects are in the model. From a practical point of view, this means
that the statistical significance of the row effect (for example) is (virtually)
unchanged if the column effect is included or omitted from the model. If the
design is very unbalanced, on the other hand, it can happen that a row effect
(for example) is highly statistically significant when the model including an
insignificant column effect is fit, but becomes insignificant if the apparently
unneeded column effect is omitted from the model, which is counterintuitive
and undesirable (but not surprising in the presence of collinearity).
Further, if the model is balanced, all observations have equal leverage. By
contrast, in very unbalanced designs, observations in cells with relatively few
observations can have high leverage, thus potentially having a strong effect
on the fitted ANOVA. In the most extreme situation, if a cell has only one
observation (nij = 1) and the model includes an interaction effect, the leverage
value for that observation will equal 1 (since one of the constructed effect
codings will effectively be an indicator for that one observation). Omitting
the observation is not a very desirable choice, since then it is impossible to
fully estimate the interaction effect (it will not be possible to estimate the
expected response for that combination of row and column levels). Indeed,
some statistical packages will not fit ANOVA models with empty cells for this
reason, and in that case it must be done by the analyst directly using regression
on indicator variables or effect codings.
In the particular balanced design where nij = 1 for all cells, the interaction
effect cannot be fit. The reason for this is that in the model that includes the
interaction effect, there are IJ parameters in the model and IJ observations
in the data, yielding identically zero residuals for all observations.
6.3.2
MULTIPLE COMPARISONS
Consider again the one-way ANOVA model (6.1). Given that it is believed
that the categorical variable is meaningful as a predictor, it is natural to wonder
which pairs of levels are different from each other. That is, if i and i′ are
two different levels of the variable, for which levels is αi −αi′ different from
zero? This would seem to be testable in a fairly straightforward way using the

6.3 Methodology
119
partial F-test for the hypothesis αi = αi′, which is equivalent to the t-test for
the slope of one level if the ANOVA is fit using indicator variables making
the other level the reference level. The problem is that there is a multiple
comparisons problem that comes from the many pairwise t-tests that are
being calculated. Say there are K levels to the grouping variable. This implies
that there are C =

K
2

= K(K −1)/2 different pairwise comparisons being
made; for example, if K = 10, there are C = 45 comparisons being made.
If each t-test was based on a .05 level of significance, and there were no
differences between the groups, (.05)(45) ≈2 of the tests would be expected
to be statistically significant just by random chance.
Multiple comparisons procedures correct for this by controlling the
experimentwise error rate. An experimentwise error rate of .05 says that in
repeated sampling from a population where there is no difference between
groups, only 5% of the time would any pair of groups be considered significantly
different from each other. The broad area of multiple comparisons is beyond
the scope of this book (for detailed discussion see, for example, Bretz et al.,
2010), but some relatively simple methods are available for ANOVA models.
The most commonly used approaches are the Bonferroni correction and
Tukey Honestly Significant Difference (HSD) methods. The Bonferroni
method argues that if the experimentwise error rate is desired to be α, each
pairwise test should be done at an α/C level, since that way the expected
number of false positives is (α/C)(C) = α. So, for example, for K = 10, each
pairwise t-test would be conducted at a significance level of .05/45 = .0011.
Equivalently, the Bonferroni-adjusted p-value for each t-test multiplies the
usual p-value based on the t-distribution by C (and is 1 if that value is greater
than 1). The Bonferroni correction is very general and very easy to apply, and
usually does a good job of controlling the experimentwise error rate. Its only
drawback is that it can sometimes be too conservative (that is, it does not
reject the null as often as it should).
The Tukey method is a multiple comparisons method specifically derived
for ANOVA multiple comparisons problems. This method determines sig-
nificance and p-values of the pairwise t-statistics using the studentized range
distribution, which is the exact correct distribution accounting for the multiple
comparisons when the design is balanced. The Tukey test is less general than
the Bonferroni correction, but is usually less conservative (particularly if the
design is reasonably balanced).
Multiple comparisons methods for deciding which rows or columns are
significantly different from each other generalize in a direct way to the two-way
ANOVA model as long as the model does not include an interaction effect.
Testing is not applicable in the presence of a fitted interaction, since the α
and β terms are not interpretable in the presence of the (αβ) terms in (6.3).
It is possible to compare underlying cells to each other in a model with an
interaction effect [that is, the mean response for cell (i, j) compared to that
for cell (i′, j′)] using multiple comparisons methods, but this seems less useful

120
CHAPTER 6 Analysis of Variance
from a practical point of view than examination of the interaction through an
interaction plot.
An alternative approach to the multiple comparisons problem that has
been increasingly investigated in the last 25 years is based on controlling
the false discovery rate, which is the expected proportion of falsely rejected
hypotheses among all rejected hypotheses. If all of the null hypotheses are true
(that is, in the ANOVA context, all of the group expected responses are equal
to each other) this is the same as the experimentwise rate controlled by the
Bonferroni and Tukey methods, but when some of the null hypotheses are
not true it is easier to reject the null, therefore making the test more sensitive
and less conservative. This method is particularly attractive in the situation
where a great many comparisons need to be made (such as in bioinformatics),
since in that situation methods that control the experimentwise error rate can
miss important differences that are actually present. Implementation of this
method is not as straightforward as is implementation of the Bonferroni and
Tukey methods, but it is becoming more generally available in software. The
complement of the false discovery rate is the so-called positive predictive value.
See Strimmer (2008) for further discussion.
6.3.3
LEVENE’S TEST AND WEIGHTED LEAST SQUARES
The ANOVA situation (where there are by definition well-defined subgroups
in the data) is one where heteroscedasticity (nonconstant variance) is common.
Just as the responses for observations from different subgroups might have
different means, the errors for observations from different subgroups might
have different variances. This is a clear violation of the assumptions of ordinary
least squares, with several negative impacts on OLS analyses.
1. The OLS estimates of the regression coefficients are inefficient. That is,
on average, the OLS estimates are not as close to the true regression
coefficients as is possible.
2. Inferential tests and intervals do not have the correct properties; confidence
intervals do not have the correct coverage; and hypothesis tests do not
have the correct significance levels.
3. Perhaps most importantly, predictions and prediction intervals are not
correct. OLS assumes that the variance of all errors is the same (σ2), which
is reflected, for example, in the rough 95% prediction interval of ˆyi ± 2ˆσ.
Clearly, if variances are different, so that V (εi) = σ2
i , the appropriate
interval is one of the form ˆyi ± 2ˆσi. So, for example, if the underlying
variability of an observation (or a certain type of observation) is larger
than that for another (type of) observation, the corresponding prediction
intervals should be wider to reflect that. OLS-based intervals obviously do
not have that property.
For these reasons, it is important to try to identify and address potential
nonconstant variance. As was noted in Section 1.3.5, residual plots are useful

6.3 Methodology
121
for this purpose, as varying heights of the point cloud can reflect different
underlying variances (this also applies to side-by-side boxplots of residuals for
categorical predictors, as can be seen in Figure 2.7 on page 46). More formal
tests for nonconstant variance are also possible. A particularly simple one is
Levene’s test, in which the absolute value of the residuals from a regression
or ANOVA fit is used as the response variable in a regression or ANOVA.
Since observations with larger values of σi would be expected to have more
extreme (and hence larger absolute) errors, evidence of a relationship between
the absolute residuals and any predictors would be evidence of a relationship
between the variance of the errors and those predictors. The Levene’s test is
the overall F-test based on the absolute residuals.
If nonconstant variance is actually a problem, there is a relatively straight-
forward cure: weighted least squares (WLS). The idea behind WLS is the
same as that behind the (Cochrane-Orcutt) GLS fit for a model exhibiting
autocorrelated errors discussed in Section 5.4.5: transform the target and
predicting variables to give a model with errors that satisfy the standard
assumptions. Indeed, WLS is a special case of GLS. To keep the presentation
simple, consider a simple regression model, although the discussion carries
over directly to multiple regression and ANOVA situations. The regression
model is
yi = β0 + β1xi + εi,
but here with V (εi) = σ2
i . We allow for nonconstant variance by setting
σ2
i = c2
i σ2. Dividing both sides of this equation by ci gives
yi
ci
= β0
 1
ci

+ β1
xi
ci

+ εi
ci
.
This can be rewritten
y∗
i = β0z1i + β1z2i + δi,
where y∗
i , z1i, z2i, and δi are the obvious substitutions from the previous
equation and V (δi) = σ2 for all i. Thus, OLS estimation (without an intercept
term) of y∗on z1 and z2 gives fully efficient estimates of β0 and β1. From
a conceptual point of view, the principle is that observations with errors
with larger variances (larger c2
i ) have less information in them about the
regression relationship, and therefore should be weighted less when estimating
the regression parameters and conducting inference. This is achieved by
estimating the regression parameters as the minimizer of the weighted residual
sum of squares,
n

i=1
wi(yi −ˆyi)2,
(6.4)
where wi = 1/c2
i is the value of the weighting variable for the ith observation.
Ordinary least squares is a special case of WLS with wi = 1 for all i. In matrix
notation, the resultant WLS estimates satisfy
ˆβ = (X′WX)−1X′Wy,
(6.5)

122
CHAPTER 6 Analysis of Variance
where W is the diagonal matrix with ith diagonal element wi. The weighted
residual mean square based on (6.4) provides ˆσ (which will be reported
in any WLS output), and ˆσi = ˆσ/√wi. The hat matrix has the form H =
X(X′WX)−1X′W, and all of the diagnostics from Chapter 3 carry over
to WLS using this H (with ith diagonal element hii), and ˆσi for the ith
observation rather than ˆσ. In particular, standardized residuals and Cook’s
distances are defined this way based on (3.2) and (3.4), respectively.
The obvious difficulty is that σ2
i (or equivalently c2
i ) is unknown, and
must be estimated. This is however easy to do in the ANOVA situation. Say
there is a predictor defining K subgroups in the data. If the errors for all of the
observations that come from group m (say) have the same variance, σ2
[m], then
the weight for each of those observations would be (any constant multiple of)
1/ˆσ2
[m]. A choice that is then easily available is to use the inverse of the sample
variance of the (standardized) residuals for the weight for members of group
m. The situation where the variances are related to numerical predictors is
more complicated, and will be discussed in Section 10.7. Technically, the fact
that the weights are not fixed, but are rather estimated from the data, will have
an effect on WLS-based inference, but this effect is generally minor as long as
the sample size is not very small.
Although usually the appropriate choice of wi is unknown and must be
estimated from the data, sometimes that is not the case. Say that the response
variable at the level of an individual follows the usual regression model,
yi = β0 + β1x1i + · · · + βpxpi + εi,
with εi ∼N(0, σ2) (this could of course be an ANOVA model). Imagine,
however, that the ith observed response is actually an average Y i for a sample
of size ni with the observed predictor values {x1i, . . . , xpi}. The observed
data thus actually satisfy
Y i = β0 + β1x1i + · · · + βpxpi + ˜εi,
where
V (˜εi) = V (Y i|{x1i, . . . , xpi}) = σ2
ni
.
This kind of situation could arise as follows. Say an analyst was interested
in modeling the relationship between student test scores and (among other
things) income. While it might be possible to obtain test scores at the level of
individual students, it would be impossible to get incomes at that level because
of privacy issues. On the other hand, average incomes at the level of census
tract or school district might be available, and could be used to predict average
test scores at that same level. This is not the same as predicting an individual’s
test scores from their particular income (since school districts are the units of
study, not students), but could be useful from a policy point of view in terms
of distributing resources over the various schools in the county.
Clearly, this is just a standard heteroscedasticity model, and WLS can be
used to fit it. In this situation, the weights do not need to be estimated at

6.3 Methodology
123
all; since V (˜εi) = σ2/ni, the weight for the ith observation is just ni, with
σ2 estimated from the WLS residual mean square. That is, quite naturally,
observations based on larger samples are weighted more heavily in estimating
the regression coefficients. A similar situation is the presidential election data
discussed in Section 2.4.1. For those data, the response variable was the
change in the percentage of votes cast for George W. Bush from 2000 to 2004.
These percentages are the empirical proportions of total voters who voted
for Bush (multiplied by 100), and a binomial approximation implies that
their variances are proportional to the total number of voters in the county.
Figure 2.7 suggests nonconstant variance related to whether a county used
electronic voting or not, but this is because larger counties were more likely to
use e-voting, so the county voter turnout effect is confused with an e-voting
effect.
6.3.4
MEMBERSHIP IN MULTIPLE GROUPS
Thus far we have focused on situations where each observation falls into one
and only one category of a categorical variable. It is possible, however, that
an individual could be a member of several of the groups defined by such
a variable. For example, in a survey of college students, a respondent might
want to identify themselves as being a member of several races or ethnicities,
or as majoring in more than one field. Such responses would come from
questions of the form “Check all that apply” when presented with a list of all
race/ethnicity or majors groups.
The common approaches to this problem are to either add an extra
category (such as “Multiracial” or “Double major,” respectively), or to add
extra categories for each of the possible combinations (such as “White and
African American/Black,” “White and Asian,” and so on, or “Economics
and Finance,” “Economics and History,” etc., respectively). Neither of these
solutions is without problems. The former approach groups all combinations
together, implying that all combinations of races or majors have the same
expected relationship with the response variable, which seems unlikely. The
latter approach is much more flexible, but is likely to result in many com-
binations with very few observations (there are 2K −1 different membership
possibilities), making inference difficult, and does not take advantage of any
information from individuals who are members of only one group that could
be relevant for individuals who are members of that group along with others.
It is possible to formulate a different approach using indicator variables
or effect codings that has neither of these shortcomings, albeit at the cost
of an additional assumption on the model. In order to handle multiple
group membership, all that is required is to redefine the indicator variable
for membership of individual i in group m to be I∗
mi = Imi/T, where T
is the total number of groups of which individual i is a member. Consider,
for example, a categorical variable that takes on K = 3 levels. Table 6.3
summarizes the different possible combinations of group membership in this
case, the adjusted indicator variable values for each combination, and the

124
CHAPTER 6 Analysis of Variance
expected response if the variable for group 3 is omitted. The first three lines
of the table show that the interpretation of the coefficients has not changed;
β0 is the expected response for the (omitted) reference group (i.e., μ3), and β1
and β2 are the differences in expected response between group 1 or group 2
and group 3, respectively (i.e., μ1 −μ3 and μ2 −μ3, respectively).
This implies an additional assumption about individuals who are members
of multiple groups. Consider, for example, individuals who are members of
both groups 1 and 2. Their expected response satisfies
E(y) = β0 + β1 + β2
2
= μ3 + μ1 −μ3 + μ2 −μ3
2
= μ1 + μ2
2
;
corresponding results hold for members of other combinations of groups.
That is, the expected response for an individual who is a member of multiple
groups is the average of the expected responses for those groups.
The effect coding version of this formulation is summarized in Table 6.4.
As before, each effect coding variable equals the indicator variable for the
omitted level subtracted from the corresponding indicator variable, now based
on the adjusted indicators I∗as given in Table 6.3. The first three lines of
the table show that the parameters have the same interpretation as before,
consistent with the one-way ANOVA model (6.1). The last line directly shows
that the expected response for individuals who are members of all three groups
is the overall level μ, which of course equals the average of the expected
responses for each of the three groups. Some simple algebraic manipulations
shows that the other group memberships operate similarly, and equivalently
to the use of (adjusted) indicator variables.
Table 6.3: Indicator variable codings for a one-way ANOVA
fit with K = 3 omitting the variable for level 3 when multiple
group membership is allowed.
Group
Expected response
membership
I∗
1
I∗
2
I∗
3
omitting group 3
X
1
0
0
β0 + β1
X
0
1
0
β0 + β2
X
0
0
1
β0
X
X
1/2
1/2
0
β0 + (β1 + β2)/2
X
X
1/2
0
1/2
β0 + β1/2
X
X
0
1/2
1/2
β0 + β2/2
X
X
X
1/3
1/3
1/3
β0 + (β1 + β2)/3

6.4 Example — DVD Sales of Movies
125
Table 6.4: Effect codings for a one-way ANOVA fit with
K = 3 omitting the variable for level 3 when multiple
group membership is allowed.
Group
Expected response
membership
E∗
1
E∗
2
omitting group 3
X
1
0
β0 + β1
X
0
1
β0 + β2
X
−1
−1
β0 −β1 −β2
X
X
1/2
1/2
β0 + (β1 + β2)/2
X
X
0
−1/2
β0 −β2/2
X
X
−1/2
0
β0 −β1/2
X
X
X
0
0
β0
6.4
Example — DVD Sales of Movies
After-market revenues for movies from DVD sales have been a major profit
center for studios since the introduction of the DVD in 1998, with revenues
exceeding $14 billion in 2004. In recent years the availability of films via
digital download has cut into these revenues, making the ability to predict
DVD sales even more important. Two characteristics of movies believed
to be related to revenues (in both ticket sales and DVD sales) are the
Motion Picture Association of America (MPAA) rating and the genre of
the film.
This analysis is based on domestic DVD sales in millions of dollars for
movies released on more than 500 screens in 2009. In order to avoid groups
with small numbers of movies, four movies rated G, a documentary, and a
musical are omitted from the analysis. Further, action and adventure movies
are combined into the Action/Adventure genre, horror, romance, and thriller
movies are included in the Drama genre, and romantic comedies are included
in the Comedy genre. DVD sales are very long right-tailed, so logged (base
10) sales are used as the response variable. Five of the movies used in earlier
analyses (Section 4.4) had missing values for DVD sales.
Figure 6.2 gives side-by-side boxplots separating logged DVD sales by
the two categorical predictors. There is weak evidence for a rating effect,
with PG-rated movies having highest sales, followed by PG-13-rated and
then R-rated movies. There is stronger evidence of a genre effect, with
action/adventure movies having highest revenues, followed by comedies and
then dramas. There is also noticeable evidence of potential nonconstant
variance, particularly related to genre.

126
CHAPTER 6 Analysis of Variance
PG
PG−13
R
0.5
1.5
MPAA rating
Logged DVD sales
0.5
1.5
Logged DVD sales
Genre
Action/
Adventure
Comedy
Drama
FIGURE 6.2: Side-by-side boxplots of logged DVD sales separated by MPAA
rating and genre of movie.
We first fit a two-way ANOVA model that includes the interaction effect:
Response: Log.dvd
Sum Sq
Df
F value
Pr(>F)
Rating
0.084
2
0.2520 0.77767
Genre
1.124
2
3.3653 0.03818 *
Rating:Genre
0.780
4
1.1676 0.32918
Residuals
18.197 109
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.4086 on 109 degrees of freedom
Multiple R-squared: 0.1536,
Adjusted R-squared: 0.09144
It is apparent that the relationship is relatively weak, with R2 only
around 15%. The interaction effect is not close to statistical significance (recall
that the F-tests for the main effects are not meaningful in the presence of
the interaction). This suggests removing it and fitting the model with only
the main effects, but boxplots of standardized residuals (Figure 6.3) show
reasonably strong evidence of nonconstant variance related to genre (it is
interesting to note that the evidence of nonconstant variance in logged DVD
sales related to MPAA rating has disappeared in the residual plots).
The Levene’s test confirms that nonconstant variance is related to genre,
but not MPAA rating.
Response: Abs.resid
Sum Sq
Df
F value
Pr(>F)
Rating
0.388
2
0.5786 0.56235
Genre
2.430
2
3.6230 0.02985 *
Residuals
37.891 113
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1

6.4 Example — DVD Sales of Movies
127
PG
PG−13
R
−3
−1
0
1
2
MPAA rating
Standardized residuals
−3
−1
0
1
2
Standardized residuals
Genre
Action/
Adventure
Comedy
Drama
FIGURE 6.3: Side-by-side boxplots of standardized residuals from the
two-way OLS ANOVA fit of logged DVD sales on MPAA rating, genre,
and their interaction.
This suggests performing a WLS analysis, with the weights being based on
genre. This is accomplished by determining the variance of the (standardized)
residuals separated by genre group, and then weighting each observation
from that group by the inverse of the variance. The resultant weights are
w = 1/1.5721 for action/adventure movies, w = 1/0.7245 for comedies,
and w = 1/0.8692 for dramas, respectively. This results in the following
WLS-based ANOVA fit:
Response: Log.dvd
Sum Sq
Df
F value
Pr(>F)
Rating
0.076
2
0.2360 0.79017
Genre
1.034
2
3.1949 0.04484 *
Rating:Genre
0.877
4
1.3552 0.25431
Residuals
17.639 109
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.4023 on 109 degrees of freedom
Multiple R-squared: 0.1543,
Adjusted R-squared: 0.09219
The entry for R2 in the output above is worth further comment. Since
the WLS estimates are based on minimizing the weighted residual sum of
squares (6.4), the interpretation of R2 as the observed proportion of variability
accounted for by the regression is lost. In fact, the R2 value typically reported
in WLS output is a transformed version of the overall F-statistic for the
ANOVA fit that uses the relationship between R2 and the overall F-statistic
for OLS fits, and thus has no physical interpretation. The WLS results are
not very different from those of OLS, with the interaction effect still not close
to statistical significance. Note, however, that now boxplots of standardized
residuals (Figure 6.4) do not show evidence of nonconstant variance, and the
Levene’s test agrees (note that the Levene’s test for a WLS fit must be based

128
CHAPTER 6 Analysis of Variance
PG
PG−13
R
−2
0
1
2
3
MPAA rating
Standardized residuals
−2
0
1
2
3
Standardized residuals
Genre
Action/
Adventure
Comedy
Drama
FIGURE 6.4: Side-by-side boxplots of standardized residuals from the
two-way WLS ANOVA fit of logged DVD sales on MPAA rating, genre, and
their interaction.
on standardized residuals, since unlike the ordinary residuals those take the
weights into account).
Response: Abs.resid
Sum Sq
Df
F value Pr(>F)
Rating
0.263
2
0.3739 0.6889
Genre
0.053
2
0.0747 0.9280
Residuals
39.711 113
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
It is now reasonable to omit the interaction effect from the model and
examine the WLS fit based only on main effects. If the interaction effect had
been needed, an interaction plot would be used to summarize its implications.
The interaction plot is given in Figure 6.5 for completeness. The plot suggests
that the genre effect of logged DVD sales for action/adventures and dramas are
similar (with highest sales for PG-13 movies), while that for comedies is the
opposite (with lowest sales for PG-13 movies), but the insignificant F-test for
the interaction effect implies that this is likely to just be random fluctuation.
The WLS-based ANOVA fit that includes only the main effects suggests
that only genre is predictive for logged DVD sales.
Response: Log.dvd
Sum Sq
Df F value
Pr(>F)
Rating
0.3275
2
0.9994 0.37133
Genre
1.3232
2
4.0375 0.02025 *
Residuals 18.5162 113
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.4048 on 113 degrees of freedom

6.4 Example — DVD Sales of Movies
129
Multiple R-squared: 0.1122,
Adjusted R-squared: 0.08078
--------------------
Response: Log.dvd
Sum Sq
Df F value
Pr(>F)
Genre
2.0127
2
6.1415 0.002923 **
Residuals 18.8437 115
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.4048 on 115 degrees of freedom
Multiple R-squared: 0.0965,
Adjusted R-squared: 0.08079
Since the final model is a one-way ANOVA, the fitted value for any
movie of a particular genre is just the mean response of movies of that
genre. Thus, the fitted logged DVD sales for action/adventure movies is 1.471
(corresponding to a geometric mean sales of 101.471 = $29.6 million), for
comedies is 1.320 ($20.9 million), and for dramas is 1.132 ($13.6 million).
Tukey multiple comparisons tests show that mean logged sales are statistically
1.0
1.1
1.2
1.3
1.4
1.5
MPAA Rating
Mean logged sales
PG
PG−13
R
Genre
Action/Adventure
Comedy
Drama
FIGURE 6.5: Interaction plot for logged DVD sales separated by MPAA
rating and genre.

130
CHAPTER 6 Analysis of Variance
significantly different between action/adventure movies and dramas and
between comedies and dramas, but not between action/adventure movies and
comedies.
Multiple Comparisons of Means: Tukey Contrasts
Linear Hypotheses:
Estimate Std.Err. t val Pr(>|t|)
Comedy = Action/Adventure -0.1517
0.1024
-1.48
0.30083
Drama = Action/Adventure
-0.3390
0.1026
-3.30
0.00357 **
Drama = Comedy
-0.1873
0.0793
-2.36
0.05068 .
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
The output from the one-way ANOVA WLS fit gives ˆσ = 0.4048. Recall-
ing that ˆσi = ˆσ/√wi, this implies estimated standard deviations of the errors of
(.4048)
√
1.5721 = .508 for action/adventure movies, (.4048)
√
0.7245 = .345
for comedies, and (.4048)
√
0.8692 = .377 for dramas, respectively. This in
turn implies exact 95% prediction intervals for logged DVD sales of (0.452,
2.491) for action/adventure movies, (0.628, 2.011) for comedies, and (0.377,
1.888) for dramas, respectively. Unfortunately, these are far too wide to be
useful in practice, since antilogging the ends of the intervals implies pre-
dictive ranges from roughly $2 to $4 million to upwards of $300 million
(for action/adventure movies). This is not particularly surprising, given the
observed weak relationship between logged DVD sales and genre. A more
reasonable model would also include other predictors, including numerical
ones (such as total gross revenues of the movie in theaters). Generalizing regres-
sion modeling to allow for the possibility of both numerical and categorical
predictors is the topic of Chapter 7.
Residual and diagnostic plots given in Figure 6.6 show that nonconstant
variance is no longer a problem (note that the weighting is taken into account
in the calculation of leverage values and Cook’s distances). There are five
marginal outliers (“The Blind Side,” “The Hangover,” and “The Twilight
Saga: New Moon” did unusually well for their respective genres, while “Shorts”
and “Sorority Row” did unusually poorly). An analysis after omitting these
points (not given) does not change the results of the model selection process
or the implications of the results, but does result in a stronger apparent
relationship between logged DVD sales and genre, to the extent that mean
logged DVD sales for comedies and dramas are now marginally statistically
significantly different from each other.
6.5
Higher-Way ANOVA
ANOVA models generalize past one or two grouping variables, at the cost
of some additional complications in the model. Consider the situation of

6.6 Higher-Way ANOVA
131
1.15
1.20
1.25
1.30
1.35
1.40
1.45
−2
0
1
2
3
Fitted values
Std. residuals
−2
0
1
2
3
Std. residuals
−2
−1
0
1
2
Normal Q−Q Plot
Theoretical Quantiles
Sample Quantiles
Genre
Action/
Adventure
Comedy
Drama
0
20
40
60
80
100
120
−3
−1
1
2
3
−2
0
2
1
3
Index
Std. residuals
0
20
40
60
80
100
120
0.00
0.02
0.04
0.06
Index
Leverage
0
20
40
60
80
100
120
0.00
0.04
0.08
Index
Cook’s D
FIGURE 6.6: Residual and diagnostic plots for the WLS-based ANOVA fit
of logged DVD sales on genre.
three grouping variables, sometimes generically referred to rows, columns, and
layers. A full three-way ANOVA model would have the form
yijkℓ= μ + αi + βj + γk + (αβ)ij + (αγ)ik + (βγ)jk + (αβγ)ijk + εijkℓ.
The main effects α, β, and γ are defined as they were before, as are the
two-way interactions (αβ), (αγ), and (βγ). Just as a two-way interaction is
defined as the presence of different main effects for one factor as the level
of the second factor changes, a three-way interaction occurs if the two-way
interaction of two factors differs depending on the level of the third factor. An
interaction plot would identify this graphically as different patterns in inter-
action plots of rows and columns for different layers. This can be extended
further to more potential grouping variables, although difficulties in interpre-
tation make interactions that are higher order than three-way rarely used in
practice.
The model is fit in the same way as the simpler ones were fit. The variables
that define the three-way interaction are all of the products taking one effect
coding each from those corresponding to rows, columns, and layers. As before,
if only one observation per cell is available, the three-way interaction (being
the highest order interaction) cannot be included in the model.

132
CHAPTER 6 Analysis of Variance
6.6
Summary
Categorical variables occur often in practice, and building models using them
as predictors is often desirable. The use of indicator variables or effect codings
makes fitting of these ANOVA models explicitly a linear regression problem,
allowing all of the power of linear modeling to be used. The nature of
categorical variables as ones that identify well-defined subgroups in the data
means that nonconstant variance related to the existence of those subgroups
is a common problem. For this reason, weighted least squares (WLS)-based
fits of ANOVA data are often advisable. Multiple comparisons methods such
as the Tukey HSD test and the use of Bonferroni corrections allow for the
comparison of mean responses for multiple pairs of groups that take into
account the potentially large number of comparisons being made.
There is no requirement that a regression relationship be based only on
numerical predictors, or only on categorical ones. In Chapter 7 we discuss the
natural generalization to the situation of a mixture of such variable types.
KEY TERMS
Balanced design: A data set in which the number of observations is the same
for all combinations of group levels.
Bonferroni correction: A correction applied in a multiple testing situation
designed to keep the overall level of significance of the set of tests, rather
than the significance level of each test, at α. When C tests are performed, the
significance level is taken to be α/C for each individual test, in order to ensure
that the overall level of significance is no greater than α.
Effect coding: A variant of indicator variables that results in regression
coefficients that represent deviations from an overall level when used to code
factors in an analysis of variance model.
Interaction effect: In two-way classified data, a pattern wherein the row
effect on an observation differs depending on the column in which it is
located, and vice versa. In higher-order classified data, a pattern wherein a
lower-order interaction effect differs depending on the level of a factor not in
the interaction.
Interaction plot: A plot of each of the row effects separated by column level,
or equivalently of each of the column effects separated by row level. Roughly
parallel lines indicate the lack of an interaction effect.
Levene’s test: A test for heteroscedasticity based on using the absolute value of
the (standardized) residuals as the response variable in a regression or ANOVA
fit.
Main effect: The effect on an observation related to the row or the column
in which it occurs.

6.6 Summary
133
Multiple comparisons: The statistical inference problem in which (adjusted)
group means are compared simultaneously in an analysis of variance model.
One-way analysis of variance (ANOVA): A methodology used to test the
equality of means of several groups or levels classified by a single factor.
Two-way analysis of variance (ANOVA): A methodology used to test the
equality of means of a several groups or levels classified by two factors. The
analysis allows variation between means to be separated into row or column
main effects, as well as an interaction effect.
Tukey’s HSD test: A test that directly addresses the multiple comparisons
problem by using the (correct) studentized range distribution for differences
between sample means.
Weighted least squares (WLS): A generalization of ordinary least squares
(OLS) that accounts for nonconstant variance by weighting observations with
smaller (estimated) variances more heavily than those with larger (estimated)
variances.

Seven
Chapter
Analysis of Covariance
7.1
Introduction
135
7.2
Methodology
136
7.2.1
Constant Shift Models
136
7.2.2
Varying Slope Models
137
7.3
Example — International Grosses of Movies
137
7.4
Summary
142
7.1
Introduction
The analysis of variance (ANOVA) models of Chapter 6 are restrictive in
that they allow only categorical predicting variables. Analysis of covariance
(ANCOVA) models remove this restriction by allowing both categorical
predictors (often called grouping variables or factors in this context) and
continuous predictors (typically called covariates) in the model. This can be
viewed as a generalization of the ANOVA models of Chapter 6 to models that
include covariates, or a generalization of the models discussed in Section 2.4
based on indicator variables to models that allow for categorical variables with
more than two categories.
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
135

136
CHAPTER 7 Analysis of Covariance
7.2
Methodology
7.2.1
CONSTANT SHIFT MODELS
Conceptually, ANCOVA models are quite straightforward, since they merely
add additional numerical predictors to the constructed (effect coding) variables
used in ANOVA modeling. If there is one grouping variable, for example, the
model is
yij = μ + αi + β1x1ij + · · · + βpxpij + εij, i = 1, . . . , K, j = 1, . . . , ni,
(7.1)
where αi is the corrected effect on y given membership in group i (corrected in
the sense that the covariates x1, . . . , xp are taken into account, and subject to
the usual constraint 
iαi = 0), and βℓis the slope coefficient corresponding
to the expected change in the response associated with a one unit change in xℓ
given group membership and all of the other covariates are held fixed. This
model is fit using K −1 effect codings to represent the grouping variable,
along with the p covariates and the constant term.
There are several obvious hypotheses of interest based on this model:
1. Are there differences in expected level between groups (given the
covariates)? This tests the null hypothesis
H0 : α1 = · · · = αK = 0.
The test used for this hypothesis is the partial F-test for the K −1
effect coding variables (that is, it is based on the residual sum of squares
using all of the variables, and the residual sum of squares using only
the covariates). Further analysis exploring which groups are significantly
different from which (given the covariates) is meaningful here, and the
multiple comparisons methods of Section 6.3.2 can be adapted to this
model.
2. Do the covariates have any predictive power for y (given the grouping
variable)? This tests the null hypothesis
H0 : β1 = · · · = βp = 0.
The test used for this hypothesis is the partial F-test for the covariates
(that is, it is based on the residual sum of squares using all of the variables,
and the residual sum of squares using only the effect codings).
3. Does a particular variable xj provide any predictive power given the
grouping variable and the other covariates? This tests the null hypothesis
H0 : βj = 0.
The test used for this hypothesis is the usual t-test for that covariate.
Note that since ANCOVA models are just regression models, all of the model
selection approaches discussed in Section 2.3.1 apply here as well, although
they are less commonly used in practice in this situation.

7.3 Example — International Grosses of Movies
137
This model generalizes to more than one grouping variable as well. For
two grouping variables, for example, the model is
yijk = μ + αi + βj + (αβ)ij + γ1x1ijk + · · · + γpxpijk + εijk,
(7.2)
which allows for two main effects (fit using effect codings for each grouping
variable) and an interaction effect (fit using the pairwise products of the
effect codings for the main effects), as well as the presence of covariates.
The usual ANOVA hypotheses about the significance of main effects and the
interaction effect are tested using the appropriate partial F-tests, as described
in Sections 6.2.2 and 6.3.1.
7.2.2
VARYING SLOPE MODELS
Models (7.1) and (7.2) are constant shift models, in the sense that the only
differences between the expected value of the target variable for a given set of
covariate values between groups is one of shift, with the slopes of the covariates
being the same no matter what group an observations falls in. This implies a nat-
ural question: might the slopes also be different for different levels of the group-
ing variable? Assume for simplicity that there is one covariate x in the model.
A generalized model that allows for different slopes for different groups is
yij = μ + αi + β1ixij + εij,
(7.3)
where β1i is the slope of x for the ith group. If the interaction of the grouping
variable and the covariate are entered as part of the general linear model (by
including the pairwise products of the effect codings and the covariate), the
partial F-test for this set of variables is a test of the hypothesis
H0 : β11 = · · · = β1K
(this is often called a test of common slope). This is easily generalized to
more than one covariate using the appropriate interaction terms. This can
also be generalized to the situation with more than one grouping variable, but
that is less common.
7.3
Example — International Grosses of Movies
Although domestic (U.S. and Canada) gross revenues of movies are the
numbers routinely reported in the American news media, revenues from
other countries can often outstrip domestic revenues and mean the difference
between profit and loss. It is thus of interest to try to model international gross
revenues. This analysis is based on revenues for movies released in the United
States on more than 500 screens in 2009, with logged (base 10) domestic
grosses and MPAA rating used to model logged (base 10) international grosses.
Figure 7.1 shows that movies with higher domestic revenues tend to have
higher international revenues, and movies rated G and PG tend to have higher

138
CHAPTER 7 Analysis of Covariance
1.0
1.5
2.0
2.5
–1
0
1
2
3
(a)
(b)
Logged domestic grosses
Logged international grosses
G
PG PG−13
R
–1
0
1
2
3
MPAA rating
Logged international grosses
FIGURE 7.1: Plots for the 2009 international movie grosses data. (a) Plot of
logged international gross versus domestic gross. (b) Side-by-side boxplots of
logged international gross by MPAA rating.
international revenues than those rated PG-13 and R. There is also evidence
of nonconstant variance, with movies with lower domestic revenues having
higher variability.
We first fit a constant shift model:
Response: Log.international.gross
Sum Sq
Df
F value
Pr(>F)
(Intercept)
3.610
1
14.2648 0.0002468 ***
Log.domestic.gross 42.630
1 168.4463 < 2.2e-16 ***
Rating
2.712
3
3.5722 0.0160807 *
Residuals
30.876 122
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
-0.710076
0.188006
-3.777 0.000247 ***
Log.domestic.gross
1.385308
0.106737
12.979
< 2e-16 ***
Rating1
0.440935
0.192664
2.289 0.023822 *
Rating2
0.008748
0.098965
0.088 0.929705
Rating3
-0.182498
0.086866
-2.101 0.037707 *
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.5031 on 122 degrees of freedom
Multiple R-squared: 0.6087,
Adjusted R-squared: 0.5958
F-statistic: 47.44 on 4 and 122 DF,
p-value: < 2.2e-16
Logged domestic gross and MPAA rating account for roughly 60% of the
variability in logged international gross. The standard error of the estimate of
ˆσ = 0.503 implies that the model can predict international grosses to within a

7.3 Example — International Grosses of Movies
139
multiplicative factor of roughly 10, 95% of the time, clearly a large range. The
coefficient for logged domestic gross (which is highly statistically significant)
implies that, given MPAA rating, a 1% increase in domestic gross is associated
with an estimated expected 1.39% increase in international gross. The three
effect codings (Rating1, Rating2, and Rating3) refer to effects for
movies rated G, PG, and PG-13, respectively, and their coefficients imply
much higher grosses for G-rated movies than the other three types, given logged
domestic gross. Since the coefficients must sum to 0, the implied estimated
coefficient for R-rated movies is −0.267185. The rating effect, while much
less strong than that of logged domestic gross, is also statistically significant.
Since there is no interaction term in this model, multiple comparison tests
based on the Tukey method (Section 6.3.2) can be used to assess which rating
classes are significantly different from each other given logged domestic gross.
Multiple Comparisons of Means: Tukey Contrasts
Linear Hypotheses:
Estimate Std. Error t value Pr(>|t|)
PG = G
-0.43219
0.27028
-1.599
0.3620
PG-13 = G
-0.62343
0.26165
-2.383
0.0774 .
R = G
-0.70812
0.26437
-2.679
0.0369 *
PG-13 = PG
-0.19125
0.11574
-1.652
0.3329
R = PG
-0.27593
0.12512
-2.205
0.1160
R = PG-13
-0.08469
0.10608
-0.798
0.8449
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
It can be seen that G-rated movies have significantly higher international
grosses than do R-rated movies (p = .037, corresponding to 10.708 = 5.1 times
the grosses given logged domestic gross) and marginally significantly higher
international grosses than do PG-13-rated movies (p = .077, corresponding
to 10.623 = 4.2 times the grosses given logged domestic gross). Note that
these effects are larger than the corresponding marginal MPAA rating effects
(for example, if logged domestic gross is omitted from the model, G-rated
movies are estimated to have only 3.6 times the international gross than
R-rated movies, not 5.1 times), showing that the poorer relative international
performance of PG-13 or R-rated movies is ameliorated somewhat by their
stronger relative domestic performance in these data.
A generalization of this model would be to model (7.3), allowing for
different slopes for logged domestic gross for different MPAA rating classes.
This involves adding the interaction between the categorical predictor and the
covariate:
Response: Log.international.gross
Sum Sq
Df F value
Pr(>F)
(Intercept)
0.1170
1
0.4892
0.485630
Log.domestic.gross
5.0368
1 21.0565 1.113e-05 ***
Rating
3.4105
3
4.7526
0.003633 **
Log.domestic.gross:Rating
2.4103
3
3.3588
0.021173 *

140
CHAPTER 7 Analysis of Covariance
Residuals
28.4652 119
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Coefficients:
Estimate Std.Error
t val
Pr(>|t|)
(Intercept)
-0.25185
0.3601
-0.70
0.4856
Log.dom.gross
1.08408
0.2363
4.59 1.11e-05 ***
Rating1
1.87935
0.9991
1.88
0.0624 .
Rating2
0.24720
0.4511
0.55
0.5847
Rating3
-0.67295
0.4019
-1.67
0.0967 .
Log.dom.gross:Rating1 -0.99180
0.6676
-1.49
0.1400
Log.dom.gross:Rating2 -0.09207
0.2793
-0.33
0.7423
Log.dom.gross:Rating3
0.32025
0.2572
1.25
0.2155
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.4891 on 119 degrees of freedom
Multiple R-squared: 0.6392,
Adjusted R-squared: 0.618
F-statistic: 30.12 on 7 and 119 DF,
p-value: < 2.2e-16
Adding the interaction effect does not increase the fit greatly, but the
partial F-test of constant slope does imply that the slopes for logged domestic
gross are statistically significantly different across MPAA ratings. Figure 7.2
indicates the different implications of the two models. The plot on the left
gives the fitted lines relating logged international and logged domestic grosses
when restricting the model to constant slope, while the plot on the right allows
the slopes to be different. The interesting pattern emerges that as the potential
audience for a movie shrinks (MPAA rating going from G to PG to PG-13 to R)
1.0
1.5
2.0
2.5
–1
0
1
2
3
(a) Constant slope
Logged domestic grosses
Logged international grosses
1.0
1.5
2.0
2.5
–1
0
1
2
3
(b) Varying slopes
Logged domestic grosses
Logged international grosses
FIGURE 7.2: ANCOVA fits for the 2009 international movie grosses data.
Solid line: G-rated movies. Short dashed line: PG-rated movies. Dotted-
and-dashed line: PG-13-rated movies. Long dashed line: R-rated movies.
(a) Fitted lines for the constant slope model. (b) Fitted lines for the varying
slope model.

7.3 Example — International Grosses of Movies
141
the importance of domestic gross as a predictor of international gross grows.
While for G-rated movies there is virtually no relationship between domestic
and international grosses, a 1% increase in domestic grosses is associated with
an estimated expected 1%, 1.4%, and 1.85% increase in international grosses
for PG-, PG-13-, and R-rated movies, respectively. Given that the proportion
of the audience that are adults grows going from G to R ratings, it appears that
internationally adults are more sensitive to the domestic success of a movie
than are children.
Residual plots given in Figure 7.3 highlight several violations of assump-
tions here, so these results cannot be viewed as definitive. There is apparent
nonconstant variance, with movies with lower domestic revenues having higher
variability in international revenues, and the residuals are long left-tailed. As
was noted earlier, the situation where the variances are related to a numerical
predictor will be discussed further in Section 10.7.
0.5
1.0
1.5
2.0
2.5
3.0
–4
–3
–2
–1
0
1
2
(a)
Fitted values
Standardized residuals
1.0
1.5
2.0
2.5
–4
–3
–2
–1
0
1
2
(b)
Logged domestic grosses
Standardized residuals
G
PG
PG−13
R
–4
–3
–2
–1
0
1
2
(c)
MPAA rating
Standardized residuals
–2
–1
0
1
2
–4
–3
–2
–1
0
1
2
(d)
Theoretical Quantiles
Sample Quantiles
FIGURE 7.3: Residual plots for the varying slopes ANCOVA model for
the 2009 international grosses data. (a) Plot of standardized residuals ver-
sus fitted values. (b) Plot of standardized residuals versus logged domestic
gross. (c) Side-by-side boxplots of standardized residuals by MPAA rating.
(d) Normal plot of standardized residuals.

142
CHAPTER 7 Analysis of Covariance
7.4
Summary
Analysis of covariance models represent the natural generalization of analysis
of variance models to allow for numeric covariates. The simplest model is a
constant shift (constant slope) model, but models that allow for varying slopes
for different groups are also easily constructed. Since various models of interest
are nested within each other, partial F-tests are a natural way to compare
models, although information measures such as AICc also can be used for this
purpose.
KEY TERMS
Analysis of covariance: A methodology used to analyze data characterized by
several groups or levels classified by a single factor or multiple factors and
numerical predictors. The analysis allows variation between group means to
be separated into main effects and (potentially) interaction effects, as well as
effects related to the numerical predictors (including the possibility of different
slopes for different groups).
Covariate: A numerical predictor in an analysis of covariance model.

Part Four
Non-Gaussian
Regression Models

Eight
Chapter
Logistic Regression
8.1
Introduction
145
8.2
Concepts and Background Material
147
8.2.1
The Logit Response Function
148
8.2.2
Bernoulli and Binomial Random Variables
149
8.2.3
Prospective and Retrospective Designs
149
8.3
Methodology
152
8.3.1
Maximum Likelihood Estimation
152
8.3.2
Inference, Model Comparison, and Model
Selection
153
8.3.3
Goodness-of-Fit
155
8.3.4
Measures of Association and Classification
Accuracy
157
8.3.5
Diagnostics
159
8.4
Example — Smoking and Mortality
159
8.5
Example — Modeling Bankruptcy
163
8.6
Summary
168
8.1
Introduction
All of the regression situations discussed thus far have been characterized
by a response variable that is continuous, but the modeling of a categorical
variable having two (or more) categories is sometimes desired. Consider a
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
145

146
CHAPTER 8 Logistic Regression
study of risk factors for cancer. Data are collected from the health records
of subjects on age, sex, weight, smoking status, dietary habits, and family’s
medical history. In this case the response variable is whether the person has
lung cancer (y = 1), or does not have lung cancer (y = 0), and the question
of interest is “What factors can be used to predict whether or not a person
will have lung cancer?”. Similarly, in financial analysis the solvency (“health”)
of a company is of great interest. In such an analysis the question of interest
is “What financial characteristics can be used to predict whether or not a
business will go bankrupt?”.
In this chapter we will focus on data where the response of interest takes
on two values, which we will take to be 0 and 1; that is, binary data. This will
be generalized to more than two values in the next chapter. In this situation,
the expected response E(yi) is the conditional probability of the event of
interest (yi = 1, generically termed a success, with yi = 0 termed a failure)
given the values of the predictors. A linear model would be
E(yi) ≡πi = β0 + β1x1i + · · · + βpxpi.
It is clear that a linear least squares modeling of this probability is not
reasonable, for several reasons.
1. Least squares modeling is based on the assumption that yi is normally
distributed (since εi is normally distributed). This is obviously not possible
when it only takes on two possible values.
2. If a predictor value is large enough or small enough (depending on the
sign of the associated slope as long as it is nonzero), a linear model implies
that the probability of a success will be outside the range [0, 1], which is
impossible.
3. A linear model implies that a specific change in a predictor variable is
associated with the same change in the probability of success for any value
of the predictor, which is unlikely to be the case. For example, a firm
with extremely high debt is likely to already have a high probability of
bankruptcy; for such a firm, it is reasonable to suppose that an increase
in debt of $1 million (say) would be associated with a small absolute
change in the probability of bankruptcy. This is related to point (2), of
course, since a probability close to 1 can only increase a small amount
and still remain less than or equal to 1. Similarly, for a firm with very
little debt (and hence a small probability of bankruptcy), this increase in
debt could reasonably be thought to be associated with a small change
in the probability of bankruptcy. On the other hand, such an increase in
debt could be expected to have a much larger effect on the probability of
bankruptcy for a firm with moderate debt, and hence moderate probability
of bankruptcy. Similar arguments (but in the opposite direction) would
apply to a predictor with an inverse relationship with bankruptcy, such
as revenues. This implies “S-shaped” curves for probabilities, as given in
Figure 8.1.

8.2 Concepts and Background Material
147
0.0
0.2
0.4
0.6
0.8
1.0
X
Probability
FIGURE 8.1: S-shaped curves for probabilities.
In this chapter, we discuss logistic regression, an alternative regression
model that is appropriate for binary data. Many of the inferential methods
and techniques discussed in earlier chapters generalize to logistic regression,
although a fundamental change is that in the present context they are not
exact, but are rather based on approximations.
8.2
Concepts and Background Material
The regression models discussed thus far are characterized by two key prop-
erties: a linear relationship between the expected response and the predictors,
and a normal distribution for the errors. Models for a binary response variable
are similarly based on two key properties. First, as was pointed out in the
previous section, an S-shaped relationship rather than a linear relationship is
the basis of the modeling. Second, a distribution that is more appropriate for
binary data than the normal distribution is used. We treat each of these points
in turn.

148
CHAPTER 8 Logistic Regression
8.2.1
THE LOGIT RESPONSE FUNCTION
The functional form underlying logistic regression that generates curves like
those in Figure 8.1 is the logit function. Let π(x) be the probability of a
success for the observed values of the predictors x. The odds of a success is the
ratio of the probability of a success to the probability of failure, or
π(x)
1 −π(x)
(since the probability of a failure is 1 −π). Note that the odds vary between 0
and ∞as the probability varies between 0 and 1.
The logit is defined as the natural log of the odds of success,
ℓ(x) = log

π(x)
1 −π(x)

.
Logistic regression hypothesizes that the logit is linearly related to the predic-
tors; that is,
ℓ(x) = log

π(x)
1 −π(x)

= β0 + β1x1i + · · · + βpxpi.
(8.1)
Note that the logit varies between −∞and ∞as the probability varies between
0 and 1, making logits more suitable for linear fitting. Solving for π(x) gives an
equivalent representation to (8.1) that explicitly provides the implied S-shaped
curve for probabilities,
πi(x) =
eβ0+β1x1i+···+βpxpi
1 + eβ0+β1x1i+···+βpxpi .
(8.2)
This inverse of the logit function is sometimes called the expit function.
Examination of (8.1) shows that it is, in fact, a semilog model for the
odds of success, in the sense of Section 4.3 and equation (4.4). That is, the
model posits an additive/multiplicative relationship between a predictor and
the odds, with a multiplicative change in the odds of success of eβj associated
with a one unit increase in xj holding all else in the model fixed. This value
is thus called the odds ratio, as it represents the ratio of the odds for xj + 1
to the odds for xj. A slope of 0 corresponds to no relationship between the
logit (and hence the probability) and the predictor given the other variables in
the model, a positive slope corresponds to a direct relationship, and a negative
slope corresponds to an inverse relationship.
The use of natural logs in the definition of the logit implies that if a
predictor is going to be modeled in the logged scale, natural logs (base e)
should be used, rather than common logs (base 10). This is because in that
case, the slope βj is an elasticity (in the sense of Section 4.2). That is, if log xj
is used as a predictor, this implies that a 1% change in xj is associated with a
βj% change in the odds of success holding all else in the model fixed.
The logit is not the only function that can generate S-shaped curves.
Indeed, any cumulative distribution function for a continuous random variable
generates curves similar to those in Figure 8.1, including (for example) the

8.2 Concepts and Background Material
149
normal distribution (which leads to probit regression). The use of the logit
response does have some advantages, which will be discussed in Section 8.2.3.
8.2.2
BERNOULLI AND BINOMIAL RANDOM VARIABLES
It is apparent that a normal distribution is not a reasonable choice for data that
take on two values, 0 and 1. The natural choice is a Bernoulli random variable,
where each observation yi is independent, with P(yi = 1) = πi. Combining
this with the functional form (8.1) defines the logistic regression model.
This model also generalizes in an important way. In some circumstances,
multiple observations with the same set of predictor variable values occur.
This is often by design; for example, a clinical trial might be designed so
that exactly 10 men and 10 women each receive a specific dosage of a drug
from the set of dosages being studied. In this situation, the ith response yi
would correspond to the number of successes (say cures) out of the ni = 10
people of the specific gender who received the specific dosage corresponding
to the ith observation. If the responses of each of the 10 individuals are
independent of each other, and each individual has the same probability πi of
being cured, then yi is a binomial random variable based on ni trials and πi
probability of success [represented yi ∼B(ni, πi)]. The ni value is sometimes
called the number of replications, as each of the underlying Bernoulli trials
has identical values of the predictors. Note that in this situation, the number
of observations n is the number of response values yi, not the total number of
replications 
ini.
8.2.3
PROSPECTIVE AND RETROSPECTIVE DESIGNS
The particular choice of the logit function to represent the relationship
between probabilities and predictors has two justifications in practice. The
first, referred to in Section 8.2.1, is the intuitive nature of (8.1) as a semilog
model for the odds of success. Multiplicative relationships are relatively easy
to understand, so the interpretation of eβj as an odds ratio is an appealing one.
The second justification is less straightforward, but of great practical
importance. It is related to the sampling design used when obtaining the data.
Consider building a model for the probability that a business will go bankrupt
as a function of the initial debt carried by the business. There are two ways
that we might imagine constructing a sample (say of size 200) of businesses to
analyze:
1. Randomly sample 200 businesses from the population of interest. Record
the initial debt, and whether or not the business went bankrupt. This is
conceptually consistent with following the 200 businesses through time
until they either do or do not go bankrupt, and is called a prospective
sampling scheme for this reason. In the biomedical literature, this is
often called a cohort study. A variation on this design is to sample 100
businesses with low debt and 100 businesses with high debt, respectively;

150
CHAPTER 8 Logistic Regression
since sampling is based on the value of a predictor (not the response), it is
still a prospective design.
2. First consider the set of all businesses in the population that did not
go bankrupt; randomly sample 100 of them and record the initial debt.
Then consider the set of all businesses in the population that did go
bankrupt; randomly sample 100 of them and record the initial debt. This
is conceptually consistent with seeing the final state of the businesses first
(bankrupt or not bankrupt), and then going backwards in time to record
the initial debt, and is called a retrospective sampling scheme for this
reason. In the biomedical literature this is often called a case-control study.
Note that whether the sampled data are current or from an earlier time
has nothing to do with whether a design is prospective or retrospective;
the distinction depends only on whether sampling is based on the response
outcome (a retrospective study) or not (a prospective study). One way to
distinguish between these two sampling approaches is that in a retrospective
study, the sampling rate is different for successes and for failures; that is, one
group is deliberately oversampled, while the other is deliberately undersampled
so as to get a “reasonable” number of observations in each group.
Each of these sampling schemes has advantages and disadvantages. The
prospective study is more consistent with the actual physical process of
interest; for example, the observed sample proportion of low-debt businesses
that go bankrupt is an estimate of the actual probability of a randomly chosen
low-debt business from this population going bankrupt, a number that cannot
be estimated using data from a retrospective study (since in that case it was
arbitrarily decided that (say) half the sample would be bankrupt businesses,
and half would be non-bankrupt businesses). Such studies also have the
advantage that they can be used to study multiple outcomes; for example,
The British Doctors Study, run by Richard Doll, Austin Bradford Hill, and
Richard Peto, followed 40,000 doctors for 50 years, and examined how various
factors (particularly smoking) related to different types of cancer, emphysema,
heart disease, stroke, and other diseases. On the other hand, if bankruptcy
rates are low (say 15%), a sample of size 200 is only going to have about 30
bankrupt businesses in it, which makes it more difficult to accurately model the
probability of bankruptcy. A retrospective study can be designed to make sure
that there are enough bankrupt companies to estimate their characteristics well.
To simplify things, assume that initial debt is recorded only as Low (L)
or High (H). This implies that the data take the form of a 2 × 2 contingency
table (whatever the sampling scheme):
Bankrupt
Yes
No
Debt
Low
nLY
nLN
nL
High
nHY
nHN
nH
nY
nN
n
Here the subscripts Y and N refer to bankrupt (“Yes”) and not bankrupt
(“No”).

8.2 Concepts and Background Material
151
Even though the data have the same form, whatever the sampling scheme,
the ways these data are generated are very different. The following two
tables give the expected counts in the four data cells, depending on the
sampling scheme. The π values are conditional probabilities (so, for example,
πY |L is the probability of a business going bankrupt given that it has low
initial debt):
PROSPECTIVE SAMPLE
RETROSPECTIVE SAMPLE
Bankrupt
Bankrupt
Yes
No
Yes
No
Debt
Low
nLπY |L
nLπN|L
Debt
Low
nY πL|Y
nNπL|N
High
nHπY |H
nHπN|H
High
nY πH|Y
nNπH|N
There is a fundamental difference between the probabilities that can
be estimated using the two sampling schemes. For example, what is the
probability that a business goes bankrupt given that it has low initial debt?
As noted above, this is πY |L. It is easily estimated from a prospective sample
(ˆπY |L = nLY /nL), as can be seen from the left table, but it is impossible to
estimate from a retrospective sample. On the other hand, given that a business
went bankrupt, what is the probability that it had low initial debt? That is
πL|Y , which is estimable from a retrospective sample (ˆπL|Y = nLY /nY ), but
not from a prospective sample.
The advantage of logistic regression is that the existence of a relationship
between debt level and bankruptcy is based on odds ratios (rather than
probabilities) through the logit function. In a prospective study, debt is
related to bankruptcy only if the odds of bankruptcy versus nonbankruptcy
are different for low debt companies than they are for high debt companies;
that is, if
πY |L
πN|L
̸=
πY |H
πN|H
,
or equivalently that the odds ratio does not equal 1,
πY |LπN|H
πN|LπY |H
̸= 1.
By the definition of conditional probability this odds ratio is equivalent to
πLY πHN
πHY πLN
,
(8.3)
where (for example) πLY is the probability of a company both having low
debt and going bankrupt.
In a retrospective study, in contrast, debt is related to bankruptcy only if
the odds of low debt versus high debt are different for bankrupt companies

152
CHAPTER 8 Logistic Regression
than they are for nonbankrupt companies; that is, if
πL|Y
πH|Y
̸=
πL|N
πH|N
,
or equivalently that the odds ratio does not equal 1,
πL|Y πH|N
πL|NπH|Y
̸= 1.
Some algebra shows that this odds ratio is identical to (8.3), the one from the
prospective study. That is, while the type of conditional probability that can
be estimated from the data depends on the sampling scheme, the odds ratio
is unambiguous whichever sampling scheme is appropriate. This property
generalizes to numerical predictors and multiple predictors. The logit function
is the only choice where effects are determined by the odds ratio, so it is the
only choice where the measure of the association between the response and
a predictor is the same under either sampling scheme. This means that the
results of studies based on the same predictors are directly comparable to each
other even if some are based on prospective designs while others are based on
retrospective designs.
Since odds ratios are uniquely defined for both prospective and retrospec-
tive studies, the slope coefficients {β1, β2, . . . , βp} are also uniquely defined.
The constant term β0, however, is driven by the observed proportions of
successes and failures, so it is affected by the construction of the study. Thus,
as noted above, in a retrospective study, the results of a logistic regression
fit cannot be used to estimate the (prospective) probability of success, since
that depends on a correct estimate of β0 through (8.2). Let πY and πN be
the true (unconditional) probabilities that a randomly chosen business goes
bankrupt or does not go bankrupt, respectively. These numbers are called
prior probabilities. If prior probabilities of success (πY ) and failure (πN) are
available, the constant term in a fitted logistic regression can be adjusted so
that correct (prospective) probabilities can be estimated. The adjusted constant
term has the form
˜β0 = ˆβ0 + log
πY nN
πNnY

(the method used to determine ˆβ0 is discussed in the next section). Since often
a data analyst is not sure about exactly what (πY , πN) are, it is reasonable
to try a range of values to assess the sensitivity of the estimated probabilities
based on the adjusted intercept to the specific choice that is made.
8.3
Methodology
8.3.1
MAXIMUM LIKELIHOOD ESTIMATION
As was noted earlier, least squares estimation of the logistic regression param-
eters β is not the best choice, as that is appropriate for normally distributed

8.3 Methodology
153
data. The generalization of least squares to binomially distributed data is max-
imum likelihood estimation. The theory of maximum likelihood is beyond
the scope of this book, but the underlying principle is that parameters are
estimated with the values that give the observed data the maximum probability
of occurring. For binary/binomial data, this corresponds to maximizing the
log-likelihood function
L =
n

i=1
[yi log πi + (ni −yi) log(1 −πi)],
(8.4)
where πi is assumed to satisfy the expit relationship (8.2) with the predictors
(terms that are not functions of the parameters are omitted from L since
they do not affect estimation). A related value is the deviance, which is twice
the difference between the maximum possible value of the log-likelihood L
and the value for the fitted model. A small value of the deviance means
that the observed data have almost as high a probability of occurring based
on the fitted model as is possible. It can be shown that maximum likeli-
hood estimates (MLEs) possess various large-sample (asymptotic) optimality
properties.
Least squares corresponds to maximum likelihood for errors that are
normally distributed, but in general (and for logistic regression in particular)
MLEs cannot be determined in closed form, but rather are obtained via an
iterative algorithm. One such algorithm (iteratively reweighted least squares,
or IRWLS) shows that the MLEs are approximately weighted least squares
estimates (6.5), with a weight and corresponding error variance for each
observation that depends on the parameter estimates. This approximation is
the basis of many of the inferential tools used to assess logistic regression fits.
Chapter 3 of Hilbe (2009) provides more details about this algorithm.
8.3.2
INFERENCE, MODEL COMPARISON, AND MODEL
SELECTION
Inferential questions arise in logistic regression that are analogous to those
in least squares regression, but the solutions are more complicated, as they
are either highly computationally intensive or are based on approximations.
For example, a test of the overall strength of the regression, testing the
hypotheses
H0 : β1 = · · · = βp = 0
versus
Ha : βj ̸= 0 for at least one j
(which for least squares fitting is tested using the overall F-test) is desirable.
The standard test of these hypotheses is the likelihood ratio test, which is
based on comparing the strength of the fit without any predictors to the
strength of the fit using predictors, as measured by the difference in the
deviance values for the models with and without predictors. The likelihood

154
CHAPTER 8 Logistic Regression
ratio test for the overall significance of the regression is
LR = 2
n

i=1

yi log
 ˆπa
i
ˆπ0
i

+ (ni −yi) log
1 −ˆπa
i
1 −ˆπ0
i

,
(8.5)
where ˆπa are the estimated probabilities based on the fitted logistic regression
model, and ˆπ0 are the estimated probabilities under the null hypothesis. This
is compared to a χ2 distribution on p degrees of freedom, which is valid as
long as either n is large or the ni values are reasonably large.
Two tests of the additional predictive power provided by an individual
predictor given the others are also available. These are tests of the hypotheses
H0 : βj = 0
versus
Ha : βj ̸= 0.
These hypotheses can be tested using the likelihood ratio test form described
above, where ˆπ0 is calculated based on all predictors except the jth predictor
and ˆπa is calculated based on all predictors, which is then compared to a
χ2 distribution on 1 degree of freedom. This requires fitting p + 1 different
models to test the significance of the p slopes, and is therefore not the typical
approach. The alternative (and standard) approach is to use the so-called Wald
test statistic,
zj =
ˆβj

s.e.(ˆβj)
,
where 
s.e.(ˆβj) is calculated based on the IRWLS approximation to the MLE.
This is analogous to a t-test for least squares fitting, but is again based
on asymptotic assumptions, so the statistics are compared to a Gaussian
distribution to determine significance rather than a t-distribution. Confidence
intervals for individual coefficients take the form ˆβj ± zα/2 
s.e.(ˆβj), and
a confidence interval for the associated odds ratio can be obtained by
exponentiating each of the endpoints of the confidence interval for βj.
Any two models where one is a special case of the other based on a
linear restriction can be compared using hypothesis testing. The simpler
model represents the null, while the more complicated model represents the
alternative, and they are tested using the likelihood ratio test as a difference of
deviance values. The appropriate degrees of freedom for the χ2 approximation
is the difference in the number of parameters estimated in the two models.
This is analogous to the partial F-test in least squares regression.
Such tests can be useful tools for model comparison and selection, but
as was discussed in Section 2.3.1, hypothesis tests are not necessarily the best
tools for model selection. The AIC criterion is applicable in logistic regression
models, taking the form
AIC = −2L + 2(p + 1),

8.3 Methodology
155
where L is the log-likelihood (8.4). Equivalently −2L can be replaced with
LR from (8.5) when calculating AIC. Although the theory underlying AICc
is not applicable for logistic regression, practical experience suggests that it
can help guard against the tendency of AIC to choose models that are overly
complex, so it can still be a useful tool. As before, it has the form
AICc = AIC + 2(p + 2)(p + 3)
n −p −3
.
8.3.3
GOODNESS-OF-FIT
The logistic model (8.1) or (8.2) is a reasonable one for probabilities, but
may not be appropriate for a particular data set. This is not the same thing as
saying that the predicting variables are not good predictors for the probability
of success. Consider the two plots in Figure 8.2. The variable X is a potential
predictor for the probability of success, while the vertical axis gives the observed
proportion of successes in samples taken at those values of X. So, for example,
X could be the dosage of a particular drug, and the target variable is the
proportion of people in a trial that were cured when given that dosage.
In the plot on the left, X is very useful for predicting success, but the
linear logistic regression model does not fit the data, since the probability of
success is not a monotone function of X (in a situation like this, treating X
as categorical, with the three categories being {[0, 15), [15, 30), [30, 50]}, would
seem to be a much more sensible strategy). In the plot on the right, X is
not a useful predictor for success (the probability of success appears to be
unrelated to X), but the linear logistic regression model fits the data, as a very
flat S-shaped curve (with βX ≈0) goes through the observed proportions of
success reasonably well. Goodness-of-fit tests are designed to assess the fit of a
model through the use of hypothesis testing. Such statistics test the hypotheses
H0 : The linear logistic regression model fits the data
0
10
20
30
40
50
0.0
0.4
0.8
X
Proportion of successes
0
10
20
30
40
50
0.0
0.4
0.8
X
Proportion of successes
FIGURE 8.2: Two plots of hypothetical observed proportions of success
versus a predictor.

156
CHAPTER 8 Logistic Regression
versus
Ha : The linear logistic regression model does not fit the data.
Such tests proceed by comparing the variability of the observed data around
the fitted model to the data’s inherent variability. This is possible because for
a binomial random variable, the observed model-based variability (based on
the residual yi −niˆπi) is distinct from the inherent variability of the random
variable [based on the fact that V (yi) = niπi(1 −πi)].
There are two standard goodness-of-fit test statistics. The Pearson
goodness-of-fit statistic equals
X2 =
n

i=1
(yi −niˆπi)2
niˆπi(1 −ˆπi),
(8.6)
while the deviance statistic (mentioned earlier) equals
G2 = 2
n

i=1

yi log
 yi
niˆπi

+ (ni −yi) log
 ni −yi
ni(1 −ˆπi)

(8.7)
(the latter statistic is sometimes referred to as the residual deviance). When the
ni values are reasonably large (ni > 5, with some values perhaps even smaller),
each of these statistics is referenced to a χ2 distribution on n −p −1 degrees
of freedom under the null hypothesis that the logistic regression model fits the
data. Thus, a small tail probability suggests that the linear logistic regression
model is not appropriate for the data, and an alternative should be sought.
The signed square roots of the values on the righthand side of (8.6) are called
the Pearson residuals, while the signed square roots of those on the righthand
side of (8.7) are called the deviance residuals. It should be noted that care
must be taken to account for the fact that the form (and implication) of these
tests can be different when using different statistical software when ni > 1,
as they depend on how the software defines the observations i (whether
responses are defined based on the binomial responses with ni replications for
each observation, as is done here, or based on the underlying Bernoulli 0/1
outcomes). See Simonoff (1998a) for fuller discussion of this point.
Unfortunately, these tests are not trustworthy when the ni values are
small, and are completely useless in the situation of Bernoulli response data
[ni = 1, so the response for each observation is simply success (y = 1) or
failure (y = 0)]. This is the justification for a third goodness-of-fit test, the
Hosmer-Lemeshow test.
This test is constructed based on a Pearson goodness-of-fit test, but one
where observations are grouped together in a data-dependent way to form
rough replications. First, all of the observations are ordered by their estimated
success probabilities ˆπ. The observations are then divided into g roughly
equisized groups, with g usually taken to be 10 except when that would lead
to too few observations in each group. Treating this new categorization of
the data as if it was the original form of the data, with the g groups defining
g observations with replications, implies observed and expected numbers of

8.3 Methodology
157
successes for each group. The Hosmer-Lemeshow test uses these by calculating
the usual X2 test based on the new categorization, which is compared to a χ2
distribution on g −2 degrees of freedom. It should be noted, however, that
even the Hosmer-Lemeshow test is suspect when the expected numbers of
successes or failures in the constructed categorization are too small (less than
two or three, say). Other alternative statistics for the ni = 1 situation have also
been proposed.
8.3.4
MEASURES OF ASSOCIATION AND CLASSIFICATION
ACCURACY
While tests of hypotheses are useful to assess the strength of a logistic regression
relationship, they do not address the question of whether the relationship is
of practical importance, as (for example) R2 can for least squares regression.
Several measures of association have been proposed for this purpose, which are
closely related to each other. Start with a fitted logistic regression model with
resultant fitted probabilities of success for each of the observations. Consider
each of the pairs (i, j) of observations where one observation is a success (i)
and the other is a failure (j). Each of these has a corresponding pair (ˆπi, ˆπj).
A “good” model would have a higher estimated probability of success for
the observation that was actually a success than for the observation that was
actually a failure; that is, ˆπi > ˆπj. Such a pair is called concordant. If for a
given pair ˆπi < ˆπj, the pair is called discordant. A model that separates the
successes from the failures well would have a high proportion of concordant
pairs and low proportion of discordant ones. There are no formal cutoffs for
what constitutes a “good enough” performance here, but observed values can
be compared for different possible models to assess relative performance in
this sense.
Various statistics are based on the observed proportions of concordant
and discordant pairs. Somers’ D, for example, is the difference between the
proportions of concordant and discordant pairs. Somers’ D is equivalent to the
well-known area under the Receiver Operating Characteristic (ROC) curve
(AUR = D/2 + .5), and also the Wilcoxon-Mann-Whitney rank sum test
statistic for comparing the distributions of probability estimates of observations
that are successes to those that are failures [WMW = AUR × nSnF, where
nS (nF) is the number of successes (failures)]. Note that while each of these is
a measure of the quality of the probability rankings implied by the model (in
the sense of concordance), a good probability ranking need not necessarily be
well-calibrated. For example, if the estimated probability of success for each
observation was exactly one-half the true probability, the probability rankings
would be perfect (implying D = AUR = 1), but not well-calibrated, since the
estimates were far from the true probabilities.
In the medical diagnostic testing literature, the following rough guide for
interpretation of D has been suggested (Hosmer et al., 2013, p. 177, provides
similar guidelines). It is perhaps useful as a way to get a sense of what the value

158
CHAPTER 8 Logistic Regression
is implying, but should be recognized as being fairly arbitrary, and should not
be taken overly seriously.
Range of D
Rough interpretation
0.8 −1.0
Excellent separation
0.6 −0.8
Good separation
0.4 −0.6
Fair separation
0.2 −0.4
Poor separation
0.0 −0.2
Little to no separation
Logistic regression also can be used for prediction of group membership,
or classification. This is typically appropriate when operating at the 0/1
(Bernoulli) response level. After estimating β, (8.2) can be used to give an
estimate of the probability of success for that observation. A success/failure
prediction for an observation is obtained based on whether the estimated
probability is greater than or less than a cutoff value. This value is often
taken to be .5, although in some situations another choice of cutoff might be
preferable (based, for example, on the relative costs of misclassifying a success
as a failure and vice versa). If this process is applied to the original data that
was used to fit the logistic regression model, a classification table results. The
resultant table takes this form:
Predicted result
Success
Failure
Actual
Success
nSS
nSF
nS·
result
Failure
nF S
nF F
nF ·
n·S
n·F
n
The proportion of observations correctly classified is (nSS + nF F )/n, and the
question is then whether this is a large number or not. The answer to this
question is not straightforward, because the same data were used to both build
the model and evaluate its ability to do classifications (that is, we have used
the data twice). As a result the observed proportion correctly classified can be
expected to be biased upwards compared to the situation where the model is
applied to completely new data.
The best solution to this problem is the same as it is when evaluating the
predictive performance of least squares models — validate the model on new
data to see how well it classifies new observations. In the absence of new data,
two diagnostics have been suggested that can be helpful. A lower bound for
what could be considered reasonable performance is the base rate, which is

8.4 Example — Smoking and Mortality
159
the proportion of the sample that comes from the larger group (sometimes
termed Cmax in this context).
A more nuanced argument is as follows. If the logistic regression had no
power to make predictions, the actual result would be independent of the
predicted result. That is, for example,
P (Actual result a success and Predicted result a success)
= P (Actual result a success) × P (Predicted result a success).
The right side of this equation can be estimated using the marginal probabilities
from the classification table, yielding
P (Actual result a success and Predicted result a success) =
	nS·
n

 	n·S
n

.
A similar calculation for the failures gives
P (Actual result a failure and Predicted result a failure) =
	nF ·
n

 	n·F
n

.
The sum of these two numbers is an estimate of the expected proportion of
observations correctly classified if the actual and predicted memberships were
independent, so achieving this level of classification accuracy would not be
evidence of a useful ability to classify observations. Since a higher observed
correct classification proportion is expected because the data were used twice,
this number is typically inflated by 25% before being compared to the observed
correct classification proportion, resulting the so-called Cpro measure.
8.3.5
DIAGNOSTICS
Unusual observations can have as strong an effect on a fitted logistic regression
as in linear regression, and therefore need to be identified and explored. The
IRWLS representation of the logistic regression MLEs provides the mechanism
to construct approximate versions of diagnostics such as standardized (Pearson)
residuals, leverage values, and Cook’s distances, using the implied hat matrix
and observation variances from IRWLS as if the model is a WLS fit, as is
described in Section 6.3.3.
8.4
Example — Smoking and Mortality
In 1972–1974 a survey was taken in Whickham, a mixed urban and rural
district near Newcastle upon Tyne, United Kingdom (Appleton et al., 1996).
Twenty years later a follow-up study was conducted, and it was determined
if the interviewee was still alive. Among the information obtained originally
was whether a person was a smoker or not and their age, divided into seven
categories. The data can be summarized in the following table:

160
CHAPTER 8 Logistic Regression
Age group
Smoking status
Survived
At risk
18–24
Smoker
53
55
18–24
Nonsmoker
61
62
25–34
Smoker
121
124
25–34
Nonsmoker
152
157
35–44
Smoker
95
109
35–44
Nonsmoker
114
121
45–54
Smoker
103
130
45–54
Nonsmoker
66
78
55–64
Smoker
64
115
55–64
Nonsmoker
81
121
65–74
Smoker
7
36
65–74
Nonsmoker
28
129
75 and older
Smoker
0
13
75 and older
Nonsmoker
0
64
As always a good first step in the analysis is to look at the data. In this case
there are two predictors, age and smoking status. A simple cross-classification
shows that twenty years later 76.1% of the 582 smokers were still alive, while
only 68.6% of 732 nonsmokers were still alive. That is, smokers had a higher
survival rate than nonsmokers, a pattern that seems puzzling at first glance.
Figure 8.3 gives more reasonable representations of the data. Since there
are multiple interviewees in each of the age categories for both smokers and
nonsmokers, the observed proportions of people who were alive 20 years later
are reasonable estimates of the true underlying probabilities. The plots treat
20
30
40
50
60
70
80
0.0
0.4
0.8
(a)
Age
Proportion survived
Smoker
Nonsmoker
20
30
40
50
60
70
80
–1
0
1
2
3
4
(b)
Age
Empirical logit
Smoker
Nonsmoker
FIGURE 8.3: Plots for the Whickham smoking and mortality data. (a)
Empirical survival proportions, separated by age group and smoking status.
(b) Empirical logits, separated by age group and smoking status.

8.4 Example — Smoking and Mortality
161
age as a numerical variable taking on a value at the midpoint of the interval
and using the value 80 for the last age group. The left plot shows the observed
survival proportions πi versus age, while the right plot gives the empirical
logits log[πi/(1 −πi)]. Note that the empirical logit is not defined for the
“75 or older” age group since no interviewees from that group were alive
at followup. If a constant shift linear logistic regression model is reasonable,
we would expect to see two roughly parallel linear relationships in the right
plot, one for smokers and one for nonsmokers. This seems to be reasonable
for younger ages, but the empirical logits are too low for the 65–74 group
(and would be −∞for the 75 and older group), suggesting a possible violation
of linearity in the logit scale.
It is also apparent that for most age groups survival is lower for smokers
than nonsmokers, as would be expected. This reversal of direction from the
marginal relationship (higher survival rates for smokers than for nonsmokers)
to the conditional one (lower survival rates for smokers than for nonsmokers
given age) is familiar in any multiple regression situation (recall that it can
lead to the misconception that a multiple regression slope coefficient has the
“wrong sign” discussed in Section 1.3.1), and in the context of categorical data
is called Simpson’s paradox. The underlying reason, of course, is correlation
between predictors, which in this case corresponds to higher smoking rates
for younger interviewees than for older ones (49.7% among interviewees less
than 65 years old versus 20.2% for those at least 65 years old). As a result what
appears to be a positive effect of smoking (higher survival rates for smokers) is
actually the positive effect of being younger at the time of original interview.
Output for a fitted logistic regression is as follows:
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
7.687751
0.447646
17.174
<2e-16 ***
Age
-0.124957
0.007274 -17.178
<2e-16 ***
Smoker
-0.266053
0.168702
-1.577
0.115
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 641.496
on 13
degrees of freedom
Residual deviance:
32.572
on 11
degrees of freedom
AIC: 85.568
The overall regression is highly statistically significant (LR = 608.9 on
2 degrees of freedom, p ≈0, obtained as the difference between the null and
residual deviances). The Wald test for age is also highly statistically significant,
but that for smoking status is only marginal. Exponentiating the slopes gives
odds ratios of 0.883 and 0.766, respectively, implying that (given smoking
status) being an additional year older is associated with an estimated 11.7%
smaller odds of being alive 20 years later, and given age being a smoker
is associated with an estimated 23.4% lower odds of survival (the constant
odds ratio for smoking for all ages reflects the constant shift nature of the
fitted model).

162
CHAPTER 8 Logistic Regression
Table 8.1: Details of models fit to the Whickham smoking study
data.
Model
LR
G2 (p-value)
X2 (p-value)
AIC
Linear
608.9
32.6 (< .001)
29.3 (.002)
85.6
Quadratic
629.9
11.6 (.311)
9.6 (.474)
66.6
Quadratic versus Linear: LR = 20.9, df = 1, p < .001
Categorical
639.1
2.4 (.882)
2.4 (.882)
65.4
Categorical versus Linear: LR = 30.2, df = 5, p < .001
Categorical versus Quadratic: LR = 9.2, df = 4, p = .055
Unfortunately, goodness-of-fit tests imply that this model does not fit
very well, as G2 = 32.6 (p < .001) and X2 = 29.3 (p = .002). Figure 8.3(b)
provides a clue as to why, since (as was noted earlier) the relationship between
age and the empirical logits is apparently not linear. The figure suggests
(at least) two possible ways of enriching the model while still maintaining
a constant shift for smoking status: including a quadratic function of age,
or treating age as a categorical variable (since it is actually only given as
membership in one of seven categories). Table 8.1 summarizes these fits, and
how they compare to the original (constant shift) linear model.
The quadratic and categorical models are both clear improvements over
the linear model, with much lower AIC values and highly statistically
significant LR tests comparing them (the linear model is a special case of the
quadratic model, and both are special cases of the categorical model). The
choice between the quadratic and categorical models is less obvious; while the
categorical model has lower AIC, the difference is small, and the hypothesis
that the quadratic model is adequate compared to the categorical model is
only weakly rejected (p = .055). Fortunately, from the point of view of the
relationship between smoking and mortality, the choice is moot; in either case,
the estimated slope for the smoking indicator is roughly −0.43, implying 35%
lower odds of survival twenty years later given age.
Output for the model treating age as a categorical variable is given below.
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
3.8601
0.5939
6.500 8.05e-11 ***
Age group = 29.5
-0.1201
0.6865
-0.175 0.861178
Age group = 39.5
-1.3411
0.6286
-2.134 0.032874 *
Age group = 49.5
-2.1134
0.6121
-3.453 0.000555 ***
Age group = 59.5
-3.1808
0.6006
-5.296 1.18e-07 ***
Age group = 69.5
-5.0880
0.6195
-8.213
< 2e-16 ***
Age group = 80
-27.8073 11293.1437
-0.002 0.998035
Smoker
-0.4274
0.1770
-2.414 0.015762 *
---

8.5 Example — Modeling Bankruptcy
163
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 641.4963
on 13
degrees of freedom
Residual deviance:
2.3809
on
6
degrees of freedom
AIC: 65.377
The age group variable is fit using six indicator variables, taking the youngest
category (18-24 years old) as the reference group. A striking result in the output
are the strange entries for the Age group = 80 variable; the estimated slope
is extremely large and negative, but the standard error is so large that the
p-value for the Wald test of whether the slope equals 0 is virtually 1. The
reason for this is that none of the interviewees in the 75 and older group
were alive at the time of follow-up, so the model is trying to estimate the
probability of survival of that group as 0. Based on (8.2) that can only occur
for a slope of −∞, and for that reason the software’s iterative algorithm pushes
the estimated slope to be as negative as possible. A similar pattern would occur
if everyone in an age group had survived, only then the coefficient would be
extremely large and positive rather than negative.
8.5
Example — Modeling Bankruptcy
As was stated at the beginning of this chapter, the study of bankruptcy of
companies has direct parallels to the study of survival of people. The data
discussed here were presented in Section 9.2 of Simonoff (2003), and are
based on a retrospective sample of 25 telecommunications firms that declared
bankruptcy between May 2000 and January 2002 that had issued financial
statements for at least two years, and information from the December 2000
financial statements of 25 telecommunications that did not declare bankruptcy.
Five financial ratios (each expressed as a percentage) were chosen as potential
predictors of bankruptcy, details of which can be found in Simonoff (2003):
1. Working capital as a percentage of total assets (WC/TA), a measure of
liquidity.
2. Retained earnings as a percentage of total assets (RE/TA), a measure of
cumulative profitability over time.
3. Earnings before interest and taxes as a percentage of total assets
(EBIT/TA), a measure of the productivity of a firm’s assets.
4. Sales as a percentage of total assets (S/TA), a measure of the ability of a
firm’s assets to generate sales.
5. Book value of equity divided by book value of total liabilities (BVE/BVL),
a measure of financial leverage (that is, debt).
In contrast to the Whickham smoking study data, these data take the form
of 50 observations, each with a 0/1 response variable, so it is not possible to
look at the data by plotting empirical proportions (or logits) versus predictors.

164
CHAPTER 8 Logistic Regression
0
1
−100
0
50
Bankrupt
Working Capital / Total Assets
0
1
−400 −200
0
Bankrupt
Retained Earnings / Total Assets
0
1
−100 −60 −20
20
Bankrupt
Earnings / Total Assets
0
1
0
40
80
120
Bankrupt
Sales / Total Assets
0
1
0
10
20
30
Bankrupt
Equity / Liabilities
FIGURE 8.4: Side-by-side boxplots for the bankruptcy data.
A good alternative is to construct side-by-side boxplots for each predictor,
separating on the response category. This doesn’t necessarily imply that a
linear logistic model is appropriate, but is still helpful in seeing if a predictor
discriminates between successes and failures.
Figure 8.4 gives these side-by-side boxplots. Working capital, retained
earnings, and earnings before interest and taxes each show strong separation
between bankrupt and nonbankrupt firms, in the ways that would have been
expected. Note that while (as always) there is no assumption regarding the
distributions of the predictors, the long right tail in the equity variable suggests
that logging this variable (using natural logs) might be helpful. In fact, there
were no appreciable differences in the implications of the analysis when either
logged or unlogged versions are used, so the unlogged variable is used here.
As a first step, a logistic regression model can be fit based on all of the
predictors. This results in the following output:
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
7.42646
6.35770
1.168
0.243
WC.TA
-0.15587
0.12208
-1.277
0.202
RE.TA
-0.07605
0.06311
-1.205
0.228

8.5 Example — Modeling Bankruptcy
165
EBIT.TA
-0.49111
0.32260
-1.522
0.128
S.TA
-0.08040
0.09216
-0.872
0.383
BVE.BVL
-2.07764
1.47488
-1.409
0.159
Null deviance: 69.315
on 49
degrees of freedom
Residual deviance: 11.847
on 44
degrees of freedom
AIC: 23.847
While the overall regression is highly statistically significant (LR = 57.5 on
5 degrees of freedom, p < .001), all of the individual Wald tests have moderately
high p-values. This suggests the possibility of simplifying the model, but an
index plot of the standardized residuals (Figure 8.5) shows that the first
observation is a clear outlier (this observation shows up as an outlier in any
reasonable simplified model as well). This is the firm 360Networks. It was one
of only two firms that ultimately went bankrupt that had positive earnings the
year before insolvency, and had $6.3 billion in total assets three months before
it declared bankruptcy because of thousands of miles of cable infrastructure
that it owned.
0
10
20
30
40
50
−2
−1
0
1
2
3
4
Index
Standardized residuals
FIGURE 8.5: Index plot of standardized (Pearson) residuals for the logistic
regression fit to the bankruptcy data.

166
CHAPTER 8 Logistic Regression
A logistic regression model based on all of the predictors after omitting
this observation results in the following output:
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
210.732
48078.738
0.004
0.997
WC.TA
-3.415
1032.286
-0.003
0.997
RE.TA
-1.206
424.416
-0.003
0.998
EBIT.TA
-13.553
2960.215
-0.005
0.996
S.TA
-2.265
618.155
-0.004
0.997
BVE.BVL
-61.575
15401.227
-0.004
0.997
Null deviance: 6.7908e+01
on 48
degrees of freedom
Residual deviance: 5.2176e-08
on 43
degrees of freedom
AIC: 12
This model fits the data perfectly, as the deviance is 0. This is termed
complete separation (since the predictors completely separate the bankrupt
firms from the nonbankrupt ones), and results in all of the estimated standard
errors of the estimated slopes being extremely large (and the Wald statistics
being correspondingly deflated). Recall that a similar problem arose in the
Whickham smoking survey data (page 162) for the same reason; the only way
for a logistic regression to estimate a probability as 0 or 1 is to send the slope
coefficient to ±∞.
A viable solution to this problem is to find a simpler model, if possible,
that fits almost as well, but where the MLEs are finite. Table 8.2 summarizes
the best models for each number of predictors p. All of the models are highly
statistically significant, with LR statistics at least 43.7, so these are not reported
in the table. All of the models fit well according to the Hosmer-Lemeshow
test. The best model according to AIC and AICc is the four-predictor model
that omits S/TA, which has a perfect fit. The three-predictor model based
on RE/TA, EBIT/TA, and BVE/BVL, however, fits almost perfectly (with
Hosmer-Lemeshow statistic H-L = 0.6 and Somers’ D = 0.99), and has finite
Table 8.2: Details of models fit to the bankruptcy data with 360Networks
omitted.
p
WC/
TA
RE/
TA
EBIT/
TA
S/
TA
BVE/
BVL
H −L
(p-value)
D
AIC
AICc
0
0.00
69.9
70.2
1
X
5.5 (.707)
0.93
28.3
28.8
2
X
X
4.8 (.778)
0.98
20.9
21.8
3
X
X
X
0.6 (> .999)
0.99
17.4
18.8
4
X
X
X
X
0.0 (1.000)
1.00
10.0
12.0
5
X
X
X
X
X
0.0 (1.000)
1.00
12.0
14.7
H-L refers to the Hosmer-Lemeshow goodness-of-fit test, and D refers to Somers’ D.

8.5 Example — Modeling Bankruptcy
167
(and hence interpretable) estimated slope coefficients. Note that neither the
Pearson goodness-of-fit test X2 or the deviance G2 should be examined for
these data, since the observations have no replications (ni = 1 for all i).
Output for the three-predictor model is as follows.
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept) -0.09166
1.47135
-0.062
0.9503
RE.TA
-0.08229
0.04230
-1.945
0.0517 .
EBIT.TA
-0.26783
0.15854
-1.689
0.0912 .
BVE.BVL
-1.21810
0.76536
-1.592
0.1115
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 67.9080
on 48
degrees of freedom
Residual deviance:
9.3841
on 45
degrees of freedom
AIC: 17.384
The Wald statistics are at first glance surprisingly small for a model with such
strong fit, but this is actually not uncommon. The estimated standard error
used in the denominator of the statistic is known to become too large when
an alternative hypothesis far from the null is actually true because of the use
of parameter estimates rather than null values (Mantel, 1987), which deflates
the statistic. The likelihood ratio test for the significance of each variable
does not suffer from this difficulty, and each of these (not reported here) is
highly statistically significant. All three coefficients are negative, as would be
expected. Exponentiating the slopes gives odds ratios, given the other variables
are held fixed; for example, e−.082 = .92, implying that a one percentage point
increase in RE/TA is associated with an estimated 8% decrease in the odds
of a firm going bankrupt, given EBIT/TA and BVE/BVL are held fixed.
Regression diagnostics do not indicate any outliers, although there are two
leverage points, IDT Corporation and eGlobe, the former of which also has
a relatively large Cook’s distance (see Figure 8.6). Given the strength of the
regression, it is not surprising that omitting these observations does not change
the implications of the modeling in a fundamental way.
A classification table for this fitting is given below. Forty-seven of 50, or
94%, of the firms were correctly classified, far higher than
Cpro = (1.25)[(.5)(.48) + (.5)(.52)] = 62.5%
and Cmax = 50%, reinforcing the strength of the logistic regression (note that
the outlier is recognized as misclassified here).
Predicted result
Bankrupt
Not bankrupt
Actual
Bankrupt
23
2
25
result
Not bankrupt
1
24
25
24
26

168
CHAPTER 8 Logistic Regression
0.0
0.2
0.4
0.6
0.8
1.0
−2
−1
0
1
2
Estimated probability
Standardized residuals
10
20
30
40
50
−2
−1
0
1
2
Index
Standardized residuals
10
20
30
40
50
0.0
0.2
0.4
0.6
0.8
Index
Leverage
10
20
30
40
50
0.0
0.2
0.4
0.6
0.8
Index
Cook's D
FIGURE 8.6: Diagnostic plots for the three-predictor model fit to the
bankruptcy data with 360Networks omitted.
Since this is a retrospective study, the estimated probabilities of bankruptcy
are not appropriate for prospective modeling, but prospective probabilities
can be obtained by adjusting the constant term of the regression using prior
probabilities of bankruptcy. Say a 10% bankruptcy prior probability is used,
which is roughly consistent with what would be expected for firms with a
corporate bond rating of B. This then yields the adjusted intercept
˜β0 = ˆβ0 + log
(.10)(25)
(.90)(24)

= ˆβ0 −2.1564 = −2.2481,
which would be used in (8.2).
8.6
Summary
In this chapter, we have discussed the application of logistic regression to
the modeling of binary response data. Hosmer et al. (2013) provides a much
more detailed discussion of logistic regression, and the discussion in Chapter 9
of Simonoff (2003) ties this material more closely to the broader analysis of
categorical data.

8.6 Summary
169
In recent years, increased computing power has allowed for the possibility
of replacing the standard asymptotic inference tools with exact methods.
Such tests and estimates are based on a conditional analysis, where inference
proceeds conditionally on the sufficient statistics for parameters not of direct
interest. Rather than appealing to approximate normal and χ2 distributions,
permutation distributions from the conditional likelihood are used, which
can require a good deal of computing power and special software. Simonoff
(2003) provides further discussion of these methods for categorical data
inference.
Logistic regression is of course based on the assumption that the response yi
follows a binomial distribution. This is potentially problematic for data where
there are replications for the ith observation (that is, ni > 1). In that case
the binomial assumption can be violated if there is correlation among the ni
individual responses, or if there is heterogeneity in the success probabilities
that hasn’t been modeled. Each of these violations can lead to overdispersion,
where the variability of the probability estimates is larger than would be
implied by a binomial random variable. In this situation methods that correct
for the overdispersion should be used, either implicitly (such as through
what is known as quasi-likelihood) or explicitly (fitting a regression model
using a response distribution that incorporates overdispersion, such as the
beta-binomial distribution). Simonoff (2003) provides further discussion, and
the corresponding problem for count data regression will be discussed in more
detail in Section 10.4.
Logistic regression for binomial data and (generalized) least squares
regression for Gaussian data are both special cases of the family of generalized
linear models. Such models also have direct applicability in the regression
analysis of count data, and will be discussed further in Chapter 10.
KEY TERMS
Area under the ROC curve: A measure of the ability of a model to separate
successes from failures. It is (literally) the area under the ROC curve, and
ranges from 0.5) (no separation) to 1.0 (perfect separation). It is equivalent to
Somers’ D through the relationship AUR = D/2 + .5.
Case-control design: A study in which sampling of units is based on the
response categories that are being studied. This is also referred to as a
retrospective sampling scheme.
Cohort design: A study in which sampling of units is not based on the
outcome responses, but rather either randomly from the population or
perhaps based on predictor(s). Such studies sometimes follow units over long
periods of time, observing outcome(s) as they manifest themselves. This is also
referred to as a prospective sampling scheme.
Complete separation: The condition when a classification procedure cor-
rectly identifies all of the observations as successes or failures. This can indicate
a model that is overspecified, since a simpler model that correctly classifies the

170
CHAPTER 8 Logistic Regression
vast majority (but not all) of the observations is often a better representation
of the underlying process.
Deviance: A measure of the fit of a model fitted by the maximum likelihood
method. The deviance equals twice the difference between the log-likelihood
for the saturated model (the model with maximum log-likelihood) and
that of the fitted model. Under certain conditions it has an asymptotic χ2
distribution with appropriate degrees of freedom and can be used as a measure
of goodness-of-fit, but is not applicable for this purpose when the number of
replications per observation is small.
Expit function: The inverse of the logit function giving the relationship
between the probability π and the logit ℓ, π = exp(ℓ)/[exp(ℓ) + 1].
Goodness-of-fit statistic: A hypothesis test of whether an observed model
provides an adequate fit to the data.
Hosmer-Lemeshow goodness-of-fit statistic: A goodness-of-fit test statistic
designed for the situation when the number of replications for (some)
observations is small.
Likelihood ratio test: A test in which a ratio of two likelihoods (or more
accurately the difference between two log-likelihoods multiplied by −2) is used
to judge the validity of a statistical hypothesis. Such tests can be used to test a
wide range of statistical hypotheses, often analogous to those based on F-tests
in least squares regression. Under certain conditions it has an asymptotic χ2
distribution with appropriate degrees of freedom.
Logistic regression: A regression model defining a (typically linear) relation-
ship between the logit of the response and a set of predictors.
Logit function: The inverse of the expit function giving the relationship
between the logit ℓand the probability π, ℓ= log[π/(1 −π)] (that is, the log
of the odds).
Maximum likelihood estimation: A method of estimating the parameters
of a model in which the estimated parameters maximize the likelihood for
the data. Least squares estimation is a special case for Gaussian data. Maxi-
mum likelihood estimates (MLEs) are known to possess various asymptotic
optimality properties.
Odds ratio: The multiplicative change in the odds of an event happening
associated with a one unit change in the value of a predictor.
Pearson goodness-of-fit statistic: A goodness-of-fit test statistic based on
comparing the model-based variability in the data to its inherent variability.
Under certain conditions it has an asymptotic χ2 distribution with appropriate
degrees of freedom, but is not applicable for this purpose when the number of
replications per observation is small.
Prior probabilities: The true (unconditional) probabilities of success and
failure. These are required in order to convert estimated probabilities of success
from a retrospective study into estimated prospective probabilities.

8.6 Summary
171
Probit regression: A regression model for binary data that uses the cumulative
distribution function of a normally distributed random variable to generate
S-shaped curves, rather than the expit function used in logistic regression.
Prospective design: A study in which sampling of units is not based on
the outcome responses, but rather either randomly from the population or
perhaps based on predictor(s). Such studies sometimes follow units over long
periods of time, observing outcome(s) as they manifest themselves. This is also
referred to as a cohort design.
Retrospective design: A study in which sampling of units is based on the
response categories that are being studied. This is also referred to as a case-
control design.
ROC (Receiver Operating Characteristic) curve: A plot of the fraction
of observations classified as successes by a model that are actually successes
(the true positive rate) versus the fraction of observations classified as failures
that are actually failures (the true negative rate), at various classification
cutoff values.
Simpson’s paradox: A situation where the marginal direction of association
of a variable with a binary response variable is in the opposite direction to the
conditional association taking into account membership in groups defined by
another variable.
Somers’ D: A measure of the ability of a model to separate successes from
failures. It is the difference between the proportion of concordant pairs and
the proportion of discordant pairs calculated from pairs of estimated success
probabilities where one member of the pair is a success and the other is a failure,
and a concordant (discordant) pair is one where the estimated probability of
success for the actual success is larger (smaller) than that for the actual failure.
D ranges from 0 (no separation) to 1 (perfect separation), and is equivalent to
the area under the ROC curve through the relationship D = 2 × AUR −1.
Wald test statistic: A test statistic for the significance of a regression coefficient
based on the ratio of the estimated coefficient to its standard error. For large
samples the statistic can be treated as a normal deviate.

Nine
Chapter
Multinomial Regression
9.1
Introduction
173
9.2
Concepts and Background Material
174
9.2.1
Nominal Response Variable
174
9.2.2
Ordinal Response Variable
176
9.3
Methodology
178
9.3.1
Estimation
178
9.3.2
Inference, Model Comparisons, and Strength
of Fit
178
9.3.3
Lack of Fit and Violations of Assumptions
180
9.4
Example — City Bond Ratings
180
9.5
Summary
184
9.1
Introduction
The formulation of logistic regression in Chapter 8 is appropriate for binary
response data, but there are situations where it is of interest to model a
categorical response that has more than two categories; that is, one that
is polytomous. For example, in a clinical trial context for a new drug,
the responses might be “No side effects,” “Headache,” “Back pain,” and
“Dizziness,” and the purpose of the study is to see what factors are related
to the chances of an individual experiencing one of the possible side effects.
Another common situation is the modeling of so-called Likert-type scale
variable, where (for example) a respondent is asked a question that has response
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
173

174
CHAPTER 9 Multinomial Regression
categories “Strongly disagree” – “Disagree” – “Neutral” – “Agree” – “Strongly
agree,” and the purpose of the study is to see if it is possible to model a
person’s response as a function of age, gender, political beliefs, and so on.
A key distinction in regression modeling of variables of this type compared
to the models discussed in the previous chapters is that in this case the response
is explicitly multivariate. If the response variable has K categories, the goal
is to model the probability vector π = {π1, . . . , πK} as a function of the
predictors, so there are K −1 numbers being modeled for each observation
(the probabilities must sum to 1, so the Kth probability is determined by the
others). In this chapter, we examine models that generalize logistic regression
(which has K = 2) to the multiple-category situation. This generalization is
accomplished in two distinct ways, depending on whether or not there is a
natural ordering to the categories.
9.2
Concepts and Background Material
Consider the ith observed response value yi of a K-level categorical variable.
The value yik is the number of replications of the ith observation that fall in
the kth category in ni replications. Just as the binomial random variable was
the key underlying distribution for binary logistic regression, in this situation,
the multinomial distribution is key. This is represented by yi ∼Mult(ni, πi).
The ni = 1 case is naturally thought of as the situation where the observation
has a single response corresponding to one of the categories (for example,
“Strongly agree”), while ni > 1 refers to the situation where there are replica-
tions for a given set of predictor values and the observation is a vector of length
K of counts for each category. Note, however, that in fact yi is a vector in both
situations, with one entry equal to 1 and all of the others equal to 0 when ni = 1.
As was noted earlier for binary response data, there are two aspects of
a regression model for a categorical response that need to be addressed: the
underlying probability distribution for the response, and the model for the
relationship between the probability of the response vector and the predictors.
The multinomial provides the distribution, but the usefulness of models for
the relationship with predictors depends on whether or not there is a natural
ordering to the response categories, as is described in the next two sections.
9.2.1
NOMINAL RESPONSE VARIABLE
Consider first the situation where the response variable is nominal; that is,
there is no natural ordering to the categories. The logistic regression model in
this case is the natural generalization from the two-category situation to one
based on K −1 different logistic relationships. Let the last (Kth) category
represent a baseline category; the model then states that the probability of
falling into group k given the set of predictor values x satisfies
log
 πk(x)
πK(x)

= β0k + β1kx1 + · · · + βpkxp,
k = 1, . . . , K −1. (9.1)

9.2 Concepts and Background Material
175
The model is based on K −1 separate equations, each having a distinct set of
parameters βk. Obviously, for the baseline category K, β0K = β1K = · · · =
βpK = 0.
The logit form of (9.1) implies that (as before) exponentiating the slope
coefficients gives odds ratios, now relative to the baseline category. That is,
eβjk is the multiplicative change in the odds of being in group k versus being
in group K associated with a one unit increase in xj holding all else in the
model fixed. In a situation where there is a naturally defined reference group,
it should be chosen as the baseline category, since then the slopes are directly
interpretable with respect to that reference. So, for example, in the clinical
trial context mentioned earlier, the reference group (and therefore baseline
category) would be “No side effects,” and the slopes would be interpreted in
terms of the odds of having a particular side effect versus having no side effects.
Despite this interpretation of odds ratios relative to the baseline category,
the choice of baseline is in fact completely arbitrary from the point of view
of the implications of the model. Say category M is instead taken to be the
baseline category. From (9.1),
log
 πk(x)
πM(x)

= log
 πk(x)/πK(x)
πM(x)/πK(x)

= log
 πk(x)
πK(x)

−log
πM(x)
πK(x)

= (β0k + β1kx1 + · · · + βpkxp)
−(β0M + β1Mx1 + · · · + βpMxp)
= (β0k −β0M) + (β1k −β1M)x1 + · · · + (βpk −βpM)xp.
(9.2)
That is, the logit coefficients for level k relative to a baseline category M (both
intercept and slopes) are the differences between the coefficients for level k
relative to baseline K and the coefficients for level M relative to baseline K.
Note that there is nothing in this derivation that requires category M to be a
baseline category; equation (9.2) applies for any pair of categories.
Note that (9.2) also implies that no matter which category is chosen as base-
line, the probabilities of falling in each level as a function of the predictors will
not change. Model (9.1) implies the familiar S-shape for a logistic relationship,
πk(x) =
exp(β0k + β1kx1 + · · · + βpkxp)
K
m=1 exp(β0m + β1mx1 + · · · + βpmxp)
.
(9.3)
Equation (9.2) shows that for this model, the (log-)odds of one category of
the response versus another does not depend on any of the other categories; that
is, other possible outcomes are not relevant. This is known as independence
of irrelevant alternatives (IIA). This is often reasonable, but in some circum-
stances might not be. Consider, for example, a discrete choice situation, where
an individual must choose between, for example, different travel options.
A well-known discrete choice example, the so-called “red bus / blue bus”
example, illustrates the problem. Suppose a traveler must choose between three
modes of transportation: red bus, car, or train. Further, say that the traveler had

176
CHAPTER 9 Multinomial Regression
individual characteristics (income, age, gender, etc.) such that he or she has no
preference between the three choices, implying that the traveler’s probability of
choosing each is 1/3. Now, say an indistinguishable alternative to the red bus,
a blue bus, becomes available. It seems reasonable that the two types of buses
would be of equal appeal to the traveler, resulting in probability 1/6 of being
chosen for each, while the probabilities of the other two possible choices would
not change. This, however, is a violation of IIA, since adding the supposedly
irrelevant alternative of a blue bus has changed the odds of a traveler choosing
the red bus versus a car or a train. For this reason, the nominal logistic regression
model should only be used in situations where this sort of effect is unlikely to
occur, such as when the different response categories are distinct and dissimilar.
9.2.2
ORDINAL RESPONSE VARIABLE
In many situations, such as the Likert-scaled variable described earlier, there is
a natural ordering to the groups, and a reasonable model should take that into
account. An obvious approach is to just use ordinary least squares regression,
with the group membership numerical identifier as the target variable. This
will not necessarily lead to very misleading impressions about the relationships
between predictors and the response variable, but there are several obvious
problems with its use in general:
1. An integral target variable clearly is inconsistent with continuous (and
hence Gaussian) errors, violating one of the assumptions when construct-
ing hypothesis tests and confidence intervals.
2. Predictions from an ordinary regression model are of course nonintegral,
resulting in difficulties in interpretation: what does a prediction to category
2.763 mean, exactly?
3. The least squares regression model doesn’t address the actual underlying
structure of the data, as it ignores the underlying probabilities π com-
pletely. A reasonable regression model should provide direct estimates of
π(x).
4. The regression model implicitly assumes that the numerical codings (for
example, 1–2–3–4–5) reflect the actual “distances” of groups from each
other, in the sense that group 1 is as far from group 2 as group 2 is from
group 3. It could easily be argued that this is not sensible for a given
data set. For example, it is reasonable to consider the possibility that a
person who strongly agrees with a statement (i.e. group 5) has a somewhat
extreme position, and is therefore “farther away” from a person who agrees
with the statement (group 4) than a person who agrees with the statement
is from a person who is neutral (group 3). A reasonable regression model
should be flexible enough to allow for this possibility.
A way of constructing such a model is through the use of a latent variable.
The idea is represented in Figure 9.1 in the situation with one predictor, but
it generalizes to multiple predictors in the obvious way. Consider the situation
where the observed response is a Likert-scaled variable with K categories. The

9.2 Concepts and Background Material
177
x
y*
x1
x2
x3
α1 α2
α3
α4
FIGURE 9.1: Graphical representation of the latent variable model for regres-
sion with an ordinal response variable.
latent variable y∗is an underlying continuous response variable representing
the “true” agreement of the respondent with the statement. This variable is not
observed, because the respondent is restricted to choosing one of K response
categories. To account for this, we assume that there is a grid {α0, . . . , αK},
with −∞= α0 < α1 < · · · < αK = ∞, such that the observed response y
satisfies
y = k if
αk−1 < y∗≤αk.
That is, we observe a response in category k when the underlying y∗falls in
the kth interval of values.
Note that Figure 9.1 also highlights the linear association between the
latent variable y∗and the predictor x. The plotted density curves represent
the probability distributions of y∗for specified values of x (x1, x2, and x3,
respectively). These curves are centered at different values of y∗because of that
linear association. It is this association that ultimately drives the relationship
between the probabilities of falling into each of the response categories of y
(the observed categorical response) and the predictor, as πk(x) is simply the
area under the density curve between αk−1 and αk for given x.
It turns out that if the assumed density of y∗given x follows a logistic
distribution, this representation implies a simple model that can be expressed
in terms of logits. (The logistic density is similar to the Gaussian, being
symmetric but slightly longer-tailed; its cumulative distribution function
generates the expit curve.) Define the cumulative logit as
Lk(x) = logit[Fk(x)] = log

Fk(x)
1 −Fk(x)

,
k = 1, . . . , K −1,

178
CHAPTER 9 Multinomial Regression
where Fk(x) = P(y ≤k|x) = k
m=1 πm(x) is the cumulative probability for
response category k given x. That is, Lk(x) is the log odds given x of observing
a response less than or equal to k versus greater than k. The proportional
odds model then says that
Lk(x) = αk + βx,
k = 1, . . . , K −1.
Note that this means that a positive β is associated with increasing odds of
being less than a given value k, so a positive coefficient implies increasing
probability of being in lower-numbered categories with increasing x; for this
reason, some statistical packages reverse the signs of the slopes so that a
positive slope is consistent with higher values of the predictor being associated
with higher categories of the response. This generalizes in the obvious way to
multiple predictors,
Lk(x) = αk + β1x1 + · · · + βpxp,
k = 1, . . . , K −1.
(9.4)
The model is called the proportional odds model because it implies that
holding all else fixed a one-unit increase in xj is associated with multiplying
the odds of being less than or equal to category k versus being greater than
that category by eβj for any category k.
9.3
Methodology
9.3.1
ESTIMATION
Estimation of the underlying parameters for either of these classes of models
can be estimated using maximum likelihood. The log-likelihood is
L =
K

k=1

yi=k
log πk(i),
where the second summation is over all observations i with response level k,
and πk(i) is the probability (9.3) for the nominal logistic regression model or
the probability implicitly implied by (9.4) for the proportional odds model,
respectively, substituting in the predictor values for the ith observation. The
resultant estimates of β are efficient for large samples. Note that although the
nominal logistic regression has the structure of K −1 separate binary logistic
regressions, the underlying relationships are fitted simultaneously.
9.3.2
INFERENCE, MODEL COMPARISONS, AND STRENGTH
OF FIT
Many of the inferential and descriptive tools described in Section 8.3.2 for
binary logistic regression also apply in the multiple-category case. Output
from the fitting of a nominal logistic regression looks very much like a set
of binary logistic regression outputs, since the model is defined as a series of

9.3 Methodology
179
logistic regressions with the same baseline category. That is, if for example the
baseline category is category K, the output will provide summaries of models
for logistic regressions of category 1 versus category K, category 2 versus
category K, and so on, resulting in (K −1)(p + 1) regression parameters. For
the logistic regression comparing the kth category to the baseline category,
Wald tests can be constructed to test the significance of an individual
regression coefficient, testing the hypotheses
H0 : βjk = 0
versus
Ha : βjk ̸= 0.
It is also possible to construct a test of the significance of a predictor xj in
all of the regressions, testing
H0 : βj1 = · · · = βjK = 0
versus the alternative that at least one of these coefficients is not zero. The
likelihood ratio statistic for testing these hypotheses is the difference between
−2L for the model that includes xj and −2L for the model that does not, and
can be compared to a χ2 critical value on K −1 degrees of freedom. A test for
whether all of the slope terms equal zero is a test of the overall significance of the
regression, and can be constructed as a likelihood ratio test comparing the fitted
model to one without any predictors. If one model is a subset of another model,
a test of the adequacy of the simpler model is constructed based on the differ-
ence between the two −2L values, referenced to a χ2 distribution with degrees
of freedom equal to the difference in the number of estimated parameters in
the two models. Different models also can be compared using AIC or AICc.
The proportional odds model has a more straightforward representation,
being based on a set of K −1 intercepts and a single set of p slope parameters.
Wald test statistics can be constructed for the significance of each of these
parameters, and a test for whether all of the slope terms equal zero is a
test of the overall significance of the regression. Clearly, the nominal logistic
regression model can be applied to ordinal response data as well and can
provide a check of the validity of the ordinal model, since it is a more
general model. For K > 2, the proportional odds model is simpler (has fewer
parameters) than the nominal logistic regression model, so if it is a reasonable
representation of the underlying relationship it would be preferred. The two
models can be compared using AIC or AICc, and also using a likelihood ratio
test. A generalization of (9.4) from constant slopes (βj) to different slopes for
different categories (βjk for the kth category) corresponds to an interaction
between the variable xj and the category; comparing the fits of the two models
using a likelihood ratio test is a test of the proportional odds assumption.
Measures of association such as Somers’ D (Section 8.3.4) can be con-
structed for ordinal response models, since in that case the concepts of
concordance and discordance are meaningful. A pair of observations from two
different categories are concordant if the observation with the lower-ordered

180
CHAPTER 9 Multinomial Regression
response value has a lower estimated mean score than the observation with the
higher-ordered response value, and D is the difference between the concordant
and discordant proportions.
If it was desired to classify observations, they would be assigned to the
category with largest estimated probability. A classification table is then formed
in the same way as when there are two groups (Section 8.3.4), and the two
benchmarks Cmax and Cpro are formed in an analogous way.
9.3.3
LACK OF FIT AND VIOLATIONS OF ASSUMPTIONS
The Pearson (8.6) and deviance (8.7) goodness-of-fit test statistics also gener-
alize to multiple-category regression models. The Pearson statistic is now
X2 =
n

i=1
K

k=1
(yik −niˆπik)2
niˆπik
,
while the deviance is
G2 = 2
n

i=1
K

k=1

yik log
 yik
niˆπik

.
Each of these statistics can be compared to a χ2 distribution on n(K −1) −
p −1 degrees of freedom, but only if the ni values are reasonably large (note
that as in Chapter 8, n is the number of replicated observations, not the total
number of replications). When the number of replications is small, or there
are no replications at all (ni = 1 for all i), the Pearson and deviance statistics
are not appropriate for testing goodness-of-fit, but Pigeon and Heyse (1999)
proposed a test that generalizes the Hosmer-Lemeshow test.
The multivariate nature of the response variable y results in some difficul-
ties in constructing diagnostics for the identification of unusual observations,
since there are K different residuals yik −niˆπik for each observation. In the
situation with replications, it is possible to see whether for any observation any
categories have unusually greater or fewer replications than would be expected
according to the fitted model. When there is only a single replication, it can
be noted that an observation has low estimated probability of falling in its
actual category, but for nominal data there is no notion of how “far away” its
actual category is from its predicted one. Unusualness for ordinal data is easier
to understand, as (for example) an individual with response “Strongly agree”
is clearly more unusual if they have a high estimated probability of saying
“Strongly disagree” than if they have a high estimated probability of saying
“Agree.”
9.4
Example — City Bond Ratings
Abzug et al. (2000) gathered data relating the bond rating of a city’s general
obligation bonds to various factors as part of a study of what types of

9.4 Example — City Bond Ratings
181
BBB
A
AA
AAA
13.0
14.0
15.0
16.0
Rating
Logged population
BBB
A
AA
AAA
50000
70000
Rating
Household income
BBB
A
AA
AAA
0
1
2
3
4
5
Rating
Nonprofits in top 10
BBB
A
AA
AAA
2
4
6
8
Rating
For profits in top 10
FIGURE 9.2: Side-by-side boxplots for the bond rating data.
organizations dominate employment in cities. We focus here on four potential
predictors of bond rating: logged population (using natural logs), average total
household income, the number of nonprofits among the top ten employers
in the city, and the number of for profits among the top ten employers in the
city, for 56 large cities that issued general obligation bonds. The bond rating
for each city falls into one of the classes AAA, AA, A, or BBB. These classes
are ordered from highest to lowest in terms of credit-worthiness.
Figure 9.2 gives side-by-side boxplots of each predictor separated by rating
class. There is not any obvious relationship between logged population and
rating or the number of for profit institutions among the top ten employers
in the city and rating, while lower average household income and a lower
number of nonprofits among the top ten employers are associated with higher
credit-worthiness.
The output below summarizes the result of a nominal logistic regression
fit based on all four predictors, with the lowest (BBB) rating class taken as the
baseline category.
Estimate
Std. Err
z
Pr(>|z|)
Logit 1: AAA versus BBB
(Intercept)
12.7268
15.025
0.85
0.397
Logged.population
-0.13988
1.3849
-0.10
0.920

182
CHAPTER 9 Multinomial Regression
Household.income
-0.00012
0.000166
-0.74
0.456
Nonprofits.in.top.10
-1.58691
0.74459
-2.13
0.033
For.profits.in.top.10
-0.15720
0.38536
-0.41
0.683
Logit 2: AA versus BBB
(Intercept)
27.6381
11.112
2.49
0.013
Logged.population
-1.6257
0.91953
-1.77
0.077
Household.income
0.00005
0.000107
0.47
0.637
Nonprofits.in.top.10
-1.5765
0.54870
-2.87
0.004
For.profits.in.top.10
-0.4239
0.31629
-1.34
0.180
Logit 3: A versus BBB
(Intercept)
7.5600
10.511
0.72
0.470
Logged.population
-0.2661
0.88151
-0.30
0.763
Household.income
-0.000003
0.000110
-0.02
0.981
Nonprofits.in.top.10
-0.4917
0.52392
-0.94
0.348
For.profits.in.top.10
-0.3239
0.31626
-1.02
0.306
Most of the Wald statistics are not statistically significant, with only the
number of nonprofits among the top ten employers in the city showing up
as statistically significant at a .05 level in the comparisons of the cities with
AAA ratings versus BBB ratings and those with AA ratings compared to those
with BBB ratings. The interpretation of the coefficients is the same as in
binary logistic regressions; for example, the slope coefficient for the number of
nonprofits in the AAA versus BBB model implies that holding all else fixed an
increase of one nonprofit among the top ten employers in a city is associated
with multiplying the odds of the city having an AAA rating versus having
a BBB rating by e−1.587 = 0.20, or an estimated 80% decrease. All of the
estimated coefficients for this predictor are negative, which is consistent with
the marginal relationship that more nonprofits is associated with less chance
of having a higher credit rating.
This model seems to be overspecified and should be simplified. Since
there are four predictors, there are 24 −1 = 15 different possible regression
models that include at least one predictor. Comparison of all of these models
via AICc (not shown) implies that only two models should be considered: the
model using the number of nonprofits among the top ten employers alone, or
the model that also includes logged population. While the simpler model has
smaller AICc, the model that adds logged population has smaller AIC, and
the likelihood ratio test comparing the two models weakly rejects the simpler
model (LR = 7.35, df = 3, p = .06). The output below summarizes the fit
based on the two-predictor model.
Estimate
Std. Err
z
Pr(>|z|)
Logit 1: AAA versus BBB
(Intercept)
17.8415
12.4327
1.44
0.151
Logged.population
-1.00619
0.818426
-1.23
0.219
Nonprofits.in.top.10
-1.70719
0.697162
-2.45
0.014
Logit 2: AA versus BBB
(Intercept)
23.2681
9.74556
2.39
0.017

9.4 Example — City Bond Ratings
183
Logged.population
-1.28134
0.624626
-2.05
0.040
Nonprofits.in.top.10
-1.44526
0.517454
-2.79
0.005
Logit 3: A versus BBB
(Intercept)
5.85505
9.42789
0.62
0.535
Logged.population
-0.269707
0.595171
-0.45
0.650
Nonprofits.in.top.10
-0.454743
0.496228
-0.92
0.359
The slopes for the number of nonprofits variable do not change greatly
from the model using all four predictors. The only Wald statistic that is
statistically significant for the logged population variable is for the comparison
of AA versus BBB rating classes, with a larger population associated with less
credit-worthiness. Note that since the predictor is in the natural log scale, the
slope is an elasticity for the odds; that is, holding the number of nonprofits
fixed, a 1% increase in population is associated with an estimated 1.28%
decrease in the odds of a city being rated AA versus BBB. The overall statistical
significance of the model is high, with LR = 20.6 (df = 6, p = .002).
This analysis, of course, does not take into account the natural ordering
of the rating classes. Here is output for a proportional odds fit to the data
based on all four of the predictors:
Coef
S.E.
Z
Pr(>|Z|)
alpha(1)
10.9797
5.2474
2.09
0.0364
alpha(2)
14.8816
5.4321
2.74
0.0062
alpha(3)
16.7583
5.5639
3.01
0.0026
Logged.population
-0.8075
0.4459 -1.81
0.0701
Household.income
0.0000
0.0000 -0.02
0.9815
Nonprofits.in.top.10
-1.0773
0.2798 -3.85
0.0001
For.profits.in.top.10
-0.0845
0.1557 -0.54
0.5874
Note that the software fitting this model reverses the signs, so (for
example) the negative coefficient for the number of nonprofits among the
top ten employers implies that more nonprofits are associated with a lower
credit rating (holding all else fixed). The only two variables close to statistical
significance are again logged population and the number of nonprofits among
the top ten employers. Comparison of all of the possible models via AICc
implies that the only model supported is that based on logged population and
the number of nonprofits among the top ten employers:
Coef
S.E.
Z
Pr(>|Z|)
alpha(1)
10.7301
4.8965
2.19
0.0284
alpha(2)
14.6202
5.0974
2.87
0.0041
alpha(3)
16.4908
5.2338
3.15
0.0016
Logged.population
-0.8205
0.3384 -2.42
0.0153
Nonprofits.in.top.10
-1.0670
0.2744 -3.89
0.0001
The overall regression is highly statistically significant (LR = 19.7, df = 2,
p < .0001). The implications of the model are similar to those of the nominal
model (higher population and more nonprofits are associated with lower credit
rating holding all else fixed), but this model is much more parsimonious, being
based on only 5 parameters rather than 9. This is reflected in the AICc values

184
CHAPTER 9 Multinomial Regression
(AICc for the ordinal model is 10.5 lower than that of the nominal model), and
also in the likelihood ratio test comparing the two models, which does not come
close to rejecting the simpler ordinal model (LR = 0.9, df = 4, p = 0.93).
Somers’ D = 0.52 for this model, suggesting moderate separation between
the rating classes. This is also reflected in the classification table (the corre-
sponding table for the nominal logistic regression model based on these two
variables is unsurprisingly very similar).
Predicted result
BBB
A
AA
AAA
BBB
2
2
2
0
6
Actual
A
0
6
6
0
12
result
AA
0
2
31
0
33
AAA
0
0
5
0
5
2
10
44
0
The model correctly classifies 69.6% of the cities, which is only mod-
erately greater than Cmax = 58.9% and Cpro = 63.1%. Two of the cities are
misclassified to two rating classes higher than they were (Buffalo and St. Louis
both had ratings of BBB but were classified to an AA rating, because of a
low number of nonprofits among the top ten employers [Buffalo] and a large
population [St. Louis], respectively). Even more troubling, none of the cities
are classified to the AAA group.
Omitting Buffalo and St. Louis from the data does not change the fitted
models very much. The proportional odds model based on logged population
and number of nonprofits among the top ten employers does now classify one
city to the AAA rating class, but it is an incorrect classification, so that is not a
point in its favor. An alternative approach could be based on Simonoff (1998b),
in which a smoothed version of the cross-classification of bond rating and
number of nonprofits is used to argue that the relationship between rating and
nonprofits is not monotone (both less and more nonprofits than one among
the top ten employers being associated with lower bond rating). This suggests
accounting for this pattern in the regression by fitting a quadratic relationship
with number of nonprofits, and if that is done the squared predictor is in fact
statistically significant. Classification accuracy does not improve based on this
model, however. Smoothing methods of this type are the subject of Chapter 15.
9.5
Summary
In this chapter we have generalized the application of logistic regression to
response data with more than two categories. Nominal logistic regression is

9.5 Summary
185
based on the principle of choosing a baseline category and then simultaneously
estimating separate (binary) logistic regressions for each of the other categories
versus that baseline. This is a flexible approach, but can involve a large number
of parameters if K is large.
The nominal logistic regression model requires the assumption of inde-
pendence of irrelevant alternatives, an assumption that can easily be violated in
discrete choice models. There is a large literature on tests for IIA and extensions
and generalizations of multiple category regression models that are appropriate
in the discrete choice framework. See Train (2009) for more details.
If the response variable has naturally ordered categories, it is appropriate
to explore models that take that into account, as they can often provide
parsimonious representations of the relationships in the data. These models
are generally underutilized in practice, as analysts tend to just use ordinary
(least squares) linear regression with the category number as the response value
to analyze these data. This is not necessarily a poor performer if the number
of categories is very large, but can be very poor for response variables with a
small number of categories. The proportional odds model is a standard first
approach, but it is not the only possibility. Other possible models include
ones based on cumulative probits, adjacent-categories logits, or continuation
ratios rather than cumulative logits. Chapter 10 of Simonoff (2003) provides
more extensive discussion of these models.
KEY TERMS
Cumulative logit: The logit based on the cumulative distribution function,
representing the log-odds of being at a specific level or lower versus being at a
higher level.
Independence of irrelevant alternatives (IIA): The property that the odds of
being in one category of a multiple-category response variable versus another
category of the variable depends only on the two categories, and not on any
other categories.
Latent variable: An underlying continuous response variable representing
the “true” agreement of a respondent with a statement, which is typically
unobservable. More generally, any such variable that is only observed indirectly
through a categorical variable that reports counts falling into (unknown)
intervals of the variable.
Likert-type scale variable: A variable typically used in surveys in which
a respondent reports his or her level of agreement or disagreement on an
ordered scale. The scale is typically thought of as reflecting an underlying
latent variable.
Multinomial random variable: A discrete random variable that takes on more
than two prespecified values. This is a generalization of the binomial variable,
which takes on only two values.
Nominal variable: A categorical variable where there is no natural ordering
of the categories.

186
CHAPTER 9 Multinomial Regression
Ordinal variable: A categorical variable where there is a natural ordering of
the categories.
Polytomous variable: A categorical response that has more than two
categories.
Proportional odds model: A regression model based on cumulative logits of
an ordinal response variable that hypothesizes an equal multiplicative effect
of a predictor on the odds of being less than or equal to category k versus
being greater than that category for any category k, holding all else fixed. It is
consistent with an underlying linear relationship between a latent variable and
the predictors if the associated error term follows a logistic distribution.

Ten
Chapter
Count Regression
10.1
Introduction
187
10.2
Concepts and Background Material
188
10.2.1 The Poisson Random Variable
188
10.2.2 Generalized Linear Models
189
10.3
Methodology
190
10.3.1 Estimation and Inference
190
10.3.2 Offsets
191
10.4
Overdispersion and Negative Binomial Regression
192
10.4.1 Quasi-likelihood
192
10.4.2 Negative Binomial Regression
193
10.5
Example — Unprovoked Shark Attacks in Florida
194
10.6
Other Count Regression Models
201
10.7
Poisson Regression and Weighted Least Squares
203
10.7.1 Example — International Grosses of Movies
(continued)
204
10.8
Summary
206
10.1
Introduction
The previous two chapters focused on situations where least squares estimation
is not appropriate because of the special nature of the response variable, with
Chapter 8 exploring binary logistic regression models for binomial response
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
187

188
CHAPTER 10 Count Regression
data and Chapter 9 outlining multiple-category logistic regression models for
multinomial response data. Another situation of this type is the focus of this
chapter: regression models when the response is a count, such as (for example)
attempting to model the expected number of homicides in a small town for
a given year or the expected number of lifetime sex partners reported by a
respondent in a social survey.
Just as was true for binary data, the use of least squares is inappropriate for
data of this type. A linear model implies the possibility of negative estimated
mean responses, but counts must be nonnegative. Counts can only take on
(nonnegative) integer values, which makes them inconsistent with Gaussian
errors. Further, it is often the case that count data exhibit heteroscedasticity,
with larger variance accompanying larger mean.
In this chapter, we examine regression models for count data. The
workhorse random variable in this context is the Poisson random variable, and
we first describe its properties. We then highlight the many parallels between
count regression models and binary regression models by showing how both
(and also Gaussian-based linear regression) are special cases of a broad class
of models called generalized linear models. This general formulation provides
a framework for inference in many regression situations, including count
regression models.
Although the Poisson random variable provides the basic random struc-
ture for count regression modeling, it is not flexible enough to handle all count
regression problems. For this reason, we also discuss various generalizations of
Poisson regression to account for more variability than expected (overdisper-
sion), and greater or fewer observed specific numbers of counts (often greater
or fewer zero counts than expected), which can be useful for some data sets.
10.2
Concepts and Background Material
10.2.1
THE POISSON RANDOM VARIABLE
The normal distribution is not an appropriate choice for count data for
the reasons noted earlier. The standard distribution for a count is a Poisson
distribution. The reason for this is that the Poisson is implied by a general model
for the occurrence of random events in time or space. In particular, if ν is the
rate at which events occur per unit time, under reasonable assumptions for the
occurrence of events in a small time interval, the total number of independent
events that occur in a period of time of length t has a Poisson distribution
with mean μ = νt. The Poisson distribution, represented as y ∼Pois(μ), is
completely determined by its mean μ, as its variance also equals μ.
The Poisson random variable is closed under summation, in the sense that
a sum of independent Poissons is itself Poisson with mean equal to the sum of
the underlying means. Since this means that a Poisson with mean μ is the sum
of μ independent Poissons with mean 1, the Central Limit Theorem implies
that as μ gets larger, a Poisson random variable is approximately normal.

10.2 Concepts and Background Material
189
The Poisson distribution also has important connections with two other
discrete data distributions. If the number of successes in n trials is binomially
distributed, with the number of trials n →∞and the probability of success
π →0 such that nπ →μ, the distribution of the number of successes is
approximately Poisson with mean μ. This means that the Poisson is a good
choice to model the number of rare events; that is, ones that are unlikely to
occur in any one situation (since π is small), but might occur in a situation
with many independent trials (that is, n is large).
The Poisson random variable also has a close connection to the multino-
mial. If K independent Poisson random variables {n1, . . . , nK} are observed,
each with mean μi, their joint distribution conditional on the total number of
counts 
jnj is multinomial with probability πi = μi/(
jμj). This connec-
tion turns out to be particularly important in the analysis of tables of counts
(contingency tables).
10.2.2
GENERALIZED LINEAR MODELS
As was noted in Section 8.2, a regression model must specify several things:
the distribution of the value of the response variable yi (the so-called random
component), the way that the predicting variables combine to relate to the
level of yi (the systematic component), and the connection between the random
and systematic components (the link function). The generalized linear model
(GLM) is a family of models that provides such a framework for a very wide
set of regression problems. Specifically, the random component requires that
the distribution of yi comes from the exponential family and the systematic
component specifies that the predictor variables relate to the level of y as
a linear combination of the predictor values (a linear predictor). The link
function then relates this linear predictor to the mean of y. Specifically, the
random component requires that the distribution of yi satisfies
f(y; θ, φ) = exp
yθ −b(θ)
a(φ)
+ c(y, φ)

,
for specified functions a(·), b(·), and c(·). The systematic component specifies
the linear predictor,
ηi = β0 + β1x1i + · · · + βpxpi,
and the link function then relates η to the mean of y, μ, being the function g
such that
g(μ) = η.
The log-likelihood for the entire sample is thus
L =
n

i=1
yiθi −b(θi)
ai(φ)
+ c(yi, φ)

,
(10.1)
and model fitting proceeds using maximum likelihood.

190
CHAPTER 10 Count Regression
The Gaussian linear model (1.1) is an example of a generalized linear
model, with identity link function and a normal distribution as the random
component. Logistic regression is also an example of a generalized linear
model, with the binomial distribution being the random component and the
logit function (8.1) defining the link function.
The Poisson regression model is also a member of the generalized
linear model family. Since the mean of a Poisson random variable must be
nonnegative, a natural link function in this case (and the one that is standard)
is the log link,
log μi ≡log[E(yi)] = β0 + β1x1i + · · · + βpxpi.
Thus, the Poisson regression model is an example of a log-linear model. By
convention, natural logs are used rather than common logs. Note that this is
a semilog model for the mean of y,
μi = eβ0+β1x1i+···+βpxpi,
(10.2)
so the slope coefficients have the usual interpretation as semielasticities, with
a one unit change in xj associated with multiplying the expected response by
eβj, holding all else in the model fixed.
10.3
Methodology
10.3.1
ESTIMATION AND INFERENCE
Maximum likelihood is typically used to estimate the parameters of generalized
linear models, including the Poisson regression model. The log-likelihood takes
the form
L =
n

i=1
(yi log μi −μi),
where μi satisfies (10.2). Just as was true for logistic regression, the maximizer
of this function (the MLE) takes the approximate form of a weighted least
squares (IRWLS) estimate with weight and corresponding error variance for
each observation that depends on the parameter estimates.
Inference proceeds in a completely analogous way to the situation for
logistic regression. Specifically:
• A test comparing any two models where one is a simpler special case of
the (more general) other is the likelihood ratio test LR, where
LR = 2(Lgeneral −Lsimpler).
This is compared to a χ2
d critical value, where d is the difference in the
number of parameters fit under the two models. This applies also to a
test of the overall significance of the regression, where the general model
is the model using all of the included predictors, and the simpler model
uses only the intercept.

10.3 Methodology
191
• Assessment of the statistical significance of any individual regression
coefficient βj also can be based on a LR test, but is often based on the
Wald test
zj =
ˆβj

s.e.(ˆβj)
.
Similarly, an asymptotic 100 × (1 −α)% confidence interval for βj has
the form ˆβj ± zα/2 
s.e.(ˆβj).
• Models can be compared using AIC, with
AIC = −2L + 2(p + 1).
As before, the corrected version has the form
AICc = AIC + 2(p + 2)(p + 3)
n −p −3
.
Note that (as was true for logistic regression models) the theory underlying
AICc does not apply to Poisson regression, but the criterion has proven
to be useful in practice.
• If the values of E(yi) are large, the Pearson (X2, the sum of squared
Pearson residuals) and deviance (G2) goodness-of-fit statistics can be
used to assess goodness-of fit. Each is compared to a χ2 distribution on
n −p −1 degrees of freedom. In the Poisson regression situation,
X2 =
n

i=1
(yi −ˆμi)2
ˆμi
(10.3)
and
G2 = 2
n

i=1

yi log
 yi
ˆμi

−(yi −ˆμi)

.
• Diagnostics are approximate, based on the IRWLS representation of the
MLEs.
10.3.2
OFFSETS
The Poisson regression model is based on modeling the expected number of
occurrences of some event as a function of different predictors, but sometimes
this is not the best choice to model. In some situations, what is of interest is
the expected rate of occurrence of an event, rather than the expected number
of occurrences. Here the rate is appropriately standardized so as to make the
values comparable across observations. So, for example, if one was analyzing
marriages by state, the actual number of marriages would not be of great
interest, since its strongest driver is simply the population of the state. Rather,
it is marriage rate (marriages per 100,000 population, for example) that is
comparable across states, and should therefore be the focus of the analysis.
This means that the appropriate model is
yi ∼Pois[ki × exp(β0 + β1x1i + · · · + βpxpi)],

192
CHAPTER 10 Count Regression
where ki is the standardizing value (such as population) for the ith observation,
and
exp(β0 + β1x1i + · · · + βpxpi)
now represents the mean rate of occurrence, rather than the mean number of
occurrences. Equivalently, the model is
yi ∼Pois{exp[β0 + β1x1i + · · · + βpxpi + log(ki)]}.
This means that a Poisson rate model can be fit by including log(ki) (the
so-called offset) as a predictor in the model, and forcing its coefficient to equal
one.
10.4
Overdispersion and Negative Binomial
Regression
As was noted earlier, the Poisson random variable is restrictive in that its
variance equals its mean. Often in practice, the observed variance of count
data is larger than the mean; this is termed overdispersion. The most common
cause of this is unmodeled heterogeneity, where differences in means between
observations are not accounted for in the model. Note that this also can occur
for binomial data (and hence in logistic regression models), since the binomial
random variable also has the property that its variance is exactly determined by
its mean. There are specific tests designed to identify overdispersion, but often
the standard goodness-of-fit statistics X2 and G2 can identify the problem.
The presence of overdispersion should not be ignored, since even if the form
of the fitted log-linear model is correct, not accounting for overdispersion
leads to estimated variances of the estimated coefficients that are too small,
making confidence intervals too narrow and p-values of significance tests too
small. In particular, the estimated standard errors of the estimated coefficients
are too small by the same factor as the ratio of the true standard deviation
of the response to the estimated one based on the Poisson regression. So, for
example, if the true standard deviation of y is 20% larger than that based on
the Poisson regression, the estimated standard errors should also be 20% larger
to reflect this.
10.4.1
QUASI-LIKELIHOOD
A simple correction for this effect is through the use of quasi-likelihood
estimation. Quasi-likelihood is based on the principle of assuming a mean
and variance structure for a response variable without specifying a specific
distribution. This leads to a set of estimating equations that are similar in form
to those for maximum likelihood estimation, and hence a similar estimation
strategy based on IRWLS.
Consider a count regression model that posits a log-linear model for the
mean (as is the case for Poisson regression), but a variance that is a simple

10.4 Overdispersion and Negative Binomial Regression
193
multiplicative inflation over the equality of mean and variance assumed by the
Poisson,
V (yi) = μi(1 + α).
In this case, the quasi-likelihood estimating equations are identical to the
Poisson regression maximum likelihood estimating equations, so standard
Poisson regression software can be used to estimate β. Since
V (yi)
μi
= E
(yi −μi)2
μi

= 1 + α,
a simple estimate of 1 + α is

1 + α =
1
n −p −1
n

i=1
(yi −ˆμi)2
ˆμi
,
which is the Pearson statistic X2 divided by its degrees of freedom. Thus,
quasi-likelihood corrects for overdispersion by dividing the Wald statistics
from the standard Poisson regression output by
	

1 + α =
	
X2/(n −p −1).
Model selection criteria also can be adapted to this situation. The quasi-
AIC criterion QAIC takes the form
QAIC = −2L
1 + ˆα + 2(p + 1),
while the corresponding bias-corrected version is
QAICc = QAIC + 2(p + 3)(p + 4)
n −p −4
(note that the correction in QAICc is slightly different than that for AICc,
since α is estimated here). Applying these criteria for model selection is not
straightforward. All of the QAIC (or QAICc) values must be calculated using
the same value of ˆα to be comparable to each other. One strategy is to determine
ˆα from the most complex model available, and then calculate the model selec-
tion criterion for each model using that value. Alternatively, ˆα could be chosen
based on the “best” Poisson regression model according to AIC (or AICc).
10.4.2
NEGATIVE BINOMIAL REGRESSION
An alternative strategy to address overdispersion is to fit a regression model
that is based on a random component that (unlike the Poisson distribution)
allows for overdispersion. The most common such distribution is the negative
binomial distribution. The negative binomial arises in several different ways,
but one in particular is most relevant here. The standard Poisson regression
model assumes that yi ∼Pois(μi), with μi a fixed mean value that is a function
of the predictor values. If instead μi is a random variable, this results in unmod-
eled heterogeneity, and hence overdispersion. The negative binomial random
variable arises if μi follows a Gamma distribution, and its variance satisfies
V (yi) = μi(1 + αμi).

194
CHAPTER 10 Count Regression
Note that unlike the quasi-likelihood situation discussed in the previous
section, here the variance is a function of the square of the mean, not the
mean. The Poisson random variable corresponds to the negative binomial with
α = 0. Some statistical packages parameterize the negative binomial using
θ = 1/α, so in that case, as θ →∞the negative binomial becomes closer to the
Poisson.
The parameters of a negative binomial regression are estimated using
maximum likelihood, based on the log-likelihood
L =
n

i=1
log Γ(yi + θ) −n log Γ(θ)
+
n

i=1

θ log

θ
θ + μi

+ yi log

μi
θ + μi

,
where Γ(·) is the gamma function
Γ(z) =

 ∞
0
e−ttz−1dt.
The usual IRWLS-based Wald tests and diagnostics apply here. AIC or
AICc can be used to compare negative binomial fits to each other, as well
as to compare negative binomial to Poisson fits, based on the appropriate
log-likelihoods and taking into account the additional α parameter.
10.5
Example — Unprovoked Shark Attacks
in Florida
The possibility of an unprovoked attack by a shark was the central theme of
the movie “Jaws” (and its sequels), but just how likely is that to happen? The
Florida Program for Shark Research at the Florida Museum of Natural History
maintains the International Shark Attack File (ISAF), a list of unprovoked
attacks of sharks on humans around the world. The data examined here cover
the years 1946 through 2011 for the state of Florida, an area of particular
interest since almost all residents and visitors are within 90 minutes of the
Atlantic Ocean or Gulf of Mexico. The data for 1946 to 1999 come from
Simonoff (2003), supplemented by information for later years given on the
International Shark Attack File web site. The response variable is the number
of confirmed unprovoked shark attacks in the state each year, and potential
predictors are the resident population of the state and the year.
Figure 10.1 gives scatter plots of the number of attacks versus year and
population. As would be expected, a higher population is associated with
a larger number of attacks. Also, there is a clear upwards trend over time.
It is also apparent that the amount of variability in attacks increases with
the number of attacks, as would be consistent with a Poisson (or negative
binomial) random variable.

10.5 Example — Unprovoked Shark Attacks in Florida
195
1950
1970
1990
2010
0
10
20
30
Year
Attacks
5.0e + 06
1.5e + 07
0
10
20
30
Population
Attacks
FIGURE 10.1: Scatter plots for the Florida shark attack data.
The Poisson regression output is as follows.
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept) -2.092e+02
5.400e+01
-3.874 0.000107 ***
Year
1.077e-01
2.775e-02
3.880 0.000104 ***
Population
-2.051e-07
9.468e-08
-2.166 0.030327 *
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 640.2
on 65
degrees of freedom
Residual deviance: 195.8
on 63
degrees of freedom
Each of the predictors is statistically significant. The coefficient for Year
implies that given population, the expected number of attacks is estimated to
be increasing 11% per year (e.108 = 1.11). It should be clear, however, that
given the great change in population in Florida since World War II, this is
a situation where modeling attack rates is more meaningful than modeling
number of attacks. Figure 10.2 gives a plot of attack rate versus year, and it is
apparent that there is still an increasing trend over time.
Output for a Poisson regression model on year taking logged population
as an offset is given below.
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept) -55.23413
5.30402 -10.414
< 2e-16 ***
Year
0.02080
0.00266
7.819 5.34e-15 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 254.89
on 65
degrees of freedom
Residual deviance: 187.55
on 64
degrees of freedom
The relationship is strongly statistically significant, and it implies an
estimated 2.1% annual increase in shark attack rates. Figure 10.3, however,
indicates problems with the model. The time series plot of the standardized

196
CHAPTER 10 Count Regression
1950
1960
1970
1980
1990
2000
2010
0.0e + 00
5.0e – 07
1.0e – 06
1.5e – 06
2.0e – 06
Year
Attack rate
FIGURE 10.2: Plot of attack rate versus year for the Florida shark attack data.
0
5
10
15
20
25
–2
0
2
4
Estimated attacks
Standardized residuals
0
10
20
30
40
50
60
–2
0
2
4
Index
Standardized residuals
0
10
20
30
40
50
60
0.02 0.04 0.06 0.08 0.10
Index
Leverage
0
10
20
30
40
50
60
0.0
0.2
0.4
0.6
Index
Cook's D
FIGURE 10.3: Diagnostic plots for the Poisson regression model for attack
rate using Year as the predictor for the Florida shark attack data.

10.5 Example — Unprovoked Shark Attacks in Florida
197
Pearson residuals gives evidence of a downward trend in the residuals for the
last ten years of the sample (that is, 2002 through 2011), and the Cook’s
distances are noticeably larger for the last two years.
This pattern is, in fact, not surprising. The ISAF Worldwide Shark Attack
Summary (Burgess, 2012) noted that there were notable slow-downs in local
economies after the September 11, 2001 terrorist attacks in the United States,
which were exacerbated by the 2008-2011 recession and the very active tropical
seasons in Florida in 2004, 2005, and 2006. As a result fewer people were
entering the water during this time period. Further, extensive media coverage
of the “do’s and don’t’s” of shark-human interactions may have led to people
reducing their interactions with sharks.
This suggests that the shark attack rate pattern might be different pre- and
post-9/11. This can be handled easily within the Poisson regression framework
by adding an indicator variable for the post-9/11 years and the product of that
indicator and the year as predictors. The resultant output is as follows.
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-85.486725
7.997612 -10.689
< 2e-16 ***
Year
0.036079
0.004024
8.967
< 2e-16 ***
Post.911
215.593383
49.542971
4.352 1.35e-05 ***
Year:Post.911
-0.107741
0.024703
-4.361 1.29e-05 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 254.89
on 65
degrees of freedom
Residual deviance: 146.31
on 62
degrees of freedom
This model is a clear improvement over the pooled regression line, as all of the
predictors are highly statistically significant. A likelihood-ratio test comparing
the pooled model to the model based on two separate lines is LR = 41.24
on 2 degrees of freedom, which is very highly statistically significant. The
model implies an estimated 3.7% annual increase in shark attack rates from
1946-2001 (e.036 = 1.037), but an estimated 6.9% annual decrease in attack
rates from 2002-2011 (e.0361−.1077 = .931). Diagnostic plots for this model
(Figure 10.4) also do not indicate any systematic problems with the model.
Unfortunately, the deviance of G2 = 146.3 on 62 degrees of freedom still
indicates lack of fit (the Pearson statistic X2 = 145.8 is similar). Since (based on
diagnostic plots) the lack of fit does not seem to come from misspecification of
the expected shark attack rate, this suggests that it is reflecting overdispersion,
which would not be surprising given changes in tourism patterns during
this 65-year period. One approach to accounting for this is quasi-likelihood,
in which the standard errors of the estimated coefficients are multiplied by
	
145.8/62 = 1.53. If this is done all of the absolute Wald statistics for the
slopes are still greater than 2.8 (corresponding to p-values less than .004), so
while statistical significance is weaker, the existence of two different patterns
for the pre- and post-9/11 time periods is still supported.

198
CHAPTER 10 Count Regression
0
5
10
15
20
25
–2
0
1
2
3
4
Estimated attacks
Standardized residuals
0
10
20
30
40
50
60
–2
0
1
2
3
4
Index
Standardized residuals
0
10
20
30
40
50
60
0.1
0.2
0.3
Index
Leverage
0
10
20
30
40
50
60
0.0
0.1
0.2
0.3
Index
Cook's D
FIGURE 10.4: Diagnostic plots for the Poisson regression model for attack
rate based on different relationships with time for the pre- and post-9/11 time
periods for the Florida shark attack data.
Alternatively, a negative binomial regression model can be fit to the data.
Output is as follows:
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-76.600673
10.808080
-7.087 1.37e-12 ***
Year
0.031600
0.005453
5.795 6.82e-09 ***
Post.911
212.958313
94.149355
2.262
0.0237 *
Year:Post.911
-0.106378
0.046933
-2.267
0.0234 *
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 124.824
on 65
degrees of freedom
Residual deviance:
77.493
on 62
degrees of freedom
Theta:
7.73
Std. Err.:
3.00
The resultant model has ˆθ = 7.73 (or equivalently ˆα = 0.129), and even after
accounting for the overdispersion still supports the existence of different
patterns for the two time periods. Diagnostic plots (Figure 10.5) are similar
to those for the Poisson regression fit, except that the residuals and Cook’s

10.5 Example — Unprovoked Shark Attacks in Florida
199
0
5
10
15
20
25
–1
0
1
2
3
Estimated attacks
Standardized residuals
0
10
20
30
40
50
60
–1
0
1
2
3
Index
Standardized residuals
0
10
20
30
40
50
60
0.05
0.15
0.25
0.35
Index
Leverage
0
10
20
30
40
50
60
0.00 0.02 0.04 0.06 0.08
Index
Cook's D
FIGURE 10.5: Diagnostic plots for the negative binomial regression model
for attack rate based on different relationships with time for the pre- and
post-9/11 time periods for the Florida shark attack data.
distances are noticeably smaller for the negative binomial fit, indicating that the
lack of fit has been addressed to a large extent. Here G2 = 77.5 (p = .09) and
X2 = 71.7 (p = .19), each on 62 degrees of freedom, indicating a moderately
reasonable fit.
In all but one year after 1955, there was at least one unprovoked shark
attack in Florida, meaning that a direct fitting of a log-linear model for
attack rates is possible by taking logs of the attack rate variable and using
least squares. This attempt to avoid count regression, while incorrect, results
in estimated coefficients not very dissimilar from the Poisson and negative
binomial fits, although implications of inference are different. This is not even
a consideration, however, in a situation where there are many zero counts
among the responses. Consider, for example, modeling the number of annual
fatalities from unprovoked shark attacks in Florida. In this case the response
only takes on the values 0, 1, or 2, and in more than 80% of the years there were
no fatalities. Taking logs is not feasible here, but a log-linear count regression
model can be easily fit. Here is output for a Poisson regression model for per
capita fatality rate on year and the number of attacks in that year (there is
no evidence of different patterns pre- and post-9/11, which is to be expected,
since lower tourism rates are already reflected in fewer attacks):

200
CHAPTER 10 Count Regression
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept) 95.84120
52.15408
1.838
0.0661 .
Year
-0.05772
0.02657
-2.173
0.0298 *
Attacks
0.07022
0.04164
1.686
0.0917 .
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 47.586
on 65
degrees of freedom
Residual deviance: 42.419
on 63
degrees of freedom
The model implies more fatalities in a given year if it has more attacks,
and given the number of attacks, a decreasing trend in fatality rate of roughly
5.6% annually (e−.0577 = .944), perhaps reflecting more understanding of
how to handle shark attacks among residents and tourists. Diagnostic plots
(Figure 10.6) indicate one pronounced outlier (1976, in which there were 2
fatalities in 5 attacks), and omitting that observation strengthens the observed
relationships slightly:
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept) 116.55824
59.28918
1.966
0.0493 *
Year
-0.06843
0.03022
-2.264
0.0235 *
0.1
0.2
0.3
0.4
0.5
0.6
0
1
2
3
4
5
Estimated fatalities
Standardized residuals
0
10
20
30
40
50
60
0
1
2
3
4
5
Index
Standardized residuals
0
10
20
30
40
50
60
0.05
0.15
0.25
Index
Leverage
0
10
20
30
40
50
60
0.00 0.05 0.10 0.15 0.20
Index
Cook's D
FIGURE 10.6: Diagnostic plots for the Poisson regression model for fatality
rate based on year and number of attacks for the Florida shark fatalities data.

10.6 Other Count Regression Models
201
Attacks
0.09389
0.04578
2.051
0.0403 *
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Null deviance: 41.163
on 64
degrees of freedom
Residual deviance: 35.200
on 62
degrees of freedom
10.6
Other Count Regression Models
In this section we briefly mention some useful generalizations of these models.
It is often the case that more observations with zero counts than expected occur
in the observed data. This can be represented through the use of a mixture
model, where an observed yi is modeled as coming from either a point mass
at zero (a constant zero) or the underlying count distribution (for example
Poisson or negative binomial). These are termed zero-inflated count regression
models. In the zero-inflated Poisson (ZIP) model, for example, a count is
generated from one of two processes: the usual Poisson random variable with
mean μi = exp(x′
iβ) with probability 1 −ψi, and zero with probability ψi.
Note that this means that an observation with yi = 0 can come from either
the underlying Poisson random variable or from the point mass at zero.
The model allows ψi to vary from observation to observation, with the
probability modeled to satisfy
ψi = F(z′
iγ),
where F(·) is a cumulative distribution function, zi is a set of predictors for
the probability of coming from the point mass distribution, and γ is a set of
parameters corresponding to the predictors. The distribution function F is
typically taken to be either the standard normal distribution (a probit model)
or the logistic distribution (a logit model). The z variables can include some
(or all) of the x’s, and can include other variables. The parameters β and γ can
be estimated using maximum likelihood. The conditional mean of the target
variable is
E(yi|xi, zi) = μi(1 −ψi),
while the conditional variance is
V (yi|xi, zi) = μi(1 −ψi)(1 + μiψi),
implying overdispersion relative to the Poisson if ψi > 0. The need for the
ZIP regression model over the Poisson regression model can be assessed using
an information criterion, such as AICc. One simple variation of the model
takes the z’s to be identical to the x’s, and assumes γ = τβ for some τ.
If the z and x predictors overlap, interpretation of the estimated parameters
ˆβ is not straightforward. For example, if a β coefficient corresponding to a
particular predictor is positive, then larger values of that predictor are associated
with a larger mean in the Poisson part of the distribution. If, however, this

202
CHAPTER 10 Count Regression
variable has a positive γ coefficient, then larger values are associated with a
higher probability of coming from the point mass distribution, lowering the
overall expected value of the target variable. These two tendencies can combine
in complex ways. If a ZIP formulation with two distinct (nonoverlapping)
processes for y is a reasonable representation of reality, things are easier, as
then the β coefficients have the usual log-linear interpretation, only now given
that the observation comes from the Poisson part of the distribution. These
models also can be generalized to incorporate inflation at count values other
than zero in the analogous way.
Negative binomial regression also can be modified to allow for zero-count
inflation. The zero-inflated negative binomial (ZINB) model uses a negative
binomial distribution in the mixture rather than a Poisson, but otherwise the
formulation is the same. The conditional mean of the response is identical to
that for the ZIP model,
E(yi|xi, zi) = μi(1 −ψi),
while the conditional variance is
V (yi|xi, zi) = μi(1 −ψi)[1 + μi(ψi + α)].
If ψi = 0 this is the standard negative binomial regression model variance,
but overdispersion relative to the negative binomial occurs if ψi > 0. The ZIP
model is a special case of the ZINB model (with α = 0), so the usual tests
can be constructed to test against the one-sided alternative α > 0. It should be
noted that misspecification of the nondegenerate portion of the model (that
is, Poisson or negative binomial) is more serious in the zero-inflated context
than in the standard situation, since (unlike in the standard case) parameter
estimators assuming a Poisson distribution are not consistent if the actual
distribution is negative binomial.
A different situation is one where the observed count response variable is
truncated from below, with no values less than a particular value observed.
This most typically occurs where truncation occurs at zero. An example of this
would be where the response variable is the number of items purchased by a
customer among a set of customers on a given day; in this case only customers
that purchased at least one item appear in the data, so yi = 0 is not possible.
In this case the zero-truncated Poisson regression is based on the density
f(yi; μi) = fPois(yi; μi)
(1 −e−μi) ,
yi = 1, 2, . . . ,
with μi following the usual logarithmic link with predictors x1, . . . , xp. The
mean of the target variable is
E(yi) =
μi
1 −e−μi ,
(10.4)
while the variance is
V (yi) =

μi
1 −e−μi
 
1 −μie−μi
1 −e−μi

.

10.7 Poisson Regression and Weighted Least Squares
203
The estimates of μi (or equivalently, of β) are not consistent if the Poisson
assumption is violated (even if the link function and linear predictor are
correct), because the expected value depends on correct specification of
P(yi = 0).
The interpretation of regression coefficients for this model is more
complicated than that for nontruncated data. If the population of interest
is actually the underlying Poisson random variable without truncation (if,
for example, values of the target equal to zero are possible, but the given
sampling scheme does not allow them to be observed), the coefficients have
the usual interpretation consistent with the log-linear model [since in that case
E(y) = exp(x′β)]. If, on the other hand, observed zeroes are truly impossible,
βj no longer has a simple connection to the expected multiplicative change
in y. Rather, the instantaneous expected change in y given a small change in
xj, holding all else fixed, is
∂E(yi|xi; yi > 0)
∂xij
= βjμi
1 −exp(−μi)(1 + μi)
[1 −exp(−μi)]2

.
(10.5)
For nontruncated data the right-hand side of (10.5) is simply βjμi (reflecting
the interpretation of βj as a semielasticity), so the truncation adds an
additional multiplicative term that is a function of the nontruncated mean
and the nontruncated probability of an observed zero. The right-hand side of
(10.5) divided by μi represents the semielasticity of the variable for this model.
If the target variable exhibits overdispersion relative to a (truncated)
Poisson variable, a truncated negative binomial is a viable alternative model.
The mean of this random variable is
E(yi) =
μi
1 −(1 + αμi)−1/α ,
while the variance is
V (yi) =

μi
1 −(1 + αμi)−1/α
 
1 + αμi −μi(1 + αμi)−1/α
1 −(1 + αμi)−1/α

.
10.7
Poisson Regression and Weighted Least
Squares
We have previously discussed how nonconstant variance related to group
membership in an ordinary least squares fit can be identified using Levene’s
test, and handled using weighted least squares with the weights for the members
of each group being the inverse of the variance of the residuals for that group.
Another way to refer to nonconstant variance related to group membership is
to say that nonconstant variance is related to the values of a predictor variable,
where that predictor variable happens to be categorical. It is also possible
that the variance of the errors could be related to a (potential) predictor
variable that is numerical. Generalizing the Levene’s test for this situation is

204
CHAPTER 10 Count Regression
straightforward: just construct a regression with the absolute residuals as the
response and the potential numerical variable as a predictor. Note that this
also can be combined with the situation with natural subgroups by running
an ANCOVA model with the absolute residuals as the response and both the
grouping variable(s) and the numerical variable(s) as predictors (of course, the
response variable should not be used as one of the predictors).
Constructing weights for WLS in the situation with numerical variance
predictors is more complicated. What is needed is a model for what the
relationship between the variances and the numerical predictor is. An expo-
nential/linear model for this relationship is often used, whose parameters can
be estimated from the data. This model has the advantage that it can only
produce positive values for the variances, as is appropriate. The model for the
variance of εi is
V (εi) ≡σ2
i = σ2 exp
⎛
⎝
j
λjzij
⎞
⎠,
(10.6)
where zij is the value of the jth variance predictor for the ith case and σ2 is
an overall “average” variance of the errors.
Poisson regression can be used to estimate the λ parameters, as is proposed
in Simonoff and Tsai (2002). The key is to recognize that since σ2
i = E(ε2
i ),
by (10.6)
log E(ε2
i ) = log σ2 +

j
λjzij ≡λ0 +

j
λjzij.
This has the form of a log-linear model for the expected squared errors, so
a Poisson regression using the squared residuals (the best estimates of the
squared errors available from the data) as the response provides an estimate of
λ, and hence estimated weights for a WLS fit to the original y. Since (10.6)
is a model for variances, the weight for the ith observation is the inverse of
the fitted value from a Poisson regression for that observation. The squared
residuals are not integer-valued, so it is possible that the Poisson regression
package will produce an error message, but it should still provide the weights.
10.7.1
EXAMPLE — INTERNATIONAL GROSSES OF MOVIES
(CONTINUED)
Recall the ANCOVA model fitting for logged international grosses of movies
discussed in Section 7.3. The chosen model in that analysis was based on
logged domestic grosses and MPAA rating, with different slopes and intercepts
for the different rating levels. As was noted on page 141, however, there
is nonconstant variance related to logged domestic grosses apparent in the
residuals, with movies with lower domestic revenues having higher variability
in international revenues. This suggests fitting a weighted least squares model
to these data, with weights based on logged domestic grosses. A Poisson

10.8 Poisson Regression and Weighted Least Squares
205
regression fit to the squared standardized residuals is

(e∗
i )2 = exp(2.476 −1.606 × Log.domestic.gross).
A WLS fit (not given) finds “Avatar” to be a leverage point due to its very
high domestic gross, which corresponds to low variance and therefore high
weight. Omitting “Avatar” gives the following fit for the WLS model with
different slopes for different rating classes:
Response: Log.international.gross
Sum Sq
Df F value
Pr(>F)
(Intercept)
0.0991
1
0.3977
0.5295
Log.domestic.gross
5.4982
1 22.0652 7.184e-06 ***
Rating
1.3839
3
1.8513
0.1417
Log.domestic.gross:Rating
1.0044
3
1.3436
0.2636
Residuals
29.4034 118
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.4992 on 118 degrees of freedom
Multiple R-squared: 0.6625,
Adjusted R-squared: 0.6425
F-statistic:
33.1 on 7 and 118 DF,
p-value: < 2.2e-16
The interaction term is not statistically significant in the WLS fitting. Omitting
it and fitting a constant shift model yields an insignificant F-test for MPAA
rating, and the best model (in terms of minimum AICc, for example) is the
simple linear regression on logged domestic grosses only:
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
-0.77623
0.18003
-4.312 3.27e-05 ***
Log.domestic.gross
1.34107
0.09107
14.726
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 0.5056 on 124 degrees of freedom
Multiple R-squared: 0.6362,
Adjusted R-squared: 0.6333
F-statistic: 216.8 on 1 and 124 DF,
p-value: < 2.2e-16
Residual plots (Figure 10.7) show that the heteroscedasticity in the OLS fit
has been addressed, and the model fits reasonably well.
In addition to resulting in different models with different implications
(a single fitted line versus four different lines), WLS and OLS can produce very
different prediction intervals for some movies. For example, the 2010 movie
“From Paris With Love” had a low domestic gross (and hence high estimated
variability), which is reflected in the difference between the OLS prediction
interval for international gross of (0.73, 67.87) (in millions of dollars) and the
WLS interval (0.86, 166.41). By contrast, “Shutter Island” has an OLS interval
of (15.15, 1569.77) and a WLS interval of (25.68, 489.68), reflecting its high
domestic gross and implied lower variability of international grosses.

206
CHAPTER 10 Count Regression
0.5
1.0
1.5
2.0
2.5
–4 –3 –2 –1
0
1
2
Fitted values
Standardized residuals
–2
–1
0
1
2
–4 –3 –2 –1
0
1
2
Normal Q−Q Plot
Theoretical Quantiles
Sample Quantiles
0
20
40
60
80 100 120
0.02
0.06
0.10
Index
Leverage
0
20
40
60
80 100 120
0.00
0.10
0.20
Index
Cook's D
FIGURE 10.7: Diagnostic plots for WLS model for 2009 international grosses
data.
10.8
Summary
In this chapter we have used the generalized linear model framework to extend
Gaussian-based least squares regression and binomial- and multinomial-based
logistic regression to count data regression models. The central distribution
for count regression is the Poisson distribution, with generalizations such as
the negative binomial, zero-inflated, and zero-truncated distributions available
to handle violations of the Poisson structure. Further discussion of generalized
linear models can be found in McCullagh and Nelder (1989) and Myers et al.
(2002), and more details on count data modeling can be found in Simonoff
(2003).
Poisson regression models also can be fit based on categorical predictors. In
the situation where all of the predictors are categorical the data take the form of
a contingency table (a table of counts), and Poisson- and multinomial-based
log-linear models have long been the basis of contingency table analyses.
Extensive discussion of the analysis of such tables can be found in Agresti
(2007), Agresti (2010), and Simonoff (2003).
The situation when the expected counts are small is termed sparse
categorical data. In this situation the theory underlying standard methods

10.8 Summary
207
does not apply, and those methods can provide misleading results. There is
an extensive literature for this situation. One approach that is particularly
appropriate for data with ordered categories is to use smoothing methods,
which borrow information from nearby categories to improve estimation in
any given category. Further discussion can be found in Section 15.5 and
Chapter 6 of Simonoff (1996).
KEY TERMS
Generalized linear model: A generalization of ordinary (least squares) regres-
sion that allows for a non-Gaussian response variable and a nonlinear link
between the weighted sum of the predictor variables and the expected response.
Least squares, logistic, and Poisson regression are all examples of generalized
linear models.
Negative binomial random variable: A discrete random variable that provides
an alternative to the Poisson random variable for count regression models. One
mechanism generating a negative binomial random variable is as a Poisson
random variable with a random, rather than fixed, mean. It is useful for the
modeling of overdispersion, as its variance is larger than its mean.
Offset: A variable included in the mean function of a generalized linear model
with a specified coefficient. When in a count regression model the offset is the
log of a standardizing variable such as population, and the coefficient is set
to 1, this results in a model for expected rates rather than expected counts.
Overdispersion: The situation where the observed variance of a random
variable is larger than would be expected given its mean. The most common
example of this is for Poisson fitting of count data, since in that case the
variance is assumed to equal the mean.
Poissonrandomvariable: The most commonly-used discrete random variable
used to model count data. Standard assumptions generate the Poisson random
variable as appropriate for modeling the number of events that occur in time
or space.
Quasi-likelihood estimation: An estimation method based on the principle
of assuming a mean and variance structure of a response variable without
specifying a specific probability distribution. This sometimes, but not always,
leads to an estimation scheme identical to likelihood estimation for some
distribution.
Zero-inflated count regression model: A generalization of count regression
models such as those based on the Poisson and negative binomial distributions
that allows for more zero counts than would be expected.
Zero-truncated count regression model: A generalization of count regression
models such as those based on the Poisson and negative binomial distributions
that does not allow for any zero counts by truncating the distribution above
zero.

Eleven
Chapter
Models for Time-to-Event
(Survival) Data
11.1
Introduction
210
11.2
Concepts and Background Material
211
11.2.1 The Nature of Survival Data
211
11.2.2 Accelerated Failure Time Models
212
11.2.3 The Proportional Hazards Model
214
11.3
Methodology
214
11.3.1 The Kaplan-Meier Estimator and the Log-Rank
Test
214
11.3.1.1 The Kaplan-Meier Estimator
214
11.3.1.2 The Log-Rank Test
215
11.3.1.3 Example — The Survival of Broadway
Shows
216
11.3.2 Parametric (Likelihood) Estimation
219
11.3.3 Semiparametric (Partial Likelihood)
Estimation
221
11.3.4 The Buckley-James Estimator
223
11.4
Example — The Survival of Broadway Shows
(continued)
223
11.5
Left-Truncated/Right-Censored Data and Time-Varying
Covariates
230
11.5.1 Left-Truncated/Right-Censored Data
230
11.5.2 Example — The Survival of Broadway Shows
(continued)
233
11.5.3 Time-Varying Covariates
233
11.5.4 Example — Female Heads of Government
235
11.6
Summary
238
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
209

210
CHAPTER 11 Models for Time-to-Event (Survival) Data
11.1
Introduction
The analysis of time-to-event data, often referred to as survival data or in
some circumstances failure time data, refers to the study of durations, whether
that is a duration of a lifetime or more generally the time until an event of
interest occurs. We will use the terms death, failure, and event interchangeably
to refer to the event of interest. At first glance it might seem that there is
nothing particularly special about time-to-event data compared to any other
type of regression modeling, but that is not the case. First, a time-to-event
value clearly must be positive, and such data tend to have a distribution that
is noticeably right-tailed, with many events occurring quickly but some not
occurring until a much later time. This is not inherent to only survival data,
of course, and the use of multiplicative models for the expected duration, as
discussed in Chapter 4, is a natural possibility.
A much more important characteristic of time-to-event data is that it is
often only incompletely observed. In many situations the observed response
value y actually only corresponds to knowledge that the survival time is at least
y. In a clinical trial, for example, the actual life length of patients who are alive
at the end of the study is only known to be at least the observed value. That
is, the survival time is (right-)censored at y. This has a fundamental effect
on analysis, since it is clear that if censoring is ignored the observed values of
censored observations are systematically too low, resulting in bias.
In this chapter we discuss regression analysis of time-to-event data. This is
an extremely heavily-studied area, from both the biomedical survival point of
view and the engineering reliability point of view, and we will only scratch the
surface of the available methodology in this chapter. We start with discussion
of the two commonly-used summaries of the distribution of survival data, the
survival function and the hazard function. We then discuss accelerated failure
time models, which are the natural generalization of the regression models
we have previously discussed. We start with parametric models, in which
the relationship between expected survival time and the predictors and the
distribution of survival times are explicitly specified, taking potential censoring
into account.
The most popular approach to modeling survival data is through the
assumption of proportional hazards, modeling the instantaneous probability
of the event of interest occurring at a given time given survival to that time.
Although this can be done parametrically, the typical approach is through the
use of the semiparametric Cox (1972) model. We discuss this model, along
with several generalizations of it, as well as the Buckley-James estimator, a
semiparametric approach to direct modeling of expected survival time.

11.2 Concepts and Background Material
211
11.2
Concepts and Background Material
11.2.1
THE NATURE OF SURVIVAL DATA
The character of survival data as that of the amount of time until an event
occurs has led to several specific ways of examining and representing it.
Description of a survival response variable has historically focused on measures
related to survival probability and risk of failure, rather than expected life
length. Specifically, the survival function (or reliability function) S(y) is
defined as the probability of surviving past time y. That is,
S(y) = 1 −F(y) =
 ∞
y
f(u)du,
(11.1)
where f(·) is the density of the survival random variable and F(·) is the
cumulative distribution function.
The hazard function h(y) focuses on risk of failure at a given y, being
the instantaneous rate of failure at time y given that the individual survives to
time y, or
h(y) = f(y)
S(y).
(11.2)
Note that (11.1) and (11.2) imply that
h(y) = −∂log S(y)
∂y
,
which means that S(y) = exp[−H(y)], where H(·) is the cumulative hazard
function.
Often the context of a problem suggests what sort of hazard function
might be reasonable. For example, the lognormal distribution can be shown to
have a hazard function that increases to a maximum and then decreases to 0,
which is unlikely to be reasonable when modeling human life length (for
example), where it would be expected that the hazard of dying increases with
old age.
As was noted earlier, an important characteristic of survival data is the pos-
sibility of censoring. For this reason, the response variable in survival regression
modeling is actually a bivariate response (yi, δi), where δi is an indicator of
whether the observed response is an actual event (a failure) versus a censored
value. Censoring can occur in many ways, but a simplifying assumption is that
of random censoring. Let {Ti, . . . , Tn} be a set of true failure times, and let
{Ci, . . . , Cn} be a set of censoring times, randomly generated independent
of T. Random censoring corresponds to the mechanism whereby the observed

212
CHAPTER 11 Models for Time-to-Event (Survival) Data
response is (yi, δi), where yi = min(Ti, Ci), and
δi =

1
if Ti ≤Ci (uncensored)
0
if Ti > Ci (censored).
An example of random censoring is in a clinical trial situation with a fixed
endpoint, where patients arrive at random times during the trial. In this
situation the censoring time is the time from entry to the end of the trial, and
patients that are still alive at that point are right-censored.
11.2.2
ACCELERATED FAILURE TIME MODELS
The accelerated failure time model is a regression model that hypothesizes
that covariates act directly on lifetime, either speeding it up or slowing it down
in a multiplicative fashion. This is typically modeled using an exponential
function, implying
yx = y0eβ1x1+···+βpxp,
where yx(y0) is the lifetime when the covariate vector is x(0). Taking (natural)
logs of both sides gives
log yx = log y0 + β1x1 + · · · + βpxp
(11.3)
= β0 + β1x1 + · · · + βpxp + σZ,
(11.4)
where y0 = β0 + σZ and Z is a standardized random variable that is analogous
to an error distribution.
This is of course a semilog (log-linear) model of the kind discussed in
Chapter 4. This multiplicative relationship accounts for the “accelerated”
terminology; for example, a parameter that implies halving the expected
survival time can be viewed as “accelerating” time by a factor of two. Taking
Z to be a standard normal random variable results in an accelerated failure
time model based on a lognormal survival distribution, and other choices of Z
result in other survival distributions. Figure 11.1(a) gives density functions for
a commonly-chosen alternative to the normal for Z, along with the normal
density (solid line) as a comparison. The dashed curve is the density function
of the standardized logistic distribution. Its shape is similar to that of the
normal density, but it has fatter tails than the normal. This is in fact the
distribution of the latent variable in Figure 9.1 that leads to the proportional
odds model. This distribution implies a log-logistic distribution for lifetimes.
Figure 11.1(b) gives several densities from a flexible family of distributions
for the logged survival, the extreme value distribution. This choice of Z
results in lifetimes that follow a Weibull distribution. The three curves
correspond to the three choices of the shape parameter α = 1 (solid line),
α = 1.5 (dashed line), and α = .5 (dotted-and-dashed line). Different choices
of α change both the effective range of the distribution and the skewness of
the density.
Figure 11.2 gives the hazard functions of the associated survival distri-
butions that correspond to the densities given in Figure 11.1, and illustrates

11.2 Concepts and Background Material
213
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
(a) Normal and Logistic
x
Density of logged survival
−4
−2
0
2
4
0.0
0.2
0.4
0.6
(b) Extreme value
x
Density of logged survival
FIGURE 11.1: Density functions of logged survival times. (a) Normal (solid
line) and logistic (dashed line) densities. (b) Extreme value densities with
shape parameter α = 1 (solid line), α = 1.5 (dashed line), and α = .5
(dotted-and-dashed line).
0
10
20
30
40
50
0.0
0.4
0.8
x
Hazard of survival
0
1
2
3
4
5
0
1
2
3
4
5
(a) Lognormal and Log−logistic
(b) Weibull
x
Hazard of survival
FIGURE 11.2: Hazard functions of survival times. (a) Lognormal (solid line)
and log-logistic (dashed line) hazard functions. (b) Weibull hazard functions
with shape parameter α = 1 (solid line), α = 1.5 (dashed line), and α = .5
(dotted-and-dashed line).
more clearly the implications of the choice of survival distribution. From
Figure 11.2(a) we see that (as was mentioned earlier) the lognormal distribu-
tion has a hazard function that increases and then decreases as the lifetime
increases, and the log-logistic hazard follows a similar pattern (different choices
of the scale parameter of the log-logistic lead to increasing hazard for smaller
values of x). The Weibull distribution (Figure 11.2(b)), on the other hand, has
very different hazard functions depending on the value of the shape parameter,
being constant for α = 1 (the solid line, which corresponds to an exponen-
tial distribution), increasing for α = 1.5 (dashed line), and decreasing for
α = .5 (dotted-and-dashed line). This flexibility makes the Weibull survival
distribution an excellent first choice in accelerated failure time modeling,
although it does have the restriction of monotonic hazard function.

214
CHAPTER 11 Models for Time-to-Event (Survival) Data
11.2.3
THE PROPORTIONAL HAZARDS MODEL
A different approach is to model the hazard of failure, rather than survival. The
proportional hazards model hypothesizes that the effect of covariates is to
multiply the hazard function. This is typically modeled using an exponential
function
h(yx) = h0(y)eβ1x1+···+βpxp,
(11.5)
which implies that
S(yx) = S0(y)exp(β1x1+···+βpxp),
(11.6)
where S0(·) is the baseline survival function. The Weibull distribution satisfies
the proportional hazards assumption, being the only distribution that is
consistent with both proportional hazards and accelerated failure times,
reinforcing its role as a natural first choice for parametric modeling. As a
special case of the Weibull the exponential distribution also clearly satisfies
both proportional hazards and accelerated failure times.
11.3
Methodology
11.3.1
THE KAPLAN-MEIER ESTIMATOR AND THE LOG-RANK
TEST
11.3.1.1
The Kaplan-Meier Estimator
The central nature of the survival function S(·) in survival analysis makes
estimation of it particularly important. If none of the observations are
censored this is straightforward; since S(y) = 1 −F(y), the natural estimate
is the empirical survival function,
ˆS(y) = 1 −ˆF(y) = number of observations > y
n
.
The presence of censored observations complicates matters. The solution
is the Kaplan-Meier product limit estimator, which is the nonparametric
MLE. The estimator is based on the idea of a risk set. Let
y(1) < y(2) < · · · < y(k)
be the set of k distinct ordered event (death) times (excluding censored times),
di be the number of deaths at time y(i), and ni be the number of subjects alive
just before y(i) (i.e., the size of the risk set R(y(i)), which is the number at risk
for death). The natural estimate of the conditional probability of surviving
through y(i) given that the subject survived past y(i−1) is the proportion of the
risk set at y(i) that did not die, or
ˆP(Y ≥y(i)|Y > y(i−1)) = 1 −di
ni
.
Since survival past any given time y corresponds to the intersection of survival
through y(1), and survival through y(2) given survival through y(1), and so on

11.3 Methodology
215
up to time y, this implies the product limit estimator
ˆS(y) =

i:y(i)<y

1 −di
ni

.
(11.7)
By convention uncensored observations precede tied censored observations,
as the existence of censoring means that the actual event time would
have been greater than the observed censored time if there had been no
censoring.
Examination of (11.7) shows that ˆS(·) is a monotone decreasing step
function from ˆS(0) = 1, with jumps at the uncensored observation times. If
the largest observation time is censored the estimate never reaches 0. Since
censored observations do not directly affect the appearance of ˆS(·), they are
typically identified with a mark such as a + or ×. Plots of Kaplan-Meier
curves separated by a categorical grouping variable (such as treatment versus
control) are a powerful graphical tool for comparing survival distributions, as
is illustrated in Section 11.3.1.3.
The Kaplan-Meier estimator is a nonparametric one, but there are still
assumptions being made. Specifically, we assume that at any time subjects
that are censored have the same survival chances as those that continue to be
followed. Further, we assume that the survival probabilities are the same for
subjects who appear early and appear late in the sample (so, for example, in a
clinical context subjects have the same survival distribution whether they are
recruited early or late into the study).
A related estimator is the Nelson-Aalen estimator, which estimates the
cumulative hazard function using
ˆH(y) =

i:y(i)<y
di
ni
.
(11.8)
The Taylor Series approximation log(1 + ε) ≈ε for ε small implies that the
two estimators are asymptotically equivalent to each other, in the sense that
ˆHNA(y) ≈−log[ ˆSKM(y)].
11.3.1.2
The Log-Rank Test
A natural question arising from examination of Kaplan-Meier curves from
several groups is whether the survival curves are significantly different from
each other. The most popular method of making that comparison is the
log-rank test. The log-rank test compares the survival curves in the different
groups at each event (failure) time. The observed number of deaths in each
group at each time is compared to the expected number of deaths in each
group if there was no difference in survival between the groups (that is,
pooling the groups). Just as is true for the Kaplan-Meier curve, censored
observations are included in the risk set up to the censoring time but not
afterwards. The total observed and expected numbers of deaths are then
compared across groups using a Poisson-based Pearson goodness-of-fit form as

216
CHAPTER 11 Models for Time-to-Event (Survival) Data
in (10.3), summing over groups rather than observations, which is compared
to a χ2
g−1 distribution, where g is the number of groups. Generalizations of
the log-rank test to identify specific forms of the alternative, such as ordered
survival distributions for an ordinal variable, have been proposed, but such
alternatives are naturally viewed through a regression framework, making such
tests somewhat superfluous.
The log-rank test is based on the same assumptions as the Kaplan-Meier
estimate, and does not make any assumptions about the underlying survival
distributions of the different groups. Having said that, the test does have higher
power to detect certain alternatives over others. It can be shown that the test
is asymptotically equivalent to the likelihood ratio test for alternatives from
distributions that satisfy proportional hazards, so it has higher power to detect
such alternatives. It is less likely to detect alternatives where survival curves
cross, since that cannot happen (for true survival curves) under proportional
hazards.
It is useful to check for proportional hazards by plotting log{−log[ ˆS(y)]}
versus log y for each group. This is a plot of the logged cumulative hazard
function versus logged time, and the plotted curves are consistent with
proportional hazards if they are parallel to each other. Curves that are roughly
straight are consistent with underlying Weibull distributions, with either the
same shape parameter (if the lines are parallel) or different shape parameters
(if they are not parallel).
11.3.1.3
Example — The Survival of Broadway Shows
The Broadway theater district, with its 41 professional theaters with 500 or
more seats, is the center of a billion-dollar entertainment industry, as well
as being one of the most popular tourist attractions in the world. Broadway
shows represent a business where sunk costs are high and consumer tastes are
unpredictable, making any understanding or the factors related to success of a
show very welcome.
This analysis is based on data describing the number of performances
(survival time) of Broadway shows that opened during the 2000-2009 time
period. The data come from Kulmatitskiy et al. (2015), building on earlier
work of Simonoff and Ma (2003) and Nygren and Simonoff (2007).
The Broadway show season is unusual in that the annual awards ceremony
of the most important awards (the Antoinette Perry, or Tony, Awards) results
in seasonal effects on the opening and closing of shows. The annual ceremony
takes place in June, with nominations in late April or early May. As a result,
many shows open in March or April, shortly before the nominations are
announced, while few shows open during the summer or winter months.
Further, shows that open in March or April almost always stay open until
the nominations are announced so they can potentially take advantage of an
attendance boost from being nominated. A similar pattern occurs during the
month between the nominations and the awards ceremony for shows that are
nominated for major awards, as shows stay open in hopes of the attendance

11.3 Methodology
217
boost a Tony win can bring. This affects the total number of performances in
ways that have little to do with the quality or commercial success of the show.
We avoid these idiosyncrasies by focusing here on the number of perfor-
mances after the Tony Awards ceremony for the 130 shows that were still open
at that time. Shows with predefined limited runs are not considered. Survival
time is measured as the number of performances to either closing or October
10, 2011, with six shows still open at that time being censored. Figure 11.3
gives Kaplan-Meier curves for the number of post-award performances of
shows separated by whether or not the show is a musical, whether or not it is a
revival of a previously-produced show, the number of major Tony Awards won
(best musical and best play [revival and non-revival], best director [musical
and play], leading actor and actress [musical and non-musical], and featured
actor and actress [musical and non-musical]), and the number of losing Tony
nominations in the major categories.
It is apparent that musicals have generally longer survival times than
non-musicals (plays), and non-revivals have longer survival times than revivals.
The latter pattern might be surprising, since it is natural to then wonder why
revivals are produced, but it should be recalled that only shows that were
still open after the Tony Awards ceremony are included here. If the total
number of performances is considered, thereby including shows that have
much shorter run times, the picture changes, since original productions
(non-revivals) are more likely to close quickly than are revivals. Indeed, while
12.5% of non-revivals closed after fewer than 50 performances, only 6.9% of
revivals did. Even more strikingly, 7.9% of non-revivals ran for fewer than
four weeks (32 performances), while only 2.8% of revivals closed that quickly.
The risks tend to eventually flip over, with non-revivals running longer in
general (that is, the effect of the variable changes over time), but apparently
the familiarity of a show audiences have seen before means that it is less
likely to crash (and close) immediately (and of course a show that does fail
spectacularly is very unlikely to be revived, so there is a natural weeding-out
process as well).
The situation for Tony Awards and nominations is not as straightforward.
Generally speaking more Tony Awards is associated with longer survival,
although the survival curves cross at various places. The pattern for losing
Tony nominations is even less clear: shows with zero losing nominations have
shortest survival and those with three losing nominations have longest survival,
but otherwise the relationship is quite muddled.
The log-rank statistics (and underlying observed and expected numbers
of closings, which are the “deaths” being modeled here) for the four variables
in Figure 11.3 are given in Table 11.1. All are statistically significant, with the
musical versus non-musical grouping most significant. The log-log cumulative
hazard plots in Figure 11.4 show that the proportional hazards assumption
might not be valid for that variable (the lines are not parallel), but the
clear separation in the survival curves as seen in Figure 11.3 means that the
difference in the survival distributions between musicals and non-musicals is
easy to see. The proportional hazards assumption seems more valid for the

218
CHAPTER 11 Models for Time-to-Event (Survival) Data
0
1000
2000
3000
4000
0.0
0.2
0.4
0.6
0.8
1.0
(a) Musical
Not musical
Musical
0
1000
2000
3000
4000
0.0
0.2
0.4
0.6
0.8
1.0
(b) Revival
Not revival
Revival
0
1000
2000
3000
4000
0.0
0.2
0.4
0.6
0.8
1.0
(c) Tony Awards
0
1
2
3
4
5
0
1000
2000
3000
4000
0.0
0.2
0.4
0.6
0.8
1.0
(d) Losing nominations
0
1
2
3
4
5
6
7
FIGURE 11.3: Kaplan-Meier curves of post-Tony Awards performances of
Broadway shows separated by various characteristics. (a) Whether or not a
show is a musical. (b) Whether or not a show is a revival. (c) The number
of major Tony Awards. (d) The number of losing Tony nominations in the
major categories.
revival indicator, supporting the use of the test to identify the shorter survival
of revivals.
The crossing survival curves in Figure 11.3 and non-parallel curves in
Figure 11.4 for the two awards variables makes it more difficult to summa-
rize the survival patterns. We can note from a comparison of the observed
and expected number of closings in Table 11.1 that shows with no major
Tony Awards had considerably more closings than expected, while those with
at least three Tony Awards had considerably fewer, reinforcing that more
Tonys is associated with longer survival (fewer closings). The pattern for
losing nominations is less clear, being driven mostly by zero losing nomina-
tions (more observed closings) and three losing nominations (fewer observed
closings).

11.3 Methodology
219
Table 11.1: Log-rank test statistics for Broadway show post-Tony Awards
performances data.
Variable
Observed
Expected
Test
df
p
Musical
Musical
63
88.2
Non-musical
61
35.8
34.8
1
3.7 × 10−9
Revival
Revival
42
30.4
Non-revival
82
93.6
6.3
1
0.012
Tony Awards
0
71
55.8
1
30
31.6
2
9
7.1
3
10
18.6
4
2
4.4
5
2
6.5
14.6
5
0.012
Losing nominations
0
21
10.3
1
24
20.7
2
33
27.7
3
23
38.6
4
16
18.4
5
4
4.3
6
1
1.5
7
2
2.7
21.8
7
0.003
11.3.2
PARAMETRIC (LIKELIHOOD) ESTIMATION
Without the presence of censored observations parametric fitting of the
accelerated failure time model (11.4) would be straightforward, with maximum
likelihood estimation and inference proceeding analogously to the fitting of
the generalized linear models in Chapters 8–10. This is adapted to data
with censored observations by recognizing that observed events (deaths)
contribute f(yi) to the likelihood function, while censored observations
contribute P(Y > yi) to it. Assuming random censoring, this means that the
log-likelihood has the form
L =

i:δi=1
log f(yi) +

i:δi=0
log S(yi)
=

i:δi=1
[log h(yi) + log S(yi)] +

i:δi=0
log S(yi)
=

i:δi=1
log h(yi) −
n

i=1
H(yi),

220
CHAPTER 11 Models for Time-to-Event (Survival) Data
5
20
50
200
1000
5000
−4
−3
−2
−1
0
1
(a) Musical
Not musical
Musical
5
20
50
200
1000
5000
−4
−3
−2
−1
0
1
(b) Revival
Not revival
Revival
5
20
50
200
1000
5000
−4
−3
−2
−1
0
1
(c) Tony Awards
0
1
2
3
4
5
5
20
50
200
1000
5000
−3
−2
−1
0
1
(d) Losing nominations
0
1
2
3
4
5
6
7
FIGURE 11.4: Logged cumulative hazards plots of post-Tony Awards perfor-
mances of Broadway shows separated by various characteristics. (a) Whether
or not a show is a musical. (b) Whether or not a show is a revival. (c) The
number of major Tony Awards. (d) The number of losing Tony nominations
in the major categories.
using the relationships between f(·), h(·), S(·), and H(·) noted earlier. So, for
example, if the survival times follow a Weibull distribution, the log-likelihood
from model (11.4) is
L =

i:δi=1
log yi −(β0 + β1x1 + · · · + βpxp)
σ
−log σ
	
−
n

i=1
exp
log yi −(β0 + β1x1 + · · · + βpxp)
σ
	
.
Model checking is again more difficult due to the presence of censored
observations. For example, several types of residuals have been suggested in
the literature. The so-called Cox-Snell residuals, which are just the estimated
cumulative hazard values ˆH(yi) based on the fitted model for each observation,
for example, are based on the fact that if
Y is a continuous nonnegative

11.3 Methodology
221
random variable with cumulative hazard function H(·), then the random
variable H(Y ) follows an exponential distribution with mean equal to 1.
These values are plotted versus their expected values under a unit exponential.
These values are not residuals in the usual sense, not being centered in any
way. The martingale residuals ˆmi = δi −ˆH(yi) correct this, as they equal
the difference between the observed value of a subject’s failure indicator and
its expected value, integrated over the time for which the observation was at
risk. A positive value means that the event time was shorter than expected,
while a negative value implies that the event time was longer than expected (or
was censored, as in that case δi = 0). Note that these residuals are asymmetric,
since they potentially range from −∞to 1, so a better choice for plotting is to
use the deviance residuals, as was discussed in Section 8.3.3.
As was mentioned in Section 11.3.1.2 in the discussion of the log-rank
test, log-log plots separated by groups of the cumulative hazard function
versus time identify an underlying Weibull survival distribution by straight
lines, with non-parallel lines indicating that the different groups have different
shape parameters. Fitting such a model can be accomplished by stratifying the
analysis by the levels of the grouping variable, which fits different distributions
for the different groups for the baseline random variable y0 in (11.3), allowing
for the possibility of different scale and shape values for each group while
keeping the effect of the (other) covariates the same across groups.
11.3.3
SEMIPARAMETRIC (PARTIAL LIKELIHOOD)
ESTIMATION
The most popular survival regression model is undoubtedly the semiparametric
Cox proportional hazards model, introduced by Cox (1972), which is based
on the relationship defined in (11.5). The model is a semiparametric one, in
that while the effect of the covariates on the hazard function has a specified
(exponential) form, the baseline hazard function h0(·) is not specified. By
not specifying the baseline hazard the likelihood function is undefined, which
precludes maximum likelihood estimation. The Cox model proceeds through
the use of the partial likelihood, which depends only on the parameters of
interest. If there are no tied event times the partial log-likelihood is
L =
k

i=1
{β1x1(i) + · · · + βpxp(i)
−log
⎡
⎣

j∈R(y(i))
exp(β1x1j + · · · + βpxpj)
⎤
⎦
⎫
⎬
⎭,
(11.9)
where x·(i) refers to the corresponding covariate values for the ith ordered
observed event time. This is treated as if it is a log-likelihood for estimation
and inference using asymptotic approximations. This includes the use of the
partial likelihood ratio test, the Wald test, or a third asymptotically equivalent
possibility, the score test, to assess statistical significance of the model and

222
CHAPTER 11 Models for Time-to-Event (Survival) Data
individual covariates (the score test for the significance of a Cox model fit on a
binary predictor is equivalent to the log-rank test). The Cox-Snell, martingale,
and deviance residuals also can be used to identify unusual observations
and model violations, although again the presence of censored observations
complicates matters.
The proportional hazards assumption is fundamental to the fitting of the
Cox model, making checking of that assumption obviously important. The
Schoenfeld residuals can help in this task. These residuals are estimates of the
contribution of an observation to the derivative of the partial log-likelihood.
They are only defined for uncensored observations, with a residual for each
predictor, and equal
ˆrik = xik −ˆxwik,
where
ˆxwik =

j∈R(y(i))xjk exp(ˆβ1x1j + · · · + ˆβpxpj)

j∈R(y(i)) exp(ˆβ1x1j + · · · + ˆβpxpj)
is the estimator of the mean of the predictor for those in the risk set. These
residuals are asymptotically uncorrelated and have expected value equal to zero
under the Cox model, so structure that appears in a plot of the residuals for a
particular variable versus time can uncover a time-varying coefficient of that
variable (in which the slope changes over time). Residuals that have been scaled
by the covariance of the slope estimates can be closer to normally distributed.
A χ2 test based on a weighted sum of squares of the scaled Schoenfeld residuals
is a score test for the hypothesis of a time-varying coefficient, which provides a
summarizing test of the evidence against proportional hazards. If proportional
hazards is violated for a categorical (grouping) variable, a natural alternative
is to fit a stratified proportional hazards model that stratifies on the levels
of that variable, as was discussed earlier for accelerated failure time models.
In this context this corresponds to different baseline hazard functions for the
different levels of the grouping variable.
The semiparametric nature of the Cox model makes prediction a challenge,
since the baseline hazard is unspecified. This formulation of the model in
terms of relative risk means that prediction is impossible without some way
of fully specifying the survival distribution for an individual with a specified
set of predictor values. This is typically done by first estimating the baseline
survival function, using (11.6) to estimate the survival function for a given
set of predictor values x0, and then reporting the median survival value. A
common estimator for the baseline survival is the Breslow estimator, which
generalizes the Nelson-Aalen cumulative hazard estimator (11.8) to account
for the covariates, and equals
ˆH0(y) =

i:y(i)<y
di

j∈R(y(i)) exp(β1x1j + · · · + βpxpj),

11.4 Example —The Survival of Broadway Shows (continued)
223
implying ˆS0(y) = exp[−H0(y)]. Note that there is no guarantee that the
estimated survival function will fall below 0.5, in which case the median value
is not defined.
Despite its great popularity, the proportional hazards model does have
weaknesses. The hazard ratio does not have a natural physical or substantive
interpretation in the way that accelerated failure time models do, as was
noted by Cox himself in Reid (1994). The empirical hazard ratio converges
to a quantity that is not only a function of the survival distribution, but
also depends on the censoring distribution, which is undesirable. Parametric
accelerated failure time models are also much more natural if prediction is a
goal, a point also noted in Reid (1994).
11.3.4
THE BUCKLEY-JAMES ESTIMATOR
An alternative semiparametric approach to survival modeling is the Buckley-
James estimator, which is based on the accelerated failure time model (11.4).
The parametric approach to fitting these models discussed in Section 11.3.2
requires specification of the underlying error distribution, but the Buckley-
James estimator avoids this by appealing to least squares. The problem of
censored observations is handled by substituting an estimate of the conditional
mean survival of an observation given that it was censored
E(log yi|δi = 0) = β0 + β1x1 + · · · + βpxp
+ σE(Zi|Zi > Ci −[β0 + β1x1 + · · · + βpxp])
for the censored value, basing the expectation on the Kaplan-Meier estimate of
the distribution of Z, and then performing least squares and iterating. Given
the basis on least squares with a logged response it would be expected that the
results of a Buckley-James fit would be broadly similar to that of a lognormal
parametric fit, in the same way a Cox model fit would be broadly similar to
that of a Weibull parametric fit. Smith (2002) provides further discussion and
extension of the Buckley-James estimator.
11.4
Example — The Survival of Broadway
Shows (continued)
In this section we explore fitting regression models for the number of
performances after the Tony Awards. In addition to the variables described
earlier, we also consider four noncategorical variables as potential predictors:
the average attendance at the show the first week after the Tony Awards as a
percentage of the total number of seats (this can be greater than 100% because
of standing room seats) and the review of the show in The New York Times, the

224
CHAPTER 11 Models for Time-to-Event (Survival) Data
40
50
60
70
80
90
100
2
3
4
5
6
7
8
(a)
First week attendance
Logged performances
2
3
4
5
2
3
4
5
6
7
8
(b)
NY Times rating
Logged performances
1
2
3
4
5
2
3
4
5
6
7
8
(c)
Daily News rating
Logged performances
1
2
3
4
5
2
3
4
5
6
7
8
(d)
USA Today rating
Logged performances
FIGURE 11.5: Scatter plots of logged number of post-Tony Awards per-
formances of Broadway shows versus various characteristics. Censored
observations are represented by ×. (a) Attendance in the first week after
the Awards. (b) New York Times review. (c) Daily News review. (d) USA Today
review.
New York Daily News, and USA Today, respectively, on a scale from 1 (worst)
to 5 (best). Figure 11.5 gives scatter plots of the logged number of post-Awards
performances versus each predictor, with censored observations represented
by ×. There is an apparent direct relationship between first week attendance
and number of performances, but there is little evidence of a relationship
between reviews and the number of performances.
The output below (from the survreg function in the survival
package of R) summarizes the fit of a Weibull accelerated failure time model
to the data.
Value Std. Error
z
p
(Intercept)
1.2378
0.72008
1.719 8.56e-02
Musical
1.0709
0.22581
4.742 2.11e-06
Revival
-0.5843
0.20434 -2.859 4.25e-03
Tony.awards
0.1310
0.09901
1.323 1.86e-01

11.4 Example —The Survival of Broadway Shows (continued)
225
Losing.nominations
0.1142
0.07857
1.454 1.46e-01
Att.post.award
0.0351
0.00643
5.455 4.90e-08
NYT.rating
-0.0532
0.12080 -0.440 6.60e-01
DN.rating
0.0493
0.09902
0.498 6.18e-01
USA.rating
0.1713
0.12678
1.351 1.77e-01
Log(scale)
0.0189
0.07114
0.266 7.90e-01
Scale= 1.02
Weibull distribution
Loglik(model)= -780
Loglik(intercept only)= -831.6
Chisq= 103.13 on 8 degrees of freedom, p= 0
The overall model is highly statistically significant, with likelihood ratio
test equaling 103.13 on 8 degrees of freedom. Wald tests indicate statistical
significance for the two variables with strong effects noted earlier, whether the
show is a musical and whether the show is a revival, and for the attendance
the first week after the Awards ceremony. Likelihood ratio tests have similar
implications. The other predictors are not statistically significant at a 0.10
level, which suggests simplifying the model. The model that minimizes AIC
(obtained through exhaustive search) adds the USA Today review score to the
other three variables:
Value Std. Error
z
p
(Intercept)
0.7713
0.57203
1.348 1.78e-01
Musical
1.1551
0.21273
5.430 5.64e-08
Revival
-0.5453
0.20445 -2.667 7.65e-03
Att.post.award
0.0393
0.00563
6.974 3.08e-12
USA.rating
0.2877
0.10931
2.632 8.49e-03
Log(scale)
0.0241
0.07172
0.336 7.37e-01
Scale= 1.02
Weibull distribution
Loglik(model)= -781.9
Loglik(intercept only)= -831.6
Chisq= 99.33 on 4 degrees of freedom, p= 0
Examination of a normal plot of the deviance residuals (Figure 11.6), how-
ever, uncovers a distinct outlier, the show Mama Mia!, with 4142 performances
as of the censoring date of October 10, 2011 (the show ultimately closed on
September 12, 2015 after 5773 performances, the ninth-longest running show
in Broadway history). The show is viewed as having an unusually long run
because of its mediocre review in USA Today, as its other characteristics (a
non-revival musical with 101.7% attendance the week after the Tony Awards)
are associated with a longer run.
The presence of the outlier apparently has an effect on the regression,
since after it is removed the model that minimizes AIC now includes both
the number of major Tony Awards and the number of losing nominations:
Value Std. Error
z
p
(Intercept)
1.3777
0.62013
2.222 2.63e-02
Musical
1.0568
0.20820
5.076 3.86e-07

226
CHAPTER 11 Models for Time-to-Event (Survival) Data
−2
−1
0
1
2
−2
−1
0
1
2
3
Theoretical Quantiles
Sample Quantiles
FIGURE 11.6: Normal plot of the deviance residuals from a Weibull regres-
sion fit on the Broadway post-Awards performance data.
Revival
-0.5121
0.19575 -2.616 8.89e-03
Att.post.award
0.0313
0.00616
5.073 3.92e-07
Tony.awards
0.1664
0.09162
1.816 6.93e-02
Losing.nominations
0.0957
0.06749
1.418 1.56e-01
USA.rating
0.2023
0.11397
1.775 7.59e-02
Log(scale)
-0.0146
0.07158 -0.204 8.38e-01
Scale= 0.986
Weibull distribution
Loglik(model)= -775
Loglik(intercept only)= -826
Chisq= 102.09 on 6 degrees of freedom, p= 0
The coefficients have the usual semilog interpretation. That is, given the
other predictors are held fixed, a show being a musical is predicted to
have e1.057 = 2.88 times the number of post-Awards performances as does a
non-musical. Similarly, given the other predictors are held fixed revivals are
estimated to have 40.1% shorter post-Awards runs, one additional percentage
point in first-week attendance is associated with 3.2% longer run, an additional
major Tony Award is associated with 18.1% longer run, each losing major
Tony nomination is associated with 1.1% longer run, and an additional point
in USA Today review rating is associated with 22.4% longer run. All of these
relationships are intuitive, and the fact that the only review variable that is
included in the model is from a national newspaper rather than either of

11.4 Example —The Survival of Broadway Shows (continued)
227
the two New York newspapers is also consistent with the fact that Broadway
theater is driven nowadays by tourism much more than by local attendance.
As was noted in Section 11.2.3, the Weibull distribution satisfies propor-
tional hazards, and it might be of interest to interpret the implications of the
model in terms of hazard rather than survival time. This is straightforward, as
there is a simple relationship between the regression coefficients in an acceler-
ated failure time formulation and in a proportional hazards formulation. The
weibreg function in the package eha package gives them directly:
Covariate
Coef Exp(Coef)
se(Coef)
Wald p
Musical
-1.072
0.342
0.224
0.000
Revival
0.520
1.681
0.200
0.010
Att.post.award
-0.032
0.969
0.007
0.000
Tony.awards
-0.169
0.845
0.093
0.068
Losing.nominations
-0.097
0.907
0.068
0.156
USA.rating
-0.205
0.814
0.117
0.080
log(scale)
1.378
3.966
0.620
0.026
log(shape)
0.015
1.015
0.072
0.838
Events
124
Total time at risk
43613
Max. log. likelihood
-774.98
LR test statistic
102
Degrees of freedom
6
Overall p-value
0
Let γj represent the slope of the jth predictor in the proportional hazards
formulation of the regression. Then ˆγj = −ˆβj/ˆσ, where ˆσ is the “Scale” value
given in the accelerated failure time output (in this case 0.986). Confusingly
enough, the inverse of this value (1.015) is given as the “shape” value in the
proportional hazards output. This latter terminology is actually more natural,
since this value corresponds to the estimate of the Weibull shape parameter α.
Both outputs give the Wald test for the hypothesis α = 1, or in other words
an exponential survival distribution, and in this case the test does not come
close to rejection (the AIC value for an exponential fit is also smaller than
that of the Weibull, reinforcing that the simplification of α = 1 is reasonable).
That is, a constant hazard is not rejected for these data, implying that for
shows that make it past the Tony Awards ceremony there is little evidence
of “aging” of a show. This is consistent with the observed pattern of shows
with a seemingly constant (low) risk of closing, which run for thousands
of performances, corresponding to runs that are many years long (these are
almost invariably big-budget [and often family-friendly] musicals).
Note that the Wald tests in the two formulations are not exactly the
same, so the p-values for a given predictor can be different, although it is
apparent here that the implications in the two formulations are identical.
Exponentiation of the proportional hazards slopes gives the estimated hazard
ratio for the predictors. So, for example, given the other variables are held
fixed, a musical has estimated 65.8% lower hazard of closing than does a

228
CHAPTER 11 Models for Time-to-Event (Survival) Data
−2
−1
0
1
2
−2
−1
0
1
2
3
Theoretical Quantiles
Sample Quantiles
FIGURE 11.7: Normal plot of the deviance residuals from a Cox regression
fit on the Broadway post-Awards performance data.
non-musical, while a revival has an estimated 68.1% higher hazard of closing
than does a non-revival.
A natural alternative to the proportional hazards formulation of the
Weibull model is of course the Cox proportional hazards model. The model
with minimum AIC includes the same variables as the Weibull choice, and
Mama Mia! is again an outlier according to the scaled deviance residuals
(Figure 11.7). When it is omitted the minimum AIC model does not change,
and it is clear that the output for the model is very similar to that of the
(proportional hazards form of the) Weibull model on page 227:
coef
se(coef)
z Pr(>|z|)
Musical
-1.037881
0.231531 -4.483 7.37e-06 ***
Revival
0.497290
0.201296
2.470
0.0135 *
Att.post.award
-0.028590
0.006725 -4.251 2.12e-05 ***
Tony.awards
-0.178659
0.094020 -1.900
0.0574 .
Losing.nominations -0.107614
0.069897 -1.540
0.1237
USA.rating
-0.173482
0.120633 -1.438
0.1504
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Likelihood ratio test= 85.73
on 6 df,
p=2.22e-16
Wald test
= 78.63
on 6 df,
p=6.883e-15
Score (logrank) test = 86.83
on 6 df,
p=1.11e-16

11.4 Example —The Survival of Broadway Shows (continued)
229
Both of these models assume proportional hazards, but an accelerated
failure time model based on a lognormal survival assumption does not, and
provides an alternative formulation to consider.
Value Std. Error
z
p
(Intercept)
1.5187
0.7099
2.140 0.032394
Musical
0.9479
0.2360
4.017 0.000059
Revival
-0.6256
0.2338 -2.676 0.007449
Att.post.award
0.0255
0.0067
3.807 0.000141
Tony.awards
0.3409
0.1099
3.103 0.001913
Losing.nominations
0.2001
0.0784
2.552 0.010706
USA.rating
0.0659
0.1490
0.442 0.658149
Log(scale)
0.2076
0.0639
3.247 0.001167
Scale= 1.23
Log Normal distribution
Loglik(model)= -780.6
Loglik(intercept only)= -817.4
Chisq= 73.68 on 6 degrees of freedom, p= 7.2e-14
Comparison to the output on pages 225–226 reveals very similar patterns for
the musical and revival indicators and post-Awards attendance predictor, but
also noticeable differences between the fits. The number of Tony Awards and
number of losing nominations have stronger effects in the lognormal model
(each award being associated with an estimated 40.6% longer survival holding
all else fixed, compared to an estimated 18.1% longer in the Weibull fit, and
each losing nomination associated with an estimated 22.2% longer survival,
compared to 9.6%), while the USA Today review is not at all predictive for
survival time.
Unsurprisingly, given the natural connection between a lognormal survival
assumption and the use of least squares with a logged survival response variable,
the Buckley-James fit is very similar to the lognormal fit:
Buckley-James Censored Data Regression
Discrimination
Indexes
Obs
129
Regression d.f.6
g
1.222
Events 124
sigma 1.2212
gr
3.393
d.f.
117
Coef
S.E.
Wald Z Pr(>|Z|)
Intercept
1.5786 0.7316
2.16
0.0309
Musical
0.9300 0.2386
3.90
<0.0001
Revival
-0.6122 0.2341 -2.62
0.0089
Att.post.award
0.0250 0.0067
3.71
0.0002
Tony.awards
0.3444 0.1129
3.05
0.0023
Losing.nominations
0.2027 0.0788
2.57
0.0101
USA.rating
0.0568 0.1555
0.37
0.7148

230
CHAPTER 11 Models for Time-to-Event (Survival) Data
The Weibull fit is favored over the lognormal fit based on AIC (its
value is 11 lower for the Weibull fit compared to the lognormal fit), but
there is evidence of lack of fit in the corresponding Cox model fit. Score
tests for nonproportional hazards are at least marginally statistically significant
for several predictors. A possible remedy for this is to construct a stratified
proportional hazards model, in which the baseline hazard function is allowed
to be different for each level of a variable or variables, thereby allowing for
nonproportionality of hazards. Stratifying on whether a show is a musical or
not is reasonable, but given the number of values many of the other predictors
exhibiting nonproportionality take on, this approach seems inadvisable in
general here.
Plots of the scaled Schoenfeld residuals for each predictor (Figure 11.8)
show evidence of coefficients that vary with time for all variables other than
the revival indicator, another way the proportional hazards assumption can be
violated (smooth curves like the ones superimposed on the plots are discussed
in Chapter 15). A full discussion of models that account for coefficients that
vary with survival time is beyond the scope of this discussion, but the curves in
Figure 11.8 provide some guidance as to potential causes of nonproportional
hazards. The plot indicates generally decreasing coefficients (implying lower
hazard) for the musical indicator and USA Today review rating, consistent
with much longer survival for tourist-friendly musicals (which is in line with
the seven shows in the data with more than 1800 performances, Avenue Q,
Hairspray, Jersey Boys, Mamma Mia!, Mary Poppins, The Producers, and
Wicked). The coefficient for the attendance the first week after the Tony
Awards decreases rapidly up to 50 performances (corresponding to roughly
six weeks) but then levels off, suggesting only a limited-time effect of positive
“buzz” about a show. The coefficients for Tony nominations (both winning
and losing) generally increase with time (although not monotonically), which
would imply that eventually such honors eventually lose their positive effects
on survival.
11.5
Left-Truncated/Right-Censored Data
and Time-Varying Covariates
11.5.1
LEFT-TRUNCATED/RIGHT-CENSORED DATA
The methods discussed thus far are all based on the assumption that event time
is measured from the time a subject enters a study, but that need not be the
case. Rather, it is sometimes the case that subjects are available for observation
a known time after the natural origin of the process being studied; that is,
it is known from the start that the ultimate event time must be greater than
some known value for that observation (with different observations having
potentially different values). Such left-truncation mainly occurs under two
(related) circumstances:

11.5 Left-Truncated/Right-Censored Data and Time-Varying Covariates
231
5 10 20
50 100
500
2000
−6
−4
−2
0
2
4
6
(a) Musical
Time
Beta(t) for Musical
5 10 20
50 100
500
2000
−2
0
2
4
(b) Revival
Time
Beta(t) for Revival
5 10 20
50 100
500
2000
−0.2
−0.1
0.0
0.1
0.2
(c) First week attendance
Time
Beta(t) for Post−Awards attendance
5 10 20
50 100
500
2000
−2
−1
0
1
2
3
(d) Tony Awards
Time
Beta(t) for Tony Awards
5 10 20
50 100
500
2000
−2
−1
0
1
2
(e) Losing Tony nominations
Time
Beta(t) for Losing Tony nominations
5 10 20
50 100
500
2000
−3
−2
−1
0
1
2
(f) USA Today rating
Time
Beta(t) for USA Today rating
FIGURE 11.8: Scatter plots of scaled Schoenfeld residuals for a Cox model
fit on the Broadway post-Awards performance data versus time, with a smooth
curve superimposed. (a) Musical. (b) Revival. (c) Attendance in the first week
after the Awards. (d) Number of Tony Awards. (e) Number of losing Tony
nominations. (f) USA Today review.

232
CHAPTER 11 Models for Time-to-Event (Survival) Data
• The event time T is the age of the subject at death and people are not
observed from birth but rather from some other time L corresponding to
their entry into the study. So, for example, in a mortality study the actual
lifetime length is of interest (not the time after a subject enters the study),
but people arrive in the study at various ages.
• T is measured from some landmark, but only subjects who experience
some intermediate event at time L are included in the study. So, for
example, subjects are followed after they have completed a treatment
regimen, but it is time from initial diagnosis that is of interest. Since only
subjects who complete the treatment regimen are eligible for inclusion,
they are subject to delayed entry.
Thus, the observed survival time for observation i is known to be at least Li, but
it can be right-censored as well, resulting in left-truncated/right-censored
(LTRC) data. Let the triple (Li, yi, δi) denote the ith LTRC observa-
tion, where Li is the left-truncation time (which is zero when there is no
left-truncation of that observation), yi is the observed survival time/censored
time and δi is the event/censoring indicator. In addition to assumptions on
the censoring process (such as random censoring), it is now assumed that the
truncation time Li is independent of the event time Ti, in order to assure that
those who enter at time L are a random sample of those in the population
who are still at risk at time L.
Adaptation of the Kaplan-Meier estimator and the Cox proportional
hazards model to LTRC data is straightforward, involving two modifications.
Since a particular observation has not necessarily been observed at a given
time y, the risk set at that time must be redefined to only include observations
that have not failed before time y and have been observed before that time.
This means that any probability statement must be defined as a conditional
probability, conditioning on the event of being in the observed sample. Thus,
the modified Kaplan-Meier estimate is
ˆS(y, ℓ) = ˆP(Y > y |Y ≥ℓ) =

i:y(i)<y

1 −di
ni

, y ≥ℓ, ℓ≥Lmin,
where ni is now the number of subjects alive just before y(i) who have
come under observation and Lmin = min Li; if all of the observations are
left-truncated (i.e., Lmin > 0) this is necessarily different from the quantity
estimated by the usual Kaplan-Meier estimate (which effectively takes ℓ= 0).
In a corresponding way, the partial log-likelihood under the Cox proportional
hazards model has the same form as in (11.9), except that now the risk
set R(y(i)) is defined as the set of observations j such that yj ≥y(i) ≥Lj
(it is this easy modification of the risk set that allows direct application of
the Kaplan-Meier estimator and the Cox model to LTRC data). The slope
coefficients are defined in the same way as always (as hazard ratios).

11.5 Left-Truncated/Right-Censored Data and Time-Varying Covariates
233
11.5.2
EXAMPLE — THE SURVIVAL OF BROADWAY SHOWS
(CONTINUED)
All of the analyses of Broadway show survival thus far have taken the number
of post-Tony Awards performances as the event time of interest, but this is
not necessarily the case. Considering that a Broadway producer faces the same
kinds of costs and same sources of revenue before the awards as after them,
he or she might be interested in analysis of the total number of performances
of a show, given the show does not close before the Awards ceremony. That
is, the response variable for each show that did not close before the ceremony
is the total number of performances rather than the number of post-Awards
performances, and this value is truncated from below by the number of
performances that occurred before the ceremony. The model that minimizes
AIC does not include the USA Today review score, but the next-best model
does, and the associated output can be compared to that on page 228.
coef
se(coef)
z Pr(>|z|)
Musical
-0.854763
0.230366 -3.710 0.000207 ***
Revival
0.435555
0.202586
2.150 0.031557 *
Att.post.award
-0.029063
0.006692 -4.343
1.4e-05 ***
Tony.awards
-0.150726
0.094246 -1.599 0.109758
Losing.nominations -0.118989
0.070374 -1.691 0.090873 .
USA.rating
-0.103715
0.125165 -0.829 0.407314
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Likelihood ratio test= 77.54
on 6 df,
p=1.155e-14
Wald test
= 74.76
on 6 df,
p=4.308e-14
Score (logrank) test = 83.21
on 6 df,
p=7.772e-16
The implications of the two model fits are very similar to each other. The
musical and revival indicators seem to have predictive power, although less
so than before, while the USA Today review rating is not close to statistical
significance. The two Tony Awards-related variables have smaller p-values
than when modeling the number post-Awards performances. The estimated
slopes for this model are generally smaller in absolute magnitude, indicating
slightly weaker effects, with the largest difference being that for the musical
indicator (the model implies an estimated 57.5% lower hazard of closing for
the total number of performances for musicals given the other variables, while
the earlier fit implies an estimated 64.6% lower hazard of closing for the
number of post-Awards performances given the other variables).
11.5.3
TIME-VARYING COVARIATES
A very important aspect of LTRC data is its connection with the analysis of
data with time-varying covariates. Such data, in which values of a covariate

234
CHAPTER 11 Models for Time-to-Event (Survival) Data
Table 11.2: Time-varying covariates survival data.
Patient.ID
Age
CD4
Time
Death (δ)
1
45
27
0
0
1
45
31
10
0
1
45
25
20
0
1
−
−
27
1
Patient.ID
Age
CD4
Start
End
Death (δ)
1
45
27
0
10
0
1
45
31
10
20
0
1
45
25
20
27
1
The top part of the table gives the original form of the data, while the bottom part gives
it as LTRC pseudo-subjects.
x(t) change over time and the hazard at that time changes accordingly, are
common in many circumstances. Examples include subject-specific variables,
such as age or potential risk factors like adherence to medication protocols, and
variables that apply to all subjects, such as study or environmental variables, or
time itself. Of course, it is crucial that the definition of time (and in particular
the zero value of time) that is used for these variables be consistent across
subjects.
Consider the hypothetical survival data given in the top of Table 11.2.
In it the subject has multiple records of measurements of risk factors during
multiple visits, consisting of three records. The event–death is observed at
time 27, while the three measurements of Age and CD4 are recorded at the
beginning and times 10 and 20 respectively. The data are reformatted in
the bottom of Table 11.2, where each row (a pseudo-subject) now becomes
left-truncated (at time Start) and right-censored/event (at time End) data. The
principle is to find intervals such that covariates do not change values inside
each interval. If x(t) is changing continuously, infinitely many intervals would
be needed to represent data this way; in practice, however, x(t) is typically
not monitored all of the time, but instead occasionally, such as when patients
are visiting a hospital or clinic. This means that in practice the time-varying
covariates are assumed constant between visits, even though this is often
unrealistic. This is called the Andersen-Gill method of processing such data
(Andersen and Gill, 1982). Note that although the covariate values change
with time the coefficients do not. If we fit the reformatted data in Table 11.2
using a Cox proportional hazards model we apparently get a time-varying
covariates fit, but this ignores that the pseudo-subjects are being treated as
independent when they are not. It turns out that this is not a problem, since

11.5 Left-Truncated/Right-Censored Data and Time-Varying Covariates
235
at any time point only one of the pseudo-subjects is being used in the partial
likelihood equations.
11.5.4
EXAMPLE — FEMALE HEADS OF GOVERNMENT
The 2016 U.S. presidential election was noteworthy for many reasons, one of
which being that it was the first time a woman was the candidate for president
of a major party. Of course, Donald Trump was elected president, not Hillary
Clinton, so the United States, like many other countries, has still not had a
female head of state. In trying to understand the process driving the chances
of a woman becoming the head of government of a country, it is reasonable
to think that those chances for a particular country at any given time could
change as a result of changing conditions in that country. That is, the amount
of time it takes for a country to have a female head of government can be
modeled as a survival problem with time-varying covariates.
In this analysis data are obtained from the World Bank for 174 countries
that existed in 1959 or came into existence since then. This year is used as
a reference point since the first female head of government of the modern
world was Sirimavo Bandaranaike, who became Prime Minister of Ceylon
(now Sri Lanka) in 1960. Countries that superseded earlier countries replace
them in the data (so, for example, Yugoslavia does not appear, but Serbia,
Croatia, Montenegro, etc., do, starting with their year of formation). Event
time is defined as the year a woman first became head of government in a
country (technically, the number of years after 1959), with countries without
such an event being censored at 55 years (the year 2014). Those countries
constitute 73% of the sample. Note that event time is being treated as the
actual year, rather than the number of years after formation of the country
for those countries that were formed after 1959, since it seems that general
world views on women in politics would be more relevant for changes in
the chance of a woman becoming head of government than how long the
country has been in existence. For countries formed after 1959 the event time
is left-truncated at the year of formation of the country. Covariates examined
include birth rate (births per 1000 people), death rate (deaths per 1000 people),
fertility rate (births per woman), (natural) logged per capita gross domestic
product (GDP), infant mortality rate (per 1000 live births), and percentage of
the population that is female, which are time-varying, and the continent in
which the country falls and the primary religion of the country (categorized as
Christianity, Islam, or Other), which are not.
Figure 11.9 gives Kaplan-Meier plots of the number of years after 1959
of the first female head of government, separated by the continent of the
country and the predominant religion of the country, respectively. There does
not appear to be any relationship between continent and the year of the first
female head of government, but there does appear to be a religion effect.
We see that countries that are predominantly Christian have generally earlier
years of the first female head of government (the first being Argentina in
1974) compared to countries that are predominantly Islamic (the first being

236
CHAPTER 11 Models for Time-to-Event (Survival) Data
0
10
20
30
40
50
0.0
0.4
0.8
Continent
Africa
Asia
Europe
North America
Oceania
South America
0
10
20
30
40
50
0.0
0.4
0.8
Religion
Christianity
Islam
Other
FIGURE 11.9: Kaplan-Meier plots of the number of years after 1959 of the
first female head of government separated by continent of the country and
predominant religion of the country, respectively.
Pakistan in 1988), with 34% of the predominantly Christian countries in the
sample having had a female head of government by 2014 but only 13% of the
predominantly Islamic countries in the sample having had one. The “Other”
category is noticeably different, characterized by early years of female heads of
government (Sri Lanka, India, and Israel in the 1960s) followed by more than
35 years until the next one.
There is no straightforward way to graphically represent the relationship
between continuous time-varying covariates and survival time, but the plots
in Figure 11.10 can provide some guidance. These side-by-side boxplots plot
each of the country-year pairs in the sample, separating potential numerical
covariates on the basis of whether a first female head of government occurred
in that country-year. In this way we can see that the first years of a female
head of government are characterized by country-years with lower birth rate,
lower death rate, lower fertility rate, lower infant mortality rate, and higher
per capita GDP, all reflecting more highly industrialized societies. It is also
striking that no country had its first year of female head of government with a
female population percentage less than 47.49%.
A best subsets Cox model analysis focuses on the model including birth
rate, death rate, and religion as the minimizer of AIC. The following output
(with religion fit using effect codings) corresponds to this fit:
coef
se(coef)
z Pr(>|z|)
Birth.rate
-0.080576
0.027462 -2.934
0.00335 **
Log.GDP.pc
-0.223672
0.185394 -1.206
0.22764
Infant.mortality.rate
0.015413
0.008876
1.737
0.08247 .
Christianity
0.688388
0.272311
2.528
0.01147 *
Islam
-0.195695
0.346917 -0.564
0.57269
Other
-0.492693
0.424150 -1.162
0.24540
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Likelihood ratio test = 15.3
on 5 df,
p=0.009

11.5 Left-Truncated/Right-Censored Data and Time-Varying Covariates
237
0
1
10
30
50
(a) Birth rate
Female head of government
Birth rate
0
1
0 10
30
50
(b) Death rate
Female head of government
Death rate
0
1
2
4
6
8
(c) Fertility rate
Female head of government
Fertility rate
0
1
5
7
9
11
(d) Logged per capita GDP
Female head of government
Logged GDP p/c
0
1
0 50
150 250
(e) Infant mortality rate
Female head of government
Infant mortality rate
0
1
25
35
45
55
(f) Percentage population female
Female head of government
Percentage female
FIGURE 11.10: Side-by-side boxplots of country-year characteristics sepa-
rated by whether the pair corresponded to the first year of a female head of
government in that country. (a) Birth rate. (b) Death rate. (c) Fertility rate.
(d) Logged per capita GDP. (e) Infant mortality rate. (f) Percentage of the
population that is female.
Wald test
= 14.24
on 5 df,
p=0.01
Score (logrank) test
= 14.58
on 5 df,
p=0.01
The regression is not very strong, but several patterns do emerge. As noted
earlier, a predominantly Christian population is associated with a greater
chance of a female first becoming head of government (the “hazard” of this
occurring in a given year being 99.1% higher than an overall level given birth
rate, logged GDP, and infant mortality rate), while a predominantly Islamic
population or other religious affiliation is associated with a smaller chance
(a hazard that is 17.8% lower and 38.1% lower, respectively, than an overall
level). A higher birth rate of one birth per 1000 people is associated with a 7.7%
lower hazard of a female first becoming head of state given the other variables,
consistent with the marginal relationship noted earlier. The slope for logged
GDP and infant mortality rate, however, imply conditional relationships

238
CHAPTER 11 Models for Time-to-Event (Survival) Data
opposite from the apparent marginal ones (given the other variables a 1%
higher per capita GDP is associated with a 0.22% lower hazard of first female
head of government, and a higher infant mortality rate of one death per 1000
live births is associated with a 1.6% higher hazard, respectively). It is not
apparent why this would be the case.
11.6
Summary
In this chapter we have discussed various approaches to the regression analysis
of survival (time-to-event) data, including parametric (accelerated failure
time) and semiparametric (Cox proportional hazards) models. These models
are characterized by the need to address potential right censoring (in almost all
situations), and other complicators of such analyses also include left-truncated
data and data with time-varying covariates. We have necessarily only given the
highlights of survival modeling here, leaving out a good amount of detail. We
have also omitted discussion of many special survival analysis topics, including
interval-censored data, in which failure times are only known to occur within
a given interval (Bogaerts et al., 2017), time-varying coefficient models, in
which the effects of covariates change over time, competing risks data, in
which the occurrence of one type of event prevents the possibility of the event
of interest occurring, recurrent events, in which the event of interest can occur
more than once for a given individual, and unmodeled heterogeneity from the
omission of important risk factors. There are many book-length treatments of
the analysis of survival data that discuss these topics, including Hosmer et al.
(2008), Lee and Wang (2013), and Moore (2016).
KEY TERMS
Accelerated failure time model: A model that hypothesizes a multiplicative
effect of covariates on the expected survival time.
Breslow estimator: An estimator of the baseline survival function in a
Cox proportional hazards model. It is a generalization of the Nelson-Aalen
estimator, which accounts for the effects of regression covariates.
Buckley-James estimator: A semiparametric estimator that uses least squares
to fit an accelerated failure time model by substituting an estimate of the
conditional mean survival of a censored observation given it was censored for
the censored value.
Censoring: A situation in which a survival time is only incompletely observed.
Right-censoring, the most common type, corresponds to the knowledge that
a survival time is at least a given value.

11.6 Summary
239
Cox proportional hazards model: A semiparametric model based on a
proportional hazards formulation, in which the baseline hazard is unspecified,
and estimation and inference process using the partial likelihood function.
Hazard function: The instantaneous rate of failure at a given time given that
the individual survives to that time as a function of survival time.
Kaplan-Meier product limit estimator:
The nonparametric maximum
likelihood estimator of the survival function. Estimation is based on the
product of estimates of the conditional probability of survival through an
ordered observed failure time given survival through the previous ordered
observed failure time, for observed failure times less than the survival time of
interest.
Left-truncation: A situation in which it is known that the survival time
cannot be less a given value.
Log-rank test: A nonparametric test comparing observed survival curves of
several groups to test whether the underlying true survival functions are equal
to each other.
Nelson-Aalen estimator: A nonparametric estimator of the cumulative hazard
function. It is similar in construction to the Kaplan-Meier estimator of the
survival function, and the two estimators are asymptotically equivalent to each
other.
Proportional hazards model: A model that hypothesizes a multiplicative
effect of covariates on the underlying hazard function.
Random censoring: A censoring mechanism whereby the observed survival is
the minimum of a true failure time and a censoring time that are independent
of each other.
Risk set: The set of individuals who have not failed just before a given time,
and are therefore at risk for failure at that time.
Stratified proportional hazards model: A proportional hazards model in
which the baseline hazard function is allowed to be different for each level of
a variable or variables.
Survival function: The probability of surviving past a particular time as a
function of survival time. It is equal to 1 −F(y), where F(·) is the cumulative
distribution function of the survival distribution.
Time-varying covariate: A covariate in a survival regression model that
potentially takes on different values at different times for a given individual.

Part Five
Other Regression Models

Twelve
Chapter
Nonlinear Regression
12.1
Introduction
243
12.2
Concepts and Background Material
244
12.3
Methodology
246
12.3.1 Nonlinear Least Squares Estimation
246
12.3.2 Inference for Nonlinear Regression Models
247
12.4
Example — Michaelis-Menten Enzyme Kinetics
248
12.5
Summary
252
12.1
Introduction
In all of the models discussed in the previous chapters the model parameters
entered linearly into the model. In those situations when it was believed that
this was inappropriate we considered the use of a transformation to linearize
the model. There are occasions, however, when that cannot (or should not)
be done. For example, consider pharmacokinetic models, which are designed
to model the course of substances administered to a living organism, such as
drugs, nutrients, and toxins. A simple model for the concentration (C) in the
blood of a substance as a function of time is a one-compartment model, which
implies exponential decay,
E[C(t)] = θ1eθ2t.
(12.1)
This model is consistent with assuming that a substance in the blood is in
rapid equilibrium with the substance in other tissues, with the rate of change
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
243

244
CHAPTER 12 Nonlinear Regression
of concentration being directly proportional to the concentration remaining.
That is, once introduced the substance mixes instantaneously in the blood
and distributes throughout the body rapidly. This is simply the semilog model
(4.5), and is of course linearizable, since
log{E[C(t)]} = log θ1 + θ2t.
Many substances, however, do not follow this pattern. Instead, the course
of the substance can be thought of as being consistent with the body being
made up of two compartments: the vascular system, including the blood, liver,
and kidneys, where it is distributed quickly, and poorly perfused tissues, such
as muscle, lean tissue, and fat, where the substance is eliminated more slowly.
Such a process is consistent with a two-compartment model,
E[C(t)] = θ1eθ2t + θ3eθ4t.
(12.2)
This model is not linearizable by a transformation, so nonlinear regression
methods are required to fit it.
Another example is the logistic model for population growth, sometimes
referred to as the Verhulst-Pearl model, which is based on a differential
equation. Let P(t) be the population at time t, P0 the population at the base
period, M the limiting size of the population (termed the carrying capacity),
and k the growth rate. Ignoring random error for the moment, under the
assumption that the rate of population change is proportional to both the
existing population and the amount of available resources (the room to grow),
the population satisfies the differential equation
dP
dt = kP

1 −P
M

.
The solution to this differential equation is
P(t) =
MP0 exp(kt)
M + P0[exp(kt) −1],
which cannot be linearized by a simple transformation. There are many pro-
cesses in the physical sciences that arise from nonlinear differential equations,
and in general nonlinear regression methods are required to fit them.
In this chapter we discuss the nonlinear regression model. Even though
the model is based on least squares, the nonlinearity of the model means that
the derivation and properties of inferential methods and techniques are similar
to those of Chapters 8 through 10.
12.2
Concepts and Background Material
The nonlinear regression model satisfies
yi = f(xi, θ) + εi.
(12.3)

12.2 Concepts and Background Material
245
The errors ε are taken to be independent and identically normally distributed,
which justifies the use of least squares. The linear model (1.1) is a special case
of this model, but (12.3) is obviously far more general. Note that x is a p × 1
vector, while θ is a q × 1 vector, and p need not equal q (for example, in the
two-compartment pharmacokinetic model (12.2), p = 1 and q = 4).
There are four key distinctions between nonlinear regression and the
linear regression models discussed in earlier chapters, two of which can be
viewed as advantages and two of which can be viewed as disadvantages.
1. Nonlinear regression is more flexible than linear regression, in that the
function f need not be linear or linearizable. If the relationship between
the expected response and the predictor(s) is not linear or linearizable,
nonlinear regression provides a tool for fitting that relationship to the
data. The only formal requirement on f is that it be differentiable with
respect to the elements of θ, which implies the existence of the least
squares estimates.
2. Nonlinear regression can be more appropriate than the use of transforma-
tions and linear regression in situations where f is linearizable. The reason
for this is the additive form of the nonlinear model (12.3). Consider
again the one-compartment semilog pharmacokinetic relationship (12.1).
Fitting this relationship via the nonlinear regression model
Ci = θ1eθ2ti + εi
is consistent with a relationship exhibiting constant variance. In contrast,
the semilog regression model based logging the concentration variable,
log(Ci) = β1 + β2ti + εi
is equivalent to a relationship for Ci that exhibits nonconstant variance,
since it implies multiplicative rather than additive errors:
Ci = eβ1 × eθ2ti × eεi
(this was noted earlier in Section 4.1). If the actual relationship does not
exhibit heteroscedasticity, it is quite possible that fitting the model as a
linear model for the logged concentration will result in a poor fit, since
taking logs will induce heteroscedasticity that was not originally there.
Further, the additive form (12.3) allows for the possibility of negative
values of y even if f only takes on positive values; in contrast the logs
of nonpositive values are undefined, so observations with y ≤0 would
simply be dropped from a linear regression fit for log y.
3. Nonlinear regression requires knowledge of f before fitting proceeds,
which implies a thorough understanding of the underlying process being
examined. Linear regression models are often viewed as exploratory, being
appropriate when a relationship between the response and the predictor(s)
is suspected but not precisely specified; the use of residual plots to suggest
potential transformations is consistent with that exploratory nature.
Nonlinear regression, on the other hand, requires precise specification of

246
CHAPTER 12 Nonlinear Regression
the relationship between the response and the predictor(s), which might
be difficult or impossible.
4. If f is misspecified the fit of the regression can be extremely poor. Indeed,
depending on the form of f, it is possible that the nonlinear regression
can fit worse than no regression at all (that is, fit worse than using Y to
estimate all values of y). The price that is paid for the flexibility of f noted
above is a lack of robustness to a poor choice of f.
12.3
Methodology
Nonlinear regression methodology is based on the same principles of maximum
likelihood described in Section 8.3.1. If the errors ε are Gaussian, maximum
likelihood corresponds to least squares, and estimation of θ proceeds using
nonlinear least squares.
12.3.1
NONLINEAR LEAST SQUARES ESTIMATION
As was true for linear models, nonlinear least squares is based on minimizing
the sum of squares of the residuals,
S =
n

i=1
[yi −f(xi, θ)]2.
The minimizer of S, ˆθ, satisfies the set of q equations setting the partial
derivatives with respect to each entry of θ equal to zero, but unlike for linear
models it is not straightforward to find that value. Estimation and inference
can proceed through the use of a (first-order) Taylor series approximation,
wherein the function f is approximated at a particular value θ0 using a linear
function,
f(xi, θ) ≈f(xi, θ0) +
q

j=1
∂f(xi, θ0)
∂θ0
j
(θj −θ0
j).
(12.4)
Using this approximation for f in S turns the minimization problem
into an (approximately) linear one, with the partial derivatives ∂f(xi, θ0)/∂θ0
j
taking the place of the columns of X in linear regression. This is an iterative
process, in which a current estimate of θ is used as θ0 in (12.4) and the residual
sum of squares S based on this linear approximation is minimized, with the
minimizer becoming the new estimate of θ. This process is continued until
a stopping criterion is met, such as a small change in S or a small change
in the estimated θ. There is no guarantee that this procedure will converge
to the global minimizer of S; indeed, not only might it converge to a local
minimizer, it can even converge to a local maximizer. A good starting value of
θ increases the chance of finding the true minimizer, and will also reduce the
number of iterations needed for convergence, but still provides no guarantees.

12.3 Methodology
247
For this reason, it is a good idea to try different sets of starting values to see if
the same solution is obtained.
Poor starting values far from the true solution are more likely to be
problematic, so putting some thought into which values to try is worthwhile.
For example, if a model is linearizable, the corresponding values based on a
linear fit to transformed variables is a very reasonable candidate. Sometimes a
special case of a model (taking particular values of θj equal to special values
such as 0 or 1, for example) is linearizable, and estimates based on the linear
fit to that special case can be worth using as starting values. It is sometimes
the case that parameters have physical interpretations that can be exploited to
choose starting values. For example, in the Michaelis-Menten model discussed
in Section 12.4, one of the parameters represents a theoretical maximum limit
for the response, so the observed maximum is a reasonable starting value to
use for that parameter.
12.3.2
INFERENCE FOR NONLINEAR REGRESSION MODELS
The Taylor series approximation used when estimating θ is also the basis of
large-sample inference for nonlinear regression models. Hypothesis tests and
confidence intervals are based on the linear approximation, with the partial
derivative evaluated at the least squares estimate ∂f(xi, ˆθ)/∂ˆθj taking the
place of the jth predictor column. Since the linear approximation is only
valid for large samples, approximate (Wald) tests and intervals for entries of θ
should perhaps be based on a normal distribution rather than a t-distribution,
although many packages use the t-distribution. Nested models (where one
is a special case of the other) can be compared using a partial F-test, which
approximately follows an F-distribution. Diagnostics such as standardized
residuals, leverage values, Cook’s distances, and variance inflation factors can
also be based on the Taylor series linear approximation, although this is
not always available in statistical packages that provide nonlinear regression
fitting. Note that this asymptotic approach is similar in spirit to the standard
inferential methods for generalized linear models discussed in Chapters 8
through 10.
On the other hand, some concepts from linear regression do not translate
in a straightforward way to nonlinear regression. Since variables do not have
a one-to-one correspondence with parameters, they often cannot be omitted
without fundamentally changing the form of f, meaning that variable selection
is not necessarily meaningful. Collinearity is as serious a problem in nonlinear
regression as it is in linear regression, resulting in instabilities in estimated
coefficients. Since it corresponds to high correlations among the vectors of
partial derivatives, however, it is not possible to simply omit predictors to
reduce collinearity. Rather, it is necessary to completely reformulate the model.
Simonoff and Tsai (1989) describe how observed collinearities can be used to
suggest an underlying partial differential equation that can be solved to yield
a reformulation that does not exhibit collinearity, while still providing a good
fit to the data. An example is provided in Niedzwiecki and Simonoff (1990),

248
CHAPTER 12 Nonlinear Regression
who showed that the two-compartment model (12.2) under collinearity is
well-fit by the one-compartment model (12.1).
12.4
Example — Michaelis-Menten Enzyme
Kinetics
The Michaelis-Menten model is a model for enzyme kinetics that relates the
rate of an enzymatic reaction (the “velocity” V ) to the concentration x of the
substrate on which the enzyme acts. The model takes the form
V (x, θ) =
θ1x
θ2 + x
(ignoring the error term). For this model θ1 represents the asymptotic
maximum reaction rate as the substrate concentration x →∞, while θ2 is the
so-called Michaelis constant, which is an inverse measure of how quickly the
rate approaches θ1 (a smaller value of θ2 implies that the rate is approached
more quickly, indicating a higher affinity of the substrate for the enzyme).
Figure 12.1 gives scatter plots and model fits based on data from Bates
and Watts (1988). The data come from an experiment relating the velocity
of reaction (in counts per minute2 of radioactive product from the reaction)
to substrate concentration (in parts per million) for an enzyme treated with
the antibiotic puromycin. In order to fit the Michaelis-Menten model to data,
first starting values for the algorithm must be found. The model is linearizable,
since
1
V = θ2 + x
θ1x
= 1
θ1
+
θ2
θ1
  1
x

≡β1 + β2
1
x.
(12.5)
This means that a linear regression of 1/V on 1/x gives (initial) estimates for
θ, since θ1 = 1/β1 and θ2 = β2/β1.
Figure 12.1(a) gives a scatter plot of 1/V versus 1/x, with the linear
least squares line superimposed on the plot. It is clear that an OLS fit to
the transformed data is not appropriate, as the transformation has induced
strong heteroscedasticity. Still, the fitted least squares line (ˆβ1 = 0.0051072,
ˆβ2 = 0.0002472) does not look unreasonable.
Despite this, when the linearized model is back-transformed into a fit
for velocity [ˆθ1 = 195.80, ˆθ2 = 0.0484) the fit is poor for high concentrations
(Figure 12.1(b)]. These values still provide good starting values for a nonlinear
least squares fit, however, which converges quickly (in five iterations):
Parameters:
Estimate Std. Error t value Pr(>|t|)
theta1 2.127e+02
6.947e+00
30.615 3.24e-11 ***
theta2 6.412e-02
8.281e-03
7.743 1.57e-05 ***
---

12.4 Example — Michaelis-Menten Enzyme Kinetics
249
0
10
20
30
40
50
0.005
0.015
(a)
1 / Concentration
1 / Velocity
0.0
0.2
0.4
0.6
0.8
1.0
50 100 150 200
(b)
Concentration
Velocity
0.0
0.2
0.4
0.6
0.8
1.0
50 100 150 200
(c)
Concentration
Velocity
FIGURE 12.1: Scatter plots and model fits for the puromycin enzyme velocity
data. (a) Scatter plot of inverse velocity versus inverse concentration, with
linear least squares line superimposed. (b) Scatter plot of velocity versus
concentration with fitted model based on the linear least squares model on
transformed variables. (c) Scatter plot of velocity versus concentration with
fitted models based on the nonlinear least squares fit using all of the data (solid
line) and omitting an outlier (dashed line).
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 10.93 on 10 degrees of freedom
The estimated coefficients (ˆθ1 = 212.68, ˆθ2 = 0.0641) provide a much better
fit to the data [Figure 12.1(c)], particularly in estimation of the asymptotic
maximum reaction rate θ1. Using other starting values yields virtually identical
estimates. There is some evidence of an outlier in the data, corresponding
to a point with unusually high velocity for a concentration of 0.02 parts per
million (Figure 12.2), but omitting this observation does not change the fitted
model appreciably [it is given by the dashed line in Figure 12.1(c)].

250
CHAPTER 12 Nonlinear Regression
50
100
150
200
–10
0
10
20
(a)
Fitted values
Residuals
−1.5
−0.5
0.5
1.5
–10
0
10
20
(b)
Theoretical Quantiles
Sample Quantiles
FIGURE 12.2: Residual plots for the puromycin enzyme velocity data. (a) Plot
of residuals versus fitted values. (b) Normal plot of the residuals.
0.0
0.2
0.4
0.6
0.8
1.0
50
100
150
200
Complete data
Concentration
Velocity
x
x
xxx
x
xx
x
x
x
0.0
0.2
0.4
0.6
0.8
1.0
50
100
150
200
Omitting outliers
Concentration
Velocity
x
xxx
x
xx
x
x
x
FIGURE 12.3: Scatter plots and model fits for the puromycin enzyme veloc-
ity data. Observations where the enzyme was treated with puromycin are
represented by dots, while observations where the enzyme was untreated are
represented by ×s. The left plot is based on all of the data, while the right plot
is based on the data after removing two outliers. The solid lines represent a
model with different values of θ for treated and untreated observations, while
the dashed lines represent a model with different values of θ1 and identical
values of θ2 for the two groups.
These data are part of a larger experiment in which the relationship
between velocity of reaction and concentration was examined when the
enzyme is not treated with puromycin. The observations corresponding to
untreated enzyme are given by ×s in Figure 12.3. It was hypothesized that
treatment by puromycin would affect the maximum velocity of reaction (θ1)
but not the affinity (θ2) of the enzyme, and the effect on maximum velocity is
apparent from the plot.
The full data set corresponds to a situation with two natural subgroups in
the data, and is thus amenable to an analysis based on incorporating an indica-
tor variable into the model, as was done in Section 2.4. Indeed, the linearized
form of the Michaelis-Menten model (12.5) shows that a “pooled/constant
shift/full” model approach is meaningful here, with the maximum velocity θ1

12.4 Example — Michaelis-Menten Enzyme Kinetics
251
taking the role of the intercept and the affinity θ2 taking the role of the slope.
If Treated is an indicator variable taking on the value 1 for an experiment
where the enzyme was treated and 0 otherwise, the “constant shift” model
adds a parameter θ3 to the model,
Velocity = (θ1 + θ3Treated)Concentration
θ2 + Concentration
,
which allows for different maximum velocity values for the two groups. The
full model adds a parameter θ4,
Velocity =
(θ1 + θ3Treated)Concentration
(θ2 + θ4Treated) + Concentration,
which allows for both different maximum velocity and different affinity
depending on whether or not the enzyme is treated with puromycin.
The following output summarizes fits of the three models:
Parameters:
Estimate Std. Error t value Pr(>|t|)
theta1 190.80632
8.76459
21.770 6.84e-16 ***
theta2
0.06039
0.01077
5.608 1.45e-05 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 18.61 on 21 degrees of freedom
-----------------
Parameters:
Estimate Std. Error t value Pr(>|t|)
theta1 166.60406
5.80743
28.688
< 2e-16 ***
theta2
0.05797
0.00591
9.809 4.37e-09 ***
theta3
42.02595
6.27214
6.700 1.61e-06 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 10.59 on 20 degrees of freedom
------------------
Parameters:
Estimate Std. Error t value Pr(>|t|)
theta1 1.603e+02
6.896e+00
23.242 2.04e-15 ***
theta2 4.771e-02
8.281e-03
5.761 1.50e-05 ***
theta3 5.240e+01
9.551e+00
5.487 2.71e-05 ***
theta4 1.641e-02
1.143e-02
1.436
0.167
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Residual standard error: 10.4 on 19 degrees of freedom

252
CHAPTER 12 Nonlinear Regression
The Wald (t) tests indicate that the “constant shift” model is a significant
improvement over the pooled model (the test of θ3 = 0 is t = 6.7), but the full
model is not a significant improvement over the “constant shift” model (the test
of θ4 = 0 is t = 1.4). Just as was true for logistic regression and other generalized
linear models (page 167), Wald tests are known to be less trustworthy than
likelihood ratio tests (the likelihood ratio test for nonlinear regression models
is a partial F-test). In this case the distinction is unimportant, since the
partial F-tests (F = 44.9 on (1, 20) degrees of freedom, p = 1.6 × 10−6, and
F = 1.72 on (1, 19) degrees of freedom, p = .206, respectively) also support
the hypothesis of a different maximum velocity (estimated to be 166.6 for
untreated enzyme versus 208.6 for treated enzyme) but same affinity (estimated
Michaelis constant .058) for the treated and untreated groups. The left plot
of Figure 12.3 gives the estimated velocity functions for the two groups based
on the “constant shift” (dashed lines) and full (solid lines) models, and it is
apparent that they are similar to each other.
Residual plots (not presented here) reveal two potential outliers at a
concentration of .02 parts per million (one in each group), but model fitting
after omitting these observations still leads to the “constant shift” model as
before (and as was originally hypothesized). The right plot of Figure 12.3 gives
the estimated velocity functions for the two groups based on the “constant
shift” (dashed lines) and full (solid lines) models (the “constant shift” model
has an estimated maximum velocity of 170.2 for untreated enzyme versus 213.2
for treated enzyme and estimated Michaelis constant .066 for both groups).
It is apparent that the estimated velocities are not very different from those
based on all of the data.
12.5
Summary
We have only briefly touched on the basics of nonlinear regression fitting
in this chapter. Bates and Watts (1988) and Seber and Wild (1989) provide
much more thorough discussion of such models, including more details
on estimation. They also discuss how differential geometry can be used
to construct curvature measures (intrinsic curvature and parameter-effects
curvature, respectively) that quantify the extent to which the linear Taylor
series approximation fails for a given data set and given model parameterization.
KEY TERMS
Nonlinear least squares: A method for estimating the parameters of a
nonlinear regression model. It is appropriate when the additive error term is
(roughly) normally distributed with constant variance, and requires an iterative
procedure to find the solution.

12.5 Summary
253
Nonlinear regression model: A model for the relationship between a response
and predictor(s) in which at least one parameter does not enter linearly into
the model.
Taylor series approximation: A method of approximating a nonlinear func-
tion with a polynomial. The first-order (linear) Taylor series approximation is
the basis of standard inferential tools when fitting nonlinear regression models.

Thirteen
Chapter
Models for Longitudinal
and Nested Data
13.1
Introduction
255
13.2
Concepts and Background Material
257
13.2.1 Nested Data and ANOVA
257
13.2.2 Longitudinal Data and Time Series
258
13.2.3 Fixed Effects Versus Random Effects
259
13.3
Methodology
260
13.3.1 The Linear Mixed Effects Model
260
13.3.2 The Generalized Linear Mixed Effects Model
262
13.3.3 Generalized Estimating Equations
262
13.3.4 Nonlinear Mixed Effects Models
263
13.4
Example — Tumor Growth in a Cancer Study
264
13.5
Example — Unprovoked Shark Attacks in the United
States
269
13.6
Summary
275
13.1
Introduction
In all of the situations discussed thus far there has been a one-to-one corre-
spondence between each member of the sample and a response; that is, each
observation corresponds to one member of the sample and has one response
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
255

256
CHAPTER 13 Models for Longitudinal and Nested Data
value. Even for the time series data of Chapter 5, where the sample is a set of
time points rather than a subset of some population, there is a single response
value for each time point.
In this chapter we examine more complex situations, where notions of
individuals and observations are more complicated. One example is when
observations arise in a nested or clustered fashion. Consider, for example, an
educational setting, in which predictive information and a response (say a test
score) are available for each student, but an experimental condition such as
a particular training program is administered at the classroom level. In this
situation each student corresponds to an observation, with observations nested
within units corresponding to classrooms. Since all of the students are exposed
to the same classroom conditions, students are nested within classrooms, and
it would be expected that errors from a model ignoring that structure would
be correlated with each other, reflecting unmodeled differences between
classrooms (such as the effect of the teacher); this apparent correlation because
of a misspecified model, rather than from a true correlation from an underlying
random process, is called an induced correlation. This sort of hierarchical
structure can extend further, with students nested within classrooms, which
are nested within schools, which are nested within school districts, and so
on. Ignoring this structure can induce further apparent correlation in the
errors, even if student scores are actually conditionally independent of each
other.
A different, but related, situation is a generalization of the time series data
situation of Chapter 5. In previous chapters data were either cross-sectional
(with independent observations) or time series (with each observation a point
in time), but a third situation is when individual units are repeatedly sampled at
different points in time; that is, a combination of cross-sectional and time series
data. In this situation each combination of unit and time point corresponds
to an observation, with observations again nested within units, this time with
an additional time-related structure. When the units correspond to people,
such as in a clinical situation with repeated visits by a particular patient, or an
educational situation with repeated examinations of a particular student, units
are naturally referred to as subjects. Such data are called longitudinal data, or
(in economic data) panel data.
In this chapter we examine models for longitudinal and nested data. We
show how a particular model, the linear mixed effects model, can be viewed as
generalizing regression models, generalizing ANOVA models, and generalizing
time series models. We see how this modeling approach can account for the
structure in the data and avoid the problems that occur if that structure is
ignored. We discuss extensions of the model to nonlinear relationships and
generalized linear modeling. We also discuss the distinction between inference
that focuses on changes in responses for a particular unit versus inference that
focuses on average changes in responses over all units, and describe a method
appropriate for the latter situation.

13.2 Concepts and Background Material
257
13.2
Concepts and Background Material
13.2.1
NESTED DATA AND ANOVA
Consider a simple experimental setting, where the response of interest y is a
numerical measure of patient health such as blood pressure level, with a nor-
mal distribution for this measure being assumed. The experiment is intended
to investigate the relationship between blood pressure and other factors. Say
interest is focused on how blood pressure relates to several types of physical and
mental exercise, such as aerobic exercise, meditation, yoga, and so on, with there
being K such types of exercise. The data consist of a sample of people, each of
whom engage in one (and only one) of these types of exercise, and these specific
types of exercise in the sample are the only ones of interest. A natural repre-
sentation of the relationships in the data is the one-way ANOVA model (6.1),
yij = μ + αi + εij, i = 1, . . . , K, j = 1, . . . , ni.
The least squares (maximum likelihood) estimate of the expected response
for any individual undertaking exercise type i is yi, the sample mean response
of all of the observations in that group. That is, under this model all of the
useful information about type i is contained in the people who participated
in type i. The value αi corresponds to the fixed effect of being in the exercise
type i group, since it is a fixed unknown parameter.
Now, imagine instead a situation where one specific exercise treatment
regimen is applied to patients in different treatment facilities, with each
treatment facility defining a group of patients. This situation is different
from the previous one, and it is reasonable here to view the set of treatment
facilities as constituting a random sample from the population of potential
treatment facilities. This is the nested data situation described in Section 13.1,
as patients are nested within treatment facilities, and their responses would be
subject to unmodeled differences (heterogeneity) between facilities. A model
that corresponds to this process is
yij = μ + ui + εij,
(13.1)
where ui is the random effect of being in the ith group (that is, treatment
facility). The key distinction between the fixed and random effects models is
that for fixed effects the K available groups are the only ones of interest (all
relevant types of exercise are part of the set of treatment regimens), while for
random effects the treatment facilities do not exhaustively include all possible
facilities, but are rather a sample from the population of treatment facilities.
Thus, while the parameters associated with different exercise regimens would
be the same if the first experiment was run again on a different set of
patients (since they are by definition unknown, but fixed), the observed
effects corresponding to different treatment facilities would be different if
the second experiment was run again on a different set of patients treated in
different treatment facilities (since the specific treatment facilities used would
be different), as they are by definition random.

258
CHAPTER 13 Models for Longitudinal and Nested Data
The random effect ui is thus a random variable, assumed to be Gaussian
with mean 0 and variance σ2
u, and independent of ε. This means that for the
fixed effects model there is only one source of variability (εij, with variance σ2),
but for the random effects model there are two components of variance (εij and
ui, with variances σ2 and σ2
u, respectively). The larger σ2
u is relative to σ2, the
more distinct the groups are from each other, and this can be quantified by
the intraclass correlation (ICC),
ρ =
σ2
u
σ2u + σ2 ,
the proportion of variability in the responses accounted for by the variability
between groups. This is also equal to the correlation between two observations
in the same group, which accounts for its name.
It can be shown that in this situation unbiased predictions of yij satisfy

μ + ui ≈niνyi + y
niν + 1 ,
(13.2)
where ν = σ2
u/σ2 is the so-called variance ratio (note that since the random
effects u are random variables these are predictions, not estimates). Thus, the
random effects model does not treat the groups as completely distinct from
each other (implying yi as a fitted value for all observations in the ith group),
as the fixed effects model does, but rather “borrows strength” from other
groups by shrinking the fitted value towards the common overall mean. The
form of (13.2) shows that for larger groups the fitted value gives more weight
to the observations from the group, and the smaller ν is (corresponding to
smaller between-group variability relative to variability of the errors) the more
weight the fitted value gives to the overall mean.
13.2.2
LONGITUDINAL DATA AND TIME SERIES
Consider now a time series regression situation, as discussed in Chapter 5.
A linear model representation of such data is
yt = β0 + β1x1t + · · · + βpxpt + εt, t = 1, . . . , T,
using the subscript t and number of time points T to emphasize the time
structure. Of course, an independence assumption for the errors ε is ques-
tionable, and various forms of a correlation structure (such as autoregressive)
could be contemplated instead.
This formulation corresponds to analysis of a single time series, but
a natural generalization would be a situation where there is a sample of
objects from some population, where each has a response and predictors
measured at repeated time points; for example, an economic response and
potential predictors measured over multiple years for a set of countries. This
corresponds to a combination of cross-sectional and time series data. A model
for such longitudinal data could take the form
yit = β0i + β1x1it + · · · + βpxpit + εit, i = 1, . . . , n, t = 1, . . . , T,
(13.3)

13.2 Concepts and Background Material
259
with the subscript i indexing the cross-sectional sample of individuals and t
indexing time. This has the same form as that of an analysis of covariance
(constant shift) model (7.1) if the β0 terms are fixed parameters, but is a
generalization of (13.1), adding in covariates, if they are taken to be random
variables. We will refer to the former formulation as a fixed effects model, and
the latter a mixed effects model. An autocorrelation structure for errors from
the same observation (that is, among the values of εi for a given i) is a natural
possibility. The problem of induced correlation described earlier is relevant
here as well. For example, if learning assessments are administered repeatedly
to a particular student at multiple time points, it would be expected that the
student’s scores would be systematically higher or lower, reflecting some sort
of innate ability or unmeasured difference of that student.
13.2.3
FIXED EFFECTS VERSUS RANDOM EFFECTS
From one point of view the question of the use of fixed effects or random
effects to identify groups is straightforward: if the observed groups constitute
the entire set of possible groups fixed effects are appropriate, but if the groups
can be viewed as a sample from some larger population random effects are
appropriate. Thus, as noted in Section 13.2.1, patients who are given one of a
set of prespecified treatments, which are the only treatments under study, are
grouped by a fixed effect, while patients who are treated at different facilities,
which might be different from each other but are only a subset of all possible
hospitals, are grouped by a random effect. A related point is that out-of-sample
predictions (that is, predictions for a member of a group that is not represented
in the original sample) are only meaningful when using random effects, since
by assumption fixed effects are used when the observed groups exhaust the
entire set of possible groups.
There are, however, other considerations that arise when contemplating
the choice between a fixed or a random effect. For example, if a predictor
in a longitudinal study takes on the same value for all time points for each
individual (that is, it does not vary with time), it is confounded with the
codings that would define a fixed effect, and that predictor cannot be included
in the model. This reflects that models with fixed effects are designed to explore
how changes in responses of an individual relate to changing properties of an
individual, and constant values of those properties are not informative about
that question. Relatedly, if a predictor has minimal variation within a group
(or is sluggish, only changing very slowly over time in a longitudinal situation),
while it isn’t perfectly collinear with the codings it can still be highly collinear
with them, resulting in high variability of the slope estimate (as would be
expected from the discussion in Section 2.2.2). Another potential issue is that
a fixed effect requires K −1 codings to include it in the model; in a situation
where the number of replications within each group is small that could reduce
the available degrees of freedom for the error term (and therefore the precision
of parameter estimates) by a considerable amount.

260
CHAPTER 13 Models for Longitudinal and Nested Data
On the other hand, mixed effects models require independence of the error
random variable ε and the random effects random variable u. Concerns (or the
lack thereof) among researchers in different fields has resulted in remarkably
strong discipline-specific patterns in the use of fixed versus mixed effects
models, with papers in fields like economics and finance overwhelmingly
based on fixed effects models, and papers in fields like psychology and
education overwhelmingly based on mixed effects models. We will not focus
on fixed effects models in this chapter, as they are simply applications and
generalizations of the analysis of variance and analysis of covariance models
discussed in Chapters 6 and 7, respectively.
13.3
Methodology
13.3.1
THE LINEAR MIXED EFFECTS MODEL
The linear mixed effects model (LMM) is a generalization of (13.1) and
(13.3) that allows more flexibility in the fixed and random effects. For each
unit i the relationship between the responses and the predictors for that unit
takes the form
yi = Xiβ + Ziui + εi,
(13.4)
where εi is (multivariate) normally distributed with mean 0 and covariance
matrix Ri and ui is normally distributed with mean 0 and covariance matrix
D, and εi and ui are independent of each other. Here yi is the set of response
values for the observations of unit i and Xi corresponds to the set of predictor
values used to construct Xiβ, the expected response for observations of unit
i given the predictor values. In this context this reflects the fixed effects. Zi
corresponds to the set of predictor values used to construct Ziu, the effect on
the observation’s responses from the random effects. Note that this generalizes
(13.1) in two ways. First, (13.4) does not restrict the unconditional expected
response (ignoring treatment facility) to be the same for each patient, but
rather can take into account potential predictors like socioeconomic and
medical history information of individual patients that could be related to the
expected response through the Xiβ term, as in (13.3). Further, (13.4) does not
restrict random effects to only shift the response value up or down (random
intercepts, where the Zi matrix is only a column of ones), but allows random
effects that are more complicated functions of predictors, such as different
slopes for different units (random slopes).
Different forms of D and Ri imply different correlation patterns in the
data, determined by the structure of the underlying random process. The
properties of random effects are driven by the entries in D; while a general
unstructured form for D is possible, often the matrix D has off-diagonal
elements equal to zero, implying independent random effects with (potentially)
different variances, resulting additive components of variance from the random
effects. Ri determines the correlation structure among observations from the

13.3 Methodology
261
same unit. The simplest structure is a diagonal one, in which Ri is a diagonal
matrix with a constant entry σ2 along the diagonal. This implies independent
errors within units, with all errors in all units having the same variance. The
compound symmetry structure takes the form
Ri =
⎛
⎜
⎜
⎜
⎜
⎝
σ2 + σ1
σ1
· · ·
σ1
σ1
σ2 + σ1
· · ·
σ1
...
...
...
...
σ1
σ1
· · ·
σ2 + σ1
⎞
⎟
⎟
⎟
⎟
⎠
.
This is a plausible structure in a nested situation of repeated outcomes under
the same conditions (for example, the responses for a set of animals born in
the same litter). For longitudinal data an autoregressive structure, such as an
AR(1) structure
Ri =
⎛
⎜
⎜
⎜
⎜
⎝
σ2
σ2ρ
· · ·
σ2ρni−1
σ2ρ
σ2
· · ·
σ2ρni−2
...
...
...
...
σ2ρni−1
σ2ρni−2
· · ·
σ2
⎞
⎟
⎟
⎟
⎟
⎠
,
(as in (5.2)), is plausible (from a practical point of view, the fits of a
random intercepts model with AR(1) errors and a random slopes model with
independent errors are often very similar to each other, but can have very
different implications from the point of view of prediction). The parameters
of the matrices D and (especially) Ri can be generalized to be different for
different units; for example, heteroscedasticity across units can accommodated
in the diagonal structure by each Ri having its own value σ2
i along the diagonal.
Models of this type are also referred to as multilevel models, which
emphasizes a hierarchical formulation of the model. Consider, for example,
a simple random slopes model for educational data with students j nested
within classrooms i. The level-1 regression model for the response variable is
yij = β0i + β1ixij + εij,
where x is a predictor at the level of an individual student. This is then
specified using the level-2 regression model for the intercepts and slopes of the
predictor at level 1,
β0i = γ00 + γ01wi + u0i,
β1i = γ10 + u1i.
Specifying a correlation structure for the εij terms corresponds to specifying
Ri above, and specifying a correlation structure for the ui terms corresponds
to specifying D. Here w is a predictor that varies across classrooms, γ00 is an
overall intercept and γ01 is an overall slope for w, and γ10 is an overall slope
for x. Additional predictors at each level are easily added to the model, and
further nesting (classrooms within schools, schools within school districts, and
so on) is accommodated by additional levels.

262
CHAPTER 13 Models for Longitudinal and Nested Data
Maximum likelihood (ML) can be used to estimate the parameters of the
model. Unfortunately, the maximum likelihood estimates of the parameters
in D and Ri are negatively biased, because they do not take into account
the degrees of freedom lost when estimating the fixed effects β (a simple
example of this is the contrast between the ML estimate of the variance from a
random sample from a normal distribution,  (yi −y)2/n, and the unbiased
estimate  (yi −y)2/(n −1)). This can be addressed using restricted maximum
likelihood (REML), which produces unbiased estimates of the parameters in D
and Ri (say θ) by maximizing a transformed version of the log-likelihood that
separates the information for the parameters of the fixed effects from that for
the parameters of the variance terms. Likelihood ratio tests based on ML can
be used to test hypotheses about β, while tests based on REML are used to test
hypotheses about θ. Wald tests also can be constructed to test such hypotheses.
Random effects are predicted using the best linear unbiased predictors, which
generalize (13.2) and are the conditional expectations of the random effects
given the set of observed responses yi. Model selection is more complicated
than for ordinary regression models (involving both choices of fixed effects and
of random effects), with (for example) several versions of information criteria
available. Questions of the best approaches are still a matter of active research,
particularly if REML estimation is used (Müller et al., 2013; Steele, 2013).
13.3.2
THE GENERALIZED LINEAR MIXED EFFECTS MODEL
The LMM assumes that responses are normally distributed, which of course
is not necessarily an appropriate choice. Just as the generalized linear model
of Section 10.2.2 provides a general framework for modeling non-Gaussian
data, so too does the generalized linear mixed effects model (GLMM) for
nested or longitudinal data. The model is a straightforward generalization of
the GLM; it specifies a distribution for the response from the exponential
family, but now the link function g(μ) equals the linear predictor
ηi = Xiβ + Ziui,
with the random effects following a specified distribution (often Gaussian).
Estimation can be based on maximum likelihood, although computationally
this is considerably more challenging than for GLMs. Inference is based on
likelihood ratio or Wald tests, as usual, and prediction of random effects is
based on minimizing the mean squared error of prediction.
13.3.3
GENERALIZED ESTIMATING EQUATIONS
The GLMM is a subject-specific model that represents the relationship between
predictors and responses through fixed effects of an individual subject, condi-
tional on the random effects. A different notion is that of a population average
model (also termed a marginal model), which represents fixed effects relation-
ships averaged over all of the members of the population. The parameters
associated with the fixed effects in a population average model are not, in

13.3 Methodology
263
general, the same as those in a subject-specific model. Consider, for example,
a study examining presence or absence of a disease fit using logit link function,
with one predictor being whether the subject is a smoker or not. The slope
of this variable in a subject-specific model is the log of the ratio of the odds
of a subject having the disease versus not having the disease if the subject is a
smoker compared to the odds of the same subject having the disease versus not
having the disease if that subject is a nonsmoker. In contrast, the slope of this
variable in a population average model is the log of the ratio of the odds of an
average member of the population who is a smoker having the disease versus
not having the disease compared to the odds of an average member of the pop-
ulation who is not a smoker having the disease versus not having the disease.
The fixed effects parameters are the same for subject-specific and population
average models when using a linear link function, but are not in general
otherwise. It is therefore important to be clear about what the goal of a study
actually is. Subject-specific formulations focus on within-subject changes in
response and how these relate to within-subject changes in predictors, which is
often natural in longitudinal studies. Population average formulations focus on
changes in the overall expected response associated with changes in predictor
values, which could be appropriate in public health contexts.
The most common approach to population average modeling is gener-
alized estimating equations (GEE). Just as was true for the mixed model
formulation, GEE proceeds by first specifying an exponential family distribu-
tion for the response variable, and a link function relating the population mean
to the linear predictor. A working correlation matrix for the within-subject
responses is also chosen, with the common choices being the ones discussed
earlier for Ri in the GLMM (independence, compound symmetry, autoregres-
sive). Estimation proceeds using iteratively reweighted least squares, updating
the entries of the working correlation matrix at each step, which also provides
standard errors for inference on the regression parameters.
13.3.4
NONLINEAR MIXED EFFECTS MODELS
The models discussed thus far have both fixed and random effects entering in
the model linearly. Transformations like the logarithmic transformation can
expand the range of relationships that can be hypothesized, but as was discussed
in Chapter 12, sometimes relationships are inherently nonlinear, and need to
be modeled using nonlinear models. Nonlinear mixed effects models can do
this for nested and longitudinal data. For example, clinical pharmacokinetic
studies based on the models described in Section 12.1 are naturally modeled
this way, as each subject’s blood concentration is measured at repeated time
points, for a sample of subjects. Another application is estimation of growth
curves, in which some quantity of a subject (along with covariates) is measured
at repeated time points for a sample of subjects, and interest is focused on the
evolution of the quantity over time.
The nonlinear mixed effects model can be formulated as a generalization of
the hierarchical (multilevel) formulation of the linear mixed model described

264
CHAPTER 13 Models for Longitudinal and Nested Data
on page 261. The level-1 regression model for the response variable is
yij = g(xij, θi) + εij,
where xij is a vector of predictor values at the observation level and g(·) is
a known nonlinear function with parameters θ, such as a pharmacokinetic
relationship or growth curve. The level-2 regression model for the parameters
at level 1 is
θi = h(wi, γ, ui),
where wi is a vector of predictors and γ a vector of parameters for the fixed
effect parameters, and h(·) is a known function (often taken to be linear, as is
the case in the linear multilevel formulation). Estimation and inference for this
model is even more challenging than for cross-sectional nonlinear regression,
with linearization using Taylor Series (page 246) one approach for maximum
likelihood estimation and inference. Generalized estimating equations also can
be applied to data of this type.
13.4
Example — Tumor Growth in a Cancer
Study
A standard precursor to clinical study of a possible new cancer treatment is
a set of pre-clinical studies designed to provide preliminary assessment of the
response of human tumors to that treatment. A way this can be done in vivo
(that is, taking place in a living organism) is through the use of xenograft
(when tissue is grafted from a donor of a different species from the recipient)
tumor experiments (Varna et al., 2014). Daskalakis (2016) provides data from
a cancer study of tumor size of 37 xenografted mice. Tumor size was measured
on work days for up to four weeks, and each mouse was subjected to one of four
treatments: control, drug only, radiation only, or drug + radiation. Figure 13.1
gives line plots of tumor size (in cubic millimeters) by number of days after graft
(Day) for each mouse, separated by type of treatment (the plot emphasizes the
{No drug / Drug} by {No radiation / Radiation} crossed design of the study).
It is apparent that the relationship between tumor size and number of
days after grafting is not linear, but rather exponential, exhibiting increasing
variance with increasing tumor size. This suggests using logged tumor size as
the response variable, and Figure 13.2 gives plots for this response based on logs
base 10, with line plots of the average logged tumor size superimposed for each
treatment type. The relationship with days after graft appears more linear, and
variances across measurement time more constant. (Logged) tumor growth
appears to be slower in the treatment groups compared to the control group,
particularly when radiation is included in treatment, with some evidence of a
leveling-off effect in the presence of radiation treatment.
This suggests fitting a mixed effects model with linear and quadratic terms
in Day in an interaction with the two main effects of treatment (drug and
radiation) with their interaction, which is as follows:

13.4 Example —Tumor Growth in a Cancer Study
265
0
10
20
No radiation
Radiation
No drug
Drug
0
10
20
0
500
1000
1500
2000
2500
0
500
1000
1500
2000
2500
Day
Tumor size
FIGURE 13.1: Line plots of tumor size versus number of days after grafting
for xenograft tumor growth data. Plots are separated by whether a mouse was
in the control group (no drug or radiation), was in the drug only group, was
in the radiation only group, or was in the group that received both drug and
radiation treatments.
Random effects:
Groups
Name
Variance Std.Dev.
Mouse
(Intercept) 0.03523
0.1877
Residual
0.02116
0.1455
Number of obs: 575, groups:
Mouse, 37
Fixed effects:
Estimate Std. Error t value
(Intercept)
1.6610820
0.0758439
21.901
Drug2
-0.0149649
0.1001974
-0.149
Radiation2
-0.0241743
0.1009585
-0.239
Day
0.1468154
0.0087300
16.817
Day.squared
-0.0031602
0.0004522
-6.988
Drug2:Radiation2
0.0582100
0.1389191
0.419

266
CHAPTER 13 Models for Longitudinal and Nested Data
No radiation
Radiation
No drug
Drug
0
10
20
0
10
20
100
300
1000
100
300
1000
Day
Tumor size
FIGURE 13.2: Line plots of logged tumor size versus number of days after
grafting for xenograft tumor growth data. The superimposed thick lines are
the average logged tumor sizes for each treatment combination.
Drug2:Day
-0.0558053
0.0101120
-5.519
Radiation2:Day
-0.0218227
0.0101799
-2.144
Drug2:Day.squared
0.0020394
0.0004944
4.125
Radiation2:Day.squared
0.0007481
0.0004909
1.524
Drug2:Radiation2:Day
0.0386287
0.0126256
3.060
Drug2:Radiation2:Day.squared -0.0019078
0.0005666
-3.367
Residuals from the model (Figure 13.3) do not indicate lack of fit. The
t-test for the highest-order interaction of drug by radiation by linear and
quadratic functions of days after graft (t = −3.37) supports different quadratic
functions of days for the four different treatment regimens. The four implied
equations are as follows:
Control (no drug, no radiation):
Logged.tumor.size = 1.6611 + 0.1468 Day −0.0032 Day2

13.4 Example —Tumor Growth in a Cancer Study
267
No radiation
Radiation
No drug
Drug
0
10
20
0
10
20
−0.25
0.00
0.25
0.50
−0.25
0.00
0.25
0.50
Day
Residuals
FIGURE 13.3: Line plots of residuals from the linear mixed model of logged
tumor size for xenograft tumor growth data.
Drug only:
Logged.tumor.size = 1.6461 + 0.0910 Day −0.0011 Day2
Radiation only:
Logged.tumor.size = 1.6369 + 0.1250 Day −0.0024 Day2
Drug + Radiation:
Logged.tumor.size = 1.6802 + 0.1078 Day −0.0023 Day2
Figure 13.4 superimposes these curves on the estimated curve for each
mouse (which includes the predicted random effect for each mouse). Table 13.1
summarizes the estimated expected logged tumor size and growth rate of logged
tumor size 5, 15, and 25 days after grafting for the four treatment regimens.
Growth rate is estimated as ˆβ1 + 2ˆβ2Day, where ˆβ1 (ˆβ2) is the estimated slope

268
CHAPTER 13 Models for Longitudinal and Nested Data
No radiation
Radiation
No drug
Drug
0
10
20
0
10
20
10
100
1000
10
100
1000
Day
Tumor size
FIGURE 13.4: Estimated logged tumor size versus number of days after
grafting for individual mice and overall for xenograft tumor growth data.
for Day (Day2), since that is the estimated derivative of the response as a
function of Day.
The model implies that drug treatment slows tumor growth in the first
few days after grafting, but its longer-term effect is relatively weak. In contrast,
radiation treatment has a weaker effect in the first few days, but a stronger
one in the long run. The combination of the two drugs combines these two
effects, particularly the long-run radiation effect, which seems to be reinforced
in the presence of drug treatment even though drug treatment by itself has
relatively little long-term effect.
This analysis has assumed that missingness for individual mice is uninfor-
mative, but that is unlikely to be the case here. While missing values due to the
weekend are likely not a problem, there is informative dropout in this process.
There is a survival (time-to-event) component in the design, in that mice are
sometimes euthanized, and it seems likely the decision to euthanize is related
to tumor size (or more generally the progress of cancer). This would account,

13.5 Example — Unprovoked Shark Attacks in the United States
269
Table 13.1: Estimated expected logged tumor size and
growth rate of logged tumor size 5, 15, and 25 days after
grafting for xenograft tumor growth data.
Treatment
Days after graft
5
15
25
Estimated expected logged tumor size
Control
2.3162
3.1523
3.3563
Drug only
2.0731
2.7591
3.2208
Radiation only
2.2016
2.9691
3.2542
Drug + Radiation
2.1622
2.7843
2.9502
Estimated growth rate of logged tumor size
Control
0.1152
0.0520
−0.0112
Drug only
0.0798
0.0574
0.0350
Radiation only
0.1009
0.0526
0.0044
Drug + Radiation
0.0850
0.0394
−0.0062
Values for the control group 25 days after grafting are given in italics
because all mice in that group were sacrificed before that day.
for example, for why no data for mice in the control group are available more
than 21 days after grafting. This issue can be addressed using joint models
for longitudinal and time-to-event data. In such models the two responses
(longitudinal and time-to-event) are linked together using random effects,
resulting in mixed models on the longitudinal side and so-called frailty models
on the survival side. These models are not discussed here, but are described,
for example, in Rizopoulos (2012).
13.5
Example — Unprovoked Shark Attacks
in the United States
Generalized linear (count) regression models were used in Section 10.5
to study unprovoked shark attacks in Florida, but of course such
attacks occur in other states in the United States. We examine attacks
from 2002 through 2016 in the eight states with at least five attacks
over that time period (Alabama, California, Florida, Hawaii, North
Carolina, Oregon, South Carolina, and Texas), based on data available at
www.sharkattackdata.com/place/united states of america
(accessed August 29, 2019). As was done in the earlier Florida attack analysis,
we model the per capita shark attack rate.
Figure 13.5 gives the annual attack rates for each state. It is clear that
rates vary widely from state to state, with Hawaii having an especially high

270
CHAPTER 13 Models for Longitudinal and Nested Data
Year
Attack rate
0.0e + 00
2.0e − 06
4.0e − 06
6.0e − 06
8.0e − 06
1.0e − 05
1.2e − 05
2005
2010
2015
Alabama
California
2005
2010
2015
Florida
Hawaii
North.Carolina
0.0e + 00
2.0e − 06
4.0e − 06
6.0e − 06
8.0e − 06
1.0e − 05
1.2e − 05
Oregon
0.0e + 00
2.0e − 06
4.0e − 06
6.0e − 06
8.0e − 06
1.0e − 05
1.2e − 05
South.Carolina
2005
2010
2015
Texas
FIGURE 13.5: Observed annual per capita unprovoked shark attack rates
separated by states for national shark attacks data.
rate (presumably at least in part because of the effect of tourism on a state
with a small resident population). No time trends are particularly apparent in
the plots. Figure 13.6 gives plots of the annual attack rates versus potential
state-level predictors (that is, predictors that do not vary over time for a given
state), with the values in the first three plots identified by the first letter of
the associated state. Shark attack rates are perhaps surprisingly not particularly
associated with the amount of shoreline of a state. There appears to be an
inverse relationship between shark attacks and latitude of the state, with more
southern states (smaller latitude value) having higher attack rates (latitude
values correspond to the northernmost and southernmost points on the shore,
respectively). This is presumably related to both the temperature of the water
(and its relationship to shark activity) and the general ambient temperature
(and associated appeal for people to go in the water). The strongest effect
is a location one, with states along the coast of the Gulf of Mexico having
considerably lower attack rates, perhaps resulting from larger populations of

13.5 Example — Unprovoked Shark Attacks in the United States
271
AAAAAAAA
0
200
600
1000
1400
0.0e + 00
6.0e − 06
1.2e − 05
(a)
Miles of shoreline
Shark attack rate
AAAAAAAA
20
25
30
35
40
45
50
0.0e + 00
6.0e − 06
1.2e − 05
(b)
Northernmost latitude
Shark attack rate
AAAAAAAA
15
20
25
30
35
40
45
0.0e + 00
6.0e − 06
1.2e − 05
(c)
Southernmost latitude
Shark attack rate
Atlantic
Gulf
Pacific
0.0e + 00
6.0e − 06
1.2e − 05
(d)
Location
Attack rate
FIGURE 13.6: Observed annual per capita unprovoked shark attack rates
versus (a) miles of shoreline, (b) northernmost latitude of shore, (c) south-
ernmost latitude of shore, and (d) location of state, for national shark attacks
data.
sharks in the open oceans (Florida, which has both an Atlantic Ocean coast
and a Gulf of Mexico coast, is treated as an Atlantic state here). There is more
variability in attack rates among Pacific states, again coming from Hawaii’s
relatively high attack rates, but otherwise little apparent difference between
Atlantic and Pacific states, as the apparent higher total number of attacks in
the Atlantic (Howard, 2019) disappears to a large extent when population is
taken into account.
The data are first analyzed using a Poisson-based generalized linear mixed
model with logarithmic link function. An argument could be made to treat
state as a fixed effect rather than a random effect, but these are not all states
with shark attacks, and more importantly a fixed effects analysis would make it
impossible to examine the effects of the state-level potential predictors (miles
of shoreline, latitude, and location). Output from this model is as follows:
AIC
BIC
logLik deviance df.resid
525.5
545.0
-255.8
511.5
113

272
CHAPTER 13 Models for Longitudinal and Nested Data
Scaled residuals:
Min
1Q
Median
3Q
Max
-2.3484 -0.8041 -0.1910
0.6506
3.7024
Random effects:
Groups Name
Variance Std.Dev.
State
(Intercept) 0.06358
0.2522
Number of obs: 120, groups:
State, 8
Fixed effects:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-9.857152
0.560406 -17.589
< 2e-16 ***
Years.since.2001
0.008451
0.008865
0.953
0.340
Northern.latitude -0.218908
0.048142
-4.547 5.44e-06 ***
Southern.latitude
0.069613
0.053422
1.303
0.193
Location1
0.905687
0.159702
5.671 1.42e-08 ***
Location2
-2.061053
0.210029
-9.813
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
The variable Year has been transformed to years since 2001 to provide
more stable fitting of the model. The two latitude predictors are highly
correlated with each other (r = .93), so it is not surprising that only one
of the two is needed in the model. Further, the variable corresponding to
year also does not provide significant additional predictive power. The model
removing year and the southernmost shore latitude provides almost identical
fit (the fitted values for the two models have correlation equal to .999, and the
marginal AIC value of the simpler model is 1.2 lower than that of the more
complex model):
AIC
BIC
logLik deviance df.resid
524.3
538.2
-257.1
514.3
115
Scaled residuals:
Min
1Q
Median
3Q
Max
-2.4880 -0.7941 -0.1687
0.7685
3.8590
Random effects:
Groups Name
Variance Std.Dev.
State
(Intercept) 0.07016
0.2649
Number of obs: 120, groups:
State, 8
Fixed effects:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-9.79483
0.59935 -16.342
< 2e-16 ***
Northern.latitude -0.15787
0.01824
-8.655
< 2e-16 ***
Location1
0.93676
0.16486
5.682 1.33e-08 ***
Location2
-1.98743
0.21121
-9.410
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1

13.5 Example — Unprovoked Shark Attacks in the United States
273
The location effect has been fit using effect codings, with the two
reported coefficients corresponding to Atlantic and Gulf states. This implies
a coefficient of −(0.93676 −1.98743) = 1.05067 for Pacific states. The very
similar coefficients for Atlantic and Pacific states suggests a simpler dichotomy
of states with ocean shores versus states without ocean shores, which is fit
below using an indicator variable:
AIC
BIC
logLik deviance df.resid
522.5
533.6
-257.2
514.5
116
Scaled residuals:
Min
1Q
Median
3Q
Max
-2.4936 -0.7836 -0.1573
0.7539
3.8989
Random effects:
Groups Name
Variance Std.Dev.
State
(Intercept) 0.06583
0.2566
Number of obs: 120, groups:
State, 8
Fixed effects:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-11.78378
0.61499 -19.161
<2e-16 ***
Northern.latitude
-0.15788
0.01874
-8.426
<2e-16 ***
Ocean
2.97597
0.31151
9.553
<2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
If a particular state with its given presence or absence of ocean shore
had an increase of one degree of latitude of its northernmost shore point
(corresponding to roughly 69 miles further north), this would be associated
with a per capita shark attack rate that is estimated to be 14.6% lower
(e−0.15788 = 0.854). A particular state with its given northernmost shore
latitude is estimated to have e2.97597 = 19.6 times the per capita shark attack
rate with an ocean shore compared to without one. Pearson residuals from
the Poisson fit (Figure 13.7) indicate unmodeled variance heterogeneity across
states; a negative binomial fit improves this somewhat, and has lower marginal
AIC value, but the estimated slopes for the fixed effects are virtually identical
to those based on the Poisson fit:
AIC
BIC
logLik deviance df.resid
509.6
523.6
-249.8
499.6
115
Scaled residuals:
Min
1Q
Median
3Q
Max
-1.5432 -0.7178 -0.1774
0.6728
3.3566
Random effects:
Groups Name
Variance Std.Dev.
State
(Intercept) 0.06107
0.2471

274
CHAPTER 13 Models for Longitudinal and Nested Data
A
C
F
H
N
O
S
T
−2
0
1
2
3
4
Poisson
State
Pearson residuals
A
C
F
H
N
O
S
T
−1
0
1
2
3
Negative binomial
State
Pearson residuals
FIGURE 13.7: Pearson residuals separated by state for Poisson and negative
binomial GLMM fits, respectively, based on northernmost shore latitude and
existence of ocean shoreline for national shark attacks data.
Number of obs: 120, groups:
State, 8
Fixed effects:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-11.78915
0.61716 -19.102
<2e-16 ***
Northern.latitude
-0.15762
0.01879
-8.391
<2e-16 ***
Ocean
2.97255
0.31280
9.503
<2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
These two mixed models are of course subject-specific models, which
accounts for the interpretation of coefficients being based on the qualifier “for
a particular state” (since coefficients only can be interpreted given the predicted
random effect). A population average version of the model might be more
natural here, as its slopes would correspond to relationships averaged over all
states in the country for which shark attacks are not extraordinarily rare. For
these data the distinction between the two interpretations is apparently minor,
however, as output for both the most complex and simplest model show:
Coefficients:
Estimate Naive S.E.
Naive z
(Intercept)
-9.252820987 0.34613974 -26.7314614
Years.since.2001
0.008425843 0.01284015
0.6562109
Northern.latitude -0.175599789 0.02640522
-6.6501929
Southern.latitude -0.003260956 0.03019793
-0.1079861
Location1
1.084089206 0.10655934
10.1735729
Location2
-2.030181565 0.18866568 -10.7607361
Estimate Robust S.E.
Robust z
(Intercept)
-9.252820987
0.31894118 -29.01105787
Years.since.2001
0.008425843
0.01230902
0.68452617
Northern.latitude -0.175599789
0.02189927
-8.01852272
Southern.latitude -0.003260956
0.03437863
-0.09485415
Location1
1.084089206
0.05408196
20.04530177

13.6 Summary
275
Location2
-2.030181565
0.08614606 -23.56673791
-------------------------
Coefficients:
Estimate
Naive S.E.
Naive z
(Intercept)
-11.2303878 0.381168189 -29.46308
Northern.latitude
-0.1778969 0.009337034 -19.05283
Ocean
3.0775357 0.266537000
11.54637
Estimate Robust S.E.
Robust z
(Intercept)
-11.2303878
0.35624072 -31.52472
Northern.latitude
-0.1778969
0.01149784 -15.47221
Ocean
3.0775357
0.11446791
26.88558
Inference on and estimation of the slopes is virtually identical in the two
approaches, indicating that in this case the distinction between subject-specific
and population average interpretations is not important.
13.6
Summary
Nested (clustered) and longitudinal data occur often in practice, and ignoring
this structure can lead to potentially highly misleading data analytic implica-
tions. Mixed models provide a systematic way to account for this structure,
allowing flexibility in both response (linear, generalized linear, or nonlinear)
and within-subject correlation (e.g., independence, compound symmetry, or
autoregressive) structure. Such subject-specific models focus on associations
between within-subject changes in response and within-subject changes in
predictors, while population average models focus on associations between
changes in the overall expected response and changes in predictor values. It is
important to note that the complex nature of both the underlying processes
and the associated methodologies leads to subtleties in analysis that have
not been discussed here. These are discussed in detail in many book-length
treatments of the subject, including Singer and Willett (2003), Gelman and
Hill (2007), Fitzmaurice et al. (2009), and Scott et al. (2013).
KEY TERMS
Compound symmetry: A correlation structure in which all observations in a
group are equally correlated with each other.
Fixed effect: An effect in a model that reflects a relationship between the
response and a predictor determined by fixed unknown parameters. A grouping
variable in which the categories represent all of the ones of interest is consistent
with a fixed effect.
Generalized estimating equations (GEE): An approach to estimating the
parameters of a generalized linear model in the presence of correlation
structure in the responses, such as a nested or longitudinal structure.

276
CHAPTER 13 Models for Longitudinal and Nested Data
Generalized linear mixed effects model (GLMM): A mixed effects model
designed for a non-Gaussian response variable, in which a generalized linear
model is expanded to include random effects in the linear predictor.
Intraclass correlation (ICC): A measure of how strongly observations in the
same group relate to each other. It can also be interpreted as the proportion of
variability in the responses accounted for by the variability between groups.
Linear mixed effects model (LMM): A mixed effects model in which both
fixed and random effects enter linearly.
Mixed effects model: A model that includes both fixed effects and random
effects.
Multilevel model: A model constructed to represent a hierarchical random
process in which parameters vary at more than one level.
Nonlinear mixed effects model: A mixed effects model in which either fixed
or random effects or both enter nonlinearly.
Population average model: A model that represents the relationship between
predictors and responses, averaged over all subjects in the population. This is
also referred to as a marginal model.
Random effect: An effect in a model that reflects a relationship between
the response and a predictor determined by a random variable. A grouping
variable in which the categories represent a random sample from a population
of categories is consistent with a random effect.
Random intercepts model: A linear mixed effects model in which the
intercepts vary (due to random effects) across groups but the slopes do not.
Random slopes model: A linear mixed effects model in which the intercepts
and slopes vary (due to random effects) across groups.
Restricted maximum likelihood (REML): An estimation scheme based on
maximizing a transformed version of the log-likelihood that separates the
information for the parameters of the fixed effects from that for the parameters
of the variance terms, resulting in unbiased estimates for the latter terms.
Subject-specific model: A model that represents the relationship between
predictors and responses, conditional on a particular subject.

Fourteen
Chapter
Regularization Methods
and Sparse Models
14.1
Introduction
277
14.2
Concepts and Background Material
278
14.2.1 The Bias–Variance Tradeoff
278
14.2.2 Large Numbers of Predictors and Sparsity
279
14.3
Methodology
280
14.3.1 Forward Stepwise Regression
280
14.3.2 Ridge Regression
281
14.3.3 The Lasso
281
14.3.4 Other Regularization Methods
283
14.3.5 Choosing the Regularization Parameter(s)
284
14.3.6 More Structured Regression Problems
285
14.3.7 Cautions About Regularization Methods
286
14.4
Example — Human Development Index
287
14.5
Summary
289
14.1
Introduction
In Chapter 2 we discussed the benefits and costs of regression model selec-
tion, and in particular the two problems of underfitting and overfitting.
Omitting important predictors is clearly undesirable, as it makes the model
a poor representation of the true underlying relationship and results in poor
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
277

278
CHAPTER 14 Regularization Methods and Sparse Models
predictions. Including unnecessary predictors leads to overly complex repre-
sentations of the underlying relationship, and can also lead to poorer future
predictions because of the lower likelihood that this complex relationship will
remain stable over time.
The focus in Chapter 2 was on best subsets regression as a way to
choose a set of “best” models on which attention should be focused. An
important restriction of best subsets is its computational limitations; even with
very powerful computers exact best subsets is not computationally feasible
for more than roughly 35 potential predictors. While recent advances in
optimization have loosened that restriction somewhat, at least in producing
an approximation to best subsets (Bertsimas et al., 2016), it is still not feasible
for the very large data sets that are becoming more common (with sample
sizes and number of potential predictors both potentially in the thousands or
more). In this chapter we discuss regularization methods, which can feasibly
lead to simplified models even in the presence of an extremely large number of
potential predictors. Of course, a natural question is whether that is actually
a worthwhile goal in that context; it is when only a relatively small number
of the potential predictors are important that this is likely to be an effective
strategy, a situation termed sparsity.
We first discuss how these methods can be viewed as providing a tradeoff
between accuracy of estimation of the expected response given the predictors
and precision of this estimation. This bias-variance tradeoff is fundamentally
connected to the tradeoff of fit versus complexity discussed in Section 2.3.1.
Regularization methods proceed through shrinkage (in the sense of
Section 13.2.1), in which estimates of regression slopes are shrunk towards or
to zero. One such method, the lasso, along with its variants, provides model
simplification by forcing estimated coefficients to be exactly zero. We describe
how model selection criteria like those discussed in Section 2.3.1 can be used
to control how much sparsity will be imposed on the fitted model. These
regularization principles can also be applied to other regression problems,
including ANOVA and ANCOVA models and generalized linear models.
14.2
Concepts and Background Material
14.2.1
THE BIAS–VARIANCE TRADEOFF
Consider a general regression relationship
y = f(x) + ε.
A natural way to assess the effectiveness of a regression prediction ˆf(x) is
through the squared error of prediction
SE = [y −ˆf(x)]2
and its expected value (mean)
MSE = E{[y −ˆf(x)]2}.

14.3 Concepts and Background Material
279
Some algebra shows that MSE can be decomposed into three parts,
MSE = {Bias[ ˆf(x)]}2 + Var[ ˆf(x)] + σ2,
(14.1)
where
Bias[ ˆf(x)] = E[ ˆf(x)] −f(x)
is the bias of the estimator,
Var[ ˆf(x)] = E({ ˆf(x) −E[ ˆf(x)]}2)
is the variance of the estimator, and σ2 is the variance of the errors.
Equation (14.1) shows that effective prediction depends on both accuracy
and precision: what is desired is an estimator of f(x) that “aims” at the right
value (low bias), and an estimator that “gets close” (low variance). An example
of this is the principle of parsimony described in Section 2.1: make a model
as simple as possible (resulting in smaller variance) while still accounting for
the important relationships in the data (resulting in smaller bias). This is also
the basis of measures like Cp, AIC, and AICc, which are designed to be
(approximately) unbiased estimates of prediction error, and also include terms
that trade off fit versus complexity.
The decomposition of (14.1), however, has implications beyond merely
balancing overfitting versus underfitting. Some estimators, including the
regularization methods of this chapter, explicitly trade off bias versus variance
in their construction, with low-bias versions having naturally higher variance
and high-bias versions having naturally lower variance. Thus, it is possible in
principle to “tune” the estimator to set these properties against each other and
find the most effective compromise. This can be done using the same model
selection criteria that were used to balance overfitting against underfitting in
Section 2.3.1.
Having said this, it is important to remember that regression is useful for
both model building (understanding underlying relationships, perhaps with
an eye to causal relationships that would imply changing inputs with changed
consequences) and prediction. The bias-variance tradeoff is a predictive notion,
and methods aimed at “optimal” prediction might not be optimal for model
building purposes.
14.2.2
LARGE NUMBERS OF PREDICTORS AND SPARSITY
The advent of high speed computing has made possible the construction
of larger and larger data sets, both in terms of sample sizes and number of
variables recorded. Analysis of regression data with a very large number
of predictors naturally suggests model simplification, but as was noted in
Section 2.3.1 this is most appropriate when there is a relatively small number
of relatively strong effects. In the context of large numbers of predictors, this
corresponds to sparsity, where only a small proportion of predictors have
nonzero effects. The methods discussed in this chapter are most effective in this
circumstance.

280
CHAPTER 14 Regularization Methods and Sparse Models
14.3
Methodology
14.3.1
FORWARD STEPWISE REGRESSION
Best subsets regression attempts to compare all 2p∗possible regression models
(where p∗is the maximum number of predictors considered, as in
Section 2.3.2), which is computationally infeasible for large p∗. A much
more efficient way of examining possible models is in a step-by-step manner,
adding variables one at a time based on some notion of the variable being
“best” at that step (typically that it gives the largest gain in predictive power to
the current model). This is far less computationally intensive than attempting
to examine all possible models.
Unfortunately, this strategy has an obvious weakness: examining variables
one at a time ignores how they might work together. It is easy to construct
artificial data in which two variables have no predictive power by themselves,
but have great (even perfect) predictive power together. If the variables are
tested for whether they should be brought into the model based on the gain
in predictive power each provides, neither variable would be brought in, and
the true relationship would be completely missed.
This problem can be avoided to a large extent, however, through the use
of forward stepwise selection. Rather than stopping the process prematurely,
forward stepwise continues to bring in variables one at a time until all variables
are stepped in. Then, the p∗+ 1 candidate models (including the one with no
predictors) can be compared using a predictive criterion, such as Cp or AICc.
An alternative criterion is K-fold cross-validation. This criterion is based
on randomly splitting the observations into K “folds” (typically K = 5 or
10). For each fold the model is fit on the other K −1 folds, with the fitted
model then used to predict the responses in the omitted fold. The quality of
these predictions is evaluated using a criterion squared errors, which are then
summed to give an estimate of the predictive error of the model; the parallel
with assessing a model based on prediction of an independent validation
(holdout) sample (page 36) is apparent. This is clearly more computationally
intensive than using Cp or AICc, as it requires K additional fits of each model,
and since the folds are determined randomly the final chosen model based
on a given data set is also itself random. A specific version of cross-validation
that avoids this randomness is leave-one-out cross-validation, in which each
observation is the only member of a fold, and K = n. K-fold cross-validation
provides both a predictive score and an estimate of its standard error, and in
order to combat the tendency of cross-validation to overfit it is sometimes
suggested to choose the simplest model with cross-validation score that is
within one standard error of the minimizer of the score (the so-called one-SE
rule), although there is no theoretical justification for this strategy.
An inherent flaw of forward stepwise is that it can tend to choose models
that include unnecessary predictors, precisely because of the fact that it ignores
how variables work together (if two particular variables are needed in the
model they might not both be in a candidate model until other variables

14.3 Methodology
281
have been stepped in). Despite this, best-case predictive performance when
choosing models using forward stepwise tends to be very competitive with
best-case performance choosing models using best subsets (Hastie et al., 2017),
as the additional variance that comes from overfitting is much less impactful
than the similar bias of the two approaches.
14.3.2
RIDGE REGRESSION
The notion of how bias and variance can be traded off against each other
is illustrated by the ridge regression method. Ridge regression was first
proposed for use in the context of collinear predictors, since (as was discussed
in Section 2.2.2) the presence of collinearity leads to inflation of the variance
of the least squares estimates.
Assume that the predictors are standardized to have zero mean and unit
standard deviation (this is not at all restrictive, since estimated coefficients can
be easily transformed back to the original scale after fitting). Ridge regression
proceeds by minimizing a penalized (regularized) form of the residual sum of
squares,
n

i=1
⎛
⎝yi −β0 −
p

j=1
βjxij
⎞
⎠
2
+ λ
p

j=1
β2
j ≡RSS + λ
p

j=1
β2
j ,
(14.2)
where RSS is the residual sum of squares. This is equivalent to solving the
constrained problem of minimizing the residual sum of squares subject to the
constraint
p

j=1
β2
j ≤s.
(14.3)
That is, depending on the value of λ, or equivalently s, ridge regression
imposes a restriction on the total magnitude of the estimated slopes, and
thereby shrinks them towards zero, reducing their variance. A third equivalent
representation for ridge regression is that the estimates satisfy
β = (X′X + kI)−1X′y.
Comparing this to (1.4) shows that unlike the least squares estimator ˆβ, the
ridge estimator is biased, with bias
E(β −β) = [(X′X + kI)−1X′X −I]β.
There always exists at least one value of λ (or s or k) such that the MSE of β
is smaller than that of the least squares estimator ˆβ, but it is a function of the
unknown β.
14.3.3
THE LASSO
Unlike best subsets and forward stepwise, ridge regression does not simplify
models by reducing the number of predictors with nonzero slopes (it merely

282
CHAPTER 14 Regularization Methods and Sparse Models
shrinks all of the least squares slopes towards zero). Remarkably, a simple
change to the regularized form of the residual sum of squares (14.2) accom-
plishes this, in the form of the lasso, which minimizes
n

i=1
⎛
⎝yi −β0 −
p

j=1
βjxij
⎞
⎠
2
+ λ
p

j=1
|βj| ≡RSS + λ
p

j=1
|βj|.
(14.4)
Minimizers of this criterion have the property that, like those of the ridge
criterion (14.2), the estimated slopes are shrunk towards zero, but unlike in
ridge regression some of those estimated slopes are forced to be exactly zero.
Thus, the lasso combines the shrinkage of ridge regression with the model
simplification of best subsets or forward stepwise.
Figure 14.1 illustrates this property of the lasso on the home price data
of Sections 1.4, 2.3.2, and 3.4. The figure plots the estimated lasso slope
estimates as a function of λ (the predicting and response variables have all
been standardized to have standard deviation equal to 1 so that the slopes
are on a similar scale). The ability of the lasso to “zero out” slope estimates
0.0
0.1
0.2
0.3
0.4
0.5
0.6
–0.2
–0.1
0.0
0.1
0.2
0.3
0.4
0.5
λ
Slopes
Bedrooms
Bathrooms
Living area
Lot size
Year built
Property tax
FIGURE 14.1: Lasso slope estimates for the Levittown house data as a
function of the regularization parameter λ.

14.3 Methodology
283
as λ increases is apparent from the plot. Clearly the choice of λ is crucial
in constructing lasso estimates, and this will be discussed in Section 14.3.5.
Informally, Figure 14.1 suggests that the number of bathrooms and the living
area are likely to be deemed important in any reasonable fit, perhaps along
with the year the house was built, which is in line with the best subsets analysis
of Section 2.3.2.
Just as was true for ridge regression, the lasso estimator can be formulated
in an equivalent way as a constrained minimization problem, only now instead
of (14.3) the criterion is to minimize the residual sum of squares subject to
p

j=1
|βj| ≤s.
(14.5)
The connection of these two regularization methods with best subsets is clear,
since the best subsets regression estimates with s predictors are defined as the
minimizers of the residual sum of squares subject to there being no more than
s nonzero slopes, or
p

j=1
I(βj ̸= 0) ≤s.
(14.6)
The constrained estimator based on squared coefficients (14.3) shrinks the
coefficients but does not force them to zero, the estimator based on the number
of nonzero coefficients (14.6) forces coefficients to zero but does not shrink
them, while the estimator based on absolute coefficients (14.5) does both.
14.3.4
OTHER REGULARIZATION METHODS
The lasso constraint (14.5) implies that if a few variables (or one variable)
have large effects on the response, the smaller, but nonzero, effects of other
predictors must be more aggressively shrunk towards zero to compensate. For
this reason the lasso is less effective in situations with a smaller number of larger
effects (higher signal-to-noise) due to the effects of bias. Correspondingly,
best subsets and forward stepwise are less effective in situations with a larger
number of smaller effects (lower signal-to-noise) due to the effects of increased
variance.
Several regularization-related methods have been proposed that can poten-
tially improve on the shrinkage-caused bias of the lasso. A conceptually natural
idea is “lasso-plus-least squares,” in which lasso is used to choose the set
of predictors with nonzero coefficients (based on a chosen regularization
parameter value λ), and then the usual least squares estimate of β, ˆβLS,λ, is
determined using that set of variables. Efron et al. (2004) referred to this as
a “LARS-OLS hybrid” estimator (referring to the LARS algorithm that can
be used to construct the lasso regularization path), and Flynn et al. (2017)
showed that this strategy can improve performance in high signal-to-noise
situations.

284
CHAPTER 14 Regularization Methods and Sparse Models
The relaxed lasso (Meinshausen, 2007) adds a second regularization
parameter φ to the penalty, being the minimizer of
n−1
n

i=1
⎛
⎝yi −β0 −

j∈Mλ
βjxij
⎞
⎠
2
+ φλ
p

j=1
|βj|,
where Mλ is the set of predictors with nonzero slopes based on the regular-
ization parameter λ, while φ ∈(0, 1] controls the amount of shrinkage of the
estimated coefficients. The ordinary lasso corresponds to φ = 1, while φ →0
produces lasso-plus-least squares. A simplified version of the estimator (Hastie
et al., 2017) is a weighted average of the lasso and least squares estimates
given λ,
γˆβlasso,λ + (1 −γ)ˆβLS,λ,
with γ ∈[0, 1].
Regularization methods generalize directly to generalized linear models.
The minimized criterion reflects a penalty on the deviance rather than the
residual sum of squares, with similar implications of sparsity and shrinkage.
14.3.5
CHOOSING THE REGULARIZATION PARAMETER(S)
It is clear that the choice of the regularization parameter(s) is crucial for the
effectiveness of regularization methods. There are obvious parallels between
this choice and choosing the appropriate model in best subsets or forward
stepwise, and similar approaches are available for this task. Information
measures like Mallows’ Cp and AICc can be evaluated on a grid of λ (or other
regularization parameter) values, with the ultimate choice being the minimizer
of the criterion. As was noted on p. 38, however, care must be taken in
implementing such measures. These criteria penalize complexity of a model
using degrees of freedom, but the standard notion of degrees of freedom can
be very misleading in this context.
Roughly speaking, the effective degrees of freedom of a predictive method
is the effective number of parameters used in producing the fitted values.
This is evidently p + 1 for a specific linear regression model with p predictors,
and that is the number used in the penalties of information criteria like AIC
(2 × [p + 1]) and BIC (log n × [p + 1]). The situation when searching over all

p∗
p

models with p predictors to find the one with minimum residual sum
of squares, however, as is done in best subsets (recall that p∗is the number of
predictors in the most complex model considered), is different; that uses more
information in the data, and has a larger effective degrees of freedom. The
effective degrees of freedom is also directly proportional to the optimism of
the method; that is, the difference between the mean squared prediction error
on the original data set (the training set) and that on a new set of data (the test
set). This is the motivation behind the adjustment to the standard error of the
estimate proposed in (2.4), which takes the effective degrees of freedom to be
p∗+ 1, a conservative choice. What this also means, however, is that the use of

14.3 Methodology
285
information measures to choose among models in best subsets with degrees of
freedom defined in the usual way, as described in Section 2.3.1, is not correct,
and can lead to overly complex models, particularly when p∗is large (the same
is true for forward stepwise selection). Tibshirani (2015) provides extensive
discussion of these issues.
Regularization methods face the same kind of increase in degrees of
freedom from the optimization of the penalized residual sum of squares for
a given λ (since that also involves “looking at” the data in a complex way),
with one remarkable exception. The shrinkage of the lasso slopes towards zero
results in a decrease in the effective degrees of freedom that perfectly balances
the increase from optimization, with the result that the effective degrees of
freedom equals the expected number of predictors with nonzero slopes for a
given λ. This suggests that the minimizer of AIC (or AICc) over the range of
λ (the so-called regularization path) for lasso estimation, taking the observed
number of nonzero slopes as the effective degrees of freedom for any given λ,
should provide an efficient selector, and Flynn et al. (2013) show that that is,
in fact, the case.
K-fold cross-validation avoids these issues, since it does not require
determination of effective degrees of freedom, but it comes with a price.
Since it requires K additional fits (one for each fold), it is roughly K
times more computationally intensive. Further, as was noted earlier, unless
K = n (leading to leave-one-out cross-validation), the minimizer of the
cross-validation criterion is itself a random variable, which means that different
implementations of K-fold cross-validation on the same data set can lead to
different choices of regularization parameter. This is probably not that serious
an issue from a predictive point of view, since the different choices are likely
to lead to similar predictive performance (since the cross-validation criterion is
itself a predictive one), but it could have unfortunate consequences if variable
selection is of particular importance (since the variables with nonzero slopes
can potentially change, perhaps considerably, for different choices of λ).
14.3.6
MORE STRUCTURED REGRESSION PROBLEMS
The simple structure of the lasso optimization problem allows it to be
generalized in natural ways to more complex regression structures. Consider,
for example, the ANOVA and ANCOVA models of Chapters 6 and 7,
corresponding to situations with categorical predictors. As was discussed in
those chapters, predictors of that type are easily incorporated into linear
regression models using representations based on indicator variables or effect
codings. It would be straightforward to include such variables into a standard
lasso formulation, but that would result in general in models where for a
given λ some indicators or effect codings for a particular categorical predictor
are included, while others are not. This, of course, does not correspond to
the underlying AN(C)OVA model, and different fitted models would be
implied for different choices of underlying variables (indicator variables or
effect codings).

286
CHAPTER 14 Regularization Methods and Sparse Models
An alternative that avoids this problem is the group lasso. Consider as a
simple example the ANCOVA model with one categorical predictor (7.1),
yij = μ + αi + β1x1ij + · · · + βpxpij + εij, i = 1, . . . , K, j = 1, . . . , ni.
The equivalent regression formulation of this model using effect codings that
omit the Kth category would be
yij = μ + α1E1ij + · · · + αK−1E(K−1)ij + β1x1ij + · · · + βpxpij + εij.
The ordinary lasso penalty would treat all of the slopes (both α and β) the
same, allowing any of them to be forced to zero. The group lasso, in contrast,
uses a different penalty for the α terms, being the minimizer of

ij
{[yij −μ −(α1E1ij + · · · + αK−1E(K−1)ij + β1x1ij + · · · + βpxpij)]2
+ λ[|β1| + · · · |βp| +
	
α2
1 + · · · + α2
K−1]}.
This penalty has the effect of driving estimates of individual entries of β to
zero as usual, but forces either none of the entries of α to zero or all of them
to zero, effectively either including the categorical predictor in the model or
not including it. Generalizing the penalty to allow for multiple categorical
predictors and interactions follows in a straightforward way.
14.3.7
CAUTIONS ABOUT REGULARIZATION METHODS
Regularization methods clearly have great appeal, particularly in a world with
potentially massive amounts of data, as they provide model simplification in
a remarkably efficient way. Despite this, it is important to recognize potential
drawbacks and issues in their use.
It is easy to characterize the model simplification of regularization methods
as “they do model selection,” but that is not really the case. The very fact that
algorithms to provide all of the parameter estimates in the regularization path
extremely efficiently exist implies that this path “looks at” only a small portion
of the possible models. This is a guided search, of course, eliminating most
models that are not viable candidates, but it does mean that it is impossible to
know if a useful model has been missed.
The simple form (14.4) of the lasso criterion (and in particular the absolute
value penalty) makes it natural to apply in a wide variety of problems. In any
such applications the shrinkage of the lasso described in Section 14.3.3 will
arise, and this has important implications. If there is a “true” model, a lasso
estimate using only those predictors with nonzero slopes could predict poorly,
because the slope coefficients are biased. Thus, the use of consistent model
selectors like BIC, which are designed to find the “true” model (as discussed
on page 34), is not well-motivated for the lasso, since reasonable models from
the point of view of residual sum of squares will overfit to reduce the bias.
Predictive-based selectors like cross-validation applied to the lasso are likely to

14.4 Example — Human Development Index
287
overfit as well, as including unnecessary predictors can help reduce the bias of
the slopes of important predictors and improve prediction accordingly.
A different justification for the lasso as a model selection tool comes
from so-called oracle inequalities, which provide guarantees for the worst-case
squared predictive error when there is a true model; in particular that this error
is not greater than a factor of log(p) times the error if the true set of predictors
was known a priori and used in a least squares fit. This would seem to imply
that including many irrelevant predictors among the candidate predictors will
not hurt performance very much, providing implicit support for making the
set of candidate predictors as large as possible (and letting the lasso “decide”
which ones to ultimately keep).
Unfortunately, these inequalities are based on using a particular determin-
istic choice of λ that is unknown, when (as discussed in Section 14.3.5) λ is in
fact chosen in a data-dependent way in practice. Flynn et al. (2017) showed
that the predictive performance of the lasso when using a data-dependent
choice of λ deteriorates far more dramatically when including an increasing
number of irrelevant predictors than oracle inequalities would imply. This can
be particularly problematic for the ordinary lasso based on categorical pre-
dictors (since interaction effects can greatly increase the number of potential
underlying predictors), but the group lasso limits this by restricting the model
search by including or omitting sets of predictors together corresponding to
such effects (Flynn et al., 2016).
14.4
Example — Human Development Index
The Human Development Index (HDI) is a statistic produced by the Human
Development Report Office of the United Nations Development Programme,
designed to measure national levels of human development. Rather than being
based solely on economic measures like national income, it also includes
information about lifespan and education level. The HDI is an exact function
(the geometric mean) of standardized measures of life expectancy, expected
and achieved years of education, and per capita gross national income, but it
is of interest to explore how general socioeconomic characteristics of a country
relate to its human development.
The analysis presented here is based on data from, and is described more
fully in, United Nations Development Programme (2018), which is provided at
www.kaggle.com/undp/human-development. We investigate models
using 28 socioeconomic variables that are not part of the definition of HDI,
including population size, character, and growth variables, age distribution
variables, labor force participation and other economic variables, measures
of health and health expenditure variables, and internet usage variables. In
this analysis all variables are scaled to have zero mean and unit standard
deviation. The predictors provide strong predictive power for HDI, resulting

288
CHAPTER 14 Regularization Methods and Sparse Models
Table 14.1: Coefficient estimates of selected models using different methods
(lasso, lasso-plus-least squares, best subsets and forward stepwise) for human
development index data.
Lasso
Lasso-
plus-OLS
BS / FS
Annual pop. growth 2000-2005
0.09595
Urban population pct.
0.13687
0.14878
0.12114
Pct. population < 5
−0.03091
−0.03847
Pct. population 15 −64
0.00087
0.02310
Pct. population 65+
0.00010
−0.00007
Median age
0.12189
0.05732
Aged dependency ratio
0.01843
0.06680
0.13641
Fertility rate 2000-2005
−0.07780
−0.07737
−0.22259
Infants lacking DTP immun.
−0.01128
−0.03566
Infant mortality rate
−0.19440
−0.17273
−0.24492
Mortality rate < 5
−0.03690
−0.05113
Labor force part. rate
−0.01765
−0.04538
Unemployment rate
−0.03621
−0.06989
−0.05288
Old age pension recipient pct.
0.09106
.10440
0.12901
Internet user pct.
0.31785
0.308170
0.28021
Internet user growth 2010-2015
−0.05596
−0.06282
−0.07745
Models are chosen based on 10-fold cross-validation, and the chosen best subsets and
forward stepwise models are identical.
in R2 = 95.0% in the regression on all predictors, but many variables exhibit
collinearity, and only six of the predictors have t-statistics with statistically
significant slopes at a .05 level: annual population growth, urban population
percentage, fertility rate, percentage of people above the statutory pension age
receiving a pension, and internet usage percentage and growth.
Table 14.1 summarizes the results of application of the lasso, best subsets,
and forward stepwise to these data. In all cases the appropriate models are
chosen as being the minimizers of the 10-fold cross-validated squared error. All
of the methods simplify the model considerably. The lasso model includes 15
predictors, with the least squares model based on those predictors accounting
for 94.4% of the variability in HDI. As expected, the lasso coefficients are
shrunk towards zero relative to the corresponding least squares coefficients
(11 of the 15 lasso slopes are closer to zero than are the lasso-plus-least squares
slopes, some by a relatively large amount). The chosen lasso model seems
to be overspecified (this tendency was noted in Section 14.3.7), with several
unnecessary predictors. Even uncorrected for model selection uncertainty the

14.5 Summary
289
only lasso-plus-least squares estimates that have p-values less than .05 are
urban population percentage, unemployment rate, percentage of people above
the statutory pension age receiving a pension, and internet usage percentage
and growth. The simplified version of the relaxed lasso would be a weighted
average of the lasso and lasso-plus-least squares estimates, although obviously
it would still have 15 nonzero slope estimates.
The best subsets and forward stepwise models that minimize the
cross-validation score each include the same nine predictors, even though
the subsets for fewer numbers of predictors are not the same (for example,
the best subsets models with three through eight predictors include median
age, which is not included in any of the forward stepwise models, and while
infant mortality is included in all forward stepwise models starting with three
predictors, it is not included in best subsets models for fewer than eight
predictors). The final chosen model accounts for 94.3% of the variability in
HDI (recall that the model using all available predictors accounts for 95.0%
of the variability in HDI). Eight of the nine predictors are included among
the predictors with nonzero lasso slopes, annual population growth being the
only exception. It is also the only predictor for which the p-value (uncorrected
for model selection uncertainty) is greater than .01 (it has p = .02).
Given the very similar fits of the lasso-plus-least squares model and best
subsets/forward stepwise model, it seems reasonable to suppose that the latter
model accounts for the important effects, while using one-third of the available
predictors and 40% fewer predictors than the lasso. The interpretations of the
final slope estimates are intuitive. Internet usage is highly correlated with gross
national income, so its strong positive association with HDI (given the other
included predictors) is presumably playing a primary role in being a proxy
for income in the model’s representation of HDI. Further, infant mortality
rate is highly negatively correlated with life expectancy, so its strong negative
association with HDI (given the other included predictors) is presumably
playing a primary role in being a proxy for life expectancy in the model’s
representation of HDI. Finally, fertility rate is highly negatively correlated
with years of schooling, so its strong negative association with HDI (given
the other included predictors) is presumably playing a primary role in being a
proxy for years of schooling in the model’s representation of HDI. These three
variables comprise the three-predictor forward stepwise model, and together
account for 91.6% of the variability in HDI, supporting this interpretation.
14.5
Summary
Regularization methods and forward stepwise regression are powerful tools
for model simplification in an era of potentially enormous numbers of
predictors. Combined with a method like K-fold cross-validation they provide
a systematic and efficient way to sort through a potentially massive number
of possible regression models. Having said that, it must be remembered that

290
CHAPTER 14 Regularization Methods and Sparse Models
their properties are less well-understood than is perhaps realized, and care
must be taken in their application. In particular, the problems of post-selection
inference (statistical inference using a model selected in a data-dependent way)
are magnified as the number of possible models increases. This is an active
area of research, and it is not clear which (if any) approach(es) will ultimately
become standard; see Berk et al. (2013) and Tibshirani et al. (2016), for
example, for further discussion.
That being said, there are still basic principles that can be followed. One
such principle is that while these methods are applicable in high-dimensional
situations that were once difficult or impossible to handle, subject area context
still matters in the formulation of candidate models and choice of candidate
predictors. Automatic screening of potential predictors before applying model
simplification methods can also help, but its effect on the properties of
estimators is unclear. This doesn’t even consider the model violation issues
discussed in Chapters 3 through 6 (the effects of unusual observations, the
need for transformations, violations of assumptions including autocorrelation
and heteroscedasticity), which would be very challenging to automate. Further,
statistical models are still important, as they can provide structure that helps
limit the effects of model search and model selection uncertainty.
This chapter has only provided an overview of simpler regularization
approaches; this is a very active area of research, and other more complex
versions have been proposed, including ones that are designed to address the
bias-related problems of the lasso. Further discussion of such methods can be
found in Bühlmann and van de Geer (2011) and Hastie et al. (2015).
KEY TERMS
Bias-variance tradeoff: The balancing of bias versus variance of a statistical
estimator. This is fundamentally related to balancing overfitting versus under-
fitting, as the former results in lower bias but higher variance, while the latter
results in higher bias but lower variance, and is also reflected in the fact that
the mean squared error of prediction of an estimator is the sum of the squared
bias and the variance.
Effective degrees of freedom: Roughly speaking, the effective number
of parameters used by an estimator in producing the fitted values. It is
proportional to the optimism of the estimator.
Forward stepwise regression: A regression model selection method in which
a predictor is added to an existing model on the basis of it adding the most
predictive power given the current set of predictors, continuing until all of
the potential predictors have been included. The resultant models are then
compared using some sort of information criterion or cross-validation, with a
set of “best” models resulting.
Group lasso: A lasso variant in which the regularization penalty is constructed
so that all of the underlying indicator or effect codings used to define a
categorical predictor or interaction are forced to either be nonzero or zero for

14.5 Summary
291
any given regularization parameter value, therefore effectively either including
or omitting the corresponding effect from the model.
K-fold cross-validation: A predictive criterion in which data are randomly
split into K subsets (“folds”) and a prediction method or model is then
constructed on K −1 of them and validated on the data in the omitted one.
The sum of these predictive errors for each model is an estimate of the squared
prediction error of that method or model.
Lasso: A regularization method in which the penalty function is the sum of
the absolute regression slopes. This results in slope estimates that are shrunk
either towards zero compared to the least squares estimates or exactly to zero,
thereby performing model simplification.
Lasso-plus-least squares estimator: The hybrid estimator formed by con-
structing an ordinary least squares fit on the predictors with nonzero slopes in
the lasso estimator based on a given regularization parameter.
Leave-one-out cross-validation: A variation of K-fold cross-validation in
which K = n, the sample size (that is, the validation samples are each
observation, omitted one at a time).
Optimism: The difference between the mean squared prediction error of an
estimator on the original data set and that on a new set of data. It is directly
proportional to the effective degrees of freedom of the estimator.
Post-selection inference: Statistical inference methods that are designed to
account for the effects of model selection uncertainty when a model is chosen
among a set of candidate models in a data-dependent way.
Regularization: The adding of a term to a loss function to impose a penalty
for complexity on an estimated function. The effect of the penalty term
depends on the chosen value of a regularization parameter or parameters.
Examples in the context of regression include ridge regression and the lasso
and its variants.
Relaxed lasso: A regularization method that adds a second regularization
parameter to the lasso penalty term, with the intention of controlling the
amount of shrinkage towards zero of the slope estimates with nonzero
estimated slopes. A simplified version of this estimator is a simple weighted
average of the lasso and lasso-plus-least squares estimators.
Ridge regression: A regularization method in which the penalty function is
the sum of the squared regression slopes. This results in slope estimates that
are shrunk towards zero compared to the least squares estimates.
Sparsity: A situation in which there is a large set of potential regression
predictors, but only a small proportion of the predictors have nonzero effects.
An assumption of sparsity is necessary to justify regularization methods like
the lasso that force regression slope estimates to zero.

Part Six
Nonparametric and
Semiparametric Models

Fifteen
Chapter
Smoothing and Additive
Models
15.1
Introduction
296
15.2
Concepts and Background Material
296
15.2.1 The Bias–Variance Tradeoff
296
15.2.2 Smoothing and Local Regression
297
15.3
Methodology
298
15.3.1 Local Polynomial Regression
298
15.3.2 Choosing the Bandwidth
298
15.3.3 Smoothing Splines
299
15.3.4 Multiple Predictors, the Curse of Dimensionality,
and Additive Models
300
15.4
Example — Prices of German Used Automobiles
301
15.5
Local and Penalized Likelihood Regression
304
15.5.1 Example — The Bechdel Rule and Hollywood
Movies
305
15.6
Using Smoothing to Identify Interactions
307
15.6.1 Example — Estimating Home Prices
(continued)
308
15.7
Summary
310
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
295

296
CHAPTER 15 Smoothing and Additive Models
15.1
Introduction
Chapter 4 (linearizable models) and Chapter 12 (nonlinear models) both
address situations where a linear relationship between the response and the
predictors is not appropriate, but in each case the exact form of nonlinearity
needs to be specified beforehand, a strong requirement. In many situations it is
not clear a priori what form the relationship actually takes, and the possibility
of being able to estimate that from the data becomes attractive. This leads
to the use of nonparametric regression methods, and specifically smoothing
methods.
Smoothing methods are based on the idea that while the precise form of
the relationship is unknown, it is reasonable to think that it corresponds to a
smooth relationship. So, for example, in the situation with a single predictor,
the simple regression model depicted in Figure 1.1 is generalized to
yi = m(xi) + εi,
where m(·) is a smooth curve and the goal is to construct an estimate ˆm(·).
There are various ways to do this, but all are based on the notion of borrowing
information about E(y|x0) from observations that are “near” x0.
15.2
Concepts and Background Material
15.2.1
THE BIAS–VARIANCE TRADEOFF
In Section 2.3 the essential tradeoff of strength of fit versus parsimony in model
selection was the driving force in strategies for choosing among candidate
models. The connection of this concept to a tradeoff of bias versus variance
when choosing a regularization parameter was the focus of Section 14.2.1.
Although it might not be obvious, the application of smoothing methods is
in fact an example of model selection, and this same bias-variance tradeoff is
fundamental in an even more direct way.
Consider Figure 15.1. The two plots correspond to two random draws
from an underlying regression relationship, with three potential estimates for
each sample. The solid lines are the linear least squares regression lines. The
two lines are very similar to each other from one sample to the next, reflecting
low estimation variance, but are obviously biased estimates of the true m(·),
being systematically low or high in different regions of x. This is analogous to
fitting a regression with too few predictors. In contrast, the dotted-and-dashed
lines interpolate the observed y values at each value of x. For this estimate the
bias is very low (being zero at the observed y values), but there is obviously
great variability from sample to sample. This is analogous to fitting a regression
with too many predictors.
What is needed is a compromise that tracks like the dashed curve in each
plot, which tracks the underlying curve by using local patterns. This estimate

15.2 Concepts and Background Material
297
0
1
2
3
4
5
0
5
10
15
20
x
y
0
1
2
3
4
5
0
5
10
15
20
x
y
FIGURE 15.1: Two samples from the same underlying smooth relationship
with three estimates: solid least squares lines, dotted-and-dashed interpolating
lines, and dashed smooth curves.
balances bias and variability, being far less variable than the interpolating curve
and far less biased than the linear model. How to produce estimates like the
dashed curve is the subject of Sections 15.3.2 and 15.3.3.
15.2.2
SMOOTHING AND LOCAL REGRESSION
As noted earlier, the idea underlying smoothing is to use local information
to construct estimates. This is illustrated in Figure 15.2, which is based on
the same data as those given in Figure 15.1. In order to estimate m(·) at a
particular value x0 a natural idea is to fit a linear regression, but only using
observations that are close to (in the x-neighborhood of) x0, and then estimate
m(x0) as the fitted value from that regression at x0. The solid lines in the plots
give such a fitted line based on the closest 20% of the observations for several
values of x. This approach clearly has the desired property of tracking the
underlying relationship, but the estimates seems to have some trouble tracking
the steeper hills and valleys of the curve. There’s nothing that requires a linear
fit at each point, and the dashed lines in Figure 15.2 illustrate quadratic least
0
1
2
3
4
5
0
5
10
15
20
x
y
0
1
2
3
4
5
0
5
10
15
20
x
y
FIGURE 15.2: Linear and quadratic least squares fits using the nearest 20%
observations at different x-values for the data from Figure 15.1.

298
CHAPTER 15 Smoothing and Additive Models
squares regression fits based on the closest 20% of the observations (note that
in each plot the linear and quadratic fits are virtually identical at one given
x-value). The quadratic fits can track the peaks and valleys of the curve more
closely, but at the cost of more variability from one sample to another. This is
another example of the bias versus variance tradeoff that arises in smoothing.
Figures 15.1 and 15.2 illustrate that the key decisions to make when
smoothing using local regression fits are the type of local model to use and the
size of the local neighborhood. These choices are made explicit in Section 15.3.
15.3
Methodology
15.3.1
LOCAL POLYNOMIAL REGRESSION
Figure 15.2 illustrates the essence of local polynomial estimation: estimate
m(x) using a regression estimate from observations near x. There are then
three details that need to be addressed. First, it makes sense to take observations
closer to x as more important in estimation of m(x), since chances are the
expected response values for those observations will be closer to m(x); that is,
performing locally weighted least squares estimation should be more effective.
The estimator ˆm(x) is thus the minimizer of the sum of weighted squared
residuals,
n

i=1
[yi −β0 −· · · −βp(x −xi)p]2K
x −xi
h

,
(15.1)
where K(·) is a kernel function that is symmetric and unimodal around 0.
Note that in this chapter we follow the usual convention of using p to
represent the degree of the local polynomial, and will use d to represent the
number of predictors. Typically functions like the Gaussian density function
(or similarly-shaped bounded curves) are used, and it can be shown that the
choice of kernel is not crucial to the properties of the estimate ˆm(x), which
is the estimated intercept term of the local regression. There are theoretical
reasons to choose the degree of the local polynomial to satisfy p ≥1 (the
Nadaraya-Watson kernel estimator takes p = 0, and is known to suffer from
increased bias at the boundaries of the estimation region), and typically p = 1
(local linear estimation) or p = 2 (local quadratic estimation) is chosen.
15.3.2
CHOOSING THE BANDWIDTH
The bandwidth h determines how large the local neighborhood is, and has the
most crucial effect on the properties of the local polynomial estimator. As seen
in Figure 15.1, a small bandwidth (with the interpolating function effectively
having h = 0) results in less bias but more variance, while a large bandwidth
(with the estimate becoming a single polynomial fit as the bandwidth increases)
results in less variance but more bias. This suggests choosing a bandwidth that
explicitly forces a tradeoff between the bias and variance. Indeed, it can be

15.3 Methodology
299
shown that for large samples the bias is of order hp+1 if p is odd and hp+2 is p is
even (reinforcing that smaller h corresponds to smaller bias), while the variance
is of order (nh)−1 (reinforcing that larger h corresponds to smaller variance),
implying that the bandwidth that minimizes mean squared error is of order
n−1/(2p+3) for p odd and of order n−1/(2p+5) for p even. This implies that the
estimator does not achieve the usual parametric √n convergence rate, which
is the price paid for not having to assume a specific parametric form for m(·).
Unfortunately the optimal bandwidth is a function of unknown quantities
such as the (p + 1)st derivative of m(·) and σ2, but so-called “plug-in” methods
estimate these values from the data. Another approach takes advantage of the
fact that the local polynomial estimator is a linear method as in (1.5), and
therefore AICc can be used to choose the bandwidth. Specifically, the local
polynomial estimate satisfies
ˆmp(x) = e′
1(X′
xWxXx)−1X′
xWxy ≡Sxy,
where er is the (p + 1) × 1 vector having the value 1 in the rth entry and zero
elsewhere, Xx is the design matrix
Xx =
⎛
⎜
⎜
⎝
1
x −x1
(x −x1)p
...
...
. . .
...
1
x −xn
(x −xn)p
⎞
⎟
⎟
⎠,
Wx is the weight matrix
Wx = h−1 diag

K
x −x1
h

, . . . , K
x −xn
h

,
and
Sx = e′
1(X′
xWxXx)−1X′
xWx.
(15.2)
This means that the fitted value at xi satisfies (15.2) with x = xi, making the
effective degrees of freedom (df) of the overall fit the sum of the ith diagonal
elements of the matrices Sxi, which is then substituted into the standard
formula for AICc
AICc = AIC + 2(df + 1)(df + 2)
n −df −2
.
The local polynomial estimator defined in (15.1) uses a fixed (constant)
bandwidth h, but another possibility is to have the bandwidth vary with
location, for example by having the local neighborhood correspond to a number
of observations that is a fixed percentage of the total sample size (this percentage
is referred to as the span of the estimate). Local polynomial estimators of this
sort are often referred to as loess estimators.
15.3.3
SMOOTHING SPLINES
The local polynomial estimator is not the only approach to smoothing-based
nonparametric regression. A popular alternative is the smoothing spline

300
CHAPTER 15 Smoothing and Additive Models
estimator. The smoothing spline is a regularization method in the same sense
as in Chapter 14, except that now the regularizing term is a penalty to encourage
smoothness, rather than a penalty to encourage sparseness. Specifically, the
cubic smoothing spline is defined as the minimizer of
L = 1
n
n

i=1
[yi −m(xi)]2 + α

m′′(u)2 du
over an appropriate class of smooth functions. Broadly speaking, this estimator
has similar large-sample properties to those of the local quadratic estimator.
Once again AICc can be used to choose the smoothing parameter (which is α
here), although a more common choice is to use generalized cross-validation,
in which α is chosen to minimize
GCV (α) =
n
i=1 [yi −ˆm(xi)]2
n{1 −n−1 trace[S(α)]}2 ,
where S(α) is the smoothing spline version of the hat matrix that corresponds
to the version (15.2) for local polynomials.
15.3.4
MULTIPLE PREDICTORS, THE CURSE
OF DIMENSIONALITY, AND ADDITIVE MODELS
In principle local polynomial estimation extends to the situation of multiple
predictors. The local linear estimator is
n

i=1
[yi −β0 −β1(x1 −x1i) −· · · −βd(xd −xdi)]2Kd[H−1(x −xi)], (15.3)
where Kd is a multivariate kernel function, H is a bandwidth matrix, and the
d × 1 vector x corresponds to the set of predictor variables. Unfortunately,
the same type of large-sample analyses discussed in Section 15.3.2 imply that
the optimal rate of convergence gets slower as d gets larger. That is, there
is a curse of dimensionality that implies that massive amounts of data will be
required for accurate estimation as the number of predictors increases.
These difficulties can be avoided through the use of an additive model,
as in
m(x) = α +
d

j=1
fj(xj).
Univariate local polynomial or smoothing spline smoothers can be used to
estimate the functions fj(·), thereby avoiding the curse of dimensionality,
with the convergence rate of ˆm(·) identical to that of the one-dimensional
smoother. This comes, however, at a crucial cost: if the additive form is not
correct, the estimator ˆm(·) need not even be consistent. The additive model has
an attractively simple interpretation, as the structure related to any predictor
is defined in a univariate way conditional on the other smooth functions
in the model. Note that taking some of the fj(·) functions to be linear
(fj(xj) = βjxj) is possible, resulting in a so-called semiparametric model.

15.4 Example — Prices of German Used Automobiles
301
15.4
Example — Prices of German Used
Automobiles
Hedonic pricing models were used in Sections 1.4 and 2.3.2 to represent the
sale price of houses, but such models also can be applied to other situations in
which it is sensible to think that the price of an item depends on its underlying
characteristics. One such situation is the pricing of automobiles, and in
particular used automobiles. This analysis is based on data on 3840 autos
scraped from eBay Kleinanzeigen on March 5, 2016, obtained from the Used
cars database on kaggle.com. Here logged (base 10) price asked for the auto
in an advertisement is modeled as a function of the year of registration and
kilometers driven of the auto, along with indicators for whether it has a manual
transmission and runs on diesel fuel. Figure 15.3 indicates a clearly nonlinear
marginal relationship between logged price and registration year, with higher
prices for newer cars less than twenty years old but higher prices for older cars
more than twenty years old. This is clearly a “classic car” effect; autos more
than twenty years old are much more likely to be collectibles, and for such cars
older is better, while autos less than twenty years old are likely to be for ordinary
use, and for such cars newer is better. The relationship with kilometers driven
is less clear in the plot, although it also appears to be nonlinear (the rounding
1970 1980 1990 2000 2010
2.0
3.0
4.0
5.0
Year of Registration
Logged price
0
50000
100000
150000
2.0
3.0
4.0
5.0
Kilometers driven
Logged price
0
1
2.0
3.0
4.0
5.0
Manual transmission
Logged price
0
1
2.0
3.0
4.0
5.0
Diesel
Logged price
FIGURE 15.3: Scatter plots and side-by-side boxplots for German used cars
data.

302
CHAPTER 15 Smoothing and Additive Models
off to the nearest 5000 or 10,000 kilometers is also clear). Further, autos with
automatic transmissions and those that run on diesel fuel tend to have higher
prices.
A semiparametric model based on local quadratic loess estimators for
year of registration and kilometers driven and constant shifts for type of
transmission and type of fuel is fit to the data. AICc can be used to choose
the smoothing parameters for the two smooth terms (spans of 15% for year
of registration and 42% for kilometers driven, respectively), which leads to
the curves in the top two plots of Figure 15.4. The curves seem to be a
bit undersmoothed, and the bottom plots use larger spans (30% and 50%,
respectively) to provide more aesthetically pleasing curves. The output below
summarizes the fit of the latter model.
Anova for Parametric Effects
Df Sum Sq Mean Sq
F value
Pr(>F)
lo(Year.of.Registration)
1970 1980 1990 2000 2010
–0.5
0.0
0.5
AICc choice
Year.of.Registration
0
50000
100000
150000
–0.02
0.02
0.06
AICc choice
Kilometers
1970 1980 1990 2000 2010
–0.4
0.0
0.4
Smoother choice
Year.of.Registration
0
50000
100000
150000
–0.02
0.02
0.06
Smoother choice
Kilometers
FIGURE 15.4: Estimated smooth terms of semiparametric model for German
used cars data. The top two plots refer to the model minimizing AICc, while
the bottom two plots refer to a model with larger spans that leads to more
smoothing.

15.4 Example — Prices of German Used Automobiles
303
1.0 375.97
375.97 4517.831 < 2e-16 ***
lo(Kilometers)
1.0
1.32
1.32
15.873 6.9e-05 ***
Manual
1.0
36.40
36.40
437.354 < 2e-16 ***
Diesel
1.0
8.61
8.61
103.445 < 2e-16 ***
Residuals
3826.6 318.44
0.08
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Anova for Nonparametric Effects
Npar Df Npar F
Pr(F)
(Intercept)
lo(Year.of.Registration)
5.5 305.05 < 2.2e-16 ***
lo(Kilometers)
2.9
6.25 0.0003826 ***
Manual
Diesel
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Both indicator variables are highly statistically significant; given the other
predictors an auto with manual transmission has estimated listed price that
is 40.3% lower than an auto with automatic transmission (the estimated
slope is ˆβ = −0.2237, and 10−.2237 = 0.597), and an auto that uses diesel
fuel has estimated listed price that is 29.4% higher than an auto that uses
gasoline (the estimated slope is ˆβ = 0.1119, and 10.1119 = 1.294). The first
part (parametric effects) of the output that refers to the smooth terms
corresponds to the linear part of the conditional relationship of the variable
with logged price given the other variables, but the second part (nonparametric
effects) shows that the additional nonlinear (smooth) part is also extremely
important. The curves in Figure 15.4 show that as noted earlier there are
clear and opposite relationships between year of registration and logged
price centered at 1995-1996, corresponding to autos roughly twenty years
old. The curve for kilometers driven shows that while there is a generally
inverse relationship between logged price and kilometers driven, as would
be expected (the estimated slope of the linear part is ˆβ = −9.035 × 10−7,
implying an estimated 2.1% lower estimated price associated with an increase
of 10,000 kilometers driven overall), there is a severe drop in logged price
when kilometers driven exceeds 100,000 kilometers, perhaps pointing to a
psychological effect of that round number.
The usefulness of smoothing here is clear from a predictive point of view.
If the semiparametric model and a linear model are each applied to the data
for the 2234 autos scraped on March 6, 2016 (the next day), predicted logged
prices based on the semiparametric model have higher correlation with the
actual logged prices compared to predicted logged prices from the linear model
(0.80 versus 0.73), smaller root mean squared error (0.313 versus 0.356), and
the absolute prediction errors are smaller for the semiparametric model for
almost 60% of the autos.

304
CHAPTER 15 Smoothing and Additive Models
15.5
Local and Penalized Likelihood Regression
Local polynomial and smoothing spline estimators, being based on minimiz-
ing squared residual terms, are natural in the context of continuous response
variables where Gaussian errors are appropriate, but other probability distri-
butions are appropriate for other types of data. This, of course, is the principle
that underlies generalized linear modeling, as discussed in Section 10.2.2. The
local polynomial likelihood estimator for a single predictor, for example,
adapts (15.1) to the log-likelihood for the generalized linear model (10.1) as
ˆβ0, where ˆβ is the maximizer of
n

i=1
yi[β0 + · · · + βp(x −xi)p] −b[β0 + · · · + βp(x −xi)p]
a(φ)
+ c(yi, φ)

× K
x −xi
h

.
Appropriate choices of the random component and link function lead to
smooth estimation for binary data (generalizing logistic regression) and count
data (generalizing Poisson regression).
Smoothing spline estimation also generalizes to the more general penalized
likelihood estimation. In this case the roughness penalty estimate analogous
to the cubic smoothing spline of the mean function η(x) using the canonical
link is the minimizer of
−1
n
n

i=1
{yiη(xi) −b[η(xi)]} + α

η′′(u)2 du.
Local and penalized likelihood estimators based on an underlying Poisson
random component have uses beyond the generalization to allow for smooth
terms of the log-linear count regression models discussed in Section 10.2.2. In
Section 10.2.1 it was noted that the Poisson random variable has a close connec-
tion to the multinomial random variable, in that the joint distribution of a set
of independent Poisson random variables conditional on the total number of
counts is distributed as a multinomial. This gives a natural approach to smooth-
ing the counts in a contingency table with ordered categories, something
that is particularly effective for sparse tables with many small (or zero) counts,
as the counts in the table can be smoothed using a Poisson likelihood using
either local likelihood or penalized likelihood, taking the cell index (or indices)
as a predictor variable (or variables).
Just as was true for the situation with Gaussian errors, local and penalized
likelihood smoothers can be used as components of an additive or semipara-
metric model in the presence of multiple predictors. In this case the resultant
additive model is termed a generalized additive model, in analogy with the
generalization of a linear model to a generalized linear model.

15.5 Local and Penalized Likelihood Regression
305
15.5.1
EXAMPLE — THE BECHDEL RULE AND HOLLYWOOD
MOVIES
In 1985 cartoonist Alison Bechdel formulated the “Bechdel Rule” to try to
quantify evidence of gender bias in Hollywood. In order to pass the rule, a
movie must satisfy three criteria: there are at least two named women in the
picture, they have a conversation with each other at some point, and that
conversation is not about a male character. The following analysis is based
on data about 1794 films provided by Hickey (2014). The data provides a
binary response for each movie as to whether it passes the Bechdel test, and
we examine the connection between this and the year of release of the movie
and its logged budget in 2013 dollars (natural logs are used here since the
canonical link for binary data is the logit link, as was done in Chapter 8).
Figure 15.5 gives side-by-side boxplots of the variables separated by
whether the movie satisfies the Bechdel Rule. The plots reveal little difference
between the two groups with respect to year of release, and only a small
difference with respect to logged budget, with movies that satisfy the rule
having somewhat lower budgets. Since action movies are less likely to satisfy the
rule, and are more likely to have a larger budget, this pattern is not surprising.
A logistic regression fit to the data is given below.
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-40.55466
10.96437
-3.699 0.000217 ***
Year
0.02171
0.00548
3.962 7.43e-05 ***
Logged budget
-0.18232
0.03544
-5.144 2.69e-07 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 2467.3
on 1793
degrees of freedom
Residual deviance: 2425.2
on 1791
degrees of freedom
AIC: 2431.2
0
1
1970
1990
2010
Pass Bechdel test
Year
0
1
10
14
18
Pass Bechdel test
Logged budget
FIGURE 15.5: Side-by-side boxplots of year of release and logged budget in
2013 dollars separated by whether the movie passes the Bechdel test for a
sample of 1794 films.

306
CHAPTER 15 Smoothing and Additive Models
Both predictors are highly statistically significant. Given (logged) budget, each
increase in year of release is associated with a 2.2% increase in the odds of a
movie passing the Bechdel test. Further, given year of release, a 1% increase
in budget in 2013 dollars is associated with a 0.18% decrease in the odds of a
movie passing the test.
Unfortunately, this model is apparently not appropriate for these data,
as shown in Figure 15.6. Figure 15.6 gives plots of the standardized Pearson
residuals from the logistic regression fit versus each of the predictors, with
loess curves superimposed on the plots. If the linear logistic relationship was
appropriate the curves would be expected to be roughly horizontal, reflecting
a lack of structure in the residuals, but it is clear that that is not the case here.
Rather, there is evidence of nonlinearities in both terms corresponding to
changes in direction of the relationship in the early 2000s and for movies with
budgets greater than $25 million in 2013 dollars. Note that there is nothing
special about superimposing smooth curves on residual plots from a logistic
regression fit; this is a sensible thing to do when plotting residuals for any
regression fit, including least squares fits.
The generalized additive fit based on a binomial random component and
logit link function with minimum AICc using local quadratic loess estimators
has span of 74% for each variable, and results in an AICc value that is more
than 35 lower than that of the (linear) logistic regression model, indicating
a much better fit. Figure 15.7 gives the resultant estimated smooth terms.
They are similar to the smooth curves superimposed on the logistic regression
residual plots, but reflect changes in direction at smaller values. Specifically,
after a steady increase in the (log) odds of a movie passing the Bechdel test
given its budget from 1970 to 2000, the pattern changed, with the increase
leveling off (it should be remembered that the curve reflects the relationship
given the underlying linear trend on year, which is positive with estimated
coefficient 0.023, so the reversal around 2000 corresponds to a leveling
off rather than a decrease in absolute terms). Further, taking into account
the general inverse relationship between logged budget and the odds of a
1970
1990
2010
–2.0
–0.5
0.5
1.5
Year
Standardized residuals
10
12
14
16
18
20
–2.0
–0.5
0.5
1.5
Logged budget
Standardized residuals
FIGURE 15.6: Plots of standardized Pearson residuals from logistic regression
fit of passing the Bechdel test on year of release and logged budget in 2013
dollars versus each predictor, with loess curves superimposed on each plot.

15.6 Using Smoothing to Identify Interactions
307
1970
1990
2010
–1.2
–0.6
0.0
Year
10
12
14
16
18
20
–1.0
–0.5
0.0
Log.budget
FIGURE 15.7: Estimated smooth terms of generalized additive model for
Bechdel test data.
movie passing the Bechdel test (with estimated slope −0.183) there is little
relationship between budget and those odds for films with a budget (in 2013
dollars) less than roughly $10 million, but a very strong inverse relationship for
movies with budgets greater than that, given the year of release. This is not a
very large budget, especially for an action movie, so clearly there is more going
on here than simply a possible male preference for action. One potential factor
could be that low-budget films have smaller casts, giving less opportunity
for characters to interact with each other, but it is hard to imagine many
movies not passing a male version of the Bechdel test no matter how small the
budget is.
15.6
Using Smoothing to Identify Interactions
Linear, generalized linear, additive, and generalized additive models all have
the restriction of the lack of interaction between predictors; the expected
relationship between the response and a predictor given the presence of the
other predictors is the same for all values of the other predictors (specified by
the slope of the predictor for a linear model and the shape of a smooth curve
for an additive model). It is not unreasonable to question this assumption, and
hypothesize that the relationship between the response and a predictor could
be different for different values of another predictor.
Handling such interactions was of course a fundamental part of the use
of indicator variables to fit different slopes for different groups (Section 2.4),
analysis of variance modeling with interaction effects (Section 6.2.2), and
analysis of covariance modeling with different slopes for different groups
(Section 7.2.2). The key to fitting such models came from constructing the
products of variables and including them in the regression modeling. A crucial
point in those examples, however, was that at most one numerical variable was
involved in those products, with the other variable(s) being indicator or effect
coding variables being used to represent a categorical variable.

308
CHAPTER 15 Smoothing and Additive Models
The fact that in those cases multiplying predictors leads to appropriate
interaction modeling has unfortunately led to the practice of trying to model
interactions between numerical variables by adding the product of those
variables to a (generalized) linear model. Consider the simplest example of
an analysis based on only two predictors x1 and x2. In order to include an
interaction between x1 and x2 the augmented model
yi = β0 + β1x1i + β2x2i + β3x1ix2i + εi
(15.4)
is fit, but this is not a very good idea for several reasons. First, using the t-test
for whether β3 = 0 as an interaction test potentially results in errors of both
types, Type I (mistakenly identifying a pattern that does not correspond to
an interaction effect as an interaction) and Type II (mistakenly deciding that
no interaction effect is present when it actually is), no matter how large the
sample is or how strong the underlying relationships are. Equation (15.4) is
not an interaction model, but rather a nonlinear one (in the x’s). If the actual
relationship between y and one of the predictors is nonlinear, this nonlinearity
can easily be mistaken for an interaction if x1 and x2 are correlated with each
other, resulting in a Type I error. Further, while (15.4) does imply that the
expected change in y associated with a one unit change in x1 differs for different
values of x2 (this is measured by the partial derivative of y with respect to x1,
which is β1 + β3x2), this is a very specific form of an interaction. If the pattern
of how the relationship between y and x1 differs for different values of x2 does
not satisfy that pattern it could easily be missed by the t-test for β3, resulting in
a Type II error. Even if the test is statistically significant, there is no reason to
think that the estimated slopes would be interpretable in any meaningful way.
A solution to all of these problems is to appeal to multiple-predictor
smooth terms and additive models. Equation (15.3) gives the formulation
of local polynomial estimation for multiple predictors. Such terms for pairs
of numerical predictors provide a natural way of representing interactions
between those predictors, since the estimated smooth surface does not put
any requirement of the lack of such interactions on the model. A graphical
representation of that surface provides an effective way of at least informally
exploring the possibility of an interaction. In particular, slices of the surface
along one variable for given values of the other variable can be plotted and
examined to see if the estimated pattern changes as the values of the second
variable change, in direct analogy with the varying slope plot of Figure 2.5
and the interaction plots of Figure 6.1 and Figure 6.5.
15.6.1
EXAMPLE — ESTIMATING HOME PRICES
(CONTINUED)
In Section 2.3.2 various model selection criteria were used for the Levittown
home price data to ultimately arrive at the linear model based on the number of
bathrooms, the living area, and the year the house was built as the final model.
It is reasonable to wonder if the relationship between the sale price of a home
and its size might be different based on the age of the house, with newer homes

15.6 Using Smoothing to Identify Interactions
309
perhaps being more efficient in their use of living area and thus having less of a
penalty for smaller size. This can be assessed using a semiparametric model that
includes the number of bathrooms as a linear term and a bivariate smooth loess
term of living area and the year the house was built. The model that minimizes
AICc is based on a span of 57% for the smooth term. The slopes of the linear
components of the model are similar to those from the linear model given on
page 35, indicating direct relationships between the number of bathrooms,
the living area, and the year the house was built (given the others), but taking
into account the bivariate smooth term indicates that more is going on.
Figure 15.8 summarizes the pattern. It is a trellis display of the estimated
home prices for each home, adjusted for the number of bathrooms by
subtracting the estimated linear component for that variable, separated by the
ten different years in which houses in the data were built (1948, 1949, 1950,
1951, 1952, 1953, 1954, 1955, 1961, and 1962). The subplots in the display
are ordered in increasing year from bottom left to top right. It is apparent that
Living area
Adjusted estimated price
200000
250000
300000
350000
1000 2000 3000
Year.built
Year.built
1000 2000 3000
Year.built
Year.built
Year.built
Year.built
Year.built
200000
250000
300000
350000
Year.built
200000
250000
300000
350000
Year.built
1000 2000 3000
Year.built
FIGURE 15.8: Trellis display of estimated home prices from the model
including a bivariate smooth term of living area and year the house was built,
adjusted for the number of bathrooms, for the home price data.

310
CHAPTER 15 Smoothing and Additive Models
there is indeed an interaction effect here. The estimated relationship between
living area and home price (accounting for the number of bathrooms in the
house) is most positive for the oldest homes, and then becomes progressively
smaller for later years. Indeed, for homes built from 1952 on there is little
evidence of any relationship at all between living area and estimated home
price, with all of the houses having adjusted estimated price between $250,000
and $300,000 (remember that unlike the earlier plots for the smooth effects
in this chapter these plots do not adjust for any underlying linear effect,
so the lack of a relationship from 1952 on is not given some underlying
relationship, but is rather directly interpretable). Thus it appears that for
older homes buyers expect more room for their money, but for newer homes
this is apparently not very important.
15.7
Summary
Smoothing methods provide a bridge between purely nonparametric methods
that make virtually no assumptions and fully parametric methods that make
specific and explicit assumptions by making the relatively weak assumption
of smoothness. They are also an alternative to linear models and nonlinear
models, which both make strong assumptions about the functional relationship
between the response and the predictors. This flexibility comes with a price,
however, as estimates of the underlying relationship cannot achieve parametric
rates of convergence even if the underlying smoothness assumptions hold.
These problems become progressively more serious as the number of predictors
in the smooth term increases, but this can be avoided through the use of
additive models of smooth terms. This again comes at a cost, however, since if
the additive form is not appropriate the resultant estimates need not even be
consistent.
We have focused here on local polynomial estimation, but there are
other approaches to smoothing as well, in addition to smoothing splines.
These include convolution kernel methods, regression splines, and penalized
splines. Simonoff (1996) discusses many of these methods, as well as other
applications of smoothing, including density estimation. The connections
between nonparametric regression, smoothing sparse contingency tables, and
density estimation are the subjects of Chapter 6 of Simonoff (1996) and of
Simonoff (1998b). More extensive discussions of local polynomial estimation
(Fan and Gijbels, 1996) and smoothing splines (Eubank, 1999) are available.
Ruppert et al. (2003) discusses penalized spline estimation, noting in partic-
ular the connection between such smoothers and the multilevel models of
Chapter 13. Hastie and Tibshirani (1990) discusses many aspects of additive
and generalized additive modeling. Stasinopoulos et al. (2017) describes the
very general GAMLSS (generalized additive models for location, scale, and
shape) family, which extends generalized linear and additive models to allow
for a wide variety of response variable distributions, with the location, scale,

15.7 Summary
311
and shape parameters of the distributions being linear or additive functions of
the predictors.
In this chapter we have focused mostly on the exploratory aspects of non-
parametric regression methods, discussing how estimated smooth terms can
help identify unstructured (and often unanticipated) nonlinearities in regres-
sion relationships. Inference for smoothing-based models is more challenging.
Although smoothers are linear estimators for Gaussian data (and asymptot-
ically linear using likelihood-based analyses for other types of data), simply
appealing to standard OLS- and WLS-based tests and intervals is not valid,
since any smoothing parameters are determined in a data-dependent way.
This is completely analogous to the difficulties in appealing to theory based
on a fixed regularization parameter for regularization methods, as discussed
in Section 14.3.7. For this reason any attempts to formalize the impressions
provided by estimated smooth curves should be viewed with caution.
KEY TERMS
Additive model: A regression model based on additive terms of predictors,
estimated using nonparametric smoothing methods.
Bandwidth: The scaling parameter used to control the local weighting of
observations around an evaluation point in local (likelihood) polynomial
estimation. When the kernel function is positive on [−1, 1] it is the half-width
of the local neighborhood over which a regression is fit to estimate the value of
the underlying smooth function. The bandwidth can either be a fixed value
or can vary locally.
Curse of dimensionality: The pattern corresponding to increasing rates of
bias of nonparametric regression estimators for a given sample size with an
increasing number of predictors, which results in potentially massive amounts
of data being needed to effectively smooth in high dimensions.
Generalized additive model: A model that generalizes the additive model
to non-Gaussian data by fitting smooth terms in the appropriate likelihood
space, in the same way generalized linear models generalize linear models.
Kernel function: The weight function that determines how much effect local
observations have on the estimation of an underlying smooth function in local
polynomial estimation.
Local polynomial likelihood estimator: A nonparametric regression estima-
tor that generalizes the local polynomial estimator to non-Gaussian data by
fitting local regressions in the appropriate log-likelihood space.
Local polynomial estimator: A nonparametric regression estimator based
on fitting local weighted least squares regressions to estimate an underlying
smooth function. The local regressions fit are usually local linear or local
quadratic regressions.
Loess estimator: A local polynomial estimator, typically local quadratic and
based on a locally-varying bandwidth that is designed to cover the set of

312
CHAPTER 15 Smoothing and Additive Models
observations closest to the evaluation point that is a fixed percentage of the
total sample size (which is termed the span of the estimator).
Penalized likelihood estimator: A nonparametric regression estimator that
generalizes the smoothing spline estimator to non-Gaussian data by replacing
the residual sum of squares with a term based on the appropriate log-likelihood.
Semiparametric model: In the smoothing context, an additive model in
which some variables enter as linear terms while others enter as smooth
functional terms.
Smoothing spline estimator: A nonparametric regression estimator based on
minimizing a criterion that adjusts (regularizes) the residual sum of squares
with a function that penalizes roughness of the resultant estimate. The most
common choice of penalty is based on the squared second derivative of the
underlying function, leading to a cubic smoothing spline.
Span: The fixed percentage of the total sample size that is covered by the
local regression in a local polynomial estimator that is based on this form of a
locally-varying bandwidth, such as a loess estimator.

Sixteen
Chapter
Tree-Based Models
16.1
Introduction
314
16.2
Concepts and Background Material
314
16.2.1 Recursive Partitioning
314
16.2.2 Types of Trees
317
16.2.2.1 Regression Trees
317
16.2.2.2 Classification Trees
317
16.3
Methodology
318
16.3.1 CART
318
16.3.2 Conditional Inference Trees
319
16.3.3 Ensemble Methods
320
16.4
Examples
321
16.4.1 Estimating Home Prices (continued)
321
16.4.2 Example — Courtesy in Airplane Travel
322
16.5
Trees for Other Types of Data
327
16.5.1 Trees for Nested and Longitudinal Data
327
16.5.1.1 Example — Wages of Young People
327
16.5.2 Survival Trees
328
16.5.2.1 Example — The Survival of Broadway
Shows (continued)
330
16.5.2.2 Example — Female Heads of Government
(continued)
332
16.6
Summary
332
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
313

314
CHAPTER 16 Tree-Based Models
16.1
Introduction
The nonparametric regression models of Chapter 15 were designed to loosen
the restrictions of specified parametric models, whether linear or nonlinear,
but they still depended on the assumption that the underlying regression
relationships could be represented through smooth curves or surfaces. Further,
effective generalization to multiple predictors was based on additive functions
of those predictors. These assumptions might not hold in a particular situation,
and smoothing methods might be ineffective as a result.
A more flexible approach is through the use of methods designed to seg-
ment the predictor space. These methods are based on the idea of constructing
regions such that responses of observations are similar to each other within
a region, but can be different from each other across regions. Through the
use of simple segmenting rules these methods provide a nonparametric way
of representing a regression relationship. If the rules are based on repeatedly
partitioning the predictor space, the resulting estimate takes the form of an
easily-interpretable tree.
16.2
Concepts and Background Material
16.2.1
RECURSIVE PARTITIONING
The construction of tree methods is based on the notion of recursive
partitioning. Consider the data presented in Figure 16.1. The data consist
of two predictors x1 and x2 and a numerical response y. The plots in Figure 16.1
suggest a moderately strong linear relationship between y and x2, and no
relationship between y and x1. Further, there appears to be nonconstant
variance related to x1, with less variability for smaller values of x1. A regression
fit apparently confirms these impressions, as only x2 is statistically significant,
R2 ≈40%, ˆσ = 12.4, and there is apparent nonconstant variance in the
residuals.
0
10
20
30
40
50
20
40
60
80
x1
y
0
10
20
30
40
50
20
40
60
80
x2
y
FIGURE 16.1: Scatter plots of simulated data.

16.2 Concepts and Background Material
315
0
10
20
30
40
50
0
10
20
30
40
50
x1
x2
FIGURE 16.2: Scatter plot of predictors of simulated data, with the area
of the plotted points proportional to the response value and the predictors
partitioned based on their relationship with the response.
Figure 16.2 provides a different view of the data based on partitioning
the predictor space. In this plot response (y) values are plotted at the
corresponding (x1, x2) coordinates using circles having area proportional
to the value. When x1 ≤25 the response y tends to be moderate for all values
of x2, but when x1 > 25 the value of x2 is important, since y tends to be large
when x2 > 20, and small when x2 ≤20. This partition is recursive, in the
sense that first the data are split by whether x1 is less than or greater than 25,
and then the partition of the space where x1 > 25 is further partitioned
by whether x2 is greater than or less than 20.
This also can be represented as a tree, as given in Figure 16.3(a). Interest-
ingly, the alternative representation in Figure 16.3(b) is equivalent, despite its
misleadingly more complicated appearance, pointing to the flexibility of the
recursive partitioning paradigm.
In fact the data were generated based on the model consistent with
Figures 16.2 and 16.3, with normally distributed errors and σ = 7. This
model can be easily represented as a linear model using two indicator variables:
I(x1 ≤25) and I(x1 > 25 and x2 > 20), yielding a much stronger fit than
the linear model based on x1 and x2 (R2 = 80% and ˆσ = 7.1). The problem,
of course, is that there was little apparent clue in the data of the presence
of this sort of interaction structure without specifically looking for it as a tree
method does.

316
CHAPTER 16 Tree-Based Models
X2
> 25
≤ 20
> 20
≤ 25
Mean=30
Mean=50
Mean=70
X1
(a)
X1
> 20
≤ 25
> 25
≤ 20
Mean=50
Mean=50
Mean=70
X2
Mean=30
X1
≤ 25
> 25
(b)
FIGURE 16.3: Equivalent tree representations of the partitioning in
Figure 16.2.

16.2 Concepts and Background Material
317
The essence of building the tree comes down to two issues: at each step
choosing the variable on which to split (the splitting variable) or choosing not
to split at all, and determining the value at which the variable will be split if
there is a split (the split point). Note that since there is a single split point this
results in a binary tree, with a parent node being split into two child nodes.
The driving principle for deciding these issues is to split the data in such a
way as the two child nodes are more homogeneous. The specific methods
used depend on the type of data being analyzed, with the two most common
examples being regression trees and classification trees.
16.2.2
TYPES OF TREES
16.2.2.1
Regression Trees
A regression tree is the tree-based alternative to a linear least squares regression
model. Given a potential splitting variable xj and potential split point s we
can define two (left and right) child regions
RL(j, s) = {x|xj < s}
and
RR(j, s) = {x|xj ≥s}.
A desirable split is one in which the child nodes are more homogeneous, which
is quantified with an (im)purity measure, such as sum of squares

i:xi∈RL(j,s)
(yi −yRL)2 +

i:xi∈RR(j,s)
(yi −yRR)2,
(16.1)
where yRL (yRR) is the mean response for observations in RL (RR). Given
the choice of xj the value s that minimizes (16.1) is chosen as the split point.
This operation is then repeated, recursively partitioning the predictor
space into T regions {R1, . . . , RT } that constitute the terminal (unsplit)
nodes of the tree. Note that this is actually a linear least squares regression
fit, corresponding to using the indicator variables that define the terminal
nodes. The predicted value for a new observation is found by determining the
appropriate terminal node R for the observation and using the appropriate
yR. The key distinction from the ANOVA models of Chapter 6, of course, is
that these subgroups are determined in a data-dependent way.
The sum of squares measure (16.1) corresponds to the deviance of a
generalized linear model with a Gaussian random component, which suggests
a generalization of the regression tree to other distributional families analogous
to that of generalized linear models. The split point in this case would
correspond to the largest drop in the deviance, based on whatever underlying
distribution for the response is appropriate.
16.2.2.2
Classification Trees
The tree-based method when faced with a categorical response variable , rather
than a numerical one, is the classification tree. The major distinction from

318
CHAPTER 16 Tree-Based Models
the regression tree is in determining the purity measure associated with a node
(region) R. A common choice is the Gini index,
GR =
K

k=1
ˆpk(R)[1 −ˆpk(R)],
(16.2)
where K is the number of categories in the response variable and ˆpk(R)
is the observed proportion of the observations falling in the node R that
have response category k. Since GR takes a smaller value if the underlying
proportions {ˆp1(R), . . . , ˆpK(R)} are close to 0 or 1, a small value corresponds
to a node that is relatively “pure” (most of the observations falling in one
class). The split point s is determined as the value that results in the largest
drop in GR, and the predicted class for a new observation is the one with
largest ˆpk(R) in the appropriate terminal node R.
An alternative purity measure to the Gini index is the cross-entropy
DR = −
K

k=1
ˆpk(R) log ˆpk(R).
(16.3)
The cross-entropy has the property that it is minimized when the underlying
proportions are close to 0 or 1, and indeed is very similar numerically to the
Gini index. Note that its use corresponds to using the deviance for a binomial
or multinomial distribution as suggested in Section 16.2.2.1.
16.3
Methodology
16.3.1
CART
Classification and Regression Trees, or CART (Breiman et al., 1984) is
undoubtedly the implementation of tree methods that has attracted the most
attention among statisticians. CART implements regression tree splitting using
residual sum of squares as in (16.1) and classification tree splitting using the
Gini index as in (16.2), choosing the splitting variable xj and split point s
simultaneously to achieve the largest possible gain in purity from the split over
all candidate variables and split points. The CART approach is to grow the
tree until it is too large (that is, overfit), and then prune it back by trimming
off nodes from the bottom up (the overfit tree typically only stops splitting
when any split would make the number of observations in each terminal node
less than a threshold).
This sort of pruning requires a method of ordering possible trees from
most to least complex (that is, from bottom to top). This is done using
cost complexity pruning, which is a regularization approach in the sense of
Chapter 14. Consider the regression tree problem as an example. A regularized
version of the residual sum of squares of a fitted tree penalizes the fit with a

16.3 Methodology
319
term based on the number of terminal nodes T,
T

t=1

i:xi∈Rt
(yi −yRt)2 + αT,
(16.4)
thereby trading off fit with parsimony. When α = 0 the minimizer is the full
tree, and as α increases branches are pruned from the tree in a specific fashion,
yielding a sequence of nested subtrees from most complex (the full tree) to
simplest (no splitting at all). For a classification tree the tradeoff is based on a
penalized version of the misclassification rate,
T

t=1
[1 −max
k
ˆpk(Rt)]ˆpt + αT,
where ˆpt is the proportion of the sample that falls in terminal node t.
The task then becomes choosing α for a particular data set. CART
accomplishes this via K-fold cross-validation (typically K = 10), estimating
the error rate as the average squared error or misclassification rate, respectively,
over the K folds. Breiman et al. (1984) suggests choosing the value of
α corresponding to the simplest tree that has estimated error rate smaller
than the cross-validated error rate plus the estimated standard error of the
cross-validated error rate (that is, the one-SE rule). This is designed to avoid
potential instability when using the tree that minimizes the cross-validated
error rate, and Simonoff (2013) found that it apparently results in a Type I
error rate of roughly 0.05 (that is, the final estimated tree has at least one split
roughly 5% of the time when the true underlying relationship between y and
x is null). Note that pruning via cross-validation means that the final tree
is itself random, in the sense that repeated application on the same data can
result in different output trees.
16.3.2
CONDITIONAL INFERENCE TREES
The CART approach suffers from a consequence of the algorithm choosing
both the splitting variable and split point at the same time via exhaustive
search: splitting variable selection bias. That is, in the null situation of no
relationship between the response and the predictors the algorithm will prefer
to split on variables with more potential split points (for example a continuous
variable) over those with fewer potential split points (for example a 0/1
indicator variable). This preference also tends to carry over to situations where
there actually is a relationship between the response and variables of different
distributional types (continuous, ordinal, nominal, or binary).
This problem can be addressed by separating the choices of splitting
variable and (given the splitting variable) split point. This can be achieved by
first determining if a global independence null hypothesis can be rejected (that
is, the hypothesis that the response is unrelated to all potential split variables).

320
CHAPTER 16 Tree-Based Models
If it can be rejected, the split variable is chosen based on some measure of
association with the response, and the split point is chosen in the usual way
using (16.1), (16.2), or (16.3); if it cannot be rejected, splitting at that node
stops. Note that this means that repeated application of the algorithm to a
given data set will always yield the same final tree, since the (random) pruning
using cross-validation is avoided.
The conditional inference tree (Hothorn et al., 2006) measures the
association between the response and the splitting variables using statistics of
the form
Tj(Ln, w) = vec
 n

i=1
wigi(xji)h(yi, (y1, ...yn))T

∈Rpj,
(16.5)
where gj : Xj →Rpj is a nonrandom transformation of covariate xj and
h : Y × Yn →R is the influence function of the response y. For a univariate
numeric response y (a regression tree), the choice of influence function is the
identity, i.e. h(yi, (y1, ...yn)) = yi, while for discrete response (classification
trees) with K response categories h (yi, (y1, ...yn)) is a 0/1 vector of length K
identifying the group to which yi belongs. The distribution of Tj is determined
using permutation distributions conditioned on all possible permutations of
the responses. The global test of independence used combines these individual
statistics, adjusting for the multiplicity of the tests for each predictor using
multiple comparison adjustments as in Section 6.3.2, allowing for overall
control of the probability of splitting under the null at level α (with α = .05
roughly corresponding to the one-SE rule of CART, as noted on page 319).
16.3.3
ENSEMBLE METHODS
Tree methods have several very positive attributes as regression estimators.
They are easy to explain and interpret graphically, can straightforwardly
handle both numerical and categorical predictors, are highly flexible and
nonparametric, and are computationally efficient. Unfortunately, they are
highly unstable, in that small changes in the data can lead to large changes
in the fitted tree. A way past this problem is through the use of ensemble
methods. Ensemble methods are based on the principle of reducing the
variability of low bias/high variance methods (like trees) by averaging the
outputs of such methods over repeated samples. If repeated samples were
available from the same population, regression trees could be built on each
sample, with the fitted value for a new observation being the average of the
fitted values from each tree. The trees would be deliberately overfit, in order
to reduce bias. For a classification problem the predicted group would be the
most commonly occurring group among the different groups.
Of course, repeated samples from the same population are not generally
available. Instead, bootstrap aggregation, or bagging, uses the bootstrap by
repeatedly sampling with replacement from the observations in the sample,
creating B bootstrap samples each of size n. An overfit “bagged” tree is fit
on each bootstrap sample, and predictions then come from aggregating those

16.4 Examples
321
fits. It can be shown that on average roughly two-thirds of the observations
are used in each bootstrap sample, leaving for each bagged tree one-third
of the observations as out-of-bag (OOB) observations. This means that each
observation is OOB for roughly B/3 of the bootstrap samples, and for
those bagged trees a prediction can be made for the observation as an
aggregation-based prediction based on the trees. An overall error estimate
from these individual ones is an unbiased estimate of the true prediction error,
and costs no more than the bagging itself.
Since a bagged prediction is based on averaging over many trees, the
easy interpretability of trees is lost, and attempts have been made to help
with this. One such attempt is through a variable importance measure to
summarize how important a variable is in prediction. This can be done by
averaging the reduction in sum of squares or Gini index for regression trees
or misclassification rate for classification trees, respectively, due to any splits
in the bagged trees on that predictor. A different approach is to compare the
predictive power when including a variable among the candidate predictors to
that when instead using a version of the variable where its values have been
randomly permuted (since in the latter case the variable shouldn’t provide any
predictive power).
A potential weakness in the bagging approach is that allowing all of the
predictors to be candidates for splitting in the bagged trees will result in
the individual trees being correlated with each other. If, for example, one of
the variables is a very strong split variable, it will be the first split variable
in every bagged tree, and the trees will look very similar to each other. The
variance of the sum of two positively correlated random variables is greater
than that of two uncorrelated random variables with the same variances, and
therefore bagging will provide less variance reduction than is desired.
This can be avoided through the random forest, in which for each bagged
tree a random subset of the available predictors is considered as potential
splitting variables at any potential split of a node. The number of possible
split variables m, as well as the number of bootstrap samples B, are tuning
parameters that need to be set when implementing a random forest. There is
not much guidance as to how to choose these values, and different software
implementations might use different values, so care is required from the data
analyst. One possibility is to choose values by validation on the out-of-bag
observations, as described earlier.
16.4
Examples
16.4.1
ESTIMATING HOME PRICES (CONTINUED)
Consider again the Levittown home prices data discussed in Sections 1.4,
2.3.2, 3.4, and 15.6.1. In those sections a linear model for sale prices was
explored, with the number of bathrooms, living area, and (perhaps) the year

322
CHAPTER 16 Tree-Based Models
the house was built being predictive for sale price. Figure 16.4 gives unpruned
and pruned CART trees for the data, based on default settings for control
parameters like the minimum number of observations for a node to be split.
The pruned tree is pruned back using the one-SE rule. It can be seen that
the first variable split on is living area, with larger houses having the highest
prices and not being split further. Smaller houses are then split by the number
of bathrooms, with houses that have only one bathroom having the lowest
prices. Smaller houses with more than one bathroom are further split based on
the year the house was built (with newer houses costing more) and living area
(with bigger houses costing more), but those splits are pruned away based on
10-fold cross-validation and the one-SE rule. Thus, the implications are similar
to those of the linear model, but the trees suggest that once a home’s living
area is more than roughly 2125 square feet none of its other characteristics
have a strong effect on price.
Figure 16.5 gives the conditional inference tree for these data (based
on the default setting of the permutation test for splitting being α = .05),
which is surprisingly different from the CART trees, given each tree’s relative
simplicity. The first split is now based on the number of bathrooms rather
than living area, with houses with only one bathroom split off with the lowest
prices. Houses with more than one bathroom are then split based on living
area, with larger houses more expensive, and smaller houses are then split
based on year of construction, with newer houses more expensive. Thus, the
implications of the trees are similar, but the choice of whether living area or
number of bathrooms is most important is different (while these two variables
are positively correlated, with a correlation of around 0.4, they are hardly
substitutes for each other).
16.4.2
EXAMPLE — COURTESY IN AIRPLANE TRAVEL
On September 6, 2014, the web site fivethirtyeight.com posted a story
describing the results of a survey they had run concerning passenger courtesy
on airplanes. Among other things, the survey contained questions about
reclining seats during a flight, asking respondents if they did so (with the
ordinal responses Never, Once in a while, About half the time, Usually,
and Always). Figure 16.6 gives the pruned CART tree and the conditional
inference tree for this response based on 586 respondents with complete
data on a set of predictors including gender, age, height, household income,
education level, home location, frequency of travel by airplane, view as to
whether you feel that a person who reclines their seat has any obligation to
the person sitting behind them if they ask that the seat not be reclined, view
as to whether reclining the seat is rude, view as to whether bringing a baby on
board an airplane is rude, view as to whether bringing an unruly child on an
airplane is rude, view as to whether waking someone up to go to the bathroom
is rude, view as to whether waking someone up to walk around is rude, and
view as to the number of times it is acceptable during a six hour flight from
New York City to Los Angeles to get up if you are not in an aisle seat.

16.4 Examples
323
Unpruned CART tree
Living.area
1
< 2126.5
≥ 2126.5
Bathrooms
2
< 1.25
≥ 1.25
Node 3 (n = 12)
2e + 05
3e + 05
4e + 05
5e + 05
Year.built
4
< 1952.5
≥ 1952.5
Living.area
5
< 1622
≥ 1622
Node 6 (n = 35)
2e + 05
3e + 05
4e + 05
5e + 05
Node 7 (n = 16)
2e + 05
3e + 05
4e + 05
5e + 05
Node 8 (n = 14)
2e + 05
3e + 05
4e + 05
5e + 05
Node 9 (n = 8)
2e + 05
3e + 05
4e + 05
5e + 05
(a)
Pruned CART tree
Living.area
1
< 2126.5
≥ 2126.5
Bathrooms
2
< 1.25
≥ 1.25
Node 3 (n = 12)
2e + 05
3e + 05
4e + 05
5e + 05
Node 4 (n = 65)
2e + 05
3e + 05
4e + 05
5e + 05
Node 5 (n = 8)
2e + 05
3e + 05
4e + 05
5e + 05
(b)
FIGURE 16.4: Unpruned and pruned CART trees for Levittown home price
data. Pruning is based on the one-SE rule.

324
CHAPTER 16 Tree-Based Models
Bathrooms
1
≤ 1.5
> 1.5
Node 2 (n = 17)
2e + 05
3e + 05
4e + 05
5e + 05
Living.area
3
≤ 2092
> 2092
Year.built
4
≤ 1952
> 1952
Node 5 (n = 46)
2e + 05
3e + 05
4e + 05
5e + 05
Node 6 (n = 14)
2e + 05
3e + 05
4e + 05
5e + 05
Node 7 (n = 8)
2e + 05
3e + 05
4e + 05
5e + 05
FIGURE 16.5: Conditional inference tree for Levittown home price data.
The CART tree identifies only the view as to whether it is rude to
recline the seat as important, with the expected pattern of a stronger rudeness
perception being associated with a lower likelihood of reclining the seat.
Specifically, 82% of the people who find it very rude to recline their seat never
do so and 8% only do so once in a while, 27% of the people who find it
somewhat rude never recline their seat and 44% only do so once in a while,
while those who did not find it rude are fairly evenly split among the four
categories other than reporting that they never recline their seat (which has a
much smaller share).
The conditional inference tree also splits first on the perception of rudeness
variable, but then for passengers who do not find it very rude to recline their
seat highlights effects related to whether the respondent feels that a passenger
has any obligation to the person sitting behind them with regards to reclining
their seat. As would be expected, for both rudeness groups (somewhat rude
to recline the seat or not rude to recline the seat) feeling that you have an
obligation to the person seated behind you makes it more likely to never recline
the seat or only do so once in a while, while feeling that you do not have such
an obligation makes it more likely to recline the seat usually or always.

16.4 Examples
325
Pruned CART tree
Rude
1
≥ Somewhat
< Somewhat
Rude
2
≥ Very
< Very
Node 3 (n = 49)
Never
Usually
0
0.2
0.4
0.6
0.8
1
Node 4 (n = 183)
Never
Usually
0
0.2
0.4
0.6
0.8
1
Node 5 (n = 354)
Never
Usually
0
0.2
0.4
0.6
0.8
1
(a)
Conditional inference tree
Rude
1
≤ No
> No
Obligation
2
No
Yes
Node 3 (n = 175)
Never Usually
0
0.2
0.4
0.6
0.8
1
Node 4 (n = 179)
Never Usually
0
0.2
0.4
0.6
0.8
1
Rude
5
≤ Somewhat
> Somewhat
Obligation
6
No
Yes
Node 7 (n = 31)
Never Usually
0
0.2
0.4
0.6
0.8
1
Node 8 (n = 152)
Never Usually
0
0.2
0.4
0.6
0.8
1
Node 9 (n = 49)
Never Usually
0
0.2
0.4
0.6
0.8
1
(b)
FIGURE 16.6: Pruned CART tree and conditional inference tree for fre-
quency of airplane seat reclining data.

326
CHAPTER 16 Tree-Based Models
Obligation
1
No
Yes
Wake.up.bathroom
2
≤ No
> No
Node 3 (n = 134)
No
Very
0
0.2
0.4
0.6
0.8
1
Frequency
4
Node 5 (n = 52)
No
Very
0
0.2
0.4
0.6
0.8
1
Node 6 (n = 22)
No
Very
0
0.2
0.4
0.6
0.8
1
Baby
7
≤ Somewhat
> Somewhat
Frequency
8
Node 9 (n = 275)
No
Very
0
0.2
0.4
0.6
0.8
1
Node 10 (n = 77)
No
Very
0
0.2
0.4
0.6
0.8
1
Node 11 (n = 26)
No
Very
0
0.2
0.4
0.6
0.8
1
<= Once a year or less > Once a year or less
<= Once a year or less > Once a year or less
FIGURE 16.7: Conditional inference tree for perception of rudeness of
reclining an airplane seat data.
What is striking in both of these trees is that none of the more objective
characteristics (like gender, age, height, frequency of travel, and so on) are
identified as important; rather, how often a passenger reclines their seat is
driven only by how they feel about the act of doing so. It might be, however,
that more objective characteristics are related to those feelings. Figure 16.7 gives
the conditional inference tree for the perception of rudeness of reclining the
seat as the response variable (the pruned CART tree contains no splits, so it is
not presented). The initial split is unsurprisingly on whether someone feels they
have an obligation to the person sitting behind them, with people who do not
feel they have an obligation far more likely to feel that it is not rude to recline the
seat. These views are particularly strong among those people who also do not
feel that it is rude to wake someone up to go to the bathroom, and less frequent
travelers among those who feel that it is rude to wake someone up to go to
the bathroom. These are presumably people who feel various actions are rights
that come with the price of a ticket. On the other hand, among people who do
feel a sense of obligation to the passenger behind them, people are much more

16.5 Trees for Other Types of Data
327
likely to feel that it is somewhat or very rude to recline the seat, particularly if
they feel that it is rude to bring a baby on board or are more frequent travelers.
These are presumably people who feel that the key to enjoyable flying is to
disturb others, and be disturbed by others, as little as possible.
16.5
Trees for Other Types of Data
16.5.1
TREES FOR NESTED AND LONGITUDINAL DATA
The close connection between trees and regression models based on indicator
variables (page 315) allows natural adaptation of trees to longitudinal and
nested data, through the use of linear mixed effects models. The underlying
model is
yi = f(Xi, β) + Ziui + εi,
(16.6)
which is of course a generalization of the linear mixed effects model (13.4).
The function f(·) represents a tree, which means that for a given specified
tree structure, (16.6) is a version of the linear mixed effects model (with
constructed indicator variables being the predictors Xi). Hajjem et al. (2011),
Sela and Simonoff (2012), and Fu and Simonoff (2015) described how an
iterative procedure can produce a tree appropriate for longitudinal and nested
data. First, a regression tree approximating f(·) is estimated based on the
response variable yij −zij ˆui and covariates xij = {xij1, . . . , xitp}. Using
indicator variables constructed based on the estimated tree, a linear mixed
effects model is then fit, and the predicted random effects ui are extracted
from the estimated model. The algorithm iterates between the two steps
until convergence. Hajjem et al. (2011) and Sela and Simonoff (2012) used
CART as the underlying regression tree method, calling the resultant trees
a mixed effects regression tree (MERT) and random effects-EM (RE-EM)
tree, respectively. The resultant trees exhibit the splitting variable selection
bias typical from a CART tree, and Fu and Simonoff (2015) proposed using
conditional inference trees as the underlying tree method, resulting in a
conditional inference RE-EM tree that does not suffer from splitting variable
bias. Hajjem et al. (2017) extend this approach to generalized mixed effects
trees based on a generalized linear mixed effects model formulation.
16.5.1.1
Example — Wages of Young People
Singer and Willett (2003) presented data on the wages of young people, with
the data focusing on experience and education level and in particular whether
a subject earned a high school degree. The response variable is logged wages,
and potential predictors include years of experience, highest grade of school
completed, whether the subject has earned a (perhaps General Education
Development, or GED) high school diploma, current unemployment rate,
and race (coded white, black, or Hispanic).

328
CHAPTER 16 Tree-Based Models
Figure 16.8 gives the conditional inference RE-EM tree for these data. It
is apparent that years of experience is the most important predictor, forming
splits at the top of the tree that separate subjects into five groups delineated by
split points at roughly 1.3, 2.9, 5.2, and 7.5 years, respectively. Unsurprisingly,
more experience is associated with higher (logged) wages. The next splits are
based on highest grade of schooling completed (H.G.C.), with more than
nine years of schooling (but no other split) consistently associated with higher
wages. Race only enters the tree for subjects predicted to have higher wages
(more than 5.2 years of experience and more than nine years of schooling),
and for them black subjects are predicted to have lower wages than white or
Hispanic subjects.
16.5.2
SURVIVAL TREES
The generalization of tree-based models to survival data (i.e. survival trees)
requires formulation of an impurity measure that is appropriate for data of
that type. Several such measures have been suggested, with a popular one being
based on a proportional hazards assumption (LeBlanc and Crowley, 1992).
The approach is based on estimating the log-likelihood of the data under a
model in which the hazard function in any node h satisfies
λh(t) = θhλ0(t),
where θh is the relative risk of node h and λ0(t) is the baseline hazard; that
is, being in a particular node increases or decreases the hazard by a fixed
multiplicative amount for all observations in that node. The homogeneity of a
node is assessed using the deviance in that node, with a decision to split based
on the drop in the difference between the deviance of the parent node and
the sum of the deviances of the children nodes (this is similar conceptually
to splitting based on the drop in sums of squares in regression trees). This
deviance depends on the cumulative baseline hazard function, which can be
estimated using the Nelson-Aalen estimator (11.8). It can be shown that
this can be formulated equivalently through the use of Poisson regression,
making generalization of CART relatively straightforward using the deviance
as suggested in Section 16.2.2.1.
The conditional inference tree also can be applied to survival data using the
standard algorithm, measuring the association of the response and a predictor
using statistics of the form given in (16.5). This requires the influence function
h() for a bivariate survival observation (yi, δi), which is the log-rank score;
it can be shown that splitting on the log-rank score is equivalent to splitting
based on the log-rank test described in Section 11.3.1.2. This effectively turns
the bivariate survival response into a univariate numerical one, and then the
usual conditional inference tree algorithm can be applied to that response.
Each of these tree methods can be adapted to time-varying data (Fu and
Simonoff, 2017). This is done using the approach described in Section 11.5.3,
in which subjects with time-varying covariates are converted using the
Anderson-Gill process into pseudo-subjects with left-truncated right-censored

16.5 Trees for Other Types of Data
329
Exper
1
≤ 2.938
> 2.938
Exper
2
≤ 1.336
> 1.336
H.G.C.
3
≤ 9
> 9
n = 936
y = 1.698
n = 410
y = 1.785
H.G.C.
6
≤ 9
> 9
Unemp.rate
7
n = 193
y = 1.863
GED
9
≤0
>0
n = 577
y = 1.735
n = 187
y = 1.811
n = 498
y = 1.855
Exper
13
≤ 7.465
> 7.465
Exper
14
≤ 5.246
> 5.246
H.G.C.
15
≤ 9
> 9
Unemp.rate
16
n = 459
y = 1.944
n = 573
y = 1.872
n = 578
y = 1.962
H.G.C.
20
≤9
> 9
Unemp.rate
21
n = 300
y = 2.042
n = 423
y = 1.917
Race
24
{H, W}
B
n = 296
y = 2.108
n = 106
y = 1.989
H.G.C.
27
≤ 9
> 9
n = 472
y = 2.029
Race
29
{H, W}
B
Unemp.rate
30
n = 18
y = 2.491
n = 296
y = 2.245
n = 80
y = 2.016
≤ 6.454 > 6.454 
≤ 3.295 > 3.295
≤ 5.495 > 5.495
≤ 5.684 > 5.684
FIGURE 16.8: Conditional inference RE-EM tree for wages data.

330
CHAPTER 16 Tree-Based Models
(LTRC) survival responses. The Poisson regression formulation of the CART
proportional hazards tree and the log-rank score of the conditional inference
tree can both be adapted to LTRC data, making the construction of each type
of time-varying survival tree possible.
16.5.2.1
Example — The Survival of Broadway Shows
(continued)
Consider again the data on survival of Broadway shows after the Tony Awards
discussed in Section 11.4. Figure 16.9 gives the pruned CART tree and the
conditional inference tree for the number of performances after the awards,
with Kaplan-Meier curves representing the estimated survival distributions at
each terminal node. The implications are similar to those of the Weibull,
lognormal, proportional hazards, and Buckley-James model fits. The CART
Pruned CART tree
Att.post.award
1
≥ 75.05
< 75.05
Musical
2
≥ 0.5
< 0.5
Node 3 (n = 47)
0
1000
2000
3000
0
0.2
0.4
0.6
0.8
1
Node 4 (n = 34)
0
1000
2000
3000
0
0.2
0.4
0.6
0.8
1
Node 5 (n = 49)
0
1000
2000
3000
0
0.2
0.4
0.6
0.8
1
(a)
FIGURE 16.9: Pruned CART tree and conditional inference tree for Broad-
way show survival data.

16.5 Trees for Other Types of Data
331
Conditional inference tree
Att.post.award
1
≤ 92.3
> 92.3
Att.post.award
2
≤ 74.3
> 74.3
Node 3 (n = 47)
0 1000
3000
0
0.2
0.4
0.6
0.8
1
Node 4 (n = 37)
0 1000
3000
0
0.2
0.4
0.6
0.8
1
Musical
5
≤ 0
> 0
Node 6 (n = 16)
0 1000
3000
0
0.2
0.4
0.6
0.8
1
Revival
7
≤ 0
> 0
Node 8 (n = 23)
0 1000
3000
0
0.2
0.4
0.6
0.8
1
Node 9 (n = 7)
0 1000
3000
0
0.2
0.4
0.6
0.8
1
(b)
FIGURE 16.9: (Continued)
tree only identifies two important predictors, the attendance the first week
after the awards and whether the show is a musical. Shows with attendance
less than 75% of capacity have the shortest predicted survival, with a median
time until closing of roughly six weeks (47 performances at 8 performances
per week). Shows with higher attendance fare better, but musicals (roughly
15 months median time until closing) do much better than do non-musicals
(roughly four months). The conditional inference tree identifies the same
poor survival chances of shows with first-week attendance less than 75% or
so, but also identifies a median survival of four to five months for shows
with first-week attendance between 74% and 92%. Musicals with excellent
attendance (above 92%) unsurprisingly stay open a long time after the awards
(a median of one year for revivals and 2 1
2 years for non-revivals), and the tree
also uncovers a relatively short survival time of non-musicals with excellent
week-after-awards attendance (median less than two months).

332
CHAPTER 16 Tree-Based Models
16.5.2.2
Example — Female Heads of Government
(continued)
Consider again the data on the time to first female head of government
discussed in Section 11.5.4, which involved survival data with time-varying
covariates. Figure 16.10 gives the conditional inference tree for the number
of years after 1959 at which the head of government was first female (the
pruned CART tree does not have any splits for these data). Note that in
this tree a particular country can move from one terminal node to another,
depending on how any time-varying covariates in the tree change over time,
so the “observations” are country-years, rather than countries.
The tree provides a noticeably different picture of the factors relating to
the time to first female head of government after becoming a country than does
the proportional hazards model, with birth rate and religion (the two strongest
effects in the proportional hazards fit) not appearing in the tree at all. The first
split is on fertility rate, with country-years with a high fertility rate (the upper
group at the split corresponds to roughly the highest 37% of fertility rate values)
having a generally low hazard of first female head of government (i.e., later
expected first female head of government, which corresponds to a survival
curve being relatively flat). For country-years with lower fertility, effects differ
by continent, with Africa and Asia split off from the other continents and
also having relatively lower hazard (later female head of government). In the
other continents the infant mortality is the most important separating variable,
with lower infant mortality rate and higher per capita GDP corresponding to
higher hazard (earlier female head of government). Note that these patterns are
consistent with the marginal pattern that more developed countries are more
likely to have female heads of government earlier, which was not true for several
of the slopes of covariates in the proportional hazards fit discussed earlier.
16.6
Summary
Trees combine easy interpretability with great flexibility and efficient com-
putation, making them ideal exploratory tools. Trees can be expected to be
effective when the data allow the analyst to take advantage of their flexibility.
Larger sample sizes can improve performance of trees far more than that of
parametric models, since in the latter case it is only the estimation error of
parameter estimates that is reduced with increasing sample size, while in the
former case the model itself changes and allows for more complex relationships.
Similarly, trees can improve on parametric models more if the underlying
relationships are stronger, so that it is easier to separate signal from noise.
Perlich et al. (2003) explore and characterize these patterns in the specific case
of classification trees and logistic regression. The price paid for this flexibility
is high variability, corresponding to the possibility of trees based on different
samples from the same population having very different appearances. This can

Fertility.rate
1
≤ 5.349
> 5.349
Continent
2
Africa, Asia
Europe, N America, Oceania, S America
Node 3 (n = 1834)
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1
Infant.mortality.rate
4
≤ 7.4
> 7.4
Infant.mortality.rate
5
≤ 3.7
> 3.7
Node 6 (n = 138)
0 1020 30 40 50
0
0.2
0.4
0.6
0.8
1
Node 7 (n = 406)
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1
Log.GDP.pc
8
≤ 9.494
> 9.494
Infant.mortality.rate
9
≤ 26.2
> 26.2
Node 10 (n = 816)
0 10 20 3040 50
0
0.2
0.4
0.6
0.8
1
Node 11 (n = 810)
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1
Node 12 (n = 685)
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1
Node 13 (n = 2785)
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1
FIGURE 16.10: Conditional inference tree for time to first female head of government data.
16.6 Summary
333

334
CHAPTER 16 Tree-Based Models
be overcome from a predictive point of view using ensemble methods, but at
the cost of sacrificing interpretability.
Trees are locally greedy algorithms, in that they attempt to find the “best”
variable and split point at each step. Just as is true in linear model selection, this
will not necessarily lead to the actual global “best” tree. Angelino et al. (2018)
propose basing tree construction on a regularized version of misclassification
rate that encourages less complex trees, and develop an algorithm that actually
finds the best tree in terms of that criterion.
The nonparametric nature of trees has encouraged novel approaches to
data problems that arise in model fitting. One example is missing data. The
presence of missing values in either the response variable or predictor variables
is a serious problem in any regression problem, and one that is beyond the
scope of this book (see Little and Rubin, 2002, for extensive discussion of this
topic in general statistical modeling). The fact that trees are not based on a
particular parametric formulation or methodology has encouraged approaches
not generally or easily available in parametric models. One such method is
probabilistic split, in which observations with observed values on the split
variable are split first, with observations with missing values included in child
nodes with weights that correspond to the proportion of values that are
not missing. Another approach is surrogate split, in which surrogate variables
with observed values are used in place of those with missing values as an
observation is passed down the tree (this is the approach used by both CART
and conditional inference trees). Often missingness is related to the response
(consider, for example, predicting bankruptcy of a company as in Chapter 8
using financial ratios, where distressed companies don’t submit required reports
because they are trying to hide their dire financial status). In this situation
a highly effective approach is to make a separate category for missingness,
allowing observations with missing values to be split off by themselves. Ding
and Simonoff (2010) provide further discussion of these issues.
KEY TERMS
Binary tree: a tree based on splitting a parent node into two child nodes at
each split.
CART: Classification And Regression Tree, a standard algorithm for con-
structing a binary tree. In this algorithm the splitting variable and split point
are chosen simultaneously so as to optimize an appropriate measure of the
homogeneity (purity) of the child nodes. The tree is typically overfit and then
simplified by being pruned back.
Classification tree: A tree where the response variable is categorical.
Cost complexity pruning: A criterion used to determine how much an
overfit CART tree should be pruned, based on trading off homogeneity of
the terminal nodes versus the number of terminal nodes in the tree. This is
typically done using a criterion estimated using cross-validation.

16.6 Summary
335
Conditional inference tree: A binary tree algorithm in which the splitting
variable is chosen using a measure of association between the response and the
predictor based on the influence function, and the decision to split is based
on a permutation-based hypothesis test of whether the response variable is
associated with any of the predictors for the observations in that node.
Ensemble method: A prediction method based on averaging (in some sense)
the predictions from trees constructed on regenerated subsamples of the data
using the bootstrap. Such methods are designed to preserve the low bias of
trees while also reducing their variability.
Longitudinal tree: A tree constructed for the situation where the data
constitute a longitudinal or nested sample.
Random forest: An ensemble method that provides improved reduction in
variability by using a randomly-selected subset of the available predictors as
candidate predictors at each split when constructing the tree based on each
bootstrap sample.
Regression tree: A tree where the response variable is numerical.
Splitting variable selection bias: The tendency for trees that simultaneously
choose the splitting variable and split point to prefer splitting on variables
with more potential split points. While CART trees suffer from this bias,
conditional inference trees do not.
Survival tree: A tree where the response variable is a survival (time-to-event)
measurement, possibly censored.
Terminal nodes: The nodes in a fitted tree that are not split further.
Predictions are made based on membership in a terminal node.
Tree: Sometimes called a decision tree, an algorithm that divides the predictor
space using recursive partitioning, in which a given predictor subspace (starting
with the entire predictor space) is split into child nodes on the basis of the
values of one predictor variable. Predictions are ultimately made based on
membership in a particular terminal node using the characteristics of the data
used in the fitting process that fall in that node.

Bibliography
Abzug, R., Simonoff, J.S., and Ahlstrom, D. (2000). Nonprofits as large employ-
ers: a city-level geographical inquiry. Nonprofit and Voluntary Sector Quarterly
29: 455–470.
Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2e. New York:
Wiley.
Agresti, A. (2010). Analysis of Ordinal Categorical Data, 2e. New York: Wiley.
Akaike, H. (1973). Information theory and an extension of the maximum likelihood
principle. In: 2nd International Symposium on Information Theory (ed. B.N. Petrov
and F. Csaki), 267–281. Budapest: Akademiai Kiado.
Andersen, P.K. and Gill, R.D. (1982). Cox’s regression model for counting processes:
a large sample study. Annals of Statistics 10: 1100–1120.
Angelino, E., Larus-Stone, N., Alabi, D. et al. (2018). Learning certifiably optimal
rule lists for categorical data. Journal of Machine Learning Research 18: 1–78.
Appleton, D.R., French, J.M., and Vanderpump, M.P.J. (1996). Ignoring a covariate:
an example of Simpson’s paradox. American Statistician 50: 340–341.
Atkinson, A. and Riani, M. (2000). Robust Diagnostic Regression Analysis. New York:
Springer-Verlag.
Bates, D.M. and Watts, D.G. (1988). Nonlinear Regression and Its Applications.
New York: Wiley.
Belsley, D.A., Kuh, E., and Welsch, R.E. (1980). Regression Diagnostics: Identifying
Data and Sources of Collinearity. New York: Wiley.
Berk, R., Brown, L., Buja, A. et al. (2013). Valid post-selection inference. Annals of
Statistics 41: 802–837.
Bertsimas, D., King, A., and Mazumder, R. (2016). Best subset selection via a
modern optimization lens. Annals of Statistics 44: 813–852.
Bogaerts, K., Komárek, A., and Lesaffre, E. (2017). Survival Analysis with
Interval-Censored Data: A Practical Approach with R, SAS and BUGS. Boca
Raton, FL: Chapman and Hall/CRC.
Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. (1984). Classification
and Regression Trees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books
& Software.
Bretz, F., Hothorn, T., and Westfall, P. (2010). Multiple Comparisons Using R. Boca
Raton, FL: Chapman and Hall/CRC.
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
337

338
BIBLIOGRAPHY
Bühlmann, P. and van de Geer, S. (2011). Statistics for High-Dimensional Data. New
York: Springer-Verlag.
Burgess, G.H. (2012). ISAF 2011 worldwide shark attack summary. http://www
.flmnh.ufl.edu/fish/sharks/isaf/2011summary.html (accessed 17 June 2012).
Burnham, K.P. and Anderson, D.R. (2002). Model Selection and Multimodel Infer-
ence: A Practical Information-Theoretic Approach, 2e. New York: Springer-Verlag.
Chatterjee, S. and Hadi, A.S. (1988). Sensitivity Analysis in Linear Regression.
New York: Wiley.
Chatterjee, S. and Hadi, A.S. (2012). Regression Analysis By Example, 5e. New York:
Wiley.
Chow, G.C. (1960). Tests of equality between sets of coefficients in two linear
regressions. Econometrica 28: 591–605.
Cochrane, D. and Orcutt, G.H. (1949). Application of least squares regression
to relationships containing auto-correlated error terms. Journal of the American
Statistical Association 44: 32–61.
Cook, R.D. (1977). Detection of influential observations in linear regression.
Technometrics 19: 15–18.
Cox, D.R. (1972). Regression models and life-tables (with discussion). Journal of the
Royal Statistical Society, Series B 34: 187–220.
Cryer, J.D. and Chan, K.-S. (2008). Time Series Analysis: With Applications in R.
New York: Springer-Verlag.
Daskalakis,
C.
(2016).
Tumor
growth
dataset.
TSHS
Resources
Portal.
https://www.causeweb.org/tshs/tumor-growth/ (accessed 21 August 2019).
Ding, Y. and Simonoff, J.S. (2010). An investigation of missing data methods for
classification trees applied to binary response data. Journal of Machine Learning
Research 11: 131–170.
Durbin, J. and Watson, G.S. (1951). Testing for serial correlation in least squares
regression. I. Biometrika 38: 159–179.
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004). Least angle regression
(with discussion). Annals of Statistics 32: 407–499.
Eubank, R.L. (1999). Nonparametric Regression and Spline Smoothing, 2e. New York:
Marcel Dekker.
Fan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Applications.
London: Chapman and Hall.
Fitzmaurice, G., Davidian, M., Verbeke, G., and Molenberghs, G. (2009). Longitu-
dinal Data Analysis. Boca Raton, FL: Chapman and Hall/CRC.
Flynn, C.J., Hurvich, C.M., and Simonoff, J.S. (2013). Efficiency for regularization
parameter selection in penalized likelihood estimation of misspecified models.
Journal of the American Statistical Association 108: 1031–1043.
Flynn, C.J., Hurvich, C.M., and Simonoff, J.S. (2016). Deterioration of performance
of the lasso with many predictors: discussion of a paper by Tutz and Gertheiss.
Statistical Modelling 16: 212–216.
Flynn, C.J., Hurvich, C.M., and Simonoff, J.S. (2017). On the sensitivity of the
lasso to the number of predictor variables. Statistical Science 32: 88–105.

BIBLIOGRAPHY
339
Fu, W. and Simonoff, J.S. (2015). Unbiased regression trees for longitudinal and
clustered data. Computational Statistics and Data Analysis 88: 53–74.
Fu, W. and Simonoff, J.S. (2017). Survival trees for left-truncated and right-censored
data, with application to time-varying covariate data. Biostatistics 18: 352–369.
Furnival, G.M. and Wilson, R.W. Jr. (1974). Regressions by leaps and bounds.
Technometrics 16: 499–511.
Gelman, A. and Hill, J. (2007). Data Analysis Using Regression and Multi-
level/Hierarchical Models. Cambridge: Cambridge University Press.
Greene, W.H. (2011). Econometric Analysis, 7e. Upper Saddle River, NJ: Prentice
Hall.
Gregoriou, G.N. (2009). Stock Market Volatility. Boca Raton, FL: Chapman and
Hall/CRC.
Hadi, A.S. and Simonoff, J.S. (1993). Procedures for the identification of multiple
outliers in linear models. Journal of the American Statistical Association 88:
1264–1272.
Hajjem, A., Bellavance, F., and Larocque, D. (2011). Mixed effects regression trees
for clustered data. Statistics and Probability Letters 81: 451–459.
Hajjem, A., Larocque, D., and Bellavance, F. (2017). Generalized mixed effects
regression trees. Statistics and Probability Letters 126: 114–118.
Hastie, T.J. and Tibshirani, R.J. (1990). Generalized Additive Models. Boca Raton,
FL: Chapman and Hall/CRC.
Hastie, T., Tibshirani, R., and Wainwright, M. (2015). Statistical Learning with
Sparsity: The Lasso and Generalizations. Boca Raton, FL: Chapman and Hall/CRC.
Hastie, T., Tibshirani, R., and Tibshirani, R.J. (2017). Extended comparisons of best
subset selection, forward stepwise selection, and the lasso: following ‘best subset
selection from a modern optimization lens’ by Bertsimas, King, and Mazumder
(2016). https://arxiv.org/abs/1707.08692 (accessed 30 May 2019).
Hickey, W. (2014). The dollar-and-cents case against Hollywood’s exclusion of
women.
https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-
hollywoods-exclusion-of-women/ (accessed 22 March 2017).
Hilbe, J.M. (2009). Logistic Regression Models. Boca Raton, FL: Chapman and
Hall/CRC.
Hosmer, D.W., Lemeshow, S., and May, S. (2008). Applied Survival Analysis:
Regression Modeling of Time to Event Data, 2e. Hoboken, NJ: Wiley.
Hosmer, D.W., Lemeshow, S., and Sturdivant, R.X. (2013). Applied Logistic
Regression, 3e. Hoboken, NJ: Wiley.
Hothorn, T., Hornik, K., and Zeileis, A. (2006). Unbiased recursive partitioning: a
conditional inference framework. Journal of Computational and Graphical Statistics
15: 651–674.
Hout, M., Mangels, L., Carlson, J., and Best, R. (2004). Working paper: the
effect of electronic voting machines on change in support for Bush in the 2004
Florida Elections. http://www.verifiedvoting.org/downloads/election04_WP.pdf
(accessed 7 July 2011).

340
BIBLIOGRAPHY
Howard, J. (2019). Why shark attacks are more common in the Atlantic than the
Pacific.
www.nationalgeographic.com/animals/2019/07/shark-attacks-atlantic-
ocean.html (accessed 26 August 2019).
Hurvich, C.M. and Tsai, C.-L. (1989). Regression and time series model selection
in small samples. Biometrika 76: 297–307.
Kedem, B. and Fokianos, K. (2002). Regression Models for Time Series Analysis.
Hoboken, NJ: Wiley.
Kulmatitskiy, N., Nygren, L.M., Nygren, K. et al. (2015). Survival of broad-
way shows: an empirical investigation of recent trends. Communications in
Statistics—Case Studies, Data Analysis and Applications 1: 114–124.
LeBlanc, M. and Crowley, J. (1992). Relative risk trees for censored survival data.
Biometrics 48: 411–425.
Lee, E.T. and Wang, J.W. (2013). Statistical Methods for Survival Data Analysis, 4e.
Hoboken, NJ: Wiley.
Little, R.J.A. and Rubin, D.B. (2002). Statistical Analysis with Missing Data, 2e.
Hoboken, NJ: Wiley.
Mallows, C.L. (1973). Some comments on CP . Technometrics 15: 661–675.
Mantel, N. (1987). Understanding Wald’s test for exponential families. American
Statistician 41: 147–148.
McCullagh, P. and Nelder, J.A. (1989). Generalized Linear Models, 2e. London:
Chapman and Hall.
Meinshausen, N. (2007). Relaxed lasso. Computational Statistics and Data Analysis
52: 374–393.
Montgomery, D.C., Peck, E.A., and Vining, G.G. (2012). Introduction to Linear
Regression Analysis, 5e. Hoboken, NJ: Wiley.
Moore, D.F. (2016). Applied Survival Analysis Using R. New York: Springer.
Müller, S., Scealy, J.L., and Welsh, A.H. (2013). Model selection in linear mixed
models. Statistical Science 28: 135–167.
Myers, R.H., Montgomery, D.C., and Vining, G.G. (2002). Generalized Linear
Models With Applications in Engineering and the Sciences. New York: Wiley.
Niedzwiecki, D. and Simonoff, J.S. (1990). Estimation and inference in pharmacoki-
netic models: the effectiveness of model reformulation and resampling methods
for functions of parameters. Journal of Pharmacokinetics and Biopharmaceutics 18:
361–377.
Nygren, L.M. and Simonoff, J.S. (2007). Bright lights, big dreams — a case study of
factors relating to the success of broadway shows. Case Studies in Business, Industry
and Government Statistics 1: 1–14.
Perlich, C., Provost, F., and Simonoff, J.S. (2003). Tree induction vs. logistic
regression: a learning-curve analysis. Journal of Machine Learning Research 4:
211–255.
Pigeon, J.G. and Heyse, J.F. (1999). An improved goodness of fit statistic for
probability prediction models. Biometrical Journal 41: 71–82.
Prais, S.J. and Winsten, C.B. (1954). Trend estimators and serial correlation. Cowles
Commission Discussion Paper Statistics #382.

BIBLIOGRAPHY
341
R Development Core Team (2017). R: A Language and Environment for Statistical
Computing. Vienna, Austria: R Foundation for Statistical Computing. http://
www.R-project.org/.
Reid, N. (1994). A conversation with Sir David Cox. Statistical Science 9: 439–455.
Rizopoulos, D. (2012). Joint models for longitudinal and time-to-event data: with
applications in R. Boca Raton, FL: CRC Press.
Rudin, C. (2019). Stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead. Nature Machine Intelligence
1: 206–215.
Ruppert, D., Wand, M.P., and Carroll, R.J. (2003). Semiparametric Regression.
Cambridge: Cambridge University Press.
Scott, M.A., Simonoff, J.S., and Marx, B.D., ed. (2013). The SAGE Handbook of
Multilevel Modeling. London: SAGE Publications.
Seber, G.A.F. and Wild, C.J. (1989). Nonlinear Regression. New York: Wiley.
Sela, R.J. and Simonoff, J.S. (2012). RE-EM trees: a data mining approach for
longitudinal and clustered data. Machine Learning 86: 169–207.
Sen, A. and Srivastava, M. (1990). Regression Analysis: Theory, Methods, and Appli-
cations. New York: Springer-Verlag.
Simonoff, J.S. (1996). Smoothing Methods in Statistics. New York: Springer-Verlag.
Simonoff, J.S. (1998a). Logistic regression, categorical predictors and goodness-of-fit:
it depends on who you ask. American Statistician 52: 10–14.
Simonoff, J.S. (1998b). Three sides of smoothing: categorical data smoothing,
nonparametric regression, and density estimation. International Statistical Review
66: 137–156.
Simonoff, J.S. (2003). Analyzing Categorical Data. New York: Springer-Verlag.
Simonoff, J.S. (2013). Regression tree-based diagnostics for linear multilevel models.
Statistical Modelling 13: 459–480.
Simonoff, J.S. and Ma, L. (2003). An empirical study of factors relating to the
success of broadway shows. Journal of Business 76: 135–150.
Simonoff, J.S. and Tsai, C.-L. (1989). The use of guided reformulations when
collinearities are present in non-linear regression. Applied Statistics 38: 115–126.
Simonoff, J.S. and Tsai, C.-L. (2002). Score tests for the single index model.
Technometrics 44: 142–151.
Singer, J.D. and Willett, J.B. (2003). Applied Longitudinal Data Analysis: Modeling
Change and Event Occurrence. Oxford: Oxford University Press.
Smith, P.J. (2002). Analysis of Failure and Survival Data. Boca Raton, FL: Chapman
and Hall/CRC.
Stasinopoulos, M.D., Rigby, R.A., Heller, G.Z. et al. (2017). Flexible Regression and
Smoothing: Using GAMLSS in R. Boca Raton, FL: Chapman and Hall/CRC.
Steele, R. (2013). Model selection for multilevel models. In: The SAGE Handbook
of Multilevel Modeling (ed. M.A. Scott, J.S. Simonoff, and B.D. Marx), 109–126.
London: SAGE Publications.
Strimmer, K. (2008). A unified approach to false discovery rate estimation. Bioin-
formatics 9: 303.

342
BIBLIOGRAPHY
Theus, M. and Urbanek, S. (2009). Interactive Graphics for Data Analysis: Principles
and Examples. Boca Raton, FL: CRC Press.
Tibshirani, R.J. (2015). Degrees of freedom and model search. Statistica Sinica 25:
1265–1296.
Tibshirani, R.J., Taylor, J., Lockhart, R., and Tibshirani, R. (2016). Exact
post-selection inference for sequential regression procedures (with discussion).
Journal of the American Statistical Association 111: 600–620.
Train, K.E. (2009). Discrete Choice Methods with Simulation. Cambridge: Cambridge
University Press.
United Nations Development Programme (2018). Human Development Indices and
Indicators: 2018 Statistical Update. New York: United Nations.
U.S. Census Bureau (2017). The X-13ARIMA-SEATS seasonal adjustment program.
https://www.census.gov/srd/www/x13as/ (accessed 14 September 2019).
Varna, M., Bertheau, P., and Legrès, L.G. (2014). Tumor microenvironment
in human tumor xenografted mouse models. Journal of Analytical Oncology 3:
159–166.
Wabe, J.S. (1971). A study of house prices as a means of establishing the value of
journey time, the rate of time preference and the valuation of some aspects of
environment in the London metropolitan region. Applied Economics 3: 247–255.
Weisberg, S. (1980). Applied Linear Regression. New York: Wiley.
Ye, J. (1998). On measuring and correcting the effects of data mining and model
selection. Journal of the American Statistical Association 93: 120–131.

Index
a
accelerated failure time model,
212–213, 223, 238
stratified, 221
ACF plot, see autocorrelation function
plot
additive model, 300, 308, 311
adjusted R2 (R2
a), 11, 32
AIC, 33–34, 38, 154–155, 179, 191,
194, 284
AICc, 33–34, 38, 48, 155, 179, 191,
194, 201, 280, 284, 299
analysis of covariance, 135–142, 204,
285–286
test of common slope, 137
analysis of variance, 285–286
higher order, 130–131
one-way, 110–111, 113–116,
118–119, 123–124, 133, 257
two-way, 111–113, 116–120, 133
ANCOVA, see analysis of covariance
ANOVA, see analysis of variance
AR(1) model, 81–85, 99–100, 105,
261
autocorrelation, 8, 15, 20, 79–106,
258–259
AR(1) model, see AR(1) model
effects of, 81–82
identifying, 83–86
autocorrelation function plot, 84–85,
87, 105
b
bagging, 320–321
out of bag observations, 321
balanced design, 118, 119, 132
bandwidth, 298–299, 311
Handbook of Regression Analysis With Applications in R, Second Edition.
Samprit Chatterjee and Jeffrey S. Simonoff.
© 2020 John Wiley & Sons, Inc. Published 2020 by John Wiley & Sons, Inc.
Bernoulli, 149, 156–159
best linear unbiased predictor, 262
best subsets regression, 30–38, 48,
284–285
beta-binomial, 169
bias-variance tradeoff, 278–279, 290,
296–297, 298–299
connection to information criteria,
279
connection to parsimony, 279
BIC, 34, 38, 284, 286
binary data, 146
binomial, 145–171, 189
connection with Poisson, 189
BLUP, see best linear unbiased
predictor
Bonferroni correction, 119, 132
Breslow estimator, 222, 238
Buckley-James estimator, 223, 238
c
CART, 318–319, 334
cost complexity pruning, 318–319,
334
case-control design, see retrospective
design
censored data, 210, 238
Chow test, 39, 48
classification and regression tree, see
CART
classification table, 158–159, 180
classification tree, 317–318, 334
purity measure, 317–318
clustered data, see nested data
Cochrane-Orcutt procedure, 99–100,
105
cohort design, see prospective design
343

344
INDEX
collinearity, 26–29, 48, 118, 247–248
complete separation, 166, 169
components of variance, 257–258
compound symmetry, 261, 275
concordant pair, 157–158, 179–180
conditional inference tree, 319–320,
335
constant shift model, 38–39, 48, 87,
136–137
contingency table, 189, 206–207, 304
smoothing, 304
sparse, 206–207, 304
Cook’s D, 58–59, 64, 122, 159, 247
count data, 187–207
covariate, 135, 142
Cox proportional hazards model, 214,
221–223, 239
estimating baseline survival,
222–223
stratified, 222, 239
cross-entropy, 318
cross-validation, 280, 286
K-fold, 280, 285, 291, 319
leave-one-out, 280, 291
cumulative logit, 177–178, 185
curse of dimensionality, 300, 311
d
data sets
airplane passenger courtesy,
322–327
bankruptcy of telecommunications
firms, 163–168
Bechdel Rule, 305–307
Broadway show survival, 216–218,
223–230, 233, 330–331
cancer tumor growth, 264–269
city bond ratings, 180–184
domestic movie grosses, 71–77
DVD sales, 125–130
e-commerce retail sales, 87–93
e-voting and the 2004 election,
40–45
enzyme kinetics, 248–252
female heads of government,
235–238, 332
Florida shark attacks, 194–201
German auto prices, 301–303
Human Development Index,
287–289
international movie grosses,
137–141, 204–205
Levittown home prices, 15–18,
31–38, 60–62, 282–283,
308–310, 321–322
national shark attacks, 269–275
Old Faithful Geyser eruptions,
100–104
stock price indexes, 94–98
wages of young people, 327–328
Whickham smoking study,
159–163
degrees of freedom, 38
dependent variable, 4, 20
deseasonalizing, 86–87, 106
detrending, 86, 106
deviance, 153–154, 170
differencing, 93–94, 105
discordant pair, 157–158, 179–180
discrete choice model, 175–176, 185
dummy variable, see indicator variable
Durbin-Watson statistic, 83–84, 105
e
effect coding, 114–117, 124, 131,
132, 136–137
effective degrees of freedom, 284–285,
290, 299
elasticity, 69, 78, 148
empirical logit, 161
ensemble method, 320–321, 335
expit function, 148, 170
extreme value distribution, 212
f
fitted value, 6, 7, 13–14, 20
confidence interval, 14, 20
fixed effect, 257, 259–260, 275
forward stepwise regression, 280–281,
285, 290
frailty model, 269

INDEX
345
g
GCV , 300
GEE, see generalized estimating
equations
generalized additive model, 304, 311
generalized cross-validation, see GCV
generalized estimating equations, 263,
275
generalized least squares, see least
squares, generalized least
squares
generalized linear mixed effects model,
262, 276
generalized linear model, 189–190,
207
Gini index, 318
GLMM, see generalized linear mixed
effects model
GLS, see least squares, generalized least
squares
goodness-of-fit statistic, 155–157,
170, 180, 191
deviance, 156, 170, 180, 191
Hosmer-Lemeshow, 156–157, 170,
180
Pearson, 156, 170, 180, 191
group lasso, 285–286, 290
h
hat matrix, 8, 57–58, 64, 118, 122,
159, 247
hazard function, 211–213, 239
heteroscedasticity, 8, 15, 20, 68,
120–123, 132, 188,
203–204, 245, 261
weighted least squares, see least
squares, weighted least
squares
higher order analysis of variance, see
analysis of variance, higher
order
holdout sample, see validation sample
homoscedasticity, 8, 20
i
ICC, see intraclass correlation
IIA, see independence of irrelevant
alternatives
independence of irrelevant
alternatives, 175–176, 185
independent variable, 4, 20
indicator variable, 38–40, 48, 87,
113–114, 116–117, 123–124
induced correlation, 256
influential point, 58–59, 64
interaction effect, 40, 48, 111–113,
116–118, 131, 137, 179,
307–308
between numerical predictors, 40,
307–308
interaction plot, 112–113, 131, 132
intraclass correlation, 258, 276
IRWLS, see iteratively reweighted least
squares
iteratively reweighted least squares,
153–154, 159, 190–191,
194, 263
j
joint models for longitudinal and
time-to-event data, 268–269
k
Kaplan-Meier estimator, 214–215,
239
kernel function, 298, 311
l
lagging, 93–94, 105
lasso, 281–283, 285, 291
oracle inequalities, 287
lasso-plus-least squares, 283, 291
latent variable, 176–178, 185
least squares
generalized least squares, 99–100,
121
nonlinear least squares, 246–248,
252
starting values, 246–247
ordinary least squares, 5–8, 21
weighted least squares, 121–123,
133, 203–204
left-truncated right-censored data, see
LTRC data
Levene’s test, 121, 132, 203–204
leverage point, 57–59, 64
leverage value (hii), 57–58, 64, 118,
122, 159, 247

346
INDEX
likelihood ratio test, 153–154, 170,
179, 190, 252
Likert-type scale variable, 173–174,
176–178, 185
linear mixed effects model, 260–262,
276, 327
linear regression coefficient
confidence intervals, 13
hypothesis tests, 12–13
interpretation, 9–10, 69–71
linear regression model, 4, 8–9, 190,
245
as a generalized linear model, 190
assumptions, 8–9
linear restriction, 25–26, 48, 154
linearizable model, 68, 244, 247
LMM, see linear mixed effects model
local linear estimator, 298
local polynomial estimator, 298–300,
311
choosing the bandwidth, 298–299
local polynomial likelihood estimator,
304, 311
local quadratic estimator, 298
loess, 299, 311
log-log model, 69, 78
log-logistic, 212
log-rank test, 215–216, 239, 328
logistic distribution, 212
logistic regression, 145–169, 186, 190
as a generalized linear model, 190
exact inference, 169
logistic regression coefficient
confidence intervals, 154
hypothesis tests, 153–154
infinite value, 163
interpretation, 148
logit response function, 148, 153, 170,
174–175, 177–178, 185, 190
lognormal, 211
longitudinal data, 256, 258–259, 327
longitudinal tree, 327, 335
LTRC data, 230–234, 239
connection with time-varying
covariates, 233–235,
328–330
m
main effect, 111, 117–118, 131, 132,
136
Mallows’ Cp, 32–34, 38, 48, 280, 284
marginal model, see population
average model
masking effect, 59–60, 64
maximum likelihood, 152–153, 170,
178, 190, 194, 201,
219–220, 246, 261–262
MERT, see longitudinal tree
mixed effects model, 276
linear, see linear mixed effects
model
nonlinear, see nonlinear mixed
effects model
MLE, see maximum likelihood
model selection
cross-validation, see cross-validation
hypothesis tests, 24–26, 29, 142,
154–155, 179, 190
information criteria, 33–34, 142,
154–155, 179, 191,
193–194, 201, 284–285
model selection uncertainty, 35–38,
48
multicollinearity, see collinearity
multilevel model, 261, 276
multinomial, 173–186, 189
connection with Poisson, 189
nominal variable, see nominal
variable
ordinal variable, see ordinal variable
multiple comparisons, 118–120, 133
Bonferroni correction, see
Bonferroni correction
false discovery rate, 120
Tukey’s HSD test, see Tukey’s
HSD test
n
Nadaraya-Watson kernel estimator,
298
negative binomial, 193–194, 207
connection with Poisson, 193–194
negative binomial regression, 194
Nelson-Aalen estimator, 215, 239, 328
nested data, 256–258, 327
nominal logistic regression coefficient
hypothesis tests, 178–179
interpretation, 175
nominal logistic regression model,
175–176, 178–180

INDEX
347
nominal variable, 174–176, 178–180,
185
nonconstant variance, see
heteroscedasticity
nonlinear least squares, see least
squares, nonlinear least
squares
nonlinear mixed effects model,
263–264, 276
nonlinear regression, 243–253
advantages and disadvantages,
245–246
nonlinear regression coefficient
confidence intervals, 247
hypothesis tests, 247
nonstationary time series, 93–94, 105
o
odds ratio, 148, 151–152, 154, 170,
175
confidence interval, 154
invariance under type of design,
151–152
offset, 191–192, 207
OLS, see least squares, ordinary least
squares
one-SE rule, 280, 319
one-way analysis of variance, see
analysis of variance, one-way
optimism, 284, 291
ordinal logistic regression model, see
proportional odds model
ordinal variable, 176–180, 186
ordinary least squares, see least squares,
ordinary least squares
outlier, 56, 59, 64
overdispersion, 169, 192–194, 202,
203, 207
overfitting, 23–24, 29, 49
AIC, 33–34
p
panel data, see longitudinal data
partial F-test, 25–26, 49, 116–117,
136–137, 247
penalized likelihood estimator, 304,
312
pharmacokinetic models, 243–244,
247–248
Poisson, 188–189, 207
connection with binomial,
188–189
connection with multinomial, 189
Poisson regression, 190–193, 204, 328
as a generalized linear model, 190
use in weighted least squares, 204
Poisson regression coefficient
confidence intervals, 191
hypothesis tests, 190–191
interpretation, 190
polytomous variable, see multinomial
pooled model, 38–39, 49
population average model, 262–263,
276
population growth model, 244
positive predictive value, 120
post-selection inference, 290, 291
Prais-Winsten procedure, 100
predicting variable, 4, 20
prediction interval, 21
exact, 13–14
rough, 11–12, 13, 34, 68, 100, 120
prior probability, 152, 170
probit regression, 148–149, 171
product limit estimator, see
Kaplan-Meier estimator
proportional hazards, 328
proportional hazards model, see Cox
proportional hazards model
proportional odds model, 176–180,
186, 212
proportional odds model regression
coefficient
hypothesis tests, 179
interpretation, 178
prospective design, 149–152, 169, 171
q
quadratic term in regression, 4, 86,
264–269
quasi-AIC, 193
quasi-AICc, 193
quasi-likelihood, 169, 192–193, 207
r
R2, 11, 20, 26, 32
effect of leverage point, 58
random censoring, 211–212

348
INDEX
random effect, 257–260, 276
borrowing strength, 258
random forest, 321, 335
random intercepts, 260, 276
random slopes, 260, 276
RE-EM tree, see longitudinal tree
recursive partitioning, see tree
regression tree, 317, 335
purity measure, 317
regularization methods, 48, 278,
281–291
relaxed lasso, 284, 291
reliability function, see survival
function
REML, see restricted maximum
likelihood
replicability, 47, 49
residual, 6, 8, 14–15, 21, 56–57, 64,
65, 84–86, 122, 156, 159,
247
Cox-Snell, 220–221
deviance, 156, 221
martingale, 221
Pearson, 156, 159, 191
plots, 14–15, 85, 306
Schoenfeld, 222
standardized, 56–57, 64, 65, 122,
159, 247
residual mean square, 11–13, 21, 122
response variable, 4, 20
restricted maximum likelihood,
261–262, 276
retrospective design, 149–152, 169,
171
adjusting logistic regression
intercept, 152
ridge regression, 281, 291
robust regression, 64
ROC curve, 157, 169, 171
runs test, 85–86, 105
s
semielasticity, 70, 78, 190, 203
semilog model, 69–71, 78, 86, 148,
190, 243–245
semiparametric model, 300, 312
simple regression model, 5, 13, 57
Simpson’s paradox, 161, 171
smoothing, 207, 296–312
smoothing spline estimator, 299–300,
312
Somers’ D, 157–158, 169, 171,
179–180
span, 299, 312
sparsity, 279, 291
standard error of the estimate, 11–12,
21, 34
subject-specific model, 262–263, 276
survival data, 210–239, 328–330
counting process, see LTRC data,
connection with
time-varying covariates
time-varying covariates, see LTRC
data, connection with
time-varying covariates
survival function, 211, 239
survival tree, 328–330, 335
t
target variable, 4, 20
Taylor series, 96, 246–247, 253
time-to-event data, see survival data
tree, 314–335
splitting variable selection bias,
319–320, 335
Tukey’s HSD test, 119, 133
two-way analysis of variance, see
analysis of variance, two-way
u
unbalanced design, 118
underfitting, 24, 49
v
validation sample, 36–37, 158
variable importance measure, 321
variance inflation factor, 28–29, 49,
247
varying slope model, see interaction
effect
V IF, see variance inflation factor
w
Wald test, 154, 163, 167, 171, 179,
191, 193, 194, 247, 252
tendency to be deflated, 167, 252
Weibull, 212–214, 216

INDEX
349
weighted least squares, see least
squares, weighted least
squares
WLS, see least squares, weighted least
squares
z
zero-inflated model, 201–202, 207
negative binomial, 202
Poisson, 201–202
Poisson regression coefficient
interpretation, 201–202
zero-inflated negative binomial, see
zero-inflated model, negative
binomial
zero-inflated Poisson, see zero-inflated
model, Poisson
zero-truncated model, 202–203, 207
negative binomial, 203
Poisson, 202–203
Poisson regression coefficient
interpretation, 203
ZINB, see zero-inflated model,
negative binomial
ZIP, see zero-inflated model, Poisson

WILEY SERIES IN PROBABILITY AND STATISTICS
Editors: David. J. Balding, Noel A.C. Cressie, Garrett M. Fitzmaurice,
Harvey Goldstein, Iain M. Johnstone, Geert Molenberghs,
David W. Scott, Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg
Editors Emeriti: Vic Barnett, Ralph A. Bradley, J. Stuart Hunter, J.B. Kadane,
David G. Kendall, Jozef L. Teugels
The Wiley Series in Probability and Statistics is well established and authoritative. It covers many
topics of current research interest in both pure and applied statistics and probability theory. Written
and classical methods.
methodological and theoretical statistics, ranging from applications and new techniques made
possible by advances in computerized practice to rigorous treatment of theoretical approaches.
This series provides essential and invaluable reading for all statisticians, whether in academia,
industry, government, or research.
† ABRAHAM and LEDOLTER ⋅Statistical Methods for Forecasting
AGRESTI ⋅Analysis of Ordinal Categorical Data, Second Edition
AGRESTI ⋅An Introduction to Categorical Data Analysis, Second Edition
AGRESTI ⋅Categorical Data Analysis, Second Edition
ALTMAN, GILL, and McDONALD ⋅Numerical Issues in Statistical Computing for the
Social Scientist
AMARATUNGA and CABRERA ⋅Exploration and Analysis of DNA Microarray and
Protein Array Data
AND\BR\EL ⋅Mathematics of Chance
ANDERSON ⋅An Introduction to Multivariate Statistical Analysis, Third Edition
* ANDERSON ⋅The Statistical Analysis of Time Series
ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG ⋅Statistical
Methods for Comparative Studies
ANDERSON and LOYNES ⋅The Teaching of Practical Statistics
ARMITAGE and DAVID (editors) ⋅Advances in Biometry
ARNOLD, BALAKRISHNAN, and NAGARAJA ⋅Records
* ARTHANARI and DODGE ⋅Mathematical Programming in Statistics
* BAILEY ⋅The Elements of Stochastic Processes with Applications to the Natural Sciences
BAJORSKI ⋅Statistics for Imaging, Optics, and Photonics
BALAKRISHNAN and KOUTRAS ⋅Runs and Scans with Applications
BALAKRISHNAN and NG ⋅Precedence-Type Tests and Applications
BARNETT ⋅Comparative Statistical Inference, Third Edition
BARNETT ⋅Environmental Statistics
BARNETT and LEWIS ⋅Outliers in Statistical Data, Third Edition
BARTHOLOMEW, KNOTT, and MOUSTAKI ⋅Latent Variable Models and Factor Analy-
Third Edition
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ ⋅Probability and Statistical Inference,
Second Edition
BASILEVSKY ⋅Statistical Factor Analysis and Related Methods: Theory and Applications
BATES and WATTS ⋅Nonlinear Regression Analysis and Its Applications
BECHHOFER, SANTNER, and GOLDSMAN ⋅Design and Analysis of Experiments for
Statistical Selection, Screening, and Multiple Comparisons
BEIRLANT, GOEGEBEUR, SEGERS, TEUGELS, and DE WAAL ⋅Statistics of Extremes:
Theory and Applications
BELSLEY ⋅Conditioning Diagnostics: Collinearity and Weak Data in Regression
† BELSLEY, KUH, and WELSCH ⋅
Sources of Collinearity
BENDAT and PIERSOL ⋅Random Data: Analysis and Measurement Procedures, Fourth
Edition
BERNARDO and SMITH ⋅Bayesian Theory
BHAT and MILLER ⋅Elements of Applied Stochastic Processes, Third Edition
BHATTACHARYA and WAYMIRE ⋅Stochastic Processes with Applications
BIEMER, GROVES, LYBERG, MATHIOWETZ, and SUDMAN ⋅Measurement Errors in
Surveys
BILLINGSLEY ⋅Convergence of Probability Measures, Second Edition
BILLINGSLEY ⋅Probability and Measure, Anniversary Edition
BIRKES and DODGE ⋅Alternative Methods of Regression
BISGAARD and KULAHCI ⋅Time Series Analysis and Forecasting by Example
BISWAS, DATTA, FINE, and SEGAL ⋅Statistical Advances in the Biomedical Sciences:
Clinical Trials, Epidemiology, Survival Analysis, and Bioinformatics
BLISCHKE and MURTHY (editors) ⋅Case Studies in Reliability and Maintenance
BLISCHKE and MURTHY ⋅Reliability: Modeling, Prediction, and Optimization
BLOOMFIELD ⋅Fourier Analysis of Time Series: An Introduction, Second Edition
BOLLEN ⋅Structural Equations with Latent Variables
BOLLEN and CURRAN ⋅Latent Curve Models: A Structural Equation Perspective
BOROVKOV ⋅Ergodicity and Stability of Stochastic Processes
BOSQ and BLANKE ⋅Inference and Prediction in Large Dimensions
BOULEAU ⋅Numerical Methods for Stochastic Processes
* BOX ⋅Bayesian Inference in Statistical Analysis
BOX ⋅Improving Almost Anything, Revised Edition
* BOX and DRAPER ⋅Evolutionary Operation: A Statistical Method for Process Improve-
ment
BOX and DRAPER ⋅Response Surfaces, Mixtures, and Ridge Analyses, Second Edition
BOX, HUNTER, and HUNTER ⋅Statistics for Experimenters: Design, Innovation, and Dis-
covery, Second Editon
BOX, JENKINS, and REINSEL ⋅Time Series Analysis: Forcasting and Control, Fourth
Edition
BOX, LUCEÑO, and PANIAGUA-QUIÑONES ⋅Statistical Control by Monitoring and
Adjustment, Second Edition
* BROWN and HOLLANDER ⋅Statistics: A Biomedical Introduction
CAIROLI and DALANG ⋅Sequential Stochastic Optimization
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

CASTILLO, HADI, BALAKRISHNAN, and SARABIA ⋅Extreme Value and Related Mod-
els with Applications in Engineering and Science
CHAN ⋅Time Series: Applications to Finance with R and S-Plus\sr\, Second Edition
CHARALAMBIDES ⋅Combinatorial Methods in Discrete Distributions
CHATTERJEE and HADI ⋅Regression Analysis by Example, Fourth Edition
CHATTERJEE and HADI ⋅Sensitivity Analysis in Linear Regression
CHERNICK ⋅Bootstrap Methods: A Guide for Practitioners and Researchers, Second Edi-
tion
CHERNICK and FRIIS ⋅Introductory Biostatistics for the Health Sciences
CHILES and DELFINER ⋅Geostatistics: Modeling Spatial Uncertainty, Second Edition
CHOW and LIU ⋅Design and Analysis of Clinical Trials: Concepts and Methodologies,
Second Edition
CLARKE ⋅Linear Models: The Theory and Application of Analysis of Variance
CLARKE and DISNEY ⋅Probability and Random Processes: A First Course with Applica-
tions, Second Edition
* COCHRAN and COX ⋅Experimental Designs, Second Edition
COLLINS and LANZA ⋅Latent Class and Latent Transition Analysis: With Applications in
the Social, Behavioral, and Health Sciences
CONGDON ⋅Applied Bayesian Modelling
CONGDON ⋅Bayesian Models for Categorical Data
CONGDON ⋅Bayesian Statistical Modelling, Second Edition
CONOVER ⋅Practical Nonparametric Statistics, Third Edition
COOK ⋅Regression Graphics
COOK and WEISBERG ⋅An Introduction to Regression Graphics
COOK and WEISBERG ⋅Applied Regression Including Computing and Graphics
CORNELL ⋅A Primer on Experiments with Mixtures
CORNELL ⋅Experiments with Mixtures, Designs, Models, and the Analysis of Mixture
Data, Third Edition
COX ⋅A Handbook of Introductory Statistical Methods
CRESSIE ⋅Statistics for Spatial Data, Revised Edition
CRESSIE and WIKLE ⋅Statistics for Spatio-Temporal Data
CSÖRG ˝O and HORVÁTH ⋅Limit Theorems in Change Point Analysis
DAGPUNAR ⋅Simulation and Monte Carlo: With Applications in Finance and MCMC
DANIEL ⋅Applications of Statistics to Industrial Experimentation
DANIEL ⋅Biostatistics: A Foundation for Analysis in the Health Sciences, Eighth Edition
* DANIEL ⋅Fitting Equations to Data: Computer Analysis of Multifactor Data, Second
Edition
DASU and JOHNSON ⋅Exploratory Data Mining and Data Cleaning
DAVID and NAGARAJA ⋅Order Statistics, Third Edition
* DEGROOT, FIENBERG, and KADANE ⋅Statistics and the Law
DEL CASTILLO ⋅Statistical Process Adjustment for Quality Control
DEMARIS ⋅Regression with Social Data: Modeling Continuous and Limited Response
Variables
DEMIDENKO ⋅Mixed Models: Theory and Applications
TXT∼∼000004
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

DENISON, HOLMES, MALLICK and SMITH ⋅
cation and Regression
DETTE and STUDDEN ⋅The Theory of Canonical Moments with Applications in Statistics,
Probability, and Analysis
DEY and MUKERJEE ⋅Fractional Factorial Plans
DE ROCQUIGNY ⋅Modelling Under Risk and Uncertainty: An Introduction to Statistical,
Phenomenological and Computational Models
DILLON and GOLDSTEIN ⋅Multivariate Analysis: Methods and Applications
* DODGE and ROMIG ⋅Sampling Inspection Tables, Second Edition
* DOOB ⋅Stochastic Processes
DOWDY, WEARDEN, and CHILKO ⋅Statistics for Research, Third Edition
DRAPER and SMITH ⋅Applied Regression Analysis, Third Edition
DRYDEN and MARDIA ⋅Statistical Shape Analysis
DUDEWICZ and MISHRA ⋅Modern Mathematical Statistics
DUNN and CLARK ⋅Basic Statistics: A Primer for the Biomedical Sciences, Fourth Edition
DUPUIS and ELLIS ⋅A Weak Convergence Approach to the Theory of Large Deviations
EDLER and KITSOS ⋅Recent Advances in Quantitative Methods in Cancer and Human
Health Risk Assessment
* ELANDT-JOHNSON and JOHNSON ⋅Survival Models and Data Analysis
ENDERS ⋅Applied Econometric Time Series, Third Edition
† ETHIER and KURTZ ⋅Markov Processes: Characterization and Convergence
EVANS, HASTINGS, and PEACOCK ⋅Statistical Distributions, Third Edition
EVERITT, LANDAU, LEESE, and STAHL ⋅Cluster Analysis, Fifth Edition
FEDERER and KING ⋅Variations on Split Plot and Split Block Experiment Designs
FELLER ⋅An Introduction to Probability Theory and Its Applications, Volume I, Third
Edition, Revised; Volume II, Second Edition
FITZMAURICE, LAIRD, and WARE ⋅Applied Longitudinal Analysis, Second Edition
* FLEISS ⋅The Design and Analysis of Clinical Experiments
FLEISS ⋅Statistical Methods for Rates and Proportions, Third Edition
† FLEMING and HARRINGTON ⋅Counting Processes and Survival Analysis
FUJIKOSHI, ULYANOV, and SHIMIZU ⋅Multivariate Statistics: High-Dimensional and
Large-Sample Approximations
FULLER ⋅Introduction to Statistical Time Series, Second Edition
† FULLER ⋅Measurement Error Models
GALLANT ⋅Nonlinear Statistical Models
GEISSER ⋅Modes of Parametric Statistical Inference
GELMAN and MENG ⋅Applied Bayesian Modeling and Causal Inference from Incomplete-
Data Perspectives
GEWEKE ⋅Contemporary Bayesian Econometrics and Statistics
GHOSH, MUKHOPADHYAY, and SEN ⋅Sequential Estimation
GIESBRECHT and GUMPERTZ ⋅Planning, Construction, and Statistical Analysis of
Comparative Experiments
GIFI ⋅Nonlinear Multivariate Analysis
GIVENS and HOETING ⋅Computational Statistics
GLASSERMAN and YAO ⋅Monotone Structure in Discrete-Event Systems
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

GNANADESIKAN ⋅Methods for Statistical Data Analysis of Multivariate Observations,
Second Edition
GOLDSTEIN ⋅Multilevel Statistical Models, Fourth Edition
GOLDSTEIN and LEWIS ⋅Assessment: Problems, Development, and Statistical Issues
GOLDSTEIN and WOOFF ⋅Bayes Linear Statistics
GREENWOOD and NIKULIN ⋅A Guide to Chi-Squared Testing
GROSS, SHORTLE, THOMPSON, and HARRIS ⋅Fundamentals of Queueing Theory,
Fourth Edition
GROSS, SHORTLE, THOMPSON, and HARRIS ⋅Solutions Manual to Accompany
Fundamentals of Queueing Theory, Fourth Edition
* HAHN and SHAPIRO ⋅Statistical Models in Engineering
HAHN and MEEKER ⋅Statistical Intervals: A Guide for Practitioners
HALD ⋅A History of Probability and Statistics and their Applications Before 1750
† HAMPEL ⋅
HARTUNG, KNAPP, and SINHA ⋅Statistical Meta-Analysis with Applications
HEIBERGER ⋅Computation for the Analysis of Designed Experiments
HEDAYAT and SINHA ⋅Design and Inference in Finite Population Sampling
HEDEKER and GIBBONS ⋅Longitudinal Data Analysis
HELLER ⋅MACSYMA for Statisticians
HERITIER, CANTONI, COPT, and VICTORIA-FESER ⋅Robust Methods in Biostatistics
HINKELMANN and KEMPTHORNE ⋅Design and Analysis of Experiments, Volume 1:
Introduction to Experimental Design, Second Edition
HINKELMANN and KEMPTHORNE ⋅Design and Analysis of Experiments, Volume 2:
Advanced Experimental Design
HINKELMANN (editor) ⋅Design and Analysis of Experiments, Volume 3: Special Designs
and Applications
* HOAGLIN, MOSTELLER, and TUKEY ⋅Fundamentals of Exploratory Analysis of
Variance
* HOAGLIN, MOSTELLER, and TUKEY ⋅Exploring Data Tables, Trends and Shapes
* HOAGLIN, MOSTELLER, and TUKEY ⋅Understanding Robust and Exploratory Data
Analysis
HOCHBERG and TAMHANE ⋅Multiple Comparison Procedures
HOCKING ⋅Methods and Applications of Linear Models: Regression and the Analysis of
Variance, Second Edition
HOEL ⋅Introduction to Mathematical Statistics, Fifth Edition
HOGG and KLUGMAN ⋅Loss Distributions
HOLLANDER and WOLFE ⋅Nonparametric Statistical Methods, Second Edition
HOSMER and LEMESHOW ⋅Applied Logistic Regression, Second Edition
HOSMER, LEMESHOW, and MAY ⋅Applied Survival Analysis: Regression Modeling of
Time-to-Event Data, Second Edition
HUBER ⋅Data Analysis: What Can Be Learned From the Past 50 Years
HUBER ⋅Robust Statistics
† HUBER and RONCHETTI ⋅Robust Statistics, Second Edition
HUBERTY ⋅Applied Discriminant Analysis, Second Edition
HUBERTY and OLEJNIK ⋅Applied MANOVA and Discriminant Analysis, Second Edition
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

HUITEMA ⋅The Analysis of Covariance and Alternatives: Statistical Methods for Experi-
ments, Quasi-Experiments, and Single-Case Studies, Second Edition
HUNT and KENNEDY ⋅Financial Derivatives in Theory and Practice, Revised Edition
HURD and MIAMEE ⋅Periodically Correlated Random Sequences: Spectral Theory and
Practice
HUSKOVA, BERAN, and DUPAC ⋅Collected Works of Jaroslav Hajek—with Commentary
HUZURBAZAR ⋅Flowgraph Models for Multistate Time-to-Event Data
INSUA, RUGGERI and WIPER ⋅Bayesian Analysis of Stochastic Process Models
JACKMAN ⋅Bayesian Analysis for the Social Sciences
† JACKSON ⋅A User’s Guide to Principle Components
JOHN ⋅Statistical Methods in Engineering and Quality Assurance
JOHNSON ⋅Multivariate Statistical Simulation
JOHNSON and BALAKRISHNAN ⋅Advances in the Theory and Practice of Statistics:
A Volume in Honor of Samuel Kotz
JOHNSON, KEMP, and KOTZ ⋅Univariate Discrete Distributions, Third Edition
JOHNSON and KOTZ (editors) ⋅Leading Personalities in Statistical Sciences: From the
Seventeenth Century to the Present
JOHNSON, KOTZ, and BALAKRISHNAN ⋅Continuous Univariate Distributions, Volume
1, Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN ⋅Continuous Univariate Distributions, Volume
2, Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN ⋅Discrete Multivariate Distributions
JUDGE, GRIFFITHS, HILL, LÜTKEPOHL, and LEE ⋅The Theory and Practice of Econo-
metrics, Second Edition
JUREK and MASON ⋅Operator-Limit Distributions in Probability Theory
KADANE ⋅Bayesian Methods and Ethics in a Clinical Trial Design
KADANE AND SCHUM ⋅A Probabilistic Analysis of the Sacco and Vanzetti Evidence
KALBFLEISCH and PRENTICE ⋅The Statistical Analysis of Failure Time Data, Second
Edition
KARIYA and KURATA ⋅Generalized Least Squares
KASS and VOS ⋅Geometrical Foundations of Asymptotic Inference
† KAUFMAN and ROUSSEEUW ⋅Finding Groups in Data: An Introduction to Cluster
Analysis
KEDEM and FOKIANOS ⋅Regression Models for Time Series Analysis
KENDALL, BARDEN, CARNE, and LE ⋅Shape and Shape Theory
KHURI ⋅Advanced Calculus with Applications in Statistics, Second Edition
KHURI, MATHEW, and SINHA ⋅Statistical Tests for Mixed Linear Models
* KISH ⋅Statistical Design for Research
KLEIBER and KOTZ ⋅Statistical Size Distributions in Economics and Actuarial Sciences
KLEMELÄ ⋅Smoothing of Multivariate Data: Density Estimation and Visualization
KLUGMAN, PANJER, and WILLMOT ⋅Loss Models: From Data to Decisions, Third
Edition
KLUGMAN, PANJER, and WILLMOT ⋅Solutions Manual to Accompany Loss Models:
From Data to Decisions, Third Edition
KOSKI and NOBLE ⋅Bayesian Networks: An Introduction
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

KOTZ, BALAKRISHNAN, and JOHNSON ⋅Continuous Multivariate Distributions, Vol-
ume 1, Second Edition
KOTZ and JOHNSON (editors) ⋅Encyclopedia of Statistical Sciences: Volumes 1 to 9 with
Index
KOTZ and JOHNSON (editors) ⋅Encyclopedia of Statistical Sciences: Supplement Volume
KOTZ, READ, and BANKS (editors) ⋅Encyclopedia of Statistical Sciences: Update
Volume 1
KOTZ, READ, and BANKS (editors) ⋅Encyclopedia of Statistical Sciences: Update
Volume 2
KOWALSKI and TU ⋅Modern Applied U-Statistics
KRISHNAMOORTHY and MATHEW ⋅Statistical Tolerance Regions: Theory, Applica-
tions, and Computation
KROESE, TAIMRE, and BOTEV ⋅Handbook of Monte Carlo Methods
KROONENBERG ⋅Applied Multiway Data Analysis
KULINSKAYA, MORGENTHALER, and STAUDTE ⋅Meta Analysis: A Guide to Cali-
brating and Combining Statistical Evidence
KULKARNI and HARMAN ⋅An Elementary Introduction to Statistical Learning Theory
KUROWICKA and COOKE ⋅Uncertainty Analysis with High Dimensional Dependence
Modelling
KVAM and VIDAKOVIC ⋅Nonparametric Statistics with Applications to Science and
Engineering
LACHIN ⋅Biostatistical Methods: The Assessment of Relative Risks, Second Edition
LAD ⋅Operational Subjective Statistical Methods: A Mathematical, Philosophical, and
Historical Introduction
LAMPERTI ⋅Probability: A Survey of the Mathematical Theory, Second Edition
LAWLESS ⋅Statistical Models and Methods for Lifetime Data, Second Edition
LAWSON ⋅Statistical Methods in Spatial Epidemiology, Second Edition
LE ⋅Applied Categorical Data Analysis, Second Edition
LE ⋅Applied Survival Analysis
LEE ⋅Structural Equation Modeling: A Bayesian Approach
LEE and WANG ⋅Statistical Methods for Survival Data Analysis, Third Edition
LEPAGE and BILLARD ⋅Exploring the Limits of Bootstrap
LESSLER and KALSBEEK ⋅Nonsampling Errors in Surveys
LEYLAND and GOLDSTEIN (editors) ⋅Multilevel Modelling of Health Statistics
LIAO ⋅Statistical Group Comparison
LIN ⋅Introductory Stochastic Analysis for Finance and Insurance
LITTLE and RUBIN ⋅Statistical Analysis with Missing Data, Second Edition
LLOYD ⋅The Statistical Analysis of Categorical Data
LOWEN and TEICH ⋅Fractal-Based Point Processes
MAGNUS and NEUDECKER ⋅Matrix Differential Calculus with Applications in Statistics
and Econometrics, Revised Edition
MALLER and ZHOU ⋅Survival Analysis with Long Term Survivors
MARCHETTE ⋅Random Graphs for Statistical Pattern Recognition
MARDIA and JUPP ⋅Directional Statistics
MARKOVICH ⋅Nonparametric Analysis of Univariate Heavy-Tailed Data: Research and
Practice
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

MARONNA, MARTIN and YOHAI ⋅Robust Statistics: Theory and Methods
MASON, GUNST, and HESS ⋅Statistical Design and Analysis of Experiments with Appli-
cations to Engineering and Science, Second Edition
McCULLOCH, SEARLE, and NEUHAUS ⋅Generalized, Linear, and Mixed Models,
Second Edition
McFADDEN ⋅Management of Data in Clinical Trials, Second Edition
* McLACHLAN ⋅Discriminant Analysis and Statistical Pattern Recognition
McLACHLAN, DO, and AMBROISE ⋅Analyzing Microarray Gene Expression Data
McLACHLAN and KRISHNAN ⋅The EM Algorithm and Extensions, Second Edition
McLACHLAN and PEEL ⋅Finite Mixture Models
McNEIL ⋅Epidemiological Research Methods
MEEKER and ESCOBAR ⋅Statistical Methods for Reliability Data
MEERSCHAERT and SCHEFFLER ⋅Limit Distributions for Sums of Independent Random
Vectors: Heavy Tails in Theory and Practice
MENGERSEN, ROBERT, and TITTERINGTON ⋅Mixtures: Estimation and Applications
MICKEY, DUNN, and CLARK ⋅Applied Statistics: Analysis of Variance and Regression,
Third Edition
* MILLER ⋅Survival Analysis, Second Edition
MONTGOMERY, JENNINGS, and KULAHCI ⋅Introduction to Time Series Analysis and
Forecasting
MONTGOMERY, PECK, and VINING ⋅Introduction to Linear Regression Analysis, Fifth
Edition
MORGENTHALER and TUKEY ⋅
Robustness
MUIRHEAD ⋅Aspects of Multivariate Statistical Theory
MULLER and STOYAN ⋅Comparison Methods for Stochastic Models and Risks
MURTHY, XIE, and JIANG ⋅Weibull Models
MYERS, MONTGOMERY, and ANDERSON-COOK ⋅Response Surface Methodology:
Process and Product Optimization Using Designed Experiments, Third Edition
MYERS, MONTGOMERY, VINING, and ROBINSON ⋅Generalized Linear Models. With
Applications in Engineering and the Sciences, Second Edition
NATVIG ⋅Multistate Systems Reliability Theory With Applications
† NELSON ⋅Accelerated Testing, Statistical Models, Test Plans, and Data Analyses
† NELSON ⋅Applied Life Data Analysis
NEWMAN ⋅Biostatistical Methods in Epidemiology
NG, TAIN, and TANG ⋅Dirichlet Theory: Theory, Methods and Applications
OKABE, BOOTS, SUGIHARA, and CHIU ⋅Spatial Tesselations: Concepts and Applica-
tions of Voronoi Diagrams, Second Edition
OLIVER and SMITH ⋅
PALTA ⋅Quantitative Methods in Population Health: Extensions of Ordinary Regressions
PANJER ⋅Operational Risk: Modeling and Analytics
PANKRATZ ⋅Forecasting with Dynamic Regression Models
PANKRATZ ⋅Forecasting with Univariate Box-Jenkins Models: Concepts and Cases
PARDOUX ⋅Markov Processes and Applications: Algorithms, Networks, Genome and
Finance
PARMIGIANI and INOUE ⋅Decision Theory: Principles and Approaches
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

* PARZEN ⋅Modern Probability Theory and Its Applications
PEÑA, TIAO, and TSAY ⋅A Course in Time Series Analysis
PESARIN and SALMASO ⋅Permutation Tests for Complex Data: Applications and
Software
PIANTADOSI ⋅Clinical Trials: A Methodologic Perspective, Second Edition
POURAHMADI ⋅Foundations of Time Series Analysis and Prediction Theory
POWELL ⋅Approximate Dynamic Programming: Solving the Curses of Dimensionality,
Second Edition
POWELL and RYZHOV ⋅Optimal Learning
PRESS ⋅Subjective and Objective Bayesian Statistics, Second Edition
PRESS and TANUR ⋅The Subjectivity of Scientists and the Bayesian Approach
PURI, VILAPLANA, and WERTZ ⋅New Perspectives in Theoretical and Applied Statistics
† PUTERMAN ⋅Markov Decision Processes: Discrete Stochastic Dynamic Programming
QIU ⋅Image Processing and Jump Regression Analysis
* RAO ⋅Linear Statistical Inference and Its Applications, Second Edition
RAO ⋅Statistical Inference for Fractional Diffusion Processes
RAUSAND and HØYLAND ⋅System Reliability Theory: Models, Statistical Methods, and
Applications, Second Edition
RAYNER, THAS, and BEST ⋅Smooth Tests of Goodnes of Fit: Using R, Second Edition
RENCHER ⋅Linear Models in Statistics, Second Edition
RENCHER ⋅Methods of Multivariate Analysis, Second Edition
RENCHER ⋅Multivariate Statistical Inference with Applications
RIGDON and BASU ⋅Statistical Methods for the Reliability of Repairable Systems
* RIPLEY ⋅Spatial Statistics
* RIPLEY ⋅Stochastic Simulation
ROHATGI and SALEH ⋅An Introduction to Probability and Statistics, Second Edition
ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS ⋅Stochastic Processes for Insurance and
Finance
ROSENBERGER and LACHIN ⋅Randomization in Clinical Trials: Theory and Practice
ROSSI, ALLENBY, and McCULLOCH ⋅Bayesian Statistics and Marketing
† ROUSSEEUW and LEROY ⋅Robust Regression and Outlier Detection
ROYSTON and SAUERBREI ⋅Multivariate Model Building: A Pragmatic Approach
to Regression Analysis Based on Fractional Polynomials for Modeling Continuous
Variables
* RUBIN ⋅Multiple Imputation for Nonresponse in Surveys
RUBINSTEIN and KROESE ⋅Simulation and the Monte Carlo Method, Second Edition
RUBINSTEIN and MELAMED ⋅Modern Simulation and Modeling
RYAN ⋅Modern Engineering Statistics
RYAN ⋅Modern Experimental Design
RYAN ⋅Modern Regression Methods, Second Edition
RYAN ⋅Statistical Methods for Quality Improvement, Third Edition
SALEH ⋅Theory of Preliminary Test and Stein-Type Estimation with Applications
SALTELLI, CHAN, and SCOTT (editors) ⋅Sensitivity Analysis
SCHERER ⋅Batch Effects and Noise in Microarray Experiments: Sources and Solutions
* SCHEFFE ⋅The Analysis of Variance
SCHIMEK ⋅Smoothing and Regression: Approaches, Computation, and Application
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

SCHOTT ⋅Matrix Analysis for Statistics, Second Edition
SCHOUTENS ⋅Levy Processes in Finance: Pricing Financial Derivatives
SCOTT ⋅Multivariate Density Estimation: Theory, Practice, and Visualization
* SEARLE ⋅Linear Models
† SEARLE ⋅Linear Models for Unbalanced Data
† SEARLE ⋅Matrix Algebra Useful for Statistics
† SEARLE, CASELLA, and McCULLOCH ⋅Variance Components
SEARLE and WILLETT ⋅Matrix Algebra for Applied Economics
SEBER ⋅A Matrix Handbook For Statisticians
† SEBER ⋅Multivariate Observations
SEBER and LEE ⋅Linear Regression Analysis, Second Edition
† SEBER and WILD ⋅Nonlinear Regression
SENNOTT ⋅Stochastic Dynamic Programming and the Control of Queueing Systems
* SERFLING ⋅Approximation Theorems of Mathematical Statistics
SHAFER and VOVK ⋅Probability and Finance: It’s Only a Game!
SHERMAN ⋅Spatial Statistics and Spatio-Temporal Data: Covariance Functions and Direc-
tional Properties
SILVAPULLE and SEN ⋅Constrained Statistical Inference: Inequality, Order, and Shape
Restrictions
SINGPURWALLA ⋅Reliability and Risk: A Bayesian Perspective
SMALL and McLEISH ⋅Hilbert Space Methods in Probability and Statistical Inference
SRIVASTAVA ⋅Methods of Multivariate Statistics
STAPLETON ⋅Linear Statistical Models, Second Edition
STAPLETON ⋅Models for Probability and Statistical Inference: Theory and Applications
STAUDTE and SHEATHER ⋅Robust Estimation and Testing
STOYAN ⋅Counterexamples in Probability, Second Edition
STOYAN, KENDALL, and MECKE ⋅Stochastic Geometry and Its Applications, Second
Edition
STOYAN and STOYAN ⋅Fractals, Random Shapes and Point Fields: Methods of Geomet-
rical Statistics
STREET and BURGESS ⋅The Construction of Optimal Stated Choice Experiments: Theory
and Methods
STYAN ⋅The Collected Papers of T. W. Anderson: 1943–1985
SUTTON, ABRAMS, JONES, SHELDON, and SONG ⋅Methods for Meta-Analysis in
Medical Research
TAKEZAWA ⋅Introduction to Nonparametric Regression
TAMHANE ⋅Statistical Analysis of Designed Experiments: Theory and Applications
TANAKA ⋅Time Series Analysis: Nonstationary and Noninvertible Distribution Theory
THOMPSON ⋅Empirical Model Building: Data, Models, and Reality, Second Edition
THOMPSON ⋅Sampling, Third Edition
THOMPSON ⋅Simulation: A Modeler’s Approach
THOMPSON and SEBER ⋅Adaptive Sampling
THOMPSON, WILLIAMS, and FINDLAY ⋅Models for Investors in Real World Markets
TIERNEY ⋅LISP-STAT: An Object-Oriented Environment for Statistical Computing and
Dynamic Graphics
TSAY ⋅Analysis of Financial Time Series, Third Edition
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

UPTON and FINGLETON ⋅Spatial Data Analysis by Example, Volume II: Categorical and
Directional Data
† VAN BELLE ⋅Statistical Rules of Thumb, Second Edition
VAN BELLE, FISHER, HEAGERTY, and LUMLEY ⋅Biostatistics: A Methodology for the
Health Sciences, Second Edition
VESTRUP ⋅The Theory of Measures and Integration
VIDAKOVIC ⋅Statistical Modeling by Wavelets
VIERTL ⋅Statistical Methods for Fuzzy Data
VINOD and REAGLE ⋅Preparing for the Worst: Incorporating Downside Risk in Stock
Market Investments
WALLER and GOTWAY ⋅Applied Spatial Statistics for Public Health Data
WANG and WANG ⋅Structural Equation Modeling: Applications Using Mplus
WEISBERG ⋅Applied Linear Regression, Third Edition
WEISBERG ⋅Bias and Causation: Models and Judgment for Valid Comparisons
WELSH ⋅Aspects of Statistical Inference
WESTFALL and YOUNG ⋅Resampling-Based Multiple Testing: Examples and Methods
for p-Value Adjustment
* WHITTAKER ⋅Graphical Models in Applied Multivariate Statistics
WINKER ⋅Optimization Heuristics in Economics: Applications of Threshold Accepting
WOOD WORTH ⋅Biostatistics: A Bayesian Introduction
WOOLSON and CLARKE ⋅Statistical Methods for the Analysis of Biomedical Data,
Second Edition
WU and HAMADA ⋅Experiments: Planning, Analysis, and Parameter Design Optimization,
Second Edition
WU and ZHANG ⋅Nonparametric Regression Methods for Longitudinal Data Analysis
YIN ⋅Clinical Trial Design: Bayesian and Frequentist Adaptive Methods
YOUNG, VALERO-MORA, and FRIENDLY ⋅Visual Statistics: Seeing Data with Dynamic
Interactive Graphics
ZACKS ⋅Stage-Wise Adaptive Designs
* ZELLNER ⋅An Introduction to Bayesian Inference in Econometrics
ZELTERMAN ⋅Discrete Distributions—Applications in the Health Sciences
ZHOU, OBUCHOWSKI, and McCLISH ⋅Statistical Methods in Diagnostic Medicine,
Second Edition
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in a lower priced paperback edition in the Wiley-Interscience Paperback Series.

