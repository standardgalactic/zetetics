Learning to Communicate and Correct Pose Errors
Nicholas Vadivelu1,2∗, Mengye Ren1,3, James Tu1,3, Jingkang Wang1,3 and Raquel Urtasun1,3
Uber Advanced Technologies Group1, University of Waterloo2, University of Toronto3
nbvadive@uwaterloo.ca, {mren3,james.tu,jingkang,urtasun}@uber.com
Abstract: Learned communication makes multi-agent systems more effective by
aggregating distributed information. However, it also exposes individual agents
to the threat of erroneous messages they might receive. In this paper, we study
the setting proposed in V2VNet [1], where nearby self-driving vehicles jointly
perform object detection and motion forecasting in a cooperative manner. Despite
a huge performance boost when the agents solve the task together, the gain is
quickly diminished in the presence of pose noise since the communication relies
on spatial transformations. Hence, we propose a novel neural reasoning framework
that learns to communicate, to estimate potential errors, and ﬁnally, to reach a
consensus about those errors. Experiments conﬁrm that our proposed framework
signiﬁcantly improves the robustness of multi-agent self-driving perception and
motion forecasting systems under realistic and severe localization noise.
Keywords: multi-agent, self-driving, perception, prediction
1
Introduction
Despite the powerful capabilities of deep neural networks in ﬁtting raw, high dimensional data, they
are limited by the computational power and sensory input available to a single agent. Thus, combining
the sensory information and computational power of multiple agents to cooperatively accomplish a
goal can greatly amplify the effectiveness of these systems [1, 2, 3, 4, 5]. For example, V2VNet [1]
has recently shown that by allowing multiple self-driving vehicles (SDVs) to communicate through a
set of learned spatially-aware feature maps, we can obtain signiﬁcant gains in detecting obstacles that
would have otherwise been occluded or far away from a single-agent perspective.
The success of V2VNet depends on the precise localization of each participating vehicle, which
is used to warp the feature maps so they can be spatially aligned. Localization noise, however, is
common in the real world. While V2VNet exhibits some implicit tolerance, the performance degrades
below single-agent performance under realistic amounts of noise. Due to the safety critical nature
of self-driving, it is paramount to study the robustness against pose noise in a vehicle-to-vehicle
communication system and to design models that can explicitly reason under such noise.
In this paper, we propose end-to-end learnable neural reasoning layers that learn to communicate, to
estimate pose errors, and ﬁnally, to reach a consensus about those errors. First, the pose regression
module predicts the relative pose noise between a pair of vehicles. Second, to ensure globally
consistent poses, we propose a consistency module based on a Markov random ﬁeld with Bayesian
reweighting. Lastly, in the communicated messages aggregation step, we propose using predicted
attention weights to attenuate the outlier messages among vehicles.
Our evaluation under the same setting as the original V2VNet shows that our model can maintain the
same level of performance under strong translation and heading localization noise, while V2VNet
eventually suffers from such input noise, even if the network is trained with data augmentation. Our
framework also outperforms other competitive pose synchronization methods.
∗Work done while at Uber ATG.
4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA.
arXiv:2011.05289v1  [cs.CV]  10 Nov 2020

Receiver Spatially-Aware Aggregation
Incorrect 
Localization
V2V
Communication
V2V Setting
Receiver Sensor Input
Feature Map
Sender Sensor Input
Good Performance
Bad Performance
Without Pose Error
GNN 
Aggregation
Well Aligned
With Pose Error
Poorly Aligned
Feature Map
F
F
G
G
H
H
Figure 1: V2V communication setting with pose noise without correction. We demonstrate the
case where there is one receiver (red) and one sender (blue). Typically, the communication would be
two-way, but we illustrate one way for clarity. Pose noise causes the features to be misaligned during
aggregation, making them unusable for detection and motion forecasting tasks.
2
Related Work
In this section, we describe the literature in the area of collaborative self-driving. We also give an
overview on related problem formulations such as transformation synchronization, visual odometry,
and point-cloud registration.
Collaborative self-driving:
Existing literature studies how to leverage multiple self-driving vehi-
cles (SDVs) to perform vehicle-to-vehicle communication (V2V) to enhance perception, prediction,
and motion planning. The beneﬁts of multiple agents can be exploited by aggregating raw sensor
data [3], communicating intermediate feature maps [1], or combining the outputs of multiple vehi-
cles [5, 6, 7]. [1, 3] show limited robustness to localization error, with no explicit steps to address it.
We follow the setting of V2VNet [1] by communicating intermediate feature maps since it achieves
better performance and more efﬁcient communication.
Transformation synchronization:
Transformation synchronization is the process of extracting
absolute poses given relative poses. Methods include spectral solutions [8, 9, 10, 11], semideﬁnite
relaxations [12, 13, 8], probabilistic approaches [12, 14], sparse matrix decomposition [15], and/or
learned approaches [10, 11, 16]. While these methods could be used to reﬁne our pairwise estimates,
they are only shown to be robust when there are many more views to synchronize than in our setting
(e.g., 30 views per scene in [17] vs. up to 7 in our setting). Hence, they are susceptible to outliers
which strongly inﬂuence the ﬁnal synchronized poses. Our approach can certainly be used in standard
transformation synchronization problems, but more importantly, we propose an end-to-end system
for robust multi-agent perception and motion forecasting.
Visual odometry:
Visual odometry is the process of determining the pose of an agent given images
from the agent’s view. In our setting, when correcting pose error, we extract the relative poses given
pairs of views. Yousif et al. [18] provide a survey on several visual odometry methods, including
feature-based [19, 20] and stereo-based [21, 22]. More recently, approaches based on RCNN [23, 24]
learn this task end-to-end. These approaches are optimized for images, LiDAR, or other raw sensory
inputs, whereas in our setting, we aim to align intermediate feature maps.
Point cloud registration:
Point cloud registration is the task of ﬁnding a (typically rigid) transfor-
mation to align two point clouds. [25, 26] propose robust methods for registration, while [27, 28]
propose deep learning based approaches. [29] provides a full review of traditional point cloud
registration methods. These methods are not suited in our setting due to the high communication
overhead required to transmit LiDAR point clouds to neighboring self-driving vehicles.
Multi-agent deep learning:
Outside of self-driving, there is broad literature on multi-agent deep
learning systems. [30, 31] communicate actions and state to other agents, while [32] use a controller
network for communication. Our setting is more similar to the former, where each vehicle communi-
cates an intermediate representation of its view to nearby vehicles. [33] uses a learned graph neural
network for communication and cooperative routing. However, many of these methods are typically
studied in toy settings, whereas we evaluate our model on a realistic self-driving dataset.
2

V2V Aggregation + 
Attention for Ego
BEV Scene 
Multiple SDV
Detection & 
Prediction
Consistency 
Module
Pose Regression 
Module
Predict correct 
relative pose
SDVs with pose noise
Compute globally 
consistent pose
Aggregate feature messages 
and filter outliers
Final BEV output
Final 
warping
Feature 
maps
Ego car
Other SDV
Other SDV
Pose
Relative 
pose
Weighted 
edges
Weighted Pose MRF
Weighted 
average 
using 
attention
Initial 
warping
V2VNet GNN +
Output Head for Ego
F
F
F
G
H
Figure 2: Our proposed method for robust V2V communication under pose error. The network’s
feature maps are communicated in the style of V2VNet [1], but before the ﬁnal warping step, we
propose end-to-end learnable modules. First, the pose regression module and the consistency module
to ﬁx pose errors. Lastly, before aggregation, the attention module predicts a soft binary attention
weight used in a weighted average of messages to ﬁlter out remaining noisy messages. In contrast,
V2VNet performs a uniform average instead of a weighted average during the GNN step.
3
Learning to Communicate and Correct Pose Errors
Pose noise has been shown to severely detriment existing collaborative multi-agent self-driving
systems. In this section, we describe our novel approach to correct pose errors in such settings. In
the following, we ﬁrst review V2VNet [1], the collaborative self-driving framework that we base
our models on. We then propose a pose error correction network composed of i) a pose regression
module to predict pairwise relative poses, ii) a consistency module to reach global consensus, and iii)
an attention aggregation module to ﬁlter out outlier messages. These modules are learned end-to-end
jointly to improve object detection and motion forecasting.
3.1
Background on V2VNet
Our pose correction approach is based on V2VNet [1], a state-of-the-art collaborative multi-vehicle
self-driving network which has been shown to provide signiﬁcant improvements in both object
detection and motion forecasting over single vehicle systems. We call the combined detection and
forecasting task perception and prediction (PnP). We ﬁrst review the background of V2VNet—an
overview diagram is illustrated in Figure 1.
Input parameterization and message computation:
Given multiple LiDAR sweeps, V2VNet
voxelizes the point cloud into 15 cm3 voxels, and concatenates them along the height dimension to
form a birds-eye view input representation. It then processes this representation using a 2D CNN,
denoted F, to produce a spatial feature map of shape c × l × w (channels, length, width). To facilitate
cooperation, each self-driving vehicle (SDV) compresses and broadcasts these spatial feature maps to
nearby SDVs. We thus call these spatial feature maps messages and denote the message from vehicle
i as mi.
Message passing and aggregation:
Vehicle i collects all incoming messages and aggregates them
via a graph neural network (GNN) G [34]. The set of vehicles which communicate with vehicle i is
denoted adj(i). When vehicle i receives message mj from vehicle j ∈adj(i), it warps mj from the
perspective of vehicle j to its own. Vehicle i uses its own pose ξi and the other vehicle’s pose ξj to
compute the relative pose ξji. The message from vehicle j (mj) is transformed via ξji to produce the
warped message mji, which is aligned to the perspective of vehicle i. Let the aggregated message for
agent i be hi := G
 {mji}j∈adj(i)

We refer to [1] for details on the aggregation algorithm.
Output parameterization and header:
Finally, vehicle i uses a CNN H to process aggregated
messages to predict the ﬁnal outputs which consist of object detections represented with their 3D
position, width, height, and orientation, as well as prediction outputs representing the locations of
objects at future time steps.
Learning objective:
V2VNet is trained using the PnP loss LP nP (yi, ˆyi), which is a combination
of a cross-entropy loss on the vehicle classiﬁcations, smooth ℓ1 on the bounding boxes, and smooth
ℓ1 on the predicted motion forecasting waypoints.
3

Pose notation:
Since processing is done in birds-eye view, poses are in SE(2). We represent each
pose as a vector ξ ∈R3, consisting of two translation components and one rotation angle. We denote
composing two transformations via ξ1 ◦ξ2, which is equivalent to multiplying their corresponding
homogeneous transformation matrices. We denote ξ−1 as the inverse pose, equivalent to inverting
the corresponding transformation matrix
3.2
Robust V2V communication against pose noise
V2VNet has been shown to be vulnerable to pose noise because misaligned incoming messages
will result in unusable features for the network. Under realistic noise, V2VNet’s performance can
be worse than single vehicle PnP. In this section we introduce details of our approach to improve
robustness against pose noise. An illustration is shown in Figure 2.
In our setting, each SDV i has a noisy estimate of its own pose denoted ˜ξi, and receives the noisy
poses of neighboring self-driving vehicles as part of the messages. These noisy poses are used to
compute the noisy relative transformation from SDV j to i denoted ˜ξji.
Pose regression module:
Since all the vehicles perceive different views of the same scene, we use a
CNN to learn the discrepancy between what a vehicle sees and the orientation of the warped incoming
messages. The network for the i-th agent takes (mi ∥mji) as input and outputs a correction bcji such
that bcji ◦˜ξji = bξji. ∥denotes concatenation along the features dimension, and bcji ◦˜ξji represents
applying the transformation bcji to the noisy relative transformation ˜ξji, to produce a predicted true
relative transformation bξji. Note that since we make an independent prediction for each directed
edge, bξji ̸= bξ
−1
ij . In our setting, concatenating the features at the input was shown empirically to be
more effective than using an architecture with two input branches that are concatenated downstream
(which is done in [35, 36]).
Consistency module:
We now reﬁne the relative pose estimates from the regression module by
ﬁnding a set of globally consistent absolute poses among all our SDVs. By allowing the SDVs to
reach a global consensus about eachothers absolute pose, we can further mitigate pose error.
We formulate our consistency as a Markov random ﬁeld (MRF), where each vehicle pose is a node
and we condition on the predicted relative poses. Since the predicted relative pose error will have
many outliers, the distribution of our true absolute poses conditioned on these will have a heavy tail.
We thus assume each pose ξi follows a multivariate student t-distribution with mean ξi ∈R3 and
scale Σi ∈R3×3 conditioned on the relative poses. We do not use any unary potentials. Our pairwise
potentials consist of three components: the likelihoods, weights, and weight priors:
ψ(i, j) = p(bξji ◦ξj)wjip(bξ
−1
ji ◦ξi)wji
|
{z
}
Weighted Likelihood given bξji
p(bξij ◦ξi)wijp(bξ
−1
ij ◦ξj)wij
|
{z
}
Weighted Likelihood given bξij
p(wji)p(wij)
|
{z
}
Weight Priors
.
(1)
The likelihood terms p(bξji◦ξj) and p(bξ
−1
ij ◦ξj), both t-distributed centered at ξi, encourage the result
of the relative transformation (bξji or bξ
−1
ij ) from a source vehicle position (ξj) to stay close to the target
vehicle’s position (ξi). Both directions are included due to symmetry of the rigid transformations.
However, not all pairwise transformations provide the same amount of information, and since our
regression module tends to produce heavy tailed errors, we would like to reweight the edge potentials
to downweight erroneous pose regression outputs. Concretely, we use a weight wji ∈R for each
term in the pairwise potential: p(bξji ◦ξj)wji, so that low weighted terms will inﬂuence the estimates
less. We use a prior distribution for each wji, where the mean of the distribution is oji ∈R—
the fraction of spatial overlap between two messages. Intuitively, we would like to trust the pose
prediction more if the two messages have more spatial overlap. Following [37], we use a Gamma
prior: p(wji) = Γ(wji | oji, k), where k is the shape parameter.
To perform inference on our MRF, we would like to estimate the values of our absolute poses ξi, the
scale parameters Σi, and the weights wji that maximize the product of all our pairwise potentials. We
achieve this via Iterated Conditional Modes [38], described in Algorithm 1. The maximization step
on Line 4 happens simultaneously for all nodes via weighted expectation-maximization (EM) for the
4

Algorithm 1 Consistency module inference
1: ξi ←˜ξi
i = 1...n
2: wji ←1
(i, j) ∈E
3: for k = 1...num_iters do
4:
ξi, Σi ←argmaxξi,Σi
Q
j∈adj(i)p(bξji ◦ξj)wji p(bξ
−1
ij ◦ξj)wij
i = 1...n
5:
wji ←argmaxwji p(wji | ξi, Σi)
(i, j) ∈E
6: end for
7: return ξi
i = 1...n
t distribution [39]. We provide the EM algorithm in the Supplementary Material. The maximization
step on Line 5 can be computed using the following closed form [37]:
argmax
wji
p(wji | ξi, Σi) =
ojik
k −log p(bξji ◦ξj) −log p(bξ
−1
ji ◦ξi)
.
(2)
We then use these estimated poses to update the relative transformations needed to warp the messages.
Attention aggregation module:
After we predict and reﬁne the relative transformations, there may
still be errors present in some messages that hinder our SDVs’ performance. In V2VNet, warped
incoming messages are averaged when being processed by the GNN G. This means each message will
make an equal contribution towards the ﬁnal predictions. Instead, we want to focus on clean messages
and ignore noisy ones. Thus, we propose a simple yet effective attention mechanism to assign a weight
to each warped message before they are averaged, to suppress the remaining noisy messages. We use
a CNN A to predict an unnormalized weight sji ∈R. Speciﬁcally, sigmoid(A(mi ∥mji)) = sji.
We compute the normalized weight aji ∈R as follows:
aji =
sji
α + P
k∈adj(i) ski
.
(3)
The learned parameter α ∈R allows the model to ignore all incoming messages if needed. Without
α, if all the incoming messages are noisy, thus all the sji are small, the resulting weights would be
large after the normalization. Then, we can compute our aggregated message:
hi = G
 {ajimji}j∈adj(i)

.
(4)
The aggregated message is then used by the network H to predict bounding boxes for object detection
and waypoints at future timesteps for motion forecasting.
3.3
Learning
Supervising attention:
We ﬁrst train V2VNet and the attention network. We treat identifying
noisy examples as a supervised binary classiﬁcation task, where clean examples get a high value and
noisy examples get a low value. For the data and labels, we generate and apply strong pose noise to
some vehicles and weak pose noise to others within one scene. Concretely, we generate the noise via
ni ∼Dw or ni ∼Ds, where Dw is a distribution of weak pose noises, and Ds of strong noises. Like
the poses, the noises have two translational components and a rotational component, thus ni ∈R3. A
ﬁxed proportion p of our agents receive noise from the strong distribution while the rest from the
weak one. When considering a message, it is considered clean when both agents have noise from the
weak distribution, and considered noisy when either vehicle has noise from the strong distribution.
This labeling is summarized in the following function:
label(j, i) =
γ
nj ∼Dw and ni ∼Dw,
1 −γ
nj ∼Ds or ni ∼Ds.
(5)
This function produces smooth labels to temper the attention module’s predictions so the attention
weights are not just 0 or 1. We deﬁne the loss for our joint training task as follows:
Ljoint(yi, ˆyi, {sji}j∈adj(i)) = λP nP LP nP (yi, ˆyi) +
λattn
|adj(i)|
X
j∈adj(i)
LCE(label(j, i), sji),
(6)
where LCE is binary cross entropy loss. This additional supervision was paramount to training the
attention mechanism—training with LP nP alone produced a signiﬁcantly less effective model.
5

0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Std. (m/deg)
40
60
80
AP @ IoU=0.7
Single Vehicle PnP
V2VNet
Data Aug
Ours
Learn2Sync
0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Std. (m/deg)
0.8
0.9
1.0
1.1
1.2
L2 @ IoU=0.5 @ 3s
 
 
 
 
Figure 3: Detection and motion forecasting performance of models across various noise levels.
Single Vehicle PnP denotes V2VNet with no SDV peers. Other methods are all trained with 0.4m/4.0◦
standard deviation positional/heading noise.
Pose regression:
Then, we freeze V2VNet and the attention and train only the regression module
using Lc. In this stage, all the SDVs get noise from the strong noise distribution Ds. We train this
network using a loss which is a sum of losses over each coordinate:
Lc(ξji, bξji) =
3
X
k=1
λkLsl1(ξji, bξji)k,
(7)
with λ = [λpos, λpos, λrot], and Lsl1 the smooth ℓ1 loss. This regression formulation was empirically
more effective than discretizing the predictions and formulating the problem as classiﬁcation.
Finally, we ﬁne-tune the entire network end-to-end with the combined loss: L = Lc + Ltask, which
is possible because our MRF inference algorithm is differentiable via backpropogation.
4
Experiments
We evaluate our method on detection, prediction, and pose correction in various noise settings,
including noise not seen during training. The speciﬁc architectures and hyperparameters are provided
in the supplementary material.
4.1
Experimental setup
Dataset:
Our model is trained on the V2V-Sim dataset [1], which is generated from a high-ﬁdelity
LiDAR simulator [40]. The simulator uses real-world snippets to ﬁrst reconstruct 3D scenes with
static and dynamic objects, then simulates LiDAR point clouds from the viewpoints of multiple
self-driving vehicles. Each scene contains up to 7 SDVs. There are 46,796/4,404 frames for the
train/test split, where each frame contains 5 LiDAR sweeps. We refer readers to [1] for more details.
Evaluation metrics:
Following [1], detection is measured using Average Precision (AP) at an
Intersection over Union (IoU) of 0.7, motion forecasting (prediction) performance is measured using
ℓ2 displacement error of the object’s center location at a future time step (e.g., 3s in the future)
on true positives. A true positive is a detection where the IoU threshold is 0.5 and the conﬁdence
threshold is set such that the recall is 0.9 (we pick the highest recall if 0.9 cannot be reached). Pose
correction performance is evaluated using mean absolute error (MAE) and root mean squared error
(RMSE). All reported metrics are for vehicles in coordinate view range of x ∈[−100m, 100m],
y ∈[−40m, 40m] around the SDV, which includes objects that are completely occluded (0 LiDAR
points hit), making the task more difﬁcult and realistic. The communicating vehicles themselves are
excluded in evaluation (as PnP of these would be trivial for the co-operative network).
Noise simulation:
Throughout training and evaluation, the noise is sampled and applied indepen-
dently to the pose of each SDV. This can be applied as a post-processing step on the data, or can
be simulated directly within LiDARSim [40]. During training, the positional noise is drawn from
a Gaussian with µ = 0, σ = 0.4 for Ds and σ = 0.01 for Dw; the rotational noise is drawn from
a von Mises distribution with µ = 0, σ = 4◦for Ds and σ = 0.1◦for Dw. During evaluation, the
parameters of these distributions are varied as described for each experiment. We show experiments
on both noise similar or greater than the noise levels seen during training. Self-driving cars utilize
geometric registration algorithms that localize the vehicle online with respect to a 3D HD map. These
6

Position Error (m)
Rotation Error (deg)
0.4 m, 4◦std.
0.8 m, 8◦std.
0.4 m, 4◦std.
0.8 m, 8◦std.
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
No Correction
2.556
1.554
5.723
4.571
5.079
3.115
11.483
9.157
Learn2Sync
0.394
0.191
0.516
0.281
1.664
0.766
2.750
1.420
Pairwise
0.587
0.211
0.707
0.307
2.083
0.743
3.112
1.209
Gaussian No Reweighting
0.391
0.185
0.492
0.265
1.602
0.726
2.623
1.303
Gaussian w/Reweighting
0.283
0.153
0.377
0.218
1.386
0.634
2.379
1.153
Ours
→Regression Only
0.644
0.245
0.825
0.377
2.186
0.803
3.275
1.326
→No Reweighting
0.249
0.128
0.340
0.187
1.160
0.465
1.853
0.819
→Ours
0.197
0.119
0.284
0.172
0.983
0.416
1.623
0.721
Table 1: Error of the predicted corrections ˆcji (as deﬁned in 3.2). No correction corresponds to
predicting ˆcji = 0, and is listed to provide context for the metrics. Pairwise refers to averaging
the relative poses of reverse edges (i.e (i, j) and (j, i)). Gaussian refers to our consistency formu-
lation with multivariate normal nodes instead of t-distributed nodes. No Reweighting refers to our
consistency formulation without the robust Bayesian reweighting.
Modules
AP @ IoU = 0.7 ↑
ℓ2 @ IoU=0.5 @ 3s ↓
Regression
Consistency
Attention
0.0 / 0
0.4 / 4
0.8 / 8
0.0 / 0
0.4 / 4
0.8 / 8
90.070
34.960
37.065
0.774
1.154
1.223

87.777
77.227
60.312
0.793
0.830
0.901


88.906
82.241
60.978
0.787
0.813
0.884

90.375
67.726
67.591
0.768
0.957
0.973


89.094
84.023
75.976
0.784
0.812
0.853



89.931
86.357
76.331
0.776
0.797
0.844
Table 2: Ablation of each component of our correction system. 0.4 / 4 indicates 0.4 m and 4◦
standard deviation of noise for position and rotation, respectively. The model with none of the
modules is V2VNet. Each component provides improvement, with the combination of the three
producing the best model at high and very high noise.
methods are very precise, with 99% of the errors being much smaller than 0.2m, which informed the
evaluation ranges chosen.
Competitive method:
We compare our method to a competitive transformation synchronization
method Learn2Sync [10], which considers the pairs of depth maps to iteratively reweight pairwise
registrations when ﬁnding globally consistent poses. To process pairs of messages instead of depth
maps, a larger version of the Learn2Sync architecture is used (see supplementary material). During
evaluation, Learn2Sync is used in place of our consistency module. Our pretrained pose correction
module produces the initial pairwise registrations for Learn2Sync.
Data augmentation baseline:
For a simple baseline to our method, V2VNet is trained with noisy
poses as a form of input data augmentation, which asks the network to implicitly handle pose noise
instead of explicitly correcting the noise. We refer to this network as Data Aug.
4.2
Experimental results
PnP performance:
As shown in Figure 3, V2VNet is quite vulnerable to pose noise, especially
heading noise. When trained with data augmentation, the model becomes signiﬁcantly more robust,
however, this is at the cost of worse performance in less noisy conditions. The original model
trusts incoming messages too much, whereas the data augmented model trusts them too little and
discards too much information. Using the correction provides signiﬁcant beneﬁts: there is little
drop in performance when faced with the noise seen in the training set (0.4 m, 4.0◦std.). The
model generalizes well to noise stronger than seen in the training set. Our consistency method
shows considerable improvement over Learn2Sync, which is expected in this case as synchronization
algorithms are commonly designed and evaluated on far larger graphs. Having so few transformations
to synchronize renders these methods vulnerable to outliers.
Pose correction performance:
Table 1 shows that our consistency module further enhances the
correction performance. Note that while the RMSE decreases signiﬁcantly with other methods, the
MAE only decreases marginally. This implies that, while the outliers are corrected, the average
7

0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Mean (m/deg)
40
60
80
AP @ IoU=0.7
Single Vehicle PnP
V2VNet
Data Aug
Ours
Learn2Sync
0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Mean (m/deg)
0.8
1.0
1.2
L2 @ IoU=0.5 @ 3s
 
 
 
 
 @ 
g 
  
  
Figure 4: Evaluation of the models against biased Gaussian noise, where the bias is varied on the
x-axis and the standard deviation is ﬁxed (0.1 m and 1.0◦). The performance stays well above single
vehicle PnP, despite the noise being stronger and of an unseen type during training.
1
2
3
4
5
6
7
Number of SDVs
40
60
80
AP @ IoU=0.7
No noise upper bound
V2VNet
Data Aug
Ours
Learn2Sync
1
2
3
4
5
6
7
Number of SDVs
0.8
1.0
1.2
L2 @ IoU=0.5 @ 3s
 
 
 
 
 
Figure 5: Performance for different numbers of SDV peers. The no noise upper bound is V2VNet
evaluated with no noise. The positional/ rotational noise has standard deviation 0.2 m / 2◦.
correction is not improved signiﬁcantly. Also, this means outliers “poison” the good predictions,
resulting in relative pose estimates that are mediocre. Improving the average case is more important
than dealing with outliers as our model with attention can ignore outliers and focus on well-aligned
messages.
Ablation studies:
Table 2 shows that all the components provide signiﬁcant beneﬁts. Interestingly,
using the attention module provides improvement over V2VNet even when no noise is present.
Biased noise:
There will always be a domain gap between the noise seen during training and the
noise an agent may experience in the real world. In our setting, the pose regression is trained on
unbiased Gaussian noise, however, in the real world, a vehicle may experience systematic, biased
error. Figure 4 evaluates the generalization ability of our method on noise that is biased and stronger
than what the model may face in reality. The performance of the model stays well above single vehicle
PnP. Furthermore, outliers become more prevalent in this setting, which affects the performance of
consistency methods not designed to deal with outliers in small graphs.
Number of SDVs:
Strong performance independent of the number of nearby SDVs is important for
safe operation of an SDV. Figure 5 shows that V2VNet’s performance drops as soon as we introduce
another SDV due to the pose noise affecting messages, even after Data Augmentation. This is not the
case with our correction: increasing the number of SDVs improves performance, almost matching the
original model evaluated with no noise. The consistency also maintains reliable performance even
with few nearby SDVs.
5
Conclusion
Collaborative self-driving cars will bring the safety of self-driving to the next level. In this paper, we
propose a collaborative self-driving framework that is made robust to pose errors in vehicle-to-vehicle
communication. Unlike traditional pose synchronization methods, our model is end-to-end learned to
improve detection and motion forecasting. We demonstrate the effectiveness of our method under
various levels of pose noise using V2V simulation. In the future, we can extend our work to exploit the
temporal consistency of the pose error in incoming messages to improve performance and efﬁciently
reuse computation. We also aim to expand our neural reasoning framework to correct more general
types of communication noises to make collaborative self-driving more robust.
8

Acknowledgments
We would like to thank Andrei Bˆarsan and Pranav Subramani for insightful discussions. We would
also like to thank all the reviewers for their helpful comments.
References
[1] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and R. Urtasun. V2VNet: Vehicle-
to-vehicle communication for joint perception and prediction. In ECCV, 2020.
[2] M. Liang, B. Yang, S. Wang, and R. Urtasun. Deep continuous fusion for multi-sensor 3d object
detection. In ECCV, 2018.
[3] Q. Chen, S. Tang, Q. Yang, and S. Fu. Cooper: Cooperative perception for connected au-
tonomous vehicles based on 3d point clouds. In ICDCS, 2019.
[4] M. Obst, L. Hobert, and P. Reisdorf. Multi-sensor data fusion for checking plausibility of V2V
communications by vision-based multiple-object tracking. In VNC, 2014.
[5] Z. Y. Rawashdeh and Z. Wang. Collaborative automated driving: A machine learning-based
method to enhance the accuracy of shared information. In ITSC, 2018.
[6] A. Rauch, F. Klanner, R. Rasshofer, and K. Dietmayer. Car2x-based perception in a high-level
fusion architecture for cooperative perception systems. In IV, 2012.
[7] M. Rockl, T. Strang, and M. Kranz. V2v communications in automotive multi-sensor multi-
target tracking. In 2008 IEEE 68th Vehicular Technology Conference, pages 1–5, 2008.
[8] F. Bernard, J. Thunberg, P. Gemmar, F. Hertel, A. Husch, and J. Goncalves. A solution for
multi-alignment by transformation synchronisation. In CVPR, 2015.
[9] F. Arrigoni, B. Rossi, and A. Fusiello. Spectral synchronization of multiple views in SE(3).
SIAM Journal on Imaging Sciences, 9(4):1963–1990, 2016. Publisher: Society for Industrial
and Applied Mathematics.
[10] X. Huang, Z. Liang, X. Zhou, Y. Xie, L. J. Guibas, and Q. Huang. Learning transformation
synchronization. In CVPR, 2019.
[11] Z. Gojcic, C. Zhou, J. D. Wegner, L. J. Guibas, and T. Birdal. Learning multiview 3d point
cloud registration. In CVPR, 2020.
[12] D. M. Rosen, L. Carlone, A. S. Bandeira, and J. J. Leonard. A certiﬁably correct algorithm for
synchronization over the special euclidean group. In Workshop on the Algorithmic Foundations
of Robotics. 2020.
[13] A. Singer. Angular synchronization by eigenvectors and semideﬁnite programming. 2009.
[14] T. Birdal, U. Simsekli, M. O. Eken, and S. Ilic. Bayesian pose graph optimization via bingham
distributions and tempered geodesic MCMC. In NeurIPS. 2018.
[15] F. Arrigoni, B. Rossi, P. Fragneto, and A. Fusiello. Robust synchronization in SO(3) and SE(3)
via low-rank and sparse matrix decomposition. CVIU, 174:95–113, 2018.
[16] P. Purkait, T.-J. Chin, and I. Reid. NeuRoRA: Neural robust rotation averaging. arXiv preprint
1912.04485, 2019.
[17] S. Choi, Q.-Y. Zhou, S. Miller, and V. Koltun. A large dataset of object scans, 2016.
[18] K. Yousif, A. Bab-Hadiashar, and R. Hoseinnezhad. An overview to visual odometry and visual
slam: Applications to mobile robotics. Intelligent Industrial Systems, 1(4):289–311, 2015.
[19] A. Talukder, S. Goldberg, L. Matthies, and A. Ansar. Real-time detection of moving objects in
a dynamic scene from moving robotic vehicles. In IROS, 2003.
[20] C. Dornhege and A. Kleiner. Visual odometry for tracked vehicles. 01 2006.
9

[21] L. Matthies and S. A. Shafer. Error Modeling in Stereo Navigation. In Autonomous Robot
Vehicles, pages 135–144. Springer, 1990.
[22] M. Kaess, K. Ni, and F. Dellaert. Flow separation for fast and robust stereo odometry. In ICRA,
2009.
[23] V. Mohanty, S. Agrawal, S. Datta, A. Ghosh, V. D. Sharma, and D. Chakravarty. DeepVO: A
deep learning approach for monocular visual odometry. arXiv preprint 1611.06069, 2016.
[24] S. Wang, R. Clark, H. Wen, and N. Trigoni. Deepvo: Towards end-to-end visual odometry with
deep recurrent convolutional neural networks. In ICRA, 2017.
[25] H. Yang and L. Carlone. A polynomial-time solution for robust registration with extreme outlier
rates. RSS, 2019.
[26] A. Fitzgibbon. Robust registration of 2d and 3d point sets. Image and Vision Computing, 21:
1145–1153, 04 2002.
[27] W. Lu, G. Wan, Y. Zhou, X. Fu, P. Yuan, and S. Song. DeepICP: An end-to-end deep neural
network for 3d point cloud registration. In ICCV, 2019.
[28] Z. J. Yew and G. H. Lee. 3dfeat-net: Weakly supervised local 3d features for point cloud
registration. In ECCV, 2018.
[29] F. Pomerleau, F. Colas, and R. Siegwart. A Review of Point Cloud Registration Algorithms for
Mobile Robotics. 2015.
[30] S. Omidshaﬁei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task
multi-agent reinforcement learning under partial observability. In ICML, 2017.
[31] N. Balachandar, J. Dieter, and G. S. Ramachandran. Collaboration of AI agents via cooperative
multi-agent deep reinforcement learning. arXiv preprint 1907.00327, 2019.
[32] S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropa-
gation. In NIPS. 2016.
[33] Q. Sykora, M. Ren, and R. Urtasun. Multi-agent routing value iteration network. In ICML,
2020.
[34] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In
ICLR, 2017.
[35] W. Luo, A. G. Schwing, and R. Urtasun. Efﬁcient deep learning for stereo matching. In CVPR,
2016.
[36] P. Agrawal, J. Carreira, and J. Malik. Learning to see by moving. In ICCV, 2015.
[37] Y. Wang, A. Kucukelbir, and D. M. Blei. Robust probabilistic modeling with bayesian data
reweighting. In ICML, 2017.
[38] J. Besag. On the Statistical Analysis of Dirty Pictures. Journal of the Royal Statistical Society.
Series B (Methodological), 48(3):259–302, 1986. Publisher: [Royal Statistical Society, Wiley].
[39] C. Liu and D. B. Rubin. ML estimation of the t distribution using EM and its extensions, ECM
and ECME. 1999.
[40] S. Manivasagam, S. Wang, K. Wong, W. Zeng, M. Sazanovich, S. Tan, B. Yang, W.-C. Ma, and
R. Urtasun. Lidarsim: Realistic lidar simulation by leveraging the real world. In CVPR, 2020.
[41] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[42] L. N. Smith and N. Topin. Super-convergence: Very fast training of neural networks using large
learning rates. arXiv preprint 1708.07120, 2018.
10

[43] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,
S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style,
high-performance deep learning library. In NeurIPS. 2019.
[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In NIPS, 2012.
11

A
EM for weighted t-distribution
Recall in Algorithm 1 on line 4 from the main manuscript we maximize the following quantity for
each i:
argmax
ξi,Σi
Y
j∈adj(i)p(bξji ◦ξj)wji p(bξ
−1
ij ◦ξj)wij.
(8)
This is equivalent to ﬁnding the weighted maximum likelihood estimate (MLE) of ξi, Σi given
observations {bξji ◦ξj}j∈adj(i) ∪{bξ
−1
ij ◦ξj}j∈adj(i). Recall that ξi, Σi are the location and scale of
the t distribution with ν degrees of freedom. We modify the EM algorithm given in [39] to compute
the weighted MLE.
The student t distribution can be deﬁned as follows:
p(bξji ◦ξj | ξi, Σi, ν) =
Z ∞
0
N

bξji ◦ξj | ξi, (1/ηji)Σi

Gamma (ηji | 1, (2/ν)) dηji,
(9)
where 1 is the mean of the Gamma, 2/ν is the shape parameter k, and N denotes the multivariate
normal distribution. We provide the full expressions for the t and Gamma distributions in section E.
For the expectation step, we compute the expectation of our latent parameter ηji. For the maximization
step, we compute ξi, Σi given ηji. We use δji to denote the difference between observation ji and
the current estimate of ξi for convenience. The full algorithm is described in Algorithm 2.
Algorithm 2 Weighted MLE of ξi, Σi .
1: ξi ←COORDINATEWISEMEDIAN({bξji ◦ξj}j∈adj(i) ∪{bξ
−1
ij ◦ξj}j∈adj(i))
2: Σi ←I3×3
3: for all j ∈adj(i) do
4:
δji ←ξi −

bξji ◦ξj

5:
δij ←ξi −

bξ
−1
ij ◦ξj

6: end for
7: for 1...num_iters do
8:
▷Expectation Step
9:
for all j ∈adj(i) do
10:
ηji ←
ν+3
ν+δ⊤
jiΣ−1
i
δji
11:
ηij ←
ν+3
v+δ⊤
ijΣ−1
i
δij
12:
end for
13:
▷Maximization Step
14:
ξi ←
P
j∈adj(i) ηjiwji(bξji◦ξj)+ηijwij

bξ
−1
ij ◦ξj

P
j∈adj(i) ηjiwji+ηijwij
15:
for all j ∈adj(i) do
16:
δji ←ξi −

bξji ◦ξj

17:
δij ←ξi −

bξ
−1
ij ◦ξj

18:
end for
19:
Σi ←
1
2|adj(i)|
P
j∈adj(i) ηjiδjiδ⊤
ji + ηijδijδ⊤
ij
20: end for
21: return ξi, Σi
When there are only two vehicles communicating, we a simple average instead of EM to estimate
ξi. Notice on line 19 we do not use the weights wji, as the small size of our graph often leads to a
singular Σi when using these weights. 15 iterations is sufﬁcient for convergence and 2 degrees of
freedom worked well.
12

0.0
0.2
0.4
0.6
0.8
40
60
80
AP @ IoU=0.7
0
2
4
6
8
40
60
80
0.0
0.2
0.4
0.6
0.8
Positional Error Std. (m)
0.8
0.9
1.0
1.1
1.2
L2 @ IoU=0.5 @ 3s
0
2
4
6
8
Heading Error Std. (deg)
0.8
0.9
1.0
1.1
1.2
Single Vehicle PnP
V2VNet
Data Aug
Ours
Learn2Sync
Figure 6: Evaluation of the models against seperated heading and positional noise.
B
Additional Experiments
We analyze the effects of positional and heading noise seperately in Figure 6. Heading noise is far
more detrimental than positional noise, as objects far from the vehicle can be displaced signiﬁcantly
even with slight heading error.
C
Qualitative Examples
Figure 7 shows PnP outputs from ﬁve scenes in the validation set when the agents are subject to pose
noise. As shown, the misaligned messages causes many detections to be innacurate, particularly
detections farther away from the ego vehicle. We also see that forecasting predictions are skewed
without the correction module.
D
Implementation Details
In this section, we provide the implementation details for the training procedure and architectures
used.
D.1
Training Hyperparameters
V2VNet and the attention network are trained using the Adam optimizer [41] with a one-cycle
learning rate [42] for 6 epochs starting from the pre-trained LiDAR backbone with a peak one-cycle
learning rate of 0.0004. Then, V2VNet and the attention network are frozen and only the regression
module is trained for 12 epochs with a peak one-cycle learning rate of 0.002. For the loss, we use
λpos = 2/3 and λrot = 1/3. Finally, the entire network is ﬁne tuned with the combined loss L for 3
epochs with a peak learning rate of 0.0001. For the consistency module, using a t-distribution with 2
degrees of freedom, k = 120 for the prior worked well, 15 iterations of EM for the t-distribution, 15
steps of ICM, and 10 reweighting steps worked well. The attention module is trained with γ = 0.9,
p = 0.5, λP nP = 0.9, and λattn = 0.1 without signiﬁcant tuning. We make slight modiﬁcations to
V2VNet detailed in the supplementary materials. These modiﬁcations resulted in virtually no change
in PnP performance.
D.2
Changes to V2VNet
Due to GPU memory limitations, we use a slightly altered V2VNet with near identical performance
to the architecture from [1]. V2VNet originally performed 3 rounds of message passing between
13

With Correction System
No Correction System
Legend:
SDV
True Positive
False Negative
False Positive
True Trajectory
Predicted Trajectory
Figure 7: Examples of Perception and Prediction Outputs. All the agents were subject to random
pose noise with 0.4 m and 4◦standard deviation.
vehicles per inference; we reduce this to 2. Our correction system only operates during the ﬁrst round
of propagation. The second round uses the corrected localization and attention weights from the
ﬁrst round. When receiving messages, V2VNet uses a convolutional neural network to process each
incoming message before aggregating and passing to the ConvGRU in the GNN. We remove this
processing step and aggregate the messages directly before passing them to the ConvGRU. Finally,
V2VNet uniformly samples between 1 and 7 SDVs per training example. We sample exactly 4 SDVs
per training example when training V2VNet and the attention, for more consistent GPU memory
utilization. We sample up to 7 SDVs per scene when training only the regression module (as some
training examples have fewer than 7 vehicles).
14

D.3
Architecture for our Method
The dimensions of a message are (c, l, w) = (80, 128, 320). Therefore, the dimensions of the input
to the regression and attention modules are (160, 128, 320). Architectures are described in terms of
PyTorch [43] modules. All convolutional layers have a padding and stride of (1, 1) unless otherwise
speciﬁed. We annotate each layer with the output activation shape.
We describe our attention architecture below.
Sequential(
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 128, 320),
LeakyReLU(negative_slope=0.01) -> (160, 128, 320),
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 64, 160),
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 64, 160),
LeakyReLU(negative_slope=0.01) -> (160, 64, 160),
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 32, 80),
AdaptiveMaxPool2d(output_size=1) -> (160, 1, 1),
Flatten() -> (160,)
Linear(in_features=160, out_features=1, bias=True) -> (1,)
)
The use of AdaptiveMaxPool2d is important: it allows our computed attention weights to be
invariant to the amount of spatial overlap between two messages.
We describe the architecture of our regression module below.
Sequential(
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 128, 320)
LeakyReLU(negative_slope=0.01) -> (160, 128, 320)
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 64, 160)
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 64, 160)
LeakyReLU(negative_slope=0.01) -> (160, 64, 160)
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 32, 80)
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 32, 80)
LeakyReLU(negative_slope=0.01) -> (160, 32, 80)
MaxPool2d(kernel_size=2, stride=2) -> (160, 16, 40)
Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2)) -> (160, 8, 20)
LeakyReLU(negative_slope=0.01) -> (160, 8, 20)
MaxPool2d(kernel_size=2, stride=2) -> (160, 4, 10)
Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2)) -> (160, 2, 5)
LeakyReLU(negative_slope=0.01) -> (160, 2, 5)
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 1, 2)
AdaptiveMaxPool2d(output_size=1) -> (160, 1, 1)
Flatten() -> (160,)
Linear(in_features=160, out_features=160, bias=True) -> (160,)
LeakyReLU(negative_slope=0.01) -> (160,)
Linear(in_features=160, out_features=160, bias=True) -> (160,)
LeakyReLU(negative_slope=0.01) -> (160,)
Linear(in_features=160, out_features=3, bias=True) -> (3,)
)
D.4
Architecture for Learn2Sync
We train Learn2Sync for 10 epochs using the Adam optimizer and a one-cycle learning with
a maximum learning rate of 0.01.
We searched for the optimal learning rate from the set
{0.1, 0.01, 0.001, 0.0001}. Learn2Sync originally used a modiﬁed AlexNet architecture [44]. We
simply increased the size as detailed below. The rest of the hyperparameters were kept from [10].
Sequential(
Conv2d(160, 160, kernel_size=(7, 7), stride=(4, 4)) -> (160, 31, 79)
ReLU() -> (160, 31, 79)
15

LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2) -> (160, 31, 79)
MaxPool2d(kernel_size=3, stride=2, padding=0) -> (160, 15, 39)
Conv2d(160, 256, kernel_size=(5, 5), padding=(2, 2)) -> (256, 15, 39)
ReLU() -> (256, 15, 39)
LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2) -> (256, 15, 39)
MaxPool2d(kernel_size=3, stride=2, padding=0) -> (256, 7, 19)
Conv2d(256, 256, kernel_size=(3, 3)) -> (256, 7, 19)
ReLU() -> (256, 7, 19)
Conv2d(256, 256, kernel_size=(3, 3)) -> (256, 7, 19)
ReLU() -> (256, 7, 19)
AdaptiveMaxPool2d(output_size=(2, 2)) -> (256, 2, 2)
Flatten() -> (1024,)
Dropout(p=0.5, inplace=False) -> (1024,)
Linear(in_features=1024, out_features=1024, bias=True) -> (1024,)
ReLU() -> (1024,)
Dropout(p=0.5, inplace=False) -> (1024,)
Linear(in_features=1024, out_features=1024, bias=True) -> (1024,)
ReLU() -> (1024,)
Linear(in_features=1024, out_features=1, bias=True) -> (1,)
)
E
Distributions
We deﬁne the t-distribution with location ξi ∈R3, scale Σi ∈R3×3, and degrees of freedom ν ∈R
below. Note that ξi is the mean when ν > 1, and Σi is proportional to the covariance when ν > 2.
p(x | ξi, Σi, ν) = Γ
  ν+3
2

Γ
  ν
2

 Σ−1
i

πν
! 1
2 
1 + (x −ξi)T Σ−1
i (x −ξi)
ν
−ν+3
2
.
(10)
The Gamma distribution with mean µ ∈R and shape k ∈R is deﬁned below:
Gamma(x | µ, k) =
1
Γ(k)
  µ
k
k xk−1e−kx
µ .
(11)
16

