Five Lectures on Sparse and
Redundant Representations
Modelling of Images
Michael Elad


IAS/Park City Mathematics Series
Volume 19, 2010
Five Lectures on Sparse and Redundant
Representations Modelling of Images
Michael Elad
Preface
The Ô¨Åeld of sparse and redundant representations has evolved tremendously over
the last decade or so. In this short series of lectures, we intend to cover this progress
through its various aspects: theoretical claims, numerical problems (and solutions),
and lastly, applications, focusing on tasks in image processing.
Our story begins with a search for a proper model, and of ways to use it. We
are surrounded by huge masses of data, coming from various sources, such as voice
signals, still images, radar images, CT images, traÔ¨Éc data and heart signals just
to name a few. Despite their diversity, these sources of data all have something in
common: each and everyone of these signals has some kind of internal structure,
which we wish to exploit for processing it better.
Consider the simplest problem of removing noise from data. Given a set of
noisy samples (e.g., value as a function of time), we wish to recover the true (noise-
free) samples with as high accuracy as possible. Suppose that we know the exact
statistical characteristics of the noise. Could we perform denoising of the given
data? The answer is negative: Without knowing anything about the properties
of the signal in mind, this is an impossible task. However, if we could assume,
for example, that the signal is piecewise-linear, the noise removal task becomes
feasible. The same can be said for almost any data processing problem, such as de-
blurring, super-resolution, inpainting, compression, anomaly detection, detection,
recognition, and more - relying on a good model is in the base of any useful solution.
When approaching a problem in image processing, there is a wide range of
models to choose from, such as Principal Component Analysis (PCA), Anisotropic
diÔ¨Äusions, Markov Random Fields, Total-Variation, Besov spaces and many more.
There are two important (and sometime somewhat contradictory) requirements for
a model to be useful. On one hand, it should be reliable, in the sense that it should
be able to represent the data well. On the other hand, it has to be simple enough
to be applied in practice to the various tasks at hand.
A simple example of a model used in a common task can be found in the JPEG
algorithm. At its core, this algorithm relies on the claim that when transforming
patches of 8√ó8 pixels in a typical image using the Discrete Cosine Transform (DCT),
only the top-left (low frequency) components are dominant, while the rest are close
The Computer Science Department, the Technion, Haifa 32000, Israel
E-mail address: elad@cs.technion.ac.il
The author wishes to thank Dr. Matan Protter for the extensive help in writing these lecture-notes.
c
‚Éù2013 American Mathematical Society
161

162
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
to zero. This model is used for compression by discarding the non-dominant com-
ponents, thus using much fewer numbers for representing the image. While simple
and relatively eÔ¨Äective, one immediately sees potential failures of this model, and
ways to further improve it.
We therefore turn our attention to the model of ‚ÄùSparse and Redundant Rep-
resentations‚Äù and example-based methods, which will be the focus of this booklet
(and corresponding course). This model has many ‚Äúfathers‚Äù, drawing from various
areas of research, such as signal processing (Wavelet theory, multi-scale analysis,
signal transforms), machine learning, Mathematics (approximation theory, linear
algebra, optimization theory) and more.
We will see that it can be applied to
various applications, such as blind source separation, compression, denoising, in-
painting, demosaicing, super-resolution and more.
What is this model all about? Let us focus on the task of representing signals,
which are 8√ó8 patches of images, as done above. We shall assume that a dictionary
of such image patches is given to us, containing 256 atom-images, each of which is
also 8 √ó 8. The Sparseland model assumption is that every image patch from the
family of interest could be constructed as a linear combination of few atoms from
the dictionary.
To represent a signal of length 64, this model uses 256 numbers, and therefore
it is redundant. However, of these 256 all but few are zero, and therefore the model
is sparse. EÔ¨Äectively, if there are 3 atoms involved in the found combination, only 6
numbers are needed to accurately represent the signal using this model ‚Äì the three
locations of the non-zeros, and their values.
While this model sounds near-trivial, it raises several very diÔ¨Écult questions.
Given an image patch, can we Ô¨Ånd what atoms it was built from? Given a set
of patches, how should we get the dictionary for them? Is this model applicable
to various signals? The Ô¨Årst question is problematic, as crunching the numbers
indicates that a search through every possible combination is unlikely to Ô¨Ånish any
time soon, even for low-dimensional problems.
However, several approximation
algorithms have been proposed, with surprising guarantees on their performance.
The Ô¨Årst two lectures will focus on such algorithms and their theoretical anal-
ysis. The third lecture will discuss how this model and theoretical foundations are
brought to the realm of image processing. The second question can be addressed by
learning the dictionary from a large collection of signals. Such algorithms have been
developed in the past several years, and will be discussed in the fourth lecture. The
last question is yet to be answered from a theoretical point of view, but empirically,
this model is extremely eÔ¨Äective in representing diÔ¨Äerent sources of signals and in
various problems, leading to state of the art results, which will be demonstrated in
the Ô¨Åfth and Ô¨Ånal lecture.
Having answered the above three important questions positively, we are now
ready to dive into the wonderful world of sparse and redundant representation
modelling of images, or Sparseland. For extensive coverage of this area, the reader
is encouraged to turn to my book:
M. Elad, Sparse and Redundant Representations:
From Theory to Applications in Signal and Image
Processing, Springer, New-York, 2010, ISBN: 978-
1-4419-7010-7
(http://www.springer.com/978-1-
4419-7010-7).

MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
163
I would like to take this opportunity and thank Springer for their permission to pub-
lish these lecture-notes with PCMI. It should be noted that the copyright of some of
the material here (e.g. the Ô¨Ågures in Chapters 4 and 5) belongs to Springer. Finally,
the reader will Ô¨Ånd it useful to read these lecture-notes while looking at the slides:
http://www.cs.technion.ac.il/Àúelad/talks/2010/Summer-School-2010-PCMI.rar.


LECTURE 1
Introduction to sparse approximations -
algorithms
1. Motivation and the sparse-coding problem
We start our journey with a pure linear algebra point of view, and speciÔ¨Åcally by
considering a linear under-determined set of equations,
(1.1)
Dn√ókŒ± = x , k > n.
In order to simplify the following developments, we will assume that the columns
of D are (‚Ñì2) normalized.
Now, as for terminology used: We shall refer hereafter to x as a signal to be
processed, and Œ± will stand for its representation. The matrix D will be referred to
as the dictionary, and its columns will be called atoms.
Assuming D is full rank, there is an inÔ¨Ånite number of solutions to this equation.
In engineering, such a situation is unacceptable, since we aim to get a single solution
to our problems.
In order to be able to choose the ‚Äúbest‚Äù single solution, we
require some quality measure J (Œ±) that basically ranks the solutions based on
some measure, this way choosing the solution that solves the problem
(1.2)
ÀÜŒ± = arg min
Œ± J(Œ±) s.t. DŒ± = x.
Of course, the natural question is how to select J properly. In engineering, the most
common answer is some quadratic term J(Œ±) = ‚à•BŒ±‚à•2
2, as it is easy to minimize,
having a closed form solution. When chosen as a non-quadratic term, J is preferred
to be convex in order to ensure the existence of a unique global minimum to the
above problem. Of course, in many cases such choices are not aligned with our
needs, as we shall see next.
In these lectures we are interested in cases when Œ± is sparse. In order to gain
a mathematical intuition, we consider the general ‚Ñìp-norm, J(Œ±) = ‚à•Œ±‚à•p
p. This
expression sums the absolute entries of the vector Œ± after being raised to a power
p. Note that when p drops below 1 it is no longer a norm, but we shall disregard
this technicality.
When p goes below 1 and approaches zero, the function |Œ±i|p resembles the
indicator function, being 1 for every Œ±i Ã∏= 0, and 0 for Œ±i = 0. EÔ¨Äectively, we deÔ¨Åne
the ‚Ñì0-norm, as a function counting the number of non-zeros in Œ± and denote it by
‚à•Œ±‚à•0
0. While ‚Ñì0 is deÔ¨Ånitely not a norm, we will abuse this notation for simplicity.
We also deÔ¨Åne the term Support as the locations of non-zeros in Œ±. We therefore
write the minimization problem (P0):
(1.3)
(P0) ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. DŒ± = x.
165

166
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
In many cases our system might contain noise, and we cannot expect a perfect
solution to the linear system DŒ± = x. Instead, we require some proximity between
DÀÜŒ± and x, arriving at the following alternative problem:
(1.4)
(P «´
0) ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ||DŒ± ‚àíx||2 ‚â§«´,
where «´ is closely related to the properties of the noise. We will be seeing this
minimization problem many times during the course.
In order to understand the complexity of the above deÔ¨Åned problem, let us
propose a method to perform this minimization in a direct way. Set L = 1, and
consider all supports {Si}I
i=1 of cardinality L (there are I =
 k
L

such supports).
For each support, we attempt to represent x using only columns from D in the
support. This can be done by solving a simple Least-Squares (LS) problem,
min
Œ± ||DŒ± ‚àíx||2 s.t. Sup(Œ±) = Sj.
If the result LS error is below the threshold «´, than we have found a solution and
can terminate. If none of the supports yields a solution with an error below the
threshold, L should be incremented and the process should be repeated.
Taking a nominal case where k = 2000, and a known support size L = 15 (so
searching the value of L is not needed), and requiring a mere 1 nano-second for each
LS operation, solving just one problem will take about 7.5 ¬∑ 1020 years. This is due
to the exponential nature of the problem. Indeed, the problem posed in Equation
(1.4) is known to be NP-Hard [41]. This limitation leads us to seek approximation
algorithms for these two problems, (P0) and (P «´
0).
There are two main approaches to approximate the solution to the problem
posed in Equation (1.4), which we will explore in this lecture. The Ô¨Årst approach is
the greedy family of methods, where we attempt to build the solution one non-zero
at a time. The second approach is the relaxation methods, which attempt to solve
the problem by smoothing the ‚Ñì0-norm and using continuous optimization tech-
niques. All these techniques that aim to approximate the solution of the problem
posed in Equation (1.4) are commonly referred to as pursuit algorithms.
While this lecture will be devoted to the various approximation algorithms, the
next lecture will discuss in depth the quality of the estimations they provide. The
second lecture will also discuss conditions that should be met in order to ensure
that (P0) has a unique solution, and (P «´
0) has a stable solution (as we shall see, we
will not be able to require uniqueness due to the noise).
2. Greedy algorithms
Our Ô¨Årst journey into approximation algorithms will deal with the family of greedy
algorithms.
These algorithms attempt to build the support of Œ± one non-zero
element at a time. There is a variety of greedy techniques ‚Äì see [22, 3, 9, 38, 40, 42]
for more details.
The most basic of algorithm is the Matching Pursuit (MP). This is an iterative
algorithm that starts by Ô¨Ånding the one atom that best describes the input signal,
i.e., Ô¨Ånding the index ÀÜi = arg mini minc ||x ‚àíc ¬∑ di||2
2. Once found, we compute the
signal residual as x ‚àíc ¬∑ di with the optimally found constant c and the atom di.
Then, at each iteration, the one atom that best describes the residual is added
to the support, with the appropriate coeÔ¨Écient contributing to reducing the residual
the most. This process is repeated until the norm of the threshold drops below the

LECTURE 1. INTRODUCTION TO SPARSE APPROXIMATIONS - ALGORITHMS
167
given threshold. Note that the same atom may be added to the support several
times ‚Äì in such cases, its corresponding coeÔ¨Écient is aggregated.
The Orthogonal Matching Pursuit (OMP) is an improvement of the MP. The
OMP re-computes the set of coeÔ¨Écients for the entire support each time an atom is
added to the support. This stage speeds up the convergence rate, as well as insures
that once an atom has been added to the support, it will not be selected again.
This is a result of the LS stage, making the residual orthogonal to all atoms in the
chosen support.
A further improvement to the OMP observes that the atom selection stage does
not completely correspond to the objective of reducing the residual (because of the
LS stage). Therefore, this version suggests to select the atom that will best reduce
the residual after the LS stage. This method is accordingly known as LS-OMP.
While LS-OMP seems to require more computations, there are some numerical
short-cuts to employ, which bring the eÔ¨Éciency of this method very close to that
of the OMP.
While improving the results of the MP, both the OMP and the LS-OMP require
more complex implementations and more calculations.
The other direction, of
simplifying the MP, has been attempted too, with the goal of reducing the amount
of computations needed at each stage. The Weak-MP is born from the observation
that in the MP, it is probably not necessary to Ô¨Ånd the atom that most reduces the
residual, but an atom that can come close to this performance might do as well.
As a result, the atoms are scanned in order, and as soon as an atom is found to
reduce the residual by some threshold (or beyond), it is added to the support and
the iteration ends.
A further simpliÔ¨Åcation can be obtained by avoiding the search at each stage
completely, and instead determine the order in which atoms are to be added in
advance. Such an algorithm is called the Thresholding algorithm. In this method,
the inner product between the signal and all the atoms are computed, and then
sorted in descending order of magnitude. This order determines the order in which
atoms will be added to the support. At each iteration, the next atom is added to
the support, and LS over the support is carried out to best represent the signal (this
can be done using recursive least-squares, which avoids a direct matrix inversion).
As soon as the norm of the residual drops below the given threshold, the process
terminates.
3. Relaxation algorithms
In the previous section we reviewed the greedy algorithms, which attempt to con-
struct the support one atom at a time. Now it is time to turn to relaxation methods,
that smooth the ‚Ñì0-norm and solve the alternative obtained problem using classical
methods from continuous optimization [22, 3].
One natural way to do this is to replace the ‚à•Œ±‚à•0
0 penalty with the expression
PK
i=1

1 ‚àíexp
 ‚àíŒ≤Œ±2
i

, and observe that as Œ≤ ‚Üí‚àû, this term better approximates
the ‚Ñì0 norm. Therefore, it is possible to approximate the ‚Ñì0 solution by optimizing
for a small value of Œ≤, and increase the value of Œ≤ every few iterations.
An alternative is to ‚Äúconvexise‚Äù the ‚Ñì0-norm, which brings us to the Basis
Pursuit (BP) algorithm [7]. In this algorithm, the ‚Ñì0-norm is simply replaced by

168
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
the ‚Ñì1-norm, leading to the optimization problem
(1.5)
min
Œ± ‚à•Œ±‚à•1 s.t. ‚à•DŒ± ‚àíx‚à•2 ‚â§«´,
with the hope that the solution of this optimization problem is close to the solution
of the original one. This problem can be formulated as linear programming for the
exact case («´ = 0), and quadratic programming for the general case. For this task,
several types of eÔ¨Écient solvers can be employed, such as Interior Point methods
[33], Least-Angle-Regression (LARS) [20] and Iterative Shrinkage [8, 21, 24, 25,
26, 27].
Another interesting option for handling the problem formed above (either (1.4)
or (1.5)), is the Iterative Reweighted Least-Squares (IRLS). Before explaining this
algorithm, we slightly change the formulation of the problem. Instead of enforcing
the constraint ‚à•DŒ± ‚àíx‚à•2 ‚â§«´, we now insert it into the optimization problem:
(1.6)
min
Œ±
Œª ¬∑ ‚à•Œ±‚à•p
p + ‚à•DŒ± ‚àíx‚à•2
2,
with Œª balancing the eÔ¨Äects of both terms. A proper choice of Œª makes the two for-
mulations equivalent. Notice that our formulation uses the ‚Ñìp-norm, thus covering
both cases posed in Equations (1.4) or (1.5).
Now, let us re-write the Ô¨Årst term:
(1.7)
‚à•Œ±‚à•p
p =
k
X
i=1
|Œ±i|p =
k
X
i=1
|Œ±i|p‚àí2 ¬∑ |Œ±i| = Œ±T A (Œ±) Œ±.
Based on this, we can suggest an iterative optimization scheme in which we freeze
A (Œ±), and minimize for Œ±:
(1.8)
ÀÜŒ±j+1 = arg min
Œ± ŒªŒ±T A (Œ±j) Œ± + 1
2‚à•DŒ± ‚àíx‚à•2
2.
This is a quadratic term, and as such, it is easy to minimize. In the following step,
we freeze Œ±j+1 and compute A (Œ±j+1), and so on, until the solution converges. More
on the IRLS (sometimes called FOCUSS) can be found in [28].
In the previously described algorithm, our only demand of the residual is that
its norm is bounded. However, in the general case, we expect the residual not only
to be bounded, but we also expect it to look like noise. In other words, we expect
the residual to show low correlation with any of the atoms in the dictionary. This
observation leads to the formulation of the Dantzig Selector (DS) [6]:
(1.9)
min
Œ± ‚à•Œ±‚à•1 s.t. ‚à•DT (DŒ± ‚àíx) ‚à•‚àû‚â§T.
Interestingly, this is a linear programming task even in the noisy case (unlike the BP,
which leads to a quadratic programming problem in the noisy case), and therefore
a wide range of tools can be used for solving it.
Simulations comparing the performance of the DS with those of the BP (see
[22]) show that these two algorithm perform (on average) at the same level. It is
well known that in the unitary case, the two are perfectly equivalent.
4. A closer look at the unitary case
What happens in the special case where the dictionary D is unitary, i.e., DT D = I
(the dictionary is of course not redundant in such a case)? Starting from the prob-
lems posed in Equations (1.4) and (1.5), let us write a more general optimization

LECTURE 1. INTRODUCTION TO SPARSE APPROXIMATIONS - ALGORITHMS
169
goal,
(1.10)
f (Œ±) = 1
2‚à•DŒ± ‚àíx‚à•2
2 + Œª ¬∑ œÅ (Œ±)
where we use a general function œÅ (Œ±) = Pk
j=1 œÅ(Œ±j) measuring the ‚Äúquality‚Äù of the
solution. This could be the ‚Ñì0 pseudo-norm, the ‚Ñì1-norm or any other separable
choice of penalty. Exploiting the fact that D is unitary, we get
f (Œ±) = 1
2‚à•DŒ± ‚àíx‚à•2
2 + Œª ¬∑ œÅ (Œ±) = 1
2‚à•DŒ± ‚àíDDT x‚à•2
2 + Œª ¬∑ œÅ (Œ±)
= 1
2‚à•D (Œ± ‚àíŒ≤) ‚à•2
2 + Œª ¬∑ œÅ (Œ±) = 1
2‚à•Œ± ‚àíŒ≤‚à•2
2 + Œª ¬∑ œÅ (Œ±) .
(1.11)
The Ô¨Årst transition employed the unitary property, DDT = I.
The second is
done by denoting Œ≤ = DT x. The last transition is due to the fact that the ‚Ñì2-
norm is invariant to a unitary transformation. Since œÅ() works on each entry of Œ±
independently, we can Ô¨Ånally write
(1.12)
f (Œ±) =
k
X
j=1
1
2 (Œ±j ‚àíŒ≤j) + Œª ¬∑ œÅ (Œ±j)

,
which indicates that this problem is separated into k easy to solve scalar optimiza-
tion problems.
As the problem becomes a set of independent 1-D problems, it is possible to
compute the output ÀÜŒ± for any input value of Œ≤, eÔ¨Äectively forming a lookup table.
Plotting this graph generally looks like a shrinkage curve1 that nulls the small
entries and the large ones intact. Of course, such a graph can be created for any
penalty function œÅ and value Œª.
4.1. ‚Ñì0 and greedy algorithms for unitary dictionaries
When working with the ‚Ñì0, the function œÅ is the indicator function, œÅ(Œ±j) = |Œ±j|0.
Therefore, the independent optimization problem to solve is
(1.13)
Œ±opt
j
= arg min
Œ±j
1
2 (Œ±j ‚àíŒ≤j)2 + Œª ¬∑ |Œ±j|0

.
This is a very simple task - there are only two logical options for the value of Œ±j.
Either Œ±j = Œ≤i, paying Œª for the second term, or Œ±j = 0, and paying 1
2Œ≤2
j in the
Ô¨Årst term. Any other value will result in a necessarily larger penalty.
Therefore, the shrinkage curve for this case is very simple. For |Œ≤| <
‚àö
2Œª, we
choose Œ± = 0, while for |Œ≤| ‚â•
‚àö
2Œª, we choose Œ± = Œ≤. Therefore, this method is
known as Hard-Thresholding, as any coeÔ¨Écient below the threshold
‚àö
2 ¬∑ Œª is set to
zero, while every coeÔ¨Écient above this threshold remains untouched. This solution
is the global minimizer of the problem we tackled.
It is possible to show that when the dictionary is unitary, all the greedy al-
gorithms we have seen (thresholding, OMP, LS-OMP, weak-OMP, MP) obtain the
exact solution, which is the one obtained by the hard-thresholding algorithm de-
scribed here. Just to illustrate this, the thresholding algorithm will compute DT x,
and work on the coeÔ¨Écients in descending order of magnitude. The amount of
atoms it will choose is exactly those that the hard-thresholding will leave intact,
1The name comes from the fact that the resulting value of ÀÜŒ± is always smaller or equal to the
input Œ≤.

170
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
i.e., the amount of atoms above the threshold. The OMP does the same, simply
choosing one atom at a time. Since the dictionary is unitary, adding another atom
to the support does not eÔ¨Äect the inner products of the residual with the remaining
atoms.
4.2. Relaxation algorithms for unitary dictionaries
The basis pursuit uses the ‚Ñì1 -norm as a penalty on Œ±. In the unitary case, this
leads to separate optimization problem of the form
(1.14)
Œ±opt
j
= arg min
Œ±j
1
2 (Œ±j ‚àíŒ≤j)2 + Œª ¬∑ |Œ±j|

.
Analyzing this problem, it turns out that the solution becomes
(1.15)
Œ±opt
j
=
Ô£±
Ô£≤
Ô£≥
Œ≤j ‚àíŒª
Œ≤j > Œª
0
|Œ≤j| ‚â§Œª
Œ≤j + Œª
Œ≤j < ‚àíŒª
The curve this solution implies is known as soft-thresholding, as when the coeÔ¨Écient
is above the threshold it does not remain the same (as in the hard-thresholding),
but is made smaller instead.
The Dantzig Selector tries to solve an alternative problem (1.9). When we use
the unitary property, it becomes
ÀÜŒ± = min
Œ± ‚à•Œ±‚à•1 s.t. ‚à•DT (DŒ± ‚àíx) ‚à•‚àû= min
Œ± ‚à•Œ±‚à•1 s.t. ‚à•Œ± ‚àíDT x‚à•‚àû,
= min
Œ± ‚à•Œ±‚à•1 s.t. ‚à•Œ± ‚àíŒ≤‚à•‚àû,
(1.16)
where in the Ô¨Årst transition we used DT D = I, which is the unitarity property,
and in the second transition simply re-introduced the notation of Œ≤. The last step
results in K independent optimization problems of the form:
(1.17)
arg min
Œ±i |Œ±j| s.t. |Œ±j ‚àíŒ≤j| ‚â§T
It is not diÔ¨Écult to see that the solution of this optimization problem is exactly
identical to the solution oÔ¨Äered for the BP algorithm in (1.15). The conclusion is
that BP and DS are exactly identical when the dictionary is unitary, and both of
them can be solved using a closed-form formula for each of the coeÔ¨Écients (such
that they do not require a complex optimization procedure).
4.3. Unitary dictionaries - summary
We have seen that when the dictionary is unitary, both families of algorithms enjoy
a simple closed-form solution. Given the signal x, multiply it by the dictionary to
obtain a vector of coeÔ¨Écients Œ≤ = DT x. On each of the coeÔ¨Écients, apply inde-
pendently a Look-Up Table operation with a shape that depends on œÅ and Œª, and
this results in the vector of coeÔ¨Écients ÀÜŒ±. This simple procedure yields the global
optimum of f(Œ±), even if f itself is not convex. This leads to the obvious question
‚Äì can this be used to gain an insight to the general case, when the dictionary is
non-unitary? Can we use it to construct simpler, more eÔ¨Äective algorithms for such
cases? In the 4th lecture we will see a family of algorithms doing exactly that.

LECTURE 2
Introduction to sparse approximations - theory
This lecture deals with the theory of sparse approximations. We will start by
deÔ¨Åning several properties of dictionaries - the Spark, the Mutual-Coherence, and
the Restricted Isometry Property (RIP). We will then show theoretical guarantees
for the uniqueness of the solution for (P0), and show the equivalence of MP and
BP for the exact case. We will then continue with the discussion of (P «´
0), and show
theoretical guarantees on the stability of the solution. Finally, we will mention
brieÔ¨Çy results related to near-oracle performance of pursuit techniques in the noisy
case.
1. Dictionary properties
The purpose of this section is Ô¨Ånding ways to characterize the dictionary D, in order
for us to be able to make claims about the conditions under which pursuit algorithms
are guaranteed to succeed. Ideally, we would like these bounds to be tight, and the
characterizations easy to compute. As we will see, this is not necessarily the case.
1.1. The matrix spark
The Spark was deÔ¨Åned by Donoho & Elad [14] (similar deÔ¨Ånitions have been made
in tensor decomposition by Kruskal [34, 35] as well as in coding theory):
DeÔ¨Ånition 2.1 (Spark). Given a matrix D, œÉ = Spark (D) is the smallest number
of atoms from D that are linearly dependent.
While this deÔ¨Ånition is likely to remind the reader of the deÔ¨Ånition of the rank
of a matrix, it is very diÔ¨Äerent. There are two diÔ¨Äerences between the deÔ¨Ånitions -
while the rank is the largest number of independent atoms, the Spark is the smallest
number of dependent atoms. For an example, consider the matrix
D =
Ô£´
Ô£¨
Ô£¨
Ô£≠
1
0
0
0
1
0
1
0
0
1
0
0
1
0
0
0
0
0
1
0
Ô£∂
Ô£∑
Ô£∑
Ô£∏
The rank of this matrix is 4, the maximal number of independent columns (e.g.,
the Ô¨Årst four columns). The Spark of this matrix is 3: one column could only be
linearly dependent if it is the zero vector; Any two columns in this dictionary are
linearly independent; There is a group of three columns - the Ô¨Årst, second and last
- that are linearly dependent. Therefore, œÉ (D) = 3.
It is easy to observe that for a dictionary Dn√ók, the Spark satisÔ¨Åes 1 ‚â§œÉ ‚â§
n+1. Also, if for some non-trivial vector v it exists that Dv = 0, than is is certain
that ‚à•v‚à•0
0 ‚â•œÉ, since the support of v constitutes a linearly dependent set of atoms.
Since the smallest possible such set is œÉ, the support of v also contains at least œÉ
atoms.
171

172
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
1.2. The mutual coherence
DeÔ¨Ånition 2.2 (Mutual Coherence). For a matrix D, the Mutual-Coherence M
is the largest (in magnitude) oÔ¨Ä-diagonal element of the Gram matrix of D, i.e.,
G = DT D.
For this deÔ¨Ånition, we assume that the columns of D are normalized. The
Mutual-Coherence measures the maximal similarity (or anti-similarity) between two
columns on the dictionary. Therefore, its value ranges between 0 (an orthonormal
basis) and 1 (when there are two identical atoms). As we will see later, a large
mutual coherence is bad news for our algorithms, as the algorithms are likely to
confuse between the two atoms.
1.3. The relationship between the spark and the mutual coherence
Our next task is to understand the relationship between the Spark and the Mutual
Coherence measures. In order to do that, we need the following well-known theorem:
Theorem 2.1 (Gershgorin‚Äôs Disk Theorem). Consider a general square matrix
A, and deÔ¨Åne the k‚Ä≤th disk as diskk =
n
x
|x ‚àíakk| ‚â§P
jÃ∏=k |ajk|
o
. Then, the
eigenvalues of A all lie in the union of all disks, {Œª1, Œª2, .., Œªn} ‚äÇ‚à™k=1..ndiskk.
Alternatively, we can write Œªmin ‚â•arg min1‚â§k‚â§n
n
akk ‚àíP
jÃ∏=k |ajk|
o
and
Œªmax ‚â§arg max1‚â§k‚â§n
n
akk + P
jÃ∏=k |ajk|
o
. One of the implications of the theorem
is that if A is diagonally dominant (assuming real entries), than A is necessarily
positive deÔ¨Ånite. This is because the origin is not in any of the disks, and therefore
all eigenvalues are positive. Using this theorem, we are now ready to make the
following claim:
Theorem 2.2 (Relation between Spark and Mutual Coherence). œÉ ‚â•1 + 1
M
Proof. Let us consider a group of columns S from D, and compute its sub-
Gram matrix.
We know that these columns are independent, if this matrix is
positive deÔ¨Ånite. From the Gershgorin‚Äôs disk theorem, we know that the matrix
will be positive deÔ¨Ånite if it is diagonally dominant.
The Mutual-Coherence bounds any oÔ¨Ä-diagonal element by M, while the diago-
nal elements are all 1. Therefore, the matrix is diagonally dominant if (S ‚àí1) M <
1. As long as S < 1 + 1/M, any group of S columns are independent. Therefore,
the minimal number of columns that could be dependent is S = 1 + 1/M, and this
is exactly the claimed result.
‚ñ°
1.3.1. Grassmanian Frames. We now turn to deal with the following question: For
a general matrix Dn√ók, with k > n, what is the best (smallest) Mutual-Coherence
we can hope for? Obviously, it can‚Äôt be zero, as that would mean that there are
k columns that are orthogonal, and that is not possible when their length is n. It
turns out that
(2.1)
Mmin =
s
k ‚àín
n (k ‚àí1) = O
 1
‚àön

,
with the second step using the substitution k = œÅn, and the approximation is true
for small values of œÅ (e.g., 2,3). We will omit the proof, and only state that this
question is closely related to questions about packing vectors in the n-dimensional

LECTURE 2. INTRODUCTION TO SPARSE APPROXIMATIONS - THEORY
173
space in the most spacious ways. As an example, for a matrix with n = 50 rows
and k = 100 columns, the minimal possible coherence is roughly 0.1.
Can matrices achieving this bound be found in practice? The answer is yes,
and this family of matrices is known as Grassmanian Frames [49]. The prevailing
property of these frames is that the oÔ¨Ä-diagonal elements in their Gram-matrices
all have the same magnitude. They are diÔ¨Écult to built, and a numerical algorithm
by Tropp has been developed for their construction [52]. It is important to note,
however, that for some pairs of n and k such frames do not exist.
1.4. The restricted isometry property
DeÔ¨Ånition 2.3 (Restricted Isometry Property). Given a matrix D, consider all
s-sparse vectors Œ±, and Ô¨Ånd the smallest value Œ¥s that satisÔ¨Åes
(1 ‚àíŒ¥s) ‚à•Œ±‚à•2
2 ‚â§‚à•DŒ±‚à•2
2 ‚â§(1 + Œ¥s) ‚à•Œ±‚à•2
2.
D is then said to have an s-RIP with a constant Œ¥s.
What is the meaning of this deÔ¨Ånition? DŒ± combines linearly atoms from D.
The RIP suggests that every such group of s atoms behaves like an isometry, i.e.,
does not considerably change the length of the vector it operates on.
As an example, when D is unitary, the length does not change for any s.
Therefore, Œ¥s(D) = 0, for every s. In this sense, the RIP asks how close is D to a
unitary matrix, when considering multiplications by s-sparse vectors.
1.5. The RIP and the mutual coherence
Let us denote DS as the sub-matrix of D containing only the columns in the support
S, and similarly, Œ±S as containing only the non-zeros in Œ±, so DSŒ±S = DŒ±. Using
this terminology, the RIP property states that
(1 ‚àíŒ¥s) ‚à•Œ±S‚à•2
2 ‚â§Œ±T
SDT
SDSŒ±S ‚â§(1 + Œ¥s) ‚à•Œ±S‚à•2
2.
On the other hand, from an eigenvalue perspective, we know that
‚à•Œ±S‚à•2
2 ¬∑ Œªmin
 DT
SDS

‚â§Œ±T
SDT
SDSŒ±S ‚â§‚à•Œ±S‚à•2
2 ¬∑ Œªmax
 DT
SDS

.
From the Gershgorin‚Äôs theorem we already know that
Œªmin
 DT
SDS

‚â•1 ‚àí(s ‚àí1) M
Œªmax
 DT
SDS

‚â§1 + (s ‚àí1) M,
and Ô¨Ånally, we see that
(2.2)
Œ¥s ‚â§(s ‚àí1) M.
1.6. The RIP and the spark
Suppose that the Spark of D is known to be œÉ. Then, for s = œÉ, there is at least
one vector Œ± of cardinality s such that DŒ± = 0, therefore indicating that Œ¥œÉ = 1.
Furthermore, for any value of s < œÉ, it follows that Œ¥s < 1.
1.7. Computing the spark, the MC and the RIP
Summarizing the discussion so far, we consider the question of computing the above
described properties. The Spark requires going through each and every support and
checking for linear dependence. Thus, it is impossible to compute in general as it
is combinatorial. There is an exception, however, as a random iid matrix has a full
Spark of (n + 1).

174
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
The RIP similarly has to scan all supports, and is therefore also impossible
to compute in practice.
There is also an exception here as well, in the case of
a random Gaussian matrix, the RIP can be bounded (with high probability) by
Œ¥s ‚â§C ¬∑
q
s
n ¬∑ log
  k
s

. The C is a coeÔ¨Écient controlling the probability ‚Äì as the
required conÔ¨Ådence grows, so does the constant C, making the bound less tight.
Fortunately, the Mutual Coherence is very easy to compute, as we have seen
- it only requires computing the Gram matrix, and scanning all elements in it.
Unfortunately, as this measure only quantiÔ¨Åes pairs of atoms, it is also the weakest
of the three measures.
Lastly, it is important to note that all of the measures introduced are of worst-
case nature, and as such lead to worse-case performance analysis of algorithms.
For example, it is enough that the dictionary contains a duplicate atom to set the
Mutual Coherence to 1. The rest of the dictionary can behave perfectly, but that
will be unknown to the coherence. The Spark and the RIP will also suÔ¨Äer in such
a case in a similar fashion. DiÔ¨Äerent types of measures are needed for average-case
performance analysis, but such measures are still very much missing.
2. Theoretical guarantees - uniqueness for P0
After the introduction of some tools to be used, we now turn to the Ô¨Årst problem
of interest. Suppose Alice holds a dictionary D, and generates some sparse vector
of coeÔ¨Écients Œ±. Multiplying the two, Alice generates the signal x = DŒ±. Then,
Alice contacts her friend, Bob, and gives him the dictionary D and the vector x,
and asks him to Ô¨Ånd the vector Œ± she had used.
Bob knows that Œ± is sparse, and so he sets out to seek the sparsest possible
solution of the system DŒ± = x, namely, solve the optimization problem
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. DŒ± = x.
For the time being, we shall assume that Bob can actually solve this problem exactly
(we have seen in the Ô¨Årst lecture that this is generally NP-hard, and therefore
impossible in practice). The question we want to ask is if (or when) can we expect
that ÀÜŒ± = Œ±. It might turn our that ÀÜŒ± is even sparser than the original Œ±. This is
essentially the question of uniqueness ‚Äì what properties must Œ± and the dictionary
D have so that Œ± is the sparsest vector to create x? The answer turns out to be
quite simple [14]:
Theorem 2.3 (Uniqueness Requirements of P0). The following two claims deÔ¨Åne
the uniqueness condition and implication:
(1) Suppose we have found a sparse solution ÀÜŒ± that satisÔ¨Åes DÀÜŒ± = x. If this
solution satisÔ¨Åes ‚à•ÀÜŒ±‚à•0
0 < œÉ/2, then this this solution is unique (i.e., it is
the sparsest solution possible).
(2) If Alice generated a signal with ‚à•Œ±‚à•0
0 < œÉ/2, than Bob is able to recover Œ±
exactly: ÀÜŒ± = Œ±.
Proof. Suppose we have found two candidate solutions Œ±1 and Œ±2 to the linear
system of equations DŒ± = x, leading to DŒ±1 = DŒ±2 and D (Œ±1 ‚àíŒ±2) = 0. From
the Spark property, the cardinality of any vector v that satisÔ¨Åed Dv = 0 is at least
the Spark, indicating that
(2.3)
‚à•Œ±1 ‚àíŒ±2‚à•0
0 ‚â•œÉ.

LECTURE 2. INTRODUCTION TO SPARSE APPROXIMATIONS - THEORY
175
We can also observe that
(2.4)
‚à•Œ±1‚à•0
0 + ‚à•Œ±2‚à•0
0 ‚â•‚à•Œ±1 ‚àíŒ±2‚à•0
0.
This property can be understood by analyzing the supports of both vectors. If there
is no overlap in their supports, then the equality holds. If there is an overlap, and
two terms in the overlap happen to cancel each other exactly, then the resulting
support is smaller than the sum of sizes of the two supports. We can refer to this
property as some sort of triangle inequality of the ‚Ñì0, even though it is not actually
a norm.
Combining the two inequalities, it turns out that the sum of cardinalities of
both vectors is at least the Spark,
(2.5)
‚à•Œ±1‚à•0
0 + ‚à•Œ±2‚à•0
0 ‚â•œÉ.
This can be viewed as an uncertainty principle ‚Äì two solutions cannot be jointly
very sparse. If one solution is indeed sparse, the other must be non-sparse. The
uncertainty principle implies that if we have found a solution Œ±1 with cardinality
less than half the Spark, i.e., Œ±1 < œÉ/2, than every other solution Œ±2 must have
a cardinality of more than half the Spark, in order to fulÔ¨Åll the requirement that
‚à•Œ±1‚à•0
0 + ‚à•Œ±2‚à•0
0 ‚â•œÉ. Therefore, Œ± is sparser than any other solution, and indeed,
the sparsest solution.
It directly follows that if Alice chose Œ± such that ‚à•Œ±‚à•0
0 < œÉ/2, than it is the
sparsest solution to DŒ± = x, and therefore Bob will recover it exactly.
‚ñ°
3. Equivalence of the MP and BP for the exact case
Now we turn to walk in Bob‚Äôs shoes. We have seen that Bob‚Äôs task is impossible
to solve directly, and therefore he has to turn to approximation algorithms, such
as Orthogonal Matching Pursuit (OMP)‚Äî, Basis Pursuit (BP), etc. The question
we ask now is: Is there any hope that any of these algorithms will actually recover
the exact solution?
Surprisingly, we will see that under some conditions, these
algorithms are guaranteed to succeed.
3.1. Analysis of the OMP
Recall the OMP algorithm, introduced in lecture 1. To make our life easier, we
assume that the s non-zero locations in Œ± are in the Ô¨Årst s locations. Furthermore,
we assume that these coeÔ¨Écients are sorted in decreasing order of magnitude, with
|Œ±1| ‚â•|Œ±2| ‚â•.. ‚â•|Œ±s| > |Œ±s+1| = 0. Thus, our signal can be written as x =
Ps
i=1 Œ±idi. This assumption does not eÔ¨Äect the generality of our claims, as we
can achieve this structure by reordering the columns of D and the locations of the
non-zeros in Œ±. The re-structured problem remains equivalent to the original one.
We now ask ourselves what conditions are needed for the Ô¨Årst OMP step to
succeed, i.e., that the Ô¨Årst atom added to the support is an atom in the true
support (one of the Ô¨Årst s atoms)?
Since the OMP selects the atom with the
largest magnitude inner-product, the Ô¨Årst step succeeds if
(2.6)
xT d1
 > max
j>s
xT dj
 .
This condition guarantees that the atom selection will prefer the Ô¨Årst atom to any
atom atom outside the support, and thus guarantees that an atom in the true
support is chosen. It may happen that another atom in the true support is chosen

176
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
instead of the Ô¨Årst, but this is also considered as a success. Therefore, the question
we want to answer is when does the condition (2.6) hold.
In order to Ô¨Ånd out under what conditions this inequality is satisÔ¨Åed, we lower
bound the left-hand-side (LHS), and upper bound the right-hand-side(RHS). We
will then Ô¨Ånd the conditions for the bound on the LHS still being greater than the
upper bound of the RHS, thus fulÔ¨Ålling the original inequality as well.
We start with an upper-bound of the Right-Hand-Side (RHS):
(2.7)
max
j>s
xT dj
 = max
j>s

s
X
i=1
Œ±idT
i dj
 ‚â§max
j>s
s
X
i=1
|Œ±i| ¬∑
dT
i dj
 ‚â§|Œ±1| ¬∑ s ¬∑ M.
The Ô¨Årst transition assigns the expression x = Ps
i=1 Œ±idi, the second uses the prop-
erty |a + b| ‚â§|a| + |b|, and the third uses the deÔ¨Ånition of the Mutual-Coherence as
an upper bound on
dT
i dj
, as well as the monotonicity of the coeÔ¨Écients, implying
that |Œ±k| ‚â§|Œ±1| for k = 2, .., s.
We now turn to lower-bound the Left-Hand-
Side(LHS) in inequality (2.6):
xT d1
 =

s
X
i=1
Œ±idT
i d1
 =
Œ±1 +
s
X
i=2
Œ±idT
i d1
 ‚â•|Œ±1| ‚àí

s
X
i=2
Œ±idT
i d1

‚â•|Œ±1| ‚àí
s
X
i=2
|Œ±i| ¬∑
dT
i d1
 ‚â•|Œ±1| ¬∑ [1 ‚àí(s ‚àí1) ¬∑ M] .
(2.8)
The transitions in the Ô¨Årst line are made by Ô¨Årst substituting x = Ps
i=1 Œ±idi and
then using the fact that the atoms are normalized, i.e., dT
1 d1 = 1.
Then, the
bounding starts by the reversed triangle inequality |a + b| ‚â•|a| ‚àí|b|, and then
repeating the steps performed in bounding the RHS (note that since the second
term is with a negative sign, increasing its magnitude reduces the magnitude of the
overall expression).
Using the bounds in (2.7) and (2.8) and plugging them into (2.6), we get that
a condition that guarantees that the Ô¨Årst OMP step succeeds is
xT d1
 ‚â•|Œ±1| ¬∑ [1 ‚àí(s ‚àí1) ¬∑ M] > |Œ±1| ¬∑ s ¬∑ M ‚â•max
j>s
xT dj
 ,
which leads to the following bound on the cardinality of the representation,
(2.9)
s < 1
2

1 + 1
M

.
Assuming this condition is met, the Ô¨Årst atom chosen is indeed an atom from
the true support, with index i‚àó. Then, a LS step is performed, and a coeÔ¨Écient
value Œ±‚àó
1 is assigned to this atom. Now, we look at the residual as our new signal,
being
r1 = x ‚àíŒ±‚àó
1di‚àó=
s
X
i=1
Œ±idi ‚àíŒ±‚àó
1di‚àó.
This signal has the same structure we the one we started with ‚Äì a linear combination
of the s Ô¨Årst atoms. This means that the exact same condition is needed for the
second step to succeed, and in fact, for all other steps to succeed as well.
This means, that if s < 1
2

1 + 1
M

the OMP is guaranteed to Ô¨Ånd the correct
sparse representation. This is done in exactly s iterations, as in each iteration one
atom in the support is chosen, and due to the LS stage, the residual is always
orthogonal to the atoms already chosen.

LECTURE 2. INTRODUCTION TO SPARSE APPROXIMATIONS - THEORY
177
Using this result, and the bound we have found in the previous section regarding
the Spark, we come to the following conclusion:
Theorem 2.4 (OMP and MP Equivalence). Given a vector x with a representation
DŒ± = x, and assuming that ‚à•Œ±‚à•0
0 < 0.5 (1 + 1/M), then (i) Œ± is the sparsest possible
solution to the system DŒ± = x, and (ii) OMP (and MP) are guaranteed to Ô¨Ånd it
exactly [50, 16].
3.2. Analysis of the BP
We now turn to perform the same analysis for the BP, checking under which
conditions are we guaranteed to succeed in recovering the sparsest solution. We
remind ourselves that the BP aims to approximate the solution of the problem
ÀÜŒ± = arg minŒ± ‚à•Œ±‚à•0
0 s.t. DŒ± = x, by solving instead ÀÜŒ± = arg minŒ± ‚à•Œ±‚à•1 s.t. DŒ± = x.
Therefore, our aim is now to show under which condition is the sparse vector Œ±
used to generate the signal x also the shortest one with respect to the ‚Ñì1 norm.
The strategy for this analysis starts by Ô¨Årst deÔ¨Åning the set of all solutions for
the linear set of equations DŒ≤ = x (excluding the one we started with), such that
their ‚Ñì1 length is shorter (or equal) to the length of the original vector Œ±,
(2.10)
C = {Œ≤ Ã∏= Œ± |x = DŒ± = DŒ≤ and ‚à•Œ≤‚à•1 ‚â§‚à•Œ±‚à•1 } .
We will gradually inÔ¨Çate this set, while simplifying it, eventually showing that it
is still empty. When done, this will imply that the initial set is also empty, and
therefore Œ± is the result of the ‚Ñì1 minimization, leading to Œ± being the vector with
the shortest ‚Ñì1 norm, therefore recovered by the BP.
Our Ô¨Årst step starts with deÔ¨Åning the vector e = Œ≤ ‚àíŒ±, or in other words,
e + Œ± = Œ≤. Our set remains unchanged, and is simply rewritten as
(2.11)
Ce = {e Ã∏= 0 |0 = De and ‚à•Œ± + e‚à•1 ‚àí‚à•Œ±‚à•1 ‚â§0} .
Next, we simplify the linear constraint 0 = De,
{e |0 = De} =

e
0 = DT De
	
=

e
‚àíe =
 DT D ‚àíI

e
	
‚äÜ

e
|e| =
 DT D ‚àíI

e
	
‚äÜ

e
|e| ‚â§
DT D ‚àíI
 ¬∑ |e|
	
‚äÜ

e
|e| ‚â§M ¬∑
 11T ‚àíI

¬∑ |e|
	
= {e |(1 + M) ¬∑ |e| ‚â§M‚à•e‚à•1 }
The transition from the Ô¨Årst row to the second is done by taking absolute value of
both sides (an absolute value of a vector is done element-wise). Obviously, some new
e‚Äôs have been added to the set, as we are no longer forcing the sign equality, implying
that the set becomes larger. The next step stems from the triangle inequality. The
transition between the third and fourth lines is slightly more complex. Observe
that the matrix DT D is eÔ¨Äectively the Gram Matrix, and all oÔ¨Ä-diagonal elements
of it are bounded by M.
Furthermore, the matrix
 DT D ‚àíI

has zeros along
the main diagonal, and all remaining elements are still bounded by M. Observing
that 11T is a matrix of the proper size, all containing ones, than M
 11T ‚àíI

is
a matrix containing zeros on the diagonal, and M on the oÔ¨Ä-diagonal terms. Thus
DT D ‚àíI
 ‚â§M ¬∑
 11T ‚àíI

element-wise. This way, we no longer need D directly,
instead characterizing it using only its Mutual-Coherence.

178
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
The last step re-orders the elements, and uses 1T |e| = ‚à•e‚à•1. Replacing the
linear constraint in (2.11) with the one derived above, we obtain an inÔ¨Çated set
(2.12)
Ce ‚äÜ

e Ã∏= 0
|e| ‚â§
M
M + 1 ¬∑ ‚à•e‚à•1 and ‚à•Œ± + e‚à•1 ‚àí‚à•Œ±‚à•1 ‚â§0

Now we turn to handle the second constraint, ‚à•Œ± + e‚à•1 ‚àí‚à•Œ±‚à•1 ‚â§0,
{e |‚à•Œ± + e‚à•1 ‚àí‚à•Œ±‚à•1 ‚â§0} =
(
e

K
X
i=1
(|Œ±i + ei| ‚àí|Œ±i|) ‚â§0
)
=
(
e

s
X
i=1
(|Œ±i + ei| ‚àí|Œ±i|) +
X
i>s
|ei| ‚â§0
)
‚äÜ
(
e

X
i>s
|ei| ‚àí
s
X
i=1
|ei| ‚â§0
)
=

e
‚à•e‚à•1 ‚àí2 ¬∑ 1T
s |e| ‚â§0
	
.
The Ô¨Årst step simply writes the ‚Ñì1 norm as a sum of elements, and the second
step uses the knowledge that only the Ô¨Årst s coeÔ¨Écients in Œ± are non-zeros (as we
have assumed earlier). The third term uses a version of the triangle inequality,
|x + y|‚àí|x| ‚â•‚àí|y|. Using this replacement, each term in the sum is made smaller,
thus inserting more possible e‚Äôs into the set. The last step merely rewrites the
expression, since P
i>s |ei| ‚àíPs
i=1 |ei| = PK
i=1 |ei| ‚àí2 ¬∑ Ps
i=1 |ei| = ‚à•e‚à•1 ‚àí2 ¬∑ 1T
s |e|,
with 1s denoting a vector of length k with the Ô¨Årst s elements being 1-es and the
remaining elements being 0-es.
Plugging this inÔ¨Çated group into our deÔ¨Ånition of the group Ce in (2.12), we
arrive at
(2.13)
Ce ‚äÜ

e Ã∏= 0
|e| ‚â§
M
M + 1 ¬∑ ‚à•e‚à•1 and ‚à•e‚à•1 ‚àí2 ¬∑ 1T
s |e| ‚â§0

.
The attractive part about this set is that it no longer depends directly on D, but
only on its Mutual-Coherence, and furthermore, it does not depend on Œ± any more,
allowing us to obtain a general bound applicable to all vectors.
Our next simpliÔ¨Åcation uses the observation that if we have found any vector
e ‚ààCe, then for every real number z, z ¬∑e is also in Ce. Since we are only interested
in the question weather Ce is empty or not, we choose to intersect this set with the
‚Ñì1-ball, i.e., all the vectors e that have an ‚Ñì1-norm of 1,
(2.14)
Ce ‚äÜ

e Ã∏= 0
|e| ‚â§
M
M + 1 and 1 ‚àí2 ¬∑ 1T
s |e| ‚â§0 and ‚à•e‚à•1 = 1

.
Our purpose now is to derive conditions for this set to be empty. The Ô¨Årst constraint
is that every element of e is smaller than M/ (1 + M). In order to satisfy the second
constraint, the energy of the vector e should be concentrated in the elements in the
support. Using the bound on the size of each element, for the second constraint to
be satisÔ¨Åed (and the set being not empty) it is required that
1 ‚àí2 ¬∑ s ¬∑
M
1 + M ‚â§0 ‚áís ‚â•M + 1
2M
.
This means that as long as s < M+1
2M , the set is empty, and therefore Ce is also
empty. This implies that under this condition, a vector Œ≤ diÔ¨Äerent from Œ±, satisfying

LECTURE 2. INTRODUCTION TO SPARSE APPROXIMATIONS - THEORY
179
DŒ≤ = x and with ‚à•Œ≤‚à•1 ‚â§‚à•Œ±‚à•1 does not exist. This leads to the BP actually arriving
at Œ± as the result of its minimization, which means that when s < M+1
2M , the basis
pursuit is guaranteed to recover Œ± exactly.
Stating this formally, we obtain the equivalence theorem for the Basis Pursuit:
Theorem 2.5 (BP Equivalence). Given a vector x with a representation DŒ± = x,
and assuming that ‚à•Œ±‚à•0
0 < 0.5 (1 + 1/M), then BP is guaranteed to Ô¨Ånd the sparsest
solution [14, 29].
Obviously, this claim is exactly identical to the equivalence claim for the OMP,
meaning that the bound to guarantee success of the two algorithms is identical.
However, it does not mean that the two algorithms are perform the same, as they
are diÔ¨Äerent in general (speciÔ¨Åcally, when this bound is not met, or in the noisy
case). It is hard to tell which is better.
Furthermore, the bounds presented here are worst-case performance guaran-
tees.
When analyzing actual performance, these bounds are usually too pes-
simistic. Analysis of average performance has also been done. In a series of papers
[5, 12, 13, 18, 19, 53], bounds are developed from a probabilistic point of view,
showing that up to some bounds, the actual vector may be recovered with a prob-
ability close to 1. These bounds are proportional to O(1/M 2), instead of O(1/M).
4. Theoretical guarantees - stability for (P «´
0)
4.1. Stability versus uniqueness
Consider the following problem, which is very similar to the one we dealt with
above: Alice chooses a sparse vector Œ±, multiplies it by D, and obtains x = DŒ±.
Alice wants to send the signal x to Bob, but unfortunately, unlike before, the
transmission line is imperfect. Therefore, noise is added to the signal, and Bob
receives the signal y = x + v, where we know that the norm of v is bounded,
‚à•v‚à•2 ‚â§«´.
Bob would like to recover Œ±. Since y is noisy, there is no sense in solving the
exact equation y = DŒ±, and thus Bob would attempt to solve
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0 s.t. ‚à•DŒ± ‚àíx‚à•2 ‚â§«´.
Due to the noise being added, there might be somewhere in the sphere around
x a vector with a sparser representation than Œ±. If not, Œ± itself will be chosen.
Therefore, we know that ‚à•ÀÜŒ±‚à•0 ‚â§‚à•Œ±‚à•0. Can we hope that ÀÜŒ± = Œ±? If not, can we at
least hope that they are similar or close-by?
It turns out (we will not show it hear, but the slides present it through a simple
and illustrative example) that exact equality in the recovery, ÀÜŒ± = Œ±, is impossible
to expect in general, and thus we will turn to discuss stability, which is a claim
about the proximity between Alice‚Äôs a, and Bob‚Äôs proposed solution ÀÜŒ±.
Suppose we created the true signal with an s-sparse vector of coeÔ¨Écients Œ±1,
and the solution we got is an s-sparse (or sparser) vector Œ±2. These vectors satisfy
‚à•DŒ±1 ‚àíy‚à•2 ‚â§«´ and ‚à•DŒ±2 ‚àíy‚à•2 ‚â§«´, which means that both of these solutions are
inside a sphere around y with radius «´. As these two points are inside the sphere,
the maximal distance between the two is twice the radius, and therefore
(2.15)
‚à•D (Œ±1 ‚àíŒ±2) ‚à•2 ‚â§2«´.

180
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
On the other hand, we can use the RIP property. The vector Œ±1 ‚àíŒ±2 is 2s-sparse
at most (if the vectors have diÔ¨Äerent supports), and we have the RIP property
(2.16)
(1 ‚àíŒ¥2s) ‚à•Œ±1 ‚àíŒ±2‚à•2
2 ‚â§‚à•D (Œ±1 ‚àíŒ±2) ‚à•2
2.
Combining (2.15) (by squaring both sides) with (2.16) leads to the relation
(2.17)
(1 ‚àíŒ¥2s) ‚à•Œ±1 ‚àíŒ±2‚à•2
2 ‚â§4«´2
Rearranging the terms leads to the following bound on the distance between the
two solutions
(2.18)
‚à•Œ±1 ‚àíŒ±2‚à•2
2 ‚â§
4«´2
1 ‚àíŒ¥2s
‚â§
4«´2
1 ‚àí(2s ‚àí1) M ,
where in the last step we have used the relationship between the RIP and the
Mutual-Coherence Œ¥s ‚â§(s ‚àí1) M.
We have obtained a bound on the distance
between the two representation vectors, and we can see that this bound is in the
order of the noise. It is slightly higher, since the denominator is smaller than 1,
but it is not far-oÔ¨Ä. This is what we strived to show ‚Äì stability.
The above analysis assumes that the power of the noise is bounded, ‚à•v‚à•2 ‚â§«´.
However, there was no assumption made as to the shape of the noise, and the noise
might have taken any shape to work against our analysis and algorithm. For this
reason, the bounds we obtain is weak, showing no eÔ¨Äect of noise attenuation.
An alternative approach could be to model the noise as a random, i.i.d Gaussian
vector, the elements of which are randomly drawn as vi ‚àºN
 0, œÉ2
. When such a
model is assumed, we can expect to get much better stability bound: The norm of
the error is expected to be close to (but not exactly) s ¬∑ œÉ2 instead of n ¬∑ œÉ2, which
is a much better result. However, this claim will no longer be deterministic, but
only probabilistic, with a probability very close to 1.
5. Near-oracle performance in the noisy case
So far we discussed the theoretical solution of (P «´
0). However, we know that this
task is impossible, and approximation algorithms are used, such as the OMP and
the BP. Very similar analysis exists that shows stability of these algorithms ‚Äì a
successful recovery of the representation vector. We will not show this here, and
we refer the readers to [15, 16, 51]
As above, there is a distinction between adversive noise, which leads to worst-
case performance bounds similar to the one given in Equation (2.18). When random
noise is assumed, these bounds improve dramatically. In that respect, it is custom-
ary to compare the performance of pursuit techniques to the one obtained by an
oracle - a pursuit technique that knows the exact support. The oracle‚Äôs error is
typically O(sœÉ2) (as described above), and practical pursuit methods show bounds
that are this expression multiplied by a constant (larger than 1) and a logk factor
[1, 6].

LECTURE 3
Sparse and redundant representation modelling
1. Modelling data with sparse and redundant representations
1.1. Inherent low-dimensionality of images
Let us think of the following virtual experiment: Consider images x of size ‚àön√ó‚àön
(e.g., n = 400 for 20 √ó 20 images). Each such image can be thought of as a point
in the IRn space. Once we accumulate many such images, we will have many such
points in IRn. However, no matter how many images we accumulate, the space
IRn is not going to be completely Ô¨Ålled, but rather many empty areas will remain.
Furthermore, the local density of the images from one place to another is expected
to vary considerably.
The bottom line from these two observations is that we believe that the true
dimension of the images is not n. Rather, we believe that the images form a low-
dimensional manifold in IRn, with spatially varying densities.
It is our task to
model this manifold reliably.
The tool with which we represent the manifold is the Probability Density Func-
tion (PDF) Pr(x). This function tell us for each x ‚ààIRn how probable it is to be
an image. Obviously, this function should be non-negative and when summed over
the entire space it should sum to 1. In order to fulÔ¨Åll our assumption of the images
forming a low-dimensional manifold, the PDF Pr(x) should be zero for most of the
cube [0, 255]n.
The function Pr(x) is called the Prior as it reÔ¨Çects our prior knowledge about
what images should look like. Unfortunately, characterizing this function explicitly
is probably an impossible task, which leads us to look for ways to simplify it.
1.2. Who needs a prior?
Let us consider one simple example in which we need a prior. Suppose we would
like to obtain an image x (lying on the low-dimensional manifold). Unfortunately,
we are only able to obtain its noisy version, z = x + v, where v is a random noise
vector with a limited magnitude ‚à•v‚à•2 ‚â§«´. Using the prior over the image x, we
can now use it to try and recover x, by solving the problem
ÀÜx = arg min
x Pr (x) s.t. ‚à•x ‚àíz‚à•2 ‚â§«´.
In eÔ¨Äect, we are looking for the most probable vector x (according to our prior),
which is also close enough to z to explain it. This approach is a very known one ‚Äì
it is called the Maximum A‚Äôposteriori Probability (MAP) approach.
This simple example is just one of many that require a good model (prior).
All inverse problems, such as denoising, deblurring, super-resolution, inpainting,
demosaicing, tomographic reconstruction, single image scale-up and more, require a
good image prior, in order to be able to diÔ¨Äerentiate between the true image content
and the degradation eÔ¨Äects. Compression algorithms require a model in order to
181

182
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
be able to Ô¨Ånd the most compact representation of the image. Anomaly detection
requires a model of the image, in order to pick up on anything not conforming to
the model. The number of problems requiring a good image model is ever growing,
which leads us to the natural question - what is this prior Pr(x)? Where can we
get it from?
1.3. Evolution of priors
The topic of a proper choice of Pr(x) for images has been at of the foundation of
the research in image processing since its early days, and many models have been
suggested. Since it is not possible to characterize Pr(x) explicitly, some form of
computing it is suggested. Usually, instead of suggesting Pr(x) directly, its minus
log is used, G (x) = ‚àílog (Pr(x)), thereby giving a penalty of 0 to the ‚Äúperfect‚Äù
image, and some positive penalty to any other, a larger value as the probability of
the image decreases.
There has been an evolution of choices for G (x) through the years. It started
with the ‚Ñì2-norm, and the very simple energy term, G (x) = Œª‚à•x‚à•2
2. It then pro-
gressed to smoothness terms, demanding low energy of some derivatives, G (x) =
Œª‚à•Lx‚à•2
2, with L being the Laplacian operator. A further step was made by adap-
tively demanding this low energy, while allowing pixels detected as edges to contain
high energy in the derivative, G (x) = Œª‚à•Lx‚à•W, where W is a pre-computed diag-
onal matrix, assigning diÔ¨Äerent weights to diÔ¨Äerent pixels.
The next major step was the abandonment of the convenient ‚Ñì2 norm and the
progression to robust statistics, G (x) = ŒªœÅ {Lx}. One of the most known priors
from this family is the total variation, G (x) = Œª‚à•|‚àáx| ‚à•1, suggested in the early
1990‚Äôs. [44].
Around the same time, the prior of Wavelet sparsity G (x) = Œª‚à•Wx‚à•1 started
evolving in the approximation theory and the harmonic analysis communities [11,
17, 38]. The idea behind this prior is that looking at the wavelet coeÔ¨Écients of an
image, they should be sparse - most of them should be zeros or very close to zero.
The prevailing model in the last decade, which is an evolution of all models
described so far, is the one involving sparse and redundant representations. Simi-
larly to the wavelets sparsity, this model assumes sparse coeÔ¨Écients, however, it no
longer relies on Ô¨Åxed transforms. In the reminder of these course, we will focus on
this model and show its applicability to image processing. Of course, this model is
not the last in the evolution, and in the coming years new models will be suggested,
and they will perform even better (hopefully).
2. The Sparseland prior
Our model (and any other model) represents a belief - how we believe an image
should look like, or what are the rules it obeys. The sparse and redundant repre-
sentation model believes that all images are generated by a ‚Äúmachine‚Äù that is able
to create ‚àön√ó‚àön images. Inside this machine there exists a dictionary Dn√ók, and
each of the columns of this dictionary is a prototype signal ‚Äì an atom.
Each time this machine is required to generate a signal, a vector Œ± is (randomly)
chosen. Not any vector can be selected, however, as this vector must be sparse -
it may only contain a small number L of non-zeros, at random locations and with
random values, which we will assume are Gaussian.
Then, the result signal is
created by multiplying the dictionary by the sparse vector of coeÔ¨Écients: x = DŒ±.

LECTURE 3. SPARSE AND REDUNDANT REPRESENTATION MODELLING
183
We call this machine Sparseland and we assume that all our signals in the family
of interest are created this way.
The Sparseland model has some interesting properties. It is simple, as any
signal generated is a linear combination of only a small number of atoms. Instead
of describing the signal using n numbers to represent the signal, we only need 2L
values ‚Äì the locations of the non-zeros and their values. These signals are thus very
compressible.
On the other hand, this source is also very rich. We have many (combinatorial)
ways to choose subsets of L atoms, and can therefore create a wealth of signals
using this model. As a matter of fact, the signals generated are a union of many
low-dimensional Gaussians.
The last reason, which is also the actual reason this model is indeed being used,
is that it is in fact familiar and has been used before, in other contexts and perhaps
in a simpliÔ¨Åed way, like wavelets and JPEG. This progress have been observed in
several trends recently. For example, the transition from JPEG to JPEG2000 is
essentially a move from the ‚Ñì2-norm and taking the leading coeÔ¨Écients, to the notion
of sparsity of wavelet coeÔ¨Écients. The progress from Wiener Ô¨Åltering (Fourier) to
robust estimation also promotes sparsity. Similar trend exists in the evolution of
wavelets, from unitary transforms to over-complete transforms with concepts and
tools such as frames, shift-invariance, steerable wavelets, contourlets, curvelets and
more, all point to the direction of redundancy in the representation [39, 10, 46, 45].
The ICA (independent component analysis) transform was also shown to be tied
to sparsity and independence between the coeÔ¨Écients.
Another Ô¨Åeld progressing towards an extensive usage of sparse and redundant
representation modeling is that of machine learning. Methods such as LDA, PCA,
SVM and feature-selection methods have all been extended or improved recently
by the introduction of sparse and redundant representations.
All of these directions clearly point to Sparseland. It is here, and we are going to
use it. In order to do that, however, we must deÔ¨Åne it clearly, analyze it, understand
it, and Ô¨Ånd out how it should be applied to the various tasks at hand.
One more thing we must take into account in our model is the model noise.
While we expect our signals to look like they have been created as DŒ± with a sparse
Œ±, we may want to allow for some perturbations from this exact model, with the
understanding that, just like any other model, this model is not perfect or exact.
For this purpose, we assume that signals are created as y = DŒ± + e, with e the
model noise with bounded energy ‚à•e‚à•2 ‚â§«´0.
3. Processing Sparseland signals
The model we will be considering in this section is the exact model, i.e. x = DŒ±.
We will assume from now on that there is no model noise, i.e., «´0 = 0. This does not
mean that we work only on clean signals; It only means that true signals can indeed
be created exactly by multiplying the dictionary by a sparse vector of coeÔ¨Écients.
Now, we ask ‚Äì How do we process such signals?
3.1. Transform
Suppose we have a Sparseland signal y, and we want to design a transform for it,
similarly to the wavelet transform or the Fourier transform. What would we like
such a transform to do?

184
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
First, we would like it to be invertible - we do not want to lose information by
applying the transformation. We would also like the transformation to be compact,
i.e., for its energy to be concentrated in a small number of coeÔ¨Écients. Lastly, we
would also want the coeÔ¨Écients to be independent of each other.
We can propose a very simple transformation that follows all these requirements
‚Äì let us transform y into the sparsest vector Œ± such that DŒ± = y,
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. DŒ± = y.
We know that such an Œ± indeed exists, making this transform possible, since it is
our model‚Äôs assumption that y was generated by a sparse vector multiplying the
dictionary.
As the dictionaries used are redundant, Œ± is usually longer than y, making the
transform suggested also redundant. It is also invertible, as multiplying D by Œ±
brings us back to our original signal. Lastly, we observe that the energy of the
transform is concentrated in only a few non-zero coeÔ¨Écients, as Œ± is sparse, and
those non-zeros are independent.
3.2. Compression
Suppose we now try to compress the Sparseland signal y. In order to allow com-
pression, we allow some error «´ between the compressed signal and the original one.
Changing this value will create diÔ¨Äerent rates of compression, thus creating the
rate-distortion curve for the compression scheme. The compression can be done by
solving
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ‚à•DŒ± ‚àíy‚à•2
2 ‚â§«´2,
which means searching for the smallest number of coeÔ¨Écients that can explain the
signal y with an error of less than «´. Note that the Ô¨Ånal rate-distortion curve is
obtained from taking into account the further error introduced by quantizing the
non-zero coeÔ¨Écients in Œ±.
3.3. Denoising
Given a noisy Sparseland signal z = y+v, created by the addition of additive noise
v (with known power ‚à•v‚à•2 = «´) to our original signal, we would like to recover y
as accurately as possible.
Since we know the signal y is sparse over the dictionary D and we expect the
noise to not be sparse over the same dictionary (since it is not a ‚Äúreal‚Äù image), the
following task can be proposed:
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ‚à•DŒ± ‚àíz‚à•2
2 ‚â§«´2,
with the Ô¨Ånal solution being ÀÜy = DÀÜŒ±. This denoising method works quite well in
practice, as we will see later on in the lectures.
There is a subtle point to address regarding denoising. Our model assumes
that the signals we operate on can be indeed represented exactly using a sparse
representation over the dictionary.
However, for real-world signals, this is not
exactly the case, and there are some model mismatches. The denoising algorithm
will remove these true details from the signal, along with the noise. This implies
that if the noise is very weak, such that its power is in the order of the model
mismatch, the denoising result might actually be worse than not denoising at all.
This is because the price of removing the model mismatch may be greater than the
price of not removing the noise. Usually, when the standard deviation of the noise

LECTURE 3. SPARSE AND REDUNDANT REPRESENTATION MODELLING
185
is 5 grey levels or more (when the image is in the range [0, 255]), it is considered to
be stronger than the model mismatches, and denoising the image indeed improves
its quality.
3.4. General inverse problems
As a generalization of the denoising problem, we now assume that the Sparseland
signal is given to us after a degradation operator H and an additive noise, z =
Hy + v.
The operator H can be any linear operator that can be written as a
matrix, representing blur, projection, down-scaling, masking and more.
We can try to recover y by looking for the sparsest Œ± that when multiplied by
the dictionary and then by H is close enough to the input signal z. This vector is
found by solving the minimization problem:
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ‚à•HDŒ± ‚àíz‚à•2
2 ‚â§«´2,
and again, the Ô¨Ånal solution is obtained by ÀÜy = DÀÜŒ±. The minimization is carried
out in practice by deÔ¨Åning the equivalent dictionary ÀúD = HD, and solving the
same minimization problem we have already encountered:
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ‚à•ÀúDŒ± ‚àíz‚à•2
2 ‚â§«´2,
There is a wide range of problems that can be solved using this mechanism: deblur-
ring, inpainting, demosaicing, super-resolution, tomographic reconstruction, image-
fusion and many more.
3.5. Compressed-sensing
Suppose that we want to sample the Sparseland signal y, such that we use much less
than n samples. There may be various reasons for this desire, such as the signal
being too long, having limited time to sample it, each sample being expensive,
and so on. Instead, we propose to sample noisy projections of it, using a known
projection matrix P, making our signal z = Py + v, with ‚à•v‚à•2 = «´ representing
the sampling noise. Again, we suggest to recover the original signal by solving
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ‚à•PDŒ± ‚àíz‚à•2
2 ‚â§«´2,
and multiplying the found vector of coeÔ¨Écients by the dictionary, ÀÜy = DÀÜŒ±. This
problem again is of the same form seen above, considering the eÔ¨Äective dictionary
as ÀúD = PD. The Compressed Sensing Ô¨Åeld deals with questions like the minimal
number of rows in P (the minimal number of projections) needed to recover the
original signal y with suÔ¨Écient accuracy, and what kind of P should be chosen.
3.6. Morphological-component-analysis
The MCA task [47, 48] deals with the case where a given signal z = y1 + y2 + v
is a mixture of two Sparseland signals y1 and y2, each created from a diÔ¨Äerent
dictionary D1 and D2 respectively, with noise also being added. The goal is to
recover each of the original signals y1 and y2, assuming their dictionaries D1 and
D2 are known.
Since both signals should have a sparse representations, it is suggested to solve
the minimization problem
ÀÜ
Œ±1, ÀÜŒ±2 = arg min
Œ±1,Œ±1 ‚à•Œ±1‚à•0
0 + ‚à•Œ±2‚à•0
0 s.t. ‚à•D1Œ±1 + D2Œ±2 ‚àíz‚à•2
2 ‚â§«´2,
and recover the original signals using ÀÜy1 = D1 ÀÜŒ±1 and ÀÜy2 = D2 ÀÜŒ±2.

186
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
By concatenating the vectors Œ±1 and Œ±2 vertically into a single vector Œ±, and
concatenating the two dictionaries by putting them side by side D = [D1D2], it
turns out that we have already seen this task:
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ‚à•DŒ± ‚àíz‚à•2
2 ‚â§«´2.
3.7. Applications: Summary
We have gone over several applications that can be solved using the Sparseland
model. There are many more that we can envision using this model ‚Äì encryption,
watermarking, scrambling, target detection, recognition, feature extraction and
more. What is common to all these problems is the need to solve a variant of
the problem (P «´
0),
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0
0 s.t. ‚à•DŒ± ‚àíz‚à•2
2 ‚â§«´2.
This is a problem we have already seen and discussed - we know it is NP-hard, but
we have seen several algorithms to approximate its solution and some guarantees on
the quality of their solutions. The conclusion is that signal processing for Sparseland
signals is reasonable and practical.

LECTURE 4
First steps in image processing
In this lecture we will start working on image processing applications that rely
on the Sparseland model.
We will consider image deblurring, image denoising,
and image inpainting. Several other, more advanced applications, using the tools
shown in the lecture, will be given in the following (and last) lecture. More on
these applications can be found in [22].
1. Image deblurring via iterative-shrinkage algorithms
1.1. DeÔ¨Åning the problem
The deblurring problem is one of the most fundamental problems in image pro-
cessing: A high-quality image x undergoes blurring, be it due to atmospheric blur,
camera blur, or any other possible source. We assume this blur is known (could
be space variant). We measure a noisy (white iid Gaussian) version of the blurred
image. The noise v has a known standard deviation. Thus, the image we actually
obtain y is related to the ideal image x through
(4.1)
y = Hx + v,
and our task is to recover x given y. Since we are dealing with the Sparseland
model, we assume that the image x has a sparse representation over a carefully
chosen dictionary, and therefore we will try to solve
(4.2)
ÀÜŒ± = arg min
Œ±
1
2‚à•HDŒ± ‚àíy‚à•2
2 + Œª ¬∑ œÅ (Œ±) .
Here, the function œÅ is sparsity promoting penalty, such as the ‚Ñì1-norm, or some-
thing similar (see the end of lecture 1). Since we are not interested in ÀÜŒ± in itself
but in the recovered image, we Ô¨Ånish the processing with ÀÜx = DÀÜŒ±.
Let us deÔ¨Åne the ingredients in Equation (4.2).
The vector y is the given
degraded image, and H is the known blur operator. The dictionary D should be
chosen by us. In the experiments we report here, we follow the work by Figueiredo
and Nowak [26, 27] and use the 2D un-decimated Haar wavelet transform with 2
resolution levels. This is a global dictionary with a redundancy factor of 7 : 1. As
for the function œÅ, we use an approximation of the ‚Ñì1 norm, with a slight smoothing,
(4.3)
œÅ (Œ±) = |Œ±| ‚àíS ¬∑ log

1 + Œ±
S

.
The function œÅ is a scalar function, operating on each entry of Œ± individually, and
summing the result over all entries. The beneÔ¨Åt of using this function rather than
the ‚Ñì1 norm directly is that it is smooth and derivable in the origin (and everywhere
else) and is therefore much more convenient to work with.
187

188
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
Lastly, the value of Œª is set manually to get the best possible result. There are
algorithms that can Ô¨Ånd its value for optimal performance automatically. However,
we will not discuss those here and proceed with the manually chosen value.
There is a wide knowledge in the area of continuous optimization, and it seems
like once we have written down our penalty function, we can use a wide variety
of methods or even general purpose optimization software packages (e.g., Matlab,
MOSEK) for its solution. However, such general purpose solvers typically perform
very poorly for this speciÔ¨Åc optimization task. One possible reason for this poor
performance is the fact that these solvers disregard the knowledge about the sparsity
of the desired solution. A second possible reason can be understood from look at
the gradient of our penalty function,
‚ñΩf (Œ±) = DT (DŒ± ‚àíy) + Œª ¬∑ œÅ‚Ä≤ (Œ±)
and the Hessian
‚ñΩ2f (Œ±) = DT D + Œª ¬∑ œÅ‚Ä≤‚Ä≤ (Œ±) .
We must remember that Œ± will contain many zero elements, and few non-zero
elements in unknown locations. In the locations of the zeros, the value of œÅ‚Ä≤‚Ä≤ will
be very large, while for the non-zeros, these will be small numbers. Therefore, the
Hessian‚Äôs main diagonal will have a high contrast of values, causing this matrix
to be of very high condition-number, hurting the performance of general-purpose
solvers. Furthermore, as we do not know in advance the locations of these non-zeros
(this is actually the goal of the optimization - recovering the support), we cannot
even pre-condition the Hessian. This forces us to seek for tailored optimization
methods that will take advantage of the expected sparsity in the solution.
How should we do that? In lecture 1 we have seen that when the matrix D is
unitary, the solution is obtained by a simple scalar-wise shrinkage step SœÅ,Œª
 DT y

.
When D is not unitary, we would like to propose a similarly simple solution. This
idea leads to a family of algorithms known as ‚ÄúIterative Shrinkage‚Äù [25]. There are
several such algorithms, and here we will present one of them.
1.2. The parallel coordinate descent (PCD) method
An interesting iterative shrinkage algorithm emerges from the classic coordinate
descent (CD) algorithm. Put generally, the CD operates as follows: Given a func-
tion to minimize with a vector as the unknown, all entries but one are Ô¨Åxed, and
the remaining entry is optimized for. Then, this coordinate is Ô¨Åxed, and a diÔ¨Äerent
entry is optimized. This is done serially over all entries, and in several iterations,
until convergence.
In our problem, solving for one entry at a time considers only the atom dj and
the coeÔ¨Écient Œ±j. Taking it out of the current support by setting Œ±j = 0, we get
that the current error in representation is ej = y ‚àíDŒ±. We now attempt to best
reduce this error using only the atom dj, by choosing the optimal value for Œ±j, i.e.,
minimizing
f (Œ±j) = 1
2‚à•Œ±j ¬∑ dj ‚àíej‚à•2
2 + ŒªœÅ (Œ±j) .
This is a scalar optimization problem that can be solved using shrinkage on the
scalar value dT
j ej. The shrinkage is applied by
Œ±OP T
j
= SœÅ,Œª/‚à•dj‚à•2  dT
j ej

,

LECTURE 4. FIRST STEPS IN IMAGE PROCESSING
189
where S denotes the shrinkage operation ‚Äì its shape is dictated by the choice of œÅ
and Œª/c. Notice that the shrinkage is done diÔ¨Äerently to each entry, based on the
norm of the j-th atom.
For low-dimensional problems (k ‚âà100) it makes sense to solve our problem
this way exactly. However, in the image deblurring problem, the unknown contains
many thousands entries, and it is impossible to use the above algorithm, as it
requires to extract one column at a time from the dictionary.
The solution to
this problem is the Parallel-Coordinate-Descent (PCD) algorithm.
The idea is
the following: Compute all the k descent trajectories {vj}k
j=1, each considering one
coordinate optimization. Since all are descent directions, their sum is also a descent
direction. The PCD takes this sum and uses this for the current iteration. The
problem with this direction is that it is not known how far to go along it. Therefore,
a 1D line-search is needed, and the overall iteration step is written as
(4.4)
Œ±k+1 = Œ±k + ¬µ ¬∑

SœÅ,QŒª
 QDT (yDŒ±k) ‚àíŒ±k

,
with Q = diag‚àí1  DT D

. ¬µ is the found step-size in the line-search step.
1.3. Results
Figure 1 shows the deblurring result obtained by the PCD algorithm after 10 iter-
ations. More details on this experiment are given in the accompanying slides.
Figure 1. Debluring. Left: The original image, Middle: the mea-
sured (blurred and noisy) image, and Right: the PCD deblurring
result.
2. Image denoising
The image denoising problem can be viewed as a special case of the image deblurring
problem discussed above. For this to be evident, all we need to do is select H = I
(i.e., no blur at all). In this section we shall present two very diÔ¨Äerent ways to
handle this problem.
2.1. Global shrinkage
Embarking from the above-described deblurring algorithm, we can suggest the fol-
lowing simpliÔ¨Åed denoising method: Instead of the iterative mechanism, we can
apply the thresholding algorithm, as follows
(4.5)
ÀÜx = DST
 DT y

.

190
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
Of course, since we are using the redundant Haar dictionary, we should normalize
its columns (since they are not ‚Ñì2 normalized) in order to have a single scalar
threshold for all atoms. Put diÔ¨Äerently, this leads to
(4.6)
ÀÜx = DWST
 W‚àí1DT y

.
The only missing ingredient is determining the value of the threshold T . Experi-
menting with this algorithm, diÔ¨Äerent values of T will have a diÔ¨Äerent quality of
denoising. As an example, on the well known image ‚Äúbarbara‚Äù with noise with
standard deviation œÉ = 20, the optimal value of T is around 55. The denoised
image using this method is more than 5 dB better in PSNR than the noisy image.
This result is shown in Figure 2.
Figure 2. Global denoising. Left: The original image, Middle:
the measured (noisy) image, and Right: the global denoising result.
2.2. From global to local processing
While the above result is good, it is deÔ¨Ånitely not the best that can be achieved.
We will now turn to a much better method (and see an even better one in the next
lecture).
When discussing algorithms for the solution of the (P «´
0) problem, we saw that
very large signals could pose a computational problem. Therefore, when we work
on the image as a whole, we are forced to use only simple algorithms, such as
(iterative) shrinkage. Also, when using dictionaries for an entire image, the result
may be sparse when compared to the overall number of atoms, but we still require
a large number of coeÔ¨Écients. These two observations lead us to attempt operating
on small image patches instead.
When processing a large image, we claim that every N √ó N patch (e.g., 8 √ó 8)
in it has a sparse representation over a dictionary. Furthermore, we use full over-
laps between the patches. By operating this way, we in fact force shift-invariance
sparsity. In order to write this formally, we deÔ¨Åne the operator Rij, which extracts
an N √ó N patch from the image around pixel (i, j). Now, the minimization task to
solve is [30, 31, 23]
(4.7)
ÀÜx = arg
min
x,{Œ±ij}ij
Ô£Æ
Ô£∞1
2‚à•x ‚àíy‚à•2
2 + ¬µ
X
ij
‚à•Rijx ‚àíDŒ±ij‚à•2
2
Ô£π
Ô£ªs.t. ‚à•Œ±ij‚à•0 ‚â§L.
The Ô¨Årst term requires a proximity between the recovered image and the input im-
age, a direct consequence of the log-likelihood. The second term requires that each

LECTURE 4. FIRST STEPS IN IMAGE PROCESSING
191
patch in the reconstructed image can be represented well as a linear combination of
atoms from the dictionary D. The constraint forces all of these linear combinations
to be sparse.
At this stage, the dictionary we will be using for this algorithm is the over-
complete DCT dictionary with 256 atoms, each representing an 8 √ó 8 patch. This
dictionary is separable, allowing it so be applied eÔ¨Éciently.
How can we perform the minimization in (4.7)?
We will take a coordinate
descent approach. First, we will Ô¨Åx x = y, and solve for {Œ±ij}ij. The Ô¨Årst term
disappears, and we are left with M ¬∑ N diÔ¨Äerent minimization tasks:
(4.8)
ÀÜŒ±ij = arg min
Œ± ‚à•Rijx ‚àíDŒ±‚à•s.t. ‚à•Œ±‚à•0 ‚â§L.
We have seen this problem before - this is the sparse coding problem, and can be
solved by any pursuit algorithm (e.g., OMP). Once we have found {Œ±ij}ij, we Ô¨Åx
them, and recover x. This is a least-squares problem and a little bit of algebra
leads to a closed-form solution
(4.9)
ÀÜx =
Ô£Æ
Ô£∞I + ¬µ
X
ij
RT
ijRij
Ô£π
Ô£ª
‚àí1 Ô£Æ
Ô£∞y + ¬µ
X
ij
RT
ijDÀÜŒ±ij
Ô£π
Ô£ª.
While this expression seems complicated, it is actually simple to implement. While
the operator Rij extracts a patch around the pixel (i, j), the operator RT
ij returns
a patch to the location (i, j). Therefore, the left matrix is in fact a diagonal one,
indicating for each pixel the amount of patches it belongs in, serving as normaliza-
tion.
Therefore, the entire denoising process is done by extracting each patch, per-
forming sparse coding, and getting a denoised patch instead. The denoised patch is
returned to its original position, with aggregation performed in the overlaps area.
Then, each pixel is normalized by the number of patches it belongs too (which is
generally N 2, apart from the borders of the image).
It turns out that this very simple method performs about 2.5dB better than
the global thresholding show previously. Furthermore, this algorithm can be im-
plemented very eÔ¨Éciently on parallel processors. We will see this local processing
approach in several more applications. The result obtained is shown in Figure 3.
Figure 3. Local denoising. Left: The original image, Middle: the
measured (noisy) image, and Right: the local denoising result.

192
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
3. Image inpainting
Assume that a signal has been created as x = DŒ±0 using a very sparse vector of
coeÔ¨Écients Œ±0. Unfortunately, we do not get x directly, but rather obtain Àúx, which
is the same signal x, but with some samples removed. We shall assume that we
know which samples are missing.
Each missing sample from x implies one lost
equation in the system x = DŒ±0. Therefore, we remove these corresponding rows
from the dictionary D, obtaining the dictionary ÀúD. Now, we may try to solve
(4.10)
ÀÜŒ± = arg min
Œ± ‚à•Œ±‚à•0 s.t. Àúx = ÀúDŒ±.
If Œ±0 was sparse enough, it will also be the solution of this modiÔ¨Åed problem. Then,
by computing DÀÜŒ± we will recover the original signal completely.
Using the above intuition, we now turn to extend the local denoising algorithm
into an inpainting one. The minimization task here is very similar to the one we
had for the denoising (4.7), with the needed modiÔ¨Åcation to take into account the
masking operator [30, 31, 36],
(4.11) ÀÜx = arg
min
x,{Œ±ij}ij
Ô£Æ
Ô£∞1
2‚à•Mx ‚àíy‚à•2
2 + ¬µ
X
ij
‚à•Rijx ‚àíDŒ±ij‚à•2
2
Ô£π
Ô£ªs.t. ‚à•Œ±ij‚à•0 ‚â§L,
where we now require that x is close to y only in the locations of un-masked pixels.
Again this minimization will be done using the coordinate descent approach.
First, we compute X = MT y (e.g., by zero Ô¨Ålling), and do sparse coding for each
patch, using the matching pursuit
(4.12)
ÀÜŒ±ij = arg min
Œ± ‚à•Mij (Rijx ‚àíDŒ±) ‚à•s.t. ‚à•Œ±‚à•0 ‚â§L,
with Mij the appropriate masking for each patch. Once the sparse coding is Ô¨Ån-
ished, the result image is again reconstructed by aggregating the patches in the
overlaps area, normalizing by the number of patches in each pixel,
(4.13)
ÀÜx =
Ô£Æ
Ô£∞MT M + ¬µ
X
ij
RT
ijRij
Ô£π
Ô£ª
‚àí1 Ô£Æ
Ô£∞MT y + ¬µ
X
ij
RT
ijDÀÜŒ±ij
Ô£π
Ô£ª.
Figure 4 presents inpainting results obtained using this algorithm.
Figure 4. Local inpainting. Left: The original image, Middle:
the measured image with 50% missing pixels, and Right: the in-
painting result.

LECTURE 4. FIRST STEPS IN IMAGE PROCESSING
193
4. Dictionary learning
4.1. General dictionary learning
Our entire model and applications rely on having a suitable dictionary. A good
dictionary would be one that sparsiÔ¨Åes our data. The question we aim look at now
is how to construct such dictionaries. We have already seen one possible solution
‚Äì we can choose a dictionary from an existing family of transforms, such as DCT,
curvelets, contourlets, wavelets and so on. This approach has its beneÔ¨Åts, as usually
these transforms can be applied rather eÔ¨Éciently. However, a dictionary selected
this way is typically not the best Ô¨Åt for the actual data.
A better approach is to train a dictionary from examples. The core idea is
to collect a set of representative examples of the images we work with, and train
a dictionary so that the examples can be sparsely represented over it.
Such a
dictionary will be better suited for our data, therefore leading to better performance
in various tasks.
Let us collect all the examples {xj} into a matrix X, where each column is one
example. Then, we can write the decomposition as X ‚âàDA, where each column
of A contains the coeÔ¨Écients for the representation of the corresponding example.
A good dictionary is therefore one that is able to adequately represent each of the
examples, while the representation for each example in indeed sparse, i.e., each
column in A has only a small number of non-zeros. Formally, we would like to
solve
(4.14)
ÀÜD, ÀÜA = arg min
D,A
P
X
j=1
‚à•DŒ±j ‚àíxj‚à•s.t. ‚àÄj, ‚à•Œ±j‚à•0 ‚â§L.
This problem will be handled as follows: The Ô¨Årst stage is sparse-coding, assuming
D is Ô¨Åxed, and only the representation vectors are sought. In this stage we should
solve for each example
(4.15)
min
Œ± ‚à•DŒ± ‚àíxj‚à•2
2 s.t. ‚à•Œ±‚à•0 ‚â§L.
We have seen in the Ô¨Årst lecture several ways to perform this minimization, via
pursuit algorithms. The second stage of dictionary training is the update of the
dictionary, once the representation vectors A have been recovered. There are two
main methods to do so: the Method of Optimal Directions (MOD) method [32]
and the K-SVD method [2].
4.2. MOD dictionary update
One option is to try and update the dictionary at once. The goal is minimizing
(4.16)
min
D ‚à•DA ‚àíX‚à•2
F ,
with X and A Ô¨Åxed. Note the use of the Frobenius norm, as both DA and X are
matrices. Few simple linear algebra steps lead to
(4.17)
D = XAT  AAT ‚àí1 = XA‚Ä†.
Once the dictionary is found this way, its columns need to be re-normalized.

194
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
4.3. K-SVD dictionary learning
Another option to train the dictionary is imitating the K-means algorithm in the
dictionary update stage.
This means that we process one atom at a time and
attempt to optimally replace it.
For processing the i-th atom, the Ô¨Årst stage is collecting only those examples
that use it into the matrix Xi. Then, the dictionary D and coeÔ¨Écient matrix A
are all Ô¨Åxed apart from the i-th column in D and the i-th row in A. Denoting
by Di and Ai these matrices with the i-th column/row removed respectively, the
following residual is computed
Ei = Xi ‚àíDiAi = Xi ‚àíDA + diŒ±i.
This is the residual matrix for all the examples that use the i-th atom, without
taking into account the i-th atom itself. The new atom is the one that is best able
to reduce the mean residual, i.e., the result of the minimization problem
(4.18)
min
di,Œ±i ‚à•Œ±idT
i ‚àíEi‚à•2
F .
The vector Œ±i is the optimal set of coeÔ¨Écients for this atom in each example. The
resulting atom ÀÜdi is inserted into the dictionary, and the new vector of coeÔ¨Écients
Œ±k replaces the i-th row in A (only in the columns relevant to the examples using
this atom).
The optimization task in (4.18) is in fact a rank-1 approximation, which can
be solved directly using the SVD decomposition. As every atom is updated using
an SVD operation, this algorithm is appropriately named K-SVD. Note, that in
the MOD training stage, the matrix of coeÔ¨Écients A is held constant, and the
dictionary D is adapted to it. In the K-SVD algorithm, on the other hand, the
atom update stage also optimizes for the k‚Äôth row of A at the same time.

LECTURE 5
Image processing - more practice
In the previous lecture we saw how to denoise and inpaint an image based on
local, patch-processing. However, in both this cases, we used a Ô¨Åxed dictionary ‚Äì the
local DCT. In this lecture we shall revisit these two applications, and incorporate
into them the K-SVD, in an attempt to improve their performance. We shall also
demonstrate the capabilities of the Sparseland model in handling image processing
tasks by considering two new applications ‚Äì an image scale-up process based on a
pair of trained dictionaries, and a facial image compression algorithm that performs
better than JPEG-2000. More details on the applications presented here can be
found in [22, 3].
1. Image denoising with a learned dictionary
When thinking about dictionary learning, both the MOD and the K-SVD are not
able to process signals with large dimensions. The reason is obvious ‚Äì a high dimen-
sional signal implies a dictionary held as a very large explicit matrix, intolerable
amounts of computations, both for the training and for using this dictionary, and
a huge amount of examples to learn from, in order to avoid over-Ô¨Åtting. Thus,
learned dictionaries are naturally used for low-dimensional signals.
We have already seen in the previous lecture that denoising (and inpainting)
can be done very eÔ¨Äectively by operating on small (e.g., 8 √ó 8) patches.
The
processing is then done on local blocks of images, using a dictionary that handles
low-dimensional signals. Formally, the denoising we have proposed is written as
(5.1)
ÀÜx = arg
min
x,{Œ±ij}ij
Ô£Æ
Ô£∞1
2‚à•x ‚àíy‚à•2
2 + ¬µ
X
ij
‚à•Rijx ‚àíDŒ±ij‚à•2
2
Ô£π
Ô£ªs.t. ‚à•Œ±ij‚à•0 ‚â§L.
The second term is the prior, expressing our belief that every patch in the image
should have a sparse representation over the dictionary D.
Our objective is to
introduce a learned dictionary into the above paradigm. One option is collecting a
set of high-quality images, extract patches from them, and use these examples to
train a dictionary. This dictionary will be a ‚Äúuniversal dictionary‚Äù, as it is trained
on general content images. This option is useful, and brings reasonably good results
- about 0.5-1dB below the state of the art.
An interesting alternative is to use patches from the corrupted image itself for
the training process. Suppose an image of 1000 √ó 1000 is to be denoised. Such an
image contains nearly 106 patches, which are more than enough to allow training
the dictionary. Furthermore, since the dictionary training process contains some
sort of averaging, the resulting dictionary will actually be (almost) noise free, and
can thus be used for denoising.
195

196
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
This option works much better than using the ‚Äúuniversal dictionary‚Äù, and leads
to state-of-the-art results [23]. In fact, going back to the penalty term in (5.1), we
can bring about this option by minimizing over D as well. Again, a coordinate
descent approach is used.
First, the image x = y is Ô¨Åxed, and the dictionary
D is Ô¨Åxed on some initialization (e.g., the redundant DCT). Each patch in the
image undergoes sparse coding.
Freezing the representations, the dictionary is
updated. All the patches then undergo sparse coding with the new dictionary, and
so. Iterating several times between sparse coding and dictionary update is exactly
the training procedure we have seen in the previous lecture, performed over the
patches in the image. After several such training iterations, the Ô¨Ånal dictionary
is used for one last sparse-coding stage.
Then, the sparse representations and
the dictionary are Ô¨Åxed, and the result image is recovered, using the same simple
aggregation method shown in the previous lecture,
(5.2)
ÀÜx =
Ô£Æ
Ô£∞I + ¬µ
X
ij
RT
ijRij
Ô£π
Ô£ª
‚àí1 Ô£Æ
Ô£∞y + ¬µ
X
ij
RT
ijDÀÜŒ±ij
Ô£π
Ô£ª.
Figure 1 presents the result obtained using this algorithm, for the same test shown in
Lecture 4. The obtained image has PSNR = 31dB, which is 1dB higher compared
to the DCT-based outcome.
Figure 1. Local denoising with a trained dictionary. Left: The
original image, Middle: the noisy image (œÉ = 20), and Right: the
denoising result.
This denoising algorithm has been applied to many variation of the denoising
problem. One is of course the denoising of gray-scale images. When published, this
method led to state-of-the-art results. Later, it was extended by Mairal et. al. by
using joint sparsity (i.e., sparse coding similar patches together) [37]. This method
is currently the best performing denoising algorithm.
The denoising algorithm could also be applied to denoising of color images.
Usually, there is a sensitivity in how the relationships between the color channels
should be handled. In this algorithm, the solution is simple ‚Äì the training is per-
formed on 8√ó8 √ó3 patches, i.e., on colored patches extracted from the image. The
dictionary contains color atoms, and the rest of the algorithm remains the same.
This method is also the state-of-the-art in color image denoising [36].
Video sequences can also be handled by this method. When processing image
sequences, the redundancy between frames can and should be used in order to
achieve better denoising. While most video denoising algorithms require motion

LECTURE 5. IMAGE PROCESSING - MORE PRACTICE
197
estimation (which is error-prone), the suggested method can simply train on space-
time patches extracted from the video. Therefore, the dictionary is also constructed
of ‚Äúmovielets‚Äù, as each atom is a very short, very small (e.g., 8 √ó 8 √ó 5) space-time
patch. Furthermore, since consecutive images are very similar, it is not necessary
to retrain a dictionary for each image. Instead, it can be propagated from one
frame to the next, only requiring Ô¨Åne tuning (1 K-SVD iteration) for each one.
This method‚Äôs performance is equivalent to the state-of-the-art [43].
2. Image inpainting with dictionary learning
Turning to image inpainting, we would like to apply dictionary learning here as
well, for handling the local patches. The process is very similar to the one done
in denoising. The algorithm starts with dictionary training using the K-SVD, by
performing alternating steps between sparse coding and dictionary update. During
the sparse coding stage, only the un-masked (i.e., known) pixels in each patch
are used. This is similar to the sparse coding undertaken in the Ô¨Åxed dictionary
scenario. Once the sparse coding is done, each atom is updated using the SVD
operation, just like the K-SVD method.
This dictionary training is repeated for several (e.g., 10) iterations. Then, the
image is reconstructed from the Ô¨Ånal sparse coding using the same formula we have
seen:
(5.3)
ÀÜx = arg
min
x,{Œ±ij}ij
Ô£Æ
Ô£∞1
2‚à•Mx ‚àíy‚à•2
2 + ¬µ
X
ij
‚à•Rijx ‚àíDŒ±ij‚à•2
2
Ô£π
Ô£ªs.t. ‚à•Œ±ij‚à•0 ‚â§L,
Of course, just like in the denoising case, this method can be easily generalized
to inpainting of color images and inpainting of video sequences using the same
extensions as in the denoising case. Figure 2 presents the results obtained with this
algorithm, showing the beneÔ¨Åt of using learned dictionary.
This approach to inpainting works well when the missing pixels are in (near-
random) locations, or organized in small groups. If the missing pixels form a large
area to reconstruct, the local approach will not work, and an alternative, global,
approach will be needed.
Another problem could arise if the missing pixels create a periodic mask, for
example, when every other pixel is missing. In this case, the dictionary will not
be able to learn the relationship between neighboring pixels, because it has no
information about such neighbors. Such is the case, for example, in image single-
image scale-up and demosaicing. The periodic nature of the mask pattern may cause
the learned dictionary to absorb this pattern into its atoms. In order to prevent
that, the dictionary should trained while allowing only a very small number of non-
zeros, and furthermore, only few training iterations in order to avoid over-Ô¨Åtting.
Such a solution leads to state of the art results, which can be seen in [36].
3. Image scale-up with a pair of dictionaries
We turn to describe an image scale-up algorithm that uses trained dictionaries.
The method described here follows the work of Yang et. al. [54], with several
important modiÔ¨Åcations that lead to a simpler and more eÔ¨Écient algorithm, with
better results [22].

198
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
Figure 2. Image inpainting with the K-SVD. Top-Left: The orig-
inal image, Top-Right: the measured image (masked with missing
pixels), Bottom Left: DCT-based inpainting, and Bottom Right:
the proposed K-SVD inpainting result.
3.1. Preliminaries
The image scale-up problem is deÔ¨Åned as follows. A high-quality (high-resolution)
image yh is blurred, decimated, and contaminated by additive noise, obtaining the
image zl,
(5.4)
zl = SHyl + v,
with H a blur operation, S the decimation operator, and v the noise. We assume
that the decimation S and blur H operators, and the statistical properties of the
noise v, are known.
We would like to reverse this process and recover the high quality image from
the given low quality one. This is a severely ill-posed inverse problem, and therefore
we shall need an image model for its regularization ‚Äì we shall use the Sparseland
model.
In order to simplify notations, we will assume the the low resolution image
has undergone simple interpolation (such as bicubic) to the same size as the target
image
(5.5)
yl = Qzl,

LECTURE 5. IMAGE PROCESSING - MORE PRACTICE
199
with Q being the interpolation operator. This initialization step will save us the
technicalities of handling two scales in our formulation.
The algorithm we suggest relies on two core ideas. The Ô¨Årst is operating locally,
in the sense that each patch in the low-quality image will undergo resolution en-
hancement on its own. Then, the improved patches will be merged (using averaging
in the overlap areas, just as in the previously described denoising and inpainting)
in order to create the Ô¨Ånal output image.
How each patch is enhanced? This enhancement requires two dictionaries, Al
and Ah for the low and high quality patches respectively. Each atom in Ah should
be the high-quality counterpart of the corresponding low quality atom in Al. In
order to enhance a low-quality patch, its sparse representation over the low-quality
dictionary Al is found. This sparse representation is then used to reconstruct a
patch from the high-quality dictionary, which is the enhancement result for the
low-quality patch. We will now go into further details of this algorithm, and focus
on learning these two dictionaries.
3.2. The core idea
As we have been practicing throughout this course, we assume that every ‚àön√ó‚àön
patch extracted from location k in the high-quality image through pk
h = Rkyh,
can be represented sparsely over a high-quality dictionary Ah, i.e., for every patch
pk
h we assume that there exists a very sparse representation vector qk such that
pk
h ‚àº= Ahqk.
Next, we aim to Ô¨Ånd a connection between a high-quality patch in yh and a
patch in the same location in yl. We can use our knowledge of the creation process
of zl from yh through blur, decimation and the addition of noise, with interpolation
used to create yl. Therefore,
(5.6)
yl = Q (DHyh + v) .
Combining all operators into one LALL = QDH, we come to the relationship for
each patch
(5.7)
‚à•Lpk
h ‚àípk
l ‚à•2 ‚â§«´,
with L a local portion of LALL. This expression ties the low-quality patch and the
high-quality patch in the same location. Using our assumption that the high quality
patch can be sparsely represented over the high quality dictionary pk
h ‚àº= Ahqk, we
obtain the relationship ‚à•LAhqk ‚àípk
l ‚à•2 ‚â§«´. DeÔ¨Åning a new dictionary, Al = LAh,
we obtain
(5.8)
‚à•Alqk ‚àípk
l ‚à•2 ‚â§«´.
This observation leads to the conclusion that the sparse representation qk is also
the representation of the low quality patch pk
l over the low quality dictionary Al.
3.3. Dictionary training
Since we want to use trained dictionaries, the obvious question is what do we train
them on. One option is obtaining one (or more) high quality images. The high-
quality image is then blurred and down-sampled it using the known (expected)
operators. The low-resolution image is then interpolated back to the size of the
high-quality image.
Then, all the patches from each image are extracted, with
patches in the same location in the original and interpolated image being paired.

200
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
Finally, each patch undergoes some pre-processing steps (diÔ¨Äerent for the high-
quality and low-quality patches), to form our Ô¨Ånal corpus for training. More on
this pre-processing can be found in [22].
An alternative to using a pre-determined image for training, we can attempt
training on the input image, as was done for denoising and inpainting and shown
to improve results.
However, we only have the low-quality image, so it seems
something is missing. To our aid will come to observation that images tend to have
a multi-resolution nature. If we consider an image pyramid, the characterizations
of the local relationship between two consecutive levels in the pyramid is relatively
the same in all levels.
From this observation comes the bootstrapping approach. The input image is
blurred, decimated and then interpolated to create an even lower quality image.
Now, we use the pair of low-quality and lowest-quality images as the basis for
the training of the high and low quality dictionaries respectively. Once these two
dictionaries are trained, the algorithm proceeds regardless of the training corpus.
Once the training patches have been collected and prepared, dictionary training
can begin. The Ô¨Årst step is training the low-quality dictionary from the set of low-
quality patches. This training is done using the K-SVD algorithm. The next step is
training the high-quality dictionary. We must remember that the two dictionaries
should align with each other, such that a high-quality patch and its low-quality
counterpart have the same representation over their dictionaries. This means that
given a low quality patch pk
l with a representation vector qk, it should also be the
representation vector of the high-quality patch pk
h. Therefore, it makes sense to use
the high-quality patches to train the high-quality dictionary such that this scale-up
process gets as close as possible to its objective. Formally, we should minimize
(5.9)
min
Ah
X
k
‚à•pk
h ‚àíAlqk‚à•2
2.
This is a simple least-squares problem to solve, and it results in a dictionary that
is coupled with the low-quality dictionary that was already trained.
Once both dictionaries have been trained, we are now armed with all that we
need to recover a high-resolution image given a low-resolution input. Note that this
training algorithm may be done oÔ¨Ä-line on a high-quality image (which we expect
to be similar to the image we would later enhance) or online, on the input image
itself.
Figure 3 presents an example for the results obtained by this algorithm. The
Ô¨Ågure shows a high-resolution image and its scaled-down version. The scaling-up
process aims to return to the original image ‚Äì we provide two possible results,
one obtained by the plain bicubic interpolation, and the second using the above-
described algorithm.
4. Image compression using sparse representation
Compressing images in one of the most researched Ô¨Åelds in image processing, be-
cause of its importance to the ability to transfer/store images easily while retaining
as much of their quality as possible. Trained dictionaries that lead to the spars-
est representations seem ideal for compression. However, there is a diÔ¨Éculty with
training a global dictionary for all images, as it is expected to be less eÔ¨Äective. The
alternative, training a dictionary tailored for the speciÔ¨Åc given image, as practiced

LECTURE 5. IMAGE PROCESSING - MORE PRACTICE
201
Figure 3. Image Scale-Up. Top-Left: The original image, Top-
Right: the measured image (blurred, decimated, and noisy), Bot-
tom Left: Bicubic scaled-up image, and Bottom Right: the pro-
posed scale-up result.
in denoising and inpainting, is also diÔ¨Écult, as it requires allocated bits for the
dictionary, which may compromise the success of the compression algorithm.
When turning to a speciÔ¨Åc family of images, the story changes, as it is now
possible to tailor speciÔ¨Åc dictionaries to the task, while not actually needing to
send them. One example for such a problem is the compression of high-quality
(e.g., 500 √ó 400) facial ID images, to put on biometric identity cards or credit
cards. It is possible to use general purpose compression algorithms, such as JPEG
and JPEG2000 for this purpose, but a speciÔ¨Åcally trained algorithm is expected to
do much better.
The proposed algorithm we describe here [4] has two stages ‚Äì Ô¨Årst, training
appropriate dictionaries, and then, applying the trained dictionaries to compress
(and decompress) the facial images.
4.1. Facial image compression algorithm - training
For training, a set of high-quality facial images are collected. This set is then used
for training a dictionary for each portion of the image. The stages are as following:

202
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
(1) Key points in each image are detected (e.g., eyes, nose). These locations
are used to transform each image (using a piece-wise aÔ¨Éne transform) such
that these key points are in exactly the same coordinates in all images.
(2) Each image is divided into non-overlapping 15 √ó 15 patches. Overlaps are
avoided as redundancy will only worsen the compression rate.
(3) All patches in the same location are collected into a set.
(4) The mean of each set of patches is computed, stored, and removed from
each patch.
(5) For each set, a dictionary is trained, either using a linear approximation
(i.e., PCA) or non-linear approximation (i.e., K-SVD).
(6) Each patch location is assigned a constant number of atoms to use, and
quantization levels to use, both varying from location to location. This
assignment is done by minimizing the mean error on the training set while
achieving the desired compression rate.
Once the training procedure ends, for each patch location, its mean patch,
dictionary, number of allotted atoms and quantization levels are stored both in the
encoder and decoder.
4.2. Compressing and decompressing
When a new image is to be compressed, it undergoes the following stages (similar
to the training stage):
(1) The same key points in the image are detected and the image is warped.
(2) The (warped) image is divided into non-overlapping 15 √ó 15 patches.
(3) For each patch, the mean of this location is removed.
(4) Sparse coding over this location‚Äôs dictionary (or projection onto the PCA)
is carried out, using the number of atoms allotted to this location.
(5) The values of the coeÔ¨Écients selected are quantized.
(6) Entropy coding is used to further compress the sent values.
Note that if K-SVD is used, when sending the atoms‚Äô coeÔ¨Écients their indices
are sent as well, while in PCA, only the coeÔ¨Écients are sent. The recovery algorithm
is the exact opposite. Entropy de-coding is performed on the sent data, and each
patch is then reconstructed from the send coeÔ¨Écients (and indices if K-SVD is
used). The inverse transform is applied to the image, returning to an image that is
as close as possible to the original. Figure 4 presents the obtained results for 550
Bytes per image.
5. Summary
The Ô¨Åeld of sparse and redundant representation modeling oÔ¨Äers a new model, and
this can be brought to various problems in image processing, often leading to state-
of-the-art results. We have seen in these lecture-notes a partial list of these success
stories, and many more have been introduced to the scientiÔ¨Åc community in the
past few years. More applications, and more details in general about this Ô¨Åeld and
its impact to image processing, can be found in my book:
M. Elad, Sparse and Redundant Representations:
From Theory to Applications in Signal and Image
Processing, Springer, New-York, 2010.

LECTURE 5. IMAGE PROCESSING - MORE PRACTICE
203
Figure 4. Facial image compression. From left to right: the orig-
inal image, JPEG results, JPEG-2000 results, local-PCA, and K-
SVD. The numbers represent the mean-squared-error per pixel af-
ter compression and decompression with 550 Bytes.


Bibliography
[1] Z. Ben-Haim, Y.C. Eldar, and M. Elad, Coherence-based performance guarantees
for estimating a sparse vector under random noise, to appear in IEEE Trans. on
Signal Processing.
[2] M. Aharon, M. Elad, and A.M. Bruckstein. K-SVD: An algorithm for designing of
overcomplete dictionaries for sparse representation, IEEE Trans. on Signal Pro-
cessing, 54(11):4311‚Äì4322, November 2006.
[3] A.M. Bruckstein, D. L. Donoho, and M. Elad, From sparse solutions of systems
of equations to sparse modeling of signals and images, SIAM Review, 51(1):34-81,
2009.
[4] O. Bryt and M. Elad, Compression of facial images using the K-SVD algorithm,
Journal of Visual Communication and Image Representation, 19(4):270‚Äì283, May
2008.
[5] E.J. Cand`es, J. Romberg, and T. Tao, Robust uncertainty principles: Exact signal
reconstruction from highly incomplete frequency information, IEEE Trans. Inform.
Theory, 52:489-509, 2006.
[6] E.J. Cand`es and T. Tao, The Dantzig selector: Statistical estimation when p is
much larger than n, Annals of Statistics, 35(6):2313‚Äì2351, June 2007.
[7] S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis pur-
suit, SIAM Journal on ScientiÔ¨Åc Computing, 20(1):33‚Äì61 (1998).
[8] I. Daubechies, M. Defrise, and C. De-Mol, An iterative thresholding algorithm for
linear inverse problems with a sparsity constraint, Communications on Pure and
Applied Mathematics, LVII:1413‚Äì1457, 2004.
[9] G. Davis, S. Mallat, and M. Avellaneda, Adaptive greedy approximations, Journal
of Constructive Approximation, 13:57‚Äì98, 1997.
[10] M.N. Do and M. Vetterli, The contourlet transform:
an eÔ¨Écient directional
multiresolution image representation, IEEE Trans. Image on Image Processing,
14(12):2091‚Äì2106, 2005.
[11] D.L. Donoho, De-noising by soft thresholding, IEEE Trans. on Information The-
ory, 41(3):613‚Äì627, 1995.
[12] D.L. Donoho, For most large underdetermined systems of linear equations, the
minimal ‚Ñì1-norm solution is also the sparsest solution, Communications On Pure
And Applied Mathematics, 59(6):797‚Äì829, June 2006.
[13] D.L. Donoho, For most large underdetermined systems of linear equations, the
minimal ‚Ñì1-norm near-solution approximates the sparsest near-solution, Commu-
nications On Pure And Applied Mathematics, 59(7):907‚Äì934, July 2006.
[14] D.L. Donoho and M. Elad, Optimally sparse representation in general (non-
orthogonal) dictionaries via l1 minimization, Proc. of the National Academy of
Sciences, 100(5):2197‚Äì2202, 2003.
[15] D.L. Donoho and M. Elad, On the stability of the basis pursuit in the presence of
noise, Signal Processing, 86(3):511‚Äì532, March 2006.
[16] D.L. Donoho, M. Elad, and V. Temlyakov, Stable recovery of sparse overcomplete
representations in the presence of noise, IEEE Trans. On Information Theory,
52(1):6‚Äì18, 2006.
[17] D.L. Donoho and I.M. Johnstone, Ideal denoising in an orthonormal basis chosen
from a library of bases, Comptes Rendus del‚ÄôAcademie des Sciences, Series A,
319:1317‚Äì1322, 1994.
205

206
MICHAEL ELAD, SPARSE REPRESENTATION MODELLING
[18] D.L. Donoho and J. Tanner, Neighborliness of randomly-projected Simplices in
high dimensions, Proceedings Of The National Academy Of Sciences, 102(27):9452‚Äì
9457, July 2005.
[19] D.L. Donoho and J. Tanner, Sparse nonnegative solutions of underdetermined lin-
ear equations by linear programming, Proc. Natl. Acad. Sci., 102:9446-9451 , 2005.
[20] B. Efron, T. Hastie, I.M. Johnstone, and R. Tibshirani, Least angle regression, The
Annals of Statistics, 32(2):407‚Äì499, 2004.
[21] M. Elad, Why simple shrinkage is still relevant for redundant representations?, to
appear in the IEEE Trans. On Information Theory.
[22] M. Elad, Sparse and Redundant Representations: From Theory to Applications in
Signal and Image Processing, Springer New-York, 2010.
[23] M. Elad and M. Aharon, Image denoising via sparse and redundant representations
over learned dictionaries, IEEE Trans. on Image Processing 15(12):3736‚Äì3745, De-
cember 2006.
[24] M. Elad, B. Matalon, and M. Zibulevsky, Coordinate and subspace optimization
methods for linear least squares with non-quadratic regularization, Applied and
Computational Harmonic Analysis, 23:346‚Äì367, November 2007.
[25] M. Elad and M. Zibulevsky, Iterative shrinkage algorithms and their acceleration for
l1-l2 signal and image processing applications‚Äù, IEEE Signal Processing Magazine,
27(3):78‚Äì88, May 2010.
[26] M.A. Figueiredo and R.D. Nowak, An EM algorithm for wavelet-based image
restoration, IEEE Trans. Image Processing, 12(8):906‚Äì916, 2003.
[27] M.A. Figueiredo, and R.D. Nowak, A bound optimization approach to wavelet-
based image deconvolution, IEEE International Conference on Image Processing -
ICIP 2005, Genoa, Italy, 2:782‚Äì785, September 2005.
[28] I.F. Gorodnitsky and B.D. Rao, Sparse signal reconstruction from limited data
using FOCUSS: A re-weighted norm minimization algorithm, IEEE Trans. On
Signal Processing, 45(3):600‚Äì616, 1997.
[29] R. Gribonval and M. Nielsen, Sparse decompositions in unions of bases, IEEE
Trans. on Information Theory, 49(12):3320‚Äì3325, 2003.
[30] O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising - Part I: Theory, IEEE Trans. on
Image Processing, 15(3):539‚Äì554, 2006.
[31] O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising - Part II: Adaptive algorithms, IEEE
Trans. on Image Processing, 15(3):555‚Äì571, 2006.
[32] K. Kreutz-Delgado, J.F. Murray, B.D. Rao, K. Engan, T-W, Lee, and T.J. Se-
jnowski, Dictionary learning algorithms for sparse representation, Neural Compu-
tation, 15(2)349‚Äì396, 2003.
[33] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, A method for large-
scale ,1-regularized least squares problems with applications in signal processing
and statistics, IEEE J. Selected Topics Signal Processing, 1(4):606-617, Dec. 2007.
[34] J.B. Kruskal, Three-way arrays: rank and uniqueness of trilinear decompositions,
with application to arithmetic complexity and statistics, Linear Algebra and its
Applications, 18(2):95‚Äì138, 1977.
[35] X. Liu and N.D. Sidiropoulos, Cramer-Rao lower bounds for low-rank decompo-
sition of multidimensional arrays, IEEE Trans. on Signal Processing, 49(9):2074‚Äì
2086, 2001.
[36] J. Mairal, M. Elad, and G. Sapiro, Sparse representation for color image restoration,
IEEE Trans. on Image Processing, 17(1):53‚Äì69, January 2008.
[37] J. Mairal, F. Bach, J. Ponce, G. Sapiro and A. Zisserman, Non-local sparse mod-
els for image restoration, International Conference on Computer Vision (ICCV),
Tokyo, Japan, 2009.
[38] S. Mallat, A Wavelet Tour of Signal Processing, Academic-Press, Second-Edition,
2009.
[39] S. Mallat and E. LePennec, Sparse geometric image representation with bandelets,
IEEE Trans. on Image Processing, 14(4):423‚Äì438, 2005.

207
BIBLIOGRAPHY
[40] S. Mallat and Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE
Trans. Signal Processing, 41(12):3397‚Äì3415, 1993.
[41] B.K. Natarajan, Sparse approximate solutions to linear systems, SIAM Journal on
Computing, 24:227‚Äì234, 1995.
[42] Y.C. Pati, R. Rezaiifar, and P.S. Krishnaprasad, Orthogonal matching pursuit:
recursive function approximation with applications to wavelet decomposition, the
twenty seventh Asilomar Conference on Signals, Systems and Computers, 1:40‚Äì44,
1993.
[43] M. Protter and Michael Elad, Image sequence denoising via sparse and redundant
representations, IEEE Trans. on Image Processing, 18(1):27‚Äì36, January 2009.
[44] L. Rudin, S. Osher, and E. Fatemi, Nonlinear total variation based noise removal
algorithms, Physica D, 60:259‚Äì268, 1992
[45] E.P. Simoncelli, W.T. Freeman, E.H. Adelsom, and D.J. Heeger, Shiftable multi-
scale transforms, IEEE Trans. on Information Theory, 38(2):587‚Äì607, 1992.
[46] J.-L. Starck, E.J. Cand`es, and D.L. Donoho, The curvelet transform for image
denoising, IEEE Trans. on Image Processing, 11:670‚Äì684, 2002.
[47] J.-L. Starck, M. Elad, and D.L. Donoho, Redundant multiscale transforms and
their application for morphological component separation, Advances in Imaging
And Electron Physics, 132:287‚Äì348, 2004.
[48] J.-L. Starck, M. Elad, and D.L. Donoho, Image decomposition via the combina-
tion of sparse representations and a variational approach. IEEE Trans. On Image
Processing, 14(10):1570‚Äì1582, 2005.
[49] T. Strohmer and R. W. Heath, Grassmannian frames with applications to coding
and communication, Applied and Computational Harmonic Analysis, 14:257‚Äì275,
2004.
[50] J.A. Tropp, Greed is good: Algorithmic results for sparse approximation, IEEE
Trans. On Information Theory, 50(10):2231‚Äì2242, October 2004.
[51] J.A. Tropp, Just relax: Convex programming methods for subset selection and
sparse approximation, IEEE Trans. On Information Theory, 52(3):1030‚Äì1051,
March 2006.
[52] J.A. Tropp, I.S. Dhillon, R.W. Heath Jr., and T. Strohmer, Designing struc-
tured tight frames via alternating projection, IEEE Trans. Information Theory,
51(1):188‚Äì209, January 2005.
[53] J.A. Tropp and A.A. Gilbert, Signal recovery from random measurements via or-
thogonal matching pursuit, Submitted for publication, April 2005.
[54] J. Yang, J. Wright, T. Huang, and Y. Ma, Image super-resolution via sparse rep-
resentation, to appear in PIEEE Trans. on Image Processing.


