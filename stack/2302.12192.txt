Aligning Text-to-Image Models using Human Feedback
Kimin Lee 1 Hao Liu 2 Moonkyung Ryu 1 Olivia Watkins 2 Yuqing Du 2
Craig Boutilier 1 Pieter Abbeel 2 Mohammad Ghavamzadeh 1 Shixiang Shane Gu 1
Abstract
Deep generative models have shown impressive
results in text-to-image synthesis. However, cur-
rent text-to-image models often generate images
that are inadequately aligned with text prompts.
We propose a ﬁne-tuning method for aligning
such models using human feedback, comprising
three stages. First, we collect human feedback
assessing model output alignment from a set of
diverse text prompts. We then use the human-
labeled image-text dataset to train a reward func-
tion that predicts human feedback. Lastly, the
text-to-image model is ﬁne-tuned by maximizing
reward-weighted likelihood to improve image-
text alignment. Our method generates objects
with speciﬁed colors, counts and backgrounds
more accurately than the pre-trained model. We
also analyze several design choices and ﬁnd that
careful investigations on such design choices
are important in balancing the alignment-ﬁdelity
tradeoffs. Our results demonstrate the potential
for learning from human feedback to signiﬁcantly
improve text-to-image models.
1. Introduction
Deep generative models have recently shown remark-
able success in generating high-quality images from text
prompts (Ramesh et al., 2021; 2022; Saharia et al., 2022;
Yu et al., 2022b; Rombach et al., 2022). This success has
been driven in part by the scaling of deep generative models
to large-scale datasets from the web such as LAION (Schuh-
mann et al., 2021; 2022).
However, major challenges
remain in domains where large-scale text-to-image mod-
els fail to generate images that are well-aligned with text
prompts (Feng et al., 2022; Liu et al., 2022a;b). For in-
stance, current text-to-image models often fail to produce
reliable visual text (Liu et al., 2022b) and struggle with
compositional image generation (Feng et al., 2022).
1Google Research 2University of California, Berkeley. Corre-
spondence to: Kimin Lee <kiminl@google.com>.
In language modeling, learning from human feedback has
emerged as a powerful solution for aligning model behavior
with human intent (Ziegler et al., 2019; Stiennon et al., 2020;
Wu et al., 2021; Nakano et al., 2021; Ouyang et al., 2022;
Bai et al., 2022a). Such methods ﬁrst learn a reward function
intended to reﬂect what humans care about in the task, using
human feedback on model outputs. The language model
is then optimized using the learned reward function by a
reinforcement learning (RL) algorithm, such as proximal
policy optimization (PPO; Schulman et al. 2017). This RL
with human feedback (RLHF) framework has successfully
aligned large-scale language models (e.g., GPT-3; Brown
et al. 2020) with complex human quality assessments.
Motivated by the success of RLHF in language domains,
we propose a ﬁne-tuning method for aligning text-to-image
models using human feedback. Our method consists of the
following steps illustrated in Figure 1: (1) We ﬁrst generate
diverse images from a set of text prompts designed to test
output alignment of a text-to-image model. Speciﬁcally,
we examine prompts where pre-trained models are more
prone to errors – generating objects with speciﬁc colors,
counts, and backgrounds. We then collect binary human
feedback assessing model outputs. (2) Using this human-
labeled dataset, we train a reward function to predict human
feedback given the image and text prompt. We propose an
auxiliary task—identifying the original text prompt within
a set of perturbed text prompts—to more effectively ex-
ploit human feedback for reward learning. This technique
improves the generalization of reward function to unseen
images and text prompts. (3) We update the text-to-image
model via reward-weighted likelihood maximization to bet-
ter align it with human feedback. Unlike the prior work (Sti-
ennon et al., 2020; Ouyang et al., 2022) that uses RL for
optimization, we update the model using semi-supervised
learning to measure model-output quality w.r.t. the learned
reward function.
We ﬁne-tune the stable diffusion model (Rombach et al.,
2022) using 27K image-text pairs with human feedback.
Our ﬁne-tuned model shows improvement in generating ob-
jects with speciﬁed colors, counts, and backgrounds. More-
over, it improves compositional generation (i.e., can bet-
arXiv:2302.12192v1  [cs.LG]  23 Feb 2023

Aligning Text-to-Image Models using Human Feedback
Step 1. Collecting human data
Step 2. Learning reward function
Step 3. Updating text-to-image model
Text-to-Image Model
Green dog
Red dog
Two dogs
One dog
(a) Predicting human feedback
(b) Auxiliary objective: prompt classification
Green dog
Reward 
model
1
Reward 
model
0
A 
Green dog
Reward 
model
B 
Red dog
C
Blue dog
D 
Purple dog
A
<
B
C
D
Green dog
Reward model
Green dog
Green dog
Green dog
Green dog
0.9
0.0
0.9
0.7
Reward-weighted likelihood 
maximization
Figure 1. The steps in our ﬁne-tuning method. (1) Multiple images sampled from the text-to-image model using the same text prompt,
followed by collection of (binary) human feedback. (2) A reward function is learned from human assessments to predict image-text
alignment. We also utilize an auxiliary objective called prompt classiﬁcation, which identiﬁes the original text prompt within a set of
perturbed text prompts. (3) We update the text-to-image model via reward-weighted likelihood maximization.
ter generate unseen objects1 given unseen combinations of
color, count, and background prompts). We also observe
that the learned reward function is better aligned with human
assessments of alignment than CLIP score (Radford et al.,
2021) on tested text prompts. We analyze several design
choices, such as using an auxiliary loss for reward learning
and the effect of using “diverse” datasets for ﬁne-tuning.
We can summarize our main contributions as follows:
• We propose a simple yet efﬁcient ﬁne-tuning method for
aligning a text-to-image model using human feedback.
• We show that ﬁne-tuning with human feedback signif-
icantly improves the image-text alignment of a text-to-
image model. On human evaluation, our model achieves
up to 47% improvement in image-text alignment at the
expense of mildly degraded image ﬁdelity.
• We show that the learned reward function predicts human
assessments of the quality more accurately than the CLIP
score (Radford et al., 2021). In addition, we show that
rejection sampling based on our learned reward function
can also signiﬁcantly improve the image-text alignment.
• Naive ﬁne-tuning with human feedback can signiﬁcantly
reduce the image ﬁdelity, despite better alignment. We
ﬁnd that careful investigations on several design choices
are important in balancing alignment-ﬁdelity tradeoffs.
1We use the term “unseen objects” w.r.t. our human dataset,
i.e., they are not included in the human dataset but pre-training
dataset would contain them.
Even though our results do not address all the failure modes
of the existing text-to-image models, we hope that this work
highlights the potential of learning from human feedback
for aligning these models.
2. Related Work
Text-to-image models.
Various deep generative models,
such as variational auto-encoders (Kingma & Welling,
2013), generative adversarial networks (Goodfellow et al.,
2020), auto-regressive models (Van Den Oord et al., 2016),
and diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,
2020) have been proposed for image distributions. Com-
bined with the large-scale language encoders (Radford et al.,
2021; Raffel et al., 2020), these models have shown impres-
sive results in text-to-image generation (Ramesh et al., 2022;
Saharia et al., 2022; Yu et al., 2022b; Rombach et al., 2022).
However, text-to-image models frequently struggle to gen-
erate images that are well-aligned with text prompts (Feng
et al., 2022; Liu et al., 2022a;b). Liu et al. (2022b) show
that current models fail to produce reliable visual text and
often perform poorly w.r.t. compositional generation (Feng
et al., 2022; Liu et al., 2022a). Several techniques, such
as character-aware text encoders (Xue et al., 2022; Liu
et al., 2022b) and structured representations of language
inputs (Feng et al., 2022) have been investigated to ad-
dress these issues. We study learning from human feedback,
which aligns text-to-image models directly using human
feedback on model outputs.

Aligning Text-to-Image Models using Human Feedback
Fine-tuning with few images (Ruiz et al., 2022; Kumari
et al., 2022; Gal et al., 2022) for personalization of text-
to-image diffusion models is also related with our work.
DreamBooth (Ruiz et al., 2022) showed that text-to-image
models can generate diverse images in a personalized way
by ﬁne-tuning with few images, and Kumari et al. (2022)
proposed a more memory and computationally efﬁcient
method.
In this work, we demonstrate that it is possi-
ble to ﬁne-tune text-to-image models using simple binary
(good/bad) human feedback.
Learning with human feedback.
Human feedback has
been used to improve various AI systems, from transla-
tion (Bahdanau et al., 2016; Kreutzer et al., 2018), to web
question-answering (Nakano et al., 2021), to story gener-
ation (Zhou & Xu, 2020), to training RL agents without
the need for hand-designed rewards (MacGlashan et al.,
2017; Christiano et al., 2017; Warnell et al., 2018; Ibarz
et al., 2018; Lee et al., 2021, inter alia), and to more truthful
and harmless instruction following and dialogue (Ouyang
et al., 2022; Bai et al., 2022a; Ziegler et al., 2019; Wu et al.,
2021; Stiennon et al., 2020; Liu et al., 2023; Scheurer et al.,
2022; Bai et al., 2022b, inter alia). In relation to prior works
that focus on improving language models and game agents
with human feedback, our work explores using human feed-
back to align multi-modal text-to-image models with human
preference. Many prior works on learning with human feed-
back consist of learning a reward function and maximizing
reward weighted likelihood (often dubbed as supervised
ﬁne-tuning) (see e.g. Ouyang et al., 2022; Ziegler et al.,
2019; Stiennon et al., 2020) Inspired by their successes,
we propose a ﬁne-tuning method with human feedback for
improving text-to-image models.
Evaluating image-text alignment.
To measure the
image-text alignment, various evaluation protocols have
been proposed (Madhyastha et al., 2019; Hessel et al.,
2021; Saharia et al., 2022; Yu et al., 2022b). Most prior
works (Ramesh et al., 2022; Saharia et al., 2022; Yu et al.,
2022b) use the alignment score of image and text embed-
dings determined by pre-trained multi-modal models, such
as CLIP (Radford et al., 2021) and CoCa (Yu et al., 2022a).
However, since scores from pre-trained models are not cali-
brated with human intent, human evaluation has also been
introduced (Saharia et al., 2022; Yu et al., 2022b). In this
work, we train a reward function that is better aligned with
human evaluations by exploiting pre-trained representations
and (a small amount) of human feedback data.
3. Main Method
To improve the alignment of generated images with their
text prompts, we ﬁne-tune a pre-trained text-to-image
model (Ramesh et al., 2022; Saharia et al., 2022; Rom-
bach et al., 2022) by repeating the following steps shown in
Figure 1. We ﬁrst generate a set of diverse images from a col-
lection of text prompts designed to test various capabilities
of the text-to-image model. Human raters provide binary
feedback on these images (Section 3.1). Next we train a re-
ward model to predict human feedback given a text prompt
and an image as inputs (Section 3.2). Finally, we ﬁne-tune
the text-to-image model using reward-weighted log likeli-
hood to improve text-image alignment (Section 3.3).
3.1. Human Data Collection
Image-text dataset.
To test speciﬁc capabilities of a given
text-to-image model, we consider three categories of text
prompts that generate objects with a speciﬁed count, color,
or background.2 For each category, we generate prompts
by combining a word or phrase from that category with
some object; e.g., combining green (or in a city)
with dog. We also consider combinations of the three cat-
egories (e.g., two green dogs in a city). From
each prompt, we generate up to 60 images using a pre-
trained text-to-image model—in this work, we use Stable
Diffusion v1.5 (Rombach et al., 2022).
Human feedback.
We collect simple binary feedback
from multiple human labelers on the image-text dataset.
Labelers are presented with three images generated from the
same prompt and are asked to assess whether each image is
well-aligned with the prompt (“good”) or not (“bad”).3 We
use binary feedback given the simplicity of our prompts—
the evaluation criterion are fairly clear. More informative
human feedback, such as ranking (Stiennon et al., 2020;
Ouyang et al., 2022), should prove useful when more com-
plex or subjective text prompts are used (e.g., artistic or
open-ended generation).
3.2. Reward Learning
To measure image-text alignment, we learn a reward func-
tion rφ(x, z) (parameterized by φ) that maps the CLIP em-
beddings4 (Radford et al., 2021) of an image x and a text
prompt z to a scalar value. It is trained to predict human
feedback y ∈{0, 1} (1 = good, 0 = bad).
Formally, given the human feedback dataset Dhuman =
{(x, z, y)}, the reward function rφ is trained by minimizing
the mean-squared-error (MSE):
LMSE(φ) =
E
(x,z,y)∼Dhuman

(y −rφ(x, z))2 
.
2For simplicity, we consider a limited class of text categories
in this work, deferring the study of broader and more complex
categories to future work.
3Labelers are instructed to skip a query if it is hard to answer.
Skipped queries are not used in training.
4To improve generalization ability, we use CLIP embeddings
pre-trained on various image-text samples.

Aligning Text-to-Image Models using Human Feedback
Prompt classiﬁcation.
Data augmentation can signiﬁ-
cantly improve the data-efﬁciency and performance of learn-
ing (Krizhevsky et al., 2017; Cubuk et al., 2019). To ef-
fectively exploit the feedback dataset, we design a simple
data augmentation scheme and auxiliary loss for reward
learning. For each image-text pair that has been labeled
good, we generate N −1 text prompts with different seman-
tics than the original text prompt. For example, we might
generate {Blue dog , . . . , Green dog} given the orig-
inal prompt Red dog.5 This process generates a dataset
Dtxt = {(x, {zj}N
j=1, i′)} with N text prompts {zj}N
j=1,
including the original, for each image x, and the index i′ of
the original prompt.
We use the augmented prompts in an auxiliary task, namely,
classifying the original prompt for reward learning. Our
prompt classiﬁer uses the reward function rφ as follows:
Pφ(i|x, {zj}N
j=1) =
exp(rφ(x, zi)/T)
P
j exp(rφ(x, zj)/T),
∀i ∈[N],
where T > 0 is the temperature. Our auxiliary loss is
Lpc(φ) =
E
(x,{zj}N
j=1,i′)∼Dtxt

LCE Pφ(i|x, {zj}N
j=1), i′
, (1)
where LCE is the standard cross-entropy loss. This encour-
ages rφ to produce low values for prompts with different
semantics than the original. Our experiments show this aux-
iliary loss improves the generalization to unseen images and
text prompts. Finally, we deﬁne the combined loss as
Lreward(φ) = LMSE(φ) + λLpc(φ),
where λ is the penalty parameter.
The pseudo code of reward learning is in Appendix E.
3.3. Updating the Text-to-Image Model
We use our learned rφ to update the text-to-image model p
with parameters θ by minimizing the loss
L(θ) =
E
(x,z)∼Dmodel

−rφ(x, z) log pθ(x|z)

+ β
E
(x,z)∼Dpre

−log pθ(x|z)

,
(2)
where Dmodel is the model-generated dataset (i.e., images
generated by the text-to-image model on the tested text
prompts), Dpre is the pre-training dataset, and β is a penalty
parameter. The ﬁrst term in (2) minimizes the reward-
weighted negative log-likelihood (NLL) on Dmodel.6 By
5We use a rule-based strategy to generate different text prompts
(see Appendix E for more details).
6To increase diversity, we collect an unlabeled dataset Dunlabel
by generating more images from the text-to-image model, and use
both the human-labeled dataset Dhuman and the unlabeled dataset
Dunlabel for training, i.e., Dmodel = Dhuman ∪Dunlabel.
Category
Examples
Count
One dog; Two dogs; Three dogs;
Four dogs; Five dogs;
Color
A green colored dog;
A red colored dog;
Background
A dog in the forest;
A dog on the moon;
Combination
Two blue dogs in the forest;
Five white dogs in the city;
Table 1. Examples of text categories.
Category
Total # of
images
Human feedback (%)
Good
Bad
Skip
Count
6480
34.4
61.0
4.6
Color
3480
70.4
20.8
8.8
Background
2400
66.9
33.1
0.0
Combination
15168
35.8
59.9
4.3
Total
27528
46.5
48.5
5.0
Table 2. Details of image-text datasets and human feedback.
evaluating the quality of the outputs using a reward func-
tion aligned with the text prompts, this term improves the
image-text alignment of the model.
Typically, the diversity of the model-generated dataset is
limited, which can result in overﬁtting. To mitigate this,
similar to Ouyang et al. (2022), we also minimize the pre-
training loss, the second term in (2). This reduces NLL on
the pre-training dataset Dpre. In our experiments, we ob-
served regularization in the loss function L(θ) in (2) enables
the model to generate more natural images.
Different
objective
functions
and
algorithms
(e.g.,
PPO; Schulman et al. 2017) could be considered for up-
dating the text-to-image model similar to RLHF ﬁne-
tuning (Ouyang et al., 2022). We believe RLHF ﬁne-tuning
may lead to better models because it uses online sample
generation during updates and KL-regularization over the
prior model. However, RL usually requires extensive hy-
perparameter tuning and engineering, thus, we defer the
extension to RLHF ﬁne-tuning to future work.
4. Experiments
We describe a set of experiments designed to test the efﬁcacy
of our ﬁne-tuning approach with human feedback.
4.1. Experimental Setup
Models.
For our baseline generative model, we use sta-
ble diffusion v1.5 (Rombach et al., 2022), which has been

Aligning Text-to-Image Models using Human Feedback
(a) Seen text prompt: Two green dogs on the table.
(b) Unseen text prompt (unseen object): Four tigers in the field.
(c) Unseen text prompt (artistic generation): Oil painting of sunflowers.
Figure 2. Samples from the original Stable Diffusion model (left) and our ﬁne-tuned model (right). (a) Our model generates high-quality
seen object (dog) with speciﬁed color, count and background. on seen text prompts. (b) Our model generates an unseen object (tiger)
with speciﬁed color, count, and background. (c) Our model still generates reasonable images from unseen text categories (artistic
generation).
pre-trained on large image-text datasets (Schuhmann et al.,
2021; 2022).7 For ﬁne-tuning, we freeze the CLIP language
encoder (Radford et al., 2021) and ﬁne-tune only the dif-
fusion module. For the reward model, we use ViT-L/14
CLIP model (Radford et al., 2021) to extract image and text
embeddings and train a MLP using these embeddings as
input. More experimental details (e.g., model architectures
and the ﬁnal hyperparameters) are reported in Appendix D.
Datasets.
From a set of 2700 English prompts (see Table 1
for examples), we generate 27K images using the stable
diffusion model (see Appendix B for further details). Table 2
shows the feedback distribution provided by multiple human
labelers, which has been class-balanced. We note that the
stable diffusion model struggles to generate the number
of objects speciﬁed by the prompt, but reliably generates
7Our ﬁne-tuning method can be used readily with other text-
to-image models, such as Imagen (Saharia et al., 2022), Parti (Yu
et al., 2022b) and Dalle-2 (Ramesh et al., 2022).
speciﬁed colors and backgrounds.
We use 23K samples for training, with the remaining sam-
ples used for validation. We also use 16K unlabeled samples
for the reward-weighted loss and a 625K subset8 of LAION-
5B (Schuhmann et al., 2022) ﬁltered by an aesthetic score
predictor 9 for the pre-training loss.
4.2. Text-Image Alignment Results
Human evaluation.
We measure human ratings of image
alignment with 120 text prompts (60 seen text prompts
and 60 unseen10 text prompts), testing the ability of the
models to render different colors, number of objects, and
8https://huggingface.co/datasets/
ChristophSchuhmann/improved_aesthetics_6.
5plus
9https://github.com/christophschuhmann/
improved-aesthetic-predictor
10Here, “unseen” text prompts consist of “unseen” objects,
which are not in our human dataset.

Aligning Text-to-Image Models using Human Feedback
Seen text prompts
Unseen text prompts
60
65
70
75
80
85
90
95
100
Preference accuracy (%)
CLIP score
Reward function without PC
Reward function
(a) Comparison with CLIP score (Radford et al., 2021)
Seen text prompts
Unseen text prompts
60
65
70
75
80
85
90
95
100
Preference accuracy (%)
Training data=11K
Training data=23K
(b) Performance of reward function versus data size
Figure 3. (a) Accuracy of CLIP score (Radford et al., 2021) and our reward functions on predicting the preferences of human labelers. For
our method, we consider a variant of our reward function, which is not trained with prompt classiﬁcation (PC) loss in (1). (b) Performance
of reward functions with varying the size of the training dataset.
Fine-tuned 
 model
Original 
 model
Tie
20%
40%
60%
80%
100%
50%
3%
10%
Image-Text Alignment
Fine-tuned 
 model
Original 
 model
Tie
20%
40%
60%
80%
100%
10%
15%
0%
Fidelity (image quality)
Number of 
     votes
9/9
8/9
7/9
6/9
5/9
4/9
3/9
2/9
1/9
0/9
Figure 4. Human evaluation results on 120 text prompts (60 seen
text prompts and 60 unseen text prompts). We generate two sets of
images (one from our ﬁne-tuned model and one from the original
model) with same text prompt. Then, human raters indicate which
one is better, or tie (i.e., two sets are similar) in terms of image-
text alignment and image ﬁdelity. Each query is evaluated by 9
independent human raters and we report the percentage of queries
based on the number of positive votes. We also highlight the
percentage of queries with two-thirds vote (7 or more positive
votes) in the black box.
backgrounds (see Appendix B for the full set of prompts).
Given two (anonymized) sets of images, one from our ﬁne-
tuned model and one from the stable diffusion model, we
ask human raters to assess which is better w.r.t. image-text
alignment and ﬁdelity (i.e., image quality).11 Each query
is evaluated by 9 independent human raters. We show the
percentage of queries based on the number of positive votes.
11We ask raters to declare a tie if they have similar quality.
As shown in Figure 4, our method signiﬁcantly improves
image-text alignment against the original model. Specif-
ically, 50% of samples from our model receive at least
two-thirds vote (7 or more positive votes) for image-text
alignment. However, ﬁne-tuning somewhat degrades image
ﬁdelity (15% compared to 10%). We expect that this is be-
cause (i) we asked the labelers to provide feedback mainly
on alignment, (ii) the diversity of our human data is limited,
and (iii) we used a small subset of pre-training dataset for
ﬁne-tuning.12 This issue can presumably be mitigated with
larger rater and pre-training datasets.
Qualitative comparison.
Figure 2 shows image samples
from the original model and our ﬁne-tuned counterpart (see
Appendix A for more image examples). While the original
often generates images with missing details (e.g., color,
background or count) (Figure 2(a)), our model generates
objects that adhere to the prompt-speciﬁed colors, counts
and backgrounds. Of special note, our model generates
high-quality images on unseen text prompts that specify
unseen objects (Figure 2(b)). Our model also generates
reasonable images given unseen text categories, such as
artistic generation (Figure 2(c)).
However, we also observe several issues of our ﬁne-tuned
models. First, for some speciﬁc text prompts, our ﬁne-
tuned model generates oversaturated and non-photorealistic
images. Our model occasionally duplicates entities within
the generated images or produces lower-diversity images
for the same prompt. We expect that it would be possible
to address these issues with larger (and diverse) human
datasets and better optimization (e.g., RL).
12Similar issue, which is akin to the alignment tax, has been
observed in language domains (Askell et al., 2021; Ouyang et al.,
2022).

Aligning Text-to-Image Models using Human Feedback
(a) Fine-tuned model only with human-labeled dataset.
(b) Fine-tuned model with human-labeled and unlabeled datasets.
(c) Fine-tuned model with human-labeled, unlabeled and pre-training datasets.
Figure 5. Samples from ﬁne-tuned models trained with different datasets on unseen text prompts. (a) Fine-tuned model only with human
dataset generates low-quality images due to overﬁtting. (b) Unlabeled samples improve the quality of generated images. (c) Fine-tuned
model can generate high-ﬁdelity images by utilizing pre-training dataset.
4.3. Results on Reward Learning
Predicting human preferences.
We investigate the qual-
ity of our learned reward function by evaluating its predic-
tion of with human ratings. Given two images from the
same text prompt (x1, x2, z), we check whether our reward
rφ generates a higher score for the human-preferred image,
i.e., rφ(x1, z) > rφ(x2, z) when rater prefers x1. As a
baseline, we compare it with the CLIP score (Hessel et al.,
2021), which measures image-text similarity in the CLIP
embedding space (Radford et al., 2021).
Figure 3(a) compares the accuracy of rφ and the CLIP score
on unseen images from both seen and unseen text prompts.
Our reward (green) more accurately predicts human eval-
uation than the CLIP score (red), hence is better aligned
with typical human intent. To show the beneﬁt of our aux-
iliary loss (prompt classiﬁcation) in (1), we also assess a
variant of our reward function which ignores the auxiliary
loss (blue). The auxiliary classiﬁcation task improves re-
ward performance on both seen and unseen text prompts.
The gain from the auxiliary loss clearly shows the impor-
tance of text diversity and our auxiliary loss in improving
data efﬁciency. Although our reward function is more ac-
curate than the CLIP score, its performance on unseen text
prompts (∼80%) suggests that it may be necessary to use
more diverse and large human datasets.
Rejection sampling.
Similar to Parti (Yu et al., 2022b)
and DALL-E (Ramesh et al., 2021), we evaluate a rejection
sampling technique, which selects the best output w.r.t. the
learned reward function.13 Speciﬁcally, we generate 16 im-
ages per text prompt from the original stable diffusion model
and select the 4 with the highest reward scores. We compare
these to 4 randomly sampled images in Figure 6(a). Rejec-
tion sampling signiﬁcantly improves image-text alignment
(46% with two-thirds preference vote by raters) without sac-
riﬁcing image ﬁdelity. This result illustrates the signiﬁcant
value of the reward function in improving text-to-image
models without any ﬁne-tuning.
13Parti and DALL-E use similarity scores of image and text
embeddings from CoCa (Yu et al., 2022a) and CLIP (Radford
et al., 2021), respectively.

Aligning Text-to-Image Models using Human Feedback
Original model 
 + rejection
Original 
 model
Tie
20%
40%
60%
80%
100%
46%
3%
10%
Image-Text Alignment
Original model 
 + rejection
Original 
 model
Tie
20%
40%
60%
80%
100%
12%
9%
1%
Fidelity (image quality)
Number of 
     votes
9/9
8/9
7/9
6/9
5/9
4/9
3/9
2/9
1/9
0/9
(a) Original model versus Rejection sampling
Fine-tuned 
 model
Original model 
 + rejection
Tie
20%
40%
60%
80%
100%
20%
10%
9%
Image-Text Alignment
Fine-tuned 
 model
Original model 
 + rejection
Tie
20%
40%
60%
80%
100%
3%
20%
0%
Fidelity (image quality)
Number of 
     votes
9/9
8/9
7/9
6/9
5/9
4/9
3/9
2/9
1/9
0/9
(b) Fine-tuned model versus Rejection sampling
Figure 6. Human evaluation on 120 tested text prompts (60 seen text prompts and 60 unseen text prompts). We generate two sets of
images with same text prompt. Then, human raters indicate which one is better, or tie (i.e., two sets are similar) in terms for image-text
alignment and image ﬁdelity. Each query is evaluated by 9 independent human raters and we report the percentage of queries based on the
number of positive votes. We also highlight the percentage of queries with two-thirds vote (7 or more positive votes) in the black box. (a)
For rejection sampling, we generate 16 images per text prompt and select best 4 images based on reward score, i.e., more inference-time
compute. (b) Comparison between ﬁne-tuned model and original model with rejection sampling.
We also compare our ﬁne-tuned model to the original with
rejection sampling in Figure 6(b).14 Our ﬁne-tuned model
achieves a 10% gain in image-text alignment (20%-10%
two-thirds vote) but sacriﬁces 17% in image ﬁdelity (3%-
20% two-thirds vote). However, as discussed in Section 4.2,
we expect degradation in ﬁdelity to be mitigated with larger
human datasets and better hyper-parameters. Note also
that rejection sampling has several drawbacks, including
increased inference-time computation and the inability to
improve the model (since it is output post-processing).
4.4. Ablation Studies
Effects of human dataset size.
To investigate how hu-
man data quality affects reward learning, we conduct an ab-
lation study, reducing the number of images per text prompt
by half before training the reward function. Figure 3(b)
shows that model accuracy decreases on both seen and un-
seen prompts as data size decreases, clearly demonstrating
the importance of diversity and the amount of rater data.
Effects of using diverse datasets.
To verify the impor-
tance of data diversity, we incrementally include unlabeled
and pre-training datasets during ﬁne-tuning. We measure
the reward score (image-text alignment) on 120 tested text
prompts and FID score (Heusel et al., 2017)—the similarity
between generated images and real images—on MS-CoCo
validation data (Lin et al., 2014). Table 3 shows that FID
score is signiﬁcantly reduced when the model is ﬁne-tuned
using only human data, despite better image-text alignment.
14We remark that rejection sampling can also be applied on top
of our ﬁne-tuned model.
FID on
MS-CoCo (↓)
Average rewards on
tested prompts (↑)
Original model
13.97
0.43
Fine-tuned model w.o
unlabeled & pre-train
26.59
0.69
Fine-tuned model
w.o pre-train
21.02
0.79
Fine-tuned model
16.76
0.79
Table 3. Comparison with the original Stable Diffusion. For evalu-
ating image ﬁdelity, we measure FID scores on the MS-CoCo. For
evaluating the image-text alignment, we measure reward scores
and CLIP scores on 120 tested text prompts. ↑(↓) indicates that
the higher (lower) number is the better.
However, by adding the unlabeled and pre-training datasets,
FID score is improved without impacting image-text align-
ment. We provide image samples from unseen text prompts
in Figure 5. We see that ﬁne-tuned models indeed generate
more natural images when exploiting more diverse datasets.
5. Discussion
In this work, we have demonstrated that ﬁne-tuning with
human feedback can effectively improve the image-text
alignment in three domains: generating objects with a speci-
ﬁed count, color, or backgrounds. We analyze several design
choices (such as using an auxiliary loss and collecting di-
verse training data) and ﬁnd that it is challenging to balance
the alignment-ﬁdelity tradeoffs without careful investiga-
tions on such design choices. Even though our results do not

Aligning Text-to-Image Models using Human Feedback
address all the failure modes of the existing text-to-image
models, we hope that our method can serve as a starting
point to study learning from human feedback for improving
text-to-image models.
Limitations and future directions. There are several limi-
tations and interesting future directions in our work:
• More nuanced human feedback. Some of the poor
generations we observed, such as highly saturated
image colors, are likely due to similar images being
highly ranked in our training set. We believe that
instructing raters to look for a more diverse set of
failure modes (oversaturated colors, unrealistic ani-
mal anatomy, physics violations, etc.) will improve
performance along these axes.
• Diverse and large human dataset. For simplicity, we
consider a limited class of text categories (count, color,
background) and thus consider a simple form of hu-
man feedback (good or bad). Due to this, the diversity
of our human data is bit limited. Extension to more
subjective text categories (like artistic generation) and
informative human feedback such as ranking would
be an important direction for future research.
• Different objectives and algorithms. For updating the
text-to-image model, we use a reward-weighted likeli-
hood maximization. However, similar to prior work in
language domains (Ouyang et al., 2022), it would be
an interesting direction to use RL algorithms (Schul-
man et al., 2017). We believe RLHF ﬁne-tuning may
lead to better models because (a) it uses online sample
generation during updates and (b) KL-regularization
over the prior model can mitigate overﬁtting to the
reward function.
Acknowledgements
We thank Peter Anderson, Douglas Eck, Junsu Kim,
Changyeon Kim, Jongjin Park, Sjoerd van Steenkiste,
Younggyo Seo, and Guy Tennenholtz for providing help-
ful comments and suggestions. Finally, we would like to
thank Sehee Yang for providing valuable feedback on user
interface and constructing the initial version of human data,
without which this project would not have been possible.
References
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D.,
Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma,
N., et al. A general language assistant as a laboratory for
alignment. arXiv preprint arXiv:2112.00861, 2021.
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R.,
Pineau, J., Courville, A., and Bengio, Y.
An actor-
critic algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086, 2016.
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,
et al. Training a helpful and harmless assistant with rein-
forcement learning from human feedback. arXiv preprint
arXiv:2204.05862, 2022a.
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-
non, C., et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073, 2022b.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,
S., and Amodei, D. Deep reinforcement learning from
human preferences. In Advances in Neural Information
Processing Systems, 2017.
Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le,
Q. V. Autoaugment: Learning augmentation strategies
from data. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2019.
Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A.,
Narayana, P., Basu, S., Wang, X. E., and Wang,
W. Y. Training-free structured diffusion guidance for
compositional text-to-image synthesis. arXiv preprint
arXiv:2212.05032, 2022.
Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano,
A. H., Chechik, G., and Cohen-Or, D. An image is worth
one word: Personalizing text-to-image generation using
textual inversion. arXiv preprint arXiv:2208.01618, 2022.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
Generative adversarial networks. Communications of the
ACM, 63(11):139–144, 2020.
Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,
Y. Clipscore: A reference-free evaluation metric for im-
age captioning. arXiv preprint arXiv:2104.08718, 2021.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and
Hochreiter, S. Gans trained by a two time-scale update
rule converge to a local nash equilibrium. In Advances in
neural information processing systems, 2017.
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion prob-
abilistic models.
In Advances in Neural Information
Processing Systems, 2020.

Aligning Text-to-Image Models using Human Feedback
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and
Amodei, D. Reward learning from human preferences
and demonstrations in atari. In Advances in Neural Infor-
mation Processing Systems, 2018.
Kingma, D. P. and Welling, M. Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114, 2013.
Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. Can
neural machine translation be improved with user feed-
back? arXiv preprint arXiv:1804.05958, 2018.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
Communications of the ACM, 60(6):84–90, 2017.
Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and
Zhu, J.-Y. Multi-concept customization of text-to-image
diffusion. arXiv preprint arXiv:2212.04488, 2022.
Lee, K., Smith, L., and Abbeel, P.
Pebble: Feedback-
efﬁcient interactive reinforcement learning via relabeling
experience and unsupervised pre-training. In Interna-
tional Conference on Machine Learning, 2021.
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-
manan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco:
Common objects in context. In European conference on
computer vision, 2014.
Liu, H., Sferrazza, C., and Abbeel, P. Chain of hindsight
aligns language models with feedback. arXiv preprint
arXiv: Arxiv-2302.02676, 2023.
Liu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum, J. B.
Compositional visual generation with composable diffu-
sion models. arXiv preprint arXiv:2206.01714, 2022a.
Liu, R., Garrette, D., Saharia, C., Chan, W., Roberts, A.,
Narang, S., Blok, I., Mical, R., Norouzi, M., and Constant,
N. Character-aware models improve visual text rendering.
arXiv preprint arXiv:2212.10562, 2022b.
Loshchilov, I. and Hutter, F. Decoupled weight decay regu-
larization. arXiv preprint arXiv:1711.05101, 2017.
MacGlashan, J., Ho, M. K., Loftin, R., Peng, B., Roberts,
D., Taylor, M. E., and Littman, M. L. Interactive learning
from policy-dependent human feedback. In International
Conference on Machine Learning, 2017.
Madhyastha, P., Wang, J., and Specia, L. Viﬁdel: Evaluating
the visual ﬁdelity of image descriptions. arXiv preprint
arXiv:1907.09340, 2019.
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,
C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al.
Webgpt: Browser-assisted question-answering with hu-
man feedback. arXiv preprint arXiv:2112.09332, 2021.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al.
Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155, 2022.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In International Conference on
Machine Learning, 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad-
ford, A., Chen, M., and Sutskever, I. Zero-shot text-
to-image generation.
In International Conference on
Machine Learning, 2021.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,
M. Hierarchical text-conditional image generation with
clip latents. arXiv preprint arXiv:2204.06125, 2022.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B. High-resolution image synthesis with latent
diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition,
2022.
Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M.,
and Aberman, K. Dreambooth: Fine tuning text-to-image
diffusion models for subject-driven generation. arXiv
preprint arXiv:2208.12242, 2022.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,
E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S.,
Lopes, R. G., et al. Photorealistic text-to-image diffusion
models with deep language understanding. In Advances
in Neural Information Processing Systems, 2022.
Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho, K.,
and Perez, E. Training language models with language
feedback. arXiv preprint arXiv: Arxiv-2204.14146, 2022.
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,
R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and
Komatsuzaki, A. Laion-400m: Open dataset of clip-
ﬁltered 400 million image-text pairs.
arXiv preprint
arXiv:2111.02114, 2021.
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,
Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,
C., Wortsman, M., et al. Laion-5b: An open large-scale
dataset for training next generation image-text models.
arXiv preprint arXiv:2210.08402, 2022.

Aligning Text-to-Image Models using Human Feedback
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
Ganguli, S. Deep unsupervised learning using nonequi-
librium thermodynamics. In International Conference on
Machine Learning, 2015.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,
R., Voss, C., Radford, A., Amodei, D., and Christiano,
P. Learning to summarize from human feedback. arXiv
preprint arXiv:2009.01325, 2020.
Van Den Oord, A., Kalchbrenner, N., and Kavukcuoglu,
K. Pixel recurrent neural networks. In International
conference on machine learning, pp. 1747–1756, 2016.
Warnell, G., Waytowich, N., Lawhern, V., and Stone,
P.
Deep tamer:
Interactive agent shaping in high-
dimensional state spaces. In Conference on Artiﬁcial
Intelligence, 2018.
Wu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe,
R., Leike, J., and Christiano, P. Recursively summa-
rizing books with human feedback.
arXiv preprint
arXiv:2109.10862, 2021.
Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S.,
Kale, M., Roberts, A., and Raffel, C. Byt5: Towards
a token-free future with pre-trained byte-to-byte mod-
els. Transactions of the Association for Computational
Linguistics, 10:291–306, 2022.
Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini,
M., and Wu, Y. Coca: Contrastive captioners are image-
text foundation models. arXiv preprint arXiv:2205.01917,
2022a.
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z.,
Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scal-
ing autoregressive models for content-rich text-to-image
generation. arXiv preprint arXiv:2206.10789, 2022b.
Zhou, W. and Xu, K. Learning to compare for better training
and evaluation of open domain natural language gener-
ation models. In Conference on Artiﬁcial Intelligence,
2020.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,
A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning
language models from human preferences. arXiv preprint
arXiv:1909.08593, 2019.

Aligning Text-to-Image Models using Human Feedback
A. Qualitative Comparison
(a) Text prompt: A red colored tiger.
(b) Text prompt: A green colored tiger.
(c) Text prompt: A pink colored tiger.
Figure 7. Samples from the original Stable Diffusion model (left) and our ﬁne-tuned model (right). The ﬁne-tuned model can generate an
unseen object (tiger) with speciﬁed colors.

Aligning Text-to-Image Models using Human Feedback
(a) Text prompt: Three wolves in the forest.
(b) Text prompt: Four wolves in the forest.
(c) Text prompt: Five wolves in the forest.
Figure 8. Samples from the original Stable Diffusion model (left) and our ﬁne-tuned model (right). The ﬁne-tuned model can generate an
unseen object (wolf) with speciﬁed counts. However, the counts are not always perfect, showing a room for improvement.

Aligning Text-to-Image Models using Human Feedback
(a) Text prompt: A cake in the city.
(b) Text prompt: A cake in the sea.
(c) Text prompt: A cake on the moon.
Figure 9. Samples from the original Stable Diffusion model (left) and our ﬁne-tuned model (right). The ﬁne-tuned model can generate
cake with speciﬁed backgrounds.

Aligning Text-to-Image Models using Human Feedback
(a) Text prompt: An oil painting of rabbit.
(b) Text prompt: A black and white sketch of rabbit.
(c) Text prompt: A 3D render of rabbit.
Figure 10. Samples from the original Stable Diffusion model (left) and our ﬁne-tuned model (right). The ﬁne-tuned model still generate
rabbit in speciﬁed styles.

Aligning Text-to-Image Models using Human Feedback
(a) Text prompt: A zombie in the style of Picasso.
(b) Text prompt: A watercolor painting of a chair that looks like an octopus.
(c) Text prompt: A painting of a squirrel eating a burger.
Figure 11. Samples from the original Stable Diffusion model (left) and our ﬁne-tuned model (right). The ﬁne-tuned model still maintains
performance across a wide distribution of text prompts.

Aligning Text-to-Image Models using Human Feedback
B. Image-text Dataset
In this section, we describe our image-text dataset. We generate 2774 text prompts by combining a word or phrase
from that category with some object. Speciﬁcally, we consider 9 colors (red, yellow, green, blue, black,
pink, purple, white, brown),
6 numbers (1-6),
8 backgrounds (forest, city, moon, field,
sea, table, desert, San Franciso)
and
25
objects
(dog, cat, lion, orange, vase, cup,
apple, chair, bird, cake, bicycle, tree, donut, box, plate, clock, backpack, car,
airplane, bear, horse, tiger, rabbit, rose, wolf).15 For each text prompt, we generate 60 or 6
images according to the text category. In total, our image-text dataset consists of 27528 image-text pairs. Labeling for
training is done by two human labelers.
For evaluation, we use 120 text prompts listed in Table 4. Given two (anonymized) sets of 4 images, we ask human raters to
assess which is better w.r.t. image-text alignment and ﬁdelity (i.e., image quality). Each query is rated by 9 independent
human raters in Figure 4 and Figure 6.
Category
Examples
Seen
A red colored dog.; A red colored donut.; A red colored cake.; A red colored vase.; A green colored dog.;
A green colored donut.; A green colored cake.; A green colored vase.; A pink colored dog.; A pink colored donut.;
A pink colored cake.; A pink colored vase.; A blue colored dog.; A blue colored donut.; A blue colored cake.;
A blue colored vase.; A black colored apple.; A green colored apple.; A pink colored apple.; A blue colored apple.;
A dog on the moon.; A donut on the moon.; A cake on the moon.; A vase on the moon.; An apple on the moon.;
A dog in the sea.; A donut in the sea.; A cake in the sea.; A vase in the sea.; An apple in the sea.;
A dog in the city.; A donut in the city.; A cake in the city.; A vase in the city.; An apple in the city.;
A dog in the forest.; A donut in the forest.; A cake in the forest.; A vase in the forest.; An apple in the forest.;
Two dogs.; Two donuts.; Two cakes.; Two vases.; Two apples.; Three dogs.; Three donuts.; Three cakes.;
Three vases.; Three apples.; Four dogs.; Four donuts.; Four cakes.; Four vases.; Four apples.; Five dogs.;
Unseen
A red colored bear.; A red colored wolf.; A red colored tiger.; A red colored rabbit.; A green colored bear.;
A green colored wolf.; A green colored tiger.; A green colored rabbit.; A pink colored bear.; A pink colored wolf.;
A pink colored tiger.; A pink colored rabbit.; A blue colored bear.; A blue colored wolf.; A blue colored tiger.;
A blue colored rabbit.; A black colored rose.; A green colored rose.; A pink colored rose.; A blue colored rose.;
A bear on the moon.; A wolf on the moon.; A tiger on the moon.; A rabbit on the moon.; A rose on the moon.;
A bear in the sea.; A wolf in the sea.; A tiger in the sea.; A rabbit in the sea.; A rose in the sea.;
A bear in the city.; A wolf in the city.; A tiger in the city.; A rabbit in the city.; A rose in the city.;
A bear in the forest.; A wolf in the forest.; A tiger in the forest.; A rabbit in the forest.; A rose in the forest.;
Two brown bears.; Two wolves.; Two tigers.; Two rabbits.; Two red roses.; Three brown bears.; Three wolves.;
Three tigers.; Three rabbits.; Three red roses.; Four brown bears.; Four wolves.; Four tigers.; Four rabbits.;
Table 4. Examples of text prompts for evaluation.
C. Additional Results
Seen prompts
Image-text alignment
Fidelity
Win
Lose
Tie
Win
Lose
Tie
Color
58.9 ± 19.5
10.0 ± 7.1
31.1 ± 26.2
53.9 ± 39.4
28.3 ± 24.0
17.8 ± 23.1
Count
69.4 ± 6.8
11.7 ± 5.3
18.9 ± 10.7
42.2 ± 36.4
37.2 ± 23.6
20.6 ± 22.9
Background
53.3 ± 13.3
14.4 ± 6.4
32.2 ± 18.0
43.9 ± 31.0
39.4 ± 17.1
16.7 ± 20.1
Unseen prompts
Image-text alignment
Fidelity
Win
Lose
Tie
Win
Lose
Tie
Color
57.2 ± 8.5
14.4 ± 9.6
28.3 ± 16.3
39.4 ± 24.7
38.9 ± 9.7
21.7 ± 25.1
Count
69.4 ± 16.1
8.3 ± 2.4
22.2 ± 18.0
42.8 ± 31.8
36.7 ± 9.1
20.6 ± 23.3
Background
55.6 ± 9.6
11.1 ± 9.9
33.3 ± 18.9
46.1 ± 28.5
35.6 ± 13.6
18.3 ± 20.7
Table 5. Percentage of generated images from our ﬁne-tuned model that are better than (win), tied with, or worse than (lose) the compared
to original stable diffusion model in terms of image-text alignment and ﬁdelity.
15We use the following 5 objects only for evaluation: bear, tiger, rabbit, rose, wolf.

Aligning Text-to-Image Models using Human Feedback
Seen prompts
Image-text alignment
Fidelity
Win
Lose
Tie
Win
Lose
Tie
Color
51.7 ± 18.6
20.6 ± 13.2
27.8 ± 31.1
37.8 ± 16.0
30.6 ± 15.2
31.7 ± 29.7
Count
67.8 ± 14.2
12.8 ± 7.1
19.4 ± 20.6
31.7 ± 18.4
31.7 ± 21.2
36.7 ± 38.1
Background
47.8 ± 15.3
10.0 ± 6.7
42.2 ± 20.3
41.1 ± 21.3
27.8 ± 14.9
31.1 ± 34.8
Unseen prompts
Image-text alignment
Fidelity
Win
Lose
Tie
Win
Lose
Tie
Color
47.8 ± 7.5
20.0 ± 14.3
32.2 ± 20.8
31.7 ± 21.9
39.4 ± 13.2
28.9 ± 32.1
Count
62.2 ± 23.8
8.3 ± 6.2
29.4 ± 29.4
40.0 ± 27.9
25.6 ± 12.6
34.4 ± 38.4
Background
62.8 ± 11.3
7.2 ± 5.8
30.0 ± 16.8
51.1 ± 17.6
23.9 ± 12.0
25.0 ± 27.8
Table 6. Percentage of generated images from original stable diffusion model with rejection sampling that are better than (win), tied with,
or worse than (lose) the compared to original stable diffusion model in terms of image-text alignment and ﬁdelity.
Seen prompts
Image-text alignment
Fidelity
Win
Lose
Tie
Win
Lose
Tie
Color
34.4 ± 19.2
27.2 ± 18.9
38.3 ± 37.6
41.1 ± 33.7
27.8 ± 8.2
31.1 ± 33.1
Count
32.8 ± 11.1
35.0 ± 15.8
32.2 ± 25.3
32.8 ± 28.4
41.7 ± 7.8
25.6 ± 29.9
Background
31.7 ± 12.2
24.4 ± 16.1
43.9 ± 27.0
36.1 ± 24.0
36.1 ± 15.2
27.8 ± 35.5
Unseen prompts
Image-text alignment
Fidelity
Win
Lose
Tie
Win
Lose
Tie
Color
40.0 ± 19.7
23.3 ± 12.7
36.7 ± 32.0
39.4 ± 20.3
32.8 ± 14.6
27.8 ± 32.7
Count
53.3 ± 15.5
27.2 ± 9.2
19.4 ± 23.5
26.1 ± 19.5
52.2 ± 7.5
21.7 ± 25.4
Background
33.9 ± 23.1
21.7 ± 9.7
44.4 ± 32.2
33.3 ± 25.6
37.8 ± 10.6
28.9 ± 33.7
Table 7. Percentage of generated images from our ﬁne-tuned model that are better than (win), tied with, or worse than (lose) the compared
to original stable diffusion model with rejection sampling in terms of image-text alignment and ﬁdelity.
D. Experimental Details
Model architecture. For our baseline generative model, we use stable diffusion v1.5 (Rombach et al., 2022), which has
been pre-trained on large image-text datasets (Schuhmann et al., 2021; 2022). For the reward model, we use ViT-L/14
CLIP model (Radford et al., 2021) to extract image and text embeddings and train a MLP using these embeddings as input.
Speciﬁcally, we use two-layer MLPs with 1024 hidden dimensions each. We use ReLUs for the activation function between
layers, and we use the Sigmoid activation function for the output. For auxiliary task, we use temperature T = 2 and penalty
parameter λ = 0.5.
Training. Our ﬁne-tuning pipeline is based on publicly released repository (https://github.com/huggingface/
diffusers/tree/main/examples/text_to_image). We update the model using AdamW (Loshchilov & Hutter,
2017) with β1 = 0.9, β2 = 0.999, ϵ = 1e −8 and weight decay 1e −2. The model is trained in half-precision on 4 40GB
NVIDIA A100 GPUs, with a per-GPU batch size of 8, resulting in a toal batch size of 512 (256 for pre-training data and 256
for model-generated data).16 It is trained for a total of 10,000 updates.
FID measurement using MS-CoCo dataset. We measure FID scores to evaluate the ﬁdelity of different models using
MS-CoCo validation dataset (i.e., val2014). There are a few caption annotations for each MS-CoCo image. We randomly
choose one caption for each image, which results in 40,504 caption and image pairs. MS-CoCo images have different
resolutions and they are resized to 256×256 before computing FID scores. We use pytorch-fid Python implementation
for the FID measurement (https://github.com/mseitzer/pytorch-fid).
16In prior work (Ouyang et al., 2022), model is optimized with bigger batch for pre-training data. However, in our experiments, using a
bigger batch does not make a big difference. We expect this is because small pre-training dataset is used in our work.

Aligning Text-to-Image Models using Human Feedback
E. Pseudocode
Algorithm 1 Reward Learning Pseudocode
# x, z, y: image, text prompt, human label
# clip: pre-trained CLIP model
# preprocess: Image transform
# pred_r: two-layers MLPs
# Get_perturbated_prompts: function to generate perturbated text prompts
# lambda: penalty parameter
# T: temperature
# N: # of perturbated text prompts
# main model
def RewardFunction(x, z):
# compute embeddings for tokens
img_embedding = clip.encode_image(prepocess(x))
txt_embedding = clip.encode_text(clip.tokenize(z))
input_embeds = concatenate(img_embedding, txt_embedding)
# predict score
return pred_r(input_embeds)
# training loop
for (x, z, y) in dataloader: # dims: (batch_size, dim)
# MSE loss
r_preds = RewardFunction(x, z)
loss = MSELoss(r_preds, y)
# Prompt classification
scores = [r_preds]
for z_neg in Get_perturbated_prompts(z, N):
scores.append(RewardFunction(x, z_neg))
scores = scores / T
labels = [0] * batch_size # origin text is always class 0
loss += lambda * CrossEntropyLoss(scores, labels)
# update reward function
optimizer.zero_grad(); loss.backward(); optimizer.step()

Aligning Text-to-Image Models using Human Feedback
Algorithm 2 Perturbated Text Prompts Generation Pseudocode
# z: image, text prompt, human label
# N: # of perturbated text prompts
def Get_perturbated_prompts(z, N):
color_list = [‘‘red’’, ‘‘yellow’’, ...]
obj_list = [‘‘dog’’, ‘‘cat’’, ...]
count_list = [‘‘One’’, ‘‘Two’’, ...]
loc_list = [‘‘in the sea.’’, ‘‘in the sky.’’, ...]
output = []
count = 0
while (count < N):
idx = random.randint(0, len(count_list)-1)
count = count_list[idx]
idx = random.randint(0, len(color_list)-1)
color = color_list[idx]
idx = random.randint(0, len(loc_list)-1)
loc = loc_list[idx]
idx = random.randint(0, len(obj_list)-1)
obj = obj_list[idx]
if count == ‘‘One’’:
text = ‘‘{} {} {} {}.’’.format(count, color, obj, loc)
else:
text = ‘‘{} {} {}s {}.’’.format(count, color, obj, loc)
if z != text:
count += 1
output.append(text)
return output

