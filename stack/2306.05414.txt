Improving Negative-Prompt Inversion via Proximal Guidance
Ligong Han1
Song Wen1
Qi Chen2
Zhixing Zhang1
Kunpeng Song1
Mengwei Ren3
Ruijiang Gao4
Yuxiao Chen1
Di Liu1
Qilong Zhangli1
Anastasis Stathopoulos1
Jindong Jiang1
Zhaoyang Xia1
Akash Srivastava5
Dimitris Metaxas1
1Rutgers University
2Laval University
3New York University
4UT Austin
5MIT-IBM AI Lab
Abstract
DDIM inversion has revealed the remarkable potential
of real image editing within diffusion-based methods. How-
ever, the accuracy of DDIM reconstruction degrades as
larger classifier-free guidance (CFG) scales being used for
enhanced editing. Null-text inversion (NTI) optimizes null
embeddings to align the reconstruction and inversion tra-
jectories with larger CFG scales, enabling real image edit-
ing with cross-attention control.
Negative-prompt inver-
sion (NPI) further offers a training-free closed-form solu-
tion of NTI. However, it may introduce artifacts and is still
constrained by DDIM reconstruction quality. To overcome
these limitations, we propose Proximal Negative-Prompt In-
version (ProxNPI), extending the concepts of NTI and NPI.
We enhance NPI with a regularization term and reconstruc-
tion guidance, which reduces artifacts while capitalizing on
its training-free nature. Our method provides an efficient
and straightforward approach, effectively addressing real
image editing tasks with minimal computational overhead.
1. Introduction
Diffusion-based methods have emerged as popular ap-
proaches for real image editing, with many of these methods
utilizing DDIM inversion [51]. DDIM inversion is known
to yield accurate reconstructions when using null embed-
dings or source prompts with a classifier-free guidance [25]
(CFG) scale of 1. However, in order to achieve better edit-
ing capabilities, it is often necessary to use a CFG scale sig-
nificantly larger than 1. Unfortunately, this scaling can lead
to inaccurate reconstructions of the source image, which
hinders the editing quality. This phenomenon is also ob-
served in prompt-to-prompt [23] editing scenarios.
To address this limitation, Null-text inversion [35] (NTI)
was introduced as a solution. NTI employs pivotal inver-
sion by optimizing the null embedding(s), ensuring that the
reconstruction trajectory aligns with the inversion trajectory
even under a larger CFG scale. While NTI has a lightweight
Input Image
Null-Text 
Inversion
Negative-Prompt 
Inversion
Ours
130s
5s
5s
Inversion time:
“a cat tiger sitting next to a mirror”
Prompt:
Figure 1. Proximal Negative-Prompt Inversion. A comparison
of editing quality between Null-text inversion (NTI), Negative-
prompt inversion (NPI), and our proposed method (ProxNPI). The
bottom row represents the time required for inversion. Our ap-
proach incorporates the fast inversion capability of NPI without
the need for test-time optimization, thereby incurring only mini-
mal additional cost during inference.
parameter set, it requires per-image optimization, which can
be time-consuming. To eliminate the need for optimiza-
tion in NTI, Negative-prompt inversion [34] (NPI) offers a
closed-form solution. By assuming equal predicted noises
between consecutive timesteps of the diffusion model, NPI
elegantly demonstrated that the solver of NTI is equivalent
to the source prompt embedding. However, NPI may oc-
casionally introduce artifacts due to its underlying assump-
tions.
Building upon the remarkable results of NPI, we further
enhance it by incorporating a regularization term to im-
prove the reconstruction of the source image. Moreover,
we recognize that NPI is still constrained by the recon-
struction quality of DDIM inversion, unable to correct er-
rors introduced during the reconstruction process. To over-
come this, we introduce a reconstruction guidance tech-
nique that performs one-step gradient descent on the pre-
dicted clean latent image, aligning it with the original latent
image. The resulting algorithm offers a straightforward ap-
proach with negligible computational overhead. By com-
bining the benefits of NPI, reconstruction guidance, and a
regularization term, our method provides an effective and
efficient optimization-free solution for real image editing.
1
arXiv:2306.05414v1  [cs.CV]  8 Jun 2023

𝜖!"#
𝜖$%"
𝜖&'((
𝜖!"#
𝜖$%"
𝜖̃
𝜖̃
𝜖!"#
(a) DDIM Inversion
(b) Negative-Prompt 
Inversion
(c) Proximal Negative-Prompt
Inversion
𝜖̃
projection
𝑤⋅𝜆
𝜖!"#
𝜖$#%
𝜖&'((
𝜆
𝜖̃
Predicted noise
Null
Source
Target
CFG
Threshold
𝑤⋅ 𝜆
𝜖$%"
𝑤
CFG scale
Figure 2. Illustration of a single inference step using classifier-free guidance (CFG) with a scale w = 2. All methods initially utilize DDIM
inversion [51] with the source prompt (and w = 1). During the inference process: (a) direct sampling is performed using the target prompt;
(b) the null embedding is replaced with the source prompt embedding; (c) a proximal gradient step is applied to the scaled noise difference
(ϵtar −ϵsrc) following step (b). Here, we are visualizing soft-thresholding with a threshold λ, which corresponds to L1 regularization on
˜ϵ. If all values are clamped to zero, resulting in ProxNPI reducing to DDIM reconstruction. Conversely, when all values are retained after
thresholding, ProxNPI reduces to NPI.
2. Related Work
Image generation with text guidance has been an well-
explored task in image synthesis field [1,2,8,13,15–21,39,
41, 43, 44, 55, 61–63, 65, 69]. Recent development of text-
to-image (T2I) diffusion models [5, 14, 24, 38, 49, 51–54]
introduced new solutions to this task. In perticular, T2I dif-
fusion models trained with large-scale image-caption pairs
have shown impressive generation ability [37, 42, 45, 47].
The development of large-scale T2I models provides a giant
and flexible design space for image manipulation methods
leveraging the pre-trained model. Recent works propose
novel controlling mechanisms tailored for these T2I mod-
els [3,6,7,10,27,28,30,31,33,40,48,50,58–60,64,66–68].
Diffusion-based image editing. Many recent works fine-
tune the pre-trained T2I models with a few personalized
images to keep the context information.
Wide design
choices have been explored in this direction.
Textual-
Inversion [9,12,57]-based methods propose fine-tuning the
text embedding.
Dreambooth [46] fine-tunes the whole
model.
[29] fine-tunes the cross-attention layers in the
UNet of Stable-Diffusion model. These methods require
hundreds of iterations at the fine-tuning stage to capture
the identity information. For better efficiency, more tech-
niques [16, 26, 32, 36] are developed by reducing the num-
ber of parameters optimized at fine-tuning stage.
While
fine-tuning the pre-trained T2I model shows extraordinary
results, the test-time efficiency of these methods remains
a great challenge.
SEGA
[4] discovers that target con-
cept can be encoded using latent dimensions falling into the
upper and lower tail of the distribution. They further dis-
cuss the numerical intuition of the corresponding semantic
space. They propose to calculate editing direction in tail
distribution to achieve one-shot generation that aligns with
the user’s intent.
Inversion-based image editing. DDIM inversion [51] ex-
hibits great potential in editing tasks by deterministically
calculating and encoding the context information in a latent
and reconstructing the original image with it. Apply editing
prompt upon the inverted latent code greatly improved the
test-time efficiency. Leveraging optimization on null-text
embedding, Null-text Inversion [35] further improved the
identity preservation of the edit. However, all these methods
rely on optimization at test-time for accurate reconstruction,
which typically requires several minutes. Negative-prompt
inversion (NPI) [34] further reduces the computation cost
for the inversion step while generates similarly competitive
reconstruction results as Null-text inversion. We show that
NPI can be considered as performing Delta Denoising Score
(DDS) [22] on the same noisy image. We will further dis-
cuss the preliminary of these methods in Section 3.1.
3. Method
3.1. Background
DDIM inversion.
DDIM is a deterministic sampling of
DDPM, which remains the same distribution at each time
step. While DDPM is the SDE process, DDIM is the corre-
sponding ODE process. The DDIM can be presented as
zt−1 =
√αt−1
√αt
zt+
(1)
√αt−1
 s
1
αt−1
−1 −
r
1
αt
−1
!
ϵθ(zt, t, C)
To reconstruct the given image, the latent variables can be
estimated by reversing the above discrete ODE sampling
process. Denote the sequence of latent variables from z0
2

Input Image
Null-Text
Editing
Negative-Prompt
Editing
Ours
Editing
(a)
(b)
(c)
(d)
(e)
“a cat sitting next to a mirror”
“a Lego cat toy sitting next to a mirror”
“orange van with surfboards on top”
“orange van with surfboards flowers on top”
“meat balls on white plate”
“meat balls sushi on white plate”
“a woman with blue hair”
“a woman storm-trooper with blue hair”
“A cat sitting on a wooden chair”
“A cat dog sitting on a wooden chair”
Ours
Reconstruction
Null-Text 
Reconstruction
Negative-Prompt 
Reconstruction
Figure 3. Qualitative comparisons of inversion methods. The figure showcases qualitative comparisons among Null-text inversion
(NTI) [35], Negative-prompt inversion (NPI) [34], and our proposed method (ProxNPI). Each row demonstrates the reconstruction results
(columns 2-4) and editing results (columns 5-7) for the respective methods. Reconstruction guidance is employed for examples (c-e) to
address minor errors in DDIM reconstruction. Errors or artifacts are marked using red circles or boxes. The comparisons highlight instances
where NPI fails to retain specific image details (a), both NTI and NPI introduce undesired changes (b), the reconstruction guidance aids in
recovering missing details (c), our method exhibits better background preservation (d), and both NTI and NPI exhibit reconstruction errors
(e).
via DDIM inversion as {z∗
t }T
t=1, we have:
z∗
t =
√αt
√αt−1
z∗
t−1+
(2)
√αt
 r
1
αt
−1 −
s
1
αt−1
−1
!
ϵθ(z∗
t , t, C),
where the ϵθ(z∗
t , t, C) can be approximated by ϵθ(z∗
t−1, t −
1, C)
Null-text inversion. Using the CFG, the noise is estimated
by
˜ϵθ(zt, t, C, ∅) = wϵθ(zt, t, C) −(1 −w)ϵθ(zt, t, ∅)
(3)
If w > 1, the accumulated error on DDIM inversion will af-
3

Input Image
Ours Reconstruction
Ours Editing
(a)
“a silver cat sculpture sitting next to a mirror”
Input Image
Ours Reconstruction
Ours Editing
(b)
“drawing of tulip lion on the coffee”
(c)
“white plate with fruits pizza on it”
(d)
“a plate with steak salmon on it”
(e)
“A yellow crochet bird stands perched on a tree branch”
(f)
“two origami birds sitting on a branch”
(g)
“white tiger cat on brown ground”
(h)
“black and white cat dog on floor”
Figure 4. Additional visual editing results. More visual editing results for our method are presented, along with their corresponding
prompts. Reconstruction guidance is applied for examples (c), (e), (f), and (g) due to imperfect DDIM reconstructions in these cases.
fect reconstruction accuracy. To address the problem, null-
text inversion optimizes null-text embedding ∅to track the
DDIM inversion trajectories when w = 1. It first computes
the latent variables {z∗
t }T
t=1 using DDIM inversion when
w = 1. We denote the sequence of latent variables from z0
via null-text inversion as ¯zT
t=1. Then it optimizes the fol-
lowing item with respect to ∅by gradient descent:
min∅t||zt−1( ¯zt, ∅t, C) −z∗
t−1||2
2
(4)
After optimization, the latent values {¯z}T
t=1 can track the
trajectories of {z∗
t }T
t=1 to reconstruct the given image more
accurately.
Negative-prompt inversion. To overcome the drawback of
null-text inversion that each image needs optimization, the
negative-prompt inversion approach approximates {z∗
t }T
t=1
by a closed-form {¯z}T
t=1 solution. In detail, if z∗
t = ¯zt, to
make z∗
t−1 = ¯zt−1, we have
ϵθ(z∗
t , t, C) = wϵθ(¯zt, t, C) −(1 −w)ϵθ(¯zt, t, ∅t) (5)
⇒ϵθ(¯zt, t, C) = ϵθ(¯zt, t, ∅t).
(6)
Therefore, ˜ϵθ(zt, t, C, ∅t) = ˜ϵθ(zt, t, C, C) = for all w.
3.2. Proximal Negative-Prompt Inversion
Negative-prompt inversion provides an elegant closed-
form solution for computing null-text inverted null-
embeddings, ∅t = C. This solution intuitively aligns with
the inversion process. Fig. 2 illustrates a single inference
step using classifier-free guidance (CFG) with a scale pa-
4

Input Image
Quantile = 0.6
Quantile = 0.7
Quantile = 0.8
Quantile = 0.9
Quantile = 0.95
L0 (hard)
L1 (soft)
𝑤= 7.5
𝑤= 15
Prompt: “black and white cat robot on floor”
“black and white cat 
on floor”
Figure 5. Ablation study of thresholds. The figure shows visual results obtained by varying the threshold λ from the 60% to 95% quantiles
of the absolute noise difference. The first row represents the impact of hard-thresholding (L0), while the second and third rows show the
effects of soft-thresholding (L1). Soft-thresholding induces less noticeable changes in the edited images compared to hard-thresholding,
aligning with our expectations. Alternatively, increasing the CFG scale, such as with w = 15, can enhance the prominence of the target
attribute, although it may introduce an intensified contrast ratio and shifted color tone.
Input Image
𝜂= 0.01
𝜂= 0.1
𝜂= 0.2
𝜂= 0.5
𝜂= 1
𝑡< 400
𝑡< 600
Prompt: “three white dimsum sushi on brown bowl”
“three white dimsum 
on brown bowl”
Figure 6. Ablation study of reconstruction guidance. The figure shows visual results obtained by varying the stepsize of performing
reconstruction guidance η from the 0.01 to 1. The first row represents performing guidance when t < 400, while the second shows the
effects of t < 600. The threshold is set to the 70% quantile and hard-thresholding is used.
rameter w = 2. Initially, all methods employ DDIM inver-
sion [51] with the source prompt (and w = 1). In Fig. 2(a),
we depict a baseline approach where direct sampling is per-
formed using the target prompt.
Fig. 2(b) demonstrates
the inference step of negative-prompt inversion, where CFG
amplifies the editing direction of (ϵtar −ϵsrc). Intuitively,
when the target prompt is close to the source prompt,
the inference trajectory for editing should closely resem-
ble the DDIM reconstruction trajectory. In fact, when the
target condition C′ = C, negative-prompt inversion ex-
actly recovers DDIM inversion. However, we observe that
negative-prompt inversion occasionally over-amplifies the
editing direction (ϵtar −ϵsrc). To address this, we propose
the addition of an extra loss term that encourages the CFG
noise ˜ϵ to align with ϵsrc.
To accomplish this, we draw inspiration from the prox-
imal gradient method [11, 56] and introduce a regulariza-
tion term to constrain (ϵtar −ϵsrc). This regularization is
achieved through the use of a proximal function,
proxλ,Lp(x) = argmin
z
1
2∥z −x∥2
2 + λ∥z∥p.
(7)
which encourages desired properties in the editing process.
When p = 1 (corresponding to L1 regularization), the
5

solver takes the form of a soft-thresholding function,
[proxλ,L1(x)]i = [Sλ(x)]i =



xi −λ
if
xi > λ
0
if
−λ ≤xi ≤λ
xi + λ
if
xi < −λ
(8)
with [·]i denoting the i-th component. if p = 0 (represent-
ing L0 regularization), the solver takes the form of a hard-
thresholding function,
[proxλ,L0(x)]i =

xi
if
|xi| >
√
2λ
0
otherwise.
(9)
Since the value range of (ϵtar−ϵsrc) does not follow a stan-
dard Gaussian distribution, we employ a dynamic threshold
rather than a fixed one by selecting a quantile of the abso-
lute values |ϵtar −ϵsrc|. It is worth noting that when using
hard thresholding with quantile thresholds, this approach is
similar to SEGA [4]. Fig. 2(c) provides a visualization of
a 2-D case when soft-thresholding is employed. If all val-
ues are clamped to zero, our method reduces to DDIM re-
construction. Conversely, when all values are retained after
thresholding, our method simplifies to negative-prompt in-
version.
Reconstruction guidance. Although effective, the afore-
mentioned approach is still limited by the quality of DDIM
inversion reconstruction.
Even if ˜ϵ converges to ϵsrc, it
cannot correct errors in cases where DDIM reconstruction
is imperfect. To address this, we introduce reconstruction
guidance by performing a single step of gradient descent on
the current predicted original sample ˆz0, aiming to align it
with the source sample z0. This gradient descent step is ap-
plied only to the “unedited” region identified by the mask
M = |ϵtar −ϵsrc| ≤λ, where we reuse the notation λ to
represent the threshold value. By choosing a step size η, the
update can be expressed as ˆz0 = ˆz0 −ηM ⊙(ˆz0 −z0),
where η = 1 corresponds to a complete replacement. The
complete algorithm is outlined in Algorithm 1.
4. Experiment
4.1. Visual Results
Comparison with baselines.
In this section,
we
present qualitative comparisons among Null-text inversion
(NTI) [35], Negative-prompt inversion (NPI) [34], and
our proposed method (ProxNPI), as illustrated in Fig. 3.
Each row in the figure showcases the reconstruction results
(columns 2-4) and editing results (columns 5-7) for each
method. It is worth noting that NPI reconstruction is equiv-
alent to DDIM reconstruction [51], as discussed previously.
For examples (c-e) in Fig. 3, we utilize reconstruction guid-
ance since DDIM reconstruction still introduces minor er-
rors. These errors or artifacts are highlighted using red cir-
cles or boxes.
Algorithm 1 Proximal Negative-Prompt Inversion
Input: Given source original sample z0, source condition
C, target condition C′, denoising model ϵθ, proximal
function proxλ(·).
1: ¯zT = DDIMInvert(z0, C, w = 1)
2: ˆzT = ¯zT
3: for t = T to 1 do
4:
ϵsrc = ϵθ(ˆzt, t, C)
5:
ϵtar = ϵθ(ˆzt, t, C′)
6:
˜ϵ = ϵsrc + w · proxλ(ϵtar −ϵsrc)
7:
M = |ϵtar −ϵsrc| ≤λ
8:
ˆz0 =
1
√αt ˆzt −
q
1
αt −1˜ϵ
9:
if reconstruction guidance and t < Trec then
10:
ˆz0 = ˆz0 −ηM ⊙(ˆz0 −z0)
11:
end if
12:
ˆzt−1 = √αt−1ˆz0 + √1 −αt−1˜ϵ
13: end for
14: return ˆz0
In Fig. 3, we observe that NPI fails to retain the shower
head in the mirror (a), while both NTI and NPI alter the
leaves on the tree to flowers (b).
In (c), both NTI and
NPI exhibit imperfect reconstruction of the input image,
whereas the missing detail is recovered through the re-
construction guidance incorporated in our method. Addi-
tionally, in (d), NPI introduces reconstruction errors, while
our method demonstrates superior preservation of the back-
ground in edited images compared to NTI and NPI. Lastly,
in (e), both NTI and NPI display reconstruction errors on
the chair.
Additional visual results. We present more visual editing
results in Fig. 4, along with the corresponding prompts pro-
vided underneath the images. Among the eight examples,
reconstruction guidance is employed for examples (c), (e),
(f), and (g) due to imperfections in DDIM reconstructions
for these specific cases.
4.2. Ablations
Threshold. In Fig. 5 we present visual results obtained
by setting the threshold λ to the 60%, 70%, ..., 95% quan-
tiles of the absolute values of the noise difference. The first
row illustrates the case of hard-thresholding (labeled as L0),
while the second and third rows display the case of soft-
thresholding (labeled as L1). As anticipated, we observe
that soft-thresholding tends to introduce fewer changes to
the edited images compared to hard-thresholding. Alter-
natively, we can increase the CFG scale, such as using
w = 15, to enhance the prominence of the target attribute.
However, this approach may lead to an amplified contrast
ratio and shifted color tone. We empirically find using hard-
thresholding with quantile 0.7 usually gives good results.
6

Reconstruction guidance. In Fig. 6, we present visual re-
sults obtained by varying the stepsize η for the reconstruc-
tion guidance. The guidance is applied when t < Trec. As
observed, when the guidance strength is small (with a small
η), the reconstruction of chopsticks is incomplete, and the
pattern on the bowl is missing. Increasing Trec results in ac-
curate reconstruction of the chopsticks, while a large η can
introduce artifacts such as an over-amplified contrast ratio.
Through experimentation, we find that setting Trec = 400
and η = 0.1 generally yields satisfactory results, although
these hyperparameters may require tuning for specific im-
ages to achieve optimal performance.
5. Discussion and Conclusion
In this paper, we have introduced Proximal Negative-
Prompt Inversion (ProxNPI), a diffusion-based method for
tuning-free real image editing.
By extending the princi-
ples of Null-text inversion (NTI) and Negative-prompt in-
version (NPI), ProxNPI enhances the accuracy of DDIM
reconstructions under larger classifier-free guidance (CFG)
scales. We show that ProxNPI improves upon NPI by incor-
porating a regularization term and reconstruction guidance,
addressing the limitations of NPI in terms of artifacts and
dependence on DDIM reconstruction quality. Our experi-
mental results have demonstrated the effectiveness of Prox-
NPI in achieving high-quality editing results while main-
taining computational efficiency. However, we acknowl-
edge that there are still limitations to consider. The per-
formance of ProxNPI can be sensitive to hyperparameters,
such as the stepsize and threshold value. It may be benefi-
cial to explore heuristics or automated methods for selecting
these parameters to improve its usability and generalizabil-
ity.
References
[1] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and
Peter Wonka.
Clip2stylegan: Unsupervised extraction of
stylegan edit directions. In ACM SIGGRAPH 2022 Confer-
ence Proceedings, pages 1–9, 2022. 2
[2] Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka.
Styleflow:
Attribute-conditioned exploration of stylegan-
generated images using conditional continuous normalizing
flows. ACM Transactions on Graphics (ToG), 40(3):1–21,
2021. 2
[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-
Or, and Dani Lischinski.
Break-a-scene:
Extracting
multiple concepts from a single image.
arXiv preprint
arXiv:2305.16311, 2023. 2
[4] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas
Struppek, Patrick Schramowski, and Kristian Kersting. Sega:
Instructing diffusion using semantic dimensions.
arXiv
preprint arXiv:2301.12247, 2023. 2, 6
[5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-
phy, William T Freeman, Michael Rubinstein, et al. Muse:
Text-to-image generation via masked generative transform-
ers. arXiv preprint arXiv:2301.00704, 2023. 2
[6] Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin,
Assaf Shocher, Michal Irani, Inbar Mosseri, and Lior Wolf.
The hidden language of diffusion models.
arXiv preprint
arXiv:2306.00966, 2023. 2
[7] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,
Yuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled
parameter-efficient tuning for subject-driven text-to-image
generation. arXiv preprint arXiv:2305.03374, 2023. 2
[8] Yuxiao Chen, Jianbo Yuan, Yu Tian, Shijie Geng, Xinyu Li,
Ding Zhou, Dimitris N. Metaxas, and Hongxia Yang. Revis-
iting multimodal representation in contrastive learning: from
patch and token embeddings to finite discrete tokens. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2023. 2
[9] Giannis Daras and Alexandros G Dimakis. Multiresolution
textual inversion. arXiv preprint arXiv:2211.17115, 2022. 2
[10] Raman Dutt, Linus Ericsson, Pedro Sanchez, Sotirios A
Tsaftaris, and Timothy Hospedales. Parameter-efficient fine-
tuning for medical image analysis: The missed opportunity.
arXiv preprint arXiv:2305.08252, 2023. 2
[11] Jeffrey A. Fessler.
Lecture notes on proximal meth-
ods. https://web.eecs.umich.edu/˜fessler/
course/598/l/n-05-prox.pdf, 2021. 5
[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or.
An image is worth one word: Personalizing text-to-
image generation using textual inversion.
arXiv preprint
arXiv:2208.01618, 2022. 2
[13] Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, and
Yongfeng Zhang. Hiclip: Contrastive language-image pre-
training with hierarchy-aware attention.
arXiv preprint
arXiv:2303.02995, 2023. 2
[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 10696–10706, 2022. 2
[15] Ligong Han, Ruijiang Gao, Mun Kim, Xin Tao, Bo Liu, and
Dimitris Metaxas. Robust conditional gan from uncertainty-
aware pairwise comparisons. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, pages
10909–10916, 2020. 2
[16] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
Dimitris Metaxas, and Feng Yang.
Svdiff: Compact pa-
rameter space for diffusion fine-tuning.
arXiv preprint
arXiv:2303.11305, 2023. 2
[17] Ligong Han, Martin Renqiang Min, Anastasis Stathopou-
los, Yu Tian, Ruijiang Gao, Asim Kadav, and Dimitris N
Metaxas. Dual projection generative adversarial networks
for conditional image generation.
In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 14438–14447, 2021. 2
[18] Ligong Han, Sri Harsha Musunuri, Martin Renqiang Min,
Ruijiang Gao, Yu Tian, and Dimitris Metaxas. Ae-stylegan:
7

Improved training of style-based auto-encoders. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 3134–3143, 2022. 2
[19] Ligong Han, Jian Ren, Hsin-Ying Lee, Francesco Barbi-
eri, Kyle Olszewski, Shervin Minaee, Dimitris Metaxas, and
Sergey Tulyakov. Show me what and tell me how: Video
synthesis via multimodal conditioning.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3615–3625, 2022. 2
[20] Ligong Han, Anastasis Stathopoulos, Tao Xue, and Dimitris
Metaxas. Unbiased auxiliary classifier gans with mine. arXiv
preprint arXiv:2006.07567, 2020. 2
[21] Erik H¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and
Sylvain Paris. Ganspace: Discovering interpretable gan con-
trols. Advances in Neural Information Processing Systems,
33:9841–9850, 2020. 2
[22] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta de-
noising score. arXiv preprint arXiv:2304.07090, 2023. 2
[23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control.
arXiv preprint
arXiv:2208.01626, 2022. 1
[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems, 33:6840–6851, 2020. 2
[25] Jonathan Ho and Tim Salimans.
Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications, 2021. 1
[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021. 2
[27] Yihao Huang, Qing Guo, and Felix Juefei-Xu.
Zero-day
backdoor attack against text-to-image diffusion models via
personalization. arXiv preprint arXiv:2305.10701, 2023. 2
[28] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,
Mingming He, Dongdong Chen, and Jing Liao.
Avatar-
craft: Transforming text into neural human avatars with
parameterized shape and pose control.
arXiv preprint
arXiv:2303.17606, 2023. 2
[29] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. 2023. 2
[30] Seung Hyun Lee, Sieun Kim, Innfarn Yoo, Feng Yang,
Donghyeon Cho, Youngseo Kim, Huiwen Chang, Jinkyu
Kim, and Sangpil Kim. Soundini: Sound-guided diffusion
for natural video editing. arXiv preprint arXiv:2304.06818,
2023. 2
[31] Pengzhi Li, QInxuan Huang, Yikang Ding, and Zhiheng Li.
Layerdiffusion: Layered controlled image editing with dif-
fusion models. arXiv preprint arXiv:2305.18676, 2023. 2
[32] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. arXiv
preprint arXiv:2301.07093, 2023. 2
[33] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,
Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-
fusion: Text-to-image diffusion model on mobile devices
within two seconds. arXiv preprint arXiv:2306.00980, 2023.
2
[34] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki
Tanaka. Negative-prompt inversion: Fast image inversion
for editing with text-guided diffusion models. arXiv preprint
arXiv:2305.16807, 2023. 1, 2, 3, 6
[35] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,
and Daniel Cohen-Or. Null-text inversion for editing real
images using guided diffusion models.
arXiv preprint
arXiv:2211.09794, 2022. 1, 2, 3, 6
[36] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2
[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021. 2
[38] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models.
In International
Conference on Machine Learning, pages 8162–8171. PMLR,
2021. 2
[39] Byong Mok Oh, Max Chen, Julie Dorsey, and Fr´edo Durand.
Image-based modeling and photo editing. In Proceedings of
the 28th annual conference on Computer graphics and inter-
active techniques, pages 433–442, 2001. 2
[40] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Edit-
ing implicit assumptions in text-to-image diffusion models.
arXiv preprint arXiv:2303.08084, 2023. 2
[41] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation of
stylegan imagery. arXiv preprint arXiv:2103.17249, 2021. 2
[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022. 2
[43] Aditya Ramesh,
Mikhail Pavlov,
Gabriel Goh,
Scott
Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever.
Zero-shot text-to-image generation.
arXiv
preprint arXiv:2102.12092, 2021. 2
[44] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido
Gerig, and Peyman Milanfar.
Image deblurring with
domain generalizable diffusion models.
arXiv preprint
arXiv:2212.01789, 2022. 2
[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer.
High-resolution image
synthesis with latent diffusion models.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 10684–10695, June 2022. 2
[46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242, 2022. 2
[47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour,
Burcu Karagol Ayan,
S Sara Mahdavi,
8

Rapha Gontijo Lopes, et al.
Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022. 2
[48] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-
booth: Personalized text-to-image generation without test-
time finetuning. arXiv preprint arXiv:2304.03411, 2023. 2
[49] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning, pages 2256–2265. PMLR, 2015.
2
[50] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro
Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,
Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image
generation in any style. arXiv preprint arXiv:2306.00983,
2023. 2
[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations, 2021. 1, 2, 5, 6
[52] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever.
Consistency
models.
arXiv
preprint
arXiv:2303.01469, 2023. 2
[53] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in Neural
Information Processing Systems, 32, 2019. 2
[54] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456, 2020. 2
[55] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan
Jing, Fei Wu, and Bingkun Bao. Df-gan: Deep fusion gener-
ative adversarial networks for text-to-image synthesis. arXiv
preprint arXiv:2008.05865, 2020. 2
[56] Ryan Tibshirani.
Lecture notes on proximal gradient de-
scent. https://www.stat.cmu.edu/˜ryantibs/
convexopt/lectures/prox-grad.pdf, 2019. 5
[57] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir
Aberman.
p+: Extended textual conditioning in text-to-
image generation. arXiv preprint arXiv:2303.09522, 2023.
2
[58] Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan
Lu, and Xiaodong Lin. Compositional text-to-image synthe-
sis with attention map control of diffusion models. arXiv
preprint arXiv:2305.13921, 2023. 2
[59] Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao
Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen,
and Mingyuan Zhou.
Patch diffusion: Faster and more
data-efficient training of diffusion models.
arXiv preprint
arXiv:2304.12526, 2023. 2
[60] Rundi Wu, Ruoshi Liu, Carl Vondrick, and Changxi Zheng.
Sin3dm: Learning a diffusion model from a single 3d tex-
tured shape. arXiv preprint arXiv:2305.15399, 2023. 2
[61] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 1316–
1324, 2018. 2
[62] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderra-
man, and Shihao Ji. Improving text-to-image synthesis us-
ing contrastive learning. arXiv preprint arXiv:2107.02423,
2021. 2
[63] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al.
Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789, 2022. 2
[64] Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang,
Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian
Theobalt, and Eric Xing. Multimodal image synthesis and
editing: A survey. arXiv preprint arXiv:2112.13592, 2021.
2
[65] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and
Yinfei Yang. Cross-modal contrastive learning for text-to-
image generation.
In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
833–842, 2021. 2
[66] Jiaxin Zhang, Kamalika Das, and Sricharan Kumar. On the
robustness of diffusion inversion in image manipulation. In
ICLR 2023 Workshop on Trustworthy and Reliable Large-
Scale Machine Learning Models, 2023. 2
[67] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang,
Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver
Deussen, and Changsheng Xu. Prospect: Expanded condi-
tioning for the personalization of attribute-aware image gen-
eration. arXiv preprint arXiv:2305.16225, 2023. 2
[68] Zhongping Zhang, Jian Zheng, Jacob Zhiyuan Fang, and
Bryan A Plummer. Text-to-image editing by image infor-
mation removal. arXiv preprint arXiv:2305.17489, 2023. 2
[69] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan:
Dynamic memory generative adversarial networks for text-
to-image synthesis. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
5802–5810, 2019. 2
9

