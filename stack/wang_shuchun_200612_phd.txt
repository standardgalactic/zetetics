Exponential Smoothing for Forecasting and
Bayesian Validation of Computer Models
A Thesis
Presented to
The Academic Faculty
by
Shuchun Wang
In Partial Fulﬁllment
of the Requirements for the Degree
Doctor of Philosophy
Industrial and Systems Engineering
Georgia Institute of Technology
December 2006
Copyright c⃝2006 by Shuchun Wang

Exponential Smoothing for Forecasting and
Bayesian Validation of Computer Models
Approved by:
Professor Kwok-Leung Tsui, Advisor
School of Industrial and Systems
Engineering
Georgia Institute of Technology
Professor Wei Jiang
Department of Systems Engineering
and Engineering Management
Stevens Institute of Technology
Professor David M. Goldsman
School of Industrial and Systems
Engineering
Georgia Institute of Technology
Professor Roshan J. Vengazhiyil
School of Industrial and Systems
Engineering
Georgia Institute of Technology
Professor Ming Yuan
School of Industrial and Systems
Engineering
Georgia Institute of Technology
Date Approved: August 15, 2006

To my parents
iii

ACKNOWLEDGEMENTS
First and foremost, I would like to express my special and sincere thanks to my
dissertation advisor, Dr. Kwok-Leung Tsui, for his inspiration, guidance, and en-
couragement during my studies in the School of Industrial and Systems Engineering
at the Georgia Institute of Technology.
I would also like to express my great appreciation to Dr. David Goldsman, Dr.
Wei Jiang, Dr. Roshan Joseph Vengazhiyil, and Dr. Ming Yuan for serving on my
dissertation committee and for their valuable suggestions and comments. I am very
grateful to Dr. Wei Jiang for his continued support, valuable discussions, and critical
comments throughout my research.
I would like to extend my appreciation to all my friends at the Georgia Institute
of Technology for their continued help and support.
Finally, I would like to thank my parents, my sister Shujie, my brother Shufeng,
and my friend Wenzhang. Their love, support, encouragement and tolerance have
helped me through diﬃcult times and given me strength and courage to face chal-
lenges.
iv

TABLE OF CONTENTS
DEDICATION
iii
ACKNOWLEDGEMENTS
iv
LIST OF TABLES
ix
LIST OF FIGURES
xii
SUMMARY
xv
I
INTRODUCTION
1
1.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
II
STATISTICAL FOUNDATIONS OF EXPONENTIAL SMOOTH-
ING (ES) METHODS
7
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
ES methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.1
Simple Exponential Smoothing (SES) . . . . . . . . . . . . .
8
2.2.2
Holt’s Method . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2.3
Holt-Winters’ Method . . . . . . . . . . . . . . . . . . . . . .
10
2.2.4
Other ES methods . . . . . . . . . . . . . . . . . . . . . . . .
12
2.3
Three Statistical Models
. . . . . . . . . . . . . . . . . . . . . . . .
13
2.3.1
ARIMA Model . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.3.2
Multiple Source of Error (MSOE) State Space Model
. . . .
16
2.3.3
Single Source of Error (SSOE) State Space Model
. . . . . .
18
2.4
Statistical Models Underlying ES methods
. . . . . . . . . . . . . .
19
2.4.1
ARIMA Model . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.4.2
MSOE State Space Model
. . . . . . . . . . . . . . . . . . .
23
2.4.3
SSOE State Space Model . . . . . . . . . . . . . . . . . . . .
27
2.5
Discussion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
v

III PERFORMANCE ANALYSIS OF ES METHODS FOR TIME SE-
RIES OF ARIMA TYPE
32
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3.2
SES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3.2.1
Nt is an ARIMA(0, 1, q) process
. . . . . . . . . . . . . . . .
34
3.2.2
Nt is an ARIMA(0, 0, q) process
. . . . . . . . . . . . . . . .
40
3.2.3
Nt is an ARIMA(1, d, 0) process . . . . . . . . . . . . . . . .
45
3.2.4
Nt is an ARIMA(1, d, 1) process . . . . . . . . . . . . . . . .
49
3.2.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3
Holt’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.3.1
Nt is an ARIMA(0, 2, q) process
. . . . . . . . . . . . . . . .
57
3.3.2
Nt is an ARIMA(0, 1, q) process
. . . . . . . . . . . . . . . .
65
3.3.3
Nt is an ARIMA(0, 0, q) process
. . . . . . . . . . . . . . . .
72
3.3.4
Nt is an ARIMA(1,d,0) process . . . . . . . . . . . . . . . . .
77
3.3.5
Nt is an ARIMA(1,d,1) process . . . . . . . . . . . . . . . . .
83
3.3.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
IV EXPONENTIAL SMOOTHING WITH COVARIATES
96
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
4.2
Exponential Smoothing with Covariates (ESCov) . . . . . . . . . . .
97
4.2.1
The Procedure . . . . . . . . . . . . . . . . . . . . . . . . . .
99
4.2.2
A General Form . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.2.3
Parameters Estimation
. . . . . . . . . . . . . . . . . . . . .
100
4.3
Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . .
102
4.3.1
Four Accuracy Measures
. . . . . . . . . . . . . . . . . . . .
102
4.3.2
Two Examples . . . . . . . . . . . . . . . . . . . . . . . . . .
102
4.4
Statistical Properties
. . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.4.1
Underlying Statistical Models
. . . . . . . . . . . . . . . . .
111
4.4.2
Maximum Likelihood Estimation . . . . . . . . . . . . . . . .
112
4.4.3
Model Selection . . . . . . . . . . . . . . . . . . . . . . . . .
114
vi

4.4.4
Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . .
116
4.5
Related Statistical Model . . . . . . . . . . . . . . . . . . . . . . . .
124
4.6
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
V
BAYESIAN VALIDATION OF COMPUTER MODELS
132
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.2
Statistical Framework . . . . . . . . . . . . . . . . . . . . . . . . . .
137
5.3
The Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . . . .
138
5.3.1
Prior Distributions for Unknown Parameters . . . . . . . . .
138
5.3.2
Posterior Distribution of Model Bias δ(·)
. . . . . . . . . . .
139
5.3.3
Posterior Distribution of Computer Output Y m(·)
. . . . . .
142
5.3.4
Posterior Distribution of Real System Output Y r(·)
. . . . .
144
5.4
When De ̸⊆Dm and φm, Pm, φδ, Pδ, and τ are unknown . . . . . .
147
5.4.1
Prediction of Y m(De −Dm)
. . . . . . . . . . . . . . . . . .
147
5.4.2
Estimation of φm, Pm, φδ, Pδ, and τ
. . . . . . . . . . . . .
147
5.5
A Bayesian Validation Procedure . . . . . . . . . . . . . . . . . . . .
149
5.6
Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . .
153
5.6.1
Example 1: Fluidized-Bed Coating . . . . . . . . . . . . . . .
154
5.6.2
Example 2: Linear Cellular Alloys . . . . . . . . . . . . . . .
155
5.6.3
Example 3: Compressible Shear Layer . . . . . . . . . . . . .
158
5.6.4
Sensitivity Study
. . . . . . . . . . . . . . . . . . . . . . . .
164
5.7
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
5.7.1
Posterior Distributions of βm and σ2
m
. . . . . . . . . . . . .
169
5.7.2
Posterior Distributions of βδ and σ2
δ . . . . . . . . . . . . . .
171
5.7.3
Posterior Distribution of δ(D)
. . . . . . . . . . . . . . . . .
171
5.7.4
Densities p(ym|φm) and p(ye|ym, φδ, τ) . . . . . . . . . . . .
174
VI BAYESIAN VALIDATION OF COMPUTER MODELS: PERFOR-
MANCE AND GENERALIZATION
175
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.2
Performance of The Proposed Bayesian Approach
. . . . . . . . . .
176
vii

6.2.1
Number of Replications in Physical Experiments . . . . . . .
177
6.2.2
Variance of Model Bias δ(x), σ2
δ
. . . . . . . . . . . . . . . .
188
6.2.3
Variance of Computer Model Y m(x), σ2
m . . . . . . . . . . . .
194
6.2.4
Conclusions
. . . . . . . . . . . . . . . . . . . . . . . . . . .
199
6.3
A Generalization to The Proposed Bayesian Approach . . . . . . . .
200
6.3.1
Correlation between Y m(x) and δ(x) . . . . . . . . . . . . . .
200
6.3.2
Posterior Distributions of δ(x) and Y m(x) . . . . . . . . . . .
201
6.3.3
Full Conditional Distributions of βm, σ2
m, βδ, and σ2
δ . . . . .
204
VII SUMMARY AND FUTURE RESEARCH
208
7.1
Exponential Smoothing for Forecasting
. . . . . . . . . . . . . . . .
208
7.1.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
7.1.2
Future Research . . . . . . . . . . . . . . . . . . . . . . . . .
208
7.2
Bayesian Validation of Computer Models
. . . . . . . . . . . . . . .
209
7.2.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
7.2.2
Future Research . . . . . . . . . . . . . . . . . . . . . . . . .
209
VITA
217
viii

LIST OF TABLES
2.1
ES methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2
Error-Correction Form Updating Equations and h-step-ahead Fore-
casts of ES methods (N - None, A - Additive, M - Multiplicative,
DA - Damped Additive, DM - Damped Multiplicative) . . . . . . . .
14
2.3
Underlying ARIMA Models for ES methods (N - None, A - Additive,
M - Multiplicative, DA - Damped Additive, DM - Damped Multi-
plicative) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.4
Underlying SSOE State Space Models for ES methods. ξt = u(βt−1)ǫt,
and constant u(βt−1) gives homoscedastic models while time-varying
u(βt−1) results in heteroscedastic models. (N - None, A - Additive, M
- Multiplicative, DA - Damped Additive, DM - Damped Multiplicative) 30
2.5
Construction of Underlying SSOE State Space Models for ES methods
( ˆβt = (lt, bt, ct, · · · , ct−M+1)T, and βt = (µt, βt, st, · · · , st−M+1)T) . . .
31
2.6
Relationships among Three Types of Underlying Statistical Models for
ES methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.1
SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,1,1) . .
36
3.2
SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,1,2) . .
39
3.3
SES – Optimal α and Minimum MSE/σ2, Nt is an MA(1) . . . . . .
42
3.4
SES – Optimal α and Minimum MSE/σ2, Nt is a MA(2) . . . . . . .
43
3.5
SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,1,0) . .
46
3.6
SES – Optimal α and Minimum MSE/σ2, Nt is an AR(1)
. . . . . .
48
3.7
SES – MSE/σ2 for φ1 = 0.5, Nt is an AR(1), αopt = 0.5.
. . . . . . .
48
3.8
SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,1,1) . .
51
3.9
SES – Optimal α and Minimum MSE/σ2, Nt is an ARMA(1,1)
. . .
54
3.10 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,2,1) 59
3.11 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,2,2) 64
3.12 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,1,1) 66
3.13 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,1,2) 71
3.14 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is a MA(1)
73
3.15 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is a MA(2)
76
ix

3.16 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,2,0) 77
3.17 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,1,0) 81
3.18 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an AR(1)
83
3.19 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,2,1) 86
3.20 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,1,1) 90
3.21 Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an ARMA(1,1) 93
4.1
UK per capita Consumption of Spirits Forecasts for 1925-1938 . . . .
107
4.2
US Motor Vehicle Death Forecasts for 1951-1970 . . . . . . . . . . . .
110
4.3
Underlying SSOE State Space Models for ESCov.
(N - None, A -
Additive, M - Multiplicative, DA - Damped Additive)
. . . . . . . .
113
4.4
Four Classes of SSOE State Space Models Underlying ESCov . . . . .
117
4.5
Models in Class 1. For k ≥0, 0k is a k × 1 vector of zeros, and Ik is a
k × k identity matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
4.6
Class 1 – Conditional Mean and Variance of Yt+h Given βt and zt+h .
119
4.7
Class 2 – Conditional Mean and Variance of Yt+h Given βt and zt+h .
121
4.8
Reduced Form Transfer Function Models of SSOE State Space Un-
derlying Additive ESCov, (N - None, A - Additive, DA - Damped
Additive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
5.1
The Fluidized-Bed Coating Example
. . . . . . . . . . . . . . . . . .
156
5.2
The Fluidized-Bed Coating Example: prediction of δ(x) . . . . . . . .
157
5.3
The Fluidized-Bed Coating Example: prediction of Y r(x) . . . . . . .
157
5.4
The Linear Cellular Alloys Example . . . . . . . . . . . . . . . . . . .
159
5.5
The Linear Cellular Alloys Example: prediction of δ(x) . . . . . . . .
160
5.6
The Linear Cellular Alloys Example: prediction of Y r(x) . . . . . . .
160
5.7
The Compressible Shear Layer Example: physical observations . . . .
164
5.8
The Compressible Shear Layer Example: computer outputs . . . . . .
165
5.9
The Compressible Shear Layer Example: model validation with ∆0 =
0.12
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
5.10 The Compressible Shear Layer Example: RMSPEs
. . . . . . . . . .
165
6.1
Computer Outputs at Dm = {xm
1 , · · · , xm
20} . . . . . . . . . . . . . . .
180
x

6.2
Physical Observations at De = {xe
1, · · · , xe
7} for J = 1 . . . . . . . . .
180
6.3
RMSPEs of Predictions of Y r(x) or Y m(x) at 201 x values (from 0 to
10 with an increment 0.05) . . . . . . . . . . . . . . . . . . . . . . . .
181
6.4
Estimated Var(Y r(xe
i)|ye, ym) at xe
i ∈De, i = 1, 2, · · · , 7
. . . . . . .
181
6.5
Estimated Experimental Error Variance σ2
ǫ . . . . . . . . . . . . . . .
181
6.6
RMSPEs (using the means of physical observations at De)
. . . . . .
182
6.7
Estimated Experimental Error Variance σ2
ǫ (using the means of physi-
cal observations at De) . . . . . . . . . . . . . . . . . . . . . . . . . .
182
6.8
RMSPEs of Predictions of Y r(x) or Y m(x) at 201 x values (from 0 to
10 with an increment 0.05) . . . . . . . . . . . . . . . . . . . . . . . .
190
6.9
Estimated Experimental Error Variance σ2
ǫ . . . . . . . . . . . . . . .
191
6.10 RMSPEs of Predictions of Y r(x) or Y m(x) at 201 x values (from 0 to
10 with an increment 0.05) . . . . . . . . . . . . . . . . . . . . . . . .
195
6.11 Estimated Experimental Error Variance σ2
ǫ . . . . . . . . . . . . . . .
196
xi

LIST OF FIGURES
1.1
US Motor Vehicle Deaths and Miles of Travel from 1911 to 1970 . . .
4
2.1
The big triangular area deﬁnes the parameter space for invertible ARIMA(0,2,2)
model; the shaded area in (a) deﬁnes the parameter space for invertible
ARIMA(0,2,2) model underlying Holt’s method with α1 and α2 falling
into the interval (0,1]; the shaded area in (b) deﬁnes the parameter
space for the invertible ARIMA(0,2,2) model reduced from the state
space model (2.58). (Dashed line – boundary not included; solid line –
boundary included) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.1
SES – MSE/σ2 as a function of α, Nt is an ARIMA(0,1,0). . . . . . .
35
3.2
SES – MSE/σ2 as a function of α, Nt is an ARIMA(0,1,1) . . . . . .
37
3.3
SES – MSE/σ2 as a function of α, Nt is an ARIMA(0,1,2) . . . . . .
38
3.4
SES – MSE/σ2 as a function of α, Nt is a white noise . . . . . . . . .
41
3.5
SES – MSE/σ2 as a function of α, Nt is an MA(1)
. . . . . . . . . .
42
3.6
SES – MSE/σ2 as a function of α, Nt is an MA(2)
. . . . . . . . . .
44
3.7
SES – MSE/σ2 as a function of α, Nt is an ARIMA(1,1,0) . . . . . .
46
3.8
SES – MSE/σ2 as a function of α, Nt is an AR(1) . . . . . . . . . . .
48
3.9
SES – MSE/σ2 as a function of α, Nt is an ARIMA(1,1,1) . . . . . .
50
3.10 SES – MSE/σ2 as a function of α, Nt is an ARMA(1,1)
. . . . . . .
53
3.11 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,0) 58
3.12 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,1) 60
3.13 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,2) 62
3.14 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,2) 63
3.15 Holt’s Method – MSE/σ2 as a function of α1, Nt is a random walk
65
3.16 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,1,1) 67
3.17 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,1,2) 69
3.18 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,1,2) 70
3.19 Holt’s Method – MSE/σ2 as a function of α1, Nt is a white noise
.
72
3.20 Holt’s Method – MSE/σ2 as a function of α1, Nt is a MA(1) . . . .
74
3.21 Holt’s Method – MSE/σ2 as a function of α1, Nt is a MA(2) . . . .
75
xii

3.22 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,2,0) 78
3.23 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,1,0) 80
3.24 Holt’s Method – MSE/σ2 as a function of α1, Nt is an AR(1)
. . .
82
3.25 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,2,1) 84
3.26 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,2,1) 85
3.27 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,1,1) 88
3.28 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,1,1) 89
3.29 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARMA(1,1)
91
3.30 Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARMA(1,1)
92
4.1
US Motor Vehicle Deaths and Miles of Travel from 1911 to 1970 . . .
98
4.2
UK Spirit Consumptions per Capita, Income per Capita and Price of
Spirits from 1870 to 1938.
. . . . . . . . . . . . . . . . . . . . . . . .
104
4.3
UK Consumptions of Spirits per Capita and Forecasts for 1929–1938 .
105
4.4
US Motor Vehicle Deaths and Forecasts for 1956–1970
. . . . . . . .
109
4.5
US Vehicle Miles of Travel and Forecasts for 1956–1970 . . . . . . . .
109
5.1
The Compressible Shear Layer Example: RMSPE as a function of φm. 166
5.2
The Compressible Shear Layer Example: prediction of Y m(x). . . . .
166
5.3
The Compressible Shear Layer Example: prediction of δ(x).
. . . . .
167
5.4
The Compressible Shear Layer Example: prediction of Y r(x) using
both computer outputs ym and physical observations ye. . . . . . . .
167
5.5
The Compressible Shear Layer Example: prediction of Y r(x) using
only physical observations ye. . . . . . . . . . . . . . . . . . . . . . .
168
5.6
The Fluidized-Bed Coating Example: boxplots of RMSPEs . . . . . .
170
6.1
Model Bias δ(x) – one realization of the Gaussian process with µδ(x) =
0.2x, σ2
δ = 1, φδ = 1, and Pδ = 2 . . . . . . . . . . . . . . . . . . . . .
182
6.2
Computer Model Y m(x) – one realization of the Gaussian process with
µm(x) = 10, σ2
m = 1, φm = 2, and Pm = 2; Real System Output
Y r(x) = Y m(x) + δ(x)
. . . . . . . . . . . . . . . . . . . . . . . . . .
183
6.3
Physical Observations ye and Computer Outputs ye for J = 10
. . .
183
6.4
Predictions of Y r(x) for J = 1 . . . . . . . . . . . . . . . . . . . . . .
184
6.5
Predictions of Y r(x) for J = 2 . . . . . . . . . . . . . . . . . . . . . .
184
xiii

6.6
Predictions of Y r(x) for J = 5 . . . . . . . . . . . . . . . . . . . . . .
185
6.7
Predictions of Y r(x) for J = 10
. . . . . . . . . . . . . . . . . . . . .
185
6.8
Predictions of Y r(x) for J = 20
. . . . . . . . . . . . . . . . . . . . .
186
6.9
Estimated Var(Y r(x)|ye, ym) . . . . . . . . . . . . . . . . . . . . . . .
186
6.10 Predictions of Y m(x) . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
6.11 Estimated Var(Y m(x)|ym) . . . . . . . . . . . . . . . . . . . . . . . .
187
6.12 Predictions of Y r(x)
. . . . . . . . . . . . . . . . . . . . . . . . . . .
188
6.13 Estimated Var(Y r(x)|ye, ym) . . . . . . . . . . . . . . . . . . . . . . .
189
6.14 Model bias δ(x) – one realization of a Gaussian process with µδ(x) =
0.2x, φδ = 1, Pδ = 2, and σ2
δ = [0.01, 0.2, 0.5, 1, 2, 5, 10, 20] . . . . . . .
192
6.15 Estimated σ2
ǫ versus log(σ2
δ) . . . . . . . . . . . . . . . . . . . . . . .
192
6.16 Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 1 and σ2
δ = 0.01 . . . . . . . . . . . . . . . . . . . . . .
193
6.17 Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 1 and σ2
δ = 20 . . . . . . . . . . . . . . . . . . . . . . .
193
6.18 Computer Model Y m(x) – one realization of a Gaussian process with
µm(x) = 10, φm = 2, Pm = 2, and σ2
m = [0.01, 0.2, 0.5, 1, 2, 5, 10, 20] . .
197
6.19 Estimated σ2
ǫ versus log(σ2
m) . . . . . . . . . . . . . . . . . . . . . . .
197
6.20 Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 0.01 and σ2
δ = 1 . . . . . . . . . . . . . . . . . . . . . .
198
6.21 Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 20 and σ2
δ = 1 . . . . . . . . . . . . . . . . . . . . . . .
198
xiv

SUMMARY
Despite their success and widespread usage in industry and business, ES
methods have received little attention from the statistical community. We investigate
three types of statistical models that have been found to underpin ES methods. They
are ARIMA models, state space models with multiple sources of error (MSOE), and
state space models with a single source of error (SSOE). We establish the relationship
among the three classes of models and conclude that the class of SSOE state space
models is broader than the other two and provides a formal statistical foundation for
ES methods. To better understand ES methods, we investigate the behaviors of ES
methods for time series generated from diﬀerent processes. We mainly focus on time
series of ARIMA type.
ES methods forecast a time series using only its own history. To include covariates
into ES methods for better forecasting a time series, we propose a new forecasting
method, Exponential Smoothing with Covariates (ESCov). ESCov uses an ES method
to model what left unexplained in a time series by covariates.
We establish the
optimality of ESCov, identify SSOE state space models underlying ESCov, and derive
analytically the variances of forecasts by ESCov. Empirical studies show that ESCov
outperforms ES methods and regression with ARIMA errors. We suggest a model
selection procedure for choosing appropriate covariates and ES methods in practice.
Computer models have been commonly used to investigate complex systems for
which physical experiments are highly expensive or very time-consuming. Before us-
ing a computer model, we need to address an important question “How well does
the computer model represent the real system?” The process of addressing this ques-
tion is called computer model validation that generally involves the comparison of
xv

computer outputs and physical observations. In this thesis, we propose a Bayesian
approach to computer model validation. This approach integrates together computer
outputs and physical observation to give a better prediction of the real system out-
put. This prediction is then used to validate the computer model. We investigate the
impacts of several factors on the performance of the proposed approach and propose
a generalization to the proposed approach.
xvi

CHAPTER I
INTRODUCTION
1.1
Motivation
Industry and business have continuously used exponential smoothing (ES) methods,
a collection of extrapolative forecasting methods that forecast a time series based on
only its historical values. According to a mail survey conducted by Mentzer and Kahn
(1995), who surveyed 207 forecasting executives, ES methods are the most common
methods of forecasting. Their popularity can be attributed to several practical con-
siderations. First, they are very simple in concept and easy to understand. Second,
they require little computational eﬀort and small data storage space. Third, they can
achieve ﬂexible adaptivity by varying smoothing parameters to account for changes
in the behaviors of the time series being forecasted. Last and more importantly, ES
methods, as shown by numerous empirical studies based on a wide class of time se-
ries, real or simulated (Makridakis and Hibon 1979, 2000; Makridakis et al. 1982;
Makridakis et al. 1993; Chen 1997), perform as well as or sometimes better than sta-
tistically sophisticated methods such as the autoregressive integrated moving average
(ARIMA) approach advocated by Box and Jenkins (Box et al. 1994). Among the
various ES methods, the three best-known and most commonly used ones are simple
exponential smoothing (Brown 1959, 1963), Holt’s linear trend method (Holt 1957),
and Holt-Winters’ seasonal method (Winters 1960).
Despite their widespread usage, ES methods, initially proposed as heuristic pro-
cedures for forecasting (Brown 1959, 1963), had received little attention from the
statistical community. Some statistical models such as the ARIMA models have been
found to underlie certain ES methods (Muth 1960, Harrison 1967, Roberts 1982,
1

Harvey 1984, and Gardner and Mckenzie 1985). In other words, those ES methods
are optimal (i.e., provide minimum mean squared error forecast) for the underlying
statistical models. However, a general statistical framework was still missing. An
important development in the study of ES methods occurred in 1985 when Snyder
(1985) proposed a class of linear state space models that rely on only a single source of
randomness and suggested the use of such models as an explanation for ES methods.
However, this insight went largely unnoticed until the work of Ord et al. (1997), who
expressed state space models with a single source of error in a very general form that
encompasses both linear and nonlinear cases with homoscedastic or heteroscedastic
variance. This general formulation provides a formal statistical foundation for ES
methods. Not only is the identiﬁcation of underlying statistical models for ES meth-
ods straightforward, but the underlying statistical models are also not unique. This
non-uniqueness could be considered as a statistical explanation for the robustness
(i.e., reasonably good performance on a wide class of time series) of ES methods.
Another way to look at the robustness of ES methods is to see how well they
perform when the data are generated from a model for which they are not optimal.
Cox (1961) studied the performance of simple exponential smoothing on a station-
ary ﬁrst-order autoregressive (AR(1)) model and concluded that simple exponential
smoothing performs quite well in terms of the mean squared one-step-ahead forecast
error when the AR(1) model has a positive lag-one autocorrelation. Cohen (1963)
examined the behaviors of simple exponential smoothing and Brown’s double expo-
nential smoothing (Brown 1963), a special case of Holt’s method, when the data were
generated from either a white noise process or an AR(1) model and provided a range
of values of the smoothing parameters to minimize the mean squared forecast error.
Cogger (1973) investigated the forecasting performance of Brown’s double exponen-
tial smoothing on an ARIMA(0,1,1) model. The performance of ES methods, mainly
simple exponential smoothing and Holt’s method, under non-optimal situations was
2

also studied in the context of process control (Ingolfsson and Sachs 1993, Castillo
2001). Nevertheless, the types of models considered were very limited.
ES methods forecast a time series using only the series’ own historical values. The
history of a time series certainly contains knowledge about its future. However, other
information beyond what is available in a series’ own history may also shed light on
the series’ movements along time and therefore, lead to more accurate forecasting of
its future if incorporated. For example, Figure 1.1(a) displays the number of deaths
(in thousands) due to motor vehicle accidents in the United States from 1911 to 1970.
To forecast the motor vehicle deaths in the years after 1970, ES methods use only
historical observations from 1911 to 1970. However, the number of motor vehicle
deaths may be aﬀected by various factors such as annual vehicle miles of travel, road
quality, driver behaviors, and weather conditions. Figure 1.1(b) shows annual vehicle
miles of travel (in billions) in the United States from 1911 to 1970. The mile series
exhibits movements that appear to be correlated with those of the death series. For
instance, both series dropped suddenly in 1942 and then started to climb rapidly
in 1944. This suggests the use of inﬂuencing factors that explain the movements of
the death series, such as ﬁtting a regression model for the number of deaths with
inﬂuencing factors as explanatory variables. Using only the series’ own history for
forecasting, ES methods might lose valuable information contained in inﬂuencing
factors. On the other hand, inﬂuencing factors may not be able to completely explain
the movements of the death series. Furthermore, although some inﬂuencing factors
such as annual miles of travel are easy to measure, some inﬂuencing factors such as
road quality are diﬃcult to quantify. A possible solution would be an approach that
uses ES methods to model what are left unexplained in the movements of the time
series being forecasted by measurable inﬂuencing factors.
Computer models are mathematic representations of real systems, such as a group
3

1910
1920
1930
1940
1950
1960
1970
0
10
20
30
40
50
60
Year
Motor Vehicle Deaths (Thousand)
(a)
1910
1920
1930
1940
1950
1960
1970
0
200
400
600
800
1000
1200
Year
Vehicle Miles of Travel (Billion)
(b)
Figure 1.1: US Motor Vehicle Deaths and Miles of Travel from 1911 to 1970
4

of partial diﬀerential equations with initial and boundary conditions for many engi-
neering problems. They have been commonly used to investigate complex systems for
which physical experiments are either highly expensive or too time consuming (Sacks
et al 1989, Welch et al 1992, Santner et al 2003).
However, before using a com-
puter model to investigate a real system, we need to address an important question
“How well does the computer model represent the real system?” Without a mean-
ingful answer to this question, any conclusions based on the analysis of outputs from
a computer model are about this computer model and can not be simply applied
to the real system being investigated. The process of determining to what degree
a computer model accurately represents the real system is referred to as computer
model validation (AIAA G-077-1998) that generally involves the comparison of the
outputs of a computer model to observations collected from physical experiments.
Recently, Oberkampf and Barone (2004) gave a comprehensive review on com-
puter model validation. They argued that computer model validation should be done
quantitatively through the use of computable measures that enable the quantitative
comparison of computer outputs and physical observations. They referred to those
computable measures as validation metrics. They then discussed a variety of con-
ceptual properties that a validation metric should possess and emphasized that a
validation metric should quantify uncertainties in the comparison of computer out-
puts and physical observations. Uncertainties could be due to random measurement
errors in physical observations and/or errors resulting from post-processing computer
outputs and/or physical observations, such as errors resulting from ﬁtting a model to
computer outputs and/or physical observations. In the same paper, they proposed
a frequentist approach to computer model validation. Their approach for the ﬁrst
time validates a computer model by quantitatively comparing computer outputs and
physical observations over a range of input variables. In this thesis, we propose a
Bayesian approach to computer model validation. The Bayesian approach has the
5

ability to take into consideration prior knowledge on the real system in the form of
prior distributions for certain parameters. It outputs the posterior distributions of
both the model bias (deﬁned as the diﬀerence of the computer model output and the
real system output) and the real system output given computer outputs and physical
observations. The posterior distribution of the model bias serves as a validation met-
ric for the quantitative comparison of computer outputs and physical observations.
Both the mean and variance of the model bias are functions of input variables, pro-
viding quantitative measures of the representativeness of the computer model over a
range of input variables. The posterior distribution of the real system output provides
a more accurate prediction of the real system output.
1.2
Organization
This thesis consists of two parts. The ﬁrst part, spanning Chapters two, three, and
four, concerns the statistical properties of ES methods and the incorporation of ex-
planatory variables into ES forecasting. The second part, consisting of Chapters ﬁve
and six, covers the topic on computer model validation. The organization of this
thesis is as follows. Chapter one explains the motivations. Chapter two investigates
statistical models underlying ES methods and provides a general statistical framework
for ES methods. Chapter three discusses the robustness of ES methods by examin-
ing their performance for diﬀerent ARIMA-type data generating processes. Chapter
four proposes a new forecasting method that bases forecasts on not only the time
series being forecasted but also explanatory variables aﬀecting the movements of the
series and studies its performance and statistical properties. Chapter ﬁve reviews
current practices in computer model validation and proposed a Bayesian approach to
the validation of computer models. Chapter six investigates the performance of the
proposed approach in diﬀerent situations and proposes a possible generalization to
the proposed approach.
6

CHAPTER II
STATISTICAL FOUNDATIONS OF
EXPONENTIAL SMOOTHING (ES) METHODS
2.1
Introduction
Widely used in industry and commerce for forecasting, ES methods are often con-
sidered as ad hoc procedures with no formal statistical foundation. Contrary to this
general-accepted but outdated perception, diﬀerent types of statistical models have
been found to underpin ES methods. Moreover, such underpinning statistical models,
as shown later, are not unique. This non-uniqueness helps to explain the robustness
of ES methods. This chapter starts with a brief review on ES methods and an in-
troduction to three types of statistical models that have been found to underlie ES
methods. Then, possible underlying models are identiﬁed for various ES methods,
followed by a discussion on the relationships among those three types of underlying
statistical models.
As regard notations, let Yt denote the observed value of a time series at time t,
ˆYt|t−h the prediction (or forecast) of Yt made at time t−h, and et|t−h the corresponding
forecast error
et|t−h = Yt −ˆYt|t−h, t > 0, h > 0
(2.1)
When h = 1, et is used in place of et|t−1 for simplicity as well as for consistency with
the literature.
7

2.2
ES methods
This section reviews ES methods. Following a detailed description of the three best-
known ES methods, simple exponential smoothing, Holt’s linear trend method, and
Holt-Winters’ seasonal method, is a discussion on the developments of other less
common methods. This section ends with a taxonomy of ES methods.
2.2.1
Simple Exponential Smoothing (SES)
A time series with a local constant level can be adequately represented by a model of
the form
Yt = µt + ǫt,
(2.2)
where ǫt is a white noise process with E[ǫt] = 0 and E[ǫ2
t] = σ2. The level µt may
change slowly with time. However, in any local time segment, a constant µ gives a
reasonably good model of the time series.
Let lt denote the estimator of the level at time t. Given lt−1, once the observation
at time t, Yt, becomes available, SES due to Brown (1959, 1963) updates the level
estimator via the recurrence equation
lt = αYt + (1 −α)lt−1,
(2.3)
where α is a smoothing parameter taking values in the interval (0, 1].
According
to equation (2.3), the estimator at time t, lt, is a weighted average of the latest
observation, Yt, and the estimator at time t −1, lt−1. The value of α can be used to
adjust the sensitivity of the estimator to changes in the series level. The larger the
α value, the higher weight Yt receives, the more sensitive the estimator to changes in
the level. The h-step-ahead forecast made at time t by SES is
ˆYt+h|t = lt, h > 0,
(2.4)
and the corresponding h-step-ahead forecast error is
et+h|t = Yt+h −ˆYt+h|t = Yt+h −lt.
(2.5)
8

Recurrence equation (2.3) has an equivalent error-correction form
lt = lt−1 + αet,
(2.6)
where et (rigorously, et should be written as et|t−1) is the one-step-ahead forecast error
et = Yt −ˆYt|t−1 = Yt −lt−1.
(2.7)
By successive substitution, equation (2.6) can be rewritten as a weighted average of
all past observations with weights declining exponentially
lt = α
∞
X
i=0
(1 −α)iYt−i,
(2.8)
thus the name exponential smoothing.
2.2.2
Holt’s Method
SES does not perform well for forecasting time series with a local linear trend, namely,
Yt = µt + ǫt = β0 + β1t + ǫt,
(2.9)
where β0 and β1 are assumed to be constant in any local time segment, but may
change slowly with time. It can be shown that the forecast by SES for a time series
with a local linear trend tends to fall behind the series itself (Brown 1963). To better
forecast time series with a local linear trend, Holt (1957) extended SES by adding
one more updating equation for the slope β1.
Let bt denote the estimator of the slope at time t. Given lt−1 and bt−1, once the
observation at time t, Yt, becomes available, Holt’s method updates the estimators
using the recurrence equations
lt = α1Yt + (1 −α1)(lt−1 + bt−1),
(2.10a)
bt = α2(lt −lt−1) + (1 −α2)bt−1,
(2.10b)
where α1 and α2 are smoothing parameters taking values in the interval (0, 1]. The
h-step-ahead forecast made at time t by Holt’s method is
ˆYt+h|t = lt + hbt, h > 0,
(2.11)
9

and the corresponding h-step-ahead forecast error is
et+h|t = Yt+h −ˆYt+h|t = Yt+h −lt −hbt.
(2.12)
Similar to SES, Holt’s method can be written in an equivalent error-correction form
(Gardner 1985)
lt = lt−1 + bt−1 + α1et,
(2.13a)
bt = bt−1 + α1α2et,
(2.13b)
where et = Yt −lt−1 −bt−1 is the one-step-ahead forecast error.
Brown (1963) also proposed a local linear trend forecasting procedure called double
exponential smoothing, which uses a single smoothing parameter to smooth both the
level and the slope. Brown’s double exponential smoothing, in error-correction form,
is given by
lt = lt−1 + bt−1 + α(2 −α)et,
(2.14a)
bt = bt−1 + α2et,
(2.14b)
Comparing equations (2.14a) and (2.14b) to (2.13a) and (2.13b) reveals that Brown’s
double exponential smoothing with a smoothing parameter α is equivalent to Holt’s
method with smoothing parameters α1 = α(2 −α) and α2 = α/(2 −α). That is,
Brown’s double exponential method is a special case of Holt’s method.
2.2.3
Holt-Winters’ Method
Neither SES nor Holt’s local linear method are appropriate for forecasting time series
with seasonal changes. To capture seasonal changes, Winters (1960) generalized Holt’s
local linear method and proposed the so-called Holt-Winters’ method, which has three
updating equations, one for the level, one for the slope, and one for the seasonality.
Depending on whether the seasonality is combined with the linear trend additively
or multiplicatively, there are two versions of Holt-Winters’ method.
10

• Additive Holt-Winters’ Method
A time series with a local linear trend and an additive seasonality can be represented
by a model of the form
Yt = β0 + β1t + st + ǫt,
(2.15)
where st is the seasonal factor at time t.
Let ct denote the estimator of st. The updating equations of the additive Holt-
Winters’ method are given by, in error-correction form,
lt = lt−1 + bt−1 + α1et,
(2.16a)
bt = bt−1 + α1α2et,
(2.16b)
ct = ct−M + (1 −α1)α3et,
(2.16c)
where M is the length of a complete seasonal cycle, and α1, α2, and α3 are smoothing
parameters taking values in the interval (0,1]. The h-step-ahead forecast made at
time t by the additive Holt-Winters’ method is
ˆYt+h|t = lt + hbt + ct−M+h, h > 0.
(2.17)
• Multiplicative Holt-Winters’ Method
A time series with a local linear trend and a multiplicative seasonality can be repre-
sented by a model of the form
Yt = (β0 + β1t) · st + ǫt.
(2.18)
The updating equations for the multiplicative Holt-Winters’ method are given by, in
error-correction form,
lt = lt−1 + bt−1 + α1et/ct−M,
(2.19a)
bt = bt−1 + α1α2et/ct−M,
(2.19b)
ct = ct−M + (1 −α1)α3et/lt.
(2.19c)
11

The h-step-ahead forecast made at time t by the multiplicative Holt-Winters’ method
is
ˆYt+h|t = (lt + hbt) · ct−M+h, h > 0.
(2.20)
2.2.4
Other ES methods
Many other ES methods have been proposed for time series forecasting.
Empiri-
cal studies have shown that Holt’s linear trend method tends to overshot the data
when used for long-term forecasting. To overcome this problem, Gardner and Mcken-
zie (1985) introduced an extra parameter, a dampening parameter φ (0 ≤φ ≤1),
into Holt’s method to gain more controls over trend extrapolations. The resulting
method is referred to as damped Holt’s method. Believing that real-life series more
likely have a multiplicative trend than an additive one, Pegels (1969) proposed a
smoothing method for forecasting time series with a multiplicative trend. Pegels’
method smoothes the ratio lt/lt−1, instead of diﬀerence lt −lt−1 as in Holt’s method,
of successive series levels. Taylor (2003), in an analogous way that Holt’s linear trend
method is damped, added a dampening parameter φ to Pegels’ method for forecasting
time series with a damped multiplicative trend. ES methods for polynomial trends
of order k, k > 1, were also developed (Gardner 1985, Montgomery, Johnson and
Gardiner 1990). However, their usages have been discouraged both from practical
considerations and by empirical studies (Makridakis et al. 1982).
Pegels (1969) proposed a taxonomy of ES methods, which was extended and
modiﬁed later by Gardner and Mckenzie (1985), Hyndman et al. (2002), and Taylor
(2003). This taxonomy is presented in Table 2.1. In this table, each method has two
components, a trend and a seasonality. For example, N-N represents the ES method
with neither trend nor seasonality, namely SES; A-N represents the method with an
additive trend but no seasonality, namely Holt’s method; and A-A and A-M represent
the additive and multiplicative Holt-Winters’ methods respectively.
12

Table 2.2 gives the updating equations in error-correction form as well as the h-
step-ahead forecast for all of the methods listed in Table 2.1. The updating equations
for the seasonal factor ct in multiplicative seasonality cases are slightly diﬀerent from
the commonly used ones. lt−1 as in N-M (or lt−1 + bt−1 as in A-M or lt−1 + φbt−1
as in DA-M) in place of lt is used in the denominator.
This small modiﬁcation
is convenient to the identiﬁcation of underlying single source of error state space
models later. Although their derivations are straightforward, the formulas for DM-A
and DM-M have not, to our knowledge, explicitly appeared in print before.
Table 2.1: ES methods
Seasonality
Trend
None
Additive
Multiplicative
None
N-N
N-A
N-M
Additive
A-N
A-A
A-M
Multiplicative
M-N
M-A
M-M
Damped Additive
DA-N
DA-A
DA-M
Damped Multiplicative
DM-N
DM-A
DM-M
2.3
Three Statistical Models
Three types of statistical models have been found to underpin ES methods. They are
autoregressive integrated moving average (ARIMA) models, state space models with
multiple sources of error, and state space models with single source of error.
2.3.1
ARIMA Model
ARIMA models and their applications in time series forecasting are discussed in many
textbooks (Box et al. 1994, Chatﬁeld 1996, Blackwell and Davis 1991). Here we only
gives a very general formula of ARIMA models. For more detailed information, such as
model identiﬁcation, estimation, and forecasting, please refer to the books mentioned
above.
13

Table 2.2: Error-Correction Form Updating Equations and h-step-ahead Forecasts of ES methods (N - None, A - Additive,
M - Multiplicative, DA - Damped Additive, DM - Damped Multiplicative)
Seasonality
Trend
N
A
M
N
lt = lt−1 + α1et
lt = lt−1 + α1et
lt = lt−1 + α1et/ct−M
ct = ct−M + (1 −α1)α3et
ct = ct−M + (1 −α1)α3et/lt−1
ˆYt+h|t = lt
ˆYt+h|t = lt + ct−M+h
ˆYt+h|t = ltct−M+h
A
lt = lt−1 + bt−1 + α1et
lt = lt−1 + bt−1 + α1et
lt = lt−1 + bt−1 + α1et/ct−M
bt = bt−1 + α1α2et
bt = bt−1 + α1α2et
bt = bt−1 + α1α2et/ct−M
ct = ct−M + (1 −α1)α3et
ct = ct−M + (1 −α1)α3et/(lt−1 + bt−1)
ˆYt+h|t = lt + hbt
ˆYt+h|t = lt + hbt + ct−M+h
ˆYt+h|t = (lt + hbt)ct−M+h
M
lt = lt−1bt−1 + α1et
lt = lt−1bt−1 + α1et
lt = lt−1bt−1 + α1et/ct−M
bt = bt−1 + α1α2et/lt−1
bt = bt−1 + α1α2et/lt−1
bt = bt−1 + α1α2et/(lt−1ct−M)
ct = ct−M + (1 −α1)α3et
ct = ct−M + (1 −α1)α3et/(lt−1bt−1)
ˆYt+h|t = ltbh
t
ˆYt+h|t = ltbh
t + ct−M+h
ˆYt+h|t = (ltbh
t )ct−M+h
DA
lt = lt−1 + φbt−1 + α1et
lt = lt−1 + φbt−1 + α1et
lt = lt−1 + φbt−1 + α1et/ct−M
bt = φbt−1 + α1α2et
bt = φbt−1 + α1α2et
bt = φbt−1 + α1α2et/ct−M
ct = ct−M + (1 −α1)α3et
ct = ct−M + (1 −α1)α3et/(lt−1 + φbt−1)
ˆYt+h|t = lt + bt
Ph
i=1 φi
ˆYt+h|t = lt + bt
Ph
i=1 φi + ct−M+h
ˆYt+h|t = (lt + bt
Ph
i=1 φi)ct−M+h
DM
lt = lt−1bφ
t−1 + α1et
lt = lt−1bφ
t−1 + α1et
lt = lt−1bφ
t−1 + α1et/ct−M
bt = bφ
t−1 + α1α2et/lt−1
bt = bφ
t−1 + α1α2et/lt−1
bt = bφ
t−1 + α1α2et/(lt−1ct−M)
ct = ct−M + (1 −α1)α3et
ct = ct−M + (1 −α1)α3et/(lt−1bφ
t−1)
ˆYt+h|t = ltb
Ph
i=1 φi
t
ˆYt+h|t = ltb
Ph
i=1 φi
t
+ ct−M+h
ˆYt+h|t = (ltb
Ph
i=1 φi
t
)ct−M+h
14

ARIMA models can be written in a very general form
(1 −
p
X
i=1
φiBi −
P
X
i=1
ΦiBiM)(1 −B)d(1 −BM)DYt = (1 +
q
X
i=1
θiBi +
P
X
i=1
ΘiBiM)ǫt
(2.21)
for an additive seasonality, or
(1 −
p
X
i=1
φiBi)(1 −
P
X
i=1
ΦiBiM)(1 −B)d(1 −BM)DYt = (1 +
q
X
i=1
θiBi)(1 +
P
X
i=1
ΘiBiM)ǫt
(2.22)
for a multiplicative seasonality, where ǫt is a white noise process with E[ǫt] = 0 and
E[ǫ2
t] = σ2; B is the back shift operator; φi, 1 ≤i ≤p, and θi, 1 ≤i ≤q, are non-
seasonal ARMA parameters; Φi, 1 ≤i ≤P, and Θi, 1 ≤i ≤Q, are seasonal ARMA
parameters; p and q are non-seasonal AR and MA orders respectively; P and Q are
seasonal AR and MA orders respectively; d is the order of non-seasonal diﬀerencing;
D is the order of seasonal diﬀerencing; and M is the length of a complete seasonal
cycle. Models (2.21) and (2.22) have a short representation
ARIMA(p, d, q) ⊕ARIMA(P, D, Q)M
(2.23)
with ⊕being “+” for additive seasonal ARIMA models and “×” for multiplicative
seasonal ARIMA models. Non-seasonal ARIMA models have an even shorter repre-
sentation
ARIMA(p, d, q).
(2.24)
We only concern these ARIMA models that are stationary and invertible. Sta-
tionarity requires that the roots of
1 −
p
X
i=1
φiBi −
P
X
i=1
ΦiBiM = 0
(2.25)
or
(1 −
p
X
i=1
φiBi)(1 −
P
X
i=1
ΦiBiM) = 0
(2.26)
15

have an absolute value great than 1. Invertibility requires that the roots of
1 +
q
X
i=1
θiBi +
P
X
i=1
ΘiBiM = 0
(2.27)
or
(1 +
q
X
i=1
θiBi)(1 +
P
X
i=1
ΘiBiM) = 0
(2.28)
are greater than 1 in absolute value. For instance, the invertibility condition for an
ARIMA(0,1,1) model is
−1 < θ1 < −1
(2.29)
while the invertibility conditions for an ARIMA(0,2,2) model are
−(1 + θ2) < θ1 < 1 + θ2 and −1 < θ2 < 1.
(2.30)
2.3.2
Multiple Source of Error (MSOE) State Space Model
MSOE State space models consist of two equations, an observation equation
Yt = xT
t βt + ǫt
(2.31)
and a transition equation
βt = Gtβt−1 + ηt,
(2.32)
where xt is a p×1 covariate vector, and Gt is a p×p transition matrix. Both xt and Gt
are assumed to be known. βt is a p×1 state vector. ǫt and ηt are mutually independent
white noise processes with zero mean and E[ǫ2
t] = σ2
t and E[ηtηT
t ] = Qt. ǫt and ηt
might be referred to as the transient error and the permanent error respectively as ǫt
aﬀects only the current observation Yt while the eﬀect of ηt will persist through time.
For a model in a state space form as given above, a recursive procedure, Kalman
Filter (KF), can be used to calculate optimal predictions of future observations and
optimal estimators of the state vector (Kalman 1960, Duncan and Horn 1972).
Let ˆβt|k denote the estimator of the state vector βt based on all the information up
to and including time k. Let Pt|k denote the p×p covariance matrix of the associated
16

estimation error, ˆβt|k −βt. That is,
Pt|k = E
h
( ˆβt|k −βt)( ˆβt|k −βt)Ti
,
(2.33)
which is also referred to as the mean squared error (MSE) matrix of ˆβt|k. To simplify,
we use ˆβt and Pt instead of ˆβt|t and Pt|t when k = t.
At time t −1, given ˆβt−1 and Pt−1, the estimator of βt and the corresponding
MSE matrix are given by the prediction equations
ˆβt|t−1 = Gt ˆβt−1,
(2.34a)
Pt|t−1 = GtPt−1GT
t + Qt,
(2.34b)
from which we can calulate the one-step-ahead prediction of Yt
ˆYt|t−1 = xT
t ˆβt|t−1
(2.35)
and the corresponding one-step-ahead prediction error
et = Yt −ˆYt|t−1 = Yt −xT
t ˆβt|t−1.
(2.36)
Once the observation at time t, Yt, becomes available, the estimator of βt and its
MSE matrix are updated via
ˆβt = ˆβt|t−1 +
Pt|t−1xt
xT
t Pt|t−1xt + σ2
t
· (Yt −xT
t ˆβt|t−1),
(2.37a)
Pt = Pt|t−1 −Pt|t−1xtxT
t Pt|t−1
xT
t Pt|t−1xt + σ2
t
.
(2.37b)
Under the assumptions that the two white noise processes {ǫt} and {ηt} are Gaus-
sian processes, and the initial state β0 has a multivariate normal distribution with
mean ˆβ0 and covariance matrix P0, the updating equations (2.37a) and (2.37b) can
be easily derived using the Bayesian theory (Meinhold and Singpurwalla, 1983) and
ˆβt simply is the posterior mean of βt|Yt, Yt−1, · · · , Y1. According to equation (2.37a),
ˆβt is a linear combination of the observations Yt, · · · , Y1, therefore a linear estimator
17

of βt. ˆβt is also known to be the minimum mean squared error (MMSE) estimator of
βt under the normality assumptions (Harvey 1990). Without the normality assump-
tions, ˆβt is the MMSE estimator of βt based on all observations up to and including
time t within the class of all linear estimators (Duncan and Horn 1972).
2.3.3
Single Source of Error (SSOE) State Space Model
The MSOE state space models in equations (2.31) and (2.32) involve two mutually
independent random errors, the transient error ǫt in the observation equation and the
permanent error ηt in the transition equation. An alternative to the independence
assumption is to assume that ǫt and ηt are perfectly correlated
ηt = ρtǫt,
(2.38)
where ρt is a p×1 vector. Under this assumption, the state space model in equations
(2.31) and (2.32) becomes
Yt = xT
t βt + ǫt,
(2.39a)
βt = Gtβt−1 + ρtǫt,
(2.39b)
which has only one source of error, namely ǫt, and is referred to as a single source
of error (SSOE) state space model. Comparing with the MSOE model in equations
(2.31) and (2.32), the SSOE model in (2.39a) and (2.39b) requires the speciﬁcation
of a p × 1 vector ρt rather than a p × p covariance matrix Qt.
Substituting equation (2.39b) into equation (2.39a) gives
Yt = xT
t Gtβt−1 + (xT
t ρt + 1)ǫt.
(2.40)
Let zt = xT
t Gt, ξt = (xT
t ρt + 1)ǫt, and αt = ρt/(xT
t ρt + 1), we have the more
traditional form of SSOE state space models (Ord et al. 1997)
Yt = zT
t βt−1 + ξt,
(2.41a)
βt = Gtβt−1 + αtξt,
(2.41b)
18

in which the observation equation involves βt−1 rather than βt.
Estimation and prediction for SSOE state space models can be carried out easily.
In equation (2.41a), zT
t βt−1 is the one-step-ahead prediction of Yt made at time t −1,
and ξt is the corresponding one-step-ahead prediction error. Solving equation (2.41a)
for ξt results in ξt = Yt −zT
t βt−1. Substituting it into equation (2.41b) gives the
updating equation for the state vector βt
βt = Gtβt−1 + αt(Yt −zT
t βt−1).
(2.42)
Snyder (1985) is the ﬁrst one discussing SSOE state space models. His model
is slightly diﬀerent from the model in equations (2.39a) and (2.39b) in that, in his
model, the transition equation involves ǫt−1 instead of ǫt
βt = Gtβt−1 + ρtǫt−1.
(2.43)
Snyder (1985) only investigated SSOE state space models that are linear and ho-
moscedastic (i.e., E[ǫ2
t] is constant). Ord et al. (1997) generalized SSOE models for
nonlinear and heteroscedastic cases.
2.4
Statistical Models Underlying ES methods
This section investigates underlying statistical models for various ES methods.
2.4.1
ARIMA Model
It is well known that (Muth 1960, Roberts 1982)
a). SES with the smoothing parameter
α = 1 + θ1
(2.44)
provides optimal forecast for the ARIMA(0,1,1) model
(1 −B)Yt = (1 + θ1B)ǫt,
(2.45a)
−1 < θ1 ≤0;
(2.45b)
19

b). Holt’s method with α1 and α2 given by
α1 = 1 −θ2 and α2 = 1 + θ1 + θ2
1 −θ2
(2.46)
provides optimal forecast for the ARIMA(0,2,2) model
(1 −B)2Yt = (1 + θ1B + θ2B2)ǫt,
(2.47a)
−(1 + θ2) < θ1 ≤−2θ2, 0 ≤θ2 < 1.
(2.47b)
The more restricted conditions on θ1 and θ2 in equations (2.45b) and (2.47b) than
these imposed by invertibility (see equations (2.29) and (2.30)) are due to the fact
that the smoothing parameters in ES methods are conﬁned to the interval (0,1].
Smoothing parameters with values greater than 1 might be used. However, they are
hard to explain and then not considered here. For example, for SES, when 1 < α < 2,
equation (2.8) shows that weights assigned to past observations will oscillate in sign,
which is counterintuitive and very unusual in real-world applications.
It was shown that Brown’s double exponential smoothing with a smoothing pa-
rameter α is equivalent to Holt’s method with smoothing parameters α1 = α(2 −α)
and α2 = α/(2 −α), at which equations (2.46) and (2.47) become
α = 1 −
p
θ2
(2.48)
and
(1 −B)2Yt = (1 −
p
θ2B)2ǫt,
(2.49a)
0 ≤θ2 < 1.
(2.49b)
That is, Brown’s double exponential smoothing with α = 1 −√θ2 provides optimal
forecast for the equal-root ARIMA(0,2,2) model in equations (2.49a) and (2.49b).
Furthermore, as Brown’s double exponential smoothing is a special case of Holt’s
method, this equal-root ARIMA(0,2,2) model is a subclass of the ARIMA(0,2,2)
model in (2.47a) and (2.47b).
20

Roberts (1982) and Gardner and Mckenzie (1985) also identiﬁed the ARIMA
models that underpin the damped Holt’s method and the additive Holt-Winters’
method.
In fact, all of the additive ES methods in Table 2.1 (i.e., ES methods
without multiplicative components, namely N-N, N-A, A-N, A-A, DA-N, and DA-A)
have underlying ARIMA models. For example, for DA-A, replacing the one-step-
ahead forecast ˆYt|t−1 by Yt −et yields
Yt = lt−1 + φbt−1 + ct−M + et,
(2.50)
where {et, t > 0} is a sequence of zero mean, uncorrelated one-step-ahead forecast
errors. Introducing the back shift operator B into the updating equations gives
(1 −B)lt = φbt−1 + α1et,
(2.51a)
(1 −φB)bt = α1α2et,
(2.51b)
(1 −BM)ct = (1 −α1)α3et.
(2.51c)
Left multiplying equation (2.50) by (1−φB)(1 −BM) on both sides and substituting
equation (2.51) into the result, we have the ARIMA model underlying DA-A
(1 −φB)(1 −BM)Yt = (1 +
M+1
X
i=1
θiBi)et,
θ1 = α1 + φα1α2 −φ,
θi = (1 −φ)α1 + φα1α2,
i = 2, · · · , M −1,
θM = (1 −φ)α1 + φα1α2 + (1 −α1)α3 −1,
θM+1 = φ(1 −α1) −φ(1 −α1)α3,
This ARIMA model, when φ = 1, becomes the ARIMA model underlying A-A; and,
when φ = 0, becomes the ARIMA model underlying N-A.
Table 2.3 gives the underlying ARIMA models for each additive ES method in
Table 2.1 and the corresponding relationships between the ARIMA parameters and
the smoothing parameters.
21

Table 2.3: Underlying ARIMA Models for ES methods (N - None, A - Additive, M - Multiplicative, DA - Damped Additive,
DM - Damped Multiplicative)
Seasonality
Trend
N
A
M
N
(1 −B)Yt = (1 + θ1B)ǫt
(1 −BM)Yt = (1 + PM
i=1 θiBi)ǫt
-
θ1 = α1 −1
θi = α1,
i = 1, · · · , M −1
θM = α1 + (1 −α1)α3 −1
A
(1 −B)2Yt = (1 + θ1B + θ2B2)ǫt
(1 −B)(1 −BM)Yt = (1 + PM+1
i=1 θiBi)ǫt
θ1 = α1 + α1α2 −2
θ1 = α1 + α1α2 −1
θ2 = 1 −α1
θi = α1α2,
i = 2, · · · , M −1
-
θM = α1α2 + (1 −α1)α3 −1
θM+1 = 1 −α1 −(1 −α1)α3
M
-
-
-
DA
(1 −φB)(1 −B)Yt = (1 + θ1B + θ2B2)ǫt
(1 −φB)(1 −BM)Yt = (1 + PM+1
i=1 θiBi)ǫt
θ1 = α1 + φα1α2 −φ −1
θ1 = α1 + φα1α2 −φ
θ2 = φ(1 −α1)
θi = (1 −φ)α1 + φα1α2,
i = 2, · · · , M −1
-
θM = (1 −φ)α1 + φα1α2 + (1 −α1)α3 −1
θM+1 = φ(1 −α1) −φ(1 −α1)α3
DM
-
-
-
22

2.4.2
MSOE State Space Model
Harrison (1967) gave the MSOE state space models that underpin SES and Holt’s
method. Harvey (1984) reached the same results from a diﬀerent viewpoint. Following
their steps, we identify, for the ﬁrst time, an MSOE state space model that underpins
the damped Holt’s methods. No underlying MSOE state space models have been
found for other methods in Table 2.1 although the basic structural model gave by
Harvey (1984) leads to something fairly close to the additive Holt-Winters’ method.
• SES
If the slowly changing level in equation (2.2) is assumed to follow a random walk
process, the resulting model is in the MSOE state space form
Yt = µt + ǫt,
(2.52a)
µt = µt−1 + ηt,
(2.52b)
where ǫt and ηt are mutually independent white noise processes with zero mean and
E[ǫ2
t] = σ2 and E[η2
t ] = σ2
η. Harrison (1967) proved that, for model (2.52), SES
provides optimal forecasts and the optimal value of the smoothing parameter α is
given by
α =
√
r2 + 4r −r
2
(2.53)
where r = σ2
η/σ2 is often referred to as the “signal to noise” ratio.
As long as
0 < r < ∞, α falls into the interval (0,1). When σ2 = 0 (i.e., r = ∞), the optimal α
is 1.
Harvey (1984) reached the same conclusion by showing that SES is equivalent to
the steady-state KF for model (2.52). KF is said to be in a steady state if the MSE
matrix Pt becomes constant (i.e., Pt = Pt−1 = P). Combining equations (2.34) and
23

(2.37) together, the steady-state KF for model (2.52) is given by
lt = lt−1 +
P + σ2
η
P + σ2
η + σ2 · (Yt −lt−1),
(2.54a)
P =
P + σ2
η
P + σ2
η + σ2 · σ2.
(2.54b)
P is used in place of the boldface P as now P is a scalar. Solving equation (2.54b)
for P gives
P =
√
r2 + 4r −r
2
· σ2 = ασ2.
(2.55)
Substituting P into equation (2.54a) yields
lt = lt−1 + α(Yt −lt−1) = αYt + (1 −α)lt−1,
(2.56)
which is the same as the recurrence equation for SES.
The optimality of SES for model (2.52) can also be seen from the fact that diﬀer-
encing Yt once reduces model (2.52) to an ARIMA(0,1,1) model with
θ1 =
√
r2 + 4r −r
2
−1.
(2.57)
With r > 0, we have −1 < θ1 ≤0 (Harvey 1990). That is, model (2.52) is the state
space representation of the ARIMA(0,1,1) model (2.45).
• Holt’s Method
Assuming that both the level and the slope in equation (2.9) change slowly with time
according to random walk processes leads to the MSOE state space model
Yt = µt + ǫt,
(2.58a)
µt = µt−1 + βt−1 + ηt,
(2.58b)
βt = βt−1 + ζt,
(2.58c)
where ǫt, ηt and ζt are mutually independent white noise processes with zero mean
and E[ǫ2
t] = σ2, E[η2
t ] = σ2
η and E[ζ2
t ] = σ2
ζ. Harrison (1967) proved that Holt’s
24

method provides optimal forecasts for model (2.58) with the optimal values of the
smoothing parameters α1 and α2 given by
rη = α2
1 + α2
1α2 −2α1α2
1 −α1
,
(2.59a)
rζ = α2
1α2
2
1 −α1
,
(2.59b)
where rη = σ2
η/σ2 and rζ = σ2
ζ/σ2.
Similar to the SES case, the optimality of Holt’s method for model (2.58) can also
be obtained by showing that
a). Holt’s method with α1 and α2 given by equations (2.59a) and (2.59b) is equiv-
alent to the steady-state KF for model (2.58) (Harvey 1990);
b). Model (2.58) is the state space representation of a subclass of the ARIMA(0,2,2)
model underlying Holt’s method.
Diﬀerencing Yt twice reduces the state space model (2.58) to an ARIMA(0,2,2)
model with θ1 and θ2 given by
rη = −θ1 + θ1θ2 + 4θ2
θ2
,
(2.60a)
rζ = (1 + θ1 + θ2)2
θ2
.
(2.60b)
For rη > 0 and rζ > 0, we have
−(1 + θ2) < θ1 < −4θ2
1 + θ2
and 0 ≤θ2 < 1.
(2.61)
The condition 0 ≤θ2 < 1 implies that
−4θ2
1 + θ2
−(−2θ2) = 2θ2(θ2 −1)
1 + θ2
≤0
(2.62)
That is, the parameter space deﬁned by (2.61) is a subset of the parameter
space for the ARIMA(0,2,2) model (2.47) (see Figure 2.1).
25

As a special case of Holt’s method, Brown’s double exponential smoothing provides
optimal forecasts for the state space model (2.58) when
rζ = (rη/2)2,
(2.63)
and the optimal value of α is given by
α =
pr2
η + 8rη −rη
4
.
(2.64)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
θ1
θ2
(a)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
θ1
θ2
(b)
Figure 2.1:
The big triangular area deﬁnes the parameter space for invertible
ARIMA(0,2,2) model; the shaded area in (a) deﬁnes the parameter space for in-
vertible ARIMA(0,2,2) model underlying Holt’s method with α1 and α2 falling into
the interval (0,1]; the shaded area in (b) deﬁnes the parameter space for the invert-
ible ARIMA(0,2,2) model reduced from the state space model (2.58). (Dashed line –
boundary not included; solid line – boundary included)
• Damped Holt’s Method
Assuming that the level in equation (2.9) follows a random walk process while the
slope follows an AR(1) process leads to the MSOE state space model
Yt = µt + ǫt,
(2.65a)
µt = µt−1 + φβt−1 + ηt,
(2.65b)
βt = φβt−1 + ζt,
(2.65c)
26

where the AR parameter φ takes values in the interval [0, 1]. It can be proven that,
for model (2.65), the damped Holt’s method with α1 and α2 satisfying
rη = α2
1 + φα2
1α2 −(1 + φ)α1α2
1 −α1
(2.66a)
rζ = φ2α2
1α2
2 + φ(1 −φ)α2
1α2 −(1 −φ)(1 −φ2)α1α2
φ2(1 −α1)
(2.66b)
provides optimal forecasts.
The optimality of damped Holt’s method for model (2.65) can also be obtained
by showing that
a). Damped Holt’s method with α1 and α2 given by equations (2.66a) and (2.66b)
is equivalent to the steady-state KF for model (2.65);
b). Model (2.65) is the state space representation of a subclass of the ARIMA(1,1,2)
model underlying damped Holt’s method.
2.4.3
SSOE State Space Model
Ord et al. (1997) introduced a very general form for SSOE state space models to
encompass linear or nonlinear models with homoscedastic or heteroscedastic variance.
With this general formulation, they explored the idea of using SSOE state space
models as the underlying statistical models for ES methods, and identiﬁed for the
ﬁrst time a statistical model that underpins the multiplicative Holt-Winters’ method.
Since their work, a series of papers have been published on using SSOE models to
explain ES methods. Chatﬁeld et al. (2001) listed arguments in favor of SSOE models
and discussed various possible SSOE models that underlie SES and the multiplicative
Holt-Winters’ method. Hyndman et al. (2002) provided two SSOE models for each
method except these in the last row of Table 2.1, one model with homoscedastic
variance and one model with heteroscedastic variance.
We expand their work by
adding underlying SSOE models for these methods in the last row of Table 2.1,
namely the damped methods for time series with multiplicative trends.
27

SSOE state space models underlying ES methods can be written in a general form
Yt = f(βt−1) + ǫt,
(2.67a)
βt = g(βt−1) + w(α, βt−1)ǫt,
(2.67b)
where ǫt is a white noise process with E[ǫt] = 0 and E[ǫ2
t] = σ2, βt is a p × 1 state
vector, α is a vector of smoothing parameters, f is a mapping from ℜp to ℜ, and g
and w are mappings from ℜp to ℜp. In addition, ǫt is assumed to be independent of
βk for any k < t. That is,
E[ǫtβk] = 0, for k < t.
(2.68)
Using SES as an example, we have βt = µt, f(µt) = µt, g(µt) = µt, and w(α, µt) =
α. As a result, the SSOE state space model in equations (2.67a) and (2.67b) becomes
Yt = µt−1 + ǫt,
(2.69a)
µt = µt−1 + αǫt.
(2.69b)
At time t −1, given µt−1, equation (2.69a) gives a one-step-ahead forecast
ˆYt|t−1 = µt−1.
(2.70)
Expressed in terms of the one-step-ahead forecast error ǫt = Yt−ˆYt|t−1, equation (2.69b),
becomes
µt = µt−1 + αǫt.
(2.71)
Equations (2.70) and (2.71) are exactly the one-step-ahead forecast equation and the
error-correction form updating equation of SES, albeit lt and et replaced by their true
values µt and ǫt respectively.
Table 2.4 contains underlying SSOE state space models for all of the ES methods
listed in Table 2.1. Comparing Table 2.4 to Table 2.1 reveals that, for an ES method,
the construction of an underlying SSOE state space model is straightforward, simply
using the updating equation and the one-step-ahead forecasting equation with an
28

extra error term adding to its right side as the transition equation and the observation
equation of the corresponding SSOE state space model respectively, albeit lt, bt, ct,
and et replaced by their true values µt, βt, st, and ǫt (see Table 2.5).
Replacing ǫt in model (2.67) with u(βt−1)ǫt, where u is a mapping from ℜp to ℜ,
we end up with a more general class of SSOE state space models, which encompasses
both homoscedastic (i.e., u(βt−1) is constant) and heteroscedastic (i.e., u(βt−1) is
time varying) cases. Furthermore, for any mapping u, model (2.67) produces the
same forecast. That is, the SSOE state space models that underpin an ES methods
are not unique. This fact helps to explain the robustness of ES methods.
2.5
Discussion
Three types of statistical models have been found to underpin ES methods: ARIMA
model, MSOE state space model, and SSOE state space model. Underlying ARIMA
models exist for only additive ES methods, and often have smaller parameter spaces
than unrestricted stationary and invertible ARIMA models do. Underlying MSOE
state space models have been identiﬁed for SES, Holt’s method, and the damped
Holt’s method. Moreover, as shown above, linear MSOE state space models can be
reduced to an ARIMA form, and the reduced form ARIMA models are a subclass of
these underlying ARIMA models for the corresponding ES methods (see Figure 2.1).
However, MSOE state space models have the advantage of being able to incorporate
explicitly structural features, which otherwise are hidden in ARIMA models by dif-
ferencing. Keeping the structural form of MSOE models, SSOE state space models
replace the independence assumption by a perfect correlation between the transient
error and the permanent error. The perfect correlation assumption gives SSOE state
space models advantages over ARIMA models and MSOE state space models. First,
underlying SSOE state space models can be identiﬁed for all ES method in Table 2.1,
and they are not unique and could be homoscedastic or heteroscedastic.
Second,
29

Table 2.4: Underlying SSOE State Space Models for ES methods. ξt = u(βt−1)ǫt, and constant u(βt−1) gives homoscedastic
models while time-varying u(βt−1) results in heteroscedastic models. (N - None, A - Additive, M - Multiplicative, DA -
Damped Additive, DM - Damped Multiplicative)
Seasonality
Trend
N
A
M
N
Yt = µt−1 + ξt
Yt = µt−1 + st−M + ξt
Yt = µt−1st−M + ξt
µt = µt−1 + α1ξt
µt = µt−1 + α1ξt
µt = µt−1 + α1ξt/st−M
st = st−M + (1 −α1)α3ξt
st = st−M + (1 −α1)α3ξt/µt−1
A
Yt = µt−1 + βt−1 + ξt
Yt = µt−1 + βt−1 + st−M + ξt
Yt = (µt−1 + βt−1)st−M + ξt
µt = µt−1 + βt−1 + α1ξt
µt = µt−1 + βt−1 + α1ξt
µt = µt−1 + βt−1 + α1ξt/st−M
βt = βt−1 + α1α2ξt
βt = βt−1 + α1α2ξt
βt = βt−1 + α1α2ξt/st−M
st = st−M + (1 −α1)α3ξt
st = st−M + (1 −α1)α3ξt/(µt−1 + βt−1)
M
Yt = µt−1βt−1 + ξt
Yt = µt−1βt−1 + st−M + ξt
Yt = (µt−1βt−1)st−M + ξt
µt = µt−1βt−1 + α1ξt
µt = µt−1βt−1 + α1ξt
µt = µt−1βt−1 + α1ξt/st−M
βt = βt−1 + α1α2ξt/µt−1
βt = βt−1 + α1α2ξt/µt−1
βt = βt−1 + α1α2ξt/(µt−1st−M)
st = st−M + (1 −α1)α3ξt
st = st−M + (1 −α1)α3ξt/(µt−1βt−1)
DA
Yt = µt−1 + φβt−1 + ξt
Yt = µt−1 + φβt−1 + st−M + ξt
Yt = (µt−1 + φβt−1)st−M + ξt
µt = µt−1 + φβt−1 + α1ξt
µt = µt−1 + φβt−1 + α1ξt
µt = µt−1 + φβt−1 + α1ξt/st−M
βt = φβt−1 + α1α2ξt
βt = φβt−1 + α1α2ξt
βt = φβt−1 + α1α2ξt/st−M
st = st−M + (1 −α1)α3ξt
st = st−M + (1 −α1)α3ξt/(µt−1 + φβt−1)
DM
Yt = µt−1βφ
t−1 + ξt
Yt = µt−1βφ
t−1 + st−M + ξt
Yt = (µt−1βφ
t−1)st−M + ξt
µt = µt−1βφ
t−1 + α1ξt
µt = µt−1βφ
t−1 + α1ξt
µt = µt−1βφ
t−1 + α1ξt/st−M
βt = βφ
t−1 + α1α2ξt/µt−1
βt = βφ
t−1 + α1α2ξt/µt−1
βt = βφ
t−1 + α1α2ξt/(µt−1st−M)
st = st−M + (1 −α1)α3ξt
st = st−M + (1 −α1)α3ξt/(µt−1βφ
t−1)
30

Table 2.5: Construction of Underlying SSOE State Space Models for ES methods (
ˆβt = (lt, bt, ct, · · · , ct−M+1)T, and βt = (µt, βt, st, · · · , st−M+1)T)
ES method
Underlying SSOE State Space Model
Updating Equation
Transition Equation
ˆβt = g( ˆβt−1) + w(α, ˆβt−1)et
βt = g(βt−1) + w(α, βt−1)ξt
One-Step-Ahead Forecast Equation
Observation Equation
ˆYt|t−1 = f( ˆβt−1)
Yt = f(βt−1) + ǫt
the transition equations of underlying SSOE models provide a transparent link with
the updating equations of the corresponding ES methods. Last, the class of under-
lying SSOE models, as shown in Table 2.6, is broader than the class of underlying
ARIMA model, and therefore the class of underlying MSOE state space models. Sny-
der (1985) showed that any ARIMA model can be written in an SSOE state space
form. SSOE state space models provides a formal statistical framework for the study
of ES methods.
Table 2.6: Relationships among Three Types of Underlying Statistical Models for
ES methods
class of underlying SSOE state space models
S
class of underlying ARIMA models
S
class of underlying MSOE state space models
31

CHAPTER III
PERFORMANCE ANALYSIS OF ES METHODS
FOR TIME SERIES OF ARIMA TYPE
3.1
Introduction
Although the widespread usage of ES methods in industry and business for forecasting
can be attributed to simple and intuitive formulation, eﬃcient computation, and ﬂex-
ible adaptivity, a more important explanation of their popularity is that ES methods
perform reasonably well for a wide class of time series. Comparing the performance
of various forecasting methods on 111 real-world time series , Makridakis and Hi-
bon (1979), found that statistically sophisticated methods do not necessarily provide
more accurate forecasts than simple methods such as ES methods. Such a ﬁnding was
not well received by statisticians. In response, Makridakis et al. launched two larger-
scale empirical studies, M-Competition (Makridakis et al. 1982) and M2-Competition
(Makridakis et al. 1993), by including more forecasting methods and larger number of
time series. Both competitions once again concluded that, as simple as they are, ES
methods such as SES and Holt’s method perform as well as, or in many cases better
than, statistically sophisticated methods such as the ARIMA models advocated by
Box and Jenkins (Box et al. 1994) in terms of forecast accuracy. This conclusion
was further conﬁrmed by their latest empirical study, M3-Competition (Makridakis
and Hibon 2000), which includes 3,003 time series of various types and 24 forecasting
methods.
To better understand ES methods and be able to answer questions, such as when
and why ES methods perform well, and how the choices of smoothing parameters
32

aﬀect the performance, we investigate the behaviors of ES methods when time series
are generated from diﬀerent processes. We mainly focus on time series of ARIMA
type. The purpose is to ﬁnd out for what kinds of ARIMA-type time series ES meth-
ods perform well and why. Also studied are the eﬀects of the values of the smoothing
parameters on performance. For example, how the choices of the smoothing parame-
ters aﬀect the performance, when the values of smoothing parameters are critical for
forecasting and when are not, and what will occur if the smoothing parameters are
underestimated or overestimated.
Only included in this investigation are SES and Holt’s linear trend method as these
two are the most frequently used ones and were singled out by the M-competitions
for their good performance. Also only one-step-ahead forecast is concerned for the
performance analysis.
3.2
SES
SES updates the one-step-ahead forecast, ˆYt|t−1, by
ˆYt|t−1 = αYt−1 + (1 −α)ˆYt−1|t−2,
(3.1)
where α is a smoothing parameter taking values in the interval (0,1].
Assume that the true data generating process follows a model of form
Yt = µ + Nt,
(3.2)
where µ is a constant, and Nt is a disturbance term with zero mean and ﬁrst diﬀerence
as a stationary process
(1 −B)Nt =
∞
X
i=0
ψiǫt−i,
(3.3)
where B is the back shift operator, and ǫt is a zero-mean white noise process (i.e.,
E[ǫt] = 0, E[ǫ2
t] = σ2, and E[ǫtǫt−k] = 0 for k ̸= 0).
SES with a smoothing parameter α for model (3.2) leads to
et −(1 −α)et−1 =
∞
X
i=0
ψiǫt−i.
(3.4)
33

Let λ = 1 −α. As 0 < α ≤1, 0 ≤λ < 1. Therefore,
et = (1 −λB)−1
∞
X
i=0
ψiǫt−i =
∞
X
j=0
∞
X
i=0
λjψiǫt−i−j =
∞
X
j=0
(
j
X
i=0
λj−iψi)ǫt−j.
(3.5)
As a result, the mean of the one-step-ahead forecast error is given by
E[et] =
∞
X
j=0
(
j
X
i=0
λj−iψi)E[ǫt−j] = 0,
(3.6)
which implies that the one-step-ahead forecast by SES for model (3.2) is unbiased.
The mean squared error (MSE) of the one-step-ahead forecast is given by
E[e2
t] = σ2
∞
X
j=0
(
j
X
i=0
λj−iψi)2.
(3.7)
3.2.1
Nt is an ARIMA(0, 1, q) process
Nt is an ARIMA(0, 1, q) process
(1 −B)Nt =
q
X
i=0
θiǫt−i
(3.8)
Without loss of generality, we assume that θ0 = 1. Therefore, ψi = θi for 0 ≤i ≤q,
and ψi = 0 for i > q. The MSE in equation (3.7) becomes
E[e2
t] = σ2[
q−1
X
j=0
(
j
X
i=0
λj−iθi)2 +
1
1 −λ2(
q
X
i=0
λq−iθi)2]
(3.9)
1). q = 0, Nt is an ARIMA(0,1,0), which is a random walk. That is, Nt = Nt−1+ǫt.
E[e2
t] = σ2 ·
1
1 −λ2.
(3.10)
According to equation (3.10) as well as Figure 3.1, MSE is a monotone increasing
function of λ for λ ∈[0, 1), therefore a monotone decreasing function of α for
α ∈(0, 1]. The minimum MSE occurs as α = 1(λ = 0), at which the one-step-
ahead forecast is ˆYt|t−1 = Yt−1. That is, the forecast for the value at time t
is simply the observed value at time t −1. This is due to the property of the
random walk
E[Yt|Yt−1, Yt−2, · · · ] = E[Yt|Yt−1],
(3.11)
34

which implies that, given the value at time t −1, the value at time t is inde-
pendent of the values at times t −k, k > 1.
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
Figure 3.1: SES – MSE/σ2 as a function of α, Nt is an ARIMA(0,1,0).
2). q = 1, Nt is an ARIMA(0,1,1).
E[e2
t] = σ2 · [1 + (λ + θ1)2
1 −λ2 ].
(3.12)
Figure 3.2 and Table 3.1 show that
– SES performs better when −1 < θ1 < 0 than when 0 < θ1 < 1. When
−1 < θ1 < 0, the minimum MSE stays at the theoretically optimal value
σ2. When 0 < θ1 < 1, the minimum MSE is greater than σ2 and increases
as θ1 increases.
– When −1 < θ1 < 0, the minimum MSE occurs as α = 1+θ1 (i.e., λ = −θ1).
When 0 < θ1 < 1, MSE is a monotone decreasing function of α and reaches
minimum at α = 1.
– Overestimation of α is less serious than the equivalent underestimation.
35

In fact, SES is optimal for an ARIMA(0,1,1)
(1 −B)Nt = ǫt + θ1ǫt−1
(3.13)
with
−1 < θ1 < 0
(3.14)
when the smoothing parameter α = 1 + θ1 (Muth 1960). Let αopt denote the
value of α, at which the minimum MSE is achieved. Taking the derivative of
E[e2
t] in equation (3.12) with respect to λ and setting it to zero gives
αopt =





1 + θ1,
−1 < θ1 < 0,
1,
0 < θ1 < 1.
(3.15)
The MSE at αopt (i.e., the minimum MSE) is
MSEmin =





σ2,
−1 < θ1 < 0,
σ2(1 + θ2
1),
0 < θ1 < 1.
(3.16)
In summary, SES is optimal for an ARIMA(0,1,1) when −1 < θ1 < 0 and
performs well when 0 < θ1 < 1 but has a small value. Also, a large α seems a
safe choice.
Table 3.1: SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,1,1)
θ1
αopt
MSE/σ2
-0.95
0.05
1.00
-0.90
0.10
1.00
-0.80
0.20
1.00
-0.70
0.30
1.00
-0.60
0.40
1.00
-0.50
0.50
1.00
-0.40
0.60
1.00
-0.30
0.70
1.00
-0.20
0.80
1.00
-0.10
0.90
1.00
θ1
αopt
MSE/σ2
0.95
1.00
1.90
0.90
1.00
1.81
0.80
1.00
1.64
0.70
1.00
1.49
0.60
1.00
1.36
0.50
1.00
1.25
0.40
1.00
1.16
0.30
1.00
1.09
0.20
1.00
1.04
0.10
1.00
1.01
36

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ1=−0.95
θ1=−0.9
θ1=−0.8
θ1=−0.7
θ1=−0.6
θ1=−0.5
θ1=−0.4
θ1=−0.3
θ1=−0.2
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ1=0.95
θ1=0.9
θ1=0.8
θ1=0.7
θ1=0.6
θ1=0.5
θ1=0.4
θ1=0.3
θ1=0.2
θ1=0.1
Figure 3.2: SES – MSE/σ2 as a function of α, Nt is an ARIMA(0,1,1)
3). q = 2, Nt is an ARIMA(0,1,2).
E[e2
t] = σ2 · [1 + (λ + θ1)2 + (λ2 + λθ1 + θ2)2
1 −λ2
].
(3.17)
From Figure 3.3 and Table 3.2, the following conclusions can be drawn:
– Given θ1, the minimum MSE increases as |θ2|, the absolute value of θ2,
increases. Given θ2, the minimum MSE remains unchanged when θ1 ≤0
and increases with θ1 when θ1 > 0.
– When θ1 < 0, αopt is less than 1 and increases as θ1 and/or θ2 increase.
When θ1 ≥0, αopt = 1.
– Overestimation of α is less serious than the equivalent underestimation.
Taking the derivative of E[e2
t] in equation (3.17) with respect to λ and setting
the result to zero gives
αopt =





(1 + θ1 + θ2)/(1 + θ2),
−(1 + θ2) < θ1 < 0,
1,
0 ≤θ1 < 1 + θ2.
(3.18)
37

The minimum MSE is
MSEmin =





σ2(1 + θ2
2),
−(1 + θ2) < θ1 < 0,
σ2(1 + θ2
1 + θ2
2),
0 ≤θ1 < 1 + θ2.
(3.19)
In summary, SES performs well for an ARIMA(0,1,2) when θ1 is negative and
|θ2| is small. A large α is a safe choice.
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=−0.2
θ1=−0.79
θ1=−0.6
θ1=−0.4
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.4
θ1=0.6
θ1=0.79
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=0.2
θ1=−0.79
θ1=−0.6
θ1=−0.4
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.4
θ1=0.6
θ1=0.79
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=−0.5
θ1=−0.49
θ1=−0.4
θ1=−0.3
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.3
θ1=0.4
θ1=0.49
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=0.5
θ1=−0.49
θ1=−0.4
θ1=−0.3
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.3
θ1=0.4
θ1=0.49
Figure 3.3: SES – MSE/σ2 as a function of α, Nt is an ARIMA(0,1,2)
38

Table 3.2: SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(0,1,2)
θ2 = −0.2
θ2 = 0.2
θ1
αopt
MSE/σ2
αopt
MSE/σ2
-1.19
-
-
0.01
1.04
-1.00
-
-
0.17
1.04
-0.79
0.01
1.04
0.34
1.04
-0.60
0.25
1.04
0.50
1.04
-0.49
0.39
1.04
0.59
1.04
-0.40
0.50
1.04
0.67
1.04
-0.20
0.75
1.04
0.83
1.04
0.00
1.00
1.04
1.00
1.04
0.20
1.00
1.08
1.00
1.08
0.40
1.00
1.20
1.00
1.20
0.49
1.00
1.28
1.00
1.28
0.60
1.00
1.40
1.00
1.40
0.79
1.00
1.66
1.00
1.66
1.00
-
-
1.00
2.04
1.19
-
-
1.00
2.46
θ2 = −0.5
θ2 = 0.5
θ1
αopt
MSE/σ2
αopt
MSE/σ2
-1.49
-
-
0.01
1.25
-1.40
-
-
0.07
1.25
-1.19
-
-
0.21
1.25
-1.00
-
-
0.33
1.25
-0.79
-
-
0.47
1.25
-0.60
-
-
0.60
1.25
-0.49
0.02
1.25
0.67
1.25
-0.40
0.20
1.25
0.73
1.25
-0.20
0.60
1.25
0.87
1.25
0.00
1.00
1.25
1.00
1.25
0.20
1.00
1.29
1.00
1.29
0.40
1.00
1.41
1.00
1.41
0.49
1.00
1.49
1.00
1.49
0.60
-
-
1.00
1.61
0.79
-
-
1.00
1.87
1.00
-
-
1.00
2.25
1.19
-
-
1.00
2.67
1.40
-
-
1.00
3.21
1.49
-
-
1.00
3.47
39

3.2.2
Nt is an ARIMA(0, 0, q) process
Nt is an ARIMA(0, 0, q) process
Nt =
q
X
i=0
θiǫt−i.
(3.20)
Let θ−1 = θq+1 = 0, we have
(1 −B)Nt =
q+1
X
i=0
(θi −θi−1)ǫt−i.
(3.21)
The MSE becomes
E[e2
t] = σ2[
q
X
j=0
(
j
X
i=0
λj−i(θi −θi−1))2 +
1
1 −λ2(
q+1
X
i=0
λq+1−i(θi −θi−1))2].
(3.22)
1). q = 0, Nt is a white noise.
E[e2
t] = σ2 ·
2
1 + λ.
(3.23)
According to equation 3.23 as well as Figure 3.4, MSE is a monotone decreasing
function of λ, therefore a monotone increasing function of α. MSE →σ2 as
α →0 (λ →1). The same result was obtained by Cohen (1963). SES with
α →0 is analogous to taking the average of a random sample with sample size
n →∞.
In fact, with a white noise disturbance, model (3.2) becomes
Yt = µ + ǫt,
(3.24)
for which the average of a random sample gives the minimum variance estimator
of E[Yt] = µ.
2). q = 1, Nt is a MA(1).
E[e2
t] = σ2 · [2θ1 + 2(1 −θ1)2
1 + λ
].
(3.25)
Figure 3.5 and Table 3.3 show that
40

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
Figure 3.4: SES – MSE/σ2 as a function of α, Nt is a white noise
– The MSE is a monotone increasing function of α and reaches minimum as
α →0 (λ →1).
– According to equation (3.25), as α →0 (λ →1), MSE →σ2(1+θ2
1). That
is, the minimum MSE only depends on |θ1|, the absolute value of θ1, and
increases as |θ1| increases.
– The larger the θ1, the less critical the choice of α. For θ1 > 0.5, the MSE
varies extremely slowly with α.
In summary, SES performs well for an MA(1) when |θ1| is small. A small α is
preferred.
3). q = 2, Nt is an MA(2).
E[e2
t] = σ2 · [2(θ1 + θ1θ2 −2θ2) + 2θ2λ + 2(1 −θ1 + θ2)2
1 + λ
].
(3.26)
Figure 3.6 and Table 3.4 show that
– SES performs better when both |θ1| and |θ2| are small. Given θ2 (or θ1),
the smaller the |θ1| (or |θ2|), the smaller the minimum MSE.
41

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ1=−0.95
θ1=−0.9
θ1=−0.8
θ1=−0.7
θ1=−0.6
θ1=−0.5
θ1=−0.4
θ1=−0.3
θ1=−0.2
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ1=0.95
θ1=0.9
θ1=0.8
θ1=0.7
θ1=0.6
θ1=0.5
θ1=0.4
θ1=0.3
θ1=0.2
θ1=0.1
Figure 3.5: SES – MSE/σ2 as a function of α, Nt is an MA(1)
Table 3.3: SES – Optimal α and Minimum MSE/σ2, Nt is an MA(1)
θ1
αopt
MSE/σ2
-0.95
→0
1.90
-0.90
→0
1.81
-0.80
→0
1.64
-0.70
→0
1.49
-0.60
→0
1.36
-0.50
→0
1.25
-0.40
→0
1.16
-0.30
→0
1.09
-0.20
→0
1.04
-0.10
→0
1.01
θ1
αopt
MSE/σ2
0.95
→0
1.90
0.90
→0
1.81
0.80
→0
1.64
0.70
→0
1.49
0.60
→0
1.36
0.50
→0
1.25
0.40
→0
1.16
0.30
→0
1.09
0.20
→0
1.04
0.10
→0
1.01
42

– αopt →0 when θ1 is small and |θ2| < 1.
– The choice of α is not critical when θ1 > 0 and |θ2| is small.
It can be shown that, using Equation (3.26),
αopt =

















→0,
−1 < θ2 ≤0,
→0,
0 < θ2 < 1, −(1 + θ2) < θ1 ≤(1 −√θ2)2,
θ1−(1−√θ2)2
√θ2
,
0 < θ2 < 1, (1 −√θ2)2 < θ1 ≤1 + θ2 −√θ2
1,
otherwise.
(3.27)
And, as αopt →0 (λ →1), the minimum MSE →σ2(1 + θ2
1 + θ2
2).
In summary, SES performs well for an MA(2) only when both |θ1| and |θ2| are
small, and a small α is a safe choice although the choice of α is not critical when
both θ1 and θ2 are positive.
Table 3.4: SES – Optimal α and Minimum MSE/σ2, Nt is a MA(2)
θ2 = −0.2
θ2 = 0.2
θ1
αopt
MSE/σ2
αopt
MSE/σ2
-1.19
-
-
→0
2.46
-1.00
-
-
→0
2.04
-0.79
→0
1.66
→0
1.66
-0.60
→0
1.40
→0
1.40
-0.49
→0
1.28
→0
1.28
-0.40
→0
1.20
→0
1.20
-0.20
→0
1.08
→0
1.08
0.00
→0
1.04
→0
1.04
0.20
→0
1.08
→0
1.08
0.40
→0
1.20
0.21
1.19
0.49
→0
1.28
0.41
1.25
0.60
→0
1.40
0.66
1.31
0.79
→0
1.66
1.00
1.43
1.00
-
-
1.00
1.68
1.19
-
-
1.00
2.06
θ2 = −0.5
θ2 = 0.5
θ1
αopt
MSE/σ2
αopt
MSE/σ2
-1.49
-
-
→0
3.47
-1.40
-
-
→0
3.21
-1.19
-
-
→0
2.67
-1.00
-
-
→0
2.25
-0.79
-
-
→0
1.87
-0.60
-
-
→0
1.61
-0.49
→0
1.49
→0
1.49
-0.40
→0
1.41
→0
1.41
-0.20
→0
1.29
→0
1.29
0.00
→0
1.25
→0
1.25
0.20
→0
1.29
0.16
1.28
0.40
→0
1.41
0.44
1.31
0.49
→0
1.49
0.57
1.33
0.60
-
-
0.73
1.35
0.79
-
-
1.00
1.38
1.00
-
-
1.00
1.50
1.19
-
-
1.00
1.76
1.40
-
-
1.00
2.22
1.49
-
-
1.00
2.47
43

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=−0.2
θ1=−0.79
θ1=−0.6
θ1=−0.4
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.4
θ1=0.6
θ1=0.79
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=0.2
θ1=−0.79
θ1=−0.6
θ1=−0.4
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.4
θ1=0.6
θ1=0.79
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=−0.5
θ1=−0.49
θ1=−0.4
θ1=−0.3
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.3
θ1=0.4
θ1=0.49
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
θ2=0.5
θ1=−0.49
θ1=−0.4
θ1=−0.3
θ1=−0.2
θ1=−0.1
θ1=0.1
θ1=0.2
θ1=0.3
θ1=0.4
θ1=0.49
Figure 3.6: SES – MSE/σ2 as a function of α, Nt is an MA(2)
44

3.2.3
Nt is an ARIMA(1, d, 0) process
Nt is an ARIMA(1, d, 0) process
(1 −B)dNt = (1 −φ1B)−1ǫt.
(3.28)
1). d = 1, Nt is an ARIMA(1, 1, 0).
(1 −B)Nt = (1 −φ1B)−1ǫt =
∞
X
i=0
φi
1ǫt−i.
(3.29)
Then, ψi = φi
1 for i ≥0. The MSE becomes
E[e2
t] = σ2
∞
X
j=0
(
j
X
i=0
λj−iφi
1)2 = σ2 ·
(1 + λφ1)
(1 −λ2)(1 −φ2
1)(1 −λφ1).
(3.30)
Figure 3.7 and Table 3.5 show that
– SES performs better when −1 < φ1 < 0 than when 0 < φ1 < 1. When
−1 < φ1 < 0, the minimum MSE increases as |φ1|, the absolute value of
φ1, increases, and the larger the |φ1|, the faster the minimum MSE grows.
Same holds when 0 < φ1 < 1.
– When −0.6 < φ1 < 0, the minimum MSE approximately equals to σ2, and
changes in φ1 have little eﬀect on the MSE. Moveover, a good choice of α
is between 0.6 and 0.8.
– αopt increases as φ1 increases and reaches 1 at φ1 = 0.
– When −1 < φ1 < 0, overestimation of α is less serious than the equivalent
underestimation especially when φ1 ∈(−0.6, 0). When 0 < φ1 < 1, αopt =
1 and MSEmin = σ2/(1 −φ2
1).
In summary, SES performs well for an ARIMA(1,1,0) when φ1 < 0 and |φ1| is
small, and a large α is a safe choice.
2). d = 0, Nt is an AR(1).
Nt = (1 −φ1B)−1ǫt,
(3.31)
45

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.95
φ1=−0.9
φ1=−0.8
φ1=−0.7
φ1=−0.6
φ1=−0.5
φ1=−0.4
φ1=−0.3
φ1=−0.2
φ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.95
φ1=0.9
φ1=0.8
φ1=0.7
φ1=0.6
φ1=0.5
φ1=0.4
φ1=0.3
φ1=0.2
φ1=0.1
Figure 3.7: SES – MSE/σ2 as a function of α, Nt is an ARIMA(1,1,0)
Table 3.5: SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,1,0)
φ1
αopt
MSE/σ2
-0.95
0.19
3.88
-0.90
0.26
2.33
-0.80
0.36
1.52
-0.70
0.44
1.25
-0.60
0.50
1.12
-0.50
0.57
1.06
-0.40
0.64
1.02
-0.30
0.72
1.01
-0.20
0.81
1.00
-0.10
0.90
1.00
φ1
αopt
MSE/σ2
0.95
1.00
10.26
0.90
1.00
5.26
0.80
1.00
2.78
0.70
1.00
1.96
0.60
1.00
1.56
0.50
1.00
1.33
0.40
1.00
1.19
0.30
1.00
1.10
0.20
1.00
1.04
0.10
1.00
1.01
46

which gives
(1 −B)Nt = (1 −B)(1 −φ1B)−1ǫt = ǫt +
∞
X
i=1
φi−1
1
(φ1 −1)ǫt−i.
(3.32)
Therefore, ψ0 = 1, ψi = φi−1
1
(φ1 −1) for i ≥1. The MSE becomes
E[e2
t] = σ2
∞
X
j=0
(λj + (φ1 −1)
j
X
i=1
λj−iφi−1
1
)2 = σ2 ·
2
(1 + λ)(1 + φ1)(1 −λφ1).
(3.33)
Figure 3.8 and Table 3.6 show that
– SES performs better when 0 < φ1 < 1 than when −1 < φ1 < 0. When
−1 < φ1 < 0, the smaller the φ1, the larger the minimum MSE. When
0 < φ1 < 1, the worse performance happens at φ1 = 0.5.
– When −1 < φ1 < 0.4, αopt →0. When 0.4 ≤φ1 < 1, αopt increases as φ1
increase.
In fact, Cohen (1963) showed that, for an AR(1) disturbance,
αopt =





(3φ1 −1)/2φ1,
1/3 < φ1 < 1,
→0,
−1 < φ1 ≤1/3,
(3.34)
and the corresponding minimum MSE is
E[e2
t] =





8σ2φ1/(1 + φ1)3,
1/3 < φ1 < 1,
σ2/(1 −φ2
1),
−1 < φ1 ≤1/3.
(3.35)
Cox (1961) conducted a rather detailed study on the performance of SES
for an AR(1). However, the formula he gave for the MSE is not correct.
– The choice of α is not critical when φ1 is close to 0.5. When φ1 = 0.5, the
MSE is rather insensitive to the choice of α (see Table 3.7).
In summary, SES performs well for an AR(1) when 0 < φ1 < 1, and the choice
of α is not critical when φ1 is close to 0.5.
47

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.95
φ1=−0.9
φ1=−0.8
φ1=−0.7
φ1=−0.6
φ1=−0.5
φ1=−0.4
φ1=−0.3
φ1=−0.2
φ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.95
φ1=0.9
φ1=0.8
φ1=0.7
φ1=0.6
φ1=0.5
φ1=0.4
φ1=0.3
φ1=0.2
φ1=0.1
Figure 3.8: SES – MSE/σ2 as a function of α, Nt is an AR(1)
Table 3.6: SES – Optimal α and Minimum MSE/σ2, Nt is an AR(1)
φ1
αopt
MSE/σ2
-0.95
→0
10.26
-0.90
→0
5.26
-0.80
→0
2.78
-0.70
→0
1.96
-0.60
→0
1.56
-0.50
→0
1.33
-0.40
→0
1.19
-0.30
→0
1.10
-0.20
→0
1.04
-0.10
→0
1.01
φ1
αopt
MSE/σ2
0.95
0.97
1.02
0.90
0.94
1.05
0.80
0.88
1.10
0.70
0.79
1.14
0.60
0.67
1.17
0.50
0.50
1.19
0.40
0.25
1.17
0.30
→0
1.10
0.20
→0
1.04
0.10
→0
1.01
Table 3.7: SES – MSE/σ2 for φ1 = 0.5, Nt is an AR(1), αopt = 0.5.
α
0.05
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
0.95
MSE/σ2
1.30
1.28
1.23
1.21
1.19
1.19
1.19
1.21
1.23
1.28
1.30
48

3.2.4
Nt is an ARIMA(1, d, 1) process
Nt is an ARIMA(1, d, 1) process
(1 −B)dNt = (1 −φ1B)−1(ǫt + θ1ǫt−1).
(3.36)
1). d = 1, Nt is an ARIMA(1,1,1).
(1 −B)Nt = (1 −φ1B)−1(ǫt + θ1ǫt−1) = ǫt + (φ1 + θ1)
∞
X
i=1
φi−1
1
ǫt−i.
(3.37)
Then, ψ0 = 1, ψi = (φ1 + θ1)φi−1
1
for i ≥1. The MSE becomes
E[e2
t]
=
σ2
∞
X
j=0
(λj + (φ1 + θ1)
j
X
i=1
λj−iφi−1
1
)2
=
σ2
∞
X
j=0
(λj(λ + θ1) −φj
1(φ1 + θ1)
λ −φ1
)2
=
σ2 · (λ + θ1)(φ1 + θ1) + (1 + λθ1)(1 + φ1θ1)
(1 −λ2)(1 −φ2
1)(1 −λφ1)
.
(3.38)
Figure 3.9 and Table 3.8 show that
– SES performs well under two situations: i) −1 < θ1 < 0 and |φ1| is small,
and ii) 0 < θ1 < 1, −1 < φ1 < 0, and |θ1| and |φ1| are close.
– αopt increases as both θ1 and φ1 increase, and overestimation of α is less
serious than the equivalent underestimation.
2). d = 0, Nt is an ARMA(1,1).
Nt = (1 −φ1B)−1(ǫt + θ1ǫt−1),
(3.39)
which gives
(1 −B)Nt = ǫt + (φ1 + θ1 −1)ǫt−1 + (φ1 + θ1)(φ1 −1)
∞
X
i=2
φi−2
1
ǫt−i.
(3.40)
49

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.9
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.5
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.1
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.9
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.5
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.1
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.9
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.5
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.1
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.9
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.5
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.1
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
Figure 3.9: SES – MSE/σ2 as a function of α, Nt is an ARIMA(1,1,1)
50

Table 3.8: SES – Optimal α and Minimum MSE/σ2, Nt is an ARIMA(1,1,1)
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
-0.90
0.02
4.91
0.05
1.30
0.08
1.01
-0.70
0.06
4.25
0.14
1.24
0.26
1.01
-0.50
0.10
3.63
0.25
1.18
0.43
1.00
-0.30
0.16
3.07
0.37
1.12
0.61
1.00
-0.10
0.22
2.57
0.50
1.08
0.80
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
-0.90
1.00
1.00
0.56
1.09
0.12
1.01
-0.70
1.00
1.21
0.79
1.02
0.36
1.00
-0.50
1.00
1.84
1.00
1.00
0.58
1.00
-0.30
1.00
2.89
1.00
1.05
0.80
1.00
-0.10
1.00
4.37
1.00
1.21
1.00
1.00
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
0.90
1.00
1.00
1.00
1.21
1.00
1.65
0.70
0.76
1.12
1.00
1.05
1.00
1.36
0.50
0.57
1.38
1.00
1.00
1.00
1.16
0.30
0.42
1.71
0.81
1.01
1.00
1.04
0.10
0.31
2.11
0.65
1.04
1.00
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
0.90
1.00
18.05
1.00
3.61
1.00
2.01
0.70
1.00
14.47
1.00
2.92
1.00
1.65
0.50
1.00
11.32
1.00
2.33
1.00
1.36
0.30
1.00
8.58
1.00
1.85
1.00
1.16
0.10
1.00
6.26
1.00
1.48
1.00
1.04
51

Therefore, ψ0 = 1, ψ1 = φ1 + θ1 −1, and ψi = (φ1 + θ1)(φ1 −1)φi−2
1
for i ≥2.
The MSE becomes
E[e2
t]
=
σ2[1 +
∞
X
j=1
(λj−1(λ + θ1)(λ −1) −φj−1
1
(φ1 + θ1)(φ1 −1)
λ −φ1
)2]
=
σ2 · [
2θ1
1 −λφ1
+
2(θ1 −1)2
(1 + λ)(1 + φ1)(1 −λφ1)].
(3.41)
Figure 3.10 and Table 3.9 show that
– SES performs well when θ1φ1 < 0 and |θ1| and |φ1| are close.
– αopt increases as both θ1 and φ1 increase. αopt →0 when either −1 < θ1 < 0
and |φ1| is small or −1 < φ1 < 0.
– When 0 < θ1 < 1, the smaller the |φ1|, the less critical the choice of α.
3.2.5
Summary
Based on the results above on the performance of SES for diﬀerent types of ARIMA
time series, the following conclusions can be draw.
• Nt is an ARIMA(0, 1, q), 0 ≤q ≤2.
αopt = 1 when θ1 > 0, αopt < 1 when θ1 <
0, and overestimation of α is less serious than the equivalent underestimation.
SES performs well when θ1 < 0 and |θ2| is small.
• Nt is an ARIMA(0, 0, q), 0 ≤q ≤2.
αopt is often extremely small (→0), and
the choice of α is not critical when θ1 > 0 and |θ2| is small. SES performs well
when both |θ1| and |θ2| are small.
• Nt is an ARIMA(1, 1, 0).
αopt = 1 when 0 < φ1 < 1, αopt < 1 when −1 < φ1 <
0, and overestimation of α is less serious than the equivalent underestimation.
SES performs well when |φ1| is small, say −0.7 < φ1 < 0.5.
52

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.9
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.5
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.1
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.9
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.5
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.1
θ1=−0.9
θ1=−0.7
θ1=−0.5
θ1=−0.3
θ1=−0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.9
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.5
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=−0.1
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.9
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.5
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α
MSE/σ2
φ1=0.1
θ1=0.9
θ1=0.7
θ1=0.5
θ1=0.3
θ1=0.1
Figure 3.10: SES – MSE/σ2 as a function of α, Nt is an ARMA(1,1)
53

Table 3.9: SES – Optimal α and Minimum MSE/σ2, Nt is an ARMA(1,1)
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
-0.90
→0
18.05
→0
3.61
→0
2.01
-0.70
→0
14.47
→0
2.92
→0
1.65
-0.50
→0
11.32
→0
2.33
→0
1.36
-0.30
→0
8.58
→0
1.85
→0
1.16
-0.10
→0
6.26
→0
1.48
→0
1.04
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
-0.90
→0
1.00
→0
1.21
→0
1.65
-0.70
0.19
1.04
→0
1.05
→0
1.36
-0.50
0.41
1.04
→0
1.00
→0
1.16
-0.30
0.63
1.05
→0
1.05
→0
1.04
-0.10
0.84
1.05
0.33
1.15
→0
1.00
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
0.90
→0
1.00
→0
1.21
→0
1.65
0.70
→0
1.21
→0
1.05
→0
1.36
0.50
→0
1.84
→0
1.00
→0
1.16
0.30
→0
2.89
→0
1.05
→0
1.04
0.10
→0
4.37
→0
1.21
→0
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αopt
MSE/σ2
αopt
MSE/σ2
αopt
MSE/σ2
0.90
1.00
1.81
1.00
1.81
1.00
1.82
0.70
1.00
1.49
1.00
1.52
0.98
1.56
0.50
1.00
1.26
1.00
1.33
0.17
1.36
0.30
1.00
1.12
0.98
1.25
→0
1.16
0.10
1.00
1.05
0.67
1.21
→0
1.04
54

• Nt is an ARIMA(1, 0, 0).
αopt →0 when −1 < φ1 ≤1/3, αopt = (3φ1 −1)/2φ1
when 1/3 < φ1 < 1, and the choice of α is not critical when φ1 is close to 0.5.
SES performs well φ1 is large, say −0.5 < φ1 < 1.
• Nt is an ARIMA(1, 1, 1).
αopt increases as θ1 and/or φ1 increase, and overesti-
mation of α is less serious than the equivalent underestimation. SES performs
well when either −1 < θ1 < 0 and |φ1| is small or θ1φ1 < 0 and |θ1| ≈|φ1|
• Nt is an ARIMA(1, 0, 1).
αopt is often extremely small (→0) and increases as
θ1 and/or φ1 increase. When 0 < θ1 < 1, the smaller the |φ1|, the less critical
the choice of α. SES performs well when θ1φ1 < 0 and |θ1| ≈|φ1|.
As a result, when Nt is an ARIMA(p, 1, q) with 0 ≤p ≤1 and 0 ≤q ≤2, αopt tends to
be large, and overestimation of α is less serious than the equivalent underestimation.
In addition, SES performs well for an IMA(1, q) with θ1 < 0 and |θ2| small, an
ARI(1, 1) with small |φ1|, and an ARIMA(1, 1, 1) with −1 < θ1 < 0 and |φ1| small .
When Nt is an ARMA(p, q) with 0 ≤p ≤1 and 0 ≤q ≤2, αopt tends to be small,
and the choice of α is often not critical. In addition, SES performs well for an MA(q)
with small |θ1| and |θ2|, an AR(1) with large φ1, and an ARMA(1, 1) with θ1φ1 < 0
and |θ1| ≈|φ1|.
55

3.3
Holt’s Method
The one-step-ahead forecast by Holt’s method is
ˆYt|t−1 = lt−1 + bt−1,
(3.42)
where lt and bt are updated as follow
lt = α1Yt + (1 −α1)(lt−1 + bt−1),
(3.43a)
bt = α2(lt −lt−1) + (1 −α2)bt−1,
(3.43b)
where α1 and α2 are smoothing parameters taking values in the interval (0,1]. As
in SES, ˆYt|t−1 can be expressed in terms of past observations Yt−k, k > 0, and past
one-step-ahead forecasts ˆYt−k(1), k > 0,
ˆYt|t−1 = (α1 + α1α2)Yt−1 −α1Yt−2 + (2 −α1 −α1α2)ˆYt−1|t−2 −(1 −α1)ˆYt−2|t−3 (3.44)
Assume that the true data generating process is
Yt = µ0 + δt + Nt,
(3.45)
where µ0 and δ are constant, and Nt is a disturbance term with zero mean and second
diﬀerence as a stationary process
(1 −B)2Nt =
∞
X
i=0
ψiǫt−i,
(3.46)
Holt’s method with smoothing parameters α1 and α2 for model (3.45) leads to
et −(2 −α1 −α1α2)et−1 −(α1 −1)et−2 =
∞
X
i=0
ψiǫt−i.
(3.47)
Let λ1 and λ2 be the roots of the equation
x2 −(2 −α1 −α1α2)x −(α1 −1) = 0.
(3.48)
The fact that 0 < α1 ≤1 and 0 < α2 ≤1 implies that
|α1 −1| < 1,
(3.49a)
(α1 −1) + (2 −α1 −α1α2) = 1 −α1α2 < 1,
(3.49b)
(α1 −1) −(2 −α1 −α1α2) = 2α1 + α1α2 −3 < 1,
(3.49c)
56

which are necessary and suﬃcient conditions for the roots λ1 and λ2 to be less than
1 in absolute value. As a result, et can be written as
et
=
1
(1 −λ1B)(1 −λ2B)
∞
X
i=0
ψiǫt−i
=
∞
X
j=0
∞
X
k=0
∞
X
i=0
λj
1λk
2ψiǫt−i−j−k
=
∞
X
j=0
(
j
X
i=0
ψi
j−i
X
k=0
λj−i−k
1
λk
2)ǫt−j.
(3.50)
As a result, the mean of the one-step-ahead forecast error is
E[et] =
∞
X
j=0
(
j
X
i=0
ψi
j−i
X
k=0
λj−i−k
1
λk
2)E[ǫt−j] = 0,
(3.51)
which implies that the one-step-ahead forecast by Holt’s method for model (3.45) is
unbiased. The mean squared one-step-ahead forecast error is
E[e2
t] = σ2
∞
X
j=0
(
j
X
i=0
ψi
j−i
X
k=0
λj−i−k
1
λk
2)2.
(3.52)
3.3.1
Nt is an ARIMA(0, 2, q) process
Nt is an ARIMA(0, 2, q) process
(1 −B)2Nt =
q
X
i=0
θiǫt−i.
(3.53)
Therefore, ψi = θi for 0 ≤i ≤q, and ψi = 0 for i > q. The MSE becomes
E[e2
t] = σ2[
q−1
X
j=0
(
j
X
i=0
θi
j−i
X
k=0
λj−i−k
1
λk
2)2 +
∞
X
j=q
(
q
X
i=0
θi
j−i
X
k=0
λj−i−k
1
λk
2)2].
(3.54)
1). q = 0, Nt is an ARIMA(0,2,0).
E[e2
t] = σ2 ·
1 + λ1λ2
(1 −λ2
1)(1 −λ2
2)(1 −λ1λ2).
(3.55)
Figure 3.11 shows that the MSE is a monotone decreasing function of both α1
and α2. The minimum MSE occurs as α1 = α2 = 1, at which the one-step-ahead
forecast given by Holt’s method is
ˆYt|t−1 = 2Yt−1 −Yt−2.
(3.56)
57

Such a result is due to the ARIMA(0,2,0) disturbance, which has the property
E[Yt|Yt−1, Yt−2, · · · ] = E[Yt|Yt−1, Yt−2] = 2Yt−1 −Yt−2.
(3.57)
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9
10
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.11: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,0)
2). q = 1, Nt is an ARIMA(0,2,1).
E[e2
t] = σ2 · (λ1 + θ1)(λ2 + θ1) + (1 + λ1θ1)(1 + λ2θ1)
(1 −λ2
1)(1 −λ2
2)(1 −λ1λ2)
.
(3.58)
Figure 3.12 and Table 3.10 shows that
– Holt’s method performs better when −1 < θ1 < 0 than when 0 < θ1 < 1.
When −1 < θ1 < 0, the minimum MSE stays at σ2, the theoretically
optimal value, regardless of what value θ1 takes. When 0 < θ1 < 1, the
minimum MSE increases as θ1 increases.
– The optimal value of α1 stays at 1, and the optimal value of α2 equals
to 1 + θ1 when −1 < θ1 < 0 and stays at 1 when 0 < θ1 < 1. When
58

α1 = α2 = 1, according to equation (3.58), the minimum MSE equals to
σ2(1 + θ2
1).
In fact, when −1 < θ1 < 0, values of α1 and α2 can be found so that
Holt’s method gives optimal forecasts. For an ARIMA(0,2,1) disturbance,
equation (3.47) becomes
et −(2 −α1 −α1α2)et−1 −(α1 −1)et−2 = ǫt + θ1ǫt−1,
(3.59)
which suggests that, when α1 = 1 and α2 = 1 + θ1, et becomes a white
noise. Therefore, the forecast by Holt’s method is optimal.
– The smaller the θ1, the less critical the choice of α2, and overestimation of
α2 is less serious than the equivalent underestimation.
In summary, Holt’s method performs well when θ1 is small. Large α1 and α2
are preferred.
Table 3.10:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(0,2,1)
θ1
αT
opt = (α1, α2)
MSE/σ2
-0.95
(1.00, 0.05)
1.00
-0.90
(1.00, 0.10)
1.00
-0.80
(1.00, 0.20)
1.00
-0.70
(1.00, 0.30)
1.00
-0.60
(1.00, 0.40)
1.00
-0.50
(1.00, 0.50)
1.00
-0.40
(1.00, 0.60)
1.00
-0.30
(1.00, 0.70)
1.00
-0.20
(1.00, 0.80)
1.00
-0.10
(1.00, 0.90)
1.00
θ1
αT
opt = (α1, α2)
MSE/σ2
0.95
(1.00, 1.00)
1.90
0.90
(1.00, 1.00)
1.81
0.80
(1.00, 1.00)
1.64
0.70
(1.00, 1.00)
1.49
0.60
(1.00, 1.00)
1.36
0.50
(1.00, 1.00)
1.25
0.40
(1.00, 1.00)
1.16
0.30
(1.00, 1.00)
1.09
0.20
(1.00, 1.00)
1.04
0.10
(1.00, 1.00)
1.01
3). q = 2, Nt is an ARIMA(0,2,2).
Figures 3.13 and 3.14 and Table 3.11 show that
59

0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9
10
Solid Lines: θ1=−0.8; Dash−Dot Lines: θ1=0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9
10
Solid Lines: θ1=−0.6; Dash−Dot Lines: θ1=0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9
10
Solid Lines: θ1=−0.4; Dash−Dot Lines: θ1=0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9
10
Solid Lines: θ1=−0.2; Dash−Dot Lines: θ1=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.12: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,1)
60

– When 0 < θ2 < 1, the minimum MSE stays at σ2 for small θ1 and then
increases as θ1 and/or θ2 increase.
– When −1 < θ2 < 0 and θ1 ≤0, the value of θ1 has no eﬀect on the
minimum MSE and the minimum MSE increases as θ2 decreases. When
−1 < θ2 < 0 and θ1 > 0, the minimum MSE increases as θ1 increases while
decreases as θ2 increases.
– When 0 < θ2 < 1, the optimal value of α1, starting from 1 −θ2, increases
with θ1 once θ1 > −2θ2 before reaching 1. In addition, overestimation of
α1 is less serious than the equivalent underestimation.
– When −1 < θ2 < 0, the optimal value of α1 stays at 1, the optimal value
of α2 increases with θ1 before reaching 1, and the smaller the θ1, the less
critical the choice of α2.
In fact, Holt’s method is optimal for an ARIMA(0,2,2) when
α1 = 1 −θ2 and α2 = 1 + θ1 + θ2
1 −θ2
(3.60)
The fact that α1 ∈(0, 1] and α2 ∈(0, 1] implies that only for an ARIMA(0,2,2)
with θ1 and θ2 falling inside the region formed by (see Figure 2.1 in Chapter 2)
−(1 + θ2) < θ1 ≤−2θ2 and
0 ≤θ2 < 1,
(3.61)
could Holt’s method be optimal.
In summary, Holt’s method is optimal for an ARIMA(0,2,2) when θ1 and θ2
satisfy equation (3.60). Large α1 and α2 are perferred.
61

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1.4; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1.19; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.79; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.13: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,2)
62

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.49; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.3; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.49; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.14: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,2,2)
63

Table 3.11:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(0,2,2)
θ2 = −0.2
θ2 = 0.2
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-1.19
-
-
(0.80, 0.01)
1.00
-1.00
-
-
(0.80, 0.25)
1.00
-0.79
(1.00, 0.01)
1.04
(0.80, 0.51)
1.00
-0.60
(1.00, 0.25)
1.04
(0.80, 0.75)
1.00
-0.49
(1.00, 0.39)
1.04
(0.80, 0.89)
1.00
-0.40
(1.00, 0.50)
1.04
(0.80, 1.00)
1.00
-0.20
(1.00, 0.75)
1.04
(0.89, 1.00)
1.01
0.00
(1.00, 1.00)
1.04
(0.97, 1.00)
1.03
0.20
(1.00, 1.00)
1.08
(1.00, 1.00)
1.08
0.40
(1.00, 1.00)
1.20
(1.00, 1.00)
1.20
0.49
(1.00, 1.00)
1.28
(1.00, 1.00)
1.28
0.60
(1.00, 1.00)
1.40
(1.00, 1.00)
1.40
0.79
(1.00, 1.00)
1.66
(1.00, 1.00)
1.66
1.00
-
-
(1.00, 1.00)
2.04
1.19
-
-
(1.00, 1.00)
2.46
θ2 = −0.5
θ2 = 0.5
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-1.49
-
-
(0.50, 0.02)
1.00
-1.40
-
-
(0.50, 0.20)
1.00
-1.19
-
-
(0.50, 0.62)
1.00
-1.00
-
-
(0.50, 1.00)
1.00
-0.79
-
-
(0.61, 1.00)
1.02
-0.60
-
-
(0.71, 1.00)
1.06
-0.49
(1.00, 0.02)
1.25
(0.76, 1.00)
1.08
-0.40
(1.00, 0.20)
1.25
(0.79, 1.00)
1.11
-0.20
(1.00, 0.60)
1.25
(0.87, 1.00)
1.16
0.00
(1.00, 1.00)
1.25
(0.94, 1.00)
1.22
0.20
(1.00, 1.00)
1.29
(1.00, 1.00)
1.29
0.40
(1.00, 1.00)
1.41
(1.00, 1.00)
1.41
0.49
(1.00, 1.00)
1.49
(1.00, 1.00)
1.49
0.60
-
-
(1.00, 1.00)
1.61
0.79
-
-
(1.00, 1.00)
1.87
1.00
-
-
(1.00, 1.00)
2.25
1.19
-
-
(1.00, 1.00)
2.67
64

3.3.2
Nt is an ARIMA(0, 1, q) process
Nt is an ARIMA(0, 1, q) process
(1 −B)Nt =
q
X
i=0
θiǫt−i.
(3.62)
Let θ−1 = θq+1 = 0, then
(1 −B)2Nt =
q+1
X
i=0
(θi −θi−1)ǫt−i.
(3.63)
1). q = 0, Nt is an ARIMA(0,1,0), which is a random walk.
E[e2
t] = σ2 ·
2
(1 + λ1)(1 + λ2)(1 −λ1λ2).
(3.64)
Figure 3.15 shows that a large α1 and a small α2 give a small MSE. The mini-
mum MSE occurs as α1 = 1 and α2 →0. Also, the choice of α2 is not as critical
as the choice of α1.
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.15: Holt’s Method – MSE/σ2 as a function of α1, Nt is a random walk
2). q = 1, Nt is an ARIMA(0,1,1).
Figure 3.16 and Table 3.12 show that
65

– Holt’s method performs better when −1 < θ1 < 0 than when 0 < θ1 < 1.
When −1 < θ1 < 0, the minimum MSE stays at σ2 no matter what value
θ1 takes. When 0 < θ1 < 1, the minimum MSE increases as θ1 increases.
– When −1 < θ1 < 0, the minimum MSE occurs as α1 = 1 + θ1 and α2 →0,
and overestimation of α1 is less serious than the equivalent underestima-
tion. When 0 < θ1 < 1, the minimum MSE occurs as α1 = 1 and α2 →0.
– The larger the θ1, the less critical the choice of α2.
In summary, Holt’s method performs well for an ARIMA(0,1,1) when −1 <
θ1 < 0, and a small α2 is preferred.
Table 3.12:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(0,1,1)
θ1
αT
opt = (α1, α2)
MSE/σ2
-0.95
(0.05, →0)
1.00
-0.90
(0.10, →0)
1.00
-0.80
(0.20, →0)
1.00
-0.70
(0.30, →0)
1.00
-0.60
(0.40, →0)
1.00
-0.50
(0.50, →0)
1.00
-0.40
(0.60, →0)
1.00
-0.30
(0.70, →0)
1.00
-0.20
(0.80, →0)
1.00
-0.10
(0.90, →0)
1.00
θ1
αT
opt = (α1, α2)
MSE/σ2
0.95
(1.00, →0)
1.90
0.90
(1.00, →0)
1.81
0.80
(1.00, →0)
1.64
0.70
(1.00, →0)
1.49
0.60
(1.00, →0)
1.36
0.50
(1.00, →0)
1.25
0.40
(1.00, →0)
1.16
0.30
(1.00, →0)
1.09
0.20
(1.00, →0)
1.04
0.10
(1.00, →0)
1.01
3). q = 2, Nt is an ARIMA(0,1,2).
Figures 3.17 and 3.18 and Table 3.13 show that
– When θ1 ≤0, the minimum MSE depends only on |θ2|, the absolute value
of θ2, and is independent of both the sign of θ2 and the value of θ1. More-
over, the larger the |θ2|, the larger the minimum MSE.
– When θ1 > 0, the minimum MSE increases as θ1 increases.
66

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.16: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,1,1)
67

– When θ1 ≤0, the optimal value of α1 increases as θ1 increases and reaches
1 at θ1 = 0, while the optimal value of α2 stays at an extremely small value
(→0).
– When θ1 > 0, the optimal value of α1 stays at 1, while the optimal value
of α2 is extremely small (→0) when −1 < θ2 < 0 and increases with θ1
when 0 < θ2 < 1.
– The smaller the θ2, the less critical the choice of α1.
In summary, Holt’s method performs well for an ARIMA(0,1,2) when θ1 < 0
and |θ2| is small, and a small α2 is preferred.
Comparing the results for ARIMA(0,1,q) from SES (Tables 3.1 and 3.2) and Holt’s
method (Tables 3.12 and 3.13) respectively reveals that Holt’s method with α2 →0
gives exactly the same results as these by SES.
When α2 →0, the updating equation for bt in Holt’s method becomes bt ≈bt−1.
That is, there is no updating for the slope estimate bt anymore. Two situations where
no updating for bt is necessary are i) the slope δ in Equation (3.45) is known; ii) a
fairly good estimate of δ is already available and including new data improves very
little. Under such situations, Holt’s method reduces to
lt = α1Yt + (1 −α1)(lt−1 + δ)
(3.65)
and the one-step-ahead forecast by Holt’s method becomes
ˆYt|t−1 = α1Yt−1 + (1 −α1)ˆYt−1|t−2 + δ
(3.66)
which looks like the one-step-ahead forecast by SES with an extra term δ. Also, the
one-step-ahead forecast errors from Holt’s method now satisfy
et −(1 −α1)et−1 = (1 −B)Nt
(3.67)
Therefore, as long as (1 −B)Nt is stationary, Holt’s method with α2 →0 for (3.45)
will give exactly the same results as these by SES for (3.2).
68

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.49; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.2; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.2; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.79; θ2=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.2; θ2=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.2; θ2=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.17: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,1,2)
69

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1.19; θ2=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.6; θ2=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.6; θ2=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.6; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.6; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.18: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(0,1,2)
70

Table 3.13:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(0,1,2)
θ2 = −0.2
θ2 = 0.2
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-1.19
-
-
(0.01, →0)
1.04
-1.00
-
-
(0.17, →0)
1.04
-0.79
(0.01, →0)
1.04
(0.34, →0)
1.04
-0.60
(0.25, →0)
1.04
(0.50, →0)
1.04
-0.49
(0.39, →0)
1.04
(0.59, →0)
1.04
-0.40
(0.50, →0)
1.04
(0.67, →0)
1.04
-0.20
(0.75, →0)
1.04
(0.83, →0)
1.04
0.00
(1.00, →0)
1.04
(1.00, →0)
1.04
0.20
(1.00, →0)
1.08
(1.00, →0)
1.08
0.40
(1.00, →0)
1.20
(1.00, 0.21)
1.19
0.49
(1.00, →0)
1.28
(1.00, 0.41)
1.25
0.60
(1.00, →0)
1.40
(1.00, 0.66)
1.31
0.79
(1.00, →0)
1.66
(1.00, 1.00)
1.43
1.00
-
-
(1.00, 1.00)
1.68
1.19
-
-
(1.00, 1.00)
2.06
θ2 = −0.5
θ2 = 0.5
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-1.49
-
-
(0.01, →0)
1.25
-1.40
-
-
(0.07, →0)
1.25
-1.19
-
-
(0.21, →0)
1.25
-1.00
-
-
(0.33, →0)
1.25
-0.79
-
-
(0.47, →0)
1.25
-0.60
-
-
(0.60, →0)
1.25
-0.49
(0.02, →0)
1.25
(0.67, →0)
1.25
-0.40
(0.20, →0)
1.25
(0.73, →0)
1.25
-0.20
(0.60, →0)
1.25
(0.87, →0)
1.25
0.00
(1.00, →0)
1.25
(1.00, →0)
1.25
0.20
(1.00, →0)
1.29
(1.00, 0.16)
1.28
0.40
(1.00, →0)
1.41
(1.00, 0.44)
1.31
0.49
(1.00, →0)
1.49
(1.00, 0.57)
1.33
0.60
-
-
(1.00, 0.73)
1.35
0.79
-
-
(1.00, 1.00)
1.38
1.00
-
-
(1.00, 1.00)
1.50
1.19
-
-
(1.00, 1.00)
1.76
71

3.3.3
Nt is an ARIMA(0, 0, q) process
Nt is an ARIMA(0, 0, q) process
Nt =
q
X
i=0
θiǫt−i.
(3.68)
Let θ−2 = θ−1 = θq+1 = θq+2 = 0, then
(1 −B)2Nt =
q+2
X
j=0
(θj −2θj−1 + θj−2)ǫt−j.
(3.69)
1). q = 0, Nt is a white noise.
Figure 3.19 shows that the MSE is a monotone decreasing function of both α1
and α2, and the minimum MSE occurs as α1 →0 and α2 →0.
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.19: Holt’s Method – MSE/σ2 as a function of α1, Nt is a white noise
2). q = 1, Nt is an MA(1).
Figure 3.20 and Table 3.14 show that
– The minimum MSE depends only on |θ1|, the absolute value of θ1, and
is free of the sign of θ1. Moreover, the minimum MSE increases as |θ1|
increases.
72

– The MSE is a monotone decreasing function of both α1 and α2, and reaches
minimum as α1 →0 and α2 →0 for any value of θ1.
– The larger the θ1, the less critical the choice of α1. When θ1, say, is greater
than 0.5, any choice in the interval (0,1] for α1 will be equally good.
In summary, Holt’s method performs well for an MA(1) only when |θ1| is small,
and small α1 and α2 are preferred.
Table 3.14: Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is a MA(1)
θ1
αT
opt = (α1, α2)
MSE/σ2
-0.95
(→0, →0)
1.90
-0.90
(→0, →0)
1.81
-0.80
(→0, →0)
1.64
-0.70
(→0, →0)
1.49
-0.60
(→0, →0)
1.36
-0.50
(→0, →0)
1.25
-0.40
(→0, →0)
1.16
-0.30
(→0, →0)
1.09
-0.20
(→0, →0)
1.04
-0.10
(→0, →0)
1.01
θ1
αT
opt = (α1, α2)
MSE/σ2
0.95
(→0, →0)
1.90
0.90
(→0, →0)
1.81
0.80
(→0, →0)
1.64
0.70
(→0, →0)
1.49
0.60
(→0, →0)
1.36
0.50
(→0, →0)
1.25
0.40
(→0, →0)
1.16
0.30
(→0, →0)
1.09
0.20
(→0, →0)
1.04
0.10
(→0, →0)
1.01
3). q = 2, Nt is an MA(2).
Figure 3.21 and Table 3.15 show that Holt’s method performs well only when
both |theta1| and |θ2| are small. The optimal value of both α1 and α2 are small.
73

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.20: Holt’s Method – MSE/σ2 as a function of α1, Nt is a MA(1)
74

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−1.19; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.79; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.79; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=1.19; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=1.49; θ2=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.49; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.49; θ2=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.21: Holt’s Method – MSE/σ2 as a function of α1, Nt is a MA(2)
75

Table 3.15: Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is a MA(2)
θ2 = −0.2
θ2 = 0.2
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-1.19
-
-
(→0, →0)
2.46
-1.00
-
-
(→0, →0)
2.04
-0.79
(→0, →0)
1.66
(→0, →0)
1.66
-0.60
(→0, →0)
1.40
(→0, →0)
1.40
-0.49
(→0, →0)
1.28
(→0, →0)
1.28
-0.40
(→0, →0)
1.20
(→0, →0)
1.20
-0.20
(→0, →0)
1.08
(→0, →0)
1.08
0.00
(→0, →0)
1.04
(→0, →0)
1.04
0.20
(→0, →0)
1.08
(→0, →0)
1.08
0.40
(→0, →0)
1.20
(0.21, →0)
1.19
0.49
(→0, →0)
1.28
(0.41, →0)
1.25
0.60
(→0, →0)
1.40
(0.66, →0)
1.31
0.79
(→0, →0)
1.66
(1.00, →0)
1.43
1.00
-
-
(1.00, →0)
1.68
1.19
-
-
(1.00, →0)
2.06
θ2 = −0.5
θ2 = 0.5
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-1.49
-
-
(→0, →0)
3.47
-1.40
-
-
(→0, →0)
3.21
-1.19
-
-
(→0, →0)
2.67
-1.00
-
-
(→0, →0)
2.25
-0.79
-
-
(→0, →0)
1.87
-0.60
-
-
(→0, →0)
1.61
-0.49
(→0, →0)
1.49
(→0, →0)
1.49
-0.40
(→0, →0)
1.41
(→0, →0)
1.41
-0.20
(→0, →0)
1.29
(→0, →0)
1.29
0.00
(→0, →0)
1.25
(→0, →0)
1.25
0.20
(→0, →0)
1.29
(0.16, →0)
1.28
0.40
(→0, →0)
1.41
(0.44, →0)
1.31
0.49
(→0, →0)
1.49
(0.57, →0)
1.33
0.60
-
-
(0.73, →0)
1.35
0.79
-
-
(1.00, →0)
1.38
1.00
-
-
(1.00, →0)
1.50
1.19
-
-
(1.00, →0)
1.76
76

3.3.4
Nt is an ARIMA(1,d,0) process
Nt is an ARIMA(1,d,0) process
(1 −B)dNt = (1 −φ1B)−1ǫt.
(3.70)
1). d = 2, Nt is an ARIMA(1,2,0).
(1 −B)2Nt = (1 −φ1B)−1ǫt =
∞
X
i=0
φi
1ǫt−i.
(3.71)
Figure 3.22 and Table 3.16 show that
– Holt’s method performs better when −1 < φ1 < 0 than when 0 < φ1 < 1.
When −1 < φ1 < 0, the minimum MSE increases as |φ1| increases, and
the larger the |φ1|, the faster the minimum MSE grows. Same holds for
0 < φ1 < 1.
– The optimal values of α1 and α2 are typically greater than 0.5 except when
φ1 is very close to -1.
In summary, Holt’s method performs well for an ARIMA(1,2,0) when either
−1 < φ1 < 0 or |φ1| is small, and large α1 and α2 are perferred.
Table 3.16:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(1,2,0)
φ1
αT
opt = (α1, α2)
MSE/σ2
-0.95
(0.45, 1.00)
2.33
-0.90
(0.52, 1.00)
1.61
-0.80
(0.61, 1.00)
1.22
-0.70
(0.67, 0.99)
1.09
-0.60
(0.74, 0.92)
1.04
-0.50
(0.80, 0.89)
1.01
-0.40
(0.86, 0.87)
1.00
-0.30
(0.92, 0.85)
1.00
-0.20
(0.96, 0.88)
1.00
-0.10
(0.99, 0.92)
1.00
φ1
αT
opt = (α1, α2)
MSE/σ2
0.95
(1.00, 1.00)
10.26
0.90
(1.00, 1.00)
5.26
0.80
(1.00, 1.00)
2.78
0.70
(1.00, 1.00)
1.96
0.60
(1.00, 1.00)
1.56
0.50
(1.00, 1.00)
1.33
0.40
(1.00, 1.00)
1.19
0.30
(1.00, 1.00)
1.10
0.20
(1.00, 1.00)
1.04
0.10
(1.00, 1.00)
1.01
77

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.22: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,2,0)
78

2). d = 1, Nt is an ARIMA(1,1,0).
(1 −B)Nt = (1 −φ1B)−1ǫt.
(3.72)
Then
(1 −B)2Nt = (1 −B)(1 −φ1B)−1ǫt = ǫt + (φ1 −1)
∞
X
i=1
φi−1
1
ǫt−i.
(3.73)
Figure 3.23 and Table 3.17 show that
– Holt’s method performs well for an ARIMA(1,1,0) disturbance except when
φ1 is close to -1. When −1 < φ1 < 0, the minimum MSE increases as φ1
decreases. When 0 < φ1 < 1, the worst performance occurs at φ1 = 0.5,
and the further away from 0.5 the φ1, the smaller the minimum MSE.
– When −1 < φ1 < 0, the optimal value of α1 increases as φ1 increases, and
overestimation of α1 is less serious than the equivalent underestimation.
When 0 < φ1 < 1, the optimal value of α1 stays at 1.
– When −1 < φ1 ≤0.3, the optimal value of α2 stays at an extremely small
value (→0); when 0.3 < φ1 < 1, the optimal value of α2 increases as φ1
increases; and the choice of α2 is not critical when 0 < φ1 < 1 and |φ1| is
small.
In summary, Holt’s method performs well for an ARIMA(1,1,0), and a large α1
is preferred.
3). d = 0, Nt is an AR(1).
Nt = (1 −φ1B)−1ǫt.
(3.74)
Then
(1 −B)2Nt
=
(1 −B)2(1 −φ1B)−1ǫt
=
ǫt + (φ1 −2)ǫt−1 + (φ1 −1)2
∞
X
i=2
φi−2
1
ǫt−i.
(3.75)
Figure 3.24 and Table 3.18 show that
79

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.23: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,1,0)
80

Table 3.17:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(1,1,0)
φ1
αT
opt = (α1, α2)
MSE/σ2
-0.95
(0.19, →0)
3.88
-0.90
(0.26, →0)
2.33
-0.80
(0.36, →0)
1.52
-0.70
(0.44, →0)
1.25
-0.60
(0.50, →0)
1.12
-0.50
(0.57, →0)
1.06
-0.40
(0.64, →0)
1.02
-0.30
(0.72, →0)
1.01
-0.20
(0.81, →0)
1.00
-0.10
(0.90, →0)
1.00
φ1
αT
opt = (α1, α2)
MSE/σ2
0.95
(1.00, 0.97)
1.02
0.90
(1.00, 0.94)
1.05
0.80
(1.00, 0.87)
1.10
0.70
(1.00, 0.79)
1.14
0.60
(1.00, 0.67)
1.17
0.50
(1.00, 0.50)
1.19
0.40
(1.00, 0.25)
1.17
0.30
(1.00, →0)
1.10
0.20
(1.00, →0)
1.04
0.10
(1.00, →0)
1.01
– Holt’s method performs well for an AR(1) disturbance except when φ1
is close to -1. When −1 < φ1 < 0, the minimum MSE increases as φ1
decreases. When 0 < φ1 < 1, the worst performance occurs at φ1 = 0.5,
and the further away from 0.5 the φ1, the smaller the minimum MSE.
– When −1 < φ1 ≤0.3, the optimal value of α1 stays at an extremely small
value (→0); when 0.3 < φ1 < 1, the optimal value of α1 increases as φ1
increases; and the choice of α1 is not critical when φ1 is close to 0.5.
– The optimal value of α2 stays at an extremely small value (→0).
In summary, Holt’s method performs well for an AR(1) except when φ1 is close
to -1, and a small α2 is preferred.
Comparing the results for ARIMA(1,1,0) and AR(1) from SES and Holt’s method
respectively (Table 3.5 vs. Table 3.17, Table 3.6 vs. Table 3.18) reveals that Holt’s
method with α2 →0 for model (3.45) produces exactly the same results as these by
SES for model (3.2) (see explanation given for ARIMA(0, 1, q)).
81

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=−0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.2
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.4
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.6
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
φ1=0.8
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.24: Holt’s Method – MSE/σ2 as a function of α1, Nt is an AR(1)
82

Table 3.18: Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an AR(1)
φ1
αT
opt = (α1, α2)
MSE/σ2
-0.95
(→0, 0.01)
10.26
-0.90
(→0, 0.01)
5.26
-0.80
(→0, →0)
2.78
-0.70
(→0, →0)
1.96
-0.60
(→0, →0)
1.56
-0.50
(→0, →0)
1.33
-0.40
(→0, →0)
1.19
-0.30
(→0, →0)
1.10
-0.20
(→0, →0)
1.04
-0.10
(→0, →0)
1.01
φ1
αT
opt = (α1, α2)
MSE/σ2
0.95
(0.97, →0)
1.02
0.90
(0.94, →0)
1.05
0.80
(0.88, →0)
1.10
0.70
(0.79, →0)
1.14
0.60
(0.67, →0)
1.17
0.50
(0.50, →0)
1.19
0.40
(0.25, →0)
1.17
0.30
(→0, →0)
1.10
0.20
(→0, →0)
1.04
0.10
(→0, →0)
1.01
3.3.5
Nt is an ARIMA(1,d,1) process
Nt is an ARIMA(1,d,1) process
(1 −B)dNt = (1 −φ1B)−1(ǫt + θ1ǫt−1).
(3.76)
1). d = 2, Nt is an ARIMA(1,2,1).
(1 −B)2Nt = (1 −φ1B)−1(ǫt + θ1ǫt−1) = ǫt + (φ1 + θ1)
∞
X
i=1
φi−1
1
ǫt−i.
(3.77)
Figures 3.25 and 3.26 and Table 3.19 show that
– Holt’s method performs well under two situations: i) −1 < θ1 < 0 and |φ1|
is small , and ii) 0 < θ1 < 1, φ1 < 0 and |θ1| and |φ1| are close.
– α1 increases as θ1 and/or φ1 increase, and α2 increases as θ1 increases.
– Overestimation of α1 is less serious than the equivalent underestimation.
2). d = 1, Nt is an ARIMA(1,1,1)
(1 −B)Nt = (1 −φ1B)−1(ǫt + θ1ǫt−1).
(3.78)
Then
(1 −B)2Nt = ǫt + (φ1 + θ1 −1)ǫt−1 + (φ1 + θ1)(φ1 −1)
∞
X
i=2
φi−2
1
ǫt−i.
(3.79)
83

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=−0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=−0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=−0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=−0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=−0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=−0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.25: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,2,1)
84

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=−0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=−0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=−0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=−0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=−0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=−0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.26: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,2,1)
85

Table 3.19:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(1,2,1)
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-0.90
(0.28, 0.22)
2.27
(0.59, 0.13)
1.05
(0.91, 0.10)
1.00
-0.70
(0.31, 0.59)
2.13
(0.63, 0.36)
1.04
(0.92, 0.30)
1.00
-0.50
(0.35, 0.86)
1.98
(0.67, 0.55)
1.03
(0.94, 0.49)
1.00
-0.30
(0.40, 1.00)
1.84
(0.72, 0.71)
1.03
(0.96, 0.67)
1.00
-0.10
(0.48, 1.00)
1.69
(0.77, 0.84)
1.02
(0.98, 0.84)
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-0.90
(1.00, 1.00)
1.00
(1.00, 0.56)
1.09
(1.00, 0.12)
1.01
-0.70
(1.00, 1.00)
1.21
(1.00, 0.79)
1.02
(1.00, 0.36)
1.00
-0.50
(1.00, 1.00)
1.84
(1.00, 1.00)
1.00
(1.00, 0.58)
1.00
-0.30
(1.00, 1.00)
2.89
(1.00, 1.00)
1.05
(1.00, 0.80)
1.00
-0.10
(1.00, 1.00)
4.37
(1.00, 1.00)
1.21
(1.00, 1.00)
1.00
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
0.90
(1.00, 1.00)
1.00
(1.00, 1.00)
1.21
(1.00, 1.00)
1.65
0.70
(0.87, 1.00)
1.08
(1.00, 1.00)
1.05
(1.00, 1.00)
1.36
0.50
(0.75, 1.00)
1.23
(1.00, 1.00)
1.00
(1.00, 1.00)
1.16
0.30
(0.65, 1.00)
1.38
(0.91, 0.98)
1.00
(1.00, 1.00)
1.04
0.10
(0.56, 1.00)
1.54
(0.84, 0.92)
1.01
(1.00, 1.00)
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
0.90
(1.00, 1.00)
18.05
(1.00, 1.00)
3.61
(1.00, 1.00)
2.01
0.70
(1.00, 1.00)
14.47
(1.00, 1.00)
2.92
(1.00, 1.00)
1.65
0.50
(1.00, 1.00)
11.32
(1.00, 1.00)
2.33
(1.00, 1.00)
1.36
0.30
(1.00, 1.00)
8.58
(1.00, 1.00)
1.85
(1.00, 1.00)
1.16
0.10
(1.00, 1.00)
6.26
(1.00, 1.00)
1.48
(1.00, 1.00)
1.04
86

Figures 3.27 and 3.28 and Table 3.20 show that
– Holt’s method performs well θ1φ1 < 0.
– α1 increases as θ1 and/or φ1 increase.
– Overestimation of α1 is less serious than the equivalent underestimation,
and, when 0 < θ1 < 1 and |φ1| is small, the choice of α2 is not critical.
3). d = 0, Nt is an ARMA(1,1)
Nt = (1 −φ1B)−1(ǫt + θ1ǫt−1).
(3.80)
Then
(1 −B)2Nt
=
ǫt + (φ1 + θ1 −2)ǫt−1 + [(φ1 + θ1)(φ1 −2) + 1]ǫt−2
+ (φ1 + θ1)(φ1 −1)2
∞
X
i=3
φi−3
1
ǫt−i.
(3.81)
Figures 3.29 and 3.30 and Table 3.21 show that
– Holt’s method performs well when θ1φ1 < 0 and |θ1| and |φ1| are close.
– The optimal value of α1, starting from an small value (≈0), becomes not
critical and then increases as θ1 and/or φ1 increases.
– The optimal value of α2 stays at an small value (≈0) regardless of what
value θ1 or φ1 takes.
3.3.6
Summary
Based on the results above on the performance of Holt’s method for diﬀerent types
of ARIMA time series, the following conclusions can be drawn:
• Nt is an ARIMA(0, 2, q) with 0 ≤q ≤2.
αopt = (1, 1)T when θ1 > 0, αopt <
(1, 1)T when θ1 < 0, and overestimation of either α1 or α2 is less serious than
the equivalent underestimation. Holt’s method performs well when θ1 < 0 and
0 ≤θ2 < 1.
87

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.27: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,1,1)
88

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.28: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARIMA(1,1,1)
89

Table 3.20:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARIMA(1,1,1)
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-0.90
(0.02, →0)
4.91
(0.05, →0)
1.30
(0.08, →0)
1.01
-0.70
(0.06, →0)
4.25
(0.14, →0)
1.24
(0.26, →0)
1.01
-0.50
(0.10, →0)
3.63
(0.25, →0)
1.18
(0.43, →0)
1.00
-0.30
(0.16, →0)
3.07
(0.37, →0)
1.12
(0.61, →0)
1.00
-0.10
(0.22, →0)
2.57
(0.50, →0)
1.08
(0.80, →0)
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-0.90
(1.00, →0)
1.00
(0.56, →0)
1.09
(0.12, →0)
1.01
-0.70
(1.00, 0.19)
1.04
(0.79, →0)
1.02
(0.36, →0)
1.00
-0.50
(1.00, 0.41)
1.04
(1.00, →0)
1.00
(0.58, →0)
1.00
-0.30
(1.00, 0.63)
1.05
(1.00, →0)
1.05
(0.80, →0)
1.00
-0.10
(1.00, 0.84)
1.05
(1.00, 0.33)
1.15
(1.00, →0)
1.00
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
0.90
(1.00, →0)
1.00
(1.00, →0)
1.21
(1.00, →0)
1.65
0.70
(0.76, →0)
1.12
(1.00, →0)
1.05
(1.00, →0)
1.36
0.50
(0.57, →0)
1.38
(1.00, →0)
1.00
(1.00, →0)
1.16
0.30
(0.42, →0)
1.71
(0.81, →0)
1.01
(1.00, →0)
1.04
0.10
(0.31, →0)
2.11
(0.65, →0)
1.04
(1.00, →0)
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
0.90
(1.00, 1.00)
1.81
(1.00, 1.00)
1.81
(1.00, 1.00)
1.82
0.70
(1.00, 1.00)
1.49
(1.00, 1.00)
1.52
(1.00, 0.98)
1.56
0.50
(1.00, 1.00)
1.26
(1.00, 1.00)
1.33
(1.00, 0.17)
1.36
0.30
(1.00, 1.00)
1.12
(1.00, 0.98)
1.25
(1.00, →0)
1.16
0.10
(1.00, 1.00)
1.05
(1.00, 0.67)
1.21
(1.00, →0)
1.04
90

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.9; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.5; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=−0.1; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.29: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARMA(1,1)
91

0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=−0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=0.1
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=0.5
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.1; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.5; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
3
3.5
4
4.5
5
θ1=0.9; φ1=0.9
α1
MSE/σ2
α2=0.1
α2=0.3
α2=0.5
α2=0.7
α2=0.9
Figure 3.30: Holt’s Method – MSE/σ2 as a function of α1, Nt is an ARMA(1,1)
92

Table 3.21:
Holt’s Method – Optimal α and Minimum MSE/σ2, Nt is an
ARMA(1,1)
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-0.90
(→0, 0.09)
18.05
(→0, 0.03)
3.61
(→0, →0)
2.01
-0.70
(→0, →0)
14.47
(→0, 0.01)
2.92
(→0, →0)
1.65
-0.50
(→0, 0.02)
11.32
(→0, →0)
2.33
(→0, →0)
1.36
-0.30
(→0, 0.01)
8.58
(→0, →0)
1.85
(→0, →0)
1.16
-0.10
(→0, →0)
6.26
(→0, →0)
1.48
(→0, →0)
1.04
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
-0.90
(→0, →0)
1.00
(→0, 0.03)
1.21
(→0, 0.01)
1.65
-0.70
(0.19, →0)
1.04
(→0, 0.01)
1.05
(→0, →0)
1.36
-0.50
(0.41, →0)
1.04
(→0, →0)
1.00
(→0, →0)
1.16
-0.30
(0.63, →0)
1.05
(→0, →0)
1.05
(→0, →0)
1.04
-0.10
(0.84, →0)
1.05
(0.33, →0)
1.15
(→0, →0)
1.00
φ1 = −0.9
φ1 = −0.5
φ1 = −0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
0.90
(→0, →0)
1.00
(→0, →0)
1.21
(→0, →0)
1.65
0.70
(→0, →0)
1.21
(→0, →0)
1.05
(→0, →0)
1.36
0.50
(→0, →0)
1.84
(→0, →0)
1.00
(→0, →0)
1.16
0.30
(→0, →0)
2.89
(→0, →0)
1.05
(→0, →0)
1.04
0.10
(→0, →0)
4.37
(→0, →0)
1.21
(→0, →0)
1.00
φ1 = 0.9
φ1 = 0.5
φ1 = 0.1
θ1
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
αT
opt = (α1, α2)
MSE/σ2
0.90
(1.00, →0)
1.81
(1.00, →0)
1.81
(1.00, →0)
1.82
0.70
(1.00, →0)
1.49
(1.00, →0)
1.52
(0.98, →0)
1.56
0.50
(1.00, →0)
1.26
(1.00, →0)
1.33
(0.17, →0)
1.36
0.30
(1.00, →0)
1.12
(0.98, →0)
1.25
(→0, →0)
1.16
0.10
(1.00, →0)
1.05
(0.67, →0)
1.21
(→0, →0)
1.04
93

• Nt is an ARIMA(0, 1, q) with 0 ≤q ≤2.
The optimal value of α1 is equal to 1
when θ1 > 0 and less than 1 when θ1 < 0, and the optimal value of α2 is often
extremely small (→0). Holt’s method performs well when θ1 < 0 and |θ2| is
small.
• Nt is an ARIMA(0, 0, q) with 0 ≤q ≤2.
The optimal value of both α1 and α2
are often extremely small (→(0, 0)T). Holt’s method performs well both |θ1|
and |θ2| are small.
• Nt is an ARIMA(1, 2, 0).
The optimal value of both α1 and α2 are large, and
overestimation of either α1 or α2 is less serious than the equivalent underesti-
mation. Holt’s method performs well when |φ1| is small, say −0.8 < φ1 < 0.4.
• Nt is an ARIMA(1, 1, 0).
The optimal value of α1 equals to 1 when 0 < φ1 < 1
and increases with φ1 when −1 < φ1 < 0. The optimal value of α2 is extremely
small (→0) when −1 < θ1 ≤1/3 and otherwise increases as φ1 increases. Holt’s
method performs well except when φ1 is close to -1.
• Nt is an ARIMA(1, 0, 0).
The optimal value of α1 is extremely small (→0)
when −1 < θ1 ≤1/3 and otherwise increases as φ1 increases. The optimal value
of α2 stays at an extremely small value (→0). Holt’s method performs well
except when φ1 is close to -1.
• Nt is an ARIMA(1, 2, 1).
The optimal value of both α1 and α2 are large. Holt’s
method performs well when either −1 < θ1 < 0 and |φ1| is small or θ1φ1 < 0
and |θ1| ≈|φ1|.
• Nt is an ARIMA(1, 1, 1).
The optimal value of α1 is often large while the
optimal value of α2 is often very small.
Holt’s method performs well when
either −1 < θ1 < 0 and |φ1| is small or θ1φ1 < 0 and |θ1| ≈|φ1|
94

• Nt is an ARIMA(1, 0, 1).
The optimal values of both α1 and α2 are often small.
Holt’s method performs well when θ1φ1 < 0 and |θ1| ≈|φ1|
As a result, Nt is an ARIMA(p, 2, q) with 0 ≤p ≤1 and 0 ≤q ≤2, the optimal
values of both α1 and α2 tend to be large, and overestimation of either α1 or α2 is less
serious than the equivalent underestimation. In addition, Holt’s method performs
well for an IMA(2, q) with θ1 < 0 and 0 < θ2 < 1, an ARI(1, 2) with small |φ1|, and
an ARIMA(1, 2, 1) with −1 < θ1 < 0 and |φ1| small. When Nt is an ARIMA(p, 1, q)
with 0 ≤p ≤1 and 0 ≤q ≤2, the optimal value of α1 tends to be large while
the optimal value of α2 tends to be small, and often overestimation of α1 is less
serious than the equivalent underestimation. In addition, Holt’s method performs
well for an IMA(1, q) with θ1 < 0 and |θ2| small, an ARI(1, 1) with large φ1, and an
ARIMA(1, 1, 1) with −1 < θ1 < 0 and |φ1| small. When Nt is an ARMA(p, q) with
0 ≤p ≤1 and 0 ≤q ≤2, the optimal values of both α1 and α2 are often extremely
small. Holt’s method performs well for an MA(q) with small |θ1| and |θ2|, an AR(1)
with large φ1 is large, and an ARMA(1, 1) with θ1φ1 < 0 and |θ1| ≈|φ1|.
95

CHAPTER IV
EXPONENTIAL SMOOTHING WITH
COVARIATES
4.1
Introduction
All of the ES methods described in Chapter 2), regardless of how they model a time
series, an additive trend with a multiplicative seasonality or a damped multiplicative
trend without seasonalities or any other forms (see Table 2.1 in Chapter 2), implic-
itly assume that past observations of a time series contain all information required
for forecasting its future. In other words, all of these methods forecast the future
of a time series using only its past observations. The history of a time series cer-
tainly contains knowledge about its future. While, other information beyond what is
available in a series’ own history may also shed light on the series’ movements along
time and therefore lead to more accurate forecasting of its future if incorporated. For
example, Figure 4.1(a) displays the number of people (in thousands) killed due to
motor vehicle accidents in the United States from 1911 to 1970. Supposing that we
are at the beginning of the year 1971 and would like to forecast the number of motor
vehicle deaths in the next three years, we could decompose the death series into two
components, a trend and a seasonality,
deaths = trend + seasonality + disturbance term
(4.1)
and choose an appropriate ES method for forecasting. On the other hand, the number
of motor vehicle deaths may be aﬀected by various factors such as annual vehicle miles
of travel, quality of roads, behaviors of drivers, and conditions of weather. Figure
4.1(b) shows annual vehicle miles of travel (in billions) in the United States from 1911
96

to 1970. The mile series exhibits movements that appear correlated to those of the
death series. For instance, the two series both dropped suddenly in 1942 and then
started to climb rapidly in 1944. This suggests a regression model
deaths = miles + road + driver + weather + disturbance term.
(4.2)
Using only the series’ own history for forecasting, model (4.1) might lose valuable
information contained in the inﬂuencing factors. Model (4.2), on the other hand,
consider only inﬂuencing factors that may not be able to completely explain the
movements of the death series.Furthermore, although some factors such as miles of
travel are easy to measure, some factors such as the quality of roads and the behav-
iors of drivers are diﬃcult to quantify. A possible solution is to take advantage of
all available information, using ES methods to model what are left unexplained by
measurable factors in the movements of the time series being forecasted.
From now on, we will refer to those measurable factors as “covariates.”
4.2
Exponential Smoothing with Covariates (ES-
Cov)
Let zt, t = 1, 2, · · · , denote the observed values of a q × 1 vector of covariates. Under
the assumption of a linear relationship between Yt and zt, the series of interested can
be adequately represented by a model of the form
Yt = µt + δTzt + ǫt,
(4.3)
where µt is the intercept at time t, δ is a vector of q constant coeﬃcients, and ǫt is
a white noise process with E[ǫt] = 0 and E[ǫ2
t] = σ2. Model (4.3) is diﬀerent from
a classical linear regression model in that the intercept µt here is time-varying (as
indicated by the index t) and more importantly is serially correlated.
A new forecasting method, ESCov, is proposed for forecasting a time series like Yt
in model (4.3). This new method estimates and forecasts the time-varying and serially
97

1910
1920
1930
1940
1950
1960
1970
0
10
20
30
40
50
60
Year
Motor Vehicle Deaths (Thousand)
(a)
1910
1920
1930
1940
1950
1960
1970
0
200
400
600
800
1000
1200
Year
Vehicle Miles of Travel (Billion)
(b)
Figure 4.1: US Motor Vehicle Deaths and Miles of Travel from 1911 to 1970
98

correlated intercept µt using ES methods, thus the name “Exponential Smoothing
with Covariates.” ESCov has two advantages by taking into account both the history
of the time series of interest and the information hidden in quantiﬁable covariates.
First, incorporating covariates may lead to a more accurate forecast of the time series
of interest compared to considering only the series’ past values. Second, to forecast,
we only need to know how the time series of interest moves along time. However, to
make the series move in the desired directions, for example to reduce deaths due to
motor vehicle accidents, we need to understand the reasons behind its movements.
Such knowledge can often be learned from its relationship with other variables.
4.2.1
The Procedure
Let lt+h|t denote the h-step-ahead forecast of µt+h made at time t. Let d denote the
estimator of δ. The h-step-ahead forecast of Yt+h at time t by ESCov is
ˆYt+h|t = lt+h|t + dTzt+h,
h > 0,
(4.4)
and the corresponding h-step-ahead forecast error is
et+h|t = Yt+h −ˆYt+h|t = Yt+h −lt+h|t −dTzt+h.
(4.5)
The h-step-ahead forecast of the intercept, lt+h|t, depends on which ES method is
used to estimate and forecast the intercept µt. Suppose that Holt’s method is chosen,
then
lt+h|t = lt + hbt,
(4.6)
in which lt and bt are calculated using the recurrence equations
lt = lt−1 + bt−1 + α1et,
(4.7a)
bt = bt−1 + α2et,
(4.7b)
where α1 and α2 are smoothing parameters taking values in the interval (0,1], and et
is the one-step-ahead forecast error from ESCov
et ≡et|t−1 = Yt −lt−1 −bt−1 −dTzt.
(4.8)
99

When the future values of covariates, zt+h, are unknown, for instance the number
of miles driven in future in the motor vehicle deaths example is unknown, their
predictions ˆzt+h|t are used instead.
4.2.2
A General Form
Any ES method could be chosen for estimating and forecasting the intercept µt. A
general form of ESCov consists of two equations,
• the recurrence equation
bt = g(bt−1) + w(α, bt−1)et
(4.9)
• and the forecasting equation
ˆYt+h|t = f(bt) + dTzt+h,
h > 0,
(4.10)
where bt is a p × 1 vector of smoothed statistics (e.g., lt and bt in Holt’s method); α
is a p × 1 vector of smoothing parameters with 0 < αi ≤1, i = 1, · · · , p ; g and w are
mappings from ℜp to ℜp; and f is a mapping from ℜp to ℜ.
4.2.3
Parameters Estimation
To use ESCov for forecasting, we have to ﬁnd appropriate values for b0 and parameters
α and d (and the damping factor φ if damped ES methods are chosen). Then, the
value of bt, t > 0, can be obtained recursively using the recurrence equation (4.9),
and forecasts for future values can be calculated using the forecasting equation (4.10).
• Estimation of b0
A heuristic procedure is used to estimate b0. Let T0 be a positive integer, and, unless
particularly mentioned, T0 is set to ten.
a). For non-seasonal data, ﬁt a linear regression model with a deterministic time
trend a(t) and the predictor vector zt on the ﬁrst k ∈{p+q, p+q+1, · · · , T0}
100

observations and compute the ordinary least squares (OLS) estimates of the
regression coeﬃcients (including the intercept). Then, set b0 to the average of
the T −p −q + 1 estimates of the coeﬃcients of the time trend.
The deterministic time trend used in the regression model depends on the ES
method chosen for the estimation of µt. For methods with no trend like N-N, a
constant trend is included (i.e., a(t) = a0), and b0 is set to the average of the
estimates of a0. For methods with additive trend like A-N and DA-N, a linear
trend is used (i.e., a(t) = a0 + a1t), and b0 is set to the average of the estimates
of (a0, a1)T.
b). For seasonal data, ﬁrst estimate the initial seasonal indices, c0, c1, · · · , c−M+1,
then apply the procedure above to the deseasonalized data. The estimated time
trend coeﬃcients together with the estimated seasonal indices form b0.
To estimate the initial seasonal indices, compute a M-moving average using the
ﬁrst few seasonal cycles, de-trend the data by dividing Yt by (for multiplica-
tive seasonality) or subtracting from Yt (for additive seasonality) the moving
averages, and average the de-trended data for each season to give the initial
seasonal indices, c0, c1, · · · , c−M+1. If data are available, four complete seasonal
cycles are used for the estimation of initial seasonal indices.
• Estimation of Parameters α and d
The parameters α and d (and φ) are estimated by minimizing the sum of squared
one-step-ahead forecast errors
SSE =
T
X
t=1
(Yt −Yt|t−1)2,
(4.11)
where T is the number of observations in the training data.
101

4.3
Numerical Experiments
Before beginning with numerical experiments, we introduce four accuracy measures
that will be used to compare the performance of diﬀerent forecasting methods.
4.3.1
Four Accuracy Measures
Let Yt denote the actual observation at time t, and ˆYt the forecast. The four accuracy
measures used are
• Mean Absolute Percentage Error (MAPE)
MAPE = 1
T
T
X
t=1

Yt −ˆYt
Yt
 × 100
(4.12)
• symmetric Mean Absolute Percentage Error (sMAPE)
sMAPE = 1
T
T
X
t=1

Yt −ˆYt
(Yt + ˆYt)/2
 × 100
(4.13)
• Median Absolute Percentage Error (MdAPE)
MdAPE = median{

Yt −ˆYt
Yt
 , t = 1, 2 · · · , T} × 100
(4.14)
• Root Mean Square Error (RMSE)
RMSE =
"
1
T
T
X
t=1
(Yt −ˆYt)2
#1/2
(4.15)
Among them, MAPE, sMAPE, and MdAPE are unit free while RMSE has the same
unit as Yt.
4.3.2
Two Examples
Example 1. UK Annual Consumption of Spirits. The data set contains 69
observations on the logarithms of three variables, consumption of spirits per capita
(Yt), real income per capita (z1,t), and relative price of spirits (z2,t) (deﬂated by cost-
of-living index), from 1870 to 1938 in the United Kingdom. Figure 4.2 shows plots
102

of these three series.
This data have frequently appeared in literature since ﬁrst
studied by Prest (1949) and are often analyzed by ﬁtting a linear regression model
with a deterministic linear or quadratic time trend and a ﬁrst-order autoregressive
error (Fuller 1996).
The consumptions of spirits were forecasted using the following ﬁve methods:
1). ESCov1 – ESCov having two covariates, income and price
2). ESCov2 – ESCov having one covariate, income
3). ESCov3 – ESCov having one covariate, price
4). Holt’s method
5). A linear regression model with a deterministic linear time trend and an AR(1)
error
Yt = β0 + β1z1,t + β2z2,t + β3z3,t + Ut,
(4.16a)
Ut = ϕUt−1 + ǫt,
(4.16b)
where z3,t = (t −1869)/100, where t denote the year.
For ESCov1, ESCov2, and ESCov3, Holt’s method was chosen to estimate the inter-
cept µt. The regression model with an AR(1) error was ﬁtted using the SAS procedure
AUTOREG.
The spirit data set was divided into two portions. The 59 observations from 1870
to 1928 formed the training data for parameter estimation, and forecasts for the
following 10 years, 1929–1938, were generated. Figure 4.3 shows forecasts by those
ﬁve methods. All of them performed well for short-term forecasts, say h < 4. For
long-term forecasts, ESCov’s did better than Holt’s method and the regression model
did. Holt’s method tended to under-predict spirit consumptions while the regression
model tended to overshot the data.
103

1870
1880
1890
1900
1910
1920
1930
1940
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
2.1
Year
log(Consumption of Spirits)
1870
1880
1890
1900
1910
1920
1930
1940
1.75
1.8
1.85
1.9
1.95
2
2.05
2.1
2.15
Year
log(Income)
1870
1880
1890
1900
1910
1920
1930
1940
1.8
1.9
2
2.1
2.2
2.3
2.4
2.5
2.6
Year
log(Price)
Figure 4.2: UK Spirit Consumptions per Capita, Income per Capita and Price of
Spirits from 1870 to 1938.
104

1928
1930
1932
1934
1936
1938
1.15
1.2
1.25
1.3
1.35
1.4
Year
log(Consumption of Spirits)
Actual Consumptions
ESCov1
ESCov2
ESCov3
Holt’s Method
Regression with Linear Trend
Figure 4.3: UK Consumptions of Spirits per Capita and Forecasts for 1929–1938
For a better comparison among ESCov’s and Holt’s method (notice that ESCov
reduces to Holt’s method when there are no covariates and the intercept µt is esti-
mated and forecasted using Holt’s method), the following procedure was employed.
First, the 50 observations from 1870 to 1919 were used as the training data to esti-
mate the parameters, and the forecasts of spirit consumptions were generated for the
following ten years from 1920 to 1929. Then, the next observation (i.e., the observa-
tion in 1920) was added to the training data, the parameters were re-estimated, and
new ten-year forecasts were generated. Repeating this procedure until all but the last
ten observations were included in the training data gave 10 h-step-ahead forecasts
for h = 1, 2, · · · , 10. Last, the 10 forecasts for each h were used to calculate MAPE,
sMAPE, MdAPE and RMSE.
Table 4.1 shows that ESCov2 and ESCov3 produced more accurate forecasts than
ESCov1 and Holt’s method did, having smaller MAPE, sMAPE and RMSE for all
10 horizons (i.e., h = 1, 2, · · · , 10) and smaller MdAPE for 8 horizons (i.e., h =
105

1, 2, 3, 4, 5, 7, 8, 10). For h = 6 and h = 9, Holt’s method gave the smallest MdAPE
as shown by numbers in bold face. In general, ESCov1 had the poorest performance
in terms of MAPE, sMAPE, MdAPE and RMSE, which implies that including more
covariates do not necessarily leads to more accurate forecasts. In addition, it can be
seen from Figure 4.3 and Table 4.1 that forecast accuracy, as expected, decreases as
the forecast horizon h increases.
Example 2.
US Annual Motor Vehicle Deaths.
The data set includes 60
observations from 1911 to 1970 on the number of deaths (in thousand) due to motor
vehicle accidents and the number of miles (in billion) driven in the United States. As
mentioned at the beginning of this chapter, many factors may contribute to motor
vehicle deaths. However, not all of them are available or quantiﬁable. In this example,
only annual miles driven is included in ESCov as the covariate.
Using the ﬁrst 45 observations from 1911 to 1955 as the training data, we generated
forecasts for the last 15 years using the follow methods
1). ESCov. Holt’s method was chosen to estimate the intercept µt, and two sets of
forecasts of motor vehicle deaths from 1956 to 1970 were obtained. One used
the real values of annual miles driven from 1956 to 1970 while the other used
the predicted annual miles driven for 1956–1970 by damped Holt’s method.
2). Holt’s method
3). Regression with an AR(1) error. Two linear regression models with an AR(1)
error
Ut = ϕUt−1 + ǫt
(4.17)
were ﬁtted. One assumed a linear time trend
Yt = β0 + β1zt + β2t + Ut
(4.18)
106

Table 4.1: UK per capita Consumption of Spirits Forecasts for 1925-1938
MAPE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov1
1.3366
2.3454
2.8955
3.3398
4.0707
4.9602
5.9875
6.5014
7.1343
7.7698
ESCov2
1.1786
1.6593
1.9935
2.1152
2.2478
2.7187
3.0760
3.6116
4.1992
5.1664
ESCov3
0.8001
1.2755
1.5371
2.0394
2.3576
2.8622
3.3288
3.7000
3.9710
4.0496
Holt’s Method
1.4081
2.1857
2.5879
2.9458
3.1254
2.9621
3.5663
3.8150
4.3982
5.6591
sMAPE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov1
0.3334
0.5851
0.7186
0.8207
0.9946
1.2047
1.4494
1.5644
1.7085
1.8515
ESCov2
0.2935
0.4105
0.4905
0.5189
0.5507
0.6650
0.7498
0.8841
1.0361
1.2808
ESCov3
0.1984
0.3149
0.3797
0.5031
0.5800
0.7019
0.8133
0.9014
0.9665
0.9821
Holt’s Method
0.3495
0.5374
0.6327
0.7196
0.7662
0.7234
0.8712
0.9377
1.0929
1.4189
MdAPE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov1
0.7084
2.5208
3.0522
4.0524
4.3166
5.4960
6.5742
7.4992
9.1791
9.6750
ESCov2
1.1547
1.5742
1.1114
0.9450
1.2239
2.1482
1.7648
2.8174
3.9334
5.5346
ESCov3
0.6264
0.7651
1.2669
1.5396
1.9491
2.6952
2.7439
3.2357
3.7863
4.3639
Holt’s Method
1.3229
1.6874
1.2853
1.8597
2.4089
1.1431
2.2659
2.8573
3.4226
6.1256
RMSE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov1
0.0250
0.0412
0.0488
0.0531
0.0640
0.0753
0.0849
0.0931
0.1020
0.1112
ESCov2
0.0199
0.0324
0.0390
0.0418
0.0439
0.0512
0.0567
0.0602
0.0669
0.0777
ESCov3
0.0190
0.0279
0.0287
0.0333
0.0384
0.0458
0.0533
0.0593
0.0628
0.0664
Holt’s Method
0.0275
0.0465
0.0569
0.0598
0.0593
0.0640
0.0688
0.0691
0.0757
0.0889
107

while the other assumed a quadratic time trend
Yt = β0 + β1zt + β2t + β3(t −30)2 + Ut,
(4.19)
where zt denotes the annual miles driven, and 1910 is the origin for t. The
regression models were ﬁtted using the SAS procedure AUTOREG, and the
real values of annual miles driven from 1956 to 1970 were used for forecasting.
Figure 4.4 shows that the forecasts from ESCov’s were closer to the actual deaths
than these from Holt’s method and the two regression models. In average, ESCov with
real miles outperformed all the other methods. The poor performance of ESCov with
predicted miles for large forecast horizon was due to the under-predictions of miles by
damped Holt’s method (see Figure 4.5). The two regression models performed worst.
Although the regression model with a quadratic time trend performed well for small
forecast horizons, say h < 5, its performance worsened quickly as the forecast horizon
increased.
Following the same procedure as described in Example 1, we obtained 10 forecasts
for each of 10 horizons using ESCov and Holt’s method. Table 4.2 shows that, ESCov
with real miles had the smallest MAPE, sMAPE and RMSE except when h = 3, for
which ESCov with predicted miles gave smaller MAPE, sMAPE and RMSE. ESCov
with predicted miles outperformed Holt’s method except when the forecast horizon
was large, say h > 7. This was as mentioned before due to the under-predictions of
miles by damped Holt’s method for large horizons.
4.4
Statistical Properties
In this section, we investigate the statistical properties of ESCov. We identify SSOE
state space models underlying ESCov, discuss the maximum likelihood estimation of
unknown parameters α and δ, propose a model section procedure for ESCov, and
derive the variances of h-step-ahead forecasts by ESCov.
108

1955
1960
1965
1970
30
35
40
45
50
55
60
65
70
75
80
Year
Motor Vehicle Deaths (Thousand)
Actual Deaths
ESCov with Real Miles
ESCov with Predicted Miles
Holt’s Method
Regression with Linear Trend
Regression with Quadratic Trend
Figure 4.4: US Motor Vehicle Deaths and Forecasts for 1956–1970
1955
1960
1965
1970
600
700
800
900
1000
1100
1200
Year
Vehicle Miles of Travel (Billion)
Actual Miles
Predicted Miles
Figure 4.5: US Vehicle Miles of Travel and Forecasts for 1956–1970
109

Table 4.2: US Motor Vehicle Death Forecasts for 1951-1970
MAPE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov with Real Miles
2.4227
4.7562
6.3842
7.4074
8.1681
9.3893
10.1389
11.0981
10.9241
9.5654
ESCov with Predicted Miles
3.3353
4.7651
5.9768
7.4242
9.9475
11.7139
12.1721
14.3293
16.3201
17.8039
Holt’s Method
3.8679
6.3261
7.6475
9.2213
11.3274
13.4502
14.0270
13.4996
12.8357
11.5241
sMAPE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov with Real Miles
0.5962
1.1640
1.5678
1.8439
2.0335
2.3262
2.4889
2.7140
2.6685
2.3338
ESCov with Predicted Miles
0.8300
1.1895
1.5039
1.9168
2.6090
3.1578
3.3724
4.0267
4.6544
5.1365
Holt’s Method
0.9434
1.5221
1.8219
2.2119
2.7026
3.2220
3.3608
3.2547
3.1115
2.8071
MdAPE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov with Real Miles
1.6667
4.7636
6.6080
6.9327
9.1203
12.3475
11.1495
11.9346
8.5822
8.7934
ESCov with Predicted Miles
2.2624
3.2958
5.8991
6.0159
7.7923
7.9660
7.5885
11.5464
15.6337
17.7483
Holt’s Method
3.0543
5.1177
6.9275
9.1603
9.9504
13.9313
14.1878
12.9546
13.2113
12.5358
RMSE
Horizon (h)
1
2
3
4
5
6
7
8
9
10
ESCov with Real Miles
1.0967
2.0165
2.6554
3.2032
3.7492
4.6116
5.1975
5.4751
5.3339
4.8728
ESCov with Predicted Miles
1.5487
2.2438
2.6301
3.7923
5.1597
6.6409
7.6883
9.0695
10.3543
11.3020
Holt’s Method
1.7269
2.6322
3.2238
3.8475
4.6996
5.6325
6.0633
6.3517
6.4796
6.0960
110

4.4.1
Underlying Statistical Models
As discussed in Chapter 2, ES methods have SSOE state space models as their un-
derlying statistical models. Similarly, an SSOE state space model underlying ESCov
can also be identiﬁed. Such an SSOE state space model consists of an observation
equation
Yt = f(βt−1) + δTzt + ǫt
(4.20)
and a transition equation
βt = g(βt−1) + w(α, βt−1)ǫt,
(4.21)
where βt is a p × 1 state vector, denoting the true value of bt in equations (4.9) and
(4.10).
According to the observation equation (4.20), the one-step-ahead forecast made
at time t −1, given that βt−1, δ, and zt are known, is
ˆYt|t−1 = f(βt−1) + δTzt,
(4.22)
and the corresponding one-step-ahead forecast error is
et = Yt −ˆYt|t−1 = ǫt.
(4.23)
As a result, the transition equation (4.21) is the same as the recurrence equation
(4.9) except that the vector of smoothed statistics, bt, in (4.9) is replaced by its true
value, βt, in (4.21). In another words, given the true initial value β0 and the true
values of parameters δ and α (and φ), ESCov will produce optimal (i.e., minimum
mean squared error) forecasts for a time series generated from the model in (4.20)
and (4.21).
Table 4.3 lists SSOE state space models that underpin ESCov with diﬀerent ES
methods chosen to estimate and forecast the intercept µt. All those SSOE models are
homoscedastic if ǫt in the observation equation is assumed to be a white noise process
111

with zero mean (i.e., E[ǫt] = 0) and constant variance (i.e., E[ǫ2
t] = σ2). Replacing ǫt
by u(βt−1, zt)ǫt, where u is a mapping from ℜp+q to ℜ, and assuming that
E[ǫtβk] = 0, for k < t
(4.24)
and
E[ǫtzk] = 0, for all k
(4.25)
give heteroscedastic underlying SSOE state space models for ESCov.
4.4.2
Maximum Likelihood Estimation
Knowing underlying statistical models for ESCov makes the maximum likelihood
estimation of unknown parameters possible. Given the data {Yt, zt; t = 1, 2, · · · , T},
we hope to obtain the maximum likelihood estimators of unknown parameters α and
δ. The SSOE model in (4.20) and (4.21) implies that the behavior of the time series
of interest depends on not only parameters α and δ but also the initial state βo. Ord
et al. (1997) proposed a conditional maximum likelihood approach that bases the
parameter estimation upon the likelihood conditional on the initial state β0.
Let p(·) denote a probability function. The likelihood function conditional on β0,
given the data {Yt, zt; t = 1, 2, · · · , T}, is
L(α, δ, σ2|β0) = p(Y1, · · · , YT; α, δ, σ2|β0).
(4.26)
The probability law, P(AB) = P(A|B)P(B), implies that
L(α, δ, σ2|β0) =
TY
t=1
p(Yt; α, δ, σ2|Y1, · · · , Yt−1, β0).
(4.27)
Under the assumption that the underlying SSOE state space model is homoscedas-
tic (i.e., u(βt−1, zt) = 1) and the white noise process ǫt is Gaussian, the conditional
likelihood function becomes
L(α, δ, σ2|β0) = (2πσ2)−T/2 · exp(−1
2σ2
T
X
t=1
ǫ2
t).
(4.28)
112

Table 4.3: Underlying SSOE State Space Models for ESCov. (N - None, A - Additive, M - Multiplicative, DA - Damped
Additive)
Seasonality
Trend
N
A
M
N
Yt = µt−1 + δTzt + ǫt
Yt = µt−1 + st−M + δTzt + ǫt
Yt = µt−1st−M + δTzt + ǫt
µt = µt−1 + α1ǫt
µt = µt−1 + α1ǫt
µt = µt−1 + α1ǫt/st−M
st = st−M + α3ǫt
st = st−M + α3ǫt/µt−1
A
Yt = µt−1 + βt−1 + δTzt + ǫt
Yt = µt−1 + βt−1 + st−M + δTzt + ǫt
Yt = (µt−1 + βt−1)st−M + δTzt + ǫt
µt = µt−1 + βt−1 + α1ǫt
µt = µt−1 + βt−1 + α1ǫt
µt = µt−1 + βt−1 + α1ǫt/st−M
βt = βt−1 + α2ǫt
βt = βt−1 + α2ǫt
βt = βt−1 + α2ǫt/st−M
st = st−M + α3ǫt
st = st−M + α3ǫt/(µt−1 + βt−1)
DA
Yt = µt−1 + φβt−1 + δTzt + ǫt
Yt = µt−1 + φβt−1 + st−M + δTzt + ǫt
Yt = (µt−1 + φβt−1)st−M + δTzt + ǫt
µt = µt−1 + φβt−1 + α1ǫt
µt = µt−1 + φβt−1 + α1ǫt
µt = µt−1 + φβt−1 + α1ǫt/st−M
βt = φβt−1 + α2ǫt
βt = φβt−1 + α2ǫt
βt = φβt−1 + α2ǫt/st−M
st = st−M + α3ǫt
st = st−M + α3ǫt/(µt−1 + φβt−1)
113

Therefore, the conditional maximum likelihood estimators of α and δ, when the
white noise process is Gaussian, are equivalent to their minimum sum of squared
one-step-ahead forecast error estimators (see equation (4.11)).
4.4.3
Model Selection
Model selection for ESCov addresses two questions: which covariates should be in-
corporated into ESCov, and which ES method should be used for the estimation of
the time-varying intercept µt.
There are two general approaches to model selection, hypothesis testing and cri-
terion optimization. The hypothesis tests are often the choice in econometric model
building and can be set up in two opposite ways, leading to two types of tests, unit
root tests and stationary tests. Unit root tests, formulated in an autoregressive frame-
work, have the null hypothesis that a time series has a unit root while the alternative
that the time series is stationary. On the contrary, stationary tests, based on models
in state space form, have the null hypothesis that a component (e.g., level, trend,
or seasonality) is deterministic while the alternative that it is stochastic and non-
stationary. A detailed review on unit root and stationary tests was given by Harvey
(2005). All of existing stationary tests are for linear MSOE state space models. The
generalization to linear SSOE state space models is possible while the generalization
to SSOE models for multiplicative trend or seasonality is another story.
Diﬀerent from hypothesis testing, the criterion-based approach selects from po-
tential candidate models the one that minimizes the selected criterion. Commonly
used criteria include Akaike’s information criterion (AIC) (Akaike 1973), a modiﬁed
AIC (Hurvich and Tsai 1989), Bayesian information criterion (BIC) (Schwarz 1973),
HQ (Hannan and Quinn 1979), to name a few. With so many criteria available, which
one to use is not a surprising question that is diﬃcult to answer as no criterion is
114

universally superior to all the others. In the context of exponential smoothing, a com-
parison of six diﬀerent criteria based on simulated and real-life time series by Billah
et al. (2006) revealed that AIC was the best for selecting from three ES methods,
N-N, A-N, and A-A.
Model selection for ESCov, which has underlying models in SSOE state space form,
could use either approach. As existing hypothesis tests are for only linear MSOE state
space models, the criterion-based approach is preferred since it works for any types of
models as long as the selected criterion are calculable. We propose a model selection
procedure that consists of two steps: preliminary analysis and criterion-based auto
selection. In the preliminary analysis, domain knowledge and time series plots are
used to select potential covariates and ES methods, based on which possible candidate
models are formulated and used as the input to the criterion-based auto selection,
which chooses from those candidates the one that minimize the selected criterion.
• Preliminary analysis
Preliminary analysis identiﬁes potential covariates and possible ES methods based
on domain knowledge and time series plots. The importance of domain knowledge in
covariate selection is quite obvious. For example, the price of a product undoubtedly
has an inﬂuence on its sale and should be included in ESCov as a covariate. Time
series plots also plays an important role in model selection. Comparing the plot of a
covariate with that of the dependent variable may indicate which of the movements in
the dependent variable are capable of being explained by the covaraite. For example,
Figure 4.1 shows that the death series and the mile series exhibit similar movements.
Both series increased globally from 1910 to 1970 but experienced a temporary, abrupt
drop in 1940. In addition, time series plots can also give hints which ES method to
choose. For example, Figure 4.1(a) implies that the death series has a trend but no
seasonality, which suggests the use of Holt’s method or damped Holt’s method.
115

• Criterion-Based Auto selection
The potential covariates and possible ES methods identiﬁed in the preliminary anal-
ysis may lead to several candidate models. The next step is to select from those
candidates by optimizing certain criterion. We uses Akaike’s information criterion
(AIC)
AIC = −2 log L(α, δ, σ2|β0) + 2K,
(4.29)
where log L(α, δ, σ2|β0) is the logarithm of the conditional likelihood, and K is the
number of model parameters. With a Gaussian white noise process, AIC becomes
AIC = T log (2π) + T log (σ2) + 1
σ2
T
X
t=1
ǫ2
t + 2K,
(4.30)
Among those candidate models, the one with minimum AIC is selected.
4.4.4
Prediction Intervals
Up to this point, predictions for future values are given as single numbers, referred
to as ”point forecasts,”, which alone are often not adequate as they tell nothing
about prediction uncertainties. To show how uncertain a prediction is, one should
supplement a point forecast with an interval of the form
point forecast ± w ·
p
varaince of the point forecast
(4.31)
where w depends on the distribution of the point forecast as well as a pre-speciﬁed
probability that the expected future value will fall into this interval. Such an interval
is referred to as ”prediction interval,” whose construction clearly requires the variance
of the point forecast.
The variances of h-step-ahead forecasts made at time t by ESCov can be derived
analytically from underlying SSOE state space models, among which we consider only
four types: linear models with additive error, nonlinear models with additive error,
linear models with multiplicative error, and nonlinear models with multiplicative
116

Table 4.4: Four Classes of SSOE State Space Models Underlying ESCov
Class
Underlying Model
Error
ES Methods for ESCov
1
Linear
Additive
N-N, A-N, DA-N, N-A, A-A, DA-A
2
Nonlinear
Additive
N-M, A-M, DA-M
3
Linear
Multiplicative
N-N, A-N, DA-N, N-A, A-A, DA-A
4
Nonlinear
Multiplicative
N-M, A-M, DA-M
error (see Table 4.4). Along with the variances, the analytic formulas for the means
of h-step-ahead forecasts are also given.
• Class 1 – Linear SSOE model with additive error
Models in class 1 are of the form
Yt = xTβt−1 + δTzt + ǫt,
(4.32a)
βt = Gβt−1 + αǫt.
(4.32b)
where G is a p × p constant matrix, x is a p × 1 vector, and ǫt is serially independent
with E[ǫt] = 0 and E[ǫ2
t] = σ2
t . This class contains six SSOE models in Table 4.3.
They are models for N-N, A-N, DA-N, N-A, A-A, and DA-A. For each model in class
1, the p, βt, α, x, and G are given in Table 4.5.
For h ≥1, the conditional mean and variance of Yt+h given βt and zt+h are derived
in Appendix A:
E[Yt+h|βt, zt+h]
=
xTGh−1βt + δTzt+h,
(4.33a)
V (Yt+h|βt, zt+h)
=





σ2
t+1
h = 1
Ph−2
i=0 (xTGiα)2 · σ2
t+h−i + σ2
t+h
h > 1
(4.33b)
The speciﬁc results for each of the six models in class 1 are given in Table 4.6. Notice
that the conditional mean for any model is, as one would expect, the same as the point
forecast from the corresponding ESCov. For example, E[Yt+h|βt, zt+h] = µt + δTzt+h
for the model for N-N while ˆYt+h|t = lt+dTzt+h for ESCov with the intercept estimated
by N-N (i.e., SES).
117

Table 4.5: Models in Class 1. For k ≥0, 0k is a k × 1 vector of zeros, and Ik is a k × k identity matrix.
Seasonality
Trend
N
A
N
p = 1, x = 1, βt = µt
p = M + 1, x = (1, 0T
M−1, 1)T, βt = (µt, st, · · · , st−M+1)T
G = 1, α = α1
G =


1
0T
M−1
0
0
0T
M−1
1
0M−1
IM−1
0M−1

,
α =


α1
α3
0T
M−1


A
p = 2, , x = (1, 1)T, βt = (µt, βt)T
p = M + 2, x = (1, 1, 0T
M−1, 1)T, βt = (µt, βt, st, · · · , st−M+1)T
G =
 1
1
0
1

,
α =
 α1
α2

G =


1
1
0T
M−1
0
0
1
0T
M−1
0
0
0
0T
M−1
1
0M−1
0M−1
IM−1
0M−1

,
α =


α1
α2
α3
0T
M−1


DA
p = 2, x = (1, φ)T, βt = (µt, βt)T
p = M + 2, x = (1, φ, 0T
M−1, 1)T, βt = (µt, βt, st, · · · , st−M+1)T
G =
 1
φ
0
φ

,
α =
 α1
α2

G =


1
φ
0T
M−1
0
0
φ
0T
M−1
0
0
0
0T
M−1
1
0M−1
0M−1
IM−1
0M−1

,
α =


α1
α2
α3
0T
M−1


118

Table 4.6: Class 1 – Conditional Mean and Variance of Yt+h Given βt and zt+h
For h ≥1,
E[Yt+h|βt, zt+h] = mh + δTzt+h,
V (Yt+h|βt, zt+h) =
 σ2
t+1,
h = 1
vh + σ2
t+h,
h > 1
h∗= h(mod M),
Ii,M−1 =
 1,
i(mod M) = M −1
0,
otherwise
mh
Seasonality
Trend
N
A
N
µt
µt + st−M+h∗
A
µt + βth
µt + hβt + st−M+h∗
DA
µt + βt
Ph
i=1 φi
µt + hβt + st−M+h∗
vh
Seasonality
Trend
N
A
N
Ph−2
i=0 α2
1σ2
t+h−i
Ph−2
i=0 [α1 + α3 · Ii,M−1]2σ2
t+h−i
A
Ph−2
i=0 [α1 + α2 · (i + 1)]2 σ2
t+h−i
Ph−2
i=0 [α1 + α2 · (i + 1) + α3 · Ii,M−1]2 σ2
t+h−i
DA
Ph−2
i=0
h
α1 + α2 · Pi+1
j=1 φji2
σ2
t+h−i
Ph−2
i=0
h
α1 + α2 · Pi+1
j=1 φj + α3 · Ii,M−1
i2
σ2
t+h−i
119

• Class 2 – Nonlinear SSOE model with additive error
Let β∗
t be the vector of all of nonseasonal components in the state vector βt. That
is, βt = (β∗T
t , st, · · · , st−M+1)T. Models in class 2 are of the form
Yt = x∗Tβ∗
t−1 · st−M + δTzt + ǫt,
(4.34a)
β∗
t = G∗β∗
t−1 + α∗ǫt/st−M,
(4.34b)
st = st−M + α3ǫt/(x∗Tβ∗
t−1).
(4.34c)
Three models in Table 4.3 belong to class 2. They are models for N-M, A-M, and
DA-M. Furthermore, x∗, β∗
t , G∗, and α∗in model (4.34) for N-M, A-M, and DA-M
are the same as x, βt, G, and α in model (4.32) for N-N, A-N, and DA-N respectively.
Derived in Appendix B, the conditional mean and variance of Yt+h given βt and
zt+h for 1 ≤h ≤M are
E[Yt+h|βt, zt+h] = x∗T(G∗)h−1β∗
t · st+h−M + δTzt+h,
(4.35a)
V (Yt+h|βt, zt+h)
=





σ2
t+1,
h = 1
Ph−2
i=0 (x∗T(G∗)iα∗)2 · σ2
t+h−i · s2
t+h−M/s2
t+h−M−i + σ2
t+h,
h > 1
(4.35b)
The speciﬁc results for the three models in class 2 are given in Table 4.7.
When h > M, the derivations of the conditional mean and variance of Yt+h give
βt involve deriving the mean and variance of the ratio of two random variables. They
often do not exist. Therefore, only 1 ≤h ≤M are considered here.
• Class 3 – Linear SSOE model with multiplicative error
Replacing ǫt in model (4.32) by u(βt−1)ǫt gives a model of the form
Yt = xTβt−1 + δTzt + u(βt−1)ǫt,
(4.36a)
βt = Gβt−1 + α · u(βt−1)ǫt.
(4.36b)
where u(βt) is a mapping linear in βt. As a result, the SSOE model is linear. In
other words, an SSOE model is said to be linear as long as it is linear in βt and δ.
120

Table 4.7: Class 2 – Conditional Mean and Variance of Yt+h Given βt and zt+h
For 1 ≤h ≤M,
E[Yt+h|βt, zt+h] = mh + δTzt+h,
V (Yt+h|βt, zt+h) =



σ2
t+1,
h = 1
vh + σ2
t+h,
h > 1
Model
mh
vh
N-M
µt · st+h−M
Ph−2
i=0 α2
1 · σ2
t+h−i · s2
t+h−M/s2
t+h−M−i
A-M
(µt + hβt) · st+h−M
Ph−2
i=0 [α1 + α2(i + 1)]2 · σ2
t+h−i · s2
t+h−M/s2
t+h−M−i
DA-M
(µt + βt
Ph
i=1 φi) · st+h−M
Ph−2
i=0 [α1 + α2
Pi+1
j=1 φj]2 · σ2
t+h−i · s2
t+h−M/s2
t+h−M−i
The mapping u(βt) could be any linear function of βt, and we consider only the case
where u(βt) = xTβt. Similar to class 1, class 3 also consists of six models for N-N,
A-N, DA-N, N-A, A-A, and DA-A respectively. The diﬀerence is that now the error
depends on βt.
Derived in Appendix C, the conditional mean and variance of Yt+h given βt and
zt+h for h ≥1 are
E[Yt+h|βt, zt+h] = xTGh−1βt + δTzt+h,
(4.37a)
V (Yt+h|βt, zt+h) = (1 + σ2
t+h)xTVh−1x + σ2
t+h(xTGh−1βt)2,
(4.37b)
where
Vh = GVh−1GT + σ2
t+hαxTVh−1xαT + σ2
t+h(xTGh−1βt)2ααT,
(4.38a)
V0 = O,
(4.38b)
and O is a square matrix of zeros. The conditional means for models in class 3 are
the same as those for the corresponding models in class 1. However, the condition
variances for class 3 depend on βt while the conditional variances for class 1 do not.
This implies that corresponding models from class 1 and class 3 produce same point
forecasts while diﬀerent prediction intervals.
121

The simplest case in class 3 is the model for N-N. For this model,
x = 1,
βt = µt,
G = 1,
α = α1.
(4.39)
With an additional assumption that ǫt has a constant variance, namely E[ǫ2
t] = σ2,
we have
Vh
=
(1 + α2
1σ2
t+h)Vh−1 + α2
1σ2
t+hµ2
t
=
α2
1σ2
h−1
X
i=0
(1 + α2
1σ2)iµ2
t = (1 + α2
1σ2)hµ2
t −µ2
t.
(4.40)
Therefore,
E[Yt+h|βt, zt+h] = µt + δTzt+h,
(4.41a)
V (Yt+h|βt, zt+h) = (1 + σ2)(1 + α2
1σ2)h−1µ2
t −µ2
t.
(4.41b)
While the model for N-N in class 1 has
E[Yt+h|βt, zt+h] = µt + δTzt+h,
(4.42a)
V (Yt+h|βt, zt+h) = (1 + (h −1)α2
1)σ2.
(4.42b)
That is, the model for N-N in class 3 has the same condition mean as but diﬀerent
conditional variance than the model for N-N in class 1.
• Class 4 – Nonlinear SSOE model with multiplicative error
Replacing ǫt in model (4.34) by u(β∗
t−1, st−M)ǫt yields a model of the form
Yt = x∗Tβ∗
t−1 · st−M + δTzt + u(β∗
t−1, st−M)ǫt,
(4.43a)
β∗
t = G∗β∗
t−1 + α∗· u(β∗
t−1, st−M)ǫt/st−M,
(4.43b)
st = st−M + α3 · u(β∗
t−1, st−M)ǫt/(x∗Tβ∗
t−1).
(4.43c)
where u(β∗
t−1, st−M) could be any mapping to ℜ. We consider only the case where
u(β∗
t−1, st−M) = x∗Tβ∗
t−1 · st−M.
(4.44)
122

This leads to a simpler form
Yt = x∗Tβ∗
t−1 · st−M + δTzt + x∗Tβ∗
t−1 · st−M · ǫt,
(4.45a)
β∗
t = G∗β∗
t−1 + α∗· x∗Tβ∗
t−1 · ǫt,
(4.45b)
st = st−M + α3 · st−M · ǫt.
(4.45c)
Class 4 contains three models that underpin ESCov with the intercept estimated by
N-M, A-M, and DA-M respectively.
Derived in Appendix D, the condition mean and variance of Yt+h given βt and
zt+h for 1 ≤h ≤M are
E[Yt+h|βt, zt+h] = x∗T(G∗)h−1β∗
t · st+h−M + δTzt+h,
(4.46a)
V (Yt+h|βt, zt+h) = s2
t+h−M

(1 + σ2
t+h)x∗TV∗
h−1x∗+ σ2
t+h(x∗T(G∗)h−1β∗
t )2
, (4.46b)
where
V∗
h = G∗V∗
h−1G∗T + σ2
t+hα∗x∗TV∗
h−1x∗α∗T + σ2
t+h(x∗T(G∗)h−1β∗
t )2α∗α∗T,
V∗
0 = O.
(4.47)
Comparing the results with those for class 2 shows that models for class 4 produce
same point forecasts as but diﬀerent prediction intervals from the corresponding mod-
els in class 2.
The simplest case in class 4 is the model for N-M. For this model,
x∗= 1,
β∗
t = µt,
G∗= 1,
α∗= α1,
(4.48)
which are the same as x, βt, G, and α in the model underlying N-N in class 3. With
the assumption that ǫt has a constant variance, namely E[ǫ2
t] = σ2, we have, for
1 ≤h ≤M,
E[Yt+h|βt, zt+h] = µt · st+h−M + δTzt+h,
(4.49a)
V (Yt+h|βt, zt+h) = s2
t+h−M · [(1 + σ2)(1 + α2
1σ2)h−1µ2
t −µ2
t].
(4.49b)
123

While the model for N-M in class 2 has
E[Yt+h|βt, zt+h] = µt · st+h−M + δTzt+h,
(4.50a)
V (Yt+h|βt, zt+h) = s2
t+h−M · α2
1 · σ2 ·
h−2
X
i=0
1
s2
t+h−M−i
.
(4.50b)
That is, the two models for N-M have same conditional means but diﬀerent conditional
variances.
4.5
Related Statistical Model
Comparing SSOE state space models underlying ESCov in Table 4.3 with those un-
derlying ES methods (see Table 2.4 in Chapter 2) reveals that the former is diﬀerent
from the latter in that an extra term, δTzt, is introduced into the observation equa-
tion. In Chapter 2, we showed that SSOE models underlying additive ES methods
can be reduced to an ARIMA form. In a similar fashion, it can be showed that SSOE
models underlying additive ESCov’s also have a reduced form, and moreover the re-
duced form is a transfer function model, an extension of the ARIMA model (Box,
Jenkins and Reinse 1994).
For example, the SSOE model underlying SES has the transition equation
µt = µt−1 + α1ǫt,
(4.51)
which can be rewritten as
(1 −B)µt = α1ǫt.
(4.52)
Left multiplying the observation equation by 1 −B and substituting equation (4.52)
into the result gives
(1 −B)Yt = δT(1 −B)zt + ǫt + (α1 −1)ǫt−1.
(4.53)
This is a transfer function model with input zt and output Yt. Table 4.8 lists the
reduced form transfer function models of SSOE state space models that underpin
additive ESCov’s.
124

Table 4.8: Reduced Form Transfer Function Models of SSOE State Space Underlying Additive ESCov, (N - None, A - Additive,
DA - Damped Additive
Seasonality
Trend
N
A
N
(1 −B)Yt = δT(1 −B)zt + (1 + θ1B)ǫt
(1 −BM)Yt = δT(1 −BM)zt + (1 + PM
i=1 θiBi)ǫt
θ1 = α1 −1
θi = α1,
i = 1, · · · , M −1
θM = α1 + α3 −1
A
(1 −B)2Yt
(1 −B)(1 −BM)Yt
= δT(1 −B)2zt + (1 + θ1B + θ2B2)ǫt
= δT(1 −B)(1 −BM)zt + (1 + PM+1
i=1 θiBi)ǫt
θ1 = α1 + α2 −2
θ1 = α1 + α2 −1
θ2 = 1 −α1
θi = α2,
i = 2, · · · , M −1
θM = α2 + α3 −1
θM+1 = 1 −α1 −α3
DA
(1 −φB)(1 −B)Yt
(1 −φB)(1 −BM)Yt
= δT(1 −φB)(1 −B)zt + (1 + θ1B + θ2B2)ǫt
= δT(1 −φB)(1 −BM)zt + (1 + PM+1
i=1 θiBi)ǫt
θ1 = α1 + φα2 −φ −1
θ1 = α1 + φα2 −φ
θ2 = φ(1 −α1)
θi = (1 −φ)α1 + φα2,
i = 2, · · · , M −1
θM = (1 −φ)α1 + φα2 + α3 −1
θM+1 = φ(1 −α1) −φα3
125

4.6
Appendix
A. Derivation of E[Yt+h|βt, zt+h] and V (Yt+h|βt, zt+h) for Models in Class 1
From equation (4.32b), we have
βt+h = Gβt+h−1 + αǫt+h = Ghβt +
h−1
X
i=0
Giαǫt+h−i,
(4.54)
which leads to the conditional mean and variance of βt+h given βt and zt+h
E[βt+h|βt] = Ghβt,
(4.55a)
V (βt+h|βt) =
h−1
X
i=0
GiααT(Gi)Tσ2
t+h−i.
(4.55b)
Since, according to equation (4.32a),
Yt+h = xTβt+h−1 + δTzt+h + ǫt+h,
(4.56)
we have the conditional mean and variance of Yt+h given βt and zt+h as
E[Yt+h|βt, zt+h] = xTE[βt+h−1|βt] + δTzt+h = xTGh−1βt + δTzt+h
(4.57)
and
V (Yt+h|βt, zt+h)
=
xTV (βt+h−1|βt)x + σ2
t+h
=





σ2
t+1
h = 1
Ph−2
i=0 (xTGiα)2 · σ2
t+h−i + σ2
t+h
h > 1
(4.58)
To get the speciﬁc results for each of the six models in class 1, we start with
the model for DA-A as the other ﬁve can be considered as its special cases. The
model for DA-A has, as given in Table 4.5,
x = (1, φ, 0′
M−1, 1)T,
βt = (µt, βt, st, · · · , st−M+1)T,
G =


1
φ
0′
M−1
0
0
φ
0′
M−1
0
0
0
0′
M−1
1
0M−1
0M−1
IM−1
0M−1


,
α =


α1
α2
α3
0′
M−1


.
(4.59)
126

As
Gk =


1
Pk
i=1 φi
0′
M−k∗
0k∗
0
φk
0′
M−k∗
0k∗
0k∗
0k∗
0k∗× 0′
M−k∗
Ik∗
0M−k∗
0M−k∗
IM−k∗
0M−k∗× 0′
k∗


,
(4.60)
where k∗= k(mod M), we have
xTGk = (1,
k+1
X
i=1
φi, 0′
M−k∗−1, 1, 0′
k∗).
(4.61)
Therefore,
mh
=
xTGh−1βt = µt + βt
h
X
i=1
φi + st−M+h∗
(4.62a)
vh
=
h−2
X
i=0
(xTGiα)2 · σ2
t+h−i
=
h−2
X
i=0
"
α1 + α2 ·
i+1
X
j=1
φj + α3 · Ii,M−1
#2
σ2
t+h−i,
h > 1
(4.62b)
where Ii,M−1 = 1, if i∗= M −1; = 0, otherwise.
mh and vh for the other ﬁve models in class 1 can be derived from equations
(4.62a) and (4.62b) by letting
(1). φ = 0; st = 0, ∀t; and α3 = 0 for N-N
(2). φ = 1; st = 0, ∀t; and α3 = 0 for A-N
(3). st = 0, ∀t; and α3 = 0 for DA-N
(4). φ = 0 for N-A
(5). φ = 1 for A-A
B. Derivation of E[Yt+h|βt, zt+h] and V (Yt+h|βt, zt+h) for Models in Class 2
According to equation (4.34b),
β∗
t+h = G∗β∗
t+h−1 + α∗ǫt+h/st+h−M = (G∗)hβ∗
t +
h−1
X
i=0
(G∗)iα∗· ǫt+h−i/st+h−M−i.
(4.63)
127

Then, for 1 ≤h ≤M, the conditional mean and variance of β∗
t+h given βt and
zt+h are
E[β∗
t+h|βt] = (G∗)hβ∗
t ,
(4.64)
and
V (β∗
t+h|βt) =
h−1
X
i=0
(G∗)iα∗α∗T((G∗)i)T · σ2
t+h−i/s2
t+h−M−i.
(4.65)
According to equation (4.34a),
Yt+h = x∗Tβ∗
t+h−1 · st+h−M + δTzt+h + ǫt+h.
(4.66)
Hence, for 1 ≤h ≤M, the conditional mean and variance of Yt+h given βt and
zt+h are
E[Yt+h|βt, zt+h]
=
x∗TE[β∗
t+h−1|βt] · st+h−M + δTzt+h
=
x∗T(G∗)h−1β∗
t · st+h−M + δTzt+h,
(4.67)
and
V (Yt+h|βt, zt+h)
=
x∗TV (β∗
t+h−1|βt)x∗· s2
t+h−M + σ2
t+h
=





σ2
t+1,
h = 1
Ph−2
i=0 (x∗T(G∗)iα∗)2 · σ2
t+h−i · s2
t+h−M/s2
t+h−M−i + σ2
t+h,
h > 1
(4.68)
Similarly, to get the results for a particular model in class 2, we start with the
general one, the model for DA-M. For this model,
x∗=


1
φ

,
β∗
t =


µt
βt

,
G∗=


1
φ
0
φ

,
α∗=


α1
α2

.
(4.69)
Since
(G∗)k =


1
Pk
i=1 φi
0
φk

,
(4.70)
128

we have
x∗T(G∗)k = (1,
k+1
X
i=1
φi).
(4.71)
Therefore,
mh
=
x∗T(G∗)h−1β∗
t · st+h−M = (µt + βt
h
X
i=1
φi) · st+h−M,
(4.72a)
vh
=
h−2
X
i=0
(x∗TG∗iα∗)2 · σ2
t+h−i · s2
t+h−M/s2
t+h−M−i
=
h−2
X
i=0
"
α1 + α2 ·
i+1
X
j=1
φj
#2
· σ2
t+h−i · s2
t+h−M/s2
t+h−M−i,
h > 1(4.72b)
Models for N-M and DA-M are the special cases of the model for DA-M, and
the mh and vh for them can be derived from equations (4.72a) and (4.72b) by
letting φ = 0 for N-M and φ = 1 for A-M.
C. Derivation of E[Yt+h|βt, zt+h] and V (Yt+h|βt, zt+h) for Models in Class 3
Let Mh = E[βt+h|βt] and Vh = V (βt+h|βt). Then, M0 = βt and V0 = O,
where O is a square matrix of zeros. According to equation (4.36b),
βt+h = Gβt+h−1 + α · xTβt+h−1 · ǫt+h,
(4.73)
Therefore, the conditional mean and variance of βt+h given βt and zt+h are
Mh
=
GMh−1 = Ghβt,
(4.74a)
Vh
=
GVh−1GT + αxTV (βt+h−1 · ǫt+h|βt)xαT
=
GVh−1GT + αxTE[βt+h−1βT
t+h−1|βt] · E[ǫ2
t+h]xαT
=
GVh−1GT + σ2
t+hαxT(Vh−1 + Mh−1MT
h−1)xαT
=
GVh−1GT + σ2
t+hαxTVh−1xαT + σ2
t+h(xTGh−1βt)2ααT.(4.74b)
Since, according to equation (4.36a),
Yt+h = xTβt+h−1 + δTzt+h + xTβt+h−1 · ǫt+h,
(4.75)
129

we have the conditional mean and variance of Yt+h given βt and zt+h as
E[Yt+h|βt, zt+h] = xTMh−1 + δTzt+h = xTGh−1βt + δTzt+h
(4.76)
and
V (Yt+h|βt, zt+h)
=
xTVh−1x + xTV (βt+h−1 · ǫt+h|βt)x
=
(1 + σ2
t+h)xTVh−1x + σ2
t+h(xTGh−1βt)2.
(4.77)
D. Derivation of E[Yt+h|βt, zt+h] and V (Yt+h|βt, zt+h) for Models in Class 4
Let M∗
h = E[β∗
t+h|βt] and V∗
h = V (β∗
t+h|βt). Then, M∗
0 = β∗
t and V∗
0 = O.
According to equation (4.45b),
β∗
t+h = G∗β∗
t+h−1 + α∗· x∗Tβ∗
t+h−1 · ǫt+h,
(4.78)
which looks the same as equation (4.73). Following the same line of reasoning,
we obtain the conditional mean and variance of βt+h given βt and zt+h
M∗
h = (G∗)hβ∗
t ,
(4.79a)
V∗
h = G∗V∗
h−1G∗T + σ2
t+hα∗x∗TV∗
h−1x∗α∗T + σ2
t+h(x∗T(G∗)h−1β∗
t )2α∗α∗T.
(4.79b)
Since, according to equation (4.45a),
Yt+h = x∗Tβ∗
t+h−1 · st+h−M + δTzt+h + x∗Tβ∗
t+h−1 · st+h−M · ǫt+h,
(4.80)
we have the conditional mean and variance of Yt+h given βt and zt+h for 1 ≤
h ≤M as
E[Yt+h|βt, zt+h]
=
x∗TM∗
h−1 · st+h−M + δTzt+h
=
x∗T(G∗)h−1β∗
t · st+h−M + δTzt+h
(4.81)
130

and
V (Yt+h|βt, zt+h)
(4.82)
=
x∗TV∗
h−1x∗· s2
t+h−M + x∗TV (β∗
t+h−1 · ǫt+h|βt)x∗· s2
t+h−M
=
s2
t+h−M(1 + σ2
t+h)x∗TV∗
h−1x∗+ s2
t+h−Mσ2
t+h(x∗T(G∗)h−1β∗
t )2. (4.83)
131

CHAPTER V
BAYESIAN VALIDATION OF COMPUTER
MODELS
5.1
Introduction
Computer models are mathematic representations of real systems, for example a group
of partial diﬀerential equations with initial and boundary conditions for many engi-
neering problems. They have been commonly used to investigate complex systems for
which physical experiments are either highly expensive or too time-consuming (Sacks
et al 1989, Welch et al 1992, Santner et al 2003).
However, before using a com-
puter model to investigate a real system, we need to address an important question
“How well does the computer model represent the real system?” Without a meaning-
ful answer to this question, any conclusions based on the analysis of outputs from a
computer model are about this computer model and can not be simply applied to the
real system of interest. The process of determining to what degree a computer model
accurately represents the real system is referred to as model validation (AIAA G-077-
1998) that generally involves the comparison of outputs computed from a computer
model to observations collected from physical experiments.
There are many ways to compare computer outputs and physical observations for
validating a computer model. For example, we can graphically display both computer
outputs and physical observations in one plot and see if computer outputs agree with
physical observations. The graphical comparison is easy and simple and probably is
the ﬁrst thing we should do before attempting any sophisticated methods. However,
132

this approach is obviously too subjective and lacks a quantitative indication of agree-
ment between computer outputs and physical observation. A more statistically sound
way for the comparison of computer outputs and physical observations is to conduct a
hypothesis testing. Hypothesis testing is not a new technique, but not until recently
did people start to use it for computer model validation (Hills and Trucano 1999,
Hills and Trucano 2002, Hills 2006). For example, Hills and Trucano (2002) used a
χ2 test for computer model validation. They assumed that the vectors of computer
outputs and physical observations follow independent multivariate normal distribu-
tions and computed a χ2 statistic to test a null hypothesis that the model bias (i.e.,
the diﬀerence of the two vectors) has a zero mean. Using hypothesis testing, we can
control the type-I error (i.e., the probability of rejecting a good computer model) but
not type-II error (i.e., the probability of failing to reject a bad computer model). In
other words, rejecting the null hypothesis means that we have strong evidence that
the computer model is not an accurate representation of the real system. However,
failing to reject a null hypothesis could be due to two reasons, the computer model
does not accurately represent the real system or we do not have enough data to reject
a bad computer model. Furthermore, hypothesis testing, after rejecting a computer
model, gives no suggestions on how the model bias behaves and how the prediction
by the computer model can be improved with available physical observations.
Recently, Oberkampf and Barone (2004) gave a comprehensive review on com-
puter model validation. They argued that computer model validation should be done
quantitatively through the use of computable measures that enable the quantitative
comparison of computer outputs and physical observations over a range of input vari-
ables. Those measures have been referred to as validation metrics. Oberkampf and
Barone (2004) discussed a variety of conceptual properties that a validation metric
should possess and emphasized that a validation metric should quantify uncertainties
in the comparison of computer outputs and physical observations. Uncertainties could
133

be due to random measurement errors in the physical observations and/or errors re-
sulting from post-processing computer outputs and/or physical observations, such as
errors resulting from ﬁtting a model to computer outputs or physical observations. In
the same paper, they proposed a validation metric that uses the statistical concept of
conﬁdence intervals for the quantiﬁcation of uncertainties. With computer outputs
and physical observations as the input data, they ﬁrst ﬁtted a nonlinear regression
model to physical observations and constructed a conﬁdent band for the ﬁtted re-
gression curve. This conﬁdence band is composed of individual conﬁdence intervals
for the ﬁtted curve at diﬀerent values of input variables. They then compared the
ﬁtted regression curve along with its conﬁdence band to computer outputs. This
comparison should be able to reveal how well computer outputs agree with physical
observations and in which regions of the input space the two agree with each other
well and in which regions they do not. Oberkampf and Barone (2004) for the ﬁrst time
provide an approach that validates a computer model by quantitatively comparing
computer outputs and physical observations over a range of input variables.
However, there are some concerns with Oberkampf and Barone’s approach. First,
the choice of the nonlinear regression model has a great inﬂuence in validation results.
A large disagreement between the ﬁtted regression curve and computer outputs might
be due to an inappropriate choice of the regression model rather than a poor computer
model. An appropriate model choice requires good scientiﬁc knowledge of the real
system being studied. Second, the fact that often only few physical observations are
available implies likely diﬃculties in the ﬁtting of the nonlinear regression model as
nonlinear models usually require a large number of observations to have a good model
estimation. Furthermore, the model ﬁtting uses only physical observations and does
not consider computer outputs. We will show later that integrating together physical
observations and computer outputs produces a better prediction of the real system.
Third, Oberkampf and Barone (2004) quantiﬁed uncertainties due to measurement
134

errors in physical observations and estimation errors in model ﬁtting by constructing
conﬁdence intervals for the ﬁtted regression curve. However, with nonlinear models,
the computation of conﬁdence intervals is rather complicated and often requires cer-
tain approximations. Fourth, with Oberkampf and Barone’s approach, it is not clear
how to improve the prediction of the real system when the comparison suggests a
large disagreement between computation and experimentation.
The validation method proposed by Oberkampf and Barone (2004) is a frequentist
approach.
In this chapter, we proposed a Bayesian approach to computer model
validation. The Bayesian approach has the ability to take into consideration prior
knowledge on the real system in the form of prior distributions for certain parameters.
The outputs of our Bayesian approach include the posterior distributions of both the
model bias and the real system output. The posterior distribution of the model bias
serves as a validation metric for the quantitative comparison of computer outputs and
physical observations. Both the mean and variance of the model bias are functions
of input variables, providing an evaluation of the representativeness of the computer
model over a range of input variables.
The proposed Bayesian approach overcomes certain problems Oberkampf and
Barone’s approach has. First, with the validation metric in the form of a probability
distribution, the construction of conﬁdence intervals is straightforward and requires
no approximations. Second, the proposed Bayesian approach models the real sys-
tem output as the sum of two Gaussian processes. Gaussian processes are essentially
nonparametric and can adapt to any shape suggested by the data with only a sim-
ple assumption for the mean function, which frees us from choosing a complicated
nonlinear model. Third, as most parameters are integrated out during the deriva-
tion of the posterior distributions, the estimation becomes much easier compared
to ﬁtting a nonlinear regression model. Fourth, as mentioned above, the proposed
Bayesian approach also produces the posterior distribution of the real system output.
135

This posterior is based on both physical observations and computer outputs, giving
a better prediction of the real system than using only either physical observations
or computer outputs as demonstrated in section 5.6. Furthermore, the posterior of
the real system output considers the uncertainties due to model ﬁtting to computer
outputs. This source of uncertainties were ignored by Oberkampf and Barone (2004).
In some aspects, our proposed Bayesian approach is similar to the Bayesian ap-
proach proposed by Kennedy and O’Hagan (2001) for the calibration of computer
models using both computer outputs and physical observations. Their approach also
uses Gaussian processes and assumes a similar relationship among physical observa-
tions, computer outputs, and the real system output, essentially equation (5.1) with
the term Y m(x) replaced by ρY m(x, Θ), where ρ is an unknown constant, and Θ is
the vector of calibration parameters. The presence of Θ is important, indicating that
their method is aimed at ﬁnding the value of Θ that brings computer outputs as close
as possible to physical observations rather than modelling the diﬀerence between them
as we will do. In addition, Kennedy and O’Hagan (2001) assume improper priors for
unknown parameters, which is equivalent to treating them as ﬁxed unknowns. While,
our approach adopts traditional priors–normal distributions for location parameters
and inverse gamma distributions for variance parameters–and is a full Bayesian anal-
ysis as we integrate out both location and variance parameters.
The organization of this chapter is as follows. In section 5.2, we give the general
statistical framework on which our Bayesian approach is based. In section 5.3, we
derive the posterior distributions of the model bias, the computer output, and the real
system output. In section 5.4, we discuss the estimation of certain parameters present
in the derived posterior distributions. In section 5.5, based on the results presents in
section 5.3 and 5.4, we describe a complete Bayesian procedure for computer model
validation. In section 5.6, we demonstrate the proposed Bayesian approach using
several numerical examples.
136

5.2
Statistical Framework
Let Y m(x) be the output of a computer model at x, where x = (x1, · · · , xp)T is a point
in a p-dimensional input space. Let Y e(x) and Y r(x) be the physical observation and
the real system output at x respectively. The proposed Bayesian approach models
the relationships among the computer model output Y m(x), the physical observation
Y e(x), and the real system output Y r(x) via
Y e(x) = Y r(x) + ǫ(x) = Y m(x) + δ(x) + ǫ(x),
(5.1)
where δ(x) is the bias of the computer model, and ǫ(x) is the measurement error of
the physical observation.
For the model in equation (5.1), we assume the following:
• The computer model output Y m(x) is a Gaussian process with mean µm(x) =
f T
m(x)βm and covariance function σ2
mRm, where fm(x) = (fm,1(x), · · · , fm,qm(x))T
is a vector of qm functions of x, and Rm has an exponential form
Rm(xi, xj) =
p
Y
k=1
exp

−φm,k(xi,k −xj,k)Pm,k	
,
(5.2)
with φm,k > 0 and 0 < Pm,k ≤2 for k = 1, · · · , p.
We denote φm =
(φm,1, · · · , φm,p)T and Pm = (Pm,1, · · · , Pm,p)T.
• The model bias δ(x) is a Gaussian process with mean µδ(x) = f T
δ (x)βδ and
covariance function σ2
δRδ, where fδ(x) = (fδ,1(x), · · · , fδ,qδ(x))T is a vector of qδ
functions of x, and Rδ has an exponential form
Rδ(xi, xj) =
p
Y
k=1
exp

−φδ,k(xi,k −xj,k)Pδ,k	
,
(5.3)
with φδ,k > 0 and 0 < Pδ,k ≤2 for k = 1, · · · , p. We denote φδ = (φδ,1, · · · , φδ,p)T
and Pδ = (Pδ,1, · · · , Pδ,p)T.
• The measurement error ǫ(x) has a normal distriution with mean zero and vari-
ance σ2
ǫ, and E[ǫ(xi) · ǫ(xj)]=0 for any xi ̸= xj.
137

• Y m(·), δ(·), and ǫ(·) are mutually independent.
Before continuing , we give a few notations. Let D = {x1, · · · , xn} and D∗=
{x∗
1, · · · , x∗
n∗} be any two sets of points in the input space.
Let Rm(D, D∗) be
the matrix of correlations between the vectors Y m(D) = (Y m(x1), · · · , Y m(xn))T
and Y m(D∗) = (Y m(x∗
1), · · · , Y m(x∗
n∗))T with the (i, j)th element Rm(xi, x∗
j), and
use Rm(D) as a shorthand for Rm(D, D).
Let Fm(D) = (fm(x1), · · · , fm(xn))T.
Similarly, we deﬁne Rδ(D, D∗), Rδ(D), and Fδ(D).
Let De = {xe
1, · · · , xe
ne} be
the set of ne points in the input space where physical observations are available,
and Dm = {xm
1 , · · · , xm
nm} the set of nm points where computer outputs are avail-
able. De and Dm may or may not overlap. Let ye = (ye(xe
1), · · · , ye(xe
ne))T and
ym = (ym(xm
1 ), · · · , ym(xm
nm))T be the vectors of physical observations at De and
computer outputs at Dm respectively. Let Ik be an k × k identity matrix.
5.3
The Bayesian Approach
In this section, we derive the posterior distributions of the model bias δ(·), the com-
puter output Y m(·), and the real system output Y r(·) under two assumptions: (1)
De ⊆Dm, and (2) certain parameters such as φδ and Pδ are known. In next section,
we deal with situations where those two assumptions do not hold.
5.3.1
Prior Distributions for Unknown Parameters
One advantage of the proposed Bayesian approach is its ability to take into account
a priori knowledge on the real system in the form of prior distributions for unknown
parameters. Let θ = {βm, σ2
m, φm, Pm, βδ, σ2
δ, φδ, Pδ, σ2
ǫ} be the collection of param-
eters in equation (5.1). We assume the following priors
βδ|σ2
δ ∼N(bδ, σ2
δVδ),
βm|σ2
m ∼N(bm, σ2
mVm),
(5.4a)
σ2
δ ∼IG(αδ, γδ),
σ2
m ∼IG(αm, γm),
(5.4b)
138

where N(b, σ2V) denotes a multivariate normal distribution with mean vector b and
covariance matrix σ2V, and IG(α, γ) denotes an inverse gamma distribution that has
a density function
p(u; α, γ) =
γα
Γ(α) u−α−1e−γ/u, u > 0, α > 0, γ > 0.
(5.5)
In addition, we assume that {βm, σ2
m, φm, Pm}, {βδ, σ2
δ, φδ, Pδ}, and σ2
ǫ, three sets
of parameters, are mutually independent. Furthermore, {βm, σ2
m} and {φm, Pm} are
independent, and same are {βδ, σ2
δ} and {φδ, Pδ}. As a result,
p(θ) = p(βm, σ2
m) · p(φm, Pm) · p(βδ, σ2
δ) · p(φδ, Pδ) · p(σ2
ǫ).
(5.6)
5.3.2
Posterior Distribution of Model Bias δ(·)
Given physical observations ye and computer outputs ym, the posterior distribution
of the model bias δ(D) = (δ(x1), · · · , δ(xn))T at any set D = {x1, · · · , xn} in the
input space can be obtained by, according to Bayes’ theorem,
p(δ(D)|ye, ym) =
Z
θ
p(δ(D)|ye, ym, θ) · p(θ|ye, ym) dθ.
(5.7)
We can easily derive the density p(δ(D)|ye, ym, θ).
Lemma 1 Under the assumption that De ⊆Dm, for any set D in the input space,
the distribution of δ(D)|ye, ym, θ is free of parameters βm, σ2
m, φm, and Pm.
Proof: Assume that computer outputs are available at every point in De (i.e., De ⊆
Dm). Let ym
ne = (ym(xe
1), · · · , ym(xe
ne))T be the vector of computer outputs at De.
According to equation (5.1), we know that, given θ,


δ(D)
ye
ym


 θ
(5.8)
139

has a multivariate normal distribution with mean vector


Fδ(D)βδ
Fm(De)βm + Fδ(De)βδ
Fm(Dm)βm


(5.9)
and covariance matrix


σ2
δRδ(D)
σ2
δRδ(D, De)
0
σ2
δRδ(De, D)
σ2
mRm(De) + σ2
δRδ(De) + σ2
ǫIne
σ2
mRm(De, Dm)
0
σ2
mRm(Dm, De)
σ2
mRm(Dm)


.
(5.10)
Therefore,
δ(D)|ye, ym, θ ∼N

Fδ(D)βδ + Rδ(D, De)(Rδ(De) + τIne)−1(ye −ym
ne −Fδ(De)βδ),
σ2
δ

Rδ(D) −Rδ(D, De)(Rδ(De) + τIne)−1Rδ(De, D)
 
, (5.11)
where τ = σ2
ǫ/σ2
δ. As a result, the distribution of δ(D)|ye, ym, θ is free of parameters
βm, σ2
m, φm, and Pm (i.e., parameters in the Gaussian process Y m(·)).
□
Theorem 2 Under the assumptions that De ⊆Dm and φδ, Pδ, and τ are known,
for any set D in the input space,
δ(D)|ye, ym = δ(D)|ye, ym
ne ∼Tn(νδ|e,m, µδ|e,m(D), Σδ|e,m(D)),
(5.12)
where
νδ|e,m = ne + 2αδ,
(5.13a)
µδ|e,m(D) = HT
δ Aδvδ + Rδ(D, De)(Rδ(De) + τIne)−1(ye −ym
ne),
(5.13b)
Σδ|e,m(D) =
Q2
δ
νδ|e,m

Rδ(D) −Rδ(D, De)(Rδ(De) + τIne)−1Rδ(De, D) + HT
δ AδHδ

,
(5.13c)
Q2
δ = 2γδ + (ye −ym
ne)T(Rδ(De) + τIne)−1(ye −ym
ne) + bδV−1
δ bδ −vT
δ Aδvδ,(5.13d)
HT
δ = Fδ(D) −Rδ(D, De)(Rδ(De) + τIne)−1Fδ(De),
(5.13e)
A−1
δ
= FT
δ (De)(Rδ(De) + τIne)−1Fδ(De) + V−1
δ ,
(5.13f)
vδ = FT
δ (De)(Rδ(De) + τIne)−1(ye −ym
ne) + V−1
δ bδ.
(5.13g)
140

Proof: By Lemma 1, the distribution of δ(D)|ye, ym, θ is free of parameters βm, σ2
m,
φm, and Pm. Therefore, the posterior of δ(D) in equation (5.7) becomes
p(δ(D)|ye, ym)
(5.14)
=
Z
βδ,σ2
δ,φδ,Pδ,τ
p(δ(D)|ye, ym, βδ, σ2
δ, φδ, Pδ, τ) p(βδ, σ2
δ, φδ, Pδ, τ|ye, ym) dβδ dσ2
δ dφδ dPδ dτ
Assume that φδ, Pδ, and τ are known. After integrating out βδ and σ2
δ (see Appendix
5.7.3), we have Theorem 2.
□.
According to Theorem 2, under the assumptions that De ⊆Dm and parameters
φδ, Pδ, and τ are known, for any set D, the posterior of the model bias δ(D) is
a multivariate noncentral t distribution with degree of freedom νδ|e,m, noncentrality
parameter µδ|e,m(D), and scale matrix Σδ|e,m(D). In other words, the posterior of
the model bias δ(·) is a noncentral t process. It is important to point out that the
posterior of δ(·) depends on only ye and ym
ne (the vector of computer outputs at De)
and computer outputs at Dm −De do not help the prediction of δ(·). The reason
for such a result is because of two assumptions we made: (1) De ⊆Dm and (2)
Y m(·) and δ(·) are mutually independent. In fact, the posterior distribution of δ(D)
in equations (5.12) and (5.13) is the same as the one obtained by ﬁtting a single
Gaussian process with an additional experimental error to ye −ym
ne, the diﬀerence of
physical observations and computer outputs at De.
With Theorem 2, we can easily construct pointwise prediction intervals for the
mode bias δ(·) by using the fact that, at any x,
δ(x)|ye, ym −µδ|e,m(x)
σδ|e,m(x)
∼T1(νδ|e,m, 0, 1)
(5.15)
where T1(νδ|e,m, 0, 1) is a univariate t distribution with degree of freedom νδ|e,m. This
gives the 100(1 −α)% prediction interval for the model bias δ(x) at any point x in
the input space as
µδ|e,m(x) ± σδ|e,m(x) · tνδ|e,m, α/2
(5.16)
141

where tνδ|e,m, α/2 is the upper α/2 critical point of a univariate t distribution with
degree of freedom νδ|e,m.
5.3.3
Posterior Distribution of Computer Output Y m(·)
The posterior distribution of the computer output Y m(D) can be derive in a very
similar way in which the posterior distribution of δ(D) is derived. By Bayes’ theorem,
we have
p(Y m(D)|ye, ym) =
Z
θ
p(Y m(D)|ye, ym, θ) · p(θ|ye, ym) dθ.
(5.17)
Lemma 3 Under the assumption that De ⊆Dm, for any set D in the input space,
the distribution of Y m(D)|ye, ym, θ is free of parameters βδ, σ2
δ, φδ, Pδ, and σ2
ǫ.
Proof: Assuming that De ⊆Dm, in a similar way in which we derive the distribution
of δ(D)|ye, ym, θ, we can easily show that
Y m(D)|ye, ym, θ ∼N

Fm(D)βm + Rm(D, Dm)R−1
m (Dm)(ym −Fm(Dm)βm),
σ2
m

Rm(D) −Rm(D, Dm)R−1
m (Dm)Rm(Dm, D)
 
,
(5.18)
which is free of parameters βδ, σ2
δ, φδ, Pδ, and σ2
ǫ.
□
Theorem 4 Under the assumptions that De ⊆Dm and φm and Pm are known, for
any set D in the input space,
Y m(D)|ye, ym = Y m(D)|ym ∼Tn(νm|m, µm|m(D), Σm|m(D)),
(5.19)
142

where
νm|m = nm + 2αm,
(5.20a)
µm|m(D) = HT
mAmvm + Rm(D, Dm)R−1
m (Dm)ym,
(5.20b)
Σm|m(D) = Q2
m
νm|m

Rm(D) −Rm(D, Dm)R−1
m (Dm)Rm(Dm, D) + HT
mAmHm

, (5.20c)
Q2
m = 2γm + (ym)TR−1
m (Dm)ym + bmV−1
m bm −vT
mAmvm,
(5.20d)
HT
m = Fm(D) −Rm(D, Dm)R−1
m (Dm)Fm(Dm),
(5.20e)
A−1
m = FT
m(Dm)R−1
m (Dm)Fm(Dm) + V−1
m ,
(5.20f)
vm = FT
m(Dm)R−1
m (Dm)ym + V−1
m bm.
(5.20g)
Proof: By Lemma 3, the distribution of Y m(D)|ye, ym, θ is free of parameters βδ,
σ2
δ, φδ, Pδ, and σ2
ǫ. Therefore, equation (5.17) becomes
p(Y m(D)|ye, ym)
(5.21)
=
Z
βm,σ2m,φm,Pm
p(Y m(D)|ye, ym, βm, σ2
m, φm, Pm) p(βm, σ2
m, φm, Pm|ye, ym) dβm dσ2
m dφm dPm
Assume that φm and Pm are known. After integrating out βm and σ2
m (in a similar
way we integrate out βδ and σ2
δ), we have Theorem 4.
□.
It is important to point out that ye does not appear in the posterior distribution
of Y m(D). In other words, the posterior distribution of Y m(D) is free of ye and
depends on only ym. We emphasize this fact by using a subscript m|m instead of
m|e, m. The reason for such a result is because of two assumptions we made: (1)
De ⊆Dm and (2) Y m(·) and δ(·) are mutually independent. In fact, the posterior
distribution of Y m(D) in equations (5.19) and (5.20) is the same as the one obtained
by ﬁtting a single Gaussian process to computer outputs ym (Santner et al 2003).
Similar to the construction of pointwise prediction intervals for the model bias
δ(·), we can construct the 100(1 −α)% prediction interval for the computer output
Y m(x) at any point x in the input space as
µm|m(x) ± σm|m(x) · tνm|m, α/2
(5.22)
143

where tνm|m, α/2 is the upper α/2 critical point of a univariate t distribution with
degree of freedom νm|m.
5.3.4
Posterior Distribution of Real System Output Y r(·)
We start the derivation of the posterior of the real system output Y r(·) given physical
observations ye and computer outputs ym by showing that the posteriors of the
computer output Y m(·) and the model bias δ(·) are independent. We have shown
that the posteriors of δ(·) and Y m(·) are both noncentral t processes. As a result, the
posterior of Y r(·) is the sum of two independent noncentral t processes.
Lemma 5 Under the assumption that De ⊆Dm, for any set D in the input space,
Y m(D)|ye, ym, θ and δ(D)|ye, ym, θ are independent.
Proof: We know that, for any set D in the input space,


Y r(D)
ye
ym


θ =


Y m(D) + δ(D)
ye
ym


θ
(5.23)
has a multivariate normal distribution with mean vector


Fm(D)βm + Fδ(D)βδ
Fm(De)βm + Fδ(De)βδ
Fm(Dm)βm


(5.24)
and covariance matrix


σ2
mRm(D) + σ2
δRδ(D)
σ2
mRm(D, De) + σ2
δRδ(D, De)
σ2
mRm(D, Dm)
σ2
mRm(De, D) + σ2
δRδ(De, D)
σ2
mRm(De) + σ2
δRδ(De) + σ2
ǫIne
σ2
mRm(De, Dm)
σ2
mRm(Dm, D)
σ2
mRm(Dm, De)
σ2
mRm(Dm)


.
(5.25)
144

Therefore, under the assumption that De ⊆Dm, Y r(D)|ye, ym, θ has a multivariate
normal distribution with mean vector
E[Y r(D)|ye, ym, θ] = Fm(D)βm + Rm(D, Dm)R−1
m (Dm)(ym −Fm(Dm)βm)
(5.26)
+ Fδ(D)βδ + Rδ(D, De)(Rδ(De) + τIne)−1(ye −ym
ne −Fδ(De)βδ),
and covariance matrix
Cov(Y r(D)|ye, ym, θ) = σ2
m

Rm(D) −Rm(D, Dm)R−1
m (Dm)Rm(Dm, D)

(5.27)
+ σ2
δ

Rδ(D) −Rδ(D, De)(Rδ(De) + τIne)−1Rδ(De, D)

.
Equation (5.27) together with equations (5.11) and (5.18) implies that, under the
assumption that De ⊆Dm,
Cov(Y r(D)|ye, ym, θ) = Cov(Y m(D)|ye, ym, θ) + Cov(δ(D)|ye, ym, θ).
(5.28)
That is, Y m(D)|ye, ym, θ and δ(D)|ye, ym, θ are uncorrelated. Therefore, they are
independent.
□
Lemma 6 Under the assumption that De ⊆Dm, for any set D in the input space,
Y m(D)|ye, ym and δ(D)|ye, ym are independent.
Proof: The joint distribution of Y m(D) and δ(D) given ye and ym can be obtained
by
p(Y m(D), δ(D)|ye, ym)
=
Z
θ
p(Y m(D)|ye, ym, θ)p(δ(D)|ye, ym, θ)p(θ|ye, ym)dθ
=
Z
θ
p(Y m(D)|ye, ym, βm, σ2
m, φm, Pm)p(δ(D)|ye, ym, βδ, σ2
δ, φδ, Pδ, σ2
ǫ)p(θ|ye, ym)dθ
=
Z
βm,σ2m,φm,Pm
p(Y m(D)|ye, ym, βm, σ2
m, φm, Pm)p(βm, σ2
m, φm, Pm|ye, ym)dβmdσ2
mdφmdPm
·
Z
βδ,σ2
δ,φδ,Pδ,τ
p(δ(D)|ye, ym, βδ, σ2
δ, φδ, Pδ, τ)p(βδ, σ2
δ, φδ, Pδ, σ2
ǫ|ye, ym)dβδdσ2
δdφδdPδdτ
=p(Y m(D)|ye, ym) · p(δ(D)|ye, ym).
(5.29)
145

That is, Y m(D)|ye, ym and δ(D)|ye, ym are independent. The ﬁrst step in equation
(5.29) is due to Lemma 5, the second step is due to Lemmas 1 and 3, and the third
step is due to the fact that
p(βm, σ2
m, φm, Pm, βδ, σ2
δ, φδ, Pδ, τ|ye, ym)
= p(βm, σ2
m, φm, Pm|ye, ym) · p(βδ, σ2
δ, φδ, Pδ, τ|ye, ym)
(5.30)
□
Theorem 7 Under the assumptions that De ⊆Dm and φm, Pm, φδ, Pδ, and τ are
known, for any set D in the input space, the distribution of Y r(D)|ye, ym is the sum
of two independent multivariate noncentral t distributions given in equations (5.12),
(5.13), (5.19) and (5.20).
Proof: By Theorem 2, δ(D)|ye, ym has a multivariate noncentral t distribution when
De ⊆Dm and φδ, Pδ, and τ are known. By Theorem 4, Y m(D)|ye, ym has a multivari-
ate noncentral t distribution when De ⊆Dm and φm and Pm are known. Therefore,
the posterior distribution of Y r(D) given ye and ym is the sum of two independent
multivariate noncentral t distributions.
□
The posterior mean and covariance matrix of Y r(D) can be easily obtained from
equations (5.12), (5.13), (5.19) and (5.20)
E[Y r(D)|ye, ym] = µr|e,m(D) = µm|m(D) + µδ|e,m(D),
(5.31a)
Cov(Y r(D)|ye, ym) = Σr|e,m(D) =
νm|m
νm|m −2 · Σm|m(D) +
νδ|e,m
νδ|e,m −2 · Σδ|e,m(D).
(5.31b)
Pointwise prediction intervals for the real system output Y r(·) can be constructed by
using the fact that, for any x,
Y r(x)|ye, ym −µr|e,m(x)
σr|e,m(x)
∼σm|m(x)
σr|e,m(x)T1(νm|m, 0, 1) + σδ|e,m(x)
σr|e,m(x)T1(νδ|e,m, 0, 1). (5.32)
146

This gives the 100(1 −α)% prediction interval for the real system output Y r(x) at
any point x in the input space as
µr|e,m(x) ± cα/2 · σr|e,m(x)
(5.33)
where the critical point cα/2 can be estimated from observations randomly generated
from the two independent univariate t distributions, T1(νδ|e,m, 0, 1) and T1(νm|m, 0, 1).
5.4
When De ̸⊆Dm and φm, Pm, φδ, Pδ, and τ are
unknown
When deriving the posteriors of the model bias δ(·) and the real system output Y r(·),
we assume that: (1) computer outputs are available at De (i.e., De ⊆Dm), and (2)
parameters φm, Pm, φδ, Pδ, and τ are known. These two assumptions are not always
true especially the second one. Below we describe how we handle those two situations.
5.4.1
Prediction of Y m(De −Dm)
When De ̸⊆Dm, we could either simulate computer outputs at De −Dm (the set of
points in De but not in Dm) by running the computer model if it is not too expensive
to do so or we could predict computer outputs at De −Dm using the posterior mean
of Y m(De −Dm)
ˆY m(De −Dm) = µm|m(De −Dm).
(5.34)
We often have a computer design set Dm large enough such that the prediction
ˆY m(De −Dm) is quite accurate and there is little loss in using ˆY m(De −Dm) as
real computer outputs Y m(De −Dm). Having computer outputs at all points in De,
we can use the results derived in previous section.
5.4.2
Estimation of φm, Pm, φδ, Pδ, and τ
The posterior of the model bias δ(D) in equation (5.12) is in fact the conditional
posterior of δ(D) given φδ, Pδ, and τ. To get the marginal posterior of δ(D), we
147

have to integrate out φδ, Pδ, and τ, which often is diﬃcult to derive analytically or
computationally prohibitive. Therefore, instead of deriving the marginal posterior of
δ(D) by integrating out φδ, Pδ, and τ, we estimate φδ, Pδ, and τ and then treat the
estimates as if they are the true values of φδ, Pδ, and τ. Same is done for φm and Pm
in the posterior of the computer output Y m(D). Instead of integrating out φm and
Pm, we estimate φm and Pm and treat the estimates as if they are the true values of
φm and Pm.
• Maximum Likelihood (ML) Estimates
The ML estimates of φm, Pm, φδ, Pδ, and τ maximize the likelihood
p(ye, ym|φm, Pm, φδ, Pδ, τ).
(5.35)
Since
p(ye, ym|φm, Pm, φδ, Pδ, τ) = p(ye|ym, φδ, Pδ, τ) · p(ym|φm, Pm),
(5.36)
and (see Appendix 5.7.4)
p(ym|φm, Pm) ∝|Rm(Dm)|−1
2 · |Am|
1
2
(5.37a)
·

2γm + (ym)TR−1
m (Dm)ym + bT
mV−1
m bm −vT
mAmvm
−nm
2 −αm ,
p(ye|ym, φδ, Pδ, σ2
ǫ) ∝|φδ + τIne|−1
2 · |Aδ|
1
2
(5.37b)
·

2γδ + (ye −ym
ne)T(Rδ(De) + τIne)−1(ye −ym
ne) + bT
δ V−1
δ bδ −vT
δ Aδvδ
−ne
2 −αδ ,
we estimate φm, Pm, φδ, Pδ, and τ in two steps:
– First, estimate φm and Pm by minimizing
log(|Rm(Dm)|) −log(|Am|)
(5.38)
+(nm + 2αm) · log

2γm + (ym)TR−1
m (Dm)ym + bT
mV−1
m bm −vT
mAmvm

,
which uses only computer outputs ym.
148

– Next, estimate φδ, Pδ, and τ by minimizing
log(|Rδ(De) + τIne|) −log(|Aδ|) + (ne + 2αδ)
(5.39)
· log

2γδ + (ye −ym
ne)T(Rδ(De) + τIne)−1(ye −ym
ne) + bT
δ V−1
δ bδ −vT
δ Aδvδ

,
which uses both physical observations ye and computer outputs ym. When com-
puter outputs at some points in De are not available, one can use the posterior
means of Y m(·) at those points instead.
• Markov Chain Monte Carlo (MCMC) Estimates
A common problem with maximum likelihood estimation is that the minimization of
either (5.38) or (5.39) is highly unstable, unable to converge or converging to a local
minimum. An alternative is to use Markov Chain Monte Carlo (MCMC) algorithms
(Lange 1999, Liu 2001) to estimate the posterior distributions of φm and Pm, and
then use the posterior means or modes of φm and Pm as the estimates of φm and Pm
(Bayarri et al 2002, Qian and Wu 2005). Same can be done for φδ, Pδ, and τ.
• Minimum Mean Squared Prediction Error (MMSPE) Estimates
Another way to estimate φm and Pm is to ﬁnd the values of φm and Pm that minimize
the mean squared prediction errors on a hold-out sample (i.e., a sample that is not
used for the calculation of the posterior of Y m(·)). The resulting estimates are the
MMSPE estimates of φm and Pm (Santner et al 2003). When there are no enough
computer outputs to aﬀord a hold-out sample, cross validation can be used instead.
Same can be done for φδ, Pδ, and τ.
5.5
A Bayesian Validation Procedure
In this section, we present a complete Bayesian validation procedure based on the
results derived above. This procedure consists of the following steps:
(1). Collect data.
149

Both physical observations and computer outputs are essential to model vali-
dation. We should collect as many as possible physical observations at input
points as close to the input region of interest as possible. Compared to physi-
cal observations, computer outputs are less costly and should be computed at
points where physical observations are available and close to if not within the
input region of interest. After data collection is done, we should have De, ye,
Dm, and ym.
(2). Determine the priors of model parameters.
One advantage the Bayesian approach has is its ability to take into account
scientiﬁc knowledge and past information on the real system in the form of
prior distributions for unknown model parameters. For the model in equation
(5.1), a priori knowledge can be expressed as priors for parameters in θ. When
there is little a priori knowledge, we use the “vague” priors:
σ2
m ∼IG(2, 1), βm|σ2
m ∼N(0qm, σ2
mIqm), σ2
δ ∼IG(2, 1), βδ|σ2
δ ∼N(0qδ, σ2
δIqδ),
(5.40)
where 0k is a k × 1 vector of zeros, and Ik is a k × k identity matrix.
(3). Estimate φm and Pm and calculate the posterior of Y m(D).
As mentioned before, the posterior of Y m(D) in equations (5.19) and (5.20) is
conditional on φm and Pm. To get the marginal posterior of Y m(D), we need
to integrate out φm and Pm, which is often diﬃcult or impossible to do. As
a result, we estimate φm and Pm and treat the estimates as their true values.
There are many ways to estimate φm and Pm. Three methods, ML, MCMC,
and MMSPE, are mentioned in section 5.4.2. With the estimates of φm and
Pm, the posterior of Y m(D) is given by equations (5.12) and (5.13).
(4). Estimate φδ, Pδ, and τ.
150

Same as for φm and Pm, instead of integrating out φδ, Pδ, and τ to give the
marginal posterior of δ(D), we estimate φδ, Pδ, and τ and treat the estimates
as their true values.
(5). Calculate the posterior of δ(D).
Having physical observations ye, computer outputs ym (computed from the
computer model or predicted using the posterior distribution of Y m(·)), the
priors for parameters, and the estimates of φm, Pm, φδ, Pδ, and τ, we can
calculate the posterior distribution of δ(D) using equations (5.12) and (5.13).
(6). Calculate the posterior of Y r(D).
As proven in section 5.3.4, under the assumption that De ⊆Dm, the posteriors
of Y m(D) and δ(D) are independent multivariate noncentral t distributions.
As a result, the posterior of Y r(D) is the sum of two independent multivariate
noncentral t distributions. The posterior mean and covariance matrix of Y r(D)
are given in equation (5.31).
(7). Validate the computer model.
Having the posterior distributions of δ(·) and Y r(·), we can construct predic-
tion intervals for δ(x) and Y r(x) at any x (see equations (5.16) and (5.33)).
Both prediction intervals can be used for the validation of the computer model.
The traditional approach is to reject the computer model at x if the prediction
interval for δ(x) (or Y r(x)) does not contain zero (or Y m(x)) and fail to reject
otherwise. There are two problems with this approach. First, we tend to reject
the computer model at points where more physical observations are available
and therefore prediction intervals are narrower while fail to reject the computer
model at points where fewer or no physical observations are available and there-
fore prediction intervals are wider. Second, failing to reject the computer model
151

at a point does not necessarily mean that the computer model is acceptable.
To overcome those two problems, we propose a new way for model validation
based on prediction intervals.
Let lr(x) and ur(x) be the lower and upper bounds of the 100(1−α)% prediction
interval for Y r(x) respectively. According to equation (5.33), we have
lr(x) = µr|e,m(x) −cα/2 · σr|e,m(x),
(5.41a)
ur(x) = µr|e,m(x) + cα/2 · σr|e,m(x).
(5.41b)
Deﬁne
∆min(x)=
(
0,
if Y m(x)∈(lr(x), ur(x)),
min

|Y m(x) −lr(x)|, |Y m(x) −ur(x)|
	
, otherwise.
(5.42)
and
∆max(x) = max

|Y m(x) −lr(x)|, |Y m(x) −ur(x)|
	
.
(5.43)
In other words, ∆min(x) and ∆max(x) are the minimum and maximum possible
deviations of Y m(x) from the real system output Y r(x).
Let ∆0 be a pre-
speciﬁed threshold.
– If ∆min(x) > ∆0, we reject the computer model as an acceptable represen-
tative of the real system at x.
– If ∆max(x) < ∆0, we conclude that the computer model is acceptable at
x.
– If ∆min(x) ≤∆0 ≤∆max(x), no conclusion can be reached and more
physical observations are needed.
The validation based on the prediction interval of δ(x) is similar. Let lδ(x) and
uδ(x) be the lower and upper bounds of the 100(1 −α)% prediction interval for
152

δ(x) respectively. According to equation (5.16), we have
lδ(x) = µδ|e,m(x) −σδ|e,m(x) · tνδ|e,m,α/2,
(5.44a)
uδ(x) = µδ|e,m(x) + σδ|e,m(x) · tνδ|e,m,α/2.
(5.44b)
The formulas for ∆min(x) and ∆max(x) are now given as
∆min(x) =
(
0,
if 0 ∈(lδ(x), uδ(x)),
min

|lδ(x)|, |uδ(x)|
	
,
otherwise.
(5.45)
and
∆max(x) = max

|lδ(x)|, |uδ(x)|
	
.
(5.46)
Having ∆min(x) and ∆max(x), with a pre-speciﬁed ∆0, we can make decisions
about the computer model in the same way as before.
The ∆min(x) and ∆max(x) based on lδ(x) and uδ(x) will be exactly the same
as the ∆min(x) and ∆max(x) based on lr(x) and ur(x) if x ∈Dm and sightly
diﬀerent from the latter if x ̸∈Dm. The reason is because that lr(x) and ur(x)
consider the variation in the computer output prediction while lδ(x) and uδ(x)
do not. However, this diﬀerence is often very small and negligible since enough
computer outputs are often available to guarantee a small prediction variation.
5.6
Numerical Experiments
We use three examples to illustrate the proposed Bayesian approach. For the ﬁrst two
examples, De = Dm. That is, the design set for the physical experiment is exactly
the same as that for the computer experiment. For the third example, De ∩Dm = ø.
That is, the two design sets have no common points, which means that we need to
predict computer outputs at De before computing the posterior distribution of δ(x).
For all three examples, we compare the results from the proposed Bayesian approach
to those from an approach proposed by Kennedy and O’Hagan (2001). Kennedy and
153

O’Hagan (2001) also use an exponential correlation function as shown in equations
(5.2) and (5.3) but have the values of Pk, k = 1, · · · , p ﬁxed at 2. In order for the
results from two approaches to be comparable, we run our Bayesian approach with
Pm,k = Pδ,k = 2, k = 1, · · · , p.
5.6.1
Example 1: Fluidized-Bed Coating
Dewettinck et al. (1999) described a Glatt GPCG-1 ﬂuidized-bed unit for coating food
products and several computer models developed for this unit to calculate the steady-
state thermodynamic operation point. They also reported 28 physical observations
of the steady-state outlet air temperature under diﬀerent values of six factors: the
room humidity (Hr) and temperature (Tr), the inlet air temperature (Ta), the ﬂow
rate of the coating liquid (Rf), the pressure of the atomization air (Pa), and the
ﬂuid velocity of the ﬂuidization air (Vf). The 28 physical observations along with the
corresponding values of the six factors are displayed in Table 5.1, which also contains
the steady-state outlet air temperatures computed from one computer model.
As the six factors have diﬀerent units, we normalize each factor before applying
the proposed Bayesian approach. We then divide the data into two parts, reserving
runs 4, 15, 17, 21, 23, 25, 26, and 28 as the testing data (the same partition used
by Qian and Wu (2005)). We assume a constant mean for both Gaussian processes,
Y m(·) and δ(·) and use the priors in equation (5.40). After getting the ML estimates
of φδ and τ, we compute the predictions of δ(·) and Y r(·) for the runs in the testing
data. The results are displayed in Tables 5.2 and 5.3, which also contain results from
another two approaches:
• The proposed Bayesian approach with Y m(·) ≡0. The purpose is to see if
including computer outputs improves the predictions of Y r(·).
• The approach by Kennedy and O’Hagan (2001)
The δ column in Table 5.2 contains observed model biases that equal to the diﬀerences
154

of physical observations and computer outputs.
The zeros in the ˆδm column are
obtained when we assume that there are no biases in computer outputs. The last
three columns in Table 5.2 give the model biases computed using the three methods
mentioned above respectively. The root mean squared prediction error (RMSPE) is
used to compare the performances of the three approaches. Both Tables 5.2 and 5.3
show that:
• Treating the computer model as it has no bias leads to a large RMSPE (1.9812).
• Using only physical observations leads to a larger RMSPE (1.5663) than using
both physical observations and computer outputs (0.6856).
• The proposed method using both computer outputs and physical observations
yields a slightly smaller RMSPE (0.6856) than Kennedy and O’Hagan’s method
(0.7119).
5.6.2
Example 2: Linear Cellular Alloys
This example is taken from Qian and Wu (2005). The quantity of interest is the
steady-state heat transfer rate of a heat exchanger used in an electrical cooling appli-
cation. Factors considered include the mass ﬂow rate (m) and temperature (Tin) of
the inlet air, the temperature of the heat source (Twall), and the thermal conductivity
(k). The data are presented in Table 5.4, in which the ye and ym columns are outputs
of two computer models. The computer model generating ye is more accurate and
complicated than the one generating ym. We use the notation ye just for consistency.
The ye’s here can be considered as physical observations without measurement errors
(i.e., τ = 0).
Same as in Example 1, we ﬁrst normalize the factors, and then divide the data into
two parts, using 24 runs as the training data and 8 runs (runs 1, 4, 9, 11, 13, 23, 25,
and 27) as the testing data (the same partition used by Qian and Wu (2005)). Using
155

Table 5.1: The Fluidized-Bed Coating Example
run
Hr(%)
Tr(oC)
Ta(oC)
Rf(g/min)
Pa(bar)
Vf(m/s)
ye
ym
1
51.0
20.7
50
5.52
2.5
3.0
30.4
31.5
2
46.4
21.3
60
5.53
2.5
3.0
37.6
38.5
3
46.6
19.2
70
5.53
2.5
3.0
45.1
45.5
4
53.1
21.1
80
5.51
2.5
3.0
50.2
52.6
5
52.0
20.4
90
5.21
2.5
3.0
57.9
59.9
6
45.6
21.4
60
7.25
2.5
3.0
32.9
34.6
7
47.3
19.5
70
7.23
2.5
3.0
39.5
41.0
8
53.3
21.4
80
7.23
2.5
3.0
45.6
48.5
9
44.0
20.1
70
8.93
2.5
3.0
34.2
36.6
10
52.3
21.6
80
8.91
2.5
3.0
41.1
44.3
11
55.0
20.2
80
7.57
1.0
3.0
45.7
49.0
12
54.0
20.6
80
7.58
1.5
3.0
44.6
48.4
13
50.8
21.1
80
7.40
2.0
3.0
44.7
48.4
14
48.0
21.2
80
7.43
2.5
3.0
44.0
48.0
15
42.8
22.4
80
7.51
3.0
3.0
43.3
47.5
16
55.7
20.8
50
3.17
1.0
3.0
37.0
38.0
17
55.2
20.7
50
3.18
1.5
3.0
37.2
38.5
18
54.4
20.7
50
3.19
2.0
3.0
37.1
37.5
19
55.4
19.8
50
3.20
2.5
3.0
36.9
38.5
20
52.9
20.0
50
3.19
3.0
3.0
36.8
37.2
21
28.5
18.3
80
7.66
2.5
3.0
46.0
47.3
22
26.1
19.0
80
7.69
2.5
4.0
54.7
56.2
23
24.2
18.9
80
7.69
2.5
4.5
57.0
58.7
24
25.4
18.5
80
7.70
2.5
5.0
58.9
60.5
25
45.1
19.6
50
3.20
2.5
3.0
35.9
37.1
26
43.1
20.3
50
3.23
2.5
4.0
40.3
40.8
27
42.7
20.4
50
3.20
2.5
4.5
41.9
42.3
28
38.7
21.6
50
3.22
2.5
5.0
43.1
43.3
156

Table 5.2: The Fluidized-Bed Coating Example: prediction of δ(x)
run
ye
ym
δ
ˆδm
ˆδe
ˆδ
ˆδKO
(ye −ym)
(ym −ym)
(ˆyr
e −ym)
(Proposed
(ˆyr
KO −ym)
Method)
4
50.20
52.60
-2.4000
0
-2.2927
-2.2108
-2.3509
15
43.30
47.50
-4.2000
0
-3.0360
-2.3824
-2.4837
17
37.20
38.50
-1.3000
0
-1.4962
-0.7623
-0.8576
21
46.00
47.30
-1.3000
0
2.6269
-1.3062
-1.2199
23
57.00
58.70
-1.7000
0
-1.6524
-1.5950
-1.5707
25
35.90
37.10
-1.2000
0
0.4054
-1.0197
-0.5741
26
40.30
40.80
-0.5000
0
-0.2511
-0.6935
-0.3865
28
43.10
43.30
-0.2000
0
0.1998
-0.4243
-0.8945
RMSPE
1.9812
1.5663
0.6856
0.7119
Table 5.3: The Fluidized-Bed Coating Example: prediction of Y r(x)
ˆyr
run
ye
ym
ˆyr
e
(Proposed
ˆyr
KO
Method)
4
50.20
52.60
50.3073
50.3892
50.2491
15
43.30
47.50
44.4640
45.1176
45.0163
17
37.20
38.50
37.0038
37.7377
37.6424
21
46.00
47.30
49.9269
45.9938
46.0801
23
57.00
58.70
57.0476
57.1050
57.1293
25
35.90
37.10
37.5054
36.0803
36.5259
26
40.30
40.80
40.5489
40.1065
40.4135
28
43.10
43.30
43.4998
42.8757
42.4055
RMSPE
1.9812
1.5663
0.6856
0.7119
157

the priors in equation (5.40) and assuming a linear mean for both Y m(x) and δ(x),
we predict δ(·) and Y r(·) for the runs in the testing data using the three methods
• The proposed Bayesian approach
• The proposed Bayesian approach with Y m(·) ≡0
• The approach by Kennedy and O’Hagan (2001)
The results are displayed in Tables 5.5 and 5.6. Both show that using the outputs
from both computer models leads to a signiﬁcant decrease in the value of RMSPE
(from 6.3059 to 2.4818). In addition, the proposed Bayesian approach yields a slightly
smaller RMSPE (2.4818) than the approach by Kennedy and O’Hagan (2.8678).
5.6.3
Example 3: Compressible Shear Layer
Oberkampf and Barone (2004) gave a detailed description on free shear layers. Ba-
sically, a free shear layer is formed when two streams with distinct velocities and
temperatures separated by a thin splitter plate mix downstream of the splitter plate
trailing edge. Of particular interest is the behavior of the free shear layer as the con-
vective Mach number Mc (representing the mixing of the two streams) increases for
ﬁxed velocity and temperature ratios. The behavior of the shear layer is usually rep-
resented by the compressibility factor deﬁned as the ratio of the compressible growth
rate (dc) to the incompressible growth rate (di) at the same velocity and temperature
ratios. Oberkampf and Barone (2004) reported both physical observations and com-
puter outputs on the compressibility factor of shear layers. The physical observations
are from several independent sources as shown by the column Source in Table 5.7.
The eleven computer outputs in Table 5.8 are the values of the compressibility factor
computed as Mc changes from 0.1 to 1.5 with an increase of 0.14 every time. The
interval (0, 1.5] is chosen to span the Mc range of the physical data.
Since little is known about model parameters beforehand, we use the priors in
equation (5.40). As computer outputs at De are unavailable, we ﬁrst compute the
158

Table 5.4: The Linear Cellular Alloys Example
run
m(kg/s)
Tin(K)
k(W/mk)
Twall(K)
ye
ym
1
0.000500
293.15
362.7
393.1
23.5
25.6
2
0.000550
315.00
310.0
365.0
20.1
21.2
3
0.000552
293.53
318.6
388.3
10.2
11.4
4
0.000557
290.18
298.3
377.5
15.3
15.0
5
0.000560
277.01
355.0
374.0
18.4
18.6
6
0.000566
285.77
266.7
367.3
20.5
20.7
7
0.000578
302.17
358.1
343.7
30.1
30.2
8
0.000580
272.26
211.7
333.6
18.2
18.1
9
0.000589
278.16
225.8
351.8
24.7
25.0
10
0.000594
279.54
258.5
360.1
19.1
17.9
11
0.000603
296.75
323.1
399.4
25.0
24.2
12
0.000612
280.83
291.5
394.7
16.9
17.5
13
0.000615
300.28
270.7
335.8
22.3
22.5
14
0.000620
275.00
225.0
340.0
19.6
25.1
15
0.000626
284.89
350.5
352.3
23.3
18.9
16
0.000627
287.60
244.0
382.5
14.4
18.2
17
0.000652
298.04
304.0
361.6
21.3
13.8
18
0.000657
294.24
330.6
375.5
36.1
29.1
19
0.000670
303.07
321.4
370.5
25.4
22.2
20
0.000680
313.28
259.1
350.0
22.9
21.6
21
0.000683
287.05
227.3
358.2
34.5
30.9
22
0.000689
272.70
260.9
355.4
14.8
13.1
23
0.000694
278.35
212.8
376.2
18.8
16.4
24
0.000698
277.52
299.4
338.4
32.9
31.1
25
0.000700
288.15
300.0
400.0
17.4
13.5
26
0.000711
292.26
273.3
392.5
7.5
7.0
27
0.000714
283.08
306.7
344.3
42.9
35.5
28
0.000730
285.51
217.7
383.9
22.0
20.9
29
0.000738
295.01
295.0
347.2
19.8
25.5
30
0.000741
270.95
275.2
356.9
4.5
10.2
31
0.000751
287.99
326.0
354.1
47.0
36.6
32
0.000757
300.64
235.0
391.7
25.8
27.2
159

Table 5.5: The Linear Cellular Alloys Example: prediction of δ(x)
run
ye
ym
δ
ˆδm
ˆδe
ˆδ
ˆδKO
(ye −ym)
(ym −ym)
(ˆyr
e −ym)
(Proposed
(ˆyr
KO −ym)
Method)
1
23.54
25.61
-2.0700
0
-8.7159
0.9291
2.9470
4
15.29
15.03
0.2600
0
3.6921
-1.1039
-0.3477
9
24.68
25.02
-0.3400
0
-7.6888
0.0524
-1.3082
11
24.96
24.20
0.7600
0
-7.9413
1.1079
2.5524
13
22.30
22.48
-0.1800
0
2.9055
-1.7799
-0.1964
23
18.78
16.40
2.3800
0
-1.2296
0.6742
-1.6294
25
17.41
13.54
3.8700
0
2.1570
1.1581
1.1535
27
42.93
35.53
7.4000
0
-2.9424
2.3681
3.8434
RMSE
3.1717
6.3059
2.4818
2.8678
Table 5.6: The Linear Cellular Alloys Example: prediction of Y r(x)
ˆyr
run
ye
ym
ˆyr
e
(Proposed
ˆyr
KO
Method)
1
23.54
25.61
16.8941
26.5391
28.5570
4
15.29
15.03
18.7221
13.9261
14.6823
9
24.68
25.02
17.3312
25.0724
23.7118
11
24.96
24.20
16.2587
25.3079
26.7524
13
22.30
22.48
25.3855
20.7001
22.2836
23
18.78
16.40
15.1704
17.0742
14.7706
25
17.41
13.54
15.6970
14.6981
14.6935
27
42.93
35.53
32.5876
37.8981
39.3734
RMSE
3.1717
6.3059
2.4818
2.8678
160

posterior of Y m(x), p(Y m(x)|ym, φm), which requires the estimate of φm. Notice that
we use φm and x instead of φm and x in boldface since this example has only one
input variable. The MMSPE estimate of φm is used for this example. That is, the
value of φm is chosen to minimize
RMSPE =
v
u
u
t 1
nm
nm
X
i=1
[ym(x′
i) −ˆym(x′
i)]2,
(5.47)
where x′
i, i = 1, · · · , nm are points in Dm, ˆym(x′
i) = E[Y m(x′
i)|ym
−i, φm] is a function
of φm (see equation (5.20b)), and ym
−i contains all available computer outputs except
ym(x′
i). In other words, ˆym(x′
i)’s are the leave-one-out cross validation predictions of
computer outputs at Dm. Figure 5.1 displays the RMSPE as a function of φm and
contains two curves, one for fm(x) ≡1 (i.e., the mean of the Gaussian process Y m(x)
is constant) and the other for fm(x) = (1, x)T (i.e., the mean of the Gaussian process
Y m(x) is a linear function of x). Figure 5.1 shows that assuming a linear mean for
Y m(x) leads to a smaller RMSPE, and the minimum RMSPE is achieved at ˆφm = 2.
Treating the MMSPE estimate of φm as its true value, we compute the posterior of
Y m(x) using equations (5.19) and (5.20) and display the posterior mean of Y m(x)
along with the corresponding 95% prediction interval in Figure 5.2, which shows a
rather small posterior variance except at the two ends of the Mc range. Figure 5.2
also displays physical observations as circles.
The next step is to ﬁnd the MMSPE estimates of φδ and τ together by minimizing
RMSPE =
v
u
u
t 1
ne
ne
X
i=1
h
ye(xi) −ˆym(xi) −ˆδ(xi)
i2
,
(5.48)
where xi, i = 1, · · · , ne are points in De, ˆym(xi) = E[Y m(xi)|ym, ˆφm] = µm|m(xi),
and ˆδ(xi) is the ten-fold cross validation prediction of δ(xi) and is a function of
φδ and τ. The results show that the RMSPE reaches minimum at ˆφδ = 1.4 and
ˆτ = 0.02 with the mean of the Gaussian process δ(x) assumed constant. Treating the
MMSPE estimates of φδ and τ as its true value, we compute the posterior of δ(x)
161

using equations (5.12) and (5.13) and display the posterior mean of δ(x) along with
the corresponding 95% prediction interval in Figure 5.3, which shows that the model
bias has a large absolute value but small variance when Mc ∈[0.5, 1.1], while a small
absolute value but large variance when Mc /∈[0.5, 1.1]. The reasons for such results
can be seen from Figure 5.2, which shows that
• The discrepancies between computer outputs and physical observation are large
when Mc ∈[0.5, 1.1] and small when Mc /∈[0.5, 1.1], which explains the absolute
value of the bias.
• There are more physical observations for Mc ∈[0.5, 1.1] than for Mc /∈[0.5, 1.1],
which explains the variance of the bias.
Having the posteriors of Y m(x) and δ(x), we calculate the prediction of Y r(x) using
equation (5.31). Figure 5.4 displays predictions of Y r(·) and 95% prediction intervals
along with physical observations as circles and computer outputs as upside triangles.
Similarly, the variance is smaller where more observations are available and larger
where less observations are available. Oberkampf and Barone (2004) ﬁtted a nonlinear
model to physical observations. Their results show that the variance is small for small
Mc and large for large Mc and approximately equals to zero for Mc ∈[0, 0.3]. This is
due to the nonlinear form they chose.
We use the results for δ(x) to illustrate the validation of the computer model.
The validation based on Y r(x) can be done similarly. For pure illustration purpose,
we set ∆0 = 0.12. For example, the 95% prediction interval for δ(0.65) is (-0.0083,
-0.1041). According to equations (5.45) and (5.46), we have ∆min(0.65) = 0.0083
and ∆max(0.65) = 0.1041. Since ∆max(0.65) < ∆0, we conclude that the computer
output at x = 0.65 is acceptable. Table 5.9 gives the validation decisions for x =
0.65 and another two points, x = 0.8 and x = 1.2. Same decisions can also made
by simply looking at Figure 5.3. Graphically speaking, with a 95% conﬁdence, we
162

accept the computer model at x if the 95% prediction interval for δ(x) is located
completely within the two boundaries δ(x) = −0.12 and δ(x) = 0.12; we reject
the computer model at x if the 95% prediction interval for δ(x) is complete located
completely outside the two boundaries; and we need more physical observations to
make a validation decision at x if the 95% prediction interval for δ(x) is partially
located within the two boundaries.
With the traditional approach, we reject the
computer model at x if the 95% prediction interval for δ(x) does not contain zero.
Therefore, we reject the computer model at x = 0.65 and x = 0.8 and fail to reject the
computer model at x = 1.2, which are diﬀerent from the validation decisions based
on ∆0.
To see if integrating computer outputs and physical observations together improves
the prediction of Y r(x), we repeat the above calculations with Y m(x) ≡0. In other
words, we use only physical observations now. The MMSPE estimates of φδ and τ
are found to be ˆφδ = 1.1 and ˆτ = 0.01 respectively. Comparing Figures 5.4 and
5.5 shows that, for this example, including computer outputs improve little on the
prediction of Y r(x). Such a result is due to the following facts: a) this example has
32 physical observations while only 11 computer outputs, b) De and Dm span the
same range, and c) the discrepancy between computation and experiment is obvious.
Therefore, the 11 computer outputs contain little additional information, and using
physical observations alone does a fairly good job in prediction. However, often we
have only few physical observations and relatively large number of computer outputs.
For a better comparison, we randomly partition the physical observations into two
parts, a training sample of size 20 and a testing sample of size 12.
We estimate
the parameters as above using the training sample and calculate the predictions for
the testing sample and the corresponding RMSPEs. Table 5.10 contains the average
RMSPEs from ten diﬀerent partitions, showing that using only physical observations
gives a RMSPE (0.0786) just slight larger than the RMSPE (0.0756) based on both
163

computer outputs and physical observations. Table 5.10 also contains the RMSPE
(0.0852) calculated using the approach by Kennedy and O’Hagan (2001). It is larger
than the RMSPEs from the proposed Bayesian approach using either only physical
observations or both physical observations and computer outputs. The predictions
based on only computer outputs have the largest RMSPE (0.1344), almost twice of
the others.
Table 5.7: The Compressible Shear Layer Example: physical observations
Source
Mc
ye
Source
Mc
ye
69
0.992
0.4640
71
0.945
0.4890
65,70
0.059
1.0000
72
0.510
0.9710
65,70
0.342
0.9780
72
0.640
0.7620
65,70
0.428
1.0000
73
0.860
0.5750
65,70
0.476
0.9810
74,75
0.206
0.9850
65,70
0.636
0.7520
74,75
0.455
0.8170
65,70
0.821
0.6010
74,75
0.691
0.5650
65,70
0.928
0.4600
74,75
0.720
0.6330
65,70
1.119
0.4530
76
0.795
0.5020
65,70
1.309
0.4220
74,75
0.862
0.4570
65,70
1.440
0.4400
74,75
0.985
0.4000
71
0.270
1.3500
77
0.525
1.0580
71
0.519
0.9570
77
0.535
0.8100
71
0.589
0.8120
78
0.580
0.9270
71
0.668
0.7330
77
0.640
0.8410
71
0.825
0.5350
78,79
1.040
0.5180
5.6.4
Sensitivity Study
In examples 1 and 2, we have estimated parameters φδ and τ by maximizing a like-
lihood function. For example, the ML estimates of φδ and τ in the ﬂuidized-bed
coating example are φδ = (0.1518, 0.2833, 1.0274, 0.3726, 0.0, 0.0)T and τ = 0.1677
respectively. We then plug in those estimates into equations (5.12) and (5.13) to get
the posterior distribution of the model bias δ(x). ML estimation involves solving
an optimization problem that has an objective function containing the determinant
164

Table 5.8: The Compressible Shear Layer Example: computer outputs
Mc
di
dc
ym = dc/di
0.100
0.0514
0.0514
1.0000
0.240
0.0711
0.0712
1.0014
0.380
0.0792
0.0760
0.9596
0.520
0.0836
0.0738
0.8828
0.660
0.0865
0.0690
0.7977
0.800
0.0837
0.0630
0.7527
0.940
0.0895
0.0568
0.6346
1.080
0.0898
0.0508
0.5657
1.220
0.0890
0.0455
0.5112
1.360
0.0863
0.0407
0.4716
1.500
0.0810
0.0367
0.4531
Table 5.9: The Compressible Shear Layer Example: model validation with ∆0 = 0.12
x = 0.65
x = 0.8
x = 1.2
95% PI for δ(x)
(-0.0083, -0.1041)
(-0.2582, -0.1491)
(-0.3145, 0.0047)
∆min(x)
0.0083
0.1491
0
∆max(x)
0.1041
0.2582
0.3145
Condition
∆max(0.65) < ∆0
∆min(0.8) > ∆0
∆min(1.2) < ∆0 < ∆max(1.2)
Decision
Accept
Reject
Uncertain
Table 5.10: The Compressible Shear Layer Example: RMSPEs
RMSPE
Partition
ˆyr
m = ˆym
ˆyr
e
ˆyr
ˆyr
KO
1
0.1363
0.0685
0.0669
0.0858
2
0.1314
0.0941
0.0892
0.0990
3
0.1423
0.0872
0.0801
0.1159
4
0.1260
0.0886
0.0852
0.0818
5
0.1478
0.0804
0.0866
0.0938
6
0.1451
0.0849
0.0774
0.0843
7
0.1200
0.0584
0.0542
0.0493
8
0.1179
0.0784
0.0797
0.0815
9
0.1477
0.0738
0.0773
0.0965
10
0.1297
0.0714
0.0624
0.0638
Average
0.1344
0.0786
0.0759
0.0852
165

1
2
3
4
5
6
7
8
9
10
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
RMSPE from leave−one−out cross validation
φm
Constant mean: fm(x)=1
Linear mean: fm(x)=(1,x)T
Figure 5.1: The Compressible Shear Layer Example: RMSPE as a function of φm.
0
0.5
1
1.5
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
y(Mc)
Mc
Physical Observations
Computer Outputs
Predictions of Ym (φm=2)
95% Prediction Intervals
Figure 5.2: The Compressible Shear Layer Example: prediction of Y m(x).
166

0
0.5
0.65
0.8
1
1.2
1.5
−0.2
−0.1
0
0.1
0.2
0.3
δ(Mc)
Mc
ye−ym
Predictions of δ (φδ=1.4, τ=0.02)
95% Prediction Intervals
(−∆0, ∆0), with ∆0=0.12
Figure 5.3: The Compressible Shear Layer Example: prediction of δ(x).
0
0.5
1
1.5
0.2
0.4
0.6
0.8
1
1.2
y(Mc)
Mc
Physical Observations
Computer Outputs
Predictions of Yr (φδ=1.4, τ=0.02)
95% Prediction Intervals
Figure 5.4: The Compressible Shear Layer Example: prediction of Y r(x) using both
computer outputs ym and physical observations ye.
167

0
0.5
1
1.5
0.2
0.4
0.6
0.8
1
1.2
y(Mc)
Mc
Physical Observations
Computer Outputs
Predictions of Yr (φδ=1.1, τ=0.01)
95% Prediction Intervals
Figure 5.5: The Compressible Shear Layer Example: prediction of Y r(x) using only
physical observations ye.
of a covariance matrix.
This leads to certain numerical diﬃculties, such as non-
convergence and locality, in solving the optimization problem. As a result, we either
can not get an optimal solution or have a local optimum. In this section, we study
how sensitive the predictions of δ(x) and therefore Y r(x) are to the values of φδ and
τ. The purpose is to see how important good estimates of parameters are to the
proposed Bayesian approach. If, for example, we ﬁnd out that the predictions of δ(x)
and Y r(x) are quite robust to the values of φδ and τ, then a local optimum might be
good enough for the purpose of prediction.
To study the sensitivity of the prediction of δ(x) to the values of φδ and τ, we use
the ﬂuidized-bed coating example and apply the proposed Bayesian approach with
diﬀerent values of φδ and τ. For simplicity, we assume that φδ,i = φδ, 1 ≤i ≤p,
that is, φδ = φδ · (1, · · · , 1)T. Same as in example 1, we reserve runs 4, 15, 17, 21,
23, 25, 26, and 28 as the testing data. We run the proposed Bayesian approach with
diﬀerent values of φδ and τ and compute the corresponding RMSPEs. The results are
displayed in Figures 5.6. Each boxplot in the upper panel of Figure 5.6 corresponds
168

to a speciﬁc value of φδ and is the boxplot of RMSPEs computed at diﬀerent values
of τ. Similarly, each boxplot in the lower panel of Figure 5.6 corresponds to a speciﬁc
value of τ and is the boxplot of RMSPEs computed at diﬀerent values of φδ.
The upper panel of Figure 5.6 shows that, given the value of φδ, the value of τ
generally has a decreasing eﬀect on RMSPE as the value of φδ increases (since the
length of the boxplot of RMSPE decreases with the increase of the value of φδ) and
has little eﬀect when the value of φδ is great than 3 (since boxplots for φδ > 3 have
a length close to zero). In other words, given the value of φδ, RMSPE becomes less
sensitive to the value of τ as the value of φδ increases and quite robust to the value of
τ when φδ > 3. The lower panel of Figure 5.6 shows that the boxplots of RMSPE for
diﬀerent values of φδ have around the same lengths and locations and are only sightly
shorter and higher for larger values of τ, which suggests that the value of τ has little
eﬀect on the sensitivity of RMSPE to the value of φδ. Also, the long boxplots in the
lower panel implies that, given the value of τ, RMSPE is sensitive to the value of
φδ. In addition, comparing the upper and lower panels shows that boxplots in the
lower panel have a greater length than those in the upper panel, which suggests that
RMSPE is more sensitive to the value of φδ than to the value of τ.
5.7
Appendix
5.7.1
Posterior Distributions of βm and σ2
m
The posteriors of βm and σ2
m are
βm|ye, ym, θ−(βm) = βm|ym, σ2
m, φm ∼N(Amvm, σ2
mAm)
(5.49)
and
σ2
m|ye, ym, θ−(βm,σ2m) = σ2
m|ym, φm
(5.50)
∼IG(αm + nm
2 , γm + 1
2

(ym)TR−1
m ym + bT
mV−1
m bm −vT
mAmvm

)
169

0.001 0.01 0.03 0.05 0.07 0.09
0.1
0.3
0.5
0.7
0.9
1
2
3
5
7
9
11
13
15
17
19
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
RMSPE
φδ
0
0.0001 0.001
0.01
0.03
0.05
0.07
0.09
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
RMSPE
τ
Figure 5.6: The Fluidized-Bed Coating Example: boxplots of RMSPEs
170

where
A−1
m = FT
mR−1
m Fm + V−1
m ,
(5.51a)
vm = FT
mR−1
m ym + V−1
m bm,
(5.51b)
and θ−(·) contains all the parameters except those in the parentheses.
5.7.2
Posterior Distributions of βδ and σ2
δ
Assume that computer outputs are available at the physical design set De.
The
posteriors of βδ and σ2
δ are
βδ|ye, ym, θ−(βδ) = βδ|ye, ym
ne, σ2
δ, φδ, σ2
ǫ ∼N(Aδvδ, σ2
δAδ)
(5.52)
and
σ2
δ|ye, ym, θ−(βδ,σ2
δ) = σ2
δ|ye, ym
ne, φδ, τ
(5.53)
∼IG(αδ + ne
2 , γδ + 1
2

(ye −ym
ne)T(Rδ(De) + τIne)−1(ye −ym
ne) + bδV−1
δ bδ −vT
δ Aδvδ

),
where
A−1
δ
= FT
δ (De)(Rδ(De) + τIne)−1Fδ(De) + V−1
δ ,
(5.54a)
vδ = FT
δ (De)(Rδ(De) + τIne)−1(ye −ym
ne) + V−1
δ bδ,
(5.54b)
τ = σ2
ǫ/σ2
δ,
(5.54c)
and Ine is an ne × ne identity matrix.
5.7.3
Posterior Distribution of δ(D)
Assuming that φδ and τ are known, we can rewrite the posterior of δ(D) in equation
(5.21) as
p(δ(D)|ye, ym)
(5.55)
=
ZZ
βδ,σ2
δ
p(δ(D)|ye, ym, βδ, σ2
δ, φδ, τ) · p(βδ|ye, ym
ne, σ2
δ, Rδ, τ) · p(σ2
δ|ye, ym
ne, Rδ, τ) dβδ dσ2
δ,
171

in which
p(δ(D)|ye, ym, βδ, σ2
δ, φδ, τ)
(5.56)
∝(σ2
δ)−n
2 · exp{−(δ(D) −Rδ(D, De)B(ye −ym
ne) −HT
δ βδ)T
· [Rδ(D) −Rδ(D, De)BRδ(De, D)]−1 · (δ(D) −Rδ(D, De)B(ye −ym
ne) −HT
δ βδ)},
p(βδ|ye, ym, σ2
δ, φδ, τ) ∝(σ2
δ)−p+1
2 · exp

−(βδ −Aδvδ)TA−1
δ (βδ −Aδvδ)
2σ2
δ

,
(5.57)
p(σ2
δ|ye, ym, φδ, τ)
(5.58)
∝(σ2
δ)−αδ−ne
2 −1 · exp
(
−γδ + 1
2

(ye −ym
ne)TB(ye −ym
ne) + bδV−1
δ bδ −vT
δ Aδvδ

σ2
δ
)
,
where B = (Rδ(De)+τIne)−1, and HT
δ = Fδ(D)−Rδ(D, De)BFδ(De). Collecting all
terms involving βδ together gives
exp

−1
2σ2
δ

βT
δ A−1βδ −2vTβδ

,
where
A−1 = A−1
δ
+ Hδ [Rδ(D) −Rδ(D, De)BRδ(De, D)]−1 HT
δ ,
v = vδ + Hδ [Rδ(D) −Rδ(D, De)BRδ(De, D)]−1 (δ(D) −Rδ(D, De)B(ye −ym
ne)).
Hence,
Z
βδ
exp

−1
2σ2
δ

βT
δ A−1βδ −2vTβδ

dβδ ∝(σ2
δ)
p+1
2 · exp
vTAv
2σ2
δ

.
Collecting all terms involving σ2
δ gives
(σ2
δ)−n
2 −αδ−ne
2 −1 · exp

−γ
σ2
δ

,
where
γ = γδ + (ye −ym
ne)TB(ye −ym
ne) + bδV−1
δ bδ
2
−vTAv
2
+1
2(δ(D) −Rδ(D, De)B(ye −ym
ne))T · [Rδ(D) −Rδ(D, De)BRδ(De, D)]−1
·(δ(D) −Rδ(D, De)B(ye −ym
ne)).
172

Performing the integration over σ2
δ yields
Z
σ2
δ
(σ2
δ)−n
2 −αδ−ne
2 −1 exp(−γ
σ2
δ
) dσ2
δ ∝γ−n+2αδ+ne
2
.
Therefore,
p(δ(D)|ye, ym) ∝γ−n+2αδ+ne
2
.
If we can write γ in the form of
γ = C ·
"
1 +
(δ(D) −µδ|e,m(D))T · Σ−1
δ|e,m(D) · (δ(D) −µδ|e,m(D))
2αδ + ne
#
,
where C is any constant, then
p(δ(D)|ye, ym) ∝
"
1 +
(δ(D) −µδ|e,m(D))T · Σ−1
δ|e,m(D) · (δ(D) −µδ|e,m(D))
2αδ + ne
#−n+2αδ+ne
2
,
which implies that δ(D)|ye, ym has a multivariate noncentral t distribution with
degree of freedom 2αδ + ne, noncentrality parameter µδ|e,m(D), and scale matrix
Σδ|e,m(D).
Since
A = Aδ −AδHδ

HT
δ AδHδ + Rδ(D) −Rδ(D, De)BRδ(De, D)
−1 HT
δ Aδ,
v = vδ + Hδ [Rδ(D) −Rδ(D, De)BRδ(De, D)]−1 (δ(D) −Rδ(D, De)B(ye −ym
ne)).
we have
γ = γδ + 1
2

(ye −ym
ne)TB(ye −ym
ne) + bδV−1
δ bδ −vT
δ Aδvδ

+1
2(δ(D) −Rδ(D, De)B(ye −ym
ne) −HT
δ Aδvδ)T
·

HT
δ AδHδ + Rδ(D) −Rδ(D, De)BRδ(De, D)
−1
·(δ(D) −Rδ(D, De)B(ye −ym
ne) −HT
δ Aδvδ).
173

Therefore,
µδ|e,m(D) = HT
δ Aδvδ + Rδ(D, De)B(ye −ym
ne)
Σδ|e,m(D) =
Q2
δ
2αδ + ne
· [HT
δ AδHδ + Rδ(D) −Rδ(D, De)BRδ(De, D)]
where
Q2
δ = 2γδ + (ye −ym
ne)TB(ye −ym
ne) + bδV−1
δ bδ −vT
δ Aδvδ
Substituting B and h into above equations for µδ|2 and σ2
δ|2, we have
µδ|2(x) = f T
δ (D)Aδvδ + rT
δ (D)(Rδ + τIne)−1(ye −ym
ne −FδAδvδ)
σ2
δ|2(x) =
Q2
2αδ + ne
·

1 −


fδ(D)
rδ(D)


T 

−V−1
δ
FT
δ
Fδ
Rδ + τIne


−1 

fδ(D)
rδ(D)



where
Q2 = 2γδ + (ye −ym
ne)T(Rδ + τIne)−1(ye −ym
ne) + bδV−1
δ bδ −vT
δ Aδvδ
5.7.4
Densities p(ym|φm) and p(ye|ym, φδ, τ)
The density of ym|φm is given by
p(ym|φm) =
ZZ
βm,σ2m
p(ym|βm, σ2
m, φm) · p(βm|σ2
m) · p(σ2
m) dβm dσ2
m.
(5.59)
Integrating out βm and σ2
m gives
p(ym|φm) ∝|Rm(Dm)|−1
2 · |Am|
1
2
(5.60)
·

2γm + (ym)TR−1
m (Dm)ym + bT
mV−1
m bm −vT
mAmvm
−nm
2 −αm
The density of bye|ym, φδ, σ2
ǫ is given by
p(ye|ym, φδ, τ)
=
ZZ
βδ,σ2
δ
p(ye|ym, βδ, σ2
δ, Rδ, τ) · p(βδ|σ2
δ) · p(σ2
δ) dβδ dσ2
δ.(5.61)
Integrating out βδ and σ2
δ gives
p(ye|ym, φδ, τ) ∝|Rδ(De) + τIne|−1
2 · |Aδ|
1
2
(5.62)
·

2γδ + (ye −ym
ne)T(Rδ(De) + τIne)−1(ye −ym
ne) + bT
δ V−1
δ bδ −vT
δ Aδvδ
−ne
2 −αδ
174

CHAPTER VI
BAYESIAN VALIDATION OF COMPUTER
MODELS: PERFORMANCE AND
GENERALIZATION
6.1
Introduction
In Chapter 5, we proposed a full Bayesian approach to the validation of computer
models.
The proposed approach is based on an assumed relationship among the
computer model output Y m(x), the physical observation Y e(x), and the real system
output Y r(x)
Y e(x) = Y r(x) + ǫ(x) = Y m(x) + δ(x) + ǫ(x),
(6.1)
in which the computer model output Y m(x) and the model bias δ(x) are assumed to
be two mutually independent Gaussian processes, and the measurement error ǫ(x) is
independently and normally distributed and independent of the two Gaussian pro-
cesses, Y m(x) and δ(x).
In this chapter, we continue the study of the proposed Bayesian approach. We
will focus on two areas:
(1). The performance of the proposed approach. By performance, we mean how
well the proposed approach predicts the real system output.
In Chapter 5,
we have illustrated the performance of the proposed Bayesian approach using
three real-life examples.
The investigation in this chapter is to understand
how the performance of the proposed approach is aﬀected by certain factors,
such as the variances of Gaussian processes Y m(x) and δ(x) and the number of
175

replications in physical experiments. Findings from such investigations could
provide valuable insights about how the proposed approach performs and when
the proposed approach performs well and why.
(2). A possible generalization to the proposed approach. The proposed approach
assumes that the two Gaussian processes Y m(x) and δ(x) are mutually inde-
pendent. This assumption simpliﬁes the derivation of the posterior distributions
of δ(x) and Y r(x). With this independence assumption, when De ⊆Dm (De
and Dm are the design sets of the physical and computer experiments respec-
tively), the posterior distributions of Y m(x) and δ(x) given physical observation
ye at De and computer outputs ym at Dm are also mutually independent. As
a result, the posterior of Y r(x) can be obtained nicely as the sum of two in-
dependent random process. However, the assumption of independence between
Y m(x) and δ(x) may not hold in reality. For example, it is possible that the
model bias δ(x) is large where the computer output Y m(x) is large and small
where Y m(x) is small. In this chapter, we explore the posterior behaviors of
δ(x) and Y r(x) given ym and ye when Y m(x) and δ(x) in equation (6.1) are
assumed to be correlated.
This chapter is organized as follows. Section 6.2 investigates the performance of
the proposed approach under diﬀerent situations, such as diﬀerent number of replica-
tions in physical experiments. Section 6.3 derives the posterior distributions of δ(x)
and Y r(x) given ym and ye when Y m(x) and δ(x) are assumed to be correlated.
6.2
Performance of The Proposed Bayesian Ap-
proach
The performance investigation is to understand how the performance of the proposed
approach is aﬀected by three factors, the number of replications in physical experi-
ments, the variance of the model bias δ(x), and the variance of the computer model
176

Y m(x). Through this investigation, we hope to have a better understanding on when
the proposed approach performs well and why. Such information could be useful in
choosing computer and physical designs for computer model validation.
6.2.1
Number of Replications in Physical Experiments
Example 1:
We use a simulated example to investigate the inﬂuences of the number
of replications in physical experiments on the prediction of the real system output
Y r(x). This example has one input variable x taking values in the interval [0, 10].
The model bias δ(x) shown in Figure 6.1 is generated as one realization of a Gaussian
process with µδ(x) = 0.2x, σ2
δ = 1, φδ = 1, and Pδ = 2.
The computer model
Y m(x) shown in Figure 6.2 is generated as one realization of a Gaussian process with
µm(x) = 10, σ2
m = 1, φm = 2, and Pm = 2. The real system output Y r(x) is calculated
as the sum of Y m(x) and δ(x).
The Y m(x), δ(x), and Y r(x) generated above are the true computer model, model
bias, and system output, which we will predict based on data collected from computer
and physical experiments.
We now simulate computer and physical experiments,
that is, determine computer design set Dm, computer outputs ym at Dm, physical
design set De, and physical observations ye at De. The computer design is given in
Table 6.1 with design set Dm = {xm
1 , · · · , xm
20}. At each xm
i
∈Dm, i = 1, · · · , 20,
we compute the corresponding computer output ym(xm
i ) as Y m(xm
i ), the output of
the generated computer model. We then have the vector of computer outputs ym =
(ym(xm
1 ), · · · , ym(xm
20))T. The physical design is given in Table 6.2 with design set
De = {xe
1, · · · , xe
7}. At each xe
i ∈De, i = 1, · · · , 7, we compute the corresponding
physical observation ye(xe
ij) as Y r(xe
i)(= Y m(xe
i) + δ(xe
i) ) plus an error term ǫ(xe
ij)
generated from a normal distribution with mean zero and variance σ2
ǫ = 1, where
j = 1, · · · , J, and J is the number of replications. Therefore, the vector of physical
observations ye = (ye(xe
11), · · · , ye(xe
1J), · · · , ye(xe
71), · · · , ye(xe
7J))T. In our study, the
177

number of replications J is set to be 1, 2, 5, 10, and 20. Figure 6.3 illustrates computer
outputs ym and physical observations ye together with the computer model Y m(x)
and the real system output Y r(x) for the number of replications J = 10.
We use Dm, ym, De, ye as the training data to estimate the posterior distributions
of δ(x), Y m(x), and Y r(x), and then predict the real system output using its estimated
posterior mean. Table 6.3 gives the RMSPEs of the predictions of Y r(x) at 201 x
values from 0 to 10 with an increment 0.05. In this table, ˆY r
m, ˆY r
e , and ˆY r denote
the predictions of Y r(x) based on only ym, only ye, and both ym and ye respectively.
Clearly, using both ym and ye leads to smaller RMSPEs than using either only ye
or ym. Furthermore, the RMSPEs based on ye (considering ym or not) decrease as
the number of replications J increases. Figures 6.4 – 6.8 display the predictions of
Y r(x) based on both ym and ye and the corresponding 95% conﬁdence intervals for
J = 1, 2, 5, 10, and 20 respectively. Also shown in those ﬁgures are the real system
output Y r(x) and the predictions of Y r(x) based on only ye. From Figures 6.4 –
6.8, it is clear that the predictions based on both ym and ye are closer to Y r(x)
than those based on only ye. Furthermore, the 95% conﬁdence intervals for Y r(x)
become narrower as the number of replications increases especially at those points
where physical observations are available (i.e., those points in the physical design set
De). This can be seen more clearly from Figure 6.9, which shows that the estimated
variances of Y r(x) decrease as the number of replications increases and decrease faster
at those points in De. Table 6.4 gives the estimated variances of Y r(x) at De, showing
that the estimated variances of Y r(x) at De approach to σ2
ǫ/J, the variance of the
experimental error ǫ(x) over the number of replications J. In addition, Figures 6.4
– 6.8 show that the choice of the physical design set De aﬀects the accuracy of
predictions.
The predictions of Y r(x) are closer to the corresponding true values
over the region (say x > 3) where design points are chosen to capture the major
characteristics (such as maxima and minima) of Y r(x). Table 6.5 gives the estimates
178

of σ2
ǫ based on only ye and both ym and ye. This table shows that the estimates of
σ2
ǫ approach to its true value 1 as the number of replications increases. Moreover,
using both ym and ye leads to a more accurate estimate of σ2
ǫ than using only ye
although the diﬀerence between the two estimates becomes smaller as the number of
replications gets larger.
The last column of Table 6.3 gives the RMSPEs of the predictions of Y m(x) at 201
x points using computer outputs ym. The small RMSPEs imply that, with computer
outputs at the chosen 20 computer design points, we can predict computer outputs
fairly well.
This can also be seen from Figure 6.10 showing that the predictions
of Y m(x) are fairly close to the corresponding true computer outputs and the 95%
conﬁdence intervals for Y m(x) are rather small over almost the entire input region
except in a small neighborhood of x = 0. Figure 6.11 displays the estimated variances
of Y m(x) that equal to zero at points in Dm and are very small (less than 0.065) at
other points except in a small neighborhood of x = 0.
We also run the proposed Bayesian procedure using the mean of physical obser-
vations at each design point xe
i in De instead of all replications. Tables 6.6 and 6.7
give the RMSPEs of the predictions of Y r(x) at 201 x values and the estimates of σ2
ǫ
respectively. The RMSPEs are slightly larger than the RMSPEs based on all replica-
tions in Table 6.3, while the estimates of σ2
ǫ are far worse than the estimates based on
all replications in Table 6.5. In other words, using the means of physical observations
leads to a slightly less accurate prediction of Y r(x) in terms of RMSPE while a much
worse estimate of σ2
ǫ. This can be explained by the fact that the means do not retain
all information contained in physical observations especially the information on the
variability of physical observations. Figures 6.12 and 6.13 compare the predictions
and estimated variances of Y r(x) based on the means to those based on all replica-
tions for J = 20. Figure 6.12 suggests that the predictions based on the means are
quite close to those based on all replications while Figure 6.13 shows an interesting
179

phenomenon that the estimated variances based on the means are greater than those
based on all replications at points close to physical design points while smaller at
points further away from physical design points.
Table 6.1: Computer Outputs at Dm = {xm
1 , · · · , xm
20}
Run i
xm
i
ym(xm
i )
Run i
xm
i
ym(xm
i )
1
0.45
9.3162
11
5.45
11.8907
2
0.95
10.0691
12
5.95
11.9983
3
1.45
9.8351
13
6.45
11.1016
4
1.95
10.2033
14
6.95
10.7167
5
2.45
11.0212
15
7.45
10.0192
6
2.95
11.4428
16
7.95
8.6365
7
3.45
8.9847
17
8.45
9.2830
8
3.95
8.8907
18
8.95
9.7437
9
4.45
9.1616
19
9.45
9.5979
10
4.95
10.4087
20
9.95
9.5222
Table 6.2: Physical Observations at De = {xe
1, · · · , xe
7} for J = 1
Run i
xe
i
ye(xe
i)
1
0.45
12.3043
2
1.95
9.0834
3
3.45
12.3266
4
4.95
12.1057
5
6.45
14.1211
6
7.95
8.1999
7
9.45
12.0574
180

Table 6.3: RMSPEs of Predictions of Y r(x) or Y m(x) at 201 x values (from 0 to 10
with an increment 0.05)
RMSPE
J
ˆY r
m = E[Y m|ym]
ˆY r
e = E[Y r|ye]
ˆY r = E[Y r|ye, ym]
ˆY m
m = E[Y m|ym]
1
1.6354
1.5111
0.9451
0.1071
2
1.6354
1.5185
0.8231
0.1071
5
1.6354
1.3736
0.7293
0.1071
10
1.6354
1.2790
0.7129
0.1071
20
1.6354
1.2350
0.7017
0.1071
Table 6.4: Estimated Var(Y r(xe
i)|ye, ym) at xe
i ∈De, i = 1, 2, · · · , 7
ˆ
Var(Y r(xe
i)|ye, ym)
J
σ2
ǫ/J
1
2
3
4
5
6
7
1
1.00
1.7892
1.2181
0.8755
0.7612
0.8755
1.2181
1.7892
2
0.50
0.6031
0.4623
0.3778
0.3497
0.3778
0.4623
0.6031
5
0.20
0.2375
0.2109
0.1949
0.1896
0.1949
0.2109
0.2375
10
0.10
0.1088
0.1028
0.0993
0.0981
0.0993
0.1028
0.1088
20
0.05
0.0537
0.0522
0.0513
0.0510
0.0513
0.0522
0.0537
Table 6.5: Estimated Experimental Error Variance σ2
ǫ
Using only ye
Using both ye and ym
J
ˆσ2
r
ˆτ
ˆσ2
ǫ
ˆσ2
m
ˆσ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
(ˆτ · ˆσ2
r)
(ˆσ2
m + ˆσ2
δ)
(ˆτ · ˆσ2
δ)
1
0.4074
14.3467
5.8449
0.9322
0.4074
1.3396
10.5076
4.2809
2
0.3748
7.5679
2.8364
0.9322
0.3468
1.2790
6.6522
2.3072
5
0.9059
1.9197
1.7391
0.9322
0.3772
1.3094
4.4357
1.6733
10
1.0508
1.2459
1.3092
0.9322
0.3647
1.2969
3.5680
1.3014
20
0.9676
1.2178
1.1783
0.9322
0.3606
1.2927
3.2637
1.1768
181

Table 6.6: RMSPEs (using the means of physical observations at De)
RMSPE
J
ˆY r
m = E[Y m|ym]
ˆY r
e = E[Y r|ye]
ˆY r = E[Y r|ye, ym]
ˆY m
m = E[Y m|ym]
1
1.6354
1.5111
0.9451
0.1071
2
1.6354
1.5214
0.8158
0.1071
5
1.6354
1.4833
0.7294
0.1071
10
1.6354
1.4632
0.7136
0.1071
20
1.6354
1.4287
0.7047
0.1071
Table 6.7: Estimated Experimental Error Variance σ2
ǫ (using the means of physical
observations at De)
Using only ye
Using both ye and ym
J
ˆσ2
r
ˆτ
ˆσ2
ǫ
ˆσ2
m
ˆσ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
(ˆτ · ˆσ2
r · J)
(ˆσ2
m + ˆσ2
δ)
(ˆτ · ˆσ2
δ · J)
1
0.4074
14.3467
5.8449
0.9322
0.4074
1.3396
10.5076
4.2809
2
0.4074
3.7662
3.0688
0.9322
0.4074
1.3396
1.3142
1.0709
5
0.4074
6.1959
12.6213
0.9322
0.4074
1.3396
1.3412
2.7320
10
0.4074
5.5868
22.7610
0.9322
0.4074
1.3396
0.5874
2.3931
20
0.4074
4.5522
37.0915
0.9322
0.4074
1.3396
0.3490
2.8433
0
1
2
3
4
5
6
7
8
9
10
−1
−0.5
0
0.5
1
1.5
2
2.5
3
x
δ(x)
Figure 6.1: Model Bias δ(x) – one realization of the Gaussian process with µδ(x) =
0.2x, σ2
δ = 1, φδ = 1, and Pδ = 2
182

0
1
2
3
4
5
6
7
8
9
10
8
9
10
11
12
13
14
15
x
y(x)
Computer Model Ym(⋅)
Real System Output Yr(⋅)
Figure 6.2: Computer Model Y m(x) – one realization of the Gaussian process with
µm(x) = 10, σ2
m = 1, φm = 2, and Pm = 2; Real System Output Y r(x) = Y m(x)+δ(x)
0
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
13
14
15
16
x
y
Number of Replications=10
Ym(x)
Yr(x)
 ym
 ye
Figure 6.3: Physical Observations ye and Computer Outputs ye for J = 10
183

0
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
13
14
15
16
x
y(x)
Number of Replications=1
 ye
Yr(x)
E[Yr(x)| ye]
E[Yr(x)| ye, ym]
95% CIs
Figure 6.4: Predictions of Y r(x) for J = 1
0
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
13
14
15
16
x
y(x)
Number of Replications=2
 ye
Yr(x)
E[Yr(x)| ye]
E[Yr(x)| ye, ym]
95% CIs
Figure 6.5: Predictions of Y r(x) for J = 2
184

0
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
13
14
15
16
x
y(x)
Number of Replications=5
 ye
Yr(x)
E[Yr(x)| ye]
E[Yr(x)| ye, ym]
95% CIs
Figure 6.6: Predictions of Y r(x) for J = 5
0
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
13
14
15
16
x
y(x)
Number of Replications=10
 ye
Yr(x)
E[Yr(x)| ye]
E[Yr(x)| ye, ym]
95% CIs
Figure 6.7: Predictions of Y r(x) for J = 10
185

0
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
13
14
15
16
x
y(x)
Number of Replications=20
 ye
Yr(x)
E[Yr(x)| ye]
E[Yr(x)| ye, ym]
95% CIs
Figure 6.8: Predictions of Y r(x) for J = 20
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
2.5
3
x
Estimated Var(Yr(x)| ye, ym)
 1
 2
 5
10
20
Figure 6.9: Estimated Var(Y r(x)|ye, ym)
186

0
1
2
3
4
5
6
7
8
9
10
8
8.5
9
9.5
10
10.5
11
11.5
12
12.5
13
x
y(x)
 ym
Ym(x)
E[Ym(x)| ym]
95% CIs
Figure 6.10: Predictions of Y m(x)
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
x
Estimated Var(Ym(x)| ym)
Figure 6.11: Estimated Var(Y m(x)|ym)
187

0
1
2
3
4
5
6
7
8
9
10
8
9
10
11
12
13
14
15
x
y(x)
Yr(x)
Using All Replications
Using Means
Figure 6.12: Predictions of Y r(x)
6.2.2
Variance of Model Bias δ(x), σ2
δ
Example 2:
Example 2 is diﬀerent from Example 1 in only one aspect: the model
bias δ(x) is generated as one realization of a Gaussian process with µδ(x) = 0.2x,
φδ = 1, Pδ = 2, and σ2
δ = [0.01, 0.2, 0.5, 1, 2, 5, 10, 20]. Figure 6.14 displays model
biases for diﬀerent values of σ2
δ. The purpose here is to study the eﬀects of the value
of σ2
δ on the prediction of Y r(x) and the estimation of σ2
ǫ. We run the proposed
Bayesian procedure with the number of replication J as two and ﬁve.
Table 6.8 contains the RMSPEs of the predictions of Y r(x) at 201 x values from
0 to 10 with an increment 0.05. The RMSPEs considering ye are smaller for a larger
number of replications. Moreover, the RMSPEs based on both ym and ye are smaller
than those based on only ye or ym. Those ﬁndings are consistent with the results
obtained in Example 1. Table 6.8 also shows that the smaller the value of σ2
δ, the
more accurate the predictions of Y r(x) in terms of RMSPE. Such a result is expected,
and the reason is given as follows. As shown in Figure 6.14, the curvature of δ(x)
increases as the value of σ2
δ increases from 0.01 to 20. Therefore, the curvature of the
188

0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
Estimated Var(Yr(x)| ye, ym)
0.05
Using All Replications
Using Means
Figure 6.13: Estimated Var(Y r(x)|ye, ym)
real system output Y r(x), dominated by the computer model Y m(x) when σ2
δ = 0.01
(see Figure 6.16), becomes more and more dominated by the model bias δ(x) as the
value of σ2
δ increases (see Figure 6.17). Since, with the same training data, a curve (or
a surface) having higher curvature is more diﬃcult to estimate than one having lower
curvature, we predict δ(x) therefore Y r(x) better when the value of σ2
δ is smaller.
Table 6.9 and Figure 6.15 display the estimates of σ2
ǫ for diﬀerent values of σ2
δ.
Both suggest that the estimates of σ2
ǫ are closer to its true value 1 when the number of
replications is larger and/or when both ym and ye are considered, which is consistent
with the results obtained in Example 1. Moreover, with the number of replications as
two, the estimate of σ2
ǫ experiences a very small increase as the value of σ2
δ changes
from 0.01 to 1 while increases rapidly with the value of σ2
δ when it is greater than
1; also, the smaller the value of σ2
δ, the closer the estimate of σ2
ǫ to its true value 1.
With the number of replications as ﬁve, the estimate of σ2
ǫ experiences only a small
change as the value of σ2
δ increases from 0.01 to 20. In summary, the estimate of σ2
ǫ
is quite robust to the value of σ2
δ when the number of replication is large or the value
of σ2
δ is small.
189

Table 6.8: RMSPEs of Predictions of Y r(x) or Y m(x) at 201 x values (from 0 to 10
with an increment 0.05)
Number of Replications = 2
RMSPE
σ2
δ
τ
ˆY r
m = E[Y m|ym]
ˆY r
e = E[Y r|ye]
ˆY r = E[Y r|ye, ym]
ˆY m
m = E[Y m|ym]
0.01
100
1.1865
1.1953
0.3721
0.1071
0.2
5
1.3165
1.3131
0.4903
0.1071
0.5
2
1.4496
1.3744
0.6336
0.1071
1
1
1.6260
1.5185
0.8231
0.1071
2
0.5
1.9084
1.7623
1.1153
0.1071
5
0.2
2.5346
2.3098
1.7239
0.1071
10
0.1
3.2919
2.9626
2.4402
0.1071
20
0.05
4.4031
4.0117
3.4854
0.1071
Number of Replications = 5
RMSPE
σ2
δ
τ
ˆY r
m = E[Y m|ym]
ˆY r
e = E[Y r|ye]
ˆY r = E[Y r|ye, ym]
ˆY m
m = E[Y m|ym]
0.01
100
1.1865
1.0265
0.2360
0.1071
0.2
5
1.3165
1.1104
0.3690
0.1071
0.5
2
1.4496
1.2053
0.5276
0.1071
1
1
1.6260
1.3736
0.7293
0.1071
2
0.5
1.9084
1.5882
1.0054
0.1071
5
0.2
2.5346
2.3206
1.7025
0.1071
10
0.1
3.2919
2.9847
2.2159
0.1071
20
0.05
4.4031
3.9613
3.1814
0.1071
190

Table 6.9: Estimated Experimental Error Variance σ2
ǫ
Number of Replications = 2
Using only ye
Using both ye and ym
σ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
ˆσ2
m
ˆσ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
(ˆτ · ˆσ2
r)
(ˆσ2
m + ˆσ2
δ)
(ˆτ · ˆσ2
δ)
0.01
0.3750
5.8942
2.2103
0.9322
0.3750
1.3072
5.6856
2.1321
0.2
0.3750
6.6480
2.4930
0.9322
0.3362
1.2684
6.4156
2.1569
0.5
0.3624
7.0904
2.5694
0.9322
0.3399
1.2721
6.4930
2.2068
1
0.3748
7.5679
2.8364
0.9322
0.3468
1.2790
6.6522
2.3072
2
0.3906
8.5170
3.3269
0.9322
0.3605
1.2927
7.0298
2.5342
5
0.4079
11.6682
4.7591
0.9322
0.3907
1.3229
8.5220
3.3294
10
0.4077
17.5744
7.1645
0.9322
0.4084
1.3406
12.0365
4.9158
20
0.1532
305.1512
46.7497
0.9322
0.4048
1.3370
21.1517
8.5631
Number of Replications = 5
Using only ye
Using both ye and ym
σ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
ˆσ2
m
ˆσ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
(ˆτ · ˆσ2
r)
(ˆσ2
m + ˆσ2
δ)
(ˆτ · ˆσ2
δ)
0.01
0.4346
3.9268
1.7065
0.9322
0.3514
1.2835
4.3376
1.5240
0.2
0.5582
3.1126
1.7374
0.9322
0.2960
1.2282
5.3494
1.5837
0.5
0.6970
2.5027
1.7443
0.9322
0.3254
1.2575
4.9905
1.6238
1
0.9059
1.9197
1.7391
0.9322
0.3772
1.3094
4.4357
1.6733
2
1.2882
1.3378
1.7234
0.9322
0.5059
1.4381
3.4172
1.7289
5
2.2980
0.7385
1.6970
0.9322
1.0779
2.0100
1.6068
1.7319
10
3.7936
0.4430
1.6804
0.9322
2.1840
3.1162
0.7779
1.6990
20
3.9868
0.5412
2.1575
0.9322
4.4016
5.3338
0.3809
1.6767
191

0
1
2
3
4
5
6
7
8
9
10
−6
−4
−2
0
2
4
6
8
10
0.01
 0.2
 0.5
   1
   2
   5
  10
  20
x
δ(x)
Figure 6.14: Model bias δ(x) – one realization of a Gaussian process with µδ(x) =
0.2x, φδ = 1, Pδ = 2, and σ2
δ = [0.01, 0.2, 0.5, 1, 2, 5, 10, 20]
−5
−4
−3
−2
−1
0
1
2
3
1
2
3
4
5
6
7
8
9
10
log(σ2
δ)
Estimated σ2
ε
Using only  ye (2 Replications)
Using both  ym and  ye (2 Replications)
Using only  ye (5 Replications)
Using both  ym and  ye (5 Replications)
Figure 6.15: Estimated σ2
ǫ versus log(σ2
δ)
192

0
1
2
3
4
5
6
7
8
9
10
0
2
4
6
8
10
12
14
x
y(x)
σ2
m=1, σ2
δ=0.01
Yr(x)
Ym(x)
δ(x)
Figure 6.16: Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 1 and σ2
δ = 0.01
0
1
2
3
4
5
6
7
8
9
10
−10
−5
0
5
10
15
20
25
x
y(x)
σ2
m=1, σ2
δ=20
Yr(x)
Ym(x)
δ(x)
Figure 6.17: Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 1 and σ2
δ = 20
193

6.2.3
Variance of Computer Model Y m(x), σ2
m
Example 3:
Similar to Example 2, Example 3 is diﬀerent from Example 1 in only
one aspect: the computer model Y m(x) is generated as one realization of a Gaussian
process with µm(x) = 10, φm = 2, Pm = 2, and σ2
m = [0.01, 0.2, 0.5, 1, 2, 5, 10, 20].
Figure 6.18 displays computer models for diﬀerent values of σ2
m. The purpose here is
to study the eﬀects of the value of σ2
m on the prediction of Y r(x) and the estimation
of σ2
ǫ. We run the proposed Bayesian procedure with the number of replication as
two and ﬁve. Results are shown in Tables 6.10 and 6.11 and Figure 6.19. They once
again conﬁrm the ﬁnding that, when the number of replications is larger and both
computer outputs ym and physical observations ye are considered, we obtain a more
accurate prediction of Y r(x) and estimate of σ2
ǫ.
Tables 6.10 shows that the increase in the value of σ2
m causes only a slight change
in RMSPEs for Y r(x) as long as ym are considered (see the ˆY r
m and ˆY r columns). Such
a result can be attributed to the fact that there are twenty points in the computer
design set Dm. Therefore, we can predict the computer model Y m(x) fairly well using
ym no matter how large the value of σ2
m is (see the ˆY m
m column. The RMSPEs of the
predictions of Y m(x) are small). This conjecture is reinforced by the fact that the
RMSPEs in the ˆY r
m column are close to the square root of the mean squared bias,
which equals to
qP201
i=1 δ2(xi)/201 = 1.6354. On the other hand, the same increase in
the value of σ2
m leads to a quick rise in RMSPE based on only physical observations ye
(see the ˆY r
e column). This can be explained by the fact that, as shown in Figure 6.18,
the curvature of the computer model Y m(x) increases as the value of σ2
m increases.
As a result, the curvature of the real system output Y r(x), dominated by the model
bias δ(x) when σ2
m = 0.01 (see Figure 6.20), become more and more dominated by
the computer model Y m(x) as the value of σ2
m increases (see Figure 6.21). Therefore,
although we can predict Y r(x) well using only ye when σ2
m is small and the curvature
of Y r(x) is dominated by the model bias δ(x), ye become less capable to capture the
194

curvature of Y r(x) as σ2
m increases, and then leads to a much larger RMSPE. Both
Table 6.11 and Figure 6.19 show that a larger number of replications leads to a more
accurate estimate of σ2
ǫ. Moreover, the value of σ2
m has no inﬂuence in the estimate
of σ2
ǫ when both ym and ye are considered and little inﬂuence in the estimate of σ2
ǫ
when only ye are considered.
Comparing Table 6.10 with Table 6.8 reveals that the RMSPEs based on both
ym and ye seems more sensitive to the value of σ2
δ than to the value of σ2
m while the
RMSPEs. This might be due to the fact that there are twenty design points in Dm
while only seven design points in De.
Table 6.10: RMSPEs of Predictions of Y r(x) or Y m(x) at 201 x values (from 0 to
10 with an increment 0.05)
Number of Replications = 2
RMSPE
σ2
m
ˆY r
m = E[Y m|ym]
ˆY r
e = E[Y r|ye]
ˆY r = E[Y r|ye, ym]
ˆY m
m = E[Y m|ym]
0.01
1.6353
0.8684
0.8259
0.0160
0.2
1.6310
1.0799
0.8218
0.0435
0.5
1.6283
1.2787
0.8214
0.0732
1
1.6260
1.5185
0.8231
0.1071
2
1.6240
1.8945
0.8271
0.1542
5
1.6240
2.6941
0.8417
0.2467
10
1.6304
3.6428
0.8705
0.3502
20
1.6504
4.3510
0.9292
0.4963
Number of Replications = 5
RMSPE
σ2
m
ˆY r
m = E[Y m|ym]
ˆY r
e = E[Y r|ye]
ˆY r = E[Y r|ye, ym]
ˆY m
m = E[Y m|ym]
0.01
1.6353
0.7634
0.7394
0.0160
0.2
1.6310
0.9528
0.7307
0.0435
0.5
1.6283
1.1211
0.7243
0.0732
1
1.6260
1.3736
0.7293
0.1071
2
1.6240
1.7041
0.7335
0.1542
5
1.6240
2.3145
0.7458
0.2467
10
1.6304
3.0627
0.7806
0.3502
20
1.6504
4.2062
0.8361
0.4963
195

Table 6.11: Estimated Experimental Error Variance σ2
ǫ
Number of Replications = 2
Using only ye
Using both ye and ym
σ2
m
ˆσ2
r
ˆτ
ˆσ2
ǫ
ˆσ2
m
ˆσ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
(ˆτ · ˆσ2
r)
(ˆσ2
m + ˆσ2
δ)
(ˆτ · ˆσ2
δ)
0.01
0.3469
6.6529
2.3076
0.4283
0.3468
0.7752
6.6522
2.3072
0.2
0.3525
6.7974
2.3962
0.3185
0.3468
0.6653
6.6522
2.3072
0.5
0.3616
7.0646
2.5545
0.5404
0.3468
0.8872
6.6522
2.3072
1
0.3748
7.5679
2.8364
0.9322
0.3468
1.2790
6.6522
2.3072
2
0.3935
8.7688
3.4503
1.7233
0.3468
2.0701
6.6522
2.3072
5
0.4094
13.6358
5.5825
4.1024
0.3468
4.4492
6.6522
2.3072
10
0.4029
23.7101
9.5517
8.0695
0.3468
8.4163
6.6522
2.3072
20
6.6490
0.5494
3.6530
16.0045
0.3468
16.3513
6.6522
2.3072
Number of Replications = 5
Using only ye
Using both ye and ym
σ2
m
ˆσ2
r
ˆτ
ˆσ2
ǫ
ˆσ2
m
ˆσ2
δ
ˆσ2
r
ˆτ
ˆσ2
ǫ
(ˆτ · ˆσ2
r)
(ˆσ2
m + ˆσ2
δ)
(ˆτ · ˆσ2
δ)
0.01
0.3941
4.2754
1.6850
0.4283
0.3772
0.8056
4.4357
1.6733
0.2
0.4987
3.4636
1.7273
0.3185
0.3772
0.6957
4.4357
1.6733
0.5
0.6457
2.7002
1.7435
0.5404
0.3772
0.9176
4.4357
1.6733
1
0.9059
1.9197
1.7391
0.9322
0.3772
1.3094
4.4357
1.6733
2
1.4413
1.1918
1.7179
1.7233
0.3772
2.1005
4.4357
1.6733
5
2.9949
0.5634
1.6875
4.1024
0.3772
4.4796
4.4357
1.6733
10
5.4633
0.3060
1.6720
8.0695
0.3772
8.4467
4.4357
1.6733
20
10.2244
0.1626
1.6627
16.0045
0.3772
16.3817
4.4357
1.6733
196

0
1
2
3
4
5
6
7
8
9
10
4
6
8
10
12
14
16
18
20
0.01
 0.2
 0.5
   1
   2
   5
  10
  20
x
Ym(x)
Figure 6.18: Computer Model Y m(x) – one realization of a Gaussian process with
µm(x) = 10, φm = 2, Pm = 2, and σ2
m = [0.01, 0.2, 0.5, 1, 2, 5, 10, 20]
−5
−4
−3
−2
−1
0
1
2
3
1
2
3
4
5
6
7
8
9
10
log(σ2
m)
Estimated σ2
ε
Using only  ye (2 Replications)
Using both  ym and  ye (2 Replications)
Using only  ye (5 Replications)
Using both  ym and  ye (5 Replications)
Figure 6.19: Estimated σ2
ǫ versus log(σ2
m)
197

0
1
2
3
4
5
6
7
8
9
10
−2
0
2
4
6
8
10
12
14
x
y(x)
σ2
m=0.01, σ2
δ=1
Yr(x)
Ym(x)
δ(x)
Figure 6.20: Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 0.01 and σ2
δ = 1
0
1
2
3
4
5
6
7
8
9
10
−5
0
5
10
15
20
25
x
y(x)
σ2
m=20, σ2
δ=1
Yr(x)
Ym(x)
δ(x)
Figure 6.21: Real System Output Y r(x), Computer Model Y m(x), and Model Bias
δ(x) for σ2
m = 20 and σ2
δ = 1
198

6.2.4
Conclusions
We have used three simulated numerical examples to investigate the performance
of the proposed Bayesian approach and study the inﬂuences of three factors, the
number of replications in physical experiments, the variance of δ(x), and the variance
of Y m(x). From the results obtained in the three examples, we can draw the following
conclusions on the proposed approach:
• The choices of design sets Dm and De are crucial to the performance of the
proposed approach.
A design set is determined by its size (i.e., the number of points in the design
set) and location (i.e., the locations of points in the design set). Since computer
outputs are usually less expensive and time-consuming compared to physical
observations, we can have a relatively large computer design set Dm. For exam-
ple, in all three examples, we have twenty points in the computer design set Dm
while only seven points in the physical design set De. The locations of design
points should be chosen such that the major characteristics of the computer
model Y m(x) and the real system output Y r(x) are captured. For example, in
Example 1, physical observations in the region of x > 3 capture the maxima
and minima of Y r(x). As a result, the predictions of Y r(x) in this region are
closer to the corresponding true values.
• As the number of replications increases, the proposed approach performs better.
All three examples show that a larger number of replications leads to a more
accurate and stable prediction of Y r(x) and estimate of σ2
ǫ.
• The values of σ2
δ and σ2
m aﬀect the behavior of Y r(x) and therefore aﬀect the
performance of the proposed approach.
When σ2
m ≫σ2
δ, the curvature of Y r(x) is dominated by the computer model
Y m(x) (see Figures 6.16 and 6.21). In other words, when σ2
m ≫σ2
δ, the computer
199

model Y m(x) might not be an accurate representation of the real system Y r(x),
but it captures the shape of Y r(x). When σ2
m ≪σ2
δ, the curvature of Y r(x) is
dominated by the model bias δ(x) (see Figures 6.17 and 6.20), and the computer
model Y m(x) does not capture the behavior of Y r(x). As a result, since we
often have a large Dm and a small De (therefore Y m(x) can be predicted better
than δ(x)), the proposed approach performs better for smaller σ2
δ (δ(x) with
smaller σ2
δ is easier to predict) and the value of σ2
m has little inﬂuence on the
performance. Furthermore, with the increase of σ2
m/σ2
δ (i.e., the curvature of
Y r(x) becomes more dominated by the computer model Y m(x)), using both ym
and ye leads to a larger improvement in prediction accuracy compared to using
only ye.
6.3
A Generalization to The Proposed Bayesian
Approach
The proposed approach assumes that the two Gaussian processes, the computer model
Y m(x) and the model bias δ(x), are mutually independent. Such independence as-
sumption may not hold in reality. For example, it is possible that the model bias
δ(x) is positively correlated with the computer output Y m(x). That is, δ(x) is large
where Y m(x) is large and small where Y m(x) is small. In this section, we derive the
posterior distributions of δ(x) and Y r(x) given computer outputs ym and physical
observations ye when Y m(x) and δ(x) in equation (6.1) are assumed to be correlated.
6.3.1
Correlation between Y m(x) and δ(x)
Instead of assuming that Y m(x) and δ(x) are mutually independent, we assume that
• (Y m(x), δ(x))T has a bivariate normal distribution


Y m(x)
δ(x)

∼N(


f T
m(x)βm
f T
δ (x)βδ

,


σ2
m
ρσmσδ
ρσmσδ
σ2
δ

)
(6.2)
200

where ρ obviously is the correlation between Y m(x) and δ(x), taking values in
the interval [−1, 1].
• Cov(Y m(xi), δ(xj)) = 0 for any i ̸= j.
• p(θ, ρ) = p(θ) · p(ρ), where θ = {βm, σ2
m, φm, Pm, βδ, σ2
δ, φδ, Pδ, σ2
ǫ}, the col-
lection of all parameters except the new correlation parameter ρ in equation
(6.1).
All the other assumptions made in Chapter 5 including the prior distributions for
βm, σ2
m, βδ, and σ2
δ remain the same. As a result, when the correlation parameter ρ
equals to zero, Y m(x) and δ(x) are independent, and we have exactly the same model
as before.
6.3.2
Posterior Distributions of δ(x) and Y m(x)
In this subsection, we derive the posterior distributions of δ(x) and Y m(x) when
De ⊆Dm. Without loss of generality, we rearrange the vector of computer outputs
ym such as the ﬁrst ne elements are computer outputs at De = {xe
1, · · · , xe
ne}, denoted
by ym
ne = (ym(xe
1), · · · , ym(xe
ne))T.
Given θ and ρ, for any set D = {x1, · · · , xn} in the input space, we derive the
posteriors of δ(D) and Y m(D) given ym and ye. For δ(D), we consider two cases:
• D ∩Dm = ∅.
• D = Dm −De, where Dm −De is the complement of De in Dm.
For Y m(D), we need consider only the ﬁrst case since, in the second case, the values
of Y m(D) are available.
• Distribution of δ(D)|ye, ym, θ, ρ with D ∩Dm = ∅
According to equation (6.1), given θ and ρ, for any set D = {x1, · · · , xn} in the input
201

space such that D ∩Dm = ∅, we have that


δ(D)
ye
ym


 θ, ρ ∼N(


Fδ(D)βδ
Fm(De)βm + Fδ(De)βδ
Fm(Dm)βm


,
(6.3)


σ2
δRδ(D)
σ2
δRδ(D, De)
0n×nm
σ2
δRδ(De, D)
Σe
Σem
0nm×n
ΣT
em
σ2
mRm(Dm)


)
where
Σe = Cov(ye, ye) = σ2
mRm(De) + σ2
δRδ(De) + 2ρσmσδIne + σ2
ǫIne
(6.4a)
Σem = Cov(ye, ym) = σ2
mRm(De, Dm) + ρσmσδ[ Ine
0∗]
(6.4b)
0∗= 0ne×(nm−ne)
(6.4c)
and 0k1×k2 is a k1 ×k2 matrix of zeros. Therefore, δ(D)|ye, ym, θ, ρ has a multivariate
normal distribution with mean vector
Fδ(D)βδ +

σ2
δRδ(D, De)
0n×nm

Σ−1


ye −Fm(De)βm −Fδ(De)βδ
ym −Fm(Dm)βm


(6.5)
and covariance matrix
σ2
δRδ(D) −

σ2
δRδ(D, De)
0n×nm

Σ−1


σ2
δRδ(De, D)
0nm×n

.
(6.6)
where
Σ =


Σe
Σem
ΣT
em
σ2
mRm(Dm)


(6.7)
After some matrix manipulations, we have that δ(D)|ye, ym, θ, ρ has a multivariate
normal distribution with mean vector
Fδ(D)βδ+Rδ(D, De)Q−1

ye −ym
ne −Fδ(De)βδ −ρ σδ
σm
P(ym −Fm(Dm)βm)

(6.8)
202

and covariance matrix
σ2
δ

Rδ(D) −Rδ(D, De)Q−1Rδ(De, D)

,
(6.9)
where
P = [Ine 0∗] R−1
m (Dm)
(6.10a)
Q = Rδ(De) + τIne −ρ2 [Ine 0∗] · R−1
m (Dm) ·


Ine
0T
∗

.
(6.10b)
• Distribution of δ(D)|ye, ym, θ, ρ with D = Dm −De
For D = Dm −De, the distribution of


δ(D)
ye
ym


θ, ρ
(6.11)
is the same as that shown in equation (6.3) except that the sub-matrix 0n×nm in the
covariance matrix is replaced by
ρσmσδ

0T
∗
Inm−ne

.
(6.12)
Therefore, when D = Dm −De, δ(D)|ye, ym, θ, ρ has a multivariate normal distribu-
tion with mean vector
Fδ(D)βδ + Rδ(D, De)Q−1

ye −ym
ne −Fδ(De)βδ −ρ σδ
σm
P(ym −Fm(Dm)βm)

(6.13)
+

0T
∗Inm−ne
 
ρ σδ
σm
Q−1
1 (ym −Fm(Dm)βm) −ρ2PTQ−1(ye −ym
ne −Fδ(De)βδ)

and covariance matrix
σ2
δ

Rδ(D) −ρ2 [0∗Inm−ne] R−1
m (Dm)


0∗
Inm−ne


(6.14)
−(Rδ(De, D) −ρ2 P


0∗
Inm−ne

)T Q−1 (Rδ(De, D) −ρ2 P


0∗
Inm−ne

)

203

• Distribution of Y m(D)|ye, ym, θ, ρ
According to equation (6.1), given θ and ρ, for any set D = {x1, · · · , xn} in the input
space such that D ∩Dm = ∅, we have that


Y m(D)
ye
ym


 θ, ρ ∼N(


Fm(D)βm
Fm(De)βm + Fδ(De)βδ
Fm(Dm)βm


,
(6.15)


σ2
mRm(D)
σ2
mRm(D, De)
σ2
mRm(D, Dm)
σ2
mRm(De, D)
Σe
Σem
σ2
mRm(Dm, D)
ΣT
em
σ2
mRm(Dm)


).
This gives the distribution of Y m(D)|ye, ym, θ, ρ as a multivariate normal distribution
with mean vector
Fm(D)βm+Rm(D, Dm)

Q−1
1 (ym −Fm(Dm)βm) −ρσm
σδ
PTQ−1(ye −ym
ne −Fδ(De)βδ)

(6.16)
and covariance matrix
σ2
m

Rm(D) −Rm(D, Dm)Q−1
1 Rm(Dm, D)

,
(6.17)
where
Q1 = Rm(Dm) −ρ2


Ine
0∗

(Rδ(De) + τIne)−1 [Ine 0∗]
(6.18)
6.3.3
Full Conditional Distributions of βm, σ2
m, βδ, and σ2
δ
The posterior distributions of δ(D) and Y m(D) derived in subsection 6.3.2 are condi-
tional on parameters θ and ρ. Instead of integrating out βδ, σ2
δ, βm, and σ2
m as we did
Chapter 5, we derive the full conditional distributions of βδ, σ2
δ, βm, and σ2
m given ye
and ym so that a Markov Chain Monte Carlo (MCMC) algorithm, Gibbs sampling,
can be used to estimate their values. Those estimated values are then plugged into
204

equations (6.8), (6.9), (6.13), (6.14), (6.14), (6.16), and (6.17) to get the posterior
distributions of δ(D) and Y m(D).
• Full Conditional Distribution of βm
The full conditional distribution βm given ye and ym can be derived by using the fact
that
p(βm|ye, ym, θ−(βm), ρ) ∝p(ye|ym, θ, ρ) · p(ym|θ, ρ) · p(βm|σ2
m),
(6.19)
where
ye|ym, θ, ρ ∼N(ym
ne + Fδ(De)βδ + ρ σδ
σm
· P(ym −Fm(Dm)βm), σ2
δQ).
(6.20)
After some matrix manipulations, we have that the full conditional distribution βm
given ye and ym is a multivariate normal distribution
βm|ye, ym, θ−(βm), ρ ∼N(Amvm, σ2
mAm)
(6.21)
where
A−1
m = FT
m(Dm)Q−1
1 Fm(Dm) + V−1
m ,
(6.22a)
vm = FT
m(Dm)

Q−1
1 ym −ρσm
σδ
PTQ−1(ye −ym
ne −Fδ((De)βδ)

+ V−1
m bm(6.22b)
and θ−(·) contains all the parameters except those inside the parentheses.
• Full Conditional Distribution of σ2
m
Similarly, we derive the full conditional distribution of σ2
m given ye and ym using the
fact that
p(σ2
m|ye, ym, θ−(σ2m), ρ) ∝p(ye|ym, θ, ρ) · p(ym|θ, ρ) · p(βm|σ2
m) · p(σ2
m)
(6.23)
which gives
p(σ2
m|ye, ym, θ−(σ2m), ρ) = Cm
0 · (σ2
m)−qm
2 −nm
2 −αm−1 · exp

−Cm
1
σ2
m
+ Cm
2
σm

,
(6.24)
205

where
Cm
1 = γm + 1
2

(βm −bm)TV−1
m (βm −bm)
(6.25a)
+(ym −Fm(Dm)βm)TQ−1
1 (ym −Fm(Dm)βm)

,
Cm
2 = ρ · 1
σδ
· (ye −ym
ne −Fδ(De)βδ)TQ−1P(ym −Fm(Dm)βm),
(6.25b)
and Cm
0 is a normalization constant such that the term to the right side of the = sign
in equation (6.24) is a density.
• Full Conditional Distribution of βδ
We derive the full conditional distribution βδ given ye and ym using the fact that
p(βδ|ye, ym, θ−(βδ), ρ) ∝p(ye|ym, θ, ρ) · p(βδ|σ2
δ),
(6.26)
which gives
βδ|ye, ym, θ−(βδ), ρ ∼N(Aδvδ, σ2
δAδ),
(6.27)
where
A−1
δ
= FT
δ (De)Q−1Fδ(De) + V−1
δ ,
(6.28a)
vδ = FT
δ (De)Q−1

ye −ym
ne −ρ σδ
σm
P(ym −Fm((Dm)βm)

+ V−1
δ bδ.(6.28b)
• Full Conditional Distribution of σ2
δ
We derive the full conditional distribution σ2
δ given ye and ym using the fact that
p(σ2
δ|ye, ym, θ−(σ2
δ), ρ) ∝p(ye|ym, θ, ρ) · p(βδ|σ2
δ) · p(σ2
δ)
(6.29)
which gives
p(σ2
δ|ye, ym, θ−(σ2
δ), ρ) = Cδ
0 · (σ2
δ)−qδ
2 −ne
2 −αδ−1 · exp

−Cδ
1
σ2
δ
+ Cδ
2
σδ

(6.30)
where
Cδ
1 = γδ + 1
2

(βδ −bδ)TV−1
δ (βδ −bδ)
(6.31a)
+(ye −ym
ne −Fδ(De)βδ)TQ−1(ye −ym
ne −Fδ(De)βδ)

,
Cδ
2 = ρ · 1
σm
· (ye −ym
ne −Fδ(De)βδ)TQ−1P(ym −Fm(Dm)βm),
(6.31b)
206

and Cδ
0 is a normalization constant such that the term to the right side of the = sign
in equation (6.30) is a density.
207

CHAPTER VII
SUMMARY AND FUTURE RESEARCH
7.1
Exponential Smoothing for Forecasting
7.1.1
Summary
In this research, we investigated three types of statistical models that have been found
to underlie ES methods. They are ARIMA model, MSOE state space model, and
SSOE state space model. We established the relationship among the three classes of
statistical models and concluded that the class of SSOE state space models is broader
than the other two and provides a general statistical framework for the study of ES
methods. To better understand ES methods, we investigated the performance of ES
methods on time series of ARIMA-type.
We then continued to propose a new forecasting method, ESCov. This new method
incorporates covariates into ES methods and uses ES methods to model what left
unexplained in the time series of interest by covariates.
Numerical studies based
on two real-life examples demonstrated that ESCov outperforms ES methods and
regression models with ARIMA errors. We identiﬁed underlying SSOE state space
models for ESCov, discussed ML estimation using underlying SSOE models, and
derived the variances of forecasts by ESCov for the construction of prediction intervals.
We also suggested a two-step model selection procedure to choose covariates and ES
methods in the use of ESCov.
7.1.2
Future Research
We have tested ESCov on two real-life examples. More work is needed to explore the
performance of ESCov and to understand when ESCov performs well and why. We
208

have assumed the time series being forecast and covariates have a linear relationship
with ﬁxed coeﬃcients. Future work would be to generalize ESCov to handle time-
varying coeﬃcients and nonlinear relationships.
7.2
Bayesian Validation of Computer Models
7.2.1
Summary
In this research, we proposed a Bayesian approach to the validation of computer mod-
els. This approach integrates computer outputs and physical observations together to
give an accurate prediction of the output of the real system for which the computer
model is built. The prediction of the real system output is then used to validate the
computer model. The performance of the proposed approach was tested on real-life
examples and investigated through the use of simulated examples. We also proposed
a generalization to the proposed approach.
7.2.2
Future Research
We have investigated the impacts of three factors (the number of replications in phys-
ical experiments, the variance of δ(x), and the variance of Y m(x)) on the performance
of the proposed Bayesian approach. More studies are needed to explore the impacts
of those three factors and other factors, such as design sets De and Dm, correlation
parameters φδ and φm, and prior distribution parameters. We have proposed a gener-
alization to the proposed approach by assuming that Y m(x) and δ(x) are correlated.
The next step along this direction would be the implementation of the generalized ap-
proach and the investigation of its performance, such as the impacts of the correlation
parameter.
209

REFERENCES
[1] American Institute of Aeronautics and Astronautics (1998). Guide for the veri-
ﬁcation and validation of computational ﬂuid daynamics simulations. AIAA-G-
077-1998.
[2] Akaike, H. (1973). Information theory and an extension of the maximum like-
lihood principle. Second international symposium on information theory, edited
by Petrov, B. N. and Csaki, F., Akademiai Kiado, Budapest.
[3] Bayarri, M. J., Berger, J. O., Higdon, D., Kennedy, M. C., Kottas, A., Paulo, R.,
Sacks, J., Cafeo, J. A., Cavendish, J., Lin, C. H., and Tu, J. (2002). A Framework
for Validation of Computer Models. Foundations for Veriﬁcation and Validation
in the 21st Century Workshop, Johns Hopkins University.
[4] Billah, B., King, M. L., Snyder, R. D., Koehler, A. B. (2006). Exponential
smoothing model selection for forecasting. International Journal of Forecasting,
22(2), 239-247
[5] Brockwell, P. J. and Davis, R. A. (1991). Time Series: Theory and Methods.
second edition, Springer-Verlag, New York.
[6] Brown, R. G. (1959). Statistical Forecasting for Inventory Control. McGraw-Hill
[7] Brown, R. G. (1963). Smoothing, Forecasting and Prediction of Discrete Time
series. Prentice-Hall
[8] Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. (1994). Time series analysis:
forecasting and control. 3rd ed, Prentice-Hall, Inc.
210

[9] Castillo, E. D. (2001). Some properties of EWMA feedback quality adjustment
schemes for drifting distribution. Journal of Quality Control, 33, 153-166.
[10] Chatﬁeld, C. (1996). The Analysis of Time Series: An Introduction. ﬁfth edition,
Chapman and Hall Ltd, London.
[11] Chatﬁeld, C., Koehler, A. B., Ord, J. K., and Snyder, R. D. (2001). A new look
at models for exponential smoothing. The Statistican, 50, 147-159.
[12] Chen, C. (1997). Robustness properties of some forecasting methods for seasonal
time series: a Monte Carlo study. International Journal of Forecasting, 13, 269-
280.
[13] Cohen, G. D. (1963). A note on exponential smoothing and autocorrelated in-
puts. Operations Research, 2, 361-367.
[14] Cogger, K. O. (1973). Speciﬁcation analysis. Journal of the American Statistical
Association, 68, 899-905.
[15] Cox, D. R. (1961). Prediction by exponentially weighted moving averages and
related methods. Journal of the Royal Statistical Society (B), 23, 414-422.
[16] Dewettinck, K., Visscher A. D., Deroo, L., and Huyghebaert, A. (1999). Model-
ing the steady-state thermodynamic operation point of top-spray ﬂuidized bed
processing. Journal of Food Engineering, 39, 131-143
[17] Duncan, D. B. and Horn, S. D. (1972). Linear dynamic recursive estimation
from the viewpoint of regression analysis. Journal of the American Statistical
Association, 67, 815-821.
[18] Easterling, R. G. and Berger, J. O. (2002). Statistical Foundations for The Val-
idation of Computer Models. Foundations for Veriﬁcation and Validation in the
21st Century Workshop, Johns Hopkins University
211

[19] Fuller, W. A. (1996). Introduction to statistical time series. 2nd edition. John
Wiley and Sons, New York.
[20] Gardner, E. Jr.(1985). Exponential smoothing: the state of the art. Journal of
Forecasting, 4(1), 1-28.
[21] Gardner, E. Jr. and Mckenzie, E. (1985). Forecasting trends in time series. Man-
agement Science, 31, 1237-1246.
[22] Hannan, E. J. and Quinn, B. G. (1979). The determination of the order of an
autoregression. Journal of the Royal Statistical Society (B), 41, 190-195.
[23] Harrison, P. J. (1967). Exponential smoothing and short-time sales forecasting.
Management Science, 13, 821-842.
[24] Harvey, A. C. (1984). A uniﬁed view of statistical forecasting procedures. Journal
of Forecasting, 3(3), 245-275.
[25] Harvey, A. C. (1990). Forecasting, Structural Time Series Models and Kalman
Filter, Cambridge University Press, Cambridge.
[26] Harvey, A. C. (1993). Time Series Models, The MIT Press, Cambridge, Mas-
sachusetts.
[27] Harvey, A. C. (2005). A uniﬁed approach to testing for stationarity and unit
roots. Identiﬁcation and Inference for Econometric Models, edited by D. W. K.
Andrews and J. H. Stock, Cambridge University Press, New York
[28] Hills, R. G., and Trucano, T. G., (1999). Statistical Validation of Engineering and
Scientiﬁc Models: Background. SAND99-1256, Sandia National Laboratories,
Albuquerque, New Mexico.
212

[29] Hills, R. G., and Trucano, T. G., (2002). Statistical Validation of Engineering
and Scientiﬁc Models: A Maximum Likelihood Based Metric. SAND2001-1783,
Sandia National Laboratories, Albuquerque, New Mexico.
[30] Hills, R. G., and Trucano, T. G., (2006). Model Validation: Model Parameter
and Measurement Uncertainty. Journal of Heat Transfer, 128, 339-351.
[31] Holt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted
moving averages. ONR Research Memorandum, Carnigie Institute 52.
[32] Hurvich, C.M. and Tsai, C.L. (1989). Regression and time series model selection
in small samples. Biometrika, 76, 297-307.
[33] Hyndman, R. J., Koehler, A. B., Snyder, R. D., and Grose, S. (2002). A state
space framework for automatic forecasting using exponential smoothing methods.
International Journal of Forecasting, 18, 439-454.
[34] Hyndman, R. J., Koehler, A. B., Ord, J. K., and Snyder, R. D. (2005). Prediction
intervals for exponential smoothing using two new classes of state space models.
Journal of Forecasting, 24, 17-37.
[35] Ingolfsson, A. and Sachs, E. (1993). Stability and sensitivity of an EWMA con-
troller. Journal of Quality Technology, 25(4), 271-287.
[36] Kalman, R. E. (1960). A new approach to linear ﬁltering and prediction problems.
Journal of Basic Engineering, 82, 34-45.
[37] Kennedy, M. C. and O’Hagan, A. (2000). Predicting the output from a complex
computer code when fast approximation are available. Biometrika, 87(1), 1-13
[38] Kennedy, M. C. and O’Hagan, A. (2001). Bayesian calibration of computer mod-
els. Journal of the Royal Statistical Society (B), 63(3), 425-464
213

[39] Lange, K. (1999). Numerical Analysis for Statistician. Springer.
[40] Liu, J. S. (2001). Monte Carlo strategies in scientiﬁc computing. Springer.
[41] Makridakis, S. and Hibon, M. (1979). Accuracy of forecasting: an empirical
investigation (with discussion). Journal of the Royal Statistical Society (A), 142,
97-145.
[42] Makridakis, S., Andersen, A., Carbone, R., Fildes, R., Hibon, M., Lewandowski,
M., Newton, J., Parzen, E., and Winkler, R. (1982). The accuracy of extrap-
olation (time series) methods: results of a forecasting competition. Journal of
Forecasting, 1, 111-153.
[43] Makridakis, S., Chatﬁeld, C., Hibon, M., Lawrence, M., Ord, K., Mills, T., and
T. F. Simmons (1993). The M2-Competition: a real-time judgmentally based
forecasting study. International Journal of Forecasting, 9, 5-23.
[44] Makridakis, S. and Hibon, M. (2000). The M3-Competition: results, conclusion,
and implications. International Journal of Forecasting, 16, 451-476.
[45] Mentzer, J. T. and Kahn, K. B. (1995). Forecasting technique familiarity, satis-
faction, usage, and application. Journal of Forecasting, 14, 465-467.
[46] Meinnhold, R. J. and Singpurwalla, N. D. (1983). Understanding the Kalman
ﬁlter. The American Statistician, 37(2), 123-127.
[47] Montgomery, D., Johnson, L., and Gardiner, J. (1990). Forecasting and Time
Series Analysis. 2nd Edition, Mcgraw-Hill.
[48] Muth, J. F. (1960). Optimal properties of exponentially weighted forecasts. Jour-
nal of the American Statistical Association, 55, 299-306.
214

[49] Oberkampf, W. L. and Trucano, T. G. (2000). Validation Methodology in Com-
putational Fluid Dynamics. Fluids 2000 Conference, AIAA 2000-2549, Denver,
Colorado.
[50] Oberkampf, W. L. and Barone, M. F. (2004). Measures of Agreement Between
Computation and Experiment: Validation Metrics. 34th Fluid Dynamics Con-
ference and Exhibit, AIAA-2004-2626, Portland, Oregon.
[51] Ord, J. K., Koehler, A. B., and Snyder, R. D. (1997). Estimation and prediction
for a class of dynamic nonlinear statistical models. Journal of the American
Statistical Association, 92, 1621-1629.
[52] Pegels, C. C. (1969). Exponential forecasting: some new variations. Management
Science, 15, 311-315.
[53] Prest, A. R. (1949). Some experiments in demand analysis. Review of Economics
and Statistics, 31, 33-49.
[54] Qian, Z. and Wu, C. F. (2005). Bayesian hierarchical modeling for integrating
low-accuracy and high-accuracy experiments. School of Industrial and Systems
Engineering, Georgia Institute of Technology.
[55] Qian, Z., Seepersad, C. C., Joseph, V. R., Allen, J. K., and Wu, C. F.
(2006). Building surrogate models based on detailed and approximate simual-
tions. ASME Journal of Mechanical Design, 128, 668-677
[56] Roberts, S. A. (1982). A general class of Holt-Winters type forecasting models.
Management Science, 28, 808-820.
[57] Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989). Design and
Analysis of Computer Experiments. Statistical Science, 4(4), 409-435.
215

[58] Santner, T. J., Williams, B. J., and Notz, W. I. (2003). The Design and Analysis
of Computer Experiments. Springer.
[59] Snyder, R. D. (1985). Recursive estimation of dynamic linear models. Journal of
the Royal Statistical Society (B), 47, 272-276.
[60] Snyder, R. D. (2004). A pedants approach to exponential smoothing. Department
of Econometrics and Business Statistics, Monash Univeristy, Australia
[61] Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statis-
tics, 6, 461-464.
[62] Taylor, J. W. (2003). Exponential smoothing with a damped multiplicative trend.
International Journal of Forecasting, 19, 715-725.
[63] Welch, W. J., Buck, R. J., Sacks, J., Wynn, H. P., Mitchell, T. J., and Morris,
M. D. (1992). Screening, Predicting, and Computer Experiments. Technometrics,
34(1), 15-25.
[64] Winters, P. R. (1960). Forecasting sales by exponentially weighted moving aver-
ages. Management Science, 6, 324-342.
[65] Young, P. (1984). Recursive Estimation and Time Series Analysis. Springer-
Verlag
216

VITA
Shuchun Wang was born in Linyi, Shanxi Province, China.
She received a B.S.
degree in 1996 and an M.S. degree in 1999 both in Civil Engineering from Tsinghua
University, Beijing, China. She obtained an M.S. degree in Civil Engineering from
the Georgia Institute of Technology in 2001. From 2001 to 2006, she was a graduate
research assistant in the School of Industrial and Systems Engineering at the Georgia
Institute of Technology. She obtained an M.S. degree in Statistics in 2005 and will
receive a Ph.D. degree in Statistics in 2006 from the Georgia Institute of Technology.
217

