See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/261132453
Reasoning under uncertainty: Variations of subjective logic deduction
Conference Paper · January 2013
CITATIONS
8
READS
204
6 authors, including:
Some of the authors of this publication are also working on these related projects:
International Technology Alliance in Network and Information Sciences (NIS- ITA)Research 2006-2016 View project
Advanced Belief Reasoning in Intelligence View project
Lance Kaplan
Army Research Laboratory
286 PUBLICATIONS   4,949 CITATIONS   
SEE PROFILE
Murat Sensoy
Ozyegin University
119 PUBLICATIONS   752 CITATIONS   
SEE PROFILE
Yuqing Tang
Carnegie Mellon University
57 PUBLICATIONS   420 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Murat Sensoy on 20 March 2016.
The user has requested enhancement of the downloaded file.

Reasoning Under Uncertainty: Variations of
Subjective Logic Deduction
Lance M. Kaplana, Murat S¸ensoyb,c, Yuqing Tangd, Supriyo Chakrabortye, Chatschik Bisdikianf, Geeth de Mela,f
aU.S. Army Research Laboratory, Adelphi, MD, USA
b ¨Ozyeˇgin University, Istanbul, TURKEY
cUniversity of Aberdeen, Aberdeen, Scotland, UK
dCarnegie Mellon University, Pittsburgh, PA, USA
eUniversity of California, Los Angeles, CA, USA
fIBM Research, T. J. Watson Research Center, Yorktown Heights, NY, USA
Abstract—This work develops alternatives to the classical sub-
jective logic deduction operator. Given antecedent and consequent
propositions, the new operators form opinions of the consequent
that match the variance of the consequent posterior distribution
given opinions on the antecedent and the conditional rules
connecting the antecedent with the consequent. As a result, the
uncertainty of the consequent actually map to the spread for the
probability projection of the opinion. Monte Carlo simulations
demonstrate this connection for the new operators. Finally, the
work uses Monte Carlo simulations to evaluate the quality of
fusing opinions from multiple agents before and after deduction.
I. INTRODUCTION
The world is not absolute and an ontological representation
of the world should account for various shades of gray. Thus,
traditional propositional logic is inadequate for general rea-
soning. Recently, probabilistic logics have emerged to account
for the fact that propositions and even the rules that connect
propositions are not always true [1]–[3]. At any given instance,
if one observes a proposition or rule, it will either be true
or false. Over the ensemble of all possible observations, a
probabilistic model states that a proposition or rule can be
observed to be true with a given ground truth probability. For
many cases, the ground truth probabilities are not known and
can only be inferred from the observations. Subjective logic
(SL) was introduced as a means to represent the belief and
uncertainty of these ground truth probabilities based upon the
evidence observed by one or multiple agents [3]. In essence,
SL is a form of evidential reasoning.
The main feature of SL is that an opinion about a proposi-
tion corresponds to a posterior distribution for the probability
Research was sponsored by the U.S. Army Research Laboratory and the
U.K Ministry of Defense and was accomplished under Agreement Number
W911NF-06-3-0001. The views and conclusions contained in this document
are those of the authors and should not be interpreted as representing the
ofﬁcial policies, either expressed or implied, of the U.S Army Research
Laboratory, the U.S. Government, the U.K. Ministry of Defense or the U.K
Government. The U.S. and U.K. Governments are authorized to reproduce and
distribute for Government purposes notwithstanding any copyright notation
hereon.
Email: lkaplan@ieee.org
that the proposition is true given the evidence or observations
thus far. Speciﬁcally, the posterior distribution is approximated
as a Dirichlet distribution whose parameters have a one-to-
one correspondence with the subjective opinion. A number
of operators have been developed for SL that generalize the
operators that exist in propositional and probabilistic logic [3].
These operators have been designed to match the mean of the
distribution of the output proposition probabilities given that
that the distributions for the input proposition probabilities are
Dirichlet. Intuitively, the variance of the output proposition
probability distribution represents the uncertainty, but most
SL operators do not explicitly account for the variance. In
some cases, the operators simply maximize uncertainty while
matching the mean. As a result, the statistical meaning of
uncertainty can be lost after such SL operators.
This work focuses on alternatives to the SL deduction
operator [4], [5] that generalizes the notion of modus ponens
from propositional logic, where opinions (or knowledge) about
the antecedent and the implication rules leads to an opinion
about the consequent. SL deduction represents the catalyst
to formulate defeasible logics that incorporate uncertainty.
Initial efforts to this end appear in [6]–[8]. In SL deduction,
the consequent opinion does represent the mean posterior
distribution for the consequent probabilities. However, the
uncertainty does not relate to the statistics of this posterior
distribution. This work develops two alternative deduction
operators to determine more meaningful uncertainty values.
Motivated from our previous work on expanding SL for
partial observation updates [9], [10], the moment matching
(MM) deduction method forms a consequent opinion that
characterizes both the mean and variance of the consequent
posterior distribution. Similarly, the mode/variance matching
(MdVM) method characterizes the mode and variance of
the consequent posterior distribution. The performance of
these various deduction operators are evaluated using Monte
Carlo simulations over known probabilistic ground truths.
Furthermore, the simulations consider distributed agents where
opinions are fused via the SL consensus operator before or

after deduction. Note that the consensus operator perfectly
account for uncertainty when the individual opinions are
formed from independent observations.
This paper is organized as follows. Section II reviews SL
including the deduction and consensus operators. Then, the
alternative deduction operators are introduced in Section III.
Section IV discusses some of the properties of the new
deduction operators relative to SL deduction, for both single
and multiagent scenarios, and Section V details the Monte
Carlo evaluations. Finally, Section VI concludes the paper.
II. SUBJECTIVE LOGIC
A. SL Basics
A full overview of SL is available in [3]. SL assumes a
probabilistic world such that at any given instant of time,
proposition x is true with probability px and it is false, i.e.,
its complement ¯x is true, with probability p¯x = 1 −px. In
general, SL can form beliefs over a mutually exclusive set
of propositions X, i.e., a frame of discernment. Based upon
observations that accumulate, a subjective opinion is formed
that consists of the beliefs in the proposition in X and an
uncertainty value that add to one. This opinion maps to a
Dirichlet distribution representing the posterior distribution
of the truthfulness of the propositions in X, i.e. px, given
the observations. For simplicity of presentation, this paper
focuses on a binary frame X consisting of proposition x and
its complement where the posterior distribution for px given
the observations is modeled by the beta distribution.
Speciﬁcally, SL represents the subjective1 opinion x as a
triple ωx = (bx, dx, ux) such that bx + dx + ux = 1 where
bx, dx, ux ≥0 are the belief, disbelief, and uncertainty in
x due to the evidence. This triple is equivalent to a beta
distribution for generating probabilities of x because there is a
bijective mapping from ωx to the beta distribution parameters
αx. This mapping assumes some base rate (prior) probability
ax that x is true along with a prior weight W that determines
the sensitivity of uncertainty to evidence.The mapping is
such that the probability projection (or mean) of the beta
distribution forms the pignistic probabilities
mx = E[px] = bx + uxax,
(1)
m¯x = E[p¯x] = dx + ux(1 −ax).
(2)
Note that these pignistic probabilities form the estimates for
ˆpx = E[px] and ˆp¯x = E[p¯x] in light of the collected evidence
and the base rates. For complete uncertainty (i.e., ux = 1),
the pignistic probabilities revert to the prior, and as uncertainty
drops to zero, they are represented by the evidence in the belief
and disbelief values.
The relationship between an opinion and the corresponding
beta distribution is given by the following mapping from
opinion ωx to αx:
αx = W
ux
bx + Wax,
α¯x = W
ux
dx + W(1 −ax).
(3)
1Subjective in the sense that the opinion is formed by an agent’s own
observations.
The reverse mapping from αx to ωx is
(bx, dx, ux) =
!αx −Wax
sx
, α¯x −W(1 −ax)
sx
, W
sx
"
,
(4)
where
sx = αx + α¯x = W
ux
(5)
is the Dirichlet strength of the distribution, which is inversely
proportional to the uncertainty.
The evidential nature of SL can be understood by realizing
that the beta distribution is the conjugate prior for the binomial
distribution. In other words, given an initial prior of αx,0 =
# Wax,
Wa¯x
$
, the posterior beta distribution of αx is the
results of sx −W “coin ﬂip” observations where αx −Wax
observations were true and α¯x −Wa¯x were false. In essence,
the uncertainty is inversely proportional to the number of direct
observations made about x.
B. SL Consensus
The SL consensus operation forms a composite opinion
for the subjective opinion of multiple agents [11]. It assumes
that the agents make independent observations to form their
opinions. Given agents A and B having opinions ωA
x and
ωB
x , the consensus opinion ωA⋄B
x
= ωA
x ⊕ωB
x is obtained
by converting these opinions into beta parameters via (3).
Using the coin ﬂip interpretation, it is easy see that the
fused beta parameters are obtained by simply summing up
the observations made by the individual agents and inserting
the prior, i.e.,
αA⋄B
x
= (αA
x −WaA
x ) + (αB
x −WaB
x ) + WaA⋄B
x
,
(6)
where the base rates for the various agents can be different.
Finally, the fused beta parameters are converted in the fused
opinion via (4). Clearly, the consensus operation is associative,
and independent opinions can be fused in a sequential manner,
e.g., fusing opinion A ⋄B with C, fusing the result with D
and so forth. Alternatively, the fusion of all agents can be
accomplished by summing up all the evidences at once in the
beta parameter space. This direct consensus of multiple agents
can be expressed as ω⋄iAi
x
= %
i ωAi
x .
C. Subjective Logic Deduction
In deduction, one uses knowledge about an antecedent
proposition x and knowledge about how the antecedent implies
a consequent proposition y to formulate knowledge about
the consequent. In probabilistic terms, the states of the x
and y propositions occur via the joint probabilities pyx, py¯x,
p¯yx, and p¯y¯x. Complete knowledge of x entails knowing the
marginal probability px. Likewise, complete knowledge of
how x implies y means knowing the conditional probabilities
py|x and py|¯x. Deduction is simply determining the marginal
probability py from px, py|x and py|¯x. However, knowledge is
usually not complete and is acquired through evidence such
as from coin ﬂip observations.
Subjective logic deduction uses the opinions ωx, ωy|x, and
ωy|¯x to determine the deduced opinion ωy||x. Note that the

conditional opinions ωy|x, and ωy|¯x can be formed by direct
coin ﬂip observations of the truth or not of y when x is
observed to be true or false, respectively. By the interpretation
of ωy∥x as a beta distribution, the mean of the distribution
is equal to the true mean of the distribution py given that
the distributions for px, py|x, and py|¯x are beta distributions
parameterized according to the corresponding opinions and
py = py|xpx + py|¯xp¯x. As shown in Section III, the mean of
py is
my = my|xmx + my|¯xm¯x.
The deduction operator calculates the opinion on y from the
three input opinions as
ωy∥x = bxωy|x + dxωy|¯x + uxωy|ˆx,
(7)
where ωy|ˆx is the opinion of y given a vacuous opinion about
x, i.e., ux = 1. The value of ωy|ˆx has maximum uncertainty
uy|ˆx such that belief and disbelief in y is bounded below by the
conditional opinions, and the mean of the vacuous conditional
opinion is consistent with the mean of py when x is vacuous
so that mx = ax. Speciﬁcally,
by|ˆx ≥min{by|x, by|¯x},
dy|ˆx ≥min{dy|x, dy|¯x},
by|ˆx + uy|ˆxay = my|xax + my|¯x(1 −ax).
(8)
Geometrically, the deduction operator assumes that the opinion
of y is a point in a simplex determined by the opinion of
x relative to the simplex vertices, which are the conditional
opinions. While the geometric interpretation is appealing, it
does not capture the statistics of the distribution of py beyond
its mean. As shown later, the uncertainty uy∥x loses any
correspondence to the variance of py.
D. Interpretation of Uncertainty
Intuitively, the bias and variance of mx as an estimator
for px should decrease to zero as uncertainty goes to zero.
This subsection determines a relationship between the bias or
variance and uncertainty by using the coin ﬂip interpretation.
First of all, mx is a biased estimate of the ground truth px
that is only unbiased asymptotically as the inﬂuence of the
base rates disappear. An unbiased estimate considers only the
observed evidence, not the prior, to estimate px. In the beta
parameter space, the unbiased estimate for px is given by
νx = αx −Wax
sx −W
,
(9)
and via (3), this unbiased estimate can be expressed as
νx =
bx
1 −ux
.
(10)
Unlike the regularized2 mean mx, the unbiased estimate is
undeﬁned when uncertainty is one. When the prior weight
W = 2 and ax = 0.5, the unbiased estimate corresponds to
the mode of the beta distribution describing the posterior for
py. This observation motivates the use of the mode/variance
2The prior regularizes the mean.
matching deduction method in the next section. Estimating the
bias of mx as the difference between mx and νx leads to the
following relationship between bias and uncertainty
bias ≈
ux
1 −ux
(ax −mx).
(11)
Clearly, the bias goes to zero as uncertainty goes to zero.
Similarly, one can relate the uncertainty value to the vari-
ance of the estimator. Given Nx coin ﬂip observations, it is
well known that the Cramer-Rao lower bound (CRLB) is given
by [12]
CRLB = pxp¯x
Nx
.
(12)
Since Nx = sx −W and mx ≈px, the approximate variance
for mx or νx can be related to uncertainty via
var ≈
ux
1 −ux
mxm¯x
W
.
(13)
Again, as uncertainty goes to zero, so does the variance. Over-
all, the two expression for bias and variance can be computed
for any subjective opinion. These expressions approximate
the true bias and spread for estimates over the ensemble
of possible observations that could have occurred from the
ground truth.
III. DEDUCTION ALTERNATIVES
The proposed deductive methods ﬁt a beta distribution to
the actual marginal distribution for py when given the opinions
ωx, ωy|x and ωy|¯x. Since these opinions are generated from
independent observations, the joint distribution for px, py|x
and py|¯x is the product of the individual beta distributions so
that
fX,Y |X(px, py|x, py|¯x) = pxαx−1(1 −px)α¯x−1
B2(αx, α¯x)
·
· py|xαy|x−1(1 −py|x)α¯
y|x−1
B2(αy|x, α¯y|x)
py|¯xαy|¯x−1(1 −py|¯x)α¯
y|¯x−1
B2(αy|¯x, α¯y|¯x)
,
(14)
where the parameters αx, αy|x and αy|¯x are determined from
the corresponding opinions via (3), and B2(·, ·) is the beta
function
B2(x, y) = Γ(x)Γ(y)
Γ(x + y) .
By making the following substitution of variables
py = py|xpx + py|¯x(1 −px),
px|y =
py|xpx
py|xpx + py|¯x(1 −px),
px|¯y =
(1 −py|x)px
(1 −py|x)px + (1 −py|¯x)(1 −px),
(15)
the joint distribution with respect to py, px|y, and px|¯y is
expressed as
fY,X|Y (py, px|y, px|¯y) = gxgygx|ygx|¯y,
(16)
where
gx = pxβx(1 −px)β¯x
B2(αx, α¯x)
,

gy = p
αy|x+αy|¯x−1
y
(1 −py)α¯
y|x+α¯
y|¯x−1,
gx|y = px|yαy|x−1(1 −px|y)αy|¯x−1
B2(αy|x, α¯y|x)
,
gx|¯y = px|¯yα¯
y|x−1(1 −px|¯y)α¯
y|¯x−1
B2(αy|¯x, α¯y|¯x)
,
px = px|ypy + px|¯y(1 −py), βx = αx −αy|x −α¯y|x and
β¯x = α¯x −αy|¯x −α¯y|¯x.
The marginal distribution is obtained from (16) via
fY (py) =
& 1
0
& 1
0
fY,X|Y (py, px|y, px|¯y)dpx|¯ydpx|y
(17)
In general, this marginal is not a beta distribution, and a simple
analytical expression for the distribution appears unobtainable.
It is possible to determine this distribution using numerical
integration techniques. Nevertheless, statistics of this distribu-
tions can be obtained directly from joint distribution of px,
py|x and py|¯x in (14). The new deduction methods use these
statistics to approximate the marginal distribution fY by a beta
distribution.
A. Moment Matching
The moment matching (MM) deduction method approx-
imates fY
by a beta distribution that matches the mean
and variance of fY . The idea of moment matching for SL
originated in [9] where a new method was introduced to update
subjective opinions based upon partial observations of the
coin ﬂip (or dice roll) when only likelihoods of the possible
results are known. This notion was further expanded in [10]
to account for uncertainty in the likelihood calculations. For
the deduction operation, the ﬁrst step is to determine the mean
and variance of fY . The mean is derived via
my = E[py]
= E[py|x]E[px] + E[py|¯x]E[1 −px]
=
& 1
0
pxαxp¯xα¯x−1
B2(αx, α¯x) dpx
& 1
0
py|xαy|xp¯y|xα¯
y|x−1
B2(αy|x, α¯y|x)
dpy|x
+
& 1
0
pxαx−1p¯xα¯x
B2(αx, α¯x) dpx
& 1
0
py|¯xαy|¯xp¯y|¯xα¯
y|¯x−1
B2(αy|¯x, α¯y|¯x)
dpy|¯x
=
αx
αx + α¯x
αy|x
αy|x + α¯y|x
+
α¯x
αx + α¯x
αy|¯x
αy|¯x + α¯y|¯x
,
which leads to
my = my|xmx + my|¯x(1 −mx).
(18)
The second order moment is determined via
E[p2
y] =
' 1
0 pxαx+1p¯xα¯x−1dpx
B2(αx, α¯x)
' 1
0 py|xαy|x+1p¯y|xα¯
y|x−1dpy|x
B2(αy|x, α¯y|x)
+
& 1
0
pxαx−1p¯xα¯x+1
B2(αx, α¯x)
dpx
& 1
0
py|¯xαy|¯x+1p¯y|¯xα¯
y|¯x−1
B2(αy|¯x, α¯y|¯x)
dpy|¯x
+ 2
& 1
0
pxαxp¯xα¯x
B2(αx, α¯x)dpx
& 1
0
py|xαy|xp¯y|xα¯
y|x−1
B2(αy|x, α¯y|x)
dpy|x
·
& 1
0
py|¯xαy|¯xp¯y|¯xα¯
y|¯x−1
B2(αy|¯x, α¯y|¯x)
dpy|¯x.
After some simplifying steps
E[p2
y] = mxmy|x
αx + 1
sx + 1
αy|x + 1
sy|x + 1
+ (1 −mx)my|¯x
α¯x + 1
sx + 1
αy|¯x + 1
sy|x + 1
+ 2mx(1 −mx)my|xmy|¯x
sx
sx + 1
Then the variance is computed as
σ2
y = E[p2
y] −(E[py])2 ,
= my|xmx
!αy|x + 1
sy|x + 1
αx + 1
sx + 1 −mxmy|x
"
+ my|¯x(1 −mx)
!αy|¯x + 1
sy|¯x + 1
α¯x + 1
sx + 1 −(1 −mx)my|¯x
"
−2mx(1 −mx)my|xmy|¯x
sx + 1
.
Final simpliﬁcation leads to
σ2
y = mxmy|x(1 −myx)
sy|x + 1
mxsx + 1
sx + 1
+ (1 −mx)my|¯x(1 −my|¯x)
sy|¯x + 1
(1 −mx)sx + 1
sx + 1
+ mx(1 −mx)(my|x −my|¯x)2
sx + 1
.
(19)
The next step is to determine the parameters of the beta
distribution αy whose mean and variance are given by (18)
and (19), respectively. It is well known that the mean and
variance of the beta distribution are determined by αy via [13]
my
=
αy
sy
,
(20)
σ2
y
=
my(1 −my)
sy + 1
.
(21)
In other words, the parameters are determined from the mo-
ments via
sy = my(1 −my)
σ2y
−1,
αy = mysy,
α¯y = (1 −my)sy.
(22)
The strength parameter sy can be computed directly from
the mean and strength values from the opinions ωx, ωy|x and
ωy|¯x. By inserting (18) and (19) into (22),
sy =
A + B + C
A
sx+1 +
B
sy|x+1
mxsx+1
sx+1
+
C
sy|¯x+1
(1−mx)sx+1
sx+1
−1. (23)
where
A
=
mx(1 −mx)(my|x −my|¯x)2,
(24)
B
=
mxmy|x(1 −my|x),
(25)
C
=
m¯xmy|¯x(1 −my|¯x).
(26)
Overall, MM deduction approach takes the steps given in
Algorithm 1.

Algorithm 1 Moment Matching Deduction
Input: ωx, ωy|x and ωy|¯x
Output: ωy∥x
1) Calculate the mx, my|x, my|¯x and sx, sy|x, sy|¯x via (1)
and (5), respectively.
2) Calculate my and s∗
y using (18) and (23), respectively.
3) Let sy = max
(
s∗
y, Way
my , W(1−ay)
1−my
)
.
4) Set ωy∥x =
*
my −Way
sy , 1 −my −W(1−ay)
sy
, W
sy
+
.
Note that Step 3 is included to ensure that the belief and
disbelief value can never become negative after the deduction
update. Changing the Dirichlet strength by this step has no
effect on the mean value. In essence, it lowers the variance of
the beta ﬁt to fY while maintaining the mean of fY so that
belief masses are not negative. Because this approach retains
the true mean of fY it ﬁts within the framework of a subjective
logic operator. Unlike the traditional subjective logic operator
as reviewed in Section II-C, the MM deduction operator better
characterizes the spread of the posterior fY . As Section V will
show, the Dirichlet strength for the MM deduction operator
provides a better approximation for the effective number of
coin ﬂips of y than deduction provides.
B. Mode and Variance Matching
As discussed earlier, when the beta distribution represents
the posterior for px due to the coin ﬂip experiment, the mean
of the density represents a biased estimator for ground truth px.
On the other hand, the mode of the posterior is the unbiased
estimator of px. Asymptotically, the mean and mode are equal
and both estimators become unbiased. It can be argued that it
is as important (if not more) for the mode of the approximate
beta distribution for py to match the mode of the marginal
distribution given by (17). At this point, we do not have a
closed formed or efﬁcient means to determine the mode of
this distribution, i.e., the marginal of (16). As a surrogate, we
use the unbiased estimator of py derived from the unbiased
estimates of px, py|x and py|¯x (see (10)), i.e.,
νy =
by|xbx
(1 −uy|x)(1 −ux) +
by|¯xdx
(1 −uy|¯x)(1 −ux).
(27)
This surrogate is only available when neither opinions ωx,
ωy|x, ωy|¯x are vacuous, i.e., ux, uy|x, uy|¯x < 1. To determine
the beta parameters, we note that the mode of the beta
distribution [13] relates to the parameters via3
νy = αy −1
sy −2 .
(28)
Then using (20), (21), and (28), it can be shown that the
Dirichlet strength that matches the mode and variance of fY
is the largest root of the following cubic polynomial
σ2
ys3
y + (σ2
y −νy(1 −νy))s2
y −(1 −2νy)2sy + (1 −2νy)2 = 0.
(29)
3The expression for the mode assumes that αy, α¯y > 1, which also implies
sy > 2.
It can also be shown that a real solution sy ≥2 always exist
for (29) when W = 2. Then the mean of the beta distribution
whose strength sy solves (29) and whose mode is given by
(28) has a mean given by
my = νy(sy −2) + 1
sy
.
(30)
Overall, the mode/variance matching (MdVM) deduction
approach is described by Algorithm 2.
Algorithm 2 Mode/Variance Matching Deduction
Input: ωx, ωy|x and ωy|¯x
Output: ωy∥x
1) Calculate the mx, my|x, my|¯x and sx, sy|x, sy|¯x via (1)
and (5), respectively.
2) Calculate νy using (27).
3) Calculate sy as the largest root of (29).
4) Calculate my using (30).
5) Set ωY =
*
my −Way
sy , 1 −my −W(1−ay)
sy
, W
sy
+
.
Unlike MM Deduction, the MdVM does not include the
step that possibly increases the Dirichlet strength in order to
ensure that the belief and disbelief values are positive. As long
as W = 2 and ay = 0.5, which represent reasonable prior
values, by and dy are guaranteed to be positive due to (30).
In short, the MdVM is only designed to work for a uniform
baseline rate with a prior weight of W = 2.
IV. PROPERTIES OF DEDUCTION
The traditional SL deduction calculates the consequent
opinion as a linear combination of conditional opinions as
given by (7). As a result, the uncertainty for y is computed as
uy = bxuy|x + dxuy|¯x + uxuy|ˆx.
(31)
The linear relationship does exist for the new deduction
approaches. In fact, the uncertainty given by SL deduction
appears to always be larger than that given by the two other
deduction operations. Figure 1 plots the uncertainty for y
over all possible opinions of x when ωy|x = (.8, .1, .1) and
ωy|¯x = (.1, .6, .3) for the three deduction methods. The ﬁgure
reveals that the uncertainty for the new deductions methods
is lower than that of SL deduction. Unlike SL deduction, the
relationship between uncertainty and ωx is nonlinear for the
new methods. All three methods provide the same uncertainty
for dogmatic ωx = (1, 0, 0) or ωx = (0, 1, 0). Certainly,
SL deduction should have higher uncertainty for the vacuous
ωx = (0, 0, 1) because it maximizes uncertainty for this case.
In general, it can be shown that for the dogmatic opinion that
x is true (or false) leads to ωy∥x = ωy|x (or ωy∥x = ωy|¯x) for
all three methods.4 As a result, when the rules are known to be
true with full certainty, all three deduction methods become
equivalent to modus ponens in standard propositional logic.
For the slightly more general case that the propositions and
4Note that MdVM assumes W = 2 and ay = 0.5.

0
0.2
0.4
0.6
0.8
1
0
0.5
1
0
0.1
0.2
0.3
0.4
0.5
0.6
 bx
 dx
 uy
MM Deduction
SL Deduction
(a)
0
0.2
0.4
0.6
0.8
1
0
0.5
1
0
0.1
0.2
0.3
0.4
0.5
0.6
 bx
 dx
 uy
SL Deduction
MdVM Deduction
(b)
Figure 1.
Uncertainty of y as a function of ωx when ωy|x = (.8, .1, .1)
and ωy|¯x = (.1, .6, .3): (a) SL and MM Deduction and (b) SL and MdVM
Deduction.
rules are not completely true or false, but the opinions are still
dogmatic, all three methods still provide the same results as
they simplify to probabilistic reasoning.
The observations from Figure 1 shows that the SL deduction
leads to an opinion whose uncertainty is larger than or equal to
the uncertainty of the other two deduction methods appear to
hold in general. However, aside from some special cases, we
do not have a proof to determine if indeed this trend always
holds. It should be emphasized that SL and MM deduction
lead to the same probability projection my. They only differ
in that MM provides a lower uncertainty value that relates to
a conﬁdence interval for my to be near the ground truth py.
On the other hand, MdVM deduction provides a different my
value than SL and MM deduction, but the uncertainty value it
provides also relates to the conﬁdence interval. Given that the
SL framework requires an accurate representation of the mean
of the posterior and only an approximation for the variance,
MM deduction can be viewed as another SL operator, but not
MdVM deduction.
In a multiagent scenario, different agents can form different
opinions about x and the conditional rules based on their
observations. In this paper, we assume each agent is able to
properly observe the coin ﬂip experiments and each agent is
truthful. These assumptions lead to the fact that the proper
processing is to perform consensus for each of the proposi-
tions and conditional rules ﬁrst followed by deduction. This
consensus before deduction (CBD) approach can be thought
of as a centralized fusion architecture where each agent sends
their opinion to a central agent who performs fusion. The
approach, while proper, can lead to heavy bandwidth usage
by sending many opinions to the central agent.
An alternative approach is for each agent to perform de-
duction using their local opinions, and send a smaller set of
inferred opinions to the central agent who simply performs
consensus. This distributed deduction before consensus (DBC)
approach is attractive from a networking perspective. However,
it must be noted that DBC can lead to vastly different
opinions than CBD. The next section will investigate how
each deduction method fares under the DBC architecture.
Nevertheless, much research is needed to understand the error
bounds associated to DBC so that fusion architectures with
guaranteed performance can be designed.
In some extreme cases, DBC can perform very poorly.
For example, consider a case where Agent A has opinions
ωA
x = (1, 0, 0), ωA
y|x = (0, 0, 1), and ωA
y|¯x = (0, 0, 1), and
that Agent B has opinions ωB
x = (0, 0, 1), ωB
y|x = (1, 0, 0),
and ωB
y|¯x = (0, 1, 0). Agent A has complete evidence about
x but has collected no evidence about the rules. Conversely,
Agent B has collected independent evidence to obtain full
knowledge of the rules but no evidence for x. Although the
evidences collected by Agents A and B are independent, CBD
and DBC give vastly different opinions. For any of the three
methods, the proper CBD approach leads to ωA⋄B
y∥x = (1, 0, 0),
but the DBC approach leads to ωA⋄B
y∥x = (0, 0, 1). The problem
is that opinions on x and on the rules enhance each other
in determining the inferred opinion. In this extreme case,
both agents can only deduce a vacuous opinion because they
either had no evidence for the proposition or for the rules.
Because the opinions enhance each other, we expect that for
independent observations, CBD provides opinions with lower
uncertainty. In short, deduction creates a dependency between
deduced opinions that SL consensus does not account for.
For another case, consider that the number of agents is
growing without bound, all agents use the same opinions for
the conditional rules, and the antecedent x is always true, i.e.,
px = 1. In the CBD approach the fusion of the opinions
on x by the agents will converge to the dogmatic opinion
ω⋄
x = (1, 0, 0), and the uncertainty of the deduced opinion
will converge to uy|x. On the other hand, DBC allows for
the fused uncertainty to converge to zero. Because the agents
are utilizing the same opinions about the rules, the deduced
opinions are not statistically independent, and consensus is
overly optimistic about the quality of the fused opinion.
V. SIMULATIONS
The three deduction methods were evaluated by simulating
opinions formed by actual coin ﬂip observations. The ground
truth for proposition x varied from px = 0 up to px = 1,
and the conditional probabilities were set to py|x = 0.8 and
py|¯x = 0.1. All agents formed opinions for the antecedent ωx
using Nx = 10 observations and formed opinions for the two

conditionals ωy|x and ωy|¯x using Ny|x = Ny|¯x = 100 obser-
vations. For a given value of px, the deduction methods were
evaluated over 100 Monte Carlo simulations of the formed
opinions. In all simulations, W = 2 and ax = ay = 0.5.
Figure 2 plots the error performance for the three methods
in terms of average bias, standard deviation, and root mean
squared (RMS) error average over the 100 Monte Carlo simu-
lation per a given px. The solid color curves are the computed
errors using the ground truth py = 0.8px + 0.1(1 −px) for
the three methods, and the corresponding color dashed curves
are the average predicted error derived from the uncertainty
uy via (11) and (13), which has no knowledge of the ground
truth. The actual errors for SL and MM deduction are the
same because they produce the same my value. However, the
high uncertainty for SL deduction overestimates the standard
deviation. On the other hand, the uncertainty for MM and
MdVM deduction is able to accurately portray the error
spread. The MdVM method exhibits the smallest bias, and the
uncertainty for MdVM also accurately portrays its bias. The
predicted bias for the MM is similar to that of the MdVM
despite the higher bias for the MM method.
Figure 3 provides the error performance results for 100
agents forming independent opinions, transmitting their de-
duced y to a central agents who computes a consensus opinion
for y, i.e., the DBC fusion approach. As mentioned in the pre-
vious section, DBC is not the optimal approach. Furthermore,
SL and MM deduction now provide different results. Overall,
the uncertainty value for the MM and MdVM methods can
still predict the standard deviation, but they predict much less
bias. The uncertainty for the SL method still overestimates the
standard deviation. It is interesting to note that SL deduction
usually achieves less RMS error than MM deduction when the
base rate ay = 0.5 does not match the ground truth py. On the
other hand, the MdVM method is able to predict the spread
and achieve better RMS error because of its lower bias.
Figure 4 shows the error performance results for the 100
agents that transmit their intrinsic opinions to the central
agent that performs CBD. For this case, the overall biases
are signiﬁcantly smaller than the DBC case. As in the single
agent case, the error performance for the SL and MM methods
are the same, but the MM method is able to predict the
actual errors from its uncertainty value. In effect, this CBD
is equivalent to a single agent with Nx = 1, 000 and Ny|x =
Ny|¯x = 10, 000 observations. With so many observations, the
mean and mode are almost equal, and the performances of the
MM and MdVM methods are almost identical. Interestingly,
the standard deviations due to the CBD approaches are similar
to that of the DBC approaches. This is probably due to the fact
that the agents collected independent evidences for both the
antecedent and rules, and the amount of evidence was balanced
across the agents. In other words, the simulated scenario
enables DBC to be viable unlike the conditions discussed in
previous section. Some preliminary results with unbalanced
observations over agents have shown much lower uncertainty
for DBC than CBD as one would expect from independent
agents since observations of propositions and rules enhance
each other in the deduction operation. These results also
indicate that MdVM is usually better than SL, which in turn
is usually better than MM.
VI. CONCLUSIONS
This work introduces two new deduction methods that
operate over subjective opinions referred to as moment and
mode/variance matching (MM and MdVM, respectively). Un-
like the traditional SL deduction method, the uncertainty from
the computed opinions for the consequent are able to predict
the conﬁdence bounds for the mean projection (or estimate)
in the two new methods. However, the MM deduction method
is unable to predict its bias. We believe that this is why the
performance of the MdVM is usually better than MM for DBC
type fusion of opinions.
SL combines probabilistic logic with uncertainty. It is com-
putationally appealing by representing uncertainty for each
proposition (or rule) as a single parameter that with the belief
values map to a beta (or more generally a Dirichlet) distribu-
tion for the possible ground truth generating probabilities given
the evidence. After many logical operations such as deduction,
this representation of uncertainty is an approximation which
comes with a cost. As seen in the simulations, the uncertainty
does not necessarily predict both the bias and variance inherent
in the posterior distribution of the consequent. Furthermore,
it is unclear how to control the approximation error when
combining deduction with fusion in a distributed manner.
For instance, the bias in DBC fusion initially goes down
when combining more agents, but it eventually saturates.
Future work should investigate the conditions that causes DBC
to provide diminishing return and determine the maximum
number agents that can be fused before saturation occurs.
Furthermore, the implication on DBC for multiple agents
using common or dependent opinions as well as variations
in the amount of evidences to form opinions need to be
better understood. This understanding will hopefully lead to
principled methods to design distributed reasoning engines that
incorporate uncertainty.
ACKNOWLEDGEMENTS
The authors wish to thank Nir Oren, Audun Jøsang, Mani
Srivastava, Federico Cerutti, Jeffery Pan, Timothy Norman,
Achille Fokoue, and Katia Sycara for valuable discussions and
suggestions that inspired this work.
REFERENCES
[1] N. J. Nilsson, “Probabilistic logic,” Artiﬁcial Intelligence, vol. 28, no. 1,
pp. 71–87, Feb. 1986.
[2] M. Richardson and P. Domingos, “Markov logic networks,” Machine
Learning, vol. 62, no. 1–2, pp. 107–136, 2006.
[3] A. Jøsang, “Subjective logic,” Jul. 2011, draft book in preparation.
[4] A. Jøsang, S. Pope, and M. Daniel, “Conditional deduction under un-
certainty,” in Proceedings of the 8th European Conference on Symbolic
and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU
2005), 2005.
[5] A. Jøsang, “Conditional reasoning with subjective logic,” in Proceedings
of the 8th European Conference on Symbolic and Quantitative Ap-
proaches to Reasoning with Uncertainty (ECSQARU 2005), Barcelona,
Spain, Jul. 2005.

0
0.2
0.4
0.6
0.8
1
−0.08
−0.06
−0.04
−0.02
0
0.02
0.04
0.06
0.08
 px
Bias
 
 
SL
MM
MdVM
(a)
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
 px
Standard Deviation
 
 
SL
MM
MdVM
(b)
0
0.2
0.4
0.6
0.8
1
0.02
0.04
0.06
0.08
0.1
0.12
0.14
 px
RMS Error
 
 
SL
MM
MdVM
(c)
Figure 2.
Quality of the deduced opinion versus ground truth for a single agent: (a) Bias, (b) standard deviation and (c) RMS error. Note that the actual
error curves for SL and MM overlap.
0
0.2
0.4
0.6
0.8
1
−0.06
−0.04
−0.02
0
0.02
0.04
0.06
 px
Bias
 
 
SL
MM
MdVM
(a)
0
0.2
0.4
0.6
0.8
1
0
0.005
0.01
0.015
 px
Standard Deviation
 
 
SL
MM
MdVM
(b)
0
0.2
0.4
0.6
0.8
1
0
0.01
0.02
0.03
0.04
0.05
0.06
 px
RMS Error
 
 
SL
MM
MdVM
(c)
Figure 3.
Quality of the deduced opinion versus ground truth from 100 agents fused via DBC: (a) Bias, (b) standard deviation and (c) RMS error.
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3 x 10
−3
 px
Bias
 
 
SL
MM
MdVM
(a)
0
0.2
0.4
0.6
0.8
1
2
4
6
8
10
12
14
16 x 10
−3
 px
Standard Deviation
 
 
SL
MM
MdVM
(b)
0
0.2
0.4
0.6
0.8
1
2
4
6
8
10
12
14
16 x 10
−3
 px
RMS Error
 
 
SL
MM
MdVM
(c)
Figure 4.
Quality of the deduced opinion versus ground truth from 100 agents fused via CBD: (a) Bias, (b) standard deviation and (c) RMS error. Note that
actual error curves for SL and MM overlap and exhibit almost similar performance as the MdVM curves.
[6] C. Chesnevar, G. Simari, T. Alsinet, and L. Godo, “A logic programming
framework for possibilistic argumentation with vague knowledge,” in
Proc. of the 20th Conference on Uncertainty in Artiﬁcial Intelligence,
Banff, Canada, Jul. 2004, pp. 76–84.
[7] M. S¸ensoy, A. Fokoue, J. Z. Pan, T. J. Norman, Y. Tang, N. Oren,
and K. Sycara, “Reasoning about uncertain information and conﬂict
resolution through trust revision,” in Proc. of the Autonomous Agents
and Multiagent Systems (AAMAS) Conference, St. Paul, MN, May 2013.
[8] S. Han, B. Koo, A. Hutter, and W. Stechele, “Forensic reasoning upon
pre-obtained surveillance metadata using uncertain spatio-temporal rules
and subjective logic,” in Analysis, Retrieval and Delivery of Multimedia
Content, N. Adami, A. Cavallaro, R. Leonardi, and P. Migliorati, Eds.
Springer, 2013, pp. 125–157.
[9] L. M. Kaplan, S. Chakraborty, and C. Bisdikian, “Fusion of classiﬁers:
A subjective logic perspective,” in Proc. of the IEEE Aerospace Con-
ference, Big Sky, MT, Mar. 2012.
[10] ——, “Subjective logic with uncertain partial observations,” in Proc.
of the International Conference on Information Fusion, Singapore, Jul.
2012.
[11] A. Jøsang, “The consensus operator for combining beliefs,” Artiﬁcial
Intelligence Journal, vol. 142, no. 1–2, pp. 157–170, Oct. 2002.
[12] K. Miura, “An introduction to maximum likelihood estimation and
information geometry,” Interdisciplinary Information Sciences, vol. 17,
no. 3, pp. 155–174, 2011.
[13] A. K. Gupta and S. Nadarajah, Eds., Handbook of Beta Distribution and
Its Applications.
CRC Press, 2004.
View publication stats
View publication stats

