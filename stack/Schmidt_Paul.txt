Paul Schmidt
Bayesian inference for structured
additive regression models for
large-scale problems with applications to
medical imaging
Dissertation an der Fakultät für Mathematik, Informatik und Statistik
der Ludwig-Maximilians-Universität München
Eingereicht am 07. November 2016


Paul Schmidt
Bayesian inference for structured
additive regression models for
large-scale problems with applications to
medical imaging
Dissertation an der Fakultät für Mathematik, Informatik und Statistik
der Ludwig-Maximilians-Universität München
Eingereicht am 07. November 2016

Erster Berichterstatter: Prof. Dr. Volker Schmid
Zweiter Berichterstatter: Prof. Dr. Thomas Kneib
Dritter Berichterstatter: Prof. Dr. Mark Mühlau
Tag der Disputation: 19. Januar 2017

Danksagung
Diese Arbeit würde ohne die Unterstützung vieler Menschen nicht existieren. Allen voran
möchte ich mich ganz herzlich bei meinem Doktorvater Prof.
Dr.
Volker Schmid für
die Möglichkeit der Promotion sowie die tolle Betreuung trotz räumlicher Entfernung
bedanken.
Ein herzlicher Dank geht auch an Prof. Dr. Mark Mühlau für die exzellente Betreuung
und Zusammenarbeit sowie die zahlreichen Diskussionen. Durch sein Vertrauen in meine
Arbeit war es mir möglich, auch fernab von München für seine Arbeitsgruppe zu arbeiten,
woraus überhaupt die Idee für diese Arbeit entstand. In diesem Rahmen möchte ich mich
auch bei der Arbeitsgruppe Morphometrie des Neuroimaging Center der TU München
bedanken, insbesondere bei Dr.
Viola Biberacher für die angenehme und reibungslose
Zusammenarbeit.
Bei der Arbeitsgruppe Schmerz bedanke ich mich bei Dr.
Elisabeth
May, Dr. Laura Thiemann sowie Moritz Nickel für die tolle Atmosphäre im und die schöne
Zeit außerhalb des Büros.
Des Weiteren möchte ich mich bei Prof. Dr. Thomas Kneib für den aufschlussreichen
E-Mail-Verkehr sowie bei Dr. Stephanie Thiemichen für die hilfreichen Erläuterungen zum
Promotionsvorgang bedanken.
Ein sehr großer Dank geht an meine Eltern, Monika und Hans-Joachim Schmidt. Ohne
ihre Unterstützung wäre ich nicht dort, wo ich jetzt bin. Ebenfalls möchte ich mich bei
Dr. Coralie Wink und Prof. Dr. Michael Wink für die nützlichen Hinweise bedanken.
Charlotte Wink und Laura Menz danke ich ganz herzlich für die angenehme Unterbringung
sowie Fürsorge während meiner zahlreichen Besuche in München.
Besonders bedanken möchte ich mich bei meiner Partnerin Lucie Wink.
Durch ihr
Vertrauen, ihren Rückhalt und Beistand hatte sie indirekt einen großen Anteil an der
Fertigstellung dieser Arbeit.
Und schließlich möchte ich mich bei unserem Sohn Jakob
dafür bedanken, dass er die letzten fünf Monate zu den schönsten meines Lebens gemacht
hat.


Zusammenfassung
In der angewandten Statistik können Regressionsmodelle mit hochdimensionalen Koeﬃzien-
ten auftreten, die sich nicht mit gewöhnlichen Computersystemen schätzen lassen. Dies be-
triﬀt unter anderem die Analyse digitaler Bilder unter Berücksichtigung räumlich-zeitlicher
Abhängigkeiten, wie sie innerhalb der medizinisch-biologischen Forschung häuﬁg vorkom-
men.
In
der
vorliegenden
Arbeit
wird
ein
Verfahren
formuliert,
das
in
der
Lage
ist, Regressionsmodelle mit hochdimensionalen Koeﬃzienten und nicht-normalverteilten
Zielgrößen unter moderaten Anforderungen an die benötigte Hardware zu schätzen. Hierzu
wird zunächst im Rahmen strukturiert additiver Regressionsmodelle aufgezeigt, worin
die Limitationen aktueller Inferenzansätze bei der Anwendung auf hochdimensionale
Problemstellungen liegen, sowie Möglichkeiten diskutiert, diese zu umgehen.
Darauf
basierend wird ein Algorithmus formuliert, dessen Stärken und Schwächen anhand von
Simulationsstudien analysiert werden. Darüber hinaus ﬁndet das Verfahren Anwendung
in drei verschiedenen Bereichen der medizinisch-biologischen Bildgebung und zeigt
dadurch, dass es ein vielversprechender Kandidat für die Beantwortung hochdimensionaler
Fragestellungen ist.
Summary
In applied statistics regression models with high-dimensional coeﬃcients can occur which
cannot be estimated using ordinary computers. Amongst others, this applies to the analysis
of digital images taking spatio-temporal dependencies into account as they commonly occur
within bio-medical research.
In this thesis a procedure is formulated which allows to ﬁt regression models with
high-dimensional coeﬃcients and non-normal response values requiring only moderate
computational equipment.
To this end, limitations of diﬀerent inference strategies for
structured additive regression models are demonstrated when applied to high-dimensional
problems and possible solutions are discussed. Based thereon an algorithm is formulated
whose strengths and weaknesses are subsequently analyzed using simulation studies.
Furthermore, the procedure is applied to three diﬀerent ﬁelds of bio-medical imaging from
which can be concluded that the algorithm is a promising candidate for answering high-
dimensional problems.


Contents
1 Introduction
1
1.1 Large-scale problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 The problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.2 Diﬀerentiation from other big data problems . . . . . . . . . . . . . . . . 2
1.1.3 How big is too large? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1.4 Previous work on high-dimensional regression models . . . . . . . . . . . 3
1.2 Applications in medical imaging
. . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.1 Tissue segmentation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.2 Pixel- and voxel-wise regression models . . . . . . . . . . . . . . . . . . . 5
1.2.3 Spatial information in object-based co-localization . . . . . . . . . . . . . 6
1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.3.1 Thesis objectives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.3.2 Structure of thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.3.3 Contributed Manuscript
. . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2 Structured additive regression models
11
2.1 Observation model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.2 Prior speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2.1 Regression coeﬃcients
. . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2.2 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.3 Chapter summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3 Inference
29
3.1 MCMC based inference for STAR models
. . . . . . . . . . . . . . . . . . . .
30
3.1.1 Regression coeﬃcients
. . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.1.2 Precision parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.1.3 Dispersion parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.1.4 Additional considerations . . . . . . . . . . . . . . . . . . . . . . . . . .
36
I

Contents
3.1.5 Fully Bayes inference based on MCMC . . . . . . . . . . . . . . . . . .
39
3.2 Empirical Bayes inference for STAR models . . . . . . . . . . . . . . . . . . .
40
3.2.1 Mixed model representation . . . . . . . . . . . . . . . . . . . . . . . .
40
3.2.2 Estimation of regression coeﬃcients . . . . . . . . . . . . . . . . . . . .
42
3.2.3 Estimation of precision and dispersion parameters . . . . . . . . . . . .
43
3.2.4 Empirical Bayes inference using mixed model representation
. . . . . .
45
3.3 Approximate inference for STAR models . . . . . . . . . . . . . . . . . . . . .
46
3.3.1 Exploring the marginal posterior of θ
. . . . . . . . . . . . . . . . . .
47
3.3.2 Approximation of the full conditional of xj . . . . . . . . . . . . . . . .
49
3.3.3 Approximation of the marginal posterior of xj . . . . . . . . . . . . . .
50
3.3.4 Approximate inference using the INLA approach . . . . . . . . . . . . .
50
3.4 Chapter summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
4 Adaptations to large-scale problems
53
4.1 General considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
4.1.1 Sparseness as a prerequisite
. . . . . . . . . . . . . . . . . . . . . . . .
53
4.1.2 Comparison of inference strategies . . . . . . . . . . . . . . . . . . . . .
54
4.1.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.2 Large-scale inference using MCMC . . . . . . . . . . . . . . . . . . . . . . . .
60
4.2.1 Sampling from zero-mean Gaussians . . . . . . . . . . . . . . . . . . . .
61
4.2.2 Solving systems of linear equations
. . . . . . . . . . . . . . . . . . . .
67
4.2.3 Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.2.4 Calculation of log-determinants
. . . . . . . . . . . . . . . . . . . . . .
72
4.2.5 On-line calculation of posterior moments . . . . . . . . . . . . . . . . .
76
4.3 Chapter summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
5 Simulation studies
81
5.1 Performance of approximations . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.1.1 Simulation setups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.1.2 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
5.1.3 Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
5.1.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
5.1.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
5.2 Assessing the impact on the ﬁnal results . . . . . . . . . . . . . . . . . . . . .
91
5.2.1 Simulation setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
5.2.2 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
II

Contents
5.2.3 Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
5.2.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
5.3 Chapter summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6 Applications
105
6.1 Supervised segmentation of MS lesions . . . . . . . . . . . . . . . . . . . . . . 105
6.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
6.1.2 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.1.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.1.4 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.2 Diﬀerences in age-related atrophy of gray matter in MS patients . . . . . . . . 116
6.2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
6.2.2 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
6.2.4 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.3 Object-based co-localization by a spatial point process . . . . . . . . . . . . . 122
6.3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
6.3.2 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
6.3.4 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7 Summary and outlook
131
7.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.2 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
Appendix
I
A Implementation details
I
A.1 Working stations and software
. . . . . . . . . . . . . . . . . . . . . . . . . . . I
A.2 Krylov subspace methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I
A.2.1 Preconditioned conjugate gradients . . . . . . . . . . . . . . . . . . . . . II
A.2.2 Preconditioned Lanczos algorithm for approximate sampling . . . . . . III
A.3 Approximation of log-determinants . . . . . . . . . . . . . . . . . . . . . . . . III
A.3.1 Stochastic Chebyshev expansion . . . . . . . . . . . . . . . . . . . . . . III
A.3.2 Lanczos algorithm for extreme eigenvalues . . . . . . . . . . . . . . . . III
References
IX
III


1 Introduction
1.1 Large-scale problems
1.1.1 The problem
The concept of regression analysis is the most common statistical problem applied
statisticians are faced with today. Here, the main goal is to describe the relationship between
a number of covariates on a response variable of interest. Despite all methodological and
technical progress the statistical community has witnessed in the last decades there are
still situations where it is not able to ﬁt even simple regression models due to the size of
the problem. Consider for example a regression model whose linear predictor contains a
random intercept to account for heterogeneity among certain statistical units:
ηi = · · · + γUnit[i] + · · · .
If the number of units, m, exceeds a certain limit, i.e. if the dimension of γ = (γUnit[1], . . . ,
γUnit[m])′ is too large, it may not be possible to obtain a solution for this relatively simple
regression problem due to restricted computational resources. The situation worsens if, in
addition, special dependency structures are assumed between units. For example, the units
may represent days of the year which may require the inclusion of some form of temporal
dependency. Similarly, if the units are spatially aligned, it may be useful to account for
spatial information by appropriate model extensions.
The statistical analysis of digital
images (Besag, 1989) represents an example of the latter. Here, the number of units, i.e.
the number of picture elements (pixels) or volume elements (voxels), is directly related to
the resolution of the image. The higher the resolution, the more pixels are included into the
analysis which leads to larger regression coeﬃcients. Furthermore, the spatial alignment
over a regular lattice represents a natural source of dependency which needs to be accounted
for.
1

1 Introduction
In this thesis, a method is provided that allows to ﬁt models with high-dimensional
or large-scale regression coeﬃcients on moderate working stations.
It is assumed that
the problem can be formulated as a regression problem, that is, a well deﬁned response
vector as well as certain explanatory variables are available.
The problem of interest
arises if the dimension of one or more regression coeﬃcients are too large. Usually, when
estimating the coeﬃcients of regression models, it is required to solve systems of linear
equations. If regression coeﬃcients are high-dimensional, solving these systems may become
impossible given only moderate computational equipment. Additional problems arise when
non-Gaussian response variables are considered such as in the framework of generalized
linear models (GLMs, Nelder and Wedderburn, 1972).
The necessity to provide a solution to the above problem may come as a surprise as we
are constantly faced with technological progress (Moore et al., 1975). In particular, one
could argue that it will not be long until these problems can be solved using aﬀordable
hardware. The following points hold against this argument: First, in order to keep pace
with the newest technological achievements a certain ﬁnancial basis is required which may
not be available to all applied statisticians. Second, regression problems that are on the
edge of current computational abilities will always exist. Thus, in providing a way to solve
such problems using available hardware means that the critical limit for which no more
solutions can be obtained is raised even for future problems. Thus, the method introduced
in this thesis allows one to encounter problems that are diﬃcult to handle even with future
equipment. This will increase the ability to solve large-scale regression problems not only
for today, but also for the forthcoming years and decades.
1.1.2 Diﬀerentiation from other big data problems
Recently, the term “big data” has gained considerable attention in the public discourse. As a
result, many resources went into the development of strategies that are able to solve such big
data problems. Usually, methods that are designed to work with big data can be identiﬁed
as data mining tools that are able to handle huge amounts of observations. In contrast, the
present thesis puts the focus on valid statistical inference by the use of regression analyses.
Thus, the expression “big data” is explicitly avoided in this thesis. Instead, the terms “large-
scale” or “high-dimensional” are used. However, for these expressions no clear deﬁnition
exists in the statistical literature. Often, “high-dimensional” refers to so-called large p small
n problems, that is, situations where there are more unknown parameters p than data points
n (Johnstone and Titterington, 2009). Such problems can be found, for example, in the
2

1.1 Large-scale problems
analysis of microarrays and related ﬁelds in genomics (Bickel et al., 2009). Handling large
p small n situations usually means to apply inference to a set of relevant predictors, i.e.
to perform some kind of variable selection. Shrinkage or penalization approaches such as
Ridge regression (Hoerl and Kennard, 1970) or the Lasso method (Tibshirani, 1996) can
be applied to these kind of problems. While, under certain conditions, large p small n
problems can also be treated with the methods presented in this thesis, they are not of
primary interest here.
1.1.3 How big is too large?
Due to new technological achievements the critical size of the dimension of regression
coeﬃcients is a dynamic variable rather than a constant.
Today, statisticians are able
to ﬁt models in a few minutes that would have taken days or weeks one or two decades ago.
Similarly, situations that are problematic today may be considered as moderate tomorrow.
Thus, technological progress will always ensure that the term “too big” does not constitute
a ﬁxed deﬁnition. Obviously, this discussion also applies to the term „moderate“ as in
„moderate working station“. Therefore, in this thesis data situations are considered as large-
scale or high-dimensional if standard software packages fail to execute the corresponding
analysis on ordinary working stations. Of course, the methods presented in this thesis also
have their limit. However, it is expected that this limit lies several factors above the one
that is processable by standard software packages.
1.1.4 Previous work on high-dimensional regression models
In recent years diﬀerent approaches for ﬁtting regression models with large-scale coeﬃcients
have been published. Due to its option of working on each coeﬃcient vector separately,
most methods perform the ﬁtting process via Gibbs sampling within a fully Bayesian setup
based on Markov chain Monte Carlo (MCMC, see Section 3.1). The main computational
bottleneck when applying the Gibbs sampler to large-scale regression problems is the
generation of samples from high-dimensional Gaussians (Section 4.2.1). Thus, the focus
of most approaches lies on techniques to overcome this problem.
For example, in the
context of high-dimensional inverse problems with regard to the analysis of digital images,
Bardsley (2012) utilized iterative methods for sparse linear systems as discussed in Section
4.2.2. Zhang et al. (2013) sampled a proposal from a high-dimensional Gaussian using
the ordinary Cholesky decomposition with special permutation techniques (Section 4.2.1).
3

1 Introduction
They were able to successfully apply their algorithm to seismic tomography data with
more than 11,000 parameters. However, this number is not comparable to the parameters
in later applications of this thesis which all include more than 500,000 components to
estimate. In order to estimate spatially varying eﬀects, Ge et al. (2014) sampled from large
Gaussians utilizing a checkerboard-like conditional independence structure which occurs
naturally on a regular lattice. Although easy to implement, this approach is limited to
special situations and further adaptations are needed to guarantee good mixing of Markov
chains for all parameters (Section 4.2.1). Furthermore, all of the approaches mentioned
above are restricted to Gaussian response models – Ge et al. utilized the auxiliary variable
method by Albert and Chib (1993) to encounter the non-normal nature of their data.
However, a declared aim of this thesis is to provide a framework that is able to ﬁt models
with general responses. On this subject, only a few works have been published. The most
promising approach is the one recently proposed by Wood et al. (2015). They presented
a relatively fast method for ﬁtting generalized additive models to large data sets within
a frequentist setting by utilizing iterative updating schemes for the factorization of model
matrices. However, their approach still fails with respect to computational requirements
when applied to the applications presented in later chapters of this thesis.
1.2 Applications in medical imaging
In the last half of the 20th century new medical imaging devices were developed.
Subsequently, new statistical methods for the analysis of the corresponding medical images
emerged which also includes diﬀerent forms of regression analysis. In the following, three
applications of regression analysis to bio-medical images are presented.
1.2.1 Tissue segmentation
Medical images play an important part in the process of diagnosis of various diseases.
Consider, for example, the identiﬁcation of cancerous tissue in digital mammography,
visualization of lung tumors by computed tomography, or the recognition of inﬂammatory
plaques in three-dimensional magnetic resonance (MR) images of the brain of patients with
multiple sclerosis (MS). In all these cases it is required to classify (segment) the depicted
parts of the body into normal appearing and suspicious tissue. In addition, non-diagnostic
applications of tissue segmentation exist as well. For example, in neuroimaging classiﬁcation
4

1.2 Applications in medical imaging
of head MR images into the three major components of the brain, i.e.
cerebrospinal
ﬂuid, gray matter (GM), and white matter (WM), is required in order to measure and
compare GM atrophy along diﬀerent cohorts or patient groups. While manual segmentation
by trained experts is a valid option, it is advantageous to use automatic segmentation
algorithms in order to minimize rater speciﬁc biases as well as costs and operator time.
In general, tissue segmentation methods can be divided into supervised and unsupervised
approaches (Bezdek et al., 1992).
The most popular methods among unsupervised
approaches are cluster algorithms. Hierarchical clustering (Ohkura et al., 2000) as well as
more sophisticated model-based clustering techniques (Wells III et al., 1996) are commonly
used. Within the latter class ﬁnite mixtures of Gaussians take a special position. The
advantage of ﬁnite mixture models is that they can easily be expanded in order to include
spatial dependencies by the use of Markov random ﬁelds (Winkler, 2003).
Supervised
approaches, on the other hand, mostly use methods from the machine learning community
such as classiﬁers based on k-nearest neighbors (Anbeek et al., 2004), neural networks
(Ghosh et al., 1991), and classiﬁers that are trained by support vector machines (Wang
et al., 2001). One approach that has received very little attention in the past is image
segmentation based on binary regression models. This is due to the fact that individual
training of voxels is often not possible as not all voxels show the tissue of interest and, thus,
no valid results can be obtained for these voxels. A possible solution to this would be to
consider all voxels jointly when training classiﬁers based on binary regression models. In
this context it may be helpful to consider spatial dependencies between voxels. Such an
approach is taken in Section 6.1 of this thesis in order to formulate a supervised MS lesion
segmentation algorithm.
1.2.2 Pixel- and voxel-wise regression models
Another application of regression models within the analysis of medical images are pixel- or
voxel-wise regression models. Such models are widely used in neuroimaging. For example,
when identifying activated brain regions by the use of functional magnetic resonance
imaging (fMRI) time series models are ﬁtted to each voxel in order to obtain activation maps
of the brain (Lindquist et al., 2008). Once these activation maps for individual subjects
have been computed they are further compared among groups or experimental conditions
by a so-called second-level analysis. Here, normalized activation maps are analyzed by a
general linear model (Worsley and Friston, 1995), that is, simple linear models are ﬁtted
for all voxels separately. Application of this approach to the analysis of local GM volume
5

1 Introduction
obtained from structural MR images is known as voxel-based morphometry (Ashburner and
Friston, 2000). With respect to non-brain medical images pixel-wise regression models are
commonly seen in quantitative analysis of dynamic contrast-enhanced MR images. Here,
nonlinear regression models are ﬁtted to each pixel of a time series of images in order to
estimate kinetic parameters within pharmacokinetic models (Tofts, 1997).
Although pixel- and voxel-wise regression models appear in many diﬀerent applications,
the main modeling approach is nevertheless rather similar.
For example, for Gaussian
responses the following procedure can often be observed. First, in order to account for
spatial dependencies among voxels the images of all subjects are smoothed by a special
ﬁlter which most probably depends on a pre-chosen smoothing parameter. Subsequently,
a linear regression model is ﬁtted to each voxel separately. Although this approach yields
useful results its disadvantage is that spatial information can only be accounted for by
modifying the actual data using a smoothing parameter that is chosen by the user. Instead,
it is favorable to estimate the amount of smoothness automatically from the original data,
as this does not only increases the signal to noise ratio (Penny et al., 2005) but also leads
to more powerful analyses (Schmidt et al., 2013).
However, estimating the smoothing
parameter adaptively from the data requires to model all voxels jointly.
Such models
have been extensively studied with respect to fMRI time-series analysis.
For example,
Gössl et al. (2000) ﬁtted Gaussian response models within a fully Bayesian setup based on
MCMC. Their procedure allows to derive the full conditionals for each voxel and, thus, to
work on each voxel separately by still considering the joint model formulation. However,
updating the parameters for each voxel independently may result in insuﬃcient mixing as
well as slow convergence of Markov chains to their corresponding stationary distributions
(Knorr-Held and Rue, 2002). In contrast, the methods derived in this thesis allow updating
these parameters jointly. For example, in Section 6.2 a large-scale voxel-wise regression
model is formulated for images of local GM volume obtained from structural MR images
over 565,475 voxels. In addition, the presented approach allows ﬁtting models with non-
Gaussian responses as well.
1.2.3 Spatial information in object-based co-localization
Statistical methods play an important role in the analysis of digital microscopy images.
Early applications are the identiﬁcation – detection as well as segmentation – of objects of
interest, such as cells or sub-cellular structures. For this task a broad range of methods exist,
see for example Xing and Yang (2016) for an extensive review. Besides object identiﬁcation
6

1.3 Outline
and labeling the analysis of spatial relationships between sub-cellular structures and their
relation to cell processes are of particular interest. This kind of analysis is known as co-
localization (Dunn et al., 2011). In particular, by using ﬂuorescence microscopy specially
marked cell components can be identiﬁed and visualized in high resolution. This allows to
draw conclusions about the appearance of certain cellular objects in the presence of other
structures within their neighborhood. Early methods in co-localization analyze intensity
values of pixels by the use of correlation coeﬃcients (Manders et al., 1992) or overlap
measures (Manders et al., 1993). In object-based co-localization, on the other hand, the
available data is limited to identiﬁed (segmented) objects of interest for which the distances
to other structures are recorded. By thresholding these distances the eﬀect of neighboring
structures on the objects of interest can be revealed. Recently, methods that do not rely on
thresholding but explore interaction of cellular structures as a function of their distances
instead, have been proposed (Helmuth et al., 2010). While these approaches allow more
ﬂexibility in modeling the relation between neighboring structures, they miss the potential
eﬀect of the location of the objects of interest within the cell itself. In Section 6.3, a Poisson
process which is able to account for this spatial information is ﬁtted to the data of a 3D
ﬂuorescence microscopy image of 514,442 voxels.
1.3 Outline
1.3.1 Thesis objectives
This work has four main objectives.
The ﬁrst goal is the identiﬁcation of potential
vulnerabilities of popular inference strategies when applied to high-dimensional problems.
Here, the focus lies on methods that are able to ﬁt structured additive regression (STAR,
Fahrmeir et al., 2004) models. Once these vulnerabilities have been identiﬁed and discussed,
an inference strategy has to be selected for which solutions to these problems are presented
in the second step. These solutions should be designed in a way so that they can be applied
with low to moderate computational equipment. At the end, a complete framework for the
estimation of large-scale STAR models should be formulated. The third objective involves
the analysis of the performance of the proposed methods, that is, it should be shown in
which way these methods have an inﬂuence on the ﬁnal inference. Of particular importance
is the identiﬁcation of situations where one can expect to obtain good results and situations
7

1 Introduction
where applying these methods is not suitable. Finally, by applying the complete framework
to diﬀerent real world problems its practical relevance will be demonstrated.
1.3.2 Structure of thesis
The rest of this thesis is organized as follows. First, the theoretical foundations of STAR
models are presented. To this end, Chapter 2 provides the conceptual bases by introducing
all necessary model components and reviewing certain eﬀect types. In Chapter 3, three
currently used approaches for ﬁtting STAR models are reviewed.
This includes a fully
Bayesian approach based on MCMC, an empirical Bayes method utilizing a mixed model
representation, and the integrated nested Laplace approximation approach for approximate
inference.
Chapter 4 analyzes these approaches ﬁrst in terms of their usefulness with
respect to high-dimensional problems. As a result, one approach is then selected which
is subsequently adapted to large-scale problems. The performance of these adaptations
and the eﬀect on the ﬁnal results are extensively analyzed in Chapter 5 by simulation
studies. These insights are used to formulate a complete framework which is applied to
diﬀerent examples from the ﬁeld of bio-medical imaging in the following chapter.
The
thesis concludes with a summary chapter and outlook (Chapter 7), as well as an appendix
with implementation details.
1.3.3 Contributed Manuscript
Some parts of this thesis can be found in the following work which has been published in
the preparation of this thesis:
Schmidt, P., Mühlau, M., and Schmid, V. Fitting large-scale structured additive
regression models using Krylov subspace methods. Computational Statistics &
Data Analysis, 105:59 – 75, 2017.
The individual contributions of the authors to this article are as follows: The main idea of
using Krylov subspace methods emerged from discussions between Volker Schmid and Paul
Schmidt. In addition, Volker Schmid supervised the overall structure of the article and all
statistical aspects. He also initiated larger simpliﬁcations and more detailed explanations of
the manuscript. Furthermore, he was involved in the formulation of answers to the reviewers
within the review process. Mark Mühlau provided the data set for the ﬁrst application and
proposed choosing data from the ADNI data base for the second application.
He also
8

1.3 Outline
provided information about Alzheimer’s disease and formulated the corresponding part in
the introduction of the second application. Most of the manuscript was written by Paul
Schmidt who also implemented the algorithm, and conducted the simulation as well as data
analysis. All authors participated in proof-reading the manuscript.
Detailed information about the content of this thesis which has already been published
in a modiﬁed form in the above article will be provided at the beginning of each chapter.
9


2 Structured additive regression models
This chapter reviews STAR models.
Adequate modeling of a large variety of eﬀect
types, including temporal, spatial and spatio-temporal eﬀects, as well as nonlinear eﬀects
for continuous covariates (Lang and Brezger, 2004) using this class of models will be
demonstrated. In addition, STAR models are appropriate for modeling large-scale problems
as discussed further in Chapter 4. For a more comprehensive introduction of STAR and
related models see, for example, Kneib (2006) and Fahrmeir and Kneib (2011).
The concept of regression considered here follows the idea of classical regression analysis,
namely that the conditional expectation of the ith response yi given the values of p
explanatory variables zi1, . . . , zip can be expressed as a function of these variables:
E(yi|zi1, . . . , zip) ≈f(zi1, . . . , zip).
(2.1)
Objective of this chapter is to specify all components of (2.1) that are required to fully
describe the functional relationship between the explanatory variables and the observed
response. Chapter 3 then presents ways to perform inference on all parameters of interest.
The information given in Section 2.1, 2.2.1, and 2.2.2 below can also be found in shortened
form in Schmidt et al. (2017, Section 2.1 and 2.2).
2.1 Observation model
Throughout the following chapters, the probability density function of the vector of response
variables y = (y1, . . . , yn)′ is denoted by p(y|θ), where θ is a collection of all unknown
parameters. It is further assumed that the response variables are conditionally independent
given explanatory variables z = (z′
1, . . . , z′
n)′ with zi = (zi1, . . . , zip)′, and unknown
parameters θ, so that p(y|θ) = Qn
i=1 p(yi|zi, θ).
The nonlinear relation given in (2.1)
is divided into the following structural assumptions: First, the conditional expectation
11

2 Structured additive regression models
µi = E(yi|zi, θ) is linked to the linear predictor ηi by a known link function g, i.e. g(µi) = ηi.
Alternatively, one can specify this relation equivalently by the inverse link function, i.e.
the response function h = g−1 and µi = h(ηi). Second, it is assumed that the general
nonlinear relationship between ηi and zi can be approximated by a linear combination of
nonparametric functions f1, . . . , fp of z:
ηi = f1(zi1) + · · · + fp(zip).
(2.2)
It is further assumed that the function evaluations f k = (fk(z1k), . . . , fk(znk))′ can be
written as a combination of a n × mk design matrix Zk and a mk × 1 vector of unknown
coeﬃcients γk as
f k = Zkγk.
(2.3)
This yields the linear predictor in compact matrix notation as
η = Z1γ1 + · · · + Zpγp.
With this deﬁnition, the set of unknown parameters θ consists of the regression coeﬃcients
γk, k = 1, . . . , p, and an additional dispersion parameter φ that may be included in p(y|θ).
If seen as a function of θ, p(y|θ) is referred to as the likelihood function.
The distributional assumption given above allows the consideration of a broad range
of distributions for the response variable.
Other authors, however, put more emphasis
on distributions that are exponential families (Fahrmeir and Tutz, 2001). This has the
advantage that well known results within the framework of generalized linear models
(GLMs, Nelder and Wedderburn, 1972) can easily be integrated into the estimation concepts
presented in Chapter 3. For exponential families the conditional distribution of yi given zi
can be written as
p(yi|zi) ∝exp
 yiϑ(µi) −b(ϑ(µi))
φ
ωi + c(yi, φ, ωi)
!
.
(2.4)
Here, ϑ is a function of the expectation µi and is called natural parameter, and φ is the
additional scale or dispersion parameter that was introduced before. The functions b(·)
and c(·) depend on the choice of the speciﬁc exponential family. Similar to the structural
assumptions above, the expectation µi is linked to the linear predictor ηi by a known
response function h, µi = h(ηi).
12

2.2 Prior speciﬁcation
2.2 Prior speciﬁcation
So far, p + 1 unknown parameters have been introduced:
the regression coeﬃcients
γk, k = 1, . . . , p, and the dispersion parameter φ.
To complete the model formulation,
prior distributions for these parameters and for additional hyperparameters need to be set
up. In order to simplify notation the subscript k is suppressed for γk in the remaining of
this chapter. Thus, the regression coeﬃcient is represented by the m-dimensional vector
γ = (γ1, . . . , γm)′.
2.2.1 Regression coeﬃcients
Within the concept of STAR models, priors for the regression coeﬃcients are usually
multivariate normals (Fahrmeir et al., 2004):
γ|κ ∼N(0, Q−1).
(2.5)
Here, N(µ, Σ) denotes for the multivariate normal distribution with mean µ and covariance-
matrix Σ = Q−1 or, alternatively, precision matrix Q = Σ−1 which usually depends on
a precision parameter κ, i.e.
Q = Q(κ).
As it is shown in subsequent sections, prior
information is typically induced by speciﬁc choices of the precision matrix: the elements in
γ can be directly related to each other in various ways by imposing diﬀerent assumptions
on the structure of Q, which, in turn, may result in appropriate prior conﬁgurations for
modeling temporal or spatial information. In order to further clarify the role of the elements
in Q it is useful to point out the connection of prior (2.5) to Gaussian Markov random ﬁelds
(GMRFs) and therefore to undirected graphs. According to Rue and Held (2005), a vector
γ = (γ1, . . . , γm)′ is called a GMRF with respect to a labelled graph G = (V, E) with vertices
or nodes V = {1, . . . , m} and edges E if and only if its density has the form
p(γ) = (2π)−m/2|Q|1/2 exp

−1
2(γ −µ)′Q(γ −µ)

.
(2.6)
In this notation, µ is the mean and Q is a symmetric positive-deﬁnite precision matrix
whose nonzero pattern deﬁnes the dependence structure of the vertices in G. From this
deﬁnition it can be seen that every multivariate normal distributed vector forms a GMRF.
Thus, the non-zero pattern of Q of prior (2.5) directly induce the neighborhood structure of
an undirected graph. Usually, this neighborhood structure is used to impose some kind of
smoothness-penalty on the elements of γ: too rough deviations from neighboring elements
13

2 Structured additive regression models
are penalized in order to obtain a fairly smooth eﬀect of interest. Thus, by (2.5) prior
information is only provided for diﬀerences of parameters, not for their expectations.
For some speciﬁcations discussed below, the precision matrices are not of full rank which
leads to (partially) improper prior distributions. Improper GMRFs are known as intrinsic
GMRFs (IGMRFs, Rue and Held, 2005). The rank deﬁciency is equivalent to the dimension
of the null space of Q, which consists of all vectors x ̸= 0 for which Qx = 0 holds. It
is important to note that, although (2.6) may be improper, the corresponding posterior
is usually proper, thus, the rank deﬁciency yields no restriction with regard to inference.
However, since Q is singular the determinant in (2.6) does not exist. If it is required the
determinant can be replaced by the generalized determinant which can be calculated by
the product of all non-zero eigenvalues of Q (Rue and Held, 2005, p. 93).
In the remainder of this section it is shown that a large variety of eﬀect types can be
represented as a GMRF and written as (2.3). With regard to the adjustments for large-scale
data an additional focus is put on the sparsity structure of design and precision matrices.
Fixed eﬀects
A non-informative prior for a m-dimensional vector of parametric or ﬁxed eﬀects can be
achieved by setting
Q = κIm
(2.7)
and choosing a ﬁxed small value for κ. In this notation, Im stands for the m-dimensional
identity matrix. For the limiting case of κ →0 this choice leads to the improper prior
p(γ) ∝1 which usually gives similar results and is preferred by other authors, for example
Fahrmeir et al. (2004).
While the m × m precision matrix of such a ﬁxed eﬀect is diagonal and can therefore
be considered as sparse, the n × m design matrix Z is usually dense. Most STAR models
include at least one ﬁxed eﬀect, namely the overall intercept.
The corresponding graph that is induced by (2.7) is trivial: all m nodes are independent,
i.e. there are no connections between any nodes in G. Since Q is diagonal, its determinant
can be calculated by the product of its diagonal elements.
14

2.2 Prior speciﬁcation
Unstructured random eﬀects
As there is no unique deﬁnition of a “random eﬀect” in statistics (Gelman and Hill,
2007, Section 11.4), it may be necessary to ﬁrst clarify this term’s meaning within the
context of STAR models, especially as in a fully Bayesian setup the phrase “random
eﬀect” maybe misleading since all unknown parameters are considered as random. However,
here, the term “unstructured random eﬀect” refers to eﬀects that are able to account for
heterogeneity among groups or clusters of observational units or that account for subject
speciﬁc deviations from population eﬀects in longitudinal data. This corresponds to eﬀects
as they are included along parametric or ﬁxed eﬀects in common mixed models (Pinheiro
and Bates, 2000).
Often, these eﬀects are further divided into eﬀects that account for
changes in the intercept (random intercept) and for changes in the coeﬃcients of certain
explanatory variables (random slope). The term “independent and identically distributed
(i.i.d.) random eﬀect” is also used within this context.
An appropriate prior for modeling unstructured random eﬀects can be obtained from
(2.7) by considering κ as an additional hyperparameter rather than being ﬁxed. This prior
speciﬁcation implies that all elements in γ are drawn from the same normal distribution
with equal precision κ. Often, the desired eﬀect of shrinkage or partial pooling along the
elements in γ is a direct consequence of this assumption. This behavior is evident from the
full conditional prior distribution for γj given all other parameters:
γj|γ−j, κ ∼N(0, κ−1).
While the overall mean is zero the precision κ plays the role of a shrinkage parameter: the
higher κ the more the elements in γ are pooled towards their overall mean.
As for the ﬁxed eﬀect deﬁnition above, the m×m precision matrix is sparse with m non-
zero elements, that is m/(m · m) · 100% = 1/m · 100% of its entries are non-zero elements.
However, in contrast to ﬁxed eﬀects, the n×m design matrices associated with unstructured
random eﬀects are sparse as well. For a random intercept Z is an indicator matrix which
allocates each observation to its corresponding group, cluster or subject. Therefore, in most
cases, Z only has n/(m · n) · 100% = 1/m · 100% non-zero elements. For random slopes
this design matrix needs to be multiplied in a column-wise manner by the values of the
explanatory variable z. Thus, the sparsity structure remains identical. An exception is
the usage of this shrinkage prior in the context of ridge regression (Hoerl and Kennard,
15

2 Structured additive regression models
1970). Here, the design matrix is a concatenation of diﬀerent explanatory variables and,
thus, usually dense.
The graph that is induced by this prior is analogous to the graph for the ﬁxed eﬀect setup
above, that is, there is no connection between any vertices in G. Again, the determinant of
Q can be obtained by the product of its diagonal elements.
Temporal eﬀects
Temporal dependence between the elements of γk can be induced by prior (2.5) by random
walk priors of order one (RW1) or two (RW2). In practice this is often used for modeling
smooth temporal eﬀects in a nonparametric fashion, for example when ﬁtting autoregressive
and dynamic models (West and Harrison, 1997).
In this setup, it is assumed that the
elements of γ are deﬁned over a set of equidistant nodes {1, . . . , m}.
The sequential
conditional prior distributions for γj given previous time points is then given by
γj|γj−1, κ ∼N(γj−1, κ−1),
j = 2, . . . , m
(2.8)
for the RW1 and
γj|γj−1, γj−2, κ ∼N(2γj−1 −γj−2, κ−1),
j = 3, . . . , m
(2.9)
for the RW2 prior. By using the Hammersley-Cliﬀord theorem (Hammersley and Cliﬀord,
1971) it can be shown that, under appropriate corrections for the ﬁrst and ﬁrst and second
nodes, respectively, the corresponding joint priors of (2.8) and (2.9) can be written as
p(γ|κ) ∝exp

−κ
2
X
j
(△γj)2


and
p(γ|κ) ∝exp

−κ
2
X
j
(△2γj)2


respectively, with △γj = γj −γj−1 and △2 = γj −2γj−1 + γj−2. To derive the joint priors
in matrix notation the (m −1) × m and (m −2) × m diﬀerence matrices
D1 =








−1
1
−1
1
...
...
−1
1








and
D2 =








1
−2
1
1
−2
1
...
...
...
1
−2
1








16

2.2 Prior speciﬁcation
2
-1
-1
6
-4
-4
1
1
(a)
(b)
Figure 2.1
Visualization of undirected graphs according to the RW1 (a) and RW2 (b) prior
speciﬁcation. Only nodes with a direct connection are considered as neighbors. Neighboring
nodes of the black nodes are colored gray. Numbers refer to the entries of the fourth row
of the corresponding structure matrices.
are deﬁned, respectively. The joint priors can then be written as
p(γ|κ)∝exp

−κ
2γ′Kdγ

(2.10)
with structure or penalty matrices Kd = D′
dDd, d = {1, 2} of dimension m × m. Hence,
a suitable prior for modeling temporal eﬀects by using random walks can be written as
(2.5) with precision Q = κKd. The graphs that correspond to these random walk priors
are deﬁned on regular locations as depicted in Figure 2.1. Panel (a) of this ﬁgure shows
the pattern of the RW1 prior where only the direct neighbors (gray) to the left and to
the right have a direct inﬂuence on node four (black) through prior (2.10). The numbers
on the nodes refer directly to the non-zero entries of the fourth row in the corresponding
structure matrix K1. In Panel (b) the structure of a RW2 is shown. Here, in contrast to
the RW1, the fourth node does also depend on the second neighbors. Similar to Panel (a),
the numbers refer to the non-zero values in the fourth row of K2.
In accordance to unstructured random eﬀects, the n × m design matrix is an indicator
matrix which assigns each observation to its correspondent element in γ. However, rather
than being diagonal, the structure matrix Kd is a band matrix with 2 + m(1 + 2d) −4d
non-zero elements. Compared to m · m elements in total, this can usually be considered as
sparse. From its construction it can also be seen that Kd is rank deﬁcient: For d = 1, the
joint prior p(γ|κ) is invariant to the addition of a constant vector (c, . . . , c)’ of dimension
m, thus rk(K1) = m −1. For d = 2, the prior is also invariant to the addition of any linear
trend, that is, the the null space also includes the vector c·(1, . . . , m)’, thus rk(K2) = m−2.
Hence, (2.10) deﬁnes an IGMRF with rk(Kd) = m −d. If |Q| is required, its generalized
version can be computed by the product of all non-zero eigenvalues.
17

2 Structured additive regression models
The simple random walk priors can be extended in diﬀerent ways, see Fahrmeir and
Kneib (2011, Section 2.1.2) and the references therein. For example, support for unequally
spaced locations can be achieved by weighting the precision of the conditional priors in (2.8)
and (2.9) by the diﬀerence between time point j and j−1, δj. This leads to Kd = D′
d∆dDd
in (2.10) with ∆d = diag(δd+1, . . . , δm). A detailed discussion of continuous random walks
can be found in Rue and Held (2005, Section 3.5). Further extensions include the modeling
of seasonal variation (Rue and Held, 2005, Section 3.4.3) and the incorporation of local
adaptivity in order to account for unsmooth elements or abrupt changes in time-series
(Fahrmeir and Kneib, 2011).
Nonlinear eﬀects for continuous covariates
Since its introduction by Eilers and Marx (1996), penalized splines (P-splines) have been
applied successfully for the approximation of smooth functions of continuous covariates
in a wide range of applications. The Bayesian approach to P-splines discussed here has
been proposed by Lang and Brezger (2004). A more thorough derivation can be found in
Fahrmeir and Kneib (2011, Chapter 2).
By using Bayesian P-splines, the elements in γ are placed on a set of knots {τ1, . . . , τm},
m < n, which covers the range of an explanatory variable z = (z1, . . . , zn)’. The smooth
function f in (2.2) is then modeled by a linear combination of B-spline basis functions of
degree D:
f(zi) =
m
X
j=1
γjBD
j (zi)
(2.11)
where the basis functions can be recursively deﬁned (de Boor, 1978) by
BD
j (zi) =
zi −τj
τj+D −τj
BD−1
j
(zi) +
τj+D+1 −zi
τj+D+1 −τj+1
BD−1
j+1 (zi)
(2.12)
with
B0
j (zi) =





1
τj ≤zi < τj+1,
0
, otherwise,
j = 1, . . . , m −1.
18

2.2 Prior speciﬁcation
Due to this transformation of z, the corresponding n × m design matrix Z is no longer an
indicator matrix. Instead, it consists of the evaluations of (2.12) at each observation zi:
Z =





BD
1 (z1)
· · ·
BD
m(z1)
...
...
BD
1 (zn)
· · ·
BD
m(zn)




.
(2.13)
With this deﬁnition, the function evaluation in (2.11) can be compactly written in matrix
notation as in (2.3).
Smoothness for f is achieved by imposing similar priors on γ as for temporal eﬀects, in
particular the RW2 prior is used for penalizing to rough deviations between the elements
of γ. Hence, a similar graph as depicted in Figure 2.1 (b) is considered. In practice, this
prior is usually combined with a moderate number of equidistant knots m (20–40, Eilers and
Marx, 1996) and cubic (D = 3) B-spline basis functions which yields attractive performance
among diﬀerent applications (Brezger and Lang, 2006).
While the m × m precision matrix has the same sparsity structure as a RW2 prior for
temporal eﬀects, the amount of sparseness for Z depends on the number of knots as well
as on the degree of the basis functions. However, usually upper limits can be given for
special cases of D and m. For example, choosing 30 equidistant knots over the range of z
and D = 3 usually results in a design matrix with a maximum of four non-zero elements
per row. Less non-zero elements result for data points that lie on the outside of the range
of z. Hence, an upper bound for the amount of non-zero elements in Z for this particular
case can be given as 4 · n/(n · m) · 100% = 4/m · 100% ≈13%. Thus, the design matrix for
this common speciﬁcation can be considered as sparse.
Spatial eﬀects
GMRF priors are often used to model discrete spatial information where data is observed
on a regular or irregular lattice, see Rue and Held (2005, Section 1.3) for an extensive review
of applications. Examples for the occurrence of regular lattices can be found in the analysis
of digital images where each observation can be assigned to one of many picture elements
(pixels) that are arranged in an equidistant two-dimensional grid structure. Here, each
element in γ is assigned to one image pixel. The corresponding graph of such a regular
lattice is depicted in Figure 2.2, Panel (a), along with a visualization of the ﬁrst order
neighborhood for a randomly chosen node, that is, the ﬁrst neighboring pixels in x- and
19

2 Structured additive regression models
-1
(a)
(b)
4
-1
-1
-1
-1
-1
-1
6
-1
-1
-1
Figure 2.2
Visualization of two-dimensional undirected graphs on (a) a 5×5 regular lattice
and (b) an irregular lattice corresponding to the districts of the city of Berlin, Germany.
In both graphs, a ﬁrst order neighborhood structure is imposed, that is, only nodes that
share a common border are considered as neighbors. Numbers refer to the non-zero entries
of the black nodes rows of (2.15).
y-direction are considered to be neighbors. Thus, this graph can be seen as an extension of
a RW1 in two dimensions. An example for a graph on an irregular lattice is shown in Panel
(b) of Figure 2.2. Here, a ﬁrst order neighborhood structure is imposed over the districts of
the city of Berlin, Germany, that is, districts that share a common border are considered as
neighbors. Both types of lattices can be modeled by a GMRF that imposes the following
conditional prior for γj given all other elements in γ:
γj|γ−j, κ ∼N

1
nj
X
l∈Nj
γl, 1
njκ

.
(2.14)
Here, the set Nj includes the indices of all neighboring sites of node j, and nj is the number
of neighbors of node j. The joint prior of (2.14) is a zero-mean multivariate normal with
precision matrix Q(κ) = κK, thus of form (2.5). The structure matrix has elements
Kjl =













nj
if j = l,
−1
if l ∈Nj,
0
otherwise.
(2.15)
Figure 2.2 shows the non-zero entries in K for the black nodes.
20

2.2 Prior speciﬁcation
As already mentioned, the neighborhood structure on the graph depicted in panel (a) of
Figure 2.2 can be seen as an extension of the RW1 in two dimensions. It turns out that,
for regular lattices of size nx and ny, the corresponding structure matrix for this graph can
be composed by the structure matrices Kx and Ky of two independent RW1 using the
Kronecker sum:
Kxy = Ky ⊕Kx
= Ky ⊗Inx + Iny ⊗Kx.
(2.16)
Here, ⊕denotes the Kronecker sum and ⊗denotes the Kronecker product. Higher order
neighborhood structures on regular lattices in two dimensions are discussed and compared
in Schmid (2004, Section 4.3). Much interest has been paid to approaches that do not only
account for dependencies along the x- and y directions but also for dependencies along the
diagonals, in particular approximations to the biharmonic diﬀerential operator as discussed
in Rue and Held (2005, Section 3.4) and Fahrmeir and Kneib (2011, Section 5.3).
Of particular interest for this thesis are extensions of two-dimensional regular lattices in
three dimensions. Let Kz be the structure matrix of a RW1 with length nz. Then, the
structure matrix of a three-dimensional random walk on a regular lattice with dimension
nx × ny × nz can be obtained by
Kxyz = Kz ⊕Kxy.
(2.17)
The design matrix Z for modeling discrete spatial information is again an indicator
matrix, hence it can be seen as sparse.
For irregular graphs, no general statement for
the structure of the precision matrix can be given. However, this matrix can usually be
considered as sparse as well if the amount of neighbors that is induced by the neighborhood
order is small compared to the number of sites m. For regular lattices, general statements
can be made for precision matrices due to its deterministic structure. For example, (2.16)
has 5nxny −2(nx + ny) non-zero elements while the three-dimensional version in (2.17)
has 7 · nxnynz −2 · (nxny + nxnz + nynz) non-zero elements, compared to (nxny) · (nxny)
and (nxnynz) · (nxnynz) elements in total, respectively.
Similar to the one-dimensional
random walk these matrices are invariant to the addition of a constant vector, hence,
rk(Kxy) = nxny −1 and rk(Kxyz) = nxnynz −1. The nx · ny −1 non-zero eigenvalues
of (2.16) can be obtained from the diagonal elements of diag(λy) ⊕diag(λx), where
λx = (λx,1, . . . , λx,nx)′ and λy = (λy,1, . . . , λy,ny)′ are the vectors of eigenvalues of Kx
21

2 Structured additive regression models
and Ky, respectively, see Besag and Higdon (1999). A similar statement can be made for
(2.17).
A popular approach for modeling spatial eﬀects has been proposed by Besag et al. (1991).
Here, the spatial eﬀect is split into two components, one unstructured eﬀect as presented
in Section 2.2.1 and one spatially structured eﬀect.
This combination is known as the
Besag-York-Mollié (BYM)-model.
While GMRF priors are suitable for modeling discrete spatial information, they are not
directly applicable in the case of continuous spatial information. Here, Gaussian processes
or equivalently Gaussian random ﬁelds are preferred (Diggle and Ribeiro, 2007). In order to
construct smooth surfaces over continuous spatial domains, also known as kriging (Krige,
1951), the covariance matrix Σ = Q−1 is usually speciﬁed directly by using a certain
covariance function. In contrast to the GMRF approach which models the precision matrix,
this leads to a dense matrix and therefore violates one of the main requirements for the
application to large-scale data, see Section 4.1.1. However, diﬀerent approaches exist to
make continuous spatial data feasible for GMRF priors. For example, spatial domains can
be discretized into disjoint regions and treated as discrete spatial data on regular or irregular
lattices. More recently, Lindgren et al. (2011) showed that an explicit link between certain
Gaussian processes and GMRFs exist. They use stochastic partial diﬀerential equations
in order to derive an explicit GMRF representation of Gaussian processes with Matérn
covariance functions over a triangulated mesh of the spatial domain. This way it is possible
to model continuous spatial data directly through the covariance matrix while still having
access to computational advantages of GMRFs, see Chapter 3 and 4.
Interaction between covariates
Varying coeﬃcient models (VCMs, Hastie and Tibshirani, 1993) represent a popular method
for modeling interactions of covariates. Here, the function f(zi1) in (2.2) is extended to
f(zi1)zi2. In this context, z1 is called the eﬀect-modiﬁer of z2. The corresponding design
matrix is obtained by multiplying each column of the design matrix of f(z1) by z2, thus, the
sparsity structure of the design and precision matrix remains identical. Note that, if z1 is
categorical and modeled as an i.i.d. random eﬀect and z2 is metric, then the extended term
f(zi1)zi2 coincides with the random slope eﬀect presented in the context of unstructured
random eﬀects in Section 2.2.1. If instead, spatial dependence between the nodes in z1 is
induced by an approach as given in Section 2.2.1, a spatially varying coeﬃcient is obtained
as used in Section 6.2.
22

2.2 Prior speciﬁcation
If both covariates are metrical, the basis function approach for nonlinear continuous
eﬀects as presented in Section 2.2.1 can be extended to obtain two-dimensional interaction
surfaces (Chen, 1993): Consider the basis function representation (2.11) for z1 and z2 that
is deﬁned over the set of knots τ 1 = {τ11, . . . , τ1m1} and τ 2 = {τ21, . . . , τ2m2}, respectively.
An approximation of the interaction surface f(z1, z2) can then be obtained by placing the
elements in γ on the grid that is spanned by τ 1 and τ 2 and by computing the design
matrix Z by the (Kronecker) tensor product of the B-spline design matrices Z1 and Z2,
thus f(z1, z2) = Zγ with
f(zi1, zi2) =
m1
X
j=1
m2
X
l=1
γj,lBD
j (zi1)BD
l (zi2).
Since the elements of γ are spatially aligned over a regular lattice, the same smoothing
priors as for discrete spatial eﬀects can be chosen. In order to obtain fairly smooth surfaces,
GMRF priors that account for higher neighborhood orders are of particular interest, such
as approximations to the biharmonic diﬀerential operator mentioned in Section 2.2.1 and
discussed in more detail in Fahrmeir and Kneib (2011, Section 5.3). These authors also
discuss an alternative deﬁnition of interaction surfaces based on radial bases along with
appropriate penalties.
The sparsity structure of the precision matrix can be considered as outlined in Section
2.2.1. It turns out that the design matrix of the tensor product P-splines is sparse as well,
since the Kronecker product of two sparse matrices remains sparse. This is obvious as the
Kronecker product between two sparse matrices Z1 and Z2 multiplies each element in Z1
with the sparse matrix Z2.
Spatio-temporal eﬀects
The types of eﬀects introduced so far can be used to model temporal and spatial information
independently by main eﬀects, i.e. the linear predictor is η = . . . ftemporal + fspatial + . . . ,
where ftemporal may be modeled by a RW1 or RW2 prior as presented in Section 2.2.1 and
fspatial may be modeled as in Section 2.2.1. In order to obtain non-separable space-time
models, Knorr-Held (2000) discuss four types of interactions of temporal and spatial main
eﬀects with an increasing amount of complexity. The most simplistic model includes two
BYM-like eﬀects for temporal and spatial information, i.e. the linear predictor includes
one unstructured and one structured eﬀect for time as well as one unstructured and
one structured eﬀect for space.
Diﬀerent interaction types are obtained by all possible
23

2 Structured additive regression models
4
4
4
4
-16
4
-1
-1
-1
-1
24
-6
-6
-6
-6
4
4
4
4
-16
4
-1
-1
-1
-1
Figure 2.3
Visualization of an undirected graph with spatio-temporal dependency.
Neighborhood structure corresponds to a Type IV interaction according to Knorr-Held
(2000) between a RW2 in temporal and a RW1 in spatial dimension. Values refer to the
row of the black node in the corresponding structure matrix.
combinations of structured and unstructured penalty matrices for the temporal and spatial
eﬀects.
Structure matrices are combined using the Kronecker product as proposed by
Clayton (1996):
K = Ktemporal ⊗Kspatial.
(2.18)
The ﬁrst interaction (Type I interaction) combines the unstructured eﬀects and yields a
priori independent regression coeﬃcients that are completely unstructured in space and
time.
In contrast, the most advanced interaction (Type IV interaction) combines the
structured main eﬀects and therefore produces regression coeﬃcients that are a priori
dependent in space and time. As an example for this type of interaction, consider the
undirected graph that is shown in Figure 2.3. Here, the neighborhood structure is the
result of the combination of a RW2 in temporal and a RW1 in spatial dimension.
Kronecker product penalties are appealing as they lead to precision matrices that
depend on a single, global precision parameter only.
This simpliﬁes inference on these
hyperparameters dramatically, see Section 3.1.2.
However, that means that the same
parameter is responsible for smoothing in time and space.
For data where the spatial
dimension dominates the temporal, usage of a global precision parameter may lead to an
underestimation of temporal smoothness. Diﬀerent approaches have been discussed in order
to overcome this problem. For example, Gössl et al. (2001) introduced pixel-wise precisions
24

2.2 Prior speciﬁcation
10
-1
-1
-1
-1
1
-4
-4
1
Figure 2.4
Visualization of an undirected graph with spatio-temporal dependency.
Neighborhood structure is imposed by a Kronecker sum of a RW2 in temporal and a RW1 in
spatial dimension. Values refer to the row of the black node in the corresponding structure
matrix.
in the context of modeling fMRI time series. In addition, they expanded the Kronecker
product penalty by a temporal main penalty-eﬀect:
Q = Qspatial ⊗Ktemporal + Λ ⊗Ktemporal
= (Qspatial + Λ) ⊗Ktemporal.
Here, Ktemporal is a RW2 structure matrix, Λ is the diagonal matrix of pixel-wise precisions
and Qspatial is given by
Qspatial,jl =













P
l∈Nj(κj + κl)
if j = l,
−(κj + κl)
if l ∈Nj,
0
otherwise.
A further alternative can be derived by considering the Kronecker sum between the temporal
and spatial structure matrices instead of the Kronecker product:
K = Ktemporal ⊗Ispatial + Itemporal ⊗Kspatial.
The neighborhood structure that is induced by applying this formula to a RW2 in temporal
and a RW1 in spatial dimension is depicted in Figure 2.4. It can be seen that, in contrast to
Figure 2.3, only the direct temporal neighbors are used from neighboring time points. The
Kronecker sum approach can easily be extended to establish diﬀerent smoothing along space
25

2 Structured additive regression models
and time. For example, by using diﬀerent precision parameters for these two dimensions
one obtains the following precision matrix:
Q = κ1Ktemporal ⊗Ispatial + κ2Itemporal ⊗Kspatial.
Rue and Held (2005, page 107) present a similar approach for modeling spatial data on a
regular lattice with diﬀerent smoothing in x- and y-directions.
In accordance to design matrices of temporal and spatial main eﬀects, the design
matrices for spatio-temporal eﬀects are indicator matrices and therefore sparse, although
the dimension is increased to n × (mspatial · mtemporal). Precision matrices can be considered
as sparse as well, however, diﬀerent rank deﬁciencies arise for the diﬀerent approaches. The
rank of Kronecker product penalties can easily be calculated by basic rules for Kronecker
products (Harville, 1997), i.e. rk(K) = rk(Ktemporal) · rk(Kspatial). For the Kronecker sum
with speciﬁcation as in Figure 2.4 one has rk(K) = mtemporal · mspatial −2.
2.2.2 Hyperparameters
If the linear predictor contains more than ﬁxed eﬀects, precision parameters κk, k = 1, . . . , p,
need to be estimated as well. Hence, in order to complete the model formulation, prior
distributions for these hyperparameters as well as for an additional dispersion parameter
need to be set up.
Precision parameters of GMRF priors
For precision parameters κk, k = 1, . . . , p, it is common practice to choose independent
Gamma distributions with shape and rate parameters ak and bk, respectively:
p(κk) =
bak
k
Γ(ak)κak−1
k
exp(−bkκk)
Note that this choice is equivalent to an Inverse Gamma prior on the variance parameter
νk = κ−1
k
with shape and rate ak and bk, respectively,
p(νk) =
bak
k
Γ(ak)ν−(ak+1)
k
exp(−bk/νk),
26

2.3 Chapter summary
or a log-Gamma prior on τk = log κk
p(τk) =
bak
k
Γ(ak) exp(akτk −bk exp(τk)),
which my be preferred by some authors.
The Gamma prior is primary chosen for the following reasons:
First, for the case
Qk = κkKk it turns out that the Gamma distribution is a conjugate family for the GMRF
prior on γk, that is, the full conditional of κk is again a Gamma distribution, see Chapter
3 for details. Second, by choosing diﬀerent values for ak and bk, this distribution is able to
cover a wide range of diﬀerent prior beliefs over a strictly positive domain. For example,
a weakly informative prior can be obtained by choosing small values for ak and bk, e.g.
ak = bk = 0.001, or, alternatively, ak = 1 and bk small (Brezger and Lang, 2006), whereas
smoothing can be increased by choosing larger values for ak and bk. Gelman (2006) noted
that for the choice of ak = bk = ϵ with ϵ →0 inference can be quite sensitive with respect
to ϵ if κk is small. Instead, he suggests to use a non-informative uniform prior on κ−1
k .
However, the potential negative eﬀect of the Gamma prior on the ﬁnal result can usually
be minimized by performing sensitivity analyses with respect to ak and bk.
Dispersion parameter
Besides the fact that the prior distribution should support strictly positive real numbers
only, no general recommendation for an appropriate prior of an additional dispersion
parameter φ of the likelihood can be given.
However, the Gamma distribution with
parameters aφ and bφ is a popular choice, especially for Gaussian response models since
the Gamma distribution is the conjugate prior for the precision parameter of a normal
likelihood. Further examples that use the Gamma distribution as a prior for φ can be
found for Gamma distributed response variables (Brezger and Lang, 2006, Section 4.2) and
for Beta models, see Section 3.1.3.
2.3 Chapter summary
In this chapter, all components that are necessary to formulate STAR models have been
introduced: the observation model and prior distributions for all unknown parameters. The
observation model has been formulated relative vague in order to be as generic as possible.
27

2 Structured additive regression models
Furthermore, it has been shown that GMRFs provide a ﬂexible framework with which one
is able to build appropriate priors for ﬁxed and unstructured random eﬀects, temporal
eﬀects and nonlinear eﬀects for continuous covariates, spatial eﬀects and interactions as
well as spatio-temporal eﬀects. Finally, prior speciﬁcation for hyperparameters has been
outlined.
28

3 Inference
Having introduced the basic model formulation for STAR models in Chapter 2, this chapter
provides an overview of popular methods that are used to ﬁt these models. Within the
Bayesian approach considered here, inference relies solely on the joint posterior. Given the
speciﬁcations in Chapter 2 this joint posterior can be written as
p(γ1, . . . , γp, κ1, . . . , κp, φ|y) ∝
n
Y
i=1
p(yi|γ1, . . . , γp, κ1, . . . , κp, φ)
×
p
Y
k=1
|Qk(κk)|1/2 exp

−1
2γ′
kQk(κk)γk

×
p
Y
k=1
κak−1
k
exp(−bkκk)
× φaφ−1 exp(−bφφ).
(3.1)
Here, Gamma priors have been imposed on all precision and dispersion parameters. For the
exploration of this posterior three diﬀerent approaches that can be found in the statistical
literature are discussed in this chapter. First, fully Bayes inference based on MCMC is
presented in Section 3.1 which is followed by an empirical Bayes approach in Section 3.2.
Finally, Section 3.3 presents an approximate Bayes approach.
By all means, this chapter does not claim to give a complete overview over the methods
that can be used to analyze joint posterior (3.1). In particular, two approaches that are
quite popular in the machine learning literature, namely variational Bayes (Bishop, 2006)
and expectation-propagation (Minka, 2001), are not presented. For a discussion on the
performance of these methods with respect to latent Gaussian models see the overview in
the introduction of Rue et al. (2009).
The MCMC approach given in the following section is also discussed in shortened form
in Schmidt et al. (2017, Section 2.3).
29

3 Inference
3.1 MCMC based inference for STAR models
Markov chain Monte Carlo (MCMC) simulation, pioneered by the work of Metropolis et al.
(1953) and Hastings (1970), has been used in statistical physics long before it was recognized
in mainstream statistics (Robert and Casella, 2011).
It was ﬁrst used in statistics for
the analysis of digital images from which the Gibbs sampler (Geman and Geman, 1984)
emerged. This procedure helped solving many other complex problems in applied statistics
for which no or only unsatisfactory solutions existed. The analysis of spatial data is an
example of such situations. Besag et al. (1991) pointed out that these situations can be
formulated as image restoration problems and, therefore, be solved by a Gibbs Sampler.
This, and the broad availability of fast computational equipment, led to a revolution in
Bayesian statistics.
Since then, a wide range of diﬀerent MCMC samplers have been
emerged, each with its own advantages and disadvantages.
In Bayesian statistics, MCMC simulation is used to simulate samples from the joint
posterior distribution which are then used to summarize this distribution in any possible
way. In its easiest form, i.e. the Gibbs sampler, a realization of each unknown parameter is
obtained one by one by sampling directly from its full conditional posterior distribution. If
direct sampling from the full conditional distribution is not possible, a Metropolis-Hastings
step can be performed; also referred to as Metropolis-within-Gibbs. The resulting sampling
scheme is quite general and applicable to a wide range of problems. The MCMC algorithm
for STAR models presented here is the basic version of the one presented in Fahrmeir et al.
(2004). Here, a Metropolis-Hastings step is performed for sampling regression coeﬃcients
γk, k = 1, . . . , p, while precision parameters are updated directly from their corresponding
full conditionals. The details on these steps are given next.
3.1.1 Regression coeﬃcients
For Gaussian response models the full conditionals of the regression coeﬃcients can be
derived in closed form and a Gibbs sampler can easily be set up. The framework of auxiliary
variable models (Rue and Held, 2005, Chapter 4.3) allows to apply this Gibbs sampler to
some non-Gaussian likelihoods, in particular for the Student-t and Laplace distribution
(Andrews and Mallows, 1974) as well as the Binomial distribution with probit (Albert and
Chib, 1993) and logit link (Holmes and Held, 2006).
In the latter work, extensions to
multinomial regression models are also presented. For many other non-Gaussian models,
however, the full conditionals of the regression coeﬃcients are no longer available in closed
30

3.1 MCMC based inference for STAR models
form. Therefore, a Metropolis-Hastings step needs to be included inside the Gibbs sampler
in order to sample from the correct full conditionals. In this section, it is shown how an
appropriate proposal for the Metropolis-Hastings step can be constructed, ﬁrst for general
likelihoods, then the case of exponential families is examined more closely.
GMRF proposal
For general likelihoods a proposal density for γk can be obtained by an approximation of
the likelihood similar to the GMRF approximation given in Rue (2001). The idea behind
this is to match the mode of the likelihood and its corresponding curvature at the mode in
order to obtain a simpler and more generic functional form that can easily be combined with
the GMRF prior. The basis for the GMRF approximation is a quadratic Taylor expansion
of the log-likelihood l(γk) = Pn
i=1 log p(yi|γk) around the current state γc
k which can be
written as
l(γk) ≈ac
k + (bc
k)′γk −1
2γ′
kCc
kγk
(3.2)
with coeﬃcients
ac
k = l(γc
k) −(γc
k)′∂l(γc
k)
∂γk
+ 1
2(γc
k)′ ∂2l(γc
k)
∂γ′
k∂γk
γc
k
bc
k = ∂l(γc
k)
∂γk
+ Cc
kγc
k
(3.3)
Cc
k = −∂2l(γc
k)
∂γ′
k∂γk
.
(3.4)
Since ac
k in (3.2) does not depend on γk, it can be neglected. For GLMs the derivatives in
the above formulas can be computed explicitly, see below. In other cases, these derivatives
may be approximated using numerical diﬀerentiation techniques (Rue and Held, 2005, page
171).
By using (3.2) as an approximation for the log-likelihood the full conditional p(γk|y, κk)
can be written as
p(γk|y, κk) ∝p(y|γk)p(γk|κk)
∝exp
 
−1
2γ′
kQkγk +
n
X
i=1
log p(yi|γk)
!
31

3 Inference
≈exp

−1
2γ′
kQkγk + ac
k + (bc
k)′γk −1
2γ′
kCc
kγk

∝exp

−1
2γ′
k(Qk + Cc
k)γk + (bc
k)′γk

.
(3.5)
This corresponds to the core of a multivariate normal distribution, thus, the proposal
distribution for γk based on the GMRF approximation has the form
γp
k|· ∼N(˜µc
k, f
Q
c
k).
(3.6)
Instead of the more familiar notation using the covariance matrix Σ = Q−1, N(·, Q) here
refers to a multivariate normal distribution with precision Q. The precision matrix of (3.6)
is given by
f
Q
c
k = Qk + Cc
k
(3.7)
and the mean ˜µc
k is the solution of the linear system
f
Q
c
k ˜µc
k = bc
k.
(3.8)
Sampling a proposal γp
k from this distribution requires the evaluation of ˜µc
k and f
Q
c
k at the
current state γc
k. The proposal is accepted with probability
α(γc
k, γp
k) = min


1, p(y|γp
k)p(γp
k|κk)ϕ(γc
k|˜µp
k, f
Q
p
k)
p(y|γc
k)p(γc
k|κk)ϕ(γp
k|˜µc
k, f
Q
c
k)



(3.9)
where ϕ(·; µ, Q) represents the density of a multivariate normal distributed random variable
with mean µ and precision matrix Q. Note that both, ˜µc
k and f
Q
c
k depend on the current
state of the chain. Therefore, in order to obtain the acceptance probability the normalizing
constant of ϕ needs to be calculated which requires the computation of the log-determinant
of f
Q
c
k. In addition, (3.7) and (3.8) need to be re-evaluated given the proposal γp
k.
In case of low acceptance rates the GMRF approximation can be further improved by
repeating the Tylor series expansion of the likelihood around the mean of the proposal
distribution, ˜µc
k. This can be iterated until convergence or just until the desired acceptance
rate has been reached (Rue and Held, 2005, p. 172).
32

3.1 MCMC based inference for STAR models
IWLS proposal
More insight into the GMRF approximation can be obtained by considering the special
case of GLMs, that is, for likelihoods that are exponential families.
In this case, the
GMRF approximation coincides with the iterated weighted least squares (IWLS) proposal
(Gamerman, 1997). This can be seen by noting that the ﬁrst derivative of the likelihood
with respect to γk, i.e. the score vector, can be written as
s(γk) = Z′
kDV −1(y −µ)
and the negative second derivation, i.e. the Fisher information, as
F (γk) = Z′
kW Zk.
Here, D is a diagonal matrix with entries ∂h(ηi)/∂η, i = 1, . . . , n, V is a diagonal matrix
with entries φv(µi)/ωi, i = 1, . . . , n, where v(µi) and ωi are the variance function and
weights corresponding to the speciﬁc exponential family, respectively, and W = DV −1D.
From (3.3) and (3.4) it follows directly that Cc
k = F (γk) and
bc
k = s(γc
k) + F (γc
k)γc
k
= Z′
kDc(V c)−1(y −µc) + Z′
kW cZkγc
k
= Z′
kDc(V c)−1Dc((Dc)−1(y −µc) + Zkγc
k)
= Z′
kW c(˜yc
k −ηc
−k)
with working observations ˜y = η + D−1(y −µ) and η−k describing the linear predictor
without the kth term.
Thus, the parameters of the proposal obtained by the GMRF
approximation can be expressed as
f
Q
c
k = Z′
kW cZk + Qk
(3.10)
and
f
Q
c
k ˜µc
k = Z′
kW c(˜yc −ηc
−k).
(3.11)
This formulation coincides directly with the IWLS proposal suggested by Gamerman (1997)
for random eﬀects in the context of generalized linear mixed models. Note that D, V and
therefore W and ˜y depend on the current state of the chain.
33

3 Inference
3.1.2 Precision parameters
If the precision matrix of γk depends on one precision parameter only through Q(κk) =
κkKk and if a Gamma distribution has been chosen as a prior for κk, then its full conditional
is given by
p(κk|·) ∝p(γk|κk)p(κk)
∝κrk(Kk)/2
k
exp

−κk
2 γ′
kKkγk

κak−1
k
exp(−bkκk)
= κak+rk(Kk)/2−1
k
exp (−(bk + γ′
kKkγk/2)κk) .
Thus, in this case, the full conditionals of these hyperparameters are again Gamma
distributions with updated parameters ˜ak = ak + rk(Kk)/2 and ˜bk = bk + γ′
kKkγk/2.
3.1.3 Dispersion parameters
Similar to its prior speciﬁcation in Chapter 2 no general advice for the full conditional
posterior distribution of an additional dispersion parameter φ can be given. However, in
the following two special cases are examined in more detail.
Gaussian distributed response
For Gaussian distributed response variables the inverse variance, i.e.
the precision is
considered as the dispersion parameter φ.
Since the Gamma distribution is conjugate
to the Gaussian likelihood the full conditional of φ can be derived in closed form in this
case:
p(φ|·) ∝p(y|θ)p(φ)
∝φ(n−1)/2 exp (−φ(y −η)′(y −η)/2) φaφ−1 exp(−bφφ)
∝φaφ+(n−1)/2−1 exp (−(bφ + (y −η)′(y −η)/2)φ) .
Hence, the full conditional is again a Gamma distribution with updated parameters
˜aφ = aφ + (n −1)/2 and ˜bφ = bφ + (y −η)′(y −η)/2.
34

3.1 MCMC based inference for STAR models
Beta distributed response
An appropriate distribution for modeling proportions, i.e. response variables yi for which
0 < yi < 1 holds, is the Beta distribution with parameters α > 0 und β > 0. For the use
of regression a common reparametrization can be obtained by deﬁning µ = α/(α + β) and
φ = α+β, that is, α = µφ and β = (1−µ)φ (Ferrari and Cribari-Neto, 2004). This yields
E(yi) = µi
and
Var(yi) = µi(1 −µi)
1 + φ
and the log likelihood can be written as
li(φ) ∝log Γ(φ) −log Γ(µiφ) −log Γ((1 −µi)φ) + (µiφ −1) log yi
+ ((1 −µi)φ −1) log(1 −yi).
If, again, a Gamma distribution with ﬁxed parameters aφ and bφ is chosen as a prior for φ
the full conditional is given by
p(φ|·) ∝p(φ)p(y|φ)
∝φaφ−1 exp(−bφφ)
n
Y
i=1
p(yi|φ)
= φaφ−1 exp

−bφφ +
X
li(yi|φ)

Since this is not a known distribution an additional Metropolis-Hastings-step needs to be
included in the MCMC algorithm. A proposal can be obtained by approximating the log
likelihood by a second order Taylor expansion around the current state of the chain φc:
li(yi|φ) ≈θc
1 + θc
2φ −1
2θc
3φ2
with coeﬃcients θc
1, θc
2 and θc
3 similar deﬁned as ak, bk and Ck in Section 3.1.1. The ﬁrst
derivative with respect to φ is given by
∂li(φ)
∂φ
∝ψ(φ) −ψ(µiφ)µi −ψ((1 −µi)φ)(1 −µi) + µi log yi + (1 −µi) log(1 −yi)
and the second by
35

3 Inference
∂2li(φ)
∂2φ2
∝ψ′(φ) −ψ′(µiφ)µ2
i −ψ′((1 −µi)φ)(1 −µi)2.
Here, ψ and ψ′ denote the digamma and trigamma function, respectively.
With these
components the full conditional can be approximated as
p(φ|·) ≈φaφ−1 exp

−bφφ + θc
2φ −1
2θc
3φ2

= φaφ−1 exp

−1
2θc
3φ2 + (θc
2 −bφ)φ

.
For the special but quite common case of aφ = 1 this distribution reduces to a normal
distribution with mean ˜µc
φ = (θc
2−bφ)/θc
3 and variance (σ2
φ)c = 1/θc
3, thus, a distribution that
can easily be used as a proposal distribution within a Metropolis-Hastings-step. However,
care must be taken with respect to the domain of φ: If φ is small the proposal density may
cover parts of the negative real line, although φ must, by deﬁnition, be strictly positive.
In order to obtain the acceptance probability the proposal φp is used to calculate θp
2 and
θp
3 and thus ˜µp
φ and (σ2
φ)p. The acceptance probability is then given by:
α(φc, φp) = min
(
1, p(y|φp)p(φp)ϕ(φc|˜µp
φ, (σ2
φ)p)
p(y|φc)p(φc)ϕ(φp|˜µc
φ, (σ2
φ)c)
)
.
3.1.4 Additional considerations
Initialization
Regression coeﬃcients can be initialized by iteratively computing the mode of the proposal
distribution (3.6) for ﬁxed values of the hyperparameters. This is similar to the initialization
procedure of Brezger and Lang (2006), who set κk = 0.1 and compute the mode via
back-ﬁtting within Fisher scoring. As explained below, for convergence assessment it may
be necessary to generate multiple chains with overdispersed starting values. This can be
achieved, for example, by sampling log κk uniformly in a pre-chosen interval.
36

3.1 MCMC based inference for STAR models
Convergence assessment
Before interpreting MCMC samples as realizations from their joint posterior distribution it
is necessary to assess weather the constructed Markov chain has converged to its stationary
distribution. Diﬀerent strategies for convergence assessment have been proposed over the
years. In practice, visual inspection of trace plots is usually combined with convergence
diagnostics.
The latter consist of summary statistics or graphical methods that try to
evaluate the quality of Markov chains with respect to convergence and mixing. See Cowles
and Carlin (1996) for an extensive review and comparison of diﬀerent approaches.
One convergence diagnostic which is extensively used in practice is Gelman and Rubin’s
potential scale reduction factor (Gelman and Rubin, 1992). This method tries to asses
both, mixing and convergence by comparing the within- and between-sequence variances
of multiple MCMC chains in an ANOVA-like fashion. In a revised version (Gelman et al.,
2014, Section 11.4), the potential scale reduction factor for m chains with length T is given
by
bR =
s
T−1
T W + 1
T B
W
with between- and within-sequence variances
B =
T
m −1
m
X
j=1
(¯x(j) −¯x)2
W = 1
m
m
X
j=1
s2
(j).
Here, x is the sampled parameter, ¯x(j) and s2
(j) the mean and variance of the jth chain,
respectively, and ¯x = P
j ¯x(j) the overall mean. Values of bR far above one indicate that the
scale of the sampled distribution of x can be reduced by further sampling. Values close
to one, on the other hand, may be interpreted as suﬃcient convergence to the stationary
distribution.
In practice,
bR is calculated for each parameter independently.
The main
advantage of the potential scale reduction factor is that it only requires the ﬁrst two
moments of each chain. This will be helpfull when considering large-scale data, see Section
4.2.5.
37

3 Inference
Sampling under linear constraints
For a given sample γ∗
k of (3.6) it may be required to take linear constraints of the form
Aγ∗
k = b into account. Here, A and b are of dimension r×mk and r×1, respectively, where
r corresponds to the number of constraints. An example which is often used in practice
are sum-to-zero constraints that may be helpful to ensure identiﬁability over all regression
coeﬃcients. In this case, A would be a 1 × mk row vector of all ones and b = 0. Such
constraints can easily be considered within MCMC frameworks by conditioning by Kriging
(Rue, 2001) in which
f
Q
−1
k A′(Af
Q
−1
k A′)−1(Aγ∗
k −b)
(3.12)
is subtracted from the unconditioned sample γ∗
k. In addition to solving (3.11) and sampling
from (3.6) this requires to solve the linear systems f
QkV = A′.
Block updating of parameters
Due to computational limitations single site updating schemes were the most common
sampling strategies for MCMC algorithms in early days (Gilks et al., 1996).
Here, all
parameters, even the single components in γk, are updated one by one according to their full
conditionals giving all other parameters. While this strategy comes with low computational
requirements it was recognized early that the resulting Markov chains can suﬀer from slow
mixing and poor convergence, especially when components are highly correlated (Gilks and
Roberts, 1996). Block-updating schemes provide an alternative in such situations. Here,
multiple parameters are collected in blocks and updated jointly. The method presented in
Section 3.1.1 is an example of block updating: all coeﬃcients in γk are accepted or rejected
within one Metropolis-Hastings step. Knorr-Held and Rue (2002) propose a generic method
for the joint update of larger blocks, i.e. blocks that consist of regression and precision
parameters, for example (κk, γk). Here, the joint proposal is obtained by ﬁrst sampling
κk from a distribution that is independent of γk. The authors suggest to use z · κk as a
proposal for κk where z is a random variable with density proportional to 1 + 1/z, with
[1/f, f] and f > 1. The value f needs to be tuned so that the desired acceptance rate is
achieved. Since this distribution does not depend on the current state of the chain it will
cancel itself out when computing the acceptance probability. A proposal for the regression
coeﬃcient γk is obtained by subsequently sampling from (3.6). The joint proposal is then
accepted with probability
38

3.1 MCMC based inference for STAR models
α(γc
k, γp
k) = min


1, p(y|γp
k)p(γp
k|κp
k)p(κp
k)ϕ(γc
k|˜µp
k, f
Q
p
k(κc))
p(y|γc
k)p(γc
k|κc
k)p(κc
k)ϕ(γp
k|˜µc
k, f
Q
c
k(κp))


.
(3.13)
Knorr-Held and Rue analyze diﬀerent compositions of blocks in the context of disease
mapping. Their blocks range from single-site blocks to blocks that contain all unknown
parameters.
It is concluded that a joint update of GMRF parameters together with
corresponding hyperparameters may be necessary to ensure proper mixing and convergence
of Markov chains.
In Chapter 4 it is shown that blocking strategies have a major inﬂuence on the
computational complexity which is induced when considering MCMC inference for large-
scale problems. Therefore, a thorough discussion of these strategies is postponed to this
chapter.
3.1.5 Fully Bayes inference based on MCMC
The approach for constructing Markov chains for all unknown parameters within a fully
Bayes setup can be summarized as follows.
Given the number of MCMC iterations as
well as initialized regression and precision parameters, perform the following steps in each
iteration:
(1) For k = 1, . . . , p do:
a) Use the current state of the chain γc
k to compute ˜Q
c
k and ˜µc
k according to (3.7)
and (3.8), respectively, and sample a proposal γp
k from (3.6). Use this proposal
to recompute ˜Q
p
k and ˜µp
k. Accept the proposal as the new state of the chain with
probability α(γc
k, γp
k) as in (3.9).
b) Generate a new state for the precision parameter κk by sampling from a Gamma
distribution with parameters ˜ak = ak + rk(Kk)/2 and ˜bk = bk + γ′
kKkγk/2.
(2) For an additional dispersion parameter φ use a customized sampling strategy. See
Section 3.1.3 for possible sampling strategies for Gaussian or Beta distributed response
variables.
If precision and regression parameters are updated jointly steps (a) and (b) have to be
merged and (3.13) used as the acceptance probability.
39

3 Inference
Inference on all parameters relies on the samples that remain after discarding the
realizations of an initial burn-in phase. From these samples, any quantity of interest with
respect to the joint posterior distribution can be approximated.
3.2 Empirical Bayes inference for STAR models
This section summarizes the empirical Bayes approach to STAR models which has been
presented by Fahrmeir et al. (2004).
Based on the work of Green (1987) and Lin and
Zhang (1999), Fahrmeir et al. showed how STAR models can be represented as generalized
linear mixed models (GLMMs) and, therefore, how well-known estimation techniques from
this class of models can be utilized. In this approach it is considered that the precision
parameters are unknown constants rather than random quantitates, thus, the term empirical
Bayes.
The empirical Bayes approach is appealing for the following reasons: First, the use of
mixed model methodology allows the application of deterministic algorithms which are
usually faster than MCMC simulations. Also, this eliminates concerns about the quality
of estimands due to slow mixing or poor convergence of Markov chains.
Second, since
precision parameters are considered as ﬁxed rather than random, no prior distributions for
these hyperparameters need to be speciﬁed. This eliminates the necessity for sensitivity
analyses with respect to parameters of such hyperpriors.
3.2.1 Mixed model representation
Starting point for the mixed model approach is the representation of general STAR models
as GLMMs.
This can be achieved by a reparametrization of the components in η as
follows:
Zkγk = Zk
 ˜U kβk + ˜Xkαk

= U kβk + Xkαk
The decomposition of γk into ˜U kβk and ˜Xkαk is closely connected to the null space of the
corresponding structure matrix Kk and can be obtained in diﬀerent ways (Fahrmeir and
Kneib, 2011, Chapter 4.2). One method which is applicable to general structure matrices
40

3.2 Empirical Bayes inference for STAR models
is based on the spectral decomposition Kk = ΓΩΓ′, where Ωis the diagonal matrix of
the eigenvalues of Kk and Γ is the concatenation of the corresponding eigenvectors. The
number of zero eigenvalues in Ωequals the rank deﬁciency of Kk. If the decomposition
is split along eigenvalues that are zero (Γ1 and Ω1) and those that are non-zero (Γ2 and
Ω2), the design matrices for the reparametrization can then be obtained by ˜U k = Γ1 and
˜Xk = Γ2Ω−1/2
2
. Thus, ˜U k corresponds to the part of γk that is unpenalized by the structure
matrix.
Using this reparametrization for all regression coeﬃcients, the complete linear predictor
can be represented as
η = Uβ + Xα
with the overall design matrices U = (U 1, . . . , U p) and X = (X1, . . . , Xp) as well as
the coeﬃcients β = (β′
1, . . . , β′
p)′ and α = (α′
1, . . . , α′
p)′.
If an intercept is present,
identity vectors in U k must be deleted in order to guarantee identiﬁability for all regression
coeﬃcients.
The use of this reparametrization requires a new setup of prior distributions for the
regression coeﬃcients α and β. Following Fahrmeir et al. (2004) a diﬀuse prior is assumed
for the ﬁxed eﬀect part, i.e. p(β) ∝1. The second part, α, is modeled as an unstructured
random eﬀect:
α|κ ∼N(0, Ψ −1)
with precision matrix Ψ = blockdiag(κ1Im1, . . . , κpImp).
Inference for all unknown parameters is then performed iteratively: First, for given
precision parameters one iteration of a Fisher scoring algorithm is performed in order
to update all regression coeﬃcients.
Second, given updated regression coeﬃcients, one
iteration of a Fisher scoring algorithm is performed in order to update the precision
parameters. The details of these steps are explained in the next sections.
41

3 Inference
3.2.2 Estimation of regression coeﬃcients
Regression coeﬃcients are estimated by maximizing the joint posterior of α and β. This
can be achieved by a second order Taylor expansion of the log-posterior around α0 and
β0:
log p(β, α|y) ∝l(β, α) + log p(β) + log(α|Ψ)
≈a0 + b′
0(β′, α′)′ −1
2(β′, α′)C0(β′, α′)′
(3.14)
This corresponds to the core of the logarithm of a multivariate Gaussian density with
precision C0 and mode (˜µ′
β, ˜µ′
α)′ which is the solution of C0(˜µ′
β, ˜µ′
α)′ = b0. Coeﬃcients b0
and C0 can be derived in complete analogy to Section 3.1.1, that is,
b0 = ∂log p(β0, α0|y)
∂(β′, α′)′
+ C0(β′
0, α′
0)′
C0 = −∂2 log p(β0, α0|y)
∂(β′, α′)∂(β′, α′)′
Using the notation of GLMs the components of these coeﬃcients can be expressed by the
score function
s(β0, α0) =


U ′D0V −1
0 (y −µ0)
X′D0V −1
0 (y −µ0) −Ψα0


and the Fisher information
F (β0, α0) =

U ′W 0U
U ′W 0X
X′W 0U
X′W 0X + Ψ

.
(3.15)
From its deﬁnition it follows immediately that C0 = F (β0, α0). By using the deﬁnition of
working observations as in Section 3.1.1, b0 can be rewritten as
b0 = s(β0, α0) + F (β0, α0)(β′
0, α′
0)′
=


U ′D0V −1
0 (y −µ0)
X′DV −1(y −µ0) −Ψα0

+


U ′W 0Uβ0 + U ′W 0Xα0
X′W 0Uβ0 + X′W 0Xα0 + Ψα0


=

U ′D0V −1
0 D0(D−1
0 (y −µ0) + Uβ0 + Xα0)
X′D0V −1
0 D0(D−1
0 (y −µ0) + Uβ0 + Xα0)


42

3.2 Empirical Bayes inference for STAR models
=

U ′W 0˜y0
X′W 0˜y0

.
Therefore, the mode of (3.14) can be written as the solution of

U ′W 0U
U ′W 0X
X′W 0U
X′W 0X + Ψ



˜µβ
˜µα

=

U ′W 0˜y0
X′W 0˜y0

.
(3.16)
Solving this system of linear equations with respect to ˜µβ and ˜µα corresponds to one
iteration of a Fisher scoring algorithm.
3.2.3 Estimation of precision and dispersion parameters
By adding −0.5˜yW ˜y to (3.14) and performing straight forward calculations it follows
that
˜y|β, α
a∼N(Uβ + Xα, W −1).
(3.17)
This coincides with a linear mixed model for the working observations. Note that for normal
distributed response variables ˜y = y holds. For this special case Harville (1974) shows how
the marginal distribution for the error contrasts u = A′y can be obtained. Here, A is a
n × (n −dim(β)) matrix given by AA′ = U(U ′U)−1U ′ with A′A = I. The advantage of
using the likelihood of error contrasts rather than y is that the resulting marginal likelihood
for precision and dispersion parameters does not depend on β anymore. This makes it
possible to obtain estimates for κ1, . . . , κp and φ by accounting for the uncertainty of β.
Estimating these parameters this way is also known as restricted maximum likelihood
(REML, Patterson and Thompson, 1971).
Applying this method to (3.17) yields the
following approximate marginal likelihood for precision and dispersion parameters (Lin
and Zhang, 1999):
lM(κ, φ) = −1
2

log |Σ| + log |U ′Σ−1U| + (˜y −Uβ)′Σ−1(˜y −Uβ)

with Σ = W −1+XΨ −1X′. Maximizing this restricted likelihood with respect to κ1, . . . , κp
and φ yields REML estimates for these precision and dispersion parameters. Fahrmeir and
Kneib (2011, Chapter 3.1.4) point out that the REML estimates for the precision and
43

3 Inference
dispersion parameters coincide with the modes of the corresponding marginal posteriors
within a fully Bayesian setup.
Numerical optimization is usually performed by Fisher scoring on the variance
parameters τ 2
k = κ−1
k , k = 1, . . . , p, rather than precision parameters (Fahrmeir et al.,
2004). Here, in iteration t + 1 a new value for (τ 2, φ) can be found via

τ 2
φ


(t+1)
=

τ 2
φ


(t)
+ F (τ 2(t), φ(t))−1s(τ 2(t), φ(t))
(3.18)
with the score vector s(τ 2, φ) = ((∂lM(τ 2, φ)/∂τ 2
k)k=1,...,,p, ∂lM(τ 2, φ)/∂φ)′ and the expected
Fisher information
F (τ 2, φ) = −E




∂2l(τ 2,φ)
∂τ 2
kτ 2
j

k,j=1,...,q
∂2l(τ 2,φ)
∂τ 2
k∂φ
∂2l(τ 2,φ)
∂φ∂τ 2
k
∂2l(τ 2,φ)
∂φ2


.
When calculating these derivatives care must be taken in order to avoid computation and
storing of huge matrices. See Kneib (2006) for details on this and for a more thorough
derivation of the following formulas for the derivatives. The ﬁrst p elements of the score
vector are given by
∂lM(τ 2, φ)
∂τ 2
k
= −1
2tr (X′
kW Xk) + 1
2tr

X′
kW (U, X)F −1(U, X)′W Xk

+ 1
2 (˜y −Uβ −Xα)′ W ZkZ′
kW (˜y −Uβ −Xα) .
Here, F −1 refers to the inverse Fisher information (3.15). If the likelihood contains an
additional dispersion parameter the corresponding derivative is given by
∂lM(τ 2, φ)
∂φ
= −n
2φ + 1
2φtr

(U, X)′W (U, X)F −1
+ 1
2φ (˜y −Uβ −Xα)′ W (˜y −Uβ −Xα) .
The components of the ﬁrst block of the expected Fisher information, that is, the main and
mixed second derivatives for the variance parameters, are given by
−E
 ∂lM(τ 2, φ)
∂τ 2
k∂τ 2
l
!
=1
2tr (X′
lW X′
kX′
kW Xl) −tr

X′
lW (U, X)F −1(U, X)′W X′
kX′
kW Xl

+ 1
2tr

X′
lW (U, X)F −1(U, X)′W X′
kX′
kW (U, X)F −1(U, X)′W Xl

.
44

3.2 Empirical Bayes inference for STAR models
The mixed derivatives with respect to the variance and dispersions parameters are
−E
 ∂2lM(τ 2, φ)
∂τ 2
k∂φ
!
= −1
2φtr (X′
kW Xk) −1
φtr

(U, X)′W X′
kXkW (U, X)F −1
+ 1
2φtr

(U, X)′W (U, X)F −1(U, X)′W X′
kXkW (U, X)

and the second derivative with respect to the dispersion parameter is
−E
 ∂2lM(τ 2, φ)
∂φ2
!
= n
2φ2 −1
φ2tr

(U, X)′W (U, X)F −1
+
1
2φ2tr

(U, X)′W (U, X)F −1(U, X)′W (U, X)F −1
.
3.2.4 Empirical Bayes inference using mixed model representation
Given a STAR model in mixed model representation the empirical Bayes approach can be
summarized by the following steps. First, initial starting values for regression coeﬃcients
need to be chosen. Here, ordinary least squares may provide a good starting point for β.
Next, the following steps are repeated until no more signiﬁcant changes in the parameters
are observed:
(1) To update β and α calculate ˜y and W and compute the mode of the approximate
posterior (3.14) by solving (3.16).
(2) To update precision and possible dispersion parameters compute the elements in
s(τ 2, φ) and F (τ 2, φ) and perform one iteration of a Fisher scoring algorithm using
(3.18).
After convergence, estimates of the original parameters of the STAR model can be retrieved
via
ˆγk = ˜U k ˆβk + ˜Xk ˆαk.
Inference with regard to the estimated ˆβ and ˆα relies on the Gaussian approximation
(3.14) of the posterior. Therefore, standard errors for these coeﬃcients can be obtained
from the diagonal elements of the inverse Fisher information (3.15).
For function
evaluations ˆf k = Zkˆγk, standard errors are given by the diagonal elements of cov(ˆf k) =
45

3 Inference
(U kXk)F −1(U kXk)′, see Lin and Zhang (1999). With this, point wise credible intervals for
ˆf k can be constructed. A formula for simultaneous credible intervals as well as a discussion
of tests on the functional form can be found in Fahrmeir and Kneib (2011, Section 4.2.1).
3.3 Approximate inference for STAR models
A method that had a huge impact on the applied Bayesian community is the integrated
nested Laplace approximation (INLA) approach presented by Rue et al. (2009).
This
approach allows to obtain striking precise results for latent Gaussian models in a
comparatively short amount of computation time.
This is made possible through the
eﬀective implementation and the massive use of numerical optimization and integration
techniques.
In order to introduce this approach some simpliﬁcations with respect to notations need
to be made.
Let x be the collection of all unknown regression coeﬃcients, i.e.
x =
(γ′
1, . . . , γ′
p)′, and let θ be the set of all unknown hyperparameters, i.e. θ = (κ1, . . . , κq, φ).
The main objective of the INLA approach is to estimate the marginal posterior distributions
for the regression coeﬃcients
p(xj|y) =
Z
θ p(xj|θ, y)p(θ|y)dθ.
(3.19)
The INLA approach solves this integral by numerical integration, as explained below. This
requires appropriate approximations to the ﬁrst part of the integral, the full conditional
posterior distribution of xj given θ and y, and to the second part of the integral, the
marginal distribution of θ.
Before going into more detail the joint distribution of x needs to be derived. In order
to guarantee a unique connection of the graph of the GMRF x to the observed data y, x
is expanded by the linear predictor η, i.e. x = (η, γ1, . . . , γp), and a small error is added
to η:
η = Z1γ1 + · · · + Zpγp + ε
with ε ∼N(0, κ−1
ε In), where κε is set high in order to keep the error small, for example
κε = 1e6. Given this parameterization the joint density of x can be derived as follows:
46

3.3 Approximate inference for STAR models
p(x|κ) ∝p(ε|κε)p(γ1|κ1) · · · · · p(γp|κp)
= p(η −Z1γ1 −· · · −Zqγq|κε)p(γ1|κ1) · · · · · p(γq|κq)
∝exp

−1
2x′Qx

(3.20)
where the precision matrix Q has the form
Q =








Qηη
Qηγ1
· · ·
Qηγp
Qγ1η
Qγ1γ1
· · ·
Qγ1γp
...
...
...
...
Qγpη
Qγpγ1
· · ·
Qγpγp








with entries
Qηη = κεIn
Qηγk = −κεZk
Qγkγk = κεZ′
kZk + κkKk
Qγkγl = κεZ′
kZl, k ̸= l.
(3.21)
Note that this is the joint prior distribution of all unknown regression coeﬃcients and the
values of the linear predictor. No observed data has been included into this distribution,
yet.
3.3.1 Exploring the marginal posterior of θ
The numerical integration of (3.19) is performed over representative points of θ, that is,
values of θ that are most likely under p(θ|y). Starting point for ﬁnding these values is the
identity
p(θ|y) =
p(x, θ, y)
p(x|θ, y)p(y)
∝p(x, θ, y)
p(x|θ, y) .
(3.22)
47

3 Inference
Within the INLA approach, p(θ|y) is approximated by a Laplace approximation (Tierney
and Kadane, 1986) as explained in the following. First, note that, in accordance to Section
3.1.1, the denominator of (3.22) can be written as
p(x|θ, y) ∝p(x|θ)p(y|x)
∝|Q|1/2 exp
 
−1
2x′Qx +
n
X
i=1
log p(yi|x)
!
.
Now, it becomes obvious why η has been added to x: The unique connection between η
and y allows to expand the log-likelihood by a quadratic Taylor expansion around η0, i.e.
log p(yi|ηi) ≈ai + biηi −1
2ciη2
i
with coeﬃcients
ai = log p(yi|η0
i ) −∂log p(yi|η0
i )
∂η
η0
i + 1
2
∂2 log p(yi|η0
i )
∂η2
(η0
i )2
bi = ∂log p(yi|η0
i )
∂η
−∂2 log p(yi|η0
i )
∂η2
η0
i
ci = −∂2 log p(yi|η0
i )
∂η2
.
When deﬁning b = (b1, . . . , bn, 0, . . . , 0) and C = diag(c1, . . . , cn, 0, . . . , 0), where the
number of zeros in b and in the main diagonal of C equals the dimension of x−η, p(x|θ, y)
can be approximated by
˜pG(x|θ, y) ≈|Q|1/2 exp
 
−1
2x′Qx +
n
X
i=1
(ai + biηi −1
2ciη2
i )
!
∝|Q|1/2 exp

−1
2x′(Q + C)x + b′x

.
(3.23)
Plugging this GMRF approximation back into the denominator of (3.22) yields the Laplace
approximation of p(θ|y) :
˜p(θ|y) ≈p(x|θ)p(θ) Qn
i=1 p(yi|xi)
˜pG(x|θ, y)

x=x∗(θ)
.
Here, x∗(θ) denotes the mode of ˜pG(x|θ, y) for θ. Given this approximation for the marginal
posterior of θ, the INLA approach starts by ﬁnding the mode of this function. This can be
achieved by using quasi-Newton methods such as the BFGS algorithm. Note that, during
48

3.3 Approximate inference for STAR models
each iteration, the mode of the GMRF approximation in the denominator needs to be
calculated for the current value of θ. After the mode of ˜p(θ|y) has been localized, the
function is further explored. To this end, the negative Hessian is computed in order to
eﬃciently lay out a grid over the parameter space of θ. The coordinates of this grid serve
as the desired representative points for θ. In addition, marginal posteriors p(θk|y) can be
derived by numerical integration of an interpolation of the function evaluations over the
grid structure.
3.3.2 Approximation of the full conditional of xj
From equation (3.19) it remains to approximate the ﬁrst part under the integral, i.e. the full
conditional posterior of xj given θ and y. Rue et al. (2009) present three approximations
for this term which diﬀer in their order of complexity and accuracy. The ﬁrst and simplest
is the GMRF approximation (3.23):
˜pG(xj|θ, y) = ϕ(xj|µj(θ), σ2
j(θ)).
This approximation is appealing since it has already been used during the exploration of
˜p(θ|y). Rue and Martino (2007) explain in detail how the missing marginal variances can be
computed recursively from the Cholesky decomposition of the precision matrix. As a more
precise alternative to the GMRF approximation, Rue et al. (2009) propose to also apply
the Laplace approximation to p(xj|θ, y). The density of this full Laplace approximation
can be written as
˜pLA(xj|θ, y) ∝ϕ(xj|µj(θ), σ2
j(θ)) exp(cubic spline(xj)).
This approximation is extremely precise but computationally more demanding. The third
approximation, the simpliﬁed Laplace approximation, can be obtained by ﬁtting a skewed
normal distribution through a series expansion of the full Laplace approximation. While
computationally less demanding than ˜pLA(xj|θ, y), this approximation is still able to correct
for possible mismatches of ˜pG(xj|θ, y) with respect to location and skewness.
49

3 Inference
3.3.3 Approximation of the marginal posterior of xj
Given representative points for θ and corresponding function evaluations as well as
˜p(xj|θ, y) the marginal posterior of xj can be approximated by solving (3.19) using
numerical integration techniques.
To be more precise, the integral is approximated by
a weighted ﬁnite sum over the support points of θ:
˜p(xj|y) =
X
k
˜p(xj|θk, y)˜p(θk|y)∆k.
(3.24)
3.3.4 Approximate inference using the INLA approach
For STAR models, approximate inference using the INLA approach can be summarized by
the following steps:
(1) Find the mode of ˜p(θ|y) using quasi-Newton methods.
(2) Create an eﬃcient grid structure using the negative Hessian and explore ˜p(θ|y) along
this grid. Keep the coordinates of this grid as support points of θ as well as the
corresponding function evaluations.
(3) Approximate p(xj|θ, y) using either the GMRF approximation,
the Laplace
approximation, or the simpliﬁed Laplace approximation.
(4) Use the results from steps 1 to 3 to compute the weighted sum (3.24) for the xj’s of
interest.
Due to the numerical exploration of p(θ|y) this approach is limited to a moderate
number of hyperparameters, that is, numerical complexity increases with the number of
hyperparameters and, therefore, with the number of (nonlinear) predictors. This issue will
be further discussed with respect to the application to large-scale problems in Chapter 4.
3.4 Chapter summary
Three diﬀerent approaches for ﬁtting STAR models have been presented in this chapter.
The ﬁrst method explores the joint posterior by creating random draws using MCMC
techniques.
It is applicable to a wide range of situations and easily expandable.
The
second method utilizes a reparametrization of the linear predictor in order to apply mixed
50

3.4 Chapter summary
model methodology. The resulting algorithm is deterministic, quite stable and well tested.
Finally, the INLA approach has been presented as an approximate method for ﬁtting STAR
models. While also deterministic, this approach leads to precise estimates which can be
obtained in a comparatively short amount of computation time.
51


4 Adaptations to large-scale problems
In the previous chapter, diﬀerent strategies for ﬁtting STAR models have been presented.
This chapter examines their performance in the presence of high-dimensional regression
coeﬃcients. It can be divided into two parts. The ﬁrst part is concerned with general
considerations: An essential requirement is discussed and the three inference strategies are
compared. Based on this comparison one inference strategy is chosen to be the main focus
in the remainder of this thesis. For this method, the second part presents adjustments that
are necessary in order to be applicable to large-scale problems.
The main ﬁndings of this Chapter have been published in Schmidt et al. (2017). In
particular, the information of the following sections can be found in this publication: Section
4.1.2, 4.2.1, 4.2.2, 4.2.3, 4.2.4, and 4.2.5.
4.1 General considerations
4.1.1 Sparseness as a prerequisite
As a direct consequence of large-scale data situations, the design and structure matrices
of the corresponding GMRF priors will also be of high dimension. Thus, in order to be
computational feasible these matrices cannot be stored as regular matrices. In Chapter 2
it has been shown that a variety of eﬀect types can be formulated using sparse matrices,
i.e. matrices with only a small amount of non-zero entries. The advantage of dealing with
sparse matrices is that they can be represented in special ways which simplify storing and
allow for faster computation. In order to give better insight in the advantage with respect to
storing, consider the sparse matrix representation implemented in Matlab (Gilbert et al.,
1992). Here, only non-zero entries are saved along with their corresponding row and column
indices. To be more precise, the following vectors are stored: A vector of row indices for the
non-zero values, the non-zero values themselves, and a vector for the cumulative column
53

4 Adaptations to large-scale problems
indices. Therefore, on a 64-bit system where real and integer values are both represented
by 8-bytes, a sparse matrix A of dimension n × m, m > 1, requires 16 · nnz(A) + (m + 1) · 8
bytes of storage, where nnz(·) describes the number of non-zero elements of a matrix. In
contrast, if A would be stored as a dense matrix, it would require 8 · n · m bytes of storage.
For a concrete example, consider a spatial GMRF prior over a three-dimensional regular
lattice of size nx × ny × nz where the dependency structure between voxels is induced by a
RW1. Thus, according to (2.17) the corresponding structure matrix can be written as
Kxyz = (Ky ⊗Inx + Iny ⊗Kx) ⊗Iz + Inx·ny ⊗Kz.
(4.1)
Here, K(·) is the structure matrix with respect to a RW1 in one dimension as in Section 2.2.1.
The left panel of Figure 4.1 shows the required log-storage by a dense (straight line) and
sparse (dashed line) representation of this structure matrix with increasing dimensionality
for the special case of nx = ny = nz on working station A, see Section A.1. For nx = 12
the dense matrix (∼22.8 MB) already requires more than 100 times more storage than
the sparse version (∼189 KB). The right panel of this ﬁgure shows the advantage with
respect to computation time. Here, the matrix-vector product Kxyzx with x ∼U(0, 1)
was computed 1,000 times for nx ∈{5, . . . , 30}. In this graph, the lines represent the mean
over all 1,000 trials. For nx = 13 this calculation is on average about 100 times faster for
the sparse representation. One can therefore conclude that sparse matrices can be seen as
a necessary component when working on large-scale problems.
4.1.2 Comparison of inference strategies
Three diﬀerent inference strategies for ﬁtting STAR models have been presented in Chapter
3:
A fully Bayes approach based on MCMC, an empirical Bayes method based on
mixed model representation, and the INLA procedure as an example for approximate
Bayesian inference.
This section examines the performance of these methods in the
presence of high-dimensional regression coeﬃcients. The main question addressed here is
whether it is possible to conduct inference on working stations with limited computational
resources. Advantages and disadvantages are discussed and potential bottlenecks identiﬁed.
Eventually, one inference strategy is chosen for which solutions to these bottlenecks are
provided later in this chapter.
54

4.1 General considerations
log(bytes)
log(seconds)
10
15
20
25
30
−12.5
−10.0
−7.5
−5.0
−2.5
25
50
75
100
10
20
30
nx = ny = nz
dense
sparse
Figure 4.1
Comparison of storage and computation times for dense and sparse matrix
representation.
Interpretation of results
The three inference strategies diﬀer remarkably in the way results are provided. In MCMC
based inference, results rely solely on generated samples of the joint posterior distribution.
From these samples any quantity of interest can be computed in order to summarize
marginal posteriors. For example, moments of any degree, quantiles as well as credible
intervals can easily be obtained.
In addition, joint posterior distributions can also be
summarized. By increasing the number of MCMC samples the accuracy of these summary
statistics can be improved to any given precision. Furthermore, since MCMC inference
does not rely on asymptotic assumptions it is an appropriate candidate for the analysis
of small samples sizes.
Of practical relevance is also the fact that transformations of
parameters can directly be obtained by transforming the corresponding samples.
In a
similar manner various contrasts, i.e. linear combinations of regression coeﬃcients, can be
computed. Moreover, availability of marginal posteriors for precision parameters allows to
assess the variability of these parameters and to make statements about the importance of
the corresponding model terms. However, with respect to the existence of high-dimensional
regression coeﬃcients the necessity to collect a large number of MCMC samples in order
55

4 Adaptations to large-scale problems
to obtain suﬃcient precise estimates may lead to computational problems. For example,
storing 10,000 samples of a 100,000-dimensional regression coeﬃcient already requires about
7.45 GB of storage.
In the empirical Bayes approach discussed in Section 3.2 regression coeﬃcients are
estimated by the posterior mode of the GMRF approximation of the corresponding joint
posterior. In addition, approximate standard errors can be obtained from the diagonal
elements of the inverse of the Fisher information (3.15).
Together, this information is
used to approximate marginal posteriors for regression coeﬃcients by univariate Gaussian
distributions from which credible intervals can be computed. Accuracy of these estimates
is asymptotical, that is, the validity of the Gaussian approximation depends mainly on
the sample size. Transformations of parameters can be obtained by change of variables
and linear combinations may be computed by using well known results for the sums
of Gaussian random variables.
However, for the latter an estimate of the covariance
between coeﬃcients is necessary. For precision parameters only point estimates can be
derived.
However, Fahrmeir and Kneib (2011, Section 4.2.1) give an introduction on
how to set up null hypothesis signiﬁcance tests for precision parameters and, thus, for
the functional form of nonparametric functions. In general, storing the results of high-
dimensional regression coeﬃcients should be possible without further adjustments since the
corresponding approximate marginal posteriors can be completely characterized by only
two parameters.
The results of the INLA approach are ﬁnite sets of function evaluations of all approximate
marginal posteriors of interest. From this, standard errors, credible intervals as well as
any other measure can be obtained by interpolation and numerical integration techniques.
The precision of these results depends on the strategy that has been chosen for the
approximation of the full conditionals of regression coeﬃcients, see Section 3.3.2. However,
the accuracy is usually comparable with that obtained from long MCMC runs (Rue et al.,
2009).
Transformations of posterior marginals can be obtained by change of variables
using numerical derivatives. Marginal posteriors for linear combinations of nodes can be
speciﬁed directly by expanding the GMRF x prior to the ﬁtting process. Alternatively, an
approximation of these marginal posterior can be obtained by combining the corresponding
means and variances and integrating out θ using numerical integration (Martins et al.,
2013).
Variability of precision parameters can usually be assessed through marginal
posteriors for these parameters.
Possible problems with respect to high-dimensional
regression coeﬃcients may arise if the number of function evaluations for the marginal
56

4.1 General considerations
posteriors is high. However, these densities can usually be well approximated by a small
number of parameters or quantiles.
Size of Gaussian Markov random ﬁelds
The size of the largest GMRF within a model is a critical factor since it has a major
inﬂuence on the computational requirements. Due to diﬀerent parametrizations this size
varies remarkably along diﬀerent inference strategies. The Gibbs-structure of the MCMC
sampling scheme allows to work on each regression coeﬃcient separately. Thus, the size of
the largest GMRF is constituted by the size of the largest regression coeﬃcient which is
given by maxk mk, where mk is the dimension of γk. For the empirical Bayes approach it is
not possible to work on each regression coeﬃcient separately. Instead, updated values for
α and β are obtained by maximizing the GMRF approximation to the joint posterior of α
and β given in (3.14). The corresponding graph of this GMRF is deﬁned over the elements
in (β′, α′)′, thus it is of dimension P
k mk. In order to evaluate p(θ|y) within the INLA
approach it is required to compute the mode of the GMRF approximation ˜pG(x|θ, y) given
in (3.23). Here, x is the joint GMRF x = (η′, γ′
1, . . . , γ′
p)′ which is of dimension n+P
k mk,
where n is the number of all observations.
How much the size of these GMRFs diﬀers depends strongly on the problem at hand.
For example, the diﬀerence between the GMRFs of the MCMC based inference and the
empirical Bayes approach for the application in Section 6.1 is limited. Here, the size of
the largest GMRF within the MCMC approach is maxk mk = 565,475 and that of (β′, α′)′
is P
k mk = 565,477. However, for the application in Section 6.2 the diﬀerence is more
pronounced (565,475 for the MCMC approach and 5 · 565,475 = 2,827,375 for empirical
Bayes). For the INLA approach this diﬀerence is even more noticeable: For the application
in Section 6.1 the corresponding GMRF is of size n+
P
k mk = 71,024,765+(565,475+2) =
71,590,242, and for that in Section 6.2 it consists of 247·565,475+5·565,475 = 142,499,700
nodes.
Sparseness
In Section 4.1.1 it has been shown that sparse matrix algebra is a main condition for
handling huge matrices. It is therefore required that the diﬀerent model parameterizations
keep existing sparsity structures and avoid dense representations of large matrices. With
respect to the presented MCMC algorithm all sparse matrices are kept during the ﬁtting
57

4 Adaptations to large-scale problems
process as no re-parameterizations of model parameters are performed. However, it may
not be obvious that this also holds for f
Qk, the precision matrix of the proposal (3.6). From
equation (3.4) it can be seen that f
Qk is sparse only if Qk and Ck are sparse. For Qk the
Markovian property of GMRFs usually ensures that only a small amount of nodes in γk are
directly connected with each other which leads to a high amount of sparseness. The sparsity
structure of Ck strongly depends on the structure of Zk. For most eﬀect types presented in
Section 2.2.1 the design matrices are indicator matrices. For temporal and spatial eﬀects,
for example, each observation is usually assigned to one time point or spatial unit such
as an administrative district or pixel. From this it follows that ∂2l(γ)/(∂γk∂γl) = 0 for
k ̸= l, thus Ck is diagonal and therefore computational undemanding. This implies that
the amount of sparseness of f
Qk is equal to that of Qk.
In order to enable the use of mixed model methodology the empirical Bayes approach
reformulates STAR models as working generalized linear mixed models which requires to
decompose γk into a penalized and an unpenalized part. In general, this decomposition
can be obtained by a spectral decomposition of the corresponding structure matrix. Since
this decomposition usually produces dense matrices it may not be possible to obtain this
factorization for regression coeﬃcients that are of high dimension. However, in some cases,
the required factorization can directly be obtained from the construction of the precision
matrix. For example, the structure matrix that corresponds to a RW1 or RW2 can be
written as K = D′D with D deﬁned as in Section 2.2.1.
A similar factorization can
be obtained for Kronecker product penalties. Let K = K1 ⊗K2 with K1 = D′
1D1 and
K2 = D′
2D2. Basic calculus for Kronecker products then yields K = (D′
1D1)⊗(D′
2D2) =
(D′
1 ⊗D′
2)(D1 ⊗D2)’. The resulting structure matrices are in both cases sparse. For
many other structure matrices, though, such decompositions cannot be given. Consider,
for example, the Kronecker sum penalty for the three-dimensional extension of the RW1
given in (2.17). For this structure matrix, which is extensively used in later chapters of
this thesis, no such simple factorization is known. Thus, the spectral decomposition needs
to be used in this situation which results in dense design matrices. A further violation of
sparseness can be observed during the estimation of precision parameters, see below.
With respect to the INLA approach sparseness concerns the precision matrix Q + C of
the GMRF approximation ˜pG(x|θ, y). For the diagonal blocks of Q the same arguments
apply as for the MCMC approach. The oﬀ-diagonal blocks are usually sparse as well since
they are the product of two sparse design matrices. Sparsity of C follows directly from its
deﬁnition.
58

4.1 General considerations
Estimation of precision parameters
Estimation of precision and dispersion parameters diﬀers greatly between inference
strategies. Within the MCMC approach, precision parameters are sampled from Gamma
distributions as explained in Section 3.1.2.
Usually, this is possible without much
computational eﬀort. Sampling of dispersion parameters depends on the problem at hand.
However, the two special cases discussed in Section 3.1.3 can easily be applied to large-scale
data situations.
The empirical Bayes approach uses REML estimates for variance parameters, see Section
3.2.3.
The required derivatives of the approximate marginal log-likelihood includes the
computation of traces of huge matrices. For example, consider the ﬁrst derivative of lM(κ, φ)
with respect to τ 2
k = κ−1
k . Here, the trace of
X′
kW (U, X)V W Xk
needs to be evaluated with V as the solution of F V
= (U, X)′.
Note that solving
this system requires solving P
k mk linear systems of the form F x = b with dim(F ) =
(P
k mk) × (P
k mk). If, in addition, U and X are dense matrices the complete product
X′
kW (U, X)V W Xk is not explicitly available if one ore more regression coeﬃcients are
of high dimension.
The INLA approach estimates precision parameters by exploring p(θ|y) as explained in
Section 3.3.1. This procedure works well for a moderate size of θ which is directly related to
the number of nonlinear eﬀects in the linear predictor. Usually, this number rarely exceeds,
for example, 10. However, there can be situations where the dimension of θ exceeds a
critical limit. For example, in Section 6.2 a linear regression model is formulated over a
three-dimensional grid structure with more than 500,000 nodes. In order to account for
heteroscedasticity a single dispersion parameter is provided for each node. It is clear from
the way p(θ|y) is explored that in this case the INLA approach is not applicable. Note
that in this situation the empirical Bayes approach is also not applicable.
Standard errors of regression coeﬃcients
In most applications standard errors are required in order to assess signiﬁcance of regression
coeﬃcients. In MCMC inference such standard errors can easily be derived by summarizing
the generated MCMC samples. Thus, obtaining precise standard errors is mainly a question
59

4 Adaptations to large-scale problems
of the runtime of the MCMC sampler. However, this means that one must be able to
(a) generate samples from the full conditional ˜p(γk|y, κk) and (b) evaluate acceptance
probability (3.9). If the dimension of f
Qk exceeds a certain size it may not be possible
to perform these steps on working stations with limited computational resources.
The
other two inference strategies suﬀer from similar problems: Within the empirical Bayes
approach standard errors are estimated by the diagonal elements of the inverse Fisher
information (3.15) which are diﬃcult to compute if one or more regression coeﬃcients are
high-dimensional. For the INLA approach marginal variances for the nodes in x can usually
be obtained in a recursive manner from the elements of the Cholesky decomposition of Q
(Rue and Martino, 2007) which is of dimension (n+ P
k mk)×(n+ P
k mk). As will be seen
in the remainder of this chapter it is computational challenging to derive this decomposition
for a matrix of this size.
4.1.3 Conclusion
Summarizing the above discussion it can be concluded that the empirical Bayes approach
is the least appropriate inference strategy for large-scale problems. The main reason for
this is the necessary representation as a mixed model which does not guarantee sparsity of
high-dimensional model components. Missing sparsity is also a concern in the estimation
of variance components.
In addition, strong limitations with respect to the number
of dispersion parameters make this inference approach not suitable for models that are
frequently used for the analysis of medical images (see Section 6.2). This does also apply
to the INLA approach, although in most other points this inference strategy outperforms
the empirical Bayes method with respect to large-scale problems. By far the best candidate
for high-dimensional data situations is MCMC based inference. This approach works on
the original regression coeﬃcients, ensures sparsity in all steps, and can easily be extended
to more general situations. Therefore, the remaining sections and chapters of this thesis
are dedicated to the construction of an eﬃcient MCMC sampling scheme that is able to ﬁt
large-scale regression models.
4.2 Large-scale inference using MCMC
From the previous sections the following bottlenecks can be identiﬁed for the MCMC
approach: First, obtaining a sample from a high-dimensional proposal distribution. This
60

4.2 Large-scale inference using MCMC
task can be split into sampling x from the zero-mean Gaussian N(0, f
Q
−1
k ) and subsequently
adding ˜µk which is the solution of (3.8), i.e.
f
Qk ˜µk = bk.
Second, the evaluation of
acceptance probability (3.9) requires to compute the log-determinant of f
Qk. The third
bottleneck is with regard to summarizing results in the absence of MCMC samples. In this
section solutions to all of these points are presented. In order to simplify notation the index
k is suppressed and Q is used for f
Q.
4.2.1 Sampling from zero-mean Gaussians
Direct approach
Usually, sampling from a GMRF is performed by using the Cholesky decomposition of the
precision matrix (Rue, 2001). The Cholesky decomposition of Q is given by Q = LL′
where L is a lower triangular matrix. A sample x ∼N(0, Q−1) can then be obtained by
ﬁrst sampling z from N(0, I) and subsequently solving the linear system L′x = z. It can be
shown that the number of non-zero oﬀ-diagonal elements in L, nL, depends on the sparsity
structure of Q. In particular, nL is always greater or equal than the number of non-zero
oﬀ-diagonal elements in the lower triangular of Q, nQ (Rue and Held, 2005, Corollary 2.2).
Thus, computational complexity of the Cholesky decomposition can be measured by the
ﬁll-in ratio, R = nL/nQ.
Over the last decades strategies for an eﬃcient computation of the Cholesky
decomposition have been developed.
The majority of these methods try to reduce
computational complexity by reordering or permuting the elements in Q. Here, the main
objective is either to reduce R, i.e.
increase the sparsity structure of L, or to reduce
the bandwidth of Q. The latter is useful as it can be shown that if Q is a band matrix
with bandwidth b then this bandwidth is preserved by the Cholesky decomposition (Rue
and Held, 2005, Theorem 2.9).
Another popular method is nested dissection (George,
1973).
In this divide-and-conquer approach the nodes of Q are recursively split and
ordered into conditional independent partitions given diﬀerent sets of separator nodes.
In order to illustrate these methods consider precision matrix (4.1) in Section 4.1.1. The
leftmost panel in Figure 4.2 shows the location of all non-zero elements of this matrix for
nx = ny = nz = 10. There are 7·nx·ny·nz−2·(nxny+nxnz+nynz) = 6,400 non-zero elements
and the maximum bandwidth is nx·ny = 100. The number of non-zero elements in the lower
triangular of Q is nQ = 2,700. The next panel displays the same matrix after permuting the
nodes by the symmetric approximate minimum degree permutation (SAMD, George and
61

4 Adaptations to large-scale problems
Figure 4.2
Sparsity pattern of precision matrix (4.1).
Top row displays the original
ordering and the results of diﬀerent permutation algorithms.
Bottom row depicts the
sparsity pattern of the corresponding Cholesky factors.
Liu, 1989), as implemented in Matlabs symamd function. As an example for a bandwidth
reduction algorithm the next panel depicts the sparsity structure of the precision matrix
reordered by the Cuthill-McKee algorithm (Cuthill and McKee, 1969), as implemented in
Matlabs symrcm function. Using this method the maximum bandwidth is reduced to 80.
Finally, the rightmost panel shows the sparsity structure after applying the spectral nested
dissection ordering (Chan et al., 1995) as implemented in the meshpart1 package. The
bottom row of Figure 4.2 depicts the structures of the corresponding Cholesky triangles.
For the unmodiﬁed matrix one obtains nL = 90,909, thus R = 33.67. The SAMD method
yields R = 11.86, the Cuthill-McKee algorithm R = 21.28, and the nested dissection
approach R = 15.8. In summary, the best solution that can be obtained by these methods
for this speciﬁc situation has still about eleven times as many non-zero elements than the
original precision matrix. For small graphs this seems manageable. However, note that the
ﬁll-in ratio does not remain constant if the dimension of the graph increases. Figure 4.3
displays the relation between the dimension of the graph and the ﬁll-in ratio (left panel)
and storage (right panel). It can be seen that both, the ﬁll in ratio as well as storage
increase dramatically when the dimension increases. For example, for nx = ny = nz = 35
all permutation methods produce Cholesky factors with nearly 100 times as many non-zero
1http://www.cerfacs.fr/algor/Softs/MESHPART/, visited on January 21st, 2016.
62

4.2 Large-scale inference using MCMC
R = nL nQ
log(bytes)
50
100
150
200
14
16
18
20
10
15
20
25
30
35
10
15
20
25
30
35
nx = ny = nz
SAMD
Cuthill−McKee
Nested dissection
Figure 4.3
Eﬀect of graph dimension on ﬁll-in-ratio and storage of Cholesky factors
obtained by diﬀerent permutation algorithms.
elements as the lower triangular of Q. In the application chapter of this thesis the sizes
of graphs are considerably larger than in Figure 4.3. For example, in Section 6.3 a graph
of size 190 × 190 × 54 with 514,442 active nodes is used for which nQ = 1,517,257. It
is clear from Figure 4.3 that, even when using permutation strategies, the factorization
of Q would represent a serious problem with respect to computation time and storage.
Therefore, the direct approach for sampling from zero-mean Gaussians is not applicable for
high-dimensional data situations as considered here.
Single-site sampler
The Markov property of GMRF priors allows to derive conditional prior distributions for
γj given γ−j, see for example (2.8) and (2.9) for temporal priors and (2.14) for spatial
priors. In general, let N(µj0, κ−1
j0 ) be the conditional prior for the jth element of γ. Given
this formulation it is possible to divide the Metropolis-Hastings updating step given in
Section 3.1.1 into m = dim(γ) updating steps. This way a one-dimensional proposal density
N(˜µj, ˜κ−1
j ) is obtained for each element of γ. Thus, the problem of sampling from high-
63

4 Adaptations to large-scale problems
dimensional Gaussians can be decomposed into the smallest sampling problems possible.
Note that ˜µj now depends on the mean of the conditional prior µj0.
From a chronological point of view single-site MCMC algorithms have greatly contributed
to the distribution of Bayesian methods (Besag, et al., 1991). They are easy to implement
and do not require a lot of resources. However, these advantages come at a price. Besides
the fact that sequential updating of hundreds of thousands of parameters can be quite time
consuming, slow mixing due to large dependencies between elements can be a serious issue
(Gilks et al., 1996). Therefore, single site updating schemes should only be considered if
alternative methods cannot be applied.
Blocking strategies
As computational power increased in the late 1990’s so did the interest in blocking strategies
as a way to overcome bad mixing behavior of single site samplers. The main idea of these
strategies is to construct a sampler that updates dependent elements in γ jointly. Thus,
blocks should be created in a way that high dependency can be found within blocks and
low dependency between blocks.
A variety of blocking strategies has been published over the last years. While only a
few approaches are suited for general MCMC problems most approaches depend on the
problem at hand. A blocking algorithm that is of particular interest for GMRFs is the
conditional prior proposal approach by Knorr-Held (1999).
Here, γ is divided into nB
blocks γj, j = 1, . . . , nB, for which proposals are generated not by their full conditionals
p(γj|γ−j, y, κ), but rather by their conditional prior distributions given the other blocks, i.e.
p(γj|γ−j, κ). One interesting aspect of this approach is that the resulting block proposal
does not depend on the current state of the chain for this block. Therefore, when calculating
the acceptance probability the proposal density cancels out and the Metropolis-Hastings
algorithm reduces to a Metropolis algorithm. Knorr-Held (1999) noted that a deterministic
or a random change of block conﬁguration may be necessary in order to guarantee good
mixing for parameters near break points. In addition, Brezger and Lang (2006) showed
that the IWLS proposal given in Section 3.1.1 outperforms the conditional prior proposals
approach with respect to mixing of regression and variance parameters.
Another general blocking strategy has been proposed by Rue (2001). Here, blocks are
updated according to their full conditional given all other blocks by using the Cholesky
factorization. For the evaluation of acceptance probabilities, the likelihood of the GMRF
64

4.2 Large-scale inference using MCMC
is approximated by a pseudo likelihood approach.
To be more precise, independence
is assumed between blocks so that the joint prior of γ can be written as p(γ|κ) ≈
p(γ1|γ−1, κ)×. . .×p(γnB|γ−nB, κ). From this approximation it is obvious that the approach
works best for partitions that minimizes dependencies between blocks.
Finding such a
conﬁguration for general graphs is a non-trivial problem. However, even when such an
optimal partition has been found it remains unclear to what extend the approximation
error aﬀects ﬁnal inference. In addition, similar to the approach by Knorr-Held (1999)
a change in blocking conﬁguration may be necessary in order to circumvent problems at
break points.
Closely related to the above blocking strategies are divide-and-conquer approaches. The
one suggested by Rue (2001) is especially suited for GMRFs. Here, the graph is partitioned
into blocks that are conditionally independent given a set of separating nodes γs. Again,
blocks are updated by their full conditionals. The diﬀerence to the blocking strategies above
is that the marginal prior distribution for the set of separating nodes is required for which
the marginal covariance matrix needs to be computed. This way problems at break points
are avoided. However, because of this the divide-and-conquer approach is either limited
to situations where this matrix can be handled by direct methods or it requires a complex
iterative procedure where the remaining blocks are partitioned in a recursive manner until
all marginal covariances are of a certain size.
In summary, blocking strategies can be used to sample from high-dimensional Gaussians
if direct methods are not available.
In contrast to single-site samplers they usually
perform better with respect to mixing and speed. However, ﬁnding an appropriate block
conﬁguration is a non-trivial task and depends highly on the problem at hand. In addition,
implementation can be diﬃcult and a careful bookkeeping of indices is required. Note that
the block update of parameters as discussed in Section 3.1.4 requires updating γ in one step
and, therefore, does not provide a solution for sampling from high-dimensional Gaussians.
Approximate sampling
Over the last years, much progress has been made with respect to approximate sampling
of zero-mean Gaussians. Most promising approaches utilize Krylov subspace methods, a
class of iterated methods for sparse linear systems (Liesen and Strakos, 2012). For example,
Chow and Saad (2014) discuss approximate sampling from zero-mean Gaussians given their
covariance matrices while Aune et al. (2013) and Simpson et al. (2013) present solutions for
the case of precision matrices. In general, Krylov subspace methods are able to provide an
65

4 Adaptations to large-scale problems
Algorithm 1 Lanczos algorithm.
1: Set v0 = 0 and β1 = 0 and initialize v1
2: for j = 1, . . . , r do
3:
w = Qvj −βjvj−1
4:
αj = w′vj
5:
w = w −αjvj
6:
βj+1 = ||w||2
7:
vj = w/βj+1
8: end for
approximation to the general problem f(Q)b by only using matrix-vector products. Here,
f is an arbitrary function, for example f(Q) = Q−1 for solving systems of linear equations,
or f(Q) = Q−1/2 for sampling from N(0, Q−1). This is made possible by projecting the
original problem into the much smaller Krylov subspace Kr(Q, v) which is spanned by the
Krylov sequence b, Qb, Q2b, . . . , Qr−1b. If V r = (v1, . . . , vr) is an orthogonal basis of Kr
the orthogonal projection of the exact solution on the Krylov subspace is given by
˜x = V rV ′
rf(Q)b.
(4.2)
In case the (modiﬁed) Gram-Schmidt orthogonalization is used to build V r one obtains the
Arnoldi algorithm (Saad, 2003, Section 6.3). If in addition Q is Hermitian, the special case
of the Lanczos algorithm (Saad, 2003, Section 6.6) is received. This algorithm is given in
Algorithm 1.
Besides the orthonormal basis V r of Kr this algorithm produces coeﬃcients
αj and βj, where j = 1, . . . , r, which form the tridiagonal matrix T r:
T r =











α1
β2
β2
α2
β3
...
...
...
βr−1
αr−1
βr
βr
αr











.
(4.3)
This setup satisﬁes
QV r = V rT r + βr+1vr+1e′
r
66

4.2 Large-scale inference using MCMC
where er is the rth column of the identity matrix. Note that V ′
rV r = I and V ′
rvr+1 = 0
since V r is an orthonormal basis. From this it follows immediately that V ′
rQV r = T r.
In order to solve Q1/2x = z the ﬁrst vector of V r is set to v1 = z/||z||2. Thus, the
approximate solution (4.2) can be rewritten as
˜x = βV rV ′
rQ−1/2V re1
with β = ||z||2. The ﬁnal approximation to Q−1/2z is obtained by further approximating
V ′
rf(Q)V r by f(V ′
rQV r), thus
˜x∗= βV rT −1/2
r
e1.
(4.4)
By using this approximation f only needs to be applied to the much smaller matrix T r
which can be obtained with low computational cost if r is small, which is usually the case.
Following Chow and Saad (2014) the algorithm can be stopped if the relative change in ˜x∗
falls below a given threshold.
As will be shown throughout the remaining chapters, sampling using the Lanczos
algorithm is, compared to other approaches, extremely fast and suﬃcient precise.
In
particular, convergence behavior is discussed in Section 4.2.3 and further investigations
with respect to approximation errors are given in Chapter 5.
4.2.2 Solving systems of linear equations
Once a sample from the zero-mean Gaussian N(0, Q−1) is available all that remains in order
to obtain a full sample of the proposal for γ is solving Qµ = b. Note that if additional
linear constraints as introduced in Section 3.1.4 are present further linear systems need to
be solved.
Direct approach
Computing µ is usually performed as proposed in Rue (2001), that is, by ﬁrst solving
Lv = b and subsequently solving L′µ = v.
Here, L is the Cholesky factor obtained
67

4 Adaptations to large-scale problems
by Q = LL′. However, with respect to large matrices this method suﬀers from the same
limitations outlined in Section 4.2.1 and is therefore not applicable for situations considered
here.
Iterative methods for sparse linear systems
For large precision matrices Rue (2001) suggests to avoid the Cholesky decomposition and
to use iterative methods for sparse linear systems instead. In particular, he recommends
to use the well known conjugate gradient method (Hestenes and Stiefel, 1952), another
Krylov subspace methods for solving sparse linear systems (Saad, 2003, Section 6.7). Several
alternatives have been proposed in the literature over the years, for example the biconjugate
gradient method (Fletcher, 1976), the (generalized) minimum residual method (Paige and
Saunders, 1975; Saad and Schultz, 1986), as well as the conjugate gradients squared method
(Sonneveld, 1989). In addition, the Lanczos algorithm presented in Section 4.2.1 can also
be used for solving linear systems. This can be achieved by using Algorithm 1 and setting
v1 = r0/β with r0 = b −Qx0 and β = ||r0|| (Saad, 2003, Algorithm 6.16). Note that due
to diﬀerent starting conﬁgurations of v1 it is not possible to use the same basis V r and
coeﬃcients αj and βj, j = 1, . . . , r for f(Q) = Q−1/2 and f(Q) = Q−1. Thus, in order to
obtain a sample from (3.6) it is required to run the Lanczos algorithm twice.
4.2.3 Preconditioning
The quality and convergence behavior of Krylov subspace methods are strongly connected
to the condition of Q. If Q is not-well conditioned, i.e. if the solution f(Q)b is sensitive
to small perturbations in Q or b, then the iterative procedures presented above may suﬀer
from slow convergence and inexact solutions, even after successful convergence. However,
even for problems that are well-conditioned the number of iterations required to obtain a
suitable approximation to f(Q)b may be inadequately high in order to be of practical use.
In both situations the performance of Krylov subspace methods can usually be improved
by preconditioning. This means to reformulate the original problem as one that has the
same solution but is easier to solve (Saad, 2003, Chapter 9).
In this section the general framework of preconditioning is presented, ﬁrst with respect
to linear systems and subsequently for approximate sampling using the Lanczos algorithm.
This is followed by a presentation of diﬀerent preconditioners and a brief discussion on the
convergence behavior of Krylov subspace methods.
68

4.2 Large-scale inference using MCMC
Preconditioning for linear systems
With respect to linear systems, a preconditioner for Q is a matrix M for which M −1Q
has a better condition than Q. Then, instead of the original linear system Qx = b the
preconditioned problem M −1Qx = M −1b is solved.
If the preconditioner is available
in factorized form, i.e. M = M LM R, the preconditioned system can be split into the
following equations
M −1
L QM −1
R u = M −1
L b,
x = M −1
R u.
From these equations it is obvious that, in order to solve Qx = b more easily, M must
be constructed in a way that the solution to Mx = b can be obtained without much
eﬀort. This observation requires M or M L and M R to be sparse. Further requirements
are that the preconditioner should be in “some sense” (Saad, 2003, p. 275) close to Q
and nonsingular. This vague formulation of requirements is the reason that the task of
ﬁnding an appropriate preconditioner for a given problem is sometimes referred to be “a
combination of art, science, and intuition” (Hafez et al., 2010, p. 81). However, with respect
to the conjugate gradient method robust preconditioners are available as discussed below.
Preconditioning for sampling
While preconditioning as discussed so far is extensively used with respect to solving linear
systems it may not be obvious that this is also a suitable strategy for sampling from high-
dimensional Gaussians, i.e. solving Q1/2x = z. This problem is addressed in Chow and
Saad (2014) with respect to covariance matrices and in Simpson et al. (2013) for precision
matrices. In both articles it is pointed out that if Q and M are symmetric positive matrices
with M = M LM R, M R = M T
L and if u ∼N(0, (M LQM R)−1), then the solution of
M Rx = u is a sample of a zero-mean Gaussian with precision Q. Thus, if M meets
the above requirements for preconditioners the sampling process can be simpliﬁed by (a)
obtaining u from a Gaussian with precision matrix M LQM R using the Lanczos algorithm
and (b) computing the ﬁnal sample via x = M −1
R u. Obligatory adjustments in Algorithm
1 are with respect to Qvj in line 2. This is replaced by ﬁrst solving M La = vj, then
calculating b = Qa and ﬁnally solving M Rw = b.
See Section A.2.2 for a complete
implementation of the preconditioned Lanczos algorithm.
69

4 Adaptations to large-scale problems
Preconditioning techniques
Deﬁning an appropriate preconditioner for a particular problem is a diﬃcult task and an
active ﬁeld of research within the ﬁeld of numerical linear algebra.
Therefore, only an
incomplete overview of this topic can be provided here.
From a historical point of view, incomplete factorizations preconditioners of the form
M = M LM R are the most frequently used type of preconditioners. Within this class
incomplete versions of the LU (ILU, Varga, 1960) and Cholesky (IC, Meijerink and van der
Vorst, 1977) decompositions are the most popular. Incompleteness of these factorizations
can be achieved in diﬀerent ways. The most generic method is to consider only non-zero
elements of Q in M L and M R. Alternatively, a drop tolerance can be speciﬁed that keeps
only those values in M L and M R that lie above this threshold (Saad, 1994). Since Q is
symmetric it makes sense to prefer the IC factorization over the ILU factorization, because
then only one matrix needs to be stored. With respect to the conjugate gradient method
the IC decomposition serves as a popular and quite robust preconditioner. This choice is
also made by Chow and Saad (2014) and Simpson et al. (2013) with respect to approximate
sampling using the Lanczos algorithm.
Over the last years approximate inverse preconditioners have been proposed as
alternatives to incomplete factorizations, see Benzi and Tuma (1999) for a review on early
methods. Here, P = M −1 is chosen in a way that it approximates the most important
aspects of Q−1. A large class of such methods consists of iterative procedures that rely
on the minimization of ||I −QP ||F, where || · ||F denotes the Frobenius norm. The well
known Newton-like iterations by Schulz (1933) and the (global) minimal residual algorithm
by Chow and Saad (1998) provide popular examples within this class of methods. Since
the number of non-zero elements in P increases with each iteration of these methods, some
sort of dropping rule needs to be applied in order to preserve sparsity. As an alternative to
iterative methods approximate inverses can be constructed by utilizing the above discussed
incomplete factorizations preconditioners. Here, the task of ﬁnding an approximate inverse
for Q is divided in approximating the inverse of M L and M R which are derived from
an ILU decomposition. As Benzi and Tuma (1999) pointed out, a serious disadvantage
of this procedure is the introduction of multiple levels of incompleteness, one for the ILU
decomposition and one for the approximate inverse of M L and M R.
As stated at the beginning, this section does not claim to provide a complete overview
of preconditioning techniques. Actually, quite the contrary is the case: Important classes
70

4.2 Large-scale inference using MCMC
such as polynomial (O’Leary, 1991; Fischer and Freund, 1994) and spectral (Giraud and
Gratton, 2006) preconditioners are not discussed. More recently, adaptive preconditioning
techniques that are able to update the preconditioner in each iteration based on the Krylov
subspace formulated so far have been studied (van den Eshof and Hochbruck, 2006; Ilić
et al., 2008). Such approaches have also been used within approximate sampling from zero
mean Gaussians (Simpson et al., 2007). However, other authors reported that under realistic
conditions, i.e. ﬁnite-precision arithmetic, adaptive methods often do not perform better
than usual preconditioners (Monteiro et al., 2004) or become unstable (Saad, 2003, Section
9.4). In addition, the more specialized a preconditioner becomes the less applicable it is
for general situations. Therefore, a detailed discussion of more advanced preconditioners is
not pursued at this point.
Convergence behavior
Convergence behavior of Krylov subspace methods is usually analyzed by providing upper
bounds for the error ||f(Q)b−x∗
m||, where x∗
m is the approximate solution to f(Q)b after m
iterations (Saad, 2011, Section 6.11). Most error bounds that can be found in the literature
are concerned with special choices of f, such as f(x) = x−1 or f(x) = exp(x). One of
the few generalizations is provided by Ilić et al. (2010) who prove that the following error
bound holds for all transforms f which are of particular interest here, i.e. f(x) = x−1 and
f(x) = x1/2:
||f(Q)b −x∗
m|| ≤f(λmin)||rm||.
(4.5)
In this notation, λmin is the smallest eigenvalue of Q and rm is the residual obtained after
applying m steps of the conjugate gradient method for solving Qx = b. The advantage of
this result is that the convergence behavior of the conjugate gradient method can directly
be assigned to the Lanczos algorithm for approximate sampling by setting f(λmin) = λ−1/2
min
(Simpson et al., 2013). According to this error bound, convergence is linear with respect
to the number of iterations, although, in practice “super-linear” convergence is usually
observed (Chow and Saad, 2014).
However, as pointed out by Chow and Saad (2014)
this and further existing error bounds do not consider improved convergence which usually
follows from preconditioning. It is therefore expected that the convergence performance for
preconditioned systems is even better.
71

4 Adaptations to large-scale problems
A disadvantage of error bounds like (4.5) is that only statements about the absolute
error between the true and approximate solution can be made.
This may not account
for convergence problems of GMRFs with high precision where realizations tend to be be
highly smoothed and are often heavily shrinked towards zero. In this case, the absolute
error becomes small rather fast while the relative error still shows large discrepancies.
Thus, convergence may be indicated by theoretical error bounds although the approximate
solution is far away from the true solution. Situations where this problem may occur are
further discussed in Chapter 5.
4.2.4 Calculation of log-determinants
For Gaussian distributed response variables the proposal density of γ corresponds to the
full conditional posterior, thus, the MCMC algorithm reduces to a Gibbs sampler.
In
this case, computation of the log-determinant of Q, log det(Q), is not necessary. For non-
Gaussian responses, however, evaluation of acceptance probability (3.9) is required for which
the normalizing constant of the proposal density is an important component. Therefore,
computation of log det(Q) is necessary in this case.
If γ and κ are updated jointly as
discussed in Section 3.1.4, the log-determinant is required in any case, no matter what
response family is considered.
Direct approach
Similar to sampling, the Cholesky factor plays a major role within the “direct” approach
of computing log det(Q), as the determinant of a triangular matrix equals the product
of its diagonal elements (Harville, 1997, Section 13.1c). Thus, log det(Q) = log det(L) +
log det(L′) can be written as
log det(Q) = 2
X
i
log Lii.
(4.6)
As shown in the previous sections, if Q exceeds a certain size, it may not be possible to
compute the Cholesky factorization. Thus, alternative strategies need to be considered.
72

4.2 Large-scale inference using MCMC
Approximation of log-determinants
Methods that try to approximate (log-)determinants started to emerge in the early 1990s.
Key to most of these methods is the identity log det(Q) = tr(log Q), where log Q denotes
the matrix logarithm. Martin (1992) was the ﬁrst who used Taylor series expansion of
the trace of the matrix logarithm.
Bai et al. (1996) used this identity to formulate a
Monte Carlo approach that yields an estimate as well as lower and upper bounds for the
log-determinant. Their method makes extensive use of Gaussian quadrature and related
theory. Later, the same group provided deterministic bounds that are not as precise but
less computational demanding (Bai and Golub, 1996). Nevertheless, the main advantage
of stochastic estimators, i.e.
the fact that their accuracy can easily be improved by
increasing the sample size, led to the development of further Monte Carlo methods for
the approximation of log-determinants. For example, Thron et al. (1996, 1998) provided
a stochastic estimator for tr(log Q) by combining the Padè approximation of the matrix
logarithm with the so called complex Z2 noise trace estimator.
Another Monte Carlo
algorithm was proposed by Barry and Pace (1999) for the special case of spatial weight
matrices as they occur within spatial econometric models (LeSage and Pace, 2009). Reusken
(2001) constructed a sparse approximate inverse (Cosgrove et al., 1992) of the Cholesky
triangle L and used its diagonal elements to formulate a deterministic approximation for
det(Q)1/n. Another deterministic approach was proposed by Pace and LeSage (2004). They
replaced the Taylor series expansion of tr(log Q) used by Martin (1992) with Chebyshev
polynomials (Mason and Handscomb, 2002) which yield more precise approximations. For
dense covariance matrices within Gaussian processes, Zhang and Leithead (2007) extended
the Taylor series expansion by a set of compensation schemes which includes stochastic
trace estimation by random seeds. This method was further improved by Zhang et al.
(2008) who used uniformly distributed seeds instead of Gaussian seeds. Another stochastic
estimate for the special case of GMRFs has been provided by Aune et al. (2014). Here,
tr(log Q) is estimated by combining Cauchy’s integral formula for the computation of
the matrix logarithm, Krylov subspace methods for solving linear systems, and stochastic
estimators for traces. In order to be applicable this approach requires careful coloring of the
adjacency graph of Q. More recently, Han et al. (2015) improved the Chebyshev expansion
by Pace and LeSage (2004) by estimating traces of huge matrices using the stochastic
Hutchinson estimator (Hutchinson, 1990), which allows to use higher order Chebyshev
polynomials resulting in a more precise approximation. The combination of this with the
general advantages of Monte Carlo estimates makes this approach a suitable candidate
73

4 Adaptations to large-scale problems
for the application within large-scale problems. Therefore, in the following, this method
is explained in more detail as it is used for the approximation of log-determinants in the
remainder of this thesis.
The approach by Han et al.
can be roughly divided into two steps: First, the log-
determinant is approximated by a Chebyshev expansion. In the second step, traces that
arise in this representation are estimated by the stochastic Hutchinson estimator. For the
Chebyshev expansion of the log-determinant ﬁrst kind Chebyshev polynomials of degree p
are used (Mason and Handscomb, 2002, Section 1.2.1). In general, Chebyshev expansions
are characterized by an excellent approximation of functions f(x) with x ∈[−1, 1]. This is
accomplished by a combination of a sequence of orthogonal polynomials and non-equidistant
nodes that are well distributed over the interval [−1, 1]. In order to apply the Chebyshev
expansion for the estimation of log det(Q) Han et al. consider the matrix A = I −Q
instead of Q for which
log det(Q) = log det(I −A)
=
X
i
log(1 −λi)
(4.7)
holds. Thus, f(xi) = log(1 −xi) with xi = λi. Here, λ1, . . . , λm are the eigenvalues of A.
The necessary condition of xi ∈[−1, 1] can be achieved by dividing all elements of Q by
δσ = σmin + σmax, where σmin and σmax are the extreme eigenvalues, i.e. the minimal and
maximal eigenvalues of Q. From this it follows that λi ∈[0, 1]. A procedure for estimating
extreme eigenvalues of positive deﬁnite matrices based on the Lanczos algorithm is presented
in Section A.3.2. Once the eigenvalues of A have been standardized the components of
(4.7) can be approximated by Chebyshev expansions: P
i log(1 −λi) ≈P
i pn(λi). Here,
pn(λi) = Pp
j=0 cjTj(λi) where Tj denotes the ﬁrst kind Chebyshev polynomial of degree j,
and cj the corresponding coeﬃcient. By setting T0(λ) = 1 and T1(λ) = λ the polynomials
can be recursively deﬁned as Tj+1(λ) = 2λTj(λ)−Tj−1(λ), j ≥1. The coeﬃcients are given
by
cj = 1 + I(j > 0)
p + 1
p
X
k=0
log(1 −xk)Tj(xk)
74

4.2 Large-scale inference using MCMC
with xk = cos
 π(k+1/2)
p+1

for k = 0, . . . , p. Since these coeﬃcients do not depend on i one
can write
X
i
log(1 −λi) ≈
X
i
p
X
j=0
cjTj(λi)
=
p
X
j=0
cj
X
i
Tj(λi).
The next and last step makes use of the fact that the sum of eigenvalues equals the trace
of a matrix (Harville, 1997, Section 21.6) and that this result can directly be expanded to
matrix polynomials. Thus, the ﬁnal Chebyshev expansion can be written as
log det(Q) ≈
p
X
j=0
cjtr(Tj(A)).
(4.8)
The most expensive step within this approximation is the calculation of tr(Tj(A)): Due
to its recursive nature Tj(A) includes powers of I −Q which may be time consuming or
even impossible to obtain. Starting point for an approximation to tr(Tj(A)) is to write the
exact solution as
tr(Tj(A)) =
n
X
i=1
e′
iTj(A)ei.
(4.9)
Here, ei is the ith column of the n-dimensional identity matrix. Hutchinson (1990) proposed
to approximate (4.9) by the following stochastic estimator: Let ul be a n-dimensional
random vector with entries {−1, 1}, where each state has probability 1/2. Then,
btr(Tj(A)) = 1
r
r
X
l=1
u′
lTj(A)ul
(4.10)
is an unbiased estimator of (4.9).
In addition, Hutchinson showed that (4.10) has the
smallest variance of all such stochastic estimators. In order to avoid direct computation
of Tj(A) its recursive deﬁnition can be exploited. That is, the vector wj,l = Tj(A)ul can
be recursively deﬁned by wj+1,l = 2Awj,l −wj−1,l. By combining this with (4.8) the ﬁnal
approximation of the log-determinant is given by
log det(Q) ≈
p
X
j=0
cj
1
r
r
X
l=1
u′
lwj,l.
75

4 Adaptations to large-scale problems
A modiﬁed sampling scheme
The evaluation of acceptance probability (3.9) requires to compute log det(Q) because
Q depends on the current state of the Markov chain, i.e. in the notation of Section
3.1.1:
f
Q
p = f
Q
p(γc).
This insight may help to provide an alternative solution to the
approximation of log-determinants: If it were possible to free f
Q
p from the dependency of
γc the log-determinant in the acceptance probability would cancel itself out. The source of
this dependency can be found in the construction of the GMRF proposal. Here, the log-
likelihood is approximated by a quadratic Taylor expansion around γc. Thus, by choosing
a diﬀerent point around which the series is expanded the desired independence between
f
Q
p and the current state of the Markov chain can be achieved. A natural candidate for
this point is the mean of the last accepted proposal density, which will be denoted by
˜µ∗.
Besides the fact that this simple modiﬁcation eliminates the necessity to compute
log det(Q), it also avoids the expensive re-computation of the mean and precision matrix
that are usually required for the evaluation of the acceptance probability.
For the special case of GLMs this modiﬁcation corresponds to replacing γc by ˜µ∗in
ηc:
η∗= ηc + Z(˜µ∗−γc).
This approach was proposed by Brezger and Lang (2006) in order to increase acceptance
rates. As Brezger and Lang pointed out, high acceptance rates can be a desirable feature for
the application to large spatial eﬀects, because the MCMC sampler becomes more sensitive
to small changes in the coeﬃcients.
Note that when updating γ and κ jointly this modiﬁcation cannot be applied since Q
now depends on the current state of the chain through κ. Thus, approximating log det(Q)
is obligatory in this case.
4.2.5 On-line calculation of posterior moments
Saving thousands of samples of high-dimensional regression coeﬃcients can be problematic
with regard to computational memory. Algorithms for the on-line calculation of posterior
moments oﬀer an attractive alternative in these situations, especially since the marginal
posteriors of regression coeﬃcients can usually be quite well approximated by Gaussian
distributions which, in turn, are fully parameterized by their mean and variance.
76

4.2 Large-scale inference using MCMC
On-line calculation of the mean and variance is rather simple given the nature of their
composition. However, to avoid numerical problems the method proposed by Welford (1962)
for the calculation of ﬁrst and second moments can be used. The vector of means, ¯γt, and
variances, s2
t = st/(t −1), of γ for the ﬁrst t samples γ1, . . . , γt can be obtained by
¯γt = ¯γt−1 + 1
t(γt −¯γt−1)
(4.11)
st = st−1 + (γt −¯γt−1) ◦(γt −¯γt),
(4.12)
where ◦denotes the Hadamard product. Thus, the marginal posterior for the jth element
of γ after t samples can be approximated by N(¯γj,t, s2
j,t).
If more than one chain is generated the results obtained by (4.11) and (4.12) can be
combined.
Suppose m chains with equal length T are available and that ¯γ(l) and s2(l)
denote the vectors of means and variances for the jth chain, respectively. These moments
can then be aggregated as follows:
¯γ = 1
m
m
X
l=1
¯γ(l)
s2 =
T −1
Tm −1


m
X
l=1
s2(l) +
T
T −1
m
X
j=1
(¯γ(l) −¯γ)2

.
Combinations of regression coeﬃcients
An important advantage of Bayesian inference based on MCMC over other inference
strategies is the possibility to conduct inference for any combination of parameters by
simply aggregating the corresponding samples in the desired way.
Having access only
to marginal posterior moments does not necessarily constitute a serious limitation. For
example, suppose γ1 and γ2 are regression coeﬃcients deﬁned over the same graph and
that the marginal posteriors for the jth elements of these vectors are given by N(¯γ1,j, s2
1,j)
and N(¯γ2,j, s2
2,j), respectively. If one is interested in a linear combination of γ1 and γ2,
for example γ1+2 = γ1 + γ2, the marginal posteriors alone are not suﬃcient to obtain the
marginal posterior for γ1+2,j. Instead, the covariance between γ1 and γ2, cov(γ1, γ2) = s2
1,2,
is required in addition:
γ1+2,j|y, κ1, κ2 ∼N(¯γ1,j + ¯γ2,j, s2
1,j + s2
2,j + 2s2
1,2,j).
77

4 Adaptations to large-scale problems
Similar to (4.11) and (4.12) the covariance can be computed on-line in a recursive manner
given the ﬁrst t samples of γ1 and γ2:
s1,2,t = s1,2,t−1 + (γ1,t −¯γ1,t−1) ◦(γ2,t −¯γ2,t).
Note that this strategy cannot be applied to nonlinear combinations of regression
coeﬃcients f(γ1, . . . , γp).
Here, the best possibility to derive an approximation to the
desired posterior marginal is to combine the involved coeﬃcient vectors during the ﬁtting
process and to compute appropriate summary statistics on-line. However, in general, for
nonlinear combinations it cannot be assumed that the posterior marginal can be accurately
approximated by its mean and variance. As an alternative one can split the support of f
into bins and count how often a sample falls in each bin. This way, the marginal posteriors
can be approximated by interpolating the resulting histogram. Note that even for high-
dimensional coeﬃcients this method does not require large computational resources. For
example, dividing the support of f(γ1, . . . , γp) into 100 bins requires about 76 MB of
storage for coeﬃcients with 100,000 elements. However, a vague idea of the domain of the
parameter may be helpful in order to set up the bins in an eﬃcient way.
Convergence assessment
Note that, although the full chains of regression coeﬃcients may not be available,
posterior moments are suﬃcient to perform convergence diagnostics using the potential scale
reduction factor as explained in Section 3.1.4. In addition, complete samples of precision
and dispersion parameters as well as of randomly selected components of high-dimensional
regression coeﬃcients may be saved and monitored visually.
4.3 Chapter summary
In this chapter, MCMC inference has been chosen as an appropriate candidate for ﬁtting
STAR models with high-dimensional regression coeﬃcients.
Solutions to all potential
bottlenecks have been provided: Iterative methods for sparse linear systems have been
discussed as a suitable way to sample from high-dimensional Gaussians and alternatives
to the computation of log-determinants of high-dimensional precision matrices have been
78

4.3 Chapter summary
shown. This includes an approximation technique as well as a modiﬁed sampling scheme.
In addition, storage problems can be avoided through the on-line calculation of posterior
moments.
79


5 Simulation studies
The goal of this chapter is to analyze the performance of the methods proposed in Chapter
4.
It is organized into two sections: The ﬁrst part examines the error and quality of
speciﬁc approximations. Its primary task is to identify situations in which the performance
of these approximations is limited. Subsequently, complete MCMC sampling schemes are
formulated whose performance is then further investigated under realistic conditions in the
second part. Here, the error that is induced by these sampling schemes on the ﬁnal result
is of particular interest. For both sections, data sets are simulated which are adapted to
the applications in Chapter 6.
A simulation study similar to the dense data setup below has been published in Schmidt
et al. (2017). Furthermore, a modiﬁed version of the ﬁrst part of Section 5.1 can be found
in the discussion of Schmidt et al.
5.1 Performance of approximations
In Section 4.2.3 it has been discussed that the quality of Krylov subspace methods depends
on the condition of the system under consideration which is usually imposed by the condition
of f
Q.
Subsequently, preconditioning techniques have been presented that are able to
improve the condition of the problem at hand. However, in some situations the beneﬁt
of preconditioners is limited.
In general, the condition of f
Q depends on many factors.
Those of most relevance can be identiﬁed from the deﬁnition of f
Q given in equation (3.7),
i.e.
f
Q = Q + C.
Here, Q = κK is the precision matrix of the GMRF prior, and C is a diagonal matrix with
entries −∂2l(γc
j)/(∂2γj) where j = 1, . . . , m, with m = dim γ. The ﬁrst factor that has a
subsequent inﬂuence on the condition of f
Q is κ. This parameter is mainly responsible for the
81

5 Simulation studies
amount of smoothness between the elements in γ. As already discussed in Section 4.2.3, a
high amount of smoothness may shrink the elements in γ strongly towards zero or towards
an overall population eﬀect. Thus, even small perturbations in the corresponding linear
systems may have a relatively large impact on the systems solution. In such situations, the
eﬀect of preconditioning may be limited.
The second factor that aﬀects the condition of f
Q is given by the diagonal matrix C.
Its elements are given by the negative second derivative of the log-likelihood around the
current state of the chain, γc. The more information in the data the more pronounced
the curvature of the log-likelihood, thus the larger the elements in C.
The larger the
elements are in C, the more f
Q tends to be diagonally dominant which is associated with
well-conditioned problems (Saad, 1995). Accordingly, less information in the data increases
the risk of creating a matrix which is nearly non-diagonal dominant1.
In this context,
Saad noted, with respect to approximate inverse preconditioners, that “in the non-diagonal
dominant case, we do not know in advance whether or not here exists indeed an approximate
inverse which is sparse enough to be practically useful.” (Saad, 1995, page 13). Thus, worse
performance of preconditioners and, therefore, limited applicability of Krylov subspace
methods must be expected in cases where diagonal dominance is not well-pronounced.
In this section, the performance of speciﬁc approximations is assessed while accounting
for the sources which aﬀect the condition of f
Q: First, diﬀerent amounts of information
are considered by setting up two diﬀerent simulations, one where the information is rather
dense and one with fairly sparse information content. Second, the eﬀect of the precision
parameter is considered by ﬁxing κ to values on a pre-speciﬁed grid. In order to account
for the complex interrelation between model parameters the approximation techniques are
evaluated within complete MCMC algorithms. The simulation setups are explained next.
5.1.1 Simulation setups
Dense data situation
This setup is adapted to the data used in the voxel-wise regression application in Section
6.2. For each subject response values are generated that are aligned over a regular lattice
(image) of dimension nx = ny = 120. Images for n = 100 subjects are generated, thus the
1Note that due to the strictly positive elements in C and the structure of the precision matrices discussed
in Section 2.2.1, eQ will always be diagonally dominant.
82

5.1 Performance of approximations
Dense data set
Sparse data set
0
1
−0.4
−0.2
0.0
0.2
0.4
Figure 5.1
True eﬀect image used for the dense data situation (right panel) and generated
data for the sparse data situation (left panel).
complete data consist of n · nx · ny = 1,440,000 observations in total. Gaussian response
values for the jth pixel of the ith subject are simulated according to the following rule:
yij ∼N(ηij, κ−1
y ), with κy = 5.0.
The linear predictor for the jth pixel ηj = (η1j, . . . , ηnj)′ is given by
ηj = f(j)z + Xβ.
Here, z is a metric covariate of dimension n × 1 whose values are randomly assigned over
the interval [−1, 1], and f is a smooth function over the pixels of the complete lattice which
is created by the following formula:
f(j) = (jx −nx
2 )(jy −ny
2 ).
In this notation, jx and jy refer to the coordinates of the jth pixel of the lattice with
respect to the rows and columns, respectively. The values of this eﬀect are linearly scaled
to the interval [−0.5, 0.5]. The left panel in Figure 5.1 displays the resulting true eﬀect
image. Furthermore, the n × 2 matrix X is a ﬁxed design matrix which consists of an
intercept and a randomly sampled dummy variable. The corresponding ﬁxed eﬀect β is set
to β = (3.8, −0.2)′.
83

5 Simulation studies
Sparse data situation
In the microscopy application in Section 6.3, a spatial eﬀect is ﬁtted to a three-dimensional
microscopy image of a cell nucleus. The challenge of this data set is that only about 0.63% of
the image voxels are non-zero. From the above discussion it can be expected that this large
amount of sparseness complicates the successful application of the proposed approximation
strategies. Therefore, an additional data set is simulated for which similar diﬃculties can
be expected. To this end, the value 1 is assigned randomly to about 0.7% of the pixels of a
regular lattice with dimension nx = ny = 120. The resulting image is displayed in the right
panel of Figure 5.1.
5.1.2 Modeling
In order to recover the smooth and ﬁxed eﬀects from the generated dense data set the
following regression model is set up for the jth pixel:
yj ∼N(ηj, κ−1
y In)
with
ηj = γjz + Xβ.
As a prior for the smooth eﬀect γ = (γ1, . . . , γm)′, with m = nx · ny, the two-dimensional
extension of the RW1 prior as discussed in Section 2.2.1 is used, that is, the structure
matrix of the GMRF prior is given by the following Kronecker-sum penalty:
Kxy = Ky ⊗Inx + Iny ⊗Kx.
(5.1)
Due to identiﬁcational reasons,
sum-to-zero constraints are applied to this eﬀect.
Considering the ﬁxed eﬀects in β a zero-mean gaussian prior with precision matrix 1e−6I2
is assigned to this parameter.
84

5.1 Performance of approximations
For the sparse data situation a spatial Poisson model is ﬁt to the data.
Here, it is
assumed that the realization of the response value for the jth pixel can be described by a
Poisson distribution with pixel-speciﬁc expectation λj:
yj ∼Po(λj)
with
λj = exp(β + γj).
Priors for the ﬁxed and smooth eﬀects are the same as for the dense data situation above.
As outlined in the beginning of this section the condition of f
Q depends on two sources:
the amount of information in the data and the magnitude of the precision parameter. In
this simulation study, the ﬁrst source is controlled by considering a dense and a sparse
data set. In order to account for the second source the above models are estimated with
diﬀerent but ﬁxed choices for κ. In particular, the values exp(0), exp(2), exp(4), and exp(8)
are used.
Approximate sampling is performed as follows: The preconditioned conjugate gradient
and the preconditioned Lanczos algorithm are used to obtain the mean and a sample from
the zero mean Gaussian, respectively.
Sum-to-zero constraints are applied by solving
f
QV
= A′ using preconditioned conjugate gradient and subtracting (3.12) from the
proposal. For all Krylov subspace methods the convergence tolerance is set to 1e−4. As a
preconditioner the incomplete Cholesky factorization with a drop tolerance (ICT) is used.
In order to analyze the eﬀect of this drop tolerance on the approximated random sample
diﬀerent values are used for this threshold.
With respect to the evaluation of log-determinants the following two strategies are
pursued: First, the stochastic Chebyshev estimator by Han et al. (2015) as discussed in
Section 4.2.4 is applied. For implementation details see Section A.3. Here, the performance
of diﬀerent degrees of the Chebyshev polynomials as well as diﬀerent sample sizes of the
stochastic trace estimator are analyzed. Second, the Cholesky factor of f
Q is approximated
by the ICT factorization and the log-determinant is subsequently calculated by formula
(4.6).
85

5 Simulation studies
5.1.3 Validation
The moderate size of γ in both data sets allows the calculation of the full Cholesky
decomposition. Thus, results from approximate sampling and approximate solutions for
log-determinants can be compared to their “true” counterparts, i.e. the solutions obtained
by the full Cholesky decomposition. To this end the above models are estimated by the
MCMC sampling scheme presented in Section 3.1 with κ ﬁxed to the values given above.
The true and approximate solutions are saved for 1,000 iterations. In each iteration, the
same random vector z ∼N(0, I) is used for direct and approximate sampling. This allows
a direct evaluation of the approximation error with respect to sampling using the relative
error
δ2(γp, ˜γp) =||γp −˜γp||2
||γp||2
.
(5.2)
In this notation, γp represents the true proposal and ˜γp the corresponding approximation.
The values of the log-determinants are assessed in a similar way: In each iteration the
log-determinant is computed by the use of the full Cholesky factorization, log det(f
Q). This
is compared to the approximations obtained by the Chebyshev estimator and the ICT
preconditioners, ^
log det(f
Q), by the relative error
δ1(log det(f
Q), ^
log det(f
Q)) =log det(f
Q) −^
log det(f
Q)
log det(f
Q)
.
(5.3)
The values of (5.2) and (5.3) for all 1,000 iterations are compared visually along all
conditions by displaying the corresponding means and ranges.
5.1.4 Results
Approximate sampling
For the dense data set the relative error between the true and approximate samples remains
below 2.5% in all cases, see Figure 5.2. Except for log(κ) = 0, the largest errors are observed
in situations where the preconditioner has been provided by the IC factorization without
additional ﬁll-in. For log(κ) = 0 the ICT(1e−2) preconditioner shows the lowest ﬁll-in ratio
which results in the highest error for this choice of κ. For a given drop tolerance of the
ICT preconditioner the error increases noticeably with increasing κ. By decreasing the
86

5.1 Performance of approximations
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
log(κ) = 0
log(κ) = 2
log(κ) = 4
log(κ) = 8
0.000
0.005
0.010
0.015
0.020
0.000
0.005
0.010
0.015
0.020
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
ICT drop tolerance
Relative error
Figure 5.2
Mean and range of (5.2) for the dense data set.
drop tolerance, or, equivalently, by increasing the ﬁll-in ratio, higher errors which result
from higher κ values can be addressed.
However, this is only feasible up to a certain
drop tolerance as the ﬁll-in ratio of the preconditioners increases dramatically with κ. For
example, for the ICT(1e−8) factorization a ﬁll-in ratio of R = 1.24 is observed for log(κ) = 0,
whereas log(κ) = 8 yields R = 24.3.
As expected, more eﬀort is needed in order to obtain comparable error sizes for the sparse
data situation, see Figure 5.3. Here, the relative error reaches up to 150%. Similar to the
dense data situation the error increases with increasing κ. Decreasing the drop tolerance
leads to an improvement. However, ﬁll-in ratios tend to get high rather fast when decreasing
the drop tolerance so that clearance for this parameter is limited. In contrast to the dense
data situation the ﬁll-in ratio remains nearly constant if κ changes and the drop tolerance
is ﬁxed. This indicates that, in this situation, the small amount of information in the data
has more inﬂuence on the condition of f
Q than the precision of the GMRF prior.
Note that in all cases the iterative methods indicates successful convergence, that is, the
relative error between the approximate samples of the last two iterations dropped below
1e−4. For the dense data situation and moderate values of κ the mean number of iterations
required for convergence lies below ﬁve for all three Krylov subspace methods. Only when
87

5 Simulation studies
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
log(κ) = 0
log(κ) = 2
log(κ) = 4
log(κ) = 8
0.0
0.5
1.0
0.0
0.5
1.0
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
ICT drop tolerance
Relative error
Figure 5.3
Mean and range of (5.2) for the sparse data set.
considering large precision values in combination with relatively high drop tolerances this
number increases up to 30 iterations. With respect to the sparse data situation the number
of iterations reﬂects well on the poor condition of the systems to solve. The values are
considerably higher than for the dense data situation, especially for high drop tolerances:
the number of iterations ranges from 10 to 120.
In both data situations, the Lanczos
algorithm for approximate sampling needs more iterations than the conjugate gradient
method for computing the mean and applying linear constraints.
Approximation of log-determinants
Compared to the dense data situation the relative errors of the Chebyshev estimator for
log-determinants are increased by a factor of 10 for the sparse data set, see Figure 5.4 and
5.5. In addition, the inﬂuence of κ seems to be reversed between both data situations: For
increasing κ the approximation worsens for the dense data while it seems to improve for
the sparse data set. However, a closer examination reveals that for the latter data set the
absolute error remains nearly constant when κ varies.
88

5.1 Performance of approximations
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G
G G G
G
G G G
G
G G G
G
G G G
G
G G G
log(κ) = 0
log(κ) = 2
log(κ) = 4
log(κ) = 8
−0.004
−0.002
0.000
0.002
−0.004
−0.002
0.000
0.002
1
5
10
25
50
1
5
10
25
50
Sample size
Relative error
Degree
5
10
15
20
Figure 5.4
Mean and range of (5.3) for the stochastic Chebyshev estimator applied to the
dense data set.
This observation is in accordance with what has been observed for approximate sampling:
The lack of information in the data seems to outweigh the eﬀect of κ. The results of the
Chebyshev estimator further depend on the number of samples for the stochastic trace
estimator and the degree of Chebyshev polynomials.
The inﬂuence of the former is as
expected: With increasing sample size the estimator becomes more precise. It is interesting
to note that even a sample size of one leads to relatively small errors. The inﬂuence of the
degree of the Chebyshev polynomials, on the other hand, is limited: The precision remains
nearly constant when increasing this parameter. However, a positive bias for the choice of
ﬁve degrees can be observed for both data situations.
Compared to the Chebyshev estimator the errors obtained from the approximation using
ICT preconditioners are about twice as large in magnitude, see Figure 5.6 and 5.7. Apart
from that the general behavior is similar: For the sparse data set the errors are increased by
a factor of 10 and the inﬂuence of κ is again reversed between both data sets. In addition,
89

5 Simulation studies
G
G G G
G
G G G
G
G G G
G
G G G
G
G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
G G G G
log(κ) = 0
log(κ) = 2
log(κ) = 4
log(κ) = 8
−0.02
0.00
0.02
−0.02
0.00
0.02
1
5
10
25
50
1
5
10
25
50
Sample size
Relative error
Degree
5
10
15
20
Figure 5.5
Mean and range of (5.3) for the stochastic Chebyshev estimator applied to the
sparse data set.
decreasing the drop tolerance, i.e. increasing the ﬁll-in ratio of preconditioners, reduces
the approximation error. Furthermore, it is interesting to note that this approximation
method strictly underestimates the log-determinant, i.e.
the approximation is always
smaller than the log-determinant obtained from the full Cholesky factor. This can be seen
as an advantage over the Chebyshev approximation because the true diﬀerence between the
log-determinants in the computation of the acceptance probability is most likely kept in the
presence of a systematic bias. Random variation as induced by the Chebyshev estimator, on
the other hand, represent an additional source of uncertainty which needs to be accounted
by the overall MCMC error.
5.1.5 Conclusion
The above simulation results can be summarized as follows: The amount of information
in the data seems to have the strongest inﬂuence on the condition of f
Q and, thus, on
90

5.2 Assessing the impact on the ﬁnal results
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
log(κ) = 0
log(κ) = 2
log(κ) = 4
log(κ) = 8
−0.004
−0.002
0.000
−0.004
−0.002
0.000
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
ICT drop tolerance
Relative error
Figure 5.6
Mean and range of (5.3) for the ICT log-determinant approximation applied
to the dense data set.
the performance of the Krylov subspace methods for approximate sampling as well as on
the approximation of the log-determinant. The magnitude of κ also appears to have an
eﬀect, especially for the dense data setup. In addition, decreasing the drop tolerance of the
incomplete Cholesky factorization leeds to more precise approximations at the cost of an
increased ﬁll-in ratio.
5.2 Assessing the impact on the ﬁnal results
In consideration of the above results the goal of this section is to analyze to what extent
the MCMC algorithm is able to account for the approximation errors, i.e. if it is possible to
obtain “exact” results although critical parts of the algorithm rely on approximations. To
this end, the simulation setups are expanded and results are compared to a gold standard.
91

5 Simulation studies
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
log(κ) = 0
log(κ) = 2
log(κ) = 4
log(κ) = 8
−0.04
−0.02
0.00
−0.04
−0.02
0.00
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
1e−8 1e−7 1e−6 1e−5 1e−4 1e−3 1e−2
0
ICT drop tolerance
Relative error
Figure 5.7
Mean and range of (5.3) for the ICT log-determinant approximation applied
to the sparse data set.
5.2.1 Simulation setup
Dense data situation
The setup of the pixel-wise data in Section 5.1.1 is expanded by two additional smooth
eﬀects. Thus, the linear predictor for the jth pixel becomes
ηj = f1(j)z1 + f2(j)z2 + f3(j)z3 + Xβ.
The components z1, f1, X, and β are constructed in the same way as in Section 5.1.1. The
new smooth functions are created by the following formulas:
f2(j) = jx −nx
2 + ny sin( jy
ny )
f3(j) =
q
(jx −nx
2 )2 + (jy −ny
2 )2.
92

5.2 Assessing the impact on the ﬁnal results
f1
f2
f3
−0.4
−0.2
0.0
0.2
0.4
Figure 5.8
Smooth eﬀects used for the simulation.
Again, the values of these eﬀects are linearly scaled to the interval [−0.5, 0.5]. Figure 5.8
displays the resulting true eﬀect images. The metric covariates z2 and z3 are randomly
assigned over the interval [−1, 1].
Sparse data situation
The sparse data set is exactly the same as in Section 5.1.1.
5.2.2 Modeling
Due to the additional smooth eﬀects the pixel-wise regression model for the dense data set
is expanded by
ηj = γj1z1 + γj2z2 + γj3z3 + Xβ.
For all random eﬀects γk = (γ1k, . . . , γmk)′, k = {1, 2, 3} , spatial GMRF priors with
Kronecker-sum penalties according to (5.1) are imposed.
Priors for ﬁxed eﬀects are
unchanged. The model for the sparse data set is unchanged.
In the ﬁrst section of this chapter it has been shown that the error of the approximate
samples expanded with increasing κ. Since the precision parameter is a random parameter
it may seem that no control over this issue can be gained.
However, by adjusting the
prior distribution of κ it may be possible to control the domain of the resulting posterior,
especially in situations where the likelihood does not dominate the prior information. Using
93

5 Simulation studies
independent Gamma priors for κ this can be achieved by choosing appropriate values for the
hyperparameters a and b. Therefore, for the sparse data situation two diﬀerent combinations
for the shape and rate parameters of the Gamma prior are chosen: First, the combination of
a = 10 and b = 0.001 is used which yields relative high values of κ. The second choice, a = 5
and b = 1, results in a prior for which more probability mass is concentrated around smaller
values of κ. For the dense data situation only one choice is made since the inﬂuence of these
parameters on the ﬁnal result is rather limited due to the large amount of information in
each pixel. Here, the values a = 1 and b = 1e−5 are chosen for all precision parameters and
the additional dispersion parameter κy.
For approximate sampling IC, ICT(1e−4), and ICT(1e−8) factorizations are used as
preconditioners. For these choices, three diﬀerent strategies with respect to the calculation
of log-determinants are chosen: The modiﬁed sampling scheme as explained in Section 4.2.4
is applied in order to avoid the computation of log-determinants completely. In addition,
block updating of (γ, κ) is performed by using the Chebyshev estimator using p = 10
degrees and 500 samples. The latter number has been chosen as a compromise between
precision on the one hand and computation time on the other hand. Furthermore, block
updating is performed by calculating the log-determinant from an approximate Cholesky
decomposition obtained by the ICT factorization. For the block updating strategies the
tuning parameter f (see Section 3.1.4) is chosen so that the acceptance rates lie between
0.2 and 0.4.
First and second moments of marginal posteriors for all high-dimensional regression
coeﬃcients are computed on-line as explained in Section 4.2.5. For precision parameters
and ﬁxed eﬀects all samples are saved. For each model of the dense data set four independent
chains of length 16,000 are generated. For the sparse data set, run length is increased to
26,000.
On-line calculation of posterior moments for the smooth eﬀects starts after an
initial burn-in period of 1,000 samples.
5.2.3 Validation
The results of all ﬁtted models are compared to the results obtained by the INLA approach
as implemented in the R (R Core Team, 2016) package INLA (Rue et al., 2009). The
INLA method is used for the following reasons: First, it gives suﬃcient precise estimates
that are comparable with those from long MCMC runs. Second, convergence problems are
avoided that may arise using MCMC. Due to computational restrictions (working station
94

5.2 Assessing the impact on the ﬁnal results
B, see Section A.1) it was not able to obtain INLA’s results for the full Laplace strategy.
Instead, the simpliﬁed Laplace strategy has been chosen In contrast, all other calculations
have been performed on working station A.
The symmetric Kullback–Leibler distance (SKLD) is used to measure the discrepancy
between the marginal posteriors obtained by the INLA approach and the corresponding
Gaussian approximations resulting from the MCMC algorithm. The SKLD is computed
by (DKL(P1||P2) + DKL(P2||P1))/2, where P1 and P2 are two posterior marginals and
DKL(P1||P2) is the Kullback–Leibler divergence given by
DKL =
∞
Z
−∞
p1(x) log p1(x)
p2(x)dx.
In order to reveal the locations with the highest discrepancies standardized coeﬃcient maps
for all smooth eﬀects are calculated and compared. To be more precise, posterior means
are divided by their corresponding standard deviations for both, MCMC and INLA. The
diﬀerences of these maps are than analyzed graphically.
5.2.4 Results
Dense data situation
According to the estimated potential scale reduction factors it can be assumed that all chains
of the nine models converged to their stationary distributions (potential scale reduction
factor < 1.1 for all parameters).
However, comparing these values along the diﬀerent
approximation strategies indicates better mixing and convergence behavior for the modiﬁed
sampling scheme. This is conﬁrmed by the sampling paths of log(κ1) displayed in Figure
5.9. Here, the ﬁrst 15,000 iterations are displayed along with the upper and lower limits
of the corresponding 95% credible interval obtained by INLA (red dotted lines). As can
be seen, mixing is best for the modiﬁed sampling scheme, although the other strategies
still yield acceptable results. The largest diﬀerences between the approximation strategies
can be observed with respect to the agreement with the marginal posterior estimated by
the INLA approach. For example, the 95% credible interval obtained by the Chebyshev
strategy using 500 samples and Chebyshev polynomials of degree 10 is about 30 to 40%
wider than INLA’s result.
In contrast, the ICT approximation of the log-determinant
yields intervals that are about 4 to 12% wider, and for the modiﬁed sampling scheme this
95

5 Simulation studies
Figure 5.9
Sampling paths for log(κ1).
discrepancy is less than 1%.
This indicates that the MCMC algorithm has diﬃculties
in accounting for the error that is induced by the stochastic trace estimator (4.10) used
within the Chebyshev approximation. Of course, this error can be reduced if the sample
size for the Hutchinson estimator is increased. However, this would increase computational
requirements dramatically which greatly limits its practical use, especially when simpler
strategies achieve better results. Also of interest is the fact that a decrease of the drop
tolerance, i.e. an increase of the ﬁll-in ratio of the ICT factorization does not yield any
noticeable diﬀerence in the quality of the sampled paths. This is a surprising result which
is in contrast to the results obtained in the ﬁrst section of this chapter where decreasing
the drop tolerance resulted in a noticeable improvement of both the sampling error and
the error of the approximations of the log-determinant. Similar results are obtained for the
precision parameters of the other smooth eﬀects.
96

5.2 Assessing the impact on the ﬁnal results
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
f1
f2
f3
0.0000
0.0005
0.0010
0.0015
0.0000
0.0005
0.0010
0.0015
0.0000
0.0005
0.0010
0.0015
IC,
modif.
ICT(1e−4),
modif.
ICT(1e−8),
modif.
IC, IC
ICT(1e−4),
ICT(1e−4)
ICT(1e−8),
ICT(1e−8)
IC,
Ch(500,10)
ICT(1e−4),
Ch(500,10)
ICT(1e−8),
Ch(500,10)
Symmetric Kullback−Leibler distance
Figure 5.10
SKLD for all three smooth eﬀects.
The values of the SKLD for all three smooth eﬀects are displayed in Figure 5.10.
Here, mean values as well as maximum and minimum values are plotted for the diﬀerent
approximation strategies.
Overall, agreement between the MCMC results and INLA’s
marginal posteriors is very good, the maximum SKLD is 0.00188. However, the modiﬁed
sampling scheme can clearly be seen as the best strategy (maximum SKLD: 0.00035) which
is in accordance with the above results. Note that the missing eﬀect of the drop tolerance
of the ICT factorization can also be conﬁrmed: An improvement of the agreement between
the MCMC and INLA results cannot be observed when increasing the ﬁll-in ratio of the
preconditioners.
Figure 5.11 shows the diﬀerences of the standardized coeﬃcient maps for all three smooth
eﬀects along the approximation strategies. Overall, the modiﬁed sampling strategy seems
to perform best, followed by the ICT block updating strategy. The largest discrepancies
97

5 Simulation studies
IC
ICT(1e−4)
ICT(1e−8)
modified
ICT
Chebyshev
−0.6
−0.3
0.0
0.3
0.6
Figure 5.11
Diﬀerences of the standardized coeﬃcient maps for all three smooth eﬀects.
can be observed in areas where the true eﬀect maps take their highest values in magnitude.
In addition, the eﬀect of decreasing the drop tolerance of the ICT factorization on the
diﬀerences of the standardized coeﬃcient maps seems limited.
Sparse data situation
First, the results for the case a = 5 and b = 1 are considered. Here, all generated MCMC
chains converged to their stationary distribution except for the chains regarding κ. The
potential scale reduction factor for this parameter varies between 1.11 and 1.37, whereas
98

5.2 Assessing the impact on the ﬁnal results
IC
ICT(1e−4)
ICT(1e−8)
0
1
2
0
1
2
0
1
2
modified
ICT
Chebyshev
5000
15000
25000
5000
15000
25000
5000
15000
25000
Iteration
log(κ)
Figure 5.12
Sampling paths for log(κ) for a = 5 and b = 1.
smaller values have been obtained for the modiﬁed sampling scheme and larger values
for the block updating strategy using the stochastic Chebyshev approximation. Sampling
paths of these chains are displayed in Figure 5.12. Again, the 95% credible intervals for this
parameter obtained by the INLA approach are indicated by the red dotted lines. As can be
seen the posterior distribution is explored rather slowly: Dependency between successive
samples is quite high which results in insuﬃcient mixing. For the block updating strategy
using the ICT approximation it can also be seen that not the entire part of the domain
of the marginal posterior is reached.
Instead, only the inner part is explored, i.e.
the
variance of the marginal posterior is underestimated. For the other two strategies it seems
that suﬃcient exploration of the marginal posterior could be achieved by increasing the
sampling size of the MCMC algorithm.
99

5 Simulation studies
IC
ICT(1e−4)
ICT(1e−8)
4
6
8
10
4
6
8
10
4
6
8
10
modified
ICT
Chebyshev
5000
15000
25000
5000
15000
25000
5000
15000
25000
Iteration
log(κ)
Figure 5.13
Sampling paths for log(κ) for a = 10 and b = 0.001.
Convergence diagnostics for the second case, i.e. a = 10 and b = 0.001, are diﬀerent:
For the modiﬁed sampling scheme all parameters converged to their stationary distribution
(all potential scale reduction factors < 1.1). In contrast, the chains of κ for the other two
strategies did not converge. Here, the potential scale reduction factor for κ varies between
1.81 and 2.43. In fact, inspecting the sampling paths for this parameter visually (Figure
5.13) reveals that joint updating (γ, κ) using the ICT and Chebyshev strategies causes the
algorithm to diverge. Interestingly though, in the subspace of the remaining parameters the
Markov chain converged independently of κ. This demonstrates that the eﬀect of the small
amount of information in the data on the performance of the MCMC algorithm is more
pronounced than the actual value of κ – a results which is in accordance with the ﬁndings
in Section 5.1. In contrast, the modiﬁed sampling scheme shows an acceptable behavior,
although exploration of the stationary distribution is rather slow. Note that, just as in the
100

5.2 Assessing the impact on the ﬁnal results
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
a = 5, b = 1
a = 10, b = 0.001
0.000
0.025
0.050
0.075
0.0
0.5
1.0
1.5
IC,
modif.
ICT(1e−4),
modif.
ICT(1e−8),
modif.
IC, IC
ICT(1e−4),
ICT(1e−4)
ICT(1e−8),
ICT(1e−8)
IC,
Ch(500,10)
ICT(1e−4),
Ch(500,10)
ICT(1e−8),
Ch(500,10)
Symmetric Kullback−Leibler distance
Figure 5.14
SKLD for the smooth eﬀect.
other cases above, an increase of the ﬁll-in ratio of the ICT preconditioner does not seem
to yield diﬀerent results.
Figure 5.14 displays the values of the SKLD for both cases. For a = 5 and b = 1 the
SKLDs are in an acceptable range. Block updating using the ICT factorization leads to
the smallest errors, followed by the Chebyshev approximation strategy and the modiﬁed
sampling scheme. Noticeable is the fact that decreasing the drop tolerance of the ICT
factorization leads to an increase of the SKLDs. This is counterintuitive to the results
obtained in Section 5.1. For the second scenario (a = 10, b = 0.001) much higher values
for the SKLD can be observed.
Here, the modiﬁed sampling scheme is as good as in
the ﬁrst case (maximum SKLD: 0.08). However, for the other approximation strategies
unacceptable high values for the SKLD are observed. This is not surprising given the fact
that the corresponding κ-chains did diverge.
Figure 5.15 reveals that, for a = 5 and b = 1, the modiﬁed sampling scheme performs
best with respect to the magnitude of the diﬀerences of standardized coeﬃcient maps.
In particular, the simplest strategy (IC and modiﬁed sampling scheme) yields the least
101

5 Simulation studies
IC
ICT(1e−4)
ICT(1e−8)
modified
ICT
Chebyshev
−0.10 −0.05 0.00 0.05 0.10
Figure 5.15
Diﬀerences of the standardized coeﬃcient maps for all three smooth eﬀects
for a = 5 and b = 1.
pronounced error map. In contrast, standardized coeﬃcients for both blocking strategies
show serious deviations from the standardized coeﬃcients obtained by the INLA approach.
In addition, it can also be seen that the error increases with decreasing drop tolerance of
the ICT factorization.
The results for a = 10 and b = 0.001 are similar (see Figure 5.16): Here, the best
match with INLA’s standardized regression coeﬃcients is obtained by the modiﬁed sampling
scheme.
The worst result is observed for the Chebyshev strategy and the ICT(1e−8)
102

5.3 Chapter summary
IC
ICT(1e−4)
ICT(1e−8)
modified
ICT
Chebyshev
−0.10 −0.05 0.00
0.05
0.10
Figure 5.16
Diﬀerences of the standardized coeﬃcient maps for all three smooth eﬀects
for a = 10 and b = 0.001.
preconditioner.
Again, increasing the ﬁll-in ratio of the preconditioners leads to larger
errors.
5.3 Chapter summary
The simulations conducted in this chapter allow the following conclusions: First, the impact
of the approximation strategies on the ﬁnal results depends on the data situation. If the
103

5 Simulation studies
information content in the data is dense, good to excellent performance can be achieved.
However, if the data set contains only little information, i.e. is sparse, it becomes more
diﬃcult for the MCMC sampler to account for the approximation errors.
Second, with respect to the diﬀerent approximation strategies the following statements
can be made: The MCMC error may not be able to account for the inaccuracy which
is induced by the stochastic Chebyshev approximation of the log-determinant using 500
samples. For the sparse data set it has been observed that the MCMC algorithm diverged
and even for the dense data set the marginal posteriors were signiﬁcantly wider. Increasing
the sample size of the Hutchington estimator may provide a remedy, however, this would
lead to a dramatic increase of computation time. Results for the approximation strategy
based on the ICT factorization are similar. For the dense data set, however, this method
yields acceptable results. By far the best results were obtained from the modiﬁed sampling
scheme. This strategy yielded stable Markov chains as well as a good agreement with INLA’s
results in both data situations, although longer MCMC runs are required for the sparse
data set. Another point in favor for this relatively simple strategy is less computation time
compared to the block update strategies. As a more surprising result an increase of the ﬁll-in
ratio of the incomplete Cholesky preconditioner does not result in better performance.
Third, it could be observed that assessing the performance of approximations and the
quality of ﬁnal results is crucial for the application to real world data sets. However, this
could be a diﬃcult task as there is no ground truth available in this case.
Therefore,
visual inspection of sample paths and calculation of convergence diagnostics must be seen
as absolutely necessary before results can be interpreted. The number of iterations that are
required by the Krylov subspace methods to converge may be of additional help in order
to indicate potential diﬃculties due to ill-conditioned linear systems.
Finally and most importantly, it has been shown that, under certain conditions,
approximation strategies exist that are able to provide excellent results to large-scale
problems.
In the following chapter these strategies are applied to three real world
applications.
104

6 Applications
The results obtained in the simulation studies of Chapter 5 can be used to formulate a
MCMC algorithm which is able to ﬁt regression models with high-dimensional components
as they appear in real-world problems.
In particular, the modiﬁed sampling scheme is
selected and for approximate sampling preconditioning using the IC factorization without
additional ﬁll-in is used. In this chapter, this algorithm is used to analyze three diﬀerent
applications in the ﬁeld of bio-medical imaging.
First, a supervised algorithm for the
segmentation of MS lesions is trained. Second, a voxel-wise regression model is ﬁtted to
segmented GM images in order to reveal diﬀerences in age-related atrophy of GM in MS
patients. Third, object-based co-localization is performed by ﬁtting a spatial point process
to the data of ﬂuorescence microscopy images.
The ﬁrst application in Schmidt et al. (2017) uses data from the same cohort as in Section
6.1. Thus, the data description given in Section 6.1.1 is partly identical to the description
given in Schmidt et al. Furthermore, the second application of this chapter and Section 5.2
in Schmidt et al. (2017) both perform voxel-based morphometry. Thus, the introduction
given in Section 6.2.1 is in large parts identical to the corresponding application in Schmidt
et al.
6.1 Supervised segmentation of MS lesions
In this chapter a procedure for the development of a supervised MS lesion segmentation
algorithm is presented. An introduction to the subject at hand will be given as well as a
description of the data set. Subsequently, details about the modeling process as well as
results are presented.
105

6 Applications
Figure 6.1
Selected axial slices of a T1-weighted and a FLAIR image for a randomly
chosen MS patient.
6.1.1 Introduction
MS is a chronic inﬂammatory disease of the central nervous system. Although it is the main
reason for early disabilities among young adults (Koch-Henriksen and Sørensen, 2010),
its etiology is still unknown.
According to the WHO’s 2008 MS Atlas (World Health
Organization, 2008) the average age of onset is 29.2 years (interquartile range, 25.3 –
31.8). Furthermore, females are about twice as much at risk as men, and strong regional
diﬀerences can be observed. The clinical picture of MS is rather heterogeneous. While most
patients experience their disease in relapses (relapsing-remitting MS), a minority suﬀer from
continuous deterioration of symptoms (primary progressive MS). However, all types of MS
have in common that inﬂammational plaques or lesions may appear in any location of the
central nervous system. As a consequence, clinical symptoms can include a large variety of
physical and mental problems that can also be associated with other diseases (Compston
and Coles, 2008). Thus, a diagnosis only on clinical symptoms is diﬃcult (Crayton et al.,
2004).
Instead, it is recommended to include radiological ﬁndings in order to increase
diagnostics sensitivity and speciﬁcity (McDonald et al., 2001; Polman et al., 2011). This
includes the identiﬁcation and location of lesions in three-dimensional T2-weighted MR
sequences (see the bright regions in the FLAIR image in Figure 6.1). Therefore, accurate
segmentation of lesions is an elementary component of clinical research with respect to
MS.
106

6.1 Supervised segmentation of MS lesions
Lesion segmentation algorithms
Manual segmentation of MS lesions in high-dimensional MR images is a diﬃcult task.
It strongly depends on the experience of the operator and is, in general, not a reliable
procedure for lesion segmentation.
Therefore, a variety of tools for the automatic
segmentation exists, which includes fully and semi-automatic as well as supervised and
unsupervised methods (Mortazavi et al., 2012). The majority of unsupervised methods
rely on ﬁnite mixture models in combination with some kind of expectation maximization
algorithm (van Leemput et al., 2001; Aït-Ali et al., 2005; Khayati et al., 2008; Freifeld et al.,
2009).
These approaches are popular because they allow including spatial information
by the use of Markov random ﬁelds.
Among supervised methods k-nearest-neighbors
classiﬁcation is a popular choice (Anbeek et al., 2004; Wu et al., 2006).
Furthermore,
classiﬁcation algorithms based on mixture models have been presented (Herskovits et al.,
2008) and, more recently, classiﬁers based on decision forests have been applied successfully
(Akselrod-Ballin et al., 2009; Geremia et al., 2011). However, one of the most obvious
supervised procedures has not been presented yet: A classiﬁer based on voxel-wise binary
regression models. There is one main diﬃculty connected with this approach: A binary
model needs to be estimated for each brain voxel. This also includes voxels for which no
lesions have been observed due to the limited sample size of the training set. For these
voxels, no valid inference based on binary regression models can be performed. A solution
to this problem can be accomplished by considering the spatial information among voxels
and ﬁtting the regression models jointly, for example using the framework of STAR models
with spatially structured regression coeﬃcients as presented in Section 2.2.1. However, this
leads to a new problem: Considering all voxels jointly would produce a model that will
probably be too large in order to be accessible with limited computational resources. Here,
the methods presented in the previous chapters are used to overcome this problem.
Data
The data set consists of MR images of 53 MS patients that are part of a larger cohort
(312 MS patients) which has been collected at the Department of Neurology, Technische
Universität München, Munich, Germany. The selected images correspond to patients with
high total lesion volume (TLV). To be more speciﬁc, only patients with TLV > 10 ml are
used within the training process in order to keep the number of total data points within an
acceptable range. Note that this reduction in sample size does not aﬀect the information
107

6 Applications
All subjects
TLV > 10
Figure 6.2
Maximum intensity projections for the position of MS lesions for all 312 subjects
(left) and the selected 53 subjects (right).
of lesion distribution over the brain: Figure 6.2 shows that the position of lesion voxels
of the full cohort (left panel) can be well approximated by the lesion structure of the 53
selected MS patients (right panel). This ﬁgure shows the maximum intensity projections
(Wallis et al., 1989) of MS lesions in MNI space, that is, lesion voxels are averaged over
saggital, cranial, and axial dimension of normalized binary lesion maps estimated by the
lesion growth algorithm implemented in the LST toolbox (Schmidt et al., 2012) for the
SPM1 package.
The actual training data consists of three-dimensional gradient echo T1-weighted and
T2-weighted ﬂuid-attenuated inverse recovery (FLAIR) images. Both types of images were
acquired on the same 3 Tesla scanner (Achieva, Philips, Netherlands). For the T1-weighted
image, 170 contiguous sagittal 1 mm slices with a ﬁeld of view of 240 × 240 mm were
recorded. Voxel size is 1.0 × 1.0 × 1.0 mm, repetition time (TR) was set to 9 ms and echo
time (TE) to 4 ms. For the FLAIR image, 144 contiguous axial 1.5 mm slices with a ﬁeld of
view of 230 × 185 mm have been obtained. The voxel size for these images is 1.0×1.0×1.5
mm; TR was set to 104 ms, TE to 140 ms, and inversion time to 2,750 ms.
In addition to the training data a test data set from the same cohort is chosen to illustrate
the performance of the ﬁnal segmentation algorithm. MR parameters of the training and
test data sets are identical.
1http://www.ﬁl.ion.ucl.ac.uk/spm/
108

6.1 Supervised segmentation of MS lesions
6.1.2 Modeling
In this section a binary regression model with spatially varying intercept is constructed to
segment lesions which appear hyperintense in FLAIR images.
Reference lesion maps
As the lesion segmentation procedure presented here is a supervised classiﬁcation algorithm,
reference lesion segmentations are required for the training data set. To this end, lesion
location was estimated by the lesion growth algorithm (LGA), an unsupervised MS lesion
segmentation algorithm implemented in the LST toolbox for SPM. For all subjects the
default settings have been used. Since the LGA requires to coregister the FLAIR image to
the T1-weighted image prior to lesion segmentation, the resulting probability lesion maps
are in the space of the T1-weighted image. The ﬁnal binary reference lesion maps have
been obtained by thresholding the probability lesion maps at 0.5.
Preprocessing and feature extraction
Two features are extracted from the available MR images, beginning with the position of
each brain voxel in a standard (MNI, Evans et al., 1993) space. To this end, FLAIR images
which have been coregistered to the T1-weighted images are normalized to MNI space using
the “Normalize” function implemented in SPM. This routine creates an inverse deformation
ﬁeld which can be used to map MNI coordinates into the subject speciﬁc native space. Note
that by using this procedure multiple voxels of the original MR image can have the same
MNI coordinate. This is due to the fact that the MNI space is usually of lower dimension
than the individual native space of the actual MR images. This way, a total number of
565,475 MNI coordinates are distributed over the individual MR images.
The second feature is the so-called lesion belief map. This image is produced by the
following steps: First, the FLAIR image is roughly segmented into the three main tissue
classes GM, WM, and CSF. Subsequently, FLAIR intensities are standardized by dividing
each voxel by the mean of segmented GM. In addition, the mean of standardized GM voxels
is subtracted from all FLAIR intensities. Only positive diﬀerences are kept, negative values
are set to zero. Furthermore, the remaining diﬀerences are multiplied by a tissue probability
map for WM which is obtained by applying the inverse deformation ﬁeld from above to the
tissue probability maps included in SPM. The resulting lesion belief map shows voxels that
109

6 Applications
appear hyperintense in the FLAIR image and which are likely to be part of WM in healthy
subjects, thus, possible lesion candidates.
Training
In the training phase, the above features are eﬃciently combined using a voxel-wise logistic
regression model which includes a spatially varying intercept.
To be more precise, the
following model is used:
yij ∼B(πij) with πij =
exp(ηij)
1 + exp(ηij).
Here, yij is the value of the reference lesion map for the jth voxel and ith subject. The
linear predictor is chosen to be
ηij = β0 + β1xij + γj
where β0 is the overall intercept, xij the value of the lesion belief map for the jth voxel
and ith subject, and β1 the corresponding eﬀect of the lesion belief map. In addition, the
vector γ = (γ1, . . . , γm)′ is a spatially varying intercept. For the vector of ﬁxed eﬀects,
β = (β0, β1)′, a zero-mean Gaussian prior with precision matrix 1e−6I2, is used.
The
spatial eﬀect, γ, is modeled by the three-dimensional extension of the RW1 prior based on
Kronecker-sum-penalties:
K = Knz ⊕(Kny ⊕Knx).
Here, nx, ny and nz are the dimensions of the MNI template, i.e. nx = 121, ny = 145,
and nz = 121.
Note that only 565,475 voxels (about 27%) of this template are active
brain voxels and therefore included in γ. Hence, it is necessary to select only relevant
columns in K and to adjust its diagonal elements by the number of neighboring voxels
accordingly. Furthermore, a sum-to-zero constraint is imposed on γ in order to guarantee
identiﬁability of all regression coeﬃcients. For the corresponding precision parameter, a
Gamma distribution with parameters a and b is chosen. Setting these hyperparameters to
small values results in a very rough spatial eﬀect where small changes in lesion structure are
overestimated. Thus, to guarantee an acceptable amount of smoothness a more informative
110

6.1 Supervised segmentation of MS lesions
prior for this precision parameter is chosen by setting a = 100 and b = 1. This choice is
robust, i.e. smaller changes in a and b do not aﬀect the ﬁnal results.
The present data situation is comparable to the dense data situation in Chapter 5:
Multiple observations are available for each node of the GMRF which is induced by γ. Thus,
it can be expected that all investigated approximation strategies perform well. However, as
already mentioned in the introduction of this chapter the above model is ﬁtted using the
simplest strategy, that is, the modiﬁed sampling scheme and the IC factorization without
ﬁll-in are used. Except for the ﬁxed eﬀect, the precision parameter as well as ﬁve selected
components of γ, posterior moments are computed on-line after an initial burn-in period
of 500 samples. In total, four chains of length 10,500 are generated.
6.1.3 Results
Mean runtime for sampling 1,000 iterations is about two hours and requires 9 GB RAM in
total. Acceptance rates are about 0.91 for the ﬁxed and 0.86 for the spatial eﬀect.
Assessment of MCMC results
The highest potential scale reduction factor among all 565,478 parameters is 1.0023. This
indicates that all chains converged to their corresponding stationary distributions. Sampling
paths of κ, β0, and one selected element of the vector γ, i.e. γ12865, are displayed in the ﬁrst
column of Figure 6.3. By visual inspection of these time series it can be seen that mixing
is excellent without any convergence problems occurring, which implies that the chosen
approximation strategy is suﬃcient for this data situation. In addition to the displayed
trajectories, the second column of Figure 6.3 shows the corresponding estimated marginal
posteriors using histograms derived from the samples of all chains. Furthermore, individual
marginal posteriors obtained from each chain are indicated by kernel density estimators
(black lines). As can be seen, the sampling scheme is able to estimate well shaped marginal
posteriors which indicates good exploration of stationary distributions. Furthermore, in all
cases results from the single chains agree well with the ﬁnal distributions. Furthermore, for
γ12865 the marginal posterior is approximated by a Gaussian density using on-line computed
posterior moments (red line).
Agreement of this approximation with the histogram is
satisfying which indicates that keeping only these values during the sampling process is
suﬃcient for elements of the high-dimensional spatial eﬀect.
111

6 Applications
log(κ1)
log(κ1)
β0
β0
γ12865
γ12865
4.39
4.40
4.41
−6.80
−6.78
−6.76
−6.74
−6.72
0.4
0.5
0.6
0.7
0.8
4.39
4.40
4.41
−6.800
−6.775
−6.750
−6.725
0
2500
5000
7500
10000
0.4
0.5
0.6
0.7
0.8
0.9
Interation
 
Figure 6.3
MCMC sampling paths (left column) and estimated marginal posteriors (right
column) for selected parameters.
Posterior means for the spatially varying intercept are displayed in Figure 6.4. Overall,
the estimated eﬀect map has similarities to WM probability maps as they are used within
other tissue segmentation algorithms. In particular, an increased risk for MS lesions can be
observed in regions of WM around lateral ventricles including the corpus callosum. It can
also be seen that the probability of observing a lesion decreases in the complete cerebellum.
This may increase the risk of missing out infratentorial lesions in the ﬁnal segmentation
algorithm. This issue is revisited in Section 6.1.3 below. From Figure 6.4 it can also be
seen that the estimated eﬀect map is slightly non-symmetric, which is not surprising given
112

6.1 Supervised segmentation of MS lesions
Figure 6.4
Estimated posterior means of the spatially varying intercept.
the relatively small number of patients that are included in the training data set. For the
application to new data it is advantageous to symmetrize the eﬀect map. To this end, the
values of each slice are mirrored along their central lateral line and all voxel values are
replaced by their corresponding mean.
MS lesion segmentation
In order to apply the estimated model for lesion segmentation relevant features as
introduced in Section 6.1.2 need to be extracted. Note that in contrast to the training phase,
a T1-weighted image is not necessary as this image was only required in order to obtain
reference lesion maps using the LGA. Thus, the following steps are performed in order to
extract all relevant features. First, inverse deformation ﬁelds are estimated in order to map
the posterior mean image of the spatially varying intercept from MNI space into subject
speciﬁc native space. This yields ˆγ. Next, intensities of FLAIR images are corrected for
bias ﬁeld inhomogeneity using SPM’s “Segment” function. From these images, the required
113

6 Applications
Figure 6.5
FLAIR images and estimated lesion probability maps for three subjects from
the test data set.
lesion belief map is computed as explained in Section 6.1.2 which yields xj, j = 1, . . . , m.
Combining this with the posterior means of β0 and β1 yields the following linear predictor
ˆηj = ˆβ0 + ˆβ1xj + ˆγj.
Lesion segmentation is then performed by computing the lesion probability for each voxel
using ˆπj = exp(ˆηj)/(1 + exp(ˆηj)).
Usually, some post processing steps are required in order to obtain a clean lesion map.
For the present segmentation method it is suﬃcient to omit all lesions that are smaller than
0.015 ml in volume.
114

6.1 Supervised segmentation of MS lesions
The results obtained by this lesion segmentation algorithm for three subjects from the
test data set are displayed in Figure 6.5. Here, FLAIR images as well as FLAIR images
overlaid with the resulting probability lesion maps are shown. The ﬁrst patient has been
selected due to its complicated lesion pattern: Many small lesions are scattered all over
the brain which complicates the segmentation of all aﬀected tissue. As can be seen, the
algorithm is able to detect most lesions very well, even tissue which appears only slightly
hyperintense. With respect to lesion ﬁlling in T1-weighted images this can be seen as an
advantage. However, it can also be seen that some smaller lesions on the transition from
WM to cortical GM are missing. The results for the second subject demonstrate nicely
that the approach is able to segment infratentorial lesions although this area is under-
represented in the spatially varying intercept, see Figure 6.4. In addition, some light voxels
mostly around the ventricles are detected as well, however, the estimated lesion probabilities
for these voxels are rather low. Finally, the third patient has been selected because of its
large lesion load. Here, the algorithm is able to segment all lesions while producing some
false positives on the border of the cortex, a behavior which seems acceptable given the size
of these false positives.
6.1.4 Summary
A binary regression model for the classiﬁcation of voxels in lesion and non-lesion voxels has
been presented in this section. By including a spatially varying intercept the model is able
to (a) account for diﬀerences in lesion appearance across the brain and (b) provide valid
results even for voxels where no lesions have been observed due to limited sample size of
the training set. This leads to a high-dimensional regression coeﬃcient which necessitates
the approximation of certain steps of the ﬁtting process.
The selected approximation
strategies perform well and produce results which can be used to formulate a useful lesion
segmentation algorithm.
It is worth mentioning that the presented approach can easily be extended in order to
include further MR modalities, such as T2- and PD-weighted images. In this context it
may also be of interest to consider interaction eﬀects of the intensities of diﬀerent MR
images. For example, one can use the fact that lesions appear hypointense in T1-weighted
images but hyperintense in FLAIR, T2- and PD-weighted images. In contrast, CSF, for
example, appears hypointense in T1-weighted and FLAIR images and hyperintense in T2-
and PD-weighted images. This information can be accounted for by interactions of metric
covariates as explained in Section 2.2.1.
However, including additional MR modalities
115

6 Applications
would also require to carry out further preprocessing steps, namely the coregistration of all
MR sequences. Especially for patients with high lesion loads this step may be vulnerable
to errors with respect to misalignment of identical brain structures.
Current experience with the above lesion segmentation algorithm shows that it also works
well for images that are not part of the data set used here. However, if the tissue contrasts
of these images are not comparable to the images used for training, the results obtained by
this segmentation procedure may not produce reliable probability lesion maps. This should
not come as a surprise since the presented approach is a supervised method. However, as
long as training data is available the model parameters can easily be adjusted in order to
render the algorithm useful to other data sets.
6.2 Diﬀerences in age-related atrophy of gray matter in
MS patients
In this section the analysis of MR images is revisited. In particular, the hypothesis of a
more pronounced age-related atrophy of GM in MS patients is analyzed using a voxel-wise
regression model.
6.2.1 Introduction
Voxel-wise regression models are widely used for the analysis of MR images. For example,
in ﬁrst-level analyses of functional MRI (fMRI) time series analyses are performed for
each voxel and subject. In a second-level analysis results from the ﬁrst-level analysis are
compared in a voxel-wise fashion along diﬀerent subjects, groups or experimental conditions
by a general linear model (Friston et al., 1994). As a further example consider the analysis
of GM atrophy in structural MR images. Here the method of voxel-wise morphometry
(VBM, Ashburner and Friston, 2000) has been widely accepted. This is subject of the
following sections.
Voxel-based morphometry
VBM is a widely used approach for the analysis of GM atrophy along the brain. Here,
individual images of local GM volume are compared in a voxel-wise manner either along
116

6.2 Diﬀerences in age-related atrophy of gray matter in MS patients
groups by performing statistical hypothesis tests or correlated with explanatory covariates
by ﬁtting linear models.
The usual procedure consists of smoothing the data prior to
analysis, ﬁtting the desired model for each brain voxel independently, and correcting the
results for multiple comparisons in order to prevent an increase of the Type I error.
Although Bayesian versions exist which eliminate the necessity for post hoc correction
(Friston and Penny, 2003), spatial information is only considered by smoothing the images
with a predetermined smoothing parameter prior to the analysis which represents a non-
trivial modiﬁcation of the original data. Instead, it would be favorable to include spatial
information in the modeling step especially as it has been shown that this leads to an
increase of the signal to noise ratio (Penny et al., 2005) and statistical power (Schmidt
et al., 2013). In the following sections it is shown that the MCMC approach presented in
the previous chapters is able to provide such a solution by ﬁtting a model with millions of
parameters with only moderate requirements on computational equipment.
Data and preprocessing
With respect to MS, a frequently asked and not yet suﬃciently answered question is that
if and where the correlation between age and GM atrophy diﬀers between MS patients and
healthy controls (HCs). This chapter tries to answer this question by analyzing the images
of an age and gender matched cohort which contains the data of n = 247 subjects (168
MS patients and 79 HCs). The data set consists of normalized images of local GM volume
which were estimated from T1-weighted sequences using the tissue segmentation pipeline
implemented in VBM82. For MS patients, lesions were ﬁlled in T1-weighted images prior
to segmentation using the “Lesion ﬁlling” routine implemented in the LST toolbox. To this
end, lesions were identiﬁed using the method presented in the previous chapter. All images
were obtained at the Department of Neurology, Technische Universität München, Munich,
Germany, using the same imaging protocol as outlined in Section 6.1.1.
6.2.2 Modeling
Given the 247 × 1 vector of voxel speciﬁc GM volume, yj, the following model is
formulated:
yj ∼N(ηj, κ−1
j In)
2http://www.neuro.uni-jena.de/vbm/download/
117

6 Applications
with
ηj = γj1 + γj2 · age + γj3 · sex + γj4 · ms + γj5 · age · ms.
Here, age, sex and ms are 247 × 1 vectors that contain the centered age, dummy coded
information about gender (male = 0, female = 1), and state of disease (HC = 0, MS =
1), respectively. For all coeﬃcient vectors γk, k = 1, . . . , 5, the same three-dimensional
spatial prior and the same brain mask as in the previous application are used. That is, only
m = 565,475 voxels are considered and the entries of the structure matrices are adjusted
accordingly.
As no overall intercept is included in the model no linear constraints are
necessary. For the corresponding precision parameters Gamma priors with shape and rate
parameter ak and bk are used. Due to the relatively large sample size the eﬀect of the
hyperparameters on the ﬁnal results is limited. However, in order to ensure smooth eﬀect
maps which are less sensitive to noise related artifacts, informative priors for the precision
parameters are chosen by setting ak = 10,000 and bk = 10.
Note that additional complexity is induced by the fact that each voxel is assigned its
own precision parameter for the Gaussian likelihood. However, updating these parameters
is straightforward if independent Gamma priors with shape and rate parameter ay = 1 and
by = 5e−5 are assumed. Then, the full conditional for the jth voxel is again a Gamma
distribution with updated parameters ˜ay = ay + n/2 and ˜by = by + 0.5(yj −ηj)′(yj −ηj),
respectively.
As an approximation strategy the modiﬁed sampling scheme using the IC factorization
without ﬁll-in is used. Four independent chains of length 11,000 are generated and on-line
calculation of posterior moments starts after 1,000 iterations. The above model represents
a three-dimensional extension of the dense data situation which has been analyzed in the
simulations chapter, thus it can be expected that the chosen approximation strategies work
well.
6.2.3 Results
Computing 1,000 iterations requires about six hours and 7 GB RAM on working station
A.
118

6.2 Diﬀerences in age-related atrophy of gray matter in MS patients
log(κ1)
log(κ2)
log(κ3)
log(κ4)
log(κ5)
log(κ117827)
2.695
2.700
2.705
2.710
12.53
12.54
12.55
12.56
8.34
8.36
8.38
8.40
11.200
11.225
11.250
11.275
12.68
12.69
12.70
12.71
2.8
3.0
3.2
3.4
0
2500
5000
7500
10000
0
2500
5000
7500
10000
Iteration
Figure 6.6
Generated chains of the precision parameters for one run.
Assessing convergence and mixing of Markov chains
According to the potential scale reduction factor all chains converged to their stationary
distributions (maximum ˆR = 1.0072). Sampled paths for the chains of log(κk), k = 1, . . . , 5
and for one selected dispersion parameter, log(κ117827), of one run are displayed in Figure 6.6.
Visual inspection of these chains indicate good mixing. In most cases, posterior distributions
are explored rather fast. However, mixing is slower for κ3 and κ4 but the corresponding
posteriors are still suﬃciently explored, especially when considering the other chains (not
shown).
119

6 Applications
Figure 6.7
Estimated eﬀects for age, sex, state of disease, and the interaction of age and
state of disease.
Assessing the estimated eﬀects
For the ﬁnal assessment of results the posterior moments obtained from all MCMC runs are
combined. Figure 6.7 shows the estimated standardized coeﬃcients (posterior mean divided
by posterior standard deviation) for age, sex, state of disease, and the interaction of age
120

6.2 Diﬀerences in age-related atrophy of gray matter in MS patients
and state of disease. For better orientation the estimated coeﬃcient maps are overlaid on
a mean T1-weighted image and only those voxels are colorized for which at least 99.9% of
their posterior probability mass lies below or above zero. The ﬁrst row depicts the eﬀect
of age on local GM volume for HCs. It can be seen that this eﬀect is mostly negative and
aligned all along the cerebral cortex. Regions where this eﬀect is more emphasized are the
left and right insula, the left and right putamen, as well as the left and right caudate nucleus.
These results are in accordance with previous recorded eﬀects, see for example Hutton et al.
(2009). Females seem to have more GM volume at the left and right thalamus, left and
right caudate nucleus, parts of the cerebellar cortex, and some minor regions of the cerebral
cortex (second row). In addition, a smaller negative cluster, i.e. a region where females
tend to have less GM concentration, can be observed near the visual cortex. The third row
shows the results for γ4, i.e. the eﬀect of MS disease on local GM volume. It seems that
MS patients have less GM volume in the region of the left and right caudate nucleus, the
thalamus, putamen, and minor pronounced regions within the remaining cerebral cortex.
In addition, a positive cluster within the right temporal lobe is noticeable. Finally, for
the interaction eﬀect of age and state of disease mostly clusters with negative signs can be
observed, i.e. regions where GM atrophy is more pronounced for MS patients than for HCs.
In particular, this includes the visual cortex as well as the putamen.
6.2.4 Summary
In this application a voxel-wise regression model with 3,392,855 parameters in total has
been estimated successfully.
Despite this large number of parameters it was able to
perform the estimation with only moderate computational requirements. The fully Bayesian
approach to voxel-wise regression has some serious advantages compared to classical or
frequentist inference. First, due to the data driven regularization of regression coeﬃcients
the occurrence of artifacts is limited. Furthermore, it is no longer necessary to smooth
the data prior to the analysis. This is an important feature as it eliminates one researchers
degree of freedom (Simmons et al., 2011). Another advantage of the fully Bayesian approach
is that no post hoc corrections for multiple testing need to be applied. This is of particular
practical importance as it has been shown that during the last years severe errors occurred
when applying correction methods in the context of voxel-wise regression models (Eklund
et al., 2016). Finally, previous studies (Penny et al., 2005; Schmidt et al., 2013) indicate that
the Bayesian approach is simply more powerful than the frequentist approach to voxel-wise
regression with respect to uncover potential clusters of atrophy or activation.
121

6 Applications
Although one can ﬁnd biological validations for some of the above results they need to
be reproduced using diﬀerent data sets before they can be further interpreted. After all,
results from voxel-based regression of brain images obtained from diﬀerent subjects must be
interpreted with care: The number of preprocessing steps and the complex nature of MRI
makes this type of analysis susceptible to artifacts induced by data preparation. In addition,
it must be noted that the formulated model most probably does not include all relevant
data which is needed to provide reproducible results. For example, other authors account
for total intracranial volume as a possible confounder for diﬀerent head sizes (Hutton et al.,
2009). Further variables of interest are handedness (Good et al., 2001), level of education
(Rzezak et al., 2015), and more detailed information of disease state, such as the total lesion
volume. However, it should not be diﬃcult to include this information once it is available
and to apply the framework to an extended model.
6.3 Object-based co-localization by a spatial point process
In this ﬁnal application a simple co-localization analysis is extended by additional spatial
information.
6.3.1 Introduction
Understanding the spatial arrangement and interaction of cell components is a crucial task
in molecular biology. Of particular interest is the question if speciﬁc cell functions can only
be executed if certain components interact with each other. In this context interaction
refers to spatial proximity and spatial correlation of some kind. The investigation of these
interactions is the subject of co-localization analyses.
Object-based co-localization
In object-based co-localization the data obtained from ﬂuorescence microscopy images
are reduced to objects. Precise identiﬁcation of these objects is feasible due to diﬀerent
coloration using ﬂuorescence markers and image segmentation or object detection methods
which are not of interest in this application. For each recorded object the distances to other
objects and cellular structures of concern are recorded. Classical procedures of object-based
122

6.3 Object-based co-localization by a spatial point process
Figure 6.8
Selected slices of the blue layer (DNA intensity), in grayscale.
co-localization try to reveal possible interactions between objects by analyzing the so-called
nearest-neighbor-distance co-localization measure, i.e. the number of objects that fall within
a pre-deﬁned distance (Lachmanovich et al., 2003). While simple to compute it has been
found that this measure oversimpliﬁes the complex nature of spatial proximity between sub-
cellular structures (Helmuth et al., 2010). Therefore, generalizations of this analysis have
been presented. For example, Helmuth et al. (2010) use spatial point processes in order to
estimate interaction potentials as a function of object distances in a non-parametric fashion.
In addition to the utilization of distances it has been noted that the location of the object
in the cell itself may provide valuable information for the understanding of cell processes.
For example, it has been shown that in human cells each chromosome can be assigned to
a speciﬁc location in the nucleus (Bolzer et al., 2005). Thus, it seems natural to consider
the spatial position of objects within a cell in the analysis. In this application, a spatial
Poisson process which is able to account for this type of information is ﬁtted to the data
of a three-dimensional cell nucleus.
Data
The data used in this application comes from the combination of 3D structured illumination
microscopy (3D-SIM) and 3D ﬂuorescence in situ hybridization (3D-FISH), see Markaki
et al. (2012) for details on these methods. Object of interest is the nucleus of a human cell in
the process of DNA replication shortly before cell division. The nucleus has been colorized
using diﬀerent ﬂuorescent stains in order to identify speciﬁc components. In particular,
three layers have been used to identify chromatin (DNA), and the two genes Ser2 and Pol3.
123

6 Applications
These structures were colorized in blue, red, and green, respectively. As an example Figure
6.8 displays selected slices of the blue layer (DNA intensity) in grayscale. The preprocessed
image is of dimension 190 × 190 × 51 with a voxel size of 0.125 × 0.125 × 0.125 micrometers
and 514,442 relevant voxels. For each voxel, dummy variables describing an aﬃliation to
Ser2 (red layer), the distance to the next gene of the same kind as well as the next Pol3
gene (green layer) and DNA intensity, that is the intensity value of the blue layer, have
been recorded.
6.3.2 Modeling
In this application, the spatial distribution and co-location of Ser2 genes that have been
unveiled in the red layer are of interest. To investigate this the following log-linear Poisson
model is set up:
yi ∼Po(exp(ηi)), i = 1, . . . , 514,442
with
ηi = β0 + f1(z1i) + f2(z2i) + f3(z3i) + fspatial(i).
In this formulation yi indicates if voxel i belongs to a Ser2 gene. Note that this holds
only for about 0.63% of all voxels, thus this data set is similar to the sparse data situation
considered in Chapter 5. The metric covariates z1, z2, and z3 are the recorded distances
to other nearest Ser2 genes, nearest Pol3 genes, and the voxels DNA intensity, respectively.
The eﬀects of these covariates are modeled using Bayesian P-splines as discussed in Section
2.2.1. To be more precise, design matrices Z1, Z2, and Z3 are constructed using B-spline
basis functions of degree D = 3 and 30 equidistant knots over the corresponding covariate
domain. The main eﬀect of interest, that is, diﬀerences in the spatial distribution of Ser2
genes, is revealed by including fspatial into the linear predictor, i.e. by a spatially varying
intercept. Overall, the linear predictor can be written as
η = β0 + Z1γ1 + Z2γ2 + Z3γ3 + γ4.
Prior distributions are set up as follows.
For the overall intercept a non-informative
GMRF prior N(0, 1e−6) is chosen.
The coeﬃcients of the nonlinear smooth eﬀects are
modeled by RW2 priors. The dimension of the spatially varying intercept is 514,442 × 1.
124

6.3 Object-based co-localization by a spatial point process
Similar to the previous applications, a three-dimensional extension of the RW1 prior is
speciﬁed for this regression coeﬃcient. In addition, sum-to-zero constraints are imposed
on all non-parametric eﬀects in order to guarantee identiﬁability. For precision parameters
independent weakly-informative Gamma distributions are chosen with ak = 1 and bk =
5e−5 for k = 1, . . . , 3.
Due to the low information content in the data the choice of
hyperparameters for the precision of γ4 is rather sensitive with respect to the ﬁnal results.
Therefore, in order to support the estimation process and to promote a more smooth result
for the spatial eﬀect a more informative prior is chosen, i.e. a Gamma distribution with
parameters a4 = 1000 and b4 = 10.
Note that the above model speciﬁcation coincides with the deﬁnition of a log-Gaussian
Cox process model: given the realizations of the GMRF components the process is a Poisson
process (Diggle et al., 2013). Similar models have been discussed previously, for example
by including multiple spatially structured eﬀects (Illian et al., 2012).
Samples for the overall intercept and the coeﬃcients of the smooth eﬀects can be
saved without any modiﬁcation. However, due to the dimension of γ4, the approximation
strategies introduced in Chapter 4 need to be applied. As only one data point is connected
to each voxel and each voxel is assigned an element of γ4 it is obvious that similar problems
must be expected as for the sparse data set considered in the simulation studies in Section
5.1.1, which includes problems with block updating strategies. Indeed, attempts with these
strategies, especially using the stochastic Chebyshev approximation of log-determinants,
did reveal convergence problems of the Markov chain for κ4. Thus, again the modiﬁed
sampling scheme is applied and for preconditioning the IC factorization without additional
ﬁll-in is used. With this strategy four independent chains of length 20,000 and an initial
burn-in period of length 1,000 are produced.
6.3.3 Results
Generating 1,000 iterations for the above model requires about two hours and less than 2
GB RAM on working station A. Acceptance rates are as follows: For the overall intercept
88% of all proposals are accepted. For γ1, γ2, and γ3 acceptance rates are about 45, 29, and
66%, respectively. Finally, for the spatially varying intercept 97% of the proposed samples
are accepted.
125

6 Applications
log(κ1)
log(κ2)
log(κ3)
log(κ4)
1
2
3
4
2
4
6
4
5
6
7
8
4.3
4.4
4.5
4.6
4.7
4.8
4.9
0
5000
10000
15000
20000
0
5000
10000
15000
20000
Iteration
Figure 6.9
Generated chains of the precision parameters for one run.
Assessing convergence and mixing of Markov chains
The estimated potential scale reduction factors indicate convergence of all chains (maximum
ˆR = 1.01), except for κ4 ( ˆR = 1.12).
Figure 6.9 displays the generated chains of the
precision parameters for one run. While mixing for κ1, . . . , κ3 is excellent, there is deﬁnitely
room for improvement for κ4. Here, realizations are characterized by strong dependencies
which result in very slow mixing. However, the chains of the remaining runs yield similar
results so that there is no reason to believe that the algorithm diverges, it just requires
more time to fully explore the posterior. Thus, the results for this parameter can still be
deemed useful.
126

6.3 Object-based co-localization by a spatial point process
−15
−10
−5
0
5
−10
−5
0
−5.0
−2.5
0.0
−6.1
−6.0
−5.9
−5.8
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
0.00
0.25
0.50
0.75
1.00
β0
Distance to nearest Ser2 gene
Distance to nearest Pol3 gene
DNA intensity
f^
1(z1)
f^
2(z2)
f^
3(z3)
Figure 6.10
Estimated marginal posterior for the overall intercept (upper left panel) and
estimated smooth eﬀects for all continuous covariates. The black lines display the posterior
median and the shaded regions correspond to the equal-tailed 50% and 95% credible
intervals.
Assessing the estimated eﬀects
The following results are constructed from the results of all runs. The marginal posterior
of the overall intercept is displayed in the upper left panel of Figure 6.10. The 95% credible
interval for this parameter is [−6.03, −5.83]. This indicates that the overall probability of
observing a Ser2 gene within the cell nucleus is rather small which is in accordance with the
sparse nature of the problem. From the upper right panel of Figure 6.10 it can be seen that
the probability of observing a Ser2 gene next to a gene of the same kind is increased if their
distance is between 0.25 and 0.6 nanometers, that is, within these distances Ser2 genes tend
to cluster. Moreover, up to a distance of 0.65 nanometers Ser2 genes are more likely located
near Pol3 genes (lower left panel of Figure 6.10): Up to about 0.65 nanometers distance
the estimated smooth eﬀect is positive. Finally, the eﬀect of the voxels DNA intensity is
127

6 Applications
positive up to an intensity of about 0.2 and negative afterwards which indicates that Ser2
is more likely in regions with low DNA intensity (lower right panel of Figure 6.10).
Figure 6.11 depicts the standardized coeﬃcients (posterior mean divided by posterior
standard deviation) for the spatially varying intercept. Here, 35 successive slices of the cell
nucleus have been selected. It can be seen that it is more likely to observe Ser2 genes at
the edge of the nucleus; for most voxels in the centre of the cell negative coeﬃcients are
estimated. Given the magnitude of these standardized coeﬃcients it becomes obvious that
the information about the location inside the nucleus may not be of practical relevance to the
question at hand, at least compared to the eﬀect sizes of the other covariates. Furthermore,
the variation along slices (z-direction) is less pronounced than along the other dimensions
(x- and y-direction). This information may be used to ignore the z-direction of the cell,
that is, the three-dimensional spatial eﬀect may be reduced to a two-dimensional spatial
eﬀect. This would increase the information available for each pixel and, most probably,
stabilize the estimation process.
6.3.4 Summary
Due to the sparse nature of the problem the proposed MCMC algorithm suﬀers from similar
problems as the sparse data situation analyzed in Chapter 5.
In particular, mixing of
Markov chains for the precision parameter of the spatially varying intercept is insuﬃcient
and longer runs are necessary in order to fully explore the corresponding marginal posterior.
Despite this problem it has been shown that it is possible to derive useful results with
moderate computational equipment.
Modeling the distances between objects of interest by nonlinear eﬀects presents a simple
but yet eﬃcient way to perform object-based co-localization under the consideration of
multiple layers.
It has been shown that it is possible to include additional spatial
information, although for the data set considered here it may not be necessary to do so. If
further investigations do not conﬁrm the necessity of accounting for the three-dimensional
information it may be reasonable to collapse the data structure and to consider a simpler
two-dimensional eﬀect.
Furthermore, the biological ﬁndings must be replicated as the
analysis performed is based on single-cell data.
128

6.3 Object-based co-localization by a spatial point process
Figure 6.11
Standardized coeﬃcients for the spatially varying intercept.
129


7 Summary and outlook
7.1 Summary
Main objective of this thesis was to ﬁnd a way to estimate regression models with high-
dimensional coeﬃcients with low to moderate computational equipment. To this end, the
framework of STAR models has been chosen as it allows to include a large variety of diﬀerent
eﬀect types including spatial eﬀects. In order to estimate models with high-dimensional
regression coeﬃcients an MCMC algorithm has been constructed.
The computational
requirements can be described as low to moderate. Thus, the algorithm allows the applied
statistician to perform appropriate inference for large-scale time-insensitive problems
without the need of well equipped working stations.
Without a question, deterministic methods for ﬁtting STAR models have clear
advantages compared to MCMC based inference.
For example, they are usually faster
and do not depend on mixing and convergence of Markov chains. This can be beneﬁcial
with respect to the complete modeling process which can be seen as an iterative procedure
in which model ﬁtting and model checking complement each other.
However, the
discussion in Chapter 4 revealed that deterministic methods either violate important
requirements (sparsity) or are not ﬂexible enough to account for application-speciﬁc
features (large number of precision or dispersion parameters). In contrast, MCMC based
inference meets all requirements and successfully addresses special data situations. The
computational bottlenecks which are associated with this method, namely sampling from
large-scale Gaussians and computing the log-determinant of huge matrices, were successfully
eliminated. With regard to sampling this could be achieved by Krylov subspace methods:
First, the conjugate gradient method is applied in order to compute the mean of the
proposal.
Next, the Lanczos algorithm is used to obtain an approximate sample from
the zero mean Gaussian. Finally, the sample may be corrected in order to adjust for linear
constraints. In order to improve convergence of these models, preconditioning is necessary.
131

7 Summary and outlook
In most cases the incomplete Cholesky factorization with zero ﬁll-in has been found to be
suﬃcient.
A modiﬁed sampling scheme has been used in order to circumvent the necessity of
the calculation of log-determinants of huge matrices. The resulting algorithm does not
only convince with its performance in the simulation studies conducted in Chapter 5,
it is also preferable to the alternative methods in terms of computational complexity.
The deterministic approximation of the log-determinant using the ICT factorization as
well as the stochastic Chebyshev approximation yielded satisfying results for the direct
approximation of the log-determinant. However, with respect to the ﬁnal results the MCMC
error was not able to compensate the error introduced by these approximations.
The simulation studies conducted in Chapter 5 have demonstrated situations in which
the proposed MCMC framework produces reliable results.
Especially in settings where
suﬃcient information in the data is present the estimation of large-scale eﬀects is well
supported.
In this case results can be generated that are extremely close to the one
obtained by the INLA approach.
However, in situations where the information in the
data is sparse problems with convergence of Markov chains of precision parameters can be
observed. Particularly, the strategies which approximate the log-determinants show a poor
behavior in these situations. Furthermore, the modiﬁed sampling scheme requires more
iterations to explore the corresponding posteriors suﬃciently.
In the last chapter it has been shown that the proposed MCMC framework can be
applied successfully to a broad range of applications. First, a supervised approach for lesion
segmentation was presented. Here, a regression model with a high-dimensional spatial eﬀect
was ﬁtted. In Section 6.2 a Gaussian response model with ﬁve of these eﬀects was applied
in the context of voxel-based morphometry. Finally, a spatial poisson process was used to
analyze the spatial distribution of genes in the nucleus of a human cell.
7.2 Outlook
The framework described in this thesis is not restricted to the analysis of three-dimensional
medical or biological images as performed in the last chapter. Instead, the MCMC algorithm
is applicable to a wide range of diﬀerent applications. For example, the data of diﬀerent
medical imaging techniques such as EEG signals can be processed by generalized additive
regression models (Meulman et al., 2015) and, thus, present a possible application. Also,
132

7.2 Outlook
inverse problems within the analysis of non-medical digital images, such as deblurring and
sharpening noisy images (Bardsley, 2012), could be analyzed. Further applications may be
found when modeling large time series as occurred previously when forecasting electricity
consumption (Gaillard and Goude, 2015), in the ﬁeld of articulography (Wieling et al.,
2015), and monitoring environmental data (Elayouty et al., 2016).
Having shown how the classical Gibbs sampler can be applied to large-scale problems,
the improvement of the algorithm has to be the next logical step. Of particular importance
are situations where Markov chains show slow mixing properties. This applies particularly
to situations where the information of the data is sparse, such as the second simulation
setup in Chapter 5 and the microscopy application in Section 6.3. Over the years diﬀerent
methods have been proposed to improve mixing of Markov chains. For example, a non-
centered version of the Gibbs sampler (Papaspiliopoulos and Roberts, 2003) can be used
to dissolve the dependency of γ and κ. Note that connections between the preconditioned
Lanczos sampler and the non-centered parametrization are already discussed in Simpson
et al. (2013).
Further promising methods to improve mixing of Markov chains are the
partially collapsed Gibbs sampler (van Dyk and Park, 2008) and the interweaving strategy
proposed by Yu and Meng (2011). However, care must be taken as these approaches need to
fulﬁll all necessary requirements for working on large-scale problems (Section 4.1.1). Note
that updating γ and κ jointly as proposed by Knorr-Held and Rue (2002) and performed
in the simulation studies in Chapter 5 did not result in improved mixing behavior. Most
probably, better approximations for the log-determinant are required in order to apply this
strategy successfully. In addition, the results of Knorr-Held and Rue suggests that larger
blocks may be necessary. However, this would increase the computational requirements
dramatically.
Increasing
the
eﬃciency
of
the
MCMC
algorithm
also
includes
an
improved
implementation in order to speed up the ﬁtting process. This can be achieved in various
ways. As a ﬁrst step the algorithm could be implemented in a lower-level programming
language.
In addition, new technological achievements oﬀer diﬀerent possibilities of
parallelization and thus promising a signiﬁcant gain in computation time. For example, the
most demanding step within the Krylov subspace methods is the computation of matrix-
vector products. Therefore, parallelization of matrix multiplications seems to be a good
starting point. Note that the Matlab implementation used in this thesis already makes
use of this feature. Another approach is to parallelize the Gibbs sampler for which diﬀerent
suggestions have been made in the literature, see for example Doshi-Velez et al. (2009)
133

7 Summary and outlook
and Gonzalez et al. (2011). However, it needs to be proven that these approaches do not
counteract on crucial assumptions that are necessary for working in large-scale settings.
Probably the simplest and most naive approach of parallelization is the generation of
multiple chains on independent processing units. For example, sampling 10,000 realizations
distributed over 100 independent MCMC chains only requires the generation of 100
samples for each chain, a task which can be performed rather quickly.
Depending on
the computational requirements such massive parallelizations can be achieved by using
GPUs or simple single board computers which can usually be purchased for a reasonable
price. However, fast convergence of the MCMC algorithm is the main requirement for this
parallelization strategy. Otherwise, the advantages of parallel setups may be undermined
by long burn-in periods.
An interesting extension of STAR models can be achieved by the framework
of generalized additive models for location, scale and shape (GAMLSS, Rigby and
Stasinopoulos, 2005).
Here, in addition to the conditional mean other distributional
parameters, in particular the variance, skewness and kurtosis, can be modeled by additive
predictors and speciﬁc link functions. First attempts within Bayesian setups seem promising
(Klein et al., 2015). With respect to the analysis of medical images the GAMLSS framework
is of particular interest for voxel-wise regression models as performed in Section 6.2. Here,
independent dispersion parameters have been estimated for each voxel. Due to the spatial
structure of the data it seems natural to include this information in the estimation of these
parameters.
134

Appendix


A Implementation details
A.1 Working stations and software
All calculations by the proposed MCMC algorithm have been performed on the following
machine:
Working station A: MacBook Pro (Retina, 13′′, late 2013) running on OS X
El Capitan (Version 10.11.4). Processor: 2.8 GHz Intel Core i7. Memory: 16
GB 1600 MHz DDR3.
Due to limited memory of this machine another working station has been used in order to
obtain the results for the INLA approach in Chapter 5. This machine has the following
speciﬁcations:
Working station B: Self-made desktop PC running Ubuntu Linux (Version
12.04). Processor: AMD Phenom II x4 955. Memory: 32 GB 667 MHz DDR3.
All code was implemented in Matlab1 version 8.3.0.532 (R2014a) on working station A,
and version 7.14.0.739 (R2012a) on working station B.
A.2 Krylov subspace methods
Up to three diﬀerent Krylov subspace methods are used in this thesis: A preconditioned
version of the conjugate gradient method for solving linear systems, a preconditioned
Lanczos
algorithm
for
approximate
sampling,
and
a
Lanczos
algorithm
for
the
approximation of extreme eigenvalues. Pseudocode for the ﬁrst two algorithms is provided
next, for the latter and the stochastic Chebyshev expansion see Section A.3.
1www.mathworks.com/products/matlab/.
I

A Implementation details
Algorithm 2 Preconditioned conjugate gradient algorithm.
1: Set r0 = b −Qµ0, ˜r0 = M −1r0, and p0 = M −T ˜r0.
2: for j = 1, . . . , r do
3:
αj = ||˜rj||2/⟨Qpj, pj⟩
4:
µj+1 = µj + αjpj
5:
˜rj+1 = ˜rj −αjM −1Qpj
6:
βj = ||˜rj+1||2/||˜rj||2
7:
pj+1 = M −T ˜rj+1 + βjpj
8: end for
A.2.1 Preconditioned conjugate gradients
A version of the preconditioned conjugate gradients algorithm for solving the linear system
Qµ = b for symmetric preconditioners M = M L = M R is displayed in Algorithm 2.
This version is taken from Saad (2003, Algorithm 9.2).
Note that there exist a wide
variety of diﬀerent – and possibly more eﬃcient – implementations of the preconditioned
conjugate gradients algorithm. In particular, versions that use parallelization can be found
in the literature, see for example O’Leary (1987) and Di Brozolo and Robert (1989) for
early applications as well as Saad (2003, Chapter, 11) for an overview.
Often, special
implementations can be found for speciﬁc software packages.
In this thesis, Matlabs
implementation in the pcg function was used. For the computation of the IC and ICT
preconditioners Matlabs ichol function has been used.
Algorithm 3 Preconditioned Lanczos algorithm for approximate sampling.
1: Set v0 = 0, β1 = 0 and initialize v1
2: for j = 1, . . . , r do
3:
a = M −1vj
4:
b = Qa
5:
w = M −Tb −βjvj−1
6:
αj = w′vj
7:
w = w −αjvj
8:
βj+1 = ||w||2
9:
vj = w/βj+1
10: end for
II

A.3 Approximation of log-determinants
Algorithm 4 Lanczos algorithm for approximation of extreme eigenvalues
1: Set β1 = 0 and initialize v0 = u/||u||2 with u ∼U(0, 1)
2: for j = 1, . . . , r do
3:
w = Qvj −βjvj−1
4:
αj = w′vj
5:
w = w −αjvj
6:
βj+1 = ||w||2
7:
vj = w/βj+1
8: end for
A.2.2 Preconditioned Lanczos algorithm for approximate sampling
For approximate sampling the preconditioned Lanczos algorithm as given in Algorithm 3
has been used, which is Algorithm 1 extended by the preconditioning steps discussed in
Section 4.2.3. The same IC and ICT preconditioners as for Algorithm 2 were used. The
result of Algorithm 3 are coeﬃcients αj and βj, j = 1, . . . , r, which are used to create matrix
T r given in (4.3). Subsequently, an approximate sample x from N(0, Q−1) is obtained by
calculating ˜x∗according to (4.4) and correcting for preconditioning, i.e. x = M −1˜x∗. As a
stopping rule for Algorithm 3 Chow and Saad (2014) suggest to monitor the relative change
between ˜x∗
j and ˜x∗
j−1.
A.3 Approximation of log-determinants
A.3.1 Stochastic Chebyshev expansion
The complete algorithm for the stochastic Chebyshev expansion of the log-determinant of
Q can be found in pseudocode in Han et al. (2015). In addition, Matlab code is available
online2.
A.3.2 Lanczos algorithm for extreme eigenvalues
For the stochastic Chebyshev expansion it is required to provide rough estimates for the
smallest and largest eigenvalues of Q. This was the original task of the algorithm proposed
by Lanczos (1950). Thus, the Lanczos algorithm in its original form (Algorithm 4) can be
used for this task. As a results this algorithm generates coeﬃcients αj and βj, j = 1, . . . , r,
2https://sites.google.com/site/mijirim/logdet_code.zip, visited on January 13th, 2016.
III

A Implementation details
which are used to formulate matrix T r according to (4.3). The extreme eigenvalues of Q
can then be approximated by the smallest and largest eigenvalues of T r. The algorithm is
stopped if no more signiﬁcant changes in these values are observed.
IV

List of Figures
2.1
Visualization of undirected graphs according to the RW1 (a) and RW2 (b) prior
speciﬁcation. Only nodes with a direct connection are considered as neighbors.
Neighboring nodes of the black nodes are colored gray. Numbers refer to the entries
of the fourth row of the corresponding structure matrices.
. . . . . . . . . . . . . . 17
2.2
Visualization of two-dimensional undirected graphs on (a) a 5 × 5 regular lattice
and (b) an irregular lattice corresponding to the districts of the city of Berlin,
Germany. In both graphs, a ﬁrst order neighborhood structure is imposed, that
is, only nodes that share a common border are considered as neighbors. Numbers
refer to the non-zero entries of the black nodes rows of (2.15). . . . . . . . . . . . . . 20
2.3
Visualization of an undirected graph with spatio-temporal dependency.
Neigh-
borhood structure corresponds to a Type IV interaction according to Knorr-Held
(2000) between a RW2 in temporal and a RW1 in spatial dimension. Values refer
to the row of the black node in the corresponding structure matrix. . . . . . . . . . . 24
2.4
Visualization of an undirected graph with spatio-temporal dependency. Neighbor-
hood structure is imposed by a Kronecker sum of a RW2 in temporal and a RW1 in
spatial dimension. Values refer to the row of the black node in the corresponding
structure matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.1
Comparison of storage and computation times for dense and sparse matrix
representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.2
Sparsity pattern of precision matrix (4.1). Top row displays the original ordering
and the results of diﬀerent permutation algorithms.
Bottom row depicts the
sparsity pattern of the corresponding Cholesky factors.
. . . . . . . . . . . . . . . . 62
4.3
Eﬀect of graph dimension on ﬁll-in-ratio and storage of Cholesky factors obtained
by diﬀerent permutation algorithms.
. . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1
True eﬀect image used for the dense data situation (right panel) and generated
data for the sparse data situation (left panel). . . . . . . . . . . . . . . . . . . . . . . 83
5.2
Mean and range of (5.2) for the dense data set. . . . . . . . . . . . . . . . . . . . . . 87
5.3
Mean and range of (5.2) for the sparse data set. . . . . . . . . . . . . . . . . . . . . . 88
V

List of Figures
5.4
Mean and range of (5.3) for the stochastic Chebyshev estimator applied to the
dense data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.5
Mean and range of (5.3) for the stochastic Chebyshev estimator applied to the
sparse data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.6
Mean and range of (5.3) for the ICT log-determinant approximation applied to the
dense data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
5.7
Mean and range of (5.3) for the ICT log-determinant approximation applied to the
sparse data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
5.8
Smooth eﬀects used for the simulation. . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.9
Sampling paths for log(κ1). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.10 SKLD for all three smooth eﬀects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.11 Diﬀerences of the standardized coeﬃcient maps for all three smooth eﬀects. . . . . . 98
5.12 Sampling paths for log(κ) for a = 5 and b = 1.
. . . . . . . . . . . . . . . . . . . . . 99
5.13 Sampling paths for log(κ) for a = 10 and b = 0.001.
. . . . . . . . . . . . . . . . . 100
5.14 SKLD for the smooth eﬀect. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.15 Diﬀerences of the standardized coeﬃcient maps for all three smooth eﬀects for
a = 5 and b = 1.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
5.16 Diﬀerences of the standardized coeﬃcient maps for all three smooth eﬀects for
a = 10 and b = 0.001.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.1
Selected axial slices of a T1-weighted and a FLAIR image for a randomly chosen
MS patient. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
6.2
Maximum intensity projections for the position of MS lesions for all 312 subjects
(left) and the selected 53 subjects (right).
. . . . . . . . . . . . . . . . . . . . . . 108
6.3
MCMC sampling paths (left column) and estimated marginal posteriors (right
column) for selected parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.4
Estimated posterior means of the spatially varying intercept.
. . . . . . . . . . . . 113
6.5
FLAIR images and estimated lesion probability maps for three subjects from the
test data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
6.6
Generated chains of the precision parameters for one run. . . . . . . . . . . . . . . 119
6.7
Estimated eﬀects for age, sex, state of disease, and the interaction of age and state
of disease.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.8
Selected slices of the blue layer (DNA intensity), in grayscale. . . . . . . . . . . . . 123
6.9
Generated chains of the precision parameters for one run.
. . . . . . . . . . . . . 126
6.10 Estimated marginal posterior for the overall intercept (upper left panel) and
estimated smooth eﬀects for all continuous covariates.
The black lines display
the posterior median and the shaded regions correspond to the equal-tailed 50%
and 95% credible intervals.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
VI

List of Figures
6.11 Standardized coeﬃcients for the spatially varying intercept. . . . . . . . . . . . . . 129
VII


References
Aït-Ali, L. S., Prima, S., Hellier, P., Carsin, B., Edan, G., and Barillot, C. (2005). Strem:
a robust multidimensional parametric method to segment MS lesions in MRI. In Medical
Image Computing and Computer-Assisted Intervention–MICCAI 2005, pages 409–416.
Springer.
Akselrod-Ballin, A., Galun, M., Gomori, J. M., Filippi, M., Valsasina, P., Basri, R., and
Brandt, A. (2009). Automatic segmentation and classiﬁcation of multiple sclerosis in
multichannel MRI. Biomedical Engineering, IEEE Transactions on, 56(10):2461–2469.
Albert, J. H. and Chib, S. (1993). Bayesian analysis of binary and polychotomous response
data. Journal of the American Statistical Association, 88(422):669–679.
Anbeek, P., Vincken, K. L., van Osch, M. J., Bisschops, R. H., and van der Grond, J.
(2004). Probabilistic segmentation of white matter lesions in MR imaging. NeuroImage,
21(3):1037–1044.
Andrews, D. F. and Mallows, C. L. (1974). Scale mixtures of normal distributions. Journal
of the Royal Statistical Society. Series B (Methodological), 36(1):99–102.
Ashburner, J. and Friston, K. J. (2000).
Voxel-based morphometry–the methods.
Neuroimage, 11(6):805–821.
Aune, E., Eidsvik, J., and Pokern, Y. (2013). Iterative numerical methods for sampling
from high dimensional Gaussian distributions. Statistics and Computing, 23(4):501–521.
Aune, E., Simpson, D. P., and Eidsvik, J. (2014). Parameter estimation in high dimensional
Gaussian distributions. Statistics and Computing, 24(2):247–263.
Bai, Z., Fahey, G., and Golub, G. (1996). Some large-scale matrix computation problems.
Journal of Computational and Applied Mathematics, 74(1):71–89.
Bai, Z. and Golub, G. H. (1996). Bounds for the trace of the inverse and the determinant
of symmetric positive deﬁnite matrices. Annals of Numerical Mathematics, 4:29–38.
IX

References
Bardsley, J. M. (2012). MCMC-based image reconstruction with uncertainty quantiﬁcation.
SIAM Journal on Scientiﬁc Computing, 34(3):A1316–A1332.
Barry, R. P. and Pace, K. R. (1999). Monte Carlo estimates of the log determinant of large
sparse matrices. Linear Algebra and its applications, 289(1):41–54.
Benzi, M. and Tuma, M. (1999).
A comparative study of sparse approximate inverse
preconditioners. Applied Numerical Mathematics, 30(2):305–340.
Besag, J. (1989). Towards Bayesian image analysis. Journal of Applied Statistics, 16(3):395–
407.
Besag, J. and Higdon, D. (1999). Bayesian analysis of agricultural ﬁeld experiments. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(4):691–746.
Besag, J., York, J., and Mollié, A. (1991). Bayesian image restoration, with two applications
in spatial statistics. Annals of the Institute of Statistical Mathematics, 43(1):1–20.
Bezdek, J. C., Hall, L., and Clarke, L. (1992). Review of MR image segmentation techniques
using pattern recognition. Medical physics, 20(4):1033–1048.
Bickel, P. J., Brown, J. B., Huang, H., and Li, Q. (2009).
An overview of recent
developments in genomics and associated statistical methods. Philosophical Transactions
of the Royal Society of London A: Mathematical, Physical and Engineering Sciences,
367(1906):4313–4337.
Bishop, C. (2006). Pattern Recognition and Machine Learning. Springer-Verlag New York.
Bolzer, A., Kreth, G., Solovei, I., Koehler, D., Saracoglu, K., Fauth, C., Müller, S., Eils,
R., Cremer, C., Speicher, M. R., and Cremer, T. (2005). Three-dimensional maps of all
chromosomes in human male ﬁbroblast nuclei and prometaphase rosettes. PLoS Biol,
3(5):e157.
Brezger, A. and Lang, S. (2006).
Generalized structured additive regression based on
Bayesian P-splines. Computational Statistics & Data Analysis, 50(4):967–991.
Chan, T., Gilbert, J. R., and Teng, S.-H. (1995). Geometric spectral partitioning. Technical
Report CSL-94-15, Xerox PARC.
Chen, Z. (1993). Fitting multivariate regression functions by interaction spline models.
Journal of the Royal Statistical Society. Series B (Methodological), 55(2):473 – 491.
Chow, E. and Saad, Y. (1998).
Approximate inverse preconditioners via sparse-sparse
iterations. SIAM Journal on Scientiﬁc Computing, 19(3):995–1023.
X

References
Chow, E. and Saad, Y. (2014).
Preconditioned Krylov subspace methods for sampling
multivariate Gaussian distributions. SIAM J. Sci. Comput., 36(2):A588 – A608.
Clayton, D. G. (1996). Generalized linear mixed models. In Gilks, W. R., Richardson, S.,
and Spiegelhalter, D., editors, Markov Chain Monte Carlo in practice. Chapman & Hall.
Compston, A. and Coles, A. (2008). Multiple sclerosis. The Lancet, 372(9648):1502–1517.
Cosgrove, J., Diaz, J., and Griewank, A. (1992). Approximate inverse preconditionings for
sparse linear systems. International Journal of Computer Mathematics, 44(1-4):91–110.
Cowles, M. K. and Carlin, B. P. (1996). Markov Chain Monte Carlo convergence diagnostics:
A comparative review. Journal of the American Statistical Association, 91(434):883—904.
Crayton, H., Heyman, R. A., and Rossman, H. S. (2004).
A multimodal approach to
managing the symptoms of multiple sclerosis. Neurology, 63(11 suppl 5):S12–S18.
Cuthill, E. and McKee, J. (1969). Reducing the bandwidth of sparse symmetric matrices. In
Proceedings of the 1969 24th National Conference, ACM ’69, pages 157–172, New York,
NY, USA. ACM.
de Boor, C. (1978). A Practical Guide to Splines. Springer, Berlin.
Di Brozolo, G. R. and Robert, Y. (1989).
Parallel conjugate gradient-like algorithms
for solving sparse nonsymmetric linear systems on a vector multiprocessor.
Parallel
Computing, 11(2):223–239.
Diggle, P. and Ribeiro, P. J. (2007). Model-based Geostatistics. Springer New York.
Diggle, P. J., Moraga, P., Rowlingson, B., Taylor, B. M., et al. (2013). Spatial and spatio-
temporal log-gaussian cox processes: extending the geostatistical paradigm. Statistical
Science, 28(4):542–563.
Doshi-Velez, F., Mohamed, S., Ghahramani, Z., and Knowles, D. A. (2009). Large scale
nonparametric Bayesian inference: Data parallelisation in the indian buﬀet process. In
Advances in Neural Information Processing Systems, pages 1294–1302.
Dunn, K. W., Kamocka, M. M., and McDonald, J. H. (2011). A practical guide to evaluating
colocalization in biological microscopy. American Journal of Physiology-Cell Physiology,
300(4):C723–C742.
Eilers, P. H. C. and Marx, B. D. (1996). Flexible smoothing with B-splines and penalties.
Statistical Science, 11(2):89—102.
XI

References
Eklund, A., Nichols, T. E., and Knutsson, H. (2016). Cluster failure: Why fMRI inferences
for spatial extent have inﬂated false-positive rates. Proceedings of the National Academy
of Sciences, 113(28):7900–7905.
Elayouty, A., Scott, M., Miller, C., Waldron, S., and Franco-Villoria, M. (2016). Challenges
in modeling detailed and complex environmental data sets: a case study modeling the
excess partial pressure of ﬂuvial CO2. Environmental and Ecological Statistics, 23(1):65–
87.
Evans, A. C., Collins, D. L., Mills, S., Brown, E., Kelly, R., and Peters, T. M. (1993). 3d
statistical neuroanatomical models from 305 mri volumes. In Nuclear Science Symposium
and Medical Imaging Conference, 1993., 1993 IEEE Conference Record., pages 1813–
1817. IEEE.
Fahrmeir, L. and Kneib, T. (2011). Bayesian smoothing and regression for longitudinal,
spatial and event history data. Oxford University Press.
Fahrmeir, L., Kneib, T., and Lang, S. (2004). Penalized structured additive regression for
space-time data: a Bayesian perspective. Statistica Sinica, 14(3):731–762.
Fahrmeir, L. and Tutz, G. (2001). Multivariate Statistical Modelling Based on Generalized
Linear Models. Springer, New York, 2nd edition.
Ferrari, S. and Cribari-Neto, F. (2004). Beta regression for modelling rates and proportions.
Journal of Applied Statistics, 31(7):799–815.
Fischer, B. and Freund, R. W. (1994). On adaptive weighted polynomial preconditioning for
Hermitian positive deﬁnite matrices. SIAM Journal on Scientiﬁc Computing, 15(2):408–
426.
Fletcher, R. (1976).
Conjugate gradient methods for indeﬁnite systems.
In Numerical
analysis, pages 73–89. Springer.
Freifeld, O., Greenspan, H., and Goldberger, J. (2009). Multiple sclerosis lesion detection
using constrained GMM and curve evolution. Journal of Biomedical Imaging, 2009:14.
Friston, K. and Penny, W. (2003). Posterior probability maps and SPMs. Neuroimage,
19(3):1240–1249.
Friston, K. J., Holmes, A. P., Worsley, K. J., Poline, J.-P., Frith, C. D., and Frackowiak,
R. S. (1994). Statistical parametric maps in functional imaging: a general linear approach.
Human brain mapping, 2(4):189–210.
XII

References
Gaillard, P. and Goude, Y. (2015).
Forecasting electricity consumption by aggregating
experts; how to design a good set of experts. In Modeling and Stochastic Learning for
Forecasting in High Dimensions, pages 95–115. Springer.
Gamerman, D. (1997). Sampling from the posterior distribution in generalized linear mixed
models. Statistics and Computing, 7(1):57–68.
Ge, T., Müller-Lenke, N., Bendfeldt, K., Nichols, T. E., and Johnson, T. D. (2014).
Analysis of multiple sclerosis lesions via spatially varying coeﬃcients.
The annals of
applied statistics, 8(2):1095–1118.
Gelman, A. (2006).
Prior distributions for variance parameters in hierarchical models.
Bayesian Analysis, 1(3):515–533.
Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (2014). Bayesian data analysis,
volume 2. Taylor & Francis.
Gelman, A. and Hill, J. (2007). Data analysis using regression and multilevel/hierarchical
models. Cambridge University Press.
Gelman, A. and Rubin, D. B. (1992). Inference from iterative simulation using multiple
sequences. Statistical science, pages 457–472.
Geman, S. and Geman, D. (1984).
Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, PAMI-6(6):721–741.
George, A. (1973). Nested dissection of a regular ﬁnite element mesh. SIAM Journal on
Numerical Analysis, 10(2):345–363.
George, A. and Liu, J. W. (1989). The evolution of the minimum degree ordering algorithm.
SIAM Review, 31(1):1–19.
Geremia, E., Clatz, O., Menze, B. H., Konukoglu, E., Criminisi, A., and Ayache, N. (2011).
Spatial decision forests for MS lesion segmentation in multi-channel magnetic resonance
images. NeuroImage, 57(2):378–390.
Ghosh, A., Pal, N. R., and Pal, S. K. (1991). Image segmentation using a neural network.
Biological Cybernetics, 66(2):151–158.
Gilbert, J. R., Moler, C., and Schreiber, R. (1992). Sparse matrices in MATLAB: design
and implementation. SIAM Journal on Matrix Analysis and Applications, 13(1):333–356.
XIII

References
Gilks, W. R., Richardson, S., and Spiegelhalter, D. J. (1996). Introducing Markov Chain
Monte Carlo. In Gilks, W. R., Richardson, S., and Spiegelhalter, D., editors, Markov
Chain Monte Carlo in practice. Chapman & Hall.
Gilks, W. R. and Roberts, G. O. (1996). Strategies for improving MCMC. In Gilks, W. R.,
Richardson, S., and Spiegelhalter, D., editors, Markov Chain Monte Carlo in practice.
Chapman & Hall.
Giraud, L. and Gratton, S. (2006). On the sensitivity of some spectral preconditioners.
SIAM journal on matrix analysis and applications, 27(4):1089–1105.
Gonzalez, J., Low, Y., Gretton, A., and Guestrin, C. (2011). Parallel Gibbs sampling: From
colored ﬁelds to thin junction trees. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 324–332.
Good, C. D., Johnsrude, I., Ashburner, J., Henson, R. N., Friston, K. J., and Frackowiak,
R. S. (2001).
Cerebral asymmetry and the eﬀects of sex and handedness on brain
structure:
a voxel-based morphometric analysis of 465 normal adult human brains.
Neuroimage, 14(3):685–700.
Gössl, C., Auer, D. P., and Fahrmeir, L. (2000). Dynamic models in fMRI. Magnetic
Resonance in Medicine, 43(1):72–81.
Gössl, C., Auer, D. P., and Fahrmeir, L. (2001). Bayesian spatio-temporal inference in
functional magnetic resonance imaging. Biometrics, 57:554–562.
Green, P. J. (1987). Penalized likelihood for general semi-parametric regression models.
International Statistical Review/Revue Internationale de Statistique, pages 245–259.
Hafez, M. M., ¯Oshima, K., and Kwak, D. (2010). Computational ﬂuid dynamics review
2010. World Scientiﬁc.
Hammersley, J. and Cliﬀord, P. (1971).
Markov ﬁelds on ﬁnite graphs and lattices.
Unpublished.
Han, I., Malioutov, D., and Shin, J. (2015).
Large-scale log-determinant computation
through stochastic Chebyshev expansions. In Blei, D. and Bach, F., editors, Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pages 908–917.
JMLR Workshop and Conference Proceedings.
Harville, D. A. (1974).
Bayesian inference for variance components using only error
contrasts. Biometrika, 61(2):383–385.
XIV

References
Harville, D. A. (1997). Matrix Algebra From a Statistician’s Perspective. Springer New
York.
Hastie, T. and Tibshirani, R. (1993). Varying-coeﬃcient models. Journal of the Royal
Statistical Society. Series B (Methodological), 55(4):757–796.
Hastings, W. K. (1970). Monte carlo sampling methods using markov chains and their
applications. Biometrika, 57(1):97–109.
Helmuth, J. A., Paul, G., and Sbalzarini, I. F. (2010). Beyond co-localization: inferring
spatial interactions between sub-cellular structures from microscopy images.
BMC
bioinformatics, 11(1).
Herskovits, E., Bryan, R., and Yang, F. (2008).
Automated Bayesian segmentation of
microvascular white-matter lesions in the ACCORD-MIND study. Advances in medical
sciences, 53(2):182.
Hestenes, M. R. and Stiefel, E. (1952). Methods of conjugate gradients for solving linear
systems, volume 49. NBS.
Hoerl, A. E. and Kennard, R. W. (1970).
Ridge regression:
Biased estimation for
nonorthogonal problems. Technometrics, 12(1):55–67.
Holmes, C. C. and Held, L. (2006). Bayesian auxiliary variable models for binary and
multinomial regression. Bayesian Anal., 1(1):145–168.
Hutchinson, M. F. (1990). A stochastic estimator of the trace of the inﬂuence matrix for
Laplacian smoothing splines. Communications in Statistics-Simulation and Computation,
19(2):433–450.
Hutton, C., Draganski, B., Ashburner, J., and Weiskopf, N. (2009).
A comparison
between voxel-based cortical thickness and voxel-based morphometry in normal aging.
Neuroimage, 48(2):371–380.
Ilić, M., Turner, I., and Anh, V. (2008).
A numerical solution using an adaptively
preconditioned Lanczos method for a class of linear systems related with the fractional
Poisson equation. International Journal of Stochastic Analysis, vol. 2008:26 pages.
Ilić, M., Turner, I. W., and Simpson, D. P. (2010). A restarted Lanczos approximation to
functions of a symmetric matrix. IMA Journal of Numerical Analysis, 30(4):1044–1061.
Illian, J. B., Sørbye, S. H., and Rue, H. (2012).
A toolbox for ﬁtting complex spatial
point process models using integrated nested laplace approximation (inla). The Annals
XV

References
of Applied Statistics, pages 1499–1530.
Johnstone, I. M. and Titterington, D. M. (2009). Statistical challenges of high-dimensional
data.
Philosophical Transactions of the Royal Society of London A: Mathematical,
Physical and Engineering Sciences, 367(1906):4237–4253.
Khayati, R., Vafadust, M., Towhidkhah, F., and Nabavi, M. (2008).
Fully automatic
segmentation of multiple sclerosis lesions in brain MR FLAIR images using adaptive
mixtures method and Markov random ﬁeld model. Computers in biology and medicine,
38(3):379–390.
Klein, N., Kneib, T., and Lang, S. (2015). Bayesian generalized additive models for location,
scale, and shape for zero-inﬂated and overdispersed count data. Journal of the American
Statistical Association, 110(509):405–419.
Kneib, T. (2006). Mixed model based inference in structured additive regression. Dr. Hut
Verlag, Munich.
Knorr-Held, L. (1999).
Conditional prior proposals in dynamic models.
Scandinavian
Journal of Statistics, 26(1):129–144.
Knorr-Held, L. (2000). Bayesian modelling of inseparable space-time variation in disease
risk. Statistics in Medicine, 19(17-18):2555–2567.
Knorr-Held, L. and Rue, H. (2002). On block updating in Markov random ﬁeld models for
disease mapping. Scandinavian Journal of Statistics, 29(4):597–614.
Koch-Henriksen, N. and Sørensen, P. S. (2010).
The changing demographic pattern of
multiple sclerosis epidemiology. The Lancet Neurology, 9(5):520–532.
Krige, D. (1951). A statistical approach to some basic mine valuation problems on the
witwatersrand.
Journal of the Chemical, Metallurgical and Mining Society of South
Africa, 52(6):119–139.
Lachmanovich, E., Shvartsman, D., Malka, Y., Botvin, C., Henis, Y., and Weiss, A. (2003).
Co-localization analysis of complex formation among membrane proteins by computerized
ﬂuorescence microscopy: application to immunoﬂuorescence co-patching studies. Journal
of microscopy, 212(2):122–131.
Lang, S. and Brezger, A. (2004). Bayesian P-splines. Journal of computational and graphical
statistics, 13(1):183–212.
XVI

References
LeSage, J. P. and Pace, R. K. (2009). Introduction to Spatial Econometrics. Chapman &
Hall/CRC.
Liesen, J. and Strakos, Z. (2012). Krylov subspace methods: principles and analysis. Oxford
University Press.
Lin, X. and Zhang, D. (1999). Inference in generalized additive mixed modelsby using
smoothing splines.
Journal of the Royal Statistical Society:
Series B (Statistical
Methodology), 61(2):381–400.
Lindgren, F., Rue, H., and Lindström, J. (2011). An explicit link between Gaussian ﬁelds
and Gaussian Markov random ﬁelds: the stochastic partial diﬀerential equation approach.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(4):423–
498.
Lindquist, M. A. et al. (2008). The statistical analysis of fMRI data. Statistical Science,
23(4):439–464.
Manders, E., Stap, J., Brakenhoﬀ, G., Van Driel, R., and Aten, J. (1992). Dynamics of
three-dimensional replication patterns during the S-phase, analysed by double labelling
of DNA and confocal microscopy. Journal of cell science, 103(3):857–862.
Manders, E., Verbeek, F., and Aten, J. (1993). Measurement of co-localization of objects
in dual-colour confocal images. Journal of microscopy, 169(3):375–382.
Markaki, Y., Smeets, D., Fiedler, S., Schmid, V. J., Schermelleh, L., Cremer, T.,
and Cremer, M. (2012).
The potential of 3D-FISH and super-resolution structured
illumination microscopy for studies of 3D nuclear architecture. Bioessays, 34(5):412–
426.
Martin, R. J. (1992).
Approximations to the determinant term in Gaussian maximum
likelihood estimation of some spatial models. Communications in Statistics - Theory and
Methods, 22(1):189–205.
Martins, T. G., Simpson, D., Lindgren, F., and Rue, H. (2013). Bayesian computing with
INLA: new features. Computational Statistics & Data Analysis, 67:68–83.
Mason, J. C. and Handscomb, D. C. (2002). Chebyshev polynomials. CRC Press.
McDonald, W. I., Compston, A., Edan, G., Goodkin, D., Hartung, H.-P., Lublin, F. D.,
McFarland, H. F., Paty, D. W., Polman, C. H., Reingold, S. C., et al. (2001).
Recommended diagnostic criteria for multiple sclerosis: guidelines from the international
panel on the diagnosis of multiple sclerosis. Annals of neurology, 50(1):121–127.
XVII

References
Meijerink, J. A. and van der Vorst, H. A. (1977).
An iterative solution method for
linear systems of which the coeﬃcient matrix is a symmetric m-matrix. Mathematics
of computation, 31(137):148–162.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953).
Equation of state calculations by fast computing machines.
The journal of chemical
physics, 21(6):1087–1092.
Meulman, N., Wieling, M., Sprenger, S. A., Stowe, L. A., and Schmid, M. S. (2015). Age
eﬀects in L2 grammar processing as revealed by ERPs and how (not) to study them.
PLoS ONE, 10(12):1–27.
Minka, T. P. (2001).
Expectation propagation for approximate Bayesian inference.
In
Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence, UAI ’01,
pages 362–369, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Monteiro, R. D., O’Neal, J. W., and Nemirovski, A. (2004). A new conjugate gradient
algorithm incorporating adaptive ellipsoid preconditioning.
Report, School of ISyE,
Georgia Tech, USA.
Moore, G. E. et al. (1975). Progress in digital integrated electronics. In Electron Devices
Meeting, volume 21, pages 11–13.
Mortazavi, D., Kouzani, A. Z., and Soltanian-Zadeh, H. (2012). Segmentation of multiple
sclerosis lesions in MR images: a review. Neuroradiology, 54(4):299–320.
Nelder, J. A. and Wedderburn, R. W. M. (1972). Generalized linear models. Journal of the
Royal Statistical Society. Series A, 135:370 – 384.
Ohkura, K., Nishizawa, H., Obi, T., Hasegawa, A., Yamaguchi, M., and Ohyama, N. (2000).
Unsupervised image segmentation using hierarchical clustering. Optical Review, 7(3):193–
198.
O’Leary, D. P. (1987). Parallel implementation of the block conjugate gradient algorithm.
Parallel Computing, 5(1):127–139.
O’Leary, D. P. (1991). Yet another polynomial preconditioner for the conjugate gradient
algorithm. Linear algebra and its applications, 154:377–388.
Pace, R. K. and LeSage, J. P. (2004). Chebyshev approximation of log-determinants of
spatial weight matrices. Computational Statistics & Data Analysis, 45(2):179–196.
XVIII

References
Paige, C. C. and Saunders, M. A. (1975). Solution of sparse indeﬁnite systems of linear
equations. SIAM journal on numerical analysis, 12(4):617–629.
Papaspiliopoulos, O. and Roberts, G. O. (2003).
Non-centered parameterisations for
hierarchical models and data augmentation. Bayesian Statistics, 7:307 – 326.
Patterson, H. D. and Thompson, R. (1971). Recovery of inter-block information when block
sizes are unequal. Biometrika, 58(3):545–554.
Penny, W. D., Trujillo-Barreto, N. J., and Friston, K. J. (2005). Bayesian fMRI time series
analysis with spatial priors. NeuroImage, 24(2):350–362.
Pinheiro, J. and Bates, D. (2000). Mixed-eﬀects models in S and S-PLUS. Springer New
York.
Polman, C. H., Reingold, S. C., Banwell, B., Clanet, M., Cohen, J. A., Filippi, M.,
Fujihara, K., Havrdova, E., Hutchinson, M., Kappos, L., et al. (2011). Diagnostic criteria
for multiple sclerosis: 2010 revisions to the McDonald criteria.
Annals of neurology,
69(2):292–302.
R Core Team (2016).
R: A Language and Environment for Statistical Computing.
R
Foundation for Statistical Computing, Vienna, Austria.
Reusken, A. (2001). Approximation of the determinant of large sparse symmetric positive
deﬁnite matrices. SIAM J. Matrix Anal. Appl., 23(3):799–818.
Rigby, R. A. and Stasinopoulos, D. M. (2005). Generalized additive models for location,
scale and shape. Journal of the Royal Statistical Society: Series C (Applied Statistics),
54(3):507–554.
Robert, C. and Casella, G. (2011). A short history of markov chain monte carlo: Subjective
recollections from incomplete data. Statist. Sci., 26(1):102–115.
Rue, H. (2001). Fast sampling of Gaussian Markov random ﬁelds. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 63(2):325–338.
Rue, H. and Held, L. (2005). Gaussian Markov random ﬁelds: theory and applications.
CRC Press.
Rue, H. and Martino, S. (2007). Approximate Bayesian inference for hierarchical Gaussian
Markov random ﬁelds models. Journal of Statistical Planning and Inference, 137:3177–
3192.
XIX

References
Rue, H., Martino, S., and Chopin, N. (2009). Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations. Journal of the royal
statistical society: Series b (statistical methodology), 71(2):319–392.
Rzezak, P., Squarzoni, P., Duran, F. L., de Toledo Ferraz Alves, T., Tamashiro-Duran, J.,
Bottino, C. M., Ribeiz, S., Lotufo, P. A., Menezes, P. R., Scazufca, M., and Busatto, G. F.
(2015). Relationship between brain age-related reduction in gray matter and educational
attainment. PloS one, 10(10):e0140945.
Saad, Y. (1994). ILUT: A dual threshold incomplete LU factorization. Numerical linear
algebra with applications, 1(4):387–402.
Saad, Y. (1995).
Preconditioned Krylov subspace methods for CFD applications.
In
Habashi, W., editor, Solution Techniques for Large-Scale CFD Problems, pages 139–158.
Wiley, New York.
Saad, Y. (2003). Iterative methods for sparse linear systems. Siam.
Saad, Y. (2011). Numerical Methods for Large Eigenvalue Problems. Society for Industrial
and Applied Mathematics.
Saad, Y. and Schultz, M. H. (1986). Gmres: A generalized minimal residual algorithm
for solving nonsymmetric linear systems.
SIAM Journal on scientiﬁc and statistical
computing, 7(3):856–869.
Schmid, V. J. (2004). Bayesianische Raum-Zeit-Modellierung in der Epidemiologie. Dr.
Hut Verlag, Munich.
Schmidt, P., Gaser, C., Arsic, M., Buck, D., Förschler, A., Berthele, A., Hoshi, M., Ilg, R.,
Schmid, V. J., Zimmer, C., Hemmer, B., and Mühlau, M. (2012). An automated tool for
detection of FLAIR-hyperintense white-matter lesions in multiple sclerosis. Neuroimage,
59(4):3774–3783.
Schmidt, P., Mühlau, M., and Schmid, V. (2017). Fitting large-scale structured additive
regression models using Krylov subspace methods.
Computational Statistics & Data
Analysis, 105:59 – 75.
Schmidt, P., Schmid, V. J., Gaser, C., Buck, D., Bührlen, S., Förschler, A., and Mühlau,
M. (2013). Fully Bayesian inference for structural MRI: application to segmentation and
statistical analysis of T2-hypointensities. PloS one, 8(7):e68196.
Schulz, G. (1933). Iterative Berechnung der reziproken Matrix. Zeitschrift für Angewandte
Mathematik und Mechanik, 13:57 – 59.
XX

References
Simmons, J. P., Nelson, L. D., and Simonsohn, U. (2011).
False-positive psychology
undisclosed ﬂexibility in data collection and analysis allows presenting anything as
signiﬁcant. Psychological science, pages 1359–1366.
Simpson, D. P., Turner, I. W., and Pettitt, A. (2007). Fast sampling from a Gaussian
Markov random ﬁeld using Krylov subspace approaches. Technical report, Queensland
University of Technology.
Simpson, D. P., Turner, I. W., Strickland, C. M., and Pettitt, A. N. (2013).
Scalable
iterative methods for sampling from massive Gaussian random vectors. arXiv preprint
arXiv:1312.1476.
Sonneveld, P. (1989). CGS, a fast Lanczos-type solver for nonsymmetric linear systems.
SIAM journal on scientiﬁc and statistical computing, 10(1):36–52.
Thron, C., Dong, S. J., Liu, K. F., and Ying, H. P. (1998).
Padé-Z2 estimator of
determinants. Phys. Rev. D, 57:1642–1653.
Thron, C., Liu, K., and Dong, S. (1996). The PZ method for estimating determinant ratios,
with applications. Nuclear Physics. B, Proceedings Supplements, pages 977 – 979.
Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the
Royal Statistical Society. Series B (Methodological), pages 267–288.
Tierney, L. and Kadane, J. B. (1986). Accurate approximations for posterior moments and
marginal densities. Journal of the American Statistical Association, 81(393):82–86.
Tofts, P. S. (1997). Modeling tracer kinetics in dynamic Gd-DTPA MR imaging. Journal
of Magnetic Resonance Imaging, 7(1):91–101.
van den Eshof, J. and Hochbruck, M. (2006). Preconditioning Lanczos approximations to
the matrix exponential. SIAM Journal on Scientiﬁc Computing, 27(4):1438–1457.
van Dyk, D. A. and Park, T. (2008).
Partially collapsed gibbs samplers: Theory and
methods. Journal of the American Statistical Association, 103(482):790–796.
van Leemput, K., Maes, F., Vandermeulen, D., Colchester, A., and Suetens, P. (2001).
Automated segmentation of multiple sclerosis lesions by model outlier detection. Medical
Imaging, IEEE Transactions on, 20(8):677–688.
Varga, R. S. (1960). Factorization and normalized iterative methods. In Langer, R. E.,
editor, Boundary Problems in Diﬀerential Equations. University of Wisconsin Press.
XXI

References
Wallis, J. W., Miller, T. R., Lerner, C. A., and Kleerup, E. C. (1989). Three-dimensional
display in nuclear medicine. Medical Imaging, IEEE Transactions on, 8(4):297–230.
Wang, S., Zhu, W., and Liang, Z.-P. (2001).
Shape deformation: Svm regression and
application to medical image segmentation. In ICCV 2001. Proceedings of the Eighth
IEEE International Conference on Computer Vision., volume 2, pages 209–216. IEEE.
Welford, B. (1962).
Note on a method for calculating corrected sums of squares and
products. Technometrics, 4(3):419–420.
Wells III, W. M., Grimson, W. E. L., Kikinis, R., and Jolesz, F. A. (1996). Adaptive
segmentation of MRI data. Medical Imaging, IEEE Transactions on, 15(4):429–442.
West, M. and Harrison, J. (1997). Bayesian Forecasting and Dynamic Models. Springer,
New York, 2nd edition.
Wieling, M., Tomaschek, F., Arnold, D., and Tiede, M. (2015). Investigating dialectal
diﬀerences using articulography. In Proceedings of ICPhS 2015. International Phonetic
Association.
Winkler, G. (2003). Image analysis, random ﬁelds and Markov chain Monte Carlo methods:
a mathematical introduction, volume 27. Springer.
Wood, S. N., Goude, Y., and Shaw, S. (2015). Generalized additive models for large data
sets. Journal of the Royal Statistical Society: Series C (Applied Statistics), 64(1):139–155.
World Health Organization (2008). Atlas: Multiple sclerosis resources in the world 2008.
Worsley, K. J. and Friston, K. J. (1995). Analysis of fMRI time-series revisited—again.
Neuroimage, 2(3):173–181.
Wu, Y., Warﬁeld, S. K., Tan, I. L., Wells, W. M., Meier, D. S., van Schijndel, R. A.,
Barkhof, F., and Guttmann, C. R. (2006). Automated segmentation of multiple sclerosis
lesion subtypes with multichannel MRI. NeuroImage, 32(3):1205–1215.
Xing, F. and Yang, L. (2016). Robust nucleus/cell detection and segmentation in digital
pathology and microscopy images: A comprehensive review. IEEE Reviews in Biomedical
Engineering, 9:234–263.
Yu, Y. and Meng, X.-L. (2011). To center or not to center: That is not the question—
an ancillarity–suﬃciency interweaving strategy (ASIS) for boosting MCMC eﬃciency.
Journal of Computational and Graphical Statistics, 20(3):531–570.
XXII

References
Zhang, R., Czado, C., Sigloch, K., et al. (2013). A Bayesian linear model for the high-
dimensional inverse problem of seismic tomography. The Annals of Applied Statistics,
7(2):1111–1138.
Zhang, Y., Leithead, W., Leith, D., and Walshe, L. (2008). Log-det approximation based on
uniformly distributed seeds and its application to Gaussian process regression. Journal
of Computational and Applied Mathematics, 220(1–2):198 – 214.
Zhang, Y. and Leithead, W. E. (2007). Approximate implementation of the logarithm of the
matrix determinant in Gaussian process regression. journal of Statistical Computation
and Simulation, 77(4):329–348.
XXIII


Eidesstattliche Versicherung
(Siehe Promotionsordnung vom 12. Juli 2011, § 8 Abs. 2 Pkt. 5)
Hiermit erkläre ich an Eides statt, dass die Dissertation von mir selbstständig,
ohne unerlaubte Beihilfe angefertigt ist.
Berlin, den 04. November 2016
Paul Schmidt


