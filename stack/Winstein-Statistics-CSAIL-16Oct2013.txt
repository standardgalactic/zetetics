And the Bayesians and the frequentists
shall lie down together. . .
Keith Winstein
MIT CSAIL
October 16, 2013

Axioms of Probability (1933)
S: a ﬁnite set (the sample space)
A: any subset of S (an event)
P(A): the probability of A satisﬁes
◮P(A) ∈R
◮P(A) ≥0
◮P(S) = 1
◮P(A ∪B) = P(A) + P(B) if A ∩B = ∅
If S inﬁnite, axiom becomes: for an inﬁnite sequence of disjoint subsets A1, A2, . . .,
P
 ∞
[
i=1
Ai
!
=
∞
X
i=1
P(Ai)

Some Theorems
◮P(A) = 1 −P(A)
◮P(∅) = 0
◮P(A) ≤P(B) if A ⊂B
◮P(A) ≤1
◮P(A ∪B) = P(A) + P(B) −P(A ∩B)
◮P(A ∪B) ≤P(A) + P(B)

Joint & Conditional Probability
◮For A, B ⊆S, P(A ∩B) is joint probability of A and B.
◮The conditional probability of A given B in:
P(A|B) = P(A ∩B)
P(B)
◮A and B are independent iﬀP(A ∩B) = P(A)P(B).
◮A, B independent →P(A|B) = P(A).

Bayes’ Theorem
We have:
◮P(A|B) = P(A∩B)
P(B)
◮P(B|A) = P(A∩B)
P(A)
Therefore: P(A ∩B) = P(A|B)P(B) = P(B|A)P(A)
Bayes’ Theorem:
P(A|B) = P(B|A)P(A)
P(B)

On the islands of Ste. Frequentiste and Bayesienne...

On the islands of Ste. Frequentiste and Bayesienne...
The king has been poisoned!

A letter goes out. . .
Dear Governor: Attached is a blood test for proximity to the
poison. It has a 0% rate of false negative and a 1% rate
of false positive. Jail those responsible.
But remember the nationwide law: You must be 95%
certain to send a citizen to jail.

On Ste. Frequentiste:
Test has a 0% rate of false negative and a 1% rate of false positive.
You must be 95% certain to send a citizen to jail.
◮P(Positive | Guilty) = 1
◮P(Negative | Guilty) = 0
◮P(Positive | Innocent) = 0.01
◮P(Negative | Innocent) = 0.99
How to interpret the law?
“We must be 95% certain” →

On Ste. Frequentiste:
Test has a 0% rate of false negative and a 1% rate of false positive.
You must be 95% certain to send a citizen to jail.
◮P(Positive | Guilty) = 1
◮P(Negative | Guilty) = 0
◮P(Positive | Innocent) = 0.01
◮P(Negative | Innocent) = 0.99
How to interpret the law?
“We must be 95% certain” →P(Jail | Innocent) ≤0.05

On Ste. Frequentiste:
Test has a 0% rate of false negative and a 1% rate of false positive.
You must be 95% certain to send a citizen to jail.
◮P(Positive | Guilty) = 1
◮P(Negative | Guilty) = 0
◮P(Positive | Innocent) = 0.01
◮P(Negative | Innocent) = 0.99
How to interpret the law?
“We must be 95% certain” →P(Jail | Innocent) ≤0.05
Can Positive →Jail? Yes.

On Isle Bayesienne:
Test has a 0% rate of false negative and a 1% rate of false positive.
You must be 95% certain to send a citizen to jail.
◮P(Positive | Guilty) = 1
◮P(Negative | Guilty) = 0
◮P(Positive | Innocent) = 0.01
◮P(Negative | Innocent) = 0.99
How to interpret the law?
“We must be 95% certain” →

On Isle Bayesienne:
Test has a 0% rate of false negative and a 1% rate of false positive.
You must be 95% certain to send a citizen to jail.
◮P(Positive | Guilty) = 1
◮P(Negative | Guilty) = 0
◮P(Positive | Innocent) = 0.01
◮P(Negative | Innocent) = 0.99
How to interpret the law?
“We must be 95% certain” →P(Innocent | Jail) ≤0.05

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent | Positive ) =
P(Positive | Innocent) P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent | Positive ) =
P(Positive | Innocent) P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
P(Positive | Innocent) P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
P(Positive | Innocent) P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
P(Positive | Innocent) P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
P(Positive)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
P(Positive|Innocent) P(Innocent)
+ P(Positive|Guilty)
P(Guilty)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
P(Positive|Innocent) P(Innocent)
+ P(Positive|Guilty)
P(Guilty)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
P(Positive|Innocent) P(Innocent)
+ P(Positive|Guilty)
P(Guilty)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
1 - (0.99)
P(Innocent)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
1 - (0.99)
P(Innocent)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
1 - (0.99)
P(Innocent)

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
1 - (0.99)
P(Innocent)
◮P(Innocent) = ???

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
1 - (0.99)
P(Innocent)
◮P(Innocent) = 0.9 →

Isle Bayesienne: the need for prior assumptions
◮“We must be 95% certain” →P(Innocent | Jail) ≤0.05
◮Can Positive →Jail?
◮Apply Bayes’ theorem
P(Innocent |
Jail
) =
(0.01)
P(Innocent)
1 - (0.99)
P(Innocent)
◮P(Innocent) = 0.9 →P(Innocent | Jail) ≈0.08
!!

On the islands of Ste. Frequentiste and Bayesienne...
◮More than 1% of Ste. Frequentiste goes to jail.
◮On Isle Bayesienne, 10% are assumed guilty, but nobody goes
to jail.
◮The disagreement wasn’t about math or how to interpret P().
◮What was it about?

The islanders’ concerns
◮Frequentist cares about the rate of jailings among innocent
people. Concern: overall rate of false positive
◮Bayesian cares about the rate of innocence among jail
inmates. Concern: rate of error among positives
◮The Bayesian had to make an assumption about the overall
probability of innocence.

Quantifying uncertainty
Underlying
parameters

Quantifying uncertainty
Underlying
parameters
th
e
 
t
r
u
th

Quantifying uncertainty
Underlying
parameters
Experiment
th
e
 
t
r
u
th

Quantifying uncertainty

Quantifying uncertainty

Quantifying uncertainty

Quantifying uncertainty

Jewel’s Cookies
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
total
100%
100%
100%
100%

Quantifying cookie jar uncertainty

Quantifying cookie jar uncertainty

Quantifying cookie jar uncertainty

Quantifying cookie jar uncertainty

Quantifying cookie jar uncertainty

Quantifying cookie jar uncertainty

Frequentist inference
A 70% conﬁdence interval method includes the
correct jar with at least 70% probability in the
worst case, no matter what.

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
25%

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
49%

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
69%

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
88%

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
88%
67%

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
88%
87%

Making 70% conﬁdence intervals
Cookie jars A, B, C, D have 100 cookies each, but diﬀerent
numbers of chocolate chips per cookie:
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
88%
87%
70%

Bayesian inference
A 70% credible interval has at least 70%
conditional probability of including the correct jar,
given the observation and the prior
assumptions.

Uniform prior
Our prior assumption: jars A, B, C, and D have
equal probability.

Conditional probabilities
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
total
100%
100%
100%
100%

Conditional probabilities
P( chips | jar )
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
total
100%
100%
100%
100%

Joint probabilities under uniform prior
P( chips ∩jar )
A
B
C
D
P(chips)
0
1/4
12/4
13/4
27/4
13.25%
1
1/4
19/4
20/4
70/4
27.50%
2
70/4
24/4
0/4
1/4
23.75%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%
total P(jar)
25%
25%
25%
25%
100%

Joint probabilities under uniform prior
P( chips ∩jar )
A
B
C
D
P(chips)
0
1/4
12/4
13/4
27/4
13.25%
1
1/4
19/4
20/4
70/4
27.50%
2
70/4
24/4
0/4
1/4
23.75%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%
total P(jar)
25%
25%
25%
25%
100%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1/4
12/4
13/4
27/4
13.25%
1
1/4
19/4
20/4
70/4
27.50%
2
70/4
24/4
0/4
1/4
23.75%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
1/4
19/4
20/4
70/4
27.50%
2
70/4
24/4
0/4
1/4
23.75%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
1/4
19/4
20/4
70/4
27.50%
2
70/4
24/4
0/4
1/4
23.75%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
70/4
24/4
0/4
1/4
23.75%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
70/4
24/4
0/4
1/4
23.75%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
73.7
25.3
0.0
1.1
100%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
73.7
25.3
0.0
1.1
100%
3
28/4
20/4
0/4
1/4
12.25%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
73.7
25.3
0.0
1.1
100%
3
57.1
40.8
0.0
2.0
100%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
73.7
25.3
0.0
1.1
100%
3
57.1
40.8
0.0
2.0
100%
4
0/4
25/4
67/4
1/4
23.25%

Posterior probabilities under uniform prior
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
73.7
25.3
0.0
1.1
100%
3
57.1
40.8
0.0
2.0
100%
4
0.0
26.9
72.0
1.1
100%

Posterior probabilities under uniform prior
P( jar | chips )
A
B
C
D
P(chips)
0
1.9
22.6
24.5
50.9
100%
1
0.9
17.3
18.2
63.6
100%
2
73.7
25.3
0.0
1.1
100%
3
57.1
40.8
0.0
2.0
100%
4
0.0
26.9
72.0
1.1
100%

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
1
0.9
17.3
18.2
63.6
2
73.7
25.3
0.0
1.1
3
57.1
40.8
0.0
2.0
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
51%
1
0.9
17.3
18.2
63.6
2
73.7
25.3
0.0
1.1
3
57.1
40.8
0.0
2.0
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
75%
1
0.9
17.3
18.2
63.6
2
73.7
25.3
0.0
1.1
3
57.1
40.8
0.0
2.0
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
75%
1
0.9
17.3
18.2
63.6
64%
2
73.7
25.3
0.0
1.1
3
57.1
40.8
0.0
2.0
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
75%
1
0.9
17.3
18.2
63.6
82%
2
73.7
25.3
0.0
1.1
3
57.1
40.8
0.0
2.0
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
75%
1
0.9
17.3
18.2
63.6
82%
2
73.7
25.3
0.0
1.1
74%
3
57.1
40.8
0.0
2.0
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
75%
1
0.9
17.3
18.2
63.6
82%
2
73.7
25.3
0.0
1.1
74%
3
57.1
40.8
0.0
2.0
57%
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
75%
1
0.9
17.3
18.2
63.6
82%
2
73.7
25.3
0.0
1.1
74%
3
57.1
40.8
0.0
2.0
98%
4
0.0
26.9
72.0
1.1

70% credible intervals
P( jar | chips )
A
B
C
D
credibility
0
1.9
22.6
24.5
50.9
75%
1
0.9
17.3
18.2
63.6
82%
2
73.7
25.3
0.0
1.1
74%
3
57.1
40.8
0.0
2.0
98%
4
0.0
26.9
72.0
1.1
72%

Conﬁdence & credible intervals together
conﬁdence
A
B
C
D
credibility
0
1
12
13
27
0%
1
1
19
20
70
99%
2
70
24
0
1
99%
3
28
20
0
1
41%
4
0
25
67
1
99%
coverage
70%
88%
87%
70%
credible
A
B
C
D
credibility
0
1
12
13
27
75%
1
1
19
20
70
82%
2
70
24
0
1
74%
3
28
20
0
1
98%
4
0
25
67
1
72%
coverage
98%
20%
100%
97%

Correlation of error

Correlation of error

Correlation of error

Criticism of frequentist style
Why Most Published Research Findings Are False, Ioannidis JPA,
PLoS Medicine Vol. 2, No. 8, e124
doi:10.1371/journal.pmed.0020124

Criticism of Bayesian style

Disagreement in the real world
◮Avandia: world’s #1 diabetes drug, approved in 1999.
◮Sold by GlaxoSmithKline PLC
◮Sales: $3 billion in 2006
◮In 2004, GSK releases results of many small studies.

GSK releases 42 small studies
Study
Avandia heart attacks
Control heart attacks
49632-020
2/391
1/207
49653-211
5/110
2/114
DREAM
15/2635
9/2634
49653-134
0/561
2/276
49653-331
0/706
0/325
...
...
...

In 2007, Dr. Nissen crashes the party

Frequentist inference


GlaxoSmithKline loses $12 billion
0
200
400
600
800
1000
1200
1400
1600
1800
2000
2002
2004
2006
2008
2010
2012
GBP (millions)
Avandia worldwide sales

Bayesian inference disagrees, for risk ratio
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
P.D.F. on Avandia’s risk ratio for heart attack
(Joint work with Joshua Mandel, Children’s Hospital Informatics Program)

Or does it? Here, risk diﬀerence model
0
100
200
300
400
500
600
700
800
-0.004
-0.003
-0.002
-0.001
0
0.001
0.002
0.003
0.004
P.D.F. on Avandia’s risk diﬀerence for heart attack
(Joint work with Joshua Mandel, Children’s Hospital Informatics Program)

The TAXUS ATLAS Experiment
◮Boston Scientiﬁc proposed to show that new heart stent was
not “inferior” to old heart stent, with 95% conﬁdence.
◮Inferior means three percentage points more “bad” events.
◮Control 7% vs. Treatment 10.5% ⇒inferior
◮Control 7% vs. Treatment 9.5% ⇒non-inferior.

ATLAS Results (May 2006)
May 16, 2006 — NATICK, Mass. and PARIS, May 16
/PRNewswire-FirstCall/ — Boston Scientiﬁc Corporation today
announced nine-month data from its TAXUS ATLAS clinical trial.
[. . . ] The trial met its primary endpoint of nine-month target
vessel revascularization (TVR), a measure of the eﬀectiveness of a
coronary stent in reducing the need for a repeat procedure.

ATLAS Results (April 2007)
Turco et al., Polymer-Based, Paclitaxel-Eluting TAXUS Libert´e
Stent in De Novo Lesions, Journal of the American College of
Cardiology, Vol. 49, No. 16, 2007.
Results: The primary non-inferiority end point was met with the
1-sided 95% conﬁdence bound of 2.98% less than the pre-speciﬁed
non-inferiority margin of 3% (p = 0.0487).
Statistical methodology. Student t test was used to compare
independent continuous variables, while chi-square or Fisher exact
test was used to compare proportions.

p-value
p < 0.05 →95% conﬁdence interval excludes inferiority

The problem
Event
No event
Total
Control
67
889
956
Treatment
68
787
855
Total
135
1676
1811

The problem
Event
No event
Total
Control
67
889
956
Treatment
68
787
855
Total
135
1676
1811
◮With uniform prior on rates,
Pr(inferior | data) ≈0.050737979 . . .
◮Posterior probability of non-inferiority is less than 95%.

ATLAS trial solution
◮Conﬁdence interval: approximate each binomial separately
with a normal distribution. Known as Wald interval.
◮p =
R ∞
0.03 N

i
m −j
n, i(m−i)
m3
+ j(n−j)
n3

≈0.0487395 . . .
◮p < 0.05 →success

The ultimate close call
Wald’s area (≈p) with (m, n) = (855, 956)
4.5
3.8
3.1
2.6
2.2
5.5
4.7
3.9
3.3
2.8
6.7
5.7
4.9
4.1
3.5
8.1
7.0
6.0
5.1
4.3
9.7
8.4
7.2
6.2
5.3
66
67
68
69
70
65
66
67
68
69
TVR (Liberte)
TVR (Express)

conﬁdence
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
88%
87%
70%

conﬁdence
A
B
C
D
0
1
12
13
27
1
1
19
20
70
2
70
24
0
1
3
28
20
0
1
4
0
25
67
1
coverage
70%
88%
87%
70%
false positive rate
30%
12%
13%
30%

The Wald interval undercovers
4.6
4.8
5
5.2
5.4
(2,5)
(4,7)
(6,9)
(8,11)
(10,13)
(12,15)
(14,17)
(16,19)
(18,21)
(20,23)
False Positive Rate (%)
(πTVRe, πTVRℓ) (%)
False Positive Rate of ATLAS non-inferiority test along critical line

Better approximation: score interval
4.6
4.8
5
5.2
5.4
(2,5)
(4,7)
(6,9)
(8,11)
(10,13)
(12,15)
(14,17)
(16,19)
(18,21)
(20,23)
False Positive Rate (%)
(πTVRe, πTVRℓ) (%)
False Positive Rate of maximum-likelihood z-test along critical line

Other methods all yield failure
Method
p-value or conﬁdence bound
Result
Wald interval
p = 0.04874
Pass
z-test, constrained max likelihood standard error
p = 0.05151
Fail

Other methods all yield failure
Method
p-value or conﬁdence bound
Result
Wald interval
p = 0.04874
Pass
z-test, constrained max likelihood standard error
p = 0.05151
Fail
z-test with Yates continuity correction
c = 0.03095
Fail
Agresti-Caﬀo I4 interval
p = 0.05021
Fail
Wilson score
c = 0.03015
Fail
Wilson score with continuity correction
c = 0.03094
Fail
Farrington & Manning score
p = 0.05151
Fail
Miettinen & Nurminen score
p = 0.05156
Fail
Gart & Nam score
p = 0.05096
Fail
NCSS’s bootstrap method
c = 0.03006
Fail
NCSS’s quasi-exact Chen
c = 0.03016
Fail
NCSS’s exact double-binomial test
p = 0.05470
Fail
StatXact’s approximate unconditional test of non-inferiority
p = 0.05151
Fail
StatXact’s exact unconditional test of non-inferiority
p = 0.05138
Fail
StatXact’s exact CI based on diﬀerence of observed rates
c = 0.03737
Fail
StatXact’s approximate CI from inverted 2-sided test
c = 0.03019
Fail
StatXact’s exact CI from inverted 2-sided test
c = 0.03032
Fail

Nerdiest chart contender?


The statistician says. . .
◮“np > 5, therefore, the Central Limit Theorem applies and a
Gaussian approximation is appropriate.”
◮“We had even more data points than we powered the study
for, so there was adequate safety margin.”
◮“‘Exact’ tests are too conservative.”

StatXact calculates an “exact” test
“Other statistical applications often rely on large-scale assumptions for
inferences, risking incorrect conclusions from data sets not normally
distributed. StatXact utilizes Cytel’s own powerful algorithms to make
exact inferences. . . ”

StatXact calculates an “exact” test
“Other statistical applications often rely on large-scale assumptions for
inferences, risking incorrect conclusions from data sets not normally
distributed. StatXact utilizes Cytel’s own powerful algorithms to make
exact inferences. . . ”

Graphing the coverage
◮Problem: hard to calculate a million “exact” p-values
◮(StatXact: about 10 minutes each)
◮Contribution: method for calculating whole tableau
◮Calculates all p-values in time for “hardest” square
◮Trick: calculate in the right order, cache partial results

StatXact calculates an “exact” test
“Other statistical applications often rely on large-scale assumptions for
inferences, risking incorrect conclusions from data sets not normally
distributed. StatXact utilizes Cytel’s own powerful algorithms to make
exact inferences. . . ”

StatXact calculates an “exact” test
“Other statistical applications often rely on large-scale assumptions for
inferences, risking incorrect conclusions from data sets not normally
distributed. StatXact utilizes Cytel’s own powerful algorithms to make
exact inferences. . . ”
4.6
4.8
5
5.2
5.4
(2,5)
(4,7)
(6,9)
(8,11)
(10,13)
(12,15)
(14,17)
(16,19)
(18,21)
(20,23)
False Positive Rate (%)
(πTVRe, πTVRℓ) (%)
Type I rate of StatXAct 8 non-inferiority test (Berger Boos-adjusted Chan)

Animation

Barnard’s test for N = 256 × 257

Assume a prior hypothesis. . .

Frequentists can beneﬁt from priors too!

Final thoughts
◮Bayesian and frequentist schools have much in common.
◮If stark disagreement between Bayesian and frequentist
methods, probably sign of bigger problems!
◮What’s important: say what we’re trying to infer, how we
get there, what we care about.

